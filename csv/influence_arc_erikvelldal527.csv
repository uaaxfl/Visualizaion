2004.tmi-1.2,P92-1005,0,0.0634629,"nent communication is in terms of sets of MRSs and, thus, can easily be managed in a distributed and (potentially) parallel client – server set-up. Both the analysis and generation grammars ‘publish’ their interface to transfer—i.e. the inventory and synopsis of semantic predicates— in the form of a Semantic Interface specification (‘SEM-I’), such that transfer can operate without knowledge about grammar internals. In practical terms SEM-Is are an important 1 In this respect, MRS is closely related to a tradition of underspecified semantics reflected in, among others, Quasi-Logical Form (QLF; Alshawi & Crouch, 1992), Underspecified Discourse Representation Theory (UDRT; Reyle, 1993), Hole Semantics (Bos, 1995), and the Constraint Language for Lambda Structures (CLLS; Egg, Koller, & Niehren, 2001). development tool (facilitating wellformedness testing of interface representations at all levels), but they also have interesting theoretical status with regard to transfer. The SEM-Is for the Norwegian analysis and English generation grammars, respectively, provide an exhaustive enumeration of legitimate semantic predicates (i.e. the transfer vocabulary) and ‘terms of use’, i.e. for each predicate its set of a"
2004.tmi-1.2,C96-1023,0,0.0187876,"o multiple phases and optionally apply output filters upon the completion of each phase. The LOGON transfer grammar, for example, includes two sets of language-specific MTRs to accommodate grammar-specific idiosyncrasies before and after the core transfer phase, in some cases simply suppressing superfluous information (e.g. predicates introduced by selected-for prepositions and some aspectual markers), in others re-arranging or augmenting semantics to facilitate English generation. Transfer outputs incorporating plural mass nouns, for example, require the insertion of a suitable ‘classifier’ (Bond, Ogura, & Ikehara, 1996), in order to generate, say, two pieces of information instead of the ungrammatical ∗ two informations. temp loc at p temp in p temp temp abstr on p temp afternoon n day n ··· year n Figure 4: Excerpt from predicate hierarchies provided by English SEM-I. Temporal, directional, and other usages of prepositions give rise to distinct, but potentially related, semantic predicates. Likewise, the SEM-I incorporates some ontological information, e.g. a classification of temporal entities, though crucially only to the extent that is actually grammaticized in the language proper. create a non-determin"
2004.tmi-1.2,1995.tmi-1.2,1,0.818746,"Missing"
2004.tmi-1.2,P00-1061,0,0.0112111,"Missing"
2005.mtsummit-papers.15,I05-1015,1,0.850946,"phrases (e.g. by using bleu or other string-similarity measures typically used for evaluation) prior to training might be a well-suited approach also for building models for realization ranking. In initial experiments, however, we were unable to improve ranker performance over the results reported here when training our MaxEnt model against a graded distribution, although we have not yet obtained conclusive results for this set115 up. More practically, the way our realization rankers actually get deployed in the LOGON system is by means of selective unpacking from the packed generator forest: Carroll & Oepen (2005) present the unpacking procedure in full detail, but quite obviously there is a tradeoﬀ between the ability to prune competing but dis-preferred realizations early, on the one hand, and improved realization ranking accuracy obtained from feature templates that take into account structural properties of larger constituents, on the other hand. Acknowledgements This paper has beneﬁted in many ways from feedback given by members of the LOGON research team at Oslo University, speciﬁcally Francis Bond and Dan Flickinger. Comments given to a preliminary report on the underlying research by the audien"
2005.mtsummit-papers.15,N04-1021,0,0.0769829,"Missing"
2005.mtsummit-papers.15,2004.tmi-1.2,1,0.884139,"Missing"
2005.mtsummit-papers.15,C02-2025,1,0.802474,"t we use for evaluating the diﬀerent statistical rankers and also for training the MaxEnt models. In order to select a preferred surface realization we want a conditional model that gives us the probability of a string given its semantics. It is worth noting that the problem of realization ranking in many ways can be seen as ‘inversely similar’ to the problem of parse selection, i.e. choosing the best analysis for a given string. Our work on constructing models for realization draws heavily on the previous work on parse disambiguation in relation to the HPSG Redwoods2 treebank, as reported by Oepen et al. (2002). 2 See ‘http://www.delph-in.net/redwoods/’ more information about the Redwoods project. for 110 Stochastic models for parse selection are typically trained on a treebank consisting of strings paired with their optimal analyses. When training the discriminative models (described in Section 3.2) for realization selection we use a treebank where this optimality relation is taken to be bidirectional in the sense that the original string is also treated as an optimal realization of the corresponding semantic analysis (i.e. ‘meaning’). For each input, the Redwoods treebank provides a full HPSG anal"
2005.mtsummit-papers.15,C00-1085,0,0.0507954,"ypes (‘Combined’). The data items are binned with respect to number of distinct realizations. others as plain wrong. In realization ranking, on the other hand, it is perhaps more meaningful to think of a graded continuum of more or less natural verbalizations (given an input semantics). All outputs of the lkb realizer are semantically equivalent and guaranteed to be well-formed with respect to the underlying grammar. This means that the kind of properties we aim at capturing with the discriminative model are soft constraints that govern the degree of ‘correctness’ among competing paraphrases. Osborne (2000) and Malouf & Noord (2004) describe an approach to parse disambiguation using maximum entropy models where the empirical distribution that deﬁnes the constraints for the model are not based on frequency counts from a corpus but rather some measure of similarity towards the reference. Deﬁning such a preference weighting of the candidate paraphrases (e.g. by using bleu or other string-similarity measures typically used for evaluation) prior to training might be a well-suited approach also for building models for realization ranking. In initial experiments, however, we were unable to improve rank"
2005.mtsummit-papers.15,P99-1069,0,0.357477,"rkers). We then rank the realizations by computing their negative log-probabilities with respect to the model. In other words, the score of a string with k tokens, score(w1k ), is computed as − ln pn (w1k ) =  − ki=1 ln pn (wi |wi−n , . . . , wi−1 ). After training and testing several language models for varying values of n, we ended up using an 4-gram model (backing-oﬀ for unobserved n-grams) for the results reported here. 3.2 A Maximum Entropy Ranker Log-linear models provide a very ﬂexible framework that has been widely used for a range of tasks in NLP, including parse selection (see e.g. Johnson, Geman, Canon, Chi, & Riezler, 1999; Malouf & Noord, 2004) and reranking for machine translation (see e.g. Och et al., 2004). A model is speciﬁed by a set of real-valued feature functions that describe properties of the data, and an associated set of learned weights that determine the contribution of each feature. Given a set of d such features, each realization r is represented as a feature vector f (r) ∈ d , and a vector of weights λ ∈ d is then ﬁtted to optimize the likelihood of the training data. A conditional log-linear model for the probability of a realization r given the semantics s, has the general parametric form"
2005.mtsummit-papers.15,P02-1040,0,0.0725411,"trained on tiny amounts of data can compete favorably on the realization ranking task when compared to a n-gram language model trained on a large text corpus. In the current paper we train and evaluate rankers on an expanded treebank and using a richer inventory of feature 109 types. Three models are described: a traditional surface-oriented n-gram model, a maximum entropy model using structural features, and a combination of these two. We evaluate the diﬀerent models, as well as the utility of individual feature types, by comparing exact match accuracy and averaged per-sentence bleu scores (Papineni, Roukos, Ward, & Zhu, 2002). The paper is organized as follows. In Section 2 we brieﬂy review our notion of symmetric treebanks and the properties of the data set used for our experiments. Section 3 describes the three models that we train, including the feature types of the maximum entropy (MaxEnt) models. An evaluation of the performance of the diﬀerent models is presented in Section 4, before we go on to discuss the results and sketch directions for ongoing work in Section 5. 2 Background and Data The LOGON system has an architecture based on semantic transfer that uses meaning representations based on Minimal Recur"
2005.mtsummit-papers.15,A00-2021,0,0.0590806,"ined best overall performance. When instantiating all feature templates as described above our models contain close to 65000 features. 3.4 A Combined Model The second MaxEnt model is a combination of the two models described in Section 3.1 and 3.2 above; in addition to the set of structural feature types it includes as a separate feature the sentence scores computed by the n-gram language model. In other words, the value of the d + 1’th feature is the log-probability of the string as given by the n-gram model pn , i.e. fd+1 (r) = ln pn (y(r)), where y(r) is the yield of r and n = 4 as before. Johnson & Riezler (2000) show an interesting equivalence between using log-probabilities as features and using a geometric mixture of the same probabilities for the default distribution q of Equation 1 (where the λ-parameters of the features would correspond to their weights in the mixture). This means that a special case of the simple combined model we present here would be a MEMD model where the uniform distribution q is replaced by the language model pn . If λd+1 = 1 then exp(fd+1 λd+1 ) = pn and we would eﬀectively have a MEMD model as described above with q = pn . Both of the log-linear models described in this"
2005.mtsummit-papers.15,W02-2030,0,0.0415427,"KLdiveregence) between the model and the reference distribution D(pλ ||q) on the one hand, and between the empirical distribution and the model D(˜ p ||pλ ) on the other. Given a MaxEnt model pλ , the scores used for ranking the candidate realizations can be com puted simply as score(r) = i λi fi (r) since we are only interested in the rank order. 3.3 Maximum Entropy Features The ﬁrst MaxEnt model that we trained uses structural features deﬁned over HPSG derivation trees as summarized in Table 3. For the purpose of parse selection, Toutanova, Manning, Shieber, Flickinger, & Oepen (2002) and Toutanova & Manning (2002) train a discriminative log-linear model on the Redwoods parse treebank, using features deﬁned over derivation trees with non-terminals representing the construction types and lexical types of the HPSG 112 # 1 1 1 1 1 2 2 3 3 3 4 4 4 0 1 0 1 2 0 0 1 2 3 1 2 3 sample features subjh hspec third sg fin verb  subjh hspec third sg fin verb hspec det the le sing noun subjh hspec det the le sing noun  subjh hspec det the le sing noun subjh third sg fin verb hspec sing noun n intr le dog det the le n intr le dog  det the le n intr le dog n intr le det the le n intr le  det"
2005.mtsummit-papers.15,W98-1426,0,0.0543788,"(excluding items with only one realization), average string length, and average structural ambiguity. The rightmost column shows a random choice baseline, i.e. the probability of selecting the preferred realization by chance. go on to look at the two maximum entropy or log-linear models that we train using structural features from the symmetric treebank described in the previous section. 3.1 A Language Model Ranker The ﬁrst statistical model that we apply for ranking the generator outputs is an n-gram language model.3 This approach is in many ways similar to those presented by, among others, Langkilde & Knight (1998) and White (2004) and quite generally still appears predominant in the realization ranking literature. The model is trained on an unannotated version of the British National Corpus (BNC), containing roughly 100 million words. As the realizations in our symmetric treebank also include punctuations, these are also treated as separate tokens by the language model (in addition to sentence boundary markers). We then rank the realizations by computing their negative log-probabilities with respect to the model. In other words, the score of a string with k tokens, score(w1k ), is computed as − ln pn ("
2005.mtsummit-papers.15,W02-2018,0,0.0213258,"ations of s. The so-called reference or default distribution q is often only implicit since in maximum entropy estimation this is just the constant function |Y 1(s) |(for a given s). One can, however, also replace this uniform distribution by some other reference distribution to incorporate prior knowledge in the model. This approach is also known as maximum entropy / minimum divergence (MEMD) modeling, and we will return to this more general framework below. The estimation4 of the λ-parameters seek to maximize the (log of) a penalized likelihood 4 3  We use the estimate open-source package (Malouf, 2002) for training, using its limited-memory variable metric as the optimization method. subjh hspec third sg fin verb det the le sing noun v unerg le the n intr le barks dog Figure 1: Sample HPSG derivation tree for the input the dog barks. Phrasal nodes are labeled with identiﬁers of grammar rules, and (pre-terminal) lexical nodes with class names for types of lexical entries. function as in (3) ˆ = arg max log L(λ) − λ λ d 2 i=1 λi 2σ 2 where L(λ) is the ‘conditionalized’ likelihood of the training data (as described by Johnson et al., 1999), computed as L(λ) = N i=1 pλ (ri |si ). The second t"
2007.tmi-papers.18,I05-1015,1,0.851038,"as a feature in the model. TRANSFER METRICS Two additional features capture information about the transfer step: the total number of transfer rules that were invoked (as a measure of transfer granularity, e.g. where idiomatic transfer of a larger cluster of EPs contrasts with stepwise transfer of component EPs), as well as the ratio of EP counts, |E|/|F |. SEMANTIC DISTANCE Generation proceeds in two phases: a chart-based bottom-up search enumerates candidate realizations, of which a final semantic compatiblity test selects the one(s) whose MRS is subsumed by the original generator input MRS (Carroll & Oepen, 2005). Given an imperfect input (or error in the generation grammar), it is possible for none of the candidate outputs to fulfill the semantic compatiblity test. In this case, the generator will gradually relax MRS com150 parison, going through seven pre-defined levels of semantic mismatch, which we encode as one integer-valued feature in the re-ranking model. Training the Model While batch translating, the LOGON controller records all candidate translations, intermediate semantic representations, and a large number of processing and resource consumption properties in a database, which we call a pr"
2007.tmi-papers.18,W97-1502,0,0.0125593,"on to the 109 items that translate in both configurations, our BLEU score over the first translation drops from 37.41 to 30.29.1 MRS { prpstn m[MARG recommend v] recommend v[ARG1 pron, ARG2 hike n] a q[ARG0 hike n] around p[ARG1 hike n, ARG2 source n] implicit q[ARG0 source n] poss[ARG1 waterway n, ARG2 source n] def q[ARG0 waterway n] } Figure 4: Variable-free reduction of the MRS for the utterance ‘We recommend a hike around the waterway’s sources’. ing set; this could be done automatically. The second approach is motivated by the hypothesis that discriminants, as used in manual annotation (Carter, 1997), represent promising alternative feature functions to the predefined templates. Initial tests (see table 2) show that the discriminant approach (which is not yet used in the LOGON system) scores better than the templatebased approach. 5 Ranking Transfer Outputs While MRS formulae are highly structured graphs, Oepen & Lønning (2006) suggest a reduction into a variable-free form that resembles elementary dependency structures. For the ranking of transfer outputs, MRSs are broken down into basic dependency triples, whose probabilities are estimated by adaptation of standard n-gram sequence model"
2007.tmi-papers.18,P99-1069,0,0.0438269,"nd a given feature can even itself be a separate statistical model. In the following we first give a brief high-level presentation of conditional log-linear modeling, and then we go on to present the actual feature functions in our setup. Given a set of m real-valued features, each pair of source sentence f and target sentence e are represented as a feature vector Φ(f, e) ∈ ℜm . A vector of weights λ ∈ ℜm is then fitted to optimize some objective function of the training data. For the experiments reported in this paper the weights are fitted to maximize the conditional (or pseudo) likelihood (Johnson, Geman, Canon, Chi, & Riezler, 1999).3 In other words, for each input source sentence in the training data we seek to maximize 3 For estimation we use the TADM open-source toolkit (Malouf, 2002), using its limited-memory variable metric as the optimization method. As is standard practice, the model is regularized by including a zero-mean Gaussian prior on the feature weights to reduce the risk of overfitting. 149 the probability of its annotated reference translation relative to the other competing candidates. However, for future work we plan to also experiment with optimizing the scores of a given evaluation metric (e.g. BLEU)"
2007.tmi-papers.18,koen-2004-pharaoh,0,0.0252225,"ise word-to-word probabilities. STRING PROBABILITY Although a part of the (conditional) realization ranker already, we include the string probability (according to the tri4 Of these, 9,410 sentences are taken from the LOGON development data, while an additional 12,946 sentences are from the English-Norwegian Parallel Corpus (Oksefjell, 1999). 5 The ML estimation of the lexical probabilities, as well as the final word alignments produced from the output of GIZA++ , are carried out using the training scripts provided by Phillip Koehn, and as distributed with the phrase-based SMT module Pharaoh (Koehn, 2004). gram language model trained on the BNC) of candidate translations ek as an independent indicator of output fluency. DISTORTION Elementary predications (EPs) in our MRS are linked to corresponding surface elements, i.e. sub-string pointers. Surface links are preserved in transfer, such that post-generation, for each EP—or group of EPs, as transfer need not be a one-to-one mapping—there is information about its original vs. its output sub-string span. To gauge reordering among constituents, for both the generator input and output, each EP is compared pairwise to other EPs in the same MRS, and"
2007.tmi-papers.18,W98-1426,0,0.119221,"Missing"
2007.tmi-papers.18,W02-2018,0,0.0150677,"resent the actual feature functions in our setup. Given a set of m real-valued features, each pair of source sentence f and target sentence e are represented as a feature vector Φ(f, e) ∈ ℜm . A vector of weights λ ∈ ℜm is then fitted to optimize some objective function of the training data. For the experiments reported in this paper the weights are fitted to maximize the conditional (or pseudo) likelihood (Johnson, Geman, Canon, Chi, & Riezler, 1999).3 In other words, for each input source sentence in the training data we seek to maximize 3 For estimation we use the TADM open-source toolkit (Malouf, 2002), using its limited-memory variable metric as the optimization method. As is standard practice, the model is regularized by including a zero-mean Gaussian prior on the feature weights to reduce the risk of overfitting. 149 the probability of its annotated reference translation relative to the other competing candidates. However, for future work we plan to also experiment with optimizing the scores of a given evaluation metric (e.g. BLEU) directly, following the Minimum Error Rate approach of Och (2003). The three most fundamental features that are supplied in our log-linear re-ranker correspon"
2007.tmi-papers.18,P03-1021,0,0.0280051,"e training data we seek to maximize 3 For estimation we use the TADM open-source toolkit (Malouf, 2002), using its limited-memory variable metric as the optimization method. As is standard practice, the model is regularized by including a zero-mean Gaussian prior on the feature weights to reduce the risk of overfitting. 149 the probability of its annotated reference translation relative to the other competing candidates. However, for future work we plan to also experiment with optimizing the scores of a given evaluation metric (e.g. BLEU) directly, following the Minimum Error Rate approach of Och (2003). The three most fundamental features that are supplied in our log-linear re-ranker correspond to the three ranking modules of the baseline system, as described in Sections § 4, § 5, and § 6 above. In other words, these features record the scores of the parse ranker, the MRS ranker, and the realization ranker, respectively. But our re-ranker also includes several other features that are not part of the baseline model. Other Features Our experiments so far have taken into account another eight properties of the translation process, in some cases observing internal features of individual compone"
2007.tmi-papers.18,P02-1038,0,0.0471966,"mony between the string lengths of the source and target. Log-linear models provide a very flexible framework for discriminative modeling that allows us to combine disparate and overlapping sources of information in a single model without running the risk of making unwarranted independence assumptions. In this section we describe a model that directly estimates the posterior translation probability Pλ (e|f ), for a given source sentence f and translation e. Although the re-ranker we describe here is built on top of a hybrid baseline system, the overall approach is similar to that described by Och & Ney (2002) in the context of SMT. Log-Linear Models A log-linear model is given in terms of (a) a set of specified features that describe properties of the data, and (b) an associated set of learned weights that determine the contribution of each feature. One advantage of working with a discriminative re-ranking setup is that the model can use global features that the baseline system would not be able to incorporate. The information that the feature functions record can be arbitrarily complex, and a given feature can even itself be a separate statistical model. In the following we first give a brief hig"
2007.tmi-papers.18,2005.eamt-1.27,1,0.855742,"Missing"
2007.tmi-papers.18,2004.tmi-1.2,1,0.736592,"Missing"
2007.tmi-papers.18,oepen-lonning-2006-discriminant,1,0.843509,"waterway n] } Figure 4: Variable-free reduction of the MRS for the utterance ‘We recommend a hike around the waterway’s sources’. ing set; this could be done automatically. The second approach is motivated by the hypothesis that discriminants, as used in manual annotation (Carter, 1997), represent promising alternative feature functions to the predefined templates. Initial tests (see table 2) show that the discriminant approach (which is not yet used in the LOGON system) scores better than the templatebased approach. 5 Ranking Transfer Outputs While MRS formulae are highly structured graphs, Oepen & Lønning (2006) suggest a reduction into a variable-free form that resembles elementary dependency structures. For the ranking of transfer outputs, MRSs are broken down into basic dependency triples, whose probabilities are estimated by adaptation of standard n-gram sequence modeling techniques. The actual training is done using the freely available CMU SLM toolkit (Clarkson & Rosenfeld, 1997). Based on a training set of some 8,500 indomain MRSs, viz. the treebanked version of the English translations of the (full) LOGON development corpus, our target language ‘semantic model’ is defined as a smoothed tri-gr"
2007.tmi-papers.18,C00-1085,0,0.0240505,"xample by assessing the relative contribution of individual features, fine-tuning parameter estimation, and including additional properties. Our current maximum likelihood training of the log-linear model is based on a binarized empirical distribution, where for each input we consider the candidate translation(s) with maximum NEVA score(s) as preferred, and all others as dis-preferred. Obviously, however, the degradation in quality among alternate candidates is continuous (rather than absolute), and we have started experimentation with a graded empirical distribution, adapting the approach of Osborne (2000) to the re-ranking task. Finally, in a parallel refinement cycle, we aim to contrast our current ( LL) re-ranking model with Minimum Error Rate (MER) training, a method that aims to estimate model parameters to directly optimize BLEU scores (or another quality metric) as its objective function. Trading coverage for increased output quality may be economic for a range of tasks—say as a complement to other tools in the workbench of a professional translator. Our re-ranking approach, with access to rich intermediate representations, probabilities, and confidence measures, provides a fertile envir"
2007.tmi-papers.18,P02-1040,0,0.0737594,"the speaker had intended. Our argument for the first translation can be illustrated within our earlier example of a wordlevel noun vs. verb ambiguity in analysis. The many different realizations of the noun in the target language may fall into classes of near synonyms, in which case it does not matter for the quality of the result which synonym is chosen. Even though each of the individual realizations has a low probability, it may be a good translation. Observe here also that an automatic evaluation measure—measuring the similarities to a set of reference translations, like the BLEU metric (Papineni, Roukos, Ward, & Zhu, 2002)—will favor the view of most likely translation. We conjecture, however, that a human evaluation will correspond better to the first translation. From a theoretical point of view, it seems most correct to go for the first translation. But it presupposes that we choose the correct interpretation of the source sentence, which we cannot expect to always do. In cases where we have chosen an incorrect analysis, this might be revealed by trying to translate it into the target language and consider the result. If all the candidate translations sound bad—or have a very low probability—in the target l"
2007.tmi-papers.18,N06-1032,0,0.0685787,"Missing"
2007.tmi-papers.18,W04-3223,0,0.0275221,"t matches and matches among the five top-ranked analyses. Figures in parentheses show a random choice baseline. Both models were trained on seven of nine treebanked texts and evaluated on the two remaining texts. was used to build a treebank for the LOGON development corpus. Parse selection in LOGON uses training data from this treebank; all sentences with full parses with low ambiguity (fewer than 100 readings) were at least partially disambiguated. The parse selection method employed in the LOGON demonstrator uses the stochastic disambiguation scheme and training software developed at PARC (Riezler & Vasserman, 2004). The XLE system provides a set of parameterized feature function templates that must be expanded in accordance with the grammar or the training set at hand. Application of these feature functions to the training data yields feature forests for both the labeled data (the partially disambiguated parse forests) and the unlabeled data (the full parse forests). These feature forests are the input to the statistical estimation algorithm, which generates a property weights file that is used to rank solutions. One of the challenges in applying the probability model to a given grammar and training set"
2020.gebnlp-1.11,W19-3805,0,0.232853,"Missing"
2020.gebnlp-1.11,W19-3809,0,0.269824,"s systems, which seemed to give higher sentiment predictions for sentences associated with one given race or gender. Hoyle et al. (2019) use a generative latent-variable model to represent collocations of positive and negative adjective and verb choices, given a gendered head noun. Their analyses goes beyond qualitative analysis, and shed light on the differences on how men and women are described differently. They use a corpus of books spanning various genres, and show for example that positive adjectives used to describe women are related to their bodies more often than is the case for men. Bhaskaran and Bhallamudi (2019) analyse the existence of occupational gender stereotypes in sentiment analysis models. They show that all their tested models (BOW+logistic regression, BiLSTM, BERT (Devlin et al., 2019)) contain occupational gender stereotypes to some extent. They also show that simple models seem to show biases in training data, while contextual models might reflect biases introduced while pretraining. Voigt et al. (2018) present an annotated corpus for the gender of the addressee and the sentiment and relevance of comments. The corpus comprised comments from responses to Facebook and Reddit comments, TED t"
2020.gebnlp-1.11,2020.acl-main.485,0,0.0503272,"Missing"
2020.gebnlp-1.11,D11-1120,0,0.0496621,"idely studied source of bias in textual content (Garimella and Mihalcea, 2016; Schofield and Mehr, 2016; Kiritchenko and Mohammad, 2018). There has been considerable previous work analyzing gender bias in NLP models and in particular, in input representations such as static and contextualized word embeddings (Kaneko and Bollegala, 2019; Friedman et al., 2019; Bolukbasi et al., 2016; Zhao et al., 2020; Basta et al., 2019). Gender-annotated datasets largely focus on the gender of the author of a specific piece of text, such as a blog (Mukherjee and Liu, 2010; Liu and Mihalcea, 2007) or a tweet (Burger et al., 2011) and has given rise to considerable research focused on author gender identification (Mukherjee and Liu, 2010; Rangel and Rosso, 2019). Datasets which enable the study of response to gender in text, however, are considerably fewer (Voigt et al., 2018). With a few noteworthy exceptions (Zhao et al., 2020; Sahlgren and Olsson, 2019), a majority of previous work has focused on gender modeling and the study of gender bias in English. Social psychological research on gender bias in language has shown that there are sociocultural stereotypes inherent in the language used to describe females and male"
2020.gebnlp-1.11,2020.acl-main.418,0,0.0707233,"Missing"
2020.gebnlp-1.11,2020.lrec-1.502,0,0.246408,"Missing"
2020.gebnlp-1.11,N19-1423,0,0.0247156,"ons of positive and negative adjective and verb choices, given a gendered head noun. Their analyses goes beyond qualitative analysis, and shed light on the differences on how men and women are described differently. They use a corpus of books spanning various genres, and show for example that positive adjectives used to describe women are related to their bodies more often than is the case for men. Bhaskaran and Bhallamudi (2019) analyse the existence of occupational gender stereotypes in sentiment analysis models. They show that all their tested models (BOW+logistic regression, BiLSTM, BERT (Devlin et al., 2019)) contain occupational gender stereotypes to some extent. They also show that simple models seem to show biases in training data, while contextual models might reflect biases introduced while pretraining. Voigt et al. (2018) present an annotated corpus for the gender of the addressee and the sentiment and relevance of comments. The corpus comprised comments from responses to Facebook and Reddit comments, TED talks, and posts on Fitocracy. This work has similarities to ours, since they look at the responses to gender e.g. how the content can differ based on the gender of the person being addres"
2020.gebnlp-1.11,W19-3821,0,0.119578,"Missing"
2020.gebnlp-1.11,W19-3803,0,0.471861,"t there are differences in how critics assess the works of authors of the same or opposite gender. For example, male critics rate crime novels written by females, and romantic and sentimental works written by males, more negatively. 1 Introduction Gender is a widely studied source of bias in textual content (Garimella and Mihalcea, 2016; Schofield and Mehr, 2016; Kiritchenko and Mohammad, 2018). There has been considerable previous work analyzing gender bias in NLP models and in particular, in input representations such as static and contextualized word embeddings (Kaneko and Bollegala, 2019; Friedman et al., 2019; Bolukbasi et al., 2016; Zhao et al., 2020; Basta et al., 2019). Gender-annotated datasets largely focus on the gender of the author of a specific piece of text, such as a blog (Mukherjee and Liu, 2010; Liu and Mihalcea, 2007) or a tweet (Burger et al., 2011) and has given rise to considerable research focused on author gender identification (Mukherjee and Liu, 2010; Rangel and Rosso, 2019). Datasets which enable the study of response to gender in text, however, are considerably fewer (Voigt et al., 2018). With a few noteworthy exceptions (Zhao et al., 2020; Sahlgren and Olsson, 2019), a majo"
2020.gebnlp-1.11,W16-4301,0,0.326765,"ale authors. We investigate if this newly annotated dataset contains differences in how the works of male and female authors are critiqued, in particular in terms of positive and negative sentiment. We also explore the differences in how this is done by male and female critics. We show that there are differences in how critics assess the works of authors of the same or opposite gender. For example, male critics rate crime novels written by females, and romantic and sentimental works written by males, more negatively. 1 Introduction Gender is a widely studied source of bias in textual content (Garimella and Mihalcea, 2016; Schofield and Mehr, 2016; Kiritchenko and Mohammad, 2018). There has been considerable previous work analyzing gender bias in NLP models and in particular, in input representations such as static and contextualized word embeddings (Kaneko and Bollegala, 2019; Friedman et al., 2019; Bolukbasi et al., 2016; Zhao et al., 2020; Basta et al., 2019). Gender-annotated datasets largely focus on the gender of the author of a specific piece of text, such as a blog (Mukherjee and Liu, 2010; Liu and Mihalcea, 2007) or a tweet (Burger et al., 2011) and has given rise to considerable research focused on a"
2020.gebnlp-1.11,W19-3621,0,0.0210253,"ns seem to maintain the existing social hierarchies that tend to focus on emotional traits when describing females, while focusing on competence traits when describing males (Menegatti and Rubini, 2017). 2 Related work Much of the previous work on bias in ML models within NLP has focused on identifying biases in word embeddings and how to mitigate them (Maudslay et al., 2019; Kaneko and Bollegala, 2019; Zmigrod et al., 2019; Friedman et al., 2019; Garg et al., 2018; Bolukbasi et al., 2016), or even make them gender neutral (Zhao et al., 2018b). However, such efforts have received criticism by Gonen and Goldberg (2019) who argue that the biases have not been removed, but only “hidden” and kept at a deeper level in the embedding space. Bias has also been investigated in several other settings, like multilingual embeddings (Zhao et al., 2020), deep contextual representations (Basta et al., 2019; May et al., 2019), language models (Qian et al., 2019), coreference resolution (Cao and Daum´e III, 2020; Zhao et al., 2018a; Rudinger et al., 2018), and machine translation (Escud´e Font and Costa-juss`a, 2019), just to name a few of the more recent efforts. Another line of work has focused on investigating gender re"
2020.gebnlp-1.11,P19-1167,0,0.117481,"Missing"
2020.gebnlp-1.11,P19-1160,0,0.252388,"female critics. We show that there are differences in how critics assess the works of authors of the same or opposite gender. For example, male critics rate crime novels written by females, and romantic and sentimental works written by males, more negatively. 1 Introduction Gender is a widely studied source of bias in textual content (Garimella and Mihalcea, 2016; Schofield and Mehr, 2016; Kiritchenko and Mohammad, 2018). There has been considerable previous work analyzing gender bias in NLP models and in particular, in input representations such as static and contextualized word embeddings (Kaneko and Bollegala, 2019; Friedman et al., 2019; Bolukbasi et al., 2016; Zhao et al., 2020; Basta et al., 2019). Gender-annotated datasets largely focus on the gender of the author of a specific piece of text, such as a blog (Mukherjee and Liu, 2010; Liu and Mihalcea, 2007) or a tweet (Burger et al., 2011) and has given rise to considerable research focused on author gender identification (Mukherjee and Liu, 2010; Rangel and Rosso, 2019). Datasets which enable the study of response to gender in text, however, are considerably fewer (Voigt et al., 2018). With a few noteworthy exceptions (Zhao et al., 2020; Sahlgren an"
2020.gebnlp-1.11,S18-2005,0,0.0918061,"set contains differences in how the works of male and female authors are critiqued, in particular in terms of positive and negative sentiment. We also explore the differences in how this is done by male and female critics. We show that there are differences in how critics assess the works of authors of the same or opposite gender. For example, male critics rate crime novels written by females, and romantic and sentimental works written by males, more negatively. 1 Introduction Gender is a widely studied source of bias in textual content (Garimella and Mihalcea, 2016; Schofield and Mehr, 2016; Kiritchenko and Mohammad, 2018). There has been considerable previous work analyzing gender bias in NLP models and in particular, in input representations such as static and contextualized word embeddings (Kaneko and Bollegala, 2019; Friedman et al., 2019; Bolukbasi et al., 2016; Zhao et al., 2020; Basta et al., 2019). Gender-annotated datasets largely focus on the gender of the author of a specific piece of text, such as a blog (Mukherjee and Liu, 2010; Liu and Mihalcea, 2007) or a tweet (Burger et al., 2011) and has given rise to considerable research focused on author gender identification (Mukherjee and Liu, 2010; Range"
2020.gebnlp-1.11,D19-1530,0,0.0945771,"ed are written by males. Also, words related to achievements with regards to literary genre or the process of publishing in general are positively used when describing the work of males, and negatively for the works of females. These observations seem to maintain the existing social hierarchies that tend to focus on emotional traits when describing females, while focusing on competence traits when describing males (Menegatti and Rubini, 2017). 2 Related work Much of the previous work on bias in ML models within NLP has focused on identifying biases in word embeddings and how to mitigate them (Maudslay et al., 2019; Kaneko and Bollegala, 2019; Zmigrod et al., 2019; Friedman et al., 2019; Garg et al., 2018; Bolukbasi et al., 2016), or even make them gender neutral (Zhao et al., 2018b). However, such efforts have received criticism by Gonen and Goldberg (2019) who argue that the biases have not been removed, but only “hidden” and kept at a deeper level in the embedding space. Bias has also been investigated in several other settings, like multilingual embeddings (Zhao et al., 2020), deep contextual representations (Basta et al., 2019; May et al., 2019), language models (Qian et al., 2019), coreference res"
2020.gebnlp-1.11,N19-1063,0,0.0208207,"ases in word embeddings and how to mitigate them (Maudslay et al., 2019; Kaneko and Bollegala, 2019; Zmigrod et al., 2019; Friedman et al., 2019; Garg et al., 2018; Bolukbasi et al., 2016), or even make them gender neutral (Zhao et al., 2018b). However, such efforts have received criticism by Gonen and Goldberg (2019) who argue that the biases have not been removed, but only “hidden” and kept at a deeper level in the embedding space. Bias has also been investigated in several other settings, like multilingual embeddings (Zhao et al., 2020), deep contextual representations (Basta et al., 2019; May et al., 2019), language models (Qian et al., 2019), coreference resolution (Cao and Daum´e III, 2020; Zhao et al., 2018a; Rudinger et al., 2018), and machine translation (Escud´e Font and Costa-juss`a, 2019), just to name a few of the more recent efforts. Another line of work has focused on investigating gender representations in corpora and models, and release gender-neutral corpora (corpora in which either the distribution of genders is balanced, or where gender stereotypes and gendered words are removed). Schofield and Mehr (2016) use film scripts to analyse the linguistic and structure variations in di"
2020.gebnlp-1.11,D10-1021,0,0.0454859,"itten by males, more negatively. 1 Introduction Gender is a widely studied source of bias in textual content (Garimella and Mihalcea, 2016; Schofield and Mehr, 2016; Kiritchenko and Mohammad, 2018). There has been considerable previous work analyzing gender bias in NLP models and in particular, in input representations such as static and contextualized word embeddings (Kaneko and Bollegala, 2019; Friedman et al., 2019; Bolukbasi et al., 2016; Zhao et al., 2020; Basta et al., 2019). Gender-annotated datasets largely focus on the gender of the author of a specific piece of text, such as a blog (Mukherjee and Liu, 2010; Liu and Mihalcea, 2007) or a tweet (Burger et al., 2011) and has given rise to considerable research focused on author gender identification (Mukherjee and Liu, 2010; Rangel and Rosso, 2019). Datasets which enable the study of response to gender in text, however, are considerably fewer (Voigt et al., 2018). With a few noteworthy exceptions (Zhao et al., 2020; Sahlgren and Olsson, 2019), a majority of previous work has focused on gender modeling and the study of gender bias in English. Social psychological research on gender bias in language has shown that there are sociocultural stereotypes"
2020.gebnlp-1.11,P19-2031,0,0.0185432,"itigate them (Maudslay et al., 2019; Kaneko and Bollegala, 2019; Zmigrod et al., 2019; Friedman et al., 2019; Garg et al., 2018; Bolukbasi et al., 2016), or even make them gender neutral (Zhao et al., 2018b). However, such efforts have received criticism by Gonen and Goldberg (2019) who argue that the biases have not been removed, but only “hidden” and kept at a deeper level in the embedding space. Bias has also been investigated in several other settings, like multilingual embeddings (Zhao et al., 2020), deep contextual representations (Basta et al., 2019; May et al., 2019), language models (Qian et al., 2019), coreference resolution (Cao and Daum´e III, 2020; Zhao et al., 2018a; Rudinger et al., 2018), and machine translation (Escud´e Font and Costa-juss`a, 2019), just to name a few of the more recent efforts. Another line of work has focused on investigating gender representations in corpora and models, and release gender-neutral corpora (corpora in which either the distribution of genders is balanced, or where gender stereotypes and gendered words are removed). Schofield and Mehr (2016) use film scripts to analyse the linguistic and structure variations in dialogues and how these differ based on"
2020.gebnlp-1.11,N18-2002,0,0.05749,"Missing"
2020.gebnlp-1.11,W19-6104,0,0.0375803,"egala, 2019; Friedman et al., 2019; Bolukbasi et al., 2016; Zhao et al., 2020; Basta et al., 2019). Gender-annotated datasets largely focus on the gender of the author of a specific piece of text, such as a blog (Mukherjee and Liu, 2010; Liu and Mihalcea, 2007) or a tweet (Burger et al., 2011) and has given rise to considerable research focused on author gender identification (Mukherjee and Liu, 2010; Rangel and Rosso, 2019). Datasets which enable the study of response to gender in text, however, are considerably fewer (Voigt et al., 2018). With a few noteworthy exceptions (Zhao et al., 2020; Sahlgren and Olsson, 2019), a majority of previous work has focused on gender modeling and the study of gender bias in English. Social psychological research on gender bias in language has shown that there are sociocultural stereotypes inherent in the language used to describe females and males (Menegatti and Rubini, 2017). While the descriptions of females tend to focus on their communal traits, males are described for their agentic traits (Menegatti and Rubini, 2017). Madera et al. (2009) show that the gender of the writer can also influence how females and males are described. They show that gender stereotypes can d"
2020.gebnlp-1.11,W16-0204,0,0.288063,"this newly annotated dataset contains differences in how the works of male and female authors are critiqued, in particular in terms of positive and negative sentiment. We also explore the differences in how this is done by male and female critics. We show that there are differences in how critics assess the works of authors of the same or opposite gender. For example, male critics rate crime novels written by females, and romantic and sentimental works written by males, more negatively. 1 Introduction Gender is a widely studied source of bias in textual content (Garimella and Mihalcea, 2016; Schofield and Mehr, 2016; Kiritchenko and Mohammad, 2018). There has been considerable previous work analyzing gender bias in NLP models and in particular, in input representations such as static and contextualized word embeddings (Kaneko and Bollegala, 2019; Friedman et al., 2019; Bolukbasi et al., 2016; Zhao et al., 2020; Basta et al., 2019). Gender-annotated datasets largely focus on the gender of the author of a specific piece of text, such as a blog (Mukherjee and Liu, 2010; Liu and Mihalcea, 2007) or a tweet (Burger et al., 2011) and has given rise to considerable research focused on author gender identificatio"
2020.gebnlp-1.11,L18-1445,0,0.15174,"tations such as static and contextualized word embeddings (Kaneko and Bollegala, 2019; Friedman et al., 2019; Bolukbasi et al., 2016; Zhao et al., 2020; Basta et al., 2019). Gender-annotated datasets largely focus on the gender of the author of a specific piece of text, such as a blog (Mukherjee and Liu, 2010; Liu and Mihalcea, 2007) or a tweet (Burger et al., 2011) and has given rise to considerable research focused on author gender identification (Mukherjee and Liu, 2010; Rangel and Rosso, 2019). Datasets which enable the study of response to gender in text, however, are considerably fewer (Voigt et al., 2018). With a few noteworthy exceptions (Zhao et al., 2020; Sahlgren and Olsson, 2019), a majority of previous work has focused on gender modeling and the study of gender bias in English. Social psychological research on gender bias in language has shown that there are sociocultural stereotypes inherent in the language used to describe females and males (Menegatti and Rubini, 2017). While the descriptions of females tend to focus on their communal traits, males are described for their agentic traits (Menegatti and Rubini, 2017). Madera et al. (2009) show that the gender of the writer can also influ"
2020.gebnlp-1.11,N18-2003,0,0.0236081,"rk of males, and negatively for the works of females. These observations seem to maintain the existing social hierarchies that tend to focus on emotional traits when describing females, while focusing on competence traits when describing males (Menegatti and Rubini, 2017). 2 Related work Much of the previous work on bias in ML models within NLP has focused on identifying biases in word embeddings and how to mitigate them (Maudslay et al., 2019; Kaneko and Bollegala, 2019; Zmigrod et al., 2019; Friedman et al., 2019; Garg et al., 2018; Bolukbasi et al., 2016), or even make them gender neutral (Zhao et al., 2018b). However, such efforts have received criticism by Gonen and Goldberg (2019) who argue that the biases have not been removed, but only “hidden” and kept at a deeper level in the embedding space. Bias has also been investigated in several other settings, like multilingual embeddings (Zhao et al., 2020), deep contextual representations (Basta et al., 2019; May et al., 2019), language models (Qian et al., 2019), coreference resolution (Cao and Daum´e III, 2020; Zhao et al., 2018a; Rudinger et al., 2018), and machine translation (Escud´e Font and Costa-juss`a, 2019), just to name a few of the mo"
2020.gebnlp-1.11,D18-1521,0,0.042522,"rk of males, and negatively for the works of females. These observations seem to maintain the existing social hierarchies that tend to focus on emotional traits when describing females, while focusing on competence traits when describing males (Menegatti and Rubini, 2017). 2 Related work Much of the previous work on bias in ML models within NLP has focused on identifying biases in word embeddings and how to mitigate them (Maudslay et al., 2019; Kaneko and Bollegala, 2019; Zmigrod et al., 2019; Friedman et al., 2019; Garg et al., 2018; Bolukbasi et al., 2016), or even make them gender neutral (Zhao et al., 2018b). However, such efforts have received criticism by Gonen and Goldberg (2019) who argue that the biases have not been removed, but only “hidden” and kept at a deeper level in the embedding space. Bias has also been investigated in several other settings, like multilingual embeddings (Zhao et al., 2020), deep contextual representations (Basta et al., 2019; May et al., 2019), language models (Qian et al., 2019), coreference resolution (Cao and Daum´e III, 2020; Zhao et al., 2018a; Rudinger et al., 2018), and machine translation (Escud´e Font and Costa-juss`a, 2019), just to name a few of the mo"
2020.gebnlp-1.11,2020.acl-main.260,0,0.279259,"he works of authors of the same or opposite gender. For example, male critics rate crime novels written by females, and romantic and sentimental works written by males, more negatively. 1 Introduction Gender is a widely studied source of bias in textual content (Garimella and Mihalcea, 2016; Schofield and Mehr, 2016; Kiritchenko and Mohammad, 2018). There has been considerable previous work analyzing gender bias in NLP models and in particular, in input representations such as static and contextualized word embeddings (Kaneko and Bollegala, 2019; Friedman et al., 2019; Bolukbasi et al., 2016; Zhao et al., 2020; Basta et al., 2019). Gender-annotated datasets largely focus on the gender of the author of a specific piece of text, such as a blog (Mukherjee and Liu, 2010; Liu and Mihalcea, 2007) or a tweet (Burger et al., 2011) and has given rise to considerable research focused on author gender identification (Mukherjee and Liu, 2010; Rangel and Rosso, 2019). Datasets which enable the study of response to gender in text, however, are considerably fewer (Voigt et al., 2018). With a few noteworthy exceptions (Zhao et al., 2020; Sahlgren and Olsson, 2019), a majority of previous work has focused on gender"
2020.gebnlp-1.11,P19-1161,0,0.0599984,"ievements with regards to literary genre or the process of publishing in general are positively used when describing the work of males, and negatively for the works of females. These observations seem to maintain the existing social hierarchies that tend to focus on emotional traits when describing females, while focusing on competence traits when describing males (Menegatti and Rubini, 2017). 2 Related work Much of the previous work on bias in ML models within NLP has focused on identifying biases in word embeddings and how to mitigate them (Maudslay et al., 2019; Kaneko and Bollegala, 2019; Zmigrod et al., 2019; Friedman et al., 2019; Garg et al., 2018; Bolukbasi et al., 2016), or even make them gender neutral (Zhao et al., 2018b). However, such efforts have received criticism by Gonen and Goldberg (2019) who argue that the biases have not been removed, but only “hidden” and kept at a deeper level in the embedding space. Bias has also been investigated in several other settings, like multilingual embeddings (Zhao et al., 2020), deep contextual representations (Basta et al., 2019; May et al., 2019), language models (Qian et al., 2019), coreference resolution (Cao and Daum´e III, 2020; Zhao et al., 20"
2020.lrec-1.559,Q17-1010,0,0.027588,"Missing"
2020.lrec-1.559,M98-1028,0,0.123751,"not have a unique entity as a reference, but rather exploit an entity as part of their semantics. Even so, if a user of the annotated data wishes to extract all information about a particular named entity, these may still be relevant, hence should be marked separately. Entity types not included Of the categories discussed above, location, person, and organization comprise the core inventory of named entity types in the literature. They formed part of the pioneering shared tasks on NER hosted by CoNLL 2002/2003 (Sang, 2002; Sang and Meulder, 2003), MUC-6 (Grishman and Sundheim, 1995) and MUC7 (Chinchor, 1998), and have been part of all major NER annotation efforts since. However, the CoNLL shared tasks also included a fourth category for names of miscellaneous entities not belonging to the aforementioned three. During the annotation of NorNE we similarly operated with an entity type MISC, but eventually we decided to discard this label in the final release of the data as it was annotated too rarely to be useful in practice (with a total of 8 occurrences in the training data but 0 occurrences in both the development and held-out splits). The MUC shared tasks, on the other hand, additionally include"
2020.lrec-1.559,Q16-1026,0,0.0362688,"appings of the label set, different label encodings (IOB2, etc), different embedding dimensionalities, as well as joint modeling of the Bokmål and Nynorsk variants. Apart from the joint modeling, the other experiments will target only the Bokmål section of the dataset. Before moving on to the results, we first briefly outline the experimental setup. 5.1. Experimental Setup The modeling is performed using NCRF++ (Yang and Zhang, 2018) – a configurable sequence labeling toolkit built upon PyTorch. Following Yang et al. (2018), our particular model configuration is similar to the architecture of Chiu and Nichols (2016) and Lample et al. (2016), achieving results that are close to state-of-the-art for English on the CoNLL-2003 dataset: it combines a characterlevel CNN and a word-level BiLSTM, finally feeding into a CRF inference layer. The input to the word-level BiLSTM is provided by the concatenation of (1) the character sequence representations from the CNN using max-pooling in addition and (2) pre-trained word embeddings from the NLPL vector repository2 (Fares et al., 2017). Further details about the latter are provided in the next section. Across all experiments we fix and re-use the same random seed fo"
2020.lrec-1.559,W17-0237,1,0.824183,"g toolkit built upon PyTorch. Following Yang et al. (2018), our particular model configuration is similar to the architecture of Chiu and Nichols (2016) and Lample et al. (2016), achieving results that are close to state-of-the-art for English on the CoNLL-2003 dataset: it combines a characterlevel CNN and a word-level BiLSTM, finally feeding into a CRF inference layer. The input to the word-level BiLSTM is provided by the concatenation of (1) the character sequence representations from the CNN using max-pooling in addition and (2) pre-trained word embeddings from the NLPL vector repository2 (Fares et al., 2017). Further details about the latter are provided in the next section. Across all experiments we fix and re-use the same random seed for initializing the models, as to reduce the effect of non-determinism, and otherwise fix the parameters to their default values.3 For model evaluation we follow the scheme defined by the SemEval 2013 task 9.1 (Segura-Bedmar et al., 2013), using the re-implementation offered by David S. Batista.4 We report F1 for exact match on the entity level, i.e., both the predicted boundary and entity label must be correct. (This measure was dubbed strict in SemEval 2013 task"
2020.lrec-1.559,M95-1001,0,0.442269,"types of forms is that these words do not have a unique entity as a reference, but rather exploit an entity as part of their semantics. Even so, if a user of the annotated data wishes to extract all information about a particular named entity, these may still be relevant, hence should be marked separately. Entity types not included Of the categories discussed above, location, person, and organization comprise the core inventory of named entity types in the literature. They formed part of the pioneering shared tasks on NER hosted by CoNLL 2002/2003 (Sang, 2002; Sang and Meulder, 2003), MUC-6 (Grishman and Sundheim, 1995) and MUC7 (Chinchor, 1998), and have been part of all major NER annotation efforts since. However, the CoNLL shared tasks also included a fourth category for names of miscellaneous entities not belonging to the aforementioned three. During the annotation of NorNE we similarly operated with an entity type MISC, but eventually we decided to discard this label in the final release of the data as it was annotated too rarely to be useful in practice (with a total of 8 occurrences in the training data but 0 occurrences in both the development and held-out splits). The MUC shared tasks, on the other"
2020.lrec-1.559,W17-0217,1,0.902326,"Missing"
2020.lrec-1.559,N16-1030,0,0.111561,"fferent label encodings (IOB2, etc), different embedding dimensionalities, as well as joint modeling of the Bokmål and Nynorsk variants. Apart from the joint modeling, the other experiments will target only the Bokmål section of the dataset. Before moving on to the results, we first briefly outline the experimental setup. 5.1. Experimental Setup The modeling is performed using NCRF++ (Yang and Zhang, 2018) – a configurable sequence labeling toolkit built upon PyTorch. Following Yang et al. (2018), our particular model configuration is similar to the architecture of Chiu and Nichols (2016) and Lample et al. (2016), achieving results that are close to state-of-the-art for English on the CoNLL-2003 dataset: it combines a characterlevel CNN and a word-level BiLSTM, finally feeding into a CRF inference layer. The input to the word-level BiLSTM is provided by the concatenation of (1) the character sequence representations from the CNN using max-pooling in addition and (2) pre-trained word embeddings from the NLPL vector repository2 (Fares et al., 2017). Further details about the latter are provided in the next section. Across all experiments we fix and re-use the same random seed for initializing the models"
2020.lrec-1.559,markert-nissim-2002-towards,0,0.201038,"e special category DRV. 3.3.2. Ambiguity and Metonymy Ambiguity is a frequent source of doubt when annotating. This is often caused by so-called metonymical usage, where an entity is referred to by the name of another, closely related entity (Lakoff and Johnson, 1980). In the annotation of the NorNE corpus we have tried to resolve the ambiguity and choose the entity type based on the context (the document). We assume that every entity has a base, or literal, meaning and that when there is ambiguity, either genuinely or due to a lack of context, we resort to the literal meaning of the word(s) (Markert and Nissim, 2002). For instance, in the example in (11) below, the context does not clearly indicate whether this is a reference to a geo-political location or organization. We here assume that the location sense is the literal sense of the word Vietnam and that the organization sense is by metonymical usage, hence the annotation is GPE_LOC. (11) VietnamGPE _LOC er flott. Vietnam is great. ‘Vietnam is great.’ 3.4. Annotation Process The annotation of the NorNE corpus was performed by two trained linguists, and all documents in the Bokmål section were doubly annotated. As the second phase of the project, the Ny"
2020.lrec-1.559,L16-1699,0,0.0518445,"Missing"
2020.lrec-1.559,L16-1250,1,0.866112,"Missing"
2020.lrec-1.559,W09-1119,0,0.100265,"rained on NorNE-full using IOB, IOBE, IOBS and IOBES encodings, when evaluated on the NorNE development set, using label sets of different granularities. 5.3.2. Label Encoding The annotations of NorNE are distributed using the standard IOB(2) scheme.5 However, this can be easily mapped to other variations like IOBES, where the extra E-label indicates the end-token of an entity and the S-label indicates single, unit-length entities. (This latter scheme also goes by other names like BIOLU.) Several studies have reported slight performance increases when using the IOBES encoding compared to IOB (Ratinov and Roth, 2009; Yang et al., 2018; Reimers and Gurevych, 2017). However, it is typically not clear whether the benefits stem from adding the Eor S-labels or both. 5.3.3. Results Table 6 report experimental results for all of these variations – i.e. isolating the effects of the E- and S-labels – and across all the three different sets of entity types discussed above. There are several things to notice here. IOBE seems to have a negative performance impact regardless of the chosen label set. Also, compared to the standard IOB encoding, IOBES also has a negative impact paired with NorNEfull, but gives improved"
2020.lrec-1.559,W03-0419,0,0.847037,"r the special treatment of these types of forms is that these words do not have a unique entity as a reference, but rather exploit an entity as part of their semantics. Even so, if a user of the annotated data wishes to extract all information about a particular named entity, these may still be relevant, hence should be marked separately. Entity types not included Of the categories discussed above, location, person, and organization comprise the core inventory of named entity types in the literature. They formed part of the pioneering shared tasks on NER hosted by CoNLL 2002/2003 (Sang, 2002; Sang and Meulder, 2003), MUC-6 (Grishman and Sundheim, 1995) and MUC7 (Chinchor, 1998), and have been part of all major NER annotation efforts since. However, the CoNLL shared tasks also included a fourth category for names of miscellaneous entities not belonging to the aforementioned three. During the annotation of NorNE we similarly operated with an entity type MISC, but eventually we decided to discard this label in the final release of the data as it was annotated too rarely to be useful in practice (with a total of 8 occurrences in the training data but 0 occurrences in both the development and held-out splits)"
2020.lrec-1.559,W02-2024,0,0.473641,"he reason for the special treatment of these types of forms is that these words do not have a unique entity as a reference, but rather exploit an entity as part of their semantics. Even so, if a user of the annotated data wishes to extract all information about a particular named entity, these may still be relevant, hence should be marked separately. Entity types not included Of the categories discussed above, location, person, and organization comprise the core inventory of named entity types in the literature. They formed part of the pioneering shared tasks on NER hosted by CoNLL 2002/2003 (Sang, 2002; Sang and Meulder, 2003), MUC-6 (Grishman and Sundheim, 1995) and MUC7 (Chinchor, 1998), and have been part of all major NER annotation efforts since. However, the CoNLL shared tasks also included a fourth category for names of miscellaneous entities not belonging to the aforementioned three. During the annotation of NorNE we similarly operated with an entity type MISC, but eventually we decided to discard this label in the final release of the data as it was annotated too rarely to be useful in practice (with a total of 8 occurrences in the training data but 0 occurrences in both the develop"
2020.lrec-1.559,solberg-etal-2014-norwegian,1,0.83317,"Missing"
2020.lrec-1.559,E12-2021,0,0.113855,"Missing"
2020.lrec-1.559,W17-0201,1,0.902569,"Missing"
2020.lrec-1.559,P18-4013,0,0.0148249,"rimental Results and Analysis In this section we present some preliminary experimental results for named entity recognition using NorNE. We investigate the effects of using different mappings of the label set, different label encodings (IOB2, etc), different embedding dimensionalities, as well as joint modeling of the Bokmål and Nynorsk variants. Apart from the joint modeling, the other experiments will target only the Bokmål section of the dataset. Before moving on to the results, we first briefly outline the experimental setup. 5.1. Experimental Setup The modeling is performed using NCRF++ (Yang and Zhang, 2018) – a configurable sequence labeling toolkit built upon PyTorch. Following Yang et al. (2018), our particular model configuration is similar to the architecture of Chiu and Nichols (2016) and Lample et al. (2016), achieving results that are close to state-of-the-art for English on the CoNLL-2003 dataset: it combines a characterlevel CNN and a word-level BiLSTM, finally feeding into a CRF inference layer. The input to the word-level BiLSTM is provided by the concatenation of (1) the character sequence representations from the CNN using max-pooling in addition and (2) pre-trained word embeddings"
2020.lrec-1.559,C18-1327,0,0.0774336,"for named entity recognition using NorNE. We investigate the effects of using different mappings of the label set, different label encodings (IOB2, etc), different embedding dimensionalities, as well as joint modeling of the Bokmål and Nynorsk variants. Apart from the joint modeling, the other experiments will target only the Bokmål section of the dataset. Before moving on to the results, we first briefly outline the experimental setup. 5.1. Experimental Setup The modeling is performed using NCRF++ (Yang and Zhang, 2018) – a configurable sequence labeling toolkit built upon PyTorch. Following Yang et al. (2018), our particular model configuration is similar to the architecture of Chiu and Nichols (2016) and Lample et al. (2016), achieving results that are close to state-of-the-art for English on the CoNLL-2003 dataset: it combines a characterlevel CNN and a word-level BiLSTM, finally feeding into a CRF inference layer. The input to the word-level BiLSTM is provided by the concatenation of (1) the character sequence representations from the CNN using max-pooling in addition and (2) pre-trained word embeddings from the NLPL vector repository2 (Fares et al., 2017). Further details about the latter are"
2020.lrec-1.618,L16-1429,0,0.026579,"Missing"
2020.lrec-1.618,P19-2035,0,0.0284519,"reatly, state-ofthe-art methods for fine-grained sentiment analysis tend to be transfer-learning approaches (Chen and Qian, 2019), often using pre-trained language models (Peters et al., 2018; Devlin et al., 2019) to improve model performance (Hu et al., 2019). Additionally, several approaches use multitask learning to incorporate useful information from similar tasks into fine-grained sentiment models (Marasovi´c and Frank, 2018; He et al., 2019; Li et al., 2019). Alternatively, researchers have proposed attention-based methods which are adapted for fine-grained sentiment (Tang et al., 2019; Bao et al., 2019). These methods make use of an attention mechanism (Bahdanau et al., 2014) which allows the model to learn a weighted representation of sentences with respect to sentiment targets. Finally, there are approaches which create task-specific models for fine-grained sentiment. Liang et al. (2019) propose an aspect-specific gate to improve GRUs. 3. Annotations In the following we present our fine-grained sentiment annotation effort in more detail. We provide an overview of the annotation guidelines and present statistics on interannotator agreement. The complete set of guidelines is distributed with"
2020.lrec-1.618,L18-1104,1,0.847384,"olar Expression Polarity Type Positive Negative Evaluative Fact-Implied Non-Personal Intensity Slight Standard Strong Target H ol de r Not-On-Topic Re l Strong Postitive Target Relation Denne this TARGET disken disk er is P OLAR svært stillegående very quiet-going Figure 2: Annotation of an EVAL sentence (transl. ‘This disk runs very quietly’). Target-is-General Implicit at io n Holder Not-First-Person Implicit Figure 1: Annotation schema for the NoReCfine dataset. ally, there has been an increased effort to create fine-grained resources for low-resource languages, such as Basque and Catalan (Barnes et al., 2018). No datasets for fine-grained SA have previously been created for Norwegian, however. 2.2. Modeling Fine-grained sentiment is most often approached as a sequence labeling problem (Yang and Cardie, 2013; Vo and Zhang, 2015) or simplified to a classification problem when the target or aspect is given (Pontiki et al., 2014). Although the specific architectures vary greatly, state-ofthe-art methods for fine-grained sentiment analysis tend to be transfer-learning approaches (Chen and Qian, 2019), often using pre-trained language models (Peters et al., 2018; Devlin et al., 2019) to improve model pe"
2020.lrec-1.618,P19-1052,0,0.0246947,"creased effort to create fine-grained resources for low-resource languages, such as Basque and Catalan (Barnes et al., 2018). No datasets for fine-grained SA have previously been created for Norwegian, however. 2.2. Modeling Fine-grained sentiment is most often approached as a sequence labeling problem (Yang and Cardie, 2013; Vo and Zhang, 2015) or simplified to a classification problem when the target or aspect is given (Pontiki et al., 2014). Although the specific architectures vary greatly, state-ofthe-art methods for fine-grained sentiment analysis tend to be transfer-learning approaches (Chen and Qian, 2019), often using pre-trained language models (Peters et al., 2018; Devlin et al., 2019) to improve model performance (Hu et al., 2019). Additionally, several approaches use multitask learning to incorporate useful information from similar tasks into fine-grained sentiment models (Marasovi´c and Frank, 2018; He et al., 2019; Li et al., 2019). Alternatively, researchers have proposed attention-based methods which are adapted for fine-grained sentiment (Tang et al., 2019; Bao et al., 2019). These methods make use of an attention mechanism (Bahdanau et al., 2014) which allows the model to learn a wei"
2020.lrec-1.618,N19-1423,0,0.0256568,"Basque and Catalan (Barnes et al., 2018). No datasets for fine-grained SA have previously been created for Norwegian, however. 2.2. Modeling Fine-grained sentiment is most often approached as a sequence labeling problem (Yang and Cardie, 2013; Vo and Zhang, 2015) or simplified to a classification problem when the target or aspect is given (Pontiki et al., 2014). Although the specific architectures vary greatly, state-ofthe-art methods for fine-grained sentiment analysis tend to be transfer-learning approaches (Chen and Qian, 2019), often using pre-trained language models (Peters et al., 2018; Devlin et al., 2019) to improve model performance (Hu et al., 2019). Additionally, several approaches use multitask learning to incorporate useful information from similar tasks into fine-grained sentiment models (Marasovi´c and Frank, 2018; He et al., 2019; Li et al., 2019). Alternatively, researchers have proposed attention-based methods which are adapted for fine-grained sentiment (Tang et al., 2019; Bao et al., 2019). These methods make use of an attention mechanism (Bahdanau et al., 2014) which allows the model to learn a weighted representation of sentences with respect to sentiment targets. Finally, there"
2020.lrec-1.618,W17-0237,1,0.867604,"This naturally leads to a lossy representation of the original data, as the relations, nested annotations, and polar intensity are ignored. Our model uses a single BiLSTM layer (100 dim.) to extract features and then a CRF layer to make predictions. We train the model using Adam (Kingma and Ba, 2014) for 40 epochs with a patience of 5, and use dropout to regularize both the BiLSTM (0.5) and CRF (0.3) layers. The word embeddings are 100 dimensional fastText SkipGram (Bojanowski et al., 2016) vectors trained on the NoWaC corpus (Guevara, 2010) and made available from the NLPL vector repository (Fares et al., 2017).3 The pre-trained embeddings are further fine-tuned during training. We report held-out test results for the model that achieves the best performance on the development set and use the standard train/development/test split provided with the dataset (shown in Table 3). All results are reported using the Proportional and Binary precision, recall and F1 scores, computed as described in Section 3.6 above. 5.2. 2 In follow-up work we plan to further enrich the annotations with additional compositional information relevant to sentiment, most importantly negation but also other forms of valence shif"
2020.lrec-1.618,W10-1501,0,0.039701,"the binary polarity of the latter, giving us nine tags in total.2 This naturally leads to a lossy representation of the original data, as the relations, nested annotations, and polar intensity are ignored. Our model uses a single BiLSTM layer (100 dim.) to extract features and then a CRF layer to make predictions. We train the model using Adam (Kingma and Ba, 2014) for 40 epochs with a patience of 5, and use dropout to regularize both the BiLSTM (0.5) and CRF (0.3) layers. The word embeddings are 100 dimensional fastText SkipGram (Bojanowski et al., 2016) vectors trained on the NoWaC corpus (Guevara, 2010) and made available from the NLPL vector repository (Fares et al., 2017).3 The pre-trained embeddings are further fine-tuned during training. We report held-out test results for the model that achieves the best performance on the development set and use the standard train/development/test split provided with the dataset (shown in Table 3). All results are reported using the Proportional and Binary precision, recall and F1 scores, computed as described in Section 3.6 above. 5.2. 2 In follow-up work we plan to further enrich the annotations with additional compositional information relevant to s"
2020.lrec-1.618,P19-1048,0,0.0654472,"; Vo and Zhang, 2015) or simplified to a classification problem when the target or aspect is given (Pontiki et al., 2014). Although the specific architectures vary greatly, state-ofthe-art methods for fine-grained sentiment analysis tend to be transfer-learning approaches (Chen and Qian, 2019), often using pre-trained language models (Peters et al., 2018; Devlin et al., 2019) to improve model performance (Hu et al., 2019). Additionally, several approaches use multitask learning to incorporate useful information from similar tasks into fine-grained sentiment models (Marasovi´c and Frank, 2018; He et al., 2019; Li et al., 2019). Alternatively, researchers have proposed attention-based methods which are adapted for fine-grained sentiment (Tang et al., 2019; Bao et al., 2019). These methods make use of an attention mechanism (Bahdanau et al., 2014) which allows the model to learn a weighted representation of sentences with respect to sentiment targets. Finally, there are approaches which create task-specific models for fine-grained sentiment. Liang et al. (2019) propose an aspect-specific gate to improve GRUs. 3. Annotations In the following we present our fine-grained sentiment annotation effort in"
2020.lrec-1.618,P19-1051,0,0.0704886,"ts for fine-grained SA have previously been created for Norwegian, however. 2.2. Modeling Fine-grained sentiment is most often approached as a sequence labeling problem (Yang and Cardie, 2013; Vo and Zhang, 2015) or simplified to a classification problem when the target or aspect is given (Pontiki et al., 2014). Although the specific architectures vary greatly, state-ofthe-art methods for fine-grained sentiment analysis tend to be transfer-learning approaches (Chen and Qian, 2019), often using pre-trained language models (Peters et al., 2018; Devlin et al., 2019) to improve model performance (Hu et al., 2019). Additionally, several approaches use multitask learning to incorporate useful information from similar tasks into fine-grained sentiment models (Marasovi´c and Frank, 2018; He et al., 2019; Li et al., 2019). Alternatively, researchers have proposed attention-based methods which are adapted for fine-grained sentiment (Tang et al., 2019; Bao et al., 2019). These methods make use of an attention mechanism (Bahdanau et al., 2014) which allows the model to learn a weighted representation of sentences with respect to sentiment targets. Finally, there are approaches which create task-specific model"
2020.lrec-1.618,P16-1087,0,0.0418271,"Missing"
2020.lrec-1.618,W16-0410,0,0.0247285,"the distribution of polarity labels and their intensity scores. We see that the intensities are clearly dominated by standard strength, while there are also 1270 strong labels for positive. Regardless of intensity, we see that positive valence is more prominent than negative, and this reflects a similar skew for the document-level ratings in this data (Velldal et al., 2018). The slight intensity is infrequent, with 190 positive and 330 negative polar expressions with this label. The relative difference here can be explained by the tendency to hedge negative statements more than positive ones (Kiritchenko and Mohammad, 2016). Slight negative is the minority class, with only 190 examples, followed by strong negative with 292 examples. Overall, the distribution of intensity Figure 9: Distribution of labels and intensities. scores in NoReCfine is very similar to what is reported for other fine-grained sentiment datasets for English and Dutch (Van de Kauter et al., 2015). As we can see from Table 3, the average number of tokens spanned by a polar expression is 4.6. Interestingly, if we break this number down further, we find that the negative expressions are on average longer than the positives for all intensities: w"
2020.lrec-1.618,klinger-cimiano-2014-usage,0,0.0798025,"r annotation scheme is followed by Van de Kauter et al. (2015), working on financial news texts in Dutch and English, also taking account of implicit expressions of sentiment in polar facts. The SemEval 2014 shared task (Pontiki et al., 2014) proposes a different annotation scheme. Given an English tweet, the annotators identify targets, the aspect category they belong to, and the polarity expressed towards the target. They do not annotate holders or polar expressions. While most fine-grained sentiment datasets are in English, there are datasets available in several languages, such as German (Klinger and Cimiano, 2014), Czech (Steinberger et al., 2014), Arabic, Chinese, Dutch, French, Russian, Spanish, Turkish (Pontiki et al., 2016), Hungarian (Szabó et al., 2016), and Hindi (Akhtar et al., 2016). Addition5025 Polar Expression Polarity Type Positive Negative Evaluative Fact-Implied Non-Personal Intensity Slight Standard Strong Target H ol de r Not-On-Topic Re l Strong Postitive Target Relation Denne this TARGET disken disk er is P OLAR svært stillegående very quiet-going Figure 2: Annotation of an EVAL sentence (transl. ‘This disk runs very quietly’). Target-is-General Implicit at io n Holder Not-First-Pers"
2020.lrec-1.618,N16-1030,0,0.027119,"eriments To provide an idea of the difficulty of the task, here we report some preliminary experimental results for the new dataset, intended as benchmarks for further experiments. Casting the problem as a sequence labeling task, we train a model to jointly predict holders, targets and polar expressions. Below, we first describe the evaluation metrics and the experimental setup, before finally discussing the results. 5.1. Experimental Setup We train a Bidirectional LSTM with a CRF inference layer, which has shown to be competitive for several other sequence labeling tasks (Huang et al., 2015; Lample et al., 2016; Panchendrarajan and Amaresan, 2018). We use the IOB2 label encoding for sources, targets, and polar expressions, including the binary polarity of the latter, giving us nine tags in total.2 This naturally leads to a lossy representation of the original data, as the relations, nested annotations, and polar intensity are ignored. Our model uses a single BiLSTM layer (100 dim.) to extract features and then a CRF layer to make predictions. We train the model using Adam (Kingma and Ba, 2014) for 40 epochs with a patience of 5, and use dropout to regularize both the BiLSTM (0.5) and CRF (0.3) layer"
2020.lrec-1.618,D19-1559,0,0.0264073,"ches use multitask learning to incorporate useful information from similar tasks into fine-grained sentiment models (Marasovi´c and Frank, 2018; He et al., 2019; Li et al., 2019). Alternatively, researchers have proposed attention-based methods which are adapted for fine-grained sentiment (Tang et al., 2019; Bao et al., 2019). These methods make use of an attention mechanism (Bahdanau et al., 2014) which allows the model to learn a weighted representation of sentences with respect to sentiment targets. Finally, there are approaches which create task-specific models for fine-grained sentiment. Liang et al. (2019) propose an aspect-specific gate to improve GRUs. 3. Annotations In the following we present our fine-grained sentiment annotation effort in more detail. We provide an overview of the annotation guidelines and present statistics on interannotator agreement. The complete set of guidelines is distributed with the corpus. Sentence-level annotations We build on the sentencelevel annotation of evaluative sentences in the NoReCeval corpus (Mæhlum et al., 2019), where two types of evaluative sentences were annotated: simple evaluative sentences (labeled EVAL), or the special case of evaluative factim"
2020.lrec-1.618,W19-6113,1,0.739617,"wegian 1. Introduction In this work, we describe the annotation of a fine-grained sentiment dataset for Norwegian, analysing opinions in terms of their polar expressions, targets, and holders. The dataset, including the annotation guidelines, is made publicly available1 and is the first of its kind for Norwegian. The underlying texts are taken from the Norwegian Review Corpus (NoReC) (Velldal et al., 2018) – a corpus of professionally authored reviews across a wide variety of domains, including literature, video games, music, various product categories, movies, TV-series, restaurants, etc. In Mæhlum et al. (2019), a subset of the documents, dubbed NoReCeval , were annotated at the sentence-level, indicating whether a sentence contains an evaluation or not. These prior annotations strictly indicated evaluativeness and did not include negative or positive polarity, as this can be mixed at the sentence-level. In the current work, the previous annotation effort has been considerably extended to include the span of polar expressions and the corresponding targets and holders of the opinion. We also indicate the intensity of the positive or negative polarity on a three-point scale, along with a number of oth"
2020.lrec-1.618,N18-1054,0,0.0377143,"Missing"
2020.lrec-1.618,Y18-1061,0,0.0270771,"n idea of the difficulty of the task, here we report some preliminary experimental results for the new dataset, intended as benchmarks for further experiments. Casting the problem as a sequence labeling task, we train a model to jointly predict holders, targets and polar expressions. Below, we first describe the evaluation metrics and the experimental setup, before finally discussing the results. 5.1. Experimental Setup We train a Bidirectional LSTM with a CRF inference layer, which has shown to be competitive for several other sequence labeling tasks (Huang et al., 2015; Lample et al., 2016; Panchendrarajan and Amaresan, 2018). We use the IOB2 label encoding for sources, targets, and polar expressions, including the binary polarity of the latter, giving us nine tags in total.2 This naturally leads to a lossy representation of the original data, as the relations, nested annotations, and polar intensity are ignored. Our model uses a single BiLSTM layer (100 dim.) to extract features and then a CRF layer to make predictions. We train the model using Adam (Kingma and Ba, 2014) for 40 epochs with a patience of 5, and use dropout to regularize both the BiLSTM (0.5) and CRF (0.3) layers. The word embeddings are 100 dimens"
2020.lrec-1.618,N18-1202,0,0.0404156,"e languages, such as Basque and Catalan (Barnes et al., 2018). No datasets for fine-grained SA have previously been created for Norwegian, however. 2.2. Modeling Fine-grained sentiment is most often approached as a sequence labeling problem (Yang and Cardie, 2013; Vo and Zhang, 2015) or simplified to a classification problem when the target or aspect is given (Pontiki et al., 2014). Although the specific architectures vary greatly, state-ofthe-art methods for fine-grained sentiment analysis tend to be transfer-learning approaches (Chen and Qian, 2019), often using pre-trained language models (Peters et al., 2018; Devlin et al., 2019) to improve model performance (Hu et al., 2019). Additionally, several approaches use multitask learning to incorporate useful information from similar tasks into fine-grained sentiment models (Marasovi´c and Frank, 2018; He et al., 2019; Li et al., 2019). Alternatively, researchers have proposed attention-based methods which are adapted for fine-grained sentiment (Tang et al., 2019; Bao et al., 2019). These methods make use of an attention mechanism (Bahdanau et al., 2014) which allows the model to learn a weighted representation of sentences with respect to sentiment ta"
2020.lrec-1.618,S14-2004,0,0.108751,"Missing"
2020.lrec-1.618,S16-1002,0,0.0475557,"Missing"
2020.lrec-1.618,W14-2605,0,0.019705,"Van de Kauter et al. (2015), working on financial news texts in Dutch and English, also taking account of implicit expressions of sentiment in polar facts. The SemEval 2014 shared task (Pontiki et al., 2014) proposes a different annotation scheme. Given an English tweet, the annotators identify targets, the aspect category they belong to, and the polarity expressed towards the target. They do not annotate holders or polar expressions. While most fine-grained sentiment datasets are in English, there are datasets available in several languages, such as German (Klinger and Cimiano, 2014), Czech (Steinberger et al., 2014), Arabic, Chinese, Dutch, French, Russian, Spanish, Turkish (Pontiki et al., 2016), Hungarian (Szabó et al., 2016), and Hindi (Akhtar et al., 2016). Addition5025 Polar Expression Polarity Type Positive Negative Evaluative Fact-Implied Non-Personal Intensity Slight Standard Strong Target H ol de r Not-On-Topic Re l Strong Postitive Target Relation Denne this TARGET disken disk er is P OLAR svært stillegående very quiet-going Figure 2: Annotation of an EVAL sentence (transl. ‘This disk runs very quietly’). Target-is-General Implicit at io n Holder Not-First-Person Implicit Figure 1: Annotation s"
2020.lrec-1.618,E12-2021,0,0.164792,"Missing"
2020.lrec-1.618,L16-1459,0,0.0133461,"ssions of sentiment in polar facts. The SemEval 2014 shared task (Pontiki et al., 2014) proposes a different annotation scheme. Given an English tweet, the annotators identify targets, the aspect category they belong to, and the polarity expressed towards the target. They do not annotate holders or polar expressions. While most fine-grained sentiment datasets are in English, there are datasets available in several languages, such as German (Klinger and Cimiano, 2014), Czech (Steinberger et al., 2014), Arabic, Chinese, Dutch, French, Russian, Spanish, Turkish (Pontiki et al., 2016), Hungarian (Szabó et al., 2016), and Hindi (Akhtar et al., 2016). Addition5025 Polar Expression Polarity Type Positive Negative Evaluative Fact-Implied Non-Personal Intensity Slight Standard Strong Target H ol de r Not-On-Topic Re l Strong Postitive Target Relation Denne this TARGET disken disk er is P OLAR svært stillegående very quiet-going Figure 2: Annotation of an EVAL sentence (transl. ‘This disk runs very quietly’). Target-is-General Implicit at io n Holder Not-First-Person Implicit Figure 1: Annotation schema for the NoReCfine dataset. ally, there has been an increased effort to create fine-grained resources for low"
2020.lrec-1.618,P19-1053,0,0.0236808,"rchitectures vary greatly, state-ofthe-art methods for fine-grained sentiment analysis tend to be transfer-learning approaches (Chen and Qian, 2019), often using pre-trained language models (Peters et al., 2018; Devlin et al., 2019) to improve model performance (Hu et al., 2019). Additionally, several approaches use multitask learning to incorporate useful information from similar tasks into fine-grained sentiment models (Marasovi´c and Frank, 2018; He et al., 2019; Li et al., 2019). Alternatively, researchers have proposed attention-based methods which are adapted for fine-grained sentiment (Tang et al., 2019; Bao et al., 2019). These methods make use of an attention mechanism (Bahdanau et al., 2014) which allows the model to learn a weighted representation of sentences with respect to sentiment targets. Finally, there are approaches which create task-specific models for fine-grained sentiment. Liang et al. (2019) propose an aspect-specific gate to improve GRUs. 3. Annotations In the following we present our fine-grained sentiment annotation effort in more detail. We provide an overview of the annotation guidelines and present statistics on interannotator agreement. The complete set of guidelines"
2020.lrec-1.618,P10-1059,0,0.199893,"at polarity is directed towards entities (either implicitly or explicitly mentioned). In this section we provide a brief overview of related work, first in terms of datasets and then modeling. 2.1. Datasets One of the earliest datasets for fine-grained opinion mining is the MPQA corpus (Wiebe et al., 2005), which contains annotations of private states in English-language texts taken from the news domain. The authors propose a detailed annotation scheme in which annotators identify subjective expressions, as well as their targets and holders. Working with sentiment in English consumer reviews, Toprak et al. (2010) annotate targets, holders and polar expressions, in addition to modifiers like negation, intensifiers and diminishers. The intensity of the polarity is marked on a three-point scale (weak, average, strong). In addition to annotating explicit expressions of subjective opinions, Toprak et al. (2010) annotate polar facts that may imply an evaluative opinion. A similar annotation scheme is followed by Van de Kauter et al. (2015), working on financial news texts in Dutch and English, also taking account of implicit expressions of sentiment in polar facts. The SemEval 2014 shared task (Pontiki et a"
2020.lrec-1.618,L18-1661,1,0.433552,"analysis of inter-annotator agreement. We also report the first experimental results on the dataset, intended as a preliminary benchmark for further experiments. Keywords: Sentiment analysis, opinion mining, Norwegian 1. Introduction In this work, we describe the annotation of a fine-grained sentiment dataset for Norwegian, analysing opinions in terms of their polar expressions, targets, and holders. The dataset, including the annotation guidelines, is made publicly available1 and is the first of its kind for Norwegian. The underlying texts are taken from the Norwegian Review Corpus (NoReC) (Velldal et al., 2018) – a corpus of professionally authored reviews across a wide variety of domains, including literature, video games, music, various product categories, movies, TV-series, restaurants, etc. In Mæhlum et al. (2019), a subset of the documents, dubbed NoReCeval , were annotated at the sentence-level, indicating whether a sentence contains an evaluation or not. These prior annotations strictly indicated evaluativeness and did not include negative or positive polarity, as this can be mixed at the sentence-level. In the current work, the previous annotation effort has been considerably extended to inc"
2020.lrec-1.618,P13-1161,0,0.0169767,"ET disken disk er is P OLAR svært stillegående very quiet-going Figure 2: Annotation of an EVAL sentence (transl. ‘This disk runs very quietly’). Target-is-General Implicit at io n Holder Not-First-Person Implicit Figure 1: Annotation schema for the NoReCfine dataset. ally, there has been an increased effort to create fine-grained resources for low-resource languages, such as Basque and Catalan (Barnes et al., 2018). No datasets for fine-grained SA have previously been created for Norwegian, however. 2.2. Modeling Fine-grained sentiment is most often approached as a sequence labeling problem (Yang and Cardie, 2013; Vo and Zhang, 2015) or simplified to a classification problem when the target or aspect is given (Pontiki et al., 2014). Although the specific architectures vary greatly, state-ofthe-art methods for fine-grained sentiment analysis tend to be transfer-learning approaches (Chen and Qian, 2019), often using pre-trained language models (Peters et al., 2018; Devlin et al., 2019) to improve model performance (Hu et al., 2019). Additionally, several approaches use multitask learning to incorporate useful information from similar tasks into fine-grained sentiment models (Marasovi´c and Frank, 2018;"
2021.acl-long.263,P15-1040,0,0.0606129,"Missing"
2021.acl-long.263,2020.acl-main.421,0,0.0150168,"1 the gains are more limited (3.3 pp/ 3.8 pp) and similarly for NSF1 and SF1 (3.6 pp/ 3.9 pp). The gains are 3394 NoReCFine MultiBEU MultiBCA MPQA DSUnis 57.0 (1.5) 75.7 (0.8) 71.7 (2.4) 38.5 (1.4) 44.5 (2.4) Table 6: Polarity F1 scores (unweighted and weighted) of models augmented with mBERT on the head-final setup. We report average and standard deviation over 5 runs. largest for the English datasets (MPQA, DSUnis ) followed by NoReCFine , and finally MultiBCA and MultiBEU . This corroborates the bias towards English and similar languages that has been found in multilingual language models (Artetxe et al., 2020; Conneau et al., 2020) and motivates the need for language-specific contextualized embeddings. 7.4 Analysis of polarity predictions Acknowledgements This work has been carried out as part of the SANT project (Sentiment Analysis for Norwegian Text), funded by the Research Council of Norway (grant number 270908). The computations were performed on resources provided by UNINETT Sigma2 - the National Infrastructure for High Performance Computing and Data Storage in Norway. References In this section we zoom in on polarity, in order to quantify how well models perform at predicting only polarity."
2021.acl-long.263,L18-1104,1,0.902863,"Missing"
2021.acl-long.263,D12-1091,0,0.015329,"entations in addition to word2vec skip-gram embeddings openly available from the NLPL vector repository8 (Fares et al., 2017). We train all models for 100 epochs and keep the model that performs best regarding LF1 on the dev set (Targeted F1 for the baselines). We use default hyperparameters from Kurtz et al. (2020) (see Appendix) and run all of our models five times with different random seeds and report the mean (standard deviation shown as well in Table 8 in the Appendix). We calculate statistical difference between the best and second best models through a bootstrap with replacement test (Berg-Kirkpatrick et al., 2012). As there are 5 runs, we require that 3 of 5 be statistically significant at p < 0.05. Table 3 shows the results for all datasets. On NoReCFine , the baselines IMN, RACL, and RACL-BERT perform well at extracting targets (35.9, 45.6, and 47.2 F1 , respectively) and expressions (48.7/55.4/56.3), but struggle with the full targeted sentiment task (18.0/20.1/30.3). The graphbased models extract targets better (50.1/54.8) and have comparable scores for expressions (54.4/55.5). The holder extraction scores have a similar range (51.1/60.4). These patterns hold throughout the other datasets, where th"
2021.acl-long.263,W17-0237,1,0.844981,"ty (holder, target, expression, polarity). A true positive is defined as an exact match at graph-level, weighting the overlap in predicted and gold spans for each element, averaged across all three spans. For precision we weight the number of correctly predicted tokens divided by the total number of predicted tokens (for recall, we divide instead by the number of gold tokens). We allow for empty holders and targets. 3392 6 Experiments All sentiment graph models use token-level mBERT representations in addition to word2vec skip-gram embeddings openly available from the NLPL vector repository8 (Fares et al., 2017). We train all models for 100 epochs and keep the model that performs best regarding LF1 on the dev set (Targeted F1 for the baselines). We use default hyperparameters from Kurtz et al. (2020) (see Appendix) and run all of our models five times with different random seeds and report the mean (standard deviation shown as well in Table 8 in the Appendix). We calculate statistical difference between the best and second best models through a bootstrap with replacement test (Berg-Kirkpatrick et al., 2012). As there are 5 runs, we require that 3 of 5 be statistically significant at p < 0.05. Table 3"
2021.acl-long.263,P19-1048,0,0.110331,"s have shown some potential (Zhang et al., 2019). However, all of this work ignores the polarity classification subtask. Targeted sentiment analysis only concentrates on extracting sentiment targets (task ii) and classifying the polarity directed towards them (task iv) (Jiang et al., 2011; Mitchell et al., 2013). Recent shared tasks on Aspect-Based Sentiment Analysis (ABSA) (Pontiki et al., 2014, 2015, 2016) also include target extraction and polarity classification subtasks. Joint approaches perform on par with pipeline methods (Li et al., 2019a) and multitask models can perform even better (He et al., 2019). Finally, pretrained language models (Devlin et al., 2019) can also lead to improvements on the ABSA data (Li et al., 2019b). End2End sentiment analysis is a recently proposed subtask which combines targeted sentiment (tasks ii and v) and sentiment expression extraction (task i), without requiring the resolution of relationships between targets and expressions. Wang et al. (2016) augment the ABSA datasets with sentiment expressions, but provide no details on the annotation process or any inter-annotator agreement. He et al. (2019) make use of this data and propose a multi-layer CNN (IMN) to c"
2021.acl-long.263,P11-1016,0,0.0618142,"entiment lexicons, dependency parsers, named-entity taggers) (Choi et al., 2006; Yang and Cardie, 2012) are strong baselines. Given the small size of the training data and the complicated task, these techniques often still outperform neural models, such as BiLSTMs (Katiyar and Cardie, 2016). Transition-based end-toend approaches have shown some potential (Zhang et al., 2019). However, all of this work ignores the polarity classification subtask. Targeted sentiment analysis only concentrates on extracting sentiment targets (task ii) and classifying the polarity directed towards them (task iv) (Jiang et al., 2011; Mitchell et al., 2013). Recent shared tasks on Aspect-Based Sentiment Analysis (ABSA) (Pontiki et al., 2014, 2015, 2016) also include target extraction and polarity classification subtasks. Joint approaches perform on par with pipeline methods (Li et al., 2019a) and multitask models can perform even better (He et al., 2019). Finally, pretrained language models (Devlin et al., 2019) can also lead to improvements on the ABSA data (Li et al., 2019b). End2End sentiment analysis is a recently proposed subtask which combines targeted sentiment (tasks ii and v) and sentiment expression extraction ("
2021.acl-long.263,P16-1087,0,0.162601,"between these elements, and v) assigning polarity. Previous work on information extraction has used pipeline methods which first extract the holders, targets, and expressions (tasks i - iii) and subsequently predict their relations (task iv), mostly on the MPQA dataset (Wiebe et al., 2005). CRFs and a number of external resources (sentiment lexicons, dependency parsers, named-entity taggers) (Choi et al., 2006; Yang and Cardie, 2012) are strong baselines. Given the small size of the training data and the complicated task, these techniques often still outperform neural models, such as BiLSTMs (Katiyar and Cardie, 2016). Transition-based end-toend approaches have shown some potential (Zhang et al., 2019). However, all of this work ignores the polarity classification subtask. Targeted sentiment analysis only concentrates on extracting sentiment targets (task ii) and classifying the polarity directed towards them (task iv) (Jiang et al., 2011; Mitchell et al., 2013). Recent shared tasks on Aspect-Based Sentiment Analysis (ABSA) (Pontiki et al., 2014, 2015, 2016) also include target extraction and polarity classification subtasks. Joint approaches perform on par with pipeline methods (Li et al., 2019a) and mult"
2021.acl-long.263,J16-4009,1,0.787534,"nes, procedure, or inter-annotator agreement. Graph parsing: Syntactic dependency graphs are regularly used in applications, supplying them with necessary grammatical information (Mintz et al., 2009; Cui et al., 2005; Bj¨orne et al., 2009; Johansson and Moschitti, 2012; Lapponi et al., 2012). The dependency graph structures used in these systems are predominantly restricted to trees. While trees are sufficient to encode syntactic dependencies, they are not expressive enough to handle meaning representations, that require nodes to have multiple incoming arcs, or having no incoming arcs at all (Kuhlmann and Oepen, 2016). While much of the early research on parsing these new structures (Oepen et al., 2014, 2015) focused on specialized decoding algorithms, Dozat and Manning (2018) presented a neural dependency parser that essentially relies only on its neural network structure to predict any type of dependency graph without restrictions to certain structures. Using the parser’s ability to learn arbitrary dependency graphs, Kurtz et al. (2020) phrased the task of negation resolution (Morante and Blanco, 2012; Morante and Daelemans, 2012) as a graph parsing task. This transformed the otherwise flat representatio"
2021.acl-long.263,2020.iwpt-1.3,1,0.851073,"rall resolution of sentiment, or do not take into account the inter-dependencies of the various sub-tasks. As such, we propose a unified approach to structured sentiment which jointly predicts all elements of an opinion tuple and their relations. Moreover, we cast sentiment analysis as a dependency graph parsing problem, where the sentiment expression is the root node, and the other elements have arcs which model the relationships between them. This methodology also enables us to take advantage of recent improvements in semantic dependency parsing (Dozat and Manning, 2018; Oepen et al., 2020; Kurtz et al., 2020) to efficiently learn a sentiment graph parser. This perspective also allows us to unify a number of approaches, including targeted, and opinion tuple mining. We aim to answer RQ1: whether graph-based approaches to structured sentiment outperform state-of-the-art sequence labeling approaches, and RQ2: how to best encode structured sentiment as parsing graphs. We perform experiments on five standard datasets in four languages (English, Norwegian, Basque, Catalan) and show that graph-based approaches outperform state-ofthe-art baselines on all datasets on several standard metrics, as well as our"
2021.acl-long.263,S12-1042,1,0.750098,"ationship between target and expression. Finally, the recently proposed aspect sentiment triplet extraction (Peng et al., 2019; ?) attempts to extract targets, expressions and their polarity. However, the datasets used are unlikely to be adequate, as they augment available targeted datasets, but do not report annotation guidelines, procedure, or inter-annotator agreement. Graph parsing: Syntactic dependency graphs are regularly used in applications, supplying them with necessary grammatical information (Mintz et al., 2009; Cui et al., 2005; Bj¨orne et al., 2009; Johansson and Moschitti, 2012; Lapponi et al., 2012). The dependency graph structures used in these systems are predominantly restricted to trees. While trees are sufficient to encode syntactic dependencies, they are not expressive enough to handle meaning representations, that require nodes to have multiple incoming arcs, or having no incoming arcs at all (Kuhlmann and Oepen, 2016). While much of the early research on parsing these new structures (Oepen et al., 2014, 2015) focused on specialized decoding algorithms, Dozat and Manning (2018) presented a neural dependency parser that essentially relies only on its neural network structure to pre"
2021.acl-long.263,D19-5505,0,0.0118353,"(Katiyar and Cardie, 2016). Transition-based end-toend approaches have shown some potential (Zhang et al., 2019). However, all of this work ignores the polarity classification subtask. Targeted sentiment analysis only concentrates on extracting sentiment targets (task ii) and classifying the polarity directed towards them (task iv) (Jiang et al., 2011; Mitchell et al., 2013). Recent shared tasks on Aspect-Based Sentiment Analysis (ABSA) (Pontiki et al., 2014, 2015, 2016) also include target extraction and polarity classification subtasks. Joint approaches perform on par with pipeline methods (Li et al., 2019a) and multitask models can perform even better (He et al., 2019). Finally, pretrained language models (Devlin et al., 2019) can also lead to improvements on the ABSA data (Li et al., 2019b). End2End sentiment analysis is a recently proposed subtask which combines targeted sentiment (tasks ii and v) and sentiment expression extraction (task i), without requiring the resolution of relationships between targets and expressions. Wang et al. (2016) augment the ABSA datasets with sentiment expressions, but provide no details on the annotation process or any inter-annotator agreement. He et al. (201"
2021.acl-long.263,P09-1113,0,0.117439,"s, generally only include sentiment-bearing words (not phrases), and do not specify the relationship between target and expression. Finally, the recently proposed aspect sentiment triplet extraction (Peng et al., 2019; ?) attempts to extract targets, expressions and their polarity. However, the datasets used are unlikely to be adequate, as they augment available targeted datasets, but do not report annotation guidelines, procedure, or inter-annotator agreement. Graph parsing: Syntactic dependency graphs are regularly used in applications, supplying them with necessary grammatical information (Mintz et al., 2009; Cui et al., 2005; Bj¨orne et al., 2009; Johansson and Moschitti, 2012; Lapponi et al., 2012). The dependency graph structures used in these systems are predominantly restricted to trees. While trees are sufficient to encode syntactic dependencies, they are not expressive enough to handle meaning representations, that require nodes to have multiple incoming arcs, or having no incoming arcs at all (Kuhlmann and Oepen, 2016). While much of the early research on parsing these new structures (Oepen et al., 2014, 2015) focused on specialized decoding algorithms, Dozat and Manning (2018) presented"
2021.acl-long.263,D13-1171,0,0.0551436,"Missing"
2021.acl-long.263,S12-1035,0,0.0332557,"representations, that require nodes to have multiple incoming arcs, or having no incoming arcs at all (Kuhlmann and Oepen, 2016). While much of the early research on parsing these new structures (Oepen et al., 2014, 2015) focused on specialized decoding algorithms, Dozat and Manning (2018) presented a neural dependency parser that essentially relies only on its neural network structure to predict any type of dependency graph without restrictions to certain structures. Using the parser’s ability to learn arbitrary dependency graphs, Kurtz et al. (2020) phrased the task of negation resolution (Morante and Blanco, 2012; Morante and Daelemans, 2012) as a graph parsing task. This transformed the otherwise flat representations to dependency structures that directly encode the often overlapping relations between the building blocks of multiple negation instances at the same time. In a simpler fashion, Yu et al. (2020) exploit the parser of Dozat and Manning (2018) to predict spans of named entities. 3 Datasets are shown in Table 1. The largest available structured sentiment dataset is the NoReCFine dataset (Øvrelid et al., 2020), a multi-domain dataset of professional reviews in Norwegian, annotated for structu"
2021.acl-long.263,morante-daelemans-2012-conandoyle,0,0.0525443,"Missing"
2021.acl-long.263,N10-1120,0,0.0553484,"Missing"
2021.acl-long.263,2020.conll-shared.1,1,0.80928,"Missing"
2021.acl-long.263,S15-2082,0,0.0767941,"Missing"
2021.acl-long.263,S14-2004,0,0.0356346,"strong baselines. Given the small size of the training data and the complicated task, these techniques often still outperform neural models, such as BiLSTMs (Katiyar and Cardie, 2016). Transition-based end-toend approaches have shown some potential (Zhang et al., 2019). However, all of this work ignores the polarity classification subtask. Targeted sentiment analysis only concentrates on extracting sentiment targets (task ii) and classifying the polarity directed towards them (task iv) (Jiang et al., 2011; Mitchell et al., 2013). Recent shared tasks on Aspect-Based Sentiment Analysis (ABSA) (Pontiki et al., 2014, 2015, 2016) also include target extraction and polarity classification subtasks. Joint approaches perform on par with pipeline methods (Li et al., 2019a) and multitask models can perform even better (He et al., 2019). Finally, pretrained language models (Devlin et al., 2019) can also lead to improvements on the ABSA data (Li et al., 2019b). End2End sentiment analysis is a recently proposed subtask which combines targeted sentiment (tasks ii and v) and sentiment expression extraction (task i), without requiring the resolution of relationships between targets and expressions. Wang et al. (2016"
2021.acl-long.263,2020.acl-demos.14,0,0.0179855,"own in Table 8 in the Appendix. The implementation of the graph structure has a large effect on all metrics, although the specific results depend on the dataset. We plot the average effect of each implementation across all datasets in Figure 3, as well as each individual dataset (Figures 4–8 in the Appendix). +inlabel tends to improve results on the non-English datasets, consistently increasing target and expression extraction and targeted sentiment. It also generally improves the graph scores UF1 and LF1 on the non-English datasets. 9 We use SpaCy (Honnibal et al., 2020) for English, Stanza (Qi et al., 2020) for Basque and Catalan and UDPipe (Straka and Strakov´a, 2017) for Norwegian. Dep. edges has the strongest positive effect on the NSF1 and SF1 (an avg. 2.52 and 2.22 percentage point (pp) over Head-final, respectively). However, this average is pulled down by poorer performance on the English datasets. Removing these two, the average benefit is 5.2 and 4.2 for NSF1 and SF1 , respectively. On span extraction and targeted sentiment, however, Dep. edges leads to poorer scores overall. Dep. labels does not lead to any consistent improvements. These results indicate that incorporating syntactic de"
2021.acl-long.263,D13-1170,0,0.0568037,"Missing"
2021.acl-long.263,2020.acl-main.577,0,0.0253182,"l dependency parser that essentially relies only on its neural network structure to predict any type of dependency graph without restrictions to certain structures. Using the parser’s ability to learn arbitrary dependency graphs, Kurtz et al. (2020) phrased the task of negation resolution (Morante and Blanco, 2012; Morante and Daelemans, 2012) as a graph parsing task. This transformed the otherwise flat representations to dependency structures that directly encode the often overlapping relations between the building blocks of multiple negation instances at the same time. In a simpler fashion, Yu et al. (2020) exploit the parser of Dozat and Manning (2018) to predict spans of named entities. 3 Datasets are shown in Table 1. The largest available structured sentiment dataset is the NoReCFine dataset (Øvrelid et al., 2020), a multi-domain dataset of professional reviews in Norwegian, annotated for structured sentiment. MultiBEU and MultiBCA (Barnes et al., 2018) are hotel reviews in Basque and Catalan, respectively. MPQA (Wiebe et al., 2005) annotates news wire text in English. Finally, DSUnis (Toprak et al., 2010) annotate English reviews of online universities and e-commerce. In our experiments, we"
2021.acl-long.263,K17-3009,0,0.056707,"Missing"
2021.eacl-main.5,P16-1087,0,0.0266305,"and 2) classification of polarity with respect to targets (“aspect term polarity”). As most targeted datasets only contain a single target, or multiple targets with the same polarity, sentence-level classifiers are strong baselines. In order to mitigate this, Jiang et al. (2019) create a Challenge dataset which has both multiple targets and multiple polarities in each sentence. Similarly, Wang et al. (2017) also point out that most targeted sentiment methods perform poorly with multiple targets and propose TDParse, a corpus of UK election tweets with multiple targets per tweet. 2.2 Modelling Katiyar and Cardie (2016) explore jointly extracting holders, targets, and expressions with LSTMs. They find that adding sentence-level and relationlevel dependencies (IS - FROM or IS - ABOUT) improve extraction, but find that the LSTM models lag behind CRFs with rich features. Regarding modelling the interaction between elements, there are several previous attempts to jointly learn to extract and classify targets, using factor graphs (Klinger and Cimiano, 2013), multitask learning (He et al., 2019) or sequence tagging with collapsed tagsets representing both tasks (Li et al., 2019). In general, the benefits are small"
2021.eacl-main.5,P14-1146,0,0.0527381,"en in the target (3) averaging all embeddings in the target phrase, (4) taking the max of the target embeddings, (5) concatenating the max, mean, and min). 2. F IRST: uses the contextualized BERT embedding from the first token of the target in context. 3. M EAN: instead takes the average of the BERT embeddings for the tokens in the target. 4. M AX: uses the max of the contextualized BERT embeddings for the tokens in the target. 5. M AX MM: takes the max, min, and mean pooled representations and passes the concatenation to the softmax layer, which has shown to perform well for sentiment tasks (Tang et al., 2014). However, this triples the size of the input representation to the softmax layer. 4.1 The TARG . and [C LS ] models correspond to the models used in Xu et al. (2019) and serve as baselines. The extraction and classification models are fine-tuned for 50 epochs using Adam with an initial learning rate of 3e−5, with a linear warmup of 0.1 and all other hyperparameters are left at default BERT settings (further details in Appendix B). The best model on the development set is used for testing. Combined with the four input manipulations (Table 2), this leads to eleven extraction experiments – TARG"
2021.eacl-main.5,P13-2147,0,0.0333764,"geted sentiment methods perform poorly with multiple targets and propose TDParse, a corpus of UK election tweets with multiple targets per tweet. 2.2 Modelling Katiyar and Cardie (2016) explore jointly extracting holders, targets, and expressions with LSTMs. They find that adding sentence-level and relationlevel dependencies (IS - FROM or IS - ABOUT) improve extraction, but find that the LSTM models lag behind CRFs with rich features. Regarding modelling the interaction between elements, there are several previous attempts to jointly learn to extract and classify targets, using factor graphs (Klinger and Cimiano, 2013), multitask learning (He et al., 2019) or sequence tagging with collapsed tagsets representing both tasks (Li et al., 2019). In general, the benefits are small and have suggested that there is only a weak relationship between target extraction and polarity classification (Hu et al., 2019). 3 Data One of the difficulties of working with finegrained sentiment analysis is that there are only a few datasets (even in English) and they come in incompatible, competing data formats, e.g., BRAT or various flavors of XML. With the goal of creating a simple unified format to work on fine-grained sentimen"
2021.eacl-main.5,P10-1059,0,0.261377,"s, etc.? (c) Can sentiment lexicons provide enough information on expressions to give improvements? 2.1 The Multi-purpose Question Answering dataset (MPQA) (Wiebe et al., 2005) is the first dataset that annotated opinion holders, targets, expressions and their relationships. The news wire data leads to complex opinions and a generally difficult task for sentiment models. Normally, the full opinion extraction task is modelled as extraction of the individual elements (holders, targets, and expressions) and the subsequent resolution of the relationship between them. The Darmstadt Review Corpora (Toprak et al., 2010) contain annotated opinions for consumer reviews of universities and services. The authors annotate holders, targets, expressions, polarity, modifiers, and intensity. They achieve between 0.5 and 0.8 agreement using the agr method (Wiebe et al., 2005), with higher disagreement on what they call “polar targets” – targets that have a polarity but no annotated sentiment expression – holders, and expressions. The Open Domain Targeted dataset (Mitchell et al., 2013) makes use of crowd sourcing to annotate NEs from scraped tweets in English and Spanish (Etter et al., 2013) with their polarities. The"
2021.eacl-main.5,E17-1046,0,0.357379,"atics {jeremycb,liljao,erikve}@ifi.uio.no Abstract as apposed to the author of the sentence can help us determine the overall polarity expressed in the sentence. Compared to document- or sentence-level sentiment analysis, where distant labelling schemes can be used to obtain annotated data, fine-grained annotation of sentiment does not occur naturally, which means that current machine learning models are often hampered by the small size of datasets. Furthermore, fine-grained annotation is demanding, leads to relatively small datasets, and has low inter-annotator agreement (Wiebe et al., 2005; Wang et al., 2017). This begs the question: is it worth it to annotate full fine-grained sentiment? Targeted sentiment (Mitchell et al., 2013; Zhang et al., 2015) is a reduction of the fine-grained sentiment task which concentrates on extracting sentiment targets and classifying their polarity, effectively ignoring sentiment holders and expressions. The benefit of this setup is that it is faster to annotate and simpler to model. But would targeted sentiment models benefit from knowing the sentiment holders and expressions? In this work, we attempt to determine whether holder and expression information is useful"
2021.eacl-main.5,W02-0109,0,0.0806583,"download, preprocess, and collect the datasets into a compatible JSON format, with the hope that this allows future research on the same data. 2 Datasets Related work Fine-grained approaches to sentiment analysis attempt to discover opinions from text, where each opinion is a tuple of (opinion holder, opinion target, opinion expression, polarity, intensity). Annotation of datasets for this granularity requires creating in-depth annotation guidelines, training 1 https://github.com/ltgoslo/ finegrained_modelling 50 a standard JSON format. The datasets are sentence and word tokenized using NLTK (Loper and Bird, 2002), except for MPQA, DS. Service and DS. Uni, which already contain sentence and token spans. All polarity annotations are mapped to positive, negative, neutral, and conflict2 . As such, each sentence contains a sentence id, the tokenized text, and a possibly empty set of opinions which contain a holder, target, expression, polarity, and intensity. We allow for empty holders and expressions in order generalize to the targeted corpora. Finally, we use 10 percent of the training data as development and another 10 percent for test for the corpora that do not contain a suggested train/dev/test split"
2021.eacl-main.5,D13-1171,0,0.0475814,"Missing"
2021.gebnlp-1.8,2020.coling-main.505,0,0.030253,"Natural Language Processing, pages 66–74 August 5, 2021. ©2021 Association for Computational Linguistics 2 Bias statement (Madotto et al., 2018), natural language inference (Chen et al., 2018), and machine translation (Zaremoodi et al., 2018). Knowledge graphs have also been used to enrich embedding information. Zhang et al. (2019) use entries from Wikidata, as well as their relation to each others, to represent and inject structural knowledge aggregates to a collection of large-scale corpora. They show that their approach reduces noisy data and improves BERT fine-tuning on limited datasets. Bourgonje and Stede (2020) enrich a German BERT model with linguistic knowledge represented as a lexicon as well as manually generated syntactic features. Peinelt et al. (2020) enrich a BERT with LDA topics, and show that this combination improves performance of semantic similarity. Ostendorff et al. (2019) use a combination of metadata about books to enrich a BERTbased multi-class classification model. They train a BERT model on the title and the texts of each book, and concatenate the output with metadata information and author embeddings from Wikipedia, and feed them into a Multilayer Perceptron (MLP). This work foc"
2021.gebnlp-1.8,P18-1224,0,0.0301768,"ng a downstream classifier. Typically, when a classifier is fitted on top of a pre-trained LM for a given task, only textual data is considered by the learned representations. In this work we investigate the effect of adding metadata information about demographic variables that are known to be associated with bias in the training data. Specifically, we focus on the task 66 Proceedings of the 3rd Workshop on Gender Bias in Natural Language Processing, pages 66–74 August 5, 2021. ©2021 Association for Computational Linguistics 2 Bias statement (Madotto et al., 2018), natural language inference (Chen et al., 2018), and machine translation (Zaremoodi et al., 2018). Knowledge graphs have also been used to enrich embedding information. Zhang et al. (2019) use entries from Wikidata, as well as their relation to each others, to represent and inject structural knowledge aggregates to a collection of large-scale corpora. They show that their approach reduces noisy data and improves BERT fine-tuning on limited datasets. Bourgonje and Stede (2020) enrich a German BERT model with linguistic knowledge represented as a lexicon as well as manually generated syntactic features. Peinelt et al. (2020) enrich a BERT wi"
2021.gebnlp-1.8,2020.gebnlp-1.3,0,0.0870409,"Missing"
2021.gebnlp-1.8,2020.lrec-1.502,0,0.0610146,"Missing"
2021.gebnlp-1.8,2020.gebnlp-1.1,0,0.0256051,"with the underlying encoding of gender itself, since only the binary gender categories of male/female are present in the data. While the dataset we use only reflects binary gender categories, we acknowledge the fact that gender as an identity spans a wider spectrum than this. 3 When it comes to gender and gender bias, previous research has been devoted to the identification of bias in textual content and models (Garimella and Mihalcea, 2016; Schofield and Mehr, 2016; Kiritchenko and Mohammad, 2018), and in input representations as static and contextualised embeddings (Takeshita et al., 2020; Bartl et al., 2020; Zhao et al., 2020; Basta et al., 2019; Kaneko and Bollegala, 2019; Friedman et al., 2019; Bolukbasi et al., 2016). A considerable amount of previous work has also gone into either mitigating existing bias in embeddings (Takeshita et al., 2020; Maudslay et al., 2019; Zmigrod et al., 2019; Garg et al., 2018), making them gender neutral (Zhao et al., 2018), or using debiased embeddings (Escud´e Font and Costa-juss`a, 2019). Instead of debiasing and mitigating bias in embeddings, some work has focused on creating gender balanced corpora (Costajuss`a et al., 2020; Costa-juss`a and de Jorge, 2020)"
2021.gebnlp-1.8,N19-1423,0,0.174686,"ious work has also gone into either mitigating existing bias in embeddings (Takeshita et al., 2020; Maudslay et al., 2019; Zmigrod et al., 2019; Garg et al., 2018), making them gender neutral (Zhao et al., 2018), or using debiased embeddings (Escud´e Font and Costa-juss`a, 2019). Instead of debiasing and mitigating bias in embeddings, some work has focused on creating gender balanced corpora (Costajuss`a et al., 2020; Costa-juss`a and de Jorge, 2020). Related work State-of-the-art results for various NLP tasks nowadays typically build on some pre-trained transformer language models like BERT (Devlin et al., 2019). Despite their great achievements, these models have been shown to include various types of bias (Zhao et al., 2020; Bartl et al., 2020; Basta et al., 2019; Kaneko and Bollegala, 2019; Friedman et al., 2019; Kurita et al., 2019). Recent works have shown the advantage of adding extra information to pre-trained language models for numerous tasks, e.g., dialog systems Several previous studies have focused on gender and gender bias in sentiment analysis, both from data and model perspectives. To name a few: Kiritchenko and Mohammad (2018) propose an evaluation corpus (Equity Evaluation Corpus) th"
2021.gebnlp-1.8,W19-3805,0,0.0268621,"Missing"
2021.gebnlp-1.8,W19-3821,0,0.0553101,"Missing"
2021.gebnlp-1.8,W19-3809,0,0.0179626,"et al., 2019; Kaneko and Bollegala, 2019; Friedman et al., 2019; Kurita et al., 2019). Recent works have shown the advantage of adding extra information to pre-trained language models for numerous tasks, e.g., dialog systems Several previous studies have focused on gender and gender bias in sentiment analysis, both from data and model perspectives. To name a few: Kiritchenko and Mohammad (2018) propose an evaluation corpus (Equity Evaluation Corpus) that can be used to mitigate biases towards a selection of genders and races. Occupational gender stereotypes exist in sentiment analysis models (Bhaskaran and Bhallamudi, 2019), both in training data and in pre-trained contextualized models. 67 Models have also been proposed to uncover gender biases (Hoyle et al., 2019). Incorporating extra demographic information into sentiment classification models have also been successful. Hovy (2015) has shown that incorporation gender information (as embeddings) in models can improve sentiment classification. They show that such an approach can reduce the bias towards minorities, as for example females, who tend to communicate differently from the norm. In this paper, we do not focus on biases present in existing systems , nor"
2021.gebnlp-1.8,W19-3803,0,0.0337151,"of male/female are present in the data. While the dataset we use only reflects binary gender categories, we acknowledge the fact that gender as an identity spans a wider spectrum than this. 3 When it comes to gender and gender bias, previous research has been devoted to the identification of bias in textual content and models (Garimella and Mihalcea, 2016; Schofield and Mehr, 2016; Kiritchenko and Mohammad, 2018), and in input representations as static and contextualised embeddings (Takeshita et al., 2020; Bartl et al., 2020; Zhao et al., 2020; Basta et al., 2019; Kaneko and Bollegala, 2019; Friedman et al., 2019; Bolukbasi et al., 2016). A considerable amount of previous work has also gone into either mitigating existing bias in embeddings (Takeshita et al., 2020; Maudslay et al., 2019; Zmigrod et al., 2019; Garg et al., 2018), making them gender neutral (Zhao et al., 2018), or using debiased embeddings (Escud´e Font and Costa-juss`a, 2019). Instead of debiasing and mitigating bias in embeddings, some work has focused on creating gender balanced corpora (Costajuss`a et al., 2020; Costa-juss`a and de Jorge, 2020). Related work State-of-the-art results for various NLP tasks nowadays typically build on"
2021.gebnlp-1.8,D19-1530,0,0.0175999,"um than this. 3 When it comes to gender and gender bias, previous research has been devoted to the identification of bias in textual content and models (Garimella and Mihalcea, 2016; Schofield and Mehr, 2016; Kiritchenko and Mohammad, 2018), and in input representations as static and contextualised embeddings (Takeshita et al., 2020; Bartl et al., 2020; Zhao et al., 2020; Basta et al., 2019; Kaneko and Bollegala, 2019; Friedman et al., 2019; Bolukbasi et al., 2016). A considerable amount of previous work has also gone into either mitigating existing bias in embeddings (Takeshita et al., 2020; Maudslay et al., 2019; Zmigrod et al., 2019; Garg et al., 2018), making them gender neutral (Zhao et al., 2018), or using debiased embeddings (Escud´e Font and Costa-juss`a, 2019). Instead of debiasing and mitigating bias in embeddings, some work has focused on creating gender balanced corpora (Costajuss`a et al., 2020; Costa-juss`a and de Jorge, 2020). Related work State-of-the-art results for various NLP tasks nowadays typically build on some pre-trained transformer language models like BERT (Devlin et al., 2019). Despite their great achievements, these models have been shown to include various types of bias (Zh"
2021.gebnlp-1.8,W16-4301,0,0.0264283,"er, by explicitly incorporating this as a variable in the model. Note that there are also issues of what could be argued to be representational harm (Blodgett et al., 2020) associated with the underlying encoding of gender itself, since only the binary gender categories of male/female are present in the data. While the dataset we use only reflects binary gender categories, we acknowledge the fact that gender as an identity spans a wider spectrum than this. 3 When it comes to gender and gender bias, previous research has been devoted to the identification of bias in textual content and models (Garimella and Mihalcea, 2016; Schofield and Mehr, 2016; Kiritchenko and Mohammad, 2018), and in input representations as static and contextualised embeddings (Takeshita et al., 2020; Bartl et al., 2020; Zhao et al., 2020; Basta et al., 2019; Kaneko and Bollegala, 2019; Friedman et al., 2019; Bolukbasi et al., 2016). A considerable amount of previous work has also gone into either mitigating existing bias in embeddings (Takeshita et al., 2020; Maudslay et al., 2019; Zmigrod et al., 2019; Garg et al., 2018), making them gender neutral (Zhao et al., 2018), or using debiased embeddings (Escud´e Font and Costa-juss`a, 2019)."
2021.gebnlp-1.8,P15-1073,0,0.0276674,"as in sentiment analysis, both from data and model perspectives. To name a few: Kiritchenko and Mohammad (2018) propose an evaluation corpus (Equity Evaluation Corpus) that can be used to mitigate biases towards a selection of genders and races. Occupational gender stereotypes exist in sentiment analysis models (Bhaskaran and Bhallamudi, 2019), both in training data and in pre-trained contextualized models. 67 Models have also been proposed to uncover gender biases (Hoyle et al., 2019). Incorporating extra demographic information into sentiment classification models have also been successful. Hovy (2015) has shown that incorporation gender information (as embeddings) in models can improve sentiment classification. They show that such an approach can reduce the bias towards minorities, as for example females, who tend to communicate differently from the norm. In this paper, we do not focus on biases present in existing systems , nor do we try to mitigate them in a traditional way. We use a dataset of Norwegian book reviews for which a previous study has indicated some degree of gender bias in the label distribution of review ratings (Touileb et al., 2020). Here, we investigate whether this bia"
2021.gebnlp-1.8,P19-1167,0,0.0221581,"re-trained language models for numerous tasks, e.g., dialog systems Several previous studies have focused on gender and gender bias in sentiment analysis, both from data and model perspectives. To name a few: Kiritchenko and Mohammad (2018) propose an evaluation corpus (Equity Evaluation Corpus) that can be used to mitigate biases towards a selection of genders and races. Occupational gender stereotypes exist in sentiment analysis models (Bhaskaran and Bhallamudi, 2019), both in training data and in pre-trained contextualized models. 67 Models have also been proposed to uncover gender biases (Hoyle et al., 2019). Incorporating extra demographic information into sentiment classification models have also been successful. Hovy (2015) has shown that incorporation gender information (as embeddings) in models can improve sentiment classification. They show that such an approach can reduce the bias towards minorities, as for example females, who tend to communicate differently from the norm. In this paper, we do not focus on biases present in existing systems , nor do we try to mitigate them in a traditional way. We use a dataset of Norwegian book reviews for which a previous study has indicated some degree"
2021.gebnlp-1.8,2020.acl-main.630,0,0.0352657,"language inference (Chen et al., 2018), and machine translation (Zaremoodi et al., 2018). Knowledge graphs have also been used to enrich embedding information. Zhang et al. (2019) use entries from Wikidata, as well as their relation to each others, to represent and inject structural knowledge aggregates to a collection of large-scale corpora. They show that their approach reduces noisy data and improves BERT fine-tuning on limited datasets. Bourgonje and Stede (2020) enrich a German BERT model with linguistic knowledge represented as a lexicon as well as manually generated syntactic features. Peinelt et al. (2020) enrich a BERT with LDA topics, and show that this combination improves performance of semantic similarity. Ostendorff et al. (2019) use a combination of metadata about books to enrich a BERTbased multi-class classification model. They train a BERT model on the title and the texts of each book, and concatenate the output with metadata information and author embeddings from Wikipedia, and feed them into a Multilayer Perceptron (MLP). This work focuses on gender bias, which we identify as the differences in language use between persons, on the unique basis of their genders. The concrete task tha"
2021.gebnlp-1.8,W16-0204,0,0.0228018,"g this as a variable in the model. Note that there are also issues of what could be argued to be representational harm (Blodgett et al., 2020) associated with the underlying encoding of gender itself, since only the binary gender categories of male/female are present in the data. While the dataset we use only reflects binary gender categories, we acknowledge the fact that gender as an identity spans a wider spectrum than this. 3 When it comes to gender and gender bias, previous research has been devoted to the identification of bias in textual content and models (Garimella and Mihalcea, 2016; Schofield and Mehr, 2016; Kiritchenko and Mohammad, 2018), and in input representations as static and contextualised embeddings (Takeshita et al., 2020; Bartl et al., 2020; Zhao et al., 2020; Basta et al., 2019; Kaneko and Bollegala, 2019; Friedman et al., 2019; Bolukbasi et al., 2016). A considerable amount of previous work has also gone into either mitigating existing bias in embeddings (Takeshita et al., 2020; Maudslay et al., 2019; Zmigrod et al., 2019; Garg et al., 2018), making them gender neutral (Zhao et al., 2018), or using debiased embeddings (Escud´e Font and Costa-juss`a, 2019). Instead of debiasing and m"
2021.gebnlp-1.8,P19-1160,0,0.0572267,"Missing"
2021.gebnlp-1.8,2020.gebnlp-1.5,0,0.0201141,"et al., 2020) associated with the underlying encoding of gender itself, since only the binary gender categories of male/female are present in the data. While the dataset we use only reflects binary gender categories, we acknowledge the fact that gender as an identity spans a wider spectrum than this. 3 When it comes to gender and gender bias, previous research has been devoted to the identification of bias in textual content and models (Garimella and Mihalcea, 2016; Schofield and Mehr, 2016; Kiritchenko and Mohammad, 2018), and in input representations as static and contextualised embeddings (Takeshita et al., 2020; Bartl et al., 2020; Zhao et al., 2020; Basta et al., 2019; Kaneko and Bollegala, 2019; Friedman et al., 2019; Bolukbasi et al., 2016). A considerable amount of previous work has also gone into either mitigating existing bias in embeddings (Takeshita et al., 2020; Maudslay et al., 2019; Zmigrod et al., 2019; Garg et al., 2018), making them gender neutral (Zhao et al., 2018), or using debiased embeddings (Escud´e Font and Costa-juss`a, 2019). Instead of debiasing and mitigating bias in embeddings, some work has focused on creating gender balanced corpora (Costajuss`a et al., 2020; Costa-juss`a"
2021.gebnlp-1.8,W19-3823,0,0.017731,"d embeddings (Escud´e Font and Costa-juss`a, 2019). Instead of debiasing and mitigating bias in embeddings, some work has focused on creating gender balanced corpora (Costajuss`a et al., 2020; Costa-juss`a and de Jorge, 2020). Related work State-of-the-art results for various NLP tasks nowadays typically build on some pre-trained transformer language models like BERT (Devlin et al., 2019). Despite their great achievements, these models have been shown to include various types of bias (Zhao et al., 2020; Bartl et al., 2020; Basta et al., 2019; Kaneko and Bollegala, 2019; Friedman et al., 2019; Kurita et al., 2019). Recent works have shown the advantage of adding extra information to pre-trained language models for numerous tasks, e.g., dialog systems Several previous studies have focused on gender and gender bias in sentiment analysis, both from data and model perspectives. To name a few: Kiritchenko and Mohammad (2018) propose an evaluation corpus (Equity Evaluation Corpus) that can be used to mitigate biases towards a selection of genders and races. Occupational gender stereotypes exist in sentiment analysis models (Bhaskaran and Bhallamudi, 2019), both in training data and in pre-trained contextuali"
2021.gebnlp-1.8,2020.gebnlp-1.11,1,0.705649,"ey train a BERT model on the title and the texts of each book, and concatenate the output with metadata information and author embeddings from Wikipedia, and feed them into a Multilayer Perceptron (MLP). This work focuses on gender bias, which we identify as the differences in language use between persons, on the unique basis of their genders. The concrete task that we deal with in the current paper is that of polarity classification of book reviews, using labels derived from the numerical ratings assigned by professional critics. We use an existing dataset of book reviews dubbed NoReCgender (Touileb et al., 2020), which is a subset of the Norwegian Review Corpus (Velldal et al., 2018), a dataset primarily used for document-level sentiment analysis. The subset NoReCgender has previously been augmented with information about the gender of both critics and book authors. Through experiments with gender predictions of both critics and book authors, we demonstrate the presence of gendered language in these reviews. Previous work has also shown that the distribution of ratings in the dataset to some degree is correlated with the gender of the critics and the authors. Consequently, work on sentiment classific"
2021.gebnlp-1.8,2021.nodalida-main.4,1,0.88802,"ibution of review ratings (Touileb et al., 2020). Here, we investigate whether this bias is reflected in the text, as measured by classification scores on two tasks, namely binary sentiment and gender classification, and whether adding metadata information explicitly providing the gender of the authors and critics of the reviews, or the sentiment score of the review increases classification performance. Similarly to (Ostendorff et al., 2019), we explore the effects of adding this metadata information to document classification tasks using a BERTbased model, in this case the Norwegian NorBERT (Kutuzov et al., 2021). 4 Unique critics Unique authors M F Total 125 1,435 74 882 199 2,317 Table 1: Total number of unique male and female critics and authors in NoReCgender . pos neg Train Dev. Test Total 568 568 69 60 71 55 708 683 Table 2: Total number of positive and negative reviews in the data splits of NoReCgender . NoReCgender (Touileb et al., 2020). As pointed out by Touileb et al. (2020), some of the reviews were written by children, unknown authors/critics, or by editors, these were not assigned genders and were therefore not included in our work. This results in a set of 4,083 documents. Table 1 shows"
2021.gebnlp-1.8,P18-2104,0,0.0122194,"classifier is fitted on top of a pre-trained LM for a given task, only textual data is considered by the learned representations. In this work we investigate the effect of adding metadata information about demographic variables that are known to be associated with bias in the training data. Specifically, we focus on the task 66 Proceedings of the 3rd Workshop on Gender Bias in Natural Language Processing, pages 66–74 August 5, 2021. ©2021 Association for Computational Linguistics 2 Bias statement (Madotto et al., 2018), natural language inference (Chen et al., 2018), and machine translation (Zaremoodi et al., 2018). Knowledge graphs have also been used to enrich embedding information. Zhang et al. (2019) use entries from Wikidata, as well as their relation to each others, to represent and inject structural knowledge aggregates to a collection of large-scale corpora. They show that their approach reduces noisy data and improves BERT fine-tuning on limited datasets. Bourgonje and Stede (2020) enrich a German BERT model with linguistic knowledge represented as a lexicon as well as manually generated syntactic features. Peinelt et al. (2020) enrich a BERT with LDA topics, and show that this combination impr"
2021.gebnlp-1.8,P19-1139,0,0.0170885,"red by the learned representations. In this work we investigate the effect of adding metadata information about demographic variables that are known to be associated with bias in the training data. Specifically, we focus on the task 66 Proceedings of the 3rd Workshop on Gender Bias in Natural Language Processing, pages 66–74 August 5, 2021. ©2021 Association for Computational Linguistics 2 Bias statement (Madotto et al., 2018), natural language inference (Chen et al., 2018), and machine translation (Zaremoodi et al., 2018). Knowledge graphs have also been used to enrich embedding information. Zhang et al. (2019) use entries from Wikidata, as well as their relation to each others, to represent and inject structural knowledge aggregates to a collection of large-scale corpora. They show that their approach reduces noisy data and improves BERT fine-tuning on limited datasets. Bourgonje and Stede (2020) enrich a German BERT model with linguistic knowledge represented as a lexicon as well as manually generated syntactic features. Peinelt et al. (2020) enrich a BERT with LDA topics, and show that this combination improves performance of semantic similarity. Ostendorff et al. (2019) use a combination of meta"
2021.gebnlp-1.8,2020.acl-main.260,0,0.0172219,"encoding of gender itself, since only the binary gender categories of male/female are present in the data. While the dataset we use only reflects binary gender categories, we acknowledge the fact that gender as an identity spans a wider spectrum than this. 3 When it comes to gender and gender bias, previous research has been devoted to the identification of bias in textual content and models (Garimella and Mihalcea, 2016; Schofield and Mehr, 2016; Kiritchenko and Mohammad, 2018), and in input representations as static and contextualised embeddings (Takeshita et al., 2020; Bartl et al., 2020; Zhao et al., 2020; Basta et al., 2019; Kaneko and Bollegala, 2019; Friedman et al., 2019; Bolukbasi et al., 2016). A considerable amount of previous work has also gone into either mitigating existing bias in embeddings (Takeshita et al., 2020; Maudslay et al., 2019; Zmigrod et al., 2019; Garg et al., 2018), making them gender neutral (Zhao et al., 2018), or using debiased embeddings (Escud´e Font and Costa-juss`a, 2019). Instead of debiasing and mitigating bias in embeddings, some work has focused on creating gender balanced corpora (Costajuss`a et al., 2020; Costa-juss`a and de Jorge, 2020). Related work Stat"
2021.gebnlp-1.8,D18-1521,0,0.0284755,"o the identification of bias in textual content and models (Garimella and Mihalcea, 2016; Schofield and Mehr, 2016; Kiritchenko and Mohammad, 2018), and in input representations as static and contextualised embeddings (Takeshita et al., 2020; Bartl et al., 2020; Zhao et al., 2020; Basta et al., 2019; Kaneko and Bollegala, 2019; Friedman et al., 2019; Bolukbasi et al., 2016). A considerable amount of previous work has also gone into either mitigating existing bias in embeddings (Takeshita et al., 2020; Maudslay et al., 2019; Zmigrod et al., 2019; Garg et al., 2018), making them gender neutral (Zhao et al., 2018), or using debiased embeddings (Escud´e Font and Costa-juss`a, 2019). Instead of debiasing and mitigating bias in embeddings, some work has focused on creating gender balanced corpora (Costajuss`a et al., 2020; Costa-juss`a and de Jorge, 2020). Related work State-of-the-art results for various NLP tasks nowadays typically build on some pre-trained transformer language models like BERT (Devlin et al., 2019). Despite their great achievements, these models have been shown to include various types of bias (Zhao et al., 2020; Bartl et al., 2020; Basta et al., 2019; Kaneko and Bollegala, 2019; Fried"
2021.gebnlp-1.8,P19-1161,0,0.0182429,"comes to gender and gender bias, previous research has been devoted to the identification of bias in textual content and models (Garimella and Mihalcea, 2016; Schofield and Mehr, 2016; Kiritchenko and Mohammad, 2018), and in input representations as static and contextualised embeddings (Takeshita et al., 2020; Bartl et al., 2020; Zhao et al., 2020; Basta et al., 2019; Kaneko and Bollegala, 2019; Friedman et al., 2019; Bolukbasi et al., 2016). A considerable amount of previous work has also gone into either mitigating existing bias in embeddings (Takeshita et al., 2020; Maudslay et al., 2019; Zmigrod et al., 2019; Garg et al., 2018), making them gender neutral (Zhao et al., 2018), or using debiased embeddings (Escud´e Font and Costa-juss`a, 2019). Instead of debiasing and mitigating bias in embeddings, some work has focused on creating gender balanced corpora (Costajuss`a et al., 2020; Costa-juss`a and de Jorge, 2020). Related work State-of-the-art results for various NLP tasks nowadays typically build on some pre-trained transformer language models like BERT (Devlin et al., 2019). Despite their great achievements, these models have been shown to include various types of bias (Zhao et al., 2020; Bartl"
2021.nodalida-main.30,W10-3110,0,0.0491213,"d data set, before presenting the first benchmark results for negation resolution in Section 5. Before concluding, we finally provide a discussion of future work in Section 6. 2 Related Work Below we discuss related work on negation, starting with datasets before moving on to modeling. 2.1 Datasets While NoReCneg is the first dataset annotated for negation for Norwegian, there are a number of existing negation datasets for a range of other languages, such as Chinese (Zou et al., 2016), Dutch (Afzal et al., 2014), English (Pyysalo et al., 2007; Vincze et al., 2008; Morante and Daelemans, 2012; Councill et al., 2010; Konstantinova et al., 2012), German (Cotik et al., 2016), Spanish (Jiménez-Zafra et al., 2018; Diaz et al., 2017), Swedish (Dalianis and Velupillai, 2010; Skeppstedt, 2011), Italian (Altuna et al., 2017), and Japanese (Matsuyoshi et al., 2014). Jiménez-Zafra et al. (2020) provide a thorough survey of existing negation datasets. A large proportion of negation corpora are based on data from the biomedical or clinical domain (Vincze et al., 2008; Dalianis and Velupillai, 2010; Cotik et al., 2016; Diaz et al., 2017). We will here focus on the corpora that are most relevant to the current annotat"
2021.nodalida-main.30,dalianis-velupillai-2010-certain,0,0.0353211,"of future work in Section 6. 2 Related Work Below we discuss related work on negation, starting with datasets before moving on to modeling. 2.1 Datasets While NoReCneg is the first dataset annotated for negation for Norwegian, there are a number of existing negation datasets for a range of other languages, such as Chinese (Zou et al., 2016), Dutch (Afzal et al., 2014), English (Pyysalo et al., 2007; Vincze et al., 2008; Morante and Daelemans, 2012; Councill et al., 2010; Konstantinova et al., 2012), German (Cotik et al., 2016), Spanish (Jiménez-Zafra et al., 2018; Diaz et al., 2017), Swedish (Dalianis and Velupillai, 2010; Skeppstedt, 2011), Italian (Altuna et al., 2017), and Japanese (Matsuyoshi et al., 2014). Jiménez-Zafra et al. (2020) provide a thorough survey of existing negation datasets. A large proportion of negation corpora are based on data from the biomedical or clinical domain (Vincze et al., 2008; Dalianis and Velupillai, 2010; Cotik et al., 2016; Diaz et al., 2017). We will here focus on the corpora that are most relevant to the current annotation effort: the SFU Corpus and the ConanDoyle-neg corpus. The SFU corpus also annotates review data, hence is similar to our work in terms of text type, wh"
2021.nodalida-main.30,W17-1808,0,0.0143249,"finally provide a discussion of future work in Section 6. 2 Related Work Below we discuss related work on negation, starting with datasets before moving on to modeling. 2.1 Datasets While NoReCneg is the first dataset annotated for negation for Norwegian, there are a number of existing negation datasets for a range of other languages, such as Chinese (Zou et al., 2016), Dutch (Afzal et al., 2014), English (Pyysalo et al., 2007; Vincze et al., 2008; Morante and Daelemans, 2012; Councill et al., 2010; Konstantinova et al., 2012), German (Cotik et al., 2016), Spanish (Jiménez-Zafra et al., 2018; Diaz et al., 2017), Swedish (Dalianis and Velupillai, 2010; Skeppstedt, 2011), Italian (Altuna et al., 2017), and Japanese (Matsuyoshi et al., 2014). Jiménez-Zafra et al. (2020) provide a thorough survey of existing negation datasets. A large proportion of negation corpora are based on data from the biomedical or clinical domain (Vincze et al., 2008; Dalianis and Velupillai, 2010; Cotik et al., 2016; Diaz et al., 2017). We will here focus on the corpora that are most relevant to the current annotation effort: the SFU Corpus and the ConanDoyle-neg corpus. The SFU corpus also annotates review data, hence is simil"
2021.nodalida-main.30,P18-2077,0,0.0819798,"andidate tokens. Fancellu et al. (2016) present and compare two neural architectures for the task of negation scope detection on the ConanDoyle-neg corpus: a simple feedforward network and a bidirectional LSTM. Note that these more recent neural systems disregard the task of cue detection altogether (Fancellu et al., 2016; Qian et al., 2016; Fancellu et al., 2017), relying instead on gold cues and focusing solely on the task of scope detection. Finally, Kurtz et al. (2020) cast negation resolution as a graph parsing problem and perform full negation resolution using a dependency graph parser (Dozat and Manning, 2018) to jointly predict cues and scopes. The neural model uses a BiLSTM to create token-level representations, and then includes two feed-forward networks to create head- and dependent-specific token representations. Finally, each possible head-dependent combination is scored using a bilinear model. Despite the conceptual simplicity, this model achieves state-of-the-art results. As such, we use this model to evaluate our annotations and include further details in Section 5. 3 Annotations In the following section we present our negation annotation effort in more detail, including the underlying sou"
2021.nodalida-main.30,W17-1810,1,0.780334,"et al., 2018). 2.2 Modeling Traditional approaches to the task of negation detection have typically employed a wide range of hand-crafted features, and often linguistically informed, derived from constituency parsing (Read et al., 2012; Packard et al., 2014), dependency parsing (Lapponi et al., 2012), or Minimal Recursion Semantics structures created by an HPSG parser (Packard et al., 2014). Scope resolution in particular has often been approached as a sequence labeling task, as pioneered by Morante and Daelemans (2009) and later done in several other works (Lapponi et al., 2012; White, 2012; Enger et al., 2017). More recently, neural approaches have been successfully applied to the task. Qian et al. (2016) propose a CNN model for negation scope detection on the abstracts section of the BioScope corpus, which operates over syntactic paths between the cue and candidate tokens. Fancellu et al. (2016) present and compare two neural architectures for the task of negation scope detection on the ConanDoyle-neg corpus: a simple feedforward network and a bidirectional LSTM. Note that these more recent neural systems disregard the task of cue detection altogether (Fancellu et al., 2016; Qian et al., 2016; Fan"
2021.nodalida-main.30,P16-1047,0,0.0170326,"t al., 2012), or Minimal Recursion Semantics structures created by an HPSG parser (Packard et al., 2014). Scope resolution in particular has often been approached as a sequence labeling task, as pioneered by Morante and Daelemans (2009) and later done in several other works (Lapponi et al., 2012; White, 2012; Enger et al., 2017). More recently, neural approaches have been successfully applied to the task. Qian et al. (2016) propose a CNN model for negation scope detection on the abstracts section of the BioScope corpus, which operates over syntactic paths between the cue and candidate tokens. Fancellu et al. (2016) present and compare two neural architectures for the task of negation scope detection on the ConanDoyle-neg corpus: a simple feedforward network and a bidirectional LSTM. Note that these more recent neural systems disregard the task of cue detection altogether (Fancellu et al., 2016; Qian et al., 2016; Fancellu et al., 2017), relying instead on gold cues and focusing solely on the task of scope detection. Finally, Kurtz et al. (2020) cast negation resolution as a graph parsing problem and perform full negation resolution using a dependency graph parser (Dozat and Manning, 2018) to jointly pre"
2021.nodalida-main.30,W17-1806,0,0.0158299,"s related work on negation, starting with datasets before moving on to modeling. 2.1 Datasets While NoReCneg is the first dataset annotated for negation for Norwegian, there are a number of existing negation datasets for a range of other languages, such as Chinese (Zou et al., 2016), Dutch (Afzal et al., 2014), English (Pyysalo et al., 2007; Vincze et al., 2008; Morante and Daelemans, 2012; Councill et al., 2010; Konstantinova et al., 2012), German (Cotik et al., 2016), Spanish (Jiménez-Zafra et al., 2018; Diaz et al., 2017), Swedish (Dalianis and Velupillai, 2010; Skeppstedt, 2011), Italian (Altuna et al., 2017), and Japanese (Matsuyoshi et al., 2014). Jiménez-Zafra et al. (2020) provide a thorough survey of existing negation datasets. A large proportion of negation corpora are based on data from the biomedical or clinical domain (Vincze et al., 2008; Dalianis and Velupillai, 2010; Cotik et al., 2016; Diaz et al., 2017). We will here focus on the corpora that are most relevant to the current annotation effort: the SFU Corpus and the ConanDoyle-neg corpus. The SFU corpus also annotates review data, hence is similar to our work in terms of text type, whereas ConanDoyle-neg is one of the most widely use"
2021.nodalida-main.30,E17-2010,0,0.0154438,"17). More recently, neural approaches have been successfully applied to the task. Qian et al. (2016) propose a CNN model for negation scope detection on the abstracts section of the BioScope corpus, which operates over syntactic paths between the cue and candidate tokens. Fancellu et al. (2016) present and compare two neural architectures for the task of negation scope detection on the ConanDoyle-neg corpus: a simple feedforward network and a bidirectional LSTM. Note that these more recent neural systems disregard the task of cue detection altogether (Fancellu et al., 2016; Qian et al., 2016; Fancellu et al., 2017), relying instead on gold cues and focusing solely on the task of scope detection. Finally, Kurtz et al. (2020) cast negation resolution as a graph parsing problem and perform full negation resolution using a dependency graph parser (Dozat and Manning, 2018) to jointly predict cues and scopes. The neural model uses a BiLSTM to create token-level representations, and then includes two feed-forward networks to create head- and dependent-specific token representations. Finally, each possible head-dependent combination is scored using a bilinear model. Despite the conceptual simplicity, this model"
2021.nodalida-main.30,D12-1091,0,0.0330494,"We run each experiment five times with different random seeds and report an averaged F1 score and its standard deviation in Table 3. The simplest graph representation point-to-root generally performs best, most visibly in FN F1 (66.8). We attribute the variation in performance to a loss of information in the head-first and headfinal variants, making it impossible to retrieve the correct governing negation cue for partially overlapping scopes, thus lowering the score. In order to see whether these performance differences are statistically significant, we perform bootstrap significance testing (Berg-Kirkpatrick et al., 2012) resampling the test set 106 times while setting the significance threshold to p = 0.05. Comparing point-to-root to head-first and headfinal shows that while the differences seem substantial they are not statistically significant. A manual error analysis on point-to-root shows that the model tends not to predict infrequent cues, e.g., null ‘zero’, istedenfor ‘instead-of’, savnet ‘missing’, while it overpredicts frequent cues, e.g., ikke ‘not’, ingen ‘no’, as well as overgeneralizing the affixal negation u- ‘un-/dis-/non-’ to other words that begin with ‘u’, but are not negated, e.g., utfrika ‘"
2021.nodalida-main.30,W16-5113,0,0.020701,"r negation resolution in Section 5. Before concluding, we finally provide a discussion of future work in Section 6. 2 Related Work Below we discuss related work on negation, starting with datasets before moving on to modeling. 2.1 Datasets While NoReCneg is the first dataset annotated for negation for Norwegian, there are a number of existing negation datasets for a range of other languages, such as Chinese (Zou et al., 2016), Dutch (Afzal et al., 2014), English (Pyysalo et al., 2007; Vincze et al., 2008; Morante and Daelemans, 2012; Councill et al., 2010; Konstantinova et al., 2012), German (Cotik et al., 2016), Spanish (Jiménez-Zafra et al., 2018; Diaz et al., 2017), Swedish (Dalianis and Velupillai, 2010; Skeppstedt, 2011), Italian (Altuna et al., 2017), and Japanese (Matsuyoshi et al., 2014). Jiménez-Zafra et al. (2020) provide a thorough survey of existing negation datasets. A large proportion of negation corpora are based on data from the biomedical or clinical domain (Vincze et al., 2008; Dalianis and Velupillai, 2010; Cotik et al., 2016; Diaz et al., 2017). We will here focus on the corpora that are most relevant to the current annotation effort: the SFU Corpus and the ConanDoyle-neg corpus."
2021.nodalida-main.30,2020.cl-1.5,0,0.06738,"ng on to modeling. 2.1 Datasets While NoReCneg is the first dataset annotated for negation for Norwegian, there are a number of existing negation datasets for a range of other languages, such as Chinese (Zou et al., 2016), Dutch (Afzal et al., 2014), English (Pyysalo et al., 2007; Vincze et al., 2008; Morante and Daelemans, 2012; Councill et al., 2010; Konstantinova et al., 2012), German (Cotik et al., 2016), Spanish (Jiménez-Zafra et al., 2018; Diaz et al., 2017), Swedish (Dalianis and Velupillai, 2010; Skeppstedt, 2011), Italian (Altuna et al., 2017), and Japanese (Matsuyoshi et al., 2014). Jiménez-Zafra et al. (2020) provide a thorough survey of existing negation datasets. A large proportion of negation corpora are based on data from the biomedical or clinical domain (Vincze et al., 2008; Dalianis and Velupillai, 2010; Cotik et al., 2016; Diaz et al., 2017). We will here focus on the corpora that are most relevant to the current annotation effort: the SFU Corpus and the ConanDoyle-neg corpus. The SFU corpus also annotates review data, hence is similar to our work in terms of text type, whereas ConanDoyle-neg is one of the most widely used datasets in the field. The English (Konstantinova et al., 2012) and"
2021.nodalida-main.30,konstantinova-etal-2012-review,0,0.0166974,"Missing"
2021.nodalida-main.30,2020.iwpt-1.3,1,0.807032,"model for negation scope detection on the abstracts section of the BioScope corpus, which operates over syntactic paths between the cue and candidate tokens. Fancellu et al. (2016) present and compare two neural architectures for the task of negation scope detection on the ConanDoyle-neg corpus: a simple feedforward network and a bidirectional LSTM. Note that these more recent neural systems disregard the task of cue detection altogether (Fancellu et al., 2016; Qian et al., 2016; Fancellu et al., 2017), relying instead on gold cues and focusing solely on the task of scope detection. Finally, Kurtz et al. (2020) cast negation resolution as a graph parsing problem and perform full negation resolution using a dependency graph parser (Dozat and Manning, 2018) to jointly predict cues and scopes. The neural model uses a BiLSTM to create token-level representations, and then includes two feed-forward networks to create head- and dependent-specific token representations. Finally, each possible head-dependent combination is scored using a bilinear model. Despite the conceptual simplicity, this model achieves state-of-the-art results. As such, we use this model to evaluate our annotations and include further"
2021.nodalida-main.30,S12-1042,1,0.760218,"to include the subject within the negation scope. This is in contrast to the annotation scheme found in the SFU corpus, where subjects are not included in the negation scope. Note that the NegPar corpus contains a re-annotated version of the ConanDoyle-neg corpus, which fixes known bugs and also adds Chinese data (Liu et al., 2018). 2.2 Modeling Traditional approaches to the task of negation detection have typically employed a wide range of hand-crafted features, and often linguistically informed, derived from constituency parsing (Read et al., 2012; Packard et al., 2014), dependency parsing (Lapponi et al., 2012), or Minimal Recursion Semantics structures created by an HPSG parser (Packard et al., 2014). Scope resolution in particular has often been approached as a sequence labeling task, as pioneered by Morante and Daelemans (2009) and later done in several other works (Lapponi et al., 2012; White, 2012; Enger et al., 2017). More recently, neural approaches have been successfully applied to the task. Qian et al. (2016) propose a CNN model for negation scope detection on the abstracts section of the BioScope corpus, which operates over syntactic paths between the cue and candidate tokens. Fancellu et"
2021.nodalida-main.30,L18-1547,0,0.245822,"with important modifications. In ConanDoyle-neg, the cue is not included in the scope, and it annotates a wide range of cue types, i.e., both sub-token (affixal), single token and multi-token negation cues. Scopes may furthermore be discontinuous, often an effect of the requirement to include the subject within the negation scope. This is in contrast to the annotation scheme found in the SFU corpus, where subjects are not included in the negation scope. Note that the NegPar corpus contains a re-annotated version of the ConanDoyle-neg corpus, which fixes known bugs and also adds Chinese data (Liu et al., 2018). 2.2 Modeling Traditional approaches to the task of negation detection have typically employed a wide range of hand-crafted features, and often linguistically informed, derived from constituency parsing (Read et al., 2012; Packard et al., 2014), dependency parsing (Lapponi et al., 2012), or Minimal Recursion Semantics structures created by an HPSG parser (Packard et al., 2014). Scope resolution in particular has often been approached as a sequence labeling task, as pioneered by Morante and Daelemans (2009) and later done in several other works (Lapponi et al., 2012; White, 2012; Enger et al.,"
2021.nodalida-main.30,matsuyoshi-etal-2014-annotating,0,0.0147736,"with datasets before moving on to modeling. 2.1 Datasets While NoReCneg is the first dataset annotated for negation for Norwegian, there are a number of existing negation datasets for a range of other languages, such as Chinese (Zou et al., 2016), Dutch (Afzal et al., 2014), English (Pyysalo et al., 2007; Vincze et al., 2008; Morante and Daelemans, 2012; Councill et al., 2010; Konstantinova et al., 2012), German (Cotik et al., 2016), Spanish (Jiménez-Zafra et al., 2018; Diaz et al., 2017), Swedish (Dalianis and Velupillai, 2010; Skeppstedt, 2011), Italian (Altuna et al., 2017), and Japanese (Matsuyoshi et al., 2014). Jiménez-Zafra et al. (2020) provide a thorough survey of existing negation datasets. A large proportion of negation corpora are based on data from the biomedical or clinical domain (Vincze et al., 2008; Dalianis and Velupillai, 2010; Cotik et al., 2016; Diaz et al., 2017). We will here focus on the corpora that are most relevant to the current annotation effort: the SFU Corpus and the ConanDoyle-neg corpus. The SFU corpus also annotates review data, hence is similar to our work in terms of text type, whereas ConanDoyle-neg is one of the most widely used datasets in the field. The English (Ko"
2021.nodalida-main.30,S12-1035,0,0.696832,"n the guidelines developed for the biomedical BioScope corpus (Vincze et al., 2008), which largely employ syntactic criteria for the determination of scope, choosing the maximal syntactic unit that contains the negated content. Unlike BioScope, however, negation cues are not included within the scope in SFU. The corpus does not annotate affixal cues, e.g. im- in impossible. The English ConanDoyle-neg corpus contains Sherlock Holmes stories manually annotated for negation cues, scopes, and events (Morante and Daelemans, 2012) and was employed in the 2012 *SEM shared task on negation detection (Morante and Blanco, 2012). The annotation scheme is also based on the scheme employed for the BioScope corpus (Vincze et al., 2008), but with important modifications. In ConanDoyle-neg, the cue is not included in the scope, and it annotates a wide range of cue types, i.e., both sub-token (affixal), single token and multi-token negation cues. Scopes may furthermore be discontinuous, often an effect of the requirement to include the subject within the negation scope. This is in contrast to the annotation scheme found in the SFU corpus, where subjects are not included in the negation scope. Note that the NegPar corpus co"
2021.nodalida-main.30,W09-1105,0,0.339051,"annotated version of the ConanDoyle-neg corpus, which fixes known bugs and also adds Chinese data (Liu et al., 2018). 2.2 Modeling Traditional approaches to the task of negation detection have typically employed a wide range of hand-crafted features, and often linguistically informed, derived from constituency parsing (Read et al., 2012; Packard et al., 2014), dependency parsing (Lapponi et al., 2012), or Minimal Recursion Semantics structures created by an HPSG parser (Packard et al., 2014). Scope resolution in particular has often been approached as a sequence labeling task, as pioneered by Morante and Daelemans (2009) and later done in several other works (Lapponi et al., 2012; White, 2012; Enger et al., 2017). More recently, neural approaches have been successfully applied to the task. Qian et al. (2016) propose a CNN model for negation scope detection on the abstracts section of the BioScope corpus, which operates over syntactic paths between the cue and candidate tokens. Fancellu et al. (2016) present and compare two neural architectures for the task of negation scope detection on the ConanDoyle-neg corpus: a simple feedforward network and a bidirectional LSTM. Note that these more recent neural systems"
2021.nodalida-main.30,morante-daelemans-2012-conandoyle,0,0.266298,"tistics of the final annotated data set, before presenting the first benchmark results for negation resolution in Section 5. Before concluding, we finally provide a discussion of future work in Section 6. 2 Related Work Below we discuss related work on negation, starting with datasets before moving on to modeling. 2.1 Datasets While NoReCneg is the first dataset annotated for negation for Norwegian, there are a number of existing negation datasets for a range of other languages, such as Chinese (Zou et al., 2016), Dutch (Afzal et al., 2014), English (Pyysalo et al., 2007; Vincze et al., 2008; Morante and Daelemans, 2012; Councill et al., 2010; Konstantinova et al., 2012), German (Cotik et al., 2016), Spanish (Jiménez-Zafra et al., 2018; Diaz et al., 2017), Swedish (Dalianis and Velupillai, 2010; Skeppstedt, 2011), Italian (Altuna et al., 2017), and Japanese (Matsuyoshi et al., 2014). Jiménez-Zafra et al. (2020) provide a thorough survey of existing negation datasets. A large proportion of negation corpora are based on data from the biomedical or clinical domain (Vincze et al., 2008; Dalianis and Velupillai, 2010; Cotik et al., 2016; Diaz et al., 2017). We will here focus on the corpora that are most relevant"
2021.nodalida-main.30,2020.lrec-1.618,1,0.656673,"ts for a subset of the Norwegian Review Corpus (NoReC). In addition to providing in-depth discussion of the annotation guidelines, we also present a first set of benchmark results based on a graphparsing approach. 1 Introduction This paper introduces a new data set annotating negation for Norwegian. As shown in the example below, the annotations identify both negation cues (in bold) and their scopes (in brackets) within the sentence: (1) Men kanskje ikke [helt troverdig] . But maybe not completely credible . ‘But maybe not completely credible.’ The underlying corpus is the NoReCfine data set (Øvrelid et al., 2020) – a subset of the Norwegian Review Corpus (NoReC) (Velldal et al., 2018) annotated for fine-grained sentiment, comprising professional reviews from a range of different domains. The new data set introduced here, named NoReCneg , is the first data set of negation for Norwegian. We also present experimental results for negation resolution based on a graph-parsing approach shown to yield state-of-the-art results for other languages. All the resources described in the paper – the data set, the annotation guidelines, the models and the associated code – are made publicly available.1 The rest of th"
2021.nodalida-main.30,P14-1007,0,0.0204614,"inuous, often an effect of the requirement to include the subject within the negation scope. This is in contrast to the annotation scheme found in the SFU corpus, where subjects are not included in the negation scope. Note that the NegPar corpus contains a re-annotated version of the ConanDoyle-neg corpus, which fixes known bugs and also adds Chinese data (Liu et al., 2018). 2.2 Modeling Traditional approaches to the task of negation detection have typically employed a wide range of hand-crafted features, and often linguistically informed, derived from constituency parsing (Read et al., 2012; Packard et al., 2014), dependency parsing (Lapponi et al., 2012), or Minimal Recursion Semantics structures created by an HPSG parser (Packard et al., 2014). Scope resolution in particular has often been approached as a sequence labeling task, as pioneered by Morante and Daelemans (2009) and later done in several other works (Lapponi et al., 2012; White, 2012; Enger et al., 2017). More recently, neural approaches have been successfully applied to the task. Qian et al. (2016) propose a CNN model for negation scope detection on the abstracts section of the BioScope corpus, which operates over syntactic paths between"
2021.nodalida-main.30,D16-1078,0,0.0201949,"y employed a wide range of hand-crafted features, and often linguistically informed, derived from constituency parsing (Read et al., 2012; Packard et al., 2014), dependency parsing (Lapponi et al., 2012), or Minimal Recursion Semantics structures created by an HPSG parser (Packard et al., 2014). Scope resolution in particular has often been approached as a sequence labeling task, as pioneered by Morante and Daelemans (2009) and later done in several other works (Lapponi et al., 2012; White, 2012; Enger et al., 2017). More recently, neural approaches have been successfully applied to the task. Qian et al. (2016) propose a CNN model for negation scope detection on the abstracts section of the BioScope corpus, which operates over syntactic paths between the cue and candidate tokens. Fancellu et al. (2016) present and compare two neural architectures for the task of negation scope detection on the ConanDoyle-neg corpus: a simple feedforward network and a bidirectional LSTM. Note that these more recent neural systems disregard the task of cue detection altogether (Fancellu et al., 2016; Qian et al., 2016; Fancellu et al., 2017), relying instead on gold cues and focusing solely on the task of scope detect"
2021.nodalida-main.30,S12-1041,1,0.792251,"thermore be discontinuous, often an effect of the requirement to include the subject within the negation scope. This is in contrast to the annotation scheme found in the SFU corpus, where subjects are not included in the negation scope. Note that the NegPar corpus contains a re-annotated version of the ConanDoyle-neg corpus, which fixes known bugs and also adds Chinese data (Liu et al., 2018). 2.2 Modeling Traditional approaches to the task of negation detection have typically employed a wide range of hand-crafted features, and often linguistically informed, derived from constituency parsing (Read et al., 2012; Packard et al., 2014), dependency parsing (Lapponi et al., 2012), or Minimal Recursion Semantics structures created by an HPSG parser (Packard et al., 2014). Scope resolution in particular has often been approached as a sequence labeling task, as pioneered by Morante and Daelemans (2009) and later done in several other works (Lapponi et al., 2012; White, 2012; Enger et al., 2017). More recently, neural approaches have been successfully applied to the task. Qian et al. (2016) propose a CNN model for negation scope detection on the abstracts section of the BioScope corpus, which operates over"
2021.nodalida-main.30,E12-2021,0,0.0856201,"Missing"
2021.nodalida-main.30,L18-1661,1,0.55644,"oviding in-depth discussion of the annotation guidelines, we also present a first set of benchmark results based on a graphparsing approach. 1 Introduction This paper introduces a new data set annotating negation for Norwegian. As shown in the example below, the annotations identify both negation cues (in bold) and their scopes (in brackets) within the sentence: (1) Men kanskje ikke [helt troverdig] . But maybe not completely credible . ‘But maybe not completely credible.’ The underlying corpus is the NoReCfine data set (Øvrelid et al., 2020) – a subset of the Norwegian Review Corpus (NoReC) (Velldal et al., 2018) annotated for fine-grained sentiment, comprising professional reviews from a range of different domains. The new data set introduced here, named NoReCneg , is the first data set of negation for Norwegian. We also present experimental results for negation resolution based on a graph-parsing approach shown to yield state-of-the-art results for other languages. All the resources described in the paper – the data set, the annotation guidelines, the models and the associated code – are made publicly available.1 The rest of the paper is structured as follows. We start by reviewing related work on n"
2021.nodalida-main.30,W08-0606,0,0.0590629,"hen summarize the statistics of the final annotated data set, before presenting the first benchmark results for negation resolution in Section 5. Before concluding, we finally provide a discussion of future work in Section 6. 2 Related Work Below we discuss related work on negation, starting with datasets before moving on to modeling. 2.1 Datasets While NoReCneg is the first dataset annotated for negation for Norwegian, there are a number of existing negation datasets for a range of other languages, such as Chinese (Zou et al., 2016), Dutch (Afzal et al., 2014), English (Pyysalo et al., 2007; Vincze et al., 2008; Morante and Daelemans, 2012; Councill et al., 2010; Konstantinova et al., 2012), German (Cotik et al., 2016), Spanish (Jiménez-Zafra et al., 2018; Diaz et al., 2017), Swedish (Dalianis and Velupillai, 2010; Skeppstedt, 2011), Italian (Altuna et al., 2017), and Japanese (Matsuyoshi et al., 2014). Jiménez-Zafra et al. (2020) provide a thorough survey of existing negation datasets. A large proportion of negation corpora are based on data from the biomedical or clinical domain (Vincze et al., 2008; Dalianis and Velupillai, 2010; Cotik et al., 2016; Diaz et al., 2017). We will here focus on the c"
2021.nodalida-main.30,S12-1044,0,0.026626,"se data (Liu et al., 2018). 2.2 Modeling Traditional approaches to the task of negation detection have typically employed a wide range of hand-crafted features, and often linguistically informed, derived from constituency parsing (Read et al., 2012; Packard et al., 2014), dependency parsing (Lapponi et al., 2012), or Minimal Recursion Semantics structures created by an HPSG parser (Packard et al., 2014). Scope resolution in particular has often been approached as a sequence labeling task, as pioneered by Morante and Daelemans (2009) and later done in several other works (Lapponi et al., 2012; White, 2012; Enger et al., 2017). More recently, neural approaches have been successfully applied to the task. Qian et al. (2016) propose a CNN model for negation scope detection on the abstracts section of the BioScope corpus, which operates over syntactic paths between the cue and candidate tokens. Fancellu et al. (2016) present and compare two neural architectures for the task of negation scope detection on the ConanDoyle-neg corpus: a simple feedforward network and a bidirectional LSTM. Note that these more recent neural systems disregard the task of cue detection altogether (Fancellu et al., 2016; Q"
2021.nodalida-main.4,K18-2005,0,0.104033,"rBERT builds heavily on this; see Section 6 for more details. Many low-resource languages do not have dedicated monolingual large-scale language models, and instead resort to using a multilingual model, such as Google’s multilingual BERT model – mBERT – which was trained on data that also included Norwegian. Up until the release of the models described in the current paper, mBERT was the only BERT-instance that could be used for Norwegian.6 Another widely used architecture for contextualised LMs is Embeddings From Language Models or ELMo (Peters et al., 2018). The ElmoForManyLangs initiative (Che et al., 2018) trained and released monolingual ELMo models for a wide range of different languages, including Norwegian (with separate models for Bokm˚al and Nynorsk). However, these models were trained on very modestly sized corpora of 20 million words for each language (randomly sampled from Wikipedia dumps and Common Crawl data). In a parallel effort to that of the current paper, the AI Lab of the National Library of Norway, through their Norwegian Transformer Model (No5 https://github.com/TurkuNLP/FinBERT A BERT model trained on Norwegian data was published at https://github.com/botxo/nordic_bert in th"
2021.nodalida-main.4,N19-1423,0,0.598307,"iling the training process, we present contrastive benchmark results on a suite of NLP tasks for Norwegian. For additional background and access to the data, models, and software, please see: http://norlm.nlpl.eu 1 Introduction In this work, we present NorLM, an ongoing community initiative and emerging collection of largescale contextualised language models for Norwegian. We here introduce the NorELMo and NorBERT models, that have been trained on around two billion tokens of running Norwegian text. We describe the training procedure and compare these models with the multilingual mBERT model (Devlin et al., 2019), as well as an additional Norwegian BERT model developed contemporaneously, with some interesting differences in training data and setup. We report results over a number of Norwegian benchmark datasets, addressing a broad range of diverse NLP tasks: part-of-speech tagging, negation resolution, sentence-level and fine-grained sentiment analysis and named entity recognition (NER). All the models are publicly available for download from the Nordic Language Processing Laboratory (NLPL) Vectors Repository1 with a CC BY 4.0 license. They are also accessible locally, together with the training and s"
2021.nodalida-main.4,P18-2077,0,0.012311,"ssifier with dropout, identical to the one we used for POS tagging earlier. This classifier is also trained for 20 epochs with early stopping and batch size 32. 14 https://github.com/ltgoslo/norne https://github.com/davidsbatista/ NER-Evaluation 15 Fine-grained sentiment analysis NoReCfine is a dataset16 comprising a subset of the Norwegian Review Corpus (NoReC; Velldal et al., 2018) annotated for sentiment holders, targets, expressions, and polarity, as well as the relationships between them (Øvrelid et al., 2020). We here cast the problem as a graph prediction task and train a graph parser (Dozat and Manning, 2018; Kurtz et al., 2020) to predict sentiment graphs. The parser creates token-level representations which is the concatenation of a word embedding, POS tag embedding, lemma embedding, and character embedding created by a character-based LSTM. We further augment these representations with contextualised embeddings from each model. Models are trained for 100 epochs, keeping the best model on development F1 . For span extraction (holders, targets, expressions), we evaluate token-level F1 , and the common Targeted F1 metric, which requires correctly extracting a target (strict) and its polarity. We"
2021.nodalida-main.4,N03-3010,0,0.337854,"Missing"
2021.nodalida-main.4,W17-0237,1,0.852813,"e recipe for recreating the environment on other HPC systems, may contribute to ‘democratising’ large-scale NLP research; if nothing else, it eliminates dependency on commercial cloud computing services. 4 Related work Large-scale deep learning language models (LM) are important components of current NLP systems. They are often based on BERT (Bidirectional Encoder Representations from Transformers) (Devlin et al., 2019) and other contextualised architectures. A number of language-specific initiatives have in recent years released monolingual versions of these models for a number of languages (Fares et al., 2017; Kutuzov and Kuzmenko, 2017; Virtanen et al., 2019; de Vries et al., ˇ 2019; Ulˇcar and Robnik-Sikonja, 2020; Koutsikakis et al., 2020; Nguyen and Nguyen, 2020; Farahani et al., 2020; Malmsten et al., 2020). For our purposes, the most important such previous training effort is that of Virtanen et al. (2019) on creating a BERT model for Finnish – FinBERT5 – as our training setup for creating NorBERT builds heavily on this; see Section 6 for more details. Many low-resource languages do not have dedicated monolingual large-scale language models, and instead resort to using a multilingual model,"
2021.nodalida-main.4,P18-1007,0,0.0130726,"e believe this to be extremely important for more linguistically-oriented studies, where it is critical to deal with words, not with arbitrarily fragmented pieces (even if they are well-performing in practical tasks). The vocabulary for the model is of size 30,000. It is much less than the 120,000 of mBERT, but it is compensated by these entities being almost exclusively Norwegian. The vocabulary was generated from raw text, without, e.g., separating punctuation from word tokens. This means one can feed raw text into NorBERT. For the vocabulary generation, we used the SentencePiece algorithm (Kudo, 2018) and Tokenizers library.10 The resulting Tokenizers model was converted to the standard BERT WordPiece format. The final vocabulary contains several thousand unused wordpiece slots which can be filled in with task-specific lexical entries for further finetuning by future NorBERT users. 6.1 Training technicalities NorBERT corresponds in its configuration to the Google’s Bert-Base Cased for English, with 12 layers and hidden size 768 (Devlin et al., 2019). We used the standard masked language modeling and next sentence prediction losses with the LAMB optimizer (You et al., 2020). The model was t"
2021.nodalida-main.4,2021.nodalida-main.3,0,0.260739,"ch language (randomly sampled from Wikipedia dumps and Common Crawl data). In a parallel effort to that of the current paper, the AI Lab of the National Library of Norway, through their Norwegian Transformer Model (No5 https://github.com/TurkuNLP/FinBERT A BERT model trained on Norwegian data was published at https://github.com/botxo/nordic_bert in the beginning of 2020. However, the vocabulary of this model seems to be broken, and to the best of our knowledge nobody has achieved any meaningful results with it. 6 TraM) project, has released a Norwegian BERT (Base, cased) model dubbed NB-BERT (Kummervold et al., 2021).7 The model is trained on the Colossal Norwegian Corpus, reported to comprise close to 18,5 billion words (109.1 GB of text). In raw numbers, this is about ten times more than the corpus we use for training the NorLM models. However, the vast majority of this is from OCR’ed historical sources, which is bound to introduce at least some noise. In Section 7 below, we demonstrate that in some NLP tasks, a language model trained on less (but arguably cleaner) data can outperform a model trained on larger but noisy corpora. 5 NorELMo NorELMo is a set of bidirectional recurrent ELMo language models"
2021.nodalida-main.4,2020.iwpt-1.3,1,0.691501,"ntical to the one we used for POS tagging earlier. This classifier is also trained for 20 epochs with early stopping and batch size 32. 14 https://github.com/ltgoslo/norne https://github.com/davidsbatista/ NER-Evaluation 15 Fine-grained sentiment analysis NoReCfine is a dataset16 comprising a subset of the Norwegian Review Corpus (NoReC; Velldal et al., 2018) annotated for sentiment holders, targets, expressions, and polarity, as well as the relationships between them (Øvrelid et al., 2020). We here cast the problem as a graph prediction task and train a graph parser (Dozat and Manning, 2018; Kurtz et al., 2020) to predict sentiment graphs. The parser creates token-level representations which is the concatenation of a word embedding, POS tag embedding, lemma embedding, and character embedding created by a character-based LSTM. We further augment these representations with contextualised embeddings from each model. Models are trained for 100 epochs, keeping the best model on development F1 . For span extraction (holders, targets, expressions), we evaluate token-level F1 , and the common Targeted F1 metric, which requires correctly extracting a target (strict) and its polarity. We also evaluate Labelle"
2021.nodalida-main.4,E17-3025,1,0.74015,"ing the environment on other HPC systems, may contribute to ‘democratising’ large-scale NLP research; if nothing else, it eliminates dependency on commercial cloud computing services. 4 Related work Large-scale deep learning language models (LM) are important components of current NLP systems. They are often based on BERT (Bidirectional Encoder Representations from Transformers) (Devlin et al., 2019) and other contextualised architectures. A number of language-specific initiatives have in recent years released monolingual versions of these models for a number of languages (Fares et al., 2017; Kutuzov and Kuzmenko, 2017; Virtanen et al., 2019; de Vries et al., ˇ 2019; Ulˇcar and Robnik-Sikonja, 2020; Koutsikakis et al., 2020; Nguyen and Nguyen, 2020; Farahani et al., 2020; Malmsten et al., 2020). For our purposes, the most important such previous training effort is that of Virtanen et al. (2019) on creating a BERT model for Finnish – FinBERT5 – as our training setup for creating NorBERT builds heavily on this; see Section 6 for more details. Many low-resource languages do not have dedicated monolingual large-scale language models, and instead resort to using a multilingual model, such as Google’s multilingua"
2021.nodalida-main.4,2021.nodalida-main.30,1,0.712644,"models on the POS tagging of Bokm˚al (BM) and Nynorsk (NN) test sets in comparison with other large pretrained models for Norwegian. Running times in minutes are given for Bokm˚al. the POS tagging task, training the models for 20 epochs and keeping the model that performs best on the development data. For ELMo models, we used a BiLSTM with global max pooling, taking ELMo token embeddings from the top layer as an input. The evaluation metric is macro F1 . Negation detection Finally, the NoReCfine dataset has recently been annotated with negation cues and their corresponding in-sentence scopes (Mæhlum et al., 2021). The resulting dataset is dubbed NoReCneg .18 We use the same graph-based modeling approach as described for fine-grained sentiment above. We evaluate on the same metrics as in the *SEM 2012 shared task (Morante and Blanco, 2012): cue-level F1 (CUE), scope token F1 over individual tokens (ST), and the combined full negation F1 (FN). 7.2 Results We present the results for the various benchmarking tasks below. POS tagging As can be seen from Table 2, NorBERT outperforms mBERT on both tasks: on POS tagging for Bokm˚al by 5 percentage points and 1 percentage point for Nynorsk. NorBERT is almost o"
2021.nodalida-main.4,S12-1035,0,0.0391265,"r 20 epochs and keeping the model that performs best on the development data. For ELMo models, we used a BiLSTM with global max pooling, taking ELMo token embeddings from the top layer as an input. The evaluation metric is macro F1 . Negation detection Finally, the NoReCfine dataset has recently been annotated with negation cues and their corresponding in-sentence scopes (Mæhlum et al., 2021). The resulting dataset is dubbed NoReCneg .18 We use the same graph-based modeling approach as described for fine-grained sentiment above. We evaluate on the same metrics as in the *SEM 2012 shared task (Morante and Blanco, 2012): cue-level F1 (CUE), scope token F1 over individual tokens (ST), and the combined full negation F1 (FN). 7.2 Results We present the results for the various benchmarking tasks below. POS tagging As can be seen from Table 2, NorBERT outperforms mBERT on both tasks: on POS tagging for Bokm˚al by 5 percentage points and 1 percentage point for Nynorsk. NorBERT is almost on par with NB-BERT on POS tagging. NorELMo models are outperformed by NB-BERT and NorBERT, but are on par with mBERT in POS tagging. Note that their adaptation to the tasks (extracting token embeddings and learning a classifier) t"
2021.nodalida-main.4,2020.findings-emnlp.92,0,0.0467805,"Missing"
2021.nodalida-main.4,L16-1250,1,0.838941,". Below we first provide an overview of the different tasks and the corresponding classifiers that we train, before turning to discuss the results. 7.1 Task descriptions We start by briefly describing each task and associated dataset, in addition to the architectures we use. The sentence counts for the different datasets and their train, dev. and test splits are provided in Table 1. Part-of-speech tagging The Norwegian Dependency Treebank (NDT) (Solberg et al., 2014) includes annotation of POS tags for both Bokm˚al and Nynorsk. NDT has also been converted to the Universal Dependencies format (Øvrelid and Hohle, 2016; Velldal et al., 2017) and this is the version we are using here (for UD 2.7) for predicting UPOS tags. We use a typical sequence labelling approach with the BERT models, adding a linear layer after the final token representations and taking the softmax to get token predictions. We fine-tune all parameters for 20 epochs, using a learning rate of 2e-5, a training batch size of 8, max length of 256, and keep the best model on the development set. ELMo models were not fine-tuned, following the recommendations from Peters et al. (2019). Instead we trained a simple neural classifier (a feed forwar"
2021.nodalida-main.4,2020.lrec-1.618,1,0.9224,"resentations across all 3 layers) for all words. Then, these token embeddings are fed to a neural classifier with dropout, identical to the one we used for POS tagging earlier. This classifier is also trained for 20 epochs with early stopping and batch size 32. 14 https://github.com/ltgoslo/norne https://github.com/davidsbatista/ NER-Evaluation 15 Fine-grained sentiment analysis NoReCfine is a dataset16 comprising a subset of the Norwegian Review Corpus (NoReC; Velldal et al., 2018) annotated for sentiment holders, targets, expressions, and polarity, as well as the relationships between them (Øvrelid et al., 2020). We here cast the problem as a graph prediction task and train a graph parser (Dozat and Manning, 2018; Kurtz et al., 2020) to predict sentiment graphs. The parser creates token-level representations which is the concatenation of a word embedding, POS tag embedding, lemma embedding, and character embedding created by a character-based LSTM. We further augment these representations with contextualised embeddings from each model. Models are trained for 100 epochs, keeping the best model on development F1 . For span extraction (holders, targets, expressions), we evaluate token-level F1 , and the"
2021.nodalida-main.4,N18-1202,0,0.459415,"nish – FinBERT5 – as our training setup for creating NorBERT builds heavily on this; see Section 6 for more details. Many low-resource languages do not have dedicated monolingual large-scale language models, and instead resort to using a multilingual model, such as Google’s multilingual BERT model – mBERT – which was trained on data that also included Norwegian. Up until the release of the models described in the current paper, mBERT was the only BERT-instance that could be used for Norwegian.6 Another widely used architecture for contextualised LMs is Embeddings From Language Models or ELMo (Peters et al., 2018). The ElmoForManyLangs initiative (Che et al., 2018) trained and released monolingual ELMo models for a wide range of different languages, including Norwegian (with separate models for Bokm˚al and Nynorsk). However, these models were trained on very modestly sized corpora of 20 million words for each language (randomly sampled from Wikipedia dumps and Common Crawl data). In a parallel effort to that of the current paper, the AI Lab of the National Library of Norway, through their Norwegian Transformer Model (No5 https://github.com/TurkuNLP/FinBERT A BERT model trained on Norwegian data was pub"
2021.nodalida-main.4,W19-4302,0,0.0601483,"Missing"
2021.nodalida-main.4,2020.acl-demos.14,0,0.119749,"is important for BERT-like models, because one of their training tasks is next sentence prediction). In total, our training corpus comprises about two billion (1,907,072,909) word tokens in 203 million (202,802,665) sentences. We conducted the following pre-processing steps: 1. Wikipedia texts were extracted from the dumps using the segment wiki script ˇ uˇrek and Sojka, from the Gensim project (Reh˚ 2010). 2. For the news texts from Norwegian Aviskorpus, we performed de-tokenization and conversion to UTF-8 encoding, where required. 3. The resulting corpus was sentencesegmented using Stanza (Qi et al., 2020). We left blank lines between documents (and 4 https://www.nb.no/sprakbanken/ ressurskatalog/oai-nb-no-sbr-4/ sections in the case of Wikipedia) so that the ‘next sentence prediction’ task of BERT does not span between documents. 3 Prerequisites: software and computing Developing very large contextualised language models is no small challenge, both in terms of engineering sophistication and computing demands. Training ELMo- and in particular BERT-like models presupposes access to specialised hardware – graphical processing units (GPUs) – over extended periods of time. Compared to the original"
2021.nodalida-main.4,solberg-etal-2014-norwegian,1,0.845115,"mBERT and to the recently released NB-BERT model described in Section 4. Where applicable, we show separate evaluation results for Bokm˚al and Nynorsk. Below we first provide an overview of the different tasks and the corresponding classifiers that we train, before turning to discuss the results. 7.1 Task descriptions We start by briefly describing each task and associated dataset, in addition to the architectures we use. The sentence counts for the different datasets and their train, dev. and test splits are provided in Table 1. Part-of-speech tagging The Norwegian Dependency Treebank (NDT) (Solberg et al., 2014) includes annotation of POS tags for both Bokm˚al and Nynorsk. NDT has also been converted to the Universal Dependencies format (Øvrelid and Hohle, 2016; Velldal et al., 2017) and this is the version we are using here (for UD 2.7) for predicting UPOS tags. We use a typical sequence labelling approach with the BERT models, adding a linear layer after the final token representations and taking the softmax to get token predictions. We fine-tune all parameters for 20 epochs, using a learning rate of 2e-5, a training batch size of 8, max length of 256, and keep the best model on the development set"
2021.nodalida-main.4,2020.lrec-1.582,0,0.0422658,"Missing"
2021.nodalida-main.4,L18-1661,1,0.755579,"Missing"
2021.nodalida-main.4,W17-0201,1,0.823933,"an overview of the different tasks and the corresponding classifiers that we train, before turning to discuss the results. 7.1 Task descriptions We start by briefly describing each task and associated dataset, in addition to the architectures we use. The sentence counts for the different datasets and their train, dev. and test splits are provided in Table 1. Part-of-speech tagging The Norwegian Dependency Treebank (NDT) (Solberg et al., 2014) includes annotation of POS tags for both Bokm˚al and Nynorsk. NDT has also been converted to the Universal Dependencies format (Øvrelid and Hohle, 2016; Velldal et al., 2017) and this is the version we are using here (for UD 2.7) for predicting UPOS tags. We use a typical sequence labelling approach with the BERT models, adding a linear layer after the final token representations and taking the softmax to get token predictions. We fine-tune all parameters for 20 epochs, using a learning rate of 2e-5, a training batch size of 8, max length of 256, and keep the best model on the development set. ELMo models were not fine-tuned, following the recommendations from Peters et al. (2019). Instead we trained a simple neural classifier (a feed forward network with one hidd"
2021.nodalida-main.4,K18-2001,0,0.0327235,"Missing"
2021.nodalida-main.41,W13-3819,0,0.0321089,"Missing"
2021.nodalida-main.41,2020.acl-main.493,0,0.0218462,"with far larger parameter spaces and more training data. This is something of a middle-of-the-road approach; future work could involve this sort of evaluation on downscaled transformer models, which we shy away from in order to provide a usable model release. We hope that the differences between these models provide some insight, and pave the way for further research, not only specifically addressing the question of sampling from a perspective of performance, but also analytically. There has already been considerable work in this direction on multilingual variants of BERT (Pires et al., 2019; Chi et al., 2020), and we hope that this work motivates papers applying the same to recurrent mELMo, as well as comparing and contrasting the two. The ELMo models described in this paper are publicly released via NLPL Vector Repository.1 Acknowledgements Our experiments were run on resources provided by UNINETT Sigma2 - the National Infrastructure for High Performance Computing and Data Storage in Norway, under the NeIC-NLPL umbrella. References Naveen Arivazhagan, Ankur Bapna, Orhan Firat, Dmitry Lepikhin, Melvin Johnson, Maxim Krikun, Mia Xu Chen, Yuan Cao, George Foster, Colin Cherry, Wolfgang Macherey, Zhi"
2021.nodalida-main.41,2020.acl-main.747,0,0.0999181,"Missing"
2021.nodalida-main.41,D18-1269,0,0.0295844,"tions onto the PoS label space. PoS tagging (transfer): We use the same architecture as for regular PoS tagging, but train on English and evaluate on our target languages. Dependency parsing: We use dependencyannotated Universal Dependencies corpora; our metrics are both unlabelled and labelled attachment scores (UAS/LAS). Our parsing architecture is a biaffine graph-based parser (Dozat and Manning, 2018). XNLI: A transfer-based language inference task; we use Chen et al.’s 2017 ESIM architecture, train a tagging head on English, and evaluate on the translated dev portions of other languages (Conneau et al., 2018). Tatoeba: The task here is to pick out, for each sentence in our source corpus (English), the appropriate translation of the sentence in our target language corpus. This, in a sense, is the most ‘raw’ tasks; target language sentences are Figure 1: Performance difference between monolingual and multilingual models, on our monolingual tasks. Absent bars indicate that the language was missing. ranked based on similarity. We follow Hu et al. (2020) and use the Tatoeba dataset. We tokenize all our text using the relevant UDPipe (Straka et al., 2019) model, and train/evaluate on each task three tim"
2021.nodalida-main.41,P19-1493,0,0.0873757,", we contextualise our work in the present literature. Section 3 describes our experimental setup and Section 4 our results. Finally, we conclude with a discussion of our results in Section 5. 2 Prior work Multilingual embedding architectures (static or contextualised) are different from cross-lingual ones (Ruder et al., 2019; Liu et al., 2019) in that they are not products of aligning several monolingual models. Instead, a deep neural model is trained end to end on texts in multiple languages, thus making the whole process more straightforward and yielding truly multilingual representations (Pires et al., 2019). Following Artetxe et al. (2020), we will use the term ‘deep multilingual pretraining’ for such approaches. One of the early examples of deep multilingual pretraining was BERT, which featured a multilingual variant trained on the 104 largest languagespecific Wikipedias (Devlin et al., 2019). To counter the effects of some languages having overwhelmingly larger Wikipedias than others, Devlin et al. (2019) used exponentially smoothed data weighting; i.e., they exponentiated the probability of a token being in a certain language by a certain α, and re-normalised. This has the effect of ‘squashin"
2021.nodalida-main.41,W19-4212,0,0.043736,"Missing"
2021.nodalida-main.41,2020.lrec-1.582,0,0.0981297,"Missing"
C10-1155,W02-1503,0,0.0438619,"different types of linguistic information for the hedge resolution task. MaltParser is based on a deterministic parsing strategy in combination with treebank-induced classifiers for predicting parse transitions. It supports a rich feature representation of the parse history in order to guide parsing and may easily be extended to take additional features into account. The procedure to enable the data-driven parser to learn from the grammar-driven parser is quite simple. We parse a treebank with the XLE platform (Crouch et al., 2008) and the English grammar developed within the ParGram project (Butt, Dyvik, King, Masuichi, & Rohrer, 2002). We then convert the LFG output to dependency structures, so that we have two parallel versions of the treebank—one gold standard and one with LFG annotation. We extend the gold standard treebank with additional information from the corresponding LFG analysis and train MaltParser on the enhanced data set. Table 2 shows the enhanced dependency representation of example (1) above, taken from the training data. For each token, the parsed data contains information on the word form, lemma, and part of speech (PoS), as well as on the head and dependency relation in columns 6 and 7. The added XLE i"
C10-1155,W01-0521,0,0.029999,"enn Treebank (PTB), converted to dependency format (Johansson & Nugues, 2007) and extended with XLE features, as described above. Parsing uses the arc-eager mode of MaltParser and an SVM with a polynomial kernel. When tested using 10-fold cross validation on the enhanced PTB, the parser achieves a labeled accuracy score of 89.8. PoS Tagging and Domain Variation Our parser is trained on financial news, and although stacking with a general-purpose LFG parser is ex1381 pected to aid domain portability, substantial differences in domain and genre are bound to negatively affect syntactic analysis (Gildea, 2001). MaltParser presupposes that inputs have been PoS tagged, leaving room for variation in preprocessing. On the one hand, we aim to make parser inputs maximally similar to its training data (i.e. the conventions established in the PTB); on the other hand we wish to benefit from specialized resources for the biomedical domain. The GENIA tagger (Tsuruoka et al., 2005) is particularly relevant in this respect (as could be the GENIA Treebank proper4 ). However, we found that GENIA tokenization does not match the PTB conventions in about one out of five sentences (for example wrongly splitting token"
C10-1155,W07-2416,0,0.0146943,"bove, taken from the training data. For each token, the parsed data contains information on the word form, lemma, and part of speech (PoS), as well as on the head and dependency relation in columns 6 and 7. The added XLE information resides in the F E A T S column, and in the XLE-specific head and dependency columns 8 and 9. Parser outputs, which in turn form the basis for our scope resolution rules discussed in Section 5, also take this same form. The parser employed in this work is trained on the Wall Street Journal sections 2 – 24 of the Penn Treebank (PTB), converted to dependency format (Johansson & Nugues, 2007) and extended with XLE features, as described above. Parsing uses the arc-eager mode of MaltParser and an SVM with a polynomial kernel. When tested using 10-fold cross validation on the enhanced PTB, the parser achieves a labeled accuracy score of 89.8. PoS Tagging and Domain Variation Our parser is trained on financial news, and although stacking with a general-purpose LFG parser is ex1381 pected to aid domain portability, substantial differences in domain and genre are bound to negatively affect syntactic analysis (Gildea, 2001). MaltParser presupposes that inputs have been PoS tagged, leavi"
C10-1155,W08-0607,0,0.0122864,"gest: This task falls within the scope of semantic analysis of sentences exploiting syntactic patterns [...]. The utility of syntactic information within various approaches to sentiment analysis in natural language has been an issue of some debate (Wilson, Wiebe, & Hwa, 2006; Ng, Dasgupta, & Arifin, 2006), and the potential contribution of syntax clearly varies with the specifics of the task. Previous work in the hedging realm has largely been concerned with cue detection, i.e. identifying uncertainty cues such as may in (1), which are predominantly individual tokens (Medlock & Briscoe, 2007; Kilicoglu & Bergler, 2008). There has been little previous work aimed at actually resolving the scope of such hedge cues, which presumably constitutes a somewhat different and likely more difficult problem. Morante and Daelemans (2009) present a machine-learning approach to this task, using token-level, lexical information only. To this end, CoNLL 2010 enters largely uncharted territory, and it remains to be seen (a) whether syntactic analysis indeed is a necessary component in approaching this task and, more generally, (b) to what degree the specific task setup can inform us about the strong and weak points in current"
C10-1155,P07-1125,0,0.0132078,"he organizers further suggest: This task falls within the scope of semantic analysis of sentences exploiting syntactic patterns [...]. The utility of syntactic information within various approaches to sentiment analysis in natural language has been an issue of some debate (Wilson, Wiebe, & Hwa, 2006; Ng, Dasgupta, & Arifin, 2006), and the potential contribution of syntax clearly varies with the specifics of the task. Previous work in the hedging realm has largely been concerned with cue detection, i.e. identifying uncertainty cues such as may in (1), which are predominantly individual tokens (Medlock & Briscoe, 2007; Kilicoglu & Bergler, 2008). There has been little previous work aimed at actually resolving the scope of such hedge cues, which presumably constitutes a somewhat different and likely more difficult problem. Morante and Daelemans (2009) present a machine-learning approach to this task, using token-level, lexical information only. To this end, CoNLL 2010 enters largely uncharted territory, and it remains to be seen (a) whether syntactic analysis indeed is a necessary component in approaching this task and, more generally, (b) to what degree the specific task setup can inform us about the stron"
C10-1155,W10-3006,0,0.331143,"approaches and technology. In this article, we investigate the contribution of syntax to hedge resolution, by reflecting on our experience in the CoNLL 2010 task.2 Our CoNLL system submission ranked fourth (of 24) on Task 1 and third (of 15) on Task 2, for an overall best average result (there appears to be very limited overlap among top performers for the two subtasks). 2 It turns out, in fact, that all the top-performing systems in Task 2 of the CoNLLShared Task rely on syntactic information provided by parsers, either in features for machine learning or as input to manually crafted rules (Morante, Asch, & Daelemans, 2010; Rei & Briscoe, 2010). 1379 Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1379–1387, Beijing, August 2010 Abstracts Articles Total Sentences Hedged Sentences 11871 2670 14541 2101 519 2620 Cues Multi-Word Cues Tokens Cue Tokens 2659 668 3327 309634 68579 378213 364 84 448 3056 782 3838 Table 1: Summary statistics for the Shared Task training data. This article transcends our CoNLL system description (Velldal, Øvrelid, & Oepen, 2010) in several respects, presenting updated and improved cue detection results (§ 3 and § 4), focusing on the rol"
C10-1155,W09-1304,0,0.0614029,"anguage has been an issue of some debate (Wilson, Wiebe, & Hwa, 2006; Ng, Dasgupta, & Arifin, 2006), and the potential contribution of syntax clearly varies with the specifics of the task. Previous work in the hedging realm has largely been concerned with cue detection, i.e. identifying uncertainty cues such as may in (1), which are predominantly individual tokens (Medlock & Briscoe, 2007; Kilicoglu & Bergler, 2008). There has been little previous work aimed at actually resolving the scope of such hedge cues, which presumably constitutes a somewhat different and likely more difficult problem. Morante and Daelemans (2009) present a machine-learning approach to this task, using token-level, lexical information only. To this end, CoNLL 2010 enters largely uncharted territory, and it remains to be seen (a) whether syntactic analysis indeed is a necessary component in approaching this task and, more generally, (b) to what degree the specific task setup can inform us about the strong and weak points in current approaches and technology. In this article, we investigate the contribution of syntax to hedge resolution, by reflecting on our experience in the CoNLL 2010 task.2 Our CoNLL system submission ranked fourth (o"
C10-1155,P06-2079,0,0.0325861,"out this paper, angle brackets highlight hedge cues, and curly braces indicate the scope of a given cue, as annotated in BioScope. sentences containing uncertainty; the objective of Task 2 is learning to resolve the in-sentence scope of hedge cues (Farkas, Vincze, Mora, Csirik, & Szarvas, 2010). The organizers further suggest: This task falls within the scope of semantic analysis of sentences exploiting syntactic patterns [...]. The utility of syntactic information within various approaches to sentiment analysis in natural language has been an issue of some debate (Wilson, Wiebe, & Hwa, 2006; Ng, Dasgupta, & Arifin, 2006), and the potential contribution of syntax clearly varies with the specifics of the task. Previous work in the hedging realm has largely been concerned with cue detection, i.e. identifying uncertainty cues such as may in (1), which are predominantly individual tokens (Medlock & Briscoe, 2007; Kilicoglu & Bergler, 2008). There has been little previous work aimed at actually resolving the scope of such hedge cues, which presumably constitutes a somewhat different and likely more difficult problem. Morante and Daelemans (2009) present a machine-learning approach to this task, using token-level,"
C10-1155,nivre-etal-2006-maltparser,0,0.0404639,"ndency representation of example (1), with MaltParser and XLE annotations. unit possible. For evaluation purposes, the task organizers provided newly annotated biomedical articles, following the same general BioScope principles. The CoNLL 2010 evaluation data comprises 5,003 additional utterances (138,276 tokens), of which 790 are annotated as hedged. The data contains a total of 1033 cues, of which 87 are so-called multiword cues (i.e. cues spanning multiple tokens), comprising 1148 cue tokens altogether. Stacked Dependency Parsing For syntactic analysis we employ the open-source MaltParser (Nivre, Hall, & Nilsson, 2006), a platform for data-driven dependency parsing. For improved accuracy and portability across domains and genres, we make our parser incorporate the predictions of a large-scale, general-purpose LFG parser—following the work of Øvrelid, Kuhn, and Spreyer (2009). A technique dubbed parser stacking enables the data-driven parser to learn, not only from gold standard treebank annotations, but from the output of another parser (Nivre & McDonald, 2008). This technique has been shown to provide significant improvements in accuracy for both English and German (Øvrelid et al., 2009), and a similar se"
C10-1155,P08-1108,0,0.032951,"ltiple tokens), comprising 1148 cue tokens altogether. Stacked Dependency Parsing For syntactic analysis we employ the open-source MaltParser (Nivre, Hall, & Nilsson, 2006), a platform for data-driven dependency parsing. For improved accuracy and portability across domains and genres, we make our parser incorporate the predictions of a large-scale, general-purpose LFG parser—following the work of Øvrelid, Kuhn, and Spreyer (2009). A technique dubbed parser stacking enables the data-driven parser to learn, not only from gold standard treebank annotations, but from the output of another parser (Nivre & McDonald, 2008). This technique has been shown to provide significant improvements in accuracy for both English and German (Øvrelid et al., 2009), and a similar setup employing an HPSG grammar has been shown to increase domain independence in data-driven dependency parsing (Zhang & Wang, 2009). The stacked parser combines two quite different approaches—data-driven dependency parsing and ‘deep’ parsing with a handcrafted grammar—and thus provides us with a broad range of different types of linguistic information for the hedge resolution task. MaltParser is based on a deterministic parsing strategy in combinat"
C10-1155,P09-2010,1,0.841272,"altParser (Nivre, Hall, & Nilsson, 2006), a platform for data-driven dependency parsing. For improved accuracy and portability across domains and genres, we make our parser incorporate the predictions of a large-scale, general-purpose LFG parser—following the work of Øvrelid, Kuhn, and Spreyer (2009). A technique dubbed parser stacking enables the data-driven parser to learn, not only from gold standard treebank annotations, but from the output of another parser (Nivre & McDonald, 2008). This technique has been shown to provide significant improvements in accuracy for both English and German (Øvrelid et al., 2009), and a similar setup employing an HPSG grammar has been shown to increase domain independence in data-driven dependency parsing (Zhang & Wang, 2009). The stacked parser combines two quite different approaches—data-driven dependency parsing and ‘deep’ parsing with a handcrafted grammar—and thus provides us with a broad range of different types of linguistic information for the hedge resolution task. MaltParser is based on a deterministic parsing strategy in combination with treebank-induced classifiers for predicting parse transitions. It supports a rich feature representation of the parse his"
C10-1155,W10-3008,0,0.197988,"s article, we investigate the contribution of syntax to hedge resolution, by reflecting on our experience in the CoNLL 2010 task.2 Our CoNLL system submission ranked fourth (of 24) on Task 1 and third (of 15) on Task 2, for an overall best average result (there appears to be very limited overlap among top performers for the two subtasks). 2 It turns out, in fact, that all the top-performing systems in Task 2 of the CoNLLShared Task rely on syntactic information provided by parsers, either in features for machine learning or as input to manually crafted rules (Morante, Asch, & Daelemans, 2010; Rei & Briscoe, 2010). 1379 Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1379–1387, Beijing, August 2010 Abstracts Articles Total Sentences Hedged Sentences 11871 2670 14541 2101 519 2620 Cues Multi-Word Cues Tokens Cue Tokens 2659 668 3327 309634 68579 378213 364 84 448 3056 782 3838 Table 1: Summary statistics for the Shared Task training data. This article transcends our CoNLL system description (Velldal, Øvrelid, & Oepen, 2010) in several respects, presenting updated and improved cue detection results (§ 3 and § 4), focusing on the role of syntactic informa"
C10-1155,W10-3007,1,0.834253,"formation provided by parsers, either in features for machine learning or as input to manually crafted rules (Morante, Asch, & Daelemans, 2010; Rei & Briscoe, 2010). 1379 Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1379–1387, Beijing, August 2010 Abstracts Articles Total Sentences Hedged Sentences 11871 2670 14541 2101 519 2620 Cues Multi-Word Cues Tokens Cue Tokens 2659 668 3327 309634 68579 378213 364 84 448 3056 782 3838 Table 1: Summary statistics for the Shared Task training data. This article transcends our CoNLL system description (Velldal, Øvrelid, & Oepen, 2010) in several respects, presenting updated and improved cue detection results (§ 3 and § 4), focusing on the role of syntactic information rather than on machine learning specifics (§ 5 and § 6), providing an analysis and discussion of Task 2 errors (§ 7), and generally aiming to gauge the value of available annotated data and processing tools (§ 8). We present a hybrid, two-level approach for hedge resolution, where a statistical classifier detects cue words, and a small set of manually crafted rules operating over syntactic structures resolve scope. We show how syntactic information—produced"
C10-1155,W08-0606,0,0.470685,"result (in terms of both combined ranks and average F1 ) in the 2010 CoNLL Shared Task. 1 Background—Motivation Recent years have witnessed an increased interest in the analysis of various aspects of sentiment in natural language (Pang & Lee, 2008). The subtask of hedge resolution deals with the analysis of uncertainty as expressed in natural language, and the linguistic means (so-called hedges) by which speculation or uncertainty are expressed. Information of this kind is of importance for various mining tasks which aim at extracting factual data. Example (1), taken from the BioScope corpus (Vincze, Szarvas, Farkas, Móra, & Csirik, 2008), shows a sentence where uncertainty is signaled by the modal verb may.1 {The unknown amino acid hmayi be used by these species}. (1) The topic of the Shared Task at the 2010 Conference for Natural Language Learning (CoNLL) is hedge detection in biomedical literature—in a sense ‘zooming in’ on one particular aspect of the broader BioNLP Shared Task in 2009 (Kim, Ohta, Pyysalo, Kano, & Tsujii, 2009). It involves two subtasks: Task 1 is described as learning to detect 1 In examples throughout this paper, angle brackets highlight hedge cues, and curly braces indicate the scope of a given cue, as"
C10-1155,P09-1043,0,0.0301455,"s, we make our parser incorporate the predictions of a large-scale, general-purpose LFG parser—following the work of Øvrelid, Kuhn, and Spreyer (2009). A technique dubbed parser stacking enables the data-driven parser to learn, not only from gold standard treebank annotations, but from the output of another parser (Nivre & McDonald, 2008). This technique has been shown to provide significant improvements in accuracy for both English and German (Øvrelid et al., 2009), and a similar setup employing an HPSG grammar has been shown to increase domain independence in data-driven dependency parsing (Zhang & Wang, 2009). The stacked parser combines two quite different approaches—data-driven dependency parsing and ‘deep’ parsing with a handcrafted grammar—and thus provides us with a broad range of different types of linguistic information for the hedge resolution task. MaltParser is based on a deterministic parsing strategy in combination with treebank-induced classifiers for predicting parse transitions. It supports a rich feature representation of the parse history in order to guide parsing and may easily be extended to take additional features into account. The procedure to enable the data-driven parser to"
C10-1155,W09-1401,0,\N,Missing
C10-1155,W10-3001,0,\N,Missing
C18-1117,P14-1023,0,0.062705,"ch queries. Researchers also studied the increase or decrease in the frequency of a word A collocating with another word B over time, and based on this inferred changes in the meaning of A (Heyer et al., 2009). However, it is clear that semantic shifts are not always accompanied with changes in word frequency (or this connection may be very subtle and non-direct). Thus, if one were able to more directly model word meaning, such an approach should be superior to frequency-proxied methods. A number of recent publications have showed that distributional word representations (Turney et al., 2010; Baroni et al., 2014) provide an efficient way to solve these tasks. They represent meaning with sparse or dense (embedding) vectors, produced from word co-occurrence counts. Although conceptually the source of the data for these models is still word frequencies, they ‘compress’ this information into continuous lexical representations which are both efficient and convenient to work with. Indeed, Kulkarni et al. (2015) explicitly demonstrated that distributional models outperform the frequency-based methods in detecting semantic shifts. They managed to trace semantic shifts more precisely and with greater explanato"
C18-1117,D17-1118,0,0.714461,"review some of these law-like generalizations below, before finally describing a study that questions their validity. Dubossarsky et al. (2015) experimented with K-means clustering applied to SGNS embeddings trained for evenly sized yearly samples for the period 1850–2009. They found that the degree of semantic change for a given word – quantified as the change in self-similarity over time – negatively correlates with its distance to the centroid of its cluster. They proposed that the likelihood for semantic shift correlates with the degree of prototypicality (the ‘law of prototypicality’ in Dubossarsky et al. (2017)). Another relevant study is reported by Eger and Mehler (2016), based on two different graph models; one being a time-series model relating embeddings across time periods to model semantic shifts and the other modeling the self-similarity of words across time. Experiments were performed with time-indexed historical corpora of English, German and Latin, using time-periods corresponding to decades, years and centuries, respectively. To enable comparison of embeddings across time, second-order embeddings encoding similarities to other words were used, as described in 3.3, limited to the ‘core vo"
C18-1117,P16-2009,0,0.517426,"ted that computational methods for the detection of semantic shifts can be robustly applied to time spans less than a decade. Zhang et al. (2015) used another yearly text collection, the New-York Times Annotated Corpus (Sandhaus, 2008), again managing to trace subtle semantic shifts. The same corpus was employed by Szymanski (2017), with 21 separate models, one for each year from 1987 to 2007, and to some extent by Yao et al. (2018), who crawled the NYT web site to get 27 yearly subcorpora (from 1990 to 2016). The inventory of diachronic corpora used in tracing semantic shifts was expanded by Eger and Mehler (2016), who used the Corpus of Historical American (COHA2 ), with time slices equal to one decade. Hamilton et al. (2016a) continued the usage of COHA (along with the Google Ngrams corpus). Kutuzov et al. (2017b) started to employ the yearly slices of the English Gigaword corpus (Parker et al., 2011) in the analysis of cultural semantic drift related to armed conflicts. 3.1.2 Test sets Diachronic corpora are needed not only as a source of training data for developing semantic shift detection systems, but also as a source of test sets to evaluate such systems. In this case, however, the situation is"
C18-1117,Q16-1003,0,0.655948,"g vein of research based on dynamic topic modeling (Blei and Lafferty, 2006; Wang and McCallum, 2006), which learns the evolution of topics over time. In Wijaya and Yeniterzi (2011), it helped solve a typical digital humanities task of finding traces of real-world events in the texts. Heyer et al. (2016) employed topic analysis to trace the so-called ‘context volatility’ of words. In the political science, topic models are also sometimes used as proxies to social trends developing over time: for example, Mueller and Rauh (2017) employed LDA to predict timing of civil wars and armed conflicts. Frermann and Lapata (2016) drew on these ideas to trace diachronic word senses development. But most scholars nowadays seem to prefer parametric distributional models, particularly predictionbased embedding algorithms like SGNS, CBOW or GloVe (Pennington et al., 2014). Following their widespread adoption in NLP in general, they have become the dominant representations for the analysis of diachronic semantic shifts as well. 3.3 Comparing vectors across time It is rather straightforward to train separate word embedding models using time-specific corpora containing texts from several different time periods. As a consequen"
C18-1117,W11-2508,0,0.66346,"erremans et al. (2010) study the very recent neologism detweet, showing the development of two separate usages/meanings for this word (‘to delete from twitter,’ vs ‘to avoid tweeting’) based on large amounts of web-crawled data. The usage-based view of lexical semantics aligns well with the assumptions underlying the distributional semantic approach (Firth, 1957) often employed in NLP . Here, the time spans studied are often considerably shorter (decades, rather than centuries) and we find that these distributional methods seem well suited for monitoring the gradual process of meaning change. Gulordava and Baroni (2011), for instance, showed that distributional models capture cultural shifts, like the word sleep acquiring more negative connotations related to sleep disorders, when comparing its 1960s contexts to its 1990s contexts. To sum up, semantic shifts are often reflected in large corpora through change in the context of the word which is undergoing a shift, as measured by co-occurring words. It is thus natural to try to detect semantic shifts automatically, in a ‘data-driven’ way. This vein of research is what we cover in the present survey. In the following sections, we overview the methods currently"
C18-1117,P16-1141,0,0.100611,"frequently co-occurs, or by the need for discrimination of synonyms caused by lexical borrowings from other languages. Semantic shifts may be also be caused by changes in the attitudes of speakers or in the general environment of the speakers. Thus, semantic shifts are naturally separated into two important classes: linguistic drifts (slow and regular changes in core meaning of words) and cultural shifts (culturally determined changes in associations of a given word). Researchers studying semantic shifts from a computational point of view have shown the existence of this division empirically (Hamilton et al., 2016c). In the traditional classification of Stern (1931), the semantic shift category of substitution describes a change that has a non-linguistic cause, namely that of technologi1385 cal progress. This may be exemplified by the word car which shifted its meaning from non-motorized vehicles after the introduction of the automobile. The availability of large corpora have enabled the development of new methodologies for the study of lexical semantic shifts within general linguistics (Traugott, 2017). A key assumption in much of this work is that changes in a word’s collocational patterns reflect ch"
C18-1117,D16-1057,0,0.0712855,"frequently co-occurs, or by the need for discrimination of synonyms caused by lexical borrowings from other languages. Semantic shifts may be also be caused by changes in the attitudes of speakers or in the general environment of the speakers. Thus, semantic shifts are naturally separated into two important classes: linguistic drifts (slow and regular changes in core meaning of words) and cultural shifts (culturally determined changes in associations of a given word). Researchers studying semantic shifts from a computational point of view have shown the existence of this division empirically (Hamilton et al., 2016c). In the traditional classification of Stern (1931), the semantic shift category of substitution describes a change that has a non-linguistic cause, namely that of technologi1385 cal progress. This may be exemplified by the word car which shifted its meaning from non-motorized vehicles after the introduction of the automobile. The availability of large corpora have enabled the development of new methodologies for the study of lexical semantic shifts within general linguistics (Traugott, 2017). A key assumption in much of this work is that changes in a word’s collocational patterns reflect ch"
C18-1117,D16-1229,0,0.0646158,"frequently co-occurs, or by the need for discrimination of synonyms caused by lexical borrowings from other languages. Semantic shifts may be also be caused by changes in the attitudes of speakers or in the general environment of the speakers. Thus, semantic shifts are naturally separated into two important classes: linguistic drifts (slow and regular changes in core meaning of words) and cultural shifts (culturally determined changes in associations of a given word). Researchers studying semantic shifts from a computational point of view have shown the existence of this division empirically (Hamilton et al., 2016c). In the traditional classification of Stern (1931), the semantic shift category of substitution describes a change that has a non-linguistic cause, namely that of technologi1385 cal progress. This may be exemplified by the word car which shifted its meaning from non-motorized vehicles after the introduction of the automobile. The availability of large corpora have enabled the development of new methodologies for the study of lexical semantic shifts within general linguistics (Traugott, 2017). A key assumption in much of this work is that changes in a word’s collocational patterns reflect ch"
C18-1117,J15-4004,0,0.04213,"s, but the overwhelming majority of publications still apply only to English corpora. It might be the case that the best methodologies are the same for different languages, but this should be shown empirically. • There is a clear need to devise algorithms that work on small datasets, as they are very common in historical linguistics, digital humanities, and similar disciplines. 1392 • Carefully designed and robust gold standard test sets of semantic shifts (of different kinds) should be created. This is a difficult task in itself, but the experience from synchronic word embeddings evaluation (Hill et al., 2015) and other NLP areas proves that it is possible. • There is a need for rigorous formal mathematical models of diachronic embeddings. Arguably, this will follow the vein of research in joint learning across several time spans, started by Bamler and Mandt (2017) and Yao et al. (2018), but other directions are also open. • Most current studies stop after stating the simple fact that a semantic shift has occurred. However, more detailed analysis of the nature of the shift is needed. This includes: 1. Sub-classification of types of semantic shifts (broadening, narrowing, etc). This problem was to s"
C18-1117,W09-4302,0,0.112362,"ely and with greater explanatory power. One of the examples from their work is the semantic evolution of the word gay: through time, its nearest semantic neighbors changed, manifesting the gradual move away from the sense of ‘cheerful’ to the sense of ‘homosexual.’ In fact, distributional models were being used in diachronic research long before the paper of Kulkarni et al. (2015), although there was no rigorous comparison to the frequentist methods. Already in 2009, it was proposed that one can use distributional methods to detect semantic shifts in a quantitative way. The pioneering work by Jurgens and Stevens (2009) described an insightful conceptualization of a sequence of distributional model updates through time: it is effectively a Word:Semantic Vector:Time tensor, in the sense that each word in a distributional model possesses a set of semantic vectors for each time span we are interested in. It paved the way for quantitatively comparing not only words with regard to their meaning, but also different stages in the development of word meaning over time. Jurgens and Stevens (2009) employed the Random Indexing (RI) algorithm (Kanerva et al., 2000) to create word vectors. Two years later, Gulordava and"
C18-1117,D17-1037,0,0.0283976,"r at least makes the models more comparable. Several works have appeared recently which aim to address the technical issues accompanying this approach of incremental updating. Among others, Peng et al. (2017) described a novel method of incrementally learning the hierarchical softmax function for the CBOW and Continuous Skipgram algorithms. In this way, one can update word embedding models with new data and new vocabulary much more efficiently, achieving faster training than when doing it from scratch, while at the same time preserving comparable performance. Continuing this line of research, Kaji and Kobayashi (2017) proposed a conceptually similar incremental extension for negative sampling, which is a method of training examples selection, widely used with prediction-based models as a faster replacement for hierarchical softmax. Even after the models for different time periods are made comparable in this or that way, one still has to choose the exact method of comparing word vectors across these models. Hamilton et al. (2016a) and Hamilton et al. (2016c) made an important observation that the distinction between linguistic and cultural semantic shifts is correlated with the distinction between global an"
C18-1117,W14-2517,0,0.580761,"etected word sense changes over several different time periods spanning from 3 to 200 years. 1 https://books.google.com/ngrams 1386 In more recent work, time spans tend to decrease in size and become more granular. In general, corpora with smaller time spans are useful for analyzing socio-cultural semantic shifts, while corpora with longer spans are necessary for the study of linguistically motivated semantic shifts. As researchers are attempting to trace increasingly subtle cultural semantic shifts (more relevant for practical tasks), the granularity of time spans is decreasing: for example, Kim et al. (2014) and Liao and Cheng (2016) analyzed the yearly changes of words. Note that, instead of using granular ‘bins’, time can also be represented as a continuous differentiable value (Rosenfeld and Erk, 2018). In addition to the Google Ngrams dataset (with granularity of 5 years), Kulkarni et al. (2015) used Amazon Movie Reviews (with granularity of 1 year) and Twitter data (with granularity of 1 month). Their results indicated that computational methods for the detection of semantic shifts can be robustly applied to time spans less than a decade. Zhang et al. (2015) used another yearly text collecti"
C18-1117,W16-4005,1,0.879246,"rough a regression analysis, Hamilton et al. (2016a) investigated how the change rates correlate with frequency and polysemy, and proposed another two ‘laws’: 1. frequent words change more slowly (‘the law of conformity’); 2. polysemous words (controlled for frequency) change more quickly (‘the law of innovation’). Azarbonyad et al. (2017) showed that these laws (at least the law of conformity) hold not only for diachronic corpora, but also for other ‘viewpoints’: for example, semantic shifts across models trained on texts produced by different political actors or written in different genres (Kutuzov et al., 2016). However, the temporal dimension allows for a view of the corpora under analysis as a sequence, making the notion of ‘semantic shift’ more meaningful. Later, Dubossarsky et al. (2017) questioned the validity of some of these proposed ‘laws’ of semantic change. In a series of replication and control experiments, they demonstrated that some of the regularities observed in previous studies are largely artifacts of the models used and frequency effects. In particular, they considered 10-year bins comprising equally sized yearly samples from Google Books 5-grams of English fiction for the period 1"
C18-1117,D17-1194,1,0.916294,"tated Corpus (Sandhaus, 2008), again managing to trace subtle semantic shifts. The same corpus was employed by Szymanski (2017), with 21 separate models, one for each year from 1987 to 2007, and to some extent by Yao et al. (2018), who crawled the NYT web site to get 27 yearly subcorpora (from 1990 to 2016). The inventory of diachronic corpora used in tracing semantic shifts was expanded by Eger and Mehler (2016), who used the Corpus of Historical American (COHA2 ), with time slices equal to one decade. Hamilton et al. (2016a) continued the usage of COHA (along with the Google Ngrams corpus). Kutuzov et al. (2017b) started to employ the yearly slices of the English Gigaword corpus (Parker et al., 2011) in the analysis of cultural semantic drift related to armed conflicts. 3.1.2 Test sets Diachronic corpora are needed not only as a source of training data for developing semantic shift detection systems, but also as a source of test sets to evaluate such systems. In this case, however, the situation is more complicated. Ideally, diachronic approaches should be evaluated on human-annotated lists of semantically shifted words (ranked by the degree of the shift). However, such gold standard data is difficu"
C18-1117,W17-2705,1,0.917496,"tated Corpus (Sandhaus, 2008), again managing to trace subtle semantic shifts. The same corpus was employed by Szymanski (2017), with 21 separate models, one for each year from 1987 to 2007, and to some extent by Yao et al. (2018), who crawled the NYT web site to get 27 yearly subcorpora (from 1990 to 2016). The inventory of diachronic corpora used in tracing semantic shifts was expanded by Eger and Mehler (2016), who used the Corpus of Historical American (COHA2 ), with time slices equal to one decade. Hamilton et al. (2016a) continued the usage of COHA (along with the Google Ngrams corpus). Kutuzov et al. (2017b) started to employ the yearly slices of the English Gigaword corpus (Parker et al., 2011) in the analysis of cultural semantic drift related to armed conflicts. 3.1.2 Test sets Diachronic corpora are needed not only as a source of training data for developing semantic shift detection systems, but also as a source of test sets to evaluate such systems. In this case, however, the situation is more complicated. Ideally, diachronic approaches should be evaluated on human-annotated lists of semantically shifted words (ranked by the degree of the shift). However, such gold standard data is difficu"
C18-1117,P12-2051,0,0.713835,"chosen before slicing the text collection into subcorpora. Earlier works dealt mainly with long-term semantic shifts (spanning decades or even centuries), as they are easier to trace. One of the early examples is Sagi et al. (2011) who studied differences between Early Middle, Late Middle and Early Modern English, using the Helsinki Corpus (Rissanen and others, 1993). The release of the Google Books Ngrams corpus1 played an important role in the development of the field and spurred work on the new discipline of ‘culturomics,’ studying human culture through digital media (Michel et al., 2011). Mihalcea and Nastase (2012) used this dataset to detect differences in word usage and meaning across 50-years time spans, while Gulordava and Baroni (2011) compared word meanings in the 1960s and in the 1990s, achieving good correlation with human judgments. Unfortunately, Google Ngrams is inherently limited in that it does not contain full texts. However, for many cases, this corpus was enough, and its usage as the source of diachronic data continued in Mitra et al. (2014) (employing syntactic ngrams), who detected word sense changes over several different time periods spanning from 3 to 200 years. 1 https://books.goog"
C18-1117,P14-1096,0,0.667009,"nt of the field and spurred work on the new discipline of ‘culturomics,’ studying human culture through digital media (Michel et al., 2011). Mihalcea and Nastase (2012) used this dataset to detect differences in word usage and meaning across 50-years time spans, while Gulordava and Baroni (2011) compared word meanings in the 1960s and in the 1990s, achieving good correlation with human judgments. Unfortunately, Google Ngrams is inherently limited in that it does not contain full texts. However, for many cases, this corpus was enough, and its usage as the source of diachronic data continued in Mitra et al. (2014) (employing syntactic ngrams), who detected word sense changes over several different time periods spanning from 3 to 200 years. 1 https://books.google.com/ngrams 1386 In more recent work, time spans tend to decrease in size and become more granular. In general, corpora with smaller time spans are useful for analyzing socio-cultural semantic shifts, while corpora with longer spans are necessary for the study of linguistically motivated semantic shifts. As researchers are attempting to trace increasingly subtle cultural semantic shifts (more relevant for practical tasks), the granularity of tim"
C18-1117,D14-1162,0,0.102176,"traces of real-world events in the texts. Heyer et al. (2016) employed topic analysis to trace the so-called ‘context volatility’ of words. In the political science, topic models are also sometimes used as proxies to social trends developing over time: for example, Mueller and Rauh (2017) employed LDA to predict timing of civil wars and armed conflicts. Frermann and Lapata (2016) drew on these ideas to trace diachronic word senses development. But most scholars nowadays seem to prefer parametric distributional models, particularly predictionbased embedding algorithms like SGNS, CBOW or GloVe (Pennington et al., 2014). Following their widespread adoption in NLP in general, they have become the dominant representations for the analysis of diachronic semantic shifts as well. 3.3 Comparing vectors across time It is rather straightforward to train separate word embedding models using time-specific corpora containing texts from several different time periods. As a consequence, these models are also time-specific. However, it is not that straightforward to compare word vectors across different models. It usually does not make sense to, for example, directly calculate cosine similarities between embeddings of one"
C18-1117,S15-2147,0,0.211295,"alone for other languages. General linguistics research on language change like that of Traugott and Dasher (2001) and others usually contain only a small number of hand-picked examples, which is not sufficient to properly evaluate an automatic unsupervised system. Various ways of overcoming this problem have been proposed. For example, Mihalcea and Nastase (2012) evaluated the ability of a system to detect the time span that specific contexts of a word undergoing a shift belong to (word epoch disambiguation). A similar problem was offered as SemEval-2015 Task 7: ‘Diachronic Text Evaluation’ (Popescu and Strapparava, 2015). Another possible evaluation method is so-called cross-time alignment, where a system has to find equivalents for certain words in different time periods (for example, ‘Obama’ in 2015 corresponds to ‘Trump’ in 2017). There exist several datasets containing such temporal equivalents for English (Yao et al., 2018). Yet another evaluation strategy is to use the detected diachronic semantic shifts to trace or predict real-world events like armed conflicts (Kutuzov et al., 2017b). Unfortunately, all these evaluation methods still require the existence of large manually annotated semantic shift dat"
C18-1117,N18-1044,0,0.105985,"ecome more granular. In general, corpora with smaller time spans are useful for analyzing socio-cultural semantic shifts, while corpora with longer spans are necessary for the study of linguistically motivated semantic shifts. As researchers are attempting to trace increasingly subtle cultural semantic shifts (more relevant for practical tasks), the granularity of time spans is decreasing: for example, Kim et al. (2014) and Liao and Cheng (2016) analyzed the yearly changes of words. Note that, instead of using granular ‘bins’, time can also be represented as a continuous differentiable value (Rosenfeld and Erk, 2018). In addition to the Google Ngrams dataset (with granularity of 5 years), Kulkarni et al. (2015) used Amazon Movie Reviews (with granularity of 1 year) and Twitter data (with granularity of 1 month). Their results indicated that computational methods for the detection of semantic shifts can be robustly applied to time spans less than a decade. Zhang et al. (2015) used another yearly text collection, the New-York Times Annotated Corpus (Sandhaus, 2008), again managing to trace subtle semantic shifts. The same corpus was employed by Szymanski (2017), with 21 separate models, one for each year fr"
C18-1117,D17-1121,0,0.340604,"ans. Szymanski (2017) frames this as the temporal word analogy problem, extending the word analogies concept into the temporal dimension. This work shows that 1391 diachronic word embeddings can successfully model relations like ‘word w1 at time period tα is like word w2 at time period tβ ’. To this end, embedding models trained on different time periods are aligned using linear transformations. Then, the temporal analogies are solved by simply finding out which word vector in the time period tβ is the closest to the vector of w1 in the time period tα . A variation of this task was studied in Rosin et al. (2017), where the authors learn the relatedness of words over time, answering queries like ‘in which time period were the words Obama and president maximally related’. This technique can be used for a more efficient user query expansion in generalpurpose search engines. Kutuzov et al. (2017a) modeled a different semantic relation: ‘words w1 and w2 at time period tα are in the same semantic relation as words w3 and w4 at time period tβ ’. To trace the temporal dynamics of these relations, they re-applied linear projections learned on sets of w1 and w2 pairs from the model for the period tn to the mod"
C18-1117,P17-2071,1,0.911369,"as a continuous differentiable value (Rosenfeld and Erk, 2018). In addition to the Google Ngrams dataset (with granularity of 5 years), Kulkarni et al. (2015) used Amazon Movie Reviews (with granularity of 1 year) and Twitter data (with granularity of 1 month). Their results indicated that computational methods for the detection of semantic shifts can be robustly applied to time spans less than a decade. Zhang et al. (2015) used another yearly text collection, the New-York Times Annotated Corpus (Sandhaus, 2008), again managing to trace subtle semantic shifts. The same corpus was employed by Szymanski (2017), with 21 separate models, one for each year from 1987 to 2007, and to some extent by Yao et al. (2018), who crawled the NYT web site to get 27 yearly subcorpora (from 1990 to 2016). The inventory of diachronic corpora used in tracing semantic shifts was expanded by Eger and Mehler (2016), who used the Corpus of Historical American (COHA2 ), with time slices equal to one decade. Hamilton et al. (2016a) continued the usage of COHA (along with the Google Ngrams corpus). Kutuzov et al. (2017b) started to employ the yearly slices of the English Gigaword corpus (Parker et al., 2011) in the analysis"
C18-1117,P15-1063,0,0.535819,"e spans is decreasing: for example, Kim et al. (2014) and Liao and Cheng (2016) analyzed the yearly changes of words. Note that, instead of using granular ‘bins’, time can also be represented as a continuous differentiable value (Rosenfeld and Erk, 2018). In addition to the Google Ngrams dataset (with granularity of 5 years), Kulkarni et al. (2015) used Amazon Movie Reviews (with granularity of 1 year) and Twitter data (with granularity of 1 month). Their results indicated that computational methods for the detection of semantic shifts can be robustly applied to time spans less than a decade. Zhang et al. (2015) used another yearly text collection, the New-York Times Annotated Corpus (Sandhaus, 2008), again managing to trace subtle semantic shifts. The same corpus was employed by Szymanski (2017), with 21 separate models, one for each year from 1987 to 2007, and to some extent by Yao et al. (2018), who crawled the NYT web site to get 27 yearly subcorpora (from 1990 to 2016). The inventory of diachronic corpora used in tracing semantic shifts was expanded by Eger and Mehler (2016), who used the Corpus of Historical American (COHA2 ), with time slices equal to one decade. Hamilton et al. (2016a) contin"
D17-1194,P16-2009,0,0.0362405,"s of geographical locations where violent armed groups were active to the embeddings of these groups. These projections are then applied to the embeddings and gold standard data from the subsequent year, thus predicting what entities act as violent groups in the next time slice. To evaluate our approach, we adapt the UCDP Armed Conflict Dataset (Gleditsch et al., 2002; Allansson et al., Attempts to detect semantic change using unsupervised methods have a long history. Significant results have already been achieved in employing word embeddings to study diachronic language change. Among others, Eger and Mehler (2016) show that the embedding of a given word for a given time period to a large extent is a linear combination of its embeddings for the previous time periods. Hamilton et al. (2016) proposed an important distinction between cultural shifts and linguistic drifts. They proved that global embedding-based measures (comparing the similarities of words to all other words in the lexicon) are sensitive to regular processes of linguistic drift, while local measures (comparing nearest neighbors’ lists) are a better fit for more irregular cultural shifts in word meaning. Our focus here is on cultural shifts"
D17-1194,D16-1229,0,0.202409,"from the subsequent year, thus predicting what entities act as violent groups in the next time slice. To evaluate our approach, we adapt the UCDP Armed Conflict Dataset (Gleditsch et al., 2002; Allansson et al., Attempts to detect semantic change using unsupervised methods have a long history. Significant results have already been achieved in employing word embeddings to study diachronic language change. Among others, Eger and Mehler (2016) show that the embedding of a given word for a given time period to a large extent is a linear combination of its embeddings for the previous time periods. Hamilton et al. (2016) proposed an important distinction between cultural shifts and linguistic drifts. They proved that global embedding-based measures (comparing the similarities of words to all other words in the lexicon) are sensitive to regular processes of linguistic drift, while local measures (comparing nearest neighbors’ lists) are a better fit for more irregular cultural shifts in word meaning. Our focus here is on cultural shifts: it is not the dictionary meanings of the names denoting locations and armed groups that change, but rather their ‘image’ in the analyzed texts. Our measurement approach can als"
D17-1194,D17-1037,0,0.0565018,"thresholds. It was initially set to the value which produced a reasonable vocabulary size of several hundred thousand words. 1826 Pairs (size) @1 @5 @10 All (38) New (7) 44.7 14.3 73.7 28.6 84.2 42.9 our approach on the whole set of UCDP conflicts in the Gigaword years (1994–2010). Table 2: Projection accuracy for the isolated example experiment mapping from 2000 → 2001. models are simply trained further with the new texts. A possible alternative to this can be incremental training of hierarchical softmax functions proposed in (Peng et al., 2017) or incremental negative sampling proposed in (Kaji and Kobayashi, 2017); we leave it for future work. The experiment involves applying a learned transformation matrix across pairs of models. While in Section 4 we evaluate the approach across the entire Gigaword time period, this section reports a preliminary example experiment for the transition from 2000 to 2001 alone. This means we will have one model saved after sequential training for the years up to 2000, and one saved after year 2001. Our aim is to find out whether the Location–Insurgent projection learned on the first model is able to reveal conflicts that appear in 2001. Thus, we extract from the UCDP dat"
D17-1194,W17-2705,1,0.781474,"Missing"
D17-1194,P14-5010,0,0.00775205,"nts to assess the hypothesis that the embeddings contain semantic relationships of the type ‘insurgent participant of an armed conflict in the location’. To this end, we trained a CBOW model on the full English Gigaword corpus (about 4.8 billion tokens in total), with a symmetric context window of 5 words, vector size 300, 10 negative samples and 5 iterations. Words with a frequency less than 100 were ignored during training. We used Genˇ uˇrek and Sojka, 2010) for training, and sim (Reh˚ in terms of corpus pre-processing we performed lemmatization, PoS-tagging and NER using Stanford CoreNLP (Manning et al., 2014). Named entities were concatenated to one token (for example, United States became United::States_PROPN). Then, we used the 137 Location–Insurgent pairs derived in Section 2 to learn a projection matrix from the embeddings for locations to the embeddings for insurgents. The idea and the theory behind this approach are extensively described in (Mikolov et al., 2013b) and (Kutuzov et al., 2016), but essentially it involves training a linear regression which minimizes the error in transforming 1825 loc→group group→loc λ @1 @5 @10 @1 @5 @10 0.0 0.5 1.0 0.0 0.7 2.2 14.6 19.0 19.7 31.4 35.0 32.8 8.8"
D18-1178,N18-1143,0,0.0457869,"Missing"
D18-1178,W16-1604,0,0.0341938,"Missing"
D18-1178,W15-0122,0,0.534904,"in the British National Corpus (Burnard, 2000) are part ´ S´eaghdha, 2008) – of noun–noun compounds (O and its relevance to other natural language processing (NLP) tasks such as question answering and information retrieval (Nakov, 2008), noun– noun compound interpretation has been the focus of much work, in theoretical linguistics (Li, 1972; Downing, 1977; Levi, 1978; Finin, 1980; Ryder, 1994), psycholinguistics (Gagn´e and Shoben, 1997; Marelli et al., 2017), and computational lin´ S´eaghdha guistics (Lauer, 1995; Nakov, 2007; O and Copestake, 2009; Girju et al., 2009; Kim and Baldwin, 2013; Dima and Hinrichs, 2015). In computational linguistics, noun–noun compound interpretation is, by and large, approached as an automatic classification problem. Hence several machine learning (ML) algorithms and models have been used to learn the semantics of nominal compounds, including Maximum Entropy (Tratz ´ and Hovy, 2010), Support Vector Machines (O S´eaghdha and Copestake, 2013) and Neural Networks (Dima and Hinrichs, 2015; Vered and Waterson, 2018). These models use information from lexical semantics such as WordNet-based features and distributional semantics such as word embeddings. Nonetheless, noun–noun comp"
D18-1178,P16-3011,1,0.651956,"e 2018 Conference on Empirical Methods in Natural Language Processing, pages 1488–1498 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics One of the primary motivations for using multitask learning is to improve generalization by “leveraging the domain-specific information contained in the training signals of related tasks” Caruana (1997). In this work, we show that TL and MTL can indeed be used as a kind of regularizer to learn to predict infrequent relations given a highly skewed distribution of relations from the noun–noun compound dataset of Fares (2016) which is especially well suited for TL and MTL experimentation as detailed in Section 3. Our contributions can be summarized as: 1. Through careful result analysis, we find that TL and MTL (mainly on the embedding layer) do improve the overall accuracy and the F1 scores of the less frequent relations in a highly skewed dataset, in comparison to a strong single-task learning baseline. 2. Even though our work focuses on TL and MTL, to the best of our knowledge, we are the first to report experimental results on the comparatively recent dataset of Fares (2016). 2 Related Work Noun–Noun Compound"
D18-1178,W17-0237,1,0.83664,"ing layer where the word embedding vectors are stored; the selected word embedding vectors are then fed to a fully connected hidden layer whose size is the same as the number of dimensions of the word embedding vectors. Finally, a softmax function is applied on the output layer and the most likely relation is selected. The compound’s constituents are represented using a 300-dimensional word embedding model trained on an English Wikipedia dump (dated February 2017) and English Gigaword Fifth Edition (Parker et al., 2011) using GloVe (Pennington et al., 2014). The embedding model was trained by Fares et al. (2017) who provide more details on the hyperparameters used to train the embedding model.5 When looking up a word in the embedding model, if the word is not found we check if the word is uppercased and look up the same word in lowercase. If a word is hyphenated and is not found in the embedding vocabulary, we split it on the hyphen and average the vectors of its parts (if they exist in the vocabulary). If after these steps the word is still not found, we use a designated vector for unknown words. Architecture and Hyperparameters Our choice of hyperparameters is motivated by several rounds of experim"
D18-1178,I05-1082,0,0.0285735,"ne-grained taxonomy of 43 relations. Others question the very assumption that noun–noun compounds are interpretable using a finite, predefined set of relations (Downing, 1977; Finin, 1980) and propose alternative paraphrasingbased approaches (Nakov, 2007; Shwartz and Dagan, 2018). We here focus on the approaches that cast the interpretation problem as a classification task over a finite predefined set of relations. A wide variety of machine learning models have been already applied to learn this task, including nearest neighbor classifiers using semantic similarity based on lexical resources (Kim and Baldwin, 2005), kernel-based methods like SVMs ´ S´eaghdha using lexical and relational features (O and Copestake, 2009), Maximum Entropy models with a relatively large selection of lexical and surface form features such as synonyms and affixes (Tratz and Hovy, 2010) and, most recently, neural networks either solely relying on word embeddings to represent noun–noun compounds (Dima and Hinrichs, 2015) or word embeddings and socalled path embeddings (which encode information about lemmas and part-of-speech tags, inter alia) in a combined paraphrasing and classification approach (Vered and Waterson, 2018). Of"
D18-1178,kingsbury-palmer-2002-treebank,0,0.365094,"Missing"
D18-1178,N15-1098,0,0.0631539,"Missing"
D18-1178,E17-1005,0,0.226126,"s of noun–noun compounds is not easily derivable from the compounds’ constituents (Vered and Waterson, 2018). Our work, in part, contributes to advancing NLP research on noun– noun compound interpretation through the use of transfer and multi-task learning. The interest in using transfer learning (TL) and multi-task learning (MTL) in NLP has surged over the past few years, showing ‘mixed’ results depending on the so-called main and auxiliary tasks involved, model architectures and datasets, among other things (Collobert and Weston, 2008; Mou et al., 2016; Søgaard and Goldberg, 2016; Mart´ınez Alonso and Plank, 2017; Bingel and Søgaard, 2017). These ‘mixed’ results, coupled with the fact that neither TL nor MTL has been applied to noun–noun compounds interpretation before, motivate our extensive empirical study on the use of TL and MTL for compound interpretation, not only to supplement existing research on the utility of TL and MTL for semantic NLP tasks in general, but also to determine their benefits for compound interpretation in particular. 1488 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1488–1498 c Brussels, Belgium, October 31 - November 4, 2018."
D18-1178,P17-1186,0,0.0528941,"Missing"
D18-1178,meyers-etal-2004-annotating,0,0.0672502,"to learn to classify the semantic relations holding between each pair of compound constituents. The difficulty of this task, obviously, depends on the label set used and its distribution, among other things. For all the experiments presented in this paper, we adapt the noun–noun compounds dataset created by Fares (2016) which consists of compounds annotated with two different taxonomies of relations; in other words, for each noun–noun compound there are two distinct relations, drawing on different linguistic schools. The dataset was derived from existing linguistic resources, such as NomBank (Meyers et al., 2004) and the Prague Czech-English Dependency Treebank 2.0 (Hajiˇc et al., 2012, PCEDT). Our motivation for using this dataset is twofold: first, dual annotation with relations over the same underlying set of compounds maximally enables TL and MTL perspectives; second, alignment of two distinct annotation frameworks over the same data facilitates contrastive analysis and comparison across frameworks. More specifically, we use a subset of the dataset created by Fares (2016), by focusing on type-based instances of so-called two-word compounds.1 The original dataset by Fares (2016) also includes multi"
D18-1178,D14-1162,0,0.0810176,"ifying the indices of a compound’s constituents in the embedding layer where the word embedding vectors are stored; the selected word embedding vectors are then fed to a fully connected hidden layer whose size is the same as the number of dimensions of the word embedding vectors. Finally, a softmax function is applied on the output layer and the most likely relation is selected. The compound’s constituents are represented using a 300-dimensional word embedding model trained on an English Wikipedia dump (dated February 2017) and English Gigaword Fifth Edition (Parker et al., 2011) using GloVe (Pennington et al., 2014). The embedding model was trained by Fares et al. (2017) who provide more details on the hyperparameters used to train the embedding model.5 When looking up a word in the embedding model, if the word is not found we check if the word is uppercased and look up the same word in lowercase. If a word is hyphenated and is not found in the embedding vocabulary, we split it on the hyphen and average the vectors of its parts (if they exist in the vocabulary). If after these steps the word is still not found, we use a designated vector for unknown words. Architecture and Hyperparameters Our choice of h"
D18-1178,D16-1046,0,0.320843,"tic construction, is very productive and 2) the semantics of noun–noun compounds is not easily derivable from the compounds’ constituents (Vered and Waterson, 2018). Our work, in part, contributes to advancing NLP research on noun– noun compound interpretation through the use of transfer and multi-task learning. The interest in using transfer learning (TL) and multi-task learning (MTL) in NLP has surged over the past few years, showing ‘mixed’ results depending on the so-called main and auxiliary tasks involved, model architectures and datasets, among other things (Collobert and Weston, 2008; Mou et al., 2016; Søgaard and Goldberg, 2016; Mart´ınez Alonso and Plank, 2017; Bingel and Søgaard, 2017). These ‘mixed’ results, coupled with the fact that neither TL nor MTL has been applied to noun–noun compounds interpretation before, motivate our extensive empirical study on the use of TL and MTL for compound interpretation, not only to supplement existing research on the utility of TL and MTL for semantic NLP tasks in general, but also to determine their benefits for compound interpretation in particular. 1488 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages"
D18-1178,P07-3013,0,0.692326,"Missing"
D18-1178,E09-1071,0,0.0811582,"Missing"
D18-1178,P18-1111,0,0.015922,"pretation vary depending on the taxonomy of compound relations as well as the machine learning models and features used to learn those rela´ S´eaghdha (2007) defines tions. For example, O a coarse-grained set of relations (viz. six relations based on theoretical work by Levi (1978)), whereas Tratz and Hovy (2010) assume a considerably more fine-grained taxonomy of 43 relations. Others question the very assumption that noun–noun compounds are interpretable using a finite, predefined set of relations (Downing, 1977; Finin, 1980) and propose alternative paraphrasingbased approaches (Nakov, 2007; Shwartz and Dagan, 2018). We here focus on the approaches that cast the interpretation problem as a classification task over a finite predefined set of relations. A wide variety of machine learning models have been already applied to learn this task, including nearest neighbor classifiers using semantic similarity based on lexical resources (Kim and Baldwin, 2005), kernel-based methods like SVMs ´ S´eaghdha using lexical and relational features (O and Copestake, 2009), Maximum Entropy models with a relatively large selection of lexical and surface form features such as synonyms and affixes (Tratz and Hovy, 2010) and,"
D18-1178,P16-2038,0,0.131782,"Missing"
D18-1178,P10-1070,0,0.495055,"g single-task learning baseline. 2. Even though our work focuses on TL and MTL, to the best of our knowledge, we are the first to report experimental results on the comparatively recent dataset of Fares (2016). 2 Related Work Noun–Noun Compound Interpretation Existing approaches to noun–noun compound interpretation vary depending on the taxonomy of compound relations as well as the machine learning models and features used to learn those rela´ S´eaghdha (2007) defines tions. For example, O a coarse-grained set of relations (viz. six relations based on theoretical work by Levi (1978)), whereas Tratz and Hovy (2010) assume a considerably more fine-grained taxonomy of 43 relations. Others question the very assumption that noun–noun compounds are interpretable using a finite, predefined set of relations (Downing, 1977; Finin, 1980) and propose alternative paraphrasingbased approaches (Nakov, 2007; Shwartz and Dagan, 2018). We here focus on the approaches that cast the interpretation problem as a classification task over a finite predefined set of relations. A wide variety of machine learning models have been already applied to learn this task, including nearest neighbor classifiers using semantic similarit"
D18-1178,N18-2035,0,0.175578,"n, 1997; Marelli et al., 2017), and computational lin´ S´eaghdha guistics (Lauer, 1995; Nakov, 2007; O and Copestake, 2009; Girju et al., 2009; Kim and Baldwin, 2013; Dima and Hinrichs, 2015). In computational linguistics, noun–noun compound interpretation is, by and large, approached as an automatic classification problem. Hence several machine learning (ML) algorithms and models have been used to learn the semantics of nominal compounds, including Maximum Entropy (Tratz ´ and Hovy, 2010), Support Vector Machines (O S´eaghdha and Copestake, 2013) and Neural Networks (Dima and Hinrichs, 2015; Vered and Waterson, 2018). These models use information from lexical semantics such as WordNet-based features and distributional semantics such as word embeddings. Nonetheless, noun–noun compound interpretation remains one of the more difficult NLP problems because: 1) noun–noun compounding, as a linguistic construction, is very productive and 2) the semantics of noun–noun compounds is not easily derivable from the compounds’ constituents (Vered and Waterson, 2018). Our work, in part, contributes to advancing NLP research on noun– noun compound interpretation through the use of transfer and multi-task learning. The in"
J12-2005,A00-1031,0,0.0143254,"8, Number 2 exhibited GENIA tokenization problems. Our pre-processing approach thus deploys a cascaded ﬁnite-state tokenizer (borrowed and adapted from the open-source English Resource Grammar: Flickinger [2002]), which aims to implement the tokenization decisions made in the Penn Treebank (Marcus, Santorini, and Marcinkiewicz 1993)—much like GENIA, in principle—but more appropriately treating corner cases like the ones noted here. 2.2 PoS Tagging and Lemmatization For part-of-speech (PoS) tagging and lemmatization, we combine GENIA (with its built-in, occasionally deviant tokenizer) and TnT (Brants 2000), which operates on pre-tokenized inputs but in its default model is trained on ﬁnancial news from the Penn Treebank. Our general goal here is to take advantage of the higher PoS accuracy provided by GENIA in the biomedical domain, while using our improved tokenization and producing inputs to the parsers that as much as possible resemble the conventions used in the original training data for the (dependency) parser (the Penn Treebank, once again). To this effect, for the vast majority of tokens we can align the GENIA tokenization with our own, and in these cases we typically use GENIA PoS tags"
J12-2005,W02-1503,0,0.0392941,"f linguistic information to draw upon for the speculation resolution task. MaltParser is based on a deterministic parsing strategy in combination with treebank-induced classiﬁers for predicting parse transitions. It supports a rich feature representation of the parse history in order to guide parsing and may easily be extended to take into account additional features. The procedure to enable the data-driven parser to learn from the grammar-driven parser is quite simple. We parse a treebank with the XLE platform (Crouch et al. 2008) and the English grammar developed within the ParGram project (Butt et al. 2002). We then convert the LFG output to dependency structures, so that we have two parallel versions of the treebank—one gold-standard and one with LFG annotation. We extend the gold-standard treebank with additional information from the corresponding LFG analysis and train MaltParser on the enhanced data set. For a description of the parse model features and the dependency substructures proposed by XLE for each word token, see Nivre and McDonald (2008). For further background on the conversion and training procedures, see Øvrelid, Kuhn, and Spreyer (2009). Table 5 shows the enhanced dependency re"
J12-2005,W10-3110,0,0.168931,"Missing"
J12-2005,W01-0521,0,0.0163915,"urrent set of scope rules, or annotation of parallel constructions may in some cases differ in subtle ways (see Section 6.1.5). The overﬁtting effects caused by the data dependencies introduced by the various GENIA-based domain adaptation steps, as described in Section 2.3, must also be taken into account. 6.1.4 PoS Tagging and Domain Variation. As mentioned in Section 6.1.1, an advantage of stacking with a general-purpose LFG parser is that it can be expected to aid domain portability. Nonetheless, substantial differences in domain and genre are bound to negatively affect syntactic analysis (Gildea 2001), and our parser is trained on ﬁnancial news. MaltParser presupposes that inputs have been PoS tagged, however, leaving room for variation in preprocessing. In this article we have aimed, on the one hand, to make parser inputs conform as much as possible to the conventions established in its PTB training data, while on the other hand taking advantage of specialized resources for the biomedical domain. To assess the impact of improved, domain-adapted inputs on our scope resolution rules, we contrast two conﬁgurations: Running the parser in the exact same manner as Øvrelid, Kuhn, and Spreyer (20"
J12-2005,W07-2416,0,0.0153954,"e XLE analysis (Features). Id Form PoS Features XHead XDep Head DepRel 1 2 3 4 5 6 7 8 9 10 11 The unknown amino acid may be used by these species . DT JJ JJ NN MD VB VBN IN DT NNS . _ degree:attributive degree:attributive pers:3|case:nom|num:sg|ntype:common mood:ind|subcat:MODAL|tense:pres|clauseType:decl _ subcat:V-SUBJ-OBJ|vtype:main|passive:+ _ deixis:proximal num:pl|pers:3|case:obl|common:count|ntype:common _ 4 4 4 3 0 7 5 9 10 7 0 SPECDET ADJUNCT ADJUNCT SUBJ ROOT PHI XCOMP PHI SPECDET OBL-AG PUNC 4 4 4 5 0 5 6 7 10 8 5 NMOD NMOD NMOD SBJ ROOT VC VC LGS NMOD PMOD P to dependency format (Johansson and Nugues 2007) and extended with XLE features, as described previously. Parsing uses the arc-eager mode of MaltParser and an SVM with a polynomial kernel. When tested using 10-fold cross validation on the enhanced PTB, the parser achieves a labeled accuracy score of 89.8, which is lower than the current stateof-the-art for transition-based dependency parsers (to wit, the 91.8 score of Zhang and Nivre 2011, although not directly comparable given that they test exclusively on WSJ Section 23), but with the advantage of providing us with the deep linguistic information from the XLE. 6.1.2 Rule Overview. Our sco"
J12-2005,P99-1069,0,0.0433025,"English Resource Grammar (ERG; Flickinger [2002]), a general-purpose, wide-coverage grammar couched in the framework of an HPSG (Pollard and Sag 1987, 1994). The approach rests on two main assumptions: Firstly, that the annotated scope of a speculation cue corresponds to a syntactic constituent and secondly, that we can automatically learn a ranking function that selects the correct constituent. Our ranking approach to scope resolution is abstractly related to statistical parse selection, and in particular work on discriminative parse selection for uniﬁcation based grammars, such as those by Johnson et al. (1999), Riezler et al. (2002), Malouf and van Noord (2004), and Toutanova et al. (2005). The overall goal is to learn a function for ranking syntactic structures, based on training data that annotates which tree(s) are correct and incorrect for each sentence. In our case, however, rather than discriminating between complete analyses for a given sentence, we want to learn a ranking function over candidate subtrees (i.e., constituents) within a parse (or possibly even within several parses). Figure 3 presents an example derivation tree that represents a complete HPSG analysis. Starting from the cue an"
J12-2005,W10-3010,0,0.271449,"extended on this system by also adding syntactic features, resulting in the top performing system of the CoNLL-2010 Shared Task at the scope-level (corresponding to the second subtask). It is interesting to note that all the top performers use various types of syntactic information in their scope resolution systems: The output from a dependency parser (MaltParser) (Morante, van Asch, and Daelemans 2010; Velldal, Øvrelid, and Oepen 2010), a tag sequence grammar (RASP) (Rei and Briscoe 2010), as well as constituent analysis in combination with dependency triplets (Stanford lexicalized parser) (Kilicoglu and Bergler 2010). The majority of systems perform classiﬁcation at the token level, using some variant of machine learning with a BIO classiﬁcation scheme and a post-processing step to assemble the full scope (Farkas et al. 2010), although several of the top performers employ manually constructed rules (Kilicoglu and Bergler 2010; Velldal, Øvrelid, and Oepen 2010) or even combinations of machine learning and rules (Rei and Briscoe 2010). 5. Identifying Speculation Cues We now turn to look at the details of our own system, starting in this section with describing a simple yet effective approach to identifying"
J12-2005,W04-3103,0,0.271137,"Missing"
J12-2005,I11-1028,1,0.819467,"carry over to any downstream components using this information. For the experiments described in this article, GENIA supplies lemmas for the n-gram features used by the cue classiﬁers, as well as PoS tags used in the input to both the dependency parser and the Head-driven Phrase Structure Grammar (HPSG) parser (which in turn provide the inputs to our various scope resolution components). For the HPSG parser, a subset of the GENIA corpus was also used as part of the training data for estimating an underlying statistical parse selection model, producing n-best lists of ranked candidate parses (MacKinlay et al. 2011). When reporting ﬁnal test results on the full papers (BSP or BSE) or the clinical reports (BSR), no such dependencies between information sources exists. It does mean, however, that we can reasonably expect to see some extra drop in performance when going from development results on data that includes the BioScope abstracts to the test results on these other data sets. 372 Velldal et al. Rules, Rankers, and the Role of Syntax 3. Evaluation Measures In this section we seek to clarify the type of measures we will be using for evaluating both the cue detection components (Section 3.1) and the sc"
J12-2005,J93-2004,0,0.0406023,"Missing"
J12-2005,D08-1017,0,0.00926869,".1.1) and quantifying the effect of using a domain-adapted PoS tagger (Section 6.1.4). 6.1.1 Stacked Dependency Parsing. For syntactic analysis we use the open-source MaltParser (Nivre, Hall, and Nilsson 2006), a platform for data-driven dependency parsing. For improved accuracy and portability across domains and genres, we make our parser incorporate the predictions of a large-scale, general-purpose Lexical-Functional Grammar parser. A technique dubbed parser stacking enables the data-driven parser to learn from the output of another parser, in addition to gold-standard treebank annotations (Martins et al. 2008; Nivre and McDonald 2008). This technique has been shown to provide signiﬁcant improvements in accuracy for both English and German (Øvrelid, Kuhn, and Spreyer 2009), and a similar set-up using an HPSG grammar has been shown to increase domain independence in data-driven dependency parsing (Zhang and Wang 2009). The stacked parser used here is identical to the parser described in Øvrelid, Kuhn, and Spreyer (2009), except for the preprocessing in terms of tokenization and PoS tagging, which is performed as detailed in Sections 2.1–2.2. The parser combines two quite different approaches—data-dr"
J12-2005,P07-1125,0,0.0169926,"a given system. The sequence of boolean values that results (FP = 0, TP = 1) can be directly paired with the corresponding sequence for a different system so that the sign-test can be applied as above. Note that our modiﬁed scorer for negation is available from our Web page of supplemental materials,2 together with the system output (in XML following the BioScope DTD) for all end-to-end runs with our ﬁnal model conﬁgurations. 4. Related Work on Speculation Labeling Although there exists a body of earlier work on identifying uncertainty on the sentence level, (Light, Qiu, and Srinivasan 2004; Medlock and Briscoe 2007; Szarvas 2008), the task of resolving the in-sentence scope of speculation cues was ﬁrst pioneered by Morante and Daelemans (2009a). In this sense, the CoNLL-2010 Shared Task (Farkas et al. 2010) entered largely uncharted territory and contributed to an increased interest for this task. Virtually all systems for resolving speculation scope implement a two-stage architecture: First there is a component that identiﬁes the speculation cues and then there is a component for resolving the in-sentence scopes of these cues. In this section we provide a brief review of previous work on this problem,"
J12-2005,morante-2010-descriptive,0,0.138902,"ules applied directly without modiﬁcations achieve 48.67 and 56.25. In order to further improve on these results, we introduce a few new rules to account speciﬁcally for negation. The general rule machinery is identical to the speculation scope rules described in Section 6.1: The rules are triggered by the part of speech of the cue and operate over the dependency representations output by the stacked dependency parser described in Section 6.1.1. In developing the rules we consulted the BioScope guidelines (Vincze et al. 2008), as well as a descriptive study of negation in the BioScope corpus (Morante 2010). 401 Computational Linguistics Volume 38, Number 2 Table 13 Additional dependency-based scope rules for negation, with information source (MaltParser or XLE), organized by PoS of the cue. PoS DT NN NNnone VB RBvb RBother Description Source Determiners scope over their head node and its descendants Nouns scope over their descendants none take scope over entire sentence if subject and otherwise over its descendants Verbs scope over their descendants Adverbs with verbal head scope over the descendants of the lexical verb Adverbs scope over the descendants of the head M M M M M, X M, X 7.2.1 Rule"
J12-2005,W09-1304,0,0.0634709,"d biomedical articles for evaluation purposes, constituting an additional 5,003 utterances. This latter data set (also detailed in Table 1) will be used for held-out testing of our speculation models. We will be using the following abbreviations when referring to the various parts of the data: BSA (BioScope abstracts), BSP (full papers), BSE (the heldout evaluation data), and BSR (clinical reports). Note that, when we get to the negation task we will be using the original version of the BioScope data. Furthermore, as BSE does not annotate negation, we instead follow the experimental set-up of Morante and Daelemans (2009b) for the negation task, reporting 10-fold cross validation on BSA and held-out testing on BSP and BSR. 2.1 Tokenization The BioScope data (and other data sets in the CoNLL-2010 Shared Task), are provided sentence-segmented only, and otherwise non-tokenized. Unsurprisingly, the GENIA tagger (Tsuruoka et al. 2005) has a central role in our pre-processing set-up. We found that its tokenization rules are not always optimally adapted for the type of text in BioScope, however. For example, GENIA unconditionally introduces token boundaries for some punctuation marks that can also occur token-intern"
J12-2005,W09-1105,0,0.644974,"(2009b) Cue classiﬁer & Scope Rules + Ranking 66.31 69.30 65.27 72.89 65.79 71.05 BSP Held-out Morante et al. (2009b) Cue classiﬁer & Scope Rules + Ranking 42.49 58.58 39.10 68.09 40.72 62.98 BSR Held-out Scope Level Data Morante et al. (2009b) Cue classiﬁer & Scope Rules + Ranking 74.03 89.62 70.54 89.41 72.25 89.52 405 Computational Linguistics Volume 38, Number 2 To some degree, some of the differences are to be expected, perhaps, at least with respect to BSP. For example, the BSP evaluation represents a held-out setting for both the cue and scope component in the machine learned system of Morante and Daelemans (2009b). While also true for our cue classiﬁer and subtree ranker, it is not strictly speaking the case for the dependency rules, and so the potential effect of any overﬁtting during learning might be less visible. The small set of manually deﬁned rules are general in nature, targeting the general syntactic constructions expressing negation, as shown in Table 13. In addition to being based on the BioScope annotation guidelines, however, both the abstracts and the full papers were consulted for patterns, and the fact that rule development has included intermediate testing on BSP (although mostly dur"
J12-2005,D08-1075,0,0.0736588,"Missing"
J12-2005,W10-3006,0,0.343028,"Missing"
J12-2005,nivre-etal-2006-maltparser,0,0.110423,"Missing"
J12-2005,P08-1108,0,0.0459012,"the effect of using a domain-adapted PoS tagger (Section 6.1.4). 6.1.1 Stacked Dependency Parsing. For syntactic analysis we use the open-source MaltParser (Nivre, Hall, and Nilsson 2006), a platform for data-driven dependency parsing. For improved accuracy and portability across domains and genres, we make our parser incorporate the predictions of a large-scale, general-purpose Lexical-Functional Grammar parser. A technique dubbed parser stacking enables the data-driven parser to learn from the output of another parser, in addition to gold-standard treebank annotations (Martins et al. 2008; Nivre and McDonald 2008). This technique has been shown to provide signiﬁcant improvements in accuracy for both English and German (Øvrelid, Kuhn, and Spreyer 2009), and a similar set-up using an HPSG grammar has been shown to increase domain independence in data-driven dependency parsing (Zhang and Wang 2009). The stacked parser used here is identical to the parser described in Øvrelid, Kuhn, and Spreyer (2009), except for the preprocessing in terms of tokenization and PoS tagging, which is performed as detailed in Sections 2.1–2.2. The parser combines two quite different approaches—data-driven dependency parsing an"
J12-2005,C10-1155,1,0.869645,"Missing"
J12-2005,N07-1051,0,0.0167452,"system output of Morante and Daelemans (2009b), however, we also computed cue-level scores for their system. Morante and Daelemans (2009b) identify cues using a small list of unambiguous cue words compiled from the abstracts in combination with applying a decision tree classiﬁer to the remaining words. Their features record information about neighboring word forms, PoS, and chunk information from GENIA. Zhu et al. (2010) train an SVM to classify tokens according to a BIO-scheme using surface-oriented n-gram features in addition to various syntactic features extracted using the Berkley parser (Petrov and Klein 2007) trained on the GENIA treebank. Looking at the results in Table 12, we see that the performance of our cue classiﬁer compares favorably with the systems of both Morante and Daelemans (2009b) and Zhu et al. (2010), achieving a higher cue-level F1 across all data sets (with differences in classiﬁer decisions with respect to Morante and Daelemans [2009b] being statistically signiﬁcant for all of them). For the 10-fold run, the biggest difference concerns token-level precision, where both the system of Zhu et al. (2010) and our own achieves a substantially higher score than that of Morante and Dae"
J12-2005,W10-3008,0,0.0167739,"ask: as a sequence labeling task and using only token-level, lexical information. Morante, van Asch, and Daelemans (2010) then extended on this system by also adding syntactic features, resulting in the top performing system of the CoNLL-2010 Shared Task at the scope-level (corresponding to the second subtask). It is interesting to note that all the top performers use various types of syntactic information in their scope resolution systems: The output from a dependency parser (MaltParser) (Morante, van Asch, and Daelemans 2010; Velldal, Øvrelid, and Oepen 2010), a tag sequence grammar (RASP) (Rei and Briscoe 2010), as well as constituent analysis in combination with dependency triplets (Stanford lexicalized parser) (Kilicoglu and Bergler 2010). The majority of systems perform classiﬁcation at the token level, using some variant of machine learning with a BIO classiﬁcation scheme and a post-processing step to assemble the full scope (Farkas et al. 2010), although several of the top performers employ manually constructed rules (Kilicoglu and Bergler 2010; Velldal, Øvrelid, and Oepen 2010) or even combinations of machine learning and rules (Rei and Briscoe 2010). 5. Identifying Speculation Cues We now tur"
J12-2005,P02-1035,0,0.0191491,"ar (ERG; Flickinger [2002]), a general-purpose, wide-coverage grammar couched in the framework of an HPSG (Pollard and Sag 1987, 1994). The approach rests on two main assumptions: Firstly, that the annotated scope of a speculation cue corresponds to a syntactic constituent and secondly, that we can automatically learn a ranking function that selects the correct constituent. Our ranking approach to scope resolution is abstractly related to statistical parse selection, and in particular work on discriminative parse selection for uniﬁcation based grammars, such as those by Johnson et al. (1999), Riezler et al. (2002), Malouf and van Noord (2004), and Toutanova et al. (2005). The overall goal is to learn a function for ranking syntactic structures, based on training data that annotates which tree(s) are correct and incorrect for each sentence. In our case, however, rather than discriminating between complete analyses for a given sentence, we want to learn a ranking function over candidate subtrees (i.e., constituents) within a parse (or possibly even within several parses). Figure 3 presents an example derivation tree that represents a complete HPSG analysis. Starting from the cue and working through the t"
J12-2005,P08-1033,0,0.0127087,"nce of boolean values that results (FP = 0, TP = 1) can be directly paired with the corresponding sequence for a different system so that the sign-test can be applied as above. Note that our modiﬁed scorer for negation is available from our Web page of supplemental materials,2 together with the system output (in XML following the BioScope DTD) for all end-to-end runs with our ﬁnal model conﬁgurations. 4. Related Work on Speculation Labeling Although there exists a body of earlier work on identifying uncertainty on the sentence level, (Light, Qiu, and Srinivasan 2004; Medlock and Briscoe 2007; Szarvas 2008), the task of resolving the in-sentence scope of speculation cues was ﬁrst pioneered by Morante and Daelemans (2009a). In this sense, the CoNLL-2010 Shared Task (Farkas et al. 2010) entered largely uncharted territory and contributed to an increased interest for this task. Virtually all systems for resolving speculation scope implement a two-stage architecture: First there is a component that identiﬁes the speculation cues and then there is a component for resolving the in-sentence scopes of these cues. In this section we provide a brief review of previous work on this problem, putting emphasi"
J12-2005,W10-3002,0,0.723173,"a two-stage architecture: First there is a component that identiﬁes the speculation cues and then there is a component for resolving the in-sentence scopes of these cues. In this section we provide a brief review of previous work on this problem, putting emphasis of the best performers from the two corresponding subtasks of the CoNLL-2010 Shared Task, cue detection (Task 1) and scope resolution (Task 2). 4.1 Related Work on Identifying Speculation Cues The top-ranked system for Task 1 in the ofﬁcial CoNLL-2010 Shared Task evaluation approached cue identiﬁcation as a sequence labeling problem (Tang et al. 2010). Similarly to the decision-tree approach of Morante and Daelemans (2009a), Tang et al. (2010) set out to label tokens according to a BIO-scheme; indicating whether they are at the Beginning, Inside, or Outside of a speculation cue. In the “cascaded” system architecture of Tang et al. (2010), the predictions of both a Conditional Random Field (CRF) sequence classiﬁer and an SVM-based Hidden Markov Model (HMM) are both combined in a second CRF. In terms of the overall approach, namely, viewing the problem as a sequence labeling task, Tang et al. (2010) are actually representative of the majorit"
J12-2005,W10-3007,1,0.912943,"Missing"
J12-2005,W08-0606,0,0.622666,"ce in this respect, where the topic was speculation detection for the domain of biomedical research literature ∗ University of Oslo, Department of Informatics, PB 1080 Blindern, 0316 Oslo, Norway. E-mail: {erikve,liljao,jread,oe}@ifi.uio.no. Submission received: 5 April 2011; revised submission received: 30 September 2011; accepted for publication: 2 December 2011. © 2012 Association for Computational Linguistics Computational Linguistics Volume 38, Number 2 (Farkas et al. 2010). This particular area has been the focus of much current research, triggered by the release of the BioScope corpus (Vincze et al. 2008)—a collection of scientiﬁc abstracts, full papers, and clinical reports with manual annotations of words that signal speculation or negation (so-called cues), as well as of the scopes of these cues within the sentences. The following examples from BioScope illustrate how sentences are annotated with respect to speculation. Cues are here shown using angle brackets, with braces corresponding to their annotated scopes: (1) {The speciﬁc role of the chromodomain is unknown} but chromodomain swapping experiments in Drosophila {suggest that they {might be protein interaction modules}} [18]. (2)"
J12-2005,W10-3003,0,0.0796274,"Missing"
J12-2005,P09-1043,0,0.0183721,"nd genres, we make our parser incorporate the predictions of a large-scale, general-purpose Lexical-Functional Grammar parser. A technique dubbed parser stacking enables the data-driven parser to learn from the output of another parser, in addition to gold-standard treebank annotations (Martins et al. 2008; Nivre and McDonald 2008). This technique has been shown to provide signiﬁcant improvements in accuracy for both English and German (Øvrelid, Kuhn, and Spreyer 2009), and a similar set-up using an HPSG grammar has been shown to increase domain independence in data-driven dependency parsing (Zhang and Wang 2009). The stacked parser used here is identical to the parser described in Øvrelid, Kuhn, and Spreyer (2009), except for the preprocessing in terms of tokenization and PoS tagging, which is performed as detailed in Sections 2.1–2.2. The parser combines two quite different approaches—data-driven dependency parsing and “deep” parsing with a hand-crafted grammar—and thus provides us with a broad range of different types of linguistic information to draw upon for the speculation resolution task. MaltParser is based on a deterministic parsing strategy in combination with treebank-induced classiﬁers for"
J12-2005,P11-2033,0,0.0199828,"nt|ntype:common _ 4 4 4 3 0 7 5 9 10 7 0 SPECDET ADJUNCT ADJUNCT SUBJ ROOT PHI XCOMP PHI SPECDET OBL-AG PUNC 4 4 4 5 0 5 6 7 10 8 5 NMOD NMOD NMOD SBJ ROOT VC VC LGS NMOD PMOD P to dependency format (Johansson and Nugues 2007) and extended with XLE features, as described previously. Parsing uses the arc-eager mode of MaltParser and an SVM with a polynomial kernel. When tested using 10-fold cross validation on the enhanced PTB, the parser achieves a labeled accuracy score of 89.8, which is lower than the current stateof-the-art for transition-based dependency parsers (to wit, the 91.8 score of Zhang and Nivre 2011, although not directly comparable given that they test exclusively on WSJ Section 23), but with the advantage of providing us with the deep linguistic information from the XLE. 6.1.2 Rule Overview. Our scope resolution rules take as input a parsed sentence that has been further tagged with speculation cues. We assume the default scope to start at the cue word and span to the end of the sentence (modulo punctuation), and this scope also provides the baseline when evaluating our rules. In developing the rules, we made use of the information provided by the guidelines for scope annotation in the"
J12-2005,D10-1070,0,0.0588464,"Missing"
J12-2005,W07-2207,1,\N,Missing
J12-2005,W10-3001,0,\N,Missing
J12-2005,E99-1043,0,\N,Missing
K16-1012,J94-2001,0,0.338051,"et al. (2016) demonstrated how words of the same part of speech cluster into distinct groups in a distributional model, and Tsvetkov et al. (2015) proved that dimensions of distributional models are correlated with different linguistic features, releasing an evaluation dataset based on this. Various types of distributional information has also played an important role in previous work done on the related problem of unsupervised PoS acquisition. As discussed in Christodoulopoulos et al. (2010), we can separate at least three main directions within this line of work: Disambiguation approaches (Merialdo, 1994; Toutanova and Johnson, 2007; Ravi and Knight, 2009) that start out from a dictionary providing possible tags for different words; prototype-driven approaches (Haghighi and Klein, 2006; Christodoulopoulos 3 PoS clusters in distributional models Our hypothesis is that for the majority of words their parts of speech can be inferred from their embeddings in a distributional model. This inference can be considered a classification problem: we are to train an algorithm that takes a word vector as input and outputs its part of speech. If the word em116 beddings do contain PoS-related data, the prop"
K16-1012,N13-1090,0,0.264179,"classes is indeed stored in distributional models. Words belonging to different parts of speech possess different contexts: in English, articles are typically followed by nouns, verbs are typically accompanied by adverbs and so on. It means that during the training stage, words of one PoS should theoretically cluster together or at least their embeddings should retain some similarity allowing for their separation from words belonging to other parts of speech. Recently, among others, Tsuboi (2014) and Plank et al. (2016) have demonstrated how word embeddings can improve supervised PoS-tagging. Mikolov et al. (2013b) showed that there also exist regular relations between words from different classes: the vector of ‘Brazil’is related to ‘Brazilian’ in the same way as ‘England’ is related to ‘English’ and so on. Later, Liu et al. (2016) demonstrated how words of the same part of speech cluster into distinct groups in a distributional model, and Tsvetkov et al. (2015) proved that dimensions of distributional models are correlated with different linguistic features, releasing an evaluation dataset based on this. Various types of distributional information has also played an important role in previous work d"
K16-1012,D10-1056,0,0.112098,"continuum which does not exhibit sharp boundaries between the categories’. When annotating natural language texts for parts of speech, the choice of a PoS tag in many 115 Proceedings of the 20th SIGNLL Conference on Computational Natural Language Learning (CoNLL), pages 115–125, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics et al., 2010) based on a small number of prototypical examples for each PoS; induction approaches that are completely unsupervised and make no use of prior knowledge. This is also the main focus of the comparative survey provided by (Christodoulopoulos et al., 2010). Work on PoS induction has a long history – including the use of distributional methods – going back at least to Schütze (1995), and recent work has demonstrated that word embeddings can be useful for this task as well (Yatbaz et al., 2012; Lin et al., 2015; Ling et al., 2015a). In terms of positioning this study relative to previous work, it falls somewhere in between the distinctions made above. It is perhaps closest to disambiguation approaches, but it is not unsupervised given that we make use of existing tag annotations when training our embeddings and predictors. The goal is also differ"
K16-1012,N06-1041,0,0.173179,"istributional models are correlated with different linguistic features, releasing an evaluation dataset based on this. Various types of distributional information has also played an important role in previous work done on the related problem of unsupervised PoS acquisition. As discussed in Christodoulopoulos et al. (2010), we can separate at least three main directions within this line of work: Disambiguation approaches (Merialdo, 1994; Toutanova and Johnson, 2007; Ravi and Knight, 2009) that start out from a dictionary providing possible tags for different words; prototype-driven approaches (Haghighi and Klein, 2006; Christodoulopoulos 3 PoS clusters in distributional models Our hypothesis is that for the majority of words their parts of speech can be inferred from their embeddings in a distributional model. This inference can be considered a classification problem: we are to train an algorithm that takes a word vector as input and outputs its part of speech. If the word em116 beddings do contain PoS-related data, the properly trained classifier will correctly predict PoS tags for the majority of words: it means that these lexical entities conform to a dominant distributional pattern of their part of spe"
K16-1012,petrov-etal-2012-universal,0,0.0709563,"Missing"
K16-1012,P16-2067,0,0.025077,"h. For several years already it has been known that some information about morphological word classes is indeed stored in distributional models. Words belonging to different parts of speech possess different contexts: in English, articles are typically followed by nouns, verbs are typically accompanied by adverbs and so on. It means that during the training stage, words of one PoS should theoretically cluster together or at least their embeddings should retain some similarity allowing for their separation from words belonging to other parts of speech. Recently, among others, Tsuboi (2014) and Plank et al. (2016) have demonstrated how word embeddings can improve supervised PoS-tagging. Mikolov et al. (2013b) showed that there also exist regular relations between words from different classes: the vector of ‘Brazil’is related to ‘Brazilian’ in the same way as ‘England’ is related to ‘English’ and so on. Later, Liu et al. (2016) demonstrated how words of the same part of speech cluster into distinct groups in a distributional model, and Tsvetkov et al. (2015) proved that dimensions of distributional models are correlated with different linguistic features, releasing an evaluation dataset based on this. V"
K16-1012,P09-1057,0,0.0238424,"same part of speech cluster into distinct groups in a distributional model, and Tsvetkov et al. (2015) proved that dimensions of distributional models are correlated with different linguistic features, releasing an evaluation dataset based on this. Various types of distributional information has also played an important role in previous work done on the related problem of unsupervised PoS acquisition. As discussed in Christodoulopoulos et al. (2010), we can separate at least three main directions within this line of work: Disambiguation approaches (Merialdo, 1994; Toutanova and Johnson, 2007; Ravi and Knight, 2009) that start out from a dictionary providing possible tags for different words; prototype-driven approaches (Haghighi and Klein, 2006; Christodoulopoulos 3 PoS clusters in distributional models Our hypothesis is that for the majority of words their parts of speech can be inferred from their embeddings in a distributional model. This inference can be considered a classification problem: we are to train an algorithm that takes a word vector as input and outputs its part of speech. If the word em116 beddings do contain PoS-related data, the properly trained classifier will correctly predict PoS ta"
K16-1012,N15-1144,0,0.0459081,"Missing"
K16-1012,E95-1020,0,0.444155,"of a PoS tag in many 115 Proceedings of the 20th SIGNLL Conference on Computational Natural Language Learning (CoNLL), pages 115–125, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics et al., 2010) based on a small number of prototypical examples for each PoS; induction approaches that are completely unsupervised and make no use of prior knowledge. This is also the main focus of the comparative survey provided by (Christodoulopoulos et al., 2010). Work on PoS induction has a long history – including the use of distributional methods – going back at least to Schütze (1995), and recent work has demonstrated that word embeddings can be useful for this task as well (Yatbaz et al., 2012; Lin et al., 2015; Ling et al., 2015a). In terms of positioning this study relative to previous work, it falls somewhere in between the distinctions made above. It is perhaps closest to disambiguation approaches, but it is not unsupervised given that we make use of existing tag annotations when training our embeddings and predictors. The goal is also different; rather than performing PoS acquisition or tagging for its own sake, the main focus here is on analyzing the boundaries of d"
K16-1012,D15-1161,0,0.0187252,"ermany, August 7-12, 2016. 2016 Association for Computational Linguistics et al., 2010) based on a small number of prototypical examples for each PoS; induction approaches that are completely unsupervised and make no use of prior knowledge. This is also the main focus of the comparative survey provided by (Christodoulopoulos et al., 2010). Work on PoS induction has a long history – including the use of distributional methods – going back at least to Schütze (1995), and recent work has demonstrated that word embeddings can be useful for this task as well (Yatbaz et al., 2012; Lin et al., 2015; Ling et al., 2015a). In terms of positioning this study relative to previous work, it falls somewhere in between the distinctions made above. It is perhaps closest to disambiguation approaches, but it is not unsupervised given that we make use of existing tag annotations when training our embeddings and predictors. The goal is also different; rather than performing PoS acquisition or tagging for its own sake, the main focus here is on analyzing the boundaries of different PoS classes. In Section 5, this analysis is complemented by experiments with using word embeddings for PoS prediction on unlabeled data, and"
K16-1012,silveira-etal-2014-gold,0,0.0485505,"Missing"
K16-1012,N15-1142,0,0.0194484,"ermany, August 7-12, 2016. 2016 Association for Computational Linguistics et al., 2010) based on a small number of prototypical examples for each PoS; induction approaches that are completely unsupervised and make no use of prior knowledge. This is also the main focus of the comparative survey provided by (Christodoulopoulos et al., 2010). Work on PoS induction has a long history – including the use of distributional methods – going back at least to Schütze (1995), and recent work has demonstrated that word embeddings can be useful for this task as well (Yatbaz et al., 2012; Lin et al., 2015; Ling et al., 2015a). In terms of positioning this study relative to previous work, it falls somewhere in between the distinctions made above. It is perhaps closest to disambiguation approaches, but it is not unsupervised given that we make use of existing tag annotations when training our embeddings and predictors. The goal is also different; rather than performing PoS acquisition or tagging for its own sake, the main focus here is on analyzing the boundaries of different PoS classes. In Section 5, this analysis is complemented by experiments with using word embeddings for PoS prediction on unlabeled data, and"
K16-1012,N03-1033,0,0.0596725,"m Hibs fans in Edinburgh pre season?’), they are very close to adjectives or adverbs, so the predictions of the distributional classifier once again suggest shifting parts of speech boundaries a bit. Error analysis on the vocabulary from the Universal Dependencies Treebank showed pretty much the same results, except for some differences already mentioned above. There exists another way to retrieve this kind of data: to process tagged data with a conventional PoS tagger and analyze the resulting confusion matrix. We tested this approach by processing the whole BNC with the Stanford PoS Tagger (Toutanova et al., 2003). Note that as an input to the tagger we used not the whole sentences from the corpora, but separate tokens, to mimic our # 172675 47202 40218 24075 9723 Actual Predicted NNP VB JJ NN JJ NN NN NN JJ VB workflow with the distributional predictor. Prior to this, BNC tags were converted to the Penn Treebank tagset3 to match the output of the tagger. As we are interested in coarse, ‘overarching’ word classes, inflectional forms were merged into one tag. That was easy to accomplish by dropping all characters of the tags after the first two (excluding proper noun tags, which were all converted to NN"
K16-1012,D14-1101,0,0.0263202,"to parts of speech. For several years already it has been known that some information about morphological word classes is indeed stored in distributional models. Words belonging to different parts of speech possess different contexts: in English, articles are typically followed by nouns, verbs are typically accompanied by adverbs and so on. It means that during the training stage, words of one PoS should theoretically cluster together or at least their embeddings should retain some similarity allowing for their separation from words belonging to other parts of speech. Recently, among others, Tsuboi (2014) and Plank et al. (2016) have demonstrated how word embeddings can improve supervised PoS-tagging. Mikolov et al. (2013b) showed that there also exist regular relations between words from different classes: the vector of ‘Brazil’is related to ‘Brazilian’ in the same way as ‘England’ is related to ‘English’ and so on. Later, Liu et al. (2016) demonstrated how words of the same part of speech cluster into distinct groups in a distributional model, and Tsvetkov et al. (2015) proved that dimensions of distributional models are correlated with different linguistic features, releasing an evaluation"
K16-1012,D15-1243,0,0.0196381,"s should retain some similarity allowing for their separation from words belonging to other parts of speech. Recently, among others, Tsuboi (2014) and Plank et al. (2016) have demonstrated how word embeddings can improve supervised PoS-tagging. Mikolov et al. (2013b) showed that there also exist regular relations between words from different classes: the vector of ‘Brazil’is related to ‘Brazilian’ in the same way as ‘England’ is related to ‘English’ and so on. Later, Liu et al. (2016) demonstrated how words of the same part of speech cluster into distinct groups in a distributional model, and Tsvetkov et al. (2015) proved that dimensions of distributional models are correlated with different linguistic features, releasing an evaluation dataset based on this. Various types of distributional information has also played an important role in previous work done on the related problem of unsupervised PoS acquisition. As discussed in Christodoulopoulos et al. (2010), we can separate at least three main directions within this line of work: Disambiguation approaches (Merialdo, 1994; Toutanova and Johnson, 2007; Ravi and Knight, 2009) that start out from a dictionary providing possible tags for different words; p"
K16-1012,D12-1086,0,0.0413494,"Missing"
K16-1012,L16-1262,0,\N,Missing
K16-1012,J15-4006,0,\N,Missing
K16-2002,J12-2005,1,0.934737,"o an overall account of discourse structure and of having annotation decisions concentrate on the individual instances of discourse relations, rather than on their interactions. Previous work on this task has usually broken it down into a set of sub-problems, which are solved in a pipeline architecture (roughly: identify connectives, then arguments, then discourse senses; Lin et al., 2014). While adopting a similar pipeline approach, the OPT discourse parser also builds on and extends a method that has previously achieved state-of-the-art results for the detection of speculation and negation (Velldal et al., 2012; Read 3 Relation Identification Explicit Connectives Our classifier for detecting explicit discourse connectives extends the work by Velldal et al. (2012) for identifying expressions of speculation and negation. The approach treats the set of connectives observed in the training data as a closed class, and ‘only’ attempts to disambiguate occurrences of these token sequences in new data. Connectives can be single- or multitoken sequences (e.g. ‘as’ vs. ‘as long as’). In cases 20 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 20–26, c Berlin, Germ"
K16-2002,K15-2002,0,0.285154,"r sense classifier described has been developed specifically for OPT. The OPT submission to the Shared Task of the 2016 Conference on Natural Language Learning (CoNLL) implements a ‘classic’ pipeline architecture, combining binary classification of (candidate) explicit connectives, heuristic rules for non-explicit discourse relations, ranking and ‘editing’ of syntactic constituents for argument identification, and an ensemble of classifiers to assign discourse senses. With an end-toend performance of 27.77 F1 on the English ‘blind’ test data, our system advances the previous state of the art (Wang & Lan, 2015) by close to four F1 points, with particularly good results for the argument identification sub-tasks. 1 2 System Architecture Our system overview is shown in Figure 1. The individual modules interface through JSON files which resemble the desired output files of the Task. Each module adds the information specified for it. We will describe them here in thematic blocks, while the exact order of the modules can be seen in the figure. Relation identification (§3) includes the detection of explicit discourse connectives and the stipulation of non-explicit relations. Our argument identification mod"
K16-2002,K15-2001,0,0.136259,"21.6 48.1 27.8 F1 91.8 52.4 75.2 44.0 34.5 64.6 76.4 52.0 21.9 48.2 27.8 Table 3: Per-component breakdown of system performance, compared to top performers in 2015/16. 6 optimizing the primal objective and setting the error penalty term C to 0.3. Experimental Results Overall Results Table 3 summarizes OPT system performance in terms of the metrics computed by the official scorer for the Shared Task, against both the WSJ and ‘blind’ test sets. To compare against the previous state of the art, we include results for the top-performing systems from the 2015 and 2016 competitions (as reported by Xue et al., 2015, and Xue et al., 2016, respectively). Where applicable, best results (when comparing F1 ) are highlighted for each sub-task and -metric. The highlighting makes it evident that the OPT system is competitive to the state of the art across the board, but particularly so on the argument identification sub-task and on the ‘blind’ test data: In terms of the WSJ test data, OPT would have ranked second in the 2015 competition, but on the ‘blind’ data it outperforms the previous state of the art on all but one metric for which contrastive results are provided by Xue et al.. Where earlier systems tend"
K16-2002,P09-2004,0,0.792028,"ve heads only, these are the unit of disambiguation in OPT. Disambiguation is performed as point-wise (‘per-connective’) classification using the support vector machine implementation of the SVMlight toolkit (Joachims, 1999). Tuning of feature configurations and the error-to-margin cost parameter (C) was performed by ten-fold cross validation on the Task training set. candidate configurations against the development data. The model used in the system submission includes n-grams of up to three preceding and following positions, full feature conjunction for the ‘self’ and ‘parent’ categories of Pitler & Nenkova (2009), but limited conjunctions involving their ‘left’ and ‘right’ sibling categories, and none of the ‘connected context’ features suggested by Wang & Lan (2015). This model has some 1.2 million feature types. Non-Explicit Relations According to the PDTB guidelines, non-explicit relations must be stipulated between each pair of sentences iff four conditions hold: two sentences (a) are adjacent; (b) are located in the same paragraph; and (c) are not yet ‘connected’ by an explicit connective; and (d) a coherence relation can be inferred or an entity-based relation holds between them. We proceed stra"
K16-2002,prasad-etal-2008-penn,0,\N,Missing
K16-2002,K15-2003,0,\N,Missing
K16-2002,S12-1041,1,\N,Missing
K16-2002,K16-2001,0,\N,Missing
L16-1272,P14-5010,0,0.00280838,"I (f ; STRONG) − NPMI (f ; WEAK)) 2 We used the strength measure to score each of the twelve expressions investigated in Lomotan, Michel, Lin, and Shiffman’s (2009) survey of health professionals’ interpretation of deontics. The resulting scores correlated reasonably strongly with the ‘level of obligation’ scores found by Lomotan et al. (Pearson’s r = 0.56), indicating that there are similarities between the results of our respective methodologies. Table 2 lists the twenty highest scoring trigrams found using this measure, where trigrams are formed from lemmas predicted by Stanford’s CoreNLP (Manning et al., 2014). The trigram be reasonable in suggests a deontic expression not covered by the survey of Lomotan et al. (2009), as in “action X is reasonable for all patients”. Other features are not deontic expressions yet seem reasonable indicators of recommendation strength—for example, all patient with narrows the focus of a recommendation to all patients of a particular class. Some trigrams seem less intuitive cues of recommendation strength and are perhaps indicative of extra-linguistic phenomena (e.g. [S] the uspstf having a high postive strength suggests that USPSTF guideline developers are more incl"
L16-1272,P02-1053,0,0.014451,"Section 5.2 investigates the multi-class problem with experiments involving all five labels. 5.1. Discriminating between STRONG and WEAK recommendations 4. Features of recommendation strength NPMI (f ; l) = log NPMI falls in the range [−1, 1] where 1 indicates perfect positive correlation, -1 indicates perfect negative correlation, and 0 indicates independence. In our experiments we computed probability using maximum likelihood estimation based on the number of recommendations containing a feature, as observed in the development corpus. Adopting the semantic orientation approach introduced by Turney (2002) for sentiment polarity, we estimate the strength conveyed by a feature by finding the difference4 between its association with STRONG and WEAK labels: 1 strength (f ) = (NPMI (f ; STRONG) − NPMI (f ; WEAK)) 2 We used the strength measure to score each of the twelve expressions investigated in Lomotan, Michel, Lin, and Shiffman’s (2009) survey of health professionals’ interpretation of deontics. The resulting scores correlated reasonably strongly with the ‘level of obligation’ scores found by Lomotan et al. (Pearson’s r = 0.56), indicating that there are similarities between the results of our"
L16-1272,J12-2005,1,0.776936,"ressions situated in otherwise identical sentences, rating obligation from 0 (no obligation) to 100 (full obligation). Participants interpreted the deontic expression “must” as conveying the highest level of obligation, while while “may” and “may consider” indicated the lowest. Lomotan et al. offer an interesting evaluation of health professionals’ interpretation of deontic expressions; the corpus presented in this paper can facilitate an extension 1724 of this research by considering the interaction of deontic expressions with other extra-propositional aspects of meaning such as speculation (Velldal et al., 2012), and by analysing associations of linguistic features with the explicit labels of recommendation strength. 3. The corpus The initial CREST release contains 170 guidelines developed by 69 different institutions. These institutions employ 30 different schemes for the rating of recommendations, with variations in grade labels and descriptions. Below we discuss a unified rating scheme we use for categorising recommendations in the corpus (Section 3.2), after discussing details about the extraction process (Section 3.1). 3.1. Data collection The guidelines were obtained from the National Guideline"
L16-1272,P15-2107,0,0.03481,"Missing"
L18-1661,P13-2088,0,0.0761269,"Missing"
L18-1661,P07-1056,0,0.386504,"Missing"
L18-1661,P12-3005,0,0.0870595,"Missing"
L18-1661,P11-1015,0,0.0529325,"is of the reviews generalize to non-review texts, and we plan to annotate aspect-based sentiment for a selection of general-domain news texts as well. Related work The dataset described in the current paper is the first of its kind for Norwegian. For other languages, however, the field has seen a substantial amount of SA research based on rated reviews, either user-generated or by professional reviewers. This has often been based on single-domain datasets, and examples include (for English unless otherwise noted) movie reviews collected from aggregator sites like IMDb.com (Pang and Lee, 2004; Maas et al., 2011) and RottenTomatoes.com (Pang and Lee, 2005; Socher et al., 2013), hotel reviews from TripAdvisor (Wang et al., 2010), book reviews (in Arabic) (Aly and Atiya, 2013), app reviews compiled from Apple App store and Google Play (Guzman and Maalej, 2014), and reviews of restaurants and other businesses in the Yelp open dataset.5 However, the unbalanced nature of these datasets (single domains) can impose inherent limitations on the ability of https://www.yelp.com/dataset Future work 6. Summary and outlook The current paper has described the creation of the Norwegian Review Corpus; NoReC (Ver. 1.0."
L18-1661,L16-1262,0,0.0525466,"Missing"
L18-1661,L16-1250,1,0.85854,"Missing"
L18-1661,P04-1035,0,0.0256263,"developed on the basis of the reviews generalize to non-review texts, and we plan to annotate aspect-based sentiment for a selection of general-domain news texts as well. Related work The dataset described in the current paper is the first of its kind for Norwegian. For other languages, however, the field has seen a substantial amount of SA research based on rated reviews, either user-generated or by professional reviewers. This has often been based on single-domain datasets, and examples include (for English unless otherwise noted) movie reviews collected from aggregator sites like IMDb.com (Pang and Lee, 2004; Maas et al., 2011) and RottenTomatoes.com (Pang and Lee, 2005; Socher et al., 2013), hotel reviews from TripAdvisor (Wang et al., 2010), book reviews (in Arabic) (Aly and Atiya, 2013), app reviews compiled from Apple App store and Google Play (Guzman and Maalej, 2014), and reviews of restaurants and other businesses in the Yelp open dataset.5 However, the unbalanced nature of these datasets (single domains) can impose inherent limitations on the ability of https://www.yelp.com/dataset Future work 6. Summary and outlook The current paper has described the creation of the Norwegian Review Corp"
L18-1661,P05-1015,0,0.197626,"texts, and we plan to annotate aspect-based sentiment for a selection of general-domain news texts as well. Related work The dataset described in the current paper is the first of its kind for Norwegian. For other languages, however, the field has seen a substantial amount of SA research based on rated reviews, either user-generated or by professional reviewers. This has often been based on single-domain datasets, and examples include (for English unless otherwise noted) movie reviews collected from aggregator sites like IMDb.com (Pang and Lee, 2004; Maas et al., 2011) and RottenTomatoes.com (Pang and Lee, 2005; Socher et al., 2013), hotel reviews from TripAdvisor (Wang et al., 2010), book reviews (in Arabic) (Aly and Atiya, 2013), app reviews compiled from Apple App store and Google Play (Guzman and Maalej, 2014), and reviews of restaurants and other businesses in the Yelp open dataset.5 However, the unbalanced nature of these datasets (single domains) can impose inherent limitations on the ability of https://www.yelp.com/dataset Future work 6. Summary and outlook The current paper has described the creation of the Norwegian Review Corpus; NoReC (Ver. 1.0.1). The final dataset comprises more than 3"
L18-1661,D13-1170,0,0.00481102,"o annotate aspect-based sentiment for a selection of general-domain news texts as well. Related work The dataset described in the current paper is the first of its kind for Norwegian. For other languages, however, the field has seen a substantial amount of SA research based on rated reviews, either user-generated or by professional reviewers. This has often been based on single-domain datasets, and examples include (for English unless otherwise noted) movie reviews collected from aggregator sites like IMDb.com (Pang and Lee, 2004; Maas et al., 2011) and RottenTomatoes.com (Pang and Lee, 2005; Socher et al., 2013), hotel reviews from TripAdvisor (Wang et al., 2010), book reviews (in Arabic) (Aly and Atiya, 2013), app reviews compiled from Apple App store and Google Play (Guzman and Maalej, 2014), and reviews of restaurants and other businesses in the Yelp open dataset.5 However, the unbalanced nature of these datasets (single domains) can impose inherent limitations on the ability of https://www.yelp.com/dataset Future work 6. Summary and outlook The current paper has described the creation of the Norwegian Review Corpus; NoReC (Ver. 1.0.1). The final dataset comprises more than 35,000 full-text review"
L18-1661,K17-3009,0,0.0209689,"Missing"
L18-1661,L16-1680,0,0.0200464,"Missing"
L18-1661,W17-0201,1,0.878115,"Missing"
L18-1661,F13-2033,0,0.0202319,"ically also reported for user-generated reviews, though with a stronger preference for the highest score (Baccianella et al., 2009). In Figures 3 and 4 we see a more detailed view of the rating distribution for each category and source. In Figure 3 we see that the ‘stage’ and ‘products’ categories are most strongly skewed towards the rating of 5. As most of the product reviews were gathered from ‘DinSide.no’, we see a similar distribution for this source in Figure 4. 4189 models to generalize. Some datasets combine reviews from multiple domains for better balance, like the French SA corpus of Vincent and Winterstein (2013), combining reviews of movies, books and hotels (from Allocine.fr, Amazon.fr, and TripAdvisor.fr, respectively), or the Arabic SA corpus of ElSahar and El-Beltagy (2015), combining reviews of hotels, restaurants, movies, restaurants and product reviews (from TripAdvisor, elCinema.com, Qaym.com and Souq.com). There also exists several datasets based on product reviews from Amazon, which can potentially also have the advantage of covering a more diverse selection of domains. An example includes the Amazon dataset of Blitzer et al. (2007), comprising reviews of books, DVDs, electronics, and kitch"
lapponi-etal-2014-road,J06-4003,0,\N,Missing
lapponi-etal-2014-road,P12-2074,1,\N,Missing
lapponi-etal-2014-road,P01-1040,0,\N,Missing
S12-1041,P05-1022,0,0.0956249,"be due to annotation errors (insuperable, unhappily, endless, listlessly). Among the FNs, two are due to MWCs not covered by our heuristics (e.g., no more), with the remainder concerning affixes. 3 Constituent-Based Scope Resolution During the development of our scope resolution system we have pursued both a rule-based and datadriven approach. Both are rooted in the assumption that the scope of negations corresponds to a syntactically meaningful unit. Our starting point here will be the syntactic analyses provided by the task organizers (see Figure 1), generated using the reranking parser of Charniak and Johnson (2005). However, as alignment between scope annotations and syntactic units is not straightforward for all cases, we apply several exception rules that ‘slacken’ the requirements for alignment, as discussed in Section 3.1. In Sections 3.2 and 3.3 we detail our rule-based and data-driven approaches, respectively. Note that the predictions of the rule-based component will be incorporated as features in the learned model, similarly to the set-up described by Read et al. (2011). Section 3.4 details the post-processing we apply to handle cases of discontinuous scope, beRB//VP/SBAR if SBARWH* RB//VP/S RB"
S12-1041,S12-1035,0,0.263085,"(UiO) to the 2012 *SEM Shared Task on resolving negation. Our submission is an adaption of the negation system of Velldal et al. (2012), which combines SVM cue classification with SVM-based ranking of syntactic constituents for scope resolution. The approach further extends our prior work in that we also identify factual negated events. While submitted for the closed track, the system was the top performer in the shared task overall. 1 (1) There was no answer. Introduction The First Joint Conference on Lexical and Computational Semantics (*SEM 2012) hosts a shared task on resolving negation (Morante and Blanco, 2012). This involves the subtasks of (i) identifying negation cues, (ii) identifying the in-sentence scope of these cues, and (iii) identifying negated (and factual) events. This paper describes a system submitted by the Language Technology Group at the University of Oslo (UiO). Our starting point is the negation system developed by Velldal et al. (2012) for the domain of biomedical texts, an SVM-based system for classifying cues and ranking syntactic constituents to resolve cue scopes. However, we extend and adapt this system in several important respects, such as in terms of the underlying lingui"
S12-1041,morante-daelemans-2012-conandoyle,0,0.137264,"nt is the negation system developed by Velldal et al. (2012) for the domain of biomedical texts, an SVM-based system for classifying cues and ranking syntactic constituents to resolve cue scopes. However, we extend and adapt this system in several important respects, such as in terms of the underlying linguistic formalisms that are used, the textual domain, handling of morphological cues and discontinuous scopes, and in that the current system also identifies negated events. The data sets used for the shared task include the following, all based on negation-annotated Conan Doyle (CD) stories (Morante and Daelemans, 2012): a training set of 3644 sentences (hereafter We describe two different system configurations, both of which were submitted for the closed track (hence we can only make use of the data provided by the task organizers). The systems only differ with respect to how they were optimized. In the first configuration, (hereafter I), all components in the pipeline had their parameters tuned by 10-fold cross-validation across CDTD. The second configuration (II) is tuned against CDD using CDT for training. The rationale for this strategy is to guard against possible overfitting effects that could result"
S12-1041,J12-2005,1,0.589072,"Oslo, Department of Informatics {jread,erikve,liljao,oe}@ifi.uio.no Abstract referred to as CDT), a development set of 787 sentences (CDD), and a held-out evaluation set of 1089 sentences (CDE). We will refer to the combination of CDT and CDD as CDTD. An example of an annotated sentence is shown in (1) below, where the cue is marked in bold, the scope is underlined, and the event marked in italics. This paper describes the first of two systems submitted from the University of Oslo (UiO) to the 2012 *SEM Shared Task on resolving negation. Our submission is an adaption of the negation system of Velldal et al. (2012), which combines SVM cue classification with SVM-based ranking of syntactic constituents for scope resolution. The approach further extends our prior work in that we also identify factual negated events. While submitted for the closed track, the system was the top performer in the shared task overall. 1 (1) There was no answer. Introduction The First Joint Conference on Lexical and Computational Semantics (*SEM 2012) hosts a shared task on resolving negation (Morante and Blanco, 2012). This involves the subtasks of (i) identifying negation cues, (ii) identifying the in-sentence scope of these"
S12-1042,W10-3110,0,0.570231,"in an interesting context of double negation; not dissatisfied. 3 Scope and event resolution In this work, we model negation scope resolution as a special instance of the classical IOB (Inside, Outside, Begin) sequence labeling problem, where negation cues are labeled to be sequence starters and scopes and events as two different kinds of chunks. CRFs allow the computation of p(X|Y), where X is a sequence of labels and Y is a sequence of observations, and have already been shown to be efficient in similar, albeit less involved, tasks of negation scope resolution (Morante and Daelemans, 2009; Councill et al., 2010). We employ the CRF implementation in the Wapiti toolkit, using default settings (Lavergne et al., 2010). A number of features were used to create the models. In addition to the information provided for each token in the CD corpus (lemma, part of speech and constituent), we extracted both left and right token distance to the closest negation cue. Features were expanded to include forward and backward bigrams and trigrams on both token and PoS level, as well as lexicalized PoS unigrams and bigrams2 . Table 2 presents a complete list of features. The more intricate, dependency-based features are"
S12-1042,de-marneffe-etal-2006-generating,0,0.0130195,"Missing"
S12-1042,J02-3001,0,0.11176,"cted Distance (DD) and Bidirectional Distance (BD). DD is extracted by following the reversed, directed edges from token X to the cue. If there is no such path, the value of the feature is -1. BD uses the Dijkstra shortest path algorithm on an undirected representation of the graph. The latter feature proved to be more effective than the former when not used together; using them in conjunction seemed to confuse the model, thus the final model utilizes only BD. We furthermore use the Dependency Graph Path (DGP) as a feature. This feature was inspired by the Parse Tree Path feature presented in Gildea and Jurafsky (2002) in the context of Semantic Role Labeling. It represents the path traversed from each token to the cue, encoding both the dependency relations and the direction of the arc that is traversed: for instance, the relation between our and no in Figure 1 is described as  poss  dobj  nsubj  det. Like Councill et al. (2010), we also encode the PoS of the first and second order syntactic head of each token. For the token no in Figure 1, for instance, we record the PoS of one and escaped, respectively. 3.2 Model-internal representation The token-wise annotations in the CD corpus contain multiple lay"
S12-1042,P10-1052,0,0.061443,"Missing"
S12-1042,W09-1105,0,0.144283,"oncern affixes, including one in an interesting context of double negation; not dissatisfied. 3 Scope and event resolution In this work, we model negation scope resolution as a special instance of the classical IOB (Inside, Outside, Begin) sequence labeling problem, where negation cues are labeled to be sequence starters and scopes and events as two different kinds of chunks. CRFs allow the computation of p(X|Y), where X is a sequence of labels and Y is a sequence of observations, and have already been shown to be efficient in similar, albeit less involved, tasks of negation scope resolution (Morante and Daelemans, 2009; Councill et al., 2010). We employ the CRF implementation in the Wapiti toolkit, using default settings (Lavergne et al., 2010). A number of features were used to create the models. In addition to the information provided for each token in the CD corpus (lemma, part of speech and constituent), we extracted both left and right token distance to the closest negation cue. Features were expanded to include forward and backward bigrams and trigrams on both token and PoS level, as well as lexicalized PoS unigrams and bigrams2 . Table 2 presents a complete list of features. The more intricate, depen"
S12-1042,morante-daelemans-2012-conandoyle,0,0.125404,"dency Features Emanuele Lapponi Erik Velldal Lilja Øvrelid Jonathon Read University of Oslo, Department of Informatics {emanuel,erikve,liljao,jread}@ifi.uio.no Abstract test set of 1089 sentences from The Cardboard Box and The Red Circle (CDE). In these sets, the concept of negation scope extends on the one adopted in the BioScope corpus in several aspects: Negation cues are not part of the scope, morphological (affixal) cues are annotated and scopes can be discontinuous. Moreover, in-scope states or events are marked as negated if they are factual and presented as events that did not happen (Morante and Daelemans, 2012). Examples (1) and (2) below are examples of affixal negation and discontinuous scope respectively: The cues are bold, the tokens contained within their scopes are underlined and the negated event is italicized. This paper describes the second of two systems submitted from the University of Oslo (UiO) to the 2012 *SEM Shared Task on resolving negation. The system combines SVM cue classification with CRF sequence labeling of events and scopes. Models for scopes and events are created using lexical and syntactic features, together with a fine-grained set of labels that capture the scopal behavio"
S12-1042,nivre-etal-2006-maltparser,0,0.0373962,"ependency relation Cue-dependent features Token distance Directed dependency distance Bidirectional dependency distance Dependency path Lexicalized dependency path Table 2: List of features used to train the CRF models. tations result from a conversion of Penn Treebankstyle phrase structure trees, combining ‘classic’ head finding rules with rules that target specific linguistic constructions, such as passives or attributive adjectives. The so-called basic format provides a dependency graph which is a directed tree, see Figure 1 for an example. For the open track submission we used Maltparser (Nivre et al., 2006) with its pre-trained parse model for English.4 The parse model has been trained on a conversion of sections 2-21 of the Wall Street Journal section of the Penn Treebank to Stanford dependencies, augmented with data from Question Bank. The parser was applied to the negation data, using the word tokens and supplied parts-of-speech as input to the parser. The features extracted via the dependency graphs aim at modeling the syntactic relationship between each token and the closest negation cue. Token distance was therefore complemented with two variants of dependency distance from each token to t"
S12-1042,S12-1041,1,0.522151,"nd hence this is also not reported as a development result. Note also that the official evaluation actually includes two different variants of the metrics mentioned above; a set of primary measures with precision computed as P=TP/(TP+FP) and a set of B measures where precision is rather computed as P=TP/SYS, where SYS is the total number of predictions made by the system. The reason why SYS is not identical with TP+FP is that partial matches are 1 Note that the cue classifier applied in the current paper is the same as that used in the other shared task submission from the University of Oslo (Read et al., 2012), and the two system descriptions will therefore have much overlap on this particular point. For all other components the architectures of the two system are completely different, however. 320 only counted as FNs (and not FPs) in order to avoid double penalties. We do not report the B measures for development testing as they were introduced for the final evaluation and hence were not considered in our system optimization. We note though, that the relative-ranking of participating systems for the primary and B measures is identical, and that the correlation between the paired lists of scores is"
S12-1042,J12-2005,1,0.607497,"ults are described in Section 2. Section 3 presents the system for scope and event resolution and details different features, the model-internal representation used for sequence-labeling, as well as the post-processing component. Error analyses for the cue, scope and event components are provided in the respective sections. Section 4 and 5 provide developmental and held-out results, respectively. Finally, we provide conclusions and some reflections regarding future work in Section 6. 2 Cue detection Identification of negation cues is based on the lightweight classification scheme presented by Velldal et al. (2012). By treating the set of cue words as a closed class, Velldal et al. (2012) showed that one could greatly reduce the number of examples presented to the learner, and correspondingly the number of features, while at the same time improving performance. This means that the classifier only attempts to “disambiguate” known cue words while ignoring any words not observed as cues in the training data. The classifier applied in the current submission is extended to also handle affixal negation cues, such as the prefix cue in impatience, the infix in carelessness, and the suffix of colourless. The typ"
S12-1042,W08-0606,0,0.390107,"Missing"
W06-1661,W98-1426,0,0.0670737,"iven the semantics s, is defined as Table 1: A small example set of generator outputs using the ERG. Where the input semantics is no specified for aspects of information structure (e.g. requesting foregrounding of a specific entity), paraphrases include all grammatically legitimate topicalizations. Other choices involve, for example, the optionality of complementizers and relative pronouns, permutation of (intersective) modifiers, and lexical and orthographic alternations. 2.1 Language Models The use of n-gram language models is the most common approach to statistical selection in generation (Langkilde & Knight, 1998; and White (2004); inter alios). In order to better assert the relative performance of the discriminative models and the structural features we present below, we also apply a trigram model to the ranking problem. Using the freely available CMU SLM Toolkit (Clarkson & Rosenfeld, 1997), we trained a trigram model on an unannotated version of the British National Corpus (BNC), containing roughly 100 million words (using Witten-Bell discounting and back-off). Given such a model pn , the score of a realization ri with surface form wik1 = (wi1 ; : : : ; wik ) is then computed as X F (s; r ) = p (w"
W06-1661,I05-1015,1,0.874424,"support vector machine (SVM). These are all models that have proved popular within the NLP community, but it is usually only the first of these three that has been applied to the task of ranking in sentence generation. The latter two models that we present here go beyond the surface information used by the n-gram model, and are trained on a symmetric treebank with features defined over the full HPSG analyses of competing realizations. Furthermore, such discriminative models are suitable for ‘on-line’ use within our generator—adopting the technique of selective unpacking from a packed forest (Carroll & Oepen, 2005)—which means our hybrid realizer obviates the need for exhaustive enumeration of candidate outputs. The present results extend our earlier work (Velldal, Oepen, & Flickinger, 2004)—and the related work of Nakanishi, Miyao, & Tsujii (2005)—to an enlarged data set, more feature types, and additional learners. The rest of this paper is structured as follows. Section 2 first gives a general summary of the various statistical models we will be considering, as well as the measures used for evaluating them. We then go on to define the task we are aiming to solve in terms of treebank data and feature"
W06-1661,W02-2018,0,0.0246851,"n of si . Fol(11) w  (s; r ) = i X j;k (s ; r )(s; r ) j k i 2.4 Evaluation Measures The models presented in this paper are evaluated with respect to two simple metrics: exact match accuracy and word accuracy. The exact match measure simply counts the number of times that the model assigns the highest score to a string that exactly matches a corresponding ‘gold’ or reference sentence (i.e. a sentence that is marked as preferred in the treebank). This score is discounted appropriately in the case of ties between preferred and non-preferred candidates. 1 We use the TADM open-source package (Malouf, 2002) for training the models, using its limited-memory variable metric as the optimization method and experimentally determine the optimal convergence threshold and variance of the prior. 519 Jotunheimen if several realizations are given the top rank by the model. We also include the exact match accuracy for the five best candidates according to the models (see the n-best columns of Table 6). The simple measure of exact match accuracy offers a very intuitive and transparent view on model performance. However, it is also in some respects too harsh as an evaluation measure in our setting since there"
W06-1661,W05-1510,0,0.245041,"Missing"
W06-1661,1995.tmi-1.2,0,0.0748515,"cal generator is part of a larger, semantic transfer MT system. 1 Introduction This paper describes the application of several different statistical models for the task of realization ranking in tactical generation, i.e. the problem of choosing among multiple paraphrases that are generated for a given meaning representation. The specific realization component we use is the opensource chart generator of the Linguistic Knowledge Builder (LKB; Carroll, Copestake, Flickinger, & Poznanski, 1999; Carroll & Oepen, 2005). Given a meaning representation in the form of Minimal Recursion Semantics (MRS; Copestake, Flickinger, Malouf, Riehemann, & Sag, 1995), the generator outputs English realizations in accordance with the HPSG LinGO English Resource Grammar (ERG; Flickinger, 2002). As an example of generator output, a sub-set of alternate realizations that are produced for a single input MRS is shown in Figure 1. For the two data sets considered in this paper, the average number of realizations produced by the generator is 85.7 and 102.2 (the maximum numbers are 4176 and 3408, respectively). Thus, there is immediate demand for a principled way of choosing a single output among the generated candidates. For this task we train and test three dif"
W06-1661,N04-1021,0,0.0357401,"uly 2006. 2006 Association for Computational Linguistics Remember that dogs must be on a leash. Remember dogs must be on a leash. On a leash, remember that dogs must be. On a leash, remember dogs must be. A leash, remember that dogs must be on. A leash, remember dogs must be on. Dogs, remember must be on a leash. 2.2 Maximum Entropy Models Maximum entropy modeling provides a very flexible framework that has been widely used for a range of tasks in NLP, including parse selection (e.g. Johnson, Geman, Canon, Chi, & Riezler, 1999; Malouf & Noord, 2004) and reranking for machine translation (e.g. Och et al., 2004). A model is specified by a set of real-valued feature functions that describe properties of the data, and an associated set of learned weights that determine the contribution of each feature. Let us first introduce some notation before we go on. Let Y (si ) = fr1 ; : : : ; rm g be the set of realizations licensed by the grammar for a semantic representation si . Now, let our (positive) training data be given as Xp = fx1 ; : : : ; xN g where each xi is a pair (si ; rj ) for which rj 2 Y (si ) and rj is annotated in the treebank as being a correct realization of si . Note that we might have sev"
W06-1661,W02-2030,0,0.046085,"Missing"
W06-1661,P99-1069,0,0.129327,"he 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 517–525, c Sydney, July 2006. 2006 Association for Computational Linguistics Remember that dogs must be on a leash. Remember dogs must be on a leash. On a leash, remember that dogs must be. On a leash, remember dogs must be. A leash, remember that dogs must be on. A leash, remember dogs must be on. Dogs, remember must be on a leash. 2.2 Maximum Entropy Models Maximum entropy modeling provides a very flexible framework that has been widely used for a range of tasks in NLP, including parse selection (e.g. Johnson, Geman, Canon, Chi, & Riezler, 1999; Malouf & Noord, 2004) and reranking for machine translation (e.g. Och et al., 2004). A model is specified by a set of real-valued feature functions that describe properties of the data, and an associated set of learned weights that determine the contribution of each feature. Let us first introduce some notation before we go on. Let Y (si ) = fr1 ; : : : ; rm g be the set of realizations licensed by the grammar for a semantic representation si . Now, let our (positive) training data be given as Xp = fx1 ; : : : ; xN g where each xi is a pair (si ; rj ) for which rj 2 Y (si ) and rj is annotat"
W10-3007,J93-2004,0,0.0437243,"E P ). our own tokenization to a GENIA token, we rely on TnT annotation only. In the merging of annotations across components, and also in downstream processing we have found it most convenient to operate predominantly in terms of characterization, i.e. sub-strings of the raw input that need not align perfectly with token boundaries. lems. Our pre-processing approach therefore deploys a home-grown, cascaded finite-state tokenizer (borrowed and adapted from the opensource English Resource Grammar; Flickinger (2000)), which aims to implement the tokenization decisions made in the Penn Treebank (Marcus et al., 1993) – much like GENIA, in principle – but properly treating corner cases like the ones above. Synchronized via characterization, this tokenization is then enriched with the output of no less than two PoS taggers, as detailed in the next section. 2.2 2.3 Dependency Parsing with LFG Features For syntactic parsing we employ a data-driven dependency parser which incorporates the predictions from a large-scale LFG grammar. A technique of parser stacking is employed, which enables a data-driven parser to learn from the output of another parser, in addition to gold standard treebank annotations (Nivre a"
W10-3007,W09-1304,0,0.272407,"Missing"
W10-3007,P08-1108,0,0.00733183,", 1993) – much like GENIA, in principle – but properly treating corner cases like the ones above. Synchronized via characterization, this tokenization is then enriched with the output of no less than two PoS taggers, as detailed in the next section. 2.2 2.3 Dependency Parsing with LFG Features For syntactic parsing we employ a data-driven dependency parser which incorporates the predictions from a large-scale LFG grammar. A technique of parser stacking is employed, which enables a data-driven parser to learn from the output of another parser, in addition to gold standard treebank annotations (Nivre and McDonald, 2008). This technique has been shown to provide significant improvements in accuracy for both English and German (Øvrelid et al., 2009), and a similar approach employing an HPSG grammar has been shown to increase domain independence in data-driven dependency parsing (Zhang and Wang, 2009). For our purposes, we decide to use a parser which incorporates analyses from two quite different parsing approaches – data-driven dependency parsing and “deep” parsing with a handcrafted grammar – providing us with a range of different types of linguistic features which may be used in hedge detection. We employ t"
W10-3007,P81-1022,0,0.0609688,"Missing"
W10-3007,W03-3017,0,0.0121603,"ice, as well as more semantic properties detailing, e.g., subcategorization frames, semantic conceptual categories such as human, time and location, etc., resides in the F E A T S column. The parser outputs, which in turn form the basis for our scope resolution rules discussed in Section 4, also take this same form. The parser employed in this work is trained on the Wall Street Journal sections 2 – 24 of the Penn Treebank, converted to dependency format (Johansson and Nugues, 2007) and extended with XLE features, as described above. Parsing is performed using the arc-eager mode of MaltParser (Nivre, 2003) and an SVM with a polynomial kernel. When tested using 10-fold cross-validation on this data set, the parser achieves a labeled accuracy 3 Identifying Hedge Cues For the task of identifying hedge cues, we developed a binary maximum entropy (MaxEnt) classifier. The identification of cue words is used for (i) classifying sentences as certain/uncertain (Task 1), and (ii) providing input to the syntactic rules that we later apply for resolving the in-sentence scope of the cues (Task 2). We also report evaluation scores for the sub-task of cue detection in isolation. As annotated in the training d"
W10-3007,A00-1031,0,0.0383377,"ich may be used in hedge detection. We employ the freely available MaltParser (Nivre et al., 2006), which is a languageindependent system for data-driven dependency parsing.2 It is based on a deterministic parsing strategy in combination with treebank-induced classifiers for predicting parse transitions. It supports a rich feature representation of the parse history in order to guide parsing and may easily be extended to take into account new features of the PoS Tagging and Lemmatization For PoS tagging and lemmatization, we combine (with its built-in, occasionally deviant tokenizer) and TnT (Brants, 2000), which operates on pre-tokenized inputs but in its default model is trained on financial news from the Penn Treebank. Our general goal here is to take advantage of the higher PoS accuracy provided by GENIA in the biomedical domain, while using our improved tokenization and producing inputs to the parsing stage (see Section 2.3 below) that as much as possible resemble the conventions used in the original training data for the parser – the Penn Treebank, once again. To this effect, for the vast majority of tokens we can align the GENIA tokenization with our own, and in these cases we typically"
W10-3007,W02-1503,0,0.121758,"icles Total 11871 2670 14541 Hedged Cues Sentences 2101 519 2620 2659 668 3327 Multi-Word Cues Tokens Cue Tokens 364 84 448 309634 68579 378213 3056 782 3838 Table 2: Some descriptive figures for the shared task training data. Token-level counts are based on the tokenization described in Section 2.1. parse history. score of 89.8 (Øvrelid et al., 2010). Parser stacking The procedure to enable the data-driven parser to learn from the grammardriven parser is quite simple. We parse a treebank with the XLE platform (Crouch et al., 2008) and the English grammar developed within the ParGram project (Butt et al., 2002). We then convert the LFG output to dependency structures, so that we have two parallel versions of the treebank – one gold standard and one with LFG-annotation. We extend the gold standard treebank with additional information from the corresponding LFG analysis and train the data-driven dependency parser on the enhanced data set. See Øvrelid et al. (2010) for details of the conversion and training of the parser. Table 1 shows the enhanced dependency representation of the English sentence The unknown amino acid may be used by these species, taken from the training data. For each token, the par"
W10-3007,P09-2010,1,0.868874,"his tokenization is then enriched with the output of no less than two PoS taggers, as detailed in the next section. 2.2 2.3 Dependency Parsing with LFG Features For syntactic parsing we employ a data-driven dependency parser which incorporates the predictions from a large-scale LFG grammar. A technique of parser stacking is employed, which enables a data-driven parser to learn from the output of another parser, in addition to gold standard treebank annotations (Nivre and McDonald, 2008). This technique has been shown to provide significant improvements in accuracy for both English and German (Øvrelid et al., 2009), and a similar approach employing an HPSG grammar has been shown to increase domain independence in data-driven dependency parsing (Zhang and Wang, 2009). For our purposes, we decide to use a parser which incorporates analyses from two quite different parsing approaches – data-driven dependency parsing and “deep” parsing with a handcrafted grammar – providing us with a range of different types of linguistic features which may be used in hedge detection. We employ the freely available MaltParser (Nivre et al., 2006), which is a languageindependent system for data-driven dependency parsing.2 It"
W10-3007,W10-3001,0,0.124802,"its tokens like ‘3,926.50’, ‘methlycobamide:CoM’, or ‘Ca(2+)’. Conversely, GENIA fails to isolate some kinds of opening single quotes, because the quoting conventions assumed in BioScope differ from those used in the GENIA Corpus; furthermore, it mis-tokenizes LATEX-style n- and m-dashes. On average, one in five sentences in the CoNLL training data exhibited GENIA tokenization probThe CoNLL-2010 shared task1 comprises two sub-tasks. Task 1 is described as learning to detect sentences containing uncertainty, while the object of Task 2 is learning to resolve the in-sentence scope of hedge cues (Farkas et al., 2010). Paralleling this two-fold task definition, the architecture of our system naturally decomposes into two main steps. First, a maximum entropy (MaxEnt) classifier is applied to automatically detect cue words. For Task 1, a given sentence is labeled as uncertain if it contains a word classified as a cue. For Task 2, we then go on to determine the scope of the identified cues using a set of manually crafted rules operating on dependency representations. For both Task 1 and Task 2, our system participates in the stricter category of ‘closed’ or ‘indomain’ systems. This means that we do not use an"
W10-3007,W08-0606,0,0.562357,"oken- and sentence-level F-scores, the effect of incrementally including a larger percentage of training data into the 10-fold cycles. (As described also for the other development results, while we are training on both the articles and the abstracts, we are testing only on the articles.) Model Development, Data Sets and Evaluation Measures hedge cues (possibly spanning multiple tokens). These latter scores are computed using the official shared task scorer script. While the training data made available for the shared task consisted of both abstracts and full articles from the BioScope corpus (Vincze et al., 2008), the test data were pre-announced to consist of biomedical articles only. In order to make the testing situation during development as similar as possible to what could be expected for the held-out testing, we only tested on sentences taken from the articles part of the training data. When developing the classifiers we performed 10-fold training and testing over the articles, while always including all sentences from the abstracts in the training set as well. Table 2 provides some basic descriptive figures summarizing the training data. As can be seen in Table 3, we will be reporting precisio"
W10-3007,W02-2018,0,0.0202726,"e task of determining whether a cue word forms part of a larger multi-word cue, is performed by a separate post-processing step, further described in Section 3.2. 3.1 Maximum Entropy Classification In the MaxEnt framework, each training example – in our case a paired word and label hwi , yi i – is represented as a feature vector f (wi , yi ) = fi ∈ <d . Each dimension or feature function fij can encode arbitrary properties of the data. The particular feature functions we are using for the cue identification are described under Section 3.4 below. For model estimation we use the TADM3 software (Malouf, 2002). For feature extraction and model tuning, we build on the experimentation environment developed by Velldal (2008) (in turn extending earlier work by Oepen et al. 3 Toolkit for Advanced Discriminative Modeling; available from http://tadm.sourceforge.net/. 50 90 (2004)). Among other things, its highly optimized feature handling – where the potentially expensive feature extraction step is performed only once and then combined with several levels of feature caching – make it computationally feasible to perform large-scale ‘grid searches’ over different configurations of features and model paramet"
W10-3007,P09-1043,0,0.0954061,"atures For syntactic parsing we employ a data-driven dependency parser which incorporates the predictions from a large-scale LFG grammar. A technique of parser stacking is employed, which enables a data-driven parser to learn from the output of another parser, in addition to gold standard treebank annotations (Nivre and McDonald, 2008). This technique has been shown to provide significant improvements in accuracy for both English and German (Øvrelid et al., 2009), and a similar approach employing an HPSG grammar has been shown to increase domain independence in data-driven dependency parsing (Zhang and Wang, 2009). For our purposes, we decide to use a parser which incorporates analyses from two quite different parsing approaches – data-driven dependency parsing and “deep” parsing with a handcrafted grammar – providing us with a range of different types of linguistic features which may be used in hedge detection. We employ the freely available MaltParser (Nivre et al., 2006), which is a languageindependent system for data-driven dependency parsing.2 It is based on a deterministic parsing strategy in combination with treebank-induced classifiers for predicting parse transitions. It supports a rich featur"
W10-3007,nivre-etal-2006-maltparser,0,\N,Missing
W10-3007,W07-2416,0,\N,Missing
W11-4631,C10-1011,0,0.0368826,"out in relation to random indexing above. The final example of related work that we will be discussing is the feature hashing approach of Ganchev and Dredze (2008). Targeting NLP applications on resource constrained devices, Ganchev and Dredze (2008) suggest eliminating the symbol-table (also known as the alphabet, dictionary, etc.), replacing it instead with a hash function. Tests on a range of tasks (sentiment analysis, spam detection, topic labeling, etc.) shows “tolerable” degradation of performance relative to savings in storage (Ganchev and Dredze, 2008). A similar approach was taken by Bohnet (2010), who uses feature hashing for speeding up the feature handling in a data-driven dependency parser. It is important to note that, rather than being primarily aimed at dimensionality reduction, the approach of Ganchev and Dredze (2008) aims to save resources by discarding the symbol-table. In fact, in order to reduce the chances of collisions, the assumed dimensionality is instead sometimes greatly increased using this approach, as in the dependency parsing experiments of Bohnet (2010). An interesting direction for future work would be to combine HRI with the feature hashing approach of Ganchev"
W11-4631,W08-0804,0,0.0325609,"(2010) for online generation of bit signatures for data streams. Taking advantage of the fact that the operations in the dot products between the data vectors and the random “hash vectors” are linear, Van Durme and Lall (2010) replace the dot products with individual additions corresponding to the random values associated with each feature as it is encountered in the stream, thereby taking a step towards the incrementality that we have several times pointed out in relation to random indexing above. The final example of related work that we will be discussing is the feature hashing approach of Ganchev and Dredze (2008). Targeting NLP applications on resource constrained devices, Ganchev and Dredze (2008) suggest eliminating the symbol-table (also known as the alphabet, dictionary, etc.), replacing it instead with a hash function. Tests on a range of tasks (sentiment analysis, spam detection, topic labeling, etc.) shows “tolerable” degradation of performance relative to savings in storage (Ganchev and Dredze, 2008). A similar approach was taken by Bohnet (2010), who uses feature hashing for speeding up the feature handling in a data-driven dependency parser. It is important to note that, rather than being pr"
W11-4631,W10-2808,0,0.0379122,"Missing"
W11-4631,P05-1077,0,0.249487,"equivalent to the result of adding ternary index vectors in the random indexing approach of Kanerva et al. (2000). Moreover, when comparing hash kernels to the general approach of random projections, Weinberger et al. (2009) note that one advantage of the former is that there is no need for storing the random matrices. With hashed RI, however, 227 Erik Velldal we need not store neither the random projection matrix R nor the original feature matrix F . Another line of work bearing resemblance to (H)RI is the use of Locality Sensitive Hashing (LSH) for identifying semantically similar words by Ravichandran et al. (2005). LSH is a method for fast but approximate nearest neighbor search based on compact bit signatures created for each data point or vector. These signatures are created by applying multiple binary hash functions to each point in a way so that close items are hashed to the same buckets with a high probability. The cosine similarity of the original word vectors is then approximated by the hamming distance of their bit signatures. In the work of Ravichandran et al. (2005), the value of each hash function ({0,1}) is defined by the sign of the dot product between each word vector and a random vector."
W11-4631,D07-1049,0,0.031402,"ments of the vectors. This means that certain common operations such as dot-products, euclidean distance, etc., might take longer to compute in the reduced (but more dense) space compared to the non-reduced (but more sparse) space. 5 #Hashing—A Trending Topic There has recently been a series of papers in the NLP and ML literature on the use of hashing for constructing faster and more compact models. Several authors have explored the use of hashbased randomized data structures for storing approximate frequency counts for large data sets, such as the generalized notion of a Bloom filter used by Talbot and Osborne (2007) and Durme and Lall (2009) in the context of language modeling, or the Count-Min sketch used by Goyal et al. (2010) for computing web-scale distributional similarities. More closely related to the work presented in the current paper perhaps, is the notion of hash kernels introduced by Shi et al. (2009) and Weinberger et al. (2009) in the context of SVM-based spam filtering and topic categorization. With hash kernels, high-dimensional input vectors are compressed using a single hash function that maps the original features into a smaller range of indices, and dot-products are then computed betw"
W11-4631,P10-2043,0,0.297169,"Missing"
W11-4631,widdows-ferraro-2008-semantic,0,\N,Missing
W12-3804,P05-1022,0,0.067393,"Missing"
W12-3804,W09-3012,0,0.0184408,"ttribution explicit. It is developed on the basis of the FactBank corpus (Saur´ı and Pustejovsky, 2009), containing manual annotations of pairs of events and sources along the dimensions of polarity (positive, negative, or underspecified) and certainty (certain, probable, possible, or underspecified. Prabhakaran et al. (2010) report experiments with belief tagging, which in many ways is similar to factuality detection. Their starting point is a corpus of 10.000 words comprising a variety of genres (newswire text, emails, instructions, etc.) annotated for speaker belief of stated propositions (Diab et al., 2009): Propositional heads are tagged as committed belief (CB), non-committed belief (NCB), or not applicable (NA), meaning no belief is expressed by the speaker. To some degree, CB and NCB can be seen as similar to our categories of factuality and nonfactuality, respectively. Applying a one-versus-all SVM classifier by 4-fold cross validation, and using wide range of both lexical and syntactical features, Prabhakaran et al. (2010) report F1 -scores of 69.6 for CB, 34.1 for NCB, and 64.5 for NA. 3 Data Sets and the Notion of Factuality The data we will be using in the current study is taken from a"
W12-3804,W10-3001,0,0.172943,"Missing"
W12-3804,W11-1827,0,0.019848,"es along with their scopes. The BioNLP shared tasks of 2009 and 2011 mainly concerned recognizing bio-molecular events in text, but optional subtasks involved detecting whether these events were affected by speculation or negation. The data set used for this task is the Genia event corpus (Kim et al., 2008) which annotates the uncertainty of events according to the three labels certain, probable and doubtful (but without explicitly annotating cue words or scope as in BioScope). The best performer in the BioNLP 2011 supporting task of detecting speculation modification of events, the system of Kilicoglu and Bergler (2011), achieved an end-to-end F1 of 27.25 using a manually compiled dictionary of trigger expressions together with a set of rules operating on syntactic dependencies for identifying events and event modification. Turning to the task of identifying speculation cues in the BioScope data, current state-of-theart systems, implementing simple supervised classification approaches on the token- or sequence-level, achieves F1 -scores of well above 80 (Tang et al., 2010; Velldal et al., 2012). For the task of resolving the scopes of these cues, the current best systems obtain end-to-end F1 -scores close to"
W12-3804,S12-1035,0,0.251101,"minating between factual and non-factual contexts, trained on weakly labeled data by taking advantage of information implicit in annotations of negated events. In addition to evaluating factuality detection in isolation, we also evaluate its impact on a system for event detection. The two components for factuality detection and event detection form part of a system for identifying negative factual events, or counterfacts, with top-ranked results in the *SEM 2012 shared task. 1 Introduction The First Joint Conference on Lexical and Computational Semantics (*SEM 2012) is hosting a shared task1 (Morante and Blanco, 2012) on identifying various elements of negation, and one of the subtasks is to identify negated events. However, only events occurring in factual statements should be labeled. This paper describes pilot experiments on how to train a factuality classifier by taking advantage of implicit information on factuality in annotations of negation. In addition to evaluating factuality detection in isolation, we also assess its impact when embedded in a system for event detection. The system was ranked first for the *SEM 2012 subtask of identifying negated events, and also formed part of the top-ranked syst"
W12-3804,morante-daelemans-2012-conandoyle,0,0.0487329,"ef (NCB), or not applicable (NA), meaning no belief is expressed by the speaker. To some degree, CB and NCB can be seen as similar to our categories of factuality and nonfactuality, respectively. Applying a one-versus-all SVM classifier by 4-fold cross validation, and using wide range of both lexical and syntactical features, Prabhakaran et al. (2010) report F1 -scores of 69.6 for CB, 34.1 for NCB, and 64.5 for NA. 3 Data Sets and the Notion of Factuality The data we will be using in the current study is taken from a recently released corpus of Conan Doyle (CD) stories annotated for negation (Morante and Daelemans, 2012). The data is annotated with negation cues, the in-sentence scope of those cues, as well as the negated event, if any. The cue is the word(s) or affix indicating a negation, The scope then indicates the maximal extent of that negation, while the event indicates the most basic negated element. In the annotation guidelines, Morante et al. (2011, p. 4) use the term event in a rather general sense; “[i]t can be a process, an action, or a state.” The guidelines occasionally also refer to the notion of negated elements as encompassing “the main event or property actually negated by the negation cue”"
W12-3804,J12-2001,0,0.0137631,"dification. Turning to the task of identifying speculation cues in the BioScope data, current state-of-theart systems, implementing simple supervised classification approaches on the token- or sequence-level, achieves F1 -scores of well above 80 (Tang et al., 2010; Velldal et al., 2012). For the task of resolving the scopes of these cues, the current best systems obtain end-to-end F1 -scores close to 60 in held-out testing (Morante et al., 2010; Velldal et al., 2012). Note that the latter reference is from a forthcoming issue of Computational Linguistics specifically on modality and negation (Morante and Sporleder, 2012). In that same issue, Saur´ı and Pustejovsky (2012) present a linguistically motivated system for factuality profiling with manually crafted rules operating on dependency graphs. Conceptually treat29 ing factuality as a perspective that a particular source (speaker) holds toward an event, the system aims to make this attribution explicit. It is developed on the basis of the FactBank corpus (Saur´ı and Pustejovsky, 2009), containing manual annotations of pairs of events and sources along the dimensions of polarity (positive, negative, or underspecified) and certainty (certain, probable, possibl"
W12-3804,W10-3006,0,0.033642,"Missing"
W12-3804,C10-2117,0,0.0452343,"Missing"
W12-3804,S12-1041,1,0.681851,"s of negation, and one of the subtasks is to identify negated events. However, only events occurring in factual statements should be labeled. This paper describes pilot experiments on how to train a factuality classifier by taking advantage of implicit information on factuality in annotations of negation. In addition to evaluating factuality detection in isolation, we also assess its impact when embedded in a system for event detection. The system was ranked first for the *SEM 2012 subtask of identifying negated events, and also formed part of the top-ranked system in the shared task overall (Read et al., 2012). The experiments presented in this paper further improves on these initial results. 1 The web site of the 2012 *SEM Shared Task: http://www.clips.ua.ac.be/sem2012-st-neg/ Note that the system was designed for submission to the closed track of the shared task, which means development is constrained to using the data provided by the task organizers. The rest of the paper is structured as follows. We start in Section 2 by giving a brief overview of related work and resources. In Section 3 we then present the problem statement in more detail, along with the relevant data sets. This section also d"
W12-3804,J12-2002,0,0.138977,"Missing"
W12-3804,W10-3002,0,0.0172685,"BioScope). The best performer in the BioNLP 2011 supporting task of detecting speculation modification of events, the system of Kilicoglu and Bergler (2011), achieved an end-to-end F1 of 27.25 using a manually compiled dictionary of trigger expressions together with a set of rules operating on syntactic dependencies for identifying events and event modification. Turning to the task of identifying speculation cues in the BioScope data, current state-of-theart systems, implementing simple supervised classification approaches on the token- or sequence-level, achieves F1 -scores of well above 80 (Tang et al., 2010; Velldal et al., 2012). For the task of resolving the scopes of these cues, the current best systems obtain end-to-end F1 -scores close to 60 in held-out testing (Morante et al., 2010; Velldal et al., 2012). Note that the latter reference is from a forthcoming issue of Computational Linguistics specifically on modality and negation (Morante and Sporleder, 2012). In that same issue, Saur´ı and Pustejovsky (2012) present a linguistically motivated system for factuality profiling with manually crafted rules operating on dependency graphs. Conceptually treat29 ing factuality as a perspective that"
W12-3804,J12-2005,1,0.828185,"performer in the BioNLP 2011 supporting task of detecting speculation modification of events, the system of Kilicoglu and Bergler (2011), achieved an end-to-end F1 of 27.25 using a manually compiled dictionary of trigger expressions together with a set of rules operating on syntactic dependencies for identifying events and event modification. Turning to the task of identifying speculation cues in the BioScope data, current state-of-theart systems, implementing simple supervised classification approaches on the token- or sequence-level, achieves F1 -scores of well above 80 (Tang et al., 2010; Velldal et al., 2012). For the task of resolving the scopes of these cues, the current best systems obtain end-to-end F1 -scores close to 60 in held-out testing (Morante et al., 2010; Velldal et al., 2012). Note that the latter reference is from a forthcoming issue of Computational Linguistics specifically on modality and negation (Morante and Sporleder, 2012). In that same issue, Saur´ı and Pustejovsky (2012) present a linguistically motivated system for factuality profiling with manually crafted rules operating on dependency graphs. Conceptually treat29 ing factuality as a perspective that a particular source (s"
W12-3804,W08-0606,0,0.0886178,"Missing"
W13-5642,heid-etal-2010-corpus,0,0.0279013,"e, users are presented with the Galaxy workspace, as shown in Figure 1. The left panel displays the installed processing tools; clicking on tool-names displays general information and configuration options in the center panel. In order to be integrated in the LAP tool-chain, existing tools are ‘wrapped’ inside scripts that decode the LAP-internal format, present the tool itself with its expected input and finally re-encode the output so that it is compatible with the next processing step. For LAP’s system-internal representation, we are at the moment looking into both TCF (Text Corpus Format, Heid et al., 2010) (the format used within WebLicht, which comes with a full, albeit closed-source API) and our own in-house JSON-based LTON format (Language Technology Object Notation) which is still currently under development. Additionally, the wrapper handles the submission of the job to the Abel cluster, a shared resource for research computing boasting more than 600 machines, totaling more than 10.000 cores (CPUs).4 The HPC connection is a very important feature, as Language technology can be computationally quite expensive, often involving sub-problems where known best solutions 3 For more information ab"
W15-1816,J92-4003,0,0.669024,". (2008), the lexicalized statistics important to disambiguation in parsing are often sparse, and modeling relationships on a more general level than the words themselves may therefore be helpful. The other motivation is domain adaptation, attempting to leverage a parsing model for use on data from a new domain. By including information about word clusters estimated from unlabeled in-domain data, one can hope to reduce the loss in performance expected from using a parser trained on an out-of-domain treebank. While previous approaches have typically relied on the n-gram–based Brown clustering (Brown et al., 1992), this paper instead describes experiments using dependency-based word clusters formed using the generic clustering algorithm Kmeans. After applying a baseline dependency parser to unlabeled text, K-means is applied to form word clusters with features based on the dependency structures produced by the parser. The parser is then re-trained using features that record information about the dependency-derived clusters, thereby introducing an element of selftraining. The re-trained parser obtains improved parsing accuracy on a range of different data sets, including the five web domains of the Engl"
W15-1816,W10-1409,0,0.0409033,"Missing"
W15-1816,D07-1101,0,0.0779514,"Missing"
W15-1816,gimenez-marquez-2004-svmtool,0,0.0196534,"Missing"
W15-1816,P08-1068,0,0.498658,"rained using information about the clusters, yielding improved parsing accuracy on a range of different data sets, including WSJ and the English Web Treebank. We report improved results using both in-domain and out-of-domain data, and also include a comparison with using n-gram–based Brown clustering. 1 Introduction Several recent studies have attempted to improve dependency parsers by including information about word clusters into their statistical parsing models. This is typically motivated by at least two concerns, both of which relate to the shortage of labeled training data. As argued by Koo et al. (2008), the lexicalized statistics important to disambiguation in parsing are often sparse, and modeling relationships on a more general level than the words themselves may therefore be helpful. The other motivation is domain adaptation, attempting to leverage a parsing model for use on data from a new domain. By including information about word clusters estimated from unlabeled in-domain data, one can hope to reduce the loss in performance expected from using a parser trained on an out-of-domain treebank. While previous approaches have typically relied on the n-gram–based Brown clustering (Brown et"
W15-1816,J93-2004,0,0.0515779,"ng algorithm Kmeans. After applying a baseline dependency parser to unlabeled text, K-means is applied to form word clusters with features based on the dependency structures produced by the parser. The parser is then re-trained using features that record information about the dependency-derived clusters, thereby introducing an element of selftraining. The re-trained parser obtains improved parsing accuracy on a range of different data sets, including the five web domains of the English Web Treebank (EWT) (Bies et al., 2012) and the Wall Street Journal (WSJ) portion of the Penn Treebank (PTB) (Marcus et al., 1993). We document improvements using both in-domain and outof-domain data, and also when compared to using Brown clusters. All our parsing experiments use MaltParser (Nivre et al., 2007), a data-driven transition-based dependency parser. The rest of the paper is structured as follows. Section 2 provides an overview of previous work. Section 3 details the data sets we use, including comments on the pre-processing. Section 4 then describes the experimental set-up, while the actual experiments and results are described in Section 5. A summary with thoughts about future directions is provided in Secti"
W15-1816,P05-1012,0,0.230474,"Missing"
W15-1816,P09-1040,0,0.0178482,"models for the baseline and the re-trained parser, where p = PoS-tag, w = word form, d = dependency label in the graph constructed so far (if any), and l = cluster label. MaltParser’s stacklazy algorithm operates over three data structures: a stack (S) of partially processed tokens, a list (I) of nodes that have been on the stack, and a “lookahead” list (L) of nodes that have not been on the stack. We refer to the top of the stack using S0 and subsequent nodes using S1 , S2 , etc., and the leftmost/rightmost dependent of S0 with S0l /S0r . Skjærholt (2012). It employs the stacklazy algorithm (Nivre, 2009), along with the LIBLINEAR package (Fan et al., 2008) for inducing parse transition SVM classifiers. 4.2.1 Parser features Table 3 describes the baseline feature set, along with three additional feature sets based on the models described in Øvrelid and Skjærholt (2012) and that in various ways include information about cluster labels: PoS simple, Form simple and Form all. These augmented feature sets are constructed by copying the full baseline feature set (all) or only the features that pertain to a single token (simple) and involve either the PoS-tag or the word form respectively. (Note that"
W15-1816,C12-2088,1,0.71791,"e created from the L’Est R´epublicain corpus (using up to 1,000 clusters), comprising 125 million words of news text, and cluster-based features are then added to the Berkeley PCFG parser with latent annotations (Petrov et al., 2006), before parsing the French Treebank (Abeill´e et al., 2003). Candito and Seddah (2010) analyze the results with respect to word frequency and find improvements in performance for all strata; unseen or rare words, as well as medium- to highfrequency words. Adding PoS-information to the lemmas also appeared beneficial, though depending on the quality of the tagger. Øvrelid and Skjærholt (2012) apply Brown clusters to improve dependency parsing of English web data using MaltParser. Augmenting a WSJ-trained parser with Brown clusters – using the cluster labels of Turian et al. (2010) computed for the Reuters corpus – is shown to improve parsing accuracy on a range of web texts, including the Twitter and user forum data from the web 2.0 data sets described by Foster et al. (2011) and web data from various sources in the OntoNotes corpus, release 4 (Weischedel et al., 2011). In the experiments of Øvrelid and Skjærholt (2012), cluster information was found to be more beneficial for pars"
W15-1816,P06-1055,0,0.0170831,"Missing"
W15-1816,W09-3829,0,0.0850245,"Experimenting with different tree cut-offs, producing different numbers of clusters, Øvrelid and Skjærholt (2012) found that using a smaller number of large and general clusters (100–320) worked better than using a higher number of smaller and more finegrained clusters (experimenting with up to 3200 clusters). As an alternative to the above approaches using n-gram-based Brown clusters, the current paper documents experiments with using syntactically informed clusters instead, generated with a generic clustering algorithm. One previous study following a related line of investigation is that of Sagae and Gordon (2009) who also used parsed data for creating syntactically informed clusters. The clustering is there performed by applying the general method of (average-link) hierarchical agglomerative clustering to the 5,000 most frequent words of Proceedings of the 20th Nordic Conference of Computational Linguistics (NODALIDA 2015) 118 the BLLIP WSJ corpus, containing approximately 30 million words of WSJ news articles, parsed with the Charniak (2000) parser. The features used for the clustering encode phrase-structure tree paths that include direction information and non-terminal node labels, but does not inc"
W15-1816,C08-1095,0,0.0255675,"us, containing approximately 30 million words of WSJ news articles, parsed with the Charniak (2000) parser. The features used for the clustering encode phrase-structure tree paths that include direction information and non-terminal node labels, but does not include lexical information or part-of-speech tags. The clusters are then added as features in a data-driven transition-based dependency parser which is again used to identify predicate-argument dependencies extracted from the HPSG Treebank developed by Miyao et al. (2004) comprising the standard PTB WSJ sections. The pipeline described by Sagae and Tsujii (2008) thus include several layers of cross-framework interactions. Cutting the cluster hierarchy to include 600 clusters was shown to given the highest F-score, significantly improving the accuracy of the predicate-argument dependency parser. The goal of Sagae and Gordon (2009) is to improve the accuracy of a fast dependency parser by using a corpus which has previously been automatically annotated using a more accurate but slower phrase-structure parser. In our experiments we seek to improve a baseline dependency parser by using clusters formed directly on the basis of the annotations of the basel"
W15-1816,A00-2018,0,\N,Missing
W15-1816,P10-1040,0,\N,Missing
W16-0413,J92-4003,0,0.35061,"s in compliance with the root acomp nsubj advmod amod i m PRON VERB Features Based on the enriched corpus, as described above, we experiment with the following sources of information for defining our features: conj dobj fucking scared VERB ADJ kill VERB them d PRON X Figure 2: Dependency parse of example sentence from the corpus, with assigned uPOS tags. ClearNLP converter (Choi and Palmer, 2012), see Figure 2 for an example dependency graph from the corpus. The corpus was further enriched with the cluster labels described in Turian et al. (2010), created using the Brown clustering algorithm (Brown et al., 1992) and induced from the Reuters corpus of English newswire text (RCV1). We vary the number of clusters to be either 100, 320, 1000 or 3200 clusters and use the full cluster label. We also make use of the WordNet resource (Fellbaum, 1998) to include information about the synset of a word, as well as its parent and grandparent synsets. Classifiers We test three different classification frameworks in our development testing: Maximum Entropy (MaxEnt), Support Vector Machines (SVM), and Random Forests (RF). We approach the task as a binary classification task, using the implementations found in the s"
W16-0413,P09-2079,0,0.0319516,"rsal POS (uPOS) – Dependency Relation • Semantic: – Brown cluster label – WordNet synset, + parent and grandparent The features are structured according to a set of feature templates, which record varying degrees of linear order and syntactic context: bag-of features (unordered), bigrams, trigrams and dependency triples. Examples of the latter, given the sentence in Figure 2, would be: {&lt;m, nsubj, i&gt;, &lt;root, root, m&gt;, &lt;scared, amod, fucking&gt;, . . . } Our lexicalized features are very specific and require the exact combination of two lexical items in order to apply to a new instance. Following Joshi and Penstein-Rose (2009), we therefore experiment with generalizing features by ‘backing off’ to a more general category, e.g., from word form to lemma or POS. For example, a dependency triple over word forms like &lt;kill, dobj, them&gt; would thus be generalized to &lt;VERB, dobj, them&gt; using head-backoff, and &lt;kill, dobj, PRON&gt; using modifier-backoff. These additional backoff features are included for bigrams and trigrams as well as dependency triples. We impose a simple count-based reduction of the feature set; only features appearing at least twice in the training data are included in the model. 4.2 Development results W"
W16-0413,petrov-etal-2012-universal,0,0.0756479,"Missing"
W16-0413,P10-1040,0,0.0466524,"et of Petrov et al. (2012). The dependency parser assigns an analysis in compliance with the root acomp nsubj advmod amod i m PRON VERB Features Based on the enriched corpus, as described above, we experiment with the following sources of information for defining our features: conj dobj fucking scared VERB ADJ kill VERB them d PRON X Figure 2: Dependency parse of example sentence from the corpus, with assigned uPOS tags. ClearNLP converter (Choi and Palmer, 2012), see Figure 2 for an example dependency graph from the corpus. The corpus was further enriched with the cluster labels described in Turian et al. (2010), created using the Brown clustering algorithm (Brown et al., 1992) and induced from the Reuters corpus of English newswire text (RCV1). We vary the number of clusters to be either 100, 320, 1000 or 3200 clusters and use the full cluster label. We also make use of the WordNet resource (Fellbaum, 1998) to include information about the synset of a word, as well as its parent and grandparent synsets. Classifiers We test three different classification frameworks in our development testing: Maximum Entropy (MaxEnt), Support Vector Machines (SVM), and Random Forests (RF). We approach the task as a b"
W16-0413,W12-2103,0,0.0711624,"e detection of threats in a data set of Dutch tweets (Oostdijk and van Halteren, 2013a; Oostdijk and van Halteren, 2013b), which consists of a collection of 5000 threatening tweets. In addition, a large number of random tweets were collected for development and testing. The system relies on manually constructed recognition patterns in the form of n-grams, but details about the strategy used to construct these patterns are not given. In Oostdijk and van Halteren (2013b), a manually crafted shallow parser is added to the system. This improves results to a precision of 0.39 and a recall of 0.59. Warner and Hirschberg (2012) present a method for detecting hate speech in user-generated web text, which relies on machine learning in combination with template-based features. The task is approached as a word-sense disambiguation task, since the same words can be used in both hateful and nonhateful contexts. The features used in the classification were combinations of uni-, bi- and trigrams, part-of-speech-tags and Brown clusters. The best results were obtained using only unigram features, Proceedings of NAACL-HLT 2016, pages 66–71, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguisti"
W16-0413,J93-2004,0,\N,Missing
W17-0201,Q16-1022,0,0.0360415,"Missing"
W17-0201,W17-0217,1,0.732885,"t-diff script of the TnT-distribution, and scores are computed over the base PoS tags, disregarding morphological features. Mate is evaluated using the MaltEval tool (Nilsson and Nivre, 2008). For the second pipeline, we rely on UDPipe’s built-in evaluation support, which also implements MaltEval. 5.1 Test Table 1: Results on the UD development data for tagging and parsing the two written standards for Norwegian, Bokmål (BM) and Nynorsk (NN), including ‘cross-standard’ training and testing. Data split For Bokmål we use the same split for training, development and testing as defined for NDT by Hohle et al. (2017). As no pre-defined split was established for Nynorsk we defined this ourselves, following the same 80-10-10 proportions and also taking care to preserve contiguous texts in the various sections while also keeping them balanced in terms of genre. 5 Train UDPipe to adapt to errors made by the tagger. While this is often achieved using jackknifing (n-fold training and tagging of the labeled training data), we here simply apply the taggers to the very same data they have been trained on, reflecting the ‘training error’ of the taggers. We have found that training on such ‘silver-standard’ tags imp"
W17-0201,Q16-1031,0,0.0272639,"ank. For the purpose of the current work, the Nynorsk section has also been automatically converted to Universal Dependencies, making use of the conversion software described in Øvrelid and Hohle (2016) with minor modifications.2 The release of universal representations for PoS tags (Petrov et al., 2012) and dependency syntax (Nivre et al., 2016) has enabled research in crosslingual parsing that does not require a languagespecific conversion procedure. Tiedemann et al. (2014) utilize statistical MT for treebank translation in order to train cross-lingual parsers for a range of language pairs. Ammar et al. (2016) employ a combination of cross-lingual word clusters and embeddings, language-specific features and typological information in a neural network architecture where one and the same parser is used to parse many languages. UD conversion of NDT Nynorsk Figure 1 provides the UD graph for our Nynorsk example sentence. The NDT and UD schemes differ in terms of both PoS tagset and morphological features, as well as structural analyses. The conversion therefore requires non-trivial transformations of the dependency trees, in addition to mappings of tags and labels that make reference to a combination I"
W17-0201,A00-1031,0,0.0834174,"tion In this work the focus is on cross-standard, rather than cross-lingual, parsing. The two standards of Norwegian can be viewed as two highly related languages, which share quite a few lexical items, hence we assume that parser lexicalization will be beneficial. Like Tiedemann et al. (2014), we experiment with machine translation of train2 The data used for these experiments follows the UD v1.4 guidelines, but its first release as a UD treebank will be in v2.0. For replicability we therefore make our data available from the companion Git repository. 2 TnT & Mate The widely used TnT tagger (Brants, 2000), implementing a 2nd order Markov model, achieves high accuracy as well as very high speed. TnT was used by Petrov et al. (2012) when evaluating the proposed universal tag set. Solberg et al. (2014) found the Mate dependency parser (Bohnet, 2010) to have the best performance for parsing of NDT, and recent dependency parser comparisons (Choi et al., 2015) have also found Mate to perform very well for English. The fast training time of Mate also facilitates rapid experimentation. Mate implements the secondorder maximum spanning tree dependency parsing algorithm of Carreras (2007) with the passiv"
W17-0201,nilsson-nivre-2008-malteval,0,0.0236766,"one can not assume (as is sometimes done, often by necessity due to unavailable resources) that tools created for, say, Bokmål can be applied to Nynorsk without a substantial increase in errors. Evaluation The taggers are evaluated in terms of tagging accuracy (Acc in the following tables) while the parsers are evaluated by labeled and unlabeled attachment score (LAS and UAS). For the TnT tagger, accuracy is computed with the tnt-diff script of the TnT-distribution, and scores are computed over the base PoS tags, disregarding morphological features. Mate is evaluated using the MaltEval tool (Nilsson and Nivre, 2008). For the second pipeline, we rely on UDPipe’s built-in evaluation support, which also implements MaltEval. 5.1 Test Table 1: Results on the UD development data for tagging and parsing the two written standards for Norwegian, Bokmål (BM) and Nynorsk (NN), including ‘cross-standard’ training and testing. Data split For Bokmål we use the same split for training, development and testing as defined for NDT by Hohle et al. (2017). As no pre-defined split was established for Nynorsk we defined this ourselves, following the same 80-10-10 proportions and also taking care to preserve contiguous texts i"
W17-0201,D07-1101,0,0.0309202,"used TnT tagger (Brants, 2000), implementing a 2nd order Markov model, achieves high accuracy as well as very high speed. TnT was used by Petrov et al. (2012) when evaluating the proposed universal tag set. Solberg et al. (2014) found the Mate dependency parser (Bohnet, 2010) to have the best performance for parsing of NDT, and recent dependency parser comparisons (Choi et al., 2015) have also found Mate to perform very well for English. The fast training time of Mate also facilitates rapid experimentation. Mate implements the secondorder maximum spanning tree dependency parsing algorithm of Carreras (2007) with the passiveaggressive perceptron algorithm of Crammer et al. (2006) implemented with a hash kernel for faster processing times (Bohnet, 2010). of various kinds of linguistic information. For instance, in terms of PoS tags, the UD scheme offers a dedicated tag for proper nouns (PROPN), where NDT contains information about noun type among its morphological features. UD further distinguishes auxiliary verbs (AUX) from main verbs (VERB). This distinction is not explicitly made in NDT, hence the conversion procedure makes use of the syntactic context of a verb; verbs that have a non-finite de"
W17-0201,D14-1082,0,0.0224774,"ouns and possessive pronouns. 4 UDPipe UDPipe (Straka et al., 2016) provides an open-source C++ implementation of an entire end-to-end pipeline for dependency parsing. All components are trainable and default settings are provided based on tuning towards the UD treebanks. The two components of UDPipe used in our experiments comprise the MorphoDiTa tagger (Straková et al., 2014) and the Parsito parser (Straka et al., 2015). MorphoDiTa implements an averaged perceptron algorithm (Collins, 2002) while Parsito is a greedy transition-based parser based on the neural network classifier described by Chen and Manning (2014). When training the components, we use the same parametrization as reported in Straka et al. (2016) after tuning the parser for version 1.2 of the Bokmål UD data. For the parser, this includes form embeddings of dimension 50, PoS tag, FEATS and arc label embeddings of dimension 20, and a 200-node hidden layer. For each experiment, we pre-train the form embeddings on the training data (i.e., the raw text of whatever portion of the labeled training data is used for a given experiment) using word2vec (Mikolov et al., 2013), again with the same parameters as reported by Straka et al. (2015) for a"
W17-0201,P15-1038,0,0.0130111,"ta used for these experiments follows the UD v1.4 guidelines, but its first release as a UD treebank will be in v2.0. For replicability we therefore make our data available from the companion Git repository. 2 TnT & Mate The widely used TnT tagger (Brants, 2000), implementing a 2nd order Markov model, achieves high accuracy as well as very high speed. TnT was used by Petrov et al. (2012) when evaluating the proposed universal tag set. Solberg et al. (2014) found the Mate dependency parser (Bohnet, 2010) to have the best performance for parsing of NDT, and recent dependency parser comparisons (Choi et al., 2015) have also found Mate to perform very well for English. The fast training time of Mate also facilitates rapid experimentation. Mate implements the secondorder maximum spanning tree dependency parsing algorithm of Carreras (2007) with the passiveaggressive perceptron algorithm of Crammer et al. (2006) implemented with a hash kernel for faster processing times (Bohnet, 2010). of various kinds of linguistic information. For instance, in terms of PoS tags, the UD scheme offers a dedicated tag for proper nouns (PROPN), where NDT contains information about noun type among its morphological features."
W17-0201,L16-1250,1,0.698351,"possibly filtered version of) the source treebank. Zeman and Resnik (2008) applied this approach to the highly related language pair of Swedish and Danish, and Skjærholt and Øvrelid (2012) extended the language inventory to also include Norwegian, and showed that parser lexicalization actually improved parsing results between these languages. 3 The Norwegian UD Treebank Universal Dependencies (UD) (de Marneffe et al., 2014; Nivre, 2015) is a community-driven effort to create cross-linguistically consistent syntactic annotation. Our experiments are based on the Universal Dependency conversion (Øvrelid and Hohle, 2016) of the Norwegian Dependency Treebank (NDT) (Solberg et al., 2014). NDT contains manually annotated syntactic and morphological information for both varieties of Norwegian; 311,000 tokens of Bokmål and 303,000 tokens of Nynorsk. The treebanked material mostly comprises newspaper text, but also includes government reports, parliament transcripts and blog excerpts. The UD version of NDT has until now been limited to the Bokmål sections of the treebank. For the purpose of the current work, the Nynorsk section has also been automatically converted to Universal Dependencies, making use of the conve"
W17-0201,W02-1001,0,0.0621248,"nd be vs bu for definiteness, and (b) rules that make reference to closed class lemmas, such as quantificational pronouns and possessive pronouns. 4 UDPipe UDPipe (Straka et al., 2016) provides an open-source C++ implementation of an entire end-to-end pipeline for dependency parsing. All components are trainable and default settings are provided based on tuning towards the UD treebanks. The two components of UDPipe used in our experiments comprise the MorphoDiTa tagger (Straková et al., 2014) and the Parsito parser (Straka et al., 2015). MorphoDiTa implements an averaged perceptron algorithm (Collins, 2002) while Parsito is a greedy transition-based parser based on the neural network classifier described by Chen and Manning (2014). When training the components, we use the same parametrization as reported in Straka et al. (2016) after tuning the parser for version 1.2 of the Bokmål UD data. For the parser, this includes form embeddings of dimension 50, PoS tag, FEATS and arc label embeddings of dimension 20, and a 200-node hidden layer. For each experiment, we pre-train the form embeddings on the training data (i.e., the raw text of whatever portion of the labeled training data is used for a give"
W17-0201,petrov-etal-2012-universal,0,0.144654,"Missing"
W17-0201,de-marneffe-etal-2014-universal,0,0.105722,"Missing"
W17-0201,P11-2120,0,0.0223967,"entence in Nynorsk (top row) and Bokmål (second row) with corresponding English gloss, UD PoS and dependency analysis. ing data, albeit using a rule-based MT system with no word alignments. Our main goal is to arrive at the best joint model that may be applied to both Norwegian standards. guage pairs and less related languages. This task has been approached via so-called ’annotation projection’, where parallel data is used to induce structure from source to target language (Hwa et al., 2005; Spreyer et al., 2010; Agi´c et al., 2016) and as delexicalized model transfer (Zeman and Resnik, 2008; Søgaard, 2011; Täckström et al., 2012). The basic procedure in the latter work has relied on a simple conversion procedure to map part-of-speech tags of the source and target languages into a common tagset and subsequent training of a delexicalized parser on (a possibly filtered version of) the source treebank. Zeman and Resnik (2008) applied this approach to the highly related language pair of Swedish and Danish, and Skjærholt and Øvrelid (2012) extended the language inventory to also include Norwegian, and showed that parser lexicalization actually improved parsing results between these languages. 3 The"
W17-0201,spreyer-etal-2010-training,1,0.80925,"not any satisfactory multiuse-house with those money ADV DET ADJ NOUN ADP DET NOUN Figure 1: Example sentence in Nynorsk (top row) and Bokmål (second row) with corresponding English gloss, UD PoS and dependency analysis. ing data, albeit using a rule-based MT system with no word alignments. Our main goal is to arrive at the best joint model that may be applied to both Norwegian standards. guage pairs and less related languages. This task has been approached via so-called ’annotation projection’, where parallel data is used to induce structure from source to target language (Hwa et al., 2005; Spreyer et al., 2010; Agi´c et al., 2016) and as delexicalized model transfer (Zeman and Resnik, 2008; Søgaard, 2011; Täckström et al., 2012). The basic procedure in the latter work has relied on a simple conversion procedure to map part-of-speech tags of the source and target languages into a common tagset and subsequent training of a delexicalized parser on (a possibly filtered version of) the source treebank. Zeman and Resnik (2008) applied this approach to the highly related language pair of Swedish and Danish, and Skjærholt and Øvrelid (2012) extended the language inventory to also include Norwegian, and sho"
W17-0201,L16-1680,0,0.0364895,"Missing"
W17-0201,P14-5003,0,0.0342168,"Missing"
W17-0201,N12-1052,0,0.0249746,"rsk (top row) and Bokmål (second row) with corresponding English gloss, UD PoS and dependency analysis. ing data, albeit using a rule-based MT system with no word alignments. Our main goal is to arrive at the best joint model that may be applied to both Norwegian standards. guage pairs and less related languages. This task has been approached via so-called ’annotation projection’, where parallel data is used to induce structure from source to target language (Hwa et al., 2005; Spreyer et al., 2010; Agi´c et al., 2016) and as delexicalized model transfer (Zeman and Resnik, 2008; Søgaard, 2011; Täckström et al., 2012). The basic procedure in the latter work has relied on a simple conversion procedure to map part-of-speech tags of the source and target languages into a common tagset and subsequent training of a delexicalized parser on (a possibly filtered version of) the source treebank. Zeman and Resnik (2008) applied this approach to the highly related language pair of Swedish and Danish, and Skjærholt and Øvrelid (2012) extended the language inventory to also include Norwegian, and showed that parser lexicalization actually improved parsing results between these languages. 3 The Norwegian UD Treebank Uni"
W17-0201,W14-1614,0,0.0434666,"Missing"
W17-0201,2009.freeopmt-1.7,0,0.0142601,". Machine-translated training data The results above show that combining training data across standards can improve parsing performance. As mentioned in the introduction, though, there is a large degree of lexical divergence between the two standards. In our next suite of experiments, we therefore attempt to further improve the results by automatically machine-translating the training texts. Given the strong degree of structural equivalence between Norwegian Bokmål and Nynorsk, we can expect MT to yield relatively accurate translations. For this, we use the two-way Bokmål–Nynorsk MT system of Unhammer and Trosterud (2009), a rule-based shallowtransfer system built on the open-source MT platform Apertium (Forcada et al., 2011). The raw text passed to Apertium is extracted The results for the development data are shown in Table 2. Adding the MT data reinforces the trend observed for mixing the original training sets: Despite that PoS tagging accuracy typically (though not always) decreases when adding data, parsing accuracy improves. For the TnT+Mate pipeline, we see that the best parser performance is obtained with the single-standard models including the MT data, while UDPipe achieves the best results when usi"
W17-0201,I08-3008,0,0.0402951,"NOUN Figure 1: Example sentence in Nynorsk (top row) and Bokmål (second row) with corresponding English gloss, UD PoS and dependency analysis. ing data, albeit using a rule-based MT system with no word alignments. Our main goal is to arrive at the best joint model that may be applied to both Norwegian standards. guage pairs and less related languages. This task has been approached via so-called ’annotation projection’, where parallel data is used to induce structure from source to target language (Hwa et al., 2005; Spreyer et al., 2010; Agi´c et al., 2016) and as delexicalized model transfer (Zeman and Resnik, 2008; Søgaard, 2011; Täckström et al., 2012). The basic procedure in the latter work has relied on a simple conversion procedure to map part-of-speech tags of the source and target languages into a common tagset and subsequent training of a delexicalized parser on (a possibly filtered version of) the source treebank. Zeman and Resnik (2008) applied this approach to the highly related language pair of Swedish and Danish, and Skjærholt and Øvrelid (2012) extended the language inventory to also include Norwegian, and showed that parser lexicalization actually improved parsing results between these la"
W17-0201,solberg-etal-2014-norwegian,1,\N,Missing
W17-0201,L16-1262,0,\N,Missing
W17-0217,A00-1031,0,0.521107,"opment required many repeated cycles of training and testing for the various modified tagsets, we sought a PoS tagger that is both reasonably fast and accurate. There is often a considerable trade-off between the two factors, as the most accurate taggers tend to suffer in terms of speed due to their complexity. However, a widely used tagger that achieves both close to state-of-the-art accuracy as 1 Our defined train/dev./test split is available for download at http://github.com/petterhh/ndt-tools and will be distributed with future releases of the treebank. 144 well as very high speed is TnT (Brants, 2000), and hence we adopt this for the current study. Parser In choosing a syntactic parser for our experiments, we considered previous work on dependency parsing of Norwegian, specifically that of Solberg et al. (2014), who found the graphbased Mate parser (Bohnet, 2010) to have the best performance for NDT. Recent dependency parser comparisons (Choi et al., 2015) show very strong results for Mate also for English, outperforming a range of contemporary state-of-the-art parsers. We will be using Mate for gauging the effects of the tagset modifications in our experiments. Training Testing LAS UAS Go"
W17-0217,D14-1103,0,0.0181425,"rser to German using tagsets of varying granularity; the 12 tags of the Universal Tagset (UTS) (Petrov et al., 2012), the 54 tags of STTS and an extended version of STTS including all the morphological information from the treebanks used for training, resulting in up to 783 tags. Maier et al. (2014) experimented with six different PoS taggers, but found TnT to have the most consistent performance across different tagsets and settings. Predictably, tagger accuracy drops as granularity increases, but the best parsing performance was observed for the medium-sized tagset, i.e., the original STTS. Müller et al. (2014) attempt to improve dependency parsing with Mate by automatically defining a more fine-grained tagset using so-called split-merge training to create Hidden Markov models with latent annotations (HMM-LA). This entails iteratively splitting every tag into two subtags, but reverting to the original tag unless a certain improvement in the likelihood function is observed. Müller et al. (2014) argue that the resulting annotations “are to a considerable extent linguistically interpretable”. Similarly to the setup of Rehbein and Hirschmann (2013), two layers of taggers are used. While the modification"
W17-0217,P15-1038,0,0.073526,"both close to state-of-the-art accuracy as 1 Our defined train/dev./test split is available for download at http://github.com/petterhh/ndt-tools and will be distributed with future releases of the treebank. 144 well as very high speed is TnT (Brants, 2000), and hence we adopt this for the current study. Parser In choosing a syntactic parser for our experiments, we considered previous work on dependency parsing of Norwegian, specifically that of Solberg et al. (2014), who found the graphbased Mate parser (Bohnet, 2010) to have the best performance for NDT. Recent dependency parser comparisons (Choi et al., 2015) show very strong results for Mate also for English, outperforming a range of contemporary state-of-the-art parsers. We will be using Mate for gauging the effects of the tagset modifications in our experiments. Training Testing LAS UAS Gold Gold Auto Gold Auto Auto 90.15 85.68 87.01 92.51 88.98 90.19 Table 3: Results of parsing with Mate using various configurations of PoS tag sources in training and testing. Gold denotes gold standard tags while Auto denotes tags automatically predicted by TnT. such ‘silver-standard’ tags actually improves parsing scores substantially compared to training on"
W17-0217,petrov-etal-2012-universal,0,0.172604,"Missing"
W17-0217,W03-0419,0,0.586305,"Missing"
W17-0217,W14-6101,0,0.0228279,"treet Journal portion of the Penn Treebank (PTB) (Marcus et al., 1993). Based on linguistic considerations, MacKinlay (2005) mapped the 45 tags of the original PTB tagset to more fine-grained tagsets to investigate whether additional linguistic information could assist the tagger. Experimenting with both lexically and syntactically conditioned modifications, they did not find any statistically significant improvements, arguing that their results do not support the hypothesis that it is possible to achieve significant performance improvements in PoS tagging by utilizing a finer-grained tagset. Maier et al. (2014) also experiment with applying the Berkeley constituency parser to German using tagsets of varying granularity; the 12 tags of the Universal Tagset (UTS) (Petrov et al., 2012), the 54 tags of STTS and an extended version of STTS including all the morphological information from the treebanks used for training, resulting in up to 783 tags. Maier et al. (2014) experimented with six different PoS taggers, but found TnT to have the most consistent performance across different tagsets and settings. Predictably, tagger accuracy drops as granularity increases, but the best parsing performance was obse"
W17-0217,solberg-etal-2014-norwegian,1,0.785034,"n NDT. tense. Selected subsets of these features are used in our tagset modifications, where the coarse PoS tag of relevant tokens is concatenated with one or more features to include more linguistic information in the tags. Table 1: Overview of the original PoS tagset of NDT (excluding punctuation tags). and demonstrate significant – and substantial – improvements in parser performance. 3 Syntactic Annotation The syntactic annotation choices in NDT are largely based on the Norwegian Reference Grammar (Faarlund et al., 1997). Some central annotation choices are outlined in Table 2, taken from Solberg et al. (2014), providing overview of the analyses of syntactic constructions that often distinguish dependency treebanks, such as coordination and the treatment of auxiliary and main verbs. The annotations comprise 29 dependency relations, including ADV (adverbial), SUBJ (subject) and KOORD (coordination). The Norwegian Dependency Treebank Our experiments are based on the newly developed Norwegian Dependency Treebank (NDT) (Solberg et al., 2014), the first publicly available treebank for Norwegian. It was developed at the National Library of Norway in collaboration with the University of Oslo, and contains"
W17-0217,W13-5644,0,0.0210758,"experimental setup. Data Set Split As there was no existing standardized data set split of NDT due to its recent development, we first needed to define separate sections for training, development and testing.1 Our proposed sectioning of the treebank follows a standard 80-10-10 split. In establishing the split, care has been taken to preserve contiguous texts in the various sections while also keeping them balanced in terms of genre. Morphological Annotation The morphological annotation and PoS tagset of NDT is based on the same inventory as used by the Oslo-Bergen Tagger (Hagen et al., 2000; Solberg, 2013), which in turn is largely based on the work of Faarlund et al. (1997). The tagset consists of 12 morphosyntactic PoS tags outlined in Table 1, with 7 additional tags for punctuation and symbols. The tagset is thus rather coarse-grained, with broad categories such as subst (noun) and verb (verb). The PoS tags are complemented by a large set of morphological features, providing information about morphological properties such as definiteness, number and Tagger As our experiments during development required many repeated cycles of training and testing for the various modified tagsets, we sought a"
W17-0217,L16-1680,0,0.0603878,"Missing"
W17-0217,W10-1401,0,0.0206416,"test set improves from 90.34 to 90.57 for English (using a HMM-LA tagset of 115 tags) and from 87.92 to 88.24 for German (using 107 tags). Moving beyond tagging, Seddah et al. (2009) focus on syntactic constituent parsing for French and show that extending the PoS tagset with information about mood and finiteness for verbs is indeed beneficial. Similarly, the recent shared tasks on parsing morphologically rich languages has seen quite a bit of work focused on evaluating the effect of various types of morphological information on syntactic parsing (both constituent-based and dependency-based) (Tsarfaty et al., 2010; Seddah et al., 2013). They find that the type of morphological information which is beneficial for parsing varies across languages and the quality of this information (i.e. whether it is gold standard or predicted) will also influence the results. Rehbein and Hirschmann (2013) report on experiments for parsing German, demonstrating small but significant improvements when introducing more fine-grained and syntactically motivated distinctions in the tagset, based on the Stuttgard-Tübingen Tagset (STTS). However, the In the current paper, we introduce linguistically motivated modifications acro"
W17-0217,J09-3003,0,0.014725,"mpacts the performance of tagging and syntactic dependency parsing. Our results show that parsing accuracy can be significantly improved by introducing more finegrained morphological information in the tagset, even if tagger accuracy is compromised. Our taggers and parsers are trained and tested using the annotations of the Norwegian Dependency Treebank. 1 Introduction Part-of-speech (PoS) tagging is an important preprocessing step for many NLP tasks, such as dependency parsing (Nivre et al., 2007; Hajiˇc et al., 2009), named entity recognition (Sang and Meulder, 2003) and sentiment analysis (Wilson et al., 2009). Whereas much effort has gone into the development of PoS taggers – to the effect that this task is often considered more or less a solved task – considerably less effort has been devoted to the empirical evaluation of the PoS tagsets themselves. Error analysis of PoS taggers indicate that, whereas tagging improvement through means of learning algorithm or feature engineering seems to have reached something of a plateau, linguistic and empirical assessment of the distinctions made in the PoS tagsets may be an avenue worth investigating further (Manning, 2011). Clearly, the utility of a PoS ta"
W17-0217,P11-2033,0,0.0397708,"arning algorithm or feature engineering seems to have reached something of a plateau, linguistic and empirical assessment of the distinctions made in the PoS tagsets may be an avenue worth investigating further (Manning, 2011). Clearly, the utility of a PoS tagset is tightly coupled with the downstream task for which it is performed. Even so, PoS tagsets are usually employed in a “one size fits all” fashion, regardless of the requirements posed by the task making use of this information. It is well known that syntactic parsing often benefits from quite fine-grained morphological distinctions (Zhang and Nivre, 2011; Seeker and 2 Previous Work This section reviews some of the previous work documenting the impact that PoS tagsets has on the performance of taggers and parsers. Megyesi (2002) trained and evaluated a range of PoS taggers on the Stockholm-Umeå Corpus (SUC) (Gustafson-Capková and Hartmann, 2006), 142 Proceedings of the 21st Nordic Conference of Computational Linguistics, pages 142–151, c Gothenburg, Sweden, 23-24 May 2017. 2017 Link¨oping University Electronic Press scope of the changes are limited to modifier distinctions and the new tagset only includes four new PoS tags, changing two of the"
W17-0217,J93-2004,0,\N,Missing
W17-0217,L16-1250,1,\N,Missing
W17-0237,P14-1023,0,0.0167,"vide indicative empirical results for a first set of embeddings made available in the repository (Section 3). Using an interactive web application, users are also able to explore and compare different pretrained models on-line (Section 4). 2 Motivation and background Over the last few years, the field of NLP at large has seen a huge revival of interest for distributional semantic representations in the form of word vectors 1 Our repository in several respects complements and updates the collection of Wikipedia-derived corpora and pretrained word embeddings published by Al-Rfou et al. (2013). (Baroni et al., 2014). In particular, the use of dense vectors embedded in low-dimensional spaces, socalled word embeddings, have proved popular. As recent studies have shown that beyond their traditional use for encoding word-to-word semantic similarity, word vectors can also encode other relational or ‘analogical’ similarities that can be retrieved by simple vector arithmetic, these models have found many new use cases. More importantly, however, the interest in word embeddings coincides in several ways with the revived interest in neural network architectures: Word embeddings are now standardly used for providi"
W17-0237,Q17-1010,0,0.0313544,"Missing"
W17-0237,P10-2043,0,0.034064,"Missing"
W17-0237,P15-1033,0,0.00750661,"Missing"
W17-0237,N16-2002,0,0.0261349,"Missing"
W17-0237,C16-1262,0,0.0223234,"Missing"
W17-0237,J15-4004,0,0.0532269,"Missing"
W17-0237,D14-1181,0,0.0154571,"Missing"
W17-0237,E17-3025,1,0.895986,"Missing"
W17-0237,N16-1030,0,0.00679306,"Missing"
W17-0237,W14-1618,0,0.0607022,"Missing"
W17-0237,Q15-1016,0,0.0582115,"Missing"
W17-0237,P14-5010,0,0.00549961,"Missing"
W17-0237,D14-1162,0,0.113148,"Missing"
W17-0237,P16-2067,0,0.0327556,"Missing"
W17-0237,P05-1077,0,0.0183182,"Missing"
W17-0237,D13-1170,0,0.0109549,"Missing"
W17-0237,W11-4631,1,0.893413,"Missing"
W17-0242,D09-1097,0,0.0583021,"Missing"
W17-0242,N15-1169,0,0.0602173,"words (skip-gram). We used the word2vec implementation provided in the ˇ uˇrek and Sojka, free python library gensim (Reh˚ 2010), using the default parameters to train skipgram models. The defaults are a minimum of 5 occurrences in the corpus for the lemmas and an embedding dimension of size 100. Five iterations over the corpus was made. 5 5.3 There are several ways one could choose to evaluate the quality of the words that are automatically inserted into the hierarchy. For example, Yamada et al. (2009) chose to manually evaluate a random sample of 200 unseen words, while Jurgens and Pilehvar (2015) treat the words already encoded in the hierarchy as gold data and then try to reattach these. We here follow the latter approach. However, while Jurgens and Pilehvar (2015) restrict their evaluation to monosemous words, we also include polysemous words in order to make the evaluation more realistic. For evaluation and tuning we split the wordnet into a development set and a test set, with 1388 target words in each. Potential targets only comprise words that have a hypernym encoded (which, in fact, are not that many, as NWN is relatively flat) and occur in the news corpus sufficiently often (≥"
W17-0242,P06-1101,0,0.0727719,"Missing"
W17-0808,W14-5204,1,0.928633,"er Science ♠ Vassar College, Department of Computer Science ♥ University of Oslo, Department of Informatics ♦ Brandeis University, Linguistics and Computational Linguistics Abstract representation—a uniform framework-internal convention—with mappings from tool-specific input and output formats. Specifically, we will take an in-depth look at how the results of morphosyntactic analysis are represented in (a) the DKPro Core component collection1 (Eckart de Castilho and Gurevych, 2014), (b) the Language Analysis Portal2 (LAP; Lapponi et al. (2014)), and (c) the Language Application (LAPPS) Grid3 (Ide et al., 2014a). These three systems all share the common goal of facilitating the creation of complex NLP workflows, allowing users to combine tools that would otherwise need input and output format conversion in order to be made compatible. While the programmatic interface of DKPro Core targets more technically inclined users, LAP and LAPPS are realized as web applications with a point-andclick graphical interface. All three have been under active development for the past several years and have—in contemporaneous, parallel work— designed and implemented framework-specific representations. These designs a"
W17-0808,lapponi-etal-2014-road,1,0.860158,"Marc Verhagen♦ ♣ Technische Universität Darmstadt, Department of Computer Science ♠ Vassar College, Department of Computer Science ♥ University of Oslo, Department of Informatics ♦ Brandeis University, Linguistics and Computational Linguistics Abstract representation—a uniform framework-internal convention—with mappings from tool-specific input and output formats. Specifically, we will take an in-depth look at how the results of morphosyntactic analysis are represented in (a) the DKPro Core component collection1 (Eckart de Castilho and Gurevych, 2014), (b) the Language Analysis Portal2 (LAP; Lapponi et al. (2014)), and (c) the Language Application (LAPPS) Grid3 (Ide et al., 2014a). These three systems all share the common goal of facilitating the creation of complex NLP workflows, allowing users to combine tools that would otherwise need input and output format conversion in order to be made compatible. While the programmatic interface of DKPro Core targets more technically inclined users, LAP and LAPPS are realized as web applications with a point-andclick graphical interface. All three have been under active development for the past several years and have—in contemporaneous, parallel work— designed"
W17-0808,P12-2074,1,0.837847,"pendencyStructure which can bind multiple dependency relations together and thus supports multiple parallel dependency structures even within a single LIF view. Media–Tokenization Mismatches Tokenizers may apply transformations to the original input text that introduce character offset mismatches with the normalized output. For example, some Penn Treebank–compliant tokenizers normalize different conventions for quotation marks (which may be rendered as straight ‘typewriter’ quotes or in multicharacter LATEX-style encodings, e.g. &quot; or ``) into opening (left) and closing (right) Unicode glyphs (Dridan and Oepen, 2012). To make such normalization accessible to downstream processing, it is insufficient to represent tokens as only a region (sub-string) of the underlying linguistic signal. In LXF, the string output of tokenizers is recorded in the annotations encapsulated with each token node, which is in turn linked to a region recording its character offsets in the original media. LIF (which is largely inspired by ISO LAF, much like LXF) also records the token string and its character offsets in the original medium. LIF supports this via the word property on tokens. DKPro Core has also recently started intro"
W17-0808,W14-5201,1,0.897235,"Missing"
W17-0808,heid-etal-2010-corpus,0,0.0249296,"Lacking interface standardization, thus, severely limits interoperability. The frameworks surveyed in this work address interoperability by means of a common 2 Terminological Definitions A number of closely interrelated concepts apply to the discussion of design choices in the repre1 https://dkpro.github.io/dkpro-core https://lap.clarino.uio.no 3 https://www.lappsgrid.org 4 There are, of course, additional designs and workflow frameworks that we would ultimately hope to include in this comparison, as for example the representations used by CONCRETE, WebLicht, and FoLiA (Ferraro et al., 2014; Heid et al., 2010; van Gompel and Reynaert, 2013), to name just a few. However, some of these frameworks are at least abstractly very similar to representatives in our current sample, and also for reasons of space we need to restrict this in-depth comparison to a relatively small selection. 2 67 Proceedings of the 11th Linguistic Annotation Workshop, pages 67–75, c Valencia, Spain, April 3, 2017. 2017 Association for Computational Linguistics root nsubj det aux nn DT The the NNP Olympic Olympic neg NNP Committee Committee VBZ does do RB n’t not xcomp VB regret regret VBG choosing choose ORGANIZATION NNP China"
W17-0808,ide-etal-2014-language,1,0.90873,"Missing"
W17-1810,W06-2920,0,0.0186187,"CoNLL-style format. While the shared task also included detection of events and focus, we only focus on cues and scopes in this work. We use the same splits for training, development testing and held-out evaluation as supplied for the shared task. Examples (1)-(2) below show two examples taken from the corpus, where negation cues are in bold and their scopes are underlined. In (1), the cue is the adverb not, whereas (2) provides an example of the affixal cue un. System requirements The input given to the system can either be raw running text or parsed data in the CoNLL-X format (Buchholz and Marsi, 2006). If the user inputs raw text, we need to tokenize, tag and parse the text before we can classify the sentences. Because our training data uses PTB PoS-tags and Stanford dependencies (following conversion), we need a pipeline providing the same standard, and hence use the CoreNLP tool (Manning et al., 2014). Beyond Python 2.7 or newer, the negation tool has the following dependencies: scikit-learn, PyStruct, NumPy, and NetworkX (in addition to CoreNLP unless pre-parsed input is provided). (1) And yet it was not quite the last. (2) Since we have been so unfortunate as to miss him and have no no"
W17-1810,P14-1007,0,0.49475,"ion toolkit builds on existing libraries that are actively maintained and easy to install, and the source1 is made freely available (GPL). While we make pre-trained classifiers available (for English), users will also be able to train their own models. The system design is based on best practices from previous work, in particular systems from the 2012 *SEM shared task. In particular, we adopt the practice of solving scope resolution as a sequence labeling task (Morante and Daelemans, 2009; Lapponi et al., 2012; White, 2012) based on syntactic features (Read et al., 2012; Lapponi et al., 2012; Packard et al., 2014). In contrast to many of the previous systems that have used constituency-based representations (Read et al., 2012; Packard et al., 2014), we base our syntactic features on dependency representations, similar to the approach of Lapponi et al. (2012). For cue detection, on the other hand, simply using surfaceoriented lexical features have been shown to be sufficient, and we here largely build on the specific approach described by Read et al. (2012; Velldal et al. (2012), using a binary SVM classifier. The main goal of this work is to arrive at a lean and light-weight system with minimal use of"
W17-1810,P05-1022,0,0.0102334,"can classify the sentences. Because our training data uses PTB PoS-tags and Stanford dependencies (following conversion), we need a pipeline providing the same standard, and hence use the CoreNLP tool (Manning et al., 2014). Beyond Python 2.7 or newer, the negation tool has the following dependencies: scikit-learn, PyStruct, NumPy, and NetworkX (in addition to CoreNLP unless pre-parsed input is provided). (1) And yet it was not quite the last. (2) Since we have been so unfortunate as to miss him and have no notion [. . . ] The Conan Doyle corpus provides phrase structure trees produced by the Charniak and Johnson (2005) parser, and we have used the Stanford Parser (Manning et al., 2014) to convert these to Stanford basic dependency representations (de Marneffe et al., 2014) prior to training. Evaluation We use the evaluation script of the 2012 *SEM shared task (Morante and Blanco, 2012) for measuring precision, recall and F-score. For scopes, it provides two different measures; token-level and scope-level. For the token-level measure the evaluation is defined similarly as for cues, simply checking whether each token in the scope sequence is correctly labeled. For scopes on the other hand, a true positive req"
W17-1810,de-marneffe-etal-2014-universal,0,0.04922,"Missing"
W17-1810,S12-1041,1,0.3603,"at is affected by the negation. Our negation toolkit builds on existing libraries that are actively maintained and easy to install, and the source1 is made freely available (GPL). While we make pre-trained classifiers available (for English), users will also be able to train their own models. The system design is based on best practices from previous work, in particular systems from the 2012 *SEM shared task. In particular, we adopt the practice of solving scope resolution as a sequence labeling task (Morante and Daelemans, 2009; Lapponi et al., 2012; White, 2012) based on syntactic features (Read et al., 2012; Lapponi et al., 2012; Packard et al., 2014). In contrast to many of the previous systems that have used constituency-based representations (Read et al., 2012; Packard et al., 2014), we base our syntactic features on dependency representations, similar to the approach of Lapponi et al. (2012). For cue detection, on the other hand, simply using surfaceoriented lexical features have been shown to be sufficient, and we here largely build on the specific approach described by Read et al. (2012; Velldal et al. (2012), using a binary SVM classifier. The main goal of this work is to arrive at a lean"
W17-1810,P16-1047,0,0.775905,"d experimental results are provided for English. 1 Introduction The task of negation detection has recently seen quite a bit of interest in the NLP community, in part spurred by the availability of annotated data and evaluation software introduced by the shared tasks at CoNLL 2010 (Farkas et al., 2010) and *SEM 2012 (Morante and Blanco, 2012). While many research-based systems have been developed, with the aim of exploring features and algorithms to advance the state-of-the-art in terms of performance (Morante and Daelemans, 2009; Read et al., 2012; Lapponi et al., 2012; Packard et al., 2014; Fancellu et al., 2016), many of them are difficult to employ in practice, due to layered architectures and many dependencies, and furthermore, most are simply not made publicly available in the first place. In this paper, we present an open-source portable toolkit for automatic negation detection, with experimental results reported for English. The system is implemented in Python on top of PyStruct (M¨uller and Behnke, 2014), a library for structured prediction based on a maximum-margin approach. The system implements two stages of negation analysis, namely cue detection, which detects words that signal negation, s"
W17-1810,J12-2005,1,0.803538,"and Daelemans, 2009; Lapponi et al., 2012; White, 2012) based on syntactic features (Read et al., 2012; Lapponi et al., 2012; Packard et al., 2014). In contrast to many of the previous systems that have used constituency-based representations (Read et al., 2012; Packard et al., 2014), we base our syntactic features on dependency representations, similar to the approach of Lapponi et al. (2012). For cue detection, on the other hand, simply using surfaceoriented lexical features have been shown to be sufficient, and we here largely build on the specific approach described by Read et al. (2012; Velldal et al. (2012), using a binary SVM classifier. The main goal of this work is to arrive at a lean and light-weight system with minimal use of extra heuristics beyond machine learned models. While achieving the highest performance was not our main goal, the results are competitive with previously reported SoA results in the literature. Moreover, the system can be employed with both raw and parsed input data. This paper presents an open-source toolkit for negation detection. It identifies negation cues and their corresponding scope in either raw or parsed text using maximummargin classification. The system des"
W17-1810,S12-1042,1,0.94257,"d scope resolution, which identifies the span of the sentence that is affected by the negation. Our negation toolkit builds on existing libraries that are actively maintained and easy to install, and the source1 is made freely available (GPL). While we make pre-trained classifiers available (for English), users will also be able to train their own models. The system design is based on best practices from previous work, in particular systems from the 2012 *SEM shared task. In particular, we adopt the practice of solving scope resolution as a sequence labeling task (Morante and Daelemans, 2009; Lapponi et al., 2012; White, 2012) based on syntactic features (Read et al., 2012; Lapponi et al., 2012; Packard et al., 2014). In contrast to many of the previous systems that have used constituency-based representations (Read et al., 2012; Packard et al., 2014), we base our syntactic features on dependency representations, similar to the approach of Lapponi et al. (2012). For cue detection, on the other hand, simply using surfaceoriented lexical features have been shown to be sufficient, and we here largely build on the specific approach described by Read et al. (2012; Velldal et al. (2012), using a binary SVM"
W17-1810,W08-0606,0,0.621224,"ccurring in the data set that are not covered by the heuristics currently implemented in the system. Beyond the multi-word cue heuristics, our implementation is abstract in the sense that it is not hard-coded for negation, instead relying on models to be learned automatically from any data using a CoNLL style format similar to that of the *SEM shared task. Importantly, this means that the tool could be trained for other similar tasks, such as speculation detection, as long as cues and scopes are marked. One interesting direction here would be to convert the annotations of the BioScope corpus (Vincze et al., 2008) to the format used by the Conan Doyle corpus. This would allow training of both speculation and negation detection models for biomedical data, and also to test cross-domain effects. Such a conversion is not entirely trivial, however, as the resources differ not merely in terms of format but also the underlying annotation rules. Developing such a mapping could greatly benefit this research field, also making it possible to use data from different domains. 7 Conclusion This paper has presented an open-source tool for detecting negation cues and their in-sentence scopes. Despite the substantial"
W17-1810,S12-1044,0,0.557811,"ich identifies the span of the sentence that is affected by the negation. Our negation toolkit builds on existing libraries that are actively maintained and easy to install, and the source1 is made freely available (GPL). While we make pre-trained classifiers available (for English), users will also be able to train their own models. The system design is based on best practices from previous work, in particular systems from the 2012 *SEM shared task. In particular, we adopt the practice of solving scope resolution as a sequence labeling task (Morante and Daelemans, 2009; Lapponi et al., 2012; White, 2012) based on syntactic features (Read et al., 2012; Lapponi et al., 2012; Packard et al., 2014). In contrast to many of the previous systems that have used constituency-based representations (Read et al., 2012; Packard et al., 2014), we base our syntactic features on dependency representations, similar to the approach of Lapponi et al. (2012). For cue detection, on the other hand, simply using surfaceoriented lexical features have been shown to be sufficient, and we here largely build on the specific approach described by Read et al. (2012; Velldal et al. (2012), using a binary SVM classifier. Th"
W17-1810,P14-5010,0,0.00390781,"m the corpus, where negation cues are in bold and their scopes are underlined. In (1), the cue is the adverb not, whereas (2) provides an example of the affixal cue un. System requirements The input given to the system can either be raw running text or parsed data in the CoNLL-X format (Buchholz and Marsi, 2006). If the user inputs raw text, we need to tokenize, tag and parse the text before we can classify the sentences. Because our training data uses PTB PoS-tags and Stanford dependencies (following conversion), we need a pipeline providing the same standard, and hence use the CoreNLP tool (Manning et al., 2014). Beyond Python 2.7 or newer, the negation tool has the following dependencies: scikit-learn, PyStruct, NumPy, and NetworkX (in addition to CoreNLP unless pre-parsed input is provided). (1) And yet it was not quite the last. (2) Since we have been so unfortunate as to miss him and have no notion [. . . ] The Conan Doyle corpus provides phrase structure trees produced by the Charniak and Johnson (2005) parser, and we have used the Stanford Parser (Manning et al., 2014) to convert these to Stanford basic dependency representations (de Marneffe et al., 2014) prior to training. Evaluation We use t"
W17-1810,S12-1035,0,0.866366,"rresponding scope in either raw or parsed text using maximummargin classification. The system design draws on best practice from the existing literature on negation detection, aiming for a simple and portable system that still achieves competitive performance. Pretrained models and experimental results are provided for English. 1 Introduction The task of negation detection has recently seen quite a bit of interest in the NLP community, in part spurred by the availability of annotated data and evaluation software introduced by the shared tasks at CoNLL 2010 (Farkas et al., 2010) and *SEM 2012 (Morante and Blanco, 2012). While many research-based systems have been developed, with the aim of exploring features and algorithms to advance the state-of-the-art in terms of performance (Morante and Daelemans, 2009; Read et al., 2012; Lapponi et al., 2012; Packard et al., 2014; Fancellu et al., 2016), many of them are difficult to employ in practice, due to layered architectures and many dependencies, and furthermore, most are simply not made publicly available in the first place. In this paper, we present an open-source portable toolkit for automatic negation detection, with experimental results reported for Englis"
W17-1810,W09-1105,0,0.485159,"Abstract and unfortunate, and scope resolution, which identifies the span of the sentence that is affected by the negation. Our negation toolkit builds on existing libraries that are actively maintained and easy to install, and the source1 is made freely available (GPL). While we make pre-trained classifiers available (for English), users will also be able to train their own models. The system design is based on best practices from previous work, in particular systems from the 2012 *SEM shared task. In particular, we adopt the practice of solving scope resolution as a sequence labeling task (Morante and Daelemans, 2009; Lapponi et al., 2012; White, 2012) based on syntactic features (Read et al., 2012; Lapponi et al., 2012; Packard et al., 2014). In contrast to many of the previous systems that have used constituency-based representations (Read et al., 2012; Packard et al., 2014), we base our syntactic features on dependency representations, similar to the approach of Lapponi et al. (2012). For cue detection, on the other hand, simply using surfaceoriented lexical features have been shown to be sufficient, and we here largely build on the specific approach described by Read et al. (2012; Velldal et al. (2012"
W17-1810,morante-daelemans-2012-conandoyle,0,0.300401,"2017. 2017 Association for Computational Linguistics using a maximum-margin approach. Cue detection is solved using a binary Support Vector Machine (SVM) classifier (Vapnik, 1995). As is fairly common, scope resolution is solved as a sequence labeling task, applying a discriminative linear-chain Conditional Random Fields (CRF) model.However, in a conventional CRF, the parameters are learned through maximum likelihood estimation. In PyStruct on the other hand, the parameters are estimated through maximum-margin learning based on SVMs, resulting in what may be called a maximum-margin CRF. pus (Morante and Daelemans, 2012) as used in the 2012 *SEM shared task (Morante and Blanco, 2012), based on a CoNLL-style format. While the shared task also included detection of events and focus, we only focus on cues and scopes in this work. We use the same splits for training, development testing and held-out evaluation as supplied for the shared task. Examples (1)-(2) below show two examples taken from the corpus, where negation cues are in bold and their scopes are underlined. In (1), the cue is the adverb not, whereas (2) provides an example of the affixal cue un. System requirements The input given to the system can ei"
W17-2705,D16-1229,0,0.494164,"time, we present a new task complete with an evaluation set and introduce the ‘anchor words’ method which outperforms previous approaches on this data. 1 Lilja Øvrelid Department of Informatics University of Oslo liljao@ifi.uio.no 1. Nothing has changed in the country conflict state year-to-year (class ‘stable’); Introduction Several recent studies have investigated how distributional word embeddings can be used for modeling language change, and particularly lexical semantic shifts. This includes tracing perspective change through time, usually for periods equal to centuries or decades; see (Hamilton et al., 2016b) among others. One of the main problems in these studies is the lack of proper ground truth resources describing the degree and direction of semantic change for particular words. Unfortunately, there is no such manually compiled compendium of all the semantic shifts that English words underwent in the last two centuries. The problem is even more severe for studies using more fine-grained time units spanning days or years, rather than decades, like in (Kulkarni et al., 2015) or (Kutuzov and Kuzmenko, 2016): When trying to uncover subtle changes of perspective (for example, ‘Trump’ 2. Armed co"
W17-2705,P16-1141,0,0.394823,"time, we present a new task complete with an evaluation set and introduce the ‘anchor words’ method which outperforms previous approaches on this data. 1 Lilja Øvrelid Department of Informatics University of Oslo liljao@ifi.uio.no 1. Nothing has changed in the country conflict state year-to-year (class ‘stable’); Introduction Several recent studies have investigated how distributional word embeddings can be used for modeling language change, and particularly lexical semantic shifts. This includes tracing perspective change through time, usually for periods equal to centuries or decades; see (Hamilton et al., 2016b) among others. One of the main problems in these studies is the lack of proper ground truth resources describing the degree and direction of semantic change for particular words. Unfortunately, there is no such manually compiled compendium of all the semantic shifts that English words underwent in the last two centuries. The problem is even more severe for studies using more fine-grained time units spanning days or years, rather than decades, like in (Kulkarni et al., 2015) or (Kutuzov and Kuzmenko, 2016): When trying to uncover subtle changes of perspective (for example, ‘Trump’ 2. Armed co"
W17-2705,D14-1162,0,0.11359,"Missing"
W19-4318,L18-1320,1,0.899937,"Missing"
W19-4318,N16-1000,0,0.447174,"thus do not have much significant annotation burden. Most of these methods are, however, structured: they rely on the sentences in training data being ordered and not randomly sampled. The aptly named SkipThoughts (Kiros et al., 2015) is a well-known earlier work, and uses recurrent encoder-decoder models to ‘decode’ sentences surrounding the encoded sentence, using the final encoder state as the encoded sentence’s representation. Cer et al. (2018) evaluate two different encoders, a deep averaging network and a transformer, on unsupervised data drawn from a variety of web sources. Hill et al. (2016) describe a model based on denoising auto-encoders, and a simplified variant of SkipThoughts, that sums up source word embeddings, that they dub (FastSent). Another SkipThoughts variant (Logeswaran and Lee, 2018) uses a multiple-choice objective for contextual sentences, over the more complicated decoder-based objective. Several supervised approaches to building representations also exist. An earlier work is CharaIntroduction In recent years, there has been a considerable amount of research into attempting to represent contexts longer than single words with fixedlength vectors. These represent"
W19-4318,D15-1075,0,0.035921,"gram (Wieting et al., 2016), which uses paraphrase data and builds on character representations to arrive at sentence representations. More recent papers use a diverse variety of target tasks to ground representations, such as visual data (Kiela et al., 2017), machine translation data (McCann et al., 2017), and even multiple tasks, in a multi-task learning framework (Subramanian et al., 2018). Relevant to this paper is Conneau et al.’s (2017a) InferSent, that uses natural language inference (NLI) data to ground representations: they learn these representations on the well-known SNLI dataset (Bowman et al., 2015). 2.2 2.3 On evaluation Work on evaluating sentence representations was encouraged by the release of the SentEval toolkit (Conneau and Kiela, 2018), which provided an easy-to-use framework that sentence representations could be ‘plugged’ into, for rapid downstream evaluation on numerous tasks: these include several classification tasks, textual entailment and similarity tasks, a paraphrase detection task, and caption/image retrieval tasks. Conneau et al. (2018a) also created a set of ‘probing tasks’, a variant on the theme of diagnostic classification (Hupkes et al., 2017; Belinkov et al., 201"
W19-4318,D18-2029,0,0.0219873,"task. 1 2 2.1 Background Sentence representation learning Numerous methods for learning sentence representations exist. Many of these methods are unsupervised, and thus do not have much significant annotation burden. Most of these methods are, however, structured: they rely on the sentences in training data being ordered and not randomly sampled. The aptly named SkipThoughts (Kiros et al., 2015) is a well-known earlier work, and uses recurrent encoder-decoder models to ‘decode’ sentences surrounding the encoded sentence, using the final encoder state as the encoded sentence’s representation. Cer et al. (2018) evaluate two different encoders, a deep averaging network and a transformer, on unsupervised data drawn from a variety of web sources. Hill et al. (2016) describe a model based on denoising auto-encoders, and a simplified variant of SkipThoughts, that sums up source word embeddings, that they dub (FastSent). Another SkipThoughts variant (Logeswaran and Lee, 2018) uses a multiple-choice objective for contextual sentences, over the more complicated decoder-based objective. Several supervised approaches to building representations also exist. An earlier work is CharaIntroduction In recent years,"
W19-4318,J06-4003,0,0.0853372,"r probing architecture downstream, and evaluate classifier performance. First, we load our mapped word representations for the language that we intend to analyse. We use these word representations to build sentence 6 6.1 Data Probing data We build our probing datasets using the relevant language’s Wikipedia dump as a corpus. Specifically, we use Wikipedia dumps (dated 2019-0201), which we process using the WikiExtractor 160 Figure 2: Probing accuracies for our six encoders on Conneau et al.’s dataset (orig), compared to our Wikipediaderived dataset (eng) utility1 . We use the Punkt tokeniser (Kiss and Strunk, 2006) to segment our Wikipedia dumps into discrete sentences. For Russian, which lacked a Punkt tokenisation model, we used the UDPipe (Straka and Strakov´a, 2017) toolkit to perform segmentation. 6.2 Mapping data For mapping our sentence representations, we were restricted by the availability of large parallel corpora we could use for our mapping procedure. We used two such corpora: the Europarl corpus (Koehn, 2005), a multilingual collection of European Parliament proceedings, and the MultiUN corpus (Tiedemann, 2012), a collection of translated documents from the United Nations. We used Europarl"
W19-4318,L18-1269,0,0.0631441,"cent papers use a diverse variety of target tasks to ground representations, such as visual data (Kiela et al., 2017), machine translation data (McCann et al., 2017), and even multiple tasks, in a multi-task learning framework (Subramanian et al., 2018). Relevant to this paper is Conneau et al.’s (2017a) InferSent, that uses natural language inference (NLI) data to ground representations: they learn these representations on the well-known SNLI dataset (Bowman et al., 2015). 2.2 2.3 On evaluation Work on evaluating sentence representations was encouraged by the release of the SentEval toolkit (Conneau and Kiela, 2018), which provided an easy-to-use framework that sentence representations could be ‘plugged’ into, for rapid downstream evaluation on numerous tasks: these include several classification tasks, textual entailment and similarity tasks, a paraphrase detection task, and caption/image retrieval tasks. Conneau et al. (2018a) also created a set of ‘probing tasks’, a variant on the theme of diagnostic classification (Hupkes et al., 2017; Belinkov et al., 2017), that would attempt to quantify precisely what sort of linguistic information was being retained by sentence representations. The authors, whose"
W19-4318,2005.mtsummit-papers.11,0,0.0196598,"60 Figure 2: Probing accuracies for our six encoders on Conneau et al.’s dataset (orig), compared to our Wikipediaderived dataset (eng) utility1 . We use the Punkt tokeniser (Kiss and Strunk, 2006) to segment our Wikipedia dumps into discrete sentences. For Russian, which lacked a Punkt tokenisation model, we used the UDPipe (Straka and Strakov´a, 2017) toolkit to perform segmentation. 6.2 Mapping data For mapping our sentence representations, we were restricted by the availability of large parallel corpora we could use for our mapping procedure. We used two such corpora: the Europarl corpus (Koehn, 2005), a multilingual collection of European Parliament proceedings, and the MultiUN corpus (Tiedemann, 2012), a collection of translated documents from the United Nations. We used Europarl for the official EU languages we analysed: German and Spanish. For Russian, we used MultiUN. We used both corpora for French, to attempt to analyse what, if any, effect the mapping corpus would have. We also truncated our MultiUN cororpora to 2 million sentences, to keep the corpus size roughly equivalent to Europarl, and also due to time and resource constraints: mapping representations on the complete 10 milli"
W19-4318,D18-1269,0,0.358793,"uses natural language inference (NLI) data to ground representations: they learn these representations on the well-known SNLI dataset (Bowman et al., 2015). 2.2 2.3 On evaluation Work on evaluating sentence representations was encouraged by the release of the SentEval toolkit (Conneau and Kiela, 2018), which provided an easy-to-use framework that sentence representations could be ‘plugged’ into, for rapid downstream evaluation on numerous tasks: these include several classification tasks, textual entailment and similarity tasks, a paraphrase detection task, and caption/image retrieval tasks. Conneau et al. (2018a) also created a set of ‘probing tasks’, a variant on the theme of diagnostic classification (Hupkes et al., 2017; Belinkov et al., 2017), that would attempt to quantify precisely what sort of linguistic information was being retained by sentence representations. The authors, whose work focussed on evaluating representations for English, provided Spearman correlations between the performance of a particular representation mechanism on being probed for specific linguistic properties, and the downstream performance on a variety of NLP tasks. Along similar lines, and contemporaneously with this"
W19-4318,N19-1112,0,0.114205,"o created a set of ‘probing tasks’, a variant on the theme of diagnostic classification (Hupkes et al., 2017; Belinkov et al., 2017), that would attempt to quantify precisely what sort of linguistic information was being retained by sentence representations. The authors, whose work focussed on evaluating representations for English, provided Spearman correlations between the performance of a particular representation mechanism on being probed for specific linguistic properties, and the downstream performance on a variety of NLP tasks. Along similar lines, and contemporaneously with this work, Liu et al. (2019) probe three pretrained contextualised word representation models – ELMo (Peters et al., 2018), BERT (Devlin et al., 2018) and the OpenAI transformer (Radford et al., 2018) – with a “suite of sixteen diverse probing tasks”. On a different note, Saphra and Lopez (2018) present a CCA-based method to compare representation learning dynamics across time and models, without explicitly requiring annotated probing corpora. They motivate the use of SVCCA (Raghu et al., 2017) to quantify precisely what an encoder learns by comparing the representations it generates with representations generated by an"
W19-4318,P08-1028,0,0.0763594,"e for contextual sentences, over the more complicated decoder-based objective. Several supervised approaches to building representations also exist. An earlier work is CharaIntroduction In recent years, there has been a considerable amount of research into attempting to represent contexts longer than single words with fixedlength vectors. These representations typically tend to focus on attempting to represent sentences, although phrase- and paragraph-centric mechanisms do exist. These have moved well beyond relatively na¨ıve compositional methods, such as additive and multiplicative methods (Mitchell and Lapata, 2008), one of the earlier papers on the subject. There have been several proposed approaches to learning these representations since, both unsupervised and supervised. Naturally, this has also sparked interest in evaluation methods for sentence representations; the focus of this paper is on probing-centric evaluations, and their extension to a multilingual domain. In Section 2, we provide a literature review of prior work in the numerous domains that our paper builds upon. Section 3 motivates the principle of cross-lingual probing and describes our 156 Proceedings of the 4th Workshop on Representat"
W19-4318,N16-1162,0,0.0228411,"rvised, and thus do not have much significant annotation burden. Most of these methods are, however, structured: they rely on the sentences in training data being ordered and not randomly sampled. The aptly named SkipThoughts (Kiros et al., 2015) is a well-known earlier work, and uses recurrent encoder-decoder models to ‘decode’ sentences surrounding the encoded sentence, using the final encoder state as the encoded sentence’s representation. Cer et al. (2018) evaluate two different encoders, a deep averaging network and a transformer, on unsupervised data drawn from a variety of web sources. Hill et al. (2016) describe a model based on denoising auto-encoders, and a simplified variant of SkipThoughts, that sums up source word embeddings, that they dub (FastSent). Another SkipThoughts variant (Logeswaran and Lee, 2018) uses a multiple-choice objective for contextual sentences, over the more complicated decoder-based objective. Several supervised approaches to building representations also exist. An earlier work is CharaIntroduction In recent years, there has been a considerable amount of research into attempting to represent contexts longer than single words with fixedlength vectors. These represent"
W19-4318,tiedemann-2012-parallel,0,0.0221718,"our Wikipediaderived dataset (eng) utility1 . We use the Punkt tokeniser (Kiss and Strunk, 2006) to segment our Wikipedia dumps into discrete sentences. For Russian, which lacked a Punkt tokenisation model, we used the UDPipe (Straka and Strakov´a, 2017) toolkit to perform segmentation. 6.2 Mapping data For mapping our sentence representations, we were restricted by the availability of large parallel corpora we could use for our mapping procedure. We used two such corpora: the Europarl corpus (Koehn, 2005), a multilingual collection of European Parliament proceedings, and the MultiUN corpus (Tiedemann, 2012), a collection of translated documents from the United Nations. We used Europarl for the official EU languages we analysed: German and Spanish. For Russian, we used MultiUN. We used both corpora for French, to attempt to analyse what, if any, effect the mapping corpus would have. We also truncated our MultiUN cororpora to 2 million sentences, to keep the corpus size roughly equivalent to Europarl, and also due to time and resource constraints: mapping representations on the complete 10 million sentence corpus would have required significant amounts of time. Having segmented our data, we used t"
W19-4318,D16-1157,0,0.0219843,"obing-centric evaluations, and their extension to a multilingual domain. In Section 2, we provide a literature review of prior work in the numerous domains that our paper builds upon. Section 3 motivates the principle of cross-lingual probing and describes our 156 Proceedings of the 4th Workshop on Representation Learning for NLP (RepL4NLP-2019), pages 156–168 c Florence, Italy, August 2, 2019. 2019 Association for Computational Linguistics bedding transfer: the authors use a parallel corpus as a ‘seed dictionary’ to fit a transformation matrix between their source and target languages. gram (Wieting et al., 2016), which uses paraphrase data and builds on character representations to arrive at sentence representations. More recent papers use a diverse variety of target tasks to ground representations, such as visual data (Kiela et al., 2017), machine translation data (McCann et al., 2017), and even multiple tasks, in a multi-task learning framework (Subramanian et al., 2018). Relevant to this paper is Conneau et al.’s (2017a) InferSent, that uses natural language inference (NLI) data to ground representations: they learn these representations on the well-known SNLI dataset (Bowman et al., 2015). 2.2 2."
W19-4318,N18-1101,0,0.0238777,"e of languages could have been more typologically diverse, we were restricted by three factors: these probing tasks is that most of them were created with the idea of evaluating representations built for English language data. In this spirit, what we propose is analogous to Abdou et al.’s (2018) work on generating multilingual evaluation corpora for word representations. Within the realm of evaluating multilingual sentence representations, Conneau et al. (2018b) describe the XNLI dataset, a set of translations of the development and test portions of the multi-genre MultiNLI inference dataset (Williams et al., 2018). This, in a sense, is an extension of a predominantly monolingual task to the multilingual domain; the authors evaluate sentence representations derived by mapping nonEnglish representations to an English representation space. The original XNLI paper provides a baseline representation mapping technique, based on minimising the mean-squared error (MSE) loss between sentence representations across a parallel corpus. Their English language sentence representations are derived from an encoder trained on NLI data (Conneau et al., 2017a), and their target language representations are randomly initi"
W19-4318,N18-1202,0,0.0531238,"pkes et al., 2017; Belinkov et al., 2017), that would attempt to quantify precisely what sort of linguistic information was being retained by sentence representations. The authors, whose work focussed on evaluating representations for English, provided Spearman correlations between the performance of a particular representation mechanism on being probed for specific linguistic properties, and the downstream performance on a variety of NLP tasks. Along similar lines, and contemporaneously with this work, Liu et al. (2019) probe three pretrained contextualised word representation models – ELMo (Peters et al., 2018), BERT (Devlin et al., 2018) and the OpenAI transformer (Radford et al., 2018) – with a “suite of sixteen diverse probing tasks”. On a different note, Saphra and Lopez (2018) present a CCA-based method to compare representation learning dynamics across time and models, without explicitly requiring annotated probing corpora. They motivate the use of SVCCA (Raghu et al., 2017) to quantify precisely what an encoder learns by comparing the representations it generates with representations generated by an architecture trained specifically for a certain task, with the intuition that a higher similar"
W19-4318,N19-1329,0,0.0335263,"Missing"
W19-4318,K17-3009,0,0.0687333,"Missing"
W19-4318,P07-2045,0,\N,Missing
W19-4318,D16-1250,0,\N,Missing
W19-4318,W17-2619,0,\N,Missing
W19-4318,D17-1070,0,\N,Missing
W19-4318,P18-1198,0,\N,Missing
W19-4318,S19-1006,0,\N,Missing
W19-4724,N16-2002,0,0.0193565,"st cosine similarity to ˆi in Mn+1 is assumed to be a candidate for an insurRelated work The issue of linguistic regularity manifested in relational similarity has been studied for a long time. Due to the long-standing criticism of strictly binary relation structure, SemEval-2012 offered the task to detect the degree of relational similarity (Jurgens et al., 2012). This means that multiple correct answers exist, but they should be ranked differently. Somewhat similar improvements to the well-known word analogies dataset from (Mikolov et al., 2013b) were presented in the BATS analogy test set (Gladkova et al., 2016), also featuring multiple correct answers.1 Our One-to-X analogy setup extends this by introducing the possibility of the correct answer being ’None’. In the cases when correct answers exist, they are equally ranked, but their number can be different. Using distributional word representations to trace diachronic semantic shifts (including those reflecting social and cultural events) has received substantial attention in the recent years. Our work shares some of the workflow with Kutuzov et al. (2017). They used a supervised approach to analogical reasoning, applying ‘semantic directions’ learn"
W19-4724,D16-1229,0,0.0302918,"inference with word embeddings in general in (Rogers et al., 2017). 197 gent armed group active in this location in the time period n + 1; however, a more involved approach is needed to handle cases when the number of insurgents (correct answers) can be different from 1 (including 0), described below. For this workflow to yield meaningful results, it is essential for the paired models to be ‘aligned’. This is why we train the models incrementally, thus ensuring that they share common structural properties. Another possible way to cope with this is by using the orthogonal Procrustes alignment (Hamilton et al., 2016). 3 Time span Locations Insurgents Conflict pairs New pairs share Conflict locations share Insurgents per location 1995–2010 52 127 136 0.37 0.46 1.65 2010–2017 42 78 102 0.39 0.56 1.50 &quot;Islamic State&quot;]’). Entities occurring less than 25 times in the corresponding yearly corpora were filtered out, since it is difficult for distributional models to learn meaningful embeddings for such rare words. We create one such conflict relation dataset for each news corpus; one corresponding to the time span of NOW and another for Gigaword. Table 1 shows various statistics across these UCDP subsets, includ"
W19-4724,S12-1047,0,0.178355,"such task is demonstrated, using a threshold based on a function of cosine distance to decrease the number of false positives; this approach is shown to be beneficial on two different corpora. Finally, we publish a readyto-use test set for one-to-X analogy evaluation on historical armed conflicts data. Performance on the task of analogical inference (or ‘word analogies’) is one of the most widespread means to evaluate distributional word representation models, with ‘KING is to QUEEN as MAN is to ? (WOMAN)’ being a famous example. It also has deep connections to the relational similarity task (Jurgens et al., 2012). Most often, analogical inference is formulated as a strict proportion, and the model has to provide exactly one best answer for each question (assuming that it is impossible that, e.g., WOMAN and GIRL are equally correct answers for the question above). We reformulate the analogical inference task and extend it to include multiple-ended or one-toX relations: one-to-one, one-to-many and one-tonone cases when an entity is not included in this particular relation type, so there is no correct answer for it. This way, the model has to provide as many correct answers as possible, while providing a"
W19-4724,C18-1117,1,0.843621,"arned on the previous year’s armed conflicts data to the subsequent year. We extend their research by significantly reformulating the analogy task, making it more realistic, and finding ways to cope with false positives (insurgent armed groups predicted for locations where no armed conflicts are registered this year). In comparison to their work, we also use newer and larger corpora of news texts and the most recent version of the UCDP dataset. For brevity, we do not describe the emerging field of diachronic word embeddings in details, referring the interested readers to the recent surveys of Kutuzov et al. (2018) and Tang (2018). 2 The model Mn+1 is initialized with the weights from the model Mn ; if there are new words in the n + 1 data which exceed the frequency threshold, then at the start of Mn+1 training they are added to it and assigned random vectors. 3 A similar approach has been used for naive translation of words from the language L1 to L2 by using monolingual word embeddings for both and a seed bilingual dictionary (set of one-to-one pairs) (Mikolov et al., 2013a). 1 See also the detailed criticism of analogical inference with word embeddings in general in (Rogers et al., 2017). 197 gent ar"
W19-4724,D17-1194,1,0.904717,"Missing"
W19-4724,W18-4501,0,0.0288729,"hese counts are accumulated and for each year standard precision, recall and F1 score are calculated. These metrics are then averaged across all years in the test set. Using false negatives ensures that we penalize the systems for yielding predictions for peaceful locations. Precision Recall F1 Baseline Threshold 0.19 0.46 0.51 0.41 0.28 0.41 Baseline Threshold 0.26 0.42 0.53 0.41 0.34 0.41 Table 3: Average diachronic performance Cosine threshold It is clear that such a system (dubbed ‘baseline’) will always yield k incorrect candidates for peaceful areas. Inspired partially by the ideas from Orlikowski et al. (2018), we implemented a simple approach based on the assumption that the correct armed groups vectors will tend to be closer to the ˆi point than other nearest neighbours. Thus, the system should pick only the candidates located within a hypersphere of a pre-defined radius r centered around ˆi. rn can be different for different years, and we infer it from the p training conflict pairs from the previous year by calculating the average cosine distance between the ‘armed conflict projections’ ˆi and armed groups: p   1X r= cos ˆip , gp + σ p Algorithm (1) p=0 199 where gp is the armed group in the p"
W19-4724,S17-1017,0,0.0355908,"Missing"
W19-4724,N18-1044,0,0.0610853,"Missing"
W19-4724,K17-3009,0,0.0478494,"Missing"
W19-4725,D17-1118,0,0.0966717,"o confirm or reject the hypothesis that evaluative adjectives are less stable than other words of the same part of speech. We want to find evidence across all three languages under analysis. We also would like to control for frequency and to exclude its influence on the results, since it is known that word frequency often correlates with the speed of semantic change (Hamilton et al., 2016c) 9 . We measure the speed of semantic changes using a variety of methods: 1. Jaccard distance (Jaccard, 1901) between sets of 10 nearest neighbours of one word (by 9 Note, however, that this was disputed in Dubossarsky et al. (2017). 205 Figure 2: Alterations in meaning of the Russian adjective ‘бескомпромиссный’ (uncompromising): from ruthless over fanatical, passion, later conviction, heroic to intransigence, confrontation Method English Norwegian Russian # fillers Freq diff 8994 0.00001 3989 0.00003 7535 0.00001 Jaccard Procrustes GlobAnchors Mean pairwise distances -11.08 -4 -15.05 -15.52 -5.04 -12.01 11.91 -4.40 12.62 Jaccard Procrustes GlobAnchors Mean deltas from 1960s 3.28 0 0 2.98 0 3.92 3.57 3.24 3.11 Table 2: Difference in the intensity of semantic shifts between evaluative adjectives and fillers. Positive val"
W19-4725,W17-0237,1,0.827637,"consideration. 3.2 Word embeddings Continuous bag-of-words (CBOW) embedding models (Mikolov et al., 2013b) were trained on each decade for each of the three languages. All the models share the same set of hyperparameters: vector size 300, symmetric context window size 3, and 10 iterations over the corpus. We discarded all the words which occurred less than 5 times in the training corpus, and additionally limited the maximum vocabulary size to be 100 000, more or less following the hyperparameters from Kutuzov et al. (2017a). The models are made available via the NLPL word vector repository6 (Fares et al., 2017). 3.3 Evaluative adjectives lists In order to find out whether evaluative adjective are more prone to diachronic semantic shifts, we need an authoritative source providing us with a list of such adjectives, more than only several words in size. Unfortunately, even for English 4 7 Available at https://www.cs.uic.edu/~liub/ FBS/sentiment-analysis.html 8 https://www.nb.no/sprakbanken/show? serial=sbr-9&lang=nb https://www.nb.no/sprakbanken/show? serial=oai:nb.no:sbr-43&lang=en 5 http://ruscorpora.ru/en/ 6 http://vectors.nlpl.eu/repository/ 204 ally filtered to remove non-evaluative adjectives. Th"
W19-4725,D16-1057,0,0.0863679,"corpora and word lists we relied upon. Our experiments are described in 4. In sections 5 and 6 we outline the limitations of the presented research, our plans for the future, and conclude. 1 See Appendix A for details on visualisation 202 Proceedings of the 1st International Workshop on Computational Approaches to Historical Language Change, pages 202–209 c Florence, Italy, August 2, 2019. 2019 Association for Computational Linguistics 2 Related work from models trained on the previous time bin (‘incremental training’) (Kim et al., 2014); Procrustes alignment of independent embedding models (Hamilton et al., 2016c); dynamic models trained across all time bins at once (Bamler and Mandt, 2017; Yao et al., 2018; Rosenfeld and Erk, 2018); Global Anchors (measuring the vectors of words’ similarities to other words) (Yin et al., 2018), etc. In this paper, we employ Procrustes alignment and the Global Anchors methods, applying them to the task of measuring the speed of semantic shifts of evaluative adjectives across time. An important publication related to our work is Hamilton et al. (2016a). In it, the authors induce historical sentiment lexicons from English corpora (using word embeddings, among other met"
W19-4725,P16-1141,0,0.355549,"corpora and word lists we relied upon. Our experiments are described in 4. In sections 5 and 6 we outline the limitations of the presented research, our plans for the future, and conclude. 1 See Appendix A for details on visualisation 202 Proceedings of the 1st International Workshop on Computational Approaches to Historical Language Change, pages 202–209 c Florence, Italy, August 2, 2019. 2019 Association for Computational Linguistics 2 Related work from models trained on the previous time bin (‘incremental training’) (Kim et al., 2014); Procrustes alignment of independent embedding models (Hamilton et al., 2016c); dynamic models trained across all time bins at once (Bamler and Mandt, 2017; Yao et al., 2018; Rosenfeld and Erk, 2018); Global Anchors (measuring the vectors of words’ similarities to other words) (Yin et al., 2018), etc. In this paper, we employ Procrustes alignment and the Global Anchors methods, applying them to the task of measuring the speed of semantic shifts of evaluative adjectives across time. An important publication related to our work is Hamilton et al. (2016a). In it, the authors induce historical sentiment lexicons from English corpora (using word embeddings, among other met"
W19-4725,W14-2517,0,0.369817,"s work in the context of previous research. In section 3, we describe the corpora and word lists we relied upon. Our experiments are described in 4. In sections 5 and 6 we outline the limitations of the presented research, our plans for the future, and conclude. 1 See Appendix A for details on visualisation 202 Proceedings of the 1st International Workshop on Computational Approaches to Historical Language Change, pages 202–209 c Florence, Italy, August 2, 2019. 2019 Association for Computational Linguistics 2 Related work from models trained on the previous time bin (‘incremental training’) (Kim et al., 2014); Procrustes alignment of independent embedding models (Hamilton et al., 2016c); dynamic models trained across all time bins at once (Bamler and Mandt, 2017; Yao et al., 2018; Rosenfeld and Erk, 2018); Global Anchors (measuring the vectors of words’ similarities to other words) (Yin et al., 2018), etc. In this paper, we employ Procrustes alignment and the Global Anchors methods, applying them to the task of measuring the speed of semantic shifts of evaluative adjectives across time. An important publication related to our work is Hamilton et al. (2016a). In it, the authors induce historical se"
W19-4725,P14-1096,0,0.0239405,"ms of semantic change or appear to actually be less prone to shifting (particularly, to ‘jitter’-type shifting). Thus, in spite of many well-known examples of semantically changing evaluative adjectives (like ‘terrific’ or ‘incredible’), it seems that such cases are not specific to this particular type of words. 1 Figure 1: Alterations in meaning of the English adjective ‘monumental’: from sculptures in the sixties to awesome in the 2000s monumental from the 60s to the 2000s (Figure 1)1 or how the word sick slowly acquires a (colloquial) evaluative sense (‘That’s sick, dude!’) as described in Mitra et al. (2014). On the other hand, intuitively, evaluative adjectives are naturally prone to amelioration and pejoration as major types of diachronic semantic shifts. One can immediately recall, for example, the English words incredible and terrific which underwent amelioration and started to denote positive instead of negative qualities. Introduction Words change their meaning over time. It has become widespread recently to trace such shifts using word embedding models (that is, using contextual cues from raw corpora). However, most of this research is centred on the English language, and focuses on nouns"
W19-4725,C18-1117,1,0.847696,"im et al., 2014; Hamilton et al., 2016c; Liao and Cheng, 2016; Kutuzov et al., 2017b,a; Rosenfeld and Erk, 2018). The main reason for this is the powerful abilities of such approaches to model word meaning based solely on non-annotated corpora. Additionally, vector representations of words allow for easy calculation of their similarities and changes. The baseline method here consists of simply training embedding models on the texts created in different time periods, and then comparing the vector representations for the same words. For further information on the current state of the field, see Kutuzov et al. (2018) and Tang (2018). One of the difficulties brought by these approaches is the necessity to somehow ‘align’ the vector spaces trained on different time bins (time periods). A variety of methods have been proposed to overcome this. They include initialising the models for each time bin with the weights 3 Data In this section, we describe our data: the corpora employed to train word embedding models, and the sentiment lexicons serving as the source of evaluative adjectives. 3.1 Corpora For the purposes of our research, we employed corpora in three languages, selecting texts which were created duri"
W19-4725,N18-1044,0,0.216997,"ns of the presented research, our plans for the future, and conclude. 1 See Appendix A for details on visualisation 202 Proceedings of the 1st International Workshop on Computational Approaches to Historical Language Change, pages 202–209 c Florence, Italy, August 2, 2019. 2019 Association for Computational Linguistics 2 Related work from models trained on the previous time bin (‘incremental training’) (Kim et al., 2014); Procrustes alignment of independent embedding models (Hamilton et al., 2016c); dynamic models trained across all time bins at once (Bamler and Mandt, 2017; Yao et al., 2018; Rosenfeld and Erk, 2018); Global Anchors (measuring the vectors of words’ similarities to other words) (Yin et al., 2018), etc. In this paper, we employ Procrustes alignment and the Global Anchors methods, applying them to the task of measuring the speed of semantic shifts of evaluative adjectives across time. An important publication related to our work is Hamilton et al. (2016a). In it, the authors induce historical sentiment lexicons from English corpora (using word embeddings, among other methods). They further show that amelioration and pejoration do occur on a massive scale: many evaluative adjectives in Englis"
W19-4725,D17-1194,1,0.932333,"the strongest and most wide-spread. As the amount of language data available to computational linguistics increased,2 the focus of research interest moved from theoretical reasoning about the nature of semantic shifts to more empirical approaches, mainly based on corpus-based analysis (see Michel et al. (2011) and Jatowt and Duh (2014), among many others). Recently, the usage of pre-trained word embeddings (Bengio et al., 2003; Mikolov et al., 2013a) has become widespread in the publications related to diachronic semantic shifts (Kim et al., 2014; Hamilton et al., 2016c; Liao and Cheng, 2016; Kutuzov et al., 2017b,a; Rosenfeld and Erk, 2018). The main reason for this is the powerful abilities of such approaches to model word meaning based solely on non-annotated corpora. Additionally, vector representations of words allow for easy calculation of their similarities and changes. The baseline method here consists of simply training embedding models on the texts created in different time periods, and then comparing the vector representations for the same words. For further information on the current state of the field, see Kutuzov et al. (2018) and Tang (2018). One of the difficulties brought by these app"
W19-4725,K17-3009,0,0.0137836,"roposed to overcome this. They include initialising the models for each time bin with the weights 3 Data In this section, we describe our data: the corpora employed to train word embedding models, and the sentiment lexicons serving as the source of evaluative adjectives. 3.1 Corpora For the purposes of our research, we employed corpora in three languages, selecting texts which were created during the five decades from 1960s to 2000s. We lemmatized (it was especially important for Russian with its rich morphology) and POS-tagged all the corpora ourselves, using the corresponding UDPipe models (Straka and Straková, 2017). For English data, we used The Corpus of Historical American English (COHA).3 This is a corpus of English texts annotated with creation dates and balanced by genres. It is composed of fiction, magazine and newspaper articles, as well as nonfiction texts. 2 For example, the Google Ngrams (https://books. google.com/ngrams) service stimulated diachronic research of texts and language greatly. 3 203 https://www.english-corpora.org/coha/ Decade 1960s 1970s 1980s 1990s 2000s English Norwegian Russian 12 12 13 14.5 15 6 21 25.5 40.5 21 10 10 9 20 39.5 such a list is hard to find in the published wor"
W19-4725,W17-2705,1,0.93308,"the strongest and most wide-spread. As the amount of language data available to computational linguistics increased,2 the focus of research interest moved from theoretical reasoning about the nature of semantic shifts to more empirical approaches, mainly based on corpus-based analysis (see Michel et al. (2011) and Jatowt and Duh (2014), among many others). Recently, the usage of pre-trained word embeddings (Bengio et al., 2003; Mikolov et al., 2013a) has become widespread in the publications related to diachronic semantic shifts (Kim et al., 2014; Hamilton et al., 2016c; Liao and Cheng, 2016; Kutuzov et al., 2017b,a; Rosenfeld and Erk, 2018). The main reason for this is the powerful abilities of such approaches to model word meaning based solely on non-annotated corpora. Additionally, vector representations of words allow for easy calculation of their similarities and changes. The baseline method here consists of simply training embedding models on the texts created in different time periods, and then comparing the vector representations for the same words. For further information on the current state of the field, see Kutuzov et al. (2018) and Tang (2018). One of the difficulties brought by these app"
W19-4725,L16-1186,0,0.047259,"Missing"
W19-4802,W14-2609,0,0.0122035,"ably stay in the shadow of its two older, more accessible Qatsi siblings” are difficult for sentiment classifiers that do not model this phenomenon explicitly. Modality None of the state-of-the-art sentiment systems deals explicitly with modality (38 total errors). While in many of the examples modality does not express a different sentiment than the same sentence without modality, in the dataset there are examples that do, e. g., “Still, I thought it could have been more.” Sarcasm/Irony Sarcasm and irony (58 errors), which are often treated separately from sentiment analysis (Filatova, 2012; Barbieri et al., 2014), are present mainly in negative and strong negative examples in the dataset. Correctly capturing sarcasm and irony is necessary to classify some negative and strong negative examples, e. g., “If Melville is creatively a great whale, this film is canned tuna.” Morphology While not the most prominent label (31 errors), the examples in the dataset that contain morphological features that effect sentiment are normally strong positive or strong negative. This most often contains creative use of English morphology, e. g., “It was fan-freakin-tastic!” or “It’s hyper-cliched”. Shifters Shifters (50 e"
W19-4802,L18-1104,1,0.898888,"Missing"
W19-4802,W17-5202,1,0.790875,"bi-attentive sentiment network currently give state-of-the-art results (Peters et al., 2018). T¨ackstr¨om dataset The T¨ackstr¨om dataset (T¨ackstr¨om and McDonald, 2011) contains product reviews which have been annotated at both document- and sentence-level for three-class sentiment, although the sentence-level annotations also have a “not relevant” label. We keep the sentencelevel annotations, which gives 3,662 sentences annotated for three-class sentiment. BiLSTM Bidirectional long short-term memory (BiLSTM) networks have shown to be strong baselines for sentiment tasks (Tai et al., 2015; Barnes et al., 2017). We implement a single-layered BiLSTM which takes pretrained skipgram embeddings as input, creates a sentence representation by concatenating the final hidden layer of both left and right LSTMs, and then passes this representation to a softmax layer for classification. Additionally, dropout serves as a regularizer. Thelwall dataset The Thelwall dataset derives from datasets provided with SentiStrength2 (Thelwall et al., 2010). It contains microblogs annotated for both positive and negative sentiment on a scale from 1 to 5. We map these to single sentiment labels such that sentences which are"
W19-4802,W12-3802,0,0.0218494,"ata corpus, the overall frequency of negation words and shifters is likely similar. This suggests that there is a Zipfian tail of shifters which are not often handled within sentiment analysis. Furthermore, the linguistic phenomenon of modality has also been shown to be problematic. Both Narayanan et al. (2009) and Liu et al. (2014) explore the effect of modality on sentiment classification and find that explicitly modeling certain modalities improves classification results. They advocate for a divide-and-conquer approach, which would address the various realizations of modality individually. Benamara et al. (2012) perform linguistic experiments using native speakers concerning the effects of both negation and modality on opinions, and similarly find that the type of negation and modality determines the final interpretation of polarity. The sentiment models inspected in these analyses, however, were lexicon- and word- and nTotal MPQA OP. Sem. SST Ta. Th. − 193 527 413 − 379 879 − 399 74 − 3,499 4,478 1,310 − 1,852 3,111 2,242 3,140 1,510 − 923 1,419 1,320 − − 2,727 1,779 1,828 − 1,133 1,731 9,287 11,855 3,662 6,334 Table 1: Statistics for the sentence-level annotations in each dataset. gram-based models"
W19-4802,D08-1083,0,0.0538621,"te (Park et al., 2018; Zhao et al., 2018; Kiritchenko and Mohammad, 2018). Specifically within the task of sentiment analysis, certain linguistic phenomena are known to be challenging. Negation is one of the aspects of language that most clearly affects expressions of sentiment and that has been studied widely within sentiment analysis (see Wiegand et al. (2010) for an early survey). The difficulties of resolving negation for sentiment analysis include determining negation scope (Hogenboom et al., 2011; Lapponi et al., 2012; Reitan et al., 2015), and semantic composition (Wilson et al., 2005; Choi and Cardie, 2008; Kiritchenko and Mohammad, 2016). Verbal polarity shifters have also been studied. Schulder et al. (2018) annotate verbal shifters at the sense-level. They conclude that, although individual negation words are more frequent in the Amazon Product Review Data corpus, the overall frequency of negation words and shifters is likely similar. This suggests that there is a Zipfian tail of shifters which are not often handled within sentiment analysis. Furthermore, the linguistic phenomenon of modality has also been shown to be problematic. Both Narayanan et al. (2009) and Liu et al. (2014) explore th"
W19-4802,W18-6219,0,0.015025,"atasets. Additionally, we use a bag-of-words model as it is a strong baseline for text classification. For the S INGLE setup, we train all models on the training and development data for each dataset and test on the corresponding test set, therefore avoiding domain problems. OpeNER The Open Polarity Enhanced Named Entity Recognition (OpeNER) sentiment datasets (Agerri et al., 2013) contain hotel reviews annotated for 4-class (strong positive, positive, negative, strong negative) sentiment classification. We take the English dataset, where self-attention networks give state-of-the-art results (Ambartsoumian and Popowich, 2018). BERT The BERT model (Devlin et al., 2018) is a bidirectional transformer that is pretrained on two tasks: 1) a cloze-like language modeling task and 2) a binary next-sentence prediction task. It is pretrained on 330 million words from the BooksCorpus (Zhu et al., 2015) and English Wikipedia. We fine-tune the available pretrained model3 on each sentiment dataset. SemEval The SemEval 2013 tweet classification dataset (Nakov et al., 2013) contains tweets collected and annotated for three-class (positive, neutral, negative) sentiment. The state-of-the-art model is a Convolutional Network (Severy"
W19-4802,P18-2006,0,0.0264231,"ence, Italy, August 1, 2019. 2019 Association for Computational Linguistics 2 Related work Label ++ + 0 − −− Neural networks are now ubiquitous in NLP tasks, often giving state-of-the-art results. However, they are known for being “black boxes” which are not easily interpretable. Recent interest in interpreting these methods has led to new lines of research which attempt to discover what linguistic phenomena neural networks are able to learn (Linzen et al., 2016; Gulordava et al., 2018; Conneau et al., 2018), how robust neural networks are to perturbations in input data (Ribeiro et al., 2018; Ebrahimi et al., 2018; Schluter and Varab, 2018), and what biases they propagate (Park et al., 2018; Zhao et al., 2018; Kiritchenko and Mohammad, 2018). Specifically within the task of sentiment analysis, certain linguistic phenomena are known to be challenging. Negation is one of the aspects of language that most clearly affects expressions of sentiment and that has been studied widely within sentiment analysis (see Wiegand et al. (2010) for an early survey). The difficulties of resolving negation for sentiment analysis include determining negation scope (Hogenboom et al., 2011; Lapponi et al., 2012; Reitan et al"
W19-4802,filatova-2012-irony,0,0.0370513,"like “Will probably stay in the shadow of its two older, more accessible Qatsi siblings” are difficult for sentiment classifiers that do not model this phenomenon explicitly. Modality None of the state-of-the-art sentiment systems deals explicitly with modality (38 total errors). While in many of the examples modality does not express a different sentiment than the same sentence without modality, in the dataset there are examples that do, e. g., “Still, I thought it could have been more.” Sarcasm/Irony Sarcasm and irony (58 errors), which are often treated separately from sentiment analysis (Filatova, 2012; Barbieri et al., 2014), are present mainly in negative and strong negative examples in the dataset. Correctly capturing sarcasm and irony is necessary to classify some negative and strong negative examples, e. g., “If Melville is creatively a great whale, this film is canned tuna.” Morphology While not the most prominent label (31 errors), the examples in the dataset that contain morphological features that effect sentiment are normally strong positive or strong negative. This most often contains creative use of English morphology, e. g., “It was fan-freakin-tastic!” or “It’s hyper-cliched”."
W19-4802,S18-2005,0,0.0325543,"orks are now ubiquitous in NLP tasks, often giving state-of-the-art results. However, they are known for being “black boxes” which are not easily interpretable. Recent interest in interpreting these methods has led to new lines of research which attempt to discover what linguistic phenomena neural networks are able to learn (Linzen et al., 2016; Gulordava et al., 2018; Conneau et al., 2018), how robust neural networks are to perturbations in input data (Ribeiro et al., 2018; Ebrahimi et al., 2018; Schluter and Varab, 2018), and what biases they propagate (Park et al., 2018; Zhao et al., 2018; Kiritchenko and Mohammad, 2018). Specifically within the task of sentiment analysis, certain linguistic phenomena are known to be challenging. Negation is one of the aspects of language that most clearly affects expressions of sentiment and that has been studied widely within sentiment analysis (see Wiegand et al. (2010) for an early survey). The difficulties of resolving negation for sentiment analysis include determining negation scope (Hogenboom et al., 2011; Lapponi et al., 2012; Reitan et al., 2015), and semantic composition (Wilson et al., 2005; Choi and Cardie, 2008; Kiritchenko and Mohammad, 2016). Verbal polarity s"
W19-4802,N18-1108,0,0.0244808,"ntiment. 12 Proceedings of the Second BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, pages 12–23 c Florence, Italy, August 1, 2019. 2019 Association for Computational Linguistics 2 Related work Label ++ + 0 − −− Neural networks are now ubiquitous in NLP tasks, often giving state-of-the-art results. However, they are known for being “black boxes” which are not easily interpretable. Recent interest in interpreting these methods has led to new lines of research which attempt to discover what linguistic phenomena neural networks are able to learn (Linzen et al., 2016; Gulordava et al., 2018; Conneau et al., 2018), how robust neural networks are to perturbations in input data (Ribeiro et al., 2018; Ebrahimi et al., 2018; Schluter and Varab, 2018), and what biases they propagate (Park et al., 2018; Zhao et al., 2018; Kiritchenko and Mohammad, 2018). Specifically within the task of sentiment analysis, certain linguistic phenomena are known to be challenging. Negation is one of the aspects of language that most clearly affects expressions of sentiment and that has been studied widely within sentiment analysis (see Wiegand et al. (2010) for an early survey). The difficulties of resol"
W19-4802,Q16-1037,0,0.145829,"essing and probing sentiment. 12 Proceedings of the Second BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, pages 12–23 c Florence, Italy, August 1, 2019. 2019 Association for Computational Linguistics 2 Related work Label ++ + 0 − −− Neural networks are now ubiquitous in NLP tasks, often giving state-of-the-art results. However, they are known for being “black boxes” which are not easily interpretable. Recent interest in interpreting these methods has led to new lines of research which attempt to discover what linguistic phenomena neural networks are able to learn (Linzen et al., 2016; Gulordava et al., 2018; Conneau et al., 2018), how robust neural networks are to perturbations in input data (Ribeiro et al., 2018; Ebrahimi et al., 2018; Schluter and Varab, 2018), and what biases they propagate (Park et al., 2018; Zhao et al., 2018; Kiritchenko and Mohammad, 2018). Specifically within the task of sentiment analysis, certain linguistic phenomena are known to be challenging. Negation is one of the aspects of language that most clearly affects expressions of sentiment and that has been studied widely within sentiment analysis (see Wiegand et al. (2010) for an early survey). T"
W19-4802,P18-1031,0,0.0334611,"Missing"
W19-4802,L18-1547,0,0.0203621,"approach to creating a dataset is also easily transferable to other tasks which are affected by linguistic or paralinguistic phenomena, such as hate speech detection or sarcasm detection. It would be more useful to have some knowledge of the phenomena that could affect the task beforehand, but a careful error analysis can also lead to insights which can be translated into annotation labels. Regarding ways of moving forward, there are already many sources of data for the linguistic phenomena we have analyzed in this work, ranging from datasets annotated for negation (Morante and Blanco, 2012; Liu et al., 2018), irony (Van Hee et al., 2018), emoji (Barbieri et al., 2018), as well as datasets for idioms (Muzny and Zettlemoyer, 2013) and their relationship with sentiment (Jochim et al., 2018). We believe that discovering ways to explicitly incorporate this available information into state-of-the-art sentiment models may provide a way to improve current approaches. Multi-task learning (Caruana, 1993) and transfer learning (Peters et al., 2018; Devlin et al., 2018; Howard and Ruder, 2018) have shown promise in this respect, but have not been exploited for improving sentiment classification with regards"
W19-4802,S13-2053,0,0.0202954,"3) the negative label, and clearly neutral sentences ( 3 &lt; pos > 2 and 3 &lt; neg > 2) the neutral. We discard all other sentences, which finally leaves 6,334 annotated sentences. 2 The data are available sentistrength.wlv.ac.uk/ at Models Bag-of-Words classifier Finally, bag-of-words classifiers are strong baselines for sentiment and when combined with other features can still give 3 https://github.com/google-research/ bert 4 https://s3-us-west-2.amazonaws.com/ allennlp/models/sst-5-elmo-biattentiveclassification-network-2018.09.04.tar.gz http:// 14 state-of-the-art results for sentiment tasks (Mohammad et al., 2013). Therefore, we train a Linear SVM on a bag-of-words representation of the training sentences. 3.3 during the error annotation process. We further chose to manually annotate for the polarity of the sentence irrespective of the gold label in order to be able to locate possible annotation errors during our analysis. The annotation scheme and (manually constructed) examples of each label are shown in Table 6. Note that we did not limit the number of labels that the annotator could assign to each sentence and in principle they should assign all suitable labels during annotation. Model performance"
W19-4802,L18-1379,0,0.0518807,"Missing"
W19-4802,S12-1035,0,0.0156704,"uages. We expect that this approach to creating a dataset is also easily transferable to other tasks which are affected by linguistic or paralinguistic phenomena, such as hate speech detection or sarcasm detection. It would be more useful to have some knowledge of the phenomena that could affect the task beforehand, but a careful error analysis can also lead to insights which can be translated into annotation labels. Regarding ways of moving forward, there are already many sources of data for the linguistic phenomena we have analyzed in this work, ranging from datasets annotated for negation (Morante and Blanco, 2012; Liu et al., 2018), irony (Van Hee et al., 2018), emoji (Barbieri et al., 2018), as well as datasets for idioms (Muzny and Zettlemoyer, 2013) and their relationship with sentiment (Jochim et al., 2018). We believe that discovering ways to explicitly incorporate this available information into state-of-the-art sentiment models may provide a way to improve current approaches. Multi-task learning (Caruana, 1993) and transfer learning (Peters et al., 2018; Devlin et al., 2018; Howard and Ruder, 2018) have shown promise in this respect, but have not been exploited for improving sentiment classific"
W19-4802,N18-1171,0,0.044734,"Missing"
W19-4802,D13-1145,0,0.075092,"Missing"
W19-4802,W16-0410,0,0.0204991,"Zhao et al., 2018; Kiritchenko and Mohammad, 2018). Specifically within the task of sentiment analysis, certain linguistic phenomena are known to be challenging. Negation is one of the aspects of language that most clearly affects expressions of sentiment and that has been studied widely within sentiment analysis (see Wiegand et al. (2010) for an early survey). The difficulties of resolving negation for sentiment analysis include determining negation scope (Hogenboom et al., 2011; Lapponi et al., 2012; Reitan et al., 2015), and semantic composition (Wilson et al., 2005; Choi and Cardie, 2008; Kiritchenko and Mohammad, 2016). Verbal polarity shifters have also been studied. Schulder et al. (2018) annotate verbal shifters at the sense-level. They conclude that, although individual negation words are more frequent in the Amazon Product Review Data corpus, the overall frequency of negation words and shifters is likely similar. This suggests that there is a Zipfian tail of shifters which are not often handled within sentiment analysis. Furthermore, the linguistic phenomenon of modality has also been shown to be problematic. Both Narayanan et al. (2009) and Liu et al. (2014) explore the effect of modality on sentiment"
W19-4802,S13-2052,0,0.0424662,", negative, strong negative) sentiment classification. We take the English dataset, where self-attention networks give state-of-the-art results (Ambartsoumian and Popowich, 2018). BERT The BERT model (Devlin et al., 2018) is a bidirectional transformer that is pretrained on two tasks: 1) a cloze-like language modeling task and 2) a binary next-sentence prediction task. It is pretrained on 330 million words from the BooksCorpus (Zhu et al., 2015) and English Wikipedia. We fine-tune the available pretrained model3 on each sentiment dataset. SemEval The SemEval 2013 tweet classification dataset (Nakov et al., 2013) contains tweets collected and annotated for three-class (positive, neutral, negative) sentiment. The state-of-the-art model is a Convolutional Network (Severyn and Moschitti, 2015). ELMo We use the bi-attentive classification network4 from Peters et al. (2018). The network uses both word embeddings, as well as creating character-based embeddings from a character-level CNN-BiLSTM network. The word representations are first passed through a feedforward layer, and then through a sequence-to-sequence network with biattention. This new representation of the text is combined with the original repre"
W19-4802,S15-2079,0,0.0465899,"Missing"
W19-4802,D09-1019,0,0.0262923,"composition (Wilson et al., 2005; Choi and Cardie, 2008; Kiritchenko and Mohammad, 2016). Verbal polarity shifters have also been studied. Schulder et al. (2018) annotate verbal shifters at the sense-level. They conclude that, although individual negation words are more frequent in the Amazon Product Review Data corpus, the overall frequency of negation words and shifters is likely similar. This suggests that there is a Zipfian tail of shifters which are not often handled within sentiment analysis. Furthermore, the linguistic phenomenon of modality has also been shown to be problematic. Both Narayanan et al. (2009) and Liu et al. (2014) explore the effect of modality on sentiment classification and find that explicitly modeling certain modalities improves classification results. They advocate for a divide-and-conquer approach, which would address the various realizations of modality individually. Benamara et al. (2012) perform linguistic experiments using native speakers concerning the effects of both negation and modality on opinions, and similarly find that the type of negation and modality determines the final interpretation of polarity. The sentiment models inspected in these analyses, however, were"
W19-4802,W02-1011,0,0.0555453,"Missing"
W19-4802,D13-1170,0,0.0282932,"Missing"
W19-4802,D18-1302,0,0.0212489,"ted work Label ++ + 0 − −− Neural networks are now ubiquitous in NLP tasks, often giving state-of-the-art results. However, they are known for being “black boxes” which are not easily interpretable. Recent interest in interpreting these methods has led to new lines of research which attempt to discover what linguistic phenomena neural networks are able to learn (Linzen et al., 2016; Gulordava et al., 2018; Conneau et al., 2018), how robust neural networks are to perturbations in input data (Ribeiro et al., 2018; Ebrahimi et al., 2018; Schluter and Varab, 2018), and what biases they propagate (Park et al., 2018; Zhao et al., 2018; Kiritchenko and Mohammad, 2018). Specifically within the task of sentiment analysis, certain linguistic phenomena are known to be challenging. Negation is one of the aspects of language that most clearly affects expressions of sentiment and that has been studied widely within sentiment analysis (see Wiegand et al. (2010) for an early survey). The difficulties of resolving negation for sentiment analysis include determining negation scope (Hogenboom et al., 2011; Lapponi et al., 2012; Reitan et al., 2015), and semantic composition (Wilson et al., 2005; Choi and Cardie, 2008"
W19-4802,P11-2100,0,0.0820226,"Missing"
W19-4802,N18-1202,0,0.0353429,"pretrained on two tasks: 1) a cloze-like language modeling task and 2) a binary next-sentence prediction task. It is pretrained on 330 million words from the BooksCorpus (Zhu et al., 2015) and English Wikipedia. We fine-tune the available pretrained model3 on each sentiment dataset. SemEval The SemEval 2013 tweet classification dataset (Nakov et al., 2013) contains tweets collected and annotated for three-class (positive, neutral, negative) sentiment. The state-of-the-art model is a Convolutional Network (Severyn and Moschitti, 2015). ELMo We use the bi-attentive classification network4 from Peters et al. (2018). The network uses both word embeddings, as well as creating character-based embeddings from a character-level CNN-BiLSTM network. The word representations are first passed through a feedforward layer, and then through a sequence-to-sequence network with biattention. This new representation of the text is combined with the original representation and passed through another sequence-to-sequence network. Finally, a max, min, mean and self-attention pool representation is created from this last sequence. For classification, these features are sent to a maxout layer. Stanford Sentiment Treebank Th"
W19-4802,P15-1150,0,0.0262008,"ns combined with a bi-attentive sentiment network currently give state-of-the-art results (Peters et al., 2018). T¨ackstr¨om dataset The T¨ackstr¨om dataset (T¨ackstr¨om and McDonald, 2011) contains product reviews which have been annotated at both document- and sentence-level for three-class sentiment, although the sentence-level annotations also have a “not relevant” label. We keep the sentencelevel annotations, which gives 3,662 sentences annotated for three-class sentiment. BiLSTM Bidirectional long short-term memory (BiLSTM) networks have shown to be strong baselines for sentiment tasks (Tai et al., 2015; Barnes et al., 2017). We implement a single-layered BiLSTM which takes pretrained skipgram embeddings as input, creates a sentence representation by concatenating the final hidden layer of both left and right LSTMs, and then passes this representation to a softmax layer for classification. Additionally, dropout serves as a regularizer. Thelwall dataset The Thelwall dataset derives from datasets provided with SentiStrength2 (Thelwall et al., 2010). It contains microblogs annotated for both positive and negative sentiment on a scale from 1 to 5. We map these to single sentiment labels such tha"
W19-4802,W15-2914,0,0.0498716,"Missing"
W19-4802,P18-1079,0,0.0197541,"LP, pages 12–23 c Florence, Italy, August 1, 2019. 2019 Association for Computational Linguistics 2 Related work Label ++ + 0 − −− Neural networks are now ubiquitous in NLP tasks, often giving state-of-the-art results. However, they are known for being “black boxes” which are not easily interpretable. Recent interest in interpreting these methods has led to new lines of research which attempt to discover what linguistic phenomena neural networks are able to learn (Linzen et al., 2016; Gulordava et al., 2018; Conneau et al., 2018), how robust neural networks are to perturbations in input data (Ribeiro et al., 2018; Ebrahimi et al., 2018; Schluter and Varab, 2018), and what biases they propagate (Park et al., 2018; Zhao et al., 2018; Kiritchenko and Mohammad, 2018). Specifically within the task of sentiment analysis, certain linguistic phenomena are known to be challenging. Negation is one of the aspects of language that most clearly affects expressions of sentiment and that has been studied widely within sentiment analysis (see Wiegand et al. (2010) for an early survey). The difficulties of resolving negation for sentiment analysis include determining negation scope (Hogenboom et al., 2011; Lapponi et"
W19-4802,P02-1053,0,0.0376856,"Missing"
W19-4802,S18-1005,0,0.0378914,"Missing"
W19-4802,W10-3111,0,0.209928,"tworks are able to learn (Linzen et al., 2016; Gulordava et al., 2018; Conneau et al., 2018), how robust neural networks are to perturbations in input data (Ribeiro et al., 2018; Ebrahimi et al., 2018; Schluter and Varab, 2018), and what biases they propagate (Park et al., 2018; Zhao et al., 2018; Kiritchenko and Mohammad, 2018). Specifically within the task of sentiment analysis, certain linguistic phenomena are known to be challenging. Negation is one of the aspects of language that most clearly affects expressions of sentiment and that has been studied widely within sentiment analysis (see Wiegand et al. (2010) for an early survey). The difficulties of resolving negation for sentiment analysis include determining negation scope (Hogenboom et al., 2011; Lapponi et al., 2012; Reitan et al., 2015), and semantic composition (Wilson et al., 2005; Choi and Cardie, 2008; Kiritchenko and Mohammad, 2016). Verbal polarity shifters have also been studied. Schulder et al. (2018) annotate verbal shifters at the sense-level. They conclude that, although individual negation words are more frequent in the Amazon Product Review Data corpus, the overall frequency of negation words and shifters is likely similar. This"
W19-4802,D18-1534,0,0.02286,"2019. 2019 Association for Computational Linguistics 2 Related work Label ++ + 0 − −− Neural networks are now ubiquitous in NLP tasks, often giving state-of-the-art results. However, they are known for being “black boxes” which are not easily interpretable. Recent interest in interpreting these methods has led to new lines of research which attempt to discover what linguistic phenomena neural networks are able to learn (Linzen et al., 2016; Gulordava et al., 2018; Conneau et al., 2018), how robust neural networks are to perturbations in input data (Ribeiro et al., 2018; Ebrahimi et al., 2018; Schluter and Varab, 2018), and what biases they propagate (Park et al., 2018; Zhao et al., 2018; Kiritchenko and Mohammad, 2018). Specifically within the task of sentiment analysis, certain linguistic phenomena are known to be challenging. Negation is one of the aspects of language that most clearly affects expressions of sentiment and that has been studied widely within sentiment analysis (see Wiegand et al. (2010) for an early survey). The difficulties of resolving negation for sentiment analysis include determining negation scope (Hogenboom et al., 2011; Lapponi et al., 2012; Reitan et al., 2015), and semantic comp"
W19-4802,H05-1044,0,0.456305,"t biases they propagate (Park et al., 2018; Zhao et al., 2018; Kiritchenko and Mohammad, 2018). Specifically within the task of sentiment analysis, certain linguistic phenomena are known to be challenging. Negation is one of the aspects of language that most clearly affects expressions of sentiment and that has been studied widely within sentiment analysis (see Wiegand et al. (2010) for an early survey). The difficulties of resolving negation for sentiment analysis include determining negation scope (Hogenboom et al., 2011; Lapponi et al., 2012; Reitan et al., 2015), and semantic composition (Wilson et al., 2005; Choi and Cardie, 2008; Kiritchenko and Mohammad, 2016). Verbal polarity shifters have also been studied. Schulder et al. (2018) annotate verbal shifters at the sense-level. They conclude that, although individual negation words are more frequent in the Amazon Product Review Data corpus, the overall frequency of negation words and shifters is likely similar. This suggests that there is a Zipfian tail of shifters which are not often handled within sentiment analysis. Furthermore, the linguistic phenomenon of modality has also been shown to be problematic. Both Narayanan et al. (2009) and Liu e"
W19-4802,L18-1222,0,0.0133724,"f sentiment analysis, certain linguistic phenomena are known to be challenging. Negation is one of the aspects of language that most clearly affects expressions of sentiment and that has been studied widely within sentiment analysis (see Wiegand et al. (2010) for an early survey). The difficulties of resolving negation for sentiment analysis include determining negation scope (Hogenboom et al., 2011; Lapponi et al., 2012; Reitan et al., 2015), and semantic composition (Wilson et al., 2005; Choi and Cardie, 2008; Kiritchenko and Mohammad, 2016). Verbal polarity shifters have also been studied. Schulder et al. (2018) annotate verbal shifters at the sense-level. They conclude that, although individual negation words are more frequent in the Amazon Product Review Data corpus, the overall frequency of negation words and shifters is likely similar. This suggests that there is a Zipfian tail of shifters which are not often handled within sentiment analysis. Furthermore, the linguistic phenomenon of modality has also been shown to be problematic. Both Narayanan et al. (2009) and Liu et al. (2014) explore the effect of modality on sentiment classification and find that explicitly modeling certain modalities impr"
W19-4802,N19-1423,0,\N,Missing
W19-6113,W17-5202,1,0.768654,"Missing"
W19-6113,W16-4011,0,0.0524904,"Missing"
W19-6113,P13-1094,0,0.280803,"Missing"
W19-6113,N18-2074,0,0.0149121,". Dropout is used after the max pooling layer and ReLU layer for regularization. B I L STM is a one-layer bidirectional Long Short-Term Network (Graves et al., 2005) with word embeddings as input. The contextualized representation of each sentence is the concatenation of the final hidden states from the left-toright and right-to-left LSTM. This representation is then passed to a softmax layer for classification. Dropout is used before the LSTM layers and softmax layers for regularization. S AN is a one-layer self-attention network (Vaswani et al., 2017) with relative position representations (Shaw et al., 2018) and a single set of attention heads, which was previously shown to perform well for sentiment analysis (Ambartsoumian and Popowich, 2018). The network uses a variant of the attention mechanism (Bahdanau et al., 2014) which creates contextualized representations of the original input sequence, such that the contextualized representations encode both information about the original input, as well as how it relates to all other positions. 8.2 Experimental Setup We apply the models to five experimental setups. The main task is to classify each sentence as evaluative (EVAL), fact-implied non-person"
W19-6113,P10-1059,0,0.416835,"we analyze the corpus experimentally and present a series of preliminary classification experiments using a wide range of state-of-the-art sentiment models including CNNs, BiLSTMs and self-attention networks, before we in Section 9 conclude and outline some remaining avenues for future work. The dataset and the annotation guidelines are made available, along with code for replicating the experiments.1 1 https://github.com/ltgoslo/norec_eval 2 Background and related work In this section we briefly review some of the previous annotation efforts (for English) that are most relevant for our work. Toprak et al. (2010) present a sentimentannotated corpus of consumer reviews. In a first pass, sentences are annotated with respect to relevancy to the overall topic and whether they express an evaluation. In a second pass, sentences that were marked as relevant and evaluative are further annotated with respect to whether they are opinionated (i.e. express a subjective opinion) or polar-facts (i.e. factual information that implies evaluation). In addition to evaluations, they also identify sources (opinion holders), targets (the entity or aspect that the sentiment is directed towards), modifiers, positive/negativ"
W19-6113,W17-0237,1,0.898818,"Missing"
W19-6113,W10-1501,0,0.579569,"in these texts. 8 Experiments In this section we apply a range of different architectures to provide first baseline results for predicting the various labels in the new corpus. Data splits for training, validation and testing are inherited from NoReC. 8.1 Models We provide a brief description of the various classifiers below. Additionally, we provide a majority baseline which always predicts the EVAL class as a lower bound. Note that all classifiers except the bag-of-words model take as input 100 dimensional fastText skipgram embeddings (Bojanowski et al., 2016), trained on the NoWaC corpus (Guevara, 2010), which contains over 680 Million tokens in Bokm˚al Norwegian. The pre-trained word embeddings were re-used from the NLPL vector repository3 (Fares et al., 2017). B OW learns to classify the sentences with a linear separation estimated based on log likelihood optimization with an L2 prior using a bagof-words representation. AVE (Barnes et al., 2017) uses the same L2 logistic regression classifier as B OW, but instead using as input the average of the word vectors from a sentence. C NN (Kim, 2014) is a single-layer convolutional neural network with one convolutional layer 3 http://vectors.nlpl."
W19-6113,wilson-2008-annotating,0,0.357339,"entences – i.e. both subjective and objective sentences that are found to be sentimentbearing – based on mixed-domain professional reviews from various news-sources. We present both the annotation scheme and first results for classification experiments. The effort represents a step toward creating a Norwegian dataset for fine-grained sentiment analysis. 1 Introduction Sentiment analysis is often approached by first locating the relevant, sentiment-bearing sentences. Traditionally, one has distinguished between subjective and objective sentences, where only the former were linked to sentiment (Wilson, 2008). Objective sentences typically present facts about the world, whereas subjective sentences express personal feelings, views, or beliefs. More recently, however, it has become widely recognized in the literature that subjectivity should not be equated with opinion (Liu, 2015): On the one hand, there are many subjective sentences that do not express sentiment, e.g., I think that he went home, and on the other hand there are many objective sentences that do, e.g., The earphone broke in two days, to quote some examples from Liu (2015). Additionally, sentences often contain several polarities in a"
W19-6113,N18-1171,0,0.0248995,"ator agreement for the attribute categories ¬OT and ¬FP, restricted to the subset of sentences labeled EVAL,2 yielding F1 of 0.59 and 0.56, respectively. In other words, we see that the agreement is somewhat lower for these subcategories compared to the top-level label EVAL. Possible reasons for this might be that although problems with these attributes seem to be resolved quickly in annotator meetings, they might pose difficulties to the individual annotator, as sometimes these attributes can be context dependent to an extent that makes them difficult to infer from the review text by itself. Kenyon-Dean et al. (2018) problematizes a practice often seen in relation to sentiment annotation, namely that complicated cases – e.g. sentences were there is annotator disagreement – are discarded from the final dataset. This makes the 2 For the FACT-NP subset there were too few instances of these attributes (prior to adjudication) for agreement to be meaningfully quantified; 1 for ¬OT and 0 for ¬FP. EVAL FACT-NP NONE all 0.84 0.22 0.87 0.82 Table 1: F1 inter-annotator agreement for each top-level label. data non-representative of real text and will artificially inflate classification results on the annotations. In"
W19-6113,D14-1181,0,0.00898412,"Missing"
W19-6119,N18-1172,0,0.10188,"that are predictive for both tasks. MTL assumes that features that are useful for a certain task should also be predictive for similar tasks, and in this sense effectively acts as a regularizer, as it prevents the weights from adapting too much to a single task. The simplest approach to MTL, hard parameter sharing (Caruana, 1993), assumes that all layers are shared between tasks except for the final predictive layer. This approach tends to improve performance when the auxiliary task is carefully chosen (Plank, 2016; Peng and Dredze, 2017; Mart´ınez Alonso and Plank, 2017; Fares et al., 2018; Augenstein et al., 2018). What characteristics determine a useful auxiliary task, however, is still not completely clear (Bingel and Søgaard, 2017; Augenstein and Søgaard, 2017; Mart´ınez Alonso and Plank, 2017; Bjerva, 2017). Søgaard and Goldberg (2016) propose an improvement over hard parameter sharing that uses the lower layers of a multi-layer recurrent neural network to make predictions for low-level auxiliary tasks, while allowing higher layers to focus on the main task. In this work, we adopt a similar approach to incorporate sentiment lexicon information as an auxiliary task to improve sentence-level sentimen"
W19-6119,P17-2054,0,0.0225945,"n this sense effectively acts as a regularizer, as it prevents the weights from adapting too much to a single task. The simplest approach to MTL, hard parameter sharing (Caruana, 1993), assumes that all layers are shared between tasks except for the final predictive layer. This approach tends to improve performance when the auxiliary task is carefully chosen (Plank, 2016; Peng and Dredze, 2017; Mart´ınez Alonso and Plank, 2017; Fares et al., 2018; Augenstein et al., 2018). What characteristics determine a useful auxiliary task, however, is still not completely clear (Bingel and Søgaard, 2017; Augenstein and Søgaard, 2017; Mart´ınez Alonso and Plank, 2017; Bjerva, 2017). Søgaard and Goldberg (2016) propose an improvement over hard parameter sharing that uses the lower layers of a multi-layer recurrent neural network to make predictions for low-level auxiliary tasks, while allowing higher layers to focus on the main task. In this work, we adopt a similar approach to incorporate sentiment lexicon information as an auxiliary task to improve sentence-level sentiment and evaluative-language classification. 3.1 English sentiment lexicon For English we use the sentiment lexicon compiled by Hu and Liu (2004), containi"
W19-6119,W17-0237,1,0.850985,"arning model (M TL) alternates between training one epoch on the main task and one epoch on the auxiliary task. Preliminary experiments showed that more complicated training strategies (alternating training between each batch or uniformly sampling batches from the two tasks) did not lead to improvements. For English we use 300 dimensional pre-trained embeddings from GoogleNews,6 while for Norwegian we use 100 dimensional skip-gram fastText embeddings (Bojanowski et al., 2016) trained on the NoWaC corpus (Guevara, 2010). The pre-trained embeddings were re-used from the NLPL vector repository7 (Fares et al., 2017). We train the model for 10 epochs using Adam (Kingma and Ba, 2014), performing early stopping determined by the improvement on the development set of the main task. Given that neural models are sensitive to the random initialization of their parameters, we perform five runs with different random seeds and show the mean and standard deviation as the final result for each model. We use the same five random seeds for all experiments to ensure a fair comparison between models. 6 Available at https://code.google.com/ archive/p/word2vec/. 7 http://vectors.nlpl.eu/repository Model SST NoReCeval 6 L"
W19-6119,P19-2035,0,0.279006,"icon approaches and that their model is more robust to domain shifts than machine learning models. Lexicons in neural approaches The general tendency in NLP when using neural approaches is to perform end-to-end learning without using external knowledge sources, relying instead solely on what can be inferred from (often pre-trained) word embeddings and the training corpus itself. This is also the case for neural sentiment modeling. However, there have been some attempts to include external knowledge like lexicon features into such models (Teng et al., 2016; Zou et al., 2018; Lei et al., 2018a; Bao et al., 2019a). One notable example is the work of Shin et al. (2017) where several approaches are tested for how to incorporate lexicon information into a CNN for sentiment classification on the SemEval 2016 Task 4 dataset and the Stanford Sentiment Treebank (SST). Shin et al. (2017) create feature vectors that encode the positive or negative polarity values of words across a broad selection of different sentiment lexicons available for English. These word-level sentiment-score vectors are then combined with standard word embeddings in different ways in the CNN: through simple concatenation, using multip"
W19-6119,D18-1178,1,0.814294,"Missing"
W19-6119,E17-2026,0,0.108169,"e for similar tasks, and in this sense effectively acts as a regularizer, as it prevents the weights from adapting too much to a single task. The simplest approach to MTL, hard parameter sharing (Caruana, 1993), assumes that all layers are shared between tasks except for the final predictive layer. This approach tends to improve performance when the auxiliary task is carefully chosen (Plank, 2016; Peng and Dredze, 2017; Mart´ınez Alonso and Plank, 2017; Fares et al., 2018; Augenstein et al., 2018). What characteristics determine a useful auxiliary task, however, is still not completely clear (Bingel and Søgaard, 2017; Augenstein and Søgaard, 2017; Mart´ınez Alonso and Plank, 2017; Bjerva, 2017). Søgaard and Goldberg (2016) propose an improvement over hard parameter sharing that uses the lower layers of a multi-layer recurrent neural network to make predictions for low-level auxiliary tasks, while allowing higher layers to focus on the main task. In this work, we adopt a similar approach to incorporate sentiment lexicon information as an auxiliary task to improve sentence-level sentiment and evaluative-language classification. 3.1 English sentiment lexicon For English we use the sentiment lexicon compiled"
W19-6119,W17-0225,0,0.0223973,"he weights from adapting too much to a single task. The simplest approach to MTL, hard parameter sharing (Caruana, 1993), assumes that all layers are shared between tasks except for the final predictive layer. This approach tends to improve performance when the auxiliary task is carefully chosen (Plank, 2016; Peng and Dredze, 2017; Mart´ınez Alonso and Plank, 2017; Fares et al., 2018; Augenstein et al., 2018). What characteristics determine a useful auxiliary task, however, is still not completely clear (Bingel and Søgaard, 2017; Augenstein and Søgaard, 2017; Mart´ınez Alonso and Plank, 2017; Bjerva, 2017). Søgaard and Goldberg (2016) propose an improvement over hard parameter sharing that uses the lower layers of a multi-layer recurrent neural network to make predictions for low-level auxiliary tasks, while allowing higher layers to focus on the main task. In this work, we adopt a similar approach to incorporate sentiment lexicon information as an auxiliary task to improve sentence-level sentiment and evaluative-language classification. 3.1 English sentiment lexicon For English we use the sentiment lexicon compiled by Hu and Liu (2004), containing 4,783 negative words and 2,006 positive words."
W19-6119,esuli-sebastiani-2006-sentiwordnet,0,0.203849,"eoretical approaches to emotion (Stone et al., 1962; Bradley et al., 1999). There are several freely available sentiment lexicons for English. One widely used lexicon is that of Hu and Liu (2004),3 which was created using a bootstrapping approach from WordNet and a corpus of product reviews. This is the lexicon that forms the basis of the experiments in the current paper and we return to it in § 3.1. Other available lexicons include the MPQA subjectivity lexicon (Wilson et al., 2005) which contains words and expressions manually annotated as positive, negative, both, or neutral. SentiWordnet (Esuli and Sebastiani, 2006) contains each synset of the English WordNet annotated with scores representing the sentiment orientation as being positive, negative, or objective. The So-Cal (Taboada et al., 2011) English sentiment lexicon contains separate lexicons of verbs, nouns, adjectives, and adverbs. The words were manually labeled on a scale from extremely positive (+5) to extremely negative (−5), and all words labeled as neutral (0) were excluded from the lexicons. While no high-quality sentiment lexicons for Norwegian are currently publicly available, there have been some previous attempts at generating lexicons f"
W19-6119,W10-1501,0,0.0188357,"e to measure the relative improvement. Multi-task model: During training, the multitask learning model (M TL) alternates between training one epoch on the main task and one epoch on the auxiliary task. Preliminary experiments showed that more complicated training strategies (alternating training between each batch or uniformly sampling batches from the two tasks) did not lead to improvements. For English we use 300 dimensional pre-trained embeddings from GoogleNews,6 while for Norwegian we use 100 dimensional skip-gram fastText embeddings (Bojanowski et al., 2016) trained on the NoWaC corpus (Guevara, 2010). The pre-trained embeddings were re-used from the NLPL vector repository7 (Fares et al., 2017). We train the model for 10 epochs using Adam (Kingma and Ba, 2014), performing early stopping determined by the improvement on the development set of the main task. Given that neural models are sensitive to the random initialization of their parameters, we perform five runs with different random seeds and show the mean and standard deviation as the final result for each model. We use the same five random seeds for all experiments to ensure a fair comparison between models. 6 Available at https://cod"
W19-6119,C04-1200,0,0.152495,"the top 10,000 most frequent words in the corpus and a list of adjectives generated from their corpus using SCARRIE. Their results showed that the translated lexicons outperformed all of their generated lexicons, but unfortunately only the latter were made publicly available. Lexicon-based approaches to SA Early approaches to sentiment analysis classified documents based on the sum of semantic orientation scores of adjectives in a document. Often, researchers used existing lexicons (Stone et al., 1962), or extended these resources in a semisupervised fashion, using WordNet (Hu and Liu, 2004; Kim and Hovy, 2004; Esuli and Sebastiani, 2006). Alternatively, an adjective’s semantic orientation could be determined as the strength of association with positive words (excellent) or negative words (poor) as measured by Pointwise Mutual Information (Turney and Littman, 2003). Researchers quickly discovered, however, that various linguistic phenomena, e.g. negation, intensifying adverbs, downtoners, etc, must be taken into account to correctly assign a sentiment score. Taboada et al. (2011) proposed an approach to determine the semantic orientation of documents which incorporates sentiment lexicons for adject"
W19-6119,konstantinova-etal-2012-review,0,0.0698456,"Missing"
W19-6119,W17-5206,0,0.0528967,"Missing"
W19-6119,W17-1903,0,0.0622486,"Missing"
W19-6119,P05-1015,0,0.243866,"n unchanged for SST. The model used by Shin et al. (2017) requires information from six different lexicons, which is overly restrictive for most languages besides English, where one will typically not have the luxury of several publicly available sentiment lexicons. Lei et al. (2018b) propose a different approach based on what they dub a ‘Multi-sentimentresource Enhanced Attention Network’, where lexicon information is used for guiding an attention mechanism when learning sentiment-specific sentence representations. The approach shows promising results on both SST and the Movie Review data of Pang and Lee (2005), although the model also incorporates other types of lexicons, like negation cues and intensifiers. In a similar spirit, Margatina et al. (2019) include features from a range of sentiment-related lexicons for guiding the self-attention mechanism in an LSTM. Bao et al. (2019b) generate features from several different lexicons that are added to an attention-based LSTM for aspect-based sentiment analysis. In the current paper we will instead explore whether lexicon information can be incorporated into neural models using the framework of multitask learning. This has two main advantages: 1) we re"
W19-6119,P18-2120,0,0.187132,"semi-supervised lexicon approaches and that their model is more robust to domain shifts than machine learning models. Lexicons in neural approaches The general tendency in NLP when using neural approaches is to perform end-to-end learning without using external knowledge sources, relying instead solely on what can be inferred from (often pre-trained) word embeddings and the training corpus itself. This is also the case for neural sentiment modeling. However, there have been some attempts to include external knowledge like lexicon features into such models (Teng et al., 2016; Zou et al., 2018; Lei et al., 2018a; Bao et al., 2019a). One notable example is the work of Shin et al. (2017) where several approaches are tested for how to incorporate lexicon information into a CNN for sentiment classification on the SemEval 2016 Task 4 dataset and the Stanford Sentiment Treebank (SST). Shin et al. (2017) create feature vectors that encode the positive or negative polarity values of words across a broad selection of different sentiment lexicons available for English. These word-level sentiment-score vectors are then combined with standard word embeddings in different ways in the CNN: through simple concaten"
W19-6119,W17-2612,0,0.0256611,"ive bias by restricting the search space of possible representations to those that are predictive for both tasks. MTL assumes that features that are useful for a certain task should also be predictive for similar tasks, and in this sense effectively acts as a regularizer, as it prevents the weights from adapting too much to a single task. The simplest approach to MTL, hard parameter sharing (Caruana, 1993), assumes that all layers are shared between tasks except for the final predictive layer. This approach tends to improve performance when the auxiliary task is carefully chosen (Plank, 2016; Peng and Dredze, 2017; Mart´ınez Alonso and Plank, 2017; Fares et al., 2018; Augenstein et al., 2018). What characteristics determine a useful auxiliary task, however, is still not completely clear (Bingel and Søgaard, 2017; Augenstein and Søgaard, 2017; Mart´ınez Alonso and Plank, 2017; Bjerva, 2017). Søgaard and Goldberg (2016) propose an improvement over hard parameter sharing that uses the lower layers of a multi-layer recurrent neural network to make predictions for low-level auxiliary tasks, while allowing higher layers to focus on the main task. In this work, we adopt a similar approach to incorporate senti"
W19-6119,C16-1059,0,0.0178372,"useful inductive bias by restricting the search space of possible representations to those that are predictive for both tasks. MTL assumes that features that are useful for a certain task should also be predictive for similar tasks, and in this sense effectively acts as a regularizer, as it prevents the weights from adapting too much to a single task. The simplest approach to MTL, hard parameter sharing (Caruana, 1993), assumes that all layers are shared between tasks except for the final predictive layer. This approach tends to improve performance when the auxiliary task is carefully chosen (Plank, 2016; Peng and Dredze, 2017; Mart´ınez Alonso and Plank, 2017; Fares et al., 2018; Augenstein et al., 2018). What characteristics determine a useful auxiliary task, however, is still not completely clear (Bingel and Søgaard, 2017; Augenstein and Søgaard, 2017; Mart´ınez Alonso and Plank, 2017; Bjerva, 2017). Søgaard and Goldberg (2016) propose an improvement over hard parameter sharing that uses the lower layers of a multi-layer recurrent neural network to make predictions for low-level auxiliary tasks, while allowing higher layers to focus on the main task. In this work, we adopt a similar approa"
W19-6119,W19-6113,1,0.84215,"pment and test splits (of 8,455 / 1,101 / 2,210 sentences, respectively). 4.2 Norwegian The Norwegian dataset used in this work forms part of the Norwegian Review Corpus NoReC (Velldal et al., 2018), consisting of full-text reviews from a range of different domains, such as restaurants, literature, and music, collected from several of the major Norwegian news sources. The particular subset used in the current work, dubbed NoReCeval , comprises 7961 sentences across 298 documents that have been manually annotated according to whether or not each sentence contains an evaluation, as described by Mæhlum et al. (2019). Two types of evaluative sentence categories are distinguished (in addition to non-evaluative sentences): simple evaluative and a special case of evaluative fact-implied nonpersonal. The latter follows the terminology of Liu (2015), denoting factual, objective sentences which are used with an evaluative intent but without reference to personal experience. Example 5 https://github.com/ltgoslo/norsentlex Unlike the English dataset discussed above, the annotation does not specify the polarity of the sentence. The rationale for this is that a sentence may contain more than one sentiment expressio"
W19-6119,P19-1385,0,0.0191389,"ost languages besides English, where one will typically not have the luxury of several publicly available sentiment lexicons. Lei et al. (2018b) propose a different approach based on what they dub a ‘Multi-sentimentresource Enhanced Attention Network’, where lexicon information is used for guiding an attention mechanism when learning sentiment-specific sentence representations. The approach shows promising results on both SST and the Movie Review data of Pang and Lee (2005), although the model also incorporates other types of lexicons, like negation cues and intensifiers. In a similar spirit, Margatina et al. (2019) include features from a range of sentiment-related lexicons for guiding the self-attention mechanism in an LSTM. Bao et al. (2019b) generate features from several different lexicons that are added to an attention-based LSTM for aspect-based sentiment analysis. In the current paper we will instead explore whether lexicon information can be incorporated into neural models using the framework of multitask learning. This has two main advantages: 1) we require only a single sentiment lexicon, unlike much previous work, and 2) our model is able to generalize to sentiment words not seen in the lexic"
W19-6119,E17-1005,0,0.014929,"h space of possible representations to those that are predictive for both tasks. MTL assumes that features that are useful for a certain task should also be predictive for similar tasks, and in this sense effectively acts as a regularizer, as it prevents the weights from adapting too much to a single task. The simplest approach to MTL, hard parameter sharing (Caruana, 1993), assumes that all layers are shared between tasks except for the final predictive layer. This approach tends to improve performance when the auxiliary task is carefully chosen (Plank, 2016; Peng and Dredze, 2017; Mart´ınez Alonso and Plank, 2017; Fares et al., 2018; Augenstein et al., 2018). What characteristics determine a useful auxiliary task, however, is still not completely clear (Bingel and Søgaard, 2017; Augenstein and Søgaard, 2017; Mart´ınez Alonso and Plank, 2017; Bjerva, 2017). Søgaard and Goldberg (2016) propose an improvement over hard parameter sharing that uses the lower layers of a multi-layer recurrent neural network to make predictions for low-level auxiliary tasks, while allowing higher layers to focus on the main task. In this work, we adopt a similar approach to incorporate sentiment lexicon information as an aux"
W19-6119,S13-2053,0,0.0353331,"ge points (ppt) worse than S TL. On NoReCeval , however, it performs much worse, which can be attributed to the the difficulty of determining if a sentence is nonevaluative or fact-implied using only unigram information, as these sentence types do not differ largely lexically. B OW +L EXICON performs better than B OW on both datasets, although the difference is larger on SST (1.5 ppt vs. 0.8 ppt). This is likely due to sentiment lexicon features being more predictive for the sentiment task. Additionally, it outperforms the S TL model by 1.1 ppt on SST, confirming that it is a strong baseline (Mohammad et al., 2013). L EX -E MB is the weakest model on the SST dataset with 34.7 F1 but performs better than the non-neural baselines on NoReCeval (48.9). S TL performs better than L EXICON, B OW, and L EX E MB on both tasks, as well as B OW +L EXICON on NoReCeval . Finally, M TL is the best performing model on both tasks, with a difference of 3.5 ppt between M TL and the next best performing model on SST, and 1.6 ppt on NoReCeval . Table 3: Macro F1 of models on the SST and NoReCeval sentence-level datasets. Neural models report mean and standard deviation of the scores over five runs. Lexicon embedding Model:"
W19-6119,morante-daelemans-2012-conandoyle,0,0.02648,"e show how MTL can enable a BiLSTM sentiment classifier to incorporate information from sentiment lexicons. Our MTL set-up is shown to improve model performance (compared to a single-task set-up) on both English and Norwegian sentence-level sentiment datasets. The paper also introduces a new sentiment lexicon for Norwegian. 1 Introduction Current state-of-the-art neural approaches to sentiment analysis tend not to incorporate available sources of external knowledge, such as polarity lexicons (Hu and Liu, 2004; Taboada et al., 2006; Mohammad and Turney, 2013), explicit negation annotated data (Morante and Daelemans, 2012; Konstantinova et al., 2012), or labels representing inter-annotator agreement (Plank et al., 2014). One reason for this is that neural models can already achieve good performance, even if they only use word embeddings given as input, as they are able to learn task-specific information (which words convey sentiment, how to resolve negation, how to resolve intensification) in a data-driven manner (Socher et al., 2013; Irsoy and Cardie, 2014). Another often overlooked reason is that it is not always entirely straightforward how we can efficiently incorporate this available external knowledge in"
W19-6119,E14-1078,0,0.0196223,"Our MTL set-up is shown to improve model performance (compared to a single-task set-up) on both English and Norwegian sentence-level sentiment datasets. The paper also introduces a new sentiment lexicon for Norwegian. 1 Introduction Current state-of-the-art neural approaches to sentiment analysis tend not to incorporate available sources of external knowledge, such as polarity lexicons (Hu and Liu, 2004; Taboada et al., 2006; Mohammad and Turney, 2013), explicit negation annotated data (Morante and Daelemans, 2012; Konstantinova et al., 2012), or labels representing inter-annotator agreement (Plank et al., 2014). One reason for this is that neural models can already achieve good performance, even if they only use word embeddings given as input, as they are able to learn task-specific information (which words convey sentiment, how to resolve negation, how to resolve intensification) in a data-driven manner (Socher et al., 2013; Irsoy and Cardie, 2014). Another often overlooked reason is that it is not always entirely straightforward how we can efficiently incorporate this available external knowledge in the model. Despite achieving strong results, neural models are known to be difficult to interpret,"
W19-6119,P13-1094,0,0.0767359,"Missing"
W19-6119,W17-5220,0,0.0683005,"domain shifts than machine learning models. Lexicons in neural approaches The general tendency in NLP when using neural approaches is to perform end-to-end learning without using external knowledge sources, relying instead solely on what can be inferred from (often pre-trained) word embeddings and the training corpus itself. This is also the case for neural sentiment modeling. However, there have been some attempts to include external knowledge like lexicon features into such models (Teng et al., 2016; Zou et al., 2018; Lei et al., 2018a; Bao et al., 2019a). One notable example is the work of Shin et al. (2017) where several approaches are tested for how to incorporate lexicon information into a CNN for sentiment classification on the SemEval 2016 Task 4 dataset and the Stanford Sentiment Treebank (SST). Shin et al. (2017) create feature vectors that encode the positive or negative polarity values of words across a broad selection of different sentiment lexicons available for English. These word-level sentiment-score vectors are then combined with standard word embeddings in different ways in the CNN: through simple concatenation, using multiple channels, or performing separate convolutions. While a"
W19-6119,D13-1170,0,0.0589301,"e available sources of external knowledge, such as polarity lexicons (Hu and Liu, 2004; Taboada et al., 2006; Mohammad and Turney, 2013), explicit negation annotated data (Morante and Daelemans, 2012; Konstantinova et al., 2012), or labels representing inter-annotator agreement (Plank et al., 2014). One reason for this is that neural models can already achieve good performance, even if they only use word embeddings given as input, as they are able to learn task-specific information (which words convey sentiment, how to resolve negation, how to resolve intensification) in a data-driven manner (Socher et al., 2013; Irsoy and Cardie, 2014). Another often overlooked reason is that it is not always entirely straightforward how we can efficiently incorporate this available external knowledge in the model. Despite achieving strong results, neural models are known to be difficult to interpret, as well as highly dependent on the training data. Resources like sentiment lexicons, on the other hand, have the benefit of being completely transparent, as well as being easy to adapt or update. Additionally, lexicons are often less sensitive to domain and frequency effects and can provide high coverage and precision"
W19-6119,P16-2038,0,0.0351761,"adapting too much to a single task. The simplest approach to MTL, hard parameter sharing (Caruana, 1993), assumes that all layers are shared between tasks except for the final predictive layer. This approach tends to improve performance when the auxiliary task is carefully chosen (Plank, 2016; Peng and Dredze, 2017; Mart´ınez Alonso and Plank, 2017; Fares et al., 2018; Augenstein et al., 2018). What characteristics determine a useful auxiliary task, however, is still not completely clear (Bingel and Søgaard, 2017; Augenstein and Søgaard, 2017; Mart´ınez Alonso and Plank, 2017; Bjerva, 2017). Søgaard and Goldberg (2016) propose an improvement over hard parameter sharing that uses the lower layers of a multi-layer recurrent neural network to make predictions for low-level auxiliary tasks, while allowing higher layers to focus on the main task. In this work, we adopt a similar approach to incorporate sentiment lexicon information as an auxiliary task to improve sentence-level sentiment and evaluative-language classification. 3.1 English sentiment lexicon For English we use the sentiment lexicon compiled by Hu and Liu (2004), containing 4,783 negative words and 2,006 positive words. The sentiment lexicon was a"
W19-6119,taboada-etal-2006-methods,0,0.133979,"earning (MTL) for incorporating external knowledge in neural models. Specifically, we show how MTL can enable a BiLSTM sentiment classifier to incorporate information from sentiment lexicons. Our MTL set-up is shown to improve model performance (compared to a single-task set-up) on both English and Norwegian sentence-level sentiment datasets. The paper also introduces a new sentiment lexicon for Norwegian. 1 Introduction Current state-of-the-art neural approaches to sentiment analysis tend not to incorporate available sources of external knowledge, such as polarity lexicons (Hu and Liu, 2004; Taboada et al., 2006; Mohammad and Turney, 2013), explicit negation annotated data (Morante and Daelemans, 2012; Konstantinova et al., 2012), or labels representing inter-annotator agreement (Plank et al., 2014). One reason for this is that neural models can already achieve good performance, even if they only use word embeddings given as input, as they are able to learn task-specific information (which words convey sentiment, how to resolve negation, how to resolve intensification) in a data-driven manner (Socher et al., 2013; Irsoy and Cardie, 2014). Another often overlooked reason is that it is not always entir"
W19-6119,J11-2001,0,0.975546,"(2004),3 which was created using a bootstrapping approach from WordNet and a corpus of product reviews. This is the lexicon that forms the basis of the experiments in the current paper and we return to it in § 3.1. Other available lexicons include the MPQA subjectivity lexicon (Wilson et al., 2005) which contains words and expressions manually annotated as positive, negative, both, or neutral. SentiWordnet (Esuli and Sebastiani, 2006) contains each synset of the English WordNet annotated with scores representing the sentiment orientation as being positive, negative, or objective. The So-Cal (Taboada et al., 2011) English sentiment lexicon contains separate lexicons of verbs, nouns, adjectives, and adverbs. The words were manually labeled on a scale from extremely positive (+5) to extremely negative (−5), and all words labeled as neutral (0) were excluded from the lexicons. While no high-quality sentiment lexicons for Norwegian are currently publicly available, there have been some previous attempts at generating lexicons for Norwegian. Hammer et al. (2014) used a set of 51 positive and 57 negative manually selected seed words to crawl three Norwegian thesauri in three iterations, to extract synonyms a"
W19-6119,D16-1169,0,0.0360708,"Missing"
W19-6119,P10-1059,0,0.0216209,"personal. The latter follows the terminology of Liu (2015), denoting factual, objective sentences which are used with an evaluative intent but without reference to personal experience. Example 5 https://github.com/ltgoslo/norsentlex Unlike the English dataset discussed above, the annotation does not specify the polarity of the sentence. The rationale for this is that a sentence may contain more than one sentiment expression and have a mixed polarity, hence this type of annotation is better performed sub-sententially following an initial annotation of evaluative or sentimentrelevant sentences (Toprak et al., 2010; Scheible and Sch¨utze, 2013). We use the training, development and test splits as defined by Mæhlum et al. (2019), see the summary of corpus statistics in Table 2. 5 Multi-task learning of lexicon information in neural models This section details our multi-task neural architecture for incorporating sentiment lexicon information into neural networks, as shown in Figure 1. Our multi-task model shares the lower layers (an embedding and fully connected layer), while allowing the higher layers to further adapt to the main and auxiliary tasks. Specifically, we use a sentiment prediction auxiliary"
W19-6119,H05-1044,0,0.146605,"Sentiment lexicons provide a valuable source of information about the prior affective orientation of words, oftentimes driven by theoretical approaches to emotion (Stone et al., 1962; Bradley et al., 1999). There are several freely available sentiment lexicons for English. One widely used lexicon is that of Hu and Liu (2004),3 which was created using a bootstrapping approach from WordNet and a corpus of product reviews. This is the lexicon that forms the basis of the experiments in the current paper and we return to it in § 3.1. Other available lexicons include the MPQA subjectivity lexicon (Wilson et al., 2005) which contains words and expressions manually annotated as positive, negative, both, or neutral. SentiWordnet (Esuli and Sebastiani, 2006) contains each synset of the English WordNet annotated with scores representing the sentiment orientation as being positive, negative, or objective. The So-Cal (Taboada et al., 2011) English sentiment lexicon contains separate lexicons of verbs, nouns, adjectives, and adverbs. The words were manually labeled on a scale from extremely positive (+5) to extremely negative (−5), and all words labeled as neutral (0) were excluded from the lexicons. While no high"
W19-6119,C18-1074,0,0.0201918,"xicons outperform semi-supervised lexicon approaches and that their model is more robust to domain shifts than machine learning models. Lexicons in neural approaches The general tendency in NLP when using neural approaches is to perform end-to-end learning without using external knowledge sources, relying instead solely on what can be inferred from (often pre-trained) word embeddings and the training corpus itself. This is also the case for neural sentiment modeling. However, there have been some attempts to include external knowledge like lexicon features into such models (Teng et al., 2016; Zou et al., 2018; Lei et al., 2018a; Bao et al., 2019a). One notable example is the work of Shin et al. (2017) where several approaches are tested for how to incorporate lexicon information into a CNN for sentiment classification on the SemEval 2016 Task 4 dataset and the Stanford Sentiment Treebank (SST). Shin et al. (2017) create feature vectors that encode the positive or negative polarity values of words across a broad selection of different sentiment lexicons available for English. These word-level sentiment-score vectors are then combined with standard word embeddings in different ways in the CNN: throu"
W19-6205,P17-1080,0,0.0197037,"ot too far in the past, work on evaluating shallow sentence representations was encouraged by the release of the SentEval toolkit (Conneau and Kiela, 2018), which provided an easy-to-use framework that sentence representations could be ‘plugged’ into, for rapid downstream evaluation on numerous tasks: these include several classification tasks, textual entailment and similarity tasks, a paraphrase detection task, and caption/image retrieval tasks. Relevant to our paper is Conneau et al.’s (2018a) set of ‘probing tasks’, a variant on the theme of diagnostic classification (Hupkes et al., 2017; Belinkov et al., 2017; Adi et al., 2016; Shi et al., 2016), that would attempt to quantify precisely what sort of linguistic information was being retained by sentence representations. Based in part on Shi et al. (2016), Conneau et al. (2018a) focus on evaluating representations for English; they provide Spearman correlations between the performance of a particular representation mechanism on being probed for specific linguistic properties, and the downstream performance on a variety of NLP tasks. Along similar lines, and contemporaneously with this work, Liu et al. (2019) probe similar deep pre-trained to the one"
W19-6205,K18-2005,0,0.0260101,"ieved (then) state-of-theart results by plugging generated fixed-length vectors into downstream classifiers. Another system that represented a significant innovation was BERT (Devlin et al., 2018). BERT introduced a language modelling variant, dubbed masked language modelling, that allowed them to use transformer encoders as their underlying encoding mechanism. 2.2 Multilingual pre-training Multilingual variants of pre-trained encoders that provide contextual representations for nonEnglish languages have also been studied; there is, however, some diversity in precisely how they are generated. Che et al. (2018) provide ELMo models (Fares et al., 2017) for 44 languages; all of these were trained on data provided as part of the CoNLL 2018 shared task on dependency parsing Universal Dependencies treebanks (Zeman et al., 2018). This makes ‘multilingual’ a bit of a misnomer: whilst this is the most obvious approach to multilingual support, these models are all monolingual. This also leads to other issues downstream, such as a complete inability to deal with true multilingual phenomena like code-switching. Throughout this text, however, when not specifically referring to ELMo, our use of the term ‘multili"
W19-6205,W19-4828,0,0.0120124,"ask to the multilingual domain; the authors evaluate sentence representations derived by mapping non-English representations to an English representation space. 2.4 BERTology Relevant to the probing theme of this paper is the sudden recent growth in papers studying precisely what is retained with the internal representations of pre-trained encoders like BERT. These include, for instance, analyses of BERT’s attentions heads, such as Michel et al. (2019), where the authors prune heads, often reducing certain layers to single heads, without a significant drop in performance in certain scenarios. Clark et al. (2019) provide a perhead analysis and attempt to quantify what information each head retains; they discover that specific aspects of syntax are well-encoded per head, and find heads that correspond to certain linguistic properties, such as heads that attend to direct objects of verbs. Other papers provide analyses of BERT’s layers, such as Tenney et al. (2019), who discover that BERT’s layers roughly correspond to the notion of the classical ‘NLP pipeline’, with lower level tasks such as tagging lower down the layer hierarchy. Hewitt and Manning (2019) define a structural probe over BERT representat"
W19-6205,L18-1269,0,0.0216192,"and words masked in both source and target sentences words are predicted using context from both. The authors here also use their own implementation of BPE – FastBPE, for which they provide a vocabulary of around 120K entries. This vocabulary is shared across all of the languages and thus improves the alignment of embedded spaces, as shown in Lample et al. (2017). 2.3 On evaluation Evaluation of contextual representations goes beyond merely deep representations; not too far in the past, work on evaluating shallow sentence representations was encouraged by the release of the SentEval toolkit (Conneau and Kiela, 2018), which provided an easy-to-use framework that sentence representations could be ‘plugged’ into, for rapid downstream evaluation on numerous tasks: these include several classification tasks, textual entailment and similarity tasks, a paraphrase detection task, and caption/image retrieval tasks. Relevant to our paper is Conneau et al.’s (2018a) set of ‘probing tasks’, a variant on the theme of diagnostic classification (Hupkes et al., 2017; Belinkov et al., 2017; Adi et al., 2016; Shi et al., 2016), that would attempt to quantify precisely what sort of linguistic information was being retained"
W19-6205,P18-1198,0,0.295024,"tions could be ‘plugged’ into, for rapid downstream evaluation on numerous tasks: these include several classification tasks, textual entailment and similarity tasks, a paraphrase detection task, and caption/image retrieval tasks. Relevant to our paper is Conneau et al.’s (2018a) set of ‘probing tasks’, a variant on the theme of diagnostic classification (Hupkes et al., 2017; Belinkov et al., 2017; Adi et al., 2016; Shi et al., 2016), that would attempt to quantify precisely what sort of linguistic information was being retained by sentence representations. Based in part on Shi et al. (2016), Conneau et al. (2018a) focus on evaluating representations for English; they provide Spearman correlations between the performance of a particular representation mechanism on being probed for specific linguistic properties, and the downstream performance on a variety of NLP tasks. Along similar lines, and contemporaneously with this work, Liu et al. (2019) probe similar deep pre-trained to the ones we do, on a set of ‘sixteen diverse probing tasks’. (Tenney et al., 2018) probe deep pre-trained encoders for sentence structure. On a different note, Saphra and Lopez (2018) present a CCA-based method to compare repre"
W19-6205,D18-1269,0,0.404632,"tions could be ‘plugged’ into, for rapid downstream evaluation on numerous tasks: these include several classification tasks, textual entailment and similarity tasks, a paraphrase detection task, and caption/image retrieval tasks. Relevant to our paper is Conneau et al.’s (2018a) set of ‘probing tasks’, a variant on the theme of diagnostic classification (Hupkes et al., 2017; Belinkov et al., 2017; Adi et al., 2016; Shi et al., 2016), that would attempt to quantify precisely what sort of linguistic information was being retained by sentence representations. Based in part on Shi et al. (2016), Conneau et al. (2018a) focus on evaluating representations for English; they provide Spearman correlations between the performance of a particular representation mechanism on being probed for specific linguistic properties, and the downstream performance on a variety of NLP tasks. Along similar lines, and contemporaneously with this work, Liu et al. (2019) probe similar deep pre-trained to the ones we do, on a set of ‘sixteen diverse probing tasks’. (Tenney et al., 2018) probe deep pre-trained encoders for sentence structure. On a different note, Saphra and Lopez (2018) present a CCA-based method to compare repre"
W19-6205,N19-1423,0,0.0796501,"Missing"
W19-6205,W17-0237,1,0.827386,"plugging generated fixed-length vectors into downstream classifiers. Another system that represented a significant innovation was BERT (Devlin et al., 2018). BERT introduced a language modelling variant, dubbed masked language modelling, that allowed them to use transformer encoders as their underlying encoding mechanism. 2.2 Multilingual pre-training Multilingual variants of pre-trained encoders that provide contextual representations for nonEnglish languages have also been studied; there is, however, some diversity in precisely how they are generated. Che et al. (2018) provide ELMo models (Fares et al., 2017) for 44 languages; all of these were trained on data provided as part of the CoNLL 2018 shared task on dependency parsing Universal Dependencies treebanks (Zeman et al., 2018). This makes ‘multilingual’ a bit of a misnomer: whilst this is the most obvious approach to multilingual support, these models are all monolingual. This also leads to other issues downstream, such as a complete inability to deal with true multilingual phenomena like code-switching. Throughout this text, however, when not specifically referring to ELMo, our use of the term ‘multilingual’ is inclusive of ELMo’s quasi-multi"
W19-6205,W18-2501,0,0.0537911,"Missing"
W19-6205,N19-1419,0,0.0283279,"significant drop in performance in certain scenarios. Clark et al. (2019) provide a perhead analysis and attempt to quantify what information each head retains; they discover that specific aspects of syntax are well-encoded per head, and find heads that correspond to certain linguistic properties, such as heads that attend to direct objects of verbs. Other papers provide analyses of BERT’s layers, such as Tenney et al. (2019), who discover that BERT’s layers roughly correspond to the notion of the classical ‘NLP pipeline’, with lower level tasks such as tagging lower down the layer hierarchy. Hewitt and Manning (2019) define a structural probe over BERT representations, that extracts notions of syntax that correspond strongly to linguistic notions of dependency syntax. 3 Corpora 3.1 Probing Our data consists of training, development and test splits for 9 linguistic tasks, that can broadly be grouped into surface, syntactic and semantic tasks. These are the same as the ones described in Conneau et al. (2018a), with minor modifications. Due to the differences in corpus domain, we alter some of their word-frequency parameters. We also exclude the top constituent (TopConst) task; we noticed that Wikipedia tend"
W19-6205,P18-1031,0,0.0435359,"Missing"
W19-6205,J06-4003,0,0.029126,"cludes morphologically agglutinative, fusional and (relatively) isolating languages, and it includes two scripts, Latin and Cyrillic. The languages also represent three families (IndoEuropean, Turkic and Uralic). We build our probing datasets using the relevant language’s Wikipedia dump as a corpus. Our motivation for doing so was that it a freely available corpus for numerous languages, large enough to extract the sizeable corpora that we need. Specifically, we use Wikipedia dumps (dated 2019-0201), which we process using the WikiExtractor utility1 . Preprocessing We use the Punkt tokeniser (Kiss and Strunk, 2006) to segment our Wikipedia dumps into discrete sentences. For Russian, which lacked a Punkt tokenisation model, we used the UDPipe (Straka and Strakov´a, 2017) toolkit to perform segmentation. Having segmented our data, we used the Moses (Koehn et al., 2007) tokeniser for the appropriate language, falling back to English tokenisation when unavailable. Next, we obtained dependency parses for our sentences, again using the UDPipe toolkit’s pretrained models, trained on Universal Dependencies treebanks (Nivre et al., 2015). We then processed these dependency parsed corpora to extract the appropria"
W19-6205,P07-2045,0,0.00448085,"t language’s Wikipedia dump as a corpus. Our motivation for doing so was that it a freely available corpus for numerous languages, large enough to extract the sizeable corpora that we need. Specifically, we use Wikipedia dumps (dated 2019-0201), which we process using the WikiExtractor utility1 . Preprocessing We use the Punkt tokeniser (Kiss and Strunk, 2006) to segment our Wikipedia dumps into discrete sentences. For Russian, which lacked a Punkt tokenisation model, we used the UDPipe (Straka and Strakov´a, 2017) toolkit to perform segmentation. Having segmented our data, we used the Moses (Koehn et al., 2007) tokeniser for the appropriate language, falling back to English tokenisation when unavailable. Next, we obtained dependency parses for our sentences, again using the UDPipe toolkit’s pretrained models, trained on Universal Dependencies treebanks (Nivre et al., 2015). We then processed these dependency parsed corpora to extract the appropriate sentences; while in principle, each task was meant to have 120K sentences, with 100K/10K/10K training/validation/test splits, often, for the rarer linguistic phenomena, we ran out of source data, in particular with Turkish and Finnish, although to a smal"
W19-6205,N19-1112,0,0.111492,"classification (Hupkes et al., 2017; Belinkov et al., 2017; Adi et al., 2016; Shi et al., 2016), that would attempt to quantify precisely what sort of linguistic information was being retained by sentence representations. Based in part on Shi et al. (2016), Conneau et al. (2018a) focus on evaluating representations for English; they provide Spearman correlations between the performance of a particular representation mechanism on being probed for specific linguistic properties, and the downstream performance on a variety of NLP tasks. Along similar lines, and contemporaneously with this work, Liu et al. (2019) probe similar deep pre-trained to the ones we do, on a set of ‘sixteen diverse probing tasks’. (Tenney et al., 2018) probe deep pre-trained encoders for sentence structure. On a different note, Saphra and Lopez (2018) present a CCA-based method to compare representation learning dynamics across time and models, without explicitly requiring annotated corpora. A visible limitation of the datasets provided by these probing tasks is that most of them were created with the idea of evaluating representations built for English language data. Within the realm of evaluating multilingual sentence repre"
W19-6205,A94-1016,0,0.0547331,"Missing"
W19-6205,N18-1202,0,0.641859,"cally, we probe each layer of a multiple monolingual RNN-based ELMo models, the transformer-based BERT’s cased and uncased multilingual variants, and a variant of BERT that uses a cross-lingual modelling scheme (XLM). 1 Introduction Recent trends in NLP have demonstrated the utility of pre-trained deep contextual representations in numerous downstream NLP tasks, where they have almost consistently resulted in significant performance improvements. Detailed evaluations have naturally followed: these have either been follow-up works to papers describing contextual representation systems, such as Peters et al. (2018b), or novel works evaluating a broad class of encoders on a broad variety of tasks (Perone et al., 2018). This paper is an example of the latter sort; we perform a comprehensive, large-scale evaluation of what linguistic phenomena these sequential encoders capture across a diverse set of languages. This has often been referred to in the literature as probing; we use this terminology throughout this work. Briefly, our goals are to probe our encoders in a multilingual setting – i.e., we use a series of probing tasks to quantify what sort of linguistic information our encoders retain, and how th"
W19-6205,N19-1329,0,0.0311831,"Missing"
W19-6205,P16-1162,0,0.00704079,"akes ‘multilingual’ a bit of a misnomer: whilst this is the most obvious approach to multilingual support, these models are all monolingual. This also leads to other issues downstream, such as a complete inability to deal with true multilingual phenomena like code-switching. Throughout this text, however, when not specifically referring to ELMo, our use of the term ‘multilingual’ is inclusive of ELMo’s quasi-multilingualism. This is contrasted with BERT’s approach to (true) multilingualism, which trains a single model that can handle all languages. The authors use WordPiece, a variant of BPE (Sennrich et al., 2016), for tokenisation, using a 110K-size vocabulary, and proceed to train a single gigantic model; they perform exponentially smoothed weighting of their data to avoid biasing their model towards better-resourced languages. Finally, XLM (Lample and Conneau, 2019) is another cross-lingual encoder based on BERT that implements a number of modifications. Along with BERT’s masked language modeling or Cloze task-based modelling (Devlin et al., 2018; Taylor, 1953), XLM training uses another similar objective during training that the authors call translation language modeling. Here, two parallel sentenc"
W19-6205,D16-1159,0,0.0132289,"g shallow sentence representations was encouraged by the release of the SentEval toolkit (Conneau and Kiela, 2018), which provided an easy-to-use framework that sentence representations could be ‘plugged’ into, for rapid downstream evaluation on numerous tasks: these include several classification tasks, textual entailment and similarity tasks, a paraphrase detection task, and caption/image retrieval tasks. Relevant to our paper is Conneau et al.’s (2018a) set of ‘probing tasks’, a variant on the theme of diagnostic classification (Hupkes et al., 2017; Belinkov et al., 2017; Adi et al., 2016; Shi et al., 2016), that would attempt to quantify precisely what sort of linguistic information was being retained by sentence representations. Based in part on Shi et al. (2016), Conneau et al. (2018a) focus on evaluating representations for English; they provide Spearman correlations between the performance of a particular representation mechanism on being probed for specific linguistic properties, and the downstream performance on a variety of NLP tasks. Along similar lines, and contemporaneously with this work, Liu et al. (2019) probe similar deep pre-trained to the ones we do, on a set of ‘sixteen diverse"
W19-6205,K17-3009,0,0.0651014,"Missing"
W19-6205,P19-1452,0,0.0364986,"Missing"
W19-6205,N18-1101,0,0.0204464,"ture. On a different note, Saphra and Lopez (2018) present a CCA-based method to compare representation learning dynamics across time and models, without explicitly requiring annotated corpora. A visible limitation of the datasets provided by these probing tasks is that most of them were created with the idea of evaluating representations built for English language data. Within the realm of evaluating multilingual sentence representations, Conneau et al. (2018b) describe the XNLI dataset, a set of translations of the development and test portions of the multi-genre MultiNLI inference dataset (Williams et al., 2018). This, in a sense, is an extension of a predominantly monolingual task to the multilingual domain; the authors evaluate sentence representations derived by mapping non-English representations to an English representation space. 2.4 BERTology Relevant to the probing theme of this paper is the sudden recent growth in papers studying precisely what is retained with the internal representations of pre-trained encoders like BERT. These include, for instance, analyses of BERT’s attentions heads, such as Michel et al. (2019), where the authors prune heads, often reducing certain layers to single hea"
W19-6205,K18-2001,0,0.0461695,"Missing"
