2020.acl-main.528,N13-1006,0,0.112491,"Missing"
2020.acl-main.528,W06-0130,0,0.562999,"ls Yang et al., 2016 Yang et al., 2016∗† Che et al., 2013∗ Wang et al., 2013∗ Word-based (LSTM) + char + bichar Word-based (LSTM) + char + bichar Char-based (LSTM) + bichar + softword + ExSoftword + bichar + ExSoftword Lattice-LSTM LR-CNN (Gui et al., 2019) SoftLexicon (LSTM) SoftLexicon (LSTM) + bichar BERT-Tagger BERT + LSTM + CRF SoftLexicon (LSTM) + BERT P 65.59 72.98 77.71 76.43 76.66 78.62 72.84 73.36 68.79 74.36 69.90 73.80 76.35 76.40 77.28 77.13 76.01 81.99 83.41 R 71.84 80.15 72.51 72.32 63.60 73.13 59.72 70.12 60.35 69.43 66.46 71.05 71.56 72.60 74.07 75.22 79.96 81.65 82.21 Models Chen et al., 2006 Zhang et al. 2006∗ Zhou et al. 2013 Lu et al. 2016 Dong et al. 2016 Char-based (LSTM) + bichar+softword + ExSoftword + bichar+ExSoftword Lattice-LSTM LR-CNN (Gui et al., 2019) SoftLexicon (LSTM) SoftLexicon (LSTM) + bichar BERT-Tagger BERT + LSTM + CRF SoftLexicon (LSTM) + BERT F1 68.57 76.40 75.02 74.32 69.52 75.77 65.63 71.70 64.30 71.89 68.13 72.40 73.88 74.45 75.64 76.16 77.93 81.82 82.81 Models Peng and Dredze, 2015 Peng and Dredze, 2016∗ He and Sun, 2017a He and Sun, 2017b∗ Char-based (LSTM) + bichar+softword + ExSoftword + bichar+ExSoftword Lattice-LSTM LR-CNN (Gui et al., 2019) SoftLe"
2020.acl-main.528,P15-1017,0,0.059364,"Chinese NER datasets show that our method achieves an inference speed up to 6.15 times faster than those of state-ofthe-art methods, along with a better performance. The experimental results also show that the proposed method can be easily incorporated with pre-trained models like BERT. 1 1 Introduction Named Entity Recognition (NER) is concerned with the identification of named entities, such as persons, locations, and organizations, in unstructured text. NER plays an important role in many downstream tasks, including knowledge base construction (Riedel et al., 2013), information retrieval (Chen et al., 2015), and question answering (Diefenbach et al., 2018). In languages where words are naturally separated (e.g., English), NER has been conventionally formulated as a sequence ∗ Equal contribution. The source code of this paper is publicly available at https://github.com/v-mipeng/ LexiconAugmentedNER. 1 labeling problem, and the state-of-the-art results have been achieved using neural-network-based models (Huang et al., 2015; Chiu and Nichols, 2016; Liu et al., 2018). Compared with NER in English, Chinese NER is more difficult since sentences in Chinese are not naturally segmented. Thus, a common p"
2020.acl-main.528,Q16-1026,0,0.127253,"unstructured text. NER plays an important role in many downstream tasks, including knowledge base construction (Riedel et al., 2013), information retrieval (Chen et al., 2015), and question answering (Diefenbach et al., 2018). In languages where words are naturally separated (e.g., English), NER has been conventionally formulated as a sequence ∗ Equal contribution. The source code of this paper is publicly available at https://github.com/v-mipeng/ LexiconAugmentedNER. 1 labeling problem, and the state-of-the-art results have been achieved using neural-network-based models (Huang et al., 2015; Chiu and Nichols, 2016; Liu et al., 2018). Compared with NER in English, Chinese NER is more difficult since sentences in Chinese are not naturally segmented. Thus, a common practice for Chinese NER is to first perform word segmentation using an existing CWS system and then apply a word-level sequence labeling model to the segmented sentence (Yang et al., 2016; He and Sun, 2017b). However, it is inevitable that the CWS system will incorrectly segment query sentences. This will result in errors in the detection of entity boundary and the prediction of entity category in NER. Therefore, some approaches resort to perf"
2020.acl-main.528,N19-1423,0,0.0703552,"Missing"
2020.acl-main.528,P19-1141,0,0.280767,"on using an existing CWS system and then apply a word-level sequence labeling model to the segmented sentence (Yang et al., 2016; He and Sun, 2017b). However, it is inevitable that the CWS system will incorrectly segment query sentences. This will result in errors in the detection of entity boundary and the prediction of entity category in NER. Therefore, some approaches resort to performing Chinese NER directly at the character level, which has been empirically proven to be effective (He and Wang, 2008; Liu et al., 2010; Li et al., 2014; Liu et al., 2019; Sui et al., 2019; Gui et al., 2019b; Ding et al., 2019). A drawback of the purely character-based NER method is that the word information is not fully exploited. With this consideration, Zhang and Yang, (2018) proposed Lattice-LSTM for incorporating word lexicons into the character-based NER model. Moreover, rather than heuristically choosing a word for the character when it matches multiple words in the lexicon, the authors proposed to preserve all words that match the character, leaving the subsequent NER model to determine which word to apply. To realize this idea, they introduced an elaborate modification to the sequence modeling layer of the"
2020.acl-main.528,D19-1096,1,0.748725,"Missing"
2020.acl-main.528,E17-2113,0,0.452886,"on. The source code of this paper is publicly available at https://github.com/v-mipeng/ LexiconAugmentedNER. 1 labeling problem, and the state-of-the-art results have been achieved using neural-network-based models (Huang et al., 2015; Chiu and Nichols, 2016; Liu et al., 2018). Compared with NER in English, Chinese NER is more difficult since sentences in Chinese are not naturally segmented. Thus, a common practice for Chinese NER is to first perform word segmentation using an existing CWS system and then apply a word-level sequence labeling model to the segmented sentence (Yang et al., 2016; He and Sun, 2017b). However, it is inevitable that the CWS system will incorrectly segment query sentences. This will result in errors in the detection of entity boundary and the prediction of entity category in NER. Therefore, some approaches resort to performing Chinese NER directly at the character level, which has been empirically proven to be effective (He and Wang, 2008; Liu et al., 2010; Li et al., 2014; Liu et al., 2019; Sui et al., 2019; Gui et al., 2019b; Ding et al., 2019). A drawback of the purely character-based NER method is that the word information is not fully exploited. With this considerati"
2020.acl-main.528,I08-4022,0,0.050302,"nese are not naturally segmented. Thus, a common practice for Chinese NER is to first perform word segmentation using an existing CWS system and then apply a word-level sequence labeling model to the segmented sentence (Yang et al., 2016; He and Sun, 2017b). However, it is inevitable that the CWS system will incorrectly segment query sentences. This will result in errors in the detection of entity boundary and the prediction of entity category in NER. Therefore, some approaches resort to performing Chinese NER directly at the character level, which has been empirically proven to be effective (He and Wang, 2008; Liu et al., 2010; Li et al., 2014; Liu et al., 2019; Sui et al., 2019; Gui et al., 2019b; Ding et al., 2019). A drawback of the purely character-based NER method is that the word information is not fully exploited. With this consideration, Zhang and Yang, (2018) proposed Lattice-LSTM for incorporating word lexicons into the character-based NER model. Moreover, rather than heuristically choosing a word for the character when it matches multiple words in the lexicon, the authors proposed to preserve all words that match the character, leaving the subsequent NER model to determine which word to"
2020.acl-main.528,W06-0115,0,0.621713,"highest conditional probability given the input sequence s: y ∗ =y p(y|s; θ), (14) which can be efficiently solved using the Viterbi algorithm (Forney, 1973). 4 4.1 Experiments Experiment Setup Most experimental settings in this work followed the protocols of Lattice-LSTM (Zhang and Yang, 2018), including tested datasets, compared baselines, evaluation metrics (P, R, F1), and so on. To make this work self-completed, we concisely illustrate some primary settings of this work. Datasets The methods were evaluated on four Chinese NER datasets, including OntoNotes (Weischedel et al., 2011), MSRA (Levow, 2006), Weibo NER (Peng 5955 OntoNotes 1× 2.23× 2.56× 2.77× 6.15× 6.08× 2.74× MSRA 1× 1.57× 2.55× 2.32× 5.78× 5.95× 2.33× Weibo 1× 2.41× 4.45× 2.84× 6.10× 5.91× 2.85× Resume 1× 1.44× 3.12× 2.38× 6.13× 6.45× 2.32× 175 Table 2: Inference speed (average sentences per second, the larger the better) of our method with LSTM layer compared with Lattice-LSTM, LR-CNN and BERT. and Dredze, 2015; He and Sun, 2017a), and Resume NER (Zhang and Yang, 2018). OntoNotes and MSRA are from the newswire domain, where gold-standard segmentation is available for training data. For OntoNotes, gold segmentation is also ava"
2020.acl-main.528,li-etal-2014-comparison,0,0.270291,"Missing"
2020.acl-main.528,N19-1247,0,0.295037,"ice for Chinese NER is to first perform word segmentation using an existing CWS system and then apply a word-level sequence labeling model to the segmented sentence (Yang et al., 2016; He and Sun, 2017b). However, it is inevitable that the CWS system will incorrectly segment query sentences. This will result in errors in the detection of entity boundary and the prediction of entity category in NER. Therefore, some approaches resort to performing Chinese NER directly at the character level, which has been empirically proven to be effective (He and Wang, 2008; Liu et al., 2010; Li et al., 2014; Liu et al., 2019; Sui et al., 2019; Gui et al., 2019b; Ding et al., 2019). A drawback of the purely character-based NER method is that the word information is not fully exploited. With this consideration, Zhang and Yang, (2018) proposed Lattice-LSTM for incorporating word lexicons into the character-based NER model. Moreover, rather than heuristically choosing a word for the character when it matches multiple words in the lexicon, the authors proposed to preserve all words that match the character, leaving the subsequent NER model to determine which word to apply. To realize this idea, they introduced an elab"
2020.acl-main.528,L16-1138,0,0.210153,"013∗ Wang et al., 2013∗ Word-based (LSTM) + char + bichar Word-based (LSTM) + char + bichar Char-based (LSTM) + bichar + softword + ExSoftword + bichar + ExSoftword Lattice-LSTM LR-CNN (Gui et al., 2019) SoftLexicon (LSTM) SoftLexicon (LSTM) + bichar BERT-Tagger BERT + LSTM + CRF SoftLexicon (LSTM) + BERT P 65.59 72.98 77.71 76.43 76.66 78.62 72.84 73.36 68.79 74.36 69.90 73.80 76.35 76.40 77.28 77.13 76.01 81.99 83.41 R 71.84 80.15 72.51 72.32 63.60 73.13 59.72 70.12 60.35 69.43 66.46 71.05 71.56 72.60 74.07 75.22 79.96 81.65 82.21 Models Chen et al., 2006 Zhang et al. 2006∗ Zhou et al. 2013 Lu et al. 2016 Dong et al. 2016 Char-based (LSTM) + bichar+softword + ExSoftword + bichar+ExSoftword Lattice-LSTM LR-CNN (Gui et al., 2019) SoftLexicon (LSTM) SoftLexicon (LSTM) + bichar BERT-Tagger BERT + LSTM + CRF SoftLexicon (LSTM) + BERT F1 68.57 76.40 75.02 74.32 69.52 75.77 65.63 71.70 64.30 71.89 68.13 72.40 73.88 74.45 75.64 76.16 77.93 81.82 82.81 Models Peng and Dredze, 2015 Peng and Dredze, 2016∗ He and Sun, 2017a He and Sun, 2017b∗ Char-based (LSTM) + bichar+softword + ExSoftword + bichar+ExSoftword Lattice-LSTM LR-CNN (Gui et al., 2019) SoftLexicon (LSTM) SoftLexicon (LSTM) + bichar BERT-Tagge"
2020.acl-main.528,D15-1064,0,0.353453,"Missing"
2020.acl-main.528,W06-0126,0,0.660456,"16 Yang et al., 2016∗† Che et al., 2013∗ Wang et al., 2013∗ Word-based (LSTM) + char + bichar Word-based (LSTM) + char + bichar Char-based (LSTM) + bichar + softword + ExSoftword + bichar + ExSoftword Lattice-LSTM LR-CNN (Gui et al., 2019) SoftLexicon (LSTM) SoftLexicon (LSTM) + bichar BERT-Tagger BERT + LSTM + CRF SoftLexicon (LSTM) + BERT P 65.59 72.98 77.71 76.43 76.66 78.62 72.84 73.36 68.79 74.36 69.90 73.80 76.35 76.40 77.28 77.13 76.01 81.99 83.41 R 71.84 80.15 72.51 72.32 63.60 73.13 59.72 70.12 60.35 69.43 66.46 71.05 71.56 72.60 74.07 75.22 79.96 81.65 82.21 Models Chen et al., 2006 Zhang et al. 2006∗ Zhou et al. 2013 Lu et al. 2016 Dong et al. 2016 Char-based (LSTM) + bichar+softword + ExSoftword + bichar+ExSoftword Lattice-LSTM LR-CNN (Gui et al., 2019) SoftLexicon (LSTM) SoftLexicon (LSTM) + bichar BERT-Tagger BERT + LSTM + CRF SoftLexicon (LSTM) + BERT F1 68.57 76.40 75.02 74.32 69.52 75.77 65.63 71.70 64.30 71.89 68.13 72.40 73.88 74.45 75.64 76.16 77.93 81.82 82.81 Models Peng and Dredze, 2015 Peng and Dredze, 2016∗ He and Sun, 2017a He and Sun, 2017b∗ Char-based (LSTM) + bichar+softword + ExSoftword + bichar+ExSoftword Lattice-LSTM LR-CNN (Gui et al., 2019) SoftLexicon (LSTM) SoftL"
2020.acl-main.528,P18-1144,0,0.753284,"However, it is inevitable that the CWS system will incorrectly segment query sentences. This will result in errors in the detection of entity boundary and the prediction of entity category in NER. Therefore, some approaches resort to performing Chinese NER directly at the character level, which has been empirically proven to be effective (He and Wang, 2008; Liu et al., 2010; Li et al., 2014; Liu et al., 2019; Sui et al., 2019; Gui et al., 2019b; Ding et al., 2019). A drawback of the purely character-based NER method is that the word information is not fully exploited. With this consideration, Zhang and Yang, (2018) proposed Lattice-LSTM for incorporating word lexicons into the character-based NER model. Moreover, rather than heuristically choosing a word for the character when it matches multiple words in the lexicon, the authors proposed to preserve all words that match the character, leaving the subsequent NER model to determine which word to apply. To realize this idea, they introduced an elaborate modification to the sequence modeling layer of the LSTM-CRF model (Huang et al., 2015). Experimental studies on four Chinese NER datasets have verified the effectiveness of Lattice-LSTM. However, the model"
2020.acl-main.528,I08-4017,0,0.0546155,"al., 2018). We performed experiments on four public Chinese NER datasets. The experimental results show that when implementing the sequence modeling layer with a single-layer Bi-LSTM, our method achieves considerable improvements over the state-of-theart methods in both inference speed and sequence labeling performance. 2 Background In this section, we introduce several previous works that influenced our work, including the Softword technique and Lattice-LSTM. 2.1 Softword Feature The Softword technique was originally used for incorporating word segmentation information into downstream tasks (Zhao and Kit, 2008; Peng and Dredze, 2016). It augments the character representation with the embedding of its corresponding segmentation label: xcj ← [xcj ; eseg (seg(cj ))]. (1) Here, seg(cj ) ∈ Yseg denotes the segmentation label of the character cj predicted by the word segmentor, eseg denotes the segmentation label embedding lookup table, and typically Yseg = {B, M, E, S}. However, gold segmentation is not provided in most datasets, and segmentation results obtained by a segmenter can be incorrect. Therefore, segmentation errors will inevitably be introduced through this approach. 2.2 Lattice-LSTM Lattice-"
2020.acl-main.528,P16-2025,0,0.595518,"rmed experiments on four public Chinese NER datasets. The experimental results show that when implementing the sequence modeling layer with a single-layer Bi-LSTM, our method achieves considerable improvements over the state-of-theart methods in both inference speed and sequence labeling performance. 2 Background In this section, we introduce several previous works that influenced our work, including the Softword technique and Lattice-LSTM. 2.1 Softword Feature The Softword technique was originally used for incorporating word segmentation information into downstream tasks (Zhao and Kit, 2008; Peng and Dredze, 2016). It augments the character representation with the embedding of its corresponding segmentation label: xcj ← [xcj ; eseg (seg(cj ))]. (1) Here, seg(cj ) ∈ Yseg denotes the segmentation label of the character cj predicted by the word segmentor, eseg denotes the segmentation label embedding lookup table, and typically Yseg = {B, M, E, S}. However, gold segmentation is not provided in most datasets, and segmentation results obtained by a segmenter can be incorrect. Therefore, segmentation errors will inevitably be introduced through this approach. 2.2 Lattice-LSTM Lattice-LSTM designs to incorpor"
2020.acl-main.528,N13-1008,0,0.0185885,"ation. Experimental studies on four benchmark Chinese NER datasets show that our method achieves an inference speed up to 6.15 times faster than those of state-ofthe-art methods, along with a better performance. The experimental results also show that the proposed method can be easily incorporated with pre-trained models like BERT. 1 1 Introduction Named Entity Recognition (NER) is concerned with the identification of named entities, such as persons, locations, and organizations, in unstructured text. NER plays an important role in many downstream tasks, including knowledge base construction (Riedel et al., 2013), information retrieval (Chen et al., 2015), and question answering (Diefenbach et al., 2018). In languages where words are naturally separated (e.g., English), NER has been conventionally formulated as a sequence ∗ Equal contribution. The source code of this paper is publicly available at https://github.com/v-mipeng/ LexiconAugmentedNER. 1 labeling problem, and the state-of-the-art results have been achieved using neural-network-based models (Huang et al., 2015; Chiu and Nichols, 2016; Liu et al., 2018). Compared with NER in English, Chinese NER is more difficult since sentences in Chinese ar"
2020.acl-main.528,D19-1396,0,0.83911,"Missing"
2020.acl-main.552,P18-3015,0,0.0185503,"ix benchmark datasets. Notably, we obtain a state-of-the-art extractive result on CNN/DailyMail (44.41 in ROUGE-1) by only using the base version of BERT. Moreover, we seek to observe where the performance gain of our model comes from. 2 Related Work 2.1 Extractive Summarization Recent research work on extractive summarization spans a large range of approaches. These work usually instantiate their encoder-decoder framework by choosing RNN (Zhou et al., 2018), Transformer (Zhong et al., 2019b; Wang et al., 2019) or GNN (Wang et al., 2020) as encoder, non-auto-regressive (Narayan et al., 2018b; Arumae and Liu, 2018) or auto-regressive decoders (Jadhav and Rajan, 2018; Liu and Lapata, 2019). Despite the effectiveness, these models are essentially sentence-level extractors with individual scoring process favor the highest scoring sentence, which probably is not the optimal one to form summary1 . The application of RL provides a means of summary-level scoring and brings improvement (Narayan et al., 2018b; Bae et al., 2019). However, these efforts are still limited to auto-regressive or non-auto-regressive architectures. Besides, in the non-neural approaches, the Integer Linear Programming (ILP) method can a"
2020.acl-main.552,D19-5402,0,0.382219,"ly, this simple method of removing duplication brings a remarkable performance improvement on CNN/DailyMail. The above systems of modeling the relationship between sentences are essentially sentence-level extractors, rather than considering the semantics 6197 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6197–6208 c July 5 - 10, 2020. 2020 Association for Computational Linguistics of the entire summary. This makes them more inclined to select highly generalized sentences while ignoring the coupling of multiple sentences. Narayan et al. (2018b); Bae et al. (2019) utilize reinforcement learning (RL) to achieve summarylevel scoring, but still limited to the architecture of sentence-level summarizers. To better understand the advantages and limitations of sentence-level and summary-level approaches, we conduct an analysis on six benchmark datasets (in Section 3) to explore the characteristics of these two methods. We find that there is indeed an inherent gap between the two approaches across these datasets, which motivates us to propose the following summary-level method. In this paper, we propose a novel summary-level framework (M ATCH S UM, Figure 1) a"
2020.acl-main.552,P18-1063,0,0.17681,"sed our codes, processed dataset, as well as generated summaries in https://github. com/maszhongming/MatchSum. 1 BERT one from the original text, model the relationship between the sentences, and then select several sentences to form a summary. Cheng and Lapata (2016); Nallapati et al. (2017) formulate the extractive summarization task as a sequence labeling problem and solve it with an encoder-decoder framework. These models make independent binary decisions for each sentence, resulting in high redundancy. A natural way to address the above problem is to introduce an auto-regressive decoder (Chen and Bansal, 2018; Jadhav and Rajan, 2018; Zhou et al., 2018), allowing the scoring operations of different sentences to influence on each other. Trigram Blocking (Paulus et al., 2017; Liu and Lapata, 2019), as a more popular method recently, has the same motivation. At the stage of selecting sentences to form a summary, it will skip the sentence that has trigram overlapping with the previously selected sentences. Surprisingly, this simple method of removing duplication brings a remarkable performance improvement on CNN/DailyMail. The above systems of modeling the relationship between sentences are essentially"
2020.acl-main.552,P16-1046,0,0.147785,"ave driven the state-of-the-art extractive result on CNN/DailyMail to a new level (44.41 in ROUGE-1). Experiments on the other five datasets also show the effectiveness of the matching framework. We believe the power of this matching-based summarization framework has not been fully exploited. To encourage more instantiations in the future, we have released our codes, processed dataset, as well as generated summaries in https://github. com/maszhongming/MatchSum. 1 BERT one from the original text, model the relationship between the sentences, and then select several sentences to form a summary. Cheng and Lapata (2016); Nallapati et al. (2017) formulate the extractive summarization task as a sequence labeling problem and solve it with an encoder-decoder framework. These models make independent binary decisions for each sentence, resulting in high redundancy. A natural way to address the above problem is to introduce an auto-regressive decoder (Chen and Bansal, 2018; Jadhav and Rajan, 2018; Zhou et al., 2018), allowing the scoring operations of different sentences to influence on each other. Trigram Blocking (Paulus et al., 2017; Liu and Lapata, 2019), as a more popular method recently, has the same motivati"
2020.acl-main.552,N18-2097,0,0.10261,"ain more convicing explanations, we perform experiments on six divergent mainstream datasets as follows. Reddit XSum CNN/DM Wiki PubMed M-News Ext Sel Size 5 1, 2 15 5 1, 2 15 5 2, 3 20 5 3, 4, 5 16 7 6 7 10 9 9 Table 2: Details about the candidate summary for different datasets. Ext denotes the number of sentences after we prune the original document, Sel denotes the number of sentences to form a candidate summary and Size is the number of final candidate summaries. CNN/DailyMail (Hermann et al., 2015) is a commonly used news summarization dataset modified by Nallapati et al. (2016). PubMed (Cohan et al., 2018) is collected from scientific papers. We modify this dataset by using the introduction section as the document and the abstract section as the corresponding summary. WikiHow (Koupaee and Wang, 2018) is a diverse dataset extracted from an online knowledge base. XSum (Narayan et al., 2018a) is a one-sentence summary dataset to answer the question “What is the article about?”. Multi-News (Fabbri et al., 2019) is a multi-document news summarization dataset, we concatenate the source documents as a single input. Reddit (Kim et al., 2019) is a highly abstractive dataset collected from social media p"
2020.acl-main.552,N19-1423,0,0.0177712,"h as information retrieval (Mitra et al., 2017), question answering (Yih et al., 2013; Severyn and Moschitti, 2015), natural language inference (Wang and Jiang, 2016; Wang et al., 2017) and so on. One of the most conventional approaches to semantic text matching is to learn a vector representation for each text fragment, and then apply typical similarity metrics to compute the matching scores. Specific to extractive summarization, we propose a Siamese-BERT architecture to compute the similarity between the source document and the candidate summary. Siamese BERT leverages the pre-trained BERT (Devlin et al., 2019) in a Siamese network structure (Bromley et al., 1994; Hoffer and Ailon, 2015; Reimers and Gurevych, 2019) to derive semantically meaningful text embeddings that can be compared using cosine-similarity. A good summary has the highest similarity among a set of candidate summaries. We evaluate the proposed matching framework and perform significance testing on a range of benchmark datasets. Our model outperforms strong baselines significantly in all cases and improve the state-of-the-art extractive result on CNN/DailyMail. Besides, we design experiments to observe the gains brought by our framew"
2020.acl-main.552,D18-1409,0,0.320551,"Missing"
2020.acl-main.552,P19-1102,0,0.0868974,"te summary and Size is the number of final candidate summaries. CNN/DailyMail (Hermann et al., 2015) is a commonly used news summarization dataset modified by Nallapati et al. (2016). PubMed (Cohan et al., 2018) is collected from scientific papers. We modify this dataset by using the introduction section as the document and the abstract section as the corresponding summary. WikiHow (Koupaee and Wang, 2018) is a diverse dataset extracted from an online knowledge base. XSum (Narayan et al., 2018a) is a one-sentence summary dataset to answer the question “What is the article about?”. Multi-News (Fabbri et al., 2019) is a multi-document news summarization dataset, we concatenate the source documents as a single input. Reddit (Kim et al., 2019) is a highly abstractive dataset collected from social media platform. We use the TIFU-long version of Reddit. 5.2 Implementation Details We use the base version of BERT to implement our models in all experiments. Adam optimizer (Kingma and Ba, 2014) with warming-up is used and our learning rate schedule follows Vaswani et al. (2017) as: lr = 2e−3 · min(step−0.5 , step · wm−1.5 ), (11) where each step is a batch size of 32 and wm denotes warmup steps of 10,000. We ch"
2020.acl-main.552,N10-1131,0,0.17166,"orts are still limited to auto-regressive or non-auto-regressive architectures. Besides, in the non-neural approaches, the Integer Linear Programming (ILP) method can also be used for summarylevel scoring (Wan et al., 2015). In addition, there is some work to solve extractive summarization from a semantic perspective before this paper, such as concept coverage (Gillick 6198 1 We will quantify this phenomenon in Section 3. and Favre, 2009), reconstruction (Miao and Blunsom, 2016) and maximize semantic volume (Yogatama et al., 2015). 2.2 Two-stage Summarization Recent studies (Alyguliyev, 2009; Galanis and Androutsopoulos, 2010; Zhang et al., 2019a) have attempted to build two-stage document summarization systems. Specific to extractive summarization, the first stage is usually to extract some fragments of the original text, and the second stage is to select or modify on the basis of these fragments. Chen and Bansal (2018) and Bae et al. (2019) follow a hybrid extract-then-rewrite architecture, with policy-based RL to bridge the two networks together. Lebanoff et al. (2019); Xu and Durrett (2019); Mendes et al. (2019) focus on the extractthen-compress learning paradigm, which will first train an extractor for conten"
2020.acl-main.552,W09-1802,0,0.140904,"Missing"
2020.acl-main.552,P18-1014,0,0.0825034,"d dataset, as well as generated summaries in https://github. com/maszhongming/MatchSum. 1 BERT one from the original text, model the relationship between the sentences, and then select several sentences to form a summary. Cheng and Lapata (2016); Nallapati et al. (2017) formulate the extractive summarization task as a sequence labeling problem and solve it with an encoder-decoder framework. These models make independent binary decisions for each sentence, resulting in high redundancy. A natural way to address the above problem is to introduce an auto-regressive decoder (Chen and Bansal, 2018; Jadhav and Rajan, 2018; Zhou et al., 2018), allowing the scoring operations of different sentences to influence on each other. Trigram Blocking (Paulus et al., 2017; Liu and Lapata, 2019), as a more popular method recently, has the same motivation. At the stage of selecting sentences to form a summary, it will skip the sentence that has trigram overlapping with the previously selected sentences. Surprisingly, this simple method of removing duplication brings a remarkable performance improvement on CNN/DailyMail. The above systems of modeling the relationship between sentences are essentially sentence-level extracto"
2020.acl-main.552,N19-1260,0,0.0298981,"zation dataset modified by Nallapati et al. (2016). PubMed (Cohan et al., 2018) is collected from scientific papers. We modify this dataset by using the introduction section as the document and the abstract section as the corresponding summary. WikiHow (Koupaee and Wang, 2018) is a diverse dataset extracted from an online knowledge base. XSum (Narayan et al., 2018a) is a one-sentence summary dataset to answer the question “What is the article about?”. Multi-News (Fabbri et al., 2019) is a multi-document news summarization dataset, we concatenate the source documents as a single input. Reddit (Kim et al., 2019) is a highly abstractive dataset collected from social media platform. We use the TIFU-long version of Reddit. 5.2 Implementation Details We use the base version of BERT to implement our models in all experiments. Adam optimizer (Kingma and Ba, 2014) with warming-up is used and our learning rate schedule follows Vaswani et al. (2017) as: lr = 2e−3 · min(step−0.5 , step · wm−1.5 ), (11) where each step is a batch size of 32 and wm denotes warmup steps of 10,000. We choose γ1 = 0 and γ2 = 0.01. When γ1 &lt;0.05 and 0.005&lt;γ2 &lt;0.05 they have little effect on performance, otherwise they will cause per"
2020.acl-main.552,P19-1209,0,0.0413831,"nd Blunsom, 2016) and maximize semantic volume (Yogatama et al., 2015). 2.2 Two-stage Summarization Recent studies (Alyguliyev, 2009; Galanis and Androutsopoulos, 2010; Zhang et al., 2019a) have attempted to build two-stage document summarization systems. Specific to extractive summarization, the first stage is usually to extract some fragments of the original text, and the second stage is to select or modify on the basis of these fragments. Chen and Bansal (2018) and Bae et al. (2019) follow a hybrid extract-then-rewrite architecture, with policy-based RL to bridge the two networks together. Lebanoff et al. (2019); Xu and Durrett (2019); Mendes et al. (2019) focus on the extractthen-compress learning paradigm, which will first train an extractor for content selection. Our model can be viewed as an extract-then-match framework, which also employs a sentence extractor to prune unnecessary information. 3 gsen (C) = 1 X R(s, C∗ ), |C| (1) s∈C where s is the sentence in C and |C |represents the number of sentences. R(·) denotes the average ROUGE score2 . Thus, gsen (C) indicates the average overlaps between each sentence in C and the gold summary C ∗ . 2) Summary-Level Score: gsum (C) = R(C, C ∗ ), (2) wher"
2020.acl-main.552,N03-1020,0,0.495644,"Missing"
2020.acl-main.552,D19-1387,0,0.477529,"s, and then select several sentences to form a summary. Cheng and Lapata (2016); Nallapati et al. (2017) formulate the extractive summarization task as a sequence labeling problem and solve it with an encoder-decoder framework. These models make independent binary decisions for each sentence, resulting in high redundancy. A natural way to address the above problem is to introduce an auto-regressive decoder (Chen and Bansal, 2018; Jadhav and Rajan, 2018; Zhou et al., 2018), allowing the scoring operations of different sentences to influence on each other. Trigram Blocking (Paulus et al., 2017; Liu and Lapata, 2019), as a more popular method recently, has the same motivation. At the stage of selecting sentences to form a summary, it will skip the sentence that has trigram overlapping with the previously selected sentences. Surprisingly, this simple method of removing duplication brings a remarkable performance improvement on CNN/DailyMail. The above systems of modeling the relationship between sentences are essentially sentence-level extractors, rather than considering the semantics 6197 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6197–6208 c July 5 - 10"
2020.acl-main.552,2021.ccl-1.108,0,0.201929,"Missing"
2020.acl-main.552,N19-1397,0,0.0500472,"(Yogatama et al., 2015). 2.2 Two-stage Summarization Recent studies (Alyguliyev, 2009; Galanis and Androutsopoulos, 2010; Zhang et al., 2019a) have attempted to build two-stage document summarization systems. Specific to extractive summarization, the first stage is usually to extract some fragments of the original text, and the second stage is to select or modify on the basis of these fragments. Chen and Bansal (2018) and Bae et al. (2019) follow a hybrid extract-then-rewrite architecture, with policy-based RL to bridge the two networks together. Lebanoff et al. (2019); Xu and Durrett (2019); Mendes et al. (2019) focus on the extractthen-compress learning paradigm, which will first train an extractor for content selection. Our model can be viewed as an extract-then-match framework, which also employs a sentence extractor to prune unnecessary information. 3 gsen (C) = 1 X R(s, C∗ ), |C| (1) s∈C where s is the sentence in C and |C |represents the number of sentences. R(·) denotes the average ROUGE score2 . Thus, gsen (C) indicates the average overlaps between each sentence in C and the gold summary C ∗ . 2) Summary-Level Score: gsum (C) = R(C, C ∗ ), (2) where gsum (C) considers sentences in C as a whol"
2020.acl-main.552,D16-1031,0,0.0217077,"ication of RL provides a means of summary-level scoring and brings improvement (Narayan et al., 2018b; Bae et al., 2019). However, these efforts are still limited to auto-regressive or non-auto-regressive architectures. Besides, in the non-neural approaches, the Integer Linear Programming (ILP) method can also be used for summarylevel scoring (Wan et al., 2015). In addition, there is some work to solve extractive summarization from a semantic perspective before this paper, such as concept coverage (Gillick 6198 1 We will quantify this phenomenon in Section 3. and Favre, 2009), reconstruction (Miao and Blunsom, 2016) and maximize semantic volume (Yogatama et al., 2015). 2.2 Two-stage Summarization Recent studies (Alyguliyev, 2009; Galanis and Androutsopoulos, 2010; Zhang et al., 2019a) have attempted to build two-stage document summarization systems. Specific to extractive summarization, the first stage is usually to extract some fragments of the original text, and the second stage is to select or modify on the basis of these fragments. Chen and Bansal (2018) and Bae et al. (2019) follow a hybrid extract-then-rewrite architecture, with policy-based RL to bridge the two networks together. Lebanoff et al. ("
2020.acl-main.552,K16-1028,0,0.105737,"Missing"
2020.acl-main.552,D18-1206,0,0.460092,"ed sentences. Surprisingly, this simple method of removing duplication brings a remarkable performance improvement on CNN/DailyMail. The above systems of modeling the relationship between sentences are essentially sentence-level extractors, rather than considering the semantics 6197 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6197–6208 c July 5 - 10, 2020. 2020 Association for Computational Linguistics of the entire summary. This makes them more inclined to select highly generalized sentences while ignoring the coupling of multiple sentences. Narayan et al. (2018b); Bae et al. (2019) utilize reinforcement learning (RL) to achieve summarylevel scoring, but still limited to the architecture of sentence-level summarizers. To better understand the advantages and limitations of sentence-level and summary-level approaches, we conduct an analysis on six benchmark datasets (in Section 3) to explore the characteristics of these two methods. We find that there is indeed an inherent gap between the two approaches across these datasets, which motivates us to propose the following summary-level method. In this paper, we propose a novel summary-level framework (M A"
2020.acl-main.552,N18-1158,0,0.356783,"ed sentences. Surprisingly, this simple method of removing duplication brings a remarkable performance improvement on CNN/DailyMail. The above systems of modeling the relationship between sentences are essentially sentence-level extractors, rather than considering the semantics 6197 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6197–6208 c July 5 - 10, 2020. 2020 Association for Computational Linguistics of the entire summary. This makes them more inclined to select highly generalized sentences while ignoring the coupling of multiple sentences. Narayan et al. (2018b); Bae et al. (2019) utilize reinforcement learning (RL) to achieve summarylevel scoring, but still limited to the architecture of sentence-level summarizers. To better understand the advantages and limitations of sentence-level and summary-level approaches, we conduct an analysis on six benchmark datasets (in Section 3) to explore the characteristics of these two methods. We find that there is indeed an inherent gap between the two approaches across these datasets, which motivates us to propose the following summary-level method. In this paper, we propose a novel summary-level framework (M A"
2020.acl-main.552,D19-1410,0,0.023607,"Moschitti, 2015), natural language inference (Wang and Jiang, 2016; Wang et al., 2017) and so on. One of the most conventional approaches to semantic text matching is to learn a vector representation for each text fragment, and then apply typical similarity metrics to compute the matching scores. Specific to extractive summarization, we propose a Siamese-BERT architecture to compute the similarity between the source document and the candidate summary. Siamese BERT leverages the pre-trained BERT (Devlin et al., 2019) in a Siamese network structure (Bromley et al., 1994; Hoffer and Ailon, 2015; Reimers and Gurevych, 2019) to derive semantically meaningful text embeddings that can be compared using cosine-similarity. A good summary has the highest similarity among a set of candidate summaries. We evaluate the proposed matching framework and perform significance testing on a range of benchmark datasets. Our model outperforms strong baselines significantly in all cases and improve the state-of-the-art extractive result on CNN/DailyMail. Besides, we design experiments to observe the gains brought by our framework. We summarize our contributions as follows: 1) Instead of scoring and extracting sentences one by one"
2020.acl-main.552,2020.acl-main.553,1,0.763028,"ework has achieved superior performance compared with strong baselines on six benchmark datasets. Notably, we obtain a state-of-the-art extractive result on CNN/DailyMail (44.41 in ROUGE-1) by only using the base version of BERT. Moreover, we seek to observe where the performance gain of our model comes from. 2 Related Work 2.1 Extractive Summarization Recent research work on extractive summarization spans a large range of approaches. These work usually instantiate their encoder-decoder framework by choosing RNN (Zhou et al., 2018), Transformer (Zhong et al., 2019b; Wang et al., 2019) or GNN (Wang et al., 2020) as encoder, non-auto-regressive (Narayan et al., 2018b; Arumae and Liu, 2018) or auto-regressive decoders (Jadhav and Rajan, 2018; Liu and Lapata, 2019). Despite the effectiveness, these models are essentially sentence-level extractors with individual scoring process favor the highest scoring sentence, which probably is not the optimal one to form summary1 . The application of RL provides a means of summary-level scoring and brings improvement (Narayan et al., 2018b; Bae et al., 2019). However, these efforts are still limited to auto-regressive or non-auto-regressive architectures. Besides, i"
2020.acl-main.552,N16-1170,0,0.0128706,"a novel summary-level framework (M ATCH S UM, Figure 1) and conceptualize extractive summarization as a semantic text matching problem. The principle idea is that a good summary should be more semantically similar as a whole to the source document than the unqualified summaries. Semantic text matching is an important research problem to estimate semantic similarity between a source and a target text fragment, which has been applied in many fields, such as information retrieval (Mitra et al., 2017), question answering (Yih et al., 2013; Severyn and Moschitti, 2015), natural language inference (Wang and Jiang, 2016; Wang et al., 2017) and so on. One of the most conventional approaches to semantic text matching is to learn a vector representation for each text fragment, and then apply typical similarity metrics to compute the matching scores. Specific to extractive summarization, we propose a Siamese-BERT architecture to compute the similarity between the source document and the candidate summary. Siamese BERT leverages the pre-trained BERT (Devlin et al., 2019) in a Siamese network structure (Bromley et al., 1994; Hoffer and Ailon, 2015; Reimers and Gurevych, 2019) to derive semantically meaningful text"
2020.acl-main.552,D19-1324,0,0.0938029,"ximize semantic volume (Yogatama et al., 2015). 2.2 Two-stage Summarization Recent studies (Alyguliyev, 2009; Galanis and Androutsopoulos, 2010; Zhang et al., 2019a) have attempted to build two-stage document summarization systems. Specific to extractive summarization, the first stage is usually to extract some fragments of the original text, and the second stage is to select or modify on the basis of these fragments. Chen and Bansal (2018) and Bae et al. (2019) follow a hybrid extract-then-rewrite architecture, with policy-based RL to bridge the two networks together. Lebanoff et al. (2019); Xu and Durrett (2019); Mendes et al. (2019) focus on the extractthen-compress learning paradigm, which will first train an extractor for content selection. Our model can be viewed as an extract-then-match framework, which also employs a sentence extractor to prune unnecessary information. 3 gsen (C) = 1 X R(s, C∗ ), |C| (1) s∈C where s is the sentence in C and |C |represents the number of sentences. R(·) denotes the average ROUGE score2 . Thus, gsen (C) indicates the average overlaps between each sentence in C and the gold summary C ∗ . 2) Summary-Level Score: gsum (C) = R(C, C ∗ ), (2) where gsum (C) considers se"
2020.acl-main.552,P13-1171,0,0.0389533,"us to propose the following summary-level method. In this paper, we propose a novel summary-level framework (M ATCH S UM, Figure 1) and conceptualize extractive summarization as a semantic text matching problem. The principle idea is that a good summary should be more semantically similar as a whole to the source document than the unqualified summaries. Semantic text matching is an important research problem to estimate semantic similarity between a source and a target text fragment, which has been applied in many fields, such as information retrieval (Mitra et al., 2017), question answering (Yih et al., 2013; Severyn and Moschitti, 2015), natural language inference (Wang and Jiang, 2016; Wang et al., 2017) and so on. One of the most conventional approaches to semantic text matching is to learn a vector representation for each text fragment, and then apply typical similarity metrics to compute the matching scores. Specific to extractive summarization, we propose a Siamese-BERT architecture to compute the similarity between the source document and the candidate summary. Siamese BERT leverages the pre-trained BERT (Devlin et al., 2019) in a Siamese network structure (Bromley et al., 1994; Hoffer and"
2020.acl-main.552,D15-1228,0,0.0168694,"g and brings improvement (Narayan et al., 2018b; Bae et al., 2019). However, these efforts are still limited to auto-regressive or non-auto-regressive architectures. Besides, in the non-neural approaches, the Integer Linear Programming (ILP) method can also be used for summarylevel scoring (Wan et al., 2015). In addition, there is some work to solve extractive summarization from a semantic perspective before this paper, such as concept coverage (Gillick 6198 1 We will quantify this phenomenon in Section 3. and Favre, 2009), reconstruction (Miao and Blunsom, 2016) and maximize semantic volume (Yogatama et al., 2015). 2.2 Two-stage Summarization Recent studies (Alyguliyev, 2009; Galanis and Androutsopoulos, 2010; Zhang et al., 2019a) have attempted to build two-stage document summarization systems. Specific to extractive summarization, the first stage is usually to extract some fragments of the original text, and the second stage is to select or modify on the basis of these fragments. Chen and Bansal (2018) and Bae et al. (2019) follow a hybrid extract-then-rewrite architecture, with policy-based RL to bridge the two networks together. Lebanoff et al. (2019); Xu and Durrett (2019); Mendes et al. (2019) fo"
2020.acl-main.552,K19-1074,0,0.12965,"essive or non-auto-regressive architectures. Besides, in the non-neural approaches, the Integer Linear Programming (ILP) method can also be used for summarylevel scoring (Wan et al., 2015). In addition, there is some work to solve extractive summarization from a semantic perspective before this paper, such as concept coverage (Gillick 6198 1 We will quantify this phenomenon in Section 3. and Favre, 2009), reconstruction (Miao and Blunsom, 2016) and maximize semantic volume (Yogatama et al., 2015). 2.2 Two-stage Summarization Recent studies (Alyguliyev, 2009; Galanis and Androutsopoulos, 2010; Zhang et al., 2019a) have attempted to build two-stage document summarization systems. Specific to extractive summarization, the first stage is usually to extract some fragments of the original text, and the second stage is to select or modify on the basis of these fragments. Chen and Bansal (2018) and Bae et al. (2019) follow a hybrid extract-then-rewrite architecture, with policy-based RL to bridge the two networks together. Lebanoff et al. (2019); Xu and Durrett (2019); Mendes et al. (2019) focus on the extractthen-compress learning paradigm, which will first train an extractor for content selection. Our mod"
2020.acl-main.552,P19-1499,0,0.256957,"essive or non-auto-regressive architectures. Besides, in the non-neural approaches, the Integer Linear Programming (ILP) method can also be used for summarylevel scoring (Wan et al., 2015). In addition, there is some work to solve extractive summarization from a semantic perspective before this paper, such as concept coverage (Gillick 6198 1 We will quantify this phenomenon in Section 3. and Favre, 2009), reconstruction (Miao and Blunsom, 2016) and maximize semantic volume (Yogatama et al., 2015). 2.2 Two-stage Summarization Recent studies (Alyguliyev, 2009; Galanis and Androutsopoulos, 2010; Zhang et al., 2019a) have attempted to build two-stage document summarization systems. Specific to extractive summarization, the first stage is usually to extract some fragments of the original text, and the second stage is to select or modify on the basis of these fragments. Chen and Bansal (2018) and Bae et al. (2019) follow a hybrid extract-then-rewrite architecture, with policy-based RL to bridge the two networks together. Lebanoff et al. (2019); Xu and Durrett (2019); Mendes et al. (2019) focus on the extractthen-compress learning paradigm, which will first train an extractor for content selection. Our mod"
2020.acl-main.552,P19-1100,1,0.9257,"and summary-level methods. 3) Our proposed framework has achieved superior performance compared with strong baselines on six benchmark datasets. Notably, we obtain a state-of-the-art extractive result on CNN/DailyMail (44.41 in ROUGE-1) by only using the base version of BERT. Moreover, we seek to observe where the performance gain of our model comes from. 2 Related Work 2.1 Extractive Summarization Recent research work on extractive summarization spans a large range of approaches. These work usually instantiate their encoder-decoder framework by choosing RNN (Zhou et al., 2018), Transformer (Zhong et al., 2019b; Wang et al., 2019) or GNN (Wang et al., 2020) as encoder, non-auto-regressive (Narayan et al., 2018b; Arumae and Liu, 2018) or auto-regressive decoders (Jadhav and Rajan, 2018; Liu and Lapata, 2019). Despite the effectiveness, these models are essentially sentence-level extractors with individual scoring process favor the highest scoring sentence, which probably is not the optimal one to form summary1 . The application of RL provides a means of summary-level scoring and brings improvement (Narayan et al., 2018b; Bae et al., 2019). However, these efforts are still limited to auto-regressive"
2020.acl-main.552,D19-5410,1,0.904806,"Missing"
2020.acl-main.552,P18-1061,0,0.591782,"nerated summaries in https://github. com/maszhongming/MatchSum. 1 BERT one from the original text, model the relationship between the sentences, and then select several sentences to form a summary. Cheng and Lapata (2016); Nallapati et al. (2017) formulate the extractive summarization task as a sequence labeling problem and solve it with an encoder-decoder framework. These models make independent binary decisions for each sentence, resulting in high redundancy. A natural way to address the above problem is to introduce an auto-regressive decoder (Chen and Bansal, 2018; Jadhav and Rajan, 2018; Zhou et al., 2018), allowing the scoring operations of different sentences to influence on each other. Trigram Blocking (Paulus et al., 2017; Liu and Lapata, 2019), as a more popular method recently, has the same motivation. At the stage of selecting sentences to form a summary, it will skip the sentence that has trigram overlapping with the previously selected sentences. Surprisingly, this simple method of removing duplication brings a remarkable performance improvement on CNN/DailyMail. The above systems of modeling the relationship between sentences are essentially sentence-level extractors, rather than cons"
2020.acl-main.553,P16-1046,0,0.589481,"l extension from a singledocument setting to multi-document via introducing document nodes. To our knowledge, we are the first one to introduce different types of nodes into graph-based neural networks for extractive document summarization and perform a comprehensive qualitative analysis to investigate their benefits. The code will be released on Github1 . 1 Introduction Extractive document summarization aims to extract relevant sentences from the original documents and reorganize them as the summary. Recent years have seen a resounding success in the use of deep neural networks on this task (Cheng and Lapata, 2016; Narayan et al., 2018; Arumae and Liu, 2018; Zhong et al., 2019a; Liu and Lapata, 2019b). These existing models mainly follow the encoder-decoder framework in which each sentence will be encoded by neural components with different forms. To effectively extract the summary-worthy sentences from a document, a core step is to model ∗ These two authors contributed equally. Corresponding author. 1 https://github.com/brxx122/ HeterSUMGraph † the cross-sentence relations. Most current models capture cross-sentence relations with recurrent neural networks (RNNs) (Cheng and Lapata, 2016; Nallapati et"
2020.acl-main.553,D18-1409,0,0.600455,"Missing"
2020.acl-main.553,P16-1188,0,0.0356539,"1 Datasets CNN/DailyMail The CNN/DailyMail question answering dataset (Hermann et al., 2015; Nallapati et al., 2016) is the most widely used benchmark dataset for single-document summarization. The standard dataset split contains 287,227/13,368/11,490 examples for training, validation, and test. For the data prepossessing, we follow Liu and Lapata (2019b), which use the nonanonymized version as See et al. (2017), to get ground-truth labels. NYT50 NYT50 is also a single-document summarization dataset, which was collected from New York Times Annotated Corpus (Sandhaus, 2008) and preprocessed by Durrett et al. (2016). It contains 110,540 articles with summaries and is split into 100,834 and 9706 for training and test. Following Durrett et al. (2016), we use the last 4,000 examples from the training set as validation and filter test examples to 3,452. Multi-News The Multi-News dataset is a largescale multi-document summarization introduced by Fabbri et al. (2019). It contains 56,216 articlessummary pairs and each example consists of 2-10 source documents and a human-written summary. Following their experimental settings, we split the dataset into 44,972/5,622/5,622 for training, validation and test example"
2020.acl-main.553,P19-1102,0,0.41741,"ile d2 contains s21 and s22 . As a relay node, the relation of document-document, sentence-sentence, and sentencedocument can be built through the common word nodes. For example, sentence s11 , s12 and s21 share the same word w1 , which connects them across documents. 3.5 Multi-document Summarization For multi-document summarization, the documentlevel relation is crucial for better understanding the core topic and most important content of this cluster. However, most existing neural models ignore this hierarchical structure and concatenate documents to a single flat sequence(Liu et al., 2018; Fabbri et al., 2019). Others try to model this relation by attention-based full-connected graph or take advantage of similarity or discourse relations(Liu and Lapata, 2019a). Our framework can establish the document-level relationship in the same way as the sentence-level by just adding supernodes for documents(as Figure 3), which means it can be easily adapted from single-document to multi-document summarization. The heterogeneous graph is then extended to three types of nodes: V = Vw ∪ Vs ∪ Vd and Vd = {d1 , · · · , dl } and l is the number of source documents. We name it as H ETER D OC SUMG RAPH. As we can see"
2020.acl-main.553,D18-1443,0,0.112597,"Missing"
2020.acl-main.553,D18-1446,0,0.0861031,"Missing"
2020.acl-main.553,N03-1020,0,0.30361,"Missing"
2020.acl-main.553,D19-1488,0,0.0417672,"nd their associated learning methods (i.e. message passing (Gilmer et al., 2017), selfattention (Velickovic et al., 2017)) are originally designed for the homogeneous graph where the whole graph shares the same type of nodes. However, the graph in the real-world application usually comes with multiple types of nodes (Shi et al., 2016), namely the heterogeneous graph. To model these structures, recent works have made preliminary exploration. Tu et al. (2019) introduced a heterogeneous graph neural network to encode documents, entities and candidates together for multihop reading comprehension. Linmei et al. (2019) focused on semi-supervised short text classification and constructed a topic-entity heterogeneous neural graph. For summarization, Wei (2012) proposes a heterogeneous graph consisting of topic, word and sentence nodes and uses the markov chain model for the iterative update. Wang et al. (2019b) modify TextRank for their graph with keywords and sentences and thus put forward HeteroRank. Inspired by the success of the heterogeneous graph-based neural network on other NLP tasks, we introduce it to extractive text summarization to learn a better node representation. 3 Methodology Given a document"
2020.acl-main.553,P19-1500,0,0.0706958,"s. To our knowledge, we are the first one to introduce different types of nodes into graph-based neural networks for extractive document summarization and perform a comprehensive qualitative analysis to investigate their benefits. The code will be released on Github1 . 1 Introduction Extractive document summarization aims to extract relevant sentences from the original documents and reorganize them as the summary. Recent years have seen a resounding success in the use of deep neural networks on this task (Cheng and Lapata, 2016; Narayan et al., 2018; Arumae and Liu, 2018; Zhong et al., 2019a; Liu and Lapata, 2019b). These existing models mainly follow the encoder-decoder framework in which each sentence will be encoded by neural components with different forms. To effectively extract the summary-worthy sentences from a document, a core step is to model ∗ These two authors contributed equally. Corresponding author. 1 https://github.com/brxx122/ HeterSUMGraph † the cross-sentence relations. Most current models capture cross-sentence relations with recurrent neural networks (RNNs) (Cheng and Lapata, 2016; Nallapati et al., 2017; Zhou et al., 2018). However, RNNs-based models are usually hard to capture s"
2020.acl-main.553,D19-1387,0,0.0591542,"s. To our knowledge, we are the first one to introduce different types of nodes into graph-based neural networks for extractive document summarization and perform a comprehensive qualitative analysis to investigate their benefits. The code will be released on Github1 . 1 Introduction Extractive document summarization aims to extract relevant sentences from the original documents and reorganize them as the summary. Recent years have seen a resounding success in the use of deep neural networks on this task (Cheng and Lapata, 2016; Narayan et al., 2018; Arumae and Liu, 2018; Zhong et al., 2019a; Liu and Lapata, 2019b). These existing models mainly follow the encoder-decoder framework in which each sentence will be encoded by neural components with different forms. To effectively extract the summary-worthy sentences from a document, a core step is to model ∗ These two authors contributed equally. Corresponding author. 1 https://github.com/brxx122/ HeterSUMGraph † the cross-sentence relations. Most current models capture cross-sentence relations with recurrent neural networks (RNNs) (Cheng and Lapata, 2016; Nallapati et al., 2017; Zhou et al., 2018). However, RNNs-based models are usually hard to capture s"
2020.acl-main.553,N19-1173,0,0.0488689,"Missing"
2020.acl-main.553,D19-1300,0,0.145067,"ed as sentence-word-sentence relationships. H ETER SUMG RAPH directly selects sentences for the summary by node classification, while H ETER SUMG RAPH with trigram blocking further utilizes the n-gram blocking to reduce redundancy. 3 The detailed experimental results are attached in the Appendix Section. R-2 R-L L EAD -3 (See et al., 2017) O RACLE (Liu and Lapata, 2019b) 40.34 17.70 36.57 52.59 31.24 48.87 REFRESH (Narayan et al., 2018) LATENT (Zhang et al., 2018) BanditSum (Dong et al., 2018) NeuSUM (Zhou et al., 2018) JECS (Xu and Durrett, 2019) LSTM+PN (Zhong et al., 2019a) HER w/o Policy (Luo et al., 2019) HER w Policy (Luo et al., 2019) 40.00 41.05 41.50 41.59 41.70 41.85 41.70 42.30 18.20 18.77 18.70 19.01 18.50 18.93 18.30 18.90 36.60 37.54 37.60 37.98 37.90 38.13 37.10 37.60 Ext-BiLSTM Ext-Transformer HSG HSG + Tri-Blocking 41.59 41.33 42.31 42.95 19.03 18.83 19.51 19.76 38.04 37.65 38.74 39.23 Table 1: Performance (Rouge) of our proposed models against recently released summarization systems on CNN/DailyMail. 5 5.1 4.3 R-1 Results and Analysis Single-document Summarization We evaluate our single-document model on CNN/DailyMail and NYT50 and report the unigram, bigram and longest common sub"
2020.acl-main.553,W04-3252,0,0.84773,"s) (Cheng and Lapata, 2016; Nallapati et al., 2017; Zhou et al., 2018). However, RNNs-based models are usually hard to capture sentence-level long-distance dependency, especially in the case of the long document or multidocuments. One more intuitive way is to model the relations of sentences using the graph structure. Nevertheless, it is challenging to find an effective graph structure for summarization. Efforts have been made in various ways. Early traditional work makes use of inter-sentence cosine similarity to build the connectivity graph like LexRank (Erkan and Radev, 2004) and TextRank (Mihalcea and Tarau, 2004). Recently, some works account for discourse inter-sentential relationships when building summarization graphs, such as the Approximate Discourse Graph (ADG) with sentence personalization features (Yasunaga et al., 2017) and Rhetorical Structure Theory (RST) graph (Xu et al., 2019). However, they usually rely on external tools and need to take account of the error propagation problem. A more straightforward way is to create a sentence-level fully-connected graph. To some extent, the Transformer encoder (Vaswani et al., 2017) used in recent work(Zhong et al., 2019a; Liu and Lapata, 2019b) can b"
2020.acl-main.553,N18-1158,0,0.504042,"edocument setting to multi-document via introducing document nodes. To our knowledge, we are the first one to introduce different types of nodes into graph-based neural networks for extractive document summarization and perform a comprehensive qualitative analysis to investigate their benefits. The code will be released on Github1 . 1 Introduction Extractive document summarization aims to extract relevant sentences from the original documents and reorganize them as the summary. Recent years have seen a resounding success in the use of deep neural networks on this task (Cheng and Lapata, 2016; Narayan et al., 2018; Arumae and Liu, 2018; Zhong et al., 2019a; Liu and Lapata, 2019b). These existing models mainly follow the encoder-decoder framework in which each sentence will be encoded by neural components with different forms. To effectively extract the summary-worthy sentences from a document, a core step is to model ∗ These two authors contributed equally. Corresponding author. 1 https://github.com/brxx122/ HeterSUMGraph † the cross-sentence relations. Most current models capture cross-sentence relations with recurrent neural networks (RNNs) (Cheng and Lapata, 2016; Nallapati et al., 2017; Zhou et al."
2020.acl-main.553,D14-1162,0,0.0827789,"Missing"
2020.acl-main.553,P17-1099,0,0.0994329,"he same update process as sentence nodes. Experiment We evaluate our models both on single- and multidocument summarization tasks. Below, we start our experiment with the description of the datasets. 4.1 Datasets CNN/DailyMail The CNN/DailyMail question answering dataset (Hermann et al., 2015; Nallapati et al., 2016) is the most widely used benchmark dataset for single-document summarization. The standard dataset split contains 287,227/13,368/11,490 examples for training, validation, and test. For the data prepossessing, we follow Liu and Lapata (2019b), which use the nonanonymized version as See et al. (2017), to get ground-truth labels. NYT50 NYT50 is also a single-document summarization dataset, which was collected from New York Times Annotated Corpus (Sandhaus, 2008) and preprocessed by Durrett et al. (2016). It contains 110,540 articles with summaries and is split into 100,834 and 9706 for training and test. Following Durrett et al. (2016), we use the last 4,000 examples from the training set as validation and filter test examples to 3,452. Multi-News The Multi-News dataset is a largescale multi-document summarization introduced by Fabbri et al. (2019). It contains 56,216 articlessummary pairs"
2020.acl-main.553,P19-1260,0,0.03723,"ogically, these works only use one type of nodes, which formulate each document as a homogeneous graph. Heterogeneous Graph for NLP Graph neural networks and their associated learning methods (i.e. message passing (Gilmer et al., 2017), selfattention (Velickovic et al., 2017)) are originally designed for the homogeneous graph where the whole graph shares the same type of nodes. However, the graph in the real-world application usually comes with multiple types of nodes (Shi et al., 2016), namely the heterogeneous graph. To model these structures, recent works have made preliminary exploration. Tu et al. (2019) introduced a heterogeneous graph neural network to encode documents, entities and candidates together for multihop reading comprehension. Linmei et al. (2019) focused on semi-supervised short text classification and constructed a topic-entity heterogeneous neural graph. For summarization, Wei (2012) proposes a heterogeneous graph consisting of topic, word and sentence nodes and uses the markov chain model for the iterative update. Wang et al. (2019b) modify TextRank for their graph with keywords and sentences and thus put forward HeteroRank. Inspired by the success of the heterogeneous graph-"
2020.acl-main.553,D19-5410,1,0.583487,"oducing document nodes. To our knowledge, we are the first one to introduce different types of nodes into graph-based neural networks for extractive document summarization and perform a comprehensive qualitative analysis to investigate their benefits. The code will be released on Github1 . 1 Introduction Extractive document summarization aims to extract relevant sentences from the original documents and reorganize them as the summary. Recent years have seen a resounding success in the use of deep neural networks on this task (Cheng and Lapata, 2016; Narayan et al., 2018; Arumae and Liu, 2018; Zhong et al., 2019a; Liu and Lapata, 2019b). These existing models mainly follow the encoder-decoder framework in which each sentence will be encoded by neural components with different forms. To effectively extract the summary-worthy sentences from a document, a core step is to model ∗ These two authors contributed equally. Corresponding author. 1 https://github.com/brxx122/ HeterSUMGraph † the cross-sentence relations. Most current models capture cross-sentence relations with recurrent neural networks (RNNs) (Cheng and Lapata, 2016; Nallapati et al., 2017; Zhou et al., 2018). However, RNNs-based models are us"
2020.acl-main.553,P18-1061,0,0.592842,"t al., 2018; Arumae and Liu, 2018; Zhong et al., 2019a; Liu and Lapata, 2019b). These existing models mainly follow the encoder-decoder framework in which each sentence will be encoded by neural components with different forms. To effectively extract the summary-worthy sentences from a document, a core step is to model ∗ These two authors contributed equally. Corresponding author. 1 https://github.com/brxx122/ HeterSUMGraph † the cross-sentence relations. Most current models capture cross-sentence relations with recurrent neural networks (RNNs) (Cheng and Lapata, 2016; Nallapati et al., 2017; Zhou et al., 2018). However, RNNs-based models are usually hard to capture sentence-level long-distance dependency, especially in the case of the long document or multidocuments. One more intuitive way is to model the relations of sentences using the graph structure. Nevertheless, it is challenging to find an effective graph structure for summarization. Efforts have been made in various ways. Early traditional work makes use of inter-sentence cosine similarity to build the connectivity graph like LexRank (Erkan and Radev, 2004) and TextRank (Mihalcea and Tarau, 2004). Recently, some works account for discourse"
2020.acl-main.553,D19-1324,0,0.321381,"Missing"
2020.acl-main.553,K17-1045,0,0.134843,"uments. One more intuitive way is to model the relations of sentences using the graph structure. Nevertheless, it is challenging to find an effective graph structure for summarization. Efforts have been made in various ways. Early traditional work makes use of inter-sentence cosine similarity to build the connectivity graph like LexRank (Erkan and Radev, 2004) and TextRank (Mihalcea and Tarau, 2004). Recently, some works account for discourse inter-sentential relationships when building summarization graphs, such as the Approximate Discourse Graph (ADG) with sentence personalization features (Yasunaga et al., 2017) and Rhetorical Structure Theory (RST) graph (Xu et al., 2019). However, they usually rely on external tools and need to take account of the error propagation problem. A more straightforward way is to create a sentence-level fully-connected graph. To some extent, the Transformer encoder (Vaswani et al., 2017) used in recent work(Zhong et al., 2019a; Liu and Lapata, 2019b) can be classified into this type, which learns the pairwise interaction between sentences. Despite their success, how to construct an effective graph structure for summarization remains an open question. In this paper, we pro"
2020.acl-main.553,D18-1088,0,0.180792,"Missing"
2020.acl-main.553,2020.acl-main.552,1,0.763251,"use recurrent neural networks (Cheng and Lapata, 2016; Nallapati et al., 2017; Zhou et al., 2018) or Transformer encoders 2 Since our proposed model is orthogonal to the methods that using pre-trained models, we believe our model can be further boosted by taking the pre-trained models to initialize the node representations, which we reserve for the future. (Zhong et al., 2019b; Wang et al., 2019a) for the sentential encoding. Recently, pre-trained language models are also applied in summarization for contextual word representations (Zhong et al., 2019a; Liu and Lapata, 2019b; Xu et al., 2019; Zhong et al., 2020). Another intuitive structure for extractive summarization is the graph, which can better utilize the statistical or linguistic information between sentences. Early works focus on document graphs constructed with the content similarity among sentences, like LexRank (Erkan and Radev, 2004) and TextRank (Mihalcea and Tarau, 2004). Some recent works aim to incorporate a relational priori into the encoder by graph neural networks (GNNs) (Yasunaga et al., 2017; Xu et al., 2019). Methodologically, these works only use one type of nodes, which formulate each document as a homogeneous graph. Heterogen"
2020.acl-main.553,P19-1100,1,0.6356,"oducing document nodes. To our knowledge, we are the first one to introduce different types of nodes into graph-based neural networks for extractive document summarization and perform a comprehensive qualitative analysis to investigate their benefits. The code will be released on Github1 . 1 Introduction Extractive document summarization aims to extract relevant sentences from the original documents and reorganize them as the summary. Recent years have seen a resounding success in the use of deep neural networks on this task (Cheng and Lapata, 2016; Narayan et al., 2018; Arumae and Liu, 2018; Zhong et al., 2019a; Liu and Lapata, 2019b). These existing models mainly follow the encoder-decoder framework in which each sentence will be encoded by neural components with different forms. To effectively extract the summary-worthy sentences from a document, a core step is to model ∗ These two authors contributed equally. Corresponding author. 1 https://github.com/brxx122/ HeterSUMGraph † the cross-sentence relations. Most current models capture cross-sentence relations with recurrent neural networks (RNNs) (Cheng and Lapata, 2016; Nallapati et al., 2017; Zhou et al., 2018). However, RNNs-based models are us"
2020.acl-main.590,D18-1316,0,0.0604228,"et al., 2018; Xu et al., 2019). Even though generating adversarial examples for texts has proven to be a more challenging task These authors contributed equally to this work. between the IN DT punct nn nsubj Introduction ∗ link NN conj cc det than for images and audios due to their discrete nature, a few methods have been proposed to generate adversarial text examples and reveal the vulnerability of deep neural networks in natural language processing (NLP) tasks including reading comprehension (Jia and Liang, 2017), text classification (Samanta and Mehta, 2017; Wong, 2017; Liang et al., 2018; Alzantot et al., 2018), machine translation (Zhao et al., 2018; Ebrahimi et al., 2018; Cheng et al., 2018) and dialogue systems (Cheng et al., 2019). These recent methods attack text examples mainly by replacing, scrambling, and erasing characters or words or other language units under certain semantics-preserving constraints. Although adversarial examples have been studied recently for NLP tasks, previous work almost exclusively focused on semantic tasks, where the attacks aim to alter the semantic prediction of ML models (e.g., sentiment prediction or question answering) without changing the meaning of original t"
2020.acl-main.590,W06-2920,0,0.149563,"Missing"
2020.acl-main.590,N19-1336,1,0.834713,"ese authors contributed equally to this work. between the IN DT punct nn nsubj Introduction ∗ link NN conj cc det than for images and audios due to their discrete nature, a few methods have been proposed to generate adversarial text examples and reveal the vulnerability of deep neural networks in natural language processing (NLP) tasks including reading comprehension (Jia and Liang, 2017), text classification (Samanta and Mehta, 2017; Wong, 2017; Liang et al., 2018; Alzantot et al., 2018), machine translation (Zhao et al., 2018; Ebrahimi et al., 2018; Cheng et al., 2018) and dialogue systems (Cheng et al., 2019). These recent methods attack text examples mainly by replacing, scrambling, and erasing characters or words or other language units under certain semantics-preserving constraints. Although adversarial examples have been studied recently for NLP tasks, previous work almost exclusively focused on semantic tasks, where the attacks aim to alter the semantic prediction of ML models (e.g., sentiment prediction or question answering) without changing the meaning of original texts. To the best of our knowledge, adversarial 6600 Proceedings of the 58th Annual Meeting of the Association for Computation"
2020.acl-main.590,N19-1423,0,0.0237455,"owing three constraints that should be satisfied by the word replacement when generating the adversarial examples x0 : (i) The substitute word x0i should fit in well with the context, and can maintain both the semantic and syntactic coherence. (ii) For any word xi in an original example, the word x0i to replace xi must have the same partof-speech (POS) as xi . (iii) Pronouns, articles, conjunctions, numerals, interjections, interrogative determiners, and punctuations are not allowed to be replaced1 . To select a substitute word that agrees well with the context of a sentence, we use the BERT (Devlin et al., 2019) to generate a set of candidate words that are suitable to replace the original word thanks to its bidirectional language model that is capable of capturing the wider context of the entire sentence2 . Words that are assigned to the same POS generally have similar grammatical properties and display similar syntactic behavior. To enforce the second constraint, we require that the substitute x0i should be assigned to the same part of speech as xi by a POS tagger like (Samanta and Mehta, 2017; Ebrahimi et al., 2018). We filter out the aforementioned words in the third constraint. We adopt the foll"
2020.acl-main.590,P18-2006,0,0.42681,"al examples for texts has proven to be a more challenging task These authors contributed equally to this work. between the IN DT punct nn nsubj Introduction ∗ link NN conj cc det than for images and audios due to their discrete nature, a few methods have been proposed to generate adversarial text examples and reveal the vulnerability of deep neural networks in natural language processing (NLP) tasks including reading comprehension (Jia and Liang, 2017), text classification (Samanta and Mehta, 2017; Wong, 2017; Liang et al., 2018; Alzantot et al., 2018), machine translation (Zhao et al., 2018; Ebrahimi et al., 2018; Cheng et al., 2018) and dialogue systems (Cheng et al., 2019). These recent methods attack text examples mainly by replacing, scrambling, and erasing characters or words or other language units under certain semantics-preserving constraints. Although adversarial examples have been studied recently for NLP tasks, previous work almost exclusively focused on semantic tasks, where the attacks aim to alter the semantic prediction of ML models (e.g., sentiment prediction or question answering) without changing the meaning of original texts. To the best of our knowledge, adversarial 6600 Proceeding"
2020.acl-main.590,N19-1165,0,0.0414214,"Missing"
2020.acl-main.590,D16-1182,0,0.0448088,"Missing"
2020.acl-main.590,P19-1147,1,0.939088,"mportant position (or token) to change; modify it slightly to maximize the model’s prediction error. This two-step can be repeated iteratively until the model’s prediction changes or certain stopping criteria are reached. Many methods have been proposed to determine the important positions by random selection (Alzantot et al., 2018), trial-and-error testing at each possible point (Kuleshov et al., 2018), analyzing the effects on the model of masking various parts of a input text (Samanta and Mehta, 2017; Gao et al., 2018; Jin et al., 2019; Yang et al., 2018), comparing their attention scores (Hsieh et al., 2019), or gradient-guided optimization methods (Ebrahimi et al., 2018; Lei et al., 2019; Wallace et al., 2019; Barham and Feizi, 2019). After the important positions are identified, the most popular way to alter text examples is to replace the characters or words at selected positions with similar substitutes. Such substitutes can be chosen from nearest neighbours in an embedding space (Alzantot et al., 2018; Kuleshov et al., 2018; Jin et al., 2019; Barham and Feizi, 2019), synonyms in a prepared dictionary (Samanta and Mehta, 2017; Hsieh et al., 2019), visually similar alternatives like typos (Sam"
2020.acl-main.590,D17-1215,0,0.490738,"Dezfooli et al., 2016; Papernot et al., 2016b; Carlini and Wagner, 2017; Yuan et al., 2019; Eykholt et al., 2018; Xu et al., 2019). Even though generating adversarial examples for texts has proven to be a more challenging task These authors contributed equally to this work. between the IN DT punct nn nsubj Introduction ∗ link NN conj cc det than for images and audios due to their discrete nature, a few methods have been proposed to generate adversarial text examples and reveal the vulnerability of deep neural networks in natural language processing (NLP) tasks including reading comprehension (Jia and Liang, 2017), text classification (Samanta and Mehta, 2017; Wong, 2017; Liang et al., 2018; Alzantot et al., 2018), machine translation (Zhao et al., 2018; Ebrahimi et al., 2018; Cheng et al., 2018) and dialogue systems (Cheng et al., 2019). These recent methods attack text examples mainly by replacing, scrambling, and erasing characters or words or other language units under certain semantics-preserving constraints. Although adversarial examples have been studied recently for NLP tasks, previous work almost exclusively focused on semantic tasks, where the attacks aim to alter the semantic prediction of M"
2020.acl-main.590,Q16-1023,0,0.0353028,"Missing"
2020.acl-main.590,de-marneffe-etal-2006-generating,0,0.178941,"Missing"
2020.acl-main.590,P05-1012,0,0.131434,"ntence x, we denote the set of all valid parse trees that can be constructed from x as Y(x). Assume that there exists a graph scoring function s, the dependency parsing problem can be formulated as finding the highest scoring directed spanning tree for the sentence x. y ∗ (x) = argmax s(x, yˆ; θ) (1) y ˆ∈Y(x) where y ∗ (x) is the parse tree with the highest score, and θ are all the parameters used to calculate the scores. Given a sentence x[1:n] that is a sequence of n words xi , 1 ≤ i ≤ n, the score of a graph is usually factorized into the sum of its arc scores to make the search tractable (McDonald et al., 2005). s(x, yˆ; θ) = X s(xh , xm ; θ) (2) (xh ,xm )∈A(ˆ y) where A(ˆ y ) represents a set of directed edges in the parse tree yˆ. The score of an arc (xh , xm ) represents the likelihood of creating a dependency from head xh to modifier xm in a dependency tree. 3.2 Problem Definition A neural network can be considered as a mapping f : X → Y from an input x ∈ X to a output y ∈ Y with parameters θ. For classification problems, y is a label which lies in some finite set of categories. For the dependency parsing, y is one of valid parses that can be built from x. The model f maps x to y ∗ with the high"
2020.acl-main.590,P15-1111,0,0.0613283,"Missing"
2020.acl-main.590,D14-1162,0,0.0837009,"the aforementioned words in the third constraint. We adopt the following two-step procedure for generating text adversarial examples: choose weak spots (or positions) to change, and then modify them to maximize the model’s error. In the blackbox setting, we first identify the weak spots of an 1 We exclude those words from being replaced because either there are very limited number of substitutes available, or such replacements easily lead to syntactic inconsistency. 2 We also tried to replace words with their nearest neighbors in the vector space of pre-trained word embeddings such as GloVe (Pennington et al., 2014). However, our preliminary experiments show that these nearest neighbors cannot fit well with the context in many cases since the neighboring words are retrieved without taking the specific context into account. 6603 input sentence with the greedy search strategy by replacing each word, one at a time, with a special “unknown” symbol (&lt;unk&gt;), and examining the changes in unlabeled attachment score (UAS) like (Yang et al., 2018; Gao et al., 2018; Hsieh et al., 2019). For each identified weak spot, we replace it with a word in the candidate set proposed by the BERT to form an attack. We select th"
2020.acl-main.590,P18-1079,0,0.0307661,"ang et al., 2019), or word (Samanta and Mehta, 2017; Alzantot et al., 2018) levels by replacement, alteration (e.g. deliberately introducing typos or misspellings), swap, insertion, erasure, or directly making small perturbations to their feature embeddings. Generally, we would like to ensure that the crafted adversarial examples are sufficiently similar to their original ones, and these modifications should be made within semantics-preserving constraints. Such semantic similarity constraints are usually defined based on Cosine similarity (Wong, 2017; Barham and Feizi, 2019; Jin et al., 2019; Ribeiro et al., 2018) or edit distance (Gao et al., 2018). Text adversarial example generation usually involves two steps: determine an important position (or token) to change; modify it slightly to maximize the model’s prediction error. This two-step can be repeated iteratively until the model’s prediction changes or certain stopping criteria are reached. Many methods have been proposed to determine the important positions by random selection (Alzantot et al., 2018), trial-and-error testing at each possible point (Kuleshov et al., 2018), analyzing the effects on the model of masking various parts of a input text"
2020.acl-main.590,N03-1033,0,0.15188,"Missing"
2020.acl-main.590,D19-1221,0,0.0212021,"is two-step can be repeated iteratively until the model’s prediction changes or certain stopping criteria are reached. Many methods have been proposed to determine the important positions by random selection (Alzantot et al., 2018), trial-and-error testing at each possible point (Kuleshov et al., 2018), analyzing the effects on the model of masking various parts of a input text (Samanta and Mehta, 2017; Gao et al., 2018; Jin et al., 2019; Yang et al., 2018), comparing their attention scores (Hsieh et al., 2019), or gradient-guided optimization methods (Ebrahimi et al., 2018; Lei et al., 2019; Wallace et al., 2019; Barham and Feizi, 2019). After the important positions are identified, the most popular way to alter text examples is to replace the characters or words at selected positions with similar substitutes. Such substitutes can be chosen from nearest neighbours in an embedding space (Alzantot et al., 2018; Kuleshov et al., 2018; Jin et al., 2019; Barham and Feizi, 2019), synonyms in a prepared dictionary (Samanta and Mehta, 2017; Hsieh et al., 2019), visually similar alternatives like typos (Samanta and Mehta, 2017; Ebrahimi et al., 2018; Liang et al., 2018) or Internet slang and trademark logos ("
2020.acl-main.611,P15-1017,0,0.068138,"enhe Pharmacy E-LOC 店 Shop 药店 Pharmacy (b) Lattice LSTM. B-LOC E-LOC B-LOC I-LOC I-LOC E-LOC Transformer Encoder 重 Chong 庆 Qing 人 People 和 And 药 Drug 店 Shop 重庆 Chongqing 人和药店 药店 Pharmacy 1 2 3 4 5 6 1 3 5 1 2 3 4 5 6 2 6 6 Renhe Pharmacy (c) Flat-Lattice Transformer. Figure 1: While lattice LSTM indicates lattice structure by dynamically adjusting its structure, FLAT only needs to leverage the span position encoding. In 1(c), , , denotes tokens, heads and tails, respectively. Named entity recognition (NER) plays an indispensable role in many downstream natural language processing (NLP) tasks (Chen et al., 2015; Diefenbach et al., 2018). Compared with English NER (Lample et al., 2016; Yang et al., 2017; Liu et al., 2017; Sun et al., 2020), Chinese NER is more difficult since it usually involves word segmentation. Recently, the lattice structure has been proved to have a great benefit to utilize the word information and avoid the error propagation of word segmentation (Zhang and Yang, 2018). We can match a sentence with a lexicon to obtain the latent words in it, and then we get a lattice like in Figure 1(a). The lattice is a directed acyclic graph, where each node is a character or a latent word. Th"
2020.acl-main.611,P19-1285,0,0.0754278,"Missing"
2020.acl-main.611,D19-1096,1,0.793881,"Missing"
2020.acl-main.611,N16-1030,0,0.14428,"OC I-LOC I-LOC E-LOC Transformer Encoder 重 Chong 庆 Qing 人 People 和 And 药 Drug 店 Shop 重庆 Chongqing 人和药店 药店 Pharmacy 1 2 3 4 5 6 1 3 5 1 2 3 4 5 6 2 6 6 Renhe Pharmacy (c) Flat-Lattice Transformer. Figure 1: While lattice LSTM indicates lattice structure by dynamically adjusting its structure, FLAT only needs to leverage the span position encoding. In 1(c), , , denotes tokens, heads and tails, respectively. Named entity recognition (NER) plays an indispensable role in many downstream natural language processing (NLP) tasks (Chen et al., 2015; Diefenbach et al., 2018). Compared with English NER (Lample et al., 2016; Yang et al., 2017; Liu et al., 2017; Sun et al., 2020), Chinese NER is more difficult since it usually involves word segmentation. Recently, the lattice structure has been proved to have a great benefit to utilize the word information and avoid the error propagation of word segmentation (Zhang and Yang, 2018). We can match a sentence with a lexicon to obtain the latent words in it, and then we get a lattice like in Figure 1(a). The lattice is a directed acyclic graph, where each node is a character or a latent word. The lattice includes a sequence of characters and potential Corresponding au"
2020.acl-main.611,W06-0115,0,0.789949,"Missing"
2020.acl-main.611,P18-2023,0,0.121094,"71.81 72.82 91.87 93.01 94.41 95.25 56.75 58.39 Lattice LSTM CNNR LGN PLT FLAT FLATmsm FLATmld YJ YJ YJ YJ YJ YJ YJ 73.88 74.45 74.85 74.60 76.45 73.39 75.35 93.18 93.71 93.63 93.26 94.12 93.11 93.83 94.46 95.11 95.41 95.40 95.45 95.03 95.28 58.79 59.92 60.15 59.92 60.32 57.98 59.63 CGN FLAT LS LS 74.79 75.70 93.47 94.35 94.12∗ 63.09 94.93 63.42 Table 2: Four datasets results (F1). BiLSTM results are from Zhang and Yang (2018). PLT denotes the porous lattice Transformer (Mengge et al., 2019). ‘YJ’ denotes the lexicon released by Zhang and Yang (2018), and ‘LS’ denotes the lexicon released by Li et al. (2018). The result of other models are from their original paper. Except that the superscript * means the result is not provided in the original paper, and we get the result by running the public source code. Subscripts ‘msm’ and ‘mld’ denote FLAT with the mask of self-matched words and long distance (&gt;10), respectively. denotes the distance between head of (ht) (th) Rij = ReLU(Wr (pd(hh) ⊕ pd(th) ⊕ pd(ht) ⊕ pd(tt) )), (8) ij ij ij ij where Wr is a learnable parameter, ⊕ denotes the concatenation operator, and pd is calculated as in Vaswani et al. (2017), (2k) pd (2k+1)   = sin d/100002k/dmodel ,"
2020.acl-main.611,D15-1064,0,0.474785,"Missing"
2020.acl-main.611,P19-1115,0,0.0312922,"erence between our model and models above is that they modify the model structure according to the lattice, while we use a well-designed position encoding to indicate the lattice structure. 5.2 Lattice-based Transformer For lattice-based Transformer, it has been used in speech translation and Chinese-source translation. The main difference between them is the way to 1 indicate lattice structure. In Chinese-source translation, Xiao et al. (2019) take the absolute position of nodes’ first characters and the relation between each pair of nodes as the structure information. In speech translation, Sperber et al. (2019) used the longest distance to the start node to indicate lattice structure, and Zhang et al. (2019) used the shortest distance between two nodes. Our span position encoding is more natural, and can be mapped to all the three ways, but not vise versa. Because NER is more sensitive to position information than translation, our model is more suitable for NER. Recently, Porous Lattice Transformer (Mengge et al., 2019) is proposed for Chinese NER. The main difference between FLAT and Porus Lattice Transformer is the way of representing position information. We use ‘head’ and ‘tail’ to represent the"
2020.acl-main.611,K19-1058,0,0.0427694,"Missing"
2020.acl-main.611,D19-1396,0,0.543645,"c July 5 - 10, 2020. 2020 Association for Computational Linguistics CNN to encode potential words at different window sizes. However, RNN and CNN are hard to model long-distance dependencies (Vaswani et al., 2017), which may be useful in NER, such as coreference (Stanislawek et al., 2019). Due to the dynamic lattice structure, these methods cannot fully utilize the parallel computation of GPU. (2) Another line is to convert lattice into graph and use a graph neural network (GNN) to encode it, such as Lexicon-based Graph Network (LGN) (Gui et al., 2019b) and Collaborative Graph Network (CGN) (Sui et al., 2019). While sequential structure is still important for NER and graph is general counterpart, their gap is not negligible. These methods need to use LSTM as the bottom encoder to carry the sequential inductive bias, which makes the model complicated. In this paper, we propose FLAT: Flat LAttice Transformer for Chinese NER. Transformer (Vaswani et al., 2017) adopts fully-connected selfattention to model the long-distance dependencies in a sequence. To keep the position information, Transformer introduces the position representation for each token in the sequence. Inspired by the idea of position re"
2020.acl-main.611,P19-1298,0,0.0271261,"ph, converting NER into a node classification task. However, due to NER’s strong alignment of label and input, their model needs an RNN module for encoding. The main difference between our model and models above is that they modify the model structure according to the lattice, while we use a well-designed position encoding to indicate the lattice structure. 5.2 Lattice-based Transformer For lattice-based Transformer, it has been used in speech translation and Chinese-source translation. The main difference between them is the way to 1 indicate lattice structure. In Chinese-source translation, Xiao et al. (2019) take the absolute position of nodes’ first characters and the relation between each pair of nodes as the structure information. In speech translation, Sperber et al. (2019) used the longest distance to the start node to indicate lattice structure, and Zhang et al. (2019) used the shortest distance between two nodes. Our span position encoding is more natural, and can be mapped to all the three ways, but not vise versa. Because NER is more sensitive to position information than translation, our model is more suitable for NER. Recently, Porous Lattice Transformer (Mengge et al., 2019) is propos"
2020.acl-main.611,P19-1649,0,0.022316,"ttice, while we use a well-designed position encoding to indicate the lattice structure. 5.2 Lattice-based Transformer For lattice-based Transformer, it has been used in speech translation and Chinese-source translation. The main difference between them is the way to 1 indicate lattice structure. In Chinese-source translation, Xiao et al. (2019) take the absolute position of nodes’ first characters and the relation between each pair of nodes as the structure information. In speech translation, Sperber et al. (2019) used the longest distance to the start node to indicate lattice structure, and Zhang et al. (2019) used the shortest distance between two nodes. Our span position encoding is more natural, and can be mapped to all the three ways, but not vise versa. Because NER is more sensitive to position information than translation, our model is more suitable for NER. Recently, Porous Lattice Transformer (Mengge et al., 2019) is proposed for Chinese NER. The main difference between FLAT and Porus Lattice Transformer is the way of representing position information. We use ‘head’ and ‘tail’ to represent the token’s position in the lattice. They use ‘head’, tokens’ relative relation (not distance) and an"
2020.acl-main.611,P18-1144,0,0.349931,"o leverage the span position encoding. In 1(c), , , denotes tokens, heads and tails, respectively. Named entity recognition (NER) plays an indispensable role in many downstream natural language processing (NLP) tasks (Chen et al., 2015; Diefenbach et al., 2018). Compared with English NER (Lample et al., 2016; Yang et al., 2017; Liu et al., 2017; Sun et al., 2020), Chinese NER is more difficult since it usually involves word segmentation. Recently, the lattice structure has been proved to have a great benefit to utilize the word information and avoid the error propagation of word segmentation (Zhang and Yang, 2018). We can match a sentence with a lexicon to obtain the latent words in it, and then we get a lattice like in Figure 1(a). The lattice is a directed acyclic graph, where each node is a character or a latent word. The lattice includes a sequence of characters and potential Corresponding author. 庆 Qing 药店 Pharmacy Introduction ∗ 人和药店 Renhe Pharmacy words in the sentence. They are not ordered sequentially, and the word’s first character and last character determine its position. Some words in lattice may be important for NER. For example, in Figure 1(a), “人和药店(Renhe Pharmacy)” can be used to disti"
2020.acl-main.611,yang-etal-2017-neural-reranking,0,\N,Missing
2020.coling-main.182,W05-0909,0,0.214213,"Missing"
2020.coling-main.182,D15-1075,0,0.0395072,"he effectiveness of our proposed approach. To dig into our approach, we perform ablation studies to explore the different effects of scaling module and prototype position indicator. 3.1 Prototype Collection In-Domain Corpus Din CommonGen is to describe a common scenario in our daily life, datasets of image captioning or video captioning would contain more knowledge about spatial relations, object properties, physical rules, temporal event knowledge and social conventions that contribute to build the target scene contains the these provided concepts. We utilize VaTeX (Wang et al., 2019), SNLI (Bowman et al., 2015), Activity (Krishna et al., 2017) and the training set of CommonGen as the external plain text knowledge datasets and retrieve prototype according to the concepts appear in the sentence. Out-of-Domain Corpus Dout In-domain corpus Din may only suitable for these description sentence for daily scenario and has difficulty in generalizing toother domains, thus we also employ wikipedia as our external knowledge dataset to retrieve prototypes to test the generalization of our model. The number of retrieved prototypes concepts that co-occur in ground truth sentence across different external knowledge"
2020.coling-main.182,P18-1015,0,0.105109,"ackground knowledge such as spatial relations, object properties, physical rules, temporal event knowledge, social conventions, etc., which may not be recorded in any existing knowledge bases, this paper focuses on retrieve knowledge from plain text in order to introduce scenario bias for concepts-set based generation. 4.2 Retrieve-and-Edit The retrieve-and-edit approaches are developed for many tasks, including dialogue generation (Weston et al., 2018; Song et al., 2016), language modeling (Guu et al., 2018), code generation (Hashimoto et al., 2018) and text summarization (Rush et al., 2015; Cao et al., 2018a; Peng et al., 2019). Ji et al. (2014) and Yan et al. (2016) focus on prototype ranking in the retrieval-based model but they do not edit these retrieved prototype. Re3Sum (Cao et al., 2018b) is an LSTM-based model developed under the retrieve-and-edit framework that retrieves multiple headlines and pick the single best retrieved headline, then edited. Hashimoto et al. (Hashimoto et al., 2018) Hossain et al. (2020) presents a framework with retrieve, edit and rerank on the basis of BERT (Devlin et al., 2018), but they do not deal with prototype noise in an explicit manner. Song et al. (2016)"
2020.coling-main.182,P16-1154,0,0.0292584,"1024 and 5k. The dropout rate is 0.1. We set the standard deviation of initialization in group embedding, scaling module and prototype position indicator to 5e-3. The optimizer of model is Adam (Kingma and Ba, 2014) with β1 = 0.9 and β2 = 0.999. During decoding, the size of beam search is 5 and the length penalty is 0.0. 3.3 Results For the compared methods, we classify them into four groups. Group 1 Models without pretraining. bRNN-CopyNet and Trans-CopyNet are based on the best popular architecture Bidirectional RNNs and Transformers (Vaswani et al., 2017) with attention and copy mechanism (Gu et al., 2016). MeanPooling-CopyNet is employed to deal with the influence of the concept ordering in the sequential based methods, where the input concepts is randomly permuted multiple times and decoding is with a mean pooling based MLP network. Levenshtein Transformer (Gu et al., 2019) is an edit-based non-autoregressive generation model, where the generated sentences go through multiple refinement. Group 2 Pretrained language generation models including GPT-2 (Radford et al., 2019), UniLM (Dong et al., 2019), UniLM-v2 (Bao et al., 2020), BERT-Gen (Bao et al., 2020), BART (Lewis et al., 2019), and T5 (Ra"
2020.coling-main.182,Q18-1031,0,0.0270196,"dy this problem. Consider quite a few relationship reasoning over these concepts require a variety of background knowledge such as spatial relations, object properties, physical rules, temporal event knowledge, social conventions, etc., which may not be recorded in any existing knowledge bases, this paper focuses on retrieve knowledge from plain text in order to introduce scenario bias for concepts-set based generation. 4.2 Retrieve-and-Edit The retrieve-and-edit approaches are developed for many tasks, including dialogue generation (Weston et al., 2018; Song et al., 2016), language modeling (Guu et al., 2018), code generation (Hashimoto et al., 2018) and text summarization (Rush et al., 2015; Cao et al., 2018a; Peng et al., 2019). Ji et al. (2014) and Yan et al. (2016) focus on prototype ranking in the retrieval-based model but they do not edit these retrieved prototype. Re3Sum (Cao et al., 2018b) is an LSTM-based model developed under the retrieve-and-edit framework that retrieves multiple headlines and pick the single best retrieved headline, then edited. Hashimoto et al. (Hashimoto et al., 2018) Hossain et al. (2020) presents a framework with retrieve, edit and rerank on the basis of BERT (Devl"
2020.coling-main.182,2020.acl-main.228,0,0.0155863,"g dialogue generation (Weston et al., 2018; Song et al., 2016), language modeling (Guu et al., 2018), code generation (Hashimoto et al., 2018) and text summarization (Rush et al., 2015; Cao et al., 2018a; Peng et al., 2019). Ji et al. (2014) and Yan et al. (2016) focus on prototype ranking in the retrieval-based model but they do not edit these retrieved prototype. Re3Sum (Cao et al., 2018b) is an LSTM-based model developed under the retrieve-and-edit framework that retrieves multiple headlines and pick the single best retrieved headline, then edited. Hashimoto et al. (Hashimoto et al., 2018) Hossain et al. (2020) presents a framework with retrieve, edit and rerank on the basis of BERT (Devlin et al., 2018), but they do not deal with prototype noise in an explicit manner. Song et al. (2016) introduces an extra encoder for the retrieved response, and the output of the encoder, together with that of the query encoder, is utilized to feed the decoder. Weston et al. (2018) simply concatenates the original query and the retrieves response as the input to the encoder. Instead of solely using the retrieved response, Wu et al. (2019) further introduces to encodes the lexical differences between the current que"
2020.coling-main.182,N03-1020,0,0.349131,"Missing"
2020.coling-main.182,D19-1282,0,0.104528,"nation and complete the scenario with introducing additional concepts. We propose to use two kind of corpus as out of domain and in domain external knowledge to retrieve the prototypes respectively. To better model the prototypes, we design two attention mechanisms to enhance the knowledge injection procedure. We conduct experiment on CommonGen benchmark, experimental results show that our method significantly improves the performance on all the metrics. 1 Introduction Recently, commonsense reasoning tasks such as SWAG (Zellers et al., 2018), CommonsenseQA (Talmor et al., 2018) and CommonGen (Lin et al., 2019b) are presented to investigate the model’s ability to make acceptable and logical assumptions about ordinary scenes in our daily life. SWAG requires to infer the probable subsequent event based on the given textual description of an event. CommonsenseQA focuses on commonsense question answering that collects commonsense questions at scale by describing the relation between concepts from CONCEPTNET. Different from these discriminative tasks, CommonGen is a generation task that not only needs to use background commonsense knowledge to conduct relational reasoning, but also compositional based g"
2020.coling-main.182,S19-1012,0,0.0196311,"missing of EKI-BART Din is more than that of BART Din , which shows that BART Din is more likely to ignore the provided concepts than BART Din and being dominated by noises in prototype. This also verifies that the ability of BART Dout in dealing with prototype noises is stronger than BART Din , and removing these noises benefits to finding out a more plausible scenario with these concepts. 4 4.1 Related Work Commonsense Reasoning Recently, there are emerging works to investigate machine commonsense reasoning ability. ATOMIC (Sap et al., 2019), Event2Mind (Rashkin et al., 2018), MCScript 2.0 (Ostermann et al., 2019), SWAG (Zellers et al., 2018), HellaSWAG (Zellers et al., 2019), Story Cloze Test (Mostafazadeh et al., 2017), CommonsenseQA (Talmor et al., 2018) and CommonGen (Lin et al., 2019b) are released to reasoning over external knowledge besides the inputs for question answering or generation. Rajani et al. (2019) explores adding human-written explanations to solve the problem. Lin et al. (2019a) constructs 2021 schema graphs from ConceptNet to reason over relevant commonsense knowledge. lv et al. (2020) focuses on automatically extracting evidence from heterogeneous external knowledge and reasoning"
2020.coling-main.182,P18-1123,0,0.0227478,"etrieve, edit and rerank on the basis of BERT (Devlin et al., 2018), but they do not deal with prototype noise in an explicit manner. Song et al. (2016) introduces an extra encoder for the retrieved response, and the output of the encoder, together with that of the query encoder, is utilized to feed the decoder. Weston et al. (2018) simply concatenates the original query and the retrieves response as the input to the encoder. Instead of solely using the retrieved response, Wu et al. (2019) further introduces to encodes the lexical differences between the current query and the retrieved query. Pandey et al. (2018) proposes to weight different training instances by context similarity. Different from these work, We explore the retrieve-and-edit framework on the basis of pretrained encoder-decoder model, and identify the importance of each token in prototype in a more fine-grained manner. 5 Conclusion and Future Work In this paper, we have proposed a pretraining enhanced retrieve-and-edit model for commonsense generation. The key of CommonGen is to identify the priority of the scene based on the concept combination, we have scaling module to softly reduce the impact of prototype noises on generation and p"
2020.coling-main.182,P02-1040,0,0.10919,"Missing"
2020.coling-main.182,N19-1263,0,0.0107029,"such as spatial relations, object properties, physical rules, temporal event knowledge, social conventions, etc., which may not be recorded in any existing knowledge bases, this paper focuses on retrieve knowledge from plain text in order to introduce scenario bias for concepts-set based generation. 4.2 Retrieve-and-Edit The retrieve-and-edit approaches are developed for many tasks, including dialogue generation (Weston et al., 2018; Song et al., 2016), language modeling (Guu et al., 2018), code generation (Hashimoto et al., 2018) and text summarization (Rush et al., 2015; Cao et al., 2018a; Peng et al., 2019). Ji et al. (2014) and Yan et al. (2016) focus on prototype ranking in the retrieval-based model but they do not edit these retrieved prototype. Re3Sum (Cao et al., 2018b) is an LSTM-based model developed under the retrieve-and-edit framework that retrieves multiple headlines and pick the single best retrieved headline, then edited. Hashimoto et al. (Hashimoto et al., 2018) Hossain et al. (2020) presents a framework with retrieve, edit and rerank on the basis of BERT (Devlin et al., 2018), but they do not deal with prototype noise in an explicit manner. Song et al. (2016) introduces an extra e"
2020.coling-main.182,P19-1487,0,0.0145731,"hese noises benefits to finding out a more plausible scenario with these concepts. 4 4.1 Related Work Commonsense Reasoning Recently, there are emerging works to investigate machine commonsense reasoning ability. ATOMIC (Sap et al., 2019), Event2Mind (Rashkin et al., 2018), MCScript 2.0 (Ostermann et al., 2019), SWAG (Zellers et al., 2018), HellaSWAG (Zellers et al., 2019), Story Cloze Test (Mostafazadeh et al., 2017), CommonsenseQA (Talmor et al., 2018) and CommonGen (Lin et al., 2019b) are released to reasoning over external knowledge besides the inputs for question answering or generation. Rajani et al. (2019) explores adding human-written explanations to solve the problem. Lin et al. (2019a) constructs 2021 schema graphs from ConceptNet to reason over relevant commonsense knowledge. lv et al. (2020) focuses on automatically extracting evidence from heterogeneous external knowledge and reasoning over the extracted evidence to study this problem. Consider quite a few relationship reasoning over these concepts require a variety of background knowledge such as spatial relations, object properties, physical rules, temporal event knowledge, social conventions, etc., which may not be recorded in any exis"
2020.coling-main.182,P18-1043,0,0.0246896,"e number of instance with no concept missing of EKI-BART Din is more than that of BART Din , which shows that BART Din is more likely to ignore the provided concepts than BART Din and being dominated by noises in prototype. This also verifies that the ability of BART Dout in dealing with prototype noises is stronger than BART Din , and removing these noises benefits to finding out a more plausible scenario with these concepts. 4 4.1 Related Work Commonsense Reasoning Recently, there are emerging works to investigate machine commonsense reasoning ability. ATOMIC (Sap et al., 2019), Event2Mind (Rashkin et al., 2018), MCScript 2.0 (Ostermann et al., 2019), SWAG (Zellers et al., 2018), HellaSWAG (Zellers et al., 2019), Story Cloze Test (Mostafazadeh et al., 2017), CommonsenseQA (Talmor et al., 2018) and CommonGen (Lin et al., 2019b) are released to reasoning over external knowledge besides the inputs for question answering or generation. Rajani et al. (2019) explores adding human-written explanations to solve the problem. Lin et al. (2019a) constructs 2021 schema graphs from ConceptNet to reason over relevant commonsense knowledge. lv et al. (2020) focuses on automatically extracting evidence from heteroge"
2020.coling-main.182,D15-1044,0,0.0470787,"uire a variety of background knowledge such as spatial relations, object properties, physical rules, temporal event knowledge, social conventions, etc., which may not be recorded in any existing knowledge bases, this paper focuses on retrieve knowledge from plain text in order to introduce scenario bias for concepts-set based generation. 4.2 Retrieve-and-Edit The retrieve-and-edit approaches are developed for many tasks, including dialogue generation (Weston et al., 2018; Song et al., 2016), language modeling (Guu et al., 2018), code generation (Hashimoto et al., 2018) and text summarization (Rush et al., 2015; Cao et al., 2018a; Peng et al., 2019). Ji et al. (2014) and Yan et al. (2016) focus on prototype ranking in the retrieval-based model but they do not edit these retrieved prototype. Re3Sum (Cao et al., 2018b) is an LSTM-based model developed under the retrieve-and-edit framework that retrieves multiple headlines and pick the single best retrieved headline, then edited. Hashimoto et al. (Hashimoto et al., 2018) Hossain et al. (2020) presents a framework with retrieve, edit and rerank on the basis of BERT (Devlin et al., 2018), but they do not deal with prototype noise in an explicit manner. S"
2020.coling-main.182,W18-5713,0,0.0167139,"l knowledge and reasoning over the extracted evidence to study this problem. Consider quite a few relationship reasoning over these concepts require a variety of background knowledge such as spatial relations, object properties, physical rules, temporal event knowledge, social conventions, etc., which may not be recorded in any existing knowledge bases, this paper focuses on retrieve knowledge from plain text in order to introduce scenario bias for concepts-set based generation. 4.2 Retrieve-and-Edit The retrieve-and-edit approaches are developed for many tasks, including dialogue generation (Weston et al., 2018; Song et al., 2016), language modeling (Guu et al., 2018), code generation (Hashimoto et al., 2018) and text summarization (Rush et al., 2015; Cao et al., 2018a; Peng et al., 2019). Ji et al. (2014) and Yan et al. (2016) focus on prototype ranking in the retrieval-based model but they do not edit these retrieved prototype. Re3Sum (Cao et al., 2018b) is an LSTM-based model developed under the retrieve-and-edit framework that retrieves multiple headlines and pick the single best retrieved headline, then edited. Hashimoto et al. (Hashimoto et al., 2018) Hossain et al. (2020) presents a framework"
2020.coling-main.182,2020.findings-emnlp.217,1,0.871963,"Missing"
2020.coling-main.182,D18-1009,0,0.103227,"rpus would benefit to discriminate the priority of different concept combination and complete the scenario with introducing additional concepts. We propose to use two kind of corpus as out of domain and in domain external knowledge to retrieve the prototypes respectively. To better model the prototypes, we design two attention mechanisms to enhance the knowledge injection procedure. We conduct experiment on CommonGen benchmark, experimental results show that our method significantly improves the performance on all the metrics. 1 Introduction Recently, commonsense reasoning tasks such as SWAG (Zellers et al., 2018), CommonsenseQA (Talmor et al., 2018) and CommonGen (Lin et al., 2019b) are presented to investigate the model’s ability to make acceptable and logical assumptions about ordinary scenes in our daily life. SWAG requires to infer the probable subsequent event based on the given textual description of an event. CommonsenseQA focuses on commonsense question answering that collects commonsense questions at scale by describing the relation between concepts from CONCEPTNET. Different from these discriminative tasks, CommonGen is a generation task that not only needs to use background commonsense know"
2020.coling-main.182,P19-1472,0,0.0167693,"ws that BART Din is more likely to ignore the provided concepts than BART Din and being dominated by noises in prototype. This also verifies that the ability of BART Dout in dealing with prototype noises is stronger than BART Din , and removing these noises benefits to finding out a more plausible scenario with these concepts. 4 4.1 Related Work Commonsense Reasoning Recently, there are emerging works to investigate machine commonsense reasoning ability. ATOMIC (Sap et al., 2019), Event2Mind (Rashkin et al., 2018), MCScript 2.0 (Ostermann et al., 2019), SWAG (Zellers et al., 2018), HellaSWAG (Zellers et al., 2019), Story Cloze Test (Mostafazadeh et al., 2017), CommonsenseQA (Talmor et al., 2018) and CommonGen (Lin et al., 2019b) are released to reasoning over external knowledge besides the inputs for question answering or generation. Rajani et al. (2019) explores adding human-written explanations to solve the problem. Lin et al. (2019a) constructs 2021 schema graphs from ConceptNet to reason over relevant commonsense knowledge. lv et al. (2020) focuses on automatically extracting evidence from heterogeneous external knowledge and reasoning over the extracted evidence to study this problem. Consider qui"
2020.coling-main.204,W05-0909,0,0.0914806,"nt for sentence generation . VST: This is the baseline version of our model without using topic information as guidance. TAVST w/o IU: This is our proposed TAVST method without IU module, which only equipped with initial topic description generator. TAVST: This is our full model. TAVST (MLE) is trained using MLE loss, while TAVST (RL) is trained via RL loss. 3.4 Automatic Evaluation Results We evaluate our model on two generation tasks i.e., story generation and topic description generation, in terms of four automatic metrics: BLEU (Papineni et al., 2002), ROUGE-L (Lin and Och, 2004), METEOR (Banerjee and Lavie, 2005), and CIDEr (Vedantam et al., 2015). Story Generation The overall experimental results are shown in Table 1. TAVST (MLE) outperforms all of the baseline models trained with MLE. This confirms the effectivness of topic information for generating better stories. Noticeably, compared with the RL-based models, our TAVST (MLE) has already achieved a competitive performance and outperforms other RL models (i.e., AREL and HSRL) in terms of METEOR and BLEU@[2, 3, 4] metrics. After equipped with RL, our TAVST (RL) model is able to further improve the performance, outperforming the two RL models in term"
2020.coling-main.204,D13-1128,0,0.0232593,". 3.7 Case Study Figure 4 shows an example of the ground-truth story and stories generated automatically by different models. The words in red, blue and yellow color represent the topic, subject, and emotion, respectively. Our model shows promising results according to topic consistency, which further confirms that our model can extract appropriate topic which serves as the guidance of generating a topic-consistent story. 2257 4 Related work This paper is related to the fields of image captioning, visual storytelling and multi-task learning. Image Captioning In early works (Yang et al., 2011; Elliott and Keller, 2013), image captioning is treated as a ranking problem, which is based on retrieval models to identify similar captions from the database. Later, the end-to-end frameworks based on the CNN and RNN are adopted by researchers (Xu et al., 2015; Karpathy and Fei-Fei, 2015; Vinyals et al., 2017; Dai et al., 2017). Such works focus on the literal description of image content, while the generated texts is limited in a single sentence. Visual Storytelling Visual storytelling is the task of generating a narrative paragraph for an image stream. Huang et al. (2016) introduce the first dataset (VIST) for visu"
2020.coling-main.204,N16-1147,0,0.520831,"g (Antol et al., 2015; Yu et al., 2017b; Singh et al., 2019), aiming at generating a short sentence or a phrase conditioned on certain visual information. With the development of deep learning and reinforcement learning models, recent years witness promising improvement of these tasks for single-image-to-single-sentence generation. Visual storytelling moves one step further, extending the input and output dimension to a sequence of images and a sequence of sentences. It requires the model to understand the main idea of an image stream and generate coherent sentences. Most of existing methods (Huang et al., 2016; Liu et al., 2017; Yu et al., 2017a; Wang et al., 2018a; Wang et al., 2020) for visual storytelling extend approaches of image captioning without considering topic information of the image sequence, which causes the problem of generating semantically incoherent content. An example of visual storytelling can be seen in Figure 1. An image stream with five images about a car accident is presented accompanied with two stories. One is constructed by a human annotator and the other is produced by an automatic storytelling approach. There are two problems with the machine generated story. First, the"
2020.coling-main.204,P18-1240,0,0.103348,"he decoder produces the a hidden state hti . Once the last topic hidden state t hM is obtained, we concatenate all topic hidden states ht = [ht1 , ..., htM ], M &gt; 1 as the topic memory, which are fed into the story generation module. 2.3 Initial Story Generator with Co-attention Network The initial story generator is responsible for generating the story with the guidance of the topic description constructed by the initial topic description generator. Co-attention Encoding In order to combine both visual information and topic information for story generation, we adopt a co-attention mechanism (Jing et al., 2018) for context information encoding. Specifically, given visual context vectors hv and topic vectors ht , the affinity matrix C is calculated by T C = tanh(ht Wb hv ) (2) where Wb is the weight parameter. After calculating this matrix, we compute attentions weights over the visual context vectors and the topic vectors via the following operations: H v = tanh(Wv hv + (Wt ht )C) H t = tanh(Wt ht + (Wv hv )C T ) T av = softmax(whv Hv) (3) T at = softmax(wht H t) T , w T are the weight parameters. Based on the attention weights, the visual and where Wv , Wt , whv ht semantic attentions are calculate"
2020.coling-main.204,P04-1077,0,0.0985818,"dance to the lower level agent for sentence generation . VST: This is the baseline version of our model without using topic information as guidance. TAVST w/o IU: This is our proposed TAVST method without IU module, which only equipped with initial topic description generator. TAVST: This is our full model. TAVST (MLE) is trained using MLE loss, while TAVST (RL) is trained via RL loss. 3.4 Automatic Evaluation Results We evaluate our model on two generation tasks i.e., story generation and topic description generation, in terms of four automatic metrics: BLEU (Papineni et al., 2002), ROUGE-L (Lin and Och, 2004), METEOR (Banerjee and Lavie, 2005), and CIDEr (Vedantam et al., 2015). Story Generation The overall experimental results are shown in Table 1. TAVST (MLE) outperforms all of the baseline models trained with MLE. This confirms the effectivness of topic information for generating better stories. Noticeably, compared with the RL-based models, our TAVST (MLE) has already achieved a competitive performance and outperforms other RL models (i.e., AREL and HSRL) in terms of METEOR and BLEU@[2, 3, 4] metrics. After equipped with RL, our TAVST (RL) model is able to further improve the performance, outp"
2020.coling-main.204,P02-1040,0,0.108952,"concept for each image as the guidance to the lower level agent for sentence generation . VST: This is the baseline version of our model without using topic information as guidance. TAVST w/o IU: This is our proposed TAVST method without IU module, which only equipped with initial topic description generator. TAVST: This is our full model. TAVST (MLE) is trained using MLE loss, while TAVST (RL) is trained via RL loss. 3.4 Automatic Evaluation Results We evaluate our model on two generation tasks i.e., story generation and topic description generation, in terms of four automatic metrics: BLEU (Papineni et al., 2002), ROUGE-L (Lin and Och, 2004), METEOR (Banerjee and Lavie, 2005), and CIDEr (Vedantam et al., 2015). Story Generation The overall experimental results are shown in Table 1. TAVST (MLE) outperforms all of the baseline models trained with MLE. This confirms the effectivness of topic information for generating better stories. Noticeably, compared with the RL-based models, our TAVST (MLE) has already achieved a competitive performance and outperforms other RL models (i.e., AREL and HSRL) in terms of METEOR and BLEU@[2, 3, 4] metrics. After equipped with RL, our TAVST (RL) model is able to further"
2020.coling-main.204,P18-1083,0,0.0760091,"019), aiming at generating a short sentence or a phrase conditioned on certain visual information. With the development of deep learning and reinforcement learning models, recent years witness promising improvement of these tasks for single-image-to-single-sentence generation. Visual storytelling moves one step further, extending the input and output dimension to a sequence of images and a sequence of sentences. It requires the model to understand the main idea of an image stream and generate coherent sentences. Most of existing methods (Huang et al., 2016; Liu et al., 2017; Yu et al., 2017a; Wang et al., 2018a; Wang et al., 2020) for visual storytelling extend approaches of image captioning without considering topic information of the image sequence, which causes the problem of generating semantically incoherent content. An example of visual storytelling can be seen in Figure 1. An image stream with five images about a car accident is presented accompanied with two stories. One is constructed by a human annotator and the other is produced by an automatic storytelling approach. There are two problems with the machine generated story. First, the sentiment expressed in the text is inappropriate. In f"
2020.coling-main.204,P19-1240,0,0.128394,"o BLEU@N all the time (Vedantam et al., 2015; Wang et al., 2018a). During the test stage, we generate the stories by performing a beam-search with a beam size of 3. 3.3 Models for Comparison We compare our proposed methods with several baselines for visual storytelling as follows: seq2seq (Huang et al., 2016): It generates caption for each single model via classic sequence-tosequence model and concatenate all captions to form the final story. h-attn-rank (Yu et al., 2017a): On top of the classic sequence-to-sequence model, it adds an additional RNN to select photos for story generation. HPSR (Wang et al., 2019a): It introduces an additional RNN stacked on the RNN-based photo encoder to detect the scene change. Information from both RNNs are fed into an RNN for story generation. 2254 Methods MLE seq2seq (Huang et al., 2016) h-attn-rank (Yu et al., 2017a) HPSR (Wang et al., 2019a) VST (MLE) TAVST w/o IU (MLE) TAVST (MLE) RL AREL (Wang et al., 2018b) HSRL (Huang et al., 2019) TAVST w/o IU (RL) TAVST (RL) B-1 B-2 B-3 B-4 R-L C M − − 61.9 62.3 63.1 63.6 − − 37.8 38.0 38.6 39.3 − 21.0 21.5 21.8 22.9 23.4 3.5 − 12.2 12.7 14.0 14.2 − 29.5 31.2 29.7 29.7 30.3 6.8 7.5 8.0 7.8 8.5 8.7 31.4 34.1 34.4 34.3 35.1"
2020.coling-main.204,H05-1044,0,0.0348739,"hoose which story is better in terms of a certain factor. Results are shown in Table 4. Our model performs better than the other two models in terms of relevance and topic consistency. The advantage of topic consistency is more promising. This proves that the topic description generator can help the story generation agent construct a more consistent story. 3.6 Further Analysis on Topic Consistency We further evaluate the quality of the generated story in terms of topic consistency from the perspective of sentiment. Specifically, we employ a lexicon-based approach using a subjectivity lexicon (Wilson et al., 2005). We count the number of sentiment words in each sentence for the polarity evaluation. The score will be 1,0,-1 if a sentence is positive, neutral and negative, respectively. Based on the score for each sentence, two qualitative experiments are designed to measure the in-story sentiment consistency and topic-story sentiment consistency. In-story Sentiment Consistency We argue that the sentiment of sentences in a story should be consistent given the album is related to a certain topic. For each story, we obtain a vector with 5 sentiment scores in correspondence to 5 sentences. We then calculate"
2020.coling-main.204,D18-1397,0,0.0158067,"ing text generation model via introducing automatic metrics (e.g., METEOR) to guide the training process (Wang et al., 2018b). We also explore the RL-based approach to train our generator. The reinforcement learning (RL) loss can be written as: ∗ 2 Linit (9) story(rl) (θ1 ) = −Ey∼pθ1 (r(y; y ) − b) where r is a sentence-level metric for the sampled sentence y and the ground-truth y ∗ ; b is the baseline which can be an arbitrary function but a linear layer in our experiments for simply. To stabilize the RL training process, a simple way is to linearly combine MLE and RL objectives as follows (Wu et al., 2018): init init Linit (10) story(com) = αLstory(rl) + (1 − α)Lstory(mle) where hyper-parameter α is employed to control the trade-off between MLE and RL objectives. init In the initial stage, a combined loss function of Linit story(com) and Ltopic is computed through: init Linit = λ1 Linit story(com) + (1 − λ1 )Ltopic(mle) (11) where hyper-parameter λ1 is employed to balance these losses. 2.4 Iterative Updating Module Considering that the generated story would also be helpful for the generation of topic description, we design an iterative updating module for the two agents to interact with each ot"
2020.coling-main.204,D17-1101,0,0.241617,"earn them simultaneously via iterative updating mechanism. We validate our approach on VIST dataset, where quantitative results, ablations, and human evaluation demonstrate our method’s good ability in generating stories with higher quality compared to state-of-the-art methods. 1 Introduction Image-to-text generation is an important topic in artificial intelligence (AI) which connects computer vision (CV) and natural language processing (NLP). Popular tasks include image captioning (Karpathy and Fei-Fei, 2015; Ren et al., 2017; Vinyals et al., 2017) and question answering (Antol et al., 2015; Yu et al., 2017b; Singh et al., 2019), aiming at generating a short sentence or a phrase conditioned on certain visual information. With the development of deep learning and reinforcement learning models, recent years witness promising improvement of these tasks for single-image-to-single-sentence generation. Visual storytelling moves one step further, extending the input and output dimension to a sequence of images and a sequence of sentences. It requires the model to understand the main idea of an image stream and generate coherent sentences. Most of existing methods (Huang et al., 2016; Liu et al., 2017;"
2020.coling-main.327,P18-1009,0,0.0281441,"h its symbolic entity, we follow P¨orner et al. (2019) and concatenate the two forms of tokens, e.g. Jean Mara ##is Jean Marais. Concretely, we conduct experiments on two knowledge-driven tasks: entity typing and relation extraction. Entity Typing. The entity typing task is to classify the semantic type of a given entity mention based on its surface form and context. We add two special tokens, [ENT] and [/ENT], before and after the entity mentions to be classified and use the final representation of the [CLS] token as the feature to conduct classification4 . We evaluate CoLAKE on Open Entity (Choi et al., 2018). To compare with ERNIE, KnowBERT, and KEPLER, we adopt the same experiment setting which considers nine general types. To be consistent with previous work, we adopt micro precision, recall, and F1 score as evaluation metrics. The experimental results are shown in Table 2. Relation Extraction. The relation extraction task is to classify the relationship between two entities mentioned in a given sentence. During fine-tuning, we add four special tokens, [HD], [/HD], [TL] and [/TL] to identify the head entity and the tail entity. Also, we use the final representation of the [CLS] token as the fea"
2020.coling-main.327,D19-1109,0,0.0601033,"Missing"
2020.coling-main.327,N19-1423,0,0.615755,"uct experiments on knowledge-driven tasks, knowledge probing tasks, and language understanding tasks. Experimental results show that CoLAKE outperforms previous counterparts on most of the tasks. Besides, CoLAKE achieves surprisingly high performance on our synthetic task called word-knowledge graph completion, which shows the superiority of simultaneously contextualizing language and knowledge representation.1 1 Introduction Deep contextualized language models pre-trained on large-scale unlabeled corpora have achieved significant improvement on a wide range of NLP tasks (Peters et al., 2018; Devlin et al., 2019; Yang et al., 2019). However, they are shown to have difficulty capturing factual knowledge (Logan et al., 2019). Recently, there is a growing interest in combining pre-trained language models (PLMs) with structured knowledge. A popular approach is to inject pre-trained entity embeddings into PLMs to better capture factual knowledge, such as ERNIE (Zhang et al., 2019) and KnowBERT (Peters et al., 2019). The shortcomings of these models can be summarized as follows: (1) The entity embeddings are separately pre-trained with some knowledge embedding (KE) models (e.g., TransE (Bordes et al., 2013"
2020.coling-main.327,D17-1277,0,0.0193316,"se subgraphs or paths as training samples, have achieved new state-of-the-art results on KG tasks (Wang et al., 2019b; Wang et al., 2020a). Joint Language and Knowledge Models. Due to the mutual information existing in language and KGs, joint models often benefit both sides. Besides, tasks such as entity linking also require entity embeddings that are compatible with word embeddings. Combining the success of Mikolov et al. (2013) and Bordes et al. (2013), Wang et al. (2014) jointly learn embeddings for language and KG. Targeting mention-entity matching in entity linking, Yamada et al. (2016), Ganea and Hofmann (2017) also proposed joint methods to map entities and words into the same vector space. Inspired by the recent success of contextualized language representation, much effort has been devoted to injecting entity embeddings into PLMs (Zhang et al., 2019; Peters et al., 2019). Despite their success, the knowledge gains are limited by the expressivity of their used pre-trained entity embeddings, which is static and inflexible. In contrast, KEPLER (Wang et al., 2019c) aims to benefit both sides so jointly learn language model and knowledge embedding. However, KEPLER does not directly learn embeddings fo"
2020.coling-main.327,D18-1514,0,0.0279947,"xperiment setting which considers nine general types. To be consistent with previous work, we adopt micro precision, recall, and F1 score as evaluation metrics. The experimental results are shown in Table 2. Relation Extraction. The relation extraction task is to classify the relationship between two entities mentioned in a given sentence. During fine-tuning, we add four special tokens, [HD], [/HD], [TL] and [/TL] to identify the head entity and the tail entity. Also, we use the final representation of the [CLS] token as the feature to be fed into the classifier. We evaluate CoLAKE on FewRel (Han et al., 2018) that is rearranged by Zhang et al. (2019). Since FewRel is built with Wikidata, we discard triplets in the FewRel test set from pre-training data to avoid information leakage. Following previous work, we report macro precision, recall and F1 score on FewRel. The experimental results can be found in Table 2. Model P Open Entity R F P FewRel R F BERT (Devlin et al., 2019) RoBERTa (Liu et al., 2019) ERNIE (Zhang et al., 2019) KnowBERT (Peters et al., 2019) KEPLER (Wang et al., 2019c) E-BERT (P¨orner et al., 2019) 76.4 77.4 78.4 78.6 77.8 - 71.0 73.6 72.9 73.7 74.6 - 73.6 75.4 75.6 76.1 76.2 - 85"
2020.coling-main.327,P19-1598,0,0.0224223,"l results show that CoLAKE outperforms previous counterparts on most of the tasks. Besides, CoLAKE achieves surprisingly high performance on our synthetic task called word-knowledge graph completion, which shows the superiority of simultaneously contextualizing language and knowledge representation.1 1 Introduction Deep contextualized language models pre-trained on large-scale unlabeled corpora have achieved significant improvement on a wide range of NLP tasks (Peters et al., 2018; Devlin et al., 2019; Yang et al., 2019). However, they are shown to have difficulty capturing factual knowledge (Logan et al., 2019). Recently, there is a growing interest in combining pre-trained language models (PLMs) with structured knowledge. A popular approach is to inject pre-trained entity embeddings into PLMs to better capture factual knowledge, such as ERNIE (Zhang et al., 2019) and KnowBERT (Peters et al., 2019). The shortcomings of these models can be summarized as follows: (1) The entity embeddings are separately pre-trained with some knowledge embedding (KE) models (e.g., TransE (Bordes et al., 2013)), and fixed during training PLMs. Thus they are not real joint models to learn the knowledge embedding and lang"
2020.coling-main.327,P09-1113,0,0.104792,"Missing"
2020.coling-main.327,D14-1162,0,0.101956,"and contextualized knowledge representation simultaneously with the extended MLM objective. (2) CoLAKE adopts the WK graph to integrate the heterogeneous input for language and knowledge. (3) CoLAKE is essentially a pre-trained graph neural network (GNN), thereby being structure-aware and easy to extend. 3661 2 Related Work Language Representation Learning. The past decade has witnessed the great success of pre-trained language representation. Initially, word representation pre-trained using multi-task objectives (Collobert and Weston, 2008) or co-occurrence statistics (Mikolov et al., 2013; Pennington et al., 2014) are static and non-contextual. Recently, contextualized word representation pre-trained on large-scale unlabeled corpora with deep neural networks has dominated across a wide range of NLP tasks (Peters et al., 2018; Devlin et al., 2019; Yang et al., 2019; Qiu et al., 2020). Knowledge Representation Learning. Knowledge Representation Learning (KRL) is also termed as Knowledge Embedding (KE), which is to map entities and relations into low-dimensional continuous vectors. Most existing methods use triplets as training samples to learn static, non-contextual embeddings for entities and relations"
2020.coling-main.327,N18-1202,0,0.49804,"rmer encoder. We conduct experiments on knowledge-driven tasks, knowledge probing tasks, and language understanding tasks. Experimental results show that CoLAKE outperforms previous counterparts on most of the tasks. Besides, CoLAKE achieves surprisingly high performance on our synthetic task called word-knowledge graph completion, which shows the superiority of simultaneously contextualizing language and knowledge representation.1 1 Introduction Deep contextualized language models pre-trained on large-scale unlabeled corpora have achieved significant improvement on a wide range of NLP tasks (Peters et al., 2018; Devlin et al., 2019; Yang et al., 2019). However, they are shown to have difficulty capturing factual knowledge (Logan et al., 2019). Recently, there is a growing interest in combining pre-trained language models (PLMs) with structured knowledge. A popular approach is to inject pre-trained entity embeddings into PLMs to better capture factual knowledge, such as ERNIE (Zhang et al., 2019) and KnowBERT (Peters et al., 2019). The shortcomings of these models can be summarized as follows: (1) The entity embeddings are separately pre-trained with some knowledge embedding (KE) models (e.g., TransE"
2020.coling-main.327,D19-1005,0,0.650335,"ntation.1 1 Introduction Deep contextualized language models pre-trained on large-scale unlabeled corpora have achieved significant improvement on a wide range of NLP tasks (Peters et al., 2018; Devlin et al., 2019; Yang et al., 2019). However, they are shown to have difficulty capturing factual knowledge (Logan et al., 2019). Recently, there is a growing interest in combining pre-trained language models (PLMs) with structured knowledge. A popular approach is to inject pre-trained entity embeddings into PLMs to better capture factual knowledge, such as ERNIE (Zhang et al., 2019) and KnowBERT (Peters et al., 2019). The shortcomings of these models can be summarized as follows: (1) The entity embeddings are separately pre-trained with some knowledge embedding (KE) models (e.g., TransE (Bordes et al., 2013)), and fixed during training PLMs. Thus they are not real joint models to learn the knowledge embedding and language embedding simultaneously. (2) The previous models only take entity embeddings to enhance PLMs, which are hard to fully capture the rich contextual information of an entity in the knowledge graph (KG). Thus their performance gains are limited by the quality of pre-trained entity embedding"
2020.coling-main.327,D19-1250,0,0.0550134,"Missing"
2020.coling-main.327,P16-1162,0,0.0181374,"the WK graph, entities are unique but relations are allowed to repeat. 3.2 Model Architecture The constructed WK graphs are then fed into the Transformer (Vaswani et al., 2017) encoder. We modify the embedding and encoder layers of vanilla Transformer to adapt to input in the form of WK graph. Embedding Layer. The input embedding is the sum of token embedding, type embedding, and position embedding. For token embedding, we maintain three lookup tables for words, entities, and relations respectively. For word embedding, we follow RoBERTa (Liu et al., 2019) which uses Byte-Pair Encoding (BPE) (Sennrich et al., 2016) to transform sequence into subwords units to handle the large vocabulary. In contrast, we directly learn embeddings for each unique entity and relation as common knowledge embedding methods do. The token embeddings are obtained by concatenating word, entity, and relation embeddings, which are of the same dimensionality. There are different types of nodes so the WK graph is heterogeneous. To handle this, we simply use type embedding to indicate the node types, i.e. word, entity, and relation. For position embedding, we need to assign each injected entity and relation a position index. Inspired"
2020.coling-main.327,D14-1167,0,0.100998,"and relations (Bordes et al., 2013; Yang et al., 2015; Lin et al., 2015). Recent advances focusing on contextualized representation, which use subgraphs or paths as training samples, have achieved new state-of-the-art results on KG tasks (Wang et al., 2019b; Wang et al., 2020a). Joint Language and Knowledge Models. Due to the mutual information existing in language and KGs, joint models often benefit both sides. Besides, tasks such as entity linking also require entity embeddings that are compatible with word embeddings. Combining the success of Mikolov et al. (2013) and Bordes et al. (2013), Wang et al. (2014) jointly learn embeddings for language and KG. Targeting mention-entity matching in entity linking, Yamada et al. (2016), Ganea and Hofmann (2017) also proposed joint methods to map entities and words into the same vector space. Inspired by the recent success of contextualized language representation, much effort has been devoted to injecting entity embeddings into PLMs (Zhang et al., 2019; Peters et al., 2019). Despite their success, the knowledge gains are limited by the expressivity of their used pre-trained entity embeddings, which is static and inflexible. In contrast, KEPLER (Wang et al."
2020.coling-main.327,K16-1025,0,0.0762338,"epresentation, which use subgraphs or paths as training samples, have achieved new state-of-the-art results on KG tasks (Wang et al., 2019b; Wang et al., 2020a). Joint Language and Knowledge Models. Due to the mutual information existing in language and KGs, joint models often benefit both sides. Besides, tasks such as entity linking also require entity embeddings that are compatible with word embeddings. Combining the success of Mikolov et al. (2013) and Bordes et al. (2013), Wang et al. (2014) jointly learn embeddings for language and KG. Targeting mention-entity matching in entity linking, Yamada et al. (2016), Ganea and Hofmann (2017) also proposed joint methods to map entities and words into the same vector space. Inspired by the recent success of contextualized language representation, much effort has been devoted to injecting entity embeddings into PLMs (Zhang et al., 2019; Peters et al., 2019). Despite their success, the knowledge gains are limited by the expressivity of their used pre-trained entity embeddings, which is static and inflexible. In contrast, KEPLER (Wang et al., 2019c) aims to benefit both sides so jointly learn language model and knowledge embedding. However, KEPLER does not di"
2020.coling-main.327,P19-1139,0,0.589772,"ing language and knowledge representation.1 1 Introduction Deep contextualized language models pre-trained on large-scale unlabeled corpora have achieved significant improvement on a wide range of NLP tasks (Peters et al., 2018; Devlin et al., 2019; Yang et al., 2019). However, they are shown to have difficulty capturing factual knowledge (Logan et al., 2019). Recently, there is a growing interest in combining pre-trained language models (PLMs) with structured knowledge. A popular approach is to inject pre-trained entity embeddings into PLMs to better capture factual knowledge, such as ERNIE (Zhang et al., 2019) and KnowBERT (Peters et al., 2019). The shortcomings of these models can be summarized as follows: (1) The entity embeddings are separately pre-trained with some knowledge embedding (KE) models (e.g., TransE (Bordes et al., 2013)), and fixed during training PLMs. Thus they are not real joint models to learn the knowledge embedding and language embedding simultaneously. (2) The previous models only take entity embeddings to enhance PLMs, which are hard to fully capture the rich contextual information of an entity in the knowledge graph (KG). Thus their performance gains are limited by the qual"
2020.coling-main.561,K16-1002,0,0.045755,"Missing"
2020.coling-main.561,C18-1288,0,0.203055,"ses the factuality of the message. Besides, disagreement and query towards descriptive statements are able to trigger drastic discussion and result in validity modification. Although some researchers explore propagation structure of rumor proliferation (Ma et al., 2017; Kumar and Carley, 2019), they typically rely on rough aggregation of locally successional messages. Moreover, the evolution of message interaction depicts the global characteristic of rumor cascades which improves the performance of verification. Figure 1 (b) illustrates the intuition using statistics drawn from PHEME dataset (Kochkina et al., 2018). It can be seen that denial tweets with supportive parent posts appear frequently in false rumors especially in an early stage, while unverified rumors constantly stimulate queries behind positive messages along with time. As rumor cascade evolves, with more dialogue context and auxiliary evidence, assessing the message credibility comprehensively becomes possible. ∗ Corresponding author This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/. License details: http:// 6377 Proceedings of the 28th International Conference on Co"
2020.coling-main.561,P19-1498,0,0.211651,"pened during the message diffusion which is deemed to be important for the identification of rumors. Figure 1 (a) shows a rumor cascade which is identified as false for devilishly suspect Ray Radley’s role in the appalling Sydney siege. As can be seen, denial to false rumor tends to evoke affirmative replies which further confuses the factuality of the message. Besides, disagreement and query towards descriptive statements are able to trigger drastic discussion and result in validity modification. Although some researchers explore propagation structure of rumor proliferation (Ma et al., 2017; Kumar and Carley, 2019), they typically rely on rough aggregation of locally successional messages. Moreover, the evolution of message interaction depicts the global characteristic of rumor cascades which improves the performance of verification. Figure 1 (b) illustrates the intuition using statistics drawn from PHEME dataset (Kochkina et al., 2018). It can be seen that denial tweets with supportive parent posts appear frequently in false rumors especially in an early stage, while unverified rumors constantly stimulate queries behind positive messages along with time. As rumor cascade evolves, with more dialogue con"
2020.coling-main.561,P17-1066,0,0.154828,"interactions happened during the message diffusion which is deemed to be important for the identification of rumors. Figure 1 (a) shows a rumor cascade which is identified as false for devilishly suspect Ray Radley’s role in the appalling Sydney siege. As can be seen, denial to false rumor tends to evoke affirmative replies which further confuses the factuality of the message. Besides, disagreement and query towards descriptive statements are able to trigger drastic discussion and result in validity modification. Although some researchers explore propagation structure of rumor proliferation (Ma et al., 2017; Kumar and Carley, 2019), they typically rely on rough aggregation of locally successional messages. Moreover, the evolution of message interaction depicts the global characteristic of rumor cascades which improves the performance of verification. Figure 1 (b) illustrates the intuition using statistics drawn from PHEME dataset (Kochkina et al., 2018). It can be seen that denial tweets with supportive parent posts appear frequently in false rumors especially in an early stage, while unverified rumors constantly stimulate queries behind positive messages along with time. As rumor cascade evolve"
2020.coling-main.561,D14-1162,0,0.085117,"nfirm the effectiveness of message interaction. Among these two tasks, verification is labeled on cascade-level while stance belongs to tweet-level annotations. PHEME is undoubtedly suitable for our exploration of message interaction as it is constructed by a large amount of conversational threads in which participants tend to launch discussion other than judge on the source tweet. 4.2 Preprocessing and Training Details We preprocess each tweet by the NLTK toolkit (Bird et al., 2009) and follow a procedure of removing url and @, tokenizing, lemmatizing, and removing all the stop words. Glove (Pennington et al., 2014) word embeddings with dimension of 300 are adopted without being fine-tuned. As for training process, we 6382 perform leave-one-event-out (LOEO) cross validation (Kochkina et al., 2018). Although it suffers a lot to handle problems such as evil-balanced instances for each event and semantic inconsistency between events, LOEO is much more representative of real world and has been adopted by latest researches (Kumar and Carley, 2019; Wei et al., 2019). Hyperparameters performing best in development set are fixed and recorded. The network is trained with back propagation using the Adagrad update"
2020.coling-main.561,D19-1485,0,0.334562,"at are ambiguous at the time of posting, then explore how users share and discuss rumors and finally assess their veracity as true, false or unverified. This can be represented as a pipeline of sub-tasks, including rumor detection, stance classification and rumor verification (Zubiaga et al., 2018a). Identifying and debunking rumors automatically has been extensively studied in the past few years. State-of-the-art approaches construct sequential representations following a chronological order and then utilize temporal features to capture dynamic signals (Zubiaga et al., 2016; Ma et al., 2016; Wei et al., 2019). Although the source content stays invariable, time-series modeling successfully locates modifiers who might import evidence to correct misinformation or stir up enmity to discredit truth (Zhang et al., 2013). These models generate promising results, however, they ignore local interactions happened during the message diffusion which is deemed to be important for the identification of rumors. Figure 1 (a) shows a rumor cascade which is identified as false for devilishly suspect Ray Radley’s role in the appalling Sydney siege. As can be seen, denial to false rumor tends to evoke affirmative rep"
2020.coling-main.561,P18-1101,0,0.10894,"poses the discrete variational autoencoders (DVAEs) which assume that the corresponding prior distribution over the latent space is characterized by independent categorical distributions. Especially for text mining, discrete variables are adaptive to holistic properties of text and much more friendly for interpreting categories of natural language such as style, topic and high-level syntactic features. For instance, in neural dialog generation, DVAE is able to learn underlying dialogue intentions that can be interpreted as actions guiding the generation of machine responses (Wen et al., 2017; Zhao et al., 2018). In this paper, we learn discrete latent variables between inherited post pairs and incorporate them with textual information to model message interaction. 3 Proposed Model Resolution of rumor cascades can be formulated as a supervised classification problem. Given a treestuctured T WITTER cascade C which corresponds to a root tweet r0 and its relevant responsive tweets {r1 , r2 , ..., rT }, the goal is to recognize the stance of each tweet Yis as support, comment, deny or query, as well as determine the class of the cascade Yv as true, false or unverified. From our dataset, for each tweet ri"
2020.coling-main.561,N19-1163,0,0.0121094,"ssages, dynamic time series structure (Ma et al., 2015) and tree model using propagation pattern (Ma et al., 2017) is effective of depicting global difference between rumor and non-rumor claims. To avoid the effort and bias of feature engineering, methods based on deep neural networks are massively applied and have demonstrated great efficacy of discovering data representation automatically. Ma 6378 et al. (2016) employ recurrent neural networks (RNNs) to capture dynamic temporal signals. Yu et al. (2017) use convolutional neural networks (CNNs) to flexibly extract evidential posts. Recently, Zhou et al. (2019) integrate reinforcement learning to select the minimum number of posts required for early rumor detection. Ma et al. (2019) generate less indicative semantic representation via generative adversarial networks to gain better generalization for rumor detection. Besides, since rumor resolution is a coherent process, researchers also combine detection and stance classification with verification under the framework of multi-task learning (Ma et al., 2018; Kochkina et al., 2018; Kumar and Carley, 2019; Wei et al., 2019). In summary, deep learning approaches for rumor resolution involves three criti"
2020.emnlp-main.181,P19-1285,0,0.181006,"han some threshold value Γ, then the draft label yi∗ has a high probability of being wrong. Hence, we utilize a novel two-stream self-attention model to refine those uncertain labels using long-term label dependencies and word-label interactions. 3.2 Two-Stream Self-Attention for Label Refinement Given the draft labels and corresponding epistemic uncertainties, we seek the help of label dependencies and word-label interactions to refine the uncertain labels. In order to refine the draft labels in parallel, we use the Transformer (Vaswani et al., 2017) incorporating relative position encoding (Dai et al., 2019) to model the words and draft labels. In the standard Transformer, the attention score incorporating absolute position encoding between query qi and key vector kj can be decomposed as &gt; &gt; &gt; &gt; Aabs i,j = Exi Wq Wk Exj + Exi Wq Wk Uj &gt; &gt; &gt; + U&gt; i Wq Wk Exj + Ui Wq Wk Uj , (6) where U ∈ RLmax ×d provides a set of positional encodings. The ith row Ui corresponds to the ith absolute position and Lmax prescribes the maximum possible length to be modeled. The relative position between labels is very important for modeling the label dependencies. Inspired by Dai et al. (2019), we modify the Eq.6 using"
2020.emnlp-main.181,Q16-1026,0,0.21736,"his paper can be summarized as follows: 1) we propose the use of Bayesian neural networks to estimate 1 We slightly modified the code using Bayesian neural networks. 2.1 Related Work and Background Sequence Labeling Traditional sequence labeling models use statistical approaches such as Hidden Markov Models (HMM) and Conditional Random Fields (CRF) (Passos et al., 2014; Cuong et al., 2014; Luo et al., 2015) with handcrafted features and task-specific resources. With advances in deep learning, neural models could achieve competitive performances without massive handcrafted feature engineering (Chiu and Nichols, 2016; Santos and Zadrozny, 2014). In recent years, modeling label dependencies has been the other focus of sequence labeling tasks, such as using a CRF layer integrated with neural encoders to capture label transition patterns (Zhou and Xu, 2015; Ma and Hovy, 2016), and introducing label embeddings to manage longer ranges of dependencies (Vaswani et al., 2016; Zhang et al., 2018; Cui and Zhang, 2019). Our work is an extension of label embedding methods, which applies label dependencies and word-label interactions to only refine the labels with high probabilities of being incorrect. The probability"
2020.emnlp-main.181,W02-1001,0,0.534138,"Missing"
2020.emnlp-main.181,D19-1422,0,0.359939,"ojhYt75BSmjl47k=&quot;&gt;AAACyXicjVHLSsNAFD2Nr1pfVZdugkVwVZIq6LLoRnBTwT6gLZJMp3VsXiYTsRZX/oBb/THxD/QvvDOmoBbRCUnOnHvPmbn3upEnEmlZrzljZnZufiG/WFhaXlldK65vNJIwjRmvs9AL45brJNwTAa9LIT3eimLu+K7Hm+7wWMWbNzxORBicy1HEu74zCERfMEcS1ehI4fPkoliyypZe5jSwM1BCtmph8QUd9BCCIYUPjgCSsAcHCT1t2LAQEdfFmLiYkNBxjnsUSJtSFqcMh9ghfQe0a2dsQHvlmWg1o1M8emNSmtghTUh5MWF1mqnjqXZW7G/eY+2p7jaiv5t5+cRKXBL7l26S+V+dqkWij0Ndg6CaIs2o6ljmkuquqJubX6qS5BARp3CP4jFhppWTPptak+jaVW8dHX/TmYpVe5blpnhXt6QB2z/HOQ0albK9V66c7ZeqR9mo89jCNnZpngeo4gQ11Mn7Co94wrNxalwbt8bdZ6qRyzSb+LaMhw/935G5&lt;/latexit&gt; ... Figure 1: Schematic of label refinement framework (Cui and Zhang, 2019). The goal is refining the label of “Arab” using contextual labels and words, while the refinement of other correct labels may be negatively impacted by incorrect draft labels. Linguistic sequence labeling is one of the fundamental tasks in natural language processing. It has the goal of predicting a linguistic label for each word, including part-of-speech (POS) tagging, text chunking, and named entity recognition (NER). Benefiting from representation learning, neural network-based approaches can achieve state-ofthe-art performance without massive handcrafted feature engineering (Ma and Hovy,"
2020.emnlp-main.181,N19-1423,0,0.206605,"words, while the refinement of other correct labels may be negatively impacted by incorrect draft labels. Linguistic sequence labeling is one of the fundamental tasks in natural language processing. It has the goal of predicting a linguistic label for each word, including part-of-speech (POS) tagging, text chunking, and named entity recognition (NER). Benefiting from representation learning, neural network-based approaches can achieve state-ofthe-art performance without massive handcrafted feature engineering (Ma and Hovy, 2016; Lample et al., 2016; Strubell et al., 2017; Peters et al., 2018; Devlin et al., 2019). Although the use of representation learning to obtain better text representation is very successful, Both authors contributed equally. X Refinement Input Introduction ∗ I-LOC creating better models for label dependencies has always been the focus of sequence labeling tasks (Collobert et al., 2011; Ye and Ling, 2018; Zhang et al., 2018). Among them, the CRF layer integrated with neural encoders to capture label transition patterns (Zhou and Xu, 2015; Ma and Hovy, 2016) has become ubiquitous in sequence labeling tasks. However, CRF only captures the neighboring label dependencies and must rely"
2020.emnlp-main.181,P06-1141,0,0.191903,"Missing"
2020.emnlp-main.181,N16-1030,0,0.824531,"goal is refining the label of “Arab” using contextual labels and words, while the refinement of other correct labels may be negatively impacted by incorrect draft labels. Linguistic sequence labeling is one of the fundamental tasks in natural language processing. It has the goal of predicting a linguistic label for each word, including part-of-speech (POS) tagging, text chunking, and named entity recognition (NER). Benefiting from representation learning, neural network-based approaches can achieve state-ofthe-art performance without massive handcrafted feature engineering (Ma and Hovy, 2016; Lample et al., 2016; Strubell et al., 2017; Peters et al., 2018; Devlin et al., 2019). Although the use of representation learning to obtain better text representation is very successful, Both authors contributed equally. X Refinement Input Introduction ∗ I-LOC creating better models for label dependencies has always been the focus of sequence labeling tasks (Collobert et al., 2011; Ye and Ling, 2018; Zhang et al., 2018). Among them, the CRF layer integrated with neural encoders to capture label transition patterns (Zhou and Xu, 2015; Ma and Hovy, 2016) has become ubiquitous in sequence labeling tasks. However,"
2020.emnlp-main.181,D15-1104,0,0.0311838,"s on three sequence labeling benchmarks demonstrated that the proposed method not only outperformed the CRF-based methods but also significantly accelerated the inference process. The main contributions of this paper can be summarized as follows: 1) we propose the use of Bayesian neural networks to estimate 1 We slightly modified the code using Bayesian neural networks. 2.1 Related Work and Background Sequence Labeling Traditional sequence labeling models use statistical approaches such as Hidden Markov Models (HMM) and Conditional Random Fields (CRF) (Passos et al., 2014; Cuong et al., 2014; Luo et al., 2015) with handcrafted features and task-specific resources. With advances in deep learning, neural models could achieve competitive performances without massive handcrafted feature engineering (Chiu and Nichols, 2016; Santos and Zadrozny, 2014). In recent years, modeling label dependencies has been the other focus of sequence labeling tasks, such as using a CRF layer integrated with neural encoders to capture label transition patterns (Zhou and Xu, 2015; Ma and Hovy, 2016), and introducing label embeddings to manage longer ranges of dependencies (Vaswani et al., 2016; Zhang et al., 2018; Cui and Z"
2020.emnlp-main.181,P16-1101,0,0.395958,"Zhang, 2019). The goal is refining the label of “Arab” using contextual labels and words, while the refinement of other correct labels may be negatively impacted by incorrect draft labels. Linguistic sequence labeling is one of the fundamental tasks in natural language processing. It has the goal of predicting a linguistic label for each word, including part-of-speech (POS) tagging, text chunking, and named entity recognition (NER). Benefiting from representation learning, neural network-based approaches can achieve state-ofthe-art performance without massive handcrafted feature engineering (Ma and Hovy, 2016; Lample et al., 2016; Strubell et al., 2017; Peters et al., 2018; Devlin et al., 2019). Although the use of representation learning to obtain better text representation is very successful, Both authors contributed equally. X Refinement Input Introduction ∗ I-LOC creating better models for label dependencies has always been the focus of sequence labeling tasks (Collobert et al., 2011; Ye and Ling, 2018; Zhang et al., 2018). Among them, the CRF layer integrated with neural encoders to capture label transition patterns (Zhou and Xu, 2015; Ma and Hovy, 2016) has become ubiquitous in sequence labe"
2020.emnlp-main.181,J93-2004,0,0.0688971,"n is collected from Reuters Corpus. The dataset divide name entities into four different types: persons, locations, organizations, and miscellaneous entities. We use the BIOES tag scheme instead of standard BIO2, which is the same as Ma and Hovy (2016). OntoNotes 5.0. English NER dataset OntoNotes 5.0 (Weischedel et al., 2013) is a large corpus consists of various genres, such as newswire, broadcast, and telephone speech. Named entities are labeled in eleven types and values are specifically divided into seven types, like DATE, TIME, ORDINAL. WSJ. Wall Street Journal portion of Penn Treebank (Marcus et al., 1993), which contains 45 types of part-of-speech tags. We adopts standard splits following previous works (Collins, 2002; Manning, 2320 Dataset CoNLL2003 OntoNotes WSJ #Train #Dev #Test class 204,567 1,088,503 912,344 51,578 147,724 131,768 46,666 152,728 129,654 17 73 45 Models Chiu and Nichols (2016) Strubell et al. (2017) Liu et al. (2018) Chen et al. (2019) BiLSTM-CRF (Ma and Hovy, 2016) BiLSTM-Softmax (Yang et al., 2018) BiLSTM-Seq2seq (Zhang et al., 2018) Rel-Transformer (Dai et al., 2019) BiLSTM-LAN (Cui and Zhang, 2019) BiLSTM-UANet (M = 8) Table 2: Statistics of CoNLL2003, OntoNotes and WS"
2020.emnlp-main.181,W14-1609,0,0.0163721,"a faster prediction. Experimental results on three sequence labeling benchmarks demonstrated that the proposed method not only outperformed the CRF-based methods but also significantly accelerated the inference process. The main contributions of this paper can be summarized as follows: 1) we propose the use of Bayesian neural networks to estimate 1 We slightly modified the code using Bayesian neural networks. 2.1 Related Work and Background Sequence Labeling Traditional sequence labeling models use statistical approaches such as Hidden Markov Models (HMM) and Conditional Random Fields (CRF) (Passos et al., 2014; Cuong et al., 2014; Luo et al., 2015) with handcrafted features and task-specific resources. With advances in deep learning, neural models could achieve competitive performances without massive handcrafted feature engineering (Chiu and Nichols, 2016; Santos and Zadrozny, 2014). In recent years, modeling label dependencies has been the other focus of sequence labeling tasks, such as using a CRF layer integrated with neural encoders to capture label transition patterns (Zhou and Xu, 2015; Ma and Hovy, 2016), and introducing label embeddings to manage longer ranges of dependencies (Vaswani et a"
2020.emnlp-main.181,N18-1202,0,0.0266918,"Missing"
2020.emnlp-main.181,K19-1058,0,0.463518,"Missing"
2020.emnlp-main.181,D17-1283,0,0.303028,"label of “Arab” using contextual labels and words, while the refinement of other correct labels may be negatively impacted by incorrect draft labels. Linguistic sequence labeling is one of the fundamental tasks in natural language processing. It has the goal of predicting a linguistic label for each word, including part-of-speech (POS) tagging, text chunking, and named entity recognition (NER). Benefiting from representation learning, neural network-based approaches can achieve state-ofthe-art performance without massive handcrafted feature engineering (Ma and Hovy, 2016; Lample et al., 2016; Strubell et al., 2017; Peters et al., 2018; Devlin et al., 2019). Although the use of representation learning to obtain better text representation is very successful, Both authors contributed equally. X Refinement Input Introduction ∗ I-LOC creating better models for label dependencies has always been the focus of sequence labeling tasks (Collobert et al., 2011; Ye and Ling, 2018; Zhang et al., 2018). Among them, the CRF layer integrated with neural encoders to capture label transition patterns (Zhou and Xu, 2015; Ma and Hovy, 2016) has become ubiquitous in sequence labeling tasks. However, CRF only captures the n"
2020.emnlp-main.181,N16-1027,0,0.150284,"been the focus of sequence labeling tasks (Collobert et al., 2011; Ye and Ling, 2018; Zhang et al., 2018). Among them, the CRF layer integrated with neural encoders to capture label transition patterns (Zhou and Xu, 2015; Ma and Hovy, 2016) has become ubiquitous in sequence labeling tasks. However, CRF only captures the neighboring label dependencies and must rely on inefficient Viterbi decoding. Many of the recent methods try to introduce label embeddings to manage longer ranges of dependencies, such as two-stage label refinement (Krishnan and Manning, 2006; Cui and Zhang, 2019) and seq2seq (Vaswani et al., 2016; Zhang et al., 2018) frameworks. In particular, Cui and Zhang (2019) introduced a hierarchically-refined representation of marginal label distributions, which predicts a sequence of draft labels in advance and then uses the word-label interactions to refine them. Although these methods can model longer label dependencies, they are vulnerable to error propagation: if a label is mistakenly predicted during inference, the error will be propagated and the other labels conditioned on this one will be impacted (Bengio et al., 2015). As shown in Figure 1, the label attention network (LAN) (Cui and 2"
2020.emnlp-main.181,D18-1279,0,0.0456029,"M = 8) Table 2: Statistics of CoNLL2003, OntoNotes and WSJ datasets, where # represents the number of tokens in datasets. The class number of NER datasets is counted under BIOES tag scheme. 2011), selecting section 0-18 for training, section 19-21 for validation and section 22-24 for test. 4.2 In this work, we mainly focus on improving decoding efficiency and enhancing label dependencies. Thus, we make comparisons with the classic methods that have different decoding layers, such as Softmax, CRF, and LAN frameworks. We also compare some recent competitive methods, such as Transformer, IntNet (Xin et al., 2018), and BERT (Devlin et al., 2019). BiLSTM-Softmax. This baseline uses bidirectional LSTM to reprensent a sequence. The BiLSTM concatenates the forward hidden state → − ← − h i and backward hidden state h i to form an → − ← − integral representation hi = [ h i ; h i ]. Finally, sentence representation H = {hi , · · · , hn } is fed to softmax layer for predicting. BiLSTM-CRF. A CRF layer is used on top of the hidden vectors H (Ma and Hovy, 2016). The CRF can model bigram interactions between two successive labels (Lample et al., 2016) instead of making independent labeling decisions for each outp"
2020.emnlp-main.181,C18-1327,0,0.121374,"Missing"
2020.emnlp-main.181,P18-4013,0,0.0156567,"ayers of multihead transformer for WSJ and CoNLL2003 and 3 for OntoNotes dataset to refine the label. The number of heads is chosen from {5, 7, 9}, and the dimension of each head is chosen from {80, 120, 160} via grid search. We use SGD as the optimizer for variational LSTM and Adam (Kingma and Ba, 2014) for transformer. Learning rates are set to 0.015 for SGD on CoNLL2003 and Ontonotes datasets and 0.2 on WSJ dataset. The learning rates for Adam are set to 0.0001 for all datasets. F1 score and accuracy are used for NER and POS tagging, respectively. All experiments are implemented in NCRF++ (Yang and Zhang, 2018) and conducted using a GeForce GTX 1080Ti with 11GB memory. More details are shown in our codes3 . 5 Results and Analysis In this section, we present the experimental results of the proposed and baseline models. We show that the proposed method not only achieves better performance but also has a significant speed advantage. Since our contribution is mainly focused on the label decoding layer, the proposed model can also be combined with the latest pretrained model to further improve performance. Rel-Transformer. This baseline model adopts self-attention mechanism with relative position represe"
2020.emnlp-main.181,P18-2038,0,0.0166228,"named entity recognition (NER). Benefiting from representation learning, neural network-based approaches can achieve state-ofthe-art performance without massive handcrafted feature engineering (Ma and Hovy, 2016; Lample et al., 2016; Strubell et al., 2017; Peters et al., 2018; Devlin et al., 2019). Although the use of representation learning to obtain better text representation is very successful, Both authors contributed equally. X Refinement Input Introduction ∗ I-LOC creating better models for label dependencies has always been the focus of sequence labeling tasks (Collobert et al., 2011; Ye and Ling, 2018; Zhang et al., 2018). Among them, the CRF layer integrated with neural encoders to capture label transition patterns (Zhou and Xu, 2015; Ma and Hovy, 2016) has become ubiquitous in sequence labeling tasks. However, CRF only captures the neighboring label dependencies and must rely on inefficient Viterbi decoding. Many of the recent methods try to introduce label embeddings to manage longer ranges of dependencies, such as two-stage label refinement (Krishnan and Manning, 2006; Cui and Zhang, 2019) and seq2seq (Vaswani et al., 2016; Zhang et al., 2018) frameworks. In particular, Cui and Zhang ("
2020.emnlp-main.181,P15-1109,0,0.0678095,"rformance without massive handcrafted feature engineering (Ma and Hovy, 2016; Lample et al., 2016; Strubell et al., 2017; Peters et al., 2018; Devlin et al., 2019). Although the use of representation learning to obtain better text representation is very successful, Both authors contributed equally. X Refinement Input Introduction ∗ I-LOC creating better models for label dependencies has always been the focus of sequence labeling tasks (Collobert et al., 2011; Ye and Ling, 2018; Zhang et al., 2018). Among them, the CRF layer integrated with neural encoders to capture label transition patterns (Zhou and Xu, 2015; Ma and Hovy, 2016) has become ubiquitous in sequence labeling tasks. However, CRF only captures the neighboring label dependencies and must rely on inefficient Viterbi decoding. Many of the recent methods try to introduce label embeddings to manage longer ranges of dependencies, such as two-stage label refinement (Krishnan and Manning, 2006; Cui and Zhang, 2019) and seq2seq (Vaswani et al., 2016; Zhang et al., 2018) frameworks. In particular, Cui and Zhang (2019) introduced a hierarchically-refined representation of marginal label distributions, which predicts a sequence of draft labels in a"
2020.emnlp-main.292,D18-1316,0,0.0189004,"ust too difficult to alter in our current generation framework. It needs more advanced models such as text style transfer (Shen et al., 2017; Jin et al., 2019b). 7 Related Work Robustness in NLP Robustness in NLP has attracted extensive attention in recent works (Hsieh et al., 2019; Li et al., 2016). As a popular method to probe the robustness of models, adversarial text generation becomes an emerging research field in NLP. Techniques include adding extraneous text to the input (Jia and Liang, 2016), character-level noise (Belinkov and Bisk, 2018; Ebrahimi et al., 2018), and word replacement (Alzantot et al., 2018; Jin et al., 2019a). Using the adversarial generation techniques, new adversarial test sets are proposed for several tasks such as paraphrasing (Zhang et al., 2019b) and entailment (Glockner et al., 2018; McCoy et al., 2019). Aspect-Based Sentiment Analysis ABSA has emerged as an active research area recently. Early works hand-craft sentiment lexicons and syntactic features for rule-based classifiers (Vo and Zhang, 2015; Kiritchenko et al., 2014). Recent neural network-based models use architectures such as LSTM (Tang et al., 2016a), CNN (Xue and Li, 2018), Attention mechanisms (Wang et al.,"
2020.emnlp-main.292,N19-1423,0,0.0480086,"aspect and word embeddings of each token. GatedCNN: Xue and Li (2018) use a Gated Convolutional Neural Networks (CNN) that applies a Tanh-ReLU gating mechanism to the CNNencoded text with aspect embeddings. MemNet: Tang et al. (2016b) use memory networks to store the sentence as external memory and calculate the attention with the target aspect. GCN: Aspect-specific Graph Convolutional Networks (GCN) (Zhang et al., 2019a) first applies 3598 GCN over the syntax tree of the sentence and then imposes an aspect-specific masking layer on its top. BERT: Xu et al. (2019a) uses a BERT-based baseline (Devlin et al., 2019) and takes as input the concatenation of the aspect term and the sentence. BERT-PT: Xu et al. (2019a) post-train BERT on other review datasets such as Amazon laptop reviews (He and McAuley, 2016) and Yelp Dataset Challenge reviews, and finetune on ABSA tasks. CapsBERT: (Jiang et al., 2019) encode the sentence and the aspect term with BERT, and then feed it into Capsule Networks to predict the polarity. BERT-Sent: For more in-depth analysis, we also implement a sentence classification baseline. BERT-Sent takes as input sentences without aspect information, and predicts the “global” sentiment. W"
2020.emnlp-main.292,P14-2009,0,0.160287,"Missing"
2020.emnlp-main.292,P18-2006,0,0.0197411,"se. For example, “a 2-hour wait” is negative bust too difficult to alter in our current generation framework. It needs more advanced models such as text style transfer (Shen et al., 2017; Jin et al., 2019b). 7 Related Work Robustness in NLP Robustness in NLP has attracted extensive attention in recent works (Hsieh et al., 2019; Li et al., 2016). As a popular method to probe the robustness of models, adversarial text generation becomes an emerging research field in NLP. Techniques include adding extraneous text to the input (Jia and Liang, 2016), character-level noise (Belinkov and Bisk, 2018; Ebrahimi et al., 2018), and word replacement (Alzantot et al., 2018; Jin et al., 2019a). Using the adversarial generation techniques, new adversarial test sets are proposed for several tasks such as paraphrasing (Zhang et al., 2019b) and entailment (Glockner et al., 2018; McCoy et al., 2019). Aspect-Based Sentiment Analysis ABSA has emerged as an active research area recently. Early works hand-craft sentiment lexicons and syntactic features for rule-based classifiers (Vo and Zhang, 2015; Kiritchenko et al., 2014). Recent neural network-based models use architectures such as LSTM (Tang et al., 2016a), CNN (Xue and L"
2020.emnlp-main.292,N19-1259,0,0.307938,"s, including adversarial training detailed in Section 5.4. 2 Data Generation As shown in Table 1, we aim to build a systematic method to generate all possible aspect-related alternations, in order to remove the confounding factors in the existing ABSA data. In the following, we will introduce three different ways to disentangle the sentiment of the target aspect from sentiments of non-target aspects. 2.1 R EV T GT The first strategy is to generate sentences that reverse the original sentiment of the target aspect. The word spans of each aspect’s sentiment of SemEval 2014 data are provided by (Fan et al., 2019a). We design two methods to reverse the sentiment, and one additional step of conjunction adjustment on top of the two methods to polish the resulting sentence. 3595 Strategy Example It’s light and easy to transport. Flip Opinion → It’s heavy and difficult to transport. The menu changes seasonally. Add Negation → The menu does not change seasonally. Adjust The food is good, and the decor is nice. Conjunctions → The food is good, but the decor is nasty. Table 2: Three strategies and examples of R EV T GT. Strategy Original sentence & sentiment R EV N ON Flip same-sentiment non-target aspects ("
2020.emnlp-main.292,P18-2103,0,0.0216594,"s in NLP has attracted extensive attention in recent works (Hsieh et al., 2019; Li et al., 2016). As a popular method to probe the robustness of models, adversarial text generation becomes an emerging research field in NLP. Techniques include adding extraneous text to the input (Jia and Liang, 2016), character-level noise (Belinkov and Bisk, 2018; Ebrahimi et al., 2018), and word replacement (Alzantot et al., 2018; Jin et al., 2019a). Using the adversarial generation techniques, new adversarial test sets are proposed for several tasks such as paraphrasing (Zhang et al., 2019b) and entailment (Glockner et al., 2018; McCoy et al., 2019). Aspect-Based Sentiment Analysis ABSA has emerged as an active research area recently. Early works hand-craft sentiment lexicons and syntactic features for rule-based classifiers (Vo and Zhang, 2015; Kiritchenko et al., 2014). Recent neural network-based models use architectures such as LSTM (Tang et al., 2016a), CNN (Xue and Li, 2018), Attention mechanisms (Wang et al., 2016), Capsule Network (Jiang et al., 2019), and the pretrained model BERT (Xu et al., 2019a). Similar to the motivation in our paper, some work shows preliminary speculation that the current ABSA dataset"
2020.emnlp-main.292,P19-1147,0,0.0350996,"Missing"
2020.emnlp-main.292,P16-1002,0,0.0208959,"stances with complicated sentiment expressions which rely on commonsense. For example, “a 2-hour wait” is negative bust too difficult to alter in our current generation framework. It needs more advanced models such as text style transfer (Shen et al., 2017; Jin et al., 2019b). 7 Related Work Robustness in NLP Robustness in NLP has attracted extensive attention in recent works (Hsieh et al., 2019; Li et al., 2016). As a popular method to probe the robustness of models, adversarial text generation becomes an emerging research field in NLP. Techniques include adding extraneous text to the input (Jia and Liang, 2016), character-level noise (Belinkov and Bisk, 2018; Ebrahimi et al., 2018), and word replacement (Alzantot et al., 2018; Jin et al., 2019a). Using the adversarial generation techniques, new adversarial test sets are proposed for several tasks such as paraphrasing (Zhang et al., 2019b) and entailment (Glockner et al., 2018; McCoy et al., 2019). Aspect-Based Sentiment Analysis ABSA has emerged as an active research area recently. Early works hand-craft sentiment lexicons and syntactic features for rule-based classifiers (Vo and Zhang, 2015; Kiritchenko et al., 2014). Recent neural network-based mo"
2020.emnlp-main.292,D19-1654,0,0.0384818,"mory and calculate the attention with the target aspect. GCN: Aspect-specific Graph Convolutional Networks (GCN) (Zhang et al., 2019a) first applies 3598 GCN over the syntax tree of the sentence and then imposes an aspect-specific masking layer on its top. BERT: Xu et al. (2019a) uses a BERT-based baseline (Devlin et al., 2019) and takes as input the concatenation of the aspect term and the sentence. BERT-PT: Xu et al. (2019a) post-train BERT on other review datasets such as Amazon laptop reviews (He and McAuley, 2016) and Yelp Dataset Challenge reviews, and finetune on ABSA tasks. CapsBERT: (Jiang et al., 2019) encode the sentence and the aspect term with BERT, and then feed it into Capsule Networks to predict the polarity. BERT-Sent: For more in-depth analysis, we also implement a sentence classification baseline. BERT-Sent takes as input sentences without aspect information, and predicts the “global” sentiment. We use it because if other ABSA models fails to pay attention to aspects, they will degenerate to a sentence classifier. If so, they will resemble BERTSent, which performs well on original tests and badly on ARTS. So BERT-Sent is a reference to check degenerated aspect-level models. 4.2 Imp"
2020.emnlp-main.292,D19-1306,1,0.885032,"Missing"
2020.emnlp-main.292,P18-1110,0,0.0579983,"Missing"
2020.emnlp-main.292,S14-2076,0,0.0322958,"adding extraneous text to the input (Jia and Liang, 2016), character-level noise (Belinkov and Bisk, 2018; Ebrahimi et al., 2018), and word replacement (Alzantot et al., 2018; Jin et al., 2019a). Using the adversarial generation techniques, new adversarial test sets are proposed for several tasks such as paraphrasing (Zhang et al., 2019b) and entailment (Glockner et al., 2018; McCoy et al., 2019). Aspect-Based Sentiment Analysis ABSA has emerged as an active research area recently. Early works hand-craft sentiment lexicons and syntactic features for rule-based classifiers (Vo and Zhang, 2015; Kiritchenko et al., 2014). Recent neural network-based models use architectures such as LSTM (Tang et al., 2016a), CNN (Xue and Li, 2018), Attention mechanisms (Wang et al., 2016), Capsule Network (Jiang et al., 2019), and the pretrained model BERT (Xu et al., 2019a). Similar to the motivation in our paper, some work shows preliminary speculation that the current ABSA datasets might be downgraded to sentence-level sentiment classification (Xu et al., 2019b). 8 Conclusion In this paper, we proposed a simple but effective mechanism to generate test instances to probe the aspect robustness of the models. We enhanced the"
2020.emnlp-main.292,P19-1334,0,0.0310923,"Missing"
2020.emnlp-main.292,S14-2004,0,0.0585091,"e sentiments are different from the target aspect’s, and then append these to the end of the original example. For example, “Great food A DD D IFF and best of all GREAT beer!” −−−−−→ “Great food and best of all GREAT beer, but management is less than accommodating, music is too heavy, and service is severely slow.” In this way, A DD D IFF enables the advanced testing of whether the model will be confused when there are more irrelevant aspects with opposite sentiments. 3 ARTS Dataset 3.1 Overview Our source data is the most5 widely used ABSA dataset, SemEval 2014 Laptop and Restaurant Reviews (Pontiki et al., 2014).6 We follow (Wang et al., 2016; Ma et al., 2017; Xu et al., 2019a) to remove instances with conflicting polarity and only keep positive, negative, and neutral labels. We use the train-dev split as in (Xu et al., 2019a). The resulting Laptop dataset has 2,163 training, 150 4 The full AspectSet is available on our GitHub. We surveyed deep learning-based ABSA papers from 2015 to 2020 at top conferences (ACL, EMNLP, NAACL, NeurIPS, ICLR, ICML, AAAI, IJCAI). Among the 63 ABSA papers, 50 use SemEval 2014 Laptop and Restaurant, which is the top 1 widely used dataset. 6 http://alt.qcri.org/semeval201"
2020.emnlp-main.292,D16-1021,0,0.207321,"et has an increasing number of all labels, and especially balances the ratio of positive-to-negative labels from the original 2.66 to 1.5 on Laptop, and from 3.71 to 1.77 on Restaurant. For the aspect-related challenge in the test set, the new test set, first of all, has a larger number of aspects per sentence than the original. Our test set also features the higher disentanglement of the target aspect from the non-target aspects that have Models For a comprehensive overview of the ABSA field, we conduct extensive experiments on models with a variety of neural network architectures. TD-LSTM: (Tang et al., 2016a) uses two Long Short-Term Memory Networks (LSTM) to encode the preceding and following contexts of the target aspect (inclusive) and concatenate the last hidden states of the two LSTMs to make the sentiment classification. AttLSTM: Wang et al. (2016) apply an Attention-based LSTM on the concatenatation of the aspect and word embeddings of each token. GatedCNN: Xue and Li (2018) use a Gated Convolutional Neural Networks (CNN) that applies a Tanh-ReLU gating mechanism to the CNNencoded text with aspect embeddings. MemNet: Tang et al. (2016b) use memory networks to store the sentence as externa"
2020.emnlp-main.292,D16-1058,0,0.555148,"target aspect’s, and then append these to the end of the original example. For example, “Great food A DD D IFF and best of all GREAT beer!” −−−−−→ “Great food and best of all GREAT beer, but management is less than accommodating, music is too heavy, and service is severely slow.” In this way, A DD D IFF enables the advanced testing of whether the model will be confused when there are more irrelevant aspects with opposite sentiments. 3 ARTS Dataset 3.1 Overview Our source data is the most5 widely used ABSA dataset, SemEval 2014 Laptop and Restaurant Reviews (Pontiki et al., 2014).6 We follow (Wang et al., 2016; Ma et al., 2017; Xu et al., 2019a) to remove instances with conflicting polarity and only keep positive, negative, and neutral labels. We use the train-dev split as in (Xu et al., 2019a). The resulting Laptop dataset has 2,163 training, 150 4 The full AspectSet is available on our GitHub. We surveyed deep learning-based ABSA papers from 2015 to 2020 at top conferences (ACL, EMNLP, NAACL, NeurIPS, ICLR, ICML, AAAI, IJCAI). Among the 63 ABSA papers, 50 use SemEval 2014 Laptop and Restaurant, which is the top 1 widely used dataset. 6 http://alt.qcri.org/semeval2014/ task4/ 5 Restaurant 1,120 3,"
2020.emnlp-main.292,N19-1242,0,0.0762749,"Missing"
2020.emnlp-main.292,P18-1234,0,0.01456,"nt of the target aspect from the non-target aspects that have Models For a comprehensive overview of the ABSA field, we conduct extensive experiments on models with a variety of neural network architectures. TD-LSTM: (Tang et al., 2016a) uses two Long Short-Term Memory Networks (LSTM) to encode the preceding and following contexts of the target aspect (inclusive) and concatenate the last hidden states of the two LSTMs to make the sentiment classification. AttLSTM: Wang et al. (2016) apply an Attention-based LSTM on the concatenatation of the aspect and word embeddings of each token. GatedCNN: Xue and Li (2018) use a Gated Convolutional Neural Networks (CNN) that applies a Tanh-ReLU gating mechanism to the CNNencoded text with aspect embeddings. MemNet: Tang et al. (2016b) use memory networks to store the sentence as external memory and calculate the attention with the target aspect. GCN: Aspect-specific Graph Convolutional Networks (GCN) (Zhang et al., 2019a) first applies 3598 GCN over the syntax tree of the sentence and then imposes an aspect-specific masking layer on its top. BERT: Xu et al. (2019a) uses a BERT-based baseline (Devlin et al., 2019) and takes as input the concatenation of the aspe"
2020.emnlp-main.292,D19-1464,0,0.224295,"Missing"
2020.emnlp-main.292,N19-1131,0,0.0432846,"Missing"
2020.emnlp-main.320,2020.acl-main.499,0,0.0209775,"rounded instances and to further extend to unseen constants, The basic idea is that hypernyms have ordering relations and such relations correspond to component-wise comparison in semantic vector space. Hu et al. (2016) introduce a teacher-student model, where the teacher model is a rule-regularized neural network, whose predictions are used to teach the student model. Wang and Poon (2018) generalize virtual evidence (Pearl, 2014) to arbitrary potential functions over inputs and outputs, and use deep probabilistic logic to integrate indirection supervision into neural networks. More recently, Asai and Hajishirzi (2020) regularize question answering systems with symmetric consistency and symmetric consistency. The former creates a symmetric question by replacing words with their antonyms in comparison question, while the latter is for causal reasoning questions through creating new examples when positive causal relationship between two cause-effect questions holds. The second group is to incorporate logic-specific modules into the inference process (Yang et al., 2017; Dong et al., 2019). For example, Rockt¨aschel and Riedel (2017) target at the problem of knowledge base completion, and use neural unification"
2020.emnlp-main.320,D19-1565,0,0.0399722,"Missing"
2020.emnlp-main.320,D16-1146,0,0.0382263,"Missing"
2020.emnlp-main.320,N19-1423,0,0.173838,"ency between sentencelevel and token-level predictions (§3.2) and textual knowledge from literal definitions of propaganda techniques (§3.3). At last, we describe the training and inference procedures (§3.4). 3.1 Base Model To better exploit the sentence-level information and further benefit token-level prediction, we develop a fine-grained multi-task method as our base model, which makes predictions for 18 propaganda techniques at both sentence level and token level. Inspired by the success of pre-trained language models on various natural language processing downstream tasks, we adopt BERT (Devlin et al., 2019) as the backbone model here. For each input sentence, the sequence is modified as “[CLS]sentence tokens[SEP ]”. Specifically, on top of BERT, we add 19 binary classifiers for finegrained sentence-level predictions, and one 19-way classifier for token-level predictions, where all classifiers are implemented as linear layers. At sentence level, we perform multiple binary classifications and this can further support leveraging declarative knowledge. The last representation of the special token [CLS] which is regarded as a summary of the semantic content of the input, is adopted to perform multipl"
2020.emnlp-main.320,Q15-1027,0,0.0123863,"ta. It is appealing to leverage the advantages from both worlds. In NLP community, the injection of logic to neural network can be generally divided into two groups. Methods in the first group regularize neural network with logicdriven loss functions (Xu et al., 2018; Fischer et al., 2019; Li et al., 2019). For example, Rockt¨aschel et al. (2015) target on the problem of knowledge base completion. After extracting and annotating propositional logical rules about relations in knowledge graph, they ground these rules to facts from knowledge graph and add a differentiable training loss function. Kruszewski et al. (2015) map text to Boolean representations, and derive loss functions based on implication at Boolean level for entailment detection. Demeester et al. (2016) propose lifted regularization for knowledge base completion to improve the logical loss functions to be independent of the number of grounded instances and to further extend to unseen constants, The basic idea is that hypernyms have ordering relations and such relations correspond to component-wise comparison in semantic vector space. Hu et al. (2016) introduce a teacher-student model, where the teacher model is a rule-regularized neural networ"
2020.emnlp-main.320,D19-1405,0,0.0184536,"h first-order logic and natural language. Neural networks have the merits of convenient end-to-end training and good generalization, however, they typically need a lot of training data and are not interpretable. On the other hand, logicbased expert systems are interpretable and require less or no training data. It is appealing to leverage the advantages from both worlds. In NLP community, the injection of logic to neural network can be generally divided into two groups. Methods in the first group regularize neural network with logicdriven loss functions (Xu et al., 2018; Fischer et al., 2019; Li et al., 2019). For example, Rockt¨aschel et al. (2015) target on the problem of knowledge base completion. After extracting and annotating propositional logical rules about relations in knowledge graph, they ground these rules to facts from knowledge graph and add a differentiable training loss function. Kruszewski et al. (2015) map text to Boolean representations, and derive loss functions based on implication at Boolean level for entailment detection. Demeester et al. (2016) propose lifted regularization for knowledge base completion to improve the logical loss functions to be independent of the number o"
2020.emnlp-main.320,P19-1028,0,0.154388,"ugh creating new examples when positive causal relationship between two cause-effect questions holds. The second group is to incorporate logic-specific modules into the inference process (Yang et al., 2017; Dong et al., 2019). For example, Rockt¨aschel and Riedel (2017) target at the problem of knowledge base completion, and use neural unification modules to recursively construct model similar to the backward chaining algorithm of Prolog. Evans and Grefenstette (2018) develop a differentiable model of forward chaining inference, where weights represent a probability distribution over clauses. Li and Srikumar (2019) inject logic-driven neurons to existing neural networks by measuring the degree of the head being true measured by probabilistic soft logic (Kimmig et al., 2012). Our approach belongs to the first direction, and to the best of knowledge our work is the first one that augments neural network with logical knowledge for propaganda detection. 6 Conclusion In this paper, we propose a fine-grained multitask learning approach, which leverages declarative knowledge to detect propaganda techniques in news articles. Specifically, the declarative knowledge is expressed in both first-order logic and natu"
2020.emnlp-main.320,D17-1317,0,0.0842897,"fluence an audience. 4. Slogan: A brief and striking phrase that may include labeling and stereotyping. Figure 1: Examples of propagandistic texts, and definitions of corresponding propaganda techniques (Bold denotes propagandistic texts). Introduction ∗ “To all Americans tonight, in all our cities and towns, I make this promise: We Will Make America Strong Again. We Will Make America Proud Again. We Will Make America Safe Again. And We Will Make America Great Again 3, 4.” thanks to the recent release of Propaganda Techniques Corpus (Da San Martino et al., 2019). Different from earlier works (Rashkin et al., 2017; Wang, 2017) that mainly study propaganda detection at a coarse-grained level, namely predicting whether a document is propagandistic or not, the fine-grained propaganda detection requires to identify the tokens of particular propaganda techniques in news articles. Da San Martino et al. (2019) propose strong baselines in a multi-task learning manner, which are trained by binary detection of propaganda at sentence level and fine-grained propaganda detection over 18 techniques at token level. Such data-driven methods have the merits of convenient end-to-end learning and strong generalization, h"
2020.emnlp-main.320,N15-1118,0,0.421682,"Missing"
2020.emnlp-main.320,N18-1074,0,0.0442108,"njection of first-order logic into neural networks. We will describe related studies in these two directions. Fake news detection draws growing attention as the spread of misinformation on social media becomes easier and leads to stronger influence. Various types of fake news detection problems are intro3901 duced. For example, there are 4-way classification of news documents (Rashkin et al., 2017), and 6way classification of short statements (Wang, 2017). There are also sentence-level fact checking problems with various genres of evidence, including natural language sentences from Wikipedia (Thorne et al., 2018), semi-structured tables (Chen et al., 2020), and images (Zlatkova et al., 2019; Nakamura et al., 2019). Our work studies propaganda detection, a fine-grained problem that requires tokenlevel prediction over 18 fine-grained propaganda techniques. The release of a large manually annotated dataset (Da San Martino et al., 2019) makes the development of large neural models possible, and also triggers our work, which improves a standard multi-task learning approach by augmenting declarative knowledge expressed in both first-order logic and natural language. Neural networks have the merits of conven"
2020.emnlp-main.320,D18-1215,0,0.0256681,"tions based on implication at Boolean level for entailment detection. Demeester et al. (2016) propose lifted regularization for knowledge base completion to improve the logical loss functions to be independent of the number of grounded instances and to further extend to unseen constants, The basic idea is that hypernyms have ordering relations and such relations correspond to component-wise comparison in semantic vector space. Hu et al. (2016) introduce a teacher-student model, where the teacher model is a rule-regularized neural network, whose predictions are used to teach the student model. Wang and Poon (2018) generalize virtual evidence (Pearl, 2014) to arbitrary potential functions over inputs and outputs, and use deep probabilistic logic to integrate indirection supervision into neural networks. More recently, Asai and Hajishirzi (2020) regularize question answering systems with symmetric consistency and symmetric consistency. The former creates a symmetric question by replacing words with their antonyms in comparison question, while the latter is for causal reasoning questions through creating new examples when positive causal relationship between two cause-effect questions holds. The second gr"
2020.emnlp-main.320,P17-2067,0,0.193758,". Slogan: A brief and striking phrase that may include labeling and stereotyping. Figure 1: Examples of propagandistic texts, and definitions of corresponding propaganda techniques (Bold denotes propagandistic texts). Introduction ∗ “To all Americans tonight, in all our cities and towns, I make this promise: We Will Make America Strong Again. We Will Make America Proud Again. We Will Make America Safe Again. And We Will Make America Great Again 3, 4.” thanks to the recent release of Propaganda Techniques Corpus (Da San Martino et al., 2019). Different from earlier works (Rashkin et al., 2017; Wang, 2017) that mainly study propaganda detection at a coarse-grained level, namely predicting whether a document is propagandistic or not, the fine-grained propaganda detection requires to identify the tokens of particular propaganda techniques in news articles. Da San Martino et al. (2019) propose strong baselines in a multi-task learning manner, which are trained by binary detection of propaganda at sentence level and fine-grained propaganda detection over 18 techniques at token level. Such data-driven methods have the merits of convenient end-to-end learning and strong generalization, however, they"
2020.emnlp-main.320,D19-1216,0,0.0257362,"tudies in these two directions. Fake news detection draws growing attention as the spread of misinformation on social media becomes easier and leads to stronger influence. Various types of fake news detection problems are intro3901 duced. For example, there are 4-way classification of news documents (Rashkin et al., 2017), and 6way classification of short statements (Wang, 2017). There are also sentence-level fact checking problems with various genres of evidence, including natural language sentences from Wikipedia (Thorne et al., 2018), semi-structured tables (Chen et al., 2020), and images (Zlatkova et al., 2019; Nakamura et al., 2019). Our work studies propaganda detection, a fine-grained problem that requires tokenlevel prediction over 18 fine-grained propaganda techniques. The release of a large manually annotated dataset (Da San Martino et al., 2019) makes the development of large neural models possible, and also triggers our work, which improves a standard multi-task learning approach by augmenting declarative knowledge expressed in both first-order logic and natural language. Neural networks have the merits of convenient end-to-end training and good generalization, however, they typically need"
2020.emnlp-main.320,P16-1228,0,\N,Missing
2020.emnlp-main.579,D19-1241,0,0.550869,"Missing"
2020.emnlp-main.579,Q18-1012,0,0.0587451,"football and generates “N2” based on the nearest node “N4”. Our proposed method can capture long-distance features and therefore generate correct results. 4 Related Work Solving math word problems has long been a challenging task (Fletcher, 1985; Bakman, 2007; Roy and Roth, 2016) and has attracted the attention of many researchers. Some methods on math word problem solving incorporate extra features by manually crafting fine-grained templates or defining math concepts. Huang et al. (2017) formulated finegrained templates and aligned numbers in math word problems to those candidate templates. Roy and Roth (2018) developed declarative rules to transform math concepts into expressions. These methods require manually formulated features and may be difficult to apply to math word problems in different domains. Recently, many studies have used deep learning methods to incorporate external knowledge from the knowledge base into many NLP tasks, such as dialogue systems (Zhong et al., 2019) and reading comprehension (Wang and Jiang, 2019; Qiu et al., 2019). These methods extend knowledge triples into natural language sequences or build multi-hop inference graphs based on relationships in the knowledge base,"
2020.emnlp-main.579,D17-1084,0,0.070361,"wo nodes [-, N4] of the expression tree. GTS does not realize that the current subexpression tree indicates the price of each football and generates “N2” based on the nearest node “N4”. Our proposed method can capture long-distance features and therefore generate correct results. 4 Related Work Solving math word problems has long been a challenging task (Fletcher, 1985; Bakman, 2007; Roy and Roth, 2016) and has attracted the attention of many researchers. Some methods on math word problem solving incorporate extra features by manually crafting fine-grained templates or defining math concepts. Huang et al. (2017) formulated finegrained templates and aligned numbers in math word problems to those candidate templates. Roy and Roth (2018) developed declarative rules to transform math concepts into expressions. These methods require manually formulated features and may be difficult to apply to math word problems in different domains. Recently, many studies have used deep learning methods to incorporate external knowledge from the knowledge base into many NLP tasks, such as dialogue systems (Zhong et al., 2019) and reading comprehension (Wang and Jiang, 2019; Qiu et al., 2019). These methods extend knowled"
2020.emnlp-main.579,P19-1219,0,0.0221675,"fine-grained templates or defining math concepts. Huang et al. (2017) formulated finegrained templates and aligned numbers in math word problems to those candidate templates. Roy and Roth (2018) developed declarative rules to transform math concepts into expressions. These methods require manually formulated features and may be difficult to apply to math word problems in different domains. Recently, many studies have used deep learning methods to incorporate external knowledge from the knowledge base into many NLP tasks, such as dialogue systems (Zhong et al., 2019) and reading comprehension (Wang and Jiang, 2019; Qiu et al., 2019). These methods extend knowledge triples into natural language sequences or build multi-hop inference graphs based on relationships in the knowledge base, and have achieved promising results. In this paper, we model the entities in the problem and their categories as entity graphs and use graph attention to generate knowledge-aware problem representations. Seq2Seq neural networks (Sutskever et al., 2014) have achieved promising results on math word problem solving. For instance, Wang et al. (2017) used a Seq2Seq model to generate math expressions. Wang et al. (2018b) incorpo"
2020.emnlp-main.579,D18-1132,0,0.463877,"nsion (Wang and Jiang, 2019; Qiu et al., 2019). These methods extend knowledge triples into natural language sequences or build multi-hop inference graphs based on relationships in the knowledge base, and have achieved promising results. In this paper, we model the entities in the problem and their categories as entity graphs and use graph attention to generate knowledge-aware problem representations. Seq2Seq neural networks (Sutskever et al., 2014) have achieved promising results on math word problem solving. For instance, Wang et al. (2017) used a Seq2Seq model to generate math expressions. Wang et al. (2018b) incorporated reinforcement learning into the model to construct a math expression step by step. Zou and Lu (2019) used a data-driven approach to semantically parsing text into math expressions. Recently, tree-structured decoder was used to further improve the seq2seq framework. Xie and Sun (2019) propose a seq2tree model to generate expression tree in a goal-driven manner based on the parent node and left sibling tree of each node. Liu et al. (2019) propose a tree-structured decoding method with an auxiliary stack that generates the abstract syntax tree of the equation in a top-down manner."
2020.emnlp-main.579,D17-1088,0,0.302848,"ee, a model can capture information between long-distance nodes. Math word problem solving has attracted increasing attention, and many math word problem solving systems have been developed. Early statistical learning methods (Feigenbaum et al., 1963; Fletcher, 1985; Bakman, 2007; Roy and Roth, 2016) extracted templates or features from problems and generated corresponding math expressions based on these templates or features. These methods require a large number of manually formulated features or can only be applied to small application problems in small areas. In recent years, many methods (Wang et al., 2017, 2018b; Xie and Sun, 2019) have been developed that apply neural networks to analyze math word problems, with Corresponding author. orange Expression tree: 1 step / 4 Introduction ∗ fruit promising results. These methods use end-to-end models to directly generate the corresponding math expressions from the problem text. Although previous methods have achieved promising results, several problems remain that need to be addressed: 1) Background knowledge and common sense should be incorporated. For example, as shown in Figure 1, both apples and oranges are fruit. Humans are naturally aware of th"
2020.emnlp-main.579,D19-1016,0,0.0616293,"Missing"
2020.emnlp-main.579,D19-1536,0,0.0798034,"es or build multi-hop inference graphs based on relationships in the knowledge base, and have achieved promising results. In this paper, we model the entities in the problem and their categories as entity graphs and use graph attention to generate knowledge-aware problem representations. Seq2Seq neural networks (Sutskever et al., 2014) have achieved promising results on math word problem solving. For instance, Wang et al. (2017) used a Seq2Seq model to generate math expressions. Wang et al. (2018b) incorporated reinforcement learning into the model to construct a math expression step by step. Zou and Lu (2019) used a data-driven approach to semantically parsing text into math expressions. Recently, tree-structured decoder was used to further improve the seq2seq framework. Xie and Sun (2019) propose a seq2tree model to generate expression tree in a goal-driven manner based on the parent node and left sibling tree of each node. Liu et al. (2019) propose a tree-structured decoding method with an auxiliary stack that generates the abstract syntax tree of the equation in a top-down manner. In this paper, we generated the pre-order math expression tree based on parent node state of each node and recursiv"
2020.emnlp-main.729,N10-1086,0,0.532672,"ted ones (GTQs). Phrases underlined are the answers to ground-truth questions. (b) Knowledge graph constructed based on the input text shown in top sub-figure. Two colored ellipsoid are two query paths related to two ground truth questions in sub-figure (a) respectively. Nodes in green are covered by ground-truth questions. Figure 1: A sample paragraph from SQuAD with machine generated questions (Zhou et al., 2017) (a), ground truth questions (a) and corresponding knowledge graph (b). Introduction Question Generation (QG) from text aims to automatically construct questions from textual input (Heilman and Smith, 2010). It receives increasing attentions from research communities recently, due to its broad applications in scenarios of dialogue system and educational reading comprehension (Piwek et al., 2007; Duan et al., 2017). It can also help to augment the question set to enhance the performance of question answering systems. ∗ Corresponding author Our code is available at https://github.com/ WangsyGit/PathQG. 1 Current QG systems mainly follow the sequenceto-sequence structure with an encoder for modeling the textual input and a decoder for text generation (Du et al., 2017). These neural-based models hav"
2020.emnlp-main.729,W05-0909,0,0.0144766,"eration. - PathQG is our proposed generation framework consisting of a query representation learner and a query-based question generator. PathQG-V is the variational version of PathQG with an additional posterior query learner. - NQG++ is an oracle model that is aware of all path information contained in the target question and encode them via BIO scheme. It can be treated as the upper bound of NQG+ (pl). We present this result for reference. 5.4 Automatic Evaluation Results For the automatic evaluation, we utlize some widely adopted metrics including BLEU 1-4 (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005) and ROUGEL (Lin and Hovy, 2003). Besides, we also compare results in the semantic content level by using a metric named SPICE (Anderson et al., 2016). It evaluates the similarity of scene graphs generated from candidate and reference questions. Evaluation results on both whole and complex datasets are shown in the Table 2 and 3. We have several findings: - PathQG-V outperforms other models in terms of all metrics on both datasets by a considerable margin. This indicates the effectiveness of our 9071 variational inference framework for modeling the query path for better question generation. -"
2020.emnlp-main.729,N18-1165,0,0.0203742,"tuitively, we divide the task of question generation from a query path into two steps, namely, query representation learning and query-based question generation. We formulate the former step as a sequence labeling problem for identifying the involved facts to form a query. For query-based question generation, an RNN-based generator is used to generate the question word by word. We first train the two modules jointly in an end-to-end fashion (PathQG in Section 3). In order to further enforce the interaction between theses two modules, we employ a variational framework to train the two modules (Chen et al., 2018; Zhang et al., 2018) that treats query representation learning as an inference process from the query path taking the generated question as the target (PathQG-V in Section 4). For model evaluation, we build the experimental environment on top of the benchmark dataset SQuAD (Rajpurkar et al., 2016). In specific, we automatically construct the KG for each piece of input text, and pair ground-truth questions with corresponding query paths from the KG. Experimental results show that our generation model outperforms other state-of-the-art QG models, especially when the questions are more complicat"
2020.emnlp-main.729,D19-1317,0,0.214464,"et al., 2018a,b), has attracted increasing attention in recent years. Most previous studies on textual question generation are rule-based and transform a declarative sentence into an interrogative sentence according to hand-crafted patterns (Heilman and Smith, 2010; Heilman, 2011). With the advance of neural network, Du et al. (2017) propose to apply a seq2seq structure with attention for automatic question generation. As follow-up, Zhou et al. (2017); Sun et al. (2018); Kim et al. (2019) propose to utilize the answers to decrease the generation uncertainty. Meanwhile, Song et al. (2018) and Li et al. (2019) explore to use answer-relevant context to guide question generation. Besides, some studies (Wang et al., 2017; Tang et al., 2017; Wang et al., 2019) take question generation as a subtask, and jointly learn it with other tasks, such as question answering and phrase extraction, which also help to alleviate the uncertainty and improve the generation performance. Another stream of research for question generation is from KG to question. Reddy et al. (2017); Elsahar et al. (2018) explore to generate questions from a single KG triple using text as context information. It is close to our setting, bu"
2020.emnlp-main.729,P17-1123,0,0.422642,"ns from textual input (Heilman and Smith, 2010). It receives increasing attentions from research communities recently, due to its broad applications in scenarios of dialogue system and educational reading comprehension (Piwek et al., 2007; Duan et al., 2017). It can also help to augment the question set to enhance the performance of question answering systems. ∗ Corresponding author Our code is available at https://github.com/ WangsyGit/PathQG. 1 Current QG systems mainly follow the sequenceto-sequence structure with an encoder for modeling the textual input and a decoder for text generation (Du et al., 2017). These neural-based models have shown promising performance, however, they suffer from generating irrelevant and uninformative questions. Figure 1a presents two sample questions generated by a nueral QG model. Q2 contains irrelevant information “Everton Fc”. Although Q1 is correct, it is a safe play without mentioning any 9066 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 9066–9075, c November 16–20, 2020. 2020 Association for Computational Linguistics specific information in the input text. One possible reason causing the problem is that curren"
2020.emnlp-main.729,N03-1020,0,0.165225,"Missing"
2020.emnlp-main.729,D17-1090,0,0.0396691,"ground truth questions in sub-figure (a) respectively. Nodes in green are covered by ground-truth questions. Figure 1: A sample paragraph from SQuAD with machine generated questions (Zhou et al., 2017) (a), ground truth questions (a) and corresponding knowledge graph (b). Introduction Question Generation (QG) from text aims to automatically construct questions from textual input (Heilman and Smith, 2010). It receives increasing attentions from research communities recently, due to its broad applications in scenarios of dialogue system and educational reading comprehension (Piwek et al., 2007; Duan et al., 2017). It can also help to augment the question set to enhance the performance of question answering systems. ∗ Corresponding author Our code is available at https://github.com/ WangsyGit/PathQG. 1 Current QG systems mainly follow the sequenceto-sequence structure with an encoder for modeling the textual input and a decoder for text generation (Du et al., 2017). These neural-based models have shown promising performance, however, they suffer from generating irrelevant and uninformative questions. Figure 1a presents two sample questions generated by a nueral QG model. Q2 contains irrelevant informat"
2020.emnlp-main.729,N18-1020,0,0.0182037,"; Kim et al. (2019) propose to utilize the answers to decrease the generation uncertainty. Meanwhile, Song et al. (2018) and Li et al. (2019) explore to use answer-relevant context to guide question generation. Besides, some studies (Wang et al., 2017; Tang et al., 2017; Wang et al., 2019) take question generation as a subtask, and jointly learn it with other tasks, such as question answering and phrase extraction, which also help to alleviate the uncertainty and improve the generation performance. Another stream of research for question generation is from KG to question. Reddy et al. (2017); Elsahar et al. (2018) explore to generate questions from a single KG triple using text as context information. It is close to our setting, but we are different in two aspects. First, we propose to form a query path consisting of multiple triples for question generation instead of a single triple. Second, the context we process is where the extracted triples from. This setting is more natural and different from using retrieved text as context as they did. 7 Conclusion and Future Work In this paper, we propose to model facts in the input text as knowledge graph for question generation. We present a novel task of gen"
2020.emnlp-main.729,C18-1150,1,0.710092,"o cases of input texts, paths, answers and questions generated by human, NQG+ and PathQG-V. Phrases underlined are irrelevant to the input text. model PathQG-V is more informative and specific, which consists of information “plymouth” and “late 18th”. In sample 2, our generated question is consistent to the input text while the one from NQG+ contains irrelevant phrase “swazi economy”. 6 Related Work Question Generation, aiming at generating questions from a range of inputs, such as raw text (Heilman and Smith, 2010), structured data (Serban et al., 2016) and images (Mostafazadeh et al., 2016; Fan et al., 2018a,b), has attracted increasing attention in recent years. Most previous studies on textual question generation are rule-based and transform a declarative sentence into an interrogative sentence according to hand-crafted patterns (Heilman and Smith, 2010; Heilman, 2011). With the advance of neural network, Du et al. (2017) propose to apply a seq2seq structure with attention for automatic question generation. As follow-up, Zhou et al. (2017); Sun et al. (2018); Kim et al. (2019) propose to utilize the answers to decrease the generation uncertainty. Meanwhile, Song et al. (2018) and Li et al. (20"
2020.emnlp-main.729,P16-1170,0,0.0988213,"estion by 9072 Figure 6: Two cases of input texts, paths, answers and questions generated by human, NQG+ and PathQG-V. Phrases underlined are irrelevant to the input text. model PathQG-V is more informative and specific, which consists of information “plymouth” and “late 18th”. In sample 2, our generated question is consistent to the input text while the one from NQG+ contains irrelevant phrase “swazi economy”. 6 Related Work Question Generation, aiming at generating questions from a range of inputs, such as raw text (Heilman and Smith, 2010), structured data (Serban et al., 2016) and images (Mostafazadeh et al., 2016; Fan et al., 2018a,b), has attracted increasing attention in recent years. Most previous studies on textual question generation are rule-based and transform a declarative sentence into an interrogative sentence according to hand-crafted patterns (Heilman and Smith, 2010; Heilman, 2011). With the advance of neural network, Du et al. (2017) propose to apply a seq2seq structure with attention for automatic question generation. As follow-up, Zhou et al. (2017); Sun et al. (2018); Kim et al. (2019) propose to utilize the answers to decrease the generation uncertainty. Meanwhile, Song et al. (2018)"
2020.emnlp-main.729,P02-1040,0,0.109663,"sing BIO scheme for question generation. - PathQG is our proposed generation framework consisting of a query representation learner and a query-based question generator. PathQG-V is the variational version of PathQG with an additional posterior query learner. - NQG++ is an oracle model that is aware of all path information contained in the target question and encode them via BIO scheme. It can be treated as the upper bound of NQG+ (pl). We present this result for reference. 5.4 Automatic Evaluation Results For the automatic evaluation, we utlize some widely adopted metrics including BLEU 1-4 (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005) and ROUGEL (Lin and Hovy, 2003). Besides, we also compare results in the semantic content level by using a metric named SPICE (Anderson et al., 2016). It evaluates the similarity of scene graphs generated from candidate and reference questions. Evaluation results on both whole and complex datasets are shown in the Table 2 and 3. We have several findings: - PathQG-V outperforms other models in terms of all metrics on both datasets by a considerable margin. This indicates the effectiveness of our 9071 variational inference framework for modeling the query path"
2020.emnlp-main.729,D14-1162,0,0.0838358,"esponding query paths from the KG for ground truth questions. In practice, a path can be determined by a start node and an end node. We thus use answer entity of the question as the start node and use the entity identified in the question as the end node. If the question contains multiple entities, we take the one farthest to the start node in the KG as the end node. We ignore the edge directions to simplify the modeling of query path. 5.2 Implementation Details We construct different vocabularies for input texts and questions respectively by keeping words which appear more than twice. Glove (Pennington et al., 2014) is used to initialize word embedding with dimension 300 and the embedding for BIO tag is randomly initialized of size 20. The size of hidden units in LSTM cell in all encoders is 300 while the size of the generation decoder is 1200. The hyperparameters to balance weights of losses are chosen as λ = 0.5 and β = 0.1. We evaluate our model on validation set to choose parameters. During test 2 The constructed KGs and complex question index can be downloaded from https://www.disc.fudan.edu. cn/data/fudan_pathqg_data.zip. 9070 Model NQG+ AFPA ASs2s NQG+ (pl) PathQG PathQG-V NQG++ BLEU 1 49.89 50.05"
2020.emnlp-main.729,D16-1264,0,0.0492678,"stion generation, an RNN-based generator is used to generate the question word by word. We first train the two modules jointly in an end-to-end fashion (PathQG in Section 3). In order to further enforce the interaction between theses two modules, we employ a variational framework to train the two modules (Chen et al., 2018; Zhang et al., 2018) that treats query representation learning as an inference process from the query path taking the generated question as the target (PathQG-V in Section 4). For model evaluation, we build the experimental environment on top of the benchmark dataset SQuAD (Rajpurkar et al., 2016). In specific, we automatically construct the KG for each piece of input text, and pair ground-truth questions with corresponding query paths from the KG. Experimental results show that our generation model outperforms other state-of-the-art QG models, especially when the questions are more complicated. Human evaluation also proves the effectiveness of our model in terms of both relevance and informativeness. 2 Task Definition We first introduce some notations in our task: - x = (x1 , ..., xn ): an input text with n tokens, where xi is the ith token; - G: a knowledge graph constructed from x,"
2020.emnlp-main.729,E17-1036,0,0.0170504,"7); Sun et al. (2018); Kim et al. (2019) propose to utilize the answers to decrease the generation uncertainty. Meanwhile, Song et al. (2018) and Li et al. (2019) explore to use answer-relevant context to guide question generation. Besides, some studies (Wang et al., 2017; Tang et al., 2017; Wang et al., 2019) take question generation as a subtask, and jointly learn it with other tasks, such as question answering and phrase extraction, which also help to alleviate the uncertainty and improve the generation performance. Another stream of research for question generation is from KG to question. Reddy et al. (2017); Elsahar et al. (2018) explore to generate questions from a single KG triple using text as context information. It is close to our setting, but we are different in two aspects. First, we propose to form a query path consisting of multiple triples for question generation instead of a single triple. Second, the context we process is where the extracted triples from. This setting is more natural and different from using retrieved text as context as they did. 7 Conclusion and Future Work In this paper, we propose to model facts in the input text as knowledge graph for question generation. We pres"
2020.emnlp-main.729,W15-2812,0,0.0136549,"tructed by other methods. Experiments Experimental Dataset Our experiments are conducted on SQuAD (Rajpurkar et al., 2016) consisting of 61,623 sentences. Each sentence is annotated with several questions together with their answers extracted from the input text. We build our experimental dataset on top of SQuAD. We construct knowledge graph for each sentence automatically and identify query paths for ground truth questions for evaluation. The resulted dataset consists of 89,976 tuples (input sentence x, query path s, ground truth question y). KG construction We employ the scene graph parser (Schuster et al., 2015) for KG construction from a textual description. It identifies entities and their relationships from a text and build a scene graph. It turns out that the generated scene graph usually misses some key information in the text, thus we employ the part-of-speech tagger to extract verb phrases between entities to further enrich relationship labels. The extended scene graph is used as the knowledge graph for the input text. The average quantities of entities and facts in each KG are dataset train valid test complex whole 12,828 68,704 1,895 10,313 1,855 10,959 len. of ques. 14.7 13.3 Table 1: Stati"
2020.emnlp-main.729,P16-1056,0,0.040077,"Missing"
2020.emnlp-main.729,N18-2090,0,0.09269,"zadeh et al., 2016; Fan et al., 2018a,b), has attracted increasing attention in recent years. Most previous studies on textual question generation are rule-based and transform a declarative sentence into an interrogative sentence according to hand-crafted patterns (Heilman and Smith, 2010; Heilman, 2011). With the advance of neural network, Du et al. (2017) propose to apply a seq2seq structure with attention for automatic question generation. As follow-up, Zhou et al. (2017); Sun et al. (2018); Kim et al. (2019) propose to utilize the answers to decrease the generation uncertainty. Meanwhile, Song et al. (2018) and Li et al. (2019) explore to use answer-relevant context to guide question generation. Besides, some studies (Wang et al., 2017; Tang et al., 2017; Wang et al., 2019) take question generation as a subtask, and jointly learn it with other tasks, such as question answering and phrase extraction, which also help to alleviate the uncertainty and improve the generation performance. Another stream of research for question generation is from KG to question. Reddy et al. (2017); Elsahar et al. (2018) explore to generate questions from a single KG triple using text as context information. It is clo"
2020.findings-emnlp.260,P15-1168,1,0.954802,"se and has an excellent transfer capability. Experiments on eight datasets with different criteria show that our model outperforms our single-criterion baseline model and other multi-criteria models. Source codes of this paper are available on Github1 . 1 Chinese word segmentation (CWS) is a preliminary step to process Chinese text. The mainstream CWS methods regard CWS as a character-based sequence labeling problem, in which each character is assigned a label to indicate its boundary information. Recently, various neural models have been explored to reduce efforts of the feature engineering (Chen et al., 2015a,b; Qun et al., 2020; Wang and Xu, 2017; Kurita et al., 2017; Ma et al., 2018). Recently, Chen et al. (2017) proposed multicriteria Chinese word segmentation (MCCWS) to effectively utilize the heterogeneous resources with different segmentation criteria. Specifically, they regard each segmentation criterion as a single ∗ Corresponding author. https://github.com/acphile/MCCWS won 赢得 赢得 赢得 the championship 总冠军 总 冠军 总 冠军 Table 1: Illustration of different segmentation criteria. Introduction 1 Lin Dan 林丹 林 丹 林丹 task under the framework of multi-task learning, where a shared layer is used to extra"
2020.findings-emnlp.260,D15-1141,1,0.916429,"Missing"
2020.findings-emnlp.260,P17-1110,1,0.894397,"our model outperforms our single-criterion baseline model and other multi-criteria models. Source codes of this paper are available on Github1 . 1 Chinese word segmentation (CWS) is a preliminary step to process Chinese text. The mainstream CWS methods regard CWS as a character-based sequence labeling problem, in which each character is assigned a label to indicate its boundary information. Recently, various neural models have been explored to reduce efforts of the feature engineering (Chen et al., 2015a,b; Qun et al., 2020; Wang and Xu, 2017; Kurita et al., 2017; Ma et al., 2018). Recently, Chen et al. (2017) proposed multicriteria Chinese word segmentation (MCCWS) to effectively utilize the heterogeneous resources with different segmentation criteria. Specifically, they regard each segmentation criterion as a single ∗ Corresponding author. https://github.com/acphile/MCCWS won 赢得 赢得 赢得 the championship 总冠军 总 冠军 总 冠军 Table 1: Illustration of different segmentation criteria. Introduction 1 Lin Dan 林丹 林 丹 林丹 task under the framework of multi-task learning, where a shared layer is used to extract the criteriainvariant features, and a private layer is used to extract the criteria-specific features. How"
2020.findings-emnlp.260,I05-3017,0,0.72965,"Missing"
2020.findings-emnlp.260,2020.coling-main.186,0,0.217287,"Missing"
2020.findings-emnlp.260,I08-4010,0,0.288022,"Missing"
2020.findings-emnlp.260,P17-1111,0,0.0281055,"eight datasets with different criteria show that our model outperforms our single-criterion baseline model and other multi-criteria models. Source codes of this paper are available on Github1 . 1 Chinese word segmentation (CWS) is a preliminary step to process Chinese text. The mainstream CWS methods regard CWS as a character-based sequence labeling problem, in which each character is assigned a label to indicate its boundary information. Recently, various neural models have been explored to reduce efforts of the feature engineering (Chen et al., 2015a,b; Qun et al., 2020; Wang and Xu, 2017; Kurita et al., 2017; Ma et al., 2018). Recently, Chen et al. (2017) proposed multicriteria Chinese word segmentation (MCCWS) to effectively utilize the heterogeneous resources with different segmentation criteria. Specifically, they regard each segmentation criterion as a single ∗ Corresponding author. https://github.com/acphile/MCCWS won 赢得 赢得 赢得 the championship 总冠军 总 冠军 总 冠军 Table 1: Illustration of different segmentation criteria. Introduction 1 Lin Dan 林丹 林 丹 林丹 task under the framework of multi-task learning, where a shared layer is used to extract the criteriainvariant features, and a private layer is use"
2020.findings-emnlp.260,N15-1142,0,0.0708094,"Missing"
2020.findings-emnlp.260,D18-1529,0,0.582053,"Missing"
2020.findings-emnlp.260,I17-1018,0,0.0637148,"he only difference for each criterion is that a unique token is taken as input to specify the target criterion, which makes the shared encoder to capture the criterion-aware representation. Figure 2 illustrates the difference between our proposed model and the previous models. A more detailed architecture for MCCWS is shown in Figure 3. 3.1 y3 y4 CRF/MLP ˜0 h ˜1 h ˜2 h ˜3 h ˜4 h Transformer Encoder Encoder h0 h1 h2 h3 h4 m x1 x2 x3 x4 Embedding Input Figure 3: Proposed Model for MCCWS. between different criteria in the latent embedding space. 2) Bigram Embedding: Based on (Chen et al., 2015b; Shao et al., 2017; Zhang et al., 2018), the character-level bigram features can significantly benefit the task of CWS. Following their settings, we also introduce the bigram embedding to augment the character-level unigram embedding. The representation of character xt is e0xt = F C(ext ⊕ ext−1 xt ⊕ ext xt+1 ), (13) where e denotes the d-dimensional embedding vector for the unigram and bigram, ⊕ is the concatenation operator, and FC is a fully connected layer to map the concatenated character embedding with the dimension 3d into the embedding e0xt ∈ Rdmodel . 3) Position Embedding: To capture the order informat"
2020.findings-emnlp.260,I17-1017,0,0.0912977,"ity. Experiments on eight datasets with different criteria show that our model outperforms our single-criterion baseline model and other multi-criteria models. Source codes of this paper are available on Github1 . 1 Chinese word segmentation (CWS) is a preliminary step to process Chinese text. The mainstream CWS methods regard CWS as a character-based sequence labeling problem, in which each character is assigned a label to indicate its boundary information. Recently, various neural models have been explored to reduce efforts of the feature engineering (Chen et al., 2015a,b; Qun et al., 2020; Wang and Xu, 2017; Kurita et al., 2017; Ma et al., 2018). Recently, Chen et al. (2017) proposed multicriteria Chinese word segmentation (MCCWS) to effectively utilize the heterogeneous resources with different segmentation criteria. Specifically, they regard each segmentation criterion as a single ∗ Corresponding author. https://github.com/acphile/MCCWS won 赢得 赢得 赢得 the championship 总冠军 总 冠军 总 冠军 Table 1: Illustration of different segmentation criteria. Introduction 1 Lin Dan 林丹 林 丹 林丹 task under the framework of multi-task learning, where a shared layer is used to extract the criteriainvariant features, and a"
2020.findings-emnlp.260,P16-2092,0,0.0189251,"sent to conditional random fields (CRF) (Lafferty et al., 2001) layer or multi-layer perceptron (MLP) for tag inference. When using CRF as decoding layer, p(Y |X) in Eq (1) could be formalized as: Ψ(Y |X) , 0 0 Y ∈Ln Ψ(Y |X) p(Y |X) = P Ψ(Y |X) = ψ(X, t, yt−1 , yt ), (4) (5) where by0 y ∈ R is trainable parameters respective to label pair (y 0 , y), score function δ(X, t) ∈ R|L| calculates scores of each label for tagging the t-th character: δ(X, t) = Wδ> ht + bδ , ∀t ∈ [1, T ] (7) where θd denotes all the parameters in MLP layer. Most current state-of-the-art CWS models (Chen et al., 2015a; Xu and Sun, 2016; Liu et al., 2016; Yang et al., 2018; Qun et al., 2020) mainly focus on single-criterion CWS (SCCWS). Figure 2a shows the architecture of SCCWS. 2.2 MCCWS with Multi-Task Learning To improve the performance of CWS by exploiting multiple heterogeneous criteria corpora, Chen et al. (2017) utilize the multi-task learning framework to model the shared information among these different criteria. Formally, assuming that there are M corpora with heterogeneous segmentation criteria, we refer Dm as corpus m with Nm samples: m Dm = {(Xn(m) , Yn(m) )}N n=1 , (m) t=2 ψ(x, t, y 0 , y) = exp(δ(X, t)y + by0"
2020.findings-emnlp.265,W17-0401,0,0.0145704,"es is the word orders in source and target languages might be different (e.g., some languages are prepositional and some are postpositional). Various studies have been dedicated to addressing this issue (Naseem et al., 2012; Zhang and Barzilay, 2015; Wang and Eisner, 2017). In particular, some studies proposed to bypass word order issue by selecting source languages that have similar word orders to the target language ˇ (Naseem et al., 2012; Rosa and Zabokrtsk` y, 2015). Good source languages can be selected by measuring the similarity of POS sequences between the source and target languages (Agic, 2017), querying the information stored in topological databases (Deri and Knight, 2016), and formalizing such selection as a ranking problem (Lin et al., 2019). Treebank translation (Tiedemann et al., 2014; Tiedemann and Agi´c, 2016) tackles this problem by transforming an annotated source treebank to instances with target language grammar through machine translation. However, this method may suffer from imperfect word alignment between two languages. Zhang et al. (2019) proposed to perform such syntactic transfer by code mixing in which only the confident words in a source treebank will be transfo"
2020.findings-emnlp.265,N19-1253,1,0.882618,"Missing"
2020.findings-emnlp.265,Q17-1010,0,0.0139043,", and RNN-Stack. These models are built upon two encoders (SelfAtt/RNN) as well as two decoders (Graph/Stack). RNN encoder uses bidirectional LSTMs while SelfAtt encoder uses a transformer (Vaswani et al., 2017) instead. Graph decoder utilizes a deep biaffine attentional scorer proposed by Dozat and Manning (2017), and Stack decoder is a top-down transition-based decoder proposed by Ma et al. (2018). Lexicalized Features Following (Ahmad et al., 2019), all the parsing models take words as well as their gold POS tags as input. We also leverage pre-trained multilingual embeddings from FastText (Bojanowski et al., 2017) that project the word embeddings from different languages into the same space using an offline transformation method (Smith et al., 2017; Conneau et al., 2018). Training Details For fair comparison, we use the same hyper-parameter settings and the training strategy as Ahmad et al. (2019) to train the parsing models. Each POS-based language model for word reordering is trained on the training set of a corresponding target language, in which the POS tag dimension is set to 50 (as the same as that in the parsing models), the hidden size h ∈ {50, 100} and the number of layers l ∈ {1, 2, 3} are tu"
2020.findings-emnlp.265,W06-2920,0,0.0446153,"Missing"
2020.findings-emnlp.265,D11-1005,0,0.0460979,"Missing"
2020.findings-emnlp.265,D12-1001,0,0.0245424,"a. T¨ackstr¨om et al. (2013) trained a parser on multiple source languages instead of a single one. Ponti et al. (2018) proposed a typologically driven method to reduce anisomorphism. Ahmad et al. (2019) designed an order-free model to extract the order features from the source language. Meng et al. (2019) embraced the linguistic knowledge of target languages to guide the inference. Some researchers also exploit lexical features to enhance the parsing models. Cross-lingual word clusters (T¨ackstr¨om et al., 2012), word embeddings (Guo et al., 2015, 2016; Ammar et al., 2016), and dictionaries (Durrett et al., 2012; Rasooli and Collins, 2017) are used as the features to better transfer linguistic knowledge among different languages. Our work is in line with a recently proposed solution, namely treebank reordering (Wang and Eisner, 2016, 2018; Rasooli and Collins, 2019), which aims to rearrange the word order in source sentences to make them more similar to the target one. Wang and Eisner (2018) proposed to permute the constituents of an existing dependency treebank to make its surface POS statistics approximately match those of the target language. However, they used POS bigrams to measure the surface c"
2020.findings-emnlp.265,P09-1042,0,0.0417624,"ection as a ranking problem (Lin et al., 2019). Treebank translation (Tiedemann et al., 2014; Tiedemann and Agi´c, 2016) tackles this problem by transforming an annotated source treebank to instances with target language grammar through machine translation. However, this method may suffer from imperfect word alignment between two languages. Zhang et al. (2019) proposed to perform such syntactic transfer by code mixing in which only the confident words in a source treebank will be transformed. Another interesting solution to cross-lingual transfer is an annotation projection (Hwa et al., 2005; Ganchev et al., 2009; Ma and Xia, 2014). In this approach, source-side sentences of a parallel corpus are parsed by the parser trained on the source treebank, then the source dependencies are projected onto the target sentences using the results of word alignments. However, the resulting treebank could be highly noisy because the source dependency trees are constructed automatically and cannot be taken as ground truth. Lacroix et al. (2016) considered removing not well-aligned sentences to obtain high-quality data. T¨ackstr¨om et al. (2013) trained a parser on multiple source languages instead of a single one. Po"
2020.findings-emnlp.265,P15-1119,0,0.0139134,"removing not well-aligned sentences to obtain high-quality data. T¨ackstr¨om et al. (2013) trained a parser on multiple source languages instead of a single one. Ponti et al. (2018) proposed a typologically driven method to reduce anisomorphism. Ahmad et al. (2019) designed an order-free model to extract the order features from the source language. Meng et al. (2019) embraced the linguistic knowledge of target languages to guide the inference. Some researchers also exploit lexical features to enhance the parsing models. Cross-lingual word clusters (T¨ackstr¨om et al., 2012), word embeddings (Guo et al., 2015, 2016; Ammar et al., 2016), and dictionaries (Durrett et al., 2012; Rasooli and Collins, 2017) are used as the features to better transfer linguistic knowledge among different languages. Our work is in line with a recently proposed solution, namely treebank reordering (Wang and Eisner, 2016, 2018; Rasooli and Collins, 2019), which aims to rearrange the word order in source sentences to make them more similar to the target one. Wang and Eisner (2018) proposed to permute the constituents of an existing dependency treebank to make its surface POS statistics approximately match those of the targe"
2020.findings-emnlp.265,K17-1024,0,0.0467333,"Missing"
2020.findings-emnlp.265,D17-1302,0,0.0257262,"008; Kiperwasser and Goldberg, 2016). However, the performance of dependency parsers heavily relies on the size of corpus. Due to the great cost and difficulty of acquiring sufficient training data, ML-based methods cannot be trivially applied to low-resource languages. Cross-lingual transfer is a promising approach to tackle the lack of sufficient data. The idea is to train a cross-lingual model that transfers knowledge learned in one or multiple high-resource source languages to target ones. This approach has been successfully applied in various tasks, including partof-speech (POS) tagging (Kim et al., 2017), dependency parsing (McDonald et al., 2011), named entity recognition (Xie et al., 2018), entity linking (Sil et al., 2018), question answering (Joty et al., 2017), and coreference resolution (Kundu et al., 2018). A key challenge for cross-lingual parsing is the difficulty to handle word order difference between source and target languages, which often causes a significant drop in performance (Rasooli and Collins, 2017; Ahmad et al., 2019). Inspired by the idea that POS sequences often reflect the syntactic structure of a language, we propose CURSOR (Cross lingUal paRSing by wOrd Reordering)"
2020.findings-emnlp.265,P16-1038,0,0.0142968,"(e.g., some languages are prepositional and some are postpositional). Various studies have been dedicated to addressing this issue (Naseem et al., 2012; Zhang and Barzilay, 2015; Wang and Eisner, 2017). In particular, some studies proposed to bypass word order issue by selecting source languages that have similar word orders to the target language ˇ (Naseem et al., 2012; Rosa and Zabokrtsk` y, 2015). Good source languages can be selected by measuring the similarity of POS sequences between the source and target languages (Agic, 2017), querying the information stored in topological databases (Deri and Knight, 2016), and formalizing such selection as a ranking problem (Lin et al., 2019). Treebank translation (Tiedemann et al., 2014; Tiedemann and Agi´c, 2016) tackles this problem by transforming an annotated source treebank to instances with target language grammar through machine translation. However, this method may suffer from imperfect word alignment between two languages. Zhang et al. (2019) proposed to perform such syntactic transfer by code mixing in which only the confident words in a source treebank will be transformed. Another interesting solution to cross-lingual transfer is an annotation proj"
2020.findings-emnlp.265,Q16-1023,0,0.0382115,"Missing"
2020.findings-emnlp.265,P18-2063,0,0.0241037,"hods cannot be trivially applied to low-resource languages. Cross-lingual transfer is a promising approach to tackle the lack of sufficient data. The idea is to train a cross-lingual model that transfers knowledge learned in one or multiple high-resource source languages to target ones. This approach has been successfully applied in various tasks, including partof-speech (POS) tagging (Kim et al., 2017), dependency parsing (McDonald et al., 2011), named entity recognition (Xie et al., 2018), entity linking (Sil et al., 2018), question answering (Joty et al., 2017), and coreference resolution (Kundu et al., 2018). A key challenge for cross-lingual parsing is the difficulty to handle word order difference between source and target languages, which often causes a significant drop in performance (Rasooli and Collins, 2017; Ahmad et al., 2019). Inspired by the idea that POS sequences often reflect the syntactic structure of a language, we propose CURSOR (Cross lingUal paRSing by wOrd Reordering) to overcome the word order difference issue in crosslingual transfer. Specifically, we assume we have a treebank in the source language and annotated POS corpus in the target language1 . We first train a POS-based"
2020.findings-emnlp.265,N16-1121,0,0.0152,"n which only the confident words in a source treebank will be transformed. Another interesting solution to cross-lingual transfer is an annotation projection (Hwa et al., 2005; Ganchev et al., 2009; Ma and Xia, 2014). In this approach, source-side sentences of a parallel corpus are parsed by the parser trained on the source treebank, then the source dependencies are projected onto the target sentences using the results of word alignments. However, the resulting treebank could be highly noisy because the source dependency trees are constructed automatically and cannot be taken as ground truth. Lacroix et al. (2016) considered removing not well-aligned sentences to obtain high-quality data. T¨ackstr¨om et al. (2013) trained a parser on multiple source languages instead of a single one. Ponti et al. (2018) proposed a typologically driven method to reduce anisomorphism. Ahmad et al. (2019) designed an order-free model to extract the order features from the source language. Meng et al. (2019) embraced the linguistic knowledge of target languages to guide the inference. Some researchers also exploit lexical features to enhance the parsing models. Cross-lingual word clusters (T¨ackstr¨om et al., 2012), word e"
2020.findings-emnlp.265,P19-1301,0,0.014848,"s studies have been dedicated to addressing this issue (Naseem et al., 2012; Zhang and Barzilay, 2015; Wang and Eisner, 2017). In particular, some studies proposed to bypass word order issue by selecting source languages that have similar word orders to the target language ˇ (Naseem et al., 2012; Rosa and Zabokrtsk` y, 2015). Good source languages can be selected by measuring the similarity of POS sequences between the source and target languages (Agic, 2017), querying the information stored in topological databases (Deri and Knight, 2016), and formalizing such selection as a ranking problem (Lin et al., 2019). Treebank translation (Tiedemann et al., 2014; Tiedemann and Agi´c, 2016) tackles this problem by transforming an annotated source treebank to instances with target language grammar through machine translation. However, this method may suffer from imperfect word alignment between two languages. Zhang et al. (2019) proposed to perform such syntactic transfer by code mixing in which only the confident words in a source treebank will be transformed. Another interesting solution to cross-lingual transfer is an annotation projection (Hwa et al., 2005; Ganchev et al., 2009; Ma and Xia, 2014). In th"
2020.findings-emnlp.265,P18-1130,0,0.0145791,"arameters and remaining 25 languages are held out for final evaluation. Parsing Models We evaluate CURSOR with four different parsing models described by Ahmad et al. (2019): SelfAtt-Graph, RNN-Graph, SelfAtt-Stack, and RNN-Stack. These models are built upon two encoders (SelfAtt/RNN) as well as two decoders (Graph/Stack). RNN encoder uses bidirectional LSTMs while SelfAtt encoder uses a transformer (Vaswani et al., 2017) instead. Graph decoder utilizes a deep biaffine attentional scorer proposed by Dozat and Manning (2017), and Stack decoder is a top-down transition-based decoder proposed by Ma et al. (2018). Lexicalized Features Following (Ahmad et al., 2019), all the parsing models take words as well as their gold POS tags as input. We also leverage pre-trained multilingual embeddings from FastText (Bojanowski et al., 2017) that project the word embeddings from different languages into the same space using an offline transformation method (Smith et al., 2017; Conneau et al., 2018). Training Details For fair comparison, we use the same hyper-parameter settings and the training strategy as Ahmad et al. (2019) to train the parsing models. Each POS-based language model for word reordering is traine"
2020.findings-emnlp.265,P14-1126,0,0.018518,"oblem (Lin et al., 2019). Treebank translation (Tiedemann et al., 2014; Tiedemann and Agi´c, 2016) tackles this problem by transforming an annotated source treebank to instances with target language grammar through machine translation. However, this method may suffer from imperfect word alignment between two languages. Zhang et al. (2019) proposed to perform such syntactic transfer by code mixing in which only the confident words in a source treebank will be transformed. Another interesting solution to cross-lingual transfer is an annotation projection (Hwa et al., 2005; Ganchev et al., 2009; Ma and Xia, 2014). In this approach, source-side sentences of a parallel corpus are parsed by the parser trained on the source treebank, then the source dependencies are projected onto the target sentences using the results of word alignments. However, the resulting treebank could be highly noisy because the source dependency trees are constructed automatically and cannot be taken as ground truth. Lacroix et al. (2016) considered removing not well-aligned sentences to obtain high-quality data. T¨ackstr¨om et al. (2013) trained a parser on multiple source languages instead of a single one. Ponti et al. (2018) p"
2020.findings-emnlp.265,D11-1006,0,0.0391273,"owever, the performance of dependency parsers heavily relies on the size of corpus. Due to the great cost and difficulty of acquiring sufficient training data, ML-based methods cannot be trivially applied to low-resource languages. Cross-lingual transfer is a promising approach to tackle the lack of sufficient data. The idea is to train a cross-lingual model that transfers knowledge learned in one or multiple high-resource source languages to target ones. This approach has been successfully applied in various tasks, including partof-speech (POS) tagging (Kim et al., 2017), dependency parsing (McDonald et al., 2011), named entity recognition (Xie et al., 2018), entity linking (Sil et al., 2018), question answering (Joty et al., 2017), and coreference resolution (Kundu et al., 2018). A key challenge for cross-lingual parsing is the difficulty to handle word order difference between source and target languages, which often causes a significant drop in performance (Rasooli and Collins, 2017; Ahmad et al., 2019). Inspired by the idea that POS sequences often reflect the syntactic structure of a language, we propose CURSOR (Cross lingUal paRSing by wOrd Reordering) to overcome the word order difference issue"
2020.findings-emnlp.265,D19-1103,1,0.819615,"Missing"
2020.findings-emnlp.265,P12-1066,0,0.0579965,"Missing"
2020.findings-emnlp.265,J08-4003,0,0.0174665,"Missing"
2020.findings-emnlp.265,P18-1142,0,0.030763,"Missing"
2020.findings-emnlp.265,Q17-1020,0,0.0599189,"knowledge learned in one or multiple high-resource source languages to target ones. This approach has been successfully applied in various tasks, including partof-speech (POS) tagging (Kim et al., 2017), dependency parsing (McDonald et al., 2011), named entity recognition (Xie et al., 2018), entity linking (Sil et al., 2018), question answering (Joty et al., 2017), and coreference resolution (Kundu et al., 2018). A key challenge for cross-lingual parsing is the difficulty to handle word order difference between source and target languages, which often causes a significant drop in performance (Rasooli and Collins, 2017; Ahmad et al., 2019). Inspired by the idea that POS sequences often reflect the syntactic structure of a language, we propose CURSOR (Cross lingUal paRSing by wOrd Reordering) to overcome the word order difference issue in crosslingual transfer. Specifically, we assume we have a treebank in the source language and annotated POS corpus in the target language1 . We first train a POS-based language model on a corpus in the target language. Then, we reorder words in each sentence on the source corpus based on the POSbased language model to create pseudo sentences with target word order. The resul"
2020.findings-emnlp.265,N19-1385,0,0.0438747,"Missing"
2020.findings-emnlp.265,P15-2040,0,0.0458404,"Missing"
2020.findings-emnlp.265,N13-1126,0,0.0544327,"Missing"
2020.findings-emnlp.265,N12-1052,0,0.0776263,"Missing"
2020.findings-emnlp.265,W14-1614,0,0.0394417,"Missing"
2020.findings-emnlp.265,Q16-1035,0,0.022913,"ee model to extract the order features from the source language. Meng et al. (2019) embraced the linguistic knowledge of target languages to guide the inference. Some researchers also exploit lexical features to enhance the parsing models. Cross-lingual word clusters (T¨ackstr¨om et al., 2012), word embeddings (Guo et al., 2015, 2016; Ammar et al., 2016), and dictionaries (Durrett et al., 2012; Rasooli and Collins, 2017) are used as the features to better transfer linguistic knowledge among different languages. Our work is in line with a recently proposed solution, namely treebank reordering (Wang and Eisner, 2016, 2018; Rasooli and Collins, 2019), which aims to rearrange the word order in source sentences to make them more similar to the target one. Wang and Eisner (2018) proposed to permute the constituents of an existing dependency treebank to make its surface POS statistics approximately match those of the target language. However, they used POS bigrams to measure the surface closeness between two languages, which is unable to capture global information. Rasooli and Collins (2019) proposed two different syntactic reordering methods, one is based on the dominant dependency direction in the target la"
2020.findings-emnlp.265,Q17-1011,0,0.0336967,"Missing"
2020.findings-emnlp.265,D18-1163,0,0.0493276,"Some researchers also exploit lexical features to enhance the parsing models. Cross-lingual word clusters (T¨ackstr¨om et al., 2012), word embeddings (Guo et al., 2015, 2016; Ammar et al., 2016), and dictionaries (Durrett et al., 2012; Rasooli and Collins, 2017) are used as the features to better transfer linguistic knowledge among different languages. Our work is in line with a recently proposed solution, namely treebank reordering (Wang and Eisner, 2016, 2018; Rasooli and Collins, 2019), which aims to rearrange the word order in source sentences to make them more similar to the target one. Wang and Eisner (2018) proposed to permute the constituents of an existing dependency treebank to make its surface POS statistics approximately match those of the target language. However, they used POS bigrams to measure the surface closeness between two languages, which is unable to capture global information. Rasooli and Collins (2019) proposed two different syntactic reordering methods, one is based on the dominant dependency direction in the target language, the other learns a reordering classifier, but both methods rely on parallel corpus. 2939 In this study, we explore the feasibility of utilizing a POS-base"
2020.findings-emnlp.265,D18-1034,0,0.0170288,"ily relies on the size of corpus. Due to the great cost and difficulty of acquiring sufficient training data, ML-based methods cannot be trivially applied to low-resource languages. Cross-lingual transfer is a promising approach to tackle the lack of sufficient data. The idea is to train a cross-lingual model that transfers knowledge learned in one or multiple high-resource source languages to target ones. This approach has been successfully applied in various tasks, including partof-speech (POS) tagging (Kim et al., 2017), dependency parsing (McDonald et al., 2011), named entity recognition (Xie et al., 2018), entity linking (Sil et al., 2018), question answering (Joty et al., 2017), and coreference resolution (Kundu et al., 2018). A key challenge for cross-lingual parsing is the difficulty to handle word order difference between source and target languages, which often causes a significant drop in performance (Rasooli and Collins, 2017; Ahmad et al., 2019). Inspired by the idea that POS sequences often reflect the syntactic structure of a language, we propose CURSOR (Cross lingUal paRSing by wOrd Reordering) to overcome the word order difference issue in crosslingual transfer. Specifically, we as"
2020.findings-emnlp.265,I08-3008,0,0.0247318,"Missing"
2020.findings-emnlp.265,D19-1092,0,0.0306955,"Missing"
2020.findings-emnlp.265,D15-1213,0,0.0269026,"Missing"
2020.findings-emnlp.422,D15-1106,0,0.0144959,"1) with a dimension of 300 and updated during training. The dimension of the hidden units for GRU (Chung et al., 2014) and GCN is 300. We initialize the parameters according to a uniform distribution with the Xavier scheme (Kumar, 2017), and the dropout rate is set to 0.5. The Adam (Kingma and Ba, 2014) method with a learning rate of 1e-3 is used for training. Baseline Methods. To evaluate the effectiveness of our proposed model, we apply the advanced baselines in two categories for comparison: (1) TF-IDF; (2) LexRank (Erkan and Radev, 2004); (3) Seq2Seq (Sutskever et al., 2014); (4) HRNNLM (Lin et al., 2015); (5) Transformer (Vaswani et al., 2017). The former two are extractive models which extract words from the gene text as the term name, and the latter three are generative models which generate words from the vocabulary space as the term name. 4.2 Experimental Results The experimental results are shown in Table 2. It is observed that the generative models perform better than the extractive models by incorporating the language probability into generation, which makes the generated term name more coherent. Whereas, the extractive models usually extract keywords independently, which are hard to f"
2020.findings-emnlp.422,P02-1040,0,0.121521,"Missing"
2020.findings-emnlp.422,P16-1154,0,0.0331563,"yer of GCN. 3.2 Graph Attention based Decoder Motivated by the effectiveness of the attention mechanism for generation (Bahdanau et al., 2014), we adopt a graph attention based decoder to generate the term name. The attentive word node representation by GCN is utilized and formulated as: n X at = αj wj0 (2) j=1 αj = sof tmax(v T tanh(Wa [ht−1 ; w0 j ])) where ht−1 is the previous hidden state, w0 j is the word node representation by GCN, v is a parameter vector, and Wa is a parameter matrix. Given the word overlaps between the gene text and term name, we utilize the copy mechanism in CopyNet (Gu et al., 2016) for decoding, making it possible to generate the word from either the vocabulary of the training set or the current gene text. The initial hidden state h0 is the term node representation (i.e., t0 ) obtained by GCN, and the hidden state is updated as: 0 ht = f ([ht−1 ; wt−1 ; at ; wSR ]) (3) where f is the RNN function, wt−1 is the word 0 embedding of the previous generated word, wSR is a selective read (SR) vector in CopyNet. When the previous generated word appears in the gene text, the next word will also probably come from it, and 0 thus wSR is the previous word node representation; other"
2020.findings-emnlp.60,D19-5709,0,0.0274029,"Missing"
2020.findings-emnlp.60,W03-2201,0,0.00780148,"pes. We formulate such a task as a partially supervised learning problem and accordingly propose an effective algorithm to solve the problem. Comprehensive experimental studies on several public NER datasets validate the effectiveness of our method. 1 Introduction Named Entity Recognition (NER) is a type of information extraction task that seeks to identify entity names from unstructured text and categorize them into a predefined list of types. It plays an important role in many downstream tasks such as knowledge base construction (Riedel et al., 2013; Shen et al., 2012), machine translation (Babych and Hartley, 2003), and search (Zhu et al., 2005), etc. In this field, the supervised methods, ranging from the conventional graph models (McCallum et al., 2000; Malouf, 2002; McCallum and Li, 2003; Settles, 2004) to the dominant deep neural methods (Collobert et al., 2011; Huang et al., 2015; Lample et al., 2016; Gridach, 2017; Liu et al., 2018; Zhang and Yang, 2018; Jiang et al., 2019; Gui et al., 2019), have achieved great success. However, these supervised methods usually require large scale labeled data to 678 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 678–688 c November 1"
2020.findings-emnlp.60,M98-1001,0,0.380345,"osed, like the bidirectional long short-term memory network (LSTM) plus a CRF layer (Huang et al., 2015), the convolutional neural network (CNN) plus a CRF layer, the combination of LSTM and CNN (Chiu and Nichols, 2016), and the BERT based LSTM+CRF model (Jiang et al., 2019; Hakala and Pyysalo, 2019). 4 4.1 Experiments Datasets Following the experimental setting of the most related work (Peng et al., 2019), we performed the experiments on the four public NER datasets, including Conll03 (en) in English (Tjong Kim Sang and De Meulder, 2003), CoNLL02 (sp) (Sang and Erik, 2002) in Spanish, MUC-7 (Chinchor, 1998), Twitter (Zhang et al., 2018) in English, and OntoNotes4.0 (Weischedel et al., 2011) in Chinese. For the former four datasets, we treated the location (LOC) and person (PER) types as the newly introduced entity types in the target task, and treated the rest entity types as the predefined entity types in the source task. While for OntoNotes4.0, we treated the GPE (countries, cities, states) and location (non-GPE locations, mountain ranges, bodies of water) types as the newly introduced entity types in the target task, which are all classified as the location type in the source task. Table 3 sh"
2020.findings-emnlp.60,Q16-1026,0,0.228132,"al., 2006; Gerner et al., 2010; Liu et al., 2015). They do not require annotated training data but heavily rely on background knowledge (rules) and lexicon resources. They work well when the lexicon is exhaustive, but fail when the lexicon is incomplete. Precision is generally high for these systems, but recall is often low due to incomplete lexicons. Current state-of-the-art NER systems are mainly based on annotated data and machine learning approaches. The lexicons introduced in some of these systems are mainly for extracting some external features (Liu et al., 2015; Agerri and Rigau, 2016; Chiu and Nichols, 2016). This field has been previously dominated by the graph πi ← |Dit |/|Dt |, i = 1, · · · , ns , since class i data is fully labeled in Dt . For estimating πns +j and πn0 s +j , j = 1, · · · , nt + 1, we apply an iteration strategy. In particular, we first initialized πns +j and πn0 s +j for j ≤ nt 0 by by |Dnt s +j |/|Dt |, and initialize πK and πK |Dut |/|Dt |and 1, respectively. Based on this, we train the classifier f and then re-estimate πns +j and πn0 s +j using the trained classifier as follows: πns +j ← πn0 s +j (10) i j=1 1 X f (w)[ns + j], |Dt | w∈Dt 1 X ← t f (w)[ns + j], |Du | t Labe"
2020.findings-emnlp.60,N19-1423,0,0.0638126,"the labeled data of a new entity type ej , we use its corresponding entity lexicon Lj to scan words of class K in Ds and find out some confident words of the entity type to construct labeled data Dnt s +j of class ns + j. This process applies nt times to obtain the labeled data of the nt new entity types. The rest words of class K in Ds not being selected by the lexicons form the unlabeled data set Dut ⊆ Dt in the target task, which contains 2.5 Model Architecture For a sentence s = [w1 , · · · , wl ] with l words, we first get the contextualized representations of words using the BERT model (Devlin et al., 2019): h1 , · · · , hl = BERT(w1 , · · · , wl ). 680 (1) Algorithm 1 Data Labeling using the Lexicons 1: 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: fully supervised learning loss, which is defined as follows: nX s +nt Lsup = πi Lii . (3) Input: entity lexicons Lj , for j = 1, · · · , nt with Lj ∩ Lk = ∅ if j 6= k, a word sequence s = {w1 , · · · , wn } ∈ class K in Ds , and the maximum mention length lw Result: the partially labeled dataset Dt Initialize: i ← 1 while i ≤ n do for k ∈ [lw , · · · , 0] do b ← f alse for j ∈ [1, · · · , nt ] do if {wi , · · · , wi+k } ∈ Lj then assign {wi"
2020.findings-emnlp.60,N16-1030,0,0.0181556,"ype of information extraction task that seeks to identify entity names from unstructured text and categorize them into a predefined list of types. It plays an important role in many downstream tasks such as knowledge base construction (Riedel et al., 2013; Shen et al., 2012), machine translation (Babych and Hartley, 2003), and search (Zhu et al., 2005), etc. In this field, the supervised methods, ranging from the conventional graph models (McCallum et al., 2000; Malouf, 2002; McCallum and Li, 2003; Settles, 2004) to the dominant deep neural methods (Collobert et al., 2011; Huang et al., 2015; Lample et al., 2016; Gridach, 2017; Liu et al., 2018; Zhang and Yang, 2018; Jiang et al., 2019; Gui et al., 2019), have achieved great success. However, these supervised methods usually require large scale labeled data to 678 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 678–688 c November 16 - 20, 2020. 2020 Association for Computational Linguistics Predict PER O O ORG O O JOB JOB JOB Ts , Tt ns nt ei , i ≤ ns ens +j Lj Ds , Dt Dit Dut πi πn0 s +j L ps = LPER + LORG + Lu Partially Supervised Learning Partial Label PER U U Source Label PER O O ORG U U U U JOB O O Data labeling usin"
2020.findings-emnlp.60,P05-1045,0,0.206195,"75.02 78.39 82.22 76.24 79.39 82.00 76.45 79.38 GPE LOC Overall 64.37 25.03 61.02 65.73 25.92 61.81 68.21 35.29 65.15 79.66 43.65 77.93 44.42 23.17 40.33 68.17 33.28 65.79 68.15 34.33 66.22 72.37 36.40 70.66 71.27 37.22 69.78 Table 4: Testing chunk-level F1 on the target task. The four label-based methods are fully supervised and trained on the fully re-annotated data of the source task. While the five lexicon-based methods train the model using only the existing labels of the source task and entity lexicons of the new entity types. The best performance in each group is marked in a boldface. Finkel et al., 2005), the bi-directional long shortterm memory network with the CRF layer BiLSTM+CRF or not BiLSTM (Huang et al., 2015), and the BERT based model (Devlin et al., 2019) described in the “Model architecture” section. These supervised models were trained on the fully re-annotated Ds according to the data labeling criteria of the target task. names, respectively. We refer you to the referred work for more information about the lexicons. Here, we address that it can only label a small part of the mentions of the person and location types using the lexicons. 4.3 Compared Methods In the following, we ref"
2020.findings-emnlp.60,D19-1096,1,0.887387,"Missing"
2020.findings-emnlp.60,W02-2019,0,0.0444266,"studies on several public NER datasets validate the effectiveness of our method. 1 Introduction Named Entity Recognition (NER) is a type of information extraction task that seeks to identify entity names from unstructured text and categorize them into a predefined list of types. It plays an important role in many downstream tasks such as knowledge base construction (Riedel et al., 2013; Shen et al., 2012), machine translation (Babych and Hartley, 2003), and search (Zhu et al., 2005), etc. In this field, the supervised methods, ranging from the conventional graph models (McCallum et al., 2000; Malouf, 2002; McCallum and Li, 2003; Settles, 2004) to the dominant deep neural methods (Collobert et al., 2011; Huang et al., 2015; Lample et al., 2016; Gridach, 2017; Liu et al., 2018; Zhang and Yang, 2018; Jiang et al., 2019; Gui et al., 2019), have achieved great success. However, these supervised methods usually require large scale labeled data to 678 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 678–688 c November 16 - 20, 2020. 2020 Association for Computational Linguistics Predict PER O O ORG O O JOB JOB JOB Ts , Tt ns nt ei , i ≤ ns ens +j Lj Ds , Dt Dit Dut πi πn0"
2020.findings-emnlp.60,W03-0430,0,0.22851,"eral public NER datasets validate the effectiveness of our method. 1 Introduction Named Entity Recognition (NER) is a type of information extraction task that seeks to identify entity names from unstructured text and categorize them into a predefined list of types. It plays an important role in many downstream tasks such as knowledge base construction (Riedel et al., 2013; Shen et al., 2012), machine translation (Babych and Hartley, 2003), and search (Zhu et al., 2005), etc. In this field, the supervised methods, ranging from the conventional graph models (McCallum et al., 2000; Malouf, 2002; McCallum and Li, 2003; Settles, 2004) to the dominant deep neural methods (Collobert et al., 2011; Huang et al., 2015; Lample et al., 2016; Gridach, 2017; Liu et al., 2018; Zhang and Yang, 2018; Jiang et al., 2019; Gui et al., 2019), have achieved great success. However, these supervised methods usually require large scale labeled data to 678 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 678–688 c November 16 - 20, 2020. 2020 Association for Computational Linguistics Predict PER O O ORG O O JOB JOB JOB Ts , Tt ns nt ei , i ≤ ns ens +j Lj Ds , Dt Dit Dut πi πn0 s +j L ps = LPER + LORG"
2020.findings-emnlp.60,O03-4002,0,0.0596991,"B (beginning), I (internal), or E (end), the words labeled by the lexicons belong to. 2.3 2.4 Method Overview Obtain the Partially Labeled Data using the Entity Lexicons In this section, we detail the construction of the partially labeled dataset Dt for the target task. As illustrated before, the labeled data Dit , i ≤ ns of class i can be easily obtained from Ds according to the data labeling of Ts . Thus, in the following, we focus on obtaining Dnt s +j , j = 1, · · · , nt and Dut using the entity lexicons. Following the idea of (Peng et al., 2019), we apply the maximum matching algorithm (Xue, 2003) to obtain words that match with the lexicon Lj and belong to class K in Ds to construct Dnt s +j . As summarized in algo. 1, this algorithm is a greedy search routine that walks through a sequence of class K words trying to find the longest string that matches with an entry of the lexicons. Note that in algo. 1, lw is intuitively set to 4, and the “for” loop is broken in step 12 because a mention must not occur in multiple lexicons, which is guaranteed by Lj ∩ Lk = ∅ if j 6= k. Based on the above label assignment mechanism, we train a (ns + nt )-class classifier to perform the target task, Tt"
2020.findings-emnlp.60,P18-1144,0,0.0165176,"ify entity names from unstructured text and categorize them into a predefined list of types. It plays an important role in many downstream tasks such as knowledge base construction (Riedel et al., 2013; Shen et al., 2012), machine translation (Babych and Hartley, 2003), and search (Zhu et al., 2005), etc. In this field, the supervised methods, ranging from the conventional graph models (McCallum et al., 2000; Malouf, 2002; McCallum and Li, 2003; Settles, 2004) to the dominant deep neural methods (Collobert et al., 2011; Huang et al., 2015; Lample et al., 2016; Gridach, 2017; Liu et al., 2018; Zhang and Yang, 2018; Jiang et al., 2019; Gui et al., 2019), have achieved great success. However, these supervised methods usually require large scale labeled data to 678 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 678–688 c November 16 - 20, 2020. 2020 Association for Computational Linguistics Predict PER O O ORG O O JOB JOB JOB Ts , Tt ns nt ei , i ≤ ns ens +j Lj Ds , Dt Dit Dut πi πn0 s +j L ps = LPER + LORG + Lu Partially Supervised Learning Partial Label PER U U Source Label PER O O ORG U U U U JOB O O Data labeling using the Job Title lexicon Input Bobick works at O O Googl"
2020.findings-emnlp.60,P19-1231,1,0.235364,"ity mention, which means that we cannot distinguish the type, B (beginning), I (internal), or E (end), the words labeled by the lexicons belong to. 2.3 2.4 Method Overview Obtain the Partially Labeled Data using the Entity Lexicons In this section, we detail the construction of the partially labeled dataset Dt for the target task. As illustrated before, the labeled data Dit , i ≤ ns of class i can be easily obtained from Ds according to the data labeling of Ts . Thus, in the following, we focus on obtaining Dnt s +j , j = 1, · · · , nt and Dut using the entity lexicons. Following the idea of (Peng et al., 2019), we apply the maximum matching algorithm (Xue, 2003) to obtain words that match with the lexicon Lj and belong to class K in Ds to construct Dnt s +j . As summarized in algo. 1, this algorithm is a greedy search routine that walks through a sequence of class K words trying to find the longest string that matches with an entry of the lexicons. Note that in algo. 1, lw is intuitively set to 4, and the “for” loop is broken in step 12 because a mention must not occur in multiple lexicons, which is guaranteed by Lj ∩ Lk = ∅ if j 6= k. Based on the above label assignment mechanism, we train a (ns +"
2020.findings-emnlp.60,P02-1060,0,0.530733,"Missing"
2020.findings-emnlp.60,N13-1008,0,0.0459908,"led data and entity lexicons of the newly introduced entity types. We formulate such a task as a partially supervised learning problem and accordingly propose an effective algorithm to solve the problem. Comprehensive experimental studies on several public NER datasets validate the effectiveness of our method. 1 Introduction Named Entity Recognition (NER) is a type of information extraction task that seeks to identify entity names from unstructured text and categorize them into a predefined list of types. It plays an important role in many downstream tasks such as knowledge base construction (Riedel et al., 2013; Shen et al., 2012), machine translation (Babych and Hartley, 2003), and search (Zhu et al., 2005), etc. In this field, the supervised methods, ranging from the conventional graph models (McCallum et al., 2000; Malouf, 2002; McCallum and Li, 2003; Settles, 2004) to the dominant deep neural methods (Collobert et al., 2011; Huang et al., 2015; Lample et al., 2016; Gridach, 2017; Liu et al., 2018; Zhang and Yang, 2018; Jiang et al., 2019; Gui et al., 2019), have achieved great success. However, these supervised methods usually require large scale labeled data to 678 Findings of the Association f"
2020.findings-emnlp.60,W02-2024,0,0.149758,"ious neural architectures have been proposed, like the bidirectional long short-term memory network (LSTM) plus a CRF layer (Huang et al., 2015), the convolutional neural network (CNN) plus a CRF layer, the combination of LSTM and CNN (Chiu and Nichols, 2016), and the BERT based LSTM+CRF model (Jiang et al., 2019; Hakala and Pyysalo, 2019). 4 4.1 Experiments Datasets Following the experimental setting of the most related work (Peng et al., 2019), we performed the experiments on the four public NER datasets, including Conll03 (en) in English (Tjong Kim Sang and De Meulder, 2003), CoNLL02 (sp) (Sang and Erik, 2002) in Spanish, MUC-7 (Chinchor, 1998), Twitter (Zhang et al., 2018) in English, and OntoNotes4.0 (Weischedel et al., 2011) in Chinese. For the former four datasets, we treated the location (LOC) and person (PER) types as the newly introduced entity types in the target task, and treated the rest entity types as the predefined entity types in the source task. While for OntoNotes4.0, we treated the GPE (countries, cities, states) and location (non-GPE locations, mountain ranges, bodies of water) types as the newly introduced entity types in the target task, which are all classified as the location"
2020.findings-emnlp.60,W04-1221,0,0.0830411,"s validate the effectiveness of our method. 1 Introduction Named Entity Recognition (NER) is a type of information extraction task that seeks to identify entity names from unstructured text and categorize them into a predefined list of types. It plays an important role in many downstream tasks such as knowledge base construction (Riedel et al., 2013; Shen et al., 2012), machine translation (Babych and Hartley, 2003), and search (Zhu et al., 2005), etc. In this field, the supervised methods, ranging from the conventional graph models (McCallum et al., 2000; Malouf, 2002; McCallum and Li, 2003; Settles, 2004) to the dominant deep neural methods (Collobert et al., 2011; Huang et al., 2015; Lample et al., 2016; Gridach, 2017; Liu et al., 2018; Zhang and Yang, 2018; Jiang et al., 2019; Gui et al., 2019), have achieved great success. However, these supervised methods usually require large scale labeled data to 678 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 678–688 c November 16 - 20, 2020. 2020 Association for Computational Linguistics Predict PER O O ORG O O JOB JOB JOB Ts , Tt ns nt ei , i ≤ ns ens +j Lj Ds , Dt Dit Dut πi πn0 s +j L ps = LPER + LORG + Lu Partially"
2020.findings-emnlp.60,W03-0419,0,0.385436,"Missing"
2020.tacl-1.6,P12-1110,0,0.147286,"s into a unified graph-based parsing framework. Because the segmentation is a characterlevel task and dependency parsing is a word-level task, we first formulate these two tasks into a character-level graph-based parsing framework. In detail, our model contains (1) a deep neural network encoder, which can capture the longterm contextual features for each character— it can be a multi-layer BiLSTM or pre-trained BERT, (2) a biaffine attentional scorer (Dozat and Manning, 2017), which unifies segmentation and dependency relations at the character level. Besides, unlike the previous joint models (Hatori et al., 2012; Zhang et al., 2014; Kurita et al., • As an added bonus, our proposed model can directly utilize the pre-trained language model BERT (Devlin et al., 2019) to boost performance significantly. The performance of many NLP tasks can be significantly enhanced when BERT was combined (Sun et al., 2019; Zhong et al., 2019). However, for Chinese, BERT is based on Chinese characters, whereas dependency parsing is conducted in the wordlevel. We cannot directly utilize BERT to enhance the word-level Chinese dependency parsing models. Nevertheless, by using the our proposed model, we can exploit BERT to i"
2020.tacl-1.6,P82-1020,0,0.777368,"Missing"
2020.tacl-1.6,D15-1141,1,0.780869,"ilayer BiLSTM, the second one is the pre-trained language model BERT (Devlin et al., 2019) which is based on self-attention. 3.1.1 BiLSTM-based Encoding Layer Given a character sequence X = {x1 , . . . , xN }, in neural models, the first step is to map discrete language symbols into distributed embedding space. Formally, each character xi is mapped as ei ∈ Rde ⊂ E, where de is a hyper-parameter indicating the size of character embedding, and E is the embedding matrix. Character bigrams and trigrams have been shown highly effective for CWS and POS tagging in previous studies (Pei et al., 2014; Chen et al., 2015; Shao et al., 2017; Zhang et al., 2018). Following their settings, we combine the character bigram and trigram to enhance the representation of each character. The 3.1 Encoding Layer The encoding layer is responsible for converting discrete characters into contextualized dense rep81 final character representation of xi is given by ei = exi ⊕ exi xi+1 ⊕ exi xi+1 xi+2 , where e denotes the embedding for unigram, bigram, and trigram, and ⊕ is the concatenation operator. To capture the long-term contextual information, we use a deep BiLSTM (Hochreiter and Schmidhuber, 1997) to incorporate informa"
2020.tacl-1.6,Q16-1023,0,0.0553463,"feature engineering. These transition-based joint models rely on a detailed handcrafted feature. Although Kurita et al. (2017) introduced neural models to reduce partial efforts of feature engineering, they still require hard work on how to design and compose the word-based features from the stack and the character-based features from the buffer. • In experiments on datasets CTB-5, CTB-7, and CTB-9, our model achieves state-ofthe-art score in joint CWS and dependency parsing, even without the POS information. Recently, graph-based models have made significant progress for dependency parsing (Kiperwasser and Goldberg, 2016; Dozat and Manning, 2017), which fully exploit the ability of the bidirectional long short-term memory network (BiLSTM) (Hochreiter and Schmidhuber, 1997) and attention mechanism (Bahdanau et al., 2015) to capture the interactions of words in a sentence. Different from the transition-based models, the graph-based models assign a score or probability to each possible arc and then construct a maximum spanning tree from these weighted arcs. In this paper, we propose a joint model for CWS and dependency parsing that integrates these two tasks into a unified graph-based parsing framework. Because"
2020.tacl-1.6,N19-1423,0,0.684533,"rmulate these two tasks into a character-level graph-based parsing framework. In detail, our model contains (1) a deep neural network encoder, which can capture the longterm contextual features for each character— it can be a multi-layer BiLSTM or pre-trained BERT, (2) a biaffine attentional scorer (Dozat and Manning, 2017), which unifies segmentation and dependency relations at the character level. Besides, unlike the previous joint models (Hatori et al., 2012; Zhang et al., 2014; Kurita et al., • As an added bonus, our proposed model can directly utilize the pre-trained language model BERT (Devlin et al., 2019) to boost performance significantly. The performance of many NLP tasks can be significantly enhanced when BERT was combined (Sun et al., 2019; Zhong et al., 2019). However, for Chinese, BERT is based on Chinese characters, whereas dependency parsing is conducted in the wordlevel. We cannot directly utilize BERT to enhance the word-level Chinese dependency parsing models. Nevertheless, by using the our proposed model, we can exploit BERT to implement CWS and dependency parsing jointly. 2 Related Work To reduce the problem of error propagation and improve the low-level tasks by incorporating the"
2020.tacl-1.6,P14-1028,0,0.185099,"parsing models. Nevertheless, by using the our proposed model, we can exploit BERT to implement CWS and dependency parsing jointly. 2 Related Work To reduce the problem of error propagation and improve the low-level tasks by incorporating the knowledge from the high-level tasks, many successful joint methods have been proposed to simultaneously solve related tasks, which can be categorized into three types. 2.1 Joint Segmentation and POS Tagging Because segmentation is a character-level task and POS tagging is a word-level task, an intuitive idea 79 segmentation and POS tagging. Zhang et al. (2014) expanded this work by using intra-character structures of words and found the intra-character dependencies were helpful in word segmentation and POS tagging. Zhang et al. (2015) proposed joint segmentation, POS tagging, and dependency re-ranking system. This system required a base parser to generate some candidate parsing results. Kurita et al. (2017) followed the work of Hatori et al. (2012); Zhang et al. (2014) and used the BiLSTM to extract features with n-gram character string embeddings as input. A related work is the full character-level neural dependency parser (Li et al., 2018), but i"
2020.tacl-1.6,D12-1046,0,0.0491387,"he work of Hatori et al. (2012); Zhang et al. (2014) and used the BiLSTM to extract features with n-gram character string embeddings as input. A related work is the full character-level neural dependency parser (Li et al., 2018), but it focuses on character-level parsing without considering the word segmentation and word-level POS tagging and parsing. Although a heuristic method could transform the character-level parsing results to word-level, the transform strategy is tedious and the result is also worse than other joint models. Besides, there are some joint models for constituency parsing. Qian and Liu (2012) proposed a joint inference model for word segmentation, POS tagging, and constituency parsing. However, their model did not train three tasks jointly and suffered from the decoding complexity due to the large combined search space. Wang et al. (2013) first segmented a Chinese sentence into a word lattice, and then predicted the POS tags and parsed tree based on the word lattice. A dual decomposition method was used to encourage the tagger and parser to predict agreed structures. The above methods show that syntactic parsing can provide useful feedback to word segmentation and POS tagging and"
2020.tacl-1.6,D11-1109,0,0.153924,"l. (2018) used a simple yet effective sequence-to-sequence neural model to improve the performance of the transition-based method. 2.2 Joint POS Tagging and Dependency Parsing Because the POS tagging task and dependency parsing task are word-level tasks, it is more natural to combine them into a joint model. Hatori et al. (2012) proposed a transition-based joint POS tagging and dependency parsing model and showed that the joint approach improved the accuracies of these two tasks. Yang et al. (2018) extended this model by neural models to alleviate the efforts of feature engineering. Li et al. (2011) utilized the graph-based model to jointly optimize POS tagging and dependency parsing in a unique model. They also proposed an effective POS tag pruning method that could greatly improve the decoding efficiency. By combining the lexicality and syntax into a unified framework, joining POS tagging and dependency parsing can improve both tagging and parsing performance over independent modeling significantly. 3 Proposed Model Previous joint methods are mainly based on the transition-based model, which modifies the standard ‘‘shift-reduce’’ operations by adding some extra operations, such as ‘‘ap"
2020.tacl-1.6,D13-1062,1,0.550848,"d measures of word-level F1, precision, and recall scores to evaluate word segmentation and dependency parsing (for both unlabeled and labeled scenario) tasks. We detail them in the following. tree based on the predicted character-level arc labels. The characters with continuous ‘‘app’’ are regarded as one word. And the predicted head character of the last character is viewed as this word’s head. Because the predicted arc points to a character, we regard the word that contains this head character as the head word. • F 1seg : F1 measure of CWS. This is the standard metric used in the CWS task (Qiu et al., 2013; Chen et al., 2017). • F 1udep : F1 measure of unlabeled dependency parsing. Following Hatori et al. (2012), Zhang et al. (2014, 2015), and Kurita et al. (2017), we use standard measures of word-level F1, precision, and recall score to evaluate dependency parsing. In the scenario of joint word segmentation and dependency parsing, the widely used unlabeled attachment score (UAS) is not enough to measure the performance, since the error arises from two aspects: One is caused by word segmentation and the other is due to the wrong prediction on the head word. A dependent-head pair is correct only"
2020.tacl-1.6,N15-1142,0,0.0180907,"n K¨ubler et al. (2009). The U AS , LAS equal to the value of the recall of unlabeled dependency parsing (Rudep ) and the recall of labeled dependency parsing (Rldep ), respectively. We also report these two values in our experiments. 4.3 Experimental Settings Pre-trained Embedding Based on Shao et al. (2017); Zhang et al. (2018), n-grams are of great benefit to CWS and POS tagging tasks. Thus we use unigram, bigram, and trigram embeddings for all of our character-based models. We first pre-train unigram, bigram, and trigram embeddings on the Chinese Wikipedia corpus by the method proposed in Ling et al. (2015), which improves standard Word2Vec by incorporating token order information. For a sentence with characters ‘‘abcd...’’, the unigram sequence is ‘‘a b c ...’’; the bigram sequence is ‘‘ab bc cd ...’’; and the trigram sequence is ‘‘abc bcd ...’’. For our word dependency parser, we use Tencent’s pre-trained word embeddings (Song et al., 2018). Because Tencent’s pre-trained word embedding dimension is 200, we set both pre-trained and random word embedding dimension as 200 for all of our word dependency parsing models. All pre-trained embeddings are fixed during our experiments. In addition to the"
2020.tacl-1.6,I17-1018,0,0.336536,"sub-tasks. Moreover, there is no related work on joint Chinese word segmentation and dependency parsing, without POS tagging. is to transfer both the tasks into character-level and incorporate them in a uniform framework. A popular method is to assign a cross-tag to each character (Ng and Low, 2004). The crosstag is composed of a word boundary part and a POS part, for example, ‘‘B-NN’’ refers to the first character in a word with POS tag ‘‘NN’’. Thus, the joint CWS and POS tagging can be regarded as a sequence labeling problem. Following this work, Zheng et al. (2013), Chen et al. (2017), and Shao et al. (2017) utilized neural models to alleviate the efforts of feature engineering. Another line of the joint segmentation and POS tagging method is the transition-based method (Zhang and Clark, 2008, 2010), in which the joint decoding process is regarded as a sequence of action predictions. Zhang et al. (2018) used a simple yet effective sequence-to-sequence neural model to improve the performance of the transition-based method. 2.2 Joint POS Tagging and Dependency Parsing Because the POS tagging task and dependency parsing task are word-level tasks, it is more natural to combine them into a joint model"
2020.tacl-1.6,D17-1002,0,0.0712813,"rst graph-based method to integrate CWS and dependency parsing both in the training phase and the decoding phase. The proposed model is very concise and easily implemented. • Compared with the previous transition-based joint models, our proposed model is a graphbased model, which results in fewer efforts of feature engineering. Additionally, our model can deal with the labeled dependency parsing task, which is not easy for transition-based joint models. (2) The second is the feature engineering. These transition-based joint models rely on a detailed handcrafted feature. Although Kurita et al. (2017) introduced neural models to reduce partial efforts of feature engineering, they still require hard work on how to design and compose the word-based features from the stack and the character-based features from the buffer. • In experiments on datasets CTB-5, CTB-7, and CTB-9, our model achieves state-ofthe-art score in joint CWS and dependency parsing, even without the POS information. Recently, graph-based models have made significant progress for dependency parsing (Kiperwasser and Goldberg, 2016; Dozat and Manning, 2017), which fully exploit the ability of the bidirectional long short-term"
2020.tacl-1.6,D18-1529,0,0.515377,"g is composed of a word boundary part and a POS part, for example, ‘‘B-NN’’ refers to the first character in a word with POS tag ‘‘NN’’. Thus, the joint CWS and POS tagging can be regarded as a sequence labeling problem. Following this work, Zheng et al. (2013), Chen et al. (2017), and Shao et al. (2017) utilized neural models to alleviate the efforts of feature engineering. Another line of the joint segmentation and POS tagging method is the transition-based method (Zhang and Clark, 2008, 2010), in which the joint decoding process is regarded as a sequence of action predictions. Zhang et al. (2018) used a simple yet effective sequence-to-sequence neural model to improve the performance of the transition-based method. 2.2 Joint POS Tagging and Dependency Parsing Because the POS tagging task and dependency parsing task are word-level tasks, it is more natural to combine them into a joint model. Hatori et al. (2012) proposed a transition-based joint POS tagging and dependency parsing model and showed that the joint approach improved the accuracies of these two tasks. Yang et al. (2018) extended this model by neural models to alleviate the efforts of feature engineering. Li et al. (2011) ut"
2020.tacl-1.6,N18-2028,0,0.0212763,"re of great benefit to CWS and POS tagging tasks. Thus we use unigram, bigram, and trigram embeddings for all of our character-based models. We first pre-train unigram, bigram, and trigram embeddings on the Chinese Wikipedia corpus by the method proposed in Ling et al. (2015), which improves standard Word2Vec by incorporating token order information. For a sentence with characters ‘‘abcd...’’, the unigram sequence is ‘‘a b c ...’’; the bigram sequence is ‘‘ab bc cd ...’’; and the trigram sequence is ‘‘abc bcd ...’’. For our word dependency parser, we use Tencent’s pre-trained word embeddings (Song et al., 2018). Because Tencent’s pre-trained word embedding dimension is 200, we set both pre-trained and random word embedding dimension as 200 for all of our word dependency parsing models. All pre-trained embeddings are fixed during our experiments. In addition to the fixed pre-trained embeddings, we also randomly initialize embeddings, and elementwisely add the pre-trained and random embeddings before other procedures. For a model with BERT encoding layer, we use the Chinese BERTbase released in Cui et al. (2019). 100 400 5 128 0.33 0.33 0.33 0.33 3 1 500 100 2e-3 .75t/5000 0.9 100 Table 1: Hyper-param"
2020.tacl-1.6,N19-1035,1,0.897442,"Missing"
2020.tacl-1.6,P08-1101,0,0.0380035,"rporate them in a uniform framework. A popular method is to assign a cross-tag to each character (Ng and Low, 2004). The crosstag is composed of a word boundary part and a POS part, for example, ‘‘B-NN’’ refers to the first character in a word with POS tag ‘‘NN’’. Thus, the joint CWS and POS tagging can be regarded as a sequence labeling problem. Following this work, Zheng et al. (2013), Chen et al. (2017), and Shao et al. (2017) utilized neural models to alleviate the efforts of feature engineering. Another line of the joint segmentation and POS tagging method is the transition-based method (Zhang and Clark, 2008, 2010), in which the joint decoding process is regarded as a sequence of action predictions. Zhang et al. (2018) used a simple yet effective sequence-to-sequence neural model to improve the performance of the transition-based method. 2.2 Joint POS Tagging and Dependency Parsing Because the POS tagging task and dependency parsing task are word-level tasks, it is more natural to combine them into a joint model. Hatori et al. (2012) proposed a transition-based joint POS tagging and dependency parsing model and showed that the joint approach improved the accuracies of these two tasks. Yang et al."
2020.tacl-1.6,D10-1082,0,0.0202611,"e just use Eq. (5) ∼ (8) to predict the labels: ‘‘seg’’ and ‘‘app’’. Thus, the word segmentation task is transformed into a binary classification problem. Figure 3 gives an illustration of the labeled arcs for the task of word segmentation only. 4 Experiments 4.1 Datasets We use the Penn Chinese Treebank 5.0 (CTB5),1 7.0 (CTB-7),2 and 9.0 (CTB-9)3 datasets to evaluate our models (Xue et al., 2005). For CTB-5, the training set is from sections 1∼270, 400∼931, and 1001∼1151, the development set is from section 301∼325, and the test set is from section 271∼300; this splitting was also adopted by Zhang and Clark (2010), Zhang et al. (2014), and Kurita et al. (2017). For CTB-7, we use the same split 1 https://catalog.ldc.upenn.edu/LDC2005T01. https://catalog.ldc.upenn.edu/LDC2010T07. 3 https://catalog.ldc.upenn.edu/LDC2016T13. 2 83 • F 1ldep : F1 measure of labeled dependency parsing. The only difference from F 1udep is that except for the match between the head and dependent words, the pair must have the same label as the golden dependent-head pair. The precision and recall are calculated correspondingly. Because the number of golden labeled dependent-head pairs and predicted labeled dependent-head pairs ar"
2020.tacl-1.6,I11-1035,0,0.0344541,"arc from xi (head) to xj (dependent). (arc−dep) (label−head) + W (label) (rij 3.2.1 Unlabeled Arc Prediction (arc−dep) rj (arc) sij (6) (label) To predict the relations of each character pair, we use the biaffine attention mechanism (Dozat and Manning, 2017) to score their probability on the top of encoding layers. According to Dozat and Manning (2017), biaffine attention is more effectively capable of measuring the relationship between two elementary units. = MLP(arc−head) (hi ), = MLP(label−dep) (hj ), = ri rij (arc−head) (5) (label) 3.2 Biaffine Layer ri = MLP(label−head) (hi ), (4) 82 as Wang et al. (2011), Zhang et al. (2014), and Kurita et al. (2017). For CTB-9, we use the dev and test files proposed by Shao et al. (2017), and we regard all left files as the training data. 4.2 Measures Figure 3: Label prediction for word segmentation only. The arc with ‘‘app’’ indicates its connected characters belong to a word, and the arc with ‘‘seg’’ indicates its connected characters belong to different words. Following Hatori et al. (2012), Zhang et al. (2014, 2015), and Kurita et al. (2017), we use standard measures of word-level F1, precision, and recall scores to evaluate word segmentation and depende"
2020.tacl-1.6,N15-1005,0,0.220987,"blem of error propagation and improve the low-level tasks by incorporating the knowledge from the high-level tasks, many successful joint methods have been proposed to simultaneously solve related tasks, which can be categorized into three types. 2.1 Joint Segmentation and POS Tagging Because segmentation is a character-level task and POS tagging is a word-level task, an intuitive idea 79 segmentation and POS tagging. Zhang et al. (2014) expanded this work by using intra-character structures of words and found the intra-character dependencies were helpful in word segmentation and POS tagging. Zhang et al. (2015) proposed joint segmentation, POS tagging, and dependency re-ranking system. This system required a base parser to generate some candidate parsing results. Kurita et al. (2017) followed the work of Hatori et al. (2012); Zhang et al. (2014) and used the BiLSTM to extract features with n-gram character string embeddings as input. A related work is the full character-level neural dependency parser (Li et al., 2018), but it focuses on character-level parsing without considering the word segmentation and word-level POS tagging and parsing. Although a heuristic method could transform the character-l"
2020.tacl-1.6,P13-2110,0,0.0822174,"acter-level parsing without considering the word segmentation and word-level POS tagging and parsing. Although a heuristic method could transform the character-level parsing results to word-level, the transform strategy is tedious and the result is also worse than other joint models. Besides, there are some joint models for constituency parsing. Qian and Liu (2012) proposed a joint inference model for word segmentation, POS tagging, and constituency parsing. However, their model did not train three tasks jointly and suffered from the decoding complexity due to the large combined search space. Wang et al. (2013) first segmented a Chinese sentence into a word lattice, and then predicted the POS tags and parsed tree based on the word lattice. A dual decomposition method was used to encourage the tagger and parser to predict agreed structures. The above methods show that syntactic parsing can provide useful feedback to word segmentation and POS tagging and the joint inference leads to improvements in all three sub-tasks. Moreover, there is no related work on joint Chinese word segmentation and dependency parsing, without POS tagging. is to transfer both the tasks into character-level and incorporate the"
2020.tacl-1.6,D13-1061,0,0.0358508,"inference leads to improvements in all three sub-tasks. Moreover, there is no related work on joint Chinese word segmentation and dependency parsing, without POS tagging. is to transfer both the tasks into character-level and incorporate them in a uniform framework. A popular method is to assign a cross-tag to each character (Ng and Low, 2004). The crosstag is composed of a word boundary part and a POS part, for example, ‘‘B-NN’’ refers to the first character in a word with POS tag ‘‘NN’’. Thus, the joint CWS and POS tagging can be regarded as a sequence labeling problem. Following this work, Zheng et al. (2013), Chen et al. (2017), and Shao et al. (2017) utilized neural models to alleviate the efforts of feature engineering. Another line of the joint segmentation and POS tagging method is the transition-based method (Zhang and Clark, 2008, 2010), in which the joint decoding process is regarded as a sequence of action predictions. Zhang et al. (2018) used a simple yet effective sequence-to-sequence neural model to improve the performance of the transition-based method. 2.2 Joint POS Tagging and Dependency Parsing Because the POS tagging task and dependency parsing task are word-level tasks, it is mor"
2020.tacl-1.6,P19-1100,1,0.82266,"e longterm contextual features for each character— it can be a multi-layer BiLSTM or pre-trained BERT, (2) a biaffine attentional scorer (Dozat and Manning, 2017), which unifies segmentation and dependency relations at the character level. Besides, unlike the previous joint models (Hatori et al., 2012; Zhang et al., 2014; Kurita et al., • As an added bonus, our proposed model can directly utilize the pre-trained language model BERT (Devlin et al., 2019) to boost performance significantly. The performance of many NLP tasks can be significantly enhanced when BERT was combined (Sun et al., 2019; Zhong et al., 2019). However, for Chinese, BERT is based on Chinese characters, whereas dependency parsing is conducted in the wordlevel. We cannot directly utilize BERT to enhance the word-level Chinese dependency parsing models. Nevertheless, by using the our proposed model, we can exploit BERT to implement CWS and dependency parsing jointly. 2 Related Work To reduce the problem of error propagation and improve the low-level tasks by incorporating the knowledge from the high-level tasks, many successful joint methods have been proposed to simultaneously solve related tasks, which can be categorized into three"
2021.acl-demo.12,D15-1141,1,0.825404,"model, users can utilize user lexicon and finetuning to enhance the performance of fastHan. As for user lexicon, users can call the add user dict function to add their lexicon, and call the set user dict weight function to change the weight coefficient. As for fine-tuning, users can call the f inetune function to load the formatted data, make fine-tuning, and save the model parameters. Evaluation We evaluate fastHan in terms of accuracy, transferability, and execution speed. 4.1 Accuracy Test The accuracy test is performed on the test set of training data. We refer to the CWS corpora used by (Chen et al., 2015; Huang et al., 2019), including PKU, MSR, AS, CITYU (Emerson, 2005), CTB-6 (Xue et al., 2005), SXU (Jin and Chen, 2008), UD, CNC, WTB (Wang et al., 2014) and ZX (Zhang et al., 2014). More details can be found in (Huang et al., 2019). For POS tagging and dependency parsing, we use the Penn Chinese Treebank 9.0 (CTB-9) (Xue et al., 2005). For NER, we use MSRA’s NER dataset and OntoNotes. We conduct an additional set of experiments to make the base version of fastHan trained on each task separately. The final results are shown in Table 1. Both base and large models perform satisfactorily. The re"
2021.acl-demo.12,P17-1110,1,0.834923,"or Chinese NLP Zhichao Geng, Hang Yan, Xipeng Qiu∗, Xuanjing Huang School of Computer Science, Fudan University Key Laboratory of Intelligent Information Processing, Fudan University {zcgeng20,hyan19,xpqiu,xjhuang}@fudan.edu.cn Abstract the tasks. Tools developed for a single task cannot achieve the highest accuracy, and loading tools for each task will take up more memory. In practical, there is a strong correlation between these four basic Chinese NLP tasks. For example, the model will perform better in the other three word-level tasks if its word segmentation ability is stronger. Recently, Chen et al. (2017a) adopt cross-label to label the POS so that POS tagging and CWS can be trained jointly. Yan et al. (2020) propose a graph-based model for joint CWS and dependency parsing, in which a special ”APP” dependency arc is used to indicate the word segmentation information. Thus, they can jointly train the word-level dependency parsing task and character-level CWS task with the biaffine parser (Dozat and Manning, 2016). Chen et al. (2017b) explore adversarial multi-criteria learning for CWS, proving more knowledge can be mined through training model on more corpora. As a result, there are many piece"
2021.acl-demo.12,P81-1022,0,0.589619,"Missing"
2021.acl-demo.12,I05-3017,0,0.0654681,"formance of fastHan. As for user lexicon, users can call the add user dict function to add their lexicon, and call the set user dict weight function to change the weight coefficient. As for fine-tuning, users can call the f inetune function to load the formatted data, make fine-tuning, and save the model parameters. Evaluation We evaluate fastHan in terms of accuracy, transferability, and execution speed. 4.1 Accuracy Test The accuracy test is performed on the test set of training data. We refer to the CWS corpora used by (Chen et al., 2015; Huang et al., 2019), including PKU, MSR, AS, CITYU (Emerson, 2005), CTB-6 (Xue et al., 2005), SXU (Jin and Chen, 2008), UD, CNC, WTB (Wang et al., 2014) and ZX (Zhang et al., 2014). More details can be found in (Huang et al., 2019). For POS tagging and dependency parsing, we use the Penn Chinese Treebank 9.0 (CTB-9) (Xue et al., 2005). For NER, we use MSRA’s NER dataset and OntoNotes. We conduct an additional set of experiments to make the base version of fastHan trained on each task separately. The final results are shown in Table 1. Both base and large models perform satisfactorily. The result shows that multi-task learning greatly improves fastHan’s perfo"
2021.acl-demo.12,2020.coling-main.186,0,0.044383,"Missing"
2021.acl-demo.12,I08-4010,0,0.0620125,"can call the add user dict function to add their lexicon, and call the set user dict weight function to change the weight coefficient. As for fine-tuning, users can call the f inetune function to load the formatted data, make fine-tuning, and save the model parameters. Evaluation We evaluate fastHan in terms of accuracy, transferability, and execution speed. 4.1 Accuracy Test The accuracy test is performed on the test set of training data. We refer to the CWS corpora used by (Chen et al., 2015; Huang et al., 2019), including PKU, MSR, AS, CITYU (Emerson, 2005), CTB-6 (Xue et al., 2005), SXU (Jin and Chen, 2008), UD, CNC, WTB (Wang et al., 2014) and ZX (Zhang et al., 2014). More details can be found in (Huang et al., 2019). For POS tagging and dependency parsing, we use the Penn Chinese Treebank 9.0 (CTB-9) (Xue et al., 2005). For NER, we use MSRA’s NER dataset and OntoNotes. We conduct an additional set of experiments to make the base version of fastHan trained on each task separately. The final results are shown in Table 1. Both base and large models perform satisfactorily. The result shows that multi-task learning greatly improves fastHan’s performance on all tasks. The large version of fastHan ou"
2021.acl-demo.12,2021.naacl-main.436,1,0.741398,"Missing"
2021.acl-demo.12,2020.acl-main.611,1,0.811281,"otes F SOTA models 97.1 85.66, 81.71 93.15 96.09 81.82 fastHan base trained separately fastHan base trained jointly fastHan large trained jointly 97.15 97.27 97.41 80.2, 75.12 81.22, 76.71 85.52, 81.38 94.27 94.88 95.66 92.2 94.33 95.50 80.3 82.86 83.82 Table 1: The results of fastHan’s accuracy result. The score of CWS is the average of 10 corpora. When training dependency parsing separately, the biaffine parser use the same architecture as Yan et al. (2020). SOTA models are best-performing work we know for each task. They came from Huang et al. (2019), Yan et al. (2020), Meng et al. (2019), Li et al. (2020) in order. Li et al. (2020) uses lexicon to enhance the model. user lexicon to enhance fastHan’s performance in specific domains. 4.2 much smaller model (262MB versus 492MB). The result proves fastHan is robust to new samples, and the fine-tuning feature allows fastHan to better be adapted to new criteria. Transferability Test Segmentation Tool jieba SnowNLP THULAC LTP-4.0 fastHan fastHan(fine-tuned) 4.3 Weibo Test Set 83.58 79.65 86.65 92.05 93.38 96.64 Speed Test Models fastHan base fastHan large Table 2: Transfer test for fastHan, using span F metric. We use the test set of Weibo, which has"
2021.acl-demo.12,D14-1122,0,0.0199705,"n to add their lexicon, and call the set user dict weight function to change the weight coefficient. As for fine-tuning, users can call the f inetune function to load the formatted data, make fine-tuning, and save the model parameters. Evaluation We evaluate fastHan in terms of accuracy, transferability, and execution speed. 4.1 Accuracy Test The accuracy test is performed on the test set of training data. We refer to the CWS corpora used by (Chen et al., 2015; Huang et al., 2019), including PKU, MSR, AS, CITYU (Emerson, 2005), CTB-6 (Xue et al., 2005), SXU (Jin and Chen, 2008), UD, CNC, WTB (Wang et al., 2014) and ZX (Zhang et al., 2014). More details can be found in (Huang et al., 2019). For POS tagging and dependency parsing, we use the Penn Chinese Treebank 9.0 (CTB-9) (Xue et al., 2005). For NER, we use MSRA’s NER dataset and OntoNotes. We conduct an additional set of experiments to make the base version of fastHan trained on each task separately. The final results are shown in Table 1. Both base and large models perform satisfactorily. The result shows that multi-task learning greatly improves fastHan’s performance on all tasks. The large version of fastHan outperforms the current best model i"
2021.acl-demo.12,C96-1035,0,0.325315,"5 and decreases linearly to 0. In the second part, We only fine-tune the base model and don’t replace the modules anymore. 2.5 User Lexicon In actual applications, users may process text of specific domains, such as technology, medical. There are proprietary vocabularies with high recall rates in such domains, and they rarely appear in ordinary corpus. It is intuitive to use a user lexicon to address this problem. Users can choose whether to add or use their lexicon. An example of combining a user lexicon is shown in Figure 3. When combined with a user lexicon, the maximum matching algorithm (Wong and Chan, 1996) is first performed to obtain a label sequence. After that, a bias will be added to the corresponding scores output by the encoder. And the result will be viewed as f1 (X, yt ) in CRF in section 2.2. The bias is (2) fastHan FastHan is a Chinese NLP toolkit based on the above model, developed based on fastNLP2 and PyTorch. We made a short video demonstrating fastHan and uploaded it to YouTube3 and bilibili4 . FastHan has been released on PYPI and users can install it by pip: pip install fastHan 3.1 Workflow When FastHan initializes, it first loads the pretrained model parameters from the file s"
2021.acl-demo.12,2020.emnlp-main.633,0,0.135254,"tween ∗ 1 Corresponding author https://github.com/fastnlp/fastHan 99 Proceedings of the Joint Conference of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations, pages 99–106, August 1st - August 6th, 2021. ©2021 Association for Computational Linguistics joint-model can reduce the occupied memory space by four times. FastHan has two versions of the backbone model, base and large. The large model uses the first eight layers of BERT, and the base model uses the Theseus strategy (Xu et al., 2020) to compress the large model to four layers. To improve the performance of the model, fastHan has done much optimization. For example, using the output of POS tagging to improve the performance of the dependency parsing task, using Theseus strategy to improve the performance of the base version model, and so on. Overall, fastHan has the following advantages: Figure 1: Architecture of the proposed model. The inputs are characters embeddings. Small size: The total parameter of the base model is 151MB, and for the large model the number is 262MB. 2 High accuracy: The base version of the model ach"
2021.acl-demo.12,2020.tacl-1.6,1,0.938575,"sity Key Laboratory of Intelligent Information Processing, Fudan University {zcgeng20,hyan19,xpqiu,xjhuang}@fudan.edu.cn Abstract the tasks. Tools developed for a single task cannot achieve the highest accuracy, and loading tools for each task will take up more memory. In practical, there is a strong correlation between these four basic Chinese NLP tasks. For example, the model will perform better in the other three word-level tasks if its word segmentation ability is stronger. Recently, Chen et al. (2017a) adopt cross-label to label the POS so that POS tagging and CWS can be trained jointly. Yan et al. (2020) propose a graph-based model for joint CWS and dependency parsing, in which a special ”APP” dependency arc is used to indicate the word segmentation information. Thus, they can jointly train the word-level dependency parsing task and character-level CWS task with the biaffine parser (Dozat and Manning, 2016). Chen et al. (2017b) explore adversarial multi-criteria learning for CWS, proving more knowledge can be mined through training model on more corpora. As a result, there are many pieces of research on how to perform multi-corpus training on these tasks and how to conduct multi-task joint tr"
2021.acl-demo.12,E14-1062,0,0.0235998,"call the set user dict weight function to change the weight coefficient. As for fine-tuning, users can call the f inetune function to load the formatted data, make fine-tuning, and save the model parameters. Evaluation We evaluate fastHan in terms of accuracy, transferability, and execution speed. 4.1 Accuracy Test The accuracy test is performed on the test set of training data. We refer to the CWS corpora used by (Chen et al., 2015; Huang et al., 2019), including PKU, MSR, AS, CITYU (Emerson, 2005), CTB-6 (Xue et al., 2005), SXU (Jin and Chen, 2008), UD, CNC, WTB (Wang et al., 2014) and ZX (Zhang et al., 2014). More details can be found in (Huang et al., 2019). For POS tagging and dependency parsing, we use the Penn Chinese Treebank 9.0 (CTB-9) (Xue et al., 2005). For NER, we use MSRA’s NER dataset and OntoNotes. We conduct an additional set of experiments to make the base version of fastHan trained on each task separately. The final results are shown in Table 1. Both base and large models perform satisfactorily. The result shows that multi-task learning greatly improves fastHan’s performance on all tasks. The large version of fastHan outperforms the current best model in CWS and POS. Although fast"
2021.acl-demo.41,D18-2029,0,0.0379272,"Missing"
2021.acl-demo.41,N19-1423,0,0.207492,"re rated on a 1-5 scale (5 denotes the best). in a general way, TextFlint supports generating massive and comprehensive transformed samples with just one command. By default, TextFlint performs all single transformations on the original dataset to form the corresponding transformed datasets, and the performance of the target models is tested on these datasets. The evaluation report provides a comparative view of model performance on datasets before and after certain types of transformations, which supports model weakness analyses and guides particular improvements. For example, take BERT base(Devlin et al., 2019) as the target model to verify its robustness on the CONLL2003 dataset(Tjong Kim Sang and De Meulder, 2003), whose robustness report is shown in Figure 5. The performance of BERT base decreases significantly in some morphology transformations, such as OCR, Keyboard, Typos, and Spelling Error. To combat these errors of input texts and improve the robustness of the model, we suggest that placing a word correction model(Pruthi et al., 2019) before BERT would be beneficial. 0.8 Case 2: Customized Evaluation For users who want to test model performance on specific aspects, they demand a customized"
2021.acl-demo.41,D18-1380,0,0.107351,"Missing"
2021.acl-demo.41,2021.naacl-demos.6,0,0.061782,"Missing"
2021.acl-demo.41,D14-1181,0,0.00437317,"Missing"
2021.acl-demo.41,2020.emnlp-main.500,1,0.864395,"like to teach kids in the kindergarten. The storm destroyed many houses in the village. ✘ Figure 1: Examples of three main generation functions. The transformation example is from ABSA (Aspectbased Sentiment Analysis) task, where the italic bold RevTgt (short for reverse target) denotes task-specific transformations, and the bold Typos denotes universal transformation. Introduction The detection of model robustness has been attracting increasing attention in recent years, given that deep neural networks (DNNs) of high accuracy can still be vulnerable to carefully crafted adversarial examples (Li et al., 2020), distribution shift (Miller et al., 2020), data transformation (Xing et al., 2020), and shortcut learning (Geirhos et al., 2020). Existing approaches to textual robustness evaluation focus on slightly modifying the input data, which maintains the original meaning and results in a different prediction. However, these methods often concentrate on either universal or task-specific generalization capabilities, which is difficult to comprehensively evaluate. In response to the shortcomings of recent works, we introduce TextFlint, a unified, multilingual, and analyzable robustness evaluation toolki"
2021.acl-demo.41,P18-1087,0,0.0272118,"Missing"
2021.acl-demo.41,P02-1040,0,0.116092,"are implemented based on TextAttack (Morris et al., 2020). Validator It is crucial to verify the quality of the samples generated by Transformation and AttackRecipe. TextFlint provides several metrics to evaluate the quality of the generated text, including (1) language model perplexity calculated based on the GPT2 model (Radford et al., 2019), (2) word replacement ratio in generated text compared with its original text, (3) edit distance between original text and generated text, (4) semantic 349 similarity calculated based on Universal Sentence Encoder (Cer et al., 2018), and (5) BLEU score (Papineni et al., 2002). 2.3 Reporter Layer Generation Layer yields three types of adversarial samples and verifies the robustness of the target model. Based on the evaluation results from Generation Layer, Report Layer aims to provide users with a standard analysis report from syntax, morphology, pragmatics, and paradigmatic relation aspects. The running process of Report Layer can be regarded as a pipeline from Analyzer to ReportGenerator. 3 Figure 4: Screenshot of TextFlint’s web interface running Ocr transformation for ABSA task. Usage Using TextFlint to verify the robustness of a specific model is as simple as"
2021.acl-demo.41,P19-1561,0,0.0287018,"Missing"
2021.acl-demo.41,2020.acl-main.442,0,0.114738,"methods equipped with pre-training LMs showed better performance than other models on the task-specific transformations, e.g., AddDiff , where the accuracy score of BERT-Aspect dropped from 90.32 to merely 81.58. All the evaluation results and comprehensive robustness analysis are available at textflint.io. 5 Related Work Our work is related to many existing open-source tools and works in different areas. Robustness Evaluation Many tools include evaluation methods for robustness, including NLPAug (Ma, 2019), Errudite (Wu et al., 2019), AllenNLP Interpret (Wallace et al., 2019), and Checklist (Ribeiro et al., 2020), which are only applicable to limited parts of robustness evaluations, while TextFlint supports comprehensive evaluation methods, e.g., subpopulation, adversarial attacks, transformations, and so on. Besides the common transformation methods like synonym substitution and typos, various task-specific transformations are tailored for each of the 12 NLP tasks, which is peculiar to TextFlint. Moreover, we are the first to provide linguistic support for the transformations, the designs for which were inspired by linguistics and have been proved plausible and readable by human annotators. Several t"
2021.acl-demo.41,C16-1311,0,0.0414402,"Missing"
2021.acl-demo.41,D16-1021,0,0.0913955,"Missing"
2021.acl-demo.41,D19-3002,0,0.0392234,"Missing"
2021.acl-demo.41,D16-1058,0,0.0373143,"Missing"
2021.acl-demo.41,P19-1073,0,0.133542,"underlying patterns about model robustness. As for the ABSA task (Table 2), methods equipped with pre-training LMs showed better performance than other models on the task-specific transformations, e.g., AddDiff , where the accuracy score of BERT-Aspect dropped from 90.32 to merely 81.58. All the evaluation results and comprehensive robustness analysis are available at textflint.io. 5 Related Work Our work is related to many existing open-source tools and works in different areas. Robustness Evaluation Many tools include evaluation methods for robustness, including NLPAug (Ma, 2019), Errudite (Wu et al., 2019), AllenNLP Interpret (Wallace et al., 2019), and Checklist (Ribeiro et al., 2020), which are only applicable to limited parts of robustness evaluations, while TextFlint supports comprehensive evaluation methods, e.g., subpopulation, adversarial attacks, transformations, and so on. Besides the common transformation methods like synonym substitution and typos, various task-specific transformations are tailored for each of the 12 NLP tasks, which is peculiar to TextFlint. Moreover, we are the first to provide linguistic support for the transformations, the designs for which were inspired by lingu"
2021.acl-demo.41,2020.emnlp-main.292,1,0.893206,"Missing"
2021.acl-long.16,N19-1423,0,0.452081,"prediction ... Layers (>i+2) Yes? B-PER ? O ? B-ORG? O ? Dell founded Dell in 1984 B-PER ? O ? B-ORG? O ? B-TIME ? Dell founded Dell in 1984 B-ORG ? O ? B-ORG? O ? B-TIME ? Dell founded Dell in 1984 B-TIME ? Confident? (Pooling) No? Confident? (Pooling) 2 Recently, PTMs (Qiu et al., 2020) have become the mainstream backbone model for various sequence labeling tasks. The typical framework consists of a backbone encoder and a task-specific decoder. Confident? (Pooling) (b) Sentence-Level Early-Exit for Sequence Labeling Early-Exit prediction ... Layers (>i+2) Encoder In this paper, we use BERT (Devlin et al., 2019) as our backbone encoder . The architecture of BERT consists of multiple stacked Transformer layers (Vaswani et al., 2017). Given a sequence of tokens x1 , · · · , xN , the hidden state of l-th transformer layer is denoted by (l) (l) H(l) = [h1 , · · · , hN ], and H(0) is the BERT input embedding. Yes? B-PER ? Layer (i+2) Dell B-PER ? Layer (i+1) Dell B-ORG ? Layer (i) Dell O ? B-ORG? O ? B-TIME ? founded Dell in 1984 O ? B-ORG? O ? B-TIME ? founded Dell in 1984 O ? B-ORG? O ? B-TIME ? founded Dell in 1984 BERT for Sequence Labeling No? (c) Token-Level Early-Exit for Sequence Labeling Figure 1"
2021.acl-long.16,P11-2008,0,0.0843056,"Missing"
2021.acl-long.16,P11-2000,0,0.222966,"Missing"
2021.acl-long.16,D18-1275,1,0.88954,"Missing"
2021.acl-long.16,E17-2113,0,0.0604904,"Missing"
2021.acl-long.16,2020.emnlp-main.518,0,0.0352134,"Dredze, 2015; He and Sun, 2017) and CLUE NER (Xu et al., 2020), POS: ARK Twitter (Gimpel et al., 2011; Owoputi et al., 2013), CTB5 POS (Xue et al., 2005) and UD POS (Nivre et al., 2016), CWS: CTB5 Seg (Xue et al., 2005) and UD Seg (Nivre et al., 2016). Besides the standard benchmark dataset like CoNLL2003 and Ontonotes 4.0, we also choose some datasets closer to realworld application to verify the actual utility of our methods, such as Twitter NER and Weibo in social media domain. We use the same dataset prepro193 cessing and split as in previous work (Huang et al., 2015; Mengge et al., 2020; Jia et al., 2020; Tian et al., 2020; Nguyen et al., 2020). 4.2.2 e.g., Chinese NER, TOKEE (4×) on BERT can still outperform LSTM-CRF significantly. This indicates the potential utility of it in complicated real-world scenario. To explore the fine-grained performance change under different speedup ratio, We visualize the speedup-performance trade-off curve on 6 datasets, in Figure2. We observe that, Baseline We compare our methods with three baselines: • BiLSTM-CRF (Huang et al., 2015; Ma and Hovy, 2016) The most widely used model in sequence labeling tasks before the pre-trained language model prevails in NLP"
2021.acl-long.16,2020.findings-emnlp.372,0,0.0315433,"eper layer even when 4×, meanwhile, the SENTEE’s average exiting layer number reduces to 2.5, where the PTM’s encoding power is severely cut down. This gives an intuitive explanation of why TOKEE is more effective than SENTEE under high speedup ratio: although both SENTEE and TOKEE can dynamically adjust computational cost on the sample-level, TOKEE can adjust do it in a more fine-grained way. 196 5 Related Work PTMs are powerful but have high computational cost. To accelerate them, many attempts have been made. A kind of methods is to reduce its size, such as distillation (Sanh et al., 2019; Jiao et al., 2020), structural pruning (Michel et al., 2019; Fan et al., 2020) and quantization (Shen et al., 2020). Another kind of methods is early-exit, which dynamically adjusts the encoding layer number of different samples (Liu et al., 2020; Xin et al., 2020; Schwartz et al., 2020; Zhou et al., 2020; Li et al., 2020). While they introduced early-exit mechanism in simple classification tasks, our methods are proposed for the more complicated scenario: sequence labeling, where it has not only one prediction probability and it’s necessary to consider the dependency of token exitings. Elbayad et al. (2020) pr"
2021.acl-long.16,2021.findings-emnlp.43,0,0.0610833,"Missing"
2021.acl-long.16,2020.acl-main.537,0,0.333853,"s are usually fundamental and highly time-demanding, therefore, apart from performance, their inference efficiency is also very important. The past few years have witnessed the prevailing of pre-trained models (PTMs) (Qiu et al., 2020) on various sequence labeling tasks (Nguyen et al., 2020; Ke et al., 2020; Tian et al., 2020; Mengge et al., 2020). Despite their significant improvements on sequence labeling, they are notorious for enormous computational cost and slow inference speed, which hinders their utility in real-time scenarios or mobile-device scenarios. Recently, early-exit mechanism (Liu et al., 2020; Xin et al., 2020; Schwartz et al., 2020; Zhou et al., 2020) has been introduced to accelerate inference for large-scale PTMs. In their methods, each layer of the PTM is coupled with a classifier to predict the label for a given instance. At inference stage, if the prediction is confident 2 enough at an earlier time, it is allowed to exit without passing through the entire model. Figure 1(a) gives an illustration of early-exit mechanism for text classification. However, most existing early-exit methods are targeted at sequence-level prediction, such as text classification, in which the predic"
2021.acl-long.16,P16-1101,0,0.0469515,"se the same dataset prepro193 cessing and split as in previous work (Huang et al., 2015; Mengge et al., 2020; Jia et al., 2020; Tian et al., 2020; Nguyen et al., 2020). 4.2.2 e.g., Chinese NER, TOKEE (4×) on BERT can still outperform LSTM-CRF significantly. This indicates the potential utility of it in complicated real-world scenario. To explore the fine-grained performance change under different speedup ratio, We visualize the speedup-performance trade-off curve on 6 datasets, in Figure2. We observe that, Baseline We compare our methods with three baselines: • BiLSTM-CRF (Huang et al., 2015; Ma and Hovy, 2016) The most widely used model in sequence labeling tasks before the pre-trained language model prevails in NLP. • BERT The powerful stacked Transformer encoder model, pre-trained on large-scale corpus, which we use as the backbone of our methods. • DistilBERT The most well-known distillation method of BERT. Huggingface released 6 layers DistilBERT for English (Sanh et al., 2019). For comparison, we distill {3, 4} and {3, 4, 6} layers DistilBERT for English and Chinese using the same method. 4.2.3 Hyper-Parameters For all datasets, We use batch size=10. We perform grid search over learning rate i"
2021.acl-long.16,2020.emnlp-main.514,0,0.474756,"nce labeling tasks, such as named entity recognition, part-of-speech tagging, ∗ Corresponding author. Our implementation is publicly available at https:// github.com/LeeSureman/Sequence-Labeling-Early-Exit. Chinese word segmentation and Semantic Role Labeling. These tasks are usually fundamental and highly time-demanding, therefore, apart from performance, their inference efficiency is also very important. The past few years have witnessed the prevailing of pre-trained models (PTMs) (Qiu et al., 2020) on various sequence labeling tasks (Nguyen et al., 2020; Ke et al., 2020; Tian et al., 2020; Mengge et al., 2020). Despite their significant improvements on sequence labeling, they are notorious for enormous computational cost and slow inference speed, which hinders their utility in real-time scenarios or mobile-device scenarios. Recently, early-exit mechanism (Liu et al., 2020; Xin et al., 2020; Schwartz et al., 2020; Zhou et al., 2020) has been introduced to accelerate inference for large-scale PTMs. In their methods, each layer of the PTM is coupled with a classifier to predict the label for a given instance. At inference stage, if the prediction is confident 2 enough at an earlier time, it is allowed"
2021.acl-long.16,2020.emnlp-demos.2,0,0.057415,"rocessing (NLP). Many NLP tasks can be converted to sequence labeling tasks, such as named entity recognition, part-of-speech tagging, ∗ Corresponding author. Our implementation is publicly available at https:// github.com/LeeSureman/Sequence-Labeling-Early-Exit. Chinese word segmentation and Semantic Role Labeling. These tasks are usually fundamental and highly time-demanding, therefore, apart from performance, their inference efficiency is also very important. The past few years have witnessed the prevailing of pre-trained models (PTMs) (Qiu et al., 2020) on various sequence labeling tasks (Nguyen et al., 2020; Ke et al., 2020; Tian et al., 2020; Mengge et al., 2020). Despite their significant improvements on sequence labeling, they are notorious for enormous computational cost and slow inference speed, which hinders their utility in real-time scenarios or mobile-device scenarios. Recently, early-exit mechanism (Liu et al., 2020; Xin et al., 2020; Schwartz et al., 2020; Zhou et al., 2020) has been introduced to accelerate inference for large-scale PTMs. In their methods, each layer of the PTM is coupled with a classifier to predict the label for a given instance. At inference stage, if the predicti"
2021.acl-long.16,L16-1262,0,0.0406356,"Missing"
2021.acl-long.16,N13-1039,0,0.0336183,"Missing"
2021.acl-long.16,D15-1064,0,0.0640046,"Missing"
2021.acl-long.16,2020.acl-main.593,0,0.242788,"time-demanding, therefore, apart from performance, their inference efficiency is also very important. The past few years have witnessed the prevailing of pre-trained models (PTMs) (Qiu et al., 2020) on various sequence labeling tasks (Nguyen et al., 2020; Ke et al., 2020; Tian et al., 2020; Mengge et al., 2020). Despite their significant improvements on sequence labeling, they are notorious for enormous computational cost and slow inference speed, which hinders their utility in real-time scenarios or mobile-device scenarios. Recently, early-exit mechanism (Liu et al., 2020; Xin et al., 2020; Schwartz et al., 2020; Zhou et al., 2020) has been introduced to accelerate inference for large-scale PTMs. In their methods, each layer of the PTM is coupled with a classifier to predict the label for a given instance. At inference stage, if the prediction is confident 2 enough at an earlier time, it is allowed to exit without passing through the entire model. Figure 1(a) gives an illustration of early-exit mechanism for text classification. However, most existing early-exit methods are targeted at sequence-level prediction, such as text classification, in which the prediction and its confidence score are calcula"
2021.acl-long.16,2020.acl-main.735,0,0.397254,"converted to sequence labeling tasks, such as named entity recognition, part-of-speech tagging, ∗ Corresponding author. Our implementation is publicly available at https:// github.com/LeeSureman/Sequence-Labeling-Early-Exit. Chinese word segmentation and Semantic Role Labeling. These tasks are usually fundamental and highly time-demanding, therefore, apart from performance, their inference efficiency is also very important. The past few years have witnessed the prevailing of pre-trained models (PTMs) (Qiu et al., 2020) on various sequence labeling tasks (Nguyen et al., 2020; Ke et al., 2020; Tian et al., 2020; Mengge et al., 2020). Despite their significant improvements on sequence labeling, they are notorious for enormous computational cost and slow inference speed, which hinders their utility in real-time scenarios or mobile-device scenarios. Recently, early-exit mechanism (Liu et al., 2020; Xin et al., 2020; Schwartz et al., 2020; Zhou et al., 2020) has been introduced to accelerate inference for large-scale PTMs. In their methods, each layer of the PTM is coupled with a classifier to predict the label for a given instance. At inference stage, if the prediction is confident 2 enough at an earli"
2021.acl-long.16,2020.acl-main.204,0,0.288513,"amental and highly time-demanding, therefore, apart from performance, their inference efficiency is also very important. The past few years have witnessed the prevailing of pre-trained models (PTMs) (Qiu et al., 2020) on various sequence labeling tasks (Nguyen et al., 2020; Ke et al., 2020; Tian et al., 2020; Mengge et al., 2020). Despite their significant improvements on sequence labeling, they are notorious for enormous computational cost and slow inference speed, which hinders their utility in real-time scenarios or mobile-device scenarios. Recently, early-exit mechanism (Liu et al., 2020; Xin et al., 2020; Schwartz et al., 2020; Zhou et al., 2020) has been introduced to accelerate inference for large-scale PTMs. In their methods, each layer of the PTM is coupled with a classifier to predict the label for a given instance. At inference stage, if the prediction is confident 2 enough at an earlier time, it is allowed to exit without passing through the entire model. Figure 1(a) gives an illustration of early-exit mechanism for text classification. However, most existing early-exit methods are targeted at sequence-level prediction, such as text classification, in which the prediction and its confi"
2021.acl-long.426,D18-1316,1,0.829419,"ERT) for NLP applications. Through extensive experimentation, we demonstrate that our method consistently outperforms recently proposed defense methods by a significant margin across different network architectures and multiple data sets. 1 Introduction Deep neural networks are powerful but vulnerable to adversarial examples that are intentionally crafted to fool the networks. Recent studies have shown the vulnerability of deep neural networks in many NLP tasks, including reading comprehension (Jia and Liang, 2017), text classification (Samanta and Mehta, 2017; Wong, 2017; Liang et al., 2018; Alzantot et al., 2018), machine translation (Zhao et al., 2018; Ebrahimi et al., 2018; Cheng et al., 2018), dialogue systems (Cheng et al., 2019), and dependency parsing (Zheng et al., 2020). These methods attack an NLP model by replacing, scrambling, and erasing characters or words under certain semantic and syntactic constraints. In particular, most of them craft adversarial examples by substituting words with their synonyms in an input text to maximally increase the prediction error while maintaining the adversarial examples’ fluency and naturalness. In this paper, we focus on these word substitution-based threa"
2021.acl-long.426,D15-1075,0,0.01475,"rage length of the sentences in IMDB (255 words on average) is much longer than that in AGNEWS (43 words on average). Longer sentences allow the adversaries to apply more word substitution-based perturbations to the examples. Generally, DNE performs better than IBP and comparable to ADV on the clean data, while it outperforms the others in all other cases. The results for both datasets show that our DNE consistently achieves better clean and robust accuracy. 4.2 Natural Language Inference We conducted the experiments of natural language inference on Stanford Natural Language Inference (SNLI) (Bowman et al., 2015) corpus. We also implemented three models for this task. The bag-ofwords model (BOW) encodes the premise and hypothesis separately by summing their word vectors, then feeds the concatenation of these encodings to a two-layer feedforward network. The other two models are similar, except they run either a Decomposable Attention (DecomAtt) (Parikh et al., 2016) or BERT (Devlin et al., 2019) on the word embeddings to generate the sentence representations, which uses attention between the premise and hypothesis to compute richer representations of each word in both sentences. All models are trained"
2021.acl-long.426,P18-1031,0,0.0592567,"Missing"
2021.acl-long.426,D19-1419,0,0.535586,"be replaced with any of its synonyms. Also, when updating word embeddings during training, the distance between a word and its synonyms in the embedding space change dynamically. Therefore, the point-wise guarantee becomes insufficient, and the resulting models have shown to be vulnerable to strong attacks (Alzantot et al., 2018). On the other hand, several certified defense methods have recently been proposed to ensure that the model predictions are unchanged when input word embeddings are perturbed within the convex hull formed by the embeddings of a word and its synonyms (Jia et al., 2019; Huang et al., 2019). However, due to the difficulty of propagating convex hull through deep neural networks, they compute a loose outer bound using Interval Bound Propagation (IBP). As a result, the convex hull may contain irrelevant words and lead to a significant performance drop on the clean data. In this paper, we propose Dirichlet Neighborhood Ensemble (DNE) to create virtual sentences by mixing the embedding of the original word in the input sentence with its synonyms. By training on these virtual sentences, the model can enhance the robustness against word substitution-based per5482 Proceedings of the 59t"
2021.acl-long.426,D17-1215,0,0.0303089,"riginal clean data. DNE is agnostic to the network architectures and scales to large models (e.g., BERT) for NLP applications. Through extensive experimentation, we demonstrate that our method consistently outperforms recently proposed defense methods by a significant margin across different network architectures and multiple data sets. 1 Introduction Deep neural networks are powerful but vulnerable to adversarial examples that are intentionally crafted to fool the networks. Recent studies have shown the vulnerability of deep neural networks in many NLP tasks, including reading comprehension (Jia and Liang, 2017), text classification (Samanta and Mehta, 2017; Wong, 2017; Liang et al., 2018; Alzantot et al., 2018), machine translation (Zhao et al., 2018; Ebrahimi et al., 2018; Cheng et al., 2018), dialogue systems (Cheng et al., 2019), and dependency parsing (Zheng et al., 2020). These methods attack an NLP model by replacing, scrambling, and erasing characters or words under certain semantic and syntactic constraints. In particular, most of them craft adversarial examples by substituting words with their synonyms in an input text to maximally increase the prediction error while maintaining the adversa"
2021.acl-long.426,D19-1423,0,0.131258,"Missing"
2021.acl-long.426,N19-1336,1,0.849581,"proposed defense methods by a significant margin across different network architectures and multiple data sets. 1 Introduction Deep neural networks are powerful but vulnerable to adversarial examples that are intentionally crafted to fool the networks. Recent studies have shown the vulnerability of deep neural networks in many NLP tasks, including reading comprehension (Jia and Liang, 2017), text classification (Samanta and Mehta, 2017; Wong, 2017; Liang et al., 2018; Alzantot et al., 2018), machine translation (Zhao et al., 2018; Ebrahimi et al., 2018; Cheng et al., 2018), dialogue systems (Cheng et al., 2019), and dependency parsing (Zheng et al., 2020). These methods attack an NLP model by replacing, scrambling, and erasing characters or words under certain semantic and syntactic constraints. In particular, most of them craft adversarial examples by substituting words with their synonyms in an input text to maximally increase the prediction error while maintaining the adversarial examples’ fluency and naturalness. In this paper, we focus on these word substitution-based threat models and discuss the strategy to defend against such attacks. The goal of adversarial defenses is to learn a model capa"
2021.acl-long.426,N19-1423,0,0.228148,"detailed analysis, we found that DNE enables the embeddings of a set of similar words to be updated together in a coordinated way. In contrast, prior approaches either fix the word vectors during training (e.g., in the certified defenses) or update individual word vectors independently (e.g., in the adversarial training). We believe it is the crucial property why DNE leads to a more robust NLP model. Furthermore, unlike most certified defenses, the proposed method is easy to implement and can be integrated into any existing neural network including those with large architecture such as BERT (Devlin et al., 2019). 2 Related Work In the text domain, adversarial training is one of the most successful defenses (Miyato et al., 2017; Sato et al., 2019; Zhu et al., 2019). A family of fast-gradient sign methods (FGSM) was introduced by Goodfellow et al. (2015) to generate adversarial examples in the image domain. They showed that the robustness and generalization of machine learning models could be improved by including high-quality adversarial examples in the training data. Miyato et al. (2017) proposed an FGSMlike adversarial training method to the text domain by applying perturbations to the word embeddin"
2021.acl-long.426,P18-2006,0,0.0537033,"demonstrate that our method consistently outperforms recently proposed defense methods by a significant margin across different network architectures and multiple data sets. 1 Introduction Deep neural networks are powerful but vulnerable to adversarial examples that are intentionally crafted to fool the networks. Recent studies have shown the vulnerability of deep neural networks in many NLP tasks, including reading comprehension (Jia and Liang, 2017), text classification (Samanta and Mehta, 2017; Wong, 2017; Liang et al., 2018; Alzantot et al., 2018), machine translation (Zhao et al., 2018; Ebrahimi et al., 2018; Cheng et al., 2018), dialogue systems (Cheng et al., 2019), and dependency parsing (Zheng et al., 2020). These methods attack an NLP model by replacing, scrambling, and erasing characters or words under certain semantic and syntactic constraints. In particular, most of them craft adversarial examples by substituting words with their synonyms in an input text to maximally increase the prediction error while maintaining the adversarial examples’ fluency and naturalness. In this paper, we focus on these word substitution-based threat models and discuss the strategy to defend against such attack"
2021.acl-long.426,P11-1015,0,0.070819,"is randomly replaced with one of its synonyms. The same random replacement is used in the inference time, and the prediction scores are ensembled to get an output. RAN can be viewed as a variant of SAFER (Ye et al., 2020), where during the training SAFER’s perturbation set is replaced with the synonym set used by the adversaries and the number of ensembles is reduced to 16 (instead of 5, 000) at the inference time, which make it feasible to be evaluated empirically under the attacks. 4.1 Text Classification We experimented on two text classification data sets: Internet Movie Database (IMDB) (Maas et al., 2011) and AG News corpus (AGNEWS) (Zhang et al., 2015). We implemented three models for these text classification tasks like (Jia et al., 2019). The bag-of-words model (BOW) averages the word embeddings for each word in the input, then passes this through a one-layer feedforward network with 100-dimensional hidden state to get a final logit. The other two models are similar, except they run either a CNN or a two-layer LSTM on the word embeddings. All models are trained on cross-entropy loss, and their hyperparameters are tuned on the validation set (see Appendix A.1 for details). Table 1 reports bo"
2021.acl-long.426,N19-1314,0,0.0314359,"Missing"
2021.acl-long.426,D16-1244,0,0.0787179,"Missing"
2021.acl-long.426,D14-1162,0,0.0973367,"Missing"
2021.acl-long.426,2020.acl-main.590,1,0.800866,"rgin across different network architectures and multiple data sets. 1 Introduction Deep neural networks are powerful but vulnerable to adversarial examples that are intentionally crafted to fool the networks. Recent studies have shown the vulnerability of deep neural networks in many NLP tasks, including reading comprehension (Jia and Liang, 2017), text classification (Samanta and Mehta, 2017; Wong, 2017; Liang et al., 2018; Alzantot et al., 2018), machine translation (Zhao et al., 2018; Ebrahimi et al., 2018; Cheng et al., 2018), dialogue systems (Cheng et al., 2019), and dependency parsing (Zheng et al., 2020). These methods attack an NLP model by replacing, scrambling, and erasing characters or words under certain semantic and syntactic constraints. In particular, most of them craft adversarial examples by substituting words with their synonyms in an input text to maximally increase the prediction error while maintaining the adversarial examples’ fluency and naturalness. In this paper, we focus on these word substitution-based threat models and discuss the strategy to defend against such attacks. The goal of adversarial defenses is to learn a model capable of achieving high test accuracy on both c"
2021.acl-long.426,P19-1103,0,0.0614039,"Missing"
2021.acl-long.426,2020.acl-main.317,0,0.643337,"to formally verify a model’s robustness against word substitutionbased perturbations. Shi et al. (2020) and Xu et al. (2020) proposed the robustness verification and training method for transformers based on linear relaxation-based perturbation analysis. However, these defenses often lead to loose upper bounds for arbitrary networks and result in a higher cost of clean accuracy. Furthermore, due to the difficulty of verification, certified defense methods are usually not scalable and remain hard to scale to complex prediction pipelines. To achieve certified robustness on large architectures, Ye et al. (2020) proposed a certified robust method called SAFER which is structure-free. However, the base classifier of SAFER is trained by the adversarial data augmentation. As shown in our experiments, randomly perturbing a word to its synonyms performs poorly in practice. In the image domain, randomization has been shown to overcome many of these obstacles in the IBP-based defense. Empirically, Xie et al. (2017) showed that random resizing and padding in the input domain could improve the robustness. Liu et al. (2018) proposed to add Gaussian noise in both the input layer and intermediate layers of CNN i"
2021.acl-long.455,P19-1635,0,0.0200443,"l. (2020b) connects each 5866 number in the problem with nearby nouns to enrich the problem representations. However, these methods rarely take numerical values into consideration. They replace all the numbers in the problems with number symbols and ignore the vital information provided by the numerical values in math word problem solving. As such, these methods will incorrectly generates the same expression for problems with different numerical values. Numerical Value Representations: Some recent studies have explored the numerical value representations in language models (Naik et al., 2019; Chen et al., 2019; Wallace et al., 2019). Spithourakis and Riedel (2018) investigated several of the strategies used for language models for their possible application to model numerals. Gong et al. (2020) proposed the use of contextual numerical value representations to enhance neural content planning by helping models to understand data values. To incorporate numerical value information into math word solving tasks, we use a digit-todigit numerical value encoder to obtain the numberaware problem representations. To further utilize the numerical properties, we propose a numerical properties prediction mechani"
2021.acl-long.455,C10-3014,0,0.00817697,"est set, respectively. We report answer accuracy as the main evaluation metrics of the math word problem solving task. 3.2 Implementation Details In this paper, we truncate the problem to a max sequence length of 150, and the expression to a max sequence length of 50. We select 4,000 words that appear most frequently in the training set of each dataset as the vocabulary, and replace the remaining words with a special token UNK. We initialize the word embedding with the pretrained 300-dimension word vectors3 . The problem encoder used two external knowledge bases: Cilin (Mei, 1985) and Hownet (Dong et al., 2010). The number of heads T in GAT is 8. The hidden size is 512 and the batch size is 64. We use the Adam optimizer (Kingma and Ba, 2014) to optimize the models an the learning rate is 0.001. We compute the final loss function with β1 , β2 , β3 of 0.5. Dropout (Srivastava et al., 2014) is set to 0.5. Models are trained in 80 epoches for the Math23K dataset and 50 epoches for the Ape210K dataset. During testing, the beam size is set to 5. Once all internal nodes in the expression tree have two child nodes, the decoder stops generating the next word. The hyper-parameters are tuned on the valid set."
2021.acl-long.455,2020.findings-emnlp.262,0,0.0207101,"eplace all the numbers in the problems with number symbols and ignore the vital information provided by the numerical values in math word problem solving. As such, these methods will incorrectly generates the same expression for problems with different numerical values. Numerical Value Representations: Some recent studies have explored the numerical value representations in language models (Naik et al., 2019; Chen et al., 2019; Wallace et al., 2019). Spithourakis and Riedel (2018) investigated several of the strategies used for language models for their possible application to model numerals. Gong et al. (2020) proposed the use of contextual numerical value representations to enhance neural content planning by helping models to understand data values. To incorporate numerical value information into math word solving tasks, we use a digit-todigit numerical value encoder to obtain the numberaware problem representations. To further utilize the numerical properties, we propose a numerical properties prediction mechanism. 5 Conclusion In this study, we proposed a novel approach called NumS2T, that better captures numerical value information and utilizes numerical properties. In this model, we use a digi"
2021.acl-long.455,P18-1039,0,0.0158951,"te-of-the-art models. 1 1 Figure 1: Example of a math word problem. The same problem with different numerical values may correspond to different math expressions. Without numerical value information, the model can hardly determine which expression is correct. Introduction Taking a math word problem as input, the math word problem solving task aims to generate a corresponding solvable expression and answer. With the advancements in natural language processing, math word problem solving has received growing attention in recent years (Roy and Roth, 2015; Mitra and Baral, 2016; Ling et al., 2017; Huang et al., 2018). Many methods have been proposed that use sequence-to-sequence (seq2seq) models with an attention mechanism (Bahdanau et al., 2014) for math word problem solving (Wang et al., 2017b, 2018b, 2019). To better utilize expression structure information, some methods use sequenceto-tree (seq2tree) models to generate expressions ∗ Corresponding author. Code is available at qinzhuowu/NumS2T/ 1 https://github.com/ and have achieved promising results (Liu et al., 2019; Xie and Sun, 2019; Wu et al., 2020). These methods convert the target expression into a binary tree, and generate a pre-order traversal"
2021.acl-long.455,P19-1619,0,0.0121074,"h word problem solving tasks (Ling et al., 2017; Wang et al., 2017b, 2018a). To better utilize expression structure information, recent studies have used Seq2Tree models (Liu et al., 2019; Zhang et al., 2020a). Xie and Sun (2019) proposed a tree structured decoder that uses a goal-driven approach to generate expression trees. Wu et al. (2020) proposed a knowledge-aware Seq2Tree model with a state aggregation mechanism that incorporates common-sense knowledge from external knowledge bases. Recently, several methods have attempted to use the contextual information of the numbers in the problem. Li et al. (2019) propose a group attention mechanism to extract quantity-related features and quantitypair features. Zhang et al. (2020b) connects each 5866 number in the problem with nearby nouns to enrich the problem representations. However, these methods rarely take numerical values into consideration. They replace all the numbers in the problems with number symbols and ignore the vital information provided by the numerical values in math word problem solving. As such, these methods will incorrectly generates the same expression for problems with different numerical values. Numerical Value Representations"
2021.acl-long.455,P17-1015,0,0.107963,"e than existing state-of-the-art models. 1 1 Figure 1: Example of a math word problem. The same problem with different numerical values may correspond to different math expressions. Without numerical value information, the model can hardly determine which expression is correct. Introduction Taking a math word problem as input, the math word problem solving task aims to generate a corresponding solvable expression and answer. With the advancements in natural language processing, math word problem solving has received growing attention in recent years (Roy and Roth, 2015; Mitra and Baral, 2016; Ling et al., 2017; Huang et al., 2018). Many methods have been proposed that use sequence-to-sequence (seq2seq) models with an attention mechanism (Bahdanau et al., 2014) for math word problem solving (Wang et al., 2017b, 2018b, 2019). To better utilize expression structure information, some methods use sequenceto-tree (seq2tree) models to generate expressions ∗ Corresponding author. Code is available at qinzhuowu/NumS2T/ 1 https://github.com/ and have achieved promising results (Liu et al., 2019; Xie and Sun, 2019; Wu et al., 2020). These methods convert the target expression into a binary tree, and generate"
2021.acl-long.455,D19-1241,0,0.0365,"Missing"
2021.acl-long.455,P16-1202,0,0.0198685,"ieves better performance than existing state-of-the-art models. 1 1 Figure 1: Example of a math word problem. The same problem with different numerical values may correspond to different math expressions. Without numerical value information, the model can hardly determine which expression is correct. Introduction Taking a math word problem as input, the math word problem solving task aims to generate a corresponding solvable expression and answer. With the advancements in natural language processing, math word problem solving has received growing attention in recent years (Roy and Roth, 2015; Mitra and Baral, 2016; Ling et al., 2017; Huang et al., 2018). Many methods have been proposed that use sequence-to-sequence (seq2seq) models with an attention mechanism (Bahdanau et al., 2014) for math word problem solving (Wang et al., 2017b, 2018b, 2019). To better utilize expression structure information, some methods use sequenceto-tree (seq2tree) models to generate expressions ∗ Corresponding author. Code is available at qinzhuowu/NumS2T/ 1 https://github.com/ and have achieved promising results (Liu et al., 2019; Xie and Sun, 2019; Wu et al., 2020). These methods convert the target expression into a binary"
2021.acl-long.455,P19-1329,0,0.0250924,"eatures. Zhang et al. (2020b) connects each 5866 number in the problem with nearby nouns to enrich the problem representations. However, these methods rarely take numerical values into consideration. They replace all the numbers in the problems with number symbols and ignore the vital information provided by the numerical values in math word problem solving. As such, these methods will incorrectly generates the same expression for problems with different numerical values. Numerical Value Representations: Some recent studies have explored the numerical value representations in language models (Naik et al., 2019; Chen et al., 2019; Wallace et al., 2019). Spithourakis and Riedel (2018) investigated several of the strategies used for language models for their possible application to model numerals. Gong et al. (2020) proposed the use of contextual numerical value representations to enhance neural content planning by helping models to understand data values. To incorporate numerical value information into math word solving tasks, we use a digit-todigit numerical value encoder to obtain the numberaware problem representations. To further utilize the numerical properties, we propose a numerical properties"
2021.acl-long.455,D15-1202,0,0.0210073,"e that our model achieves better performance than existing state-of-the-art models. 1 1 Figure 1: Example of a math word problem. The same problem with different numerical values may correspond to different math expressions. Without numerical value information, the model can hardly determine which expression is correct. Introduction Taking a math word problem as input, the math word problem solving task aims to generate a corresponding solvable expression and answer. With the advancements in natural language processing, math word problem solving has received growing attention in recent years (Roy and Roth, 2015; Mitra and Baral, 2016; Ling et al., 2017; Huang et al., 2018). Many methods have been proposed that use sequence-to-sequence (seq2seq) models with an attention mechanism (Bahdanau et al., 2014) for math word problem solving (Wang et al., 2017b, 2018b, 2019). To better utilize expression structure information, some methods use sequenceto-tree (seq2tree) models to generate expressions ∗ Corresponding author. Code is available at qinzhuowu/NumS2T/ 1 https://github.com/ and have achieved promising results (Liu et al., 2019; Xie and Sun, 2019; Wu et al., 2020). These methods convert the target ex"
2021.acl-long.455,P18-1196,0,0.0571618,"Missing"
2021.acl-long.455,D19-1534,0,0.018856,"each 5866 number in the problem with nearby nouns to enrich the problem representations. However, these methods rarely take numerical values into consideration. They replace all the numbers in the problems with number symbols and ignore the vital information provided by the numerical values in math word problem solving. As such, these methods will incorrectly generates the same expression for problems with different numerical values. Numerical Value Representations: Some recent studies have explored the numerical value representations in language models (Naik et al., 2019; Chen et al., 2019; Wallace et al., 2019). Spithourakis and Riedel (2018) investigated several of the strategies used for language models for their possible application to model numerals. Gong et al. (2020) proposed the use of contextual numerical value representations to enhance neural content planning by helping models to understand data values. To incorporate numerical value information into math word solving tasks, we use a digit-todigit numerical value encoder to obtain the numberaware problem representations. To further utilize the numerical properties, we propose a numerical properties prediction mechanism. 5 Conclusion In thi"
2021.acl-long.455,D18-1132,0,0.0325612,"Missing"
2021.acl-long.455,P17-1018,0,0.225624,"lue information, the model can hardly determine which expression is correct. Introduction Taking a math word problem as input, the math word problem solving task aims to generate a corresponding solvable expression and answer. With the advancements in natural language processing, math word problem solving has received growing attention in recent years (Roy and Roth, 2015; Mitra and Baral, 2016; Ling et al., 2017; Huang et al., 2018). Many methods have been proposed that use sequence-to-sequence (seq2seq) models with an attention mechanism (Bahdanau et al., 2014) for math word problem solving (Wang et al., 2017b, 2018b, 2019). To better utilize expression structure information, some methods use sequenceto-tree (seq2tree) models to generate expressions ∗ Corresponding author. Code is available at qinzhuowu/NumS2T/ 1 https://github.com/ and have achieved promising results (Liu et al., 2019; Xie and Sun, 2019; Wu et al., 2020). These methods convert the target expression into a binary tree, and generate a pre-order traversal sequence of this expression tree based on the parent and sibling nodes of each node. Although promising results have been achieved, previous methods rarely take numerical values in"
2021.acl-long.455,D17-1088,0,0.0519152,"Missing"
2021.acl-long.455,2020.emnlp-main.579,1,0.866795,"growing attention in recent years (Roy and Roth, 2015; Mitra and Baral, 2016; Ling et al., 2017; Huang et al., 2018). Many methods have been proposed that use sequence-to-sequence (seq2seq) models with an attention mechanism (Bahdanau et al., 2014) for math word problem solving (Wang et al., 2017b, 2018b, 2019). To better utilize expression structure information, some methods use sequenceto-tree (seq2tree) models to generate expressions ∗ Corresponding author. Code is available at qinzhuowu/NumS2T/ 1 https://github.com/ and have achieved promising results (Liu et al., 2019; Xie and Sun, 2019; Wu et al., 2020). These methods convert the target expression into a binary tree, and generate a pre-order traversal sequence of this expression tree based on the parent and sibling nodes of each node. Although promising results have been achieved, previous methods rarely take numerical values into consideration, despite the fact that in math word problem solving, numerical values provide vital information. As an infinite number of numerals can appear in math word problems, it is impossible to list them all in the vocabulary. Previous methods replace all the numbers in the problems with number symbols (e.g.,"
2021.acl-long.455,2020.acl-main.362,0,0.0258773,"nerates the sub-expression “80-52”. However, this problem is about the fares that have already been sold rather than how many tickets are left. With numerical properties, NumS2T is able to realize that 80 is not related to the target expression and should not appear in the generated result. 4 Related Work Math Word Problem Solving: In recent years, Seq2Seq (Sutskever et al., 2014) has been widely used in math word problem solving tasks (Ling et al., 2017; Wang et al., 2017b, 2018a). To better utilize expression structure information, recent studies have used Seq2Tree models (Liu et al., 2019; Zhang et al., 2020a). Xie and Sun (2019) proposed a tree structured decoder that uses a goal-driven approach to generate expression trees. Wu et al. (2020) proposed a knowledge-aware Seq2Tree model with a state aggregation mechanism that incorporates common-sense knowledge from external knowledge bases. Recently, several methods have attempted to use the contextual information of the numbers in the problem. Li et al. (2019) propose a group attention mechanism to extract quantity-related features and quantitypair features. Zhang et al. (2020b) connects each 5866 number in the problem with nearby nouns to enrich"
2021.acl-long.484,P19-1134,0,0.0222896,"Missing"
2021.acl-long.484,N19-1310,0,0.0168086,"Zeng et al. (2015); Shang (2019); Qin et al. (2018); Han et al. (2018a)) and the incorporation with extra information such as KGs, multi-lingual corpora or other information (Ji et al. (2017); Lei et al. (2018); Vashishth et al. (2018); Han et al. (2018b); Zhang et al. (2019); Qu et al. (2019); Verga et al. (2016); Lin et al. (2017); Wang et al. (2018); Deng and Sun (2019); Beltagy et al. (2019)). Other approaches include soft-label strategy for denoising (Liu et al. (2017)), leveraging pre-trained LM (Alt et al. (2019)), pattern-based method (Zheng et al. (2019)), structured learning method (Bai and Ritter (2019)) and so forth (Luo et al. (2017); Chen et al. (2019)). In this work, we focus on sentence-level relation extraction. Several previous studies also perform Distant RE on sentence-level. Feng et al. (2018) proposes a reinforcement learning framework for sentence selecting, where the reward is given by the classification scores on bag labels. Jia et al. (2019) builds an initial training set and further select confident instances based on selected patterns. The difference between the proposed work and previous works is that we do not rely on bag-level labels for sentence selecting. Furthermore, w"
2021.acl-long.484,N19-1184,0,0.0277828,"Missing"
2021.acl-long.484,D19-1031,0,0.0237491,"et al. (2018a)) and the incorporation with extra information such as KGs, multi-lingual corpora or other information (Ji et al. (2017); Lei et al. (2018); Vashishth et al. (2018); Han et al. (2018b); Zhang et al. (2019); Qu et al. (2019); Verga et al. (2016); Lin et al. (2017); Wang et al. (2018); Deng and Sun (2019); Beltagy et al. (2019)). Other approaches include soft-label strategy for denoising (Liu et al. (2017)), leveraging pre-trained LM (Alt et al. (2019)), pattern-based method (Zheng et al. (2019)), structured learning method (Bai and Ritter (2019)) and so forth (Luo et al. (2017); Chen et al. (2019)). In this work, we focus on sentence-level relation extraction. Several previous studies also perform Distant RE on sentence-level. Feng et al. (2018) proposes a reinforcement learning framework for sentence selecting, where the reward is given by the classification scores on bag labels. Jia et al. (2019) builds an initial training set and further select confident instances based on selected patterns. The difference between the proposed work and previous works is that we do not rely on bag-level labels for sentence selecting. Furthermore, we leverage NT to dynamically separate the noisy data"
2021.acl-long.484,D19-1039,0,0.0151829,"018c); Li et al. (2020); Hu et al. (2019a); Ye and Ling (2019); Yuan et al. (2019a); Zhu et al. (2019); Yuan et al. (2019b); Wu et al. (2017)), the selection strategies such as RL or adversarial training to remove noisy sentences from the bag (Zeng et al. (2015); Shang (2019); Qin et al. (2018); Han et al. (2018a)) and the incorporation with extra information such as KGs, multi-lingual corpora or other information (Ji et al. (2017); Lei et al. (2018); Vashishth et al. (2018); Han et al. (2018b); Zhang et al. (2019); Qu et al. (2019); Verga et al. (2016); Lin et al. (2017); Wang et al. (2018); Deng and Sun (2019); Beltagy et al. (2019)). Other approaches include soft-label strategy for denoising (Liu et al. (2017)), leveraging pre-trained LM (Alt et al. (2019)), pattern-based method (Zheng et al. (2019)), structured learning method (Bai and Ritter (2019)) and so forth (Luo et al. (2017); Chen et al. (2019)). In this work, we focus on sentence-level relation extraction. Several previous studies also perform Distant RE on sentence-level. Feng et al. (2018) proposes a reinforcement learning framework for sentence selecting, where the reward is given by the classification scores on bag labels. Jia et al."
2021.acl-long.484,N19-1423,0,0.0209143,"Missing"
2021.acl-long.484,D18-1247,0,0.140547,", 2019; Yuan et al., 2019a,b); 2) the hard de-noise methods that remove noisy sentences from the bag (Zeng et al., 2015; Qin et al., 2018; Han et al., 2018a; Shang, 2019). However, these bag-level approaches fail to map each sentence inside bags with explicit sentence labels. This problem limits the application of RE in some downstream tasks that require sentencelevel relation type, e.g., Yao and Van Durme (2014) and Xu et al. (2016) use sentence-level relation extraction to identify the relation between the answer and the entity in the question. Therefore, several studies (Jia et al. (2019); Feng et al. (2018)) have made efforts on sentence-level (or instance-level) 6201 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 6201–6213 August 1–6, 2021. ©2021 Association for Computational Linguistics distant RE, empirically verifying the deficiency of bag-level methods on sentence-level evaluation. However, the instance selection approaches of these methods depend on rewards(Feng et al., 2018) or frequent patterns(Jia et al., 2019) determined by bag-level labels, which contain much nois"
2021.acl-long.484,P11-1055,0,0.0732522,"nt improvement over previous methods in terms of both RE performance and de-noise effect. 1 https://github.com/rtmaww/SENT 2 2.1 Related Work Distant Supervision for RE Supervised relation extraction (RE) has been constrained by the lack of large-scale labeled data. Therefore, distant supervision (DS) is introduced by Mintz et al. (2009), which employs existing knowledge bases (KBs) as source of supervision instead of annotated text. Riedel et al. (2010) relaxes the DS assumption to the express-at-least-once assumption. As a result, multi-instance learning is introduced (Riedel et al. (2010); Hoffmann et al. (2011); Surdeanu et al. (2012)) for this task, where the training and evaluating process are performed in bag-level, with potential noisy sentences existing in each bag. Most following studies in distant RE adopt this paradigm, aiming to decrease the impact of noisy sentences in each bag. These studies include the attention-based methods to attend to useful information ( Lin et al. (2016); Han et al. (2018c); Li et al. (2020); Hu et al. (2019a); Ye and Ling (2019); Yuan et al. (2019a); Zhu et al. (2019); Yuan et al. (2019b); Wu et al. (2017)), the selection strategies such as RL or adversarial train"
2021.acl-long.484,D19-1395,0,0.140457,"lignment between a database and plain text. Such annotation paradigm results in an inevitable noise problem, which is alleviated by previous studies using multiinstance learning (MIL). In MIL, the training and testing processes are performed at the bag level, where a bag contains noisy sentences mentioning the same entity pair but possibly not describing the same relation. Studies using MIL can be broadly classified into two categories: 1) the soft de-noise methods that leverage soft weights to differentiate the influence of each sentence (Lin et al., 2016; Han et al., 2018c; Li et al., 2020; Hu et al., 2019a; Ye and Ling, 2019; Yuan et al., 2019a,b); 2) the hard de-noise methods that remove noisy sentences from the bag (Zeng et al., 2015; Qin et al., 2018; Han et al., 2018a; Shang, 2019). However, these bag-level approaches fail to map each sentence inside bags with explicit sentence labels. This problem limits the application of RE in some downstream tasks that require sentencelevel relation type, e.g., Yao and Van Durme (2014) and Xu et al. (2016) use sentence-level relation extraction to identify the relation between the answer and the entity in the question. Therefore, several studies (Jia e"
2021.acl-long.484,C18-1036,0,0.027544,"ase the impact of noisy sentences in each bag. These studies include the attention-based methods to attend to useful information ( Lin et al. (2016); Han et al. (2018c); Li et al. (2020); Hu et al. (2019a); Ye and Ling (2019); Yuan et al. (2019a); Zhu et al. (2019); Yuan et al. (2019b); Wu et al. (2017)), the selection strategies such as RL or adversarial training to remove noisy sentences from the bag (Zeng et al. (2015); Shang (2019); Qin et al. (2018); Han et al. (2018a)) and the incorporation with extra information such as KGs, multi-lingual corpora or other information (Ji et al. (2017); Lei et al. (2018); Vashishth et al. (2018); Han et al. (2018b); Zhang et al. (2019); Qu et al. (2019); Verga et al. (2016); Lin et al. (2017); Wang et al. (2018); Deng and Sun (2019); Beltagy et al. (2019)). Other approaches include soft-label strategy for denoising (Liu et al. (2017)), leveraging pre-trained LM (Alt et al. (2019)), pattern-based method (Zheng et al. (2019)), structured learning method (Bai and Ritter (2019)) and so forth (Luo et al. (2017); Chen et al. (2019)). In this work, we focus on sentence-level relation extraction. Several previous studies also perform Distant RE on sentence-level. Fen"
2021.acl-long.484,P17-1004,0,0.0126913,"tion ( Lin et al. (2016); Han et al. (2018c); Li et al. (2020); Hu et al. (2019a); Ye and Ling (2019); Yuan et al. (2019a); Zhu et al. (2019); Yuan et al. (2019b); Wu et al. (2017)), the selection strategies such as RL or adversarial training to remove noisy sentences from the bag (Zeng et al. (2015); Shang (2019); Qin et al. (2018); Han et al. (2018a)) and the incorporation with extra information such as KGs, multi-lingual corpora or other information (Ji et al. (2017); Lei et al. (2018); Vashishth et al. (2018); Han et al. (2018b); Zhang et al. (2019); Qu et al. (2019); Verga et al. (2016); Lin et al. (2017); Wang et al. (2018); Deng and Sun (2019); Beltagy et al. (2019)). Other approaches include soft-label strategy for denoising (Liu et al. (2017)), leveraging pre-trained LM (Alt et al. (2019)), pattern-based method (Zheng et al. (2019)), structured learning method (Bai and Ritter (2019)) and so forth (Luo et al. (2017); Chen et al. (2019)). In this work, we focus on sentence-level relation extraction. Several previous studies also perform Distant RE on sentence-level. Feng et al. (2018) proposes a reinforcement learning framework for sentence selecting, where the reward is given by the classif"
2021.acl-long.484,P16-1200,0,0.0704369,"s proposed to gather training data through automatic alignment between a database and plain text. Such annotation paradigm results in an inevitable noise problem, which is alleviated by previous studies using multiinstance learning (MIL). In MIL, the training and testing processes are performed at the bag level, where a bag contains noisy sentences mentioning the same entity pair but possibly not describing the same relation. Studies using MIL can be broadly classified into two categories: 1) the soft de-noise methods that leverage soft weights to differentiate the influence of each sentence (Lin et al., 2016; Han et al., 2018c; Li et al., 2020; Hu et al., 2019a; Ye and Ling, 2019; Yuan et al., 2019a,b); 2) the hard de-noise methods that remove noisy sentences from the bag (Zeng et al., 2015; Qin et al., 2018; Han et al., 2018a; Shang, 2019). However, these bag-level approaches fail to map each sentence inside bags with explicit sentence labels. This problem limits the application of RE in some downstream tasks that require sentencelevel relation type, e.g., Yao and Van Durme (2014) and Xu et al. (2016) use sentence-level relation extraction to identify the relation between the answer and the enti"
2021.acl-long.484,D17-1189,0,0.0203554,"uan et al. (2019b); Wu et al. (2017)), the selection strategies such as RL or adversarial training to remove noisy sentences from the bag (Zeng et al. (2015); Shang (2019); Qin et al. (2018); Han et al. (2018a)) and the incorporation with extra information such as KGs, multi-lingual corpora or other information (Ji et al. (2017); Lei et al. (2018); Vashishth et al. (2018); Han et al. (2018b); Zhang et al. (2019); Qu et al. (2019); Verga et al. (2016); Lin et al. (2017); Wang et al. (2018); Deng and Sun (2019); Beltagy et al. (2019)). Other approaches include soft-label strategy for denoising (Liu et al. (2017)), leveraging pre-trained LM (Alt et al. (2019)), pattern-based method (Zheng et al. (2019)), structured learning method (Bai and Ritter (2019)) and so forth (Luo et al. (2017); Chen et al. (2019)). In this work, we focus on sentence-level relation extraction. Several previous studies also perform Distant RE on sentence-level. Feng et al. (2018) proposes a reinforcement learning framework for sentence selecting, where the reward is given by the classification scores on bag labels. Jia et al. (2019) builds an initial training set and further select confident instances based on selected patterns"
2021.acl-long.484,P09-1113,0,0.249095,"s noisy information. Furthermore, the model trained with NT is able to separate the noisy data from the training data. Based on NT, we propose a sentence-level framework, SENT, for distant relation extraction. SENT not only filters the noisy data to construct a cleaner dataset, but also performs a relabeling process to transform the noisy data into useful training data, thus further benefiting the model’s performance. Experimental results show the significant improvement of the proposed method over previous methods on sentence-level evaluation and de-noise effect. 1 Which label ? supervision (Mintz et al., 2009) is proposed to gather training data through automatic alignment between a database and plain text. Such annotation paradigm results in an inevitable noise problem, which is alleviated by previous studies using multiinstance learning (MIL). In MIL, the training and testing processes are performed at the bag level, where a bag contains noisy sentences mentioning the same entity pair but possibly not describing the same relation. Studies using MIL can be broadly classified into two categories: 1) the soft de-noise methods that leverage soft weights to differentiate the influence of each sentence"
2021.acl-long.484,P18-1199,0,0.226852,"sing multiinstance learning (MIL). In MIL, the training and testing processes are performed at the bag level, where a bag contains noisy sentences mentioning the same entity pair but possibly not describing the same relation. Studies using MIL can be broadly classified into two categories: 1) the soft de-noise methods that leverage soft weights to differentiate the influence of each sentence (Lin et al., 2016; Han et al., 2018c; Li et al., 2020; Hu et al., 2019a; Ye and Ling, 2019; Yuan et al., 2019a,b); 2) the hard de-noise methods that remove noisy sentences from the bag (Zeng et al., 2015; Qin et al., 2018; Han et al., 2018a; Shang, 2019). However, these bag-level approaches fail to map each sentence inside bags with explicit sentence labels. This problem limits the application of RE in some downstream tasks that require sentencelevel relation type, e.g., Yao and Van Durme (2014) and Xu et al. (2016) use sentence-level relation extraction to identify the relation between the answer and the entity in the question. Therefore, several studies (Jia et al. (2019); Feng et al. (2018)) have made efforts on sentence-level (or instance-level) 6201 Proceedings of the 59th Annual Meeting of the Associatio"
2021.acl-long.484,N16-1103,0,0.0326703,"Missing"
2021.acl-long.484,C18-1099,0,0.013895,"2016); Han et al. (2018c); Li et al. (2020); Hu et al. (2019a); Ye and Ling (2019); Yuan et al. (2019a); Zhu et al. (2019); Yuan et al. (2019b); Wu et al. (2017)), the selection strategies such as RL or adversarial training to remove noisy sentences from the bag (Zeng et al. (2015); Shang (2019); Qin et al. (2018); Han et al. (2018a)) and the incorporation with extra information such as KGs, multi-lingual corpora or other information (Ji et al. (2017); Lei et al. (2018); Vashishth et al. (2018); Han et al. (2018b); Zhang et al. (2019); Qu et al. (2019); Verga et al. (2016); Lin et al. (2017); Wang et al. (2018); Deng and Sun (2019); Beltagy et al. (2019)). Other approaches include soft-label strategy for denoising (Liu et al. (2017)), leveraging pre-trained LM (Alt et al. (2019)), pattern-based method (Zheng et al. (2019)), structured learning method (Bai and Ritter (2019)) and so forth (Luo et al. (2017); Chen et al. (2019)). In this work, we focus on sentence-level relation extraction. Several previous studies also perform Distant RE on sentence-level. Feng et al. (2018) proposes a reinforcement learning framework for sentence selecting, where the reward is given by the classification scores on ba"
2021.acl-long.484,P10-1013,0,0.0635427,"of noise exist in bag-level labels: 1) Multi-label noise: the exact label (“place of birth” or “employee of”) for each sentence is unclear; 2) Wrong-label noise: the third sentence inside the bag actually expresses “live in” which is not included in the bag labels. Relation extraction (RE), which aims to extract the relation between entity pairs from unstructured text, is a fundamental task in natural language processing. The extracted relation facts can benefit various downstream applications, e.g., knowledge graph completion (Bordes et al., 2013; Wang et al., 2014), information extraction (Wu and Weld, 2010) and question answering (Yao and Van Durme, 2014; Fader et al., 2014). A significant challenge for relation extraction is the lack of large-scale labeled data. Thus, distant Corresponding authors.  Place_of_birth Obama Lived_in (unincluded label) Introduction ∗ ? ? Distant supervision for relation extraction provides uniform bag labels for each sentence inside the bag, while accurate sentence labels are important for downstream applications that need the exact relation type. Directly using bag labels for sentence-level training will introduce much noise, thus severely degrading performance. I"
2021.acl-long.484,D17-1187,0,0.0429042,"Missing"
2021.acl-long.484,P14-1090,0,0.0510755,"Missing"
2021.acl-long.484,N19-1288,0,0.0619595,"database and plain text. Such annotation paradigm results in an inevitable noise problem, which is alleviated by previous studies using multiinstance learning (MIL). In MIL, the training and testing processes are performed at the bag level, where a bag contains noisy sentences mentioning the same entity pair but possibly not describing the same relation. Studies using MIL can be broadly classified into two categories: 1) the soft de-noise methods that leverage soft weights to differentiate the influence of each sentence (Lin et al., 2016; Han et al., 2018c; Li et al., 2020; Hu et al., 2019a; Ye and Ling, 2019; Yuan et al., 2019a,b); 2) the hard de-noise methods that remove noisy sentences from the bag (Zeng et al., 2015; Qin et al., 2018; Han et al., 2018a; Shang, 2019). However, these bag-level approaches fail to map each sentence inside bags with explicit sentence labels. This problem limits the application of RE in some downstream tasks that require sentencelevel relation type, e.g., Yao and Van Durme (2014) and Xu et al. (2016) use sentence-level relation extraction to identify the relation between the answer and the entity in the question. Therefore, several studies (Jia et al. (2019); Feng e"
2021.acl-long.484,D15-1203,0,0.463709,"previous studies using multiinstance learning (MIL). In MIL, the training and testing processes are performed at the bag level, where a bag contains noisy sentences mentioning the same entity pair but possibly not describing the same relation. Studies using MIL can be broadly classified into two categories: 1) the soft de-noise methods that leverage soft weights to differentiate the influence of each sentence (Lin et al., 2016; Han et al., 2018c; Li et al., 2020; Hu et al., 2019a; Ye and Ling, 2019; Yuan et al., 2019a,b); 2) the hard de-noise methods that remove noisy sentences from the bag (Zeng et al., 2015; Qin et al., 2018; Han et al., 2018a; Shang, 2019). However, these bag-level approaches fail to map each sentence inside bags with explicit sentence labels. This problem limits the application of RE in some downstream tasks that require sentencelevel relation type, e.g., Yao and Van Durme (2014) and Xu et al. (2016) use sentence-level relation extraction to identify the relation between the answer and the entity in the question. Therefore, several studies (Jia et al. (2019); Feng et al. (2018)) have made efforts on sentence-level (or instance-level) 6201 Proceedings of the 59th Annual Meeting"
2021.acl-long.484,C14-1220,0,0.129393,"Missing"
2021.acl-long.484,N19-1306,0,0.0227406,"Missing"
2021.acl-long.484,Y15-1009,0,0.0673948,"Missing"
2021.acl-long.484,D17-1004,0,0.0603697,"Missing"
2021.acl-long.484,P19-1137,0,0.0120284,"training to remove noisy sentences from the bag (Zeng et al. (2015); Shang (2019); Qin et al. (2018); Han et al. (2018a)) and the incorporation with extra information such as KGs, multi-lingual corpora or other information (Ji et al. (2017); Lei et al. (2018); Vashishth et al. (2018); Han et al. (2018b); Zhang et al. (2019); Qu et al. (2019); Verga et al. (2016); Lin et al. (2017); Wang et al. (2018); Deng and Sun (2019); Beltagy et al. (2019)). Other approaches include soft-label strategy for denoising (Liu et al. (2017)), leveraging pre-trained LM (Alt et al. (2019)), pattern-based method (Zheng et al. (2019)), structured learning method (Bai and Ritter (2019)) and so forth (Luo et al. (2017); Chen et al. (2019)). In this work, we focus on sentence-level relation extraction. Several previous studies also perform Distant RE on sentence-level. Feng et al. (2018) proposes a reinforcement learning framework for sentence selecting, where the reward is given by the classification scores on bag labels. Jia et al. (2019) builds an initial training set and further select confident instances based on selected patterns. The difference between the proposed work and previous works is that we do not rely on bag"
2021.acl-long.558,C18-1139,0,0.107383,"T0tGDCQERcBxPiOCFX5RmmyJM2oSpGFRaxI/oOaNVK2YDW0jNWaod28ejlpNRxSJqQ6jhhuZuu8olyluxv3hPlKc82pr+devnECgyJ/Us3r/yvTvYi0Me56sGlniLFyO6c1CVRtyJPrn/pSpBDRJzEPcpzwo5Szu9ZV5pY9S7v1lL5N1UpWbl20toE7/KUNGDz5zgXQf20ZBol84omfYJZZLGPAxzTPM9QxiUqqJE3xyOe8KxVtbF2p93PSrVMqtnDt9AePgFrq5a0&lt;/latexit> how researchers formulate the task. For example, recent years have seen frequent paradigm shifts for the task of named entity recognition (NER) from token-level tagging, which conceptualize NER as a sequence labeling (S EQ L AB) task (Chiu and Nichols, 2015; Huang et al., 2015; Ma and Hovy, 2016; Lample et al., 2016; Akbik et al., 2018; Peters et al., 2018; Devlin et al., 2018; Xia et al., 2019; Luo et al., 2020; Lin et al., 2020; Fu et al., 2021), to span-level prediction (S PAN N ER) (Li et al., 2020; Mengge et al., 2020; Jiang et al., 2020; Ouchi et al., 2020; Yu et al., 2020), which regards NER either as question answering (Li et al., 2020; Mengge et al., 2020), span classification (Jiang et al., 2020; Ouchi et al., 2020; Yamada et al., 2020), and dependency parsing tasks (Yu et al., 2020). However, despite the success of span predictionbased systems, as a relatively newly-explored framework, the understanding of its ar"
2021.acl-long.558,Q17-1010,0,0.0119914,"coder. √ indicates the embedding/structure is utilized in the current S EQ L AB system. For example, “sq0” denotes a model that uses Flair, GloVe, LSTM, and CRF as the character-, word-level embedding, sentence-level encoder, and decoder, respectively. “–” indicates not applicable.8 Regarding S EQ L AB-base systems, following (Fu et al., 2020b), their designs are diverse in four components: (1) character/subword-sensitive representation: ELMo (Peters et al., 2018), Flair (Akbik et al., 2018, 2019), BERT 9 (Devlin et al., 2018) 2) word representation: GloVe (Pennington et al., 2014), fastText (Bojanowski et al., 2017); (3) sentence-level encoders: LSTM (Hochreiter and Schmidhuber, 1997), CNN (Kalchbrenner et al., 2014b; Chen et al., 2019); (4) decoders: CRF (Lample et al., 2016; Collobert et al., 2011). We keep the testing result from the model with the best performance on the development set, terminating training when the performance of the development set is not improved in 20 epochs. 5.2 Baselines We extensively explore six system combination methods as competitors, which involves supervised and unsupervised fashions. 5.2.1 Voting-based Approaches Voting, as an unsupervised method, has been commonly use"
2021.acl-long.558,W17-4418,0,0.0607179,"Missing"
2021.acl-long.558,N19-1423,0,0.0160104,"Missing"
2021.acl-long.558,2020.emnlp-main.393,0,0.427901,"Missing"
2021.acl-long.558,W03-0425,0,0.477173,"Missing"
2021.acl-long.558,2020.emnlp-main.489,1,0.612955,"Missing"
2021.acl-long.558,2020.emnlp-main.457,1,0.472097,"stem diagnosis. sqi represents different S EQ L AB systems. Each value in heatmap entry (i, j) represents the performance gap between S EQ L AB and S PAN N ER (F 1sq − F 1span ) on j-th bucket. The green area indicates S EQ L AB performs better while the red area implies S PAN N ER is better. eCon, sLen, eLen, and oDen represent different attributes. 3.3 Exp-II: Analysis of Complementarity The holistic results in Tab. 1 make it hard for us to interpret the relative advantages of NER systems with different structural biases. To address this problem, we follow the interpretable evaluation idea (Fu et al., 2020a,c) that proposes to breakdown the holistic performance into different buckets from different perspectives and use a performance heatmap to illustrate relative advantages between two systems, i.e., system-pair diagnosis. Setup As a comparison, we replicate five topscoring S EQ L AB-based NER systems, which are sq1 : 92.41, sq2 : 92.01, sq3 : 92.46, sq4 : 92.11, sq5 : 91.99. Notably, to make a fair comparison, all five S EQ L ABs are with closed performance comparing to the above S PAN N ERs. Although we will detail configurations of these systems later (to reduce content redundancy) in §5.1 T"
2021.acl-long.558,C18-1161,0,0.0172363,"n. Here, we only consider the English (EN) dataset collected from the Reuters Corpus. CoNLL-2002 3 (Sang, 2002) contains annotated corpus in Dutch (NL) collected from De Morgen news, and Spanish (ES) collected from Spanish EFE News Agency. We evaluate both languages. OntoNotes 5.0 4 (Weischedel et al., 2013) is a large corpus consisting of three different languages: English, Chinese, and Arabic, involving six genres: newswire (NW), broadcast news (BN), broadcast conversation (BC), magazine (MZ), web data (WB), and telephone conversation (TC). Following previous works (Durrett and Klein, 2014; Ghaddar and Langlais, 2018), we utilize different domains in English to test the robustness of proposed models. 1 http://www.clips.uantwerpen.be/ conll2000/chunking/conlleval.txt 2 https://www.clips.uantwerpen.be/ conll2003/ner/ 3 https://www.clips.uantwerpen.be/ conll2002/ner/ 4 https://catalog.ldc.upenn.edu/ LDC2013T19 7184 WNUT-2016 5 and WNUT-2017 6 (Strauss et al., 2016; Derczynski et al., 2017) are social media data from Twitter, which were public as a shared task at WNUT-2016 (W16) and WNUT-2017 (W17). 3 Span Prediction for NE Recognition Although this is not the first work that formulates NER as a span predictio"
2021.acl-long.558,P11-1127,0,0.709198,"Missing"
2021.acl-long.558,P08-1067,0,0.15142,"Missing"
2021.acl-long.558,2020.acl-main.192,0,0.0476544,"Missing"
2021.acl-long.558,P14-1062,0,0.659847,"oMK/71n/I="">AAAC0XicjVHLSsNAFD2Nr7a+qi7dBIsoLkriRpcFNy4rtQ9oa0nSaRuaF5OJUEpB3PoDbvWnxD/QpX/gnWkKahG9IcmZc+85M3euHXluLAzjNaMtLa+srmVz+fWNza3tws5uPQ4T7rCaE3ohb9pWzDw3YDXhCo81I84s3/ZYwx5dyHzjlvHYDYNrMY5Yx7cGgdt3HUsQddP2LTHk/qQ6jqdds1soGiVDhb4IzBQUy7mPI1BUwsIL2ughhIMEPhgCCMIeLMT0tGDCQERcBxPiOCFX5RmmyJM2oSpGFRaxI/oOaNVK2YDW0jNWaod28ejlpNRxSJqQ6jhhuZuu8olyluxv3hPlKc82pr+devnECgyJ/Us3r/yvTvYi0Me56sGlniLFyO6c1CVRtyJPrn/pSpBDRJzEPcpzwo5Szu9ZV5pY9S7v1lL5N1UpWbl20toE7/KUNGDz5zgXQf20ZBol84omfYJZZLGPAxzTPM9QxiUqqJE3xyOe8KxVtbF2p93PSrVMqtnDt9AePgHdHJZ4&lt;/latexit> The rapid evolution of neural architectures (Kalchbrenner et al., 2014a; Kim, 2014; Hochreiter and Schmidhuber, 1997) and large pre-trained models (Devlin et al., 2019; Lewis et al., 2020) not only drive the state-of-the-art performance of many NLP tasks (Devlin et al., 2019; Liu and Lapata, 2019) to a new level but also change the way † This work is done when Jinlan visited CMU remotely. Corresponding author. SeqLab ... Sysm Sys1 Sys2 Figure 1: ONE span prediction model (S PAN NER) finishes TWO things: (1) named entity recognition (2) combination of different NER systems. Introduction ∗ SpanNer &lt;latexit sha1_base64=""CvJCMPVwdVTi+QwayXJ4kUndUbo="">AAAC0XicjVHLSsN"
2021.acl-long.558,P16-1101,0,0.143103,"VDhb4IzBQUy7mPI1BUwsIL2ughhIMEPhgCCMIeLMT0tGDCQERcBxPiOCFX5RmmyJM2oSpGFRaxI/oOaNVK2YDW0jNWaod28ejlpNRxSJqQ6jhhuZuu8olyluxv3hPlKc82pr+devnECgyJ/Us3r/yvTvYi0Me56sGlniLFyO6c1CVRtyJPrn/pSpBDRJzEPcpzwo5Szu9ZV5pY9S7v1lL5N1UpWbl20toE7/KUNGDz5zgXQf20ZBol84omfYJZZLGPAxzTPM9QxiUqqJE3xyOe8KxVtbF2p93PSrVMqtnDt9AePgFrq5a0&lt;/latexit> how researchers formulate the task. For example, recent years have seen frequent paradigm shifts for the task of named entity recognition (NER) from token-level tagging, which conceptualize NER as a sequence labeling (S EQ L AB) task (Chiu and Nichols, 2015; Huang et al., 2015; Ma and Hovy, 2016; Lample et al., 2016; Akbik et al., 2018; Peters et al., 2018; Devlin et al., 2018; Xia et al., 2019; Luo et al., 2020; Lin et al., 2020; Fu et al., 2021), to span-level prediction (S PAN N ER) (Li et al., 2020; Mengge et al., 2020; Jiang et al., 2020; Ouchi et al., 2020; Yu et al., 2020), which regards NER either as question answering (Li et al., 2020; Mengge et al., 2020), span classification (Jiang et al., 2020; Ouchi et al., 2020; Yamada et al., 2020), and dependency parsing tasks (Yu et al., 2020). However, despite the success of span predictionbased systems, as a relatively newly-explor"
2021.acl-long.558,2020.emnlp-main.514,0,0.122214,"NGDz5zgXQf20ZBol84omfYJZZLGPAxzTPM9QxiUqqJE3xyOe8KxVtbF2p93PSrVMqtnDt9AePgFrq5a0&lt;/latexit> how researchers formulate the task. For example, recent years have seen frequent paradigm shifts for the task of named entity recognition (NER) from token-level tagging, which conceptualize NER as a sequence labeling (S EQ L AB) task (Chiu and Nichols, 2015; Huang et al., 2015; Ma and Hovy, 2016; Lample et al., 2016; Akbik et al., 2018; Peters et al., 2018; Devlin et al., 2018; Xia et al., 2019; Luo et al., 2020; Lin et al., 2020; Fu et al., 2021), to span-level prediction (S PAN N ER) (Li et al., 2020; Mengge et al., 2020; Jiang et al., 2020; Ouchi et al., 2020; Yu et al., 2020), which regards NER either as question answering (Li et al., 2020; Mengge et al., 2020), span classification (Jiang et al., 2020; Ouchi et al., 2020; Yamada et al., 2020), and dependency parsing tasks (Yu et al., 2020). However, despite the success of span predictionbased systems, as a relatively newly-explored framework, the understanding of its architectural bias has not been fully understood so far. For example, what are the complementary advantages compared with S EQ L AB frameworks and how to make full use of them? Motivated by thi"
2021.acl-long.558,D14-1181,0,0.00548704,"FD2Nr7a+qi7dBIsoLkriRpcFNy4rtQ9oa0nSaRuaF5OJUEpB3PoDbvWnxD/QpX/gnWkKahG9IcmZc+85M3euHXluLAzjNaMtLa+srmVz+fWNza3tws5uPQ4T7rCaE3ohb9pWzDw3YDXhCo81I84s3/ZYwx5dyHzjlvHYDYNrMY5Yx7cGgdt3HUsQddP2LTHk/qQ6jqdds1soGiVDhb4IzBQUy7mPI1BUwsIL2ughhIMEPhgCCMIeLMT0tGDCQERcBxPiOCFX5RmmyJM2oSpGFRaxI/oOaNVK2YDW0jNWaod28ejlpNRxSJqQ6jhhuZuu8olyluxv3hPlKc82pr+devnECgyJ/Us3r/yvTvYi0Me56sGlniLFyO6c1CVRtyJPrn/pSpBDRJzEPcpzwo5Szu9ZV5pY9S7v1lL5N1UpWbl20toE7/KUNGDz5zgXQf20ZBol84omfYJZZLGPAxzTPM9QxiUqqJE3xyOe8KxVtbF2p93PSrVMqtnDt9AePgHdHJZ4&lt;/latexit> The rapid evolution of neural architectures (Kalchbrenner et al., 2014a; Kim, 2014; Hochreiter and Schmidhuber, 1997) and large pre-trained models (Devlin et al., 2019; Lewis et al., 2020) not only drive the state-of-the-art performance of many NLP tasks (Devlin et al., 2019; Liu and Lapata, 2019) to a new level but also change the way † This work is done when Jinlan visited CMU remotely. Corresponding author. SeqLab ... Sysm Sys1 Sys2 Figure 1: ONE span prediction model (S PAN NER) finishes TWO things: (1) named entity recognition (2) combination of different NER systems. Introduction ∗ SpanNer &lt;latexit sha1_base64=""CvJCMPVwdVTi+QwayXJ4kUndUbo="">AAAC0XicjVHLSsNAFD3GV+uz6tJ"
2021.acl-long.558,N16-1030,0,0.718952,"sIL2ughhIMEPhgCCMIeLMT0tGDCQERcBxPiOCFX5RmmyJM2oSpGFRaxI/oOaNVK2YDW0jNWaod28ejlpNRxSJqQ6jhhuZuu8olyluxv3hPlKc82pr+devnECgyJ/Us3r/yvTvYi0Me56sGlniLFyO6c1CVRtyJPrn/pSpBDRJzEPcpzwo5Szu9ZV5pY9S7v1lL5N1UpWbl20toE7/KUNGDz5zgXQf20ZBol84omfYJZZLGPAxzTPM9QxiUqqJE3xyOe8KxVtbF2p93PSrVMqtnDt9AePgFrq5a0&lt;/latexit> how researchers formulate the task. For example, recent years have seen frequent paradigm shifts for the task of named entity recognition (NER) from token-level tagging, which conceptualize NER as a sequence labeling (S EQ L AB) task (Chiu and Nichols, 2015; Huang et al., 2015; Ma and Hovy, 2016; Lample et al., 2016; Akbik et al., 2018; Peters et al., 2018; Devlin et al., 2018; Xia et al., 2019; Luo et al., 2020; Lin et al., 2020; Fu et al., 2021), to span-level prediction (S PAN N ER) (Li et al., 2020; Mengge et al., 2020; Jiang et al., 2020; Ouchi et al., 2020; Yu et al., 2020), which regards NER either as question answering (Li et al., 2020; Mengge et al., 2020), span classification (Jiang et al., 2020; Ouchi et al., 2020; Yamada et al., 2020), and dependency parsing tasks (Yu et al., 2020). However, despite the success of span predictionbased systems, as a relatively newly-explored framework, the und"
2021.acl-long.558,2020.acl-main.703,0,0.015985,"La+srmVz+fWNza3tws5uPQ4T7rCaE3ohb9pWzDw3YDXhCo81I84s3/ZYwx5dyHzjlvHYDYNrMY5Yx7cGgdt3HUsQddP2LTHk/qQ6jqdds1soGiVDhb4IzBQUy7mPI1BUwsIL2ughhIMEPhgCCMIeLMT0tGDCQERcBxPiOCFX5RmmyJM2oSpGFRaxI/oOaNVK2YDW0jNWaod28ejlpNRxSJqQ6jhhuZuu8olyluxv3hPlKc82pr+devnECgyJ/Us3r/yvTvYi0Me56sGlniLFyO6c1CVRtyJPrn/pSpBDRJzEPcpzwo5Szu9ZV5pY9S7v1lL5N1UpWbl20toE7/KUNGDz5zgXQf20ZBol84omfYJZZLGPAxzTPM9QxiUqqJE3xyOe8KxVtbF2p93PSrVMqtnDt9AePgHdHJZ4&lt;/latexit> The rapid evolution of neural architectures (Kalchbrenner et al., 2014a; Kim, 2014; Hochreiter and Schmidhuber, 1997) and large pre-trained models (Devlin et al., 2019; Lewis et al., 2020) not only drive the state-of-the-art performance of many NLP tasks (Devlin et al., 2019; Liu and Lapata, 2019) to a new level but also change the way † This work is done when Jinlan visited CMU remotely. Corresponding author. SeqLab ... Sysm Sys1 Sys2 Figure 1: ONE span prediction model (S PAN NER) finishes TWO things: (1) named entity recognition (2) combination of different NER systems. Introduction ∗ SpanNer &lt;latexit sha1_base64=""CvJCMPVwdVTi+QwayXJ4kUndUbo="">AAAC0XicjVHLSsNAFD3GV+uz6tJNUBRxUZJudCm4cVmp1UJ9MImjDubFzEQsRRC3/oBb/SZB/AO79AfEO9MIPhCdkOTMufecmXtvkEVCac97HnAGh4ZHRkvlsfGJyanpyszsr"
2021.acl-long.558,2020.acl-main.519,0,0.190268,"5N1UpWbl20toE7/KUNGDz5zgXQf20ZBol84omfYJZZLGPAxzTPM9QxiUqqJE3xyOe8KxVtbF2p93PSrVMqtnDt9AePgFrq5a0&lt;/latexit> how researchers formulate the task. For example, recent years have seen frequent paradigm shifts for the task of named entity recognition (NER) from token-level tagging, which conceptualize NER as a sequence labeling (S EQ L AB) task (Chiu and Nichols, 2015; Huang et al., 2015; Ma and Hovy, 2016; Lample et al., 2016; Akbik et al., 2018; Peters et al., 2018; Devlin et al., 2018; Xia et al., 2019; Luo et al., 2020; Lin et al., 2020; Fu et al., 2021), to span-level prediction (S PAN N ER) (Li et al., 2020; Mengge et al., 2020; Jiang et al., 2020; Ouchi et al., 2020; Yu et al., 2020), which regards NER either as question answering (Li et al., 2020; Mengge et al., 2020), span classification (Jiang et al., 2020; Ouchi et al., 2020; Yamada et al., 2020), and dependency parsing tasks (Yu et al., 2020). However, despite the success of span predictionbased systems, as a relatively newly-explored framework, the understanding of its architectural bias has not been fully understood so far. For example, what are the complementary advantages compared with S EQ L AB frameworks and how to make full use of t"
2021.acl-long.558,2020.acl-main.752,0,0.0151927,"CgyJ/Us3r/yvTvYi0Me56sGlniLFyO6c1CVRtyJPrn/pSpBDRJzEPcpzwo5Szu9ZV5pY9S7v1lL5N1UpWbl20toE7/KUNGDz5zgXQf20ZBol84omfYJZZLGPAxzTPM9QxiUqqJE3xyOe8KxVtbF2p93PSrVMqtnDt9AePgFrq5a0&lt;/latexit> how researchers formulate the task. For example, recent years have seen frequent paradigm shifts for the task of named entity recognition (NER) from token-level tagging, which conceptualize NER as a sequence labeling (S EQ L AB) task (Chiu and Nichols, 2015; Huang et al., 2015; Ma and Hovy, 2016; Lample et al., 2016; Akbik et al., 2018; Peters et al., 2018; Devlin et al., 2018; Xia et al., 2019; Luo et al., 2020; Lin et al., 2020; Fu et al., 2021), to span-level prediction (S PAN N ER) (Li et al., 2020; Mengge et al., 2020; Jiang et al., 2020; Ouchi et al., 2020; Yu et al., 2020), which regards NER either as question answering (Li et al., 2020; Mengge et al., 2020), span classification (Jiang et al., 2020; Ouchi et al., 2020; Yamada et al., 2020), and dependency parsing tasks (Yu et al., 2020). However, despite the success of span predictionbased systems, as a relatively newly-explored framework, the understanding of its architectural bias has not been fully understood so far. For example, what are the complementary a"
2021.acl-long.558,2021.acl-demo.34,1,0.751707,"vantages of systems based on different paradigms. We then reveal that span prediction, simultaneously, can serve as a system combiner to re-recognize named entities from different systems’ outputs. We experimentally implement 154 systems on 11 datasets, covering three languages, comprehensive results show the effectiveness of span prediction models that both serve as base NER systems and system combiners. We make all code and datasets available: https:// github.com/neulab/spanner, as well as an online system demo: http://spanner. sh. Our model also has been deployed into the E XPLAINA B OARD (Liu et al., 2021) platform, which allows users to flexibly perform the system combination of top-scoring systems in an interactive way: http://explainaboard. nlpedia.ai/leaderboard/task-ner/. 1 &lt;latexit sha1_base64=""DZv0jzXJT2tUzVZoBGtnbFfy+Jg="">AAAC1XicjVHLSsNAFD2Nr1pfUZdugkXoqiTd6LLgxpVUtA+opSTTaQ1NkzCZFEvpTtz6A271l8Q/0L/wzpiCWkQnJDlz7j1n5t7rxYGfSNt+zRlLyyura/n1wsbm1vaOubvXSKJUMF5nURCJlucmPPBDXpe+DHgrFtwdeQFvesNTFW+OuUj8KLySk5h3Ru4g9Ps+cyVRXdO8lvxWJmx6GbvhORezrlm0y7Ze1iJwMlBEtmqR+YJr9BCBIcUIHCEk4QAuEnracGAjJq6DKXGCkK/jHDMUSJtSFqcMl9ghfQe0a2dsSHvlmWg1o1MCegUpLRyRJqI8QVidZul4qp0V+5v3VHuqu03o72VeI2Ilboj9SzfP/K9"
2021.acl-long.558,D19-1387,0,0.0209121,"GiVDhb4IzBQUy7mPI1BUwsIL2ughhIMEPhgCCMIeLMT0tGDCQERcBxPiOCFX5RmmyJM2oSpGFRaxI/oOaNVK2YDW0jNWaod28ejlpNRxSJqQ6jhhuZuu8olyluxv3hPlKc82pr+devnECgyJ/Us3r/yvTvYi0Me56sGlniLFyO6c1CVRtyJPrn/pSpBDRJzEPcpzwo5Szu9ZV5pY9S7v1lL5N1UpWbl20toE7/KUNGDz5zgXQf20ZBol84omfYJZZLGPAxzTPM9QxiUqqJE3xyOe8KxVtbF2p93PSrVMqtnDt9AePgHdHJZ4&lt;/latexit> The rapid evolution of neural architectures (Kalchbrenner et al., 2014a; Kim, 2014; Hochreiter and Schmidhuber, 1997) and large pre-trained models (Devlin et al., 2019; Lewis et al., 2020) not only drive the state-of-the-art performance of many NLP tasks (Devlin et al., 2019; Liu and Lapata, 2019) to a new level but also change the way † This work is done when Jinlan visited CMU remotely. Corresponding author. SeqLab ... Sysm Sys1 Sys2 Figure 1: ONE span prediction model (S PAN NER) finishes TWO things: (1) named entity recognition (2) combination of different NER systems. Introduction ∗ SpanNer &lt;latexit sha1_base64=""CvJCMPVwdVTi+QwayXJ4kUndUbo="">AAAC0XicjVHLSsNAFD3GV+uz6tJNUBRxUZJudCm4cVmp1UJ9MImjDubFzEQsRRC3/oBb/SZB/AO79AfEO9MIPhCdkOTMufecmXtvkEVCac97HnAGh4ZHRkvlsfGJyanpyszsrkpzGfJmmEapbAVM8UgkvKmFjngrk5zFQcT3gvNNE9+74FKJNNnRnYwfxOw0ESciZJqow/2Y6TMZdxsddXVUO6oselXPLvcn8AuwuFF+Xe5dPr7"
2021.acl-long.558,N16-1133,0,0.0621377,"Missing"
2021.acl-long.558,D14-1162,0,0.0874003,"nction to get the probability w.r.t label y. score(si , y) P(y|si ) = X , score(si , y0 ) S PAN N ER as NER System Overall, the span prediction-based framework for NER consists of three major modules: token representation layer, span representation layer, and span prediction layer. 3.1.1 Token Representation Layer Given a sentence X = {x1 , · · · , xn } with n tokens, the token representation hi is as follows: u1 , · · · , un = E MB(x1 , · · · , xn ), h1 , · · · , hn = B I LSTM(u1 , · · · , un ), (1) (2) where E MB(·) is the pre-trained embeddings, such as non-contextualized embeddings GloVe (Pennington et al., 2014) or contextualized pre-trained embeddings BERT (Devlin et al., 2018). B I LSTM is the bidirectional LSTM (Hochreiter and Schmidhuber, 1997). 3.1.2 Span Representation Layer First, we enumerate all the possible m spans S = {s1 , · · · , si , · · · , sm } for sentence X = {x1 , · · · , xn } and then re-assign a label y ∈ Y for each span s. For example, for sentence: “London1 is2 beautiful3 ”, the possible span’s (start, end) indices are {(1, 1), (2, 2), (3, 3), (1, 2), (2, 3), (1, 3)}, and the labels of these spans are all “O” except (1, 1) (London) is “LOC”. We use bi and ei to denote the start"
2021.acl-long.558,N18-1202,0,0.250669,"mmyJM2oSpGFRaxI/oOaNVK2YDW0jNWaod28ejlpNRxSJqQ6jhhuZuu8olyluxv3hPlKc82pr+devnECgyJ/Us3r/yvTvYi0Me56sGlniLFyO6c1CVRtyJPrn/pSpBDRJzEPcpzwo5Szu9ZV5pY9S7v1lL5N1UpWbl20toE7/KUNGDz5zgXQf20ZBol84omfYJZZLGPAxzTPM9QxiUqqJE3xyOe8KxVtbF2p93PSrVMqtnDt9AePgFrq5a0&lt;/latexit> how researchers formulate the task. For example, recent years have seen frequent paradigm shifts for the task of named entity recognition (NER) from token-level tagging, which conceptualize NER as a sequence labeling (S EQ L AB) task (Chiu and Nichols, 2015; Huang et al., 2015; Ma and Hovy, 2016; Lample et al., 2016; Akbik et al., 2018; Peters et al., 2018; Devlin et al., 2018; Xia et al., 2019; Luo et al., 2020; Lin et al., 2020; Fu et al., 2021), to span-level prediction (S PAN N ER) (Li et al., 2020; Mengge et al., 2020; Jiang et al., 2020; Ouchi et al., 2020; Yu et al., 2020), which regards NER either as question answering (Li et al., 2020; Mengge et al., 2020), span classification (Jiang et al., 2020; Ouchi et al., 2020; Yamada et al., 2020), and dependency parsing tasks (Yu et al., 2020). However, despite the success of span predictionbased systems, as a relatively newly-explored framework, the understanding of its architectural bias has"
2021.acl-long.558,W03-0419,0,0.446523,"Missing"
2021.acl-long.558,W02-2024,0,0.279681,"t sequence and Y = {y1 , y2 , . . . , yT } is the output label (e.g., “B-PER”, “I-LOC”, “O”) sequence. The goal of this task is to accurately predict entities by assigning output label yt for each token xt . We take the F1-score1 as the evaluation metric for the NER task. 2.2 Datasets To make a comprehensive evaluation, in this paper, we use multiple NER datasets that cover different domains and languages. CoNLL-2003 2 (Sang and De Meulder, 2003) covers two different languages: English and German. Here, we only consider the English (EN) dataset collected from the Reuters Corpus. CoNLL-2002 3 (Sang, 2002) contains annotated corpus in Dutch (NL) collected from De Morgen news, and Spanish (ES) collected from Spanish EFE News Agency. We evaluate both languages. OntoNotes 5.0 4 (Weischedel et al., 2013) is a large corpus consisting of three different languages: English, Chinese, and Arabic, involving six genres: newswire (NW), broadcast news (BN), broadcast conversation (BC), magazine (MZ), web data (WB), and telephone conversation (TC). Following previous works (Durrett and Klein, 2014; Ghaddar and Langlais, 2018), we utilize different domains in English to test the robustness of proposed models."
2021.acl-long.558,P17-1076,0,0.0510281,"Missing"
2021.acl-long.558,P17-2060,0,0.0575205,"Missing"
2021.acl-long.558,P11-1125,0,0.0332102,"Missing"
2021.acl-long.558,W03-0433,0,0.512573,"l bias of the span prediction framework: it can not only be used as a base system for named entity recognition but also serve as a meta-system to combine multiple NER systems’ outputs. In other words, the span prediction model play two roles showing in Fig. 1: (i) as a base NER system; and (ii) as a system combiner of multiple base systems. We claim that compared with traditional ensemble learning of the NER task, S PAN N ER combiners are advantageous in the following aspects: 1. Most of the existing NER combiners rely on heavy feature engineering and external knowledge (Florian et al., 2003; Wu et al., 2003; Saha and Ekbal, 2013). Instead, the S PAN N ER models we proposed for system combination train in an end-to-end fashion. 2. Combining complementarities of different paradigms: most previous works perform NER system combination solely focusing on the sequence labeling framework. It is still an understudied topic how systems from different frameworks help each other. 3. No extra training overhead and flexibility of use: Existing ensemble learning algorithms are expensive, which usually need to collect training samples by k-fold cross-validation for system combiner (Speck and Ngomo, 2014), redu"
2021.acl-long.558,P19-1138,0,0.017933,"SJqQ6jhhuZuu8olyluxv3hPlKc82pr+devnECgyJ/Us3r/yvTvYi0Me56sGlniLFyO6c1CVRtyJPrn/pSpBDRJzEPcpzwo5Szu9ZV5pY9S7v1lL5N1UpWbl20toE7/KUNGDz5zgXQf20ZBol84omfYJZZLGPAxzTPM9QxiUqqJE3xyOe8KxVtbF2p93PSrVMqtnDt9AePgFrq5a0&lt;/latexit> how researchers formulate the task. For example, recent years have seen frequent paradigm shifts for the task of named entity recognition (NER) from token-level tagging, which conceptualize NER as a sequence labeling (S EQ L AB) task (Chiu and Nichols, 2015; Huang et al., 2015; Ma and Hovy, 2016; Lample et al., 2016; Akbik et al., 2018; Peters et al., 2018; Devlin et al., 2018; Xia et al., 2019; Luo et al., 2020; Lin et al., 2020; Fu et al., 2021), to span-level prediction (S PAN N ER) (Li et al., 2020; Mengge et al., 2020; Jiang et al., 2020; Ouchi et al., 2020; Yu et al., 2020), which regards NER either as question answering (Li et al., 2020; Mengge et al., 2020), span classification (Jiang et al., 2020; Ouchi et al., 2020; Yamada et al., 2020), and dependency parsing tasks (Yu et al., 2020). However, despite the success of span predictionbased systems, as a relatively newly-explored framework, the understanding of its architectural bias has not been fully understood so far. For e"
2021.acl-long.558,2020.emnlp-main.523,0,0.405558,"Missing"
2021.acl-long.558,2020.acl-main.577,0,0.0647271,"rVMqtnDt9AePgFrq5a0&lt;/latexit> how researchers formulate the task. For example, recent years have seen frequent paradigm shifts for the task of named entity recognition (NER) from token-level tagging, which conceptualize NER as a sequence labeling (S EQ L AB) task (Chiu and Nichols, 2015; Huang et al., 2015; Ma and Hovy, 2016; Lample et al., 2016; Akbik et al., 2018; Peters et al., 2018; Devlin et al., 2018; Xia et al., 2019; Luo et al., 2020; Lin et al., 2020; Fu et al., 2021), to span-level prediction (S PAN N ER) (Li et al., 2020; Mengge et al., 2020; Jiang et al., 2020; Ouchi et al., 2020; Yu et al., 2020), which regards NER either as question answering (Li et al., 2020; Mengge et al., 2020), span classification (Jiang et al., 2020; Ouchi et al., 2020; Yamada et al., 2020), and dependency parsing tasks (Yu et al., 2020). However, despite the success of span predictionbased systems, as a relatively newly-explored framework, the understanding of its architectural bias has not been fully understood so far. For example, what are the complementary advantages compared with S EQ L AB frameworks and how to make full use of them? Motivated by this, in this paper, we make two scientific contributions. We"
2021.acl-long.99,N15-1171,0,0.018176,"Although roll call data is the major resource for legislator behavior modeling, it has two limitations. Firstly, it fails to uncover detailed opinions of legislators towards legislative issues. Therefore, we have no clue about the motivation behind their voting. Secondly, it is unable to model the behavior of newly-elected legislators because their historical voting records are not available (i.e., coldstart problem). Meanwhile, researchers explore to use public statements to characterize the ideology of legislators with the guidance of framing theory (Entman, 1993; Chong and Druckman, 2007; Baumer et al., 2015; Vafa et al., 2020). Vafa et al. (2020) propose a text-based ideal point model to analyze tweets of legislators independent of roll call data. Experiment results show some correlations between distributions of ideal points learned 1236 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 1236–1246 August 1–6, 2021. ©2021 Association for Computational Linguistics from legislative data and public statements. However, they treat the two resources separately and fail to uncover dee"
2021.acl-long.99,2020.acl-main.476,0,0.0273936,"oth legislators and legislation in the same space, and voting behavior is characterized as the distance between them. However, this simple spatial model fails to predict votes on new legislation. Text-based models have emerged to address this issue. Gerrish and Blei (2011, 2012); Gu et al. (2014); Nguyen et al. (2015) extended ideal point model with latent topics and issue-adjusted methods. Some embedding methods (Kraft et al., 2016) also promote learning of legislators. More recently, external context information including party, sponsor and donors (Kornilova et al., 2018; Yang et al., 2020; Davoodi et al., 2020) have been introduced to better describe the legislative process. Since votes are not the only way to express political preferences, other sources of data including speech and knowledge graph (Budhwar et al., 2018; Gentzkow et al., 2019; Patil et al., 2019; Vafa et al., 2020) have been applied to estimate ideology. Although previous studies (Bruns and Highfield, 2013; Golbeck and Hansen, 2014; Barber´a, 2015; Peng et al., 2016; Wong et al., 2016; Boutyline and Willer, 2017; Johnson et al., 2017) have incorporated social network of following or retweeting on Twitter to learn legislators, fine-g"
2021.acl-long.99,N19-1423,0,0.00883048,"nodes and six types of relations with two categories (relations between homogeneous nodes and relations between heterogeneous nodes). We will introduce the structure of the graph in this subsection. 3.1.1 Initialization of Nodes Legislator Nodes We follow Yang et al. (2020) to map each legislator to a continuous low-dimension vector, utilizing information of member ID, state and party. The legislator representation is Xm = eID ⊕ eP arty ⊕ eState Legislation Nodes For legislation, we pay attention to title and description and represent each legislation by sentence embedding generated by BERT (Devlin et al., 2019). Thus, the legislation representation is Xl = BERT (title + description) Hashtag Nodes To represent a hashtag, we randomly choose K tweets with the tag and use BERT to get sentence embedding of each tweet text. After that, we take the average of these vectors, Xt = Avg(BERT (tweeti )) i = 1, 2, ...K 3.1.2 Relations between Homogeneous Nodes R1: Co-sponsorship of Legislators Each legislation is initialized by a sponsor and several cosponsors. Previous study (Yang et al., 2020) has proved the effectiveness of modeling cosponsorship in legislator representation learning. Obviously, more legislat"
2021.acl-long.99,P17-1069,0,0.0253998,"Missing"
2021.acl-long.99,P18-2081,0,0.0746966,"ur model with some state-of-the-art approaches. - majority is a baseline which assumes all legislators vote yea. - ideal-point-wf (Gerrish and Blei, 2011): a regression model that takes the word frequency of legislation text as features. The training paradigm follows the traditional ideal point model. Thus, it can only predict on legislators present in the training data. - ideal-point-tfidf : similar to ideal-point-wf, it uses TFIDF of legislation text as features instead. - ideal-vector (Kraft et al., 2016): it learns multidimensional ideal vectors for legislators based on bill texts. - CNN (Kornilova et al., 2018): it uses CNN to encode legislation. - CNN+meta (Kornilova et al., 2018): on the basis of CNN, it adds percentage of sponsors of different parties as bill’s authorship information. - LSTM+GCN (Yang et al., 2020): it uses LSTM to encode legislation and applies a GCN to update representations of legislators. - Vote: the single task of roll call vote in our framework. - Ours: our framework. 4.2 Overall Performance We report the average accuracy of all experiment sets following Kornilova et al. (2018); Yang et al. (2020). Besides, macro F1 score is also provided for more information. Table 1 shows"
2021.acl-long.99,D16-1221,0,0.401345,"tional graph neural networks to learn the representation of legislators with the guidance of historical records of their voting and hashtag usage. Experiment results indicate that our model yields significant improvements for the task of roll call vote prediction. Further analysis further demonstrates that legislator representation we learned captures nuances in statements. 1 Figure 1: An illustration of correspondence of vote behavior and public statements on Twitter. Supporters of the abortion-banning legislation frequently mention the tag life while opponents focus on choice. et al., 2014; Kraft et al., 2016) and report positive results for roll call vote prediction. Introduction Modeling the behavior of legislators is one of the most important topics of quantitative political science. Existing researches largely rely on roll call data, i.e. historical voting records, to estimate the political preference of legislators. The most widely used approach for roll call data analysis is ideal point model (Clinton et al., 2004) that represents legislators and legislation as points in a one-dimension latent space. Researchers enhance ideal point model by incorporating textual information of legislation (Ge"
2021.acl-long.99,P15-1139,0,0.0101923,"ccurately related to a specific bill in an automatic and complete way, we will explore the frequency of inconsistency in the future. 6 Related Work Ideal point estimation has become a mainstream approach to model ideology of legislators. Classical ideal point model (Clinton et al., 2004) represents both legislators and legislation in the same space, and voting behavior is characterized as the distance between them. However, this simple spatial model fails to predict votes on new legislation. Text-based models have emerged to address this issue. Gerrish and Blei (2011, 2012); Gu et al. (2014); Nguyen et al. (2015) extended ideal point model with latent topics and issue-adjusted methods. Some embedding methods (Kraft et al., 2016) also promote learning of legislators. More recently, external context information including party, sponsor and donors (Kornilova et al., 2018; Yang et al., 2020; Davoodi et al., 2020) have been introduced to better describe the legislative process. Since votes are not the only way to express political preferences, other sources of data including speech and knowledge graph (Budhwar et al., 2018; Gentzkow et al., 2019; Patil et al., 2019; Vafa et al., 2020) have been applied to"
2021.acl-long.99,K19-1053,0,0.0400706,"nd Blei (2011, 2012); Gu et al. (2014); Nguyen et al. (2015) extended ideal point model with latent topics and issue-adjusted methods. Some embedding methods (Kraft et al., 2016) also promote learning of legislators. More recently, external context information including party, sponsor and donors (Kornilova et al., 2018; Yang et al., 2020; Davoodi et al., 2020) have been introduced to better describe the legislative process. Since votes are not the only way to express political preferences, other sources of data including speech and knowledge graph (Budhwar et al., 2018; Gentzkow et al., 2019; Patil et al., 2019; Vafa et al., 2020) have been applied to estimate ideology. Although previous studies (Bruns and Highfield, 2013; Golbeck and Hansen, 2014; Barber´a, 2015; Peng et al., 2016; Wong et al., 2016; Boutyline and Willer, 2017; Johnson et al., 2017) have incorporated social network of following or retweeting on Twitter to learn legislators, fine-grained attitudes of legislators remain unknown since the texts themselves have not been mined. Until recently, Preot¸iuc-Pietro et al. (2017) started to analyze linguistic differences between ideologically different groups using a broad range of handcrafte"
2021.acl-long.99,P17-1068,0,0.0606298,"Missing"
2021.acl-long.99,2020.emnlp-main.46,0,0.224288,"crease in the Federal minimum wage”, the vote-based model predicts that Senator Harry Reid will vote nay, which is also the ground truth. But our model wrongly predicts that he will vote yea. We probe into his tweets and find that he 1243 used #raisethewage frequently to call for raise in minimum wage, as those who support the bill. On the one hand, hashtags may have difficulty capturing more fine-grained decisions, which can be influenced by various factors; on the other hand, legislators may behave differently from what they say, since they may make certain statements to get public support (Spell et al., 2020). When legislators do not accord their words to deed, our model may be misled by legislators’ statements. As it’s difficult to find hashtags directly and accurately related to a specific bill in an automatic and complete way, we will explore the frequency of inconsistency in the future. 6 Related Work Ideal point estimation has become a mainstream approach to model ideology of legislators. Classical ideal point model (Clinton et al., 2004) represents both legislators and legislation in the same space, and voting behavior is characterized as the distance between them. However, this simple spati"
2021.acl-long.99,2020.acl-main.475,0,0.0676393,"ata is the major resource for legislator behavior modeling, it has two limitations. Firstly, it fails to uncover detailed opinions of legislators towards legislative issues. Therefore, we have no clue about the motivation behind their voting. Secondly, it is unable to model the behavior of newly-elected legislators because their historical voting records are not available (i.e., coldstart problem). Meanwhile, researchers explore to use public statements to characterize the ideology of legislators with the guidance of framing theory (Entman, 1993; Chong and Druckman, 2007; Baumer et al., 2015; Vafa et al., 2020). Vafa et al. (2020) propose a text-based ideal point model to analyze tweets of legislators independent of roll call data. Experiment results show some correlations between distributions of ideal points learned 1236 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 1236–1246 August 1–6, 2021. ©2021 Association for Computational Linguistics from legislative data and public statements. However, they treat the two resources separately and fail to uncover deep relationships of b"
2021.acl-short.56,N19-1423,0,0.0283701,"clean data. 2 2.1 Method Problem Definition Chinese Spelling Correction aims to identify incorrectly used characters in Chinese texts and giving its correct version. Given an input Chinese sentence X = {x1 , ..., xn } consisting of n characters, which may contain some spelling errors, the model takes X as input and outputs an output sentence Y = {y1 , ..., yn }, where all the incorrect characters are expected to be corrected. This task can be formulated as a conditional generation problem by modeling and maximizing the conditional probability of P (Y |X). 2.2 Base Models We use vanilla BERT (Devlin et al., 2019) and two recently proposed BERT-based models (Cheng et al., 2020b; Zhang et al., 2020) as our base models. When applying BERT to the CSC task, the input is a sentence with spelling errors, and the output representations are fed into an output layer to predict target tokens. We tie the input and output embedding layer, and all the parameters are fine-tuned using task-specific corpora. Soft-Masked BERT (Zhang et al., 2020) uses a Bi-GRU network to detect errors, and applies a BERT-based network to correct errors. SpellGCN (Cheng et al., 2020b) utilizes visual and phonological similarity knowledg"
2021.acl-short.56,D19-5522,0,0.50589,"n and robustness of multiple CSC models across three different datasets, achieving stateof-the-art performance for CSC task.1 1 Introduction Chinese Spelling Correction (CSC) aims to detect and correct spelling mistakes in Chinese texts. Many Chinese characters are visually or phonologically similar, while their semantic meaning may differ greatly. Spelling errors are usually caused by careless writing, automatic speech recognition, and optical character recognition systems. The CSC task has received steady attention over the past two decades (Chang, 1995; Xin et al., 2014; Wang et al., 2018; Hong et al., 2019). Unlike English, Chinese texts are written without using whitespace to delimit words, and it is hard to identify whether and which characters are misspelled without the information of word boundaries. The context information should be taken into account to reconstruct † These authors contributed equally to this work. Author 1 The source codes are available at https://github.com/ FDChongli/TwoWaysToImproveCSC. * Corresponding the word boundaries when correcting spelling mistakes, which makes CSC a long-standing challenge for Chinese NLP community. Many early CSC systems follow the same recipe"
2021.acl-short.56,D17-1215,0,0.067838,"Missing"
2021.acl-short.56,P15-1152,0,0.0513775,"Missing"
2021.acl-short.56,2020.acl-main.81,0,0.452913,"challenge for Chinese NLP community. Many early CSC systems follow the same recipe with minor variations, adopting a three-step strategy: detect the positions of spelling errors; generate candidate characters for these positions; and select a most appropriate one from the candidates to replace the misspelling (Yeh et al., 2013; Yu and Li, 2014; Zhang et al., 2015; Wang et al., 2019). Recently, a sequence-to-sequence (seq2seq) learning framework with neural networks has empirically proven to be effective for CSC, which transforms a sentence with errors to the corrected one (Zhang et al., 2020; Cheng et al., 2020b). However, even if training a CSC model with the seq2seq framework normally requires a huge amount of high-quality training data, it is still unreasonable to assume that all possible spelling errors have been covered by the confusion sets (i.e. a set of characters and their visually or phonologically similar characters which can be potentially confused) extracted from the training samples. New spelling errors occur everyday. A good CSC model should be able to exploit what it has already seen in the training instances in order to achieve reasonable performance on easy spelling mistakes, but i"
2021.acl-short.56,D18-1273,0,0.577173,"h the generalization and robustness of multiple CSC models across three different datasets, achieving stateof-the-art performance for CSC task.1 1 Introduction Chinese Spelling Correction (CSC) aims to detect and correct spelling mistakes in Chinese texts. Many Chinese characters are visually or phonologically similar, while their semantic meaning may differ greatly. Spelling errors are usually caused by careless writing, automatic speech recognition, and optical character recognition systems. The CSC task has received steady attention over the past two decades (Chang, 1995; Xin et al., 2014; Wang et al., 2018; Hong et al., 2019). Unlike English, Chinese texts are written without using whitespace to delimit words, and it is hard to identify whether and which characters are misspelled without the information of word boundaries. The context information should be taken into account to reconstruct † These authors contributed equally to this work. Author 1 The source codes are available at https://github.com/ FDChongli/TwoWaysToImproveCSC. * Corresponding the word boundaries when correcting spelling mistakes, which makes CSC a long-standing challenge for Chinese NLP community. Many early CSC systems fol"
2021.acl-short.56,P19-1578,0,0.285014,"Missing"
2021.acl-short.56,2020.emnlp-main.228,1,0.745446,"Missing"
2021.acl-short.56,W13-4406,0,0.514208,"Missing"
2021.acl-short.56,W14-6825,0,0.0252265,"gy can improve both the generalization and robustness of multiple CSC models across three different datasets, achieving stateof-the-art performance for CSC task.1 1 Introduction Chinese Spelling Correction (CSC) aims to detect and correct spelling mistakes in Chinese texts. Many Chinese characters are visually or phonologically similar, while their semantic meaning may differ greatly. Spelling errors are usually caused by careless writing, automatic speech recognition, and optical character recognition systems. The CSC task has received steady attention over the past two decades (Chang, 1995; Xin et al., 2014; Wang et al., 2018; Hong et al., 2019). Unlike English, Chinese texts are written without using whitespace to delimit words, and it is hard to identify whether and which characters are misspelled without the information of word boundaries. The context information should be taken into account to reconstruct † These authors contributed equally to this work. Author 1 The source codes are available at https://github.com/ FDChongli/TwoWaysToImproveCSC. * Corresponding the word boundaries when correcting spelling mistakes, which makes CSC a long-standing challenge for Chinese NLP community. Many ea"
2021.acl-short.56,W13-4407,0,0.0650514,"Missing"
2021.acl-short.56,W14-6835,0,0.684631,"Missing"
2021.acl-short.56,2020.acl-main.82,0,0.698105,"CSC a long-standing challenge for Chinese NLP community. Many early CSC systems follow the same recipe with minor variations, adopting a three-step strategy: detect the positions of spelling errors; generate candidate characters for these positions; and select a most appropriate one from the candidates to replace the misspelling (Yeh et al., 2013; Yu and Li, 2014; Zhang et al., 2015; Wang et al., 2019). Recently, a sequence-to-sequence (seq2seq) learning framework with neural networks has empirically proven to be effective for CSC, which transforms a sentence with errors to the corrected one (Zhang et al., 2020; Cheng et al., 2020b). However, even if training a CSC model with the seq2seq framework normally requires a huge amount of high-quality training data, it is still unreasonable to assume that all possible spelling errors have been covered by the confusion sets (i.e. a set of characters and their visually or phonologically similar characters which can be potentially confused) extracted from the training samples. New spelling errors occur everyday. A good CSC model should be able to exploit what it has already seen in the training instances in order to achieve reasonable performance on easy spel"
2021.acl-short.56,W15-3107,0,0.0545434,"Missing"
2021.acl-short.56,D18-1316,0,0.036525,"Missing"
2021.findings-acl.121,P19-1279,0,0.0206704,"dopt a large-scale relation classification dataset TACRED (Zhang et al., 2017). To fine-tune our models for this task, we modify the input token sequence by adding special token “@” before and after the first entity, adding “#” before and after the second entity. Then the token representations of the first special token “@” and “#” are concatenated to perform relation classification. We adopt micro F1 score as the metric to represent the model performance as previous works. Baselines C-GCN (Zhang et al., 2018) employs graph convolutional networks to model dependency trees. BERT-large (Baldini Soares et al., 2019) is a baseline BERT-large model. BERT+MTB (Baldini Soares et al., 2019) is a method of training relation representation without supervision from a knowledge base by matching the blanks. Other baseline models are described in Section 2 and 4.1. Results and Discussion Table 4 shows the performances of different models on TACRED. The results indicate that K-A DAPTER models significantly outperform all baselines, which directly demonstrate our models can benefit relation classification. In particular, (1) K-A DAPTER models outperform RoBERTa, which proves the effectiveness of infusing knowledge in"
2021.findings-acl.121,P18-1009,0,0.0216531,"d analyses with the case study and probing experiments to explore the effectiveness and ability of models for learning factual knowledge. The notations of K-A DAPTER (F+L), K-A DAPTER (F), and K-A DAPTER (L) denote our model which consists of both factual adapter and linguistic adapter, only factual adapter and only linguistic adapter, respectively. Implementation details, and statistics of datasets are in the Appendix. 4.1 Entity Typing We conduct experiments on fine-grained entity typing which aims to predict the types of a given entity and its context. We evaluate our models on OpenEntity (Choi et al., 2018) and FIGER (Ling et al., 2015) following the same split setting as Zhang et al. (2019). To fine-tune our models for entity typing, we modify the input token sequence by adding the special token “@” before and after a certain entity, then the first “@” special token representation is adopted to perform classification. As for OpenEntity, we adopt micro F1 score as the final metric to represent the model performance. As for FIGER, we adopt strict accuracy, loose macro, loose micro F1 scores (Ling and Weld, 2012) for evaluation following the same evaluation criteria used in previous works. Baselin"
2021.findings-acl.121,P19-1285,0,0.0618623,"Missing"
2021.findings-acl.121,N19-1423,0,0.0429486,"texttriplets on Wikipedia and Wikidata and (2) linguistic knowledge obtained via dependency parsing. Results on three knowledge-driven tasks, including relation classification, entity typing, and question answering, demonstrate that each adapter improves the performance and the combination of both adapters brings further improvements. Further analysis indicates that K-A DAPTER captures versatile knowledge than RoBERTa. 1 1 Introduction Language representation models, which are pretrained on large-scale text corpus through unsupervised objectives like (masked) language modeling, such as BERT (Devlin et al., 2019), GPT (Radford et al., 2018, 2019), XLNet (Yang et al., ∗ Work is done during internship at Microsoft. Zhongyu Wei and Duyu Tang are corresponding authors. 1 Codes are publicly available at https://github. com/microsoft/K-Adapter 2019), RoBERTa (Liu et al., 2019) and T5 (Raffel et al., 2020), have established state-of-the-art performances on various NLP downstream tasks. Despite the huge success of these pre-trained models in empirical studies, recent studies suggest that models learned in such an unsupervised manner struggle to capture rich knowledge. For example, Poerner et al. (2020) sugges"
2021.findings-acl.121,L18-1544,0,0.0249766,"ayer, we denote the number of transformer layer as N , the hidden dimension of transformer layer as HA , the number of self-attention heads as AA , the hidden dimension of down-projection and up-projection layers as Hd and Hu . In detail, we have the following adapter size: N = 2, HA = 768, AA = 12, Hu = 1024 and Hd = 768. The RoBERTa lay2 3.3 Factual Adapter Factual knowledge can be described as the basic information that is concerned with facts. In this work, we acquire factual knowledge from the relationships among entities in natural language. We extract a sub-dataset T-REx-rc from T-REx (ElSahar et al., 2018) which is a large scale alignment dataset between Wikipedia abstracts and Wikidata triples. We discard all relations having less than 50 entity pairs, collecting 430 relations and 5.5M sentences. In order to inject factual knowledge, we propose to pre-train a knowledge-specific adapter called facAdapter on the relation classification task. This task requires a model to classify relation labels of given entity pairs based on context. Specifically, the last hidden features of RoBERTa and facAdapter are concatenated as the input representation, and the pooling layer is applied to the input repres"
2021.findings-acl.121,2020.findings-emnlp.71,0,0.198207,"Missing"
2021.findings-acl.121,W16-1313,0,0.118964,"-the-shell dependency parser from Stanford Parser3 on a part of Book Corpus (Zhu et al., 2015). To inject linguistic knowledge, we pre-train another knowledge-specific adapter called linAdapter on the task of dependency relation prediction. This task aims to predict the head index of each token in the given sentence. We concatenate the last hidden features of RoBERTa and linAdapter as the input representation, and then apply a linear layer 3 https://github.com/huggingface/transformers 1408 http://nlp.stanford.edu/software/lex-parser.html OpenEntity Model FIGER P R Mi-F1 Acc Ma-F1 Mi-F1 NFGEC (Shimaoka et al., 2016) BERT-base (Zhang et al., 2019) ERNIE (Zhang et al., 2019) KnowBERT (Peters et al., 2019) KEPLER (Wang et al., 2021) WKLM (Xiong et al., 2020) 68.80 76.37 78.42 78.60 77.20 - 53.30 70.96 72.90 73.70 74.20 - 60.10 73.56 75.56 76.10 75.70 - 55.60 52.04 57.19 60.21 75.15 75.16 75.61 81.99 71.73 71.63 73.39 77.00 RoBERTa RoBERTa + multitask K-A DAPTER (w/o knowledge) K-A DAPTER (F) K-A DAPTER (L) K-A DAPTER (F+L) 77.55 77.96 74.47 79.30 80.01 78.99 74.95 76.00 74.91 75.84 74.00 76.27 76.23 76.97 76.17 77.53 76.89 77.61 56.31 59.86 56.93 59.50 61.10 61.81 82.43 84.45 82.56 84.52 83.61 84.87 77.83 7"
2021.findings-acl.121,D18-1244,0,0.0240646,"ion classification aims to determine the correct relation between two entities in a given sentence. We adopt a large-scale relation classification dataset TACRED (Zhang et al., 2017). To fine-tune our models for this task, we modify the input token sequence by adding special token “@” before and after the first entity, adding “#” before and after the second entity. Then the token representations of the first special token “@” and “#” are concatenated to perform relation classification. We adopt micro F1 score as the metric to represent the model performance as previous works. Baselines C-GCN (Zhang et al., 2018) employs graph convolutional networks to model dependency trees. BERT-large (Baldini Soares et al., 2019) is a baseline BERT-large model. BERT+MTB (Baldini Soares et al., 2019) is a method of training relation representation without supervision from a knowledge base by matching the blanks. Other baseline models are described in Section 2 and 4.1. Results and Discussion Table 4 shows the performances of different models on TACRED. The results indicate that K-A DAPTER models significantly outperform all baselines, which directly demonstrate our models can benefit relation classification. In part"
2021.findings-acl.121,D17-1004,0,0.069357,"Missing"
2021.findings-acl.121,P19-1139,0,0.231713,"s suggest that models learned in such an unsupervised manner struggle to capture rich knowledge. For example, Poerner et al. (2020) suggest that although language models do well in reasoning about the surface form of entity names, they fail in capturing rich factual knowledge. Kassner and Sch¨utze (2020) observe that BERT mostly did not learn the meaning of negation (e.g. “not”). These observations motivate us to study the injection of knowledge into pre-trained models like BERT and RoBERTa. Recently, some efforts have been made to exploit injecting knowledge into pre-trained language models (Zhang et al., 2019; Lauscher et al., 2019; Levine et al., 2020; Peters et al., 2019; He et al., 2020; Xiong et al., 2020). Most previous works (as shown in Table 1) augment the standard language modeling objective with knowledge-driven objectives and update the entire model parameters. Although these methods obtain better performance on downstream tasks, they struggle at supporting the development of versatile models with multiple kinds of knowledge injected (Kirkpatrick et al., 2017). When new kinds of knowledge are injected, model parameters need to be retrained so that previously injected knowledge would fad"
2021.findings-acl.121,P17-1018,1,0.80079,"elected. We report accuracy scores obtained from the leaderboard. Open-domain QA aims to answer questions using external resources such as collections of documents and webpages. We evaluate our modes on two public datasets, i.e., Quasar-T (Dhingra et al., 2017) and SearchQA (Dunn et al., 2017). Specifically, we first retrieve paragraphs corresponding to the question using the information retrieval system and then extract the answer from these retrieved paragraphs through the reading comprehension technique. Following previous work(Lin et al., 2018), we use the retrieved paragraphs provided by Wang et al. (2017) for these two datasets. To fine-tune our models for this task, the input token sequence is modified as “<SEP>question </SEP>paragraph</SEP>”. We apply linear layers over the last hidden features of our model to predict the start and end position of the answer span. We adopt two metrics including ExactMatch (EM) and loose F1 (Ling and Weld, 2012) scores to evaluate our models. Baselines BERT-FTRACE+SW AG (Huang et al., 2019) is the BERT model sequentially fine-tuned on both RACE and SWAG datasets. BiDAF (Seo et al., 2017) adopts a bi-directional attention network. AQA (Buck et al., 2018) propo"
2021.findings-acl.189,D18-1216,0,0.162103,"standard attention (LSTM-ATT) infers a correct sentiment label while pays more attention to the irrelevant word “movie”, making the result difﬁcult to explain. In general, the attention weights are only optimized to encode the task-relevant information while are not restricted to imitate human behavior. In order to enhance the interpretability of the attention mechanism, recent studies turn to integrate the human provided explanation signals into the attention models. Rei and Søgaard (2018) regularized the attention weights with a small amount of word-level annotations. Barrett et al. (2018); Bao et al. (2018) improved the explanation of attention by aligning explanations with human-provided rationales. These methods rely on additional labour consuming labelling for enhancing explanations, which is hard to extend to other datasets or tasks. In this paper, we aim to train a more efﬁcient and effective interpretable attention model without any pre-deﬁned annotations or pre-collected explanations. Speciﬁcally, we propose a framework consisting of a learner and a compressor, which enhances the performance and interpretability of the attention model for text classiﬁcation1 . The learner learns text repr"
2021.findings-acl.189,K18-1030,0,0.0939096,"term memory model with standard attention (LSTM-ATT) infers a correct sentiment label while pays more attention to the irrelevant word “movie”, making the result difﬁcult to explain. In general, the attention weights are only optimized to encode the task-relevant information while are not restricted to imitate human behavior. In order to enhance the interpretability of the attention mechanism, recent studies turn to integrate the human provided explanation signals into the attention models. Rei and Søgaard (2018) regularized the attention weights with a small amount of word-level annotations. Barrett et al. (2018); Bao et al. (2018) improved the explanation of attention by aligning explanations with human-provided rationales. These methods rely on additional labour consuming labelling for enhancing explanations, which is hard to extend to other datasets or tasks. In this paper, we aim to train a more efﬁcient and effective interpretable attention model without any pre-deﬁned annotations or pre-collected explanations. Speciﬁcally, we propose a framework consisting of a learner and a compressor, which enhances the performance and interpretability of the attention model for text classiﬁcation1 . The learn"
2021.findings-acl.189,N19-1423,0,0.0132863,"represents Kullback-Leibler divergence. Speciﬁcally, we regard ppyq as constant and then minimize Epθ py,zq rlog qφ py |zqs. Since we must ﬁrst sample r to sample y, z from pθ pr, y, zq, the lower bound of IpZ; Y q is computed as, IpZ; Y q ě Eppr,yq rEpθ pz|rq rlog qφ py |zqss IpZ;Rq upper bound hkkkkkkkkkkkkkkkkkkkkkikkkkkkkkkkkkkkkkkkkkkj hkkkkkkkkkkkkkkkkkkkkkikkkkkkkkkkkkkkkkkkkkkj pθ pz |rq pθ pz |rq Epprq rEpθ pz|rq rlog ss ´ Epprq rEpθ pz|rq rlog ss rψ pzq ppzq Experiment Setup We adopt two typical neural network models, attention-based LSTM (Hochreiter and Schmidhuber, 1997) and BERT (Devlin et al., 2019), to explore our VAT algorithm. 4.1 Datasets and Baselines Datasets To evaluate the effectiveness of our VAT model, we conduct the experiments over eight benchmark datasets: IMDB (Maas et al., 2011), Stanford Sentiment Treebank with (includes SST1 and its binary version SST-2) (Socher et al., 2013), Yelp (Zhang et al., 2015), AG News (Zhang et al., 2015), TREC (Li and Roth, 2002), subjective/objective classiﬁcation Subj (Pang and Lee, 2005) and Twitter (Rosenthal et al., 2015, 2014). The statistics information of these datasets are shown in Table 1. 2155 Class Length #train #dev #test IMDB 2 2"
2021.findings-acl.189,N19-1357,0,0.0691095,"ce but also the interpretability of the models (Mullenbach et al., 2018; Xie et al., 2017; Xu et al., 2015). Li et al. (2016) pointed the view: “Attention provides an important way to explain the workings of neural models”. Additionally, Wiegreffe and Pinter (2019) showed that attention mechanisms could help understand the inner workings of a model. The basic assumption of understanding of models with attention scores is that the inputs (e.g., words) with high attentive weights are essential for making decisions. However, as far as we know, it has not been formally veriﬁed. Existing research (Jain and Wallace, 2019) also shows that attention is not explicable, and there are a lot of controversy regarding to the result explanations (Wiegreffe and Pinter, 2019; Jain and Wallace, 2019). Moreover, we ﬁnd that though the attention mechanism can help improve the performance for text classiﬁcation in our experiments, it may focus on the irrelevant information. For example, in the sentence “A very funny movie.”, the long short-term memory model with standard attention (LSTM-ATT) infers a correct sentiment label while pays more attention to the irrelevant word “movie”, making the result difﬁcult to explain. In ge"
2021.findings-acl.189,D19-1276,0,0.0857347,"the performance and interpretability of the attention model for text classiﬁcation1 . The learner learns text representations by ﬁne-tuning 1 We focus on the task of text classiﬁcation, but our method can be easily extended to other NLP or CV tasks with attention mechanisms. 2152 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 2152–2161 August 1–6, 2021. ©2021 Association for Computational Linguistics the encoder. Regarding to the compressor, we are motivated by the effectiveness of the information bottleneck (IB) (Tishby et al., 1999) to enhance performance (Li and Eisner, 2019) or detect important features (Bang et al., 2019; Chen and Ji, 2020; Jiang et al., 2020; Schulz et al., 2020), and present a Variational information bottleneck ATtention (VAT) mechanism using IB to keep the most relevant clues and forget the irrelevant ones for better attention explanations. In particular, IB is integrated into attention to minimize the mutual information (MI) with the input while preserving as much MI as possible with the output, which provides more accurate and reliable explanations by controlling the information ﬂow. To evaluate the effectiveness of our proposed approach, w"
2021.findings-acl.189,2020.emnlp-main.347,0,0.277583,"lassiﬁcation1 . The learner learns text representations by ﬁne-tuning 1 We focus on the task of text classiﬁcation, but our method can be easily extended to other NLP or CV tasks with attention mechanisms. 2152 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 2152–2161 August 1–6, 2021. ©2021 Association for Computational Linguistics the encoder. Regarding to the compressor, we are motivated by the effectiveness of the information bottleneck (IB) (Tishby et al., 1999) to enhance performance (Li and Eisner, 2019) or detect important features (Bang et al., 2019; Chen and Ji, 2020; Jiang et al., 2020; Schulz et al., 2020), and present a Variational information bottleneck ATtention (VAT) mechanism using IB to keep the most relevant clues and forget the irrelevant ones for better attention explanations. In particular, IB is integrated into attention to minimize the mutual information (MI) with the input while preserving as much MI as possible with the output, which provides more accurate and reliable explanations by controlling the information ﬂow. To evaluate the effectiveness of our proposed approach, we adapt two advanced neural models (LSTM and BERT) within the frame"
2021.findings-acl.189,P11-1015,0,0.0834101,"nd of IpZ; Y q is computed as, IpZ; Y q ě Eppr,yq rEpθ pz|rq rlog qφ py |zqss IpZ;Rq upper bound hkkkkkkkkkkkkkkkkkkkkkikkkkkkkkkkkkkkkkkkkkkj hkkkkkkkkkkkkkkkkkkkkkikkkkkkkkkkkkkkkkkkkkkj pθ pz |rq pθ pz |rq Epprq rEpθ pz|rq rlog ss ´ Epprq rEpθ pz|rq rlog ss rψ pzq ppzq Experiment Setup We adopt two typical neural network models, attention-based LSTM (Hochreiter and Schmidhuber, 1997) and BERT (Devlin et al., 2019), to explore our VAT algorithm. 4.1 Datasets and Baselines Datasets To evaluate the effectiveness of our VAT model, we conduct the experiments over eight benchmark datasets: IMDB (Maas et al., 2011), Stanford Sentiment Treebank with (includes SST1 and its binary version SST-2) (Socher et al., 2013), Yelp (Zhang et al., 2015), AG News (Zhang et al., 2015), TREC (Li and Roth, 2002), subjective/objective classiﬁcation Subj (Pang and Lee, 2005) and Twitter (Rosenthal et al., 2015, 2014). The statistics information of these datasets are shown in Table 1. 2155 Class Length #train #dev #test IMDB 2 268 20,000 5,000 25,000 SST-1 5 18 8,544 1,1101 2,210 SST-2 2 19 6,920 872 1,821 Yelp 2 138 500,000 60,000 38,000 AG News 4 32 114,000 6,000 7,600 Trec 6 10 5,000 452 500 Subj 2 23 8,000 1,000 1,000"
2021.findings-acl.189,N18-1100,0,0.0291649,"with a Variational information bottleneck ATtention (VAT) mechanism. Extensive experiments on eight benchmark datasets show the great advantages of our proposed approach in terms of both performance and interpretability. 1 Introduction Attention mechanisms (Bahdanau et al., 2014) have achieved great success in various natural language processing (NLP) tasks. They are introduced to mimic the human eye focusing on important parts in the inputs when predicting labels. The existing studies show attention mechanisms can improve not only the performance but also the interpretability of the models (Mullenbach et al., 2018; Xie et al., 2017; Xu et al., 2015). Li et al. (2016) pointed the view: “Attention provides an important way to explain the workings of neural models”. Additionally, Wiegreffe and Pinter (2019) showed that attention mechanisms could help understand the inner workings of a model. The basic assumption of understanding of models with attention scores is that the inputs (e.g., words) with high attentive weights are essential for making decisions. However, as far as we know, it has not been formally veriﬁed. Existing research (Jain and Wallace, 2019) also shows that attention is not explicable, an"
2021.findings-acl.189,N18-1097,0,0.026584,"space into two-dimensional space. Accuracy AOPC Accuracy AOPC LSTM-base Random LSTM-ATT LSTM-VAT BERT-base Random BERT-ATT BERT-VAT IMDB 88.79 0.30 5.27 6.13 91.90 0.60 2.81 3.17 SST-1 45.20 5.97 12.94 14.34 51.44 33.26 33.98 34.03 SST-2 85.45 7.58 20.54 21.58 91.60 41.46 41.52 41.52 Yelp 95.10 1.02 6.64 7.12 96.07 3.60 4.73 6.64 AG News 91.91 1.87 5.99 6.59 93.52 44.20 52.22 54.70 Trec 90.00 19.40 31.00 37.20 96.60 65.80 71.60 72.20 Subj 89.00 1.50 2.10 6.30 96.50 45.70 45.70 45.80 Twitter 71.25 4.72 19.10 20.37 75.28 59.21 59.39 59.45 Table 3: The results of AOPC. perturbation curve (AOPC) (Nguyen, 2018; Samek et al., 2016) metric. It calculates the average change of accuracy over test data by deleting top K words via attentive weights. The larger the value of AOPC, the better the explanations of the models. (a) IMDB (LSTM) (b) IMDB (BERT) Figure 4: The inﬂuence of Top-K for LSTM/BERTbased models in terms of AOPC. servations show our VAT model can learn a better task-speciﬁc representation by enforcing the model to reduce the task-irrelevant information. 5.2 Quantitative Evaluation In this section, we evaluate our VAT model using two metrics, AOPC and post-hoc accuracy, which are widely used"
2021.findings-acl.189,P05-1015,0,0.274964,"θ pz|rq rlog ss rψ pzq ppzq Experiment Setup We adopt two typical neural network models, attention-based LSTM (Hochreiter and Schmidhuber, 1997) and BERT (Devlin et al., 2019), to explore our VAT algorithm. 4.1 Datasets and Baselines Datasets To evaluate the effectiveness of our VAT model, we conduct the experiments over eight benchmark datasets: IMDB (Maas et al., 2011), Stanford Sentiment Treebank with (includes SST1 and its binary version SST-2) (Socher et al., 2013), Yelp (Zhang et al., 2015), AG News (Zhang et al., 2015), TREC (Li and Roth, 2002), subjective/objective classiﬁcation Subj (Pang and Lee, 2005) and Twitter (Rosenthal et al., 2015, 2014). The statistics information of these datasets are shown in Table 1. 2155 Class Length #train #dev #test IMDB 2 268 20,000 5,000 25,000 SST-1 5 18 8,544 1,1101 2,210 SST-2 2 19 6,920 872 1,821 Yelp 2 138 500,000 60,000 38,000 AG News 4 32 114,000 6,000 7,600 Trec 6 10 5,000 452 500 Subj 2 23 8,000 1,000 1,000 Twitter 3 22 7,969 1,375 3,795 Table 1: The statistics information of the datasets, where Class is the number of the class, Length is average text length, and #train/#dev/#test counts the number of samples in the train/dev/test sets. LSTM-base LS"
2021.findings-acl.189,D14-1162,0,0.0851244,"sic models (LSTM/BERT-base) and attention-based models (LSTM/BERT-ATT). LSTM-base takes the max-pooling of the LSTM’s hidden vectors as text representation. For BERTbase, the “[CLS]” representation is obtained as the sentence representation. LSTM-ATT model is a standard attention-based LSTM model that has the same structure as the learner. We obtain the BERTATT by replacing the LSTM encoder with BERT in LSTM-ATT. Our models are marked with VAT (LSTM-VAT, BERT-VAT), which integrate VIB into attention-based neural models. 4.2 Implementation Details For LSTM-based models, we use GloVe embedding (Pennington et al., 2014) with 300-dimension to initialize the word embedding and ﬁne-tune it during the training. We randomly initialize all outof-vocabulary words and weights with the uniform distribution U p´0.1, 0.1q. For the BERT-based models, we ﬁne-tune pre-trained BERT-base model. The dimension of hidden state vectors of LSTM is 100 and the max sentence length is 256 in our experiments. Adam (Kingma and Ba, 2014) is utilized as the optimizer with learning rate 0.001 (for LSTM-based model) and 0.00001 (for BERTbased model). We also search different values β P t0.01, 0.1, 1, 10u. 5 Experiments First, we perform"
2021.findings-acl.189,D19-1002,0,0.0521488,"rformance and interpretability. 1 Introduction Attention mechanisms (Bahdanau et al., 2014) have achieved great success in various natural language processing (NLP) tasks. They are introduced to mimic the human eye focusing on important parts in the inputs when predicting labels. The existing studies show attention mechanisms can improve not only the performance but also the interpretability of the models (Mullenbach et al., 2018; Xie et al., 2017; Xu et al., 2015). Li et al. (2016) pointed the view: “Attention provides an important way to explain the workings of neural models”. Additionally, Wiegreffe and Pinter (2019) showed that attention mechanisms could help understand the inner workings of a model. The basic assumption of understanding of models with attention scores is that the inputs (e.g., words) with high attentive weights are essential for making decisions. However, as far as we know, it has not been formally veriﬁed. Existing research (Jain and Wallace, 2019) also shows that attention is not explicable, and there are a lot of controversy regarding to the result explanations (Wiegreffe and Pinter, 2019; Jain and Wallace, 2019). Moreover, we ﬁnd that though the attention mechanism can help improve"
2021.findings-acl.189,N18-1027,0,0.254065,"focus on the irrelevant information. For example, in the sentence “A very funny movie.”, the long short-term memory model with standard attention (LSTM-ATT) infers a correct sentiment label while pays more attention to the irrelevant word “movie”, making the result difﬁcult to explain. In general, the attention weights are only optimized to encode the task-relevant information while are not restricted to imitate human behavior. In order to enhance the interpretability of the attention mechanism, recent studies turn to integrate the human provided explanation signals into the attention models. Rei and Søgaard (2018) regularized the attention weights with a small amount of word-level annotations. Barrett et al. (2018); Bao et al. (2018) improved the explanation of attention by aligning explanations with human-provided rationales. These methods rely on additional labour consuming labelling for enhancing explanations, which is hard to extend to other datasets or tasks. In this paper, we aim to train a more efﬁcient and effective interpretable attention model without any pre-deﬁned annotations or pre-collected explanations. Speciﬁcally, we propose a framework consisting of a learner and a compressor, which e"
2021.findings-acl.189,S15-2078,0,0.154169,"iment Setup We adopt two typical neural network models, attention-based LSTM (Hochreiter and Schmidhuber, 1997) and BERT (Devlin et al., 2019), to explore our VAT algorithm. 4.1 Datasets and Baselines Datasets To evaluate the effectiveness of our VAT model, we conduct the experiments over eight benchmark datasets: IMDB (Maas et al., 2011), Stanford Sentiment Treebank with (includes SST1 and its binary version SST-2) (Socher et al., 2013), Yelp (Zhang et al., 2015), AG News (Zhang et al., 2015), TREC (Li and Roth, 2002), subjective/objective classiﬁcation Subj (Pang and Lee, 2005) and Twitter (Rosenthal et al., 2015, 2014). The statistics information of these datasets are shown in Table 1. 2155 Class Length #train #dev #test IMDB 2 268 20,000 5,000 25,000 SST-1 5 18 8,544 1,1101 2,210 SST-2 2 19 6,920 872 1,821 Yelp 2 138 500,000 60,000 38,000 AG News 4 32 114,000 6,000 7,600 Trec 6 10 5,000 452 500 Subj 2 23 8,000 1,000 1,000 Twitter 3 22 7,969 1,375 3,795 Table 1: The statistics information of the datasets, where Class is the number of the class, Length is average text length, and #train/#dev/#test counts the number of samples in the train/dev/test sets. LSTM-base LSTM-ATT LSTM-VAT BERT-base BERT-ATT B"
2021.findings-acl.189,S14-2009,0,0.0991451,"Missing"
2021.findings-acl.189,P17-1088,0,0.0167323,"mation bottleneck ATtention (VAT) mechanism. Extensive experiments on eight benchmark datasets show the great advantages of our proposed approach in terms of both performance and interpretability. 1 Introduction Attention mechanisms (Bahdanau et al., 2014) have achieved great success in various natural language processing (NLP) tasks. They are introduced to mimic the human eye focusing on important parts in the inputs when predicting labels. The existing studies show attention mechanisms can improve not only the performance but also the interpretability of the models (Mullenbach et al., 2018; Xie et al., 2017; Xu et al., 2015). Li et al. (2016) pointed the view: “Attention provides an important way to explain the workings of neural models”. Additionally, Wiegreffe and Pinter (2019) showed that attention mechanisms could help understand the inner workings of a model. The basic assumption of understanding of models with attention scores is that the inputs (e.g., words) with high attentive weights are essential for making decisions. However, as far as we know, it has not been formally veriﬁed. Existing research (Jain and Wallace, 2019) also shows that attention is not explicable, and there are a lot"
2021.findings-acl.189,D13-1170,0,0.00379002,"kkkkkkkkkkkkkkkkkkkkikkkkkkkkkkkkkkkkkkkkkj hkkkkkkkkkkkkkkkkkkkkkikkkkkkkkkkkkkkkkkkkkkj pθ pz |rq pθ pz |rq Epprq rEpθ pz|rq rlog ss ´ Epprq rEpθ pz|rq rlog ss rψ pzq ppzq Experiment Setup We adopt two typical neural network models, attention-based LSTM (Hochreiter and Schmidhuber, 1997) and BERT (Devlin et al., 2019), to explore our VAT algorithm. 4.1 Datasets and Baselines Datasets To evaluate the effectiveness of our VAT model, we conduct the experiments over eight benchmark datasets: IMDB (Maas et al., 2011), Stanford Sentiment Treebank with (includes SST1 and its binary version SST-2) (Socher et al., 2013), Yelp (Zhang et al., 2015), AG News (Zhang et al., 2015), TREC (Li and Roth, 2002), subjective/objective classiﬁcation Subj (Pang and Lee, 2005) and Twitter (Rosenthal et al., 2015, 2014). The statistics information of these datasets are shown in Table 1. 2155 Class Length #train #dev #test IMDB 2 268 20,000 5,000 25,000 SST-1 5 18 8,544 1,1101 2,210 SST-2 2 19 6,920 872 1,821 Yelp 2 138 500,000 60,000 38,000 AG News 4 32 114,000 6,000 7,600 Trec 6 10 5,000 452 500 Subj 2 23 8,000 1,000 1,000 Twitter 3 22 7,969 1,375 3,795 Table 1: The statistics information of the datasets, where Class is th"
2021.naacl-main.115,D14-1162,0,0.081962,"as achieved larger improvement by average, which can be further enhanced by introducing contextualized pre-trained models (e.g. BERT). 3) Incorporating larger-context information with some aggregators also can lead to performance drop on some datasets (e.g, using graph aggregaSettings and Hyper-parameters We adopt CNN-LSTM-CRF as a prototype and augment it with larger-context information by four categories of aggregators: bow, seq, graph, and cPre-seq. We use Word2Vec (Mikolov et al., 2013) (trained on simplified Chinese Wikipedia dump) as noncontextualized embeddings for CWS task, and GloVe (Pennington et al., 2014) for NER, Chunk, and POS tasks. The window size (the number of sentence) k of larger-context aggregators will be explored with a range of k = {1, 2, 3, 4, 5, 6, 10} for seq, bow, and cPre-seq. We chose the best performance that the larger-context aggregator achieved with window 1466 4 The settings of window size k are listed in the appendix. CWS Emb. Agg. NER CITYU NCC SXU PKU CN03 92.26 +0.42 -0.61 +0.34 94.94 +0.03 -0.02 +0.18 94.35 +0.04 +0.33 +0.08 90.46 -0.39 +1.47 -0.14 Chunk POS BC BN MZ WB NW TC 75.38 +1.66 +0.17 +0.65 86.89 +0.32 +0.42 -0.50 85.42 +1.51 -0.16 +1.49 62.09 +3.49 +4.84 +"
2021.naacl-main.115,N18-1202,0,0.0996018,"apid development of deep neural models has and Hovy, 2016; Lample et al., 2016) (RNNs) or shown impressive performances on sequence tag- graph topology by graph neural networks (Kipf ging tasks that aim to assign labels to each token and Welling, 2016; Schlichtkrull et al., 2018). of an input sequence (Sang and De Meulder, 2003; Understanding the discrepancies of these aggreLample et al., 2016; Ma and Hovy, 2016). More gators can help us reach a more generalized conrecently, the use of unsupervised pre-trained modclusion about the effectiveness of larger-context els (Akbik et al., 2018, 2019; Peters et al., 2018; training. To this end, we study larger-context aggreDevlin et al., 2018) (especially contextualized vergators with three different structural priors (defined sion) has driven state-of-the-art performance to a in Sec. 3.2) and comprehensively evaluate their ∗ Corresponding author efficacy. 1463 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1463–1475 June 6–11, 2021. ©2021 Association for Computational Linguistics Q2: Can the larger-context training easily play to its strengths with the help"
2021.naacl-main.115,W03-0419,0,0.170672,"Missing"
2021.naacl-main.115,D17-1283,0,0.0391961,"Missing"
2021.naacl-main.115,D19-1585,0,0.0360579,"Missing"
2021.naacl-main.115,C18-1327,0,0.0153608,"haddar and Langlais, 2018) that utilize different domains of this dataset, which also paves the way for our fine-grained analysis. Chinese Word Segmentation (CWS) We use four mainstream datasets from SIGHAN2005 and SIGHAN2008, in which CITYU is traditional Chinese, while PKU, NCC, and SXU are simplified ones. Chunking (Chunk) CoNLL-2000 (CN00) is a benchmark dataset for text chunking. Part-of-Speech (POS) We use the Penn Treebank (PTB) III dataset for POS tagging.2 2.3 Neural Tagging Models Despite the emergence of a bunch of architectural explorations (Ma and Hovy, 2016; Lample et al., 2016; Yang et al., 2018; Peters et al., 2018; Akbik et al., 2018; Devlin et al., 2018) for sequence tagging, two general frameworks can be summarized: (i) cEnc-wEnc-CRF consists of the wordlevel encoder, sentence-level encoder, and CRF 2 It’s hard to cover all datasets for all tasks. For Chunk and POS tasks, we adopt the two most popular benchmark datasets. 1464 layer (Lafferty et al., 2001); (ii) ContPre-MLP is composed of a contextualized pre-trained layer, followed by an MLP or CRF layer. In this paper, we take both frameworks as study objects for our three research questions first, 3 and instantiate them as two"
2021.naacl-main.115,2021.eacl-main.324,1,0.680233,"Missing"
2021.naacl-main.115,2020.acl-main.306,0,0.0200424,"Missing"
2021.naacl-main.135,P19-1285,0,0.0508064,"Missing"
2021.naacl-main.135,N19-1122,0,0.0178446,"ion as a new self-attention network. Zhao et al. (2019) explores parallel multi-scale representation learning to capture both long-range and short-range language structures with combination of convolution and self-attention. In our work, DMAN, SAN and FFN are unified in Mask Attention Networks, where DMAN is a supplement of SAN and FFN that specializes in localness modeling. Moreover, we investigate different collaboration mechanisms. Related Work Recently, there is a large body of work on im- 6 Conclusion proving Transformer (Vaswani et al., 2017) for various issues. For recurrence modeling, Hao et al. (2019) introduces a novel attentive recurrent netIn this paper, we introduce Mask Attention Network to leverage the strengths of both attention and works and reformulate SAN and FFN to point recurrent networks. For context modeling, Yang out they are two special cases with static mask et al. (2019a) focuses on improving self-attention in MANs. We analyze the the deficiency of through capturing the richness of context and pro- SAN and FFN in localness modeling. Dynamic poses to contextualize the transformations of the Mask Attention Network is derived from MANs query and key layers. Wu et al. (2019)"
2021.naacl-main.135,N03-1020,0,0.173018,"Missing"
2021.naacl-main.135,W18-6301,0,0.0396285,"Missing"
2021.naacl-main.135,P02-1040,0,0.110664,"warmup steps is 8k and the total updates is 50k. with 0.1 label smoothing rate. Inverse-sqrt learning The optimizer of model is Adam with (0.9,0.98). rate scheduler are employed, the peak learning rates The dropout and clip-norm are both 0.1. During are 1.5e-2, 1e-2 and 7e-3 with 8k warmup, 50k decoding, the beam size are both 5, the max length update, 80k update and 80k update for transformer and length penalty are 50 and 2.0 for CNN/Daily 1696 Mail, 30 and 1.0 for Gigaword. The models are trained on 4 P40 GPUs. 3.2 Experimental Results 3.2.1 Machine Translation In machine translation, BLEU (Papineni et al., 2002) is employed as the evaluation measure. Following common practice, we use tokenized casesensitive BLEU and case-insensitive BLEU for WMT14 En-De and IWSLT14 De-En, respectively. We take Transformer (Vaswani et al., 2017) as the baseline and compare with other concurrent methods. Convolutional Transformer (Yang et al., 2019b) restricts the attention scope to a window of neighboring elements in order to model locality for self-attention model. Local Transformer (Yang et al., 2018) casts localness modeling as a learnable Gaussian bias, which indicates the central and scope of the local region to"
2021.naacl-main.135,D15-1044,0,0.145263,"Missing"
2021.naacl-main.135,P17-1099,0,0.221759,"IWSLT14 German-to-English (De-En) and WMT14 EnglishAutomatic summarization aims to produce a conto-German (En-De). IWSLT14 De-En dataset con- cise and fluent summary conveying the key inforsists of about 153K/7K/7K sentence pairs for train- mation in the input text. We focus on abstractive ing/validation/testing. WMT14 En-De dataset con- summarization, a generation task where the sumsists of about 4.5M sentence pairs, and the models mary is not limited in reusing the phrases or senwere validated on newstest2013 and examined on tences in the input text. We use the CNN/Daily newstest2014. Mail (See et al., 2017) and Gigaword (Rush et al., Our data processing follows Lu et al. (2019). 2015) for model evaluation. For IWSLT2014, we set our model into the small Following Song et al. (2019), we set the hidden one, the hidden size, embeddings and attention size, embeddings and attention heads to 768, 768, heads to 512, 512, and 4 respectively. For the and 12 respectively. Our model consists of a 6-layer WMT14 dataset, following the Transformer setting encoder and 6-layer decoder. For the convenience of Vaswani et al. (2017), we set our model into the of comparison, the training follows classic seq2seq base"
2021.naacl-main.135,N18-2074,0,0.445559,"rforms the original Transformer. 1 Introduction Recently, Transformer (Vaswani et al., 2017) has been widely applied in various natural language processing tasks, such as neural machine translation (Vaswani et al., 2017) and text summarization (Zhang et al., 2019). To further improve the performance of the text representation, Transformer-based variants have attracted a lot of attention (Lu et al., 2019; Sukhbaatar et al., 2019a,b; Bugliarello and Okazaki, 2019; Ma et al., 2020). Each building block of Transformer has two sublayers: Self-Attention Network (SAN) and Feed-Forward Network (FFN). Shaw et al. (2018) Figure 1: The mask matrices of (a) SAN, (b) DMAN and (c) FFN in Mask Attention Networks. Color that fades from black to white means the values in mask matrices decrease from 1 to 0. presents an extension to SAN which incorporates the relative positional information for the sequence. Sukhbaatar et al. (2019a) proposes attention span to control the maximum context size used in SAN and scales Transformer to long-range (∼ 8192 tokens) language modeling. Recently, some works targeting on FFN have been proposed. Lu et al. (2019) gives a new understanding of Transformer from a multi-particle dynamic"
2021.naacl-main.135,P19-1032,0,0.107927,"ial layered structure to combine the three types of layers. Extensive experiments on various tasks, including neural machine translation and text summarization demonstrate that our model outperforms the original Transformer. 1 Introduction Recently, Transformer (Vaswani et al., 2017) has been widely applied in various natural language processing tasks, such as neural machine translation (Vaswani et al., 2017) and text summarization (Zhang et al., 2019). To further improve the performance of the text representation, Transformer-based variants have attracted a lot of attention (Lu et al., 2019; Sukhbaatar et al., 2019a,b; Bugliarello and Okazaki, 2019; Ma et al., 2020). Each building block of Transformer has two sublayers: Self-Attention Network (SAN) and Feed-Forward Network (FFN). Shaw et al. (2018) Figure 1: The mask matrices of (a) SAN, (b) DMAN and (c) FFN in Mask Attention Networks. Color that fades from black to white means the values in mask matrices decrease from 1 to 0. presents an extension to SAN which incorporates the relative positional information for the sequence. Sukhbaatar et al. (2019a) proposes attention span to control the maximum context size used in SAN and scales Transformer to long"
2021.naacl-main.135,K16-1028,0,0.0242906,"n MANs. Under our design principles, there are three elements: FFN, SAN, and DMAN. For the convenience of comparison, we take FFN as the last component in the sequential layered structure. We try different collaboration methods and test them on IWSLT2014 German-to-English (De-En). The results are shown in the Table 3. We conclude that: Abstractive Summarization We use the F1 score of ROUGE (Lin and Hovy, 2003) as the evaluation metric1 . In Table 2, we compare our model against the baseline Transformer (Vaswani et al., 2017) and several generation models on CNN/Daily Mail and Gigaword. LEAD3 (Nallapati et al., 2016) extracts the first three sentences in a document as its summary. PTGEN+Converage (See et al., 2017) is a sequenceto-sequence model based on the pointer-generator network. As shown in Table 2, our model outperforms Transformer by 1.4 in ROUGE-1, 2.2 in 1 Further Analysis 1. Our proposed C#5 achieves the best performance that verify the effectiveness of our proposed sequential layered structure. 2. All of C#3, C#4 and C#5 outperform C#1 and C#2, and the least improvement in BLEU is 0.2. This shows that no matter what collaboration method, models with the participation of DMAN perform better tha"
2021.naacl-main.135,D18-1475,0,0.336421,"ng and capture the global 1692 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1692–1701 June 6–11, 2021. ©2021 Association for Computational Linguistics semantics. In contrast, mask of FFN disables it to perceive the information of other tokens and forces it into self-evolution. We believe that these two specialties endowed by two mask matrices make the success of Transformer in text representation. Although positive results of Transformer have been reported, recent works (Shaw et al., 2018; Yang et al., 2018; Guo et al., 2019) have shown that modeling localness would further improve the performance through experiments. We argue that deficiency of Transformer in local structure modeling is caused by the attention computation with static mask matrix. In the framework of MANs, we find a problem that irrelevant tokens with overlapping neighbors incorrectly attend to each other with relatively large attention scores. For example “a black dog jump to catch the frisbee”, though “catch” and “black” are neither relevant nor neighbors, for the reason that both of them are highly related to their common nei"
2021.naacl-main.135,N19-1407,0,0.0934995,"ild a distance-dependent then globalness, and take the step for self-evolution mask matrix SM. If each token only model the in the end. relationship with those tokens within b units of itself, we can set 3 Experiments  0, |t − s |&gt; b SM[t, s] = (11) In this section, we introduce our experiments. 1, |t − s |≤ b We first describe the experimental details in § 3.1. where t, s are the positions of query and key, and Then we show the experimental results in § 3.2. 1695 Model IWSLT14 De-En small params WMT14 En-De params big params base Transformer (Vaswani et al., 2017) Convolutional Transformer (Yang et al., 2019b) Weighted Transformer (Ahmed et al., 2017) Local Transformer (Yang et al., 2018) Relative Transformer (Shaw et al., 2018) Scaling NMT (Ott et al., 2018) Dynamic Conv (Wu et al., 2019) 34.4 35.2 36M - 27.3 28.2 28.4 28.5 26.8 - 62M 88M 65M 89M - 28.4 28.7 28.9 29.2 29.2 29.3 29.7 213M 213M 268M 213M 213M Ours 36.3 37M 29.1 63M 30.4 215M Table 1: Translation performance (BLEU) on IWSLT14 De-En and WMT14 En-De testsets. Finally we conduct the ablation study and analysis in § 4. 3.1 3.1.1 Experimental Setting Machine Translation big, base and small model with max-tokens 4096, 12288 and 8192 per"
2021.naacl-main.135,K19-1074,0,0.021388,"tion network (DMAN) with a learnable mask matrix which is able to model localness adaptively. To incorporate advantages of DMAN, SAN, and FFN, we propose a sequential layered structure to combine the three types of layers. Extensive experiments on various tasks, including neural machine translation and text summarization demonstrate that our model outperforms the original Transformer. 1 Introduction Recently, Transformer (Vaswani et al., 2017) has been widely applied in various natural language processing tasks, such as neural machine translation (Vaswani et al., 2017) and text summarization (Zhang et al., 2019). To further improve the performance of the text representation, Transformer-based variants have attracted a lot of attention (Lu et al., 2019; Sukhbaatar et al., 2019a,b; Bugliarello and Okazaki, 2019; Ma et al., 2020). Each building block of Transformer has two sublayers: Self-Attention Network (SAN) and Feed-Forward Network (FFN). Shaw et al. (2018) Figure 1: The mask matrices of (a) SAN, (b) DMAN and (c) FFN in Mask Attention Networks. Color that fades from black to white means the values in mask matrices decrease from 1 to 0. presents an extension to SAN which incorporates the relative po"
2021.naacl-main.431,P16-2085,0,0.0264799,"tation consists of two posts from change my view, a sub-forum of Reddit.com. Different types of underlines are used to highlight the interactive argument pairs. Introduction Arguments play a central role in decision making on social issues. Striving to automatically understand human arguments, computational argumentation becomes a growing field in natural language processing. It can be analyzed at two levels — monological argumentation and dialogical argumentation. Existing research on monological argumentation covers argument structure prediction (Stab and Gurevych, 2014), claims generation (Bilu and Slonim, 2016), essay scoring (Taghipour and Ng, 2016), etc. Recently, dialogical argumentation becomes an active topic. In the process of dialogical arguments, participants exchange arguments on a given topic (Asterhan and Schwarz, 2007; Hunter, 2013). With the popularity of online debating forums, large volume of dialogical arguments are daily formed, concerning wide range of topics. A social media dialogical argumentation example from ChangeMyView subreddit is shown in Figure 1. There we show two ∗ *Corresponding author posts holding opposite stances over the same topic. One is the original post and the"
2021.naacl-main.431,P16-1150,0,0.0235027,"ise ranking problem. The performance of different models are shown in Table 4. Note that we use the original CMV dataset and follow the previous setup in Tan et al. (2016); Ji et al. (2018). We find that our model outperforms the state-ofthe-art method (Ji et al., 2018) by a large margin, which indicates that our learned representation can well help downstream tasks. 6 Related Work In this section, we will introduce two major areas related to our work, which are dialogical argumentation and argument representation learning. 6.1 Dialogical Argumentation ing the quality of persuasive arguments (Habernal and Gurevych, 2016). Gottipati et al. (2013) use sentiment lexicons as a preprocessing step and propose a probabilistic graphical model to predict stance of arguments in their dataset. Park et al. (2011) design several argumentation-motivated features to finish the debate stance classification in Korean newswire discourse. Sridhar et al. (2015) consider the joint stance classification of arguments and relations among them and find a multi-level model will work better. for a combination of post-level and authorlevel collective modeling of both stance and disagreement could bring further improvements in performanc"
2021.naacl-main.431,W18-5211,0,0.0272452,"s reply. As can be seen, opinions from both sides are voiced with multiple arguments and the reply post B is organized in-line with post A’s arguments. Here we define an interactive argument pair formed with two arguments from both sides (with the same underline), which focuses on the same perspective of the discussion topic. The automatic identification of these pairs will be a fundamental step towards the understanding of dialogical argumentative structure. Moreover, it can benefit downstream tasks, such as debate summarization (Sanchan et al., 2017) and logical chain extraction in debates (Botschen et al., 2018). However, it is non-trivial to extract the interactive argument pairs holding opposite stances. Back to the example. Given argument b1 with only four words contained, it is difficult, without richer contextual information, to understand why it has interactive relationship with a1. In addition, without modeling the debating focuses of arguments, it is likely for models to wrongly predict that b2 has interactive relationship with a4 for sharing more words. Motivated by these observations, we pro5467 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computat"
2021.naacl-main.431,N16-1162,0,0.0465159,"Missing"
2021.naacl-main.431,K16-1002,0,0.0151145,"ention mechanism to learn sentence represening the relationship between arguments (Wang and tations (Wang et al., 2017a) and their relations with Cardie, 2014; Persing and Ng, 2017) and evaluat- others (Wang et al., 2017b). Our task is inherently 5474 different from theirs because our target arguments naturally occur in the complex interaction context of dialogues, which requires additional efforts for understanding the discourse structure therein. 6.2 Argument Representation Learning the encoder to encode a particular argument and then using the decoder to decode words in adjacent arguments. Bowman et al. (2016) introduce variational autoencoders to incorporate distributed latent representations of entire arguments. In addition, Hill et al. (2016) propose the FastSent model, using bag-of-words of arguments to predict the adjacent arguments. Logeswaran and Lee (2018) propose the Quick Thoughts to exploit the closeness of adjacent arguments. They formulate the argument representation learning as a classification problem. Argument representation learning for natural language has been studied widely in the past few years. Previous work discuss prior approaches to learning argument representations from la"
2021.naacl-main.431,J96-2004,0,0.875242,"Missing"
2021.naacl-main.431,D17-1070,0,0.0181352,"and Blunsom (2013) explore a language-specific contexts of arguments and induce latent representaencoder applied to each argument and represent tions via discrete variational autoencoders. Experithe argument by the mean vector of the words mental results on the dataset show that our model involved. They consider minimizing the inner significantly outperforms the competitive baselines. product between paired arguments in different Further analyses reveal why our model yields sulanguages as the training objective and do not perior performance and prove the usefulness of rely on word alignments. Conneau et al. (2017) discrete argument representations. propose a model called InferSent, which is used The future work will be carried out in two direcas the baseline as it served as the inspiration for tions. First, we will study the usage of our model the inclusion of the SNLI task in the multitask for applying to other dialogical argumentation remodel. They prove that NLI is an effective task lated tasks, such as debate summarization. Second, for pre-training and transfer learning in obtaining we will utilize neural topic model for learning disgeneric argument representations. They train crete argument repres"
2021.naacl-main.431,K17-1017,0,0.0196911,"the probability distribution of zi over K categories, which contains salient features of the argument on varying aspects. Therefore, we obtain the discrete argument representation by the posterior distribution of discrete latent variables z. R= M X Wei q(zi |x) (7) i=1 3.2 Argumentative Context Modeling Here, we introduce contextual information of the quotation and the reply to help identify the interactive argument pairs. The argumentative context contains a list of arguments. Following previous setting in Ji et al. (2018), we consider each sentence as an argument in the context. Inspired by Dong et al. (2017), we employ a hierarchical architecture to obtain argumentative context representations. Argument-level CNN. Given an argument and their embedding forms {e1 , e2 , ..., en }, we employ a convolution layer to incorporate the context information on word level. si = f (Ws · [ei : ei+ws−1 ] + bs ) where Ws and bs are weight matrix and bias vector. ws is the window size in the convolution layer and si is the feature representation. Then, we conduct an attention pooling operation over all the words to get argument embedding vectors. mi = tanh(Wm · si + bm ) eWu ·mi ui = P eWu ·mj (9) (10) j a= X ui"
2021.naacl-main.431,D13-1191,0,0.0328623,"rmance of different models are shown in Table 4. Note that we use the original CMV dataset and follow the previous setup in Tan et al. (2016); Ji et al. (2018). We find that our model outperforms the state-ofthe-art method (Ji et al., 2018) by a large margin, which indicates that our learned representation can well help downstream tasks. 6 Related Work In this section, we will introduce two major areas related to our work, which are dialogical argumentation and argument representation learning. 6.1 Dialogical Argumentation ing the quality of persuasive arguments (Habernal and Gurevych, 2016). Gottipati et al. (2013) use sentiment lexicons as a preprocessing step and propose a probabilistic graphical model to predict stance of arguments in their dataset. Park et al. (2011) design several argumentation-motivated features to finish the debate stance classification in Korean newswire discourse. Sridhar et al. (2015) consider the joint stance classification of arguments and relations among them and find a multi-level model will work better. for a combination of post-level and authorlevel collective modeling of both stance and disagreement could bring further improvements in performance. Wang and Cardie (2014)"
2021.naacl-main.431,C18-1314,1,0.788169,"do you believe those aspects to be? ... Figure 2: An example illustrating the formation process of a quotation-reply pair in CMV. extract interactive argument pairs with the relation of quotation-reply. In general, the content of posts 2 Task Definition and Dataset Collection in CMV is informal, making it difficult to parse an In this section, we first define our task of inter- argument in a finer-grain with premise, conclusion and other components. Therefore, following previactive argument pair identification, followed by a description of how we collect the data for this task. ous setting in Ji et al. (2018), we treat each sentence as an argument. Specifically, we only consider the quotation containing one argument and view the 2.1 Task Definition first sentence after the quotation as the reply. We Given a argument q from the original post, a candi- treat the quotation-reply pairs extracted as posidate set of replies consisting of one positive reply tive samples and randomly select four replies from r+ , several negative replies r1− ∼ ru− , and their cor- other posts that are also related to the original post responding argumentative contexts, our goal is to to pair with the quotation as negative"
2021.naacl-main.431,P18-1040,0,0.0257888,"structure in texts. Recently, the dialogical argumen- teractions between two arguments in debate. Howtation has become an active topic. ever, there is limited research on the interactions Dialogical argumentation refers to a series of between posts. In this work, we propose a novel interactive arguments related to a given topic, in- task of identifying interactive argument pairs from volving argument retraction, view exchange, and argumentative posts to further understand the inso on. Existing research covers discourse struc- teractions between posts. Our work is also related ture prediction (Liu et al., 2018), dialog summa- with some similar tasks, such as question answering rization (Hsueh and Moore, 2007), etc. There are and sentence alignment. They focus on the design several attempts to address tasks related to analyz- of attention mechanism to learn sentence represening the relationship between arguments (Wang and tations (Wang et al., 2017a) and their relations with Cardie, 2014; Persing and Ng, 2017) and evaluat- others (Wang et al., 2017b). Our task is inherently 5474 different from theirs because our target arguments naturally occur in the complex interaction context of dialogues, which r"
2021.naacl-main.431,P11-1035,0,0.0212355,"find that our model outperforms the state-ofthe-art method (Ji et al., 2018) by a large margin, which indicates that our learned representation can well help downstream tasks. 6 Related Work In this section, we will introduce two major areas related to our work, which are dialogical argumentation and argument representation learning. 6.1 Dialogical Argumentation ing the quality of persuasive arguments (Habernal and Gurevych, 2016). Gottipati et al. (2013) use sentiment lexicons as a preprocessing step and propose a probabilistic graphical model to predict stance of arguments in their dataset. Park et al. (2011) design several argumentation-motivated features to finish the debate stance classification in Korean newswire discourse. Sridhar et al. (2015) consider the joint stance classification of arguments and relations among them and find a multi-level model will work better. for a combination of post-level and authorlevel collective modeling of both stance and disagreement could bring further improvements in performance. Wang and Cardie (2014) create a dispute corpus from Wikipedia and use a sentiment analysis to predict the dispute label of arguments. Wei et al. (2016) collect a dataset from CMV an"
2021.naacl-main.431,D14-1162,0,0.0856575,"Missing"
2021.naacl-main.431,sanchan-etal-2017-automatic,0,0.0177199,"s over the same topic. One is the original post and the other is reply. As can be seen, opinions from both sides are voiced with multiple arguments and the reply post B is organized in-line with post A’s arguments. Here we define an interactive argument pair formed with two arguments from both sides (with the same underline), which focuses on the same perspective of the discussion topic. The automatic identification of these pairs will be a fundamental step towards the understanding of dialogical argumentative structure. Moreover, it can benefit downstream tasks, such as debate summarization (Sanchan et al., 2017) and logical chain extraction in debates (Botschen et al., 2018). However, it is non-trivial to extract the interactive argument pairs holding opposite stances. Back to the example. Given argument b1 with only four words contained, it is difficult, without richer contextual information, to understand why it has interactive relationship with a1. In addition, without modeling the debating focuses of arguments, it is likely for models to wrongly predict that b2 has interactive relationship with a4 for sharing more words. Motivated by these observations, we pro5467 Proceedings of the 2021 Conferen"
2021.naacl-main.431,P15-1012,0,0.0276404,"ion can well help downstream tasks. 6 Related Work In this section, we will introduce two major areas related to our work, which are dialogical argumentation and argument representation learning. 6.1 Dialogical Argumentation ing the quality of persuasive arguments (Habernal and Gurevych, 2016). Gottipati et al. (2013) use sentiment lexicons as a preprocessing step and propose a probabilistic graphical model to predict stance of arguments in their dataset. Park et al. (2011) design several argumentation-motivated features to finish the debate stance classification in Korean newswire discourse. Sridhar et al. (2015) consider the joint stance classification of arguments and relations among them and find a multi-level model will work better. for a combination of post-level and authorlevel collective modeling of both stance and disagreement could bring further improvements in performance. Wang and Cardie (2014) create a dispute corpus from Wikipedia and use a sentiment analysis to predict the dispute label of arguments. Wei et al. (2016) collect a dataset from CMV and analyze the correlation between disputing quality and disputation behaviors. analyze the disputation action in the online debate. Given an or"
2021.naacl-main.431,D14-1006,0,0.0284413,"1 Figure 1: An example of dialogical argumentation consists of two posts from change my view, a sub-forum of Reddit.com. Different types of underlines are used to highlight the interactive argument pairs. Introduction Arguments play a central role in decision making on social issues. Striving to automatically understand human arguments, computational argumentation becomes a growing field in natural language processing. It can be analyzed at two levels — monological argumentation and dialogical argumentation. Existing research on monological argumentation covers argument structure prediction (Stab and Gurevych, 2014), claims generation (Bilu and Slonim, 2016), essay scoring (Taghipour and Ng, 2016), etc. Recently, dialogical argumentation becomes an active topic. In the process of dialogical arguments, participants exchange arguments on a given topic (Asterhan and Schwarz, 2007; Hunter, 2013). With the popularity of online debating forums, large volume of dialogical arguments are daily formed, concerning wide range of topics. A social media dialogical argumentation example from ChangeMyView subreddit is shown in Figure 1. There we show two ∗ *Corresponding author posts holding opposite stances over the sa"
2021.naacl-main.431,D16-1193,0,0.0203612,"e my view, a sub-forum of Reddit.com. Different types of underlines are used to highlight the interactive argument pairs. Introduction Arguments play a central role in decision making on social issues. Striving to automatically understand human arguments, computational argumentation becomes a growing field in natural language processing. It can be analyzed at two levels — monological argumentation and dialogical argumentation. Existing research on monological argumentation covers argument structure prediction (Stab and Gurevych, 2014), claims generation (Bilu and Slonim, 2016), essay scoring (Taghipour and Ng, 2016), etc. Recently, dialogical argumentation becomes an active topic. In the process of dialogical arguments, participants exchange arguments on a given topic (Asterhan and Schwarz, 2007; Hunter, 2013). With the popularity of online debating forums, large volume of dialogical arguments are daily formed, concerning wide range of topics. A social media dialogical argumentation example from ChangeMyView subreddit is shown in Figure 1. There we show two ∗ *Corresponding author posts holding opposite stances over the same topic. One is the original post and the other is reply. As can be seen, opinions"
2021.naacl-main.431,P14-2113,0,0.0260124,"ottipati et al. (2013) use sentiment lexicons as a preprocessing step and propose a probabilistic graphical model to predict stance of arguments in their dataset. Park et al. (2011) design several argumentation-motivated features to finish the debate stance classification in Korean newswire discourse. Sridhar et al. (2015) consider the joint stance classification of arguments and relations among them and find a multi-level model will work better. for a combination of post-level and authorlevel collective modeling of both stance and disagreement could bring further improvements in performance. Wang and Cardie (2014) create a dispute corpus from Wikipedia and use a sentiment analysis to predict the dispute label of arguments. Wei et al. (2016) collect a dataset from CMV and analyze the correlation between disputing quality and disputation behaviors. analyze the disputation action in the online debate. Given an original argument and an argument disputing it, they aims to evaluate the quality of a disputing comment based on the original argument and the discussed topic. Habernal and Gurevych (2016) crowdsource the UKPConvArg1 corpus to study what makes an informal social media argument convincing. They crow"
2021.naacl-main.431,S18-1073,0,0.0409519,"Missing"
2021.naacl-main.431,P17-1018,0,0.0168005,"identifying interactive argument pairs from volving argument retraction, view exchange, and argumentative posts to further understand the inso on. Existing research covers discourse struc- teractions between posts. Our work is also related ture prediction (Liu et al., 2018), dialog summa- with some similar tasks, such as question answering rization (Hsueh and Moore, 2007), etc. There are and sentence alignment. They focus on the design several attempts to address tasks related to analyz- of attention mechanism to learn sentence represening the relationship between arguments (Wang and tations (Wang et al., 2017a) and their relations with Cardie, 2014; Persing and Ng, 2017) and evaluat- others (Wang et al., 2017b). Our task is inherently 5474 different from theirs because our target arguments naturally occur in the complex interaction context of dialogues, which requires additional efforts for understanding the discourse structure therein. 6.2 Argument Representation Learning the encoder to encode a particular argument and then using the decoder to decode words in adjacent arguments. Bowman et al. (2016) introduce variational autoencoders to incorporate distributed latent representations of entire ar"
2021.naacl-main.431,W16-2820,1,0.80987,"arguments in their dataset. Park et al. (2011) design several argumentation-motivated features to finish the debate stance classification in Korean newswire discourse. Sridhar et al. (2015) consider the joint stance classification of arguments and relations among them and find a multi-level model will work better. for a combination of post-level and authorlevel collective modeling of both stance and disagreement could bring further improvements in performance. Wang and Cardie (2014) create a dispute corpus from Wikipedia and use a sentiment analysis to predict the dispute label of arguments. Wei et al. (2016) collect a dataset from CMV and analyze the correlation between disputing quality and disputation behaviors. analyze the disputation action in the online debate. Given an original argument and an argument disputing it, they aims to evaluate the quality of a disputing comment based on the original argument and the discussed topic. Habernal and Gurevych (2016) crowdsource the UKPConvArg1 corpus to study what makes an informal social media argument convincing. They crowdsource the UKPConvArg1 corpus and use SVM and bidirectional LSTM to experiment on their annotated datasets. Tan et al. (2016) pa"
2021.naacl-main.431,P17-1190,0,0.038311,"Missing"
2021.naacl-main.431,D17-1026,0,0.0153416,"ork discuss prior approaches to learning argument representations from labelled and unlabelled data. There have been attempts to use laPrevious work focuses on learning continuous arbeled/structured data to learn argument repgument representations with no interpretability. In resentations. Wieting et al. (2016) and Wieting this work, we study the discrete argument represenand Gimpel (2017) introduce a large sentential tations, capturing varying aspects in argumentation paraphrase dataset and use paraphrase data to languages. learn an encoder that maps synonymous phrases to similar embeddings. Wieting et al. (2017) explore the use of machine translation to obtain more 7 Conclusion and Future Work paraphrase data via back-translation of bilingual argument pairs for learning paraphrastic embed- In this paper, we propose a novel task of interactive dings. They show how neural backtranslation argument pair identification from two posts with could be used to generate paraphrases. Hermann opposite stances on a certain topic. We examine and Blunsom (2013) explore a language-specific contexts of arguments and induce latent representaencoder applied to each argument and represent tions via discrete variational a"
2021.naacl-main.431,P18-1101,0,0.0232342,"the two WOF (Ji et al., 2018), the state-of-the-art model to loss terms. The first loss term is defined on the evaluate the persuasiveness of argumentative comDVAE and cross entropy loss is defined as the re- ments, which is tailored to fit our task. In addition, construction loss. We apply the regularization on we compare with some ablations to study the contriKL cost term to solve posterior collapse issue. Due bution from our components. Here we first consider to the space limitation, we leave out the derivation M ATCHrnn , which uses BiGRU to learn argument details and refer the readers to Zhao et al. (2018). representations and explore the match of arguments LDV AE = Eq(z|x) [log p(x|z)]−KL(q(z|x)||p(z)) without modeling the context therein. Then we (19) compare with other ablations that adopt varying The second loss term is defined on the argument argument context modeling methods. Here we conmatching. We formalize this issue as a ranking task sider BiGRU (henceforth M ATCHrnn +Cb ), which 5471 3.4 Joint Learning P@1 MRR 28.36* 28.70* 51.66* 52.03* 31.26* 52.97* 56.04* 73.03* 0.60 0.59 0.58 0.57 0 51.52* 55.98* 57.46* 58.27‡ 58.61‡ 61.17 70.57* 73.20* 73.72* 74.16* 74.66‡ 76.16 Table 2: The per"
C10-1102,W09-1210,0,0.127431,"al applications, decoding speed is very important. Modern structured learning technique adopts template based method to generate millions of features. Such as shallow parsing (Sha and Pereira, 2003), named entity recognition (Kazama and Torisawa, ), dependency parsing (McDonald et al., 2005), etc. The problem arises when the number of templates increases, more features generated, making the extraction step time consuming. Especially for maximum spanning tree (MST) dependency parsing, since feature extraction requires quadratic time even using a first order model. According to Bohnet’s report (Bohnet, 2009), a fast feature extraction beside of a fast parsing algorithm is important for the parsing and training speed. He takes 3 measures for a 40X speedup, despite the same inference algorithm. One important measure is to store the feature vectors in file to skip feature extraction, otherwise it will be the bottleneck. Now we quickly review the feature extraction stage of structured learning. Typically, it consists of 2 steps. First, features represented by strings are generated using templates. Then a feature indexing structure searches feature indexes to get corresponding feature weights. Figure"
C10-1102,P09-1087,0,0.0429088,"Missing"
C10-1102,P08-2060,0,0.14195,"f words) is much larger than standard double array Trie, which has only 256 children, i.e, range of a byte. Therefore, inserting a 2D Trie node is more strict, yielding sparser double arrays. 5.3 Comparison against state-of-the-art Recent works on dependency parsing speedup mainly focus on inference, such as expected linear time non-projective dependency parsing (Nivre, 2009), integer linear programming (ILP) for higher order non-projective parsing (Martins et al., 2009). They achieve 0.632 seconds per sentence over several languages. On the other hand, Goldberg and Elhadad proposed splitSVM (Goldberg and Elhadad, 2008) for fast low-degree polynomial kernel classifiers, and applied it to transition based parsing (Nivre, 2003). They achieve 53 sentences per second parsing speed on English corpus, which is faster than our results, since transition based parsing is linear time, while for graph based method, complexity of feature extraction is quadratic. Xavier Llu´ıs et al. (Llu´ıs et al., 2009) achieve 8.07 seconds per sentence speed on CoNLL09 (Hajiˇc et al., 2009) Chinese Tree Bank test data with a second order graphic model. Bernd Bohnet (Bohnet, 2009) also uses second order model, and achieves 610 minutes"
C10-1102,P09-1040,0,0.0279857,"set, yielding 13.4 sentences per second parsing speed, about 4.3 times faster. Space requirement of 2D Trie is about 2.1 times as binary search, and 1.7 times as Trie. One possible reason is that column number of 2D Trie (e.g. size of words) is much larger than standard double array Trie, which has only 256 children, i.e, range of a byte. Therefore, inserting a 2D Trie node is more strict, yielding sparser double arrays. 5.3 Comparison against state-of-the-art Recent works on dependency parsing speedup mainly focus on inference, such as expected linear time non-projective dependency parsing (Nivre, 2009), integer linear programming (ILP) for higher order non-projective parsing (Martins et al., 2009). They achieve 0.632 seconds per sentence over several languages. On the other hand, Goldberg and Elhadad proposed splitSVM (Goldberg and Elhadad, 2008) for fast low-degree polynomial kernel classifiers, and applied it to transition based parsing (Nivre, 2003). They achieve 53 sentences per second parsing speed on English corpus, which is faster than our results, since transition based parsing is linear time, while for graph based method, complexity of feature extraction is quadratic. Xavier Llu´ıs"
C10-1102,N03-1028,0,0.0397539,"r than traditional Trie structure, making parsing speed 4.3 times faster. 1 Feature: lucky/ADJ Feature Retrieval Parse Tree Index: 3228~3233 Build lattice, inference etc. Figure 1: Flow chart of dependency parsing. p0 .word, p0 .pos denotes the word and POS tag of parent node respectively. Indexes correspond to the features conjoined with dependency types, e.g., lucky/ADJ/OBJ, lucky/ADJ/NMOD, etc. Introduction In practical applications, decoding speed is very important. Modern structured learning technique adopts template based method to generate millions of features. Such as shallow parsing (Sha and Pereira, 2003), named entity recognition (Kazama and Torisawa, ), dependency parsing (McDonald et al., 2005), etc. The problem arises when the number of templates increases, more features generated, making the extraction step time consuming. Especially for maximum spanning tree (MST) dependency parsing, since feature extraction requires quadratic time even using a first order model. According to Bohnet’s report (Bohnet, 2009), a fast feature extraction beside of a fast parsing algorithm is important for the parsing and training speed. He takes 3 measures for a 40X speedup, despite the same inference algorit"
C10-1102,D07-1033,0,0.0582839,"Missing"
C10-1102,W09-1212,0,0.032854,"Missing"
C10-1102,P09-1039,0,0.131367,"quirement of 2D Trie is about 2.1 times as binary search, and 1.7 times as Trie. One possible reason is that column number of 2D Trie (e.g. size of words) is much larger than standard double array Trie, which has only 256 children, i.e, range of a byte. Therefore, inserting a 2D Trie node is more strict, yielding sparser double arrays. 5.3 Comparison against state-of-the-art Recent works on dependency parsing speedup mainly focus on inference, such as expected linear time non-projective dependency parsing (Nivre, 2009), integer linear programming (ILP) for higher order non-projective parsing (Martins et al., 2009). They achieve 0.632 seconds per sentence over several languages. On the other hand, Goldberg and Elhadad proposed splitSVM (Goldberg and Elhadad, 2008) for fast low-degree polynomial kernel classifiers, and applied it to transition based parsing (Nivre, 2003). They achieve 53 sentences per second parsing speed on English corpus, which is faster than our results, since transition based parsing is linear time, while for graph based method, complexity of feature extraction is quadratic. Xavier Llu´ıs et al. (Llu´ıs et al., 2009) achieve 8.07 seconds per sentence speed on CoNLL09 (Hajiˇc et al.,"
C10-1102,P05-1012,0,0.0376088,"Feature Retrieval Parse Tree Index: 3228~3233 Build lattice, inference etc. Figure 1: Flow chart of dependency parsing. p0 .word, p0 .pos denotes the word and POS tag of parent node respectively. Indexes correspond to the features conjoined with dependency types, e.g., lucky/ADJ/OBJ, lucky/ADJ/NMOD, etc. Introduction In practical applications, decoding speed is very important. Modern structured learning technique adopts template based method to generate millions of features. Such as shallow parsing (Sha and Pereira, 2003), named entity recognition (Kazama and Torisawa, ), dependency parsing (McDonald et al., 2005), etc. The problem arises when the number of templates increases, more features generated, making the extraction step time consuming. Especially for maximum spanning tree (MST) dependency parsing, since feature extraction requires quadratic time even using a first order model. According to Bohnet’s report (Bohnet, 2009), a fast feature extraction beside of a fast parsing algorithm is important for the parsing and training speed. He takes 3 measures for a 40X speedup, despite the same inference algorithm. One important measure is to store the feature vectors in file to skip feature extraction,"
C10-1102,W03-3017,0,0.0209086,"rting a 2D Trie node is more strict, yielding sparser double arrays. 5.3 Comparison against state-of-the-art Recent works on dependency parsing speedup mainly focus on inference, such as expected linear time non-projective dependency parsing (Nivre, 2009), integer linear programming (ILP) for higher order non-projective parsing (Martins et al., 2009). They achieve 0.632 seconds per sentence over several languages. On the other hand, Goldberg and Elhadad proposed splitSVM (Goldberg and Elhadad, 2008) for fast low-degree polynomial kernel classifiers, and applied it to transition based parsing (Nivre, 2003). They achieve 53 sentences per second parsing speed on English corpus, which is faster than our results, since transition based parsing is linear time, while for graph based method, complexity of feature extraction is quadratic. Xavier Llu´ıs et al. (Llu´ıs et al., 2009) achieve 8.07 seconds per sentence speed on CoNLL09 (Hajiˇc et al., 2009) Chinese Tree Bank test data with a second order graphic model. Bernd Bohnet (Bohnet, 2009) also uses second order model, and achieves 610 minutes on CoNLL09 English data (2399 sentences, 15.3 second per sentence). Although direct comparison of parsing ti"
C10-1102,W09-1201,0,\N,Missing
C12-2027,P09-1082,0,0.0137345,"to as a vocabulary gap problem. Tweet At the WWDC conference 2012, Apple introduces its new operating system release-Lion. Annotated tags Apple Inc, WWDC, MAC OS Lion Table 1: An example of a tweet with annotated hashtags. Tweet˖At the WWDC conference 2012, Apple introduces its new opera!ng system release-Lion. Word alignment Tags˖ Apple Inc, WWDC, MAC OS Lion Figure 1: The basic idea of word alignment method for suggesting hashtags. To solve the vocabulary gap problem, most researchers applied a statistic machine translation model to learn the word alignment probabilities(Zhou et al., 2011; Bernhard and Gurevych, 2009). Liu et al. (2011) proposed a simple word alignment method to suggest tags for book reviews and online bibliographies. In this work, tags are trigged by the important words of the resource. Figure 1 shows the basic idea of using word alignment method for tag suggestion. Due to the open access in microblogs, topics tend to be more diverse in microblogs than in formal documents. However, all the existing models did not take into account any contextual information in modeling word translation probabilities. Beyond word-level, contextual-level topical information can help word-alignment choice be"
C12-2027,J93-2003,0,0.0269332,"B t,w,c have a potential size of W T K, assuming the vocabulary sizes for words, hashtags and topics are W , T and K. The data sparsity poses a more serious problem in estimating B t,w,c than the topic-free word alignment case. To reduce the data sparsity problem, we introduce the remedy in our model. We can employ a linear interpolation with topic-free word alignment probability to avoid data sparseness: B ∗t,w,c = λB t,w,c + (1 − λ)P(t|w), where P(t|w) is topic-free word alignment probability from the word w and the hashtag t, λ is tradeoff of two probabilities. Here we explore IBM model-1 (Brown et al., 1993), which is a widely used word alignment model, to obtain P(t|w). 3.3 Tag recommendation using Topic-specific translation probabilities 3.3.1 Topic identification Suppose given an unlabeled dataset W∗ = {w d∗ }Ud=1 with U tweets, where the dth tweet w d∗ = Ld ∗ Ld {w dn }n=1 consists of L d words. zd∗ = {zd∗ n }n=1 denotes topics of words in dth tweet and Z∗ = ∗ U {zd }d=1 . we first identify topics for each tweet using the standard LDA model. The collapsed Gibbs sampling is also applied for inference. After the topics of each word become stable, we can estimate the distribution of topic choice"
C12-2027,C10-2028,0,0.159363,": Posters, pages 265–274, COLING 2012, Mumbai, December 2012. 265 1 Introduction Hashtags, which are usually prefixed with the symbol # in microblogging services, represent the relevance of a tweet to a particular group, or a particular topic (Kwak et al., 2010). Popularity of hashtags grows concurrently with the rise and popularity of microblogging services. Many microblog posts contain a wide variety of user-defined hashtags. It has been proven to be useful for many applications, including microblog retrieval (Efron, 2010), query expansion (A.Bandyopadhyay et al., 2011), sentiment analysis (Davidov et al., 2010; Wang et al., 2011), and many other applications. However, not all posts are marked with hashtags. How to automatically generate or recommend hashtags has become an important research topic. The task of hashtag recommendation is to automatically generate hashtags for a given tweet. It is similar to the task of keyphrase extraction, but it has several different aspects. Keyphrases are defined as a short list of phrases to capture the main topics of a given document (Turney, 2000). Keyphrases are usually extracted from the given document. However, hashtags indicate where a tweet is about a part"
C12-2027,W03-1028,0,0.151163,"Missing"
C12-2027,D11-1146,0,0.419161,"m. Tweet At the WWDC conference 2012, Apple introduces its new operating system release-Lion. Annotated tags Apple Inc, WWDC, MAC OS Lion Table 1: An example of a tweet with annotated hashtags. Tweet˖At the WWDC conference 2012, Apple introduces its new opera!ng system release-Lion. Word alignment Tags˖ Apple Inc, WWDC, MAC OS Lion Figure 1: The basic idea of word alignment method for suggesting hashtags. To solve the vocabulary gap problem, most researchers applied a statistic machine translation model to learn the word alignment probabilities(Zhou et al., 2011; Bernhard and Gurevych, 2009). Liu et al. (2011) proposed a simple word alignment method to suggest tags for book reviews and online bibliographies. In this work, tags are trigged by the important words of the resource. Figure 1 shows the basic idea of using word alignment method for tag suggestion. Due to the open access in microblogs, topics tend to be more diverse in microblogs than in formal documents. However, all the existing models did not take into account any contextual information in modeling word translation probabilities. Beyond word-level, contextual-level topical information can help word-alignment choice because sometimes tra"
C12-2027,P11-1066,0,0.00966198,"is usually refered to as a vocabulary gap problem. Tweet At the WWDC conference 2012, Apple introduces its new operating system release-Lion. Annotated tags Apple Inc, WWDC, MAC OS Lion Table 1: An example of a tweet with annotated hashtags. Tweet˖At the WWDC conference 2012, Apple introduces its new opera!ng system release-Lion. Word alignment Tags˖ Apple Inc, WWDC, MAC OS Lion Figure 1: The basic idea of word alignment method for suggesting hashtags. To solve the vocabulary gap problem, most researchers applied a statistic machine translation model to learn the word alignment probabilities(Zhou et al., 2011; Bernhard and Gurevych, 2009). Liu et al. (2011) proposed a simple word alignment method to suggest tags for book reviews and online bibliographies. In this work, tags are trigged by the important words of the resource. Figure 1 shows the basic idea of using word alignment method for tag suggestion. Due to the open access in microblogs, topics tend to be more diverse in microblogs than in formal documents. However, all the existing models did not take into account any contextual information in modeling word translation probabilities. Beyond word-level, contextual-level topical information can"
C12-2027,W04-3252,0,\N,Missing
C12-2093,W02-1001,0,0.105085,"variable, and C is a positive parameter which controls the influence of the slack term on the objective function. Following the derivation in PA (Crammer et al., 2006), we can get the update rule, wk+1 = wk + τk (Φ(·) − Φ(∗)), where τk = min(C, ℓk (w; (·)) ) ∥Φ(·) − Φ(∗)∥2 (10) (11) Our training algorithm is based on PA algorithm and shown in Algorithm 1. In our algorithm, the input examples are randomly selected in each round k. According to the source of the selected example, we obtain the best response by using the proper inference algorithm, and finally update the parameters w. Following (Collins, 2002), the average strategy is also adopted to avoid overfitting problem. 4 Experiments We employ two joint sequence labeling tasks to show the performance of our model. In the following section, we would report our experiment settings and discuss the experiment results. We compare our method with cross-label model and factorial model with PA algorithm. The factorial model is similar with Factorial CRF(Sutton et al., 2007), but its parameters are learning with PA algorithm. We use the standard evaluation metrics F 1 score, which is the harmonic mean of precision P (percentage of predict phrases tha"
C12-2093,P05-2004,0,0.0159689,"nce, our model provides a slight decoding speedup. The reason is that the states of segmentation are much less than tagging. However, on the only segmentation task, our model provides a decoding speedup over 10 times, since we can use the segmentation chain independently in our model. 5 Related Works Several methods have been proposed to cope with the problems of joint S&T task. Sutton et al. (2004, 2007) proposed Dynamic Conditional Random Fields (DCRF) to jointly represent the different tasks in a single graphical model. However, the exact training and inference for DCRF are time-consuming. Duh (2005) proposed a model for jointly labeling multiple sequences. The model is based on the Factorial Hidden Markov Model (FHMM). Since FHMM is directed graphical model, FHMM requires considerably less computation than DCRFs and exact inference is easily achievable. However, the FHMM’s generative framework cannot take full advantage of context features, so its performance is lower than DCRF. Different with our model applied in joint S&T task, both the DCRF and FHMM are used in POS tagging and NP Chunking tasks. These two tasks are not strongly dependent on each other. Therefore, their models are rela"
C12-2093,I08-4010,0,0.109374,"nd tagging tasks, respectively English shallow parsing and Chinese word segmentation and POS tagging (Chinese S&T). 958 In English shallow parsing, the corpus from CoNLL 2000 shared task is commonly used, which contains 8936 sentences for training and 2012 sentences for testing. We employ the commonly used label set {B, M, E, S} in the segmentation task. 12 tagging labels, such as noun phrase (NP), verb phrase (VP),…and others (O), are used in the sequence tagging task. In Chinese S&T, we employ the Chinese Treebank (CTB) corpus, obtained from the Fourth International SIGHAN Bakeoff datasets (Jin and Chen, 2008). The label set {B, M, E, S} is also used for segmentation task. 4.2 Performance of Coupled Sequences Labeling Model In the first experiment, we aim to compare the performances of our coupled sequences labeling model with other traditional joint models. Feature templates used in this experiment are summarized in Table 1 for English shallow parsing and in Table 2 for Chinese word segmenation and POS tagging, in which wi denotes ith word, pi denotes ith POS tag, ci denotes ith Chinese character. We compare the total performance between traditional joint cross-label model, factorial model and our"
C12-2093,W04-3236,0,0.123095,"rds in Chinese sentence. Therefore, sentence must be segmented into sequence of words, in which each word would be assigned with a POS tag. Recently many research works focused on joint S&T tasks, which can be categorized into two ways: pipeline and cross-label. The pipeline approaches are to solve two subtasks in order, segmentation and tagging. However, the obvious disadvantage of these approaches is error propagation, which significantly affects the whole performance. The cross-label approaches can avoid the problem of error propagation and achieve more higher performance on both subtasks (Ng and Low, 2004). However, due to the large number of labels, two problems arise: (1) The amount of parameters increases rapidly and would be apt to overfit to the training corpus; (2) The decoding efficiency by dynamic programming would decrease. In addition, joint cross-label approaches cannot segment or tag sentences separately. For example, in Chinese POS tagging task, the joint model cannot segment sentences individually without tagging the sentences. Moreover, if the sentences are already segmented, the joint model can not tag individually with the existing segmentation information. In this paper, we pr"
C12-2093,W96-0213,0,0.348063,"); predict (1st Viterbi): ˆsl = arg max ⟨w, Φs (xl , s)⟩; s if ˆsl = si then update with w with Eq. 10, where (·) is (xl , sl ) and (∗) is (xl , ˆsl ); end end end w = cw/K ; Algorithm 1: Online Learning Algorithm for Coupled Sequences Labeling Model. (双链序列标 注在线学习算法) 953 1 Introduction In the fields of natural language processing (NLP), joint segmentation and tagging (S&T) task is an important research topic. Many NLP problems can be transformed to joint S&T task, such as shallow parsing(Sha and Pereira, 2003), named entity recognition(Zhou and Su, 2002), Chinese part-of-speech (POS) tagging(Ratnaparkhi, 1996) and so on. For example, there are no explicitly boundaries between words in Chinese sentence. Therefore, sentence must be segmented into sequence of words, in which each word would be assigned with a POS tag. Recently many research works focused on joint S&T tasks, which can be categorized into two ways: pipeline and cross-label. The pipeline approaches are to solve two subtasks in order, segmentation and tagging. However, the obvious disadvantage of these approaches is error propagation, which significantly affects the whole performance. The cross-label approaches can avoid the problem of er"
C12-2093,N03-1028,0,0.0807399,"h Eq. 10, where (·) is (xl , sl , tl ) and (∗) is (xl , ˆsl , tˆl ); end else receive an example (xl , sl ); predict (1st Viterbi): ˆsl = arg max ⟨w, Φs (xl , s)⟩; s if ˆsl = si then update with w with Eq. 10, where (·) is (xl , sl ) and (∗) is (xl , ˆsl ); end end end w = cw/K ; Algorithm 1: Online Learning Algorithm for Coupled Sequences Labeling Model. (双链序列标 注在线学习算法) 953 1 Introduction In the fields of natural language processing (NLP), joint segmentation and tagging (S&T) task is an important research topic. Many NLP problems can be transformed to joint S&T task, such as shallow parsing(Sha and Pereira, 2003), named entity recognition(Zhou and Su, 2002), Chinese part-of-speech (POS) tagging(Ratnaparkhi, 1996) and so on. For example, there are no explicitly boundaries between words in Chinese sentence. Therefore, sentence must be segmented into sequence of words, in which each word would be assigned with a POS tag. Recently many research works focused on joint S&T tasks, which can be categorized into two ways: pipeline and cross-label. The pipeline approaches are to solve two subtasks in order, segmentation and tagging. However, the obvious disadvantage of these approaches is error propagation, whi"
C12-2093,P99-1023,0,0.0701627,"ion and would be assigned to the same label. 3.2 Inference Algorithm According to the theory of probabilistic graphical models (Koller and Friedman, 2009), we can define a score function F (·) as the logarithmic potential function: F (w, Φ(x, s, t)) = L ∑ {wT ΦC1 (si−1 , ti−1 , si , x, i) + wT ΦC2 (ti−1 , si , ti , x, i)}, (3) i Given an observed sequence x, the aim of inference algorithm is to find two best label sequences simultaneously with the highest score. In order to adapt to our model with two kinds of 3-variable cliques, we make some modifications of a second order Viterbi algorithm (Thede and Harper, 1999). We define two functions for recording the score of the best partial path from the beginning of the sequence to the position i: δi (ti−1 , si ) , F (w, Φ(x, s0:i , t0:i−1 )) = arg max{ηi−1 (si−1 , ti−1 ) + wT ΦC1 (si−1 , ti−1 , si , x, i)},(4) ηi (si , ti ) , F (w, Φ(x, s0:i , t0:i )) = arg max{δi (ti−1 , si ) + wT ΦC2 (ti−1 , si , ti , x, i)}, si−1 ti−1 Initially, only features associated with variables s0 and t0 are hired. Without loss of generality, we set s−1 = “BoS” 1 , t−1 = “BoT” 2 and η−1 (s−1 , t−1 ) = 0. Then iteratively calculate these two 1 denotes 2 denotes “Beginning of Segmenta"
C12-2093,P02-1060,0,0.0640538,"(xl , ˆsl , tˆl ); end else receive an example (xl , sl ); predict (1st Viterbi): ˆsl = arg max ⟨w, Φs (xl , s)⟩; s if ˆsl = si then update with w with Eq. 10, where (·) is (xl , sl ) and (∗) is (xl , ˆsl ); end end end w = cw/K ; Algorithm 1: Online Learning Algorithm for Coupled Sequences Labeling Model. (双链序列标 注在线学习算法) 953 1 Introduction In the fields of natural language processing (NLP), joint segmentation and tagging (S&T) task is an important research topic. Many NLP problems can be transformed to joint S&T task, such as shallow parsing(Sha and Pereira, 2003), named entity recognition(Zhou and Su, 2002), Chinese part-of-speech (POS) tagging(Ratnaparkhi, 1996) and so on. For example, there are no explicitly boundaries between words in Chinese sentence. Therefore, sentence must be segmented into sequence of words, in which each word would be assigned with a POS tag. Recently many research works focused on joint S&T tasks, which can be categorized into two ways: pipeline and cross-label. The pipeline approaches are to solve two subtasks in order, segmentation and tagging. However, the obvious disadvantage of these approaches is error propagation, which significantly affects the whole performanc"
C14-1021,J93-2003,0,0.0649756,"the microblogs. where Nz,w For the probability alignment ϕ between hashtag and word, the potential size is W · V · K. The data sparsity poses a more serious problem in estimating ϕ than the topic-free word alignment case. To remedy the problem, we use interpolation smoothing technique for ϕ. In this paper, we emplogy smoothing as follows: ϕ∗h,w,z = γϕh,w,z + (1 − γ)P (h|w), (4) where ϕ∗h,w,z is the smoothed topical alignment probabilities, ϕh,w,z is the original topical alignment probabilities. P (h|w) is topic-free word alignment probability. Here we obtain P (h|w) by exploring IBM model-1 (Brown et al., 1993). γ is trade-off of two probabilities ranging from 0.0 to 1.0. When γ = 0.0, ϕ∗h,w,z will be reduce to topic-free word alignment probability; and when γ = 1.0, there will be no smoothing in ϕ∗h,w,z . For the word itself there are no smoothing, because it is a pseudo-count. 3.3 Hashtag Extraction We perform hashtag extraction as follows. Suppose given an unlabeled dataset, we perform Gibbs Sampling to iteratively estimate the topic and determine topic/background words for each microblog. The process is the same as described in Section 3.2. After the hidden variables of topic/background words an"
C14-1021,C10-2028,0,0.0381562,"Missing"
C14-1021,P11-1016,0,0.0268478,"Missing"
C14-1021,C12-1105,0,0.444475,"egg design Leo Taurus like saint sunday employees together film yourself life husband loyalty incentive street children parent Aries Venus Apple happyness like charges Pluto Horoscope iphone pumpkin 0 2012-04 2012-06 2012-08 2012-10 2012-12 2013-02 2013-04 Figure 1: An example of the topics of retweets in each month. Each colored stripe represents a topic, whose height is the number of words assigned to the topic. For each topic, the top words of this topic in each month are placed on the stripe. Motivated by the methods proposed to handle the vocabulary gap problem for keyphrase extraction (Liu et al., 2012) and hashtag suggestion (Ding et al., 2013), in this work, we also assume that the hashtags and textual content in a microblog are parallel descriptions of the same thing in different languages. To model the document themes, in this paper, we adopt the topical translation model to facilitate the translation process. Topic-specific word triggers are used to bridge the gap between the words and hashtags. Since existing topical translation models can only recommend hashtags learned from the training data, we also incorporate an extraction process into the model. This work makes three main contrib"
C14-1021,J03-1002,0,0.038208,"uly assigned” among “hashtags manually assigned”. F1-score is the harmonic mean of precision and recall. We do 500 iterations of Gibbs sampling to train the model. For optimize the hyperparmeters of the proposed method and alternative methods, we use 5-fold cross-validation in the training data to do it. The number of topics is set to 70. The other settings of hyperparameters are as follows: α = 50/K, β w = 0.1, β h = 0.1, λ = 0.01, and δ = 0.01. The smoothing factor γ in Eq.(3) is set to 0.6. For estimating the translation probability without topical information, we use GIZA++ 1.07 to do it (Och and Ney, 2003). For baselines, we compare the proposed model with the following alternative models. • TWTM: Topical word trigger model (TWTM) was proposed by Liu et al. for keyphrase extraction using only textual information (Liu et al., 2012). We implemented the model and used it to achieve the task. • TTM: Ding et al. (2013) proposed the topical translation model (TTM) for hash tag extraction. We implemented and extended their method for evaluating it on the corpus constructed in this work. 4.3 Experimental Results Table 2 shows the comparisons of the proposed method with the state-of-the-art methods on t"
C14-1021,P11-1039,0,\N,Missing
C14-1065,P11-1016,0,0.116116,"Missing"
C14-1065,W11-2213,0,0.0277192,"is fundamental problems in many NLP applications. The task aims to distinguish the real world relevant of a given name with the same surface in context. WePS36 (Amig´o et al., 2010) and RepLab 20137 (Amig´o et al., 2013) evaluation campaigns have also addressed the problem from the perspective of disambiguation organization names in microblogs. Pedersen et al. (2006) proposed an unsupervised method for name discrimination. Yerva et al. (2010) used support vector machines (SVM) classiﬁer with various external resources, such as WordNet, metadata proﬁle, category proﬁle, Google set, and so on. Kozareva and Ravi (2011) proposed to use latent dirichlet allocation to incorporate topical information. Zhang et al. (2012) proposed to use adaptive method for this task. However, most of these methods focused on the text with predeﬁned surface words. The documents which do not contain organization names or person names can not be well processed by these methods. To bridge the vocabulary gap between content and hashtags, Liu et al. (2012b) proposed to use translation model to handle it. They modeled the tag suggestion task as a translation process from 6 7 http://nlp.uned.es/weps/weps-3 http://www.limosine-project.e"
C14-1065,D12-1123,0,0.0140633,". Yerva et al. (2010) used support vector machines (SVM) classiﬁer with various external resources, such as WordNet, metadata proﬁle, category proﬁle, Google set, and so on. Kozareva and Ravi (2011) proposed to use latent dirichlet allocation to incorporate topical information. Zhang et al. (2012) proposed to use adaptive method for this task. However, most of these methods focused on the text with predeﬁned surface words. The documents which do not contain organization names or person names can not be well processed by these methods. To bridge the vocabulary gap between content and hashtags, Liu et al. (2012b) proposed to use translation model to handle it. They modeled the tag suggestion task as a translation process from 6 7 http://nlp.uned.es/weps/weps-3 http://www.limosine-project.eu/events/replab2013 694 Table 4: The inﬂuence of the smoothing parameter σ of the propose method. σ Precision Recall F1 0.0 0.2 0.4 0.6 0.8 1.0 0.471 0.490 0.495 0.511 0.522 0.519 0.432 0.449 0.454 0.468 0.479 0.476 0.451 0.469 0.474 0.489 0.500 0.496 0.7 0.6 F1-Score 0.5 0.4 0.3 0.2 0.1 0 1 NB 2 3 SVM 4 5 IBM1 6 TTM 7 8 9 Our w/o BG 10 Our Figure 2: Evaluation results of NB, SVM, IBM1, TTM, and our method on the d"
C14-1065,C12-1105,0,0.0295422,". Yerva et al. (2010) used support vector machines (SVM) classiﬁer with various external resources, such as WordNet, metadata proﬁle, category proﬁle, Google set, and so on. Kozareva and Ravi (2011) proposed to use latent dirichlet allocation to incorporate topical information. Zhang et al. (2012) proposed to use adaptive method for this task. However, most of these methods focused on the text with predeﬁned surface words. The documents which do not contain organization names or person names can not be well processed by these methods. To bridge the vocabulary gap between content and hashtags, Liu et al. (2012b) proposed to use translation model to handle it. They modeled the tag suggestion task as a translation process from 6 7 http://nlp.uned.es/weps/weps-3 http://www.limosine-project.eu/events/replab2013 694 Table 4: The inﬂuence of the smoothing parameter σ of the propose method. σ Precision Recall F1 0.0 0.2 0.4 0.6 0.8 1.0 0.471 0.490 0.495 0.511 0.522 0.519 0.432 0.449 0.454 0.468 0.479 0.476 0.451 0.469 0.474 0.489 0.500 0.496 0.7 0.6 F1-Score 0.5 0.4 0.3 0.2 0.1 0 1 NB 2 3 SVM 4 5 IBM1 6 TTM 7 8 9 Our w/o BG 10 Our Figure 2: Evaluation results of NB, SVM, IBM1, TTM, and our method on the d"
C14-1065,P13-1172,0,0.0781602,"Missing"
C14-1065,H05-1043,0,0.168454,"Missing"
C14-1065,J11-1002,0,0.0957434,"Missing"
C14-1065,D09-1159,1,0.889039,"Missing"
C14-1065,C10-2167,0,0.0545261,"Missing"
C14-1065,Y12-1025,0,0.166176,"ceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 688–697, Dublin, Ireland, August 23-29 2014. The WePS-32 (Amig´o et al., 2010) and RepLab 20133 (Amig´o et al., 2013) evaluation campaigns also addressed the problem from the perspective of the disambiguation of company names in microblogs. Microblogs that contain company names at a lexical level are classiﬁed based on whether it refers to the company or not. Various approaches have been proposed to address the task with different methods (Pedersen et al., 2006; Yerva et al., 2010; Zhang et al., 2012; Spina et al., 2012; Spina et al., 2013). However, the microblogs that do not contain company names cannot be correctly processed using these methods. From analyzing the data, we observe that a variety of microblog posts belong to this type. They only contain products names, slang terms, and other related company content. To achieve this task, in this paper, we propose the use of a translation based model to identify the targets of microblogs. We assume that the microblog posts and targets describe the same topic using different languages. Hence, the target identiﬁcation problem can be regard"
C14-1065,H05-2017,0,\N,Missing
C14-1109,J96-1002,0,0.0248015,"des a new viewpoint to enhance the performance of CWS by automatically expanding corpus rather than developing complicated algorithms or features. 1 Introduction Word segmentation is a fundamental task for Chinese language processing. In recent years, Chinese word segmentation (CWS) has undergone great development. The popular method is to regard word segmentation as a sequence labeling problems (Xue, 2003; Peng et al., 2004). The goal of sequence labeling is to assign labels to all elements in a sequence, which can be handled with supervised learning algorithms, such as Maximum Entropy (ME) (Berger et al., 1996), Conditional Random Fields (CRF)(Laﬀerty et al., 2001). After years of intensive researches, Chinese word segmentation achieves a quite high precision. However, the performance of segmentation is not so satisfying for the practical demands to analyze Chinese texts. The key reason is that most of annotated corpora are drawn from news texts. Therefore, the system trained on these corpora cannot work well with the out-of-domain texts. Since these supervised approaches often has a high requirement on the quality and quantity of annotated corpus, which is always not easy to create. As a result, ma"
C14-1109,W03-0407,0,0.0519952,"ber of Continuous OOV Words (b) OOV rate (c) Word Length The blue horizontal line is the overall F1 score, and the red line is the F1 scores with diﬀerent values of the factor. Figure 1: Analysis of Inﬂuence Factors Typically the most conﬁdent unlabeled points, together with their predicted labels, are added to the training set. The classiﬁer is re-trained and the procedure repeated. Note that the classiﬁer uses its own predictions to teach itself. Self-training has been applied to several natural language processing (NLP) tasks, such as word sense disambiguation (Yarowsky, 1995), POStagging (Clark et al., 2003; Jiang and Zhai, 2007; Liu and Zhang, 2012), parsing (Steedman et al., 2003; McClosky et al., 2006; Reichart and Rappoport, 2007; Sagae, 2010), information extraction(Etzioni et al., 2004)and so on. It has been proven that self-training can improve system performance on the target domain by simultaneously modeling annotated source-domain data and unannotated target domain data in the training process. However, the data on target domain cannot always help itself (Steedman et al., 2003). The third is weakly supervised learning. (Li and Sun, 2009; Jiang et al., 2013) utilized the massive manual"
C14-1109,W02-1001,0,0.00699872,"o use a margin to ﬁnd the reliable ones as new training data. In our experiments, the number of selected sentence is 1 ∼ 5 for each seed. Thus, we can re-train a new segmenter on the expanded corpus. After several iteration, we will get a segmenter with the best performance. 1159 5 Base Segmenter We use discriminative character-based sequence labeling for base word segmentation. Each character is labeled as one of {B, M, E, S} to indicate the segmentation. We use online Passive-Aggressive (PA) algorithm (Crammer and Singer, 2003; Crammer et al., 2006) to train the model parameters. Following (Collins, 2002), the average strategy is used to avoid the overﬁtting problem. 6 Experiment To evaluate our algorithm, we use both CTB6.0 and CTB7.0 datasets in our experiments. CTB is a segmented, part-of-speech tagged, and fully bracketed corpus in the constituency formalism. It is also a popular data set to evaluate word segmentation methods, such as (Sun and Xu, 2011). Since CTB dataset is collected from diﬀerent sources, such as newswire, magazine, broadcast news and web blogs, it is suitable to evaluate the performance of CWS systems on diﬀerent domains. We conduct two experiments on diﬀerent divisions"
C14-1109,J04-1004,0,0.0154582,"Magistry and Sagot, 2012). This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4. 0/ 1154 Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 1154–1164, Dublin, Ireland, August 23-29 2014. The second is to use unsupervised or domain-independent features in supervised learning for Chinese word segmentation, such as punctuation and mutual information(MI), word accessory variance (Feng et al., 2004; Zhao and Kit, 2008; Sun and Xu, 2011) The third is to use semi-supervised learning (Zhu, 2005) in sequence labeling to address the diﬀerence in source and target distributions (Jiao et al., 2006; Altun et al., 2006; Suzuki and Isozaki, 2008). Although these methods improve the performance of out-of-domain texts, the performance is still worse than that of in-domain texts obviously. We ﬁrstly investigate the reasons of lower performance in new domain for state-of-the-art CWS systems and ﬁnd that most of error segmentation were caused by out-of-vocabulary (OOV) words, also called new words or"
C14-1109,P07-1034,0,0.0114724,"V Words (b) OOV rate (c) Word Length The blue horizontal line is the overall F1 score, and the red line is the F1 scores with diﬀerent values of the factor. Figure 1: Analysis of Inﬂuence Factors Typically the most conﬁdent unlabeled points, together with their predicted labels, are added to the training set. The classiﬁer is re-trained and the procedure repeated. Note that the classiﬁer uses its own predictions to teach itself. Self-training has been applied to several natural language processing (NLP) tasks, such as word sense disambiguation (Yarowsky, 1995), POStagging (Clark et al., 2003; Jiang and Zhai, 2007; Liu and Zhang, 2012), parsing (Steedman et al., 2003; McClosky et al., 2006; Reichart and Rappoport, 2007; Sagae, 2010), information extraction(Etzioni et al., 2004)and so on. It has been proven that self-training can improve system performance on the target domain by simultaneously modeling annotated source-domain data and unannotated target domain data in the training process. However, the data on target domain cannot always help itself (Steedman et al., 2003). The third is weakly supervised learning. (Li and Sun, 2009; Jiang et al., 2013) utilized the massive manual natural annotations or"
C14-1109,P13-1075,0,0.249193,"(Yarowsky, 1995), POStagging (Clark et al., 2003; Jiang and Zhai, 2007; Liu and Zhang, 2012), parsing (Steedman et al., 2003; McClosky et al., 2006; Reichart and Rappoport, 2007; Sagae, 2010), information extraction(Etzioni et al., 2004)and so on. It has been proven that self-training can improve system performance on the target domain by simultaneously modeling annotated source-domain data and unannotated target domain data in the training process. However, the data on target domain cannot always help itself (Steedman et al., 2003). The third is weakly supervised learning. (Li and Sun, 2009; Jiang et al., 2013) utilized the massive manual natural annotations or punctuation information on the Internet to improve the performance of CWS. However, these natural annotations are just partial annotations and their roles depend on the qualities of the selected resource, such as Wikipedia. In this paper, we wish to propose a method to obtain new fully-annotated data in more aggressive way, which can combine the advantages of the above works. 3 Analysis of Inﬂuence Factors for CWS Before describing our method, we give an analysis of the impact of out-of-vocabulary (OOV) words for segmentation. We ﬁrst conduct"
C14-1109,P06-1027,0,0.0329959,"ttp://creativecommons.org/licenses/by/4. 0/ 1154 Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 1154–1164, Dublin, Ireland, August 23-29 2014. The second is to use unsupervised or domain-independent features in supervised learning for Chinese word segmentation, such as punctuation and mutual information(MI), word accessory variance (Feng et al., 2004; Zhao and Kit, 2008; Sun and Xu, 2011) The third is to use semi-supervised learning (Zhu, 2005) in sequence labeling to address the diﬀerence in source and target distributions (Jiao et al., 2006; Altun et al., 2006; Suzuki and Isozaki, 2008). Although these methods improve the performance of out-of-domain texts, the performance is still worse than that of in-domain texts obviously. We ﬁrstly investigate the reasons of lower performance in new domain for state-of-the-art CWS systems and ﬁnd that most of error segmentation were caused by out-of-vocabulary (OOV) words, also called new words or unknown words (see details in Section 3). It is diﬃcult to devote eﬀorts to building a corpus for out-of-domain texts, since new words are produced frequently as the development of the society, es"
C14-1109,P06-2056,0,0.0257515,"to analyze Chinese texts. The key reason is that most of annotated corpora are drawn from news texts. Therefore, the system trained on these corpora cannot work well with the out-of-domain texts. Since these supervised approaches often has a high requirement on the quality and quantity of annotated corpus, which is always not easy to create. As a result, many methods were proposed to utilize the information of unlabeled data. There are three kinds of methods for domain adaptation problem in CWS. The ﬁrst is to use unsupervised learning algorithm to segment texts, like branching entropy (BE) (Jin and Tanaka-Ishii, 2006), normalized variation of branching entropy (nVBE)(Magistry and Sagot, 2012). This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4. 0/ 1154 Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 1154–1164, Dublin, Ireland, August 23-29 2014. The second is to use unsupervised or domain-independent features in supervised learning for Chinese word segmentation, such as punctuation an"
C14-1109,J09-4006,0,0.254472,"se disambiguation (Yarowsky, 1995), POStagging (Clark et al., 2003; Jiang and Zhai, 2007; Liu and Zhang, 2012), parsing (Steedman et al., 2003; McClosky et al., 2006; Reichart and Rappoport, 2007; Sagae, 2010), information extraction(Etzioni et al., 2004)and so on. It has been proven that self-training can improve system performance on the target domain by simultaneously modeling annotated source-domain data and unannotated target domain data in the training process. However, the data on target domain cannot always help itself (Steedman et al., 2003). The third is weakly supervised learning. (Li and Sun, 2009; Jiang et al., 2013) utilized the massive manual natural annotations or punctuation information on the Internet to improve the performance of CWS. However, these natural annotations are just partial annotations and their roles depend on the qualities of the selected resource, such as Wikipedia. In this paper, we wish to propose a method to obtain new fully-annotated data in more aggressive way, which can combine the advantages of the above works. 3 Analysis of Inﬂuence Factors for CWS Before describing our method, we give an analysis of the impact of out-of-vocabulary (OOV) words for segmenta"
C14-1109,C12-2067,0,0.110619,"en we describe our method in section 4. Section 5 introduces the base segmenter. Section 6 gives the experimental results. Finally we conclude our work in section 7. 2 Related Works The idea of exploring information redundancy on Web was introduced in question answering system (Kwok et al., 2001; Clarke et al., 2001; Banko et al., 2002) and the famous information extraction system KNOWITALL(Etzioni et al., 2004). However, this idea is rarely mentioned in Chinese word segmentation. Nonetheless, there are three kinds of related methods on Chinese word segmentation. One is active learning. Both (Li et al., 2012) and (Sassano, 2002) try to use active learning method to expand annotated corpus, but they still need to manually label some new raw texts in order to enlarge the training corpus. Diﬀerent with these methods, our method do not require any manual oracle labeling at all. Another is self-training, also called bootstrapping or self-teaching (Zhu, 2005). Self-training is a general semi-supervised learning approach. In self-training, a classiﬁer is ﬁrst trained with the small amount of labeled data. The classiﬁer is then used to classify the unlabeled data. 1155 F1 F1 0.9 0.85 0.8 0.75 1.00 1.00 0."
C14-1109,C12-2073,0,0.235485,"c) Word Length The blue horizontal line is the overall F1 score, and the red line is the F1 scores with diﬀerent values of the factor. Figure 1: Analysis of Inﬂuence Factors Typically the most conﬁdent unlabeled points, together with their predicted labels, are added to the training set. The classiﬁer is re-trained and the procedure repeated. Note that the classiﬁer uses its own predictions to teach itself. Self-training has been applied to several natural language processing (NLP) tasks, such as word sense disambiguation (Yarowsky, 1995), POStagging (Clark et al., 2003; Jiang and Zhai, 2007; Liu and Zhang, 2012), parsing (Steedman et al., 2003; McClosky et al., 2006; Reichart and Rappoport, 2007; Sagae, 2010), information extraction(Etzioni et al., 2004)and so on. It has been proven that self-training can improve system performance on the target domain by simultaneously modeling annotated source-domain data and unannotated target domain data in the training process. However, the data on target domain cannot always help itself (Steedman et al., 2003). The third is weakly supervised learning. (Li and Sun, 2009; Jiang et al., 2013) utilized the massive manual natural annotations or punctuation informati"
C14-1109,P12-2075,0,0.0177905,"e drawn from news texts. Therefore, the system trained on these corpora cannot work well with the out-of-domain texts. Since these supervised approaches often has a high requirement on the quality and quantity of annotated corpus, which is always not easy to create. As a result, many methods were proposed to utilize the information of unlabeled data. There are three kinds of methods for domain adaptation problem in CWS. The ﬁrst is to use unsupervised learning algorithm to segment texts, like branching entropy (BE) (Jin and Tanaka-Ishii, 2006), normalized variation of branching entropy (nVBE)(Magistry and Sagot, 2012). This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4. 0/ 1154 Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 1154–1164, Dublin, Ireland, August 23-29 2014. The second is to use unsupervised or domain-independent features in supervised learning for Chinese word segmentation, such as punctuation and mutual information(MI), word accessory variance (Feng et al., 2004; Zhao a"
C14-1109,N06-1020,0,0.126733,"F1 score, and the red line is the F1 scores with diﬀerent values of the factor. Figure 1: Analysis of Inﬂuence Factors Typically the most conﬁdent unlabeled points, together with their predicted labels, are added to the training set. The classiﬁer is re-trained and the procedure repeated. Note that the classiﬁer uses its own predictions to teach itself. Self-training has been applied to several natural language processing (NLP) tasks, such as word sense disambiguation (Yarowsky, 1995), POStagging (Clark et al., 2003; Jiang and Zhai, 2007; Liu and Zhang, 2012), parsing (Steedman et al., 2003; McClosky et al., 2006; Reichart and Rappoport, 2007; Sagae, 2010), information extraction(Etzioni et al., 2004)and so on. It has been proven that self-training can improve system performance on the target domain by simultaneously modeling annotated source-domain data and unannotated target domain data in the training process. However, the data on target domain cannot always help itself (Steedman et al., 2003). The third is weakly supervised learning. (Li and Sun, 2009; Jiang et al., 2013) utilized the massive manual natural annotations or punctuation information on the Internet to improve the performance of CWS. H"
C14-1109,C04-1081,0,0.058855,"segmenter to resolve the original complex segmentation. The experimental results show that our approach can more eﬀectively and stably improve the performance of CWS. Our method also provides a new viewpoint to enhance the performance of CWS by automatically expanding corpus rather than developing complicated algorithms or features. 1 Introduction Word segmentation is a fundamental task for Chinese language processing. In recent years, Chinese word segmentation (CWS) has undergone great development. The popular method is to regard word segmentation as a sequence labeling problems (Xue, 2003; Peng et al., 2004). The goal of sequence labeling is to assign labels to all elements in a sequence, which can be handled with supervised learning algorithms, such as Maximum Entropy (ME) (Berger et al., 1996), Conditional Random Fields (CRF)(Laﬀerty et al., 2001). After years of intensive researches, Chinese word segmentation achieves a quite high precision. However, the performance of segmentation is not so satisfying for the practical demands to analyze Chinese texts. The key reason is that most of annotated corpora are drawn from news texts. Therefore, the system trained on these corpora cannot work well wi"
C14-1109,P07-1078,0,0.0174796,"line is the F1 scores with diﬀerent values of the factor. Figure 1: Analysis of Inﬂuence Factors Typically the most conﬁdent unlabeled points, together with their predicted labels, are added to the training set. The classiﬁer is re-trained and the procedure repeated. Note that the classiﬁer uses its own predictions to teach itself. Self-training has been applied to several natural language processing (NLP) tasks, such as word sense disambiguation (Yarowsky, 1995), POStagging (Clark et al., 2003; Jiang and Zhai, 2007; Liu and Zhang, 2012), parsing (Steedman et al., 2003; McClosky et al., 2006; Reichart and Rappoport, 2007; Sagae, 2010), information extraction(Etzioni et al., 2004)and so on. It has been proven that self-training can improve system performance on the target domain by simultaneously modeling annotated source-domain data and unannotated target domain data in the training process. However, the data on target domain cannot always help itself (Steedman et al., 2003). The third is weakly supervised learning. (Li and Sun, 2009; Jiang et al., 2013) utilized the massive manual natural annotations or punctuation information on the Internet to improve the performance of CWS. However, these natural annotati"
C14-1109,W10-2606,0,0.0131988,"erent values of the factor. Figure 1: Analysis of Inﬂuence Factors Typically the most conﬁdent unlabeled points, together with their predicted labels, are added to the training set. The classiﬁer is re-trained and the procedure repeated. Note that the classiﬁer uses its own predictions to teach itself. Self-training has been applied to several natural language processing (NLP) tasks, such as word sense disambiguation (Yarowsky, 1995), POStagging (Clark et al., 2003; Jiang and Zhai, 2007; Liu and Zhang, 2012), parsing (Steedman et al., 2003; McClosky et al., 2006; Reichart and Rappoport, 2007; Sagae, 2010), information extraction(Etzioni et al., 2004)and so on. It has been proven that self-training can improve system performance on the target domain by simultaneously modeling annotated source-domain data and unannotated target domain data in the training process. However, the data on target domain cannot always help itself (Steedman et al., 2003). The third is weakly supervised learning. (Li and Sun, 2009; Jiang et al., 2013) utilized the massive manual natural annotations or punctuation information on the Internet to improve the performance of CWS. However, these natural annotations are just p"
C14-1109,P02-1064,0,0.042688,"hod in section 4. Section 5 introduces the base segmenter. Section 6 gives the experimental results. Finally we conclude our work in section 7. 2 Related Works The idea of exploring information redundancy on Web was introduced in question answering system (Kwok et al., 2001; Clarke et al., 2001; Banko et al., 2002) and the famous information extraction system KNOWITALL(Etzioni et al., 2004). However, this idea is rarely mentioned in Chinese word segmentation. Nonetheless, there are three kinds of related methods on Chinese word segmentation. One is active learning. Both (Li et al., 2012) and (Sassano, 2002) try to use active learning method to expand annotated corpus, but they still need to manually label some new raw texts in order to enlarge the training corpus. Diﬀerent with these methods, our method do not require any manual oracle labeling at all. Another is self-training, also called bootstrapping or self-teaching (Zhu, 2005). Self-training is a general semi-supervised learning approach. In self-training, a classiﬁer is ﬁrst trained with the small amount of labeled data. The classiﬁer is then used to classify the unlabeled data. 1155 F1 F1 0.9 0.85 0.8 0.75 1.00 1.00 0.90 0.90 0.80 0.80 0."
C14-1109,E03-1008,0,0.0517668,"tal line is the overall F1 score, and the red line is the F1 scores with diﬀerent values of the factor. Figure 1: Analysis of Inﬂuence Factors Typically the most conﬁdent unlabeled points, together with their predicted labels, are added to the training set. The classiﬁer is re-trained and the procedure repeated. Note that the classiﬁer uses its own predictions to teach itself. Self-training has been applied to several natural language processing (NLP) tasks, such as word sense disambiguation (Yarowsky, 1995), POStagging (Clark et al., 2003; Jiang and Zhai, 2007; Liu and Zhang, 2012), parsing (Steedman et al., 2003; McClosky et al., 2006; Reichart and Rappoport, 2007; Sagae, 2010), information extraction(Etzioni et al., 2004)and so on. It has been proven that self-training can improve system performance on the target domain by simultaneously modeling annotated source-domain data and unannotated target domain data in the training process. However, the data on target domain cannot always help itself (Steedman et al., 2003). The third is weakly supervised learning. (Li and Sun, 2009; Jiang et al., 2013) utilized the massive manual natural annotations or punctuation information on the Internet to improve th"
C14-1109,D11-1090,0,0.45407,"licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4. 0/ 1154 Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 1154–1164, Dublin, Ireland, August 23-29 2014. The second is to use unsupervised or domain-independent features in supervised learning for Chinese word segmentation, such as punctuation and mutual information(MI), word accessory variance (Feng et al., 2004; Zhao and Kit, 2008; Sun and Xu, 2011) The third is to use semi-supervised learning (Zhu, 2005) in sequence labeling to address the diﬀerence in source and target distributions (Jiao et al., 2006; Altun et al., 2006; Suzuki and Isozaki, 2008). Although these methods improve the performance of out-of-domain texts, the performance is still worse than that of in-domain texts obviously. We ﬁrstly investigate the reasons of lower performance in new domain for state-of-the-art CWS systems and ﬁnd that most of error segmentation were caused by out-of-vocabulary (OOV) words, also called new words or unknown words (see details in Section 3"
C14-1109,P08-1076,0,0.0356686,". 0/ 1154 Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 1154–1164, Dublin, Ireland, August 23-29 2014. The second is to use unsupervised or domain-independent features in supervised learning for Chinese word segmentation, such as punctuation and mutual information(MI), word accessory variance (Feng et al., 2004; Zhao and Kit, 2008; Sun and Xu, 2011) The third is to use semi-supervised learning (Zhu, 2005) in sequence labeling to address the diﬀerence in source and target distributions (Jiao et al., 2006; Altun et al., 2006; Suzuki and Isozaki, 2008). Although these methods improve the performance of out-of-domain texts, the performance is still worse than that of in-domain texts obviously. We ﬁrstly investigate the reasons of lower performance in new domain for state-of-the-art CWS systems and ﬁnd that most of error segmentation were caused by out-of-vocabulary (OOV) words, also called new words or unknown words (see details in Section 3). It is diﬃcult to devote eﬀorts to building a corpus for out-of-domain texts, since new words are produced frequently as the development of the society, especially the Internet society. It is also impra"
C14-1109,O03-4002,0,0.0663566,"in a better segmenter to resolve the original complex segmentation. The experimental results show that our approach can more eﬀectively and stably improve the performance of CWS. Our method also provides a new viewpoint to enhance the performance of CWS by automatically expanding corpus rather than developing complicated algorithms or features. 1 Introduction Word segmentation is a fundamental task for Chinese language processing. In recent years, Chinese word segmentation (CWS) has undergone great development. The popular method is to regard word segmentation as a sequence labeling problems (Xue, 2003; Peng et al., 2004). The goal of sequence labeling is to assign labels to all elements in a sequence, which can be handled with supervised learning algorithms, such as Maximum Entropy (ME) (Berger et al., 1996), Conditional Random Fields (CRF)(Laﬀerty et al., 2001). After years of intensive researches, Chinese word segmentation achieves a quite high precision. However, the performance of segmentation is not so satisfying for the practical demands to analyze Chinese texts. The key reason is that most of annotated corpora are drawn from news texts. Therefore, the system trained on these corpora"
C14-1109,P12-1083,0,0.0660079,"Missing"
C14-1109,P95-1026,0,0.479798,"9 10 Word Length OOV Rate Number of Continuous OOV Words (b) OOV rate (c) Word Length The blue horizontal line is the overall F1 score, and the red line is the F1 scores with diﬀerent values of the factor. Figure 1: Analysis of Inﬂuence Factors Typically the most conﬁdent unlabeled points, together with their predicted labels, are added to the training set. The classiﬁer is re-trained and the procedure repeated. Note that the classiﬁer uses its own predictions to teach itself. Self-training has been applied to several natural language processing (NLP) tasks, such as word sense disambiguation (Yarowsky, 1995), POStagging (Clark et al., 2003; Jiang and Zhai, 2007; Liu and Zhang, 2012), parsing (Steedman et al., 2003; McClosky et al., 2006; Reichart and Rappoport, 2007; Sagae, 2010), information extraction(Etzioni et al., 2004)and so on. It has been proven that self-training can improve system performance on the target domain by simultaneously modeling annotated source-domain data and unannotated target domain data in the training process. However, the data on target domain cannot always help itself (Steedman et al., 2003). The third is weakly supervised learning. (Li and Sun, 2009; Jiang et al., 20"
C14-1109,I08-4017,0,0.023954,"2012). This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4. 0/ 1154 Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 1154–1164, Dublin, Ireland, August 23-29 2014. The second is to use unsupervised or domain-independent features in supervised learning for Chinese word segmentation, such as punctuation and mutual information(MI), word accessory variance (Feng et al., 2004; Zhao and Kit, 2008; Sun and Xu, 2011) The third is to use semi-supervised learning (Zhu, 2005) in sequence labeling to address the diﬀerence in source and target distributions (Jiao et al., 2006; Altun et al., 2006; Suzuki and Isozaki, 2008). Although these methods improve the performance of out-of-domain texts, the performance is still worse than that of in-domain texts obviously. We ﬁrstly investigate the reasons of lower performance in new domain for state-of-the-art CWS systems and ﬁnd that most of error segmentation were caused by out-of-vocabulary (OOV) words, also called new words or unknown words (see d"
C14-1109,E14-1062,0,\N,Missing
C16-1090,D11-1146,0,0.0276258,"ariety of circumstances. Hashtagged words that become very popular are often trending topics. Various works have also shown that hashtags can provide valuable information about different problems such as twitter spammer detection (Benevenuto et al., 2010), popularity prediction (Tsur and Rappoport, 2012), and sentiment analysis (Wang et al., 2011). With the increasing requirements, the hashtag recommendation task has received considerable attention in recent years. Discriminative models have been proposed from different aspects using various kinds of features and models (Heymann et al., 2008; Liu et al., 2011), collaborative filtering (Kywe et al., 2012), generative models (Ding et al., 2013; Godin et al., 2013; She and Chen, 2014), and convolutional neural networks (CNN) (Gong and Zhang, 2016). Some of the previous works treated this task as a multiclass classification problem and used word-level features and exquisitely designed patterns to perform the task. Numerous existing studies utilized the word trigger assumption (Liu et al., 2011; Ding et al., 2013) and introduced topical machine translation models to achieve the task. Due to the advantages of deep neural networks and the effectiveness of"
C16-1090,C14-1021,1,\N,Missing
C16-1238,H05-1091,0,0.0685635,"been applied to relation extraction. As mentioned earlier, supervised methods have shown to perform well in this task. In the supervised paradigm, relation classification is considered as a multi-classification problem, and researchers concentrate on extracting complex features, either feature-based or kernel-based. (Kambhatla, 2004; Suchanek et al., 2006) converted the classification clues (such as sequences and parse trees) into feature vectors. Various kernels, such as the convolution tree kernel (Qian et al., 2008), subsequence kernel (Mooney and Bunescu, 2005) and dependency tree kernel (Bunescu and Mooney, 2005), have been proposed to solve the relation classification problem. (Plank and Moschitti, 2013) introduced semantic information into kernel methods in addition to considering structural information only. However, the reliance on manual annotation, which is expensive to produce and thus limited in quantity has provided the impetus for distant-supervision (Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Takamatsu et al., 2012). With the recent revival of interest in deep neural networks, many researchers have concentrated on using deep networks to learn features. In NLP, such meth"
C16-1238,J81-4005,0,0.763917,"Missing"
C16-1238,N15-1133,0,0.0138805,"o called a word embedding (Turian et al., 2010). (Socher et al., 2012) presented a recursive neural network (RNN) for relation classification to learn vectors in the syntactic tree path connecting two nominals to determine their semantic relationship. (Hashimoto et al., 2013) also employed a neural relation extraction model allowing for the explicit weighting of important phrases for the target task. (Zeng et al., 2014) exploited a convolutional deep neural network to extract lexical and sentence level features. These two levels of features were concatenated to form the final feature vector. (Ebrahimi and Dou, 2015) rebuilt an RNN on the dependency path between two marked entities. (Xu et al., 2015b) used the convolutional network and proposed a ranking loss function with data cleaning. (Xu et al., 2015c) leveraged heterogeneous information along the shortest dependency path between two entities. (Xu et al., 2016) proposed a data augmentation method by leveraging the directionality of relations. Another line of research is the attention mechanism for deep learning. (Bahdanau et al., 2014) pro2527 Output Concatenation Feature Sentence Convolution Attention-based Context Extraction Vector Vector Word Repre"
C16-1238,D15-1205,0,0.0467369,"Missing"
C16-1238,D13-1137,0,0.0186006,"pervision (Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Takamatsu et al., 2012). With the recent revival of interest in deep neural networks, many researchers have concentrated on using deep networks to learn features. In NLP, such methods are primarily based on learning a distributed representation for each word, which is also called a word embedding (Turian et al., 2010). (Socher et al., 2012) presented a recursive neural network (RNN) for relation classification to learn vectors in the syntactic tree path connecting two nominals to determine their semantic relationship. (Hashimoto et al., 2013) also employed a neural relation extraction model allowing for the explicit weighting of important phrases for the target task. (Zeng et al., 2014) exploited a convolutional deep neural network to extract lexical and sentence level features. These two levels of features were concatenated to form the final feature vector. (Ebrahimi and Dou, 2015) rebuilt an RNN on the dependency path between two marked entities. (Xu et al., 2015b) used the convolutional network and proposed a ranking loss function with data cleaning. (Xu et al., 2015c) leveraged heterogeneous information along the shortest depe"
C16-1238,W09-2415,0,0.10423,"Missing"
C16-1238,P11-1055,0,0.333219,"Here, the marked entities “valuables” and “safe” are of the relation “Content-Container(e1; e2)”. Relation classification plays a key role in various NLP applications, and has become a hot research topic in recent years. Various machine learning based relation classification methods have been proposed for the task, based on either human-designed features (Kambhatla, 2004; Suchanek et al., 2006), or kernels (Kambhatla, 2004; Suchanek et al., 2006). Some researchers also employed the existing known facts to label the text corpora via distant supervision (Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Takamatsu et al., 2012). All of these approaches are effective because they leverage a large body of linguistic knowledge. However, these methods may suffer from two limitations. First, the extracted features or elaborately designed kernels are often derived from the output of pre-existing NLP systems, which leads to the propagation of the errors in the existing tools and hinders the performance of such systems (Bach and Badaskar, 2007). Second, the methods mentioned above do not scale well during relation extraction, which makes it very hard to engineer effective task-specific features and"
C16-1238,D15-1106,0,0.0210927,"This attention mechanism is used to select the reference words in the original language for words in the foreign language before translation. (Xu et al., 2015a) used the attention mechanism in image caption generation to select the relevant image regions when generating words in the captions. Further uses of the attention mechanism included paraphrase identification (Yin et al., 2015), document classification (Yang et al., 2016), parsing (Vinyals et al., 2015), natural language question answering (Sukhbaatar et al., 2015; Kumar et al., 2015; Hermann et al., 2015) and image question answering (Lin et al., 2015). (Wang et al., 2016) introduced attention mechanism into relation classification which relied on two levels of attention for pattern extraction. In this paper, we will explore the word level attention mechanism in order to discover better patterns in heterogeneous contexts for the relation classification task. 3 Methodology Given a set of sentences x1 , x2 , ...xn and two corresponding entities, our model measures the probability of each relation r. The architecture of our proposed method is shown in Figure 1. Here, feature extraction is the main component, which is composed of sentence convo"
C16-1238,P14-5010,0,0.00328803,"we use the CBOW model to pretrain position embeddings on two relative distance files respectively (Mikolov et al., 2013). The dimension of position embedding is set 5. Part-of-speech tag Embeddings. Our word embeddings are obtained from the Google News corpus, which is slightly different to the relation classification corpus. We deal with this problem by allying each input word with its POS tag to improve the robustness. In our experiment, we only take into use a coarsegrained POS category, containing 15 different tags. We use the Stanford CoreNLP Toolkit to obtain the part-of-speech tagging (Manning et al., 2014) . Then we pretrain the embeddings by the CBOW model on the taggings, and the dimension of part-of-speech tag embedding is set 10. Finally, we concatenate the word embedding, position embedding, and part-of-speech tag embedding of each word and denote it as a vector of sequence w = [W F, pF, P OSF ]. 3.1.2 Convolution, Max-pooling and Non-linear Layers In relation extraction, one of the main challenges is that, the length of the sentences is variable and important information can appear anywhere. Hence, we should merge all local features and perform relation prediction globally. Here, we use a"
C16-1238,P09-1113,0,0.496682,"afe h/e2 i or a closet with a dead-bolt. Here, the marked entities “valuables” and “safe” are of the relation “Content-Container(e1; e2)”. Relation classification plays a key role in various NLP applications, and has become a hot research topic in recent years. Various machine learning based relation classification methods have been proposed for the task, based on either human-designed features (Kambhatla, 2004; Suchanek et al., 2006), or kernels (Kambhatla, 2004; Suchanek et al., 2006). Some researchers also employed the existing known facts to label the text corpora via distant supervision (Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Takamatsu et al., 2012). All of these approaches are effective because they leverage a large body of linguistic knowledge. However, these methods may suffer from two limitations. First, the extracted features or elaborately designed kernels are often derived from the output of pre-existing NLP systems, which leads to the propagation of the errors in the existing tools and hinders the performance of such systems (Bach and Badaskar, 2007). Second, the methods mentioned above do not scale well during relation extraction, which makes it very hard to en"
C16-1238,P14-1028,0,0.0122499,"ures or elaborately designed kernels are often derived from the output of pre-existing NLP systems, which leads to the propagation of the errors in the existing tools and hinders the performance of such systems (Bach and Badaskar, 2007). Second, the methods mentioned above do not scale well during relation extraction, which makes it very hard to engineer effective task-specific features and learn parameters. Recently, neural network models have been increasingly focused on for their ability to minimize the effort in feature engineering of NLP tasks (Collobert et al., 2011; Zheng et al., 2013; Pei et al., 2014). Moreover, some researchers have also paid attention to feature learning of neural networks in the field of relation extraction. (Socher et al., 2012) introduced a recursive neural network model to learn compositional vector representations for phrases and sentences of arbitrary syntactic types and length. (Zeng et al., 2014; Xu et al., 2015b) utilized convolutional neural networks (CNNs) for relation classification. (Xu et al., 2015c) applied long short term memory (LSTM)-based recurrent neural networks (RNNs) along the shortest dependency path. This work is licensed under a Creative Commons"
C16-1238,P13-1147,0,0.0213304,"erform well in this task. In the supervised paradigm, relation classification is considered as a multi-classification problem, and researchers concentrate on extracting complex features, either feature-based or kernel-based. (Kambhatla, 2004; Suchanek et al., 2006) converted the classification clues (such as sequences and parse trees) into feature vectors. Various kernels, such as the convolution tree kernel (Qian et al., 2008), subsequence kernel (Mooney and Bunescu, 2005) and dependency tree kernel (Bunescu and Mooney, 2005), have been proposed to solve the relation classification problem. (Plank and Moschitti, 2013) introduced semantic information into kernel methods in addition to considering structural information only. However, the reliance on manual annotation, which is expensive to produce and thus limited in quantity has provided the impetus for distant-supervision (Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Takamatsu et al., 2012). With the recent revival of interest in deep neural networks, many researchers have concentrated on using deep networks to learn features. In NLP, such methods are primarily based on learning a distributed representation for each word, which is also"
C16-1238,C08-1088,0,0.0200277,"Missing"
C16-1238,D12-1110,0,0.146554,"the existing tools and hinders the performance of such systems (Bach and Badaskar, 2007). Second, the methods mentioned above do not scale well during relation extraction, which makes it very hard to engineer effective task-specific features and learn parameters. Recently, neural network models have been increasingly focused on for their ability to minimize the effort in feature engineering of NLP tasks (Collobert et al., 2011; Zheng et al., 2013; Pei et al., 2014). Moreover, some researchers have also paid attention to feature learning of neural networks in the field of relation extraction. (Socher et al., 2012) introduced a recursive neural network model to learn compositional vector representations for phrases and sentences of arbitrary syntactic types and length. (Zeng et al., 2014; Xu et al., 2015b) utilized convolutional neural networks (CNNs) for relation classification. (Xu et al., 2015c) applied long short term memory (LSTM)-based recurrent neural networks (RNNs) along the shortest dependency path. This work is licensed under a Creative Commons Attribution 4.0 International Licence. s:http://creativecommons.org/licenses/by/4.0/. Licence detail2526 Proceedings of COLING 2016, the 26th Internat"
C16-1238,P12-1076,0,0.227305,"es “valuables” and “safe” are of the relation “Content-Container(e1; e2)”. Relation classification plays a key role in various NLP applications, and has become a hot research topic in recent years. Various machine learning based relation classification methods have been proposed for the task, based on either human-designed features (Kambhatla, 2004; Suchanek et al., 2006), or kernels (Kambhatla, 2004; Suchanek et al., 2006). Some researchers also employed the existing known facts to label the text corpora via distant supervision (Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Takamatsu et al., 2012). All of these approaches are effective because they leverage a large body of linguistic knowledge. However, these methods may suffer from two limitations. First, the extracted features or elaborately designed kernels are often derived from the output of pre-existing NLP systems, which leads to the propagation of the errors in the existing tools and hinders the performance of such systems (Bach and Badaskar, 2007). Second, the methods mentioned above do not scale well during relation extraction, which makes it very hard to engineer effective task-specific features and learn parameters. Recentl"
C16-1238,P10-1040,0,0.0135119,"rmation into kernel methods in addition to considering structural information only. However, the reliance on manual annotation, which is expensive to produce and thus limited in quantity has provided the impetus for distant-supervision (Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Takamatsu et al., 2012). With the recent revival of interest in deep neural networks, many researchers have concentrated on using deep networks to learn features. In NLP, such methods are primarily based on learning a distributed representation for each word, which is also called a word embedding (Turian et al., 2010). (Socher et al., 2012) presented a recursive neural network (RNN) for relation classification to learn vectors in the syntactic tree path connecting two nominals to determine their semantic relationship. (Hashimoto et al., 2013) also employed a neural relation extraction model allowing for the explicit weighting of important phrases for the target task. (Zeng et al., 2014) exploited a convolutional deep neural network to extract lexical and sentence level features. These two levels of features were concatenated to form the final feature vector. (Ebrahimi and Dou, 2015) rebuilt an RNN on the d"
C16-1238,P16-1123,0,0.147451,"Missing"
C16-1238,D15-1062,0,0.146231,"o engineer effective task-specific features and learn parameters. Recently, neural network models have been increasingly focused on for their ability to minimize the effort in feature engineering of NLP tasks (Collobert et al., 2011; Zheng et al., 2013; Pei et al., 2014). Moreover, some researchers have also paid attention to feature learning of neural networks in the field of relation extraction. (Socher et al., 2012) introduced a recursive neural network model to learn compositional vector representations for phrases and sentences of arbitrary syntactic types and length. (Zeng et al., 2014; Xu et al., 2015b) utilized convolutional neural networks (CNNs) for relation classification. (Xu et al., 2015c) applied long short term memory (LSTM)-based recurrent neural networks (RNNs) along the shortest dependency path. This work is licensed under a Creative Commons Attribution 4.0 International Licence. s:http://creativecommons.org/licenses/by/4.0/. Licence detail2526 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 2526–2536, Osaka, Japan, December 11-17 2016. We have noticed that these neural models are all designed as the way that al"
C16-1238,D15-1206,0,0.0767423,"o engineer effective task-specific features and learn parameters. Recently, neural network models have been increasingly focused on for their ability to minimize the effort in feature engineering of NLP tasks (Collobert et al., 2011; Zheng et al., 2013; Pei et al., 2014). Moreover, some researchers have also paid attention to feature learning of neural networks in the field of relation extraction. (Socher et al., 2012) introduced a recursive neural network model to learn compositional vector representations for phrases and sentences of arbitrary syntactic types and length. (Zeng et al., 2014; Xu et al., 2015b) utilized convolutional neural networks (CNNs) for relation classification. (Xu et al., 2015c) applied long short term memory (LSTM)-based recurrent neural networks (RNNs) along the shortest dependency path. This work is licensed under a Creative Commons Attribution 4.0 International Licence. s:http://creativecommons.org/licenses/by/4.0/. Licence detail2526 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 2526–2536, Osaka, Japan, December 11-17 2016. We have noticed that these neural models are all designed as the way that al"
C16-1238,C16-1138,0,0.0403963,"traction model allowing for the explicit weighting of important phrases for the target task. (Zeng et al., 2014) exploited a convolutional deep neural network to extract lexical and sentence level features. These two levels of features were concatenated to form the final feature vector. (Ebrahimi and Dou, 2015) rebuilt an RNN on the dependency path between two marked entities. (Xu et al., 2015b) used the convolutional network and proposed a ranking loss function with data cleaning. (Xu et al., 2015c) leveraged heterogeneous information along the shortest dependency path between two entities. (Xu et al., 2016) proposed a data augmentation method by leveraging the directionality of relations. Another line of research is the attention mechanism for deep learning. (Bahdanau et al., 2014) pro2527 Output Concatenation Feature Sentence Convolution Attention-based Context Extraction Vector Vector Word Representation Both his [feet]e1 have been moving into the [ball]e2 Figure 1: Architecture of the attention-based convolution neural network. posed the attention mechanism in machine translation task, which is also the first use of it in natural language processing. This attention mechanism is used to select"
C16-1238,N16-1174,0,0.0160386,"hitecture of the attention-based convolution neural network. posed the attention mechanism in machine translation task, which is also the first use of it in natural language processing. This attention mechanism is used to select the reference words in the original language for words in the foreign language before translation. (Xu et al., 2015a) used the attention mechanism in image caption generation to select the relevant image regions when generating words in the captions. Further uses of the attention mechanism included paraphrase identification (Yin et al., 2015), document classification (Yang et al., 2016), parsing (Vinyals et al., 2015), natural language question answering (Sukhbaatar et al., 2015; Kumar et al., 2015; Hermann et al., 2015) and image question answering (Lin et al., 2015). (Wang et al., 2016) introduced attention mechanism into relation classification which relied on two levels of attention for pattern extraction. In this paper, we will explore the word level attention mechanism in order to discover better patterns in heterogeneous contexts for the relation classification task. 3 Methodology Given a set of sentences x1 , x2 , ...xn and two corresponding entities, our model measu"
C16-1238,C14-1220,0,0.425449,"akes it very hard to engineer effective task-specific features and learn parameters. Recently, neural network models have been increasingly focused on for their ability to minimize the effort in feature engineering of NLP tasks (Collobert et al., 2011; Zheng et al., 2013; Pei et al., 2014). Moreover, some researchers have also paid attention to feature learning of neural networks in the field of relation extraction. (Socher et al., 2012) introduced a recursive neural network model to learn compositional vector representations for phrases and sentences of arbitrary syntactic types and length. (Zeng et al., 2014; Xu et al., 2015b) utilized convolutional neural networks (CNNs) for relation classification. (Xu et al., 2015c) applied long short term memory (LSTM)-based recurrent neural networks (RNNs) along the shortest dependency path. This work is licensed under a Creative Commons Attribution 4.0 International Licence. s:http://creativecommons.org/licenses/by/4.0/. Licence detail2526 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 2526–2536, Osaka, Japan, December 11-17 2016. We have noticed that these neural models are all designed a"
C16-1238,D13-1061,0,0.0149501,", the extracted features or elaborately designed kernels are often derived from the output of pre-existing NLP systems, which leads to the propagation of the errors in the existing tools and hinders the performance of such systems (Bach and Badaskar, 2007). Second, the methods mentioned above do not scale well during relation extraction, which makes it very hard to engineer effective task-specific features and learn parameters. Recently, neural network models have been increasingly focused on for their ability to minimize the effort in feature engineering of NLP tasks (Collobert et al., 2011; Zheng et al., 2013; Pei et al., 2014). Moreover, some researchers have also paid attention to feature learning of neural networks in the field of relation extraction. (Socher et al., 2012) introduced a recursive neural network model to learn compositional vector representations for phrases and sentences of arbitrary syntactic types and length. (Zeng et al., 2014; Xu et al., 2015b) utilized convolutional neural networks (CNNs) for relation classification. (Xu et al., 2015c) applied long short term memory (LSTM)-based recurrent neural networks (RNNs) along the shortest dependency path. This work is licensed under"
C18-1074,baccianella-etal-2010-sentiwordnet,0,0.278007,"s on the sentiment content, thus generating sentiment-informative representations. Compared with general attention models, our model has better interpretability and less noise. Experimental results on three large-scale sentiment classification datasets showed that the proposed method outperforms previous methods. 1 Introduction Sentiment analysis has the goal of analyzing people’s sentiments or opinions and has been well explored (Turney, 2002; Tang et al., 2014b; Chen et al., 2016). In order to improve sentiment analysis results, large sentiment lexicons have been built (Wilson et al., 2005; Baccianella et al., 2010; Tang et al., 2014a). A sentiment lexicon is a set of words such as excellent, terrible and ordinary, each of which is assigned a fixed positive or negative score that presents its sentiment polarity and strength (Tang et al., 2014a). Such information can serve as sentiment-informative features and significantly boost the classification performance (Agarwal et al., 2013; Teng et al., 2016; Qian et al., 2016). In sentiment classification tasks, sentiment words such as great and terrible tend to play a more critical role than other words in texts (Liu, 2010). One attribute of words annotated in"
C18-1074,D16-1171,0,0.520128,", in this work, we propose a novel lexicon-based supervised attention model (LBSA), which allows a recurrent neural network to focus on the sentiment content, thus generating sentiment-informative representations. Compared with general attention models, our model has better interpretability and less noise. Experimental results on three large-scale sentiment classification datasets showed that the proposed method outperforms previous methods. 1 Introduction Sentiment analysis has the goal of analyzing people’s sentiments or opinions and has been well explored (Turney, 2002; Tang et al., 2014b; Chen et al., 2016). In order to improve sentiment analysis results, large sentiment lexicons have been built (Wilson et al., 2005; Baccianella et al., 2010; Tang et al., 2014a). A sentiment lexicon is a set of words such as excellent, terrible and ordinary, each of which is assigned a fixed positive or negative score that presents its sentiment polarity and strength (Tang et al., 2014a). Such information can serve as sentiment-informative features and significantly boost the classification performance (Agarwal et al., 2013; Teng et al., 2016; Qian et al., 2016). In sentiment classification tasks, sentiment word"
C18-1074,D16-1057,0,0.0571043,"Missing"
C18-1074,C16-1291,0,0.0151058,"erage number of sentences in a document, while Words/sent denotes the average length of a sentence. 2.3 Jointly Supervised Classification and Attention The final document vector d is used to conduct sentiment classification, which requires the probability of labeling a document with sentiment polarity c, c ∈ [1, C], where C represents the total number of classes. The probability is computed by a softmax function: p = sof tmax(Wc d + bc ). (11) To jointly supervise the classification and attention, we introduce a soft constraint method that is employed by other tasks with supervised attention (Liu et al., 2016). Specifically, it is defined as follows: L=− C X c=1 gc log(pc ) + µw · T X i=1 4(a?i , ai ) + µs · 4(a? , a), (12) where the classification loss function is cross entropy. gc ∈ RC denotes the ground truth of the sentiment classification, presented by a one-hot vector. pc ∈ RC is the predicted probability for each class. a?i and a? are gold attention vectors for the word and sentence levels, respectively. ai and a are learned attention vectors for the word and sentence levels, respectively. 4 is a loss function that indicates the disagreement between two vectors. Because the learned attention"
C18-1074,D17-1048,0,0.333232,"mechanism with such sentiment information. Yang et al. (2016) and Chen et al. (2016) proposed a hierarchical RNN model to learn attention weights based on the local context using an unsupervised method. However, their method may induce much noise and suffers from a lack of interpretability because it tends to capture some domain-specific words (Mudinas et al., 2012) instead of real sentiment information. For example, in movie reviews, the name of a movie with a good reputation tends to be regarded as positive words, and is thereby assigned higher weights, which does not work in other domains. Long et al. (2017) incorporated the reading time of human beings into the attention mechanism, but their method also has much noise because during the reading process, people tend to spend more time on intricate contents (Goodman, 1988) than on real sentiment words like good and bad. Other methods employed external information such as users and products to guide attention weights (Ma et al., 2017; Chen et al., 2016), which can boost the classification performance by a large margin. However, most of the time, we have no access to such external information. In this paper, we propose a novel Lexicon-Based Supervis"
C18-1074,I17-1064,0,0.078577,"sentiment information. For example, in movie reviews, the name of a movie with a good reputation tends to be regarded as positive words, and is thereby assigned higher weights, which does not work in other domains. Long et al. (2017) incorporated the reading time of human beings into the attention mechanism, but their method also has much noise because during the reading process, people tend to spend more time on intricate contents (Goodman, 1988) than on real sentiment words like good and bad. Other methods employed external information such as users and products to guide attention weights (Ma et al., 2017; Chen et al., 2016), which can boost the classification performance by a large margin. However, most of the time, we have no access to such external information. In this paper, we propose a novel Lexicon-Based Supervised Attention model (LBSA) that combines sentiment lexicons and an attention mechanism to better extract sentiment information and form This work is licenced under a Creative Commons Attribution 4.0 International Licence. creativecommons.org/licenses/by/4.0/ Licence details: http:// 868 Proceedings of the 27th International Conference on Computational Linguistics, pages 868–877 S"
C18-1074,D13-1170,0,0.308042,"Net3.0 (SWN) (Baccianella et al., 2010). The original scores in SWN are the probabilities of positive, negative, or neutral polarities for words. We take the maximum sum of the positive and negative scores as the sentiment degree for different part-of-speech tags. The second part is extracted from MPQA (Wilson et al., 2005), which tags polarity ratings for sentiment words. To a word whose polarity is positive or negative, we assign 1 as its sentiment degree. Otherwise, the word’s sentiment degree is 0. The third part consists of the leaf nodes of the Stanford Sentiment Treebank (SST) dataset (Socher et al., 2013). The sentiment scores of 1 ˜ http://ir.hit.edu.cn/dytang/paper/acl2015/dataset.7z 872 SST are in the range of [0, 1]. We scale it to [-1, 1] and keep the absolute value as the sentiment degree. The last part is the sentiment lexicon of the Hownet Knowledge Database (HKD)2 , which contains a list of positive or negative words. We conduct the pre-processing in the same way as for MPQA. Finally, we combine the four parts and average the sentiment degree for words appearing in two or more lexicon parts. All of the neutral words in different parts are included in our lexicon because of the ignoran"
C18-1074,C14-1018,0,0.266272,"eve the above target, in this work, we propose a novel lexicon-based supervised attention model (LBSA), which allows a recurrent neural network to focus on the sentiment content, thus generating sentiment-informative representations. Compared with general attention models, our model has better interpretability and less noise. Experimental results on three large-scale sentiment classification datasets showed that the proposed method outperforms previous methods. 1 Introduction Sentiment analysis has the goal of analyzing people’s sentiments or opinions and has been well explored (Turney, 2002; Tang et al., 2014b; Chen et al., 2016). In order to improve sentiment analysis results, large sentiment lexicons have been built (Wilson et al., 2005; Baccianella et al., 2010; Tang et al., 2014a). A sentiment lexicon is a set of words such as excellent, terrible and ordinary, each of which is assigned a fixed positive or negative score that presents its sentiment polarity and strength (Tang et al., 2014a). Such information can serve as sentiment-informative features and significantly boost the classification performance (Agarwal et al., 2013; Teng et al., 2016; Qian et al., 2016). In sentiment classification"
C18-1074,P14-1146,0,0.505478,"eve the above target, in this work, we propose a novel lexicon-based supervised attention model (LBSA), which allows a recurrent neural network to focus on the sentiment content, thus generating sentiment-informative representations. Compared with general attention models, our model has better interpretability and less noise. Experimental results on three large-scale sentiment classification datasets showed that the proposed method outperforms previous methods. 1 Introduction Sentiment analysis has the goal of analyzing people’s sentiments or opinions and has been well explored (Turney, 2002; Tang et al., 2014b; Chen et al., 2016). In order to improve sentiment analysis results, large sentiment lexicons have been built (Wilson et al., 2005; Baccianella et al., 2010; Tang et al., 2014a). A sentiment lexicon is a set of words such as excellent, terrible and ordinary, each of which is assigned a fixed positive or negative score that presents its sentiment polarity and strength (Tang et al., 2014a). Such information can serve as sentiment-informative features and significantly boost the classification performance (Agarwal et al., 2013; Teng et al., 2016; Qian et al., 2016). In sentiment classification"
C18-1074,P15-1098,0,0.512148,"as the metric: X 4(a? , a) = − a?i log(ai ). (13) i µw and µs are the coefficients for attention loss functions, which can balance the preference between classification and attention disagreements, to alleviate the overfitting problem. 3 Experimental Settings In this section, we introduce the experimental settings in detail, including the datasets and lexicons, hyper-parameter settings, and baseline models. 3.1 Datasets and Lexicons We conduct experiments to evaluate the effectiveness of our method on three document-level review datasets: IMDB, Yelp 2013 and Yelp 2014, which are developed by Tang et al. (2015). We split the datasets into training, development and testing sets in the proportion of 8:1:1, using pre-processing in the same way as Tang et al. (2015). Table 1 summarizes the statistics of the datasets. All of these datasets can be publicly accessed1 . The sentiment lexicon contains four parts. During the process of constructing our lexicon, we only use the sentiment scores for unigrams. The first part comes from SentimentWordNet3.0 (SWN) (Baccianella et al., 2010). The original scores in SWN are the probabilities of positive, negative, or neutral polarities for words. We take the maximum"
C18-1074,D16-1169,0,0.536361,"pinions and has been well explored (Turney, 2002; Tang et al., 2014b; Chen et al., 2016). In order to improve sentiment analysis results, large sentiment lexicons have been built (Wilson et al., 2005; Baccianella et al., 2010; Tang et al., 2014a). A sentiment lexicon is a set of words such as excellent, terrible and ordinary, each of which is assigned a fixed positive or negative score that presents its sentiment polarity and strength (Tang et al., 2014a). Such information can serve as sentiment-informative features and significantly boost the classification performance (Agarwal et al., 2013; Teng et al., 2016; Qian et al., 2016). In sentiment classification tasks, sentiment words such as great and terrible tend to play a more critical role than other words in texts (Liu, 2010). One attribute of words annotated in sentiment lexicons is their sentiment strength, which is intuitively associated with a word’s contribution to the sentiment representation of a sentence. It is similar to the basic idea of the attention mechanism that not all words have the same importance. However, very few studies have focused on a method that combines an attention mechanism with such sentiment information. Yang et al."
C18-1074,P02-1053,0,0.0548251,"lysis. To achieve the above target, in this work, we propose a novel lexicon-based supervised attention model (LBSA), which allows a recurrent neural network to focus on the sentiment content, thus generating sentiment-informative representations. Compared with general attention models, our model has better interpretability and less noise. Experimental results on three large-scale sentiment classification datasets showed that the proposed method outperforms previous methods. 1 Introduction Sentiment analysis has the goal of analyzing people’s sentiments or opinions and has been well explored (Turney, 2002; Tang et al., 2014b; Chen et al., 2016). In order to improve sentiment analysis results, large sentiment lexicons have been built (Wilson et al., 2005; Baccianella et al., 2010; Tang et al., 2014a). A sentiment lexicon is a set of words such as excellent, terrible and ordinary, each of which is assigned a fixed positive or negative score that presents its sentiment polarity and strength (Tang et al., 2014a). Such information can serve as sentiment-informative features and significantly boost the classification performance (Agarwal et al., 2013; Teng et al., 2016; Qian et al., 2016). In sentim"
C18-1074,H05-1044,0,0.714174,"eural network to focus on the sentiment content, thus generating sentiment-informative representations. Compared with general attention models, our model has better interpretability and less noise. Experimental results on three large-scale sentiment classification datasets showed that the proposed method outperforms previous methods. 1 Introduction Sentiment analysis has the goal of analyzing people’s sentiments or opinions and has been well explored (Turney, 2002; Tang et al., 2014b; Chen et al., 2016). In order to improve sentiment analysis results, large sentiment lexicons have been built (Wilson et al., 2005; Baccianella et al., 2010; Tang et al., 2014a). A sentiment lexicon is a set of words such as excellent, terrible and ordinary, each of which is assigned a fixed positive or negative score that presents its sentiment polarity and strength (Tang et al., 2014a). Such information can serve as sentiment-informative features and significantly boost the classification performance (Agarwal et al., 2013; Teng et al., 2016; Qian et al., 2016). In sentiment classification tasks, sentiment words such as great and terrible tend to play a more critical role than other words in texts (Liu, 2010). One attri"
C18-1074,D16-1172,1,0.908819,"ssifier. Paragraph + Vector (Le and Mikolov, 2014) learns distributed representations of a document for classification. RNTN + RNN (Socher et al., 2013) represents sentences with the Recursive Neural Tensor Network (RNTN) and feeds them into a recurrent neural networks (RNN) to obtain document representations. UPNN (Tang et al., 2015) uses a text preference matrix and vector for each user and product as extra information to train a CNN sentiment classifier. UPNN(noUP) only uses CNN without considering user and product information. The models in Group 2 are based on sentiment lexicons. BiLSTM (Xu et al., 2016) is a bidirectional LSTM baseline with a simple average pooling layer. AveLex naively averages sentiment scores of words in the document to measure the overall sentiment polarities according to our sentiment lexicon. BiLSTM + Lex (Teng et al., 2016) takes the local context into consideration, which leverages a bidirectional LSTM to capture context information and calculates the weighted sum of the sentiment scores. The two lexicon-based methods only work on the word level because sentences do not have a gold sentiment score. For a fair comparison, BiLSTM + LBSA is our model, which removes the"
C18-1074,N16-1174,0,0.444225,"et al., 2016; Qian et al., 2016). In sentiment classification tasks, sentiment words such as great and terrible tend to play a more critical role than other words in texts (Liu, 2010). One attribute of words annotated in sentiment lexicons is their sentiment strength, which is intuitively associated with a word’s contribution to the sentiment representation of a sentence. It is similar to the basic idea of the attention mechanism that not all words have the same importance. However, very few studies have focused on a method that combines an attention mechanism with such sentiment information. Yang et al. (2016) and Chen et al. (2016) proposed a hierarchical RNN model to learn attention weights based on the local context using an unsupervised method. However, their method may induce much noise and suffers from a lack of interpretability because it tends to capture some domain-specific words (Mudinas et al., 2012) instead of real sentiment information. For example, in movie reviews, the name of a movie with a good reputation tends to be regarded as positive words, and is thereby assigned higher weights, which does not work in other domains. Long et al. (2017) incorporated the reading time of human bei"
C18-1074,D16-1024,0,0.0241342,"an attention mechanism, which was proven to be effective in our experiments. Recently, the attention mechanism has been widely studied in sentiment classification. Yang et al. (2016) proposed two attention-based bidirectional GRUs to enforce the neural networks to attend to 875 the related part of a sentence or document. Apart from local contexts, Chen et al. (2016) incorporated extra information such as users and products in review datasets into a hierarchical attention mechanism. Ma et al. (2017) cascaded multiway of user and product information to enhance the effects of different aspects. Zhou et al. (2016) proposed a LSTM network based on an attention mechanism for cross-lingual sentiment classification at the document level. The model consists of two attention-based hierarchical LSTMs for bilingual representation. Long et al. (2017) took cognition into consideration, leveraging extra resources including the reading time of human beings. They proposed a mutimodel that learns to predict the reading time to construct the cognitive attention. The main difference between our method and others is that we supervise the attention weights using sentiment information. As a result, it is able to capture"
C18-1150,W05-0909,0,0.240213,", and we compress it to a 300-dimension vector using 2-layer fully-connected layer. The upper settings are the same for all neural network models. We set batch size, rollout size, D1-step, G1-step, G2-step and G3-step as 64, 16, 5, 1, 2 and 1, respectively. γ for the training of D2 is set to 2.0 empirically. 3.4 Automatic Evaluation There is no direct evaluation metric to determine whether a question is natural or not. We thus use several relevance scores for the automatic evaluation following the setting of existing researches, including Corpus BLEU-4, BLEU-4 (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), ROUGE (Lin and Hovy, 2003) and CIDEr (Vedantam et al., 2015). The overall experiment results in terms of five relevance scores are shown in Table 1. We have some findings as follows: 1 The original dataset contains three parts, namely, MS-COCO, Flickr and Bing, VQG-MSCOCO. However, images from Flickr and Bing are quite different from those in Visual Question Answering (VQA) dataset (Antol et al., 2015), and this makes it difficult to use questions from VQA as negative samples to train D2 . 1768 Model BLEU-4 KNN Img2Seq Img2Seqpre−train MIXER-BLEU-4 ReinforceD1 ReinforceD2 ReinforceD1 +D2 37."
C18-1150,D14-1179,0,0.0420663,"Missing"
C18-1150,P17-1123,0,0.0666076,"is paper locates in the research filed of question generation and reinforcement learning for sequence generation. We will focus on related works from these two domains. Question Generation Question generation has been researched for years from textual input (Rus et al., 2010; Heilman, 2011). Researchers start from rule-based method that extracts key aspects from the input text and then insert these aspects into human generated templates for interrogative sentence generation (Heilman, 2011). Recently, sequence-to-sequence model is utilized for question generation in description-question pairs (Du et al., 2017; Tang et al., 2017; Serban et al., 2016). Although these models generate better performance, the characteristics of question is still ignored. On the other hand, research about visual question generation is much less (Ren et al., 2015; Vijayakumar et al., 2018; 1770 Mostafazadeh et al., 2016; Shijie et al., 2017). Diversity as another important characteristic of question also draws much attention. Li et al. (2016) proposed to use Maximum Mutual Information (MMI) as the objective function for result diversification. Vijayakumar et al. (2018) proposed a diverse beam search for generated multipl"
C18-1150,N16-1014,0,0.0721589,"Missing"
C18-1150,N03-1020,0,0.326693,"nsion vector using 2-layer fully-connected layer. The upper settings are the same for all neural network models. We set batch size, rollout size, D1-step, G1-step, G2-step and G3-step as 64, 16, 5, 1, 2 and 1, respectively. γ for the training of D2 is set to 2.0 empirically. 3.4 Automatic Evaluation There is no direct evaluation metric to determine whether a question is natural or not. We thus use several relevance scores for the automatic evaluation following the setting of existing researches, including Corpus BLEU-4, BLEU-4 (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), ROUGE (Lin and Hovy, 2003) and CIDEr (Vedantam et al., 2015). The overall experiment results in terms of five relevance scores are shown in Table 1. We have some findings as follows: 1 The original dataset contains three parts, namely, MS-COCO, Flickr and Bing, VQG-MSCOCO. However, images from Flickr and Bing are quite different from those in Visual Question Answering (VQA) dataset (Antol et al., 2015), and this makes it difficult to use questions from VQA as negative samples to train D2 . 1768 Model BLEU-4 KNN Img2Seq Img2Seqpre−train MIXER-BLEU-4 ReinforceD1 ReinforceD2 ReinforceD1 +D2 37.062 36.744 37.522 41.674 38."
C18-1150,P16-1170,0,0.0939854,"Missing"
C18-1150,P02-1040,0,0.100776,"riginal dimension of fc7 is 4096, and we compress it to a 300-dimension vector using 2-layer fully-connected layer. The upper settings are the same for all neural network models. We set batch size, rollout size, D1-step, G1-step, G2-step and G3-step as 64, 16, 5, 1, 2 and 1, respectively. γ for the training of D2 is set to 2.0 empirically. 3.4 Automatic Evaluation There is no direct evaluation metric to determine whether a question is natural or not. We thus use several relevance scores for the automatic evaluation following the setting of existing researches, including Corpus BLEU-4, BLEU-4 (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), ROUGE (Lin and Hovy, 2003) and CIDEr (Vedantam et al., 2015). The overall experiment results in terms of five relevance scores are shown in Table 1. We have some findings as follows: 1 The original dataset contains three parts, namely, MS-COCO, Flickr and Bing, VQG-MSCOCO. However, images from Flickr and Bing are quite different from those in Visual Question Answering (VQA) dataset (Antol et al., 2015), and this makes it difficult to use questions from VQA as negative samples to train D2 . 1768 Model BLEU-4 KNN Img2Seq Img2Seqpre−train MIXER-BLEU-4 Reinforc"
C18-1150,D14-1162,0,0.0820416,"rial learning network to better train the generator under the reinforcement learning framework. - ReinforceD2 : it uses the score of D2 as the reward to guide the training of the generator. This model is comparable to MIXER-BLEU-4 because both models utilize a static way to produce reward (BLEU score with ground truth questions in MIXER-BLEU-4 and classification confidence in ReinforceD2 ) - ReinforceD1 +D2 : this is our proposed model. 3.3 Training Details For the generator, we use GRU cell and the number of cells is 512; the dimension of word embedding is 300 and is pre-trained using GloVe (Pennington et al., 2014). The image feature, fc7 is the output of the 7th fully-connected layer in VGGNet. The original dimension of fc7 is 4096, and we compress it to a 300-dimension vector using 2-layer fully-connected layer. The upper settings are the same for all neural network models. We set batch size, rollout size, D1-step, G1-step, G2-step and G3-step as 64, 16, 5, 1, 2 and 1, respectively. γ for the training of D2 is set to 2.0 empirically. 3.4 Automatic Evaluation There is no direct evaluation metric to determine whether a question is natural or not. We thus use several relevance scores for the automatic ev"
C18-1150,W10-4234,0,0.035598,"iddle-level. This indicates that optimizing a single evaluation metric is not sufficient enough for generating high quality natural questions. - The gap between ground-truth questions and machine generated questions is still large. This indicates that there is still a large room for question generation system to improve. 4 Related Works This paper locates in the research filed of question generation and reinforcement learning for sequence generation. We will focus on related works from these two domains. Question Generation Question generation has been researched for years from textual input (Rus et al., 2010; Heilman, 2011). Researchers start from rule-based method that extracts key aspects from the input text and then insert these aspects into human generated templates for interrogative sentence generation (Heilman, 2011). Recently, sequence-to-sequence model is utilized for question generation in description-question pairs (Du et al., 2017; Tang et al., 2017; Serban et al., 2016). Although these models generate better performance, the characteristics of question is still ignored. On the other hand, research about visual question generation is much less (Ren et al., 2015; Vijayakumar et al., 201"
C18-1150,P16-1056,0,0.080342,"Missing"
C18-1150,D17-1090,0,0.0513244,"in the research filed of question generation and reinforcement learning for sequence generation. We will focus on related works from these two domains. Question Generation Question generation has been researched for years from textual input (Rus et al., 2010; Heilman, 2011). Researchers start from rule-based method that extracts key aspects from the input text and then insert these aspects into human generated templates for interrogative sentence generation (Heilman, 2011). Recently, sequence-to-sequence model is utilized for question generation in description-question pairs (Du et al., 2017; Tang et al., 2017; Serban et al., 2016). Although these models generate better performance, the characteristics of question is still ignored. On the other hand, research about visual question generation is much less (Ren et al., 2015; Vijayakumar et al., 2018; 1770 Mostafazadeh et al., 2016; Shijie et al., 2017). Diversity as another important characteristic of question also draws much attention. Li et al. (2016) proposed to use Maximum Mutual Information (MMI) as the objective function for result diversification. Vijayakumar et al. (2018) proposed a diverse beam search for generated multiple questions. Fan et"
C18-1232,D16-1053,0,0.0229658,"long and short term dependency information in a sequence. Specific to text classification task, most of the models cannot deal with the texts of several sentences (paragraphs, documents), such as MV-RNN (Socher et al., 2012), RNTN (Socher et al., 2013), CNN (Kim, 2014), AdaSent (Zhao et al., 2015), and so on. The simple neural bag-of-words model can deal with long texts, but it loses the word order information. PV (Le and Mikolov, 2014) works in an unsupervised way, and the learned vector cannot be fine-tuned on the specific task. There are also many works (Liu et al., 2015; Xu et al., 2016; Cheng et al., 2016) to improve LSTM’s ability to carrying information for a long distance. A line of orthogonal researches (Lin et al., 2017; Yang et al., 2016; Shen et al., 2018a; Shen et al., 2018b) is to introduce attention mechanism (Vaswani et al., 2017) to weighted average the outputs of CNN/RNN layer. The attention mechanism can effectively reduce the burden of CNN/RNN. The CNN/RNN encoding layer is only expected to extract local context information for each word, while the global semantics of text sequence can be aggregated from the local encoding vectors. The attention based aggregation collects informa"
C18-1232,J81-4005,0,0.746067,"Missing"
C18-1232,D14-1181,0,0.273671,"rtant natural language processing applications. A primary challenge is how to encode the variable-length text sequence into a fixed-size vector, which should fully capture the semantics of text. Many successful text encoding methods usually contain three key steps: (1) converting each word in a text sequence into its embedding; (2) taking as input the sequence of word embeddings, and computing the context-aware representation for each word with a recurrent neural network (RNN) (Hochreiter and Schmidhuber, 1997; Chung et al., 2014) or convolutional neural network (CNN) (Collobert et al., 2011; Kim, 2014); (3) summarizing the sentence meaning into a fixed-size vector by an aggregation operation. Then, these models are trained by combining a downstream task in a supervised or unsupervised way. Currently, much attention is paid to the first two steps, while the aggregation step is less emphasized on. Some simple aggregation methods, such as max (or average) pooling, is used to sum the RNN hidden states or convolved vectors, computed in the previous step, into a single vector. This kind of methods aggregate information in a bottom-up and passive way and are lack of the guide of task information."
C18-1232,D15-1280,1,0.840149,"ated encoding models to capture the long and short term dependency information in a sequence. Specific to text classification task, most of the models cannot deal with the texts of several sentences (paragraphs, documents), such as MV-RNN (Socher et al., 2012), RNTN (Socher et al., 2013), CNN (Kim, 2014), AdaSent (Zhao et al., 2015), and so on. The simple neural bag-of-words model can deal with long texts, but it loses the word order information. PV (Le and Mikolov, 2014) works in an unsupervised way, and the learned vector cannot be fine-tuned on the specific task. There are also many works (Liu et al., 2015; Xu et al., 2016; Cheng et al., 2016) to improve LSTM’s ability to carrying information for a long distance. A line of orthogonal researches (Lin et al., 2017; Yang et al., 2016; Shen et al., 2018a; Shen et al., 2018b) is to introduce attention mechanism (Vaswani et al., 2017) to weighted average the outputs of CNN/RNN layer. The attention mechanism can effectively reduce the burden of CNN/RNN. The CNN/RNN encoding layer is only expected to extract local context information for each word, while the global semantics of text sequence can be aggregated from the local encoding vectors. The attent"
C18-1232,D14-1162,0,0.0841878,"this section, we are going to introduce a general text classification framework. It consists of an Embedding Layer, Encoding Layer, Aggregation Layer and Prediction Layer. 2.1 Embedding Layer Given a text sequence with words S = w1 , w2 , · · · , wL . Since the words are symbols that could not be processed directly using prominent neural architectures, so we first map each word into a d dimensional embedding vector, X = [x1 , x1 , x2 , · · · , xL ]. (1) In order to transfer knowledge from a vast unlabeled corpus, the embeddings can be taken from the pre-trained word embedding, such as Glove (Pennington et al., 2014). 2.2 Encoding Layer However, each word representation in X is still independent with each other. To gain some dependency between adjacent words, we then build a bi-directional LSTM (BiLSTM) layer (Hochreiter and Schmidhuber, 1997) to incorporate forward and backward context information of a sequence. Then we can get phrase-level encoding ht of a word by concatenating forward hft and backward output vector hbt correspond to the target word. hft = LSTM(hft−1 , xt ), (2) hbt = LSTM(hbt+1 , xt ), (3) ht = [hft ; hbt ]. (4) Thus, the outputs of BiLSTM encoder are a sequence of vectors H = [h1 , h2"
C18-1232,D12-1110,0,0.0346568,"2 3 Iteration 4 5 Figure 2: Relationship between test accuracy and routing iteration, where the vertical axis denotes test accuracy and the horizontal axis denotes routing iteration. When the iteration is set to 3 performance peaks on several different capsule number setting 6 Related Work Currently, much attention has been paid to how developing a sophisticated encoding models to capture the long and short term dependency information in a sequence. Specific to text classification task, most of the models cannot deal with the texts of several sentences (paragraphs, documents), such as MV-RNN (Socher et al., 2012), RNTN (Socher et al., 2013), CNN (Kim, 2014), AdaSent (Zhao et al., 2015), and so on. The simple neural bag-of-words model can deal with long texts, but it loses the word order information. PV (Le and Mikolov, 2014) works in an unsupervised way, and the learned vector cannot be fine-tuned on the specific task. There are also many works (Liu et al., 2015; Xu et al., 2016; Cheng et al., 2016) to improve LSTM’s ability to carrying information for a long distance. A line of orthogonal researches (Lin et al., 2017; Yang et al., 2016; Shen et al., 2018a; Shen et al., 2018b) is to introduce attentio"
C18-1232,D13-1170,0,0.0181608,"ts. Note that we use the same document level datasets provided in (Tang et al., 2015). Yelp reviews Yelp-2013 and Yelp-2014 are reviews from Yelp, each example consists of several review sentences and a rating score range from 1 to 5 (higher is better). IMDB is a movie review dataset extracted from IMDB website. It is a multi-sentence dataset that for each example there are several review sentences. A rating score range from 1 to 10 is also associated with each example. SST-1 Stanford Sentiment Treebank is a movie review dataset which has been parsed and further splited to train/dev/test set (Socher et al., 2013). For each example in the dataset, there exists only one sentence and a label associated with it. And the labels can be one of {negative, somewhat negative, neutral, somewhat positive, positive}. SST-2 This dataset is a binary-class version of SST-1, with neutral reviews removed and the remaining reviews categorized to either negative or positive. 2747 Yelp-2013 Yelp-2014 IMDB SST-1 SST-2 Embedding size LSTM hidden unit Capsule dimension Capsule number Iteration number Regularization rate Initial learning rate learning rate decay learning rate decay steps Initial Batch size Batch size low boun"
C18-1232,P15-1098,0,0.0337063,"ocument encoding. 5 Experiment We test the empirical performance of our proposed model on 5 benchmark datasets for document and sentence level classification and compare our proposed model to other competitor models. 5.1 Datasets To evaluate the effectiveness of our proposed aggregation method, we have conducted experiments on 5 datasets, the statistics of experimented datasets are shown in Table 1. As shown in the table, Yelp-2013, Yelp-2014, and IMDB are document level datasets, while SST-1 and SST-2 are sentence level datasets. Note that we use the same document level datasets provided in (Tang et al., 2015). Yelp reviews Yelp-2013 and Yelp-2014 are reviews from Yelp, each example consists of several review sentences and a rating score range from 1 to 5 (higher is better). IMDB is a movie review dataset extracted from IMDB website. It is a multi-sentence dataset that for each example there are several review sentences. A rating score range from 1 to 10 is also associated with each example. SST-1 Stanford Sentiment Treebank is a movie review dataset which has been parsed and further splited to train/dev/test set (Socher et al., 2013). For each example in the dataset, there exists only one sentence"
C18-1232,D16-1172,1,0.867699,"ls to capture the long and short term dependency information in a sequence. Specific to text classification task, most of the models cannot deal with the texts of several sentences (paragraphs, documents), such as MV-RNN (Socher et al., 2012), RNTN (Socher et al., 2013), CNN (Kim, 2014), AdaSent (Zhao et al., 2015), and so on. The simple neural bag-of-words model can deal with long texts, but it loses the word order information. PV (Le and Mikolov, 2014) works in an unsupervised way, and the learned vector cannot be fine-tuned on the specific task. There are also many works (Liu et al., 2015; Xu et al., 2016; Cheng et al., 2016) to improve LSTM’s ability to carrying information for a long distance. A line of orthogonal researches (Lin et al., 2017; Yang et al., 2016; Shen et al., 2018a; Shen et al., 2018b) is to introduce attention mechanism (Vaswani et al., 2017) to weighted average the outputs of CNN/RNN layer. The attention mechanism can effectively reduce the burden of CNN/RNN. The CNN/RNN encoding layer is only expected to extract local context information for each word, while the global semantics of text sequence can be aggregated from the local encoding vectors. The attention based aggrega"
C18-1232,N16-1174,0,0.700966,"gation operation. Then, these models are trained by combining a downstream task in a supervised or unsupervised way. Currently, much attention is paid to the first two steps, while the aggregation step is less emphasized on. Some simple aggregation methods, such as max (or average) pooling, is used to sum the RNN hidden states or convolved vectors, computed in the previous step, into a single vector. This kind of methods aggregate information in a bottom-up and passive way and are lack of the guide of task information. Recently, several works employ self-attention mechanism (Lin et al., 2017; Yang et al., 2016) on top of the recurrent or convolutional encoding layer to replace simple pooling. A basic assumption is that the words (or even sentences) are not equally important. One or several task-specific context vectors are used to assign a different weight to each word and select task-specific encodings. The context vectors are parameters learned jointly with other parameters during the training process. These attentive aggregation can select task-dependent information. However, the context vectors are fixed once learned. ∗ 1 Corresponding Author https://github.com/FudanNLP/Capsule4TextClassificatio"
C18-1314,N16-1165,0,0.0352315,"Missing"
C18-1314,D16-1053,0,0.0182811,"lignment matrix based on two inputs, which can model complex interactions between the two inputs. Xiong et al. (2016) present a co-attention encoder to focus on relevant parts of the representations of the question and document and use a dynamic pointing decoder to locate the answer. Cui et al. (2016) propose a two-way attention mechanism to encode the passage and question mutually and induce attended attention for final answer predictions. Self-attention mechanism is an attention mechanism aiming at aligning the sequence with itself, which has been successfully used in a variety of tasks. In Cheng et al. (2016), both encoder and decoder are modeled as LSTMs with self-attention for extractive summarization of documents. In Lin et al. (2017), the authors conduct a self-attention over the hidden states of a BiLSTM to extract the sentence embedding. Instead of sentence vector, they use a 2-D matrix to represent the embedding, with each row of the matrix attending on a different part of the sentence. In this work, we employ a co-attention mechanism to capture the interactions between the original post and the reply on argument level. What’s more, we use a self-attention mechanism to obtain the argument r"
C18-1314,W14-4012,0,0.0128942,"Missing"
C18-1314,K17-1017,0,0.0774691,"Matrix Alignment Matrix Post to reply argument attention + Weights GRU GRU GRU u OP Argument Vector Original Post Softmax r1R r1O P uOP GRU r r ... r Co-Attention Network Attention Post argument to reply argument attention R m OP 2 n {U*i }i=1 Co-Attention Network r ... R 2 r1O P Softmax Aggregation Network r X feat O* Attention Pooling R 1 r1R Original Post Reply ... r2R rmR Reply Figure 2: Overall architecture of the proposed model. The left part is the main framework of this work. The right part is the detailed structure of the co-attention network. 2.1 Argument Representation Inspired by Dong et al. (2017), we employ a hierarchical architecture to obtain two different representations for each single argument. For simplicity, we consider each sentence as an argument. Representation based on internal words given an argument with words w1 , w2 , ..., wT , we first map each word to a dense vector obtaining x1 , x2 , ..., xT correspondingly. We then employ a convolution layer to incorporate the context information on word level. zi = f (Wz · [xi : xi+hw −1 ] + bz ) (1) where Wz and bz are weight matrix and bias vector. hw is the window size in the convolution layer and zi is the feature representati"
C18-1314,C14-1089,0,0.0293406,"results on a publicly available dataset show that the proposed model significantly outperforms some state-of-the-art methods for persuasiveness evaluation. Further analysis reveals that attention weights computed in our model are able to extract interactive argument pairs from the original post and the reply. 1 Introduction Computational argumentation is a growing field in natural language processing. Existing research covers argument unit detection (Al-Khatib et al., 2016), argument structure prediction (Peldszus and Stede, 2015; Stab and Gurevych, 2014), argumentation scheme classification (Feng et al., 2014), etc. Recently, the automatic assessment of argumentation quality has started gaining attention. It can be analyzed at two levels, namely monological argumentation and dialogical argumentation. Monological argumentation refers to a composition of arguments on a certain issue (Wachsmuth et al., 2017). A typical example of quality evaluation for monological argumentation is automated essay scoring, which aims to process argumentative essays without human interference (Taghipour and Ng, 2016). It takes an essay as the input and outputs a numeric score, considering features of content, grammar, d"
C18-1314,D16-1129,0,0.350129,"nt, grammar, discourse structure and lexical richness (Burstein et al., 2013). Most of the efforts are made on the exploration for better document representation. Dialogical argumentation refers to a series of interactive arguments related to a given topic, involving argument retraction, view exchange, and so on (Besnard et al., 2014). With the popularity of online debating forums like convinceme 1 , debatepedia 2 and change my view (CMV) 3 , researchers pay increasing attention to evaluating the quality of debating or persuasive content (Tan et al., 2016; Wei and Liu, 2016; Wei et al., 2016; Habernal and Gurevych, 2016a). Although some similarity features of content between the reply and the original post are used to evaluate the quality of the given reply, they are computed on word level without considering the exchange of opinions on the basis of arguments. An example of dialogical argumentation is shown in Figure 1. There are three posts, one is original and the other two are replies. The repliers post to change the view of the original poster. And the positive reply is deemed to be more persuasive than the negative reply. We have three observations. First, viewpoints of the original poster and repliers"
C18-1314,P16-1150,0,0.458692,"nt, grammar, discourse structure and lexical richness (Burstein et al., 2013). Most of the efforts are made on the exploration for better document representation. Dialogical argumentation refers to a series of interactive arguments related to a given topic, involving argument retraction, view exchange, and so on (Besnard et al., 2014). With the popularity of online debating forums like convinceme 1 , debatepedia 2 and change my view (CMV) 3 , researchers pay increasing attention to evaluating the quality of debating or persuasive content (Tan et al., 2016; Wei and Liu, 2016; Wei et al., 2016; Habernal and Gurevych, 2016a). Although some similarity features of content between the reply and the original post are used to evaluate the quality of the given reply, they are computed on word level without considering the exchange of opinions on the basis of arguments. An example of dialogical argumentation is shown in Figure 1. There are three posts, one is original and the other two are replies. The repliers post to change the view of the original poster. And the positive reply is deemed to be more persuasive than the negative reply. We have three observations. First, viewpoints of the original poster and repliers"
C18-1314,P15-1107,0,0.0352229,"xt similarity. The interactions among argument pairs are ignored. In this work, we evaluate the quality of debate comments through the interactions among them on argument level. 5.2 Attention mechanism Attention mechanism allows models to focus on specific parts of inputs at each step of a task. Moreover, attention mechanism has been proved to be significantly effective in natural language processing tasks. Co-attention mechanism has recently attracted lots of research interest in the fields of machine translation (Bahdanau et al., 2014), question answering (Wu et al., 2017), text generation (Li et al., 2015), etc. It is computed as an alignment matrix based on two inputs, which can model complex interactions between the two inputs. Xiong et al. (2016) present a co-attention encoder to focus on relevant parts of the representations of the question and document and use a dynamic pointing decoder to locate the answer. Cui et al. (2016) propose a two-way attention mechanism to encode the passage and question mutually and induce attended attention for final answer predictions. Self-attention mechanism is an attention mechanism aiming at aligning the sequence with itself, which has been successfully us"
C18-1314,D15-1110,0,0.0767786,"network to capture the interactions between participants on argument level. Experimental results on a publicly available dataset show that the proposed model significantly outperforms some state-of-the-art methods for persuasiveness evaluation. Further analysis reveals that attention weights computed in our model are able to extract interactive argument pairs from the original post and the reply. 1 Introduction Computational argumentation is a growing field in natural language processing. Existing research covers argument unit detection (Al-Khatib et al., 2016), argument structure prediction (Peldszus and Stede, 2015; Stab and Gurevych, 2014), argumentation scheme classification (Feng et al., 2014), etc. Recently, the automatic assessment of argumentation quality has started gaining attention. It can be analyzed at two levels, namely monological argumentation and dialogical argumentation. Monological argumentation refers to a composition of arguments on a certain issue (Wachsmuth et al., 2017). A typical example of quality evaluation for monological argumentation is automated essay scoring, which aims to process argumentative essays without human interference (Taghipour and Ng, 2016). It takes an essay as"
C18-1314,D14-1162,0,0.0817283,"as a ranking task and utilize a pairwise hinge loss for training. Given a triple (OP, R+ , R− ), where R+ and R− respectively denote the positive and the negative reply for OP . The loss function is defined in Equation 18. L = max(0, 1 − S(OP, R+ ) + S(OP, R− )) (18) where S(OP, R+ ) and S(OP, R− ) are the corresponding persuasiveness scores. The model is trained by stochastic gradient descent on 105 epochs, and evaluated on the development set at every epoch to select the best model. Dropout (Srivastava et al., 2014) has proved to be an effective method and is used in our work. We use Glove (Pennington et al., 2014) word embeddings, which are 50-dimension word vectors trained with a crawled large corpus with 840 billion tokens. Embeddings for words not present are randomly initialized with sampled numbers from a uniform distribution [0.25,0.25]. We set initial learning rate to 0.1, batch size to 20, filter sizes to 5, filter numbers to 100 and the hidden unit of BiGRU to 200. Early stopping was used with a patience of 15 epochs. We implemented our model using TensorFlow. The model converged in 23 hours on an NVIDIA Titan X machine. 3707 Original post Positive reply Negative reply Avew 10 10 10 Training S"
C18-1314,D14-1006,0,0.0506881,"eractions between participants on argument level. Experimental results on a publicly available dataset show that the proposed model significantly outperforms some state-of-the-art methods for persuasiveness evaluation. Further analysis reveals that attention weights computed in our model are able to extract interactive argument pairs from the original post and the reply. 1 Introduction Computational argumentation is a growing field in natural language processing. Existing research covers argument unit detection (Al-Khatib et al., 2016), argument structure prediction (Peldszus and Stede, 2015; Stab and Gurevych, 2014), argumentation scheme classification (Feng et al., 2014), etc. Recently, the automatic assessment of argumentation quality has started gaining attention. It can be analyzed at two levels, namely monological argumentation and dialogical argumentation. Monological argumentation refers to a composition of arguments on a certain issue (Wachsmuth et al., 2017). A typical example of quality evaluation for monological argumentation is automated essay scoring, which aims to process argumentative essays without human interference (Taghipour and Ng, 2016). It takes an essay as the input and outputs a n"
C18-1314,D16-1193,0,0.0383443,"tructure prediction (Peldszus and Stede, 2015; Stab and Gurevych, 2014), argumentation scheme classification (Feng et al., 2014), etc. Recently, the automatic assessment of argumentation quality has started gaining attention. It can be analyzed at two levels, namely monological argumentation and dialogical argumentation. Monological argumentation refers to a composition of arguments on a certain issue (Wachsmuth et al., 2017). A typical example of quality evaluation for monological argumentation is automated essay scoring, which aims to process argumentative essays without human interference (Taghipour and Ng, 2016). It takes an essay as the input and outputs a numeric score, considering features of content, grammar, discourse structure and lexical richness (Burstein et al., 2013). Most of the efforts are made on the exploration for better document representation. Dialogical argumentation refers to a series of interactive arguments related to a given topic, involving argument retraction, view exchange, and so on (Besnard et al., 2014). With the popularity of online debating forums like convinceme 1 , debatepedia 2 and change my view (CMV) 3 , researchers pay increasing attention to evaluating the quality"
C18-1314,E17-1017,0,0.0698165,"and the reply. 1 Introduction Computational argumentation is a growing field in natural language processing. Existing research covers argument unit detection (Al-Khatib et al., 2016), argument structure prediction (Peldszus and Stede, 2015; Stab and Gurevych, 2014), argumentation scheme classification (Feng et al., 2014), etc. Recently, the automatic assessment of argumentation quality has started gaining attention. It can be analyzed at two levels, namely monological argumentation and dialogical argumentation. Monological argumentation refers to a composition of arguments on a certain issue (Wachsmuth et al., 2017). A typical example of quality evaluation for monological argumentation is automated essay scoring, which aims to process argumentative essays without human interference (Taghipour and Ng, 2016). It takes an essay as the input and outputs a numeric score, considering features of content, grammar, discourse structure and lexical richness (Burstein et al., 2013). Most of the efforts are made on the exploration for better document representation. Dialogical argumentation refers to a series of interactive arguments related to a given topic, involving argument retraction, view exchange, and so on ("
C18-1314,Q17-1016,0,0.22711,"all of the attention representations in a linear function to get the integrated information. The detail is illustrated in Equation 12. n m U = f (U 1 , U 2 , U 3 , {riOP }i=1 , {rjR }j=1 ) (12) where f is a simple linear function, U 3 is a matrix that is tiled n times by u3 . 2.3 Aggregation Network After acquiring the local alignment representation by the co-attention network, we employ a filtration gate to hold the interactive information. Then, we fuse the interactive information via a bi-directional GRU and compute the persuasiveness score. Filtration Gate We utilize the filtration gate (Wang et al., 2017b) to hold the information that helps to understand the argument-level interactions between the original post and the reply. The formulas are in Equation 13 and 14. gt = sigmoid(Wg U + b) (13) U ∗ = gt U (14) We fuse the interactive information reserved by the filtration gate via a bi-directional GRU. The calculation is described in Equation 15. Ot = BiGRU (Ot−1 , Ut∗ ) (15) Then, we use an attention pooling operation over the whole hidden states of this BiGRU to summarize the interactive features into a dense vector O∗ . Scoring Tay et al. (2017) prove that adding some manual features such as"
C18-1314,P17-1018,0,0.170286,"all of the attention representations in a linear function to get the integrated information. The detail is illustrated in Equation 12. n m U = f (U 1 , U 2 , U 3 , {riOP }i=1 , {rjR }j=1 ) (12) where f is a simple linear function, U 3 is a matrix that is tiled n times by u3 . 2.3 Aggregation Network After acquiring the local alignment representation by the co-attention network, we employ a filtration gate to hold the interactive information. Then, we fuse the interactive information via a bi-directional GRU and compute the persuasiveness score. Filtration Gate We utilize the filtration gate (Wang et al., 2017b) to hold the information that helps to understand the argument-level interactions between the original post and the reply. The formulas are in Equation 13 and 14. gt = sigmoid(Wg U + b) (13) U ∗ = gt U (14) We fuse the interactive information reserved by the filtration gate via a bi-directional GRU. The calculation is described in Equation 15. Ot = BiGRU (Ot−1 , Ut∗ ) (15) Then, we use an attention pooling operation over the whole hidden states of this BiGRU to summarize the interactive features into a dense vector O∗ . Scoring Tay et al. (2017) prove that adding some manual features such as"
C18-1314,P16-2032,1,0.941255,"score, considering features of content, grammar, discourse structure and lexical richness (Burstein et al., 2013). Most of the efforts are made on the exploration for better document representation. Dialogical argumentation refers to a series of interactive arguments related to a given topic, involving argument retraction, view exchange, and so on (Besnard et al., 2014). With the popularity of online debating forums like convinceme 1 , debatepedia 2 and change my view (CMV) 3 , researchers pay increasing attention to evaluating the quality of debating or persuasive content (Tan et al., 2016; Wei and Liu, 2016; Wei et al., 2016; Habernal and Gurevych, 2016a). Although some similarity features of content between the reply and the original post are used to evaluate the quality of the given reply, they are computed on word level without considering the exchange of opinions on the basis of arguments. An example of dialogical argumentation is shown in Figure 1. There are three posts, one is original and the other two are replies. The repliers post to change the view of the original poster. And the positive reply is deemed to be more persuasive than the negative reply. We have three observations. First,"
C18-1314,W16-2820,1,0.734769,"features of content, grammar, discourse structure and lexical richness (Burstein et al., 2013). Most of the efforts are made on the exploration for better document representation. Dialogical argumentation refers to a series of interactive arguments related to a given topic, involving argument retraction, view exchange, and so on (Besnard et al., 2014). With the popularity of online debating forums like convinceme 1 , debatepedia 2 and change my view (CMV) 3 , researchers pay increasing attention to evaluating the quality of debating or persuasive content (Tan et al., 2016; Wei and Liu, 2016; Wei et al., 2016; Habernal and Gurevych, 2016a). Although some similarity features of content between the reply and the original post are used to evaluate the quality of the given reply, they are computed on word level without considering the exchange of opinions on the basis of arguments. An example of dialogical argumentation is shown in Figure 1. There are three posts, one is original and the other two are replies. The repliers post to change the view of the original poster. And the positive reply is deemed to be more persuasive than the negative reply. We have three observations. First, viewpoints of the"
D09-1159,H05-1045,0,0.0102432,"Missing"
D09-1159,W06-1651,0,0.0530414,"work on extracting opinion units including: opinion holder, subject, aspect and evaluation. Subject and aspect belong to product features, while evaluation is the opinion expression in our work. They converted the task to two kinds of relation extraction tasks and proposed a machine learning-based method which combines contextual clues and statistical clues. Their experimental results showed that the model using contextual clues improved the performance. However since the contextual information in a domain is specific, the model got by their approach can not easily converted to other domains. Choi et al. (2006) used an integer linear programming approach to jointly extract entities and relations in the context of opinion oriented information extraction. They identified expressions of opinions, sources of opinions and the linking relation that exists between them. The sources of opinions denote to the person or entity that holds the opinion. Another area related to our work is opinion expressions identification (Wilson et al., 2005a; Breck et al., 2007). They worked on identifying the words and phrases that express opinions in text. According to Wiebe et al. (2005), there are two types of opinion exp"
D09-1159,P04-1054,0,0.108616,"an that could be represented by a feature extraction-based approach, we define a new tree kernel over phrase dependency trees and incorporate this kernel within an SVM to extract relations between opinion expressions and product features. The potential relation set consists of the all combinations between candidate product features and candidate opinion expressions in a sentence. Given a phrase dependency parsing tree, we choose the subtree rooted at the lowest common parent(LCP) of opinion expression and product feature to represent the relation. Dependency tree kernels has been proposed by (Culotta and Sorensen, 2004). Their kernel is defined on lexical dependency tree by the convolution of similarities between all possible subtrees. However, if the convolution containing too many irrelevant subtrees, over-fitting may occur and decreases the performance of the classifier. In phrase dependency tree, local words in a same phrase are compacted, therefore it provides a way to treat “local dependencies” and “global dependencies” differently (Fig. 3). As a consequence, these two kinds of dependencies will not disturb each other in measuring similarity. Later experiments prove the validity of this statement. Phra"
D09-1159,C04-1200,0,0.59445,"lyzing this content are impossible tasks if they were to be manually done. However, advances in machine learning and natural language processing present us with a unique opportunity to automate the decoding of consumers’ opinions from online reviews. Previous works on mining opinions can be divided into two directions: sentiment classification and sentiment related information extraction. The former is a task of identifying positive and negative sentiments from a text which can be a passage, a sentence, a phrase and even a word (Somasundaran et al., 2008; Pang et al., 2002; Dave et al., 2003; Kim and Hovy, 2004; Takamura et al., 2005). The latter focuses on extracting the elements composing a sentiment text. The elements 1. I highly [recommend](1) the Canon SD500(1) to anybody looking for a compact camera that can take [good](2) pictures(2) . 2. This camera takes [amazing](3) image qualities(3) and its size(4) [cannot be beat](4) . The phrases underlined are the product features, marked with square brackets are opinion expressions. Product features and opinion expressions with identical superscript compose a relation. For the first sentence, an opinion relation exists between “the Canon SD500” and “"
D09-1159,D07-1114,0,0.71835,"product review. In practice, for a certain domain of product reviews, a language model is build on easily acquired unlabeled data. Each candidate NP or VP chunk in the output of shallow parser is scored by the model, and cut off if its score is less than a threshold. Opinion expressions are spans of text that express a comment or attitude of the opinion holder, which are usually evaluative or subjective phrases. We also analyze the labeled corpus for opinion expressions and observe that many opinion expressions are used in multiple domains, which is identical with the conclusion presented by Kobayashi et al. (2007). They collected 5,550 opinion expressions from various sources . The coverage of the dictionary is high in multiple domains. Motivated by those observations, we use a dictionary which contains 8221 opinion expressions to select candidates (Wilson et al., 2005b). An assumption we use to filter candidate opinion expressions is that opinion expressions tend to appear closely with product features, which is also used to extract product features by Hu and Liu (2004). In our experiments, the tree distance between product feature and opinion expression in a relation should be less than 5 in the phra"
D09-1159,W02-1011,0,0.023691,"s. Retrieving this information and analyzing this content are impossible tasks if they were to be manually done. However, advances in machine learning and natural language processing present us with a unique opportunity to automate the decoding of consumers’ opinions from online reviews. Previous works on mining opinions can be divided into two directions: sentiment classification and sentiment related information extraction. The former is a task of identifying positive and negative sentiments from a text which can be a passage, a sentence, a phrase and even a word (Somasundaran et al., 2008; Pang et al., 2002; Dave et al., 2003; Kim and Hovy, 2004; Takamura et al., 2005). The latter focuses on extracting the elements composing a sentiment text. The elements 1. I highly [recommend](1) the Canon SD500(1) to anybody looking for a compact camera that can take [good](2) pictures(2) . 2. This camera takes [amazing](3) image qualities(3) and its size(4) [cannot be beat](4) . The phrases underlined are the product features, marked with square brackets are opinion expressions. Product features and opinion expressions with identical superscript compose a relation. For the first sentence, an opinion relation"
D09-1159,H05-1043,0,0.956926,"Missing"
D09-1159,C08-1101,0,0.0140112,"d tracking customer opinions. Retrieving this information and analyzing this content are impossible tasks if they were to be manually done. However, advances in machine learning and natural language processing present us with a unique opportunity to automate the decoding of consumers’ opinions from online reviews. Previous works on mining opinions can be divided into two directions: sentiment classification and sentiment related information extraction. The former is a task of identifying positive and negative sentiments from a text which can be a passage, a sentence, a phrase and even a word (Somasundaran et al., 2008; Pang et al., 2002; Dave et al., 2003; Kim and Hovy, 2004; Takamura et al., 2005). The latter focuses on extracting the elements composing a sentiment text. The elements 1. I highly [recommend](1) the Canon SD500(1) to anybody looking for a compact camera that can take [good](2) pictures(2) . 2. This camera takes [amazing](3) image qualities(3) and its size(4) [cannot be beat](4) . The phrases underlined are the product features, marked with square brackets are opinion expressions. Product features and opinion expressions with identical superscript compose a relation. For the first sentence,"
D09-1159,P05-1017,0,0.272784,"are impossible tasks if they were to be manually done. However, advances in machine learning and natural language processing present us with a unique opportunity to automate the decoding of consumers’ opinions from online reviews. Previous works on mining opinions can be divided into two directions: sentiment classification and sentiment related information extraction. The former is a task of identifying positive and negative sentiments from a text which can be a passage, a sentence, a phrase and even a word (Somasundaran et al., 2008; Pang et al., 2002; Dave et al., 2003; Kim and Hovy, 2004; Takamura et al., 2005). The latter focuses on extracting the elements composing a sentiment text. The elements 1. I highly [recommend](1) the Canon SD500(1) to anybody looking for a compact camera that can take [good](2) pictures(2) . 2. This camera takes [amazing](3) image qualities(3) and its size(4) [cannot be beat](4) . The phrases underlined are the product features, marked with square brackets are opinion expressions. Product features and opinion expressions with identical superscript compose a relation. For the first sentence, an opinion relation exists between “the Canon SD500” and “recommend”, but not betw"
D09-1159,H05-2018,0,0.103904,"old. Opinion expressions are spans of text that express a comment or attitude of the opinion holder, which are usually evaluative or subjective phrases. We also analyze the labeled corpus for opinion expressions and observe that many opinion expressions are used in multiple domains, which is identical with the conclusion presented by Kobayashi et al. (2007). They collected 5,550 opinion expressions from various sources . The coverage of the dictionary is high in multiple domains. Motivated by those observations, we use a dictionary which contains 8221 opinion expressions to select candidates (Wilson et al., 2005b). An assumption we use to filter candidate opinion expressions is that opinion expressions tend to appear closely with product features, which is also used to extract product features by Hu and Liu (2004). In our experiments, the tree distance between product feature and opinion expression in a relation should be less than 5 in the phrase dependency parsing tree. 2.3 Relation Extraction This section describes our method on extracting relations between opinion expressions and product features using phrase dependency tree. Manually built patterns were used in previous works which have an obvio"
D09-1159,H05-1044,0,0.194139,"old. Opinion expressions are spans of text that express a comment or attitude of the opinion holder, which are usually evaluative or subjective phrases. We also analyze the labeled corpus for opinion expressions and observe that many opinion expressions are used in multiple domains, which is identical with the conclusion presented by Kobayashi et al. (2007). They collected 5,550 opinion expressions from various sources . The coverage of the dictionary is high in multiple domains. Motivated by those observations, we use a dictionary which contains 8221 opinion expressions to select candidates (Wilson et al., 2005b). An assumption we use to filter candidate opinion expressions is that opinion expressions tend to appear closely with product features, which is also used to extract product features by Hu and Liu (2004). In our experiments, the tree distance between product feature and opinion expression in a relation should be less than 5 in the phrase dependency parsing tree. 2.3 Relation Extraction This section describes our method on extracting relations between opinion expressions and product features using phrase dependency tree. Manually built patterns were used in previous works which have an obvio"
D09-1159,H01-1014,0,0.00472748,"roblem for dependency grammar. Fig. 2(a) shows the dependency representation of an example sentence. The root of the sentence is “enjoyed”. There are seven pairs of dependency relationships, depicted by seven arcs from heads to dependents. 2.1.2 Phrase Dependency Parsing Currently, the mainstream of dependency parsing is conducted on lexical elements: relations are built between single words. A major information loss of this word level dependency tree compared with constituent tree is that it doesn’t explicitly provide local structures and syntactic categories (i.e. NP, VP labels) of phrases (Xia and Palmer, 2001). On the other hand, dependency tree provides connections between distant words, which are useful in extracting long distance relations. Therefore, compromising between the two, we extend the dependency tree node with phrases. That implies a noun phrase “Cannon SD500 PowerShot” can be a dependent that modifies a verb phrase head “really enjoy using” with relation type “dobj”. The feasibility behind is that a phrase is a syntactic unit regardless of the length or syntactic category (Santorini and Kroch, 2007), and it is acceptable to substitute a single word by a phrase with same syntactic cate"
D09-1159,H05-2017,0,\N,Missing
D10-1019,D08-1070,0,0.0257223,"modular approach. While, the key disadvantage of such method is that errors propagate between stages, significantly affecting the quality of the final results. To cope with this problem, Shi and Wang (2007) proposed a reranking framework in which N-best segment candidates generated in the first stage are passed to the tagging model, and the final output is the one with the highest overall segmentation and tagging probability score. The main drawback of this method is that the interaction between tagging and segmentation is restricted by the number of candidate segmentation outputs. Razvan C. Bunescu (2008) presented an improved pipeline model in which upstream subtask outputs are regarded as hidden variables, together with their probabilities are used as probabilistic features in the downstream subtasks. One shortcoming of this method is that calculation of marginal probabilities of features may be inefficient and some approximations are required for fast computation. Another disadvantage of these two methods is that they employ separate training and the segmentation model could not take advantages of tagging information in the training procedure. On the other hand, joint learning and decoding"
D10-1019,P08-1102,0,0.157897,"Missing"
D10-1019,I08-4010,0,0.125398,"4, supervised learning is fundamental. We believe that combination of our method and semi-supervised learning will achieve further improvement. 4.2 Chinese word segmentation and POS tagging Our second experiment is the Chinese word segmentation and POS tagging task. To facilitate comparison, we focus only on the closed test, which means that the system is trained only with a designated training corpus, any extra knowledge is not allowed, including Chinese and Arabic numbers, letters and so on. We use the Chinese Treebank (CTB) POS corpus from the Fourth International SIGHAN Bakeoff data sets (Jin and Chen, 2008). The training data consist of 23444 sentences, 642246 Chinese words, 1.05M Chinese characters and testing data consist of 2079 sentences, 59955 Chinese words, 0.1M Chinese characters. We compare our hybrid CRFs with pipeline and candidate reranking methods (Shi and Wang, 2007) 192 Table 4: Comparison with other systems on shallow parsing task Method F1 Cross-Product CRFs Hybrid CRFs SVM combination (Kudo and Matsumoto, 2001) Voted Perceptrons (Carreras and Marquez, 2003) ETL (Milidiu et al., 2008) (Wu et al., 2006) 93.88 94.31 93.91 HySOL (Suzuki et al., 2007) ASO-semi (Ando and Zhang, 2005)"
D10-1019,D07-1033,0,0.0239896,"Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 187–195, c MIT, Massachusetts, USA, 9-11 October 2010. 2010 Association for Computational Linguistics ever, DCRFs do not guarantee non-violation of hardconstraints that nodes within the same segment get a single consistent tagging label. Another drawback of DCRFs is that exact inference is generally time consuming, some approximations are required to make it tractable. Recently, perceptron based learning framework has been well studied for incorporating node level and segment level features together (Kazama and Torisawa, 2007; Zhang and Clark, 2008). The main shortcoming is that exact inference is intractable for those dynamically generated segment level features, so candidate based searching algorithm is used for approximation. On the other hand, Jiang (2008) proposed a cascaded linear model which has a two layer structure, the inside-layer model uses node level features to generate candidates with their weights as inputs of the outside layer model which captures non-local features. As pipeline models, error propagation problem exists for such method. In this paper, we present a novel graph structure that exploit"
D10-1019,N01-1025,0,0.163696,"Missing"
D10-1019,P08-1074,0,0.026393,"Missing"
D10-1019,W04-3236,0,0.0233341,"abilistic features in the downstream subtasks. One shortcoming of this method is that calculation of marginal probabilities of features may be inefficient and some approximations are required for fast computation. Another disadvantage of these two methods is that they employ separate training and the segmentation model could not take advantages of tagging information in the training procedure. On the other hand, joint learning and decoding using cross-product of segmentation states and tagging states does not suffer from error propagation problem and achieves higher accuracy on both subtasks (Ng and Low, 2004). However, two problems arises due to the large state space, one is that the amount of parameters increases rapidly, which is apt to overfit on the training corpus, the other is that the inference by dynamic programming could be inefficient. Sutton (2004) proposed Dynamic Conditional Random Fields (DCRFs) to perform joint training/decoding of subtasks using much fewer parameters than the cross-product approach. How187 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 187–195, c MIT, Massachusetts, USA, 9-11 October 2010. 2010 Association for Computat"
D10-1019,P08-1076,0,0.0303963,"Missing"
D10-1019,D07-1083,0,0.0280086,"Missing"
D10-1019,P08-1101,0,0.0604991,"ference on Empirical Methods in Natural Language Processing, pages 187–195, c MIT, Massachusetts, USA, 9-11 October 2010. 2010 Association for Computational Linguistics ever, DCRFs do not guarantee non-violation of hardconstraints that nodes within the same segment get a single consistent tagging label. Another drawback of DCRFs is that exact inference is generally time consuming, some approximations are required to make it tractable. Recently, perceptron based learning framework has been well studied for incorporating node level and segment level features together (Kazama and Torisawa, 2007; Zhang and Clark, 2008). The main shortcoming is that exact inference is intractable for those dynamically generated segment level features, so candidate based searching algorithm is used for approximation. On the other hand, Jiang (2008) proposed a cascaded linear model which has a two layer structure, the inside-layer model uses node level features to generate candidates with their weights as inputs of the outside layer model which captures non-local features. As pipeline models, error propagation problem exists for such method. In this paper, we present a novel graph structure that exploits joint training and dec"
D10-1019,I08-4017,0,0.189923,"Missing"
D10-1019,P05-1001,0,\N,Missing
D11-1123,P09-1079,0,0.0857641,"Missing"
D11-1123,N07-1030,0,0.0131716,"rative sentences and proposed learning approaches to identify them. Sentiment analysis of conditional sentences were studied by Narayanan et al. (2009). They aimed 1340 to determine whether opinions expressed on different topics in a conditional sentence are positive, negative or neutral. They analyzed the conditional sentences in both linguistic and computitional perspectives and used learning method to do it. They followed the feature-based sentiment analysis model (Hu and Liu, 2004), which also use flat frames to represent evaluations. Integer linear programming was used in many NLP tasks (Denis and Baldridge, 2007), for its power in both expressing and approximating various inference problems, especially in parsing (Riedel and Clarke, 2006; Martins et al., 2009). Martins etc. (2009) also applied ILP with flow formulation for maximum spanning tree, besides, they also handled dependency parse trees involving high order features(sibling, grandparent), and with projective constraint. 6 Conclusions This paper introduces a representation method for opinions in online reviews. Inspections on corpus show that the information ignored in previous sentiment representation can cause incorrect or incomplete mining r"
D11-1123,P10-1041,0,0.0347343,"Missing"
D11-1123,P06-2063,0,0.0289828,"Missing"
D11-1123,D07-1114,0,0.155196,"Results on vertices extraction with 10 folder cross validation. We use two criterion: 1) the vertex is correct if it is exactly same as ground truth(“E”), 2) the vertex is correct if it overlaps with ground truth(“O”). 5 Related Work Opinion mining has recently received considerable attentions. Large amount of work has been done on sentimental classification in different levels and sentiment related information extraction. Researches on different types of sentences such as comparative sentences (Jindal and Liu, 2006) and conditional sentences (Narayanan et al., 2009) have also been proposed. Kobayashi et al. (2007) presented their work on extracting opinion units including: opinion holder, subject, aspect and evaluation. They used slots to represent evaluations, converted the task to two kinds of relation extraction tasks and proposed a machine learning-based method which used both contextual and statistical clues. Jindal and Liu (2006) studied the problem of identifying comparative sentences. They analyzed different types of comparative sentences and proposed learning approaches to identify them. Sentiment analysis of conditional sentences were studied by Narayanan et al. (2009). They aimed 1340 to det"
D11-1123,P09-1039,0,0.115006,"connects to more than one opinion expression rarely occur comparing with those vertices which have a single parent. An explaination for this sparseness is that opinions in online reviews always concentrate in local context and have local semantic connections. 3.2.2 ILP Formulation Based on the property 3, we divide the inference algorithm into two steps: i) constructing G’s spanning tree (arborescence) with property 1 and 2; ii) finding additional non-tree edges as a post processing task. The first step is close to the works on ILP formulations of dependency parsing (Riedel and Clarke, 2006; Martins et al., 2009). In the second step, we use a heuristic method which greedily adds non-tree edges. A similar approximation method is also used in (Mcdonald and Pereira, 2006) for acyclic dependency graphs. Step 1. Find MST. Following the multicommodity flow formulation of maximum spanning tree (MST) problem in (Magnanti and Wolsey, 1994), the ILP for MST is: ∑ max. yij · score(xi , xj ) (3) i,j ∑ s.t. ∑ i i,j (4) yij = |V |− 1 fiju − ∑ ∑ k u f0k u fjk = δju ,1 ≤ u, j ≤ |V |(5) = 1, k fiju fiju (6) 1 ≤ u ≤ |V | ≤ yij , 1 ≤ u, j ≤ |V |, ≥ 0, 1 ≤ u, j ≤ |V |, yij ∈ { 0, 1}, 0 ≤ i ≤ |V | (7) 0 ≤ i ≤ |V | (8) 0 ≤"
D11-1123,E06-1011,0,0.0461743,"is that opinions in online reviews always concentrate in local context and have local semantic connections. 3.2.2 ILP Formulation Based on the property 3, we divide the inference algorithm into two steps: i) constructing G’s spanning tree (arborescence) with property 1 and 2; ii) finding additional non-tree edges as a post processing task. The first step is close to the works on ILP formulations of dependency parsing (Riedel and Clarke, 2006; Martins et al., 2009). In the second step, we use a heuristic method which greedily adds non-tree edges. A similar approximation method is also used in (Mcdonald and Pereira, 2006) for acyclic dependency graphs. Step 1. Find MST. Following the multicommodity flow formulation of maximum spanning tree (MST) problem in (Magnanti and Wolsey, 1994), the ILP for MST is: ∑ max. yij · score(xi , xj ) (3) i,j ∑ s.t. ∑ i i,j (4) yij = |V |− 1 fiju − ∑ ∑ k u f0k u fjk = δju ,1 ≤ u, j ≤ |V |(5) = 1, k fiju fiju (6) 1 ≤ u ≤ |V | ≤ yij , 1 ≤ u, j ≤ |V |, ≥ 0, 1 ≤ u, j ≤ |V |, yij ∈ { 0, 1}, 0 ≤ i ≤ |V | (7) 0 ≤ i ≤ |V | (8) 0 ≤ i, j ≤ |V |. (9) In this formulation, yij is an edge indicator variable that (xi , xj ) is a spanning tree edge when yij = 1, (xi , xj ) is a non-tree edge wh"
D11-1123,D09-1019,0,0.0509771,"5.1 47.9 57.2 60.2 F 50.3 52.1 63.7 65.6 Table 7: Results on vertices extraction with 10 folder cross validation. We use two criterion: 1) the vertex is correct if it is exactly same as ground truth(“E”), 2) the vertex is correct if it overlaps with ground truth(“O”). 5 Related Work Opinion mining has recently received considerable attentions. Large amount of work has been done on sentimental classification in different levels and sentiment related information extraction. Researches on different types of sentences such as comparative sentences (Jindal and Liu, 2006) and conditional sentences (Narayanan et al., 2009) have also been proposed. Kobayashi et al. (2007) presented their work on extracting opinion units including: opinion holder, subject, aspect and evaluation. They used slots to represent evaluations, converted the task to two kinds of relation extraction tasks and proposed a machine learning-based method which used both contextual and statistical clues. Jindal and Liu (2006) studied the problem of identifying comparative sentences. They analyzed different types of comparative sentences and proposed learning approaches to identify them. Sentiment analysis of conditional sentences were studied b"
D11-1123,W02-1011,0,0.0140594,"Missing"
D11-1123,W06-1616,0,0.0622205,"the cases that a modifier connects to more than one opinion expression rarely occur comparing with those vertices which have a single parent. An explaination for this sparseness is that opinions in online reviews always concentrate in local context and have local semantic connections. 3.2.2 ILP Formulation Based on the property 3, we divide the inference algorithm into two steps: i) constructing G’s spanning tree (arborescence) with property 1 and 2; ii) finding additional non-tree edges as a post processing task. The first step is close to the works on ILP formulations of dependency parsing (Riedel and Clarke, 2006; Martins et al., 2009). In the second step, we use a heuristic method which greedily adds non-tree edges. A similar approximation method is also used in (Mcdonald and Pereira, 2006) for acyclic dependency graphs. Step 1. Find MST. Following the multicommodity flow formulation of maximum spanning tree (MST) problem in (Magnanti and Wolsey, 1994), the ILP for MST is: ∑ max. yij · score(xi , xj ) (3) i,j ∑ s.t. ∑ i i,j (4) yij = |V |− 1 fiju − ∑ ∑ k u f0k u fjk = δju ,1 ≤ u, j ≤ |V |(5) = 1, k fiju fiju (6) 1 ≤ u ≤ |V | ≤ yij , 1 ≤ u, j ≤ |V |, ≥ 0, 1 ≤ u, j ≤ |V |, yij ∈ { 0, 1}, 0 ≤ i ≤ |V | ("
D11-1123,W03-0404,0,0.0879412,"Missing"
D11-1123,C08-1101,0,0.0508096,"Missing"
D11-1123,P05-1017,0,0.0851631,"Missing"
D11-1123,D09-1159,1,0.419347,"Missing"
D12-1126,D08-1070,0,0.0276057,"tagging, we also review some works on word-level features. 1386 Semi-Markov Conditional Random Fields (semiCRF) (Sarawagi and Cohen, 2004) is a model in which segmentation task is implicitly included into the decoding algorithm. In this model, feature representation would be more flexible than traditional CRFs, since features can be extracted from the previous/the next segmentation within a window of variable size. The problem of this approach lies in that the decoding algorithm depends on the predefined window size to exploit the boundaries of segmentations but not the real length of words. Bunescu (2008) presents an improved pipeline model in which the output of the previous subtasks are considered as hidden variables, and the hidden variables together with their probabilities denoting the confidence are used as probabilistic features in the next subtasks. One shortcoming of this method is inefficiency caused by the calculation of marginal probabilities of features. The other disadvantages of the pipeline method are error propagation and the need of separate training of different subtasks in the pipeline. Another disadvantage of pipeline method is error propagation. Jiang et al. (2008) propos"
D12-1126,W02-1001,0,0.0281002,"3 and analyze its complexity in section 4. Section 5 describes the training method. The experimental results are manifested in section 6. Finally, We review the relevant research works in section 7 and conclude our work in section 8. 1380 (1) where w is the parameter of function F (·). For sequence labeling, the feature can be denoted as ϕk (yi , yi−1 , x, i), where i stands for the position in the sequence and k stands for the number of feature templates. we use online Passive-Aggressive (PA) algorithm (Crammer and Singer, 2003; Crammer et al., 2006) to train the model parameters. Following (Collins, 2002), the average strategy is used to avoid the overfitting problem. 3 Dynamic Features The form of traditional features is shown in Table 2, where C represents a Chinese character, and T represents the character-based tag. The subscript i indicates its position related to the current character. Table 2: Traditional Feature Templates Ci , T0 (i = −2, −1, 0, 1, 2) Ci , Cj , T0 (i, j = −2, −1, 0, 1, 2 and i = j) T−1 , T0 Traditional features are generated by positionfixed templates. Since the length of Chinese word is unfixed, their meanings are incomplete. We categorize them as “static” features s"
D12-1126,W02-2006,0,0.0107998,"81.82 77.59 84.48 39.29 53.57 37.14 45.71 F1 81.03 83.17 64.29 71.01 53.66 66.67 44.83 50.00 Related Works In recent years, POS tagging has undergone great development. The mainstream method is to regard POS tagging as sequence labeling problems (Rabiner, 1990; Xue, 2003; Peng et al., 2004; Ng and Low, 2004). However, the analysis of Chinese-English mixed texts is rarely involved in previous literature. In the aspect of the general multilingual POS tagging, most works focus on modeling cross-lingual correlations and tagging multilingual POS on respective monolingual texts, not on mixed texts (Cucerzan and Yarowsky, 2002; Yarowsky et al., 2001; Naseem et al., 2009). Since we choose to use dynamic word-level features to improve the performance of POS tagging, we also review some works on word-level features. 1386 Semi-Markov Conditional Random Fields (semiCRF) (Sarawagi and Cohen, 2004) is a model in which segmentation task is implicitly included into the decoding algorithm. In this model, feature representation would be more flexible than traditional CRFs, since features can be extracted from the previous/the next segmentation within a window of variable size. The problem of this approach lies in that the dec"
D12-1126,P08-1102,0,0.0955264,"OS tagging is that the two languages have character-based features and word-based features respectively. To ensure the consistency of tagging models, we prefer to use word-level information in Chinese, which is both useful for ChineseEnglish mixed texts and Chinese-only texts. For instance, in a sentence “X 或者 Y ... (X or Y ...)”, the word Y ought to have the same POS tag as the word X. Another example is that the word following a pronoun is usually a verb, and adjectives often describe nouns. Some related works show that word-level features can improve the performance of Chinese POS tagging (Jiang et al., 2008; Sun, 2011). In this paper, we propose a method to tag mixed texts with dynamic features. Our method combines these dynamic features, which are dynamically generated at the decoding stage, with traditional static features. For Chinese-English mixed texts, the traditional features cannot yield a satisfied result due to lack of training data. The proposed dynamic features can improve the performance by using the information of a word, such as POS tag or length of the whole word, which is proven effective by experiments. The rest of the paper is organized as follows: In section 2, we introduce t"
D12-1126,I08-4010,0,0.0732247,"Missing"
D12-1126,W04-3236,0,0.226022,"lish mixed texts in daily conversation, especially in communication among employers in large international corporations. There are some challenges for analyzing ChineseEnglish mixed texts: 1. How to define the POS tags for English words in these mixed texts. Since the standard of POS tags for English and Chinese are different, we cannot use English POS to tag the English words in mixed texts. Currently, the mainstream method of Chinese POS tagging is joint segmentation & tagging with cross-labels, which can avoid the problem of error propagation and achieve higher performance on both subtasks(Ng and Low, 2004). Each label is the crossproduct of a segmentation label and a tagging label, e.g. {B-NN, I-NN, E-NN, S-NN, ...}. The features are generated by position-based templates on character-level. Since the main part of mixed texts is in Chinese and the role of English word is more like Chinese, we use Chinese POS tags (Xia, 2000) to tag English words. Since the categories of the most commonly used English words are nouns, verbs and adjectives, we can use “NN”, “NR”, “VV”, “VA”, “JJ” to label their POS tags. For the English proper nouns and verbs, there are no significant differences in Chinese and En"
D12-1126,C04-1081,0,0.0424897,"Baseline Our Baseline Our P 88.62 91.67 48.31 60.53 78.95 84.21 R 78.31 87.30 74.14 79.31 53.57 57.14 F1 83.15 89.43 58.50 68.66 63.83 68.09 Table 22: Performances of Model C on Dataset R POS tag NN VV VA NR 7 Method Baseline Our Baseline Our Baseline Our Baseline Our P 80.25 84.56 54.88 61.25 84.62 88.24 56.52 55.17 R 81.82 81.82 77.59 84.48 39.29 53.57 37.14 45.71 F1 81.03 83.17 64.29 71.01 53.66 66.67 44.83 50.00 Related Works In recent years, POS tagging has undergone great development. The mainstream method is to regard POS tagging as sequence labeling problems (Rabiner, 1990; Xue, 2003; Peng et al., 2004; Ng and Low, 2004). However, the analysis of Chinese-English mixed texts is rarely involved in previous literature. In the aspect of the general multilingual POS tagging, most works focus on modeling cross-lingual correlations and tagging multilingual POS on respective monolingual texts, not on mixed texts (Cucerzan and Yarowsky, 2002; Yarowsky et al., 2001; Naseem et al., 2009). Since we choose to use dynamic word-level features to improve the performance of POS tagging, we also review some works on word-level features. 1386 Semi-Markov Conditional Random Fields (semiCRF) (Sarawagi and Cohen"
D12-1126,P11-1139,0,0.062839,"he two languages have character-based features and word-based features respectively. To ensure the consistency of tagging models, we prefer to use word-level information in Chinese, which is both useful for ChineseEnglish mixed texts and Chinese-only texts. For instance, in a sentence “X 或者 Y ... (X or Y ...)”, the word Y ought to have the same POS tag as the word X. Another example is that the word following a pronoun is usually a verb, and adjectives often describe nouns. Some related works show that word-level features can improve the performance of Chinese POS tagging (Jiang et al., 2008; Sun, 2011). In this paper, we propose a method to tag mixed texts with dynamic features. Our method combines these dynamic features, which are dynamically generated at the decoding stage, with traditional static features. For Chinese-English mixed texts, the traditional features cannot yield a satisfied result due to lack of training data. The proposed dynamic features can improve the performance by using the information of a word, such as POS tag or length of the whole word, which is proven effective by experiments. The rest of the paper is organized as follows: In section 2, we introduce the sequence"
D12-1126,O03-4002,0,0.0482067,"seline Our Baseline Our Baseline Our P 88.62 91.67 48.31 60.53 78.95 84.21 R 78.31 87.30 74.14 79.31 53.57 57.14 F1 83.15 89.43 58.50 68.66 63.83 68.09 Table 22: Performances of Model C on Dataset R POS tag NN VV VA NR 7 Method Baseline Our Baseline Our Baseline Our Baseline Our P 80.25 84.56 54.88 61.25 84.62 88.24 56.52 55.17 R 81.82 81.82 77.59 84.48 39.29 53.57 37.14 45.71 F1 81.03 83.17 64.29 71.01 53.66 66.67 44.83 50.00 Related Works In recent years, POS tagging has undergone great development. The mainstream method is to regard POS tagging as sequence labeling problems (Rabiner, 1990; Xue, 2003; Peng et al., 2004; Ng and Low, 2004). However, the analysis of Chinese-English mixed texts is rarely involved in previous literature. In the aspect of the general multilingual POS tagging, most works focus on modeling cross-lingual correlations and tagging multilingual POS on respective monolingual texts, not on mixed texts (Cucerzan and Yarowsky, 2002; Yarowsky et al., 2001; Naseem et al., 2009). Since we choose to use dynamic word-level features to improve the performance of POS tagging, we also review some works on word-level features. 1386 Semi-Markov Conditional Random Fields (semiCRF)"
D12-1126,H01-1035,0,0.00973311,"37.14 45.71 F1 81.03 83.17 64.29 71.01 53.66 66.67 44.83 50.00 Related Works In recent years, POS tagging has undergone great development. The mainstream method is to regard POS tagging as sequence labeling problems (Rabiner, 1990; Xue, 2003; Peng et al., 2004; Ng and Low, 2004). However, the analysis of Chinese-English mixed texts is rarely involved in previous literature. In the aspect of the general multilingual POS tagging, most works focus on modeling cross-lingual correlations and tagging multilingual POS on respective monolingual texts, not on mixed texts (Cucerzan and Yarowsky, 2002; Yarowsky et al., 2001; Naseem et al., 2009). Since we choose to use dynamic word-level features to improve the performance of POS tagging, we also review some works on word-level features. 1386 Semi-Markov Conditional Random Fields (semiCRF) (Sarawagi and Cohen, 2004) is a model in which segmentation task is implicitly included into the decoding algorithm. In this model, feature representation would be more flexible than traditional CRFs, since features can be extracted from the previous/the next segmentation within a window of variable size. The problem of this approach lies in that the decoding algorithm depends"
D13-1062,W02-1001,0,0.0107096,"s; L is the length of x. (4) + ⟨v1 , ⟨u, ∑ f(x, y)⟩+ y∈φ(zi ) ⟨s, h(x, zi )⟩ ∑ g1 (x, yi−1:i )⟩ yi−1 ∈φ(zi−1 ) yi ∈φ(zi ) ) + ⟨v2 , g2 (x, zi−1:i )⟩ , (7) where f and h represent the state feature vectors on two label sets Y and Z respectively. In Eq.(6) and (7), the score of the label of every character is decided by the weights of the corresponding mapping labels and itself. 6 Training Input sequence: x TaggerPPD Shared Information Output: PPD-style Tags We use online Passive-Aggressive (PA) algorithm (Crammer and Singer, 2003; Crammer et al., 2006) to train the model parameters. Following (Collins, 2002), the average strategy is used to avoid the overﬁtting problem. For the sake of simplicity, we merge the Eq.(6) and (7) into a uniﬁed formula. Given a sequence x and the expect type of tags T , the merged model is ∑ ˆ = arg max⟨w, y Φ(x, z)⟩, (8) TaggerCTB Output: CTB-style Tags y t(y)=T Figure 3: Our model for Heterogeneous POS Tagging The main challenge of our model is the eﬃciency of decoding algorithm, which is similar to structured learning with latent variables(Liang et al., 2006) (Yu and Joachims, 2009). Most methods for structured learning with latent variables have not expand all poss"
D13-1062,P04-1059,0,0.110752,"tagsets. The correct mapping relations can be automatically built in training phase. The rest of the paper is organized as follows: We ﬁrst introduce the related works in section 2 and describe the background of character-based method for joint Chinese S&T in section 3. Section 4 presents an automatic method to build the loose mapping function. Then we propose our method on heterogeneous corpora in 5 and 6. The experimental results are given in section 7. Finally, we conclude our work in section 8. 2 Related Works There are some works to exploit heterogeneous annotation data for Chinese S&T. (Gao et al., 2004) described a transformationbased converter to transfer a certain annotationstyle word segmentation result to another style. However, this converter need human designed transformation templates, and is hard to be generalized to POS tagging. (Jiang et al., 2009) proposed an automatic adaptation method of heterogeneous annotation standards, which depicts a general pipeline to integrate the knowledge of corpora with diﬀerent al., 2013), which is a simpliﬁed case of the work in this paper and has a relative low complexity. Diﬀerent with the multiple task learning, whose tasks are actually diﬀerent"
D13-1062,P08-1102,0,0.0203957,"ks. Multiple task learning has been proven quite successful in practice and has been also applied to NLP (Ando and Zhang, 2005). We also preliminarily veriﬁed that multiple task learning can improve the performance on this problem in our previous work (Zhao et 660 3 Joint Chinese Word Segmentation and POS Tagging Currently, the mainstream method of Chinese POS tagging is joint segmentation & tagging with character-based sequence labeling models(Laﬀerty et al., 2001), which can avoid the problem of segmentation error propagation and achieve higher performance on both subtasks(Ng and Low, 2004; Jiang et al., 2008; Sun, 2011; Qiu et al., 2012). The label of each character is the crossproduct of a segmentation label and a tagging label. If we employ the commonly used label set {B, I, E, S} for the segmentation part of crosslabels ({B, I, E} represent Begin, Inside, End of a multi-node segmentation respectively, and S represents a Single node segmentation), the label of character can be in the form of {B-T}(T represents POS tag). For example, B-NN indicates that the character is the begin of a noun. 4 Automatically Establishing the Loose Mapping Function for the Labels of Characters To combine two human-"
D13-1062,P09-1059,0,0.787797,"CTB PDD Liu Xiang 刘翔/NR 刘/nrf 翔/nrg reachs 进入/VV 进入/v China 中国区/NN 中国/ns 区/n ﬁnal 总决赛/NN 总/b 决赛/vn Table 1: Incompatible word segmentation and POS tagging standards between CTB and PDD CTB, into two words. The POS tagsets are also signiﬁcantly diﬀerent. For example, PDD gives diverse tags “n” and “vn” for the noun, while CTB just gives “NN”. For proper names, they may be tagged as “nr”, “ns”, etc in PDD, while they are just tagged as “NR” in CTB. Recently, it has attracted more and more research interests to exploit heterogeneous annotation data for Chinese word segmentation and POS tagging. (Jiang et al., 2009) presented a preliminary study for the annotation adaptation topic. (Sun and Wan, 2012) proposed a structure-based stacking model to fully utilize heterogeneous word structures. They also reported that there is no one-to-one mapping between the heterogeneous word classiﬁcation and the mapping between heterogeneous tags is very uncertain. These methods usually have a two-step process. The ﬁrst step is to train the preliminary taggers on heterogeneous annotations. The second step is to train the ﬁnal taggers by using the outputs of the preliminary taggers as features. We call these methods as “p"
D13-1062,D12-1038,0,0.0110443,"multiple task learning, whose tasks are actually diﬀerent labels in the same classiﬁcation task, our model utilizes the shared information between the real diﬀerent tasks and can produce the corresponding diﬀerent styles of outputs. Input: x TaggerPPD z=f(x) Output: f(x) TaggerCTB y=h(x,f(x)) Output: CTB-style Tags Figure 1: Traditional Pipeline-based Strategy for Heterogeneous POS Tagging underling annotation guidelines. They further proposed two optimization strategies, iterative training and predict-self re-estimation, to further improve the accuracy of annotation guideline transformation (Jiang et al., 2012). (Sun and Wan, 2012) proposed a structurebased stacking model to fully utilize heterogeneous word structures. These methods regard one annotation as the main target and another annotation as the complementary/auxiliary purposes. For example, in their solution, an auxiliary tagger TaggerPPD is trained on a complementary corpus PPD, to assist the target CTB-style TaggerCTB . To reﬁne the character-based tagger, PPD-style character labels are directly incorporated as new features. The brief sketch of these methods is shown in Figure 1. The related work in machine learning literature is multiple"
D13-1062,I08-4010,0,0.087153,"et al., 2013). 7 Experiments 7.1 Datasets We use the two representative corpora mentioned above, Penn Chinese Treebank (CTB) and PKU’s People’s Daily (PPD) in our experiments. Dataset Partition Training CTB-5 CTB-S PPD Develop Test Training Test Training Test Sections 1−270 400−931 1001−1151 301−325 271−300 - Words 0.47M 6.66K 7.82K 0.64M 59.96K 1.11M 0.16M Table 3: Data partitioning for CTB and PD 7.1.1 CTB Dataset To better comparison with the previous works, we use two commonly used criterions to partition CTB dataset into the train and test sets. • One is the partition criterion used in (Jin and Chen, 2008; Jiang et al., 2009; Sun and Wan, 2012) for CTB 5.0. • Another is the CTB dataset from the POS tagging task of the Fourth International Chinese Language Processing Bakeoﬀ (SIGHAN Bakeoﬀ 2008)(Jin and Chen, 2008). 7.1.2 PPD Dataset For the PPD dataset, we use the PKU dataset from SIGHAN Bakeoﬀ 2008. The details of all datasets are shown in Table 3. Our experiment on these datasets may lead to a fair comparison of our system and the related works. 7.2 Setting We conduct two experiments on CTB-5 + PPD and CTB-S + PPD respectively. The form of feature templates we used is shown in Table 7.2, wher"
D13-1062,P06-1096,0,0.0211414,"ressive (PA) algorithm (Crammer and Singer, 2003; Crammer et al., 2006) to train the model parameters. Following (Collins, 2002), the average strategy is used to avoid the overﬁtting problem. For the sake of simplicity, we merge the Eq.(6) and (7) into a uniﬁed formula. Given a sequence x and the expect type of tags T , the merged model is ∑ ˆ = arg max⟨w, y Φ(x, z)⟩, (8) TaggerCTB Output: CTB-style Tags y t(y)=T Figure 3: Our model for Heterogeneous POS Tagging The main challenge of our model is the eﬃciency of decoding algorithm, which is similar to structured learning with latent variables(Liang et al., 2006) (Yu and Joachims, 2009). Most methods for structured learning with latent variables have not expand all possible mappings. In this paper, we also only expand the mapping that with highest according to the current model. Our model is shown in Figure 3 and the ﬂowchart is shown in Algorithm 1. If given the output type of label T , we only consider the labels in T to initialize the Viterbi matrix, and the score of each node is determined by all the involved heterogeneous labels according to the loose mapping function. z∈ψ(y) where t(y) is a function to judge the type of output tags; ψ(y) represe"
D13-1062,W04-3236,0,0.0426626,"lity among the tasks. Multiple task learning has been proven quite successful in practice and has been also applied to NLP (Ando and Zhang, 2005). We also preliminarily veriﬁed that multiple task learning can improve the performance on this problem in our previous work (Zhao et 660 3 Joint Chinese Word Segmentation and POS Tagging Currently, the mainstream method of Chinese POS tagging is joint segmentation & tagging with character-based sequence labeling models(Laﬀerty et al., 2001), which can avoid the problem of segmentation error propagation and achieve higher performance on both subtasks(Ng and Low, 2004; Jiang et al., 2008; Sun, 2011; Qiu et al., 2012). The label of each character is the crossproduct of a segmentation label and a tagging label. If we employ the commonly used label set {B, I, E, S} for the segmentation part of crosslabels ({B, I, E} represent Begin, Inside, End of a multi-node segmentation respectively, and S represents a Single node segmentation), the label of character can be in the form of {B-T}(T represents POS tag). For example, B-NN indicates that the character is the begin of a noun. 4 Automatically Establishing the Loose Mapping Function for the Labels of Characters T"
D13-1062,C12-2093,1,0.828004,"been proven quite successful in practice and has been also applied to NLP (Ando and Zhang, 2005). We also preliminarily veriﬁed that multiple task learning can improve the performance on this problem in our previous work (Zhao et 660 3 Joint Chinese Word Segmentation and POS Tagging Currently, the mainstream method of Chinese POS tagging is joint segmentation & tagging with character-based sequence labeling models(Laﬀerty et al., 2001), which can avoid the problem of segmentation error propagation and achieve higher performance on both subtasks(Ng and Low, 2004; Jiang et al., 2008; Sun, 2011; Qiu et al., 2012). The label of each character is the crossproduct of a segmentation label and a tagging label. If we employ the commonly used label set {B, I, E, S} for the segmentation part of crosslabels ({B, I, E} represent Begin, Inside, End of a multi-node segmentation respectively, and S represents a Single node segmentation), the label of character can be in the form of {B-T}(T represents POS tag). For example, B-NN indicates that the character is the begin of a noun. 4 Automatically Establishing the Loose Mapping Function for the Labels of Characters To combine two human-annotated corpora, the relatio"
D13-1062,P13-4009,1,0.818558,"distinguish the right tag. And if the right tag is one of “n,nt,nz/PDD” but not “n/PDD” (for example, “nt/PDD”), which means it is a “NN/CTB”, the weight of “NN/CTB” will remain unchanged according to the algorithm (updating “n/PDD” changes the “NN/CTB”, but updating “nt/PDD” changes it back). Therefore, after multiple iterations, useful features derived from the mapping function are typically receive more updates, which take relatively more responsibility for correct prediction. The ﬁnal model has good parameter estimates for the shared information. We implement our method based on FudanNLP(Qiu et al., 2013). 7 Experiments 7.1 Datasets We use the two representative corpora mentioned above, Penn Chinese Treebank (CTB) and PKU’s People’s Daily (PPD) in our experiments. Dataset Partition Training CTB-5 CTB-S PPD Develop Test Training Test Training Test Sections 1−270 400−931 1001−1151 301−325 271−300 - Words 0.47M 6.66K 7.82K 0.64M 59.96K 1.11M 0.16M Table 3: Data partitioning for CTB and PD 7.1.1 CTB Dataset To better comparison with the previous works, we use two commonly used criterions to partition CTB dataset into the train and test sets. • One is the partition criterion used in (Jin and Chen,"
D13-1062,P12-1025,0,0.447924,"总/b 决赛/vn Table 1: Incompatible word segmentation and POS tagging standards between CTB and PDD CTB, into two words. The POS tagsets are also signiﬁcantly diﬀerent. For example, PDD gives diverse tags “n” and “vn” for the noun, while CTB just gives “NN”. For proper names, they may be tagged as “nr”, “ns”, etc in PDD, while they are just tagged as “NR” in CTB. Recently, it has attracted more and more research interests to exploit heterogeneous annotation data for Chinese word segmentation and POS tagging. (Jiang et al., 2009) presented a preliminary study for the annotation adaptation topic. (Sun and Wan, 2012) proposed a structure-based stacking model to fully utilize heterogeneous word structures. They also reported that there is no one-to-one mapping between the heterogeneous word classiﬁcation and the mapping between heterogeneous tags is very uncertain. These methods usually have a two-step process. The ﬁrst step is to train the preliminary taggers on heterogeneous annotations. The second step is to train the ﬁnal taggers by using the outputs of the preliminary taggers as features. We call these methods as “pipelinebased” methods. In this paper, we propose a method for joint Chinese word segmen"
D13-1062,P11-1139,0,0.0206805,"arning has been proven quite successful in practice and has been also applied to NLP (Ando and Zhang, 2005). We also preliminarily veriﬁed that multiple task learning can improve the performance on this problem in our previous work (Zhao et 660 3 Joint Chinese Word Segmentation and POS Tagging Currently, the mainstream method of Chinese POS tagging is joint segmentation & tagging with character-based sequence labeling models(Laﬀerty et al., 2001), which can avoid the problem of segmentation error propagation and achieve higher performance on both subtasks(Ng and Low, 2004; Jiang et al., 2008; Sun, 2011; Qiu et al., 2012). The label of each character is the crossproduct of a segmentation label and a tagging label. If we employ the commonly used label set {B, I, E, S} for the segmentation part of crosslabels ({B, I, E} represent Begin, Inside, End of a multi-node segmentation respectively, and S represents a Single node segmentation), the label of character can be in the form of {B-T}(T represents POS tag). For example, B-NN indicates that the character is the begin of a noun. 4 Automatically Establishing the Loose Mapping Function for the Labels of Characters To combine two human-annotated c"
D13-1097,baccianella-etal-2010-sentiwordnet,0,0.0955458,"Missing"
D13-1097,C10-1021,0,0.014323,"t analysis model (Hu and Liu, 2004), which also use flat frames to represent evaluations. Since the cross sentences relations are considered in this work, the discourse-level relation extraction methods are also related to ours. Marcu and Echihabi (2002) proposed to use an unsupervised approach to recognizing discourse relations. Lin et al.(2009) analyzed the impacts of features extracted from contextual information, constituent parse trees, dependency parse trees, and word pairs. Asher et al.(2009) studied discourse segments containing opinion expressions from the perspective of linguistics. Chen et al. (2010) introduced a multi-label model to detect emotion causes. They developed two sets of linguistic features for this task base on linguistic cues. Zirn et al. (2011) proposed to use MLN framework to capture the context information in analysing (sub-)sentences. The most similar work to ours was proposed by Somasundaran et al.(2009). They proposed to use iterative classification algorithm to capture discourselevel associations. However different to us, they focused on pairwise relationships between opinion expressions. In this paper, we used MLN framework to capture another different discourse-leve"
D13-1097,P09-1079,0,0.0441758,"Missing"
D13-1097,P12-1105,0,0.0280042,"Missing"
D13-1097,P12-1007,0,0.0182653,"mented several state7 • CRF-Subj: We follow the method proposed by Zhao et al. (2008), which regard the subjectivity of all clauses throughout a paragraph as a sequential flow of sentiments and use CRFs to model it. The feature sets are similar as the local formulas for MLN including words, POS tags, dependency relations, and opinion lexicon. an explanation of the opinion sentence. click of-the-art methods for comparison. http://code.google.com/p/thebeast 952 • SVM-Rel: We also use LibSVM (Chang and Lin, 2011) to classify the relations between clauses. Following the configurations reported by Feng and Hirst (2012), we use linear kernel and probability estimation to model it. 4.3 Results Table 3 shows the comparisons of the proposed method with the state-of-the-art systems on subjectivity classification and explanatory relation extraction. From the results, we can observe that recursive autoencoders based subjectivity classification method achieves slightly better performance than our method and conditional random fields based method. The performances of the proposed method are similar as CRFs’. We think that the main reason is that only lexical features are used in MLN models for subjective classificat"
D13-1097,P10-1041,0,0.051271,"Missing"
D13-1097,C04-1200,0,0.0806372,"cts of clause distance and sentence distance are not as significant as the other features. 5 Related Work Our work relates to three research areas: sentiment analysis/opinion mining, discourse-level relation extraction, and Markov logic networks. Along with the increasing requirement, subjectivity classification has recently received considerable attention from both the industry and researchers. A variety of approaches and methods have been proposed for this task from different aspects. Among them, a number of approaches focus on classifying sentiments of text in different levels (e.g. words (Kim and Hovy, 2004), phrases (Wilson et al., 2005), sentences (Zhao et al., 2008), documents (Pang et al., 2002) and so on.), and detecting the overall polarity of them. Another research direction tries to convert the sentiment analysis task into entity identification and relation extraction. Hu and Liu (2004) proposed to use a set of methods to produce feature-based summary of a large number of customer reviews. Kobayashi et al. (2007) assumed that evaluative opinions could be structured as a frame which is composed by opinion holder, subject, aspect, and evaluation. They converted the task to two kinds of rela"
D13-1097,D07-1114,0,0.147274,"an be automatically identified from reviews, sentiment analysis applications may benefit from it. Existing opinion mining approaches mainly focus on subjective text. They try to determine the subjectivity and polarity of fragments of documents (e.g. a paragraph, a sentence, a phrase and a word) (Pang et al., 2002; Riloff et al., 2003; Takamura et al., 2005; Mihalcea et al., 2007; Dasgupta and Ng, ; Hassan and Radev, 2010; Meng et al., 2012; Dragut et al., 2012). Fine-grained methods were also introduced to extract opinion holder, opinion expression, opinion target, and other opinion elements (Kobayashi et al., 2007; Wu et al., reviews-guidelines 946 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 946–957, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics 2011; Xu et al., 2013; Yang and Cardie, 2013). Major research directions and challenges of sentiment analysis can also be found in surveys (Pang and Lee, 2008; Liu, 2012). In this work, we aim to identify subjective text segments and extract their corresponding explanations from product reviews in discourse level. We propose to use Markov Logic Networks (MLN) (Ric"
D13-1097,D09-1036,0,0.447174,". (c4) (c3) (c7) C5 C2 The S90 is a true pocket camera. It is very compact. also top notch. grip. C1 Both cameras are quite different but truly (c6) (c5) C6 C3 The build quality is It feels solid and it is easy to It is so small and convenient, (c8) C7 C4 you C8 will find that you will always carry it with you. (b) Directed Graph Representation (a) Example Review Figure 1: Directed graph representation of a sample document. and fits more than enough stuff. Many sentences, which express explanatory relation, do not contain any connectives (e.g. “because”, “the reason is”, and so on). Lin et al.(2009) generalized four challenges (include ambiguity, inference, context, and world knowledge) to automated implicit discourse relation recognition. In this task, we also need to address those challenges. From the these examples, we can observe that extracting explanatory relations from product reviews is a challenging task. Both linguistic and global constraints should be carefully studied. 3 The Proposed Approach In this section, we present our method for jointly classifying the subjectivity of text segments and extracting explanatory relations. Firstly, we briefly describe the framework of Marko"
D13-1097,P02-1047,0,0.106981,"Missing"
D13-1097,P12-1060,0,0.0144076,"tion, to the best of our knowledge, there is no existing work that deals with explanation extraction for opinions in discourse level. We think that if explanatory relations can be automatically identified from reviews, sentiment analysis applications may benefit from it. Existing opinion mining approaches mainly focus on subjective text. They try to determine the subjectivity and polarity of fragments of documents (e.g. a paragraph, a sentence, a phrase and a word) (Pang et al., 2002; Riloff et al., 2003; Takamura et al., 2005; Mihalcea et al., 2007; Dasgupta and Ng, ; Hassan and Radev, 2010; Meng et al., 2012; Dragut et al., 2012). Fine-grained methods were also introduced to extract opinion holder, opinion expression, opinion target, and other opinion elements (Kobayashi et al., 2007; Wu et al., reviews-guidelines 946 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 946–957, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics 2011; Xu et al., 2013; Yang and Cardie, 2013). Major research directions and challenges of sentiment analysis can also be found in surveys (Pang and Lee, 2008; Liu, 2012). In this work, w"
D13-1097,P07-1123,0,0.0303499,"have been proposed for this task from different aspects. Among them, a number of approaches focus on classifying sentiments of text in different levels (e.g. words (Kim and Hovy, 2004), phrases (Wilson et al., 2005), sentences (Zhao et al., 2008), documents (Pang et al., 2002) and so on.), and detecting the overall polarity of them. Another research direction tries to convert the sentiment analysis task into entity identification and relation extraction. Hu and Liu (2004) proposed to use a set of methods to produce feature-based summary of a large number of customer reviews. Kobayashi et al. (2007) assumed that evaluative opinions could be structured as a frame which is composed by opinion holder, subject, aspect, and evaluation. They converted the task to two kinds of relation extraction tasks and proposed a machine learning-based method which used both contextual and statistical clues. Analysis of some special types of sentences were also introduced in recent years. Jindal and Liu (2006) studied the problem of identifying comparative sentences. They analyzed different types of comparative sentences and proposed learning approaches to identify them. Conditional sentences were studied b"
D13-1097,D09-1019,0,0.0486385,"Missing"
D13-1097,W02-1011,0,0.0150022,"ail descriptions are used to explain the reflection problem of the camera screen. Although, explanations provide valuable information, to the best of our knowledge, there is no existing work that deals with explanation extraction for opinions in discourse level. We think that if explanatory relations can be automatically identified from reviews, sentiment analysis applications may benefit from it. Existing opinion mining approaches mainly focus on subjective text. They try to determine the subjectivity and polarity of fragments of documents (e.g. a paragraph, a sentence, a phrase and a word) (Pang et al., 2002; Riloff et al., 2003; Takamura et al., 2005; Mihalcea et al., 2007; Dasgupta and Ng, ; Hassan and Radev, 2010; Meng et al., 2012; Dragut et al., 2012). Fine-grained methods were also introduced to extract opinion holder, opinion expression, opinion target, and other opinion elements (Kobayashi et al., 2007; Wu et al., reviews-guidelines 946 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 946–957, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics 2011; Xu et al., 2013; Yang and Cardie, 2013). Major resea"
D13-1097,P09-2004,0,0.154187,"g both generative and discriminative approaches (Richardson and Domingos, 2006; Singla and Domingos, 2006). There are also several MLN learning packages available online such as thebeast2 , Tuffy3 , PyMLNs4 , Alchemy5 , and so on. 2 http://code.google.com/p/thebeast http://hazy.cs.wisc.edu/hazy/tuffy/ 4 http://www9-old.in.tum.de/people/jain/mlns/ 5 http://alchemy.cs.washington.edu/ 3 Describing the attributes of words subjLexicon(w) The word w belongs to the subjective lexicon (Baccianella et al., 2010). relationLexicon(w) The word w belongs to the lexicon of explanation relation connectives (Pitler and Nenkova, 2009). Describing the attributes of the clause ci word(i, w) The clause ci has word w. f irstW ord(i, w) The first word of clause ci is word w. pos(i, w, t) The POS tag of word w is t in clause ci . dep(i, h, m) Word m and h are governor and dependent of a dependency relation in clause ci . Describing the attributes of relations between clause ci and clause cj clauseDistance(i, j, m) Distance between clause ci and clause cj in clauses is m. sentenceDistance(i, j, n) Distance between clause ci and clause cj in sentences is n. Table 1: Descriptions of observed predicates. 3.2 Clause Identification We"
D13-1097,D08-1068,0,0.22553,"ational Linguistics 2011; Xu et al., 2013; Yang and Cardie, 2013). Major research directions and challenges of sentiment analysis can also be found in surveys (Pang and Lee, 2008; Liu, 2012). In this work, we aim to identify subjective text segments and extract their corresponding explanations from product reviews in discourse level. We propose to use Markov Logic Networks (MLN) (Richardson and Domingos, 2006) to learn the joint model for subjective classification and explanatory relation extraction. MLN has been applied in several natural language processing tasks (Singla and Domingos, 2006; Poon and Domingos, 2008; Yoshikawa et al., 2009; Andrzejewski et al., 2011; Song et al., 2012) and demonstrated its advantages. It can easily incorporate rich linguistic features and global constraints by designing various logic formulas, which can also be viewed as templates or rules. Logic formulas are combined in a probabilistic framework to model soft constraints. Hence, the proposed approach can benefit a lot from this framework. To evaluate the proposed method, we crawled a large number of product reviews and constructed a labeled corpus through Amazon’s Mechanical Turk. Two tasks were deployed for labeling th"
D13-1097,W08-2125,0,0.022149,"algorithm to capture discourselevel associations. However different to us, they focused on pairwise relationships between opinion expressions. In this paper, we used MLN framework to capture another different discourse-level relation, which exists between subject clauses or subject clause and objective clause. Richardson and Domingos (2006) proposed Markov Logic Networks, which combines firstorder logic and probabilistic graphical models. In 954 recent years, MLN has been adopted for several natural language processing tasks and achieved a certain level of success (Singla and Domingos, 2006; Riedel and Meza-Ruiz, 2008; Yoshikawa et al., 2009; Andrzejewski et al., 2011; Jiang et al., 2012; Huang et al., 2012). Singla and Domingos (2006) modeled the entity resolution problem with MLN. They demonstrated the capability of MLN to seamlessly combine a number of previous approaches. Poon and Domingos (2008) proposed to use MLN for joint unsupervised coreference resolution. Yoshikawa et al. (2009) proposed to use Markov logic to incorporate both local features and global constraints that hold between temporal relations. Andrzejewski et al. (2011) introduced a framework for incorporating general domain knowledge, w"
D13-1097,W03-0404,0,0.154829,"Missing"
D13-1097,D11-1014,0,0.0581746,"life is something I come to expect from this line of camera. ()Subjective ()Objective I have the camera set to shut off the sensor after about 30 seconds ()Subjective ()Objective Task2: Help us check whether a sentence is The opinion sentence (red one) is extracted from product reviews and express opinion towards some attributes/parts of a product. Please help us check whether the following blue sentences describe a set of facts which clarifies the causes, reason, and consequences of the opinion given in the opinion sentence. &quot;yes&quot; if there is an explanation relation between them, • RAE-Subj: Socher et al. (2011) proposed to use recursive autoencoders for sentence-level predication of sentiment label distributions. To compare with it, we also reimplement their method without any hand designed lexicon. &quot;no&quot; otherwise. • PDTB-Rel: For discourse relation extraction, we use “PDTB-Styled End-to-End Discourse Parser” (Lin et al., 2010) to extract discourse level relations as baseline. Since it is a general discourse relations identification algorithms, “Cause”, “Pragmatic Cause”, “Instantiation”, and “Restatement” relation types are treated as explanatory relation in this work. The battery life is something"
D13-1097,W09-3210,0,0.224164,"Missing"
D13-1097,D12-1114,0,0.0968763,"earch directions and challenges of sentiment analysis can also be found in surveys (Pang and Lee, 2008; Liu, 2012). In this work, we aim to identify subjective text segments and extract their corresponding explanations from product reviews in discourse level. We propose to use Markov Logic Networks (MLN) (Richardson and Domingos, 2006) to learn the joint model for subjective classification and explanatory relation extraction. MLN has been applied in several natural language processing tasks (Singla and Domingos, 2006; Poon and Domingos, 2008; Yoshikawa et al., 2009; Andrzejewski et al., 2011; Song et al., 2012) and demonstrated its advantages. It can easily incorporate rich linguistic features and global constraints by designing various logic formulas, which can also be viewed as templates or rules. Logic formulas are combined in a probabilistic framework to model soft constraints. Hence, the proposed approach can benefit a lot from this framework. To evaluate the proposed method, we crawled a large number of product reviews and constructed a labeled corpus through Amazon’s Mechanical Turk. Two tasks were deployed for labeling the corpus. We compared the proposed method with state-ofthe-art methods"
D13-1097,P05-1017,0,0.0292598,"reflection problem of the camera screen. Although, explanations provide valuable information, to the best of our knowledge, there is no existing work that deals with explanation extraction for opinions in discourse level. We think that if explanatory relations can be automatically identified from reviews, sentiment analysis applications may benefit from it. Existing opinion mining approaches mainly focus on subjective text. They try to determine the subjectivity and polarity of fragments of documents (e.g. a paragraph, a sentence, a phrase and a word) (Pang et al., 2002; Riloff et al., 2003; Takamura et al., 2005; Mihalcea et al., 2007; Dasgupta and Ng, ; Hassan and Radev, 2010; Meng et al., 2012; Dragut et al., 2012). Fine-grained methods were also introduced to extract opinion holder, opinion expression, opinion target, and other opinion elements (Kobayashi et al., 2007; Wu et al., reviews-guidelines 946 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 946–957, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics 2011; Xu et al., 2013; Yang and Cardie, 2013). Major research directions and challenges of sentiment a"
D13-1097,W01-0708,0,0.393394,"Missing"
D13-1097,H05-1044,0,0.0318563,"tence distance are not as significant as the other features. 5 Related Work Our work relates to three research areas: sentiment analysis/opinion mining, discourse-level relation extraction, and Markov logic networks. Along with the increasing requirement, subjectivity classification has recently received considerable attention from both the industry and researchers. A variety of approaches and methods have been proposed for this task from different aspects. Among them, a number of approaches focus on classifying sentiments of text in different levels (e.g. words (Kim and Hovy, 2004), phrases (Wilson et al., 2005), sentences (Zhao et al., 2008), documents (Pang et al., 2002) and so on.), and detecting the overall polarity of them. Another research direction tries to convert the sentiment analysis task into entity identification and relation extraction. Hu and Liu (2004) proposed to use a set of methods to produce feature-based summary of a large number of customer reviews. Kobayashi et al. (2007) assumed that evaluative opinions could be structured as a frame which is composed by opinion holder, subject, aspect, and evaluation. They converted the task to two kinds of relation extraction tasks and propo"
D13-1097,D11-1123,1,0.874054,"Missing"
D13-1097,P13-1161,0,0.0301006,"rase and a word) (Pang et al., 2002; Riloff et al., 2003; Takamura et al., 2005; Mihalcea et al., 2007; Dasgupta and Ng, ; Hassan and Radev, 2010; Meng et al., 2012; Dragut et al., 2012). Fine-grained methods were also introduced to extract opinion holder, opinion expression, opinion target, and other opinion elements (Kobayashi et al., 2007; Wu et al., reviews-guidelines 946 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 946–957, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics 2011; Xu et al., 2013; Yang and Cardie, 2013). Major research directions and challenges of sentiment analysis can also be found in surveys (Pang and Lee, 2008; Liu, 2012). In this work, we aim to identify subjective text segments and extract their corresponding explanations from product reviews in discourse level. We propose to use Markov Logic Networks (MLN) (Richardson and Domingos, 2006) to learn the joint model for subjective classification and explanatory relation extraction. MLN has been applied in several natural language processing tasks (Singla and Domingos, 2006; Poon and Domingos, 2008; Yoshikawa et al., 2009; Andrzejewski et"
D13-1097,P09-1046,0,0.175952,"Xu et al., 2013; Yang and Cardie, 2013). Major research directions and challenges of sentiment analysis can also be found in surveys (Pang and Lee, 2008; Liu, 2012). In this work, we aim to identify subjective text segments and extract their corresponding explanations from product reviews in discourse level. We propose to use Markov Logic Networks (MLN) (Richardson and Domingos, 2006) to learn the joint model for subjective classification and explanatory relation extraction. MLN has been applied in several natural language processing tasks (Singla and Domingos, 2006; Poon and Domingos, 2008; Yoshikawa et al., 2009; Andrzejewski et al., 2011; Song et al., 2012) and demonstrated its advantages. It can easily incorporate rich linguistic features and global constraints by designing various logic formulas, which can also be viewed as templates or rules. Logic formulas are combined in a probabilistic framework to model soft constraints. Hence, the proposed approach can benefit a lot from this framework. To evaluate the proposed method, we crawled a large number of product reviews and constructed a labeled corpus through Amazon’s Mechanical Turk. Two tasks were deployed for labeling the corpus. We compared th"
D13-1097,D08-1013,0,0.0246806,"cant as the other features. 5 Related Work Our work relates to three research areas: sentiment analysis/opinion mining, discourse-level relation extraction, and Markov logic networks. Along with the increasing requirement, subjectivity classification has recently received considerable attention from both the industry and researchers. A variety of approaches and methods have been proposed for this task from different aspects. Among them, a number of approaches focus on classifying sentiments of text in different levels (e.g. words (Kim and Hovy, 2004), phrases (Wilson et al., 2005), sentences (Zhao et al., 2008), documents (Pang et al., 2002) and so on.), and detecting the overall polarity of them. Another research direction tries to convert the sentiment analysis task into entity identification and relation extraction. Hu and Liu (2004) proposed to use a set of methods to produce feature-based summary of a large number of customer reviews. Kobayashi et al. (2007) assumed that evaluative opinions could be structured as a frame which is composed by opinion holder, subject, aspect, and evaluation. They converted the task to two kinds of relation extraction tasks and proposed a machine learning-based me"
D13-1097,I11-1038,0,0.0226505,"course-level relation extraction methods are also related to ours. Marcu and Echihabi (2002) proposed to use an unsupervised approach to recognizing discourse relations. Lin et al.(2009) analyzed the impacts of features extracted from contextual information, constituent parse trees, dependency parse trees, and word pairs. Asher et al.(2009) studied discourse segments containing opinion expressions from the perspective of linguistics. Chen et al. (2010) introduced a multi-label model to detect emotion causes. They developed two sets of linguistic features for this task base on linguistic cues. Zirn et al. (2011) proposed to use MLN framework to capture the context information in analysing (sub-)sentences. The most similar work to ours was proposed by Somasundaran et al.(2009). They proposed to use iterative classification algorithm to capture discourselevel associations. However different to us, they focused on pairwise relationships between opinion expressions. In this paper, we used MLN framework to capture another different discourse-level relation, which exists between subject clauses or subject clause and objective clause. Richardson and Domingos (2006) proposed Markov Logic Networks, which comb"
D13-1097,prasad-etal-2008-penn,0,\N,Missing
D15-1046,P11-1016,0,0.0360938,"Missing"
D15-1046,J93-2003,0,0.0542942,"βh Mw,2 k Mw,(.) + 2β h (12) The potential size of the probability alignment ϕ1 between hashtag and word is W · V · K. The data sparsity may pose a more serious problem in estimating ϕ1 than the topic-free word alignment case. We use interpolation smoothing technique for ϕ1 . In this paper, we employ smoothing as follows: 1 ϕ1∗ h,k,w = γϕh,k,w + (1 − γ)P (h|w), (13) where ϕ1∗ h,k,w is the smoothed topical alignment probabilities, ϕ1h,k,w is the original topical alignment probabilities, P (h|w) is topic-free word alignment probability. In this work, we obtain P (h|w) by exploring IBM model-1 (Brown et al., 1993). γ is trade-off of two probabilities ranging from 0 to 1. When γ = 0, ϕ1∗ h,k,w reduces to topicfree word alignment probability, and when γ = 1, there will be no smoothing in ϕ1∗ h,k,w . ¯ w¬d , y, β w ) = p(wd |β w ) = p(wd |zd = k, Z (9) pe(wd |φk¯ )h(φk¯ )dφk¯ , φk¯ Q where pe(wd |φk¯ ) = 1≤n≤Nd ,yd =1 p(wdn |φk¯ ). n We can calculate the probabilities of generating hashtags from two situations as follows: p(hd |z, wd , y, x, β h ) =  k,¬d Mw +β h Q Md P  dn ,hdm    m=1 n∈Ned M k,¬d +β h V = k + βh Mw,h 2.2.3 Hashtag Recommendation Suppose given an unlabeled dataset, we firstly wd ,(."
D15-1046,C10-2028,0,0.0282227,"Missing"
D15-1046,D11-1146,0,0.175799,"ere obtained by starting from a set of seed users and their follower/followee relations. We extract the microblogs posted with hashtags between Jan. 2012 and July 2013. Finally, 1,118,792 microblogs posted are selected for this work. The unique number of hashtags in the corpus is 305,227. We randomly select 100K as training data, 10K as development data, and 10K as test set. The hashtags marked in the original microblogs are considered as the golden standards. 3.2 • Translation model (IBM-1): IBM model 1 is directly applied to obtain the alignment probability between the word and the hashtag (Liu et al., 2011). • Topical translation model (TTM): Ding et al. (2013) proposed the TTM for hashtag extraction. We implemented and extended their method for evaluating on the corpus constructed in this work. The number of topics in TTM is set to 20, and α is set to 50/K. The hyperparameters used in TTM are also selected based on the development data set. 3.3 Table 1 shows the comparisons of the proposed method with the state-of-the-art methods on the constructed evaluation dataset. “CNHR” denotes the method proposed in this paper. “NHR1” is a degenerate variation of CNHR, in which we consider all the hashtag"
D15-1046,C12-1105,0,0.224987,"corresponding microblog. While, the aim of hashtag #BREAKING in the example 2 is used as a label of the microblog. The different uses greatly 401 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 401–410, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. impact the strategy of hashtag recommendation. However, there has been relatively few studies which take this issue into consideration. In this paper, we propose a novel nonparametric Bayesian method to perform this problem. Inspired by the methods proposed by Liu et al. (2012), we assume that the hashtags and textual content in the corresponding microblog are parallel descriptions of the same thing in different languages. We adapt a translation model with topic distribution to achieve this task. Because of the ability of Dirichlet Process Mixture Models (DPMM) (Antoniak and others, 1974; Ferguson, 1983) to handle an unbounded number of topics, the proposed method is extended from them. Based on the different uses of hashtags, we incorporate the type of hashtag into the DPMM as a hidden variable. The main contributions of this work can be summarized as follows: A pe"
D15-1046,J03-1002,0,0.0434618,"w Nk dn +β (.) Nk +W β assigned” among “hashtags manually assigned”. F1 is the harmonic mean of precision and recall. We do 500 iterations of Gibbs sampling to train the model. For optimizing the hyperparmeters of the proposed method and alternative methods, we use development data set to do it. In this work, the scale parameter α is set to Gamma(5, 0.5). The other settings of hyperparameters are as follows: β w = 0.1, β h = 0.1, η = 0.01, and σ = 0.01. The smoothing factor γ in Eq.(13) is set to 0.8. For estimating the translation probability without topical information, we use GIZA++ 1.07 (Och and Ney, 2003) to do it. Since hashtag recommendation task can also be modeled as a classification problem, we compare the proposed model with the following alternative methods: w and Nk dn is a count of words wdn that are assigned to topic k in the corpus. And p(k) = N Nk+α is regarded as a prior (.) for topic distribution, where Z is the normalized factor. With topic distribution χ and topic-specific word alignment table ϕ∗ , we can rank hashtags for the microblog d in the unlabeled data through the following equation: p(hdm |wd , χd , ϕ∗ ) ∝ Nd X C K X X p(zd |χd ) · p(wdn |wd ) · p(xdm ) zd =1 n=1 x=1 ·"
D15-1092,P14-1062,0,0.52296,"to the top layer, and applied them on Chinese word segmentation (Chen et al., 2015a) and dependency parsing (Chen et al., 2015b) tasks. However, these structures are not suitable for modeling sentences. Introduction Recently, neural network based sentence modeling approaches have been increasingly focused on for their ability to minimize the efforts in feature engineering, such as Neural Bag-of-Words (NBoW), Recurrent Neural Network (RNN) (Mikolov et al., 2010), Recursive Neural Network (RecNN) (Pollack, 1990; Socher et al., 2013b; Socher et al., 2012) and Convolutional Neural Network (CNN) (Kalchbrenner et al., 2014; Hu et al., 2014). Among these methods, recursive neural networks (RecNNs) have shown their excellent abilities to model the word combinations in sentence. However, RecNNs require a pre-defined topological structure, like parse tree, to encode sentence, which limits the scope of its application. Cho et al. (2014) proposed the gated recursive convolutional neural network (grConv) by utilizing the directed acyclic graph (DAG) structure instead of parse tree ∗ cannot agree Corresponding author. 793 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 793–"
D15-1092,D14-1181,0,0.127395,"Missing"
D15-1092,C02-1150,0,0.144163,"zN , zL and zR decide what to preserve when combining the children’s information. Intuitively, these gates seem to decide how to update and exploit the combination information. In the case of text classification, for each given (i) sentence xi = w1:N (i) and the corresponding class (i) (i) (i) w(i) w(i) w3 w4 w5 1 2 Figure 2: Architecture of Gated Recursive Neural Network (GRNN). In this paper, we adopt the full binary tree as the topological structure to reduce the model complexity. Experiments on the Stanford Sentiment Treebank dataset (Socher et al., 2013b) and the TREC questions dataset (Li and Roth, 2002) show the effectiveness of our approach. 2 (i) yi , we first represent each word wj into its corresponding embedding ww(i) ∈ Rd , where N (i) inj dicates the length of i-th sentence and d is dimensionality of word embeddings. Then, the embeddings are sent to the first layer of GRNN as inputs, whose outputs are recursively applied to upper layers until it outputs a single fixed-length vector. Next, we receive the class distribution P(·|xi ; θ) for the given sentence xi by a softmax transformation of ui , where ui is the top node of the network (a fixed length vectorial representation): Gated Re"
D15-1092,P15-1168,1,0.463731,"ly increases with the length of sentences. Another difference is that we introduce two kinds of gates, reset and update gates (Chung et al., 2014), to control the combinations in recursive structure. With these two gating mechanisms, our model can better model the complicated combinations of features and capture the long dependency interactions. In our previous works, we have investigated several different topological structures (tree and directed acyclic graph) to recursively model the semantic composition from the bottom layer to the top layer, and applied them on Chinese word segmentation (Chen et al., 2015a) and dependency parsing (Chen et al., 2015b) tasks. However, these structures are not suitable for modeling sentences. Introduction Recently, neural network based sentence modeling approaches have been increasingly focused on for their ability to minimize the efforts in feature engineering, such as Neural Bag-of-Words (NBoW), Recurrent Neural Network (RNN) (Mikolov et al., 2010), Recursive Neural Network (RecNN) (Pollack, 1990; Socher et al., 2013b; Socher et al., 2012) and Convolutional Neural Network (CNN) (Kalchbrenner et al., 2014; Hu et al., 2014). Among these methods, recursive neural"
D15-1092,D11-1014,0,0.0609733,"L) criterion to train our model. Given training set (xi , yi ) and the parameter set of our model θ, the goal is to minimize the loss function: 3.3 Experiment Results Table 2 shows the performance of our GRNN on three datasets. 1 m 1 ∑ λ J(θ) = − log P(yi |xi ; θ) + ∥θ∥22 , (7) m 2m http://nlp.stanford.edu/sentiment http://cogcomp.cs.illinois.edu/Data/ QA/QC/ 2 i=1 795 Methods NBoW (Kalchbrenner et al., 2014) PV (Le and Mikolov, 2014) CNN-non-static (Kim, 2014) CNN-multichannel (Kim, 2014) MaxTDNN (Collobert and Weston, 2008) DCNN (Kalchbrenner et al., 2014) RecNTN (Socher et al., 2013b) RAE (Socher et al., 2011) MV-RecNN (Socher et al., 2012) AdaSent (Zhao et al., 2015) GRNN (our approach) SST-1 42.4 44.6∗ 48.0 47.4 37.4 48.5 45.7 43.2 44.4 47.5 SST-2 80.5 82.7∗ 87.2 88.1 77.1 86.8 85.4 82.4 82.9 85.5 QC 88.2 91.8∗ 93.6 92.2 84.4 93.0 92.4 93.8 Table 2: Performances of the different models. The result of PV is from our own implementation based on Gensim. Competitor Models Neural Bag-of-Words (NBOW) model is a simple and intuitive method which ignores the word order. Paragraph Vector (PV) (Le and Mikolov, 2014) learns continuous distributed vector representations for pieces of texts, which can be rega"
D15-1092,P13-1045,0,0.251571,"acyclic graph) to recursively model the semantic composition from the bottom layer to the top layer, and applied them on Chinese word segmentation (Chen et al., 2015a) and dependency parsing (Chen et al., 2015b) tasks. However, these structures are not suitable for modeling sentences. Introduction Recently, neural network based sentence modeling approaches have been increasingly focused on for their ability to minimize the efforts in feature engineering, such as Neural Bag-of-Words (NBoW), Recurrent Neural Network (RNN) (Mikolov et al., 2010), Recursive Neural Network (RecNN) (Pollack, 1990; Socher et al., 2013b; Socher et al., 2012) and Convolutional Neural Network (CNN) (Kalchbrenner et al., 2014; Hu et al., 2014). Among these methods, recursive neural networks (RecNNs) have shown their excellent abilities to model the word combinations in sentence. However, RecNNs require a pre-defined topological structure, like parse tree, to encode sentence, which limits the scope of its application. Cho et al. (2014) proposed the gated recursive convolutional neural network (grConv) by utilizing the directed acyclic graph (DAG) structure instead of parse tree ∗ cannot agree Corresponding author. 793 Proceedin"
D15-1092,D13-1170,0,0.353549,"acyclic graph) to recursively model the semantic composition from the bottom layer to the top layer, and applied them on Chinese word segmentation (Chen et al., 2015a) and dependency parsing (Chen et al., 2015b) tasks. However, these structures are not suitable for modeling sentences. Introduction Recently, neural network based sentence modeling approaches have been increasingly focused on for their ability to minimize the efforts in feature engineering, such as Neural Bag-of-Words (NBoW), Recurrent Neural Network (RNN) (Mikolov et al., 2010), Recursive Neural Network (RecNN) (Pollack, 1990; Socher et al., 2013b; Socher et al., 2012) and Convolutional Neural Network (CNN) (Kalchbrenner et al., 2014; Hu et al., 2014). Among these methods, recursive neural networks (RecNNs) have shown their excellent abilities to model the word combinations in sentence. However, RecNNs require a pre-defined topological structure, like parse tree, to encode sentence, which limits the scope of its application. Cho et al. (2014) proposed the gated recursive convolutional neural network (grConv) by utilizing the directed acyclic graph (DAG) structure instead of parse tree ∗ cannot agree Corresponding author. 793 Proceedin"
D15-1092,D12-1110,0,\N,Missing
D15-1092,W14-4012,0,\N,Missing
D15-1092,D15-1215,1,\N,Missing
D15-1141,J96-1002,0,0.251174,"ments on PKU, MSRA and CTB6 benchmark datasets show that our model outperforms the previous neural network models and state-of-the-art methods. 1 Introduction Word segmentation is a fundamental task for Chinese language processing. In recent years, Chinese word segmentation (CWS) has undergone great development. The popular method is to regard word segmentation task as a sequence labeling problem (Xue, 2003; Peng et al., 2004). The goal of sequence labeling is to assign labels to all elements in a sequence, which can be handled with supervised learning algorithms such as Maximum Entropy (ME) (Berger et al., 1996) and Conditional Random Fields (CRF) (Lafferty et al., 2001). However, the ability of these models is restricted by the design of features, and the number of features could be so large that the result models are too large for practical use and prone to overfit on training corpus. Recently, neural network models have increasingly used for NLP tasks for their ability to minimize the effort in feature engineering (Collobert ∗ Corresponding author. et al., 2011; Socher et al., 2013; Turian et al., 2010; Mikolov et al., 2013b; Bengio et al., 2003). Collobert et al. (2011) developed the SENNA system"
D15-1141,P15-1168,1,0.526007,"urian et al., 2010; Mikolov et al., 2013b; Bengio et al., 2003). Collobert et al. (2011) developed the SENNA system that approaches or surpasses the state-of-theart systems on a variety of sequence labeling tasks for English. Zheng et al. (2013) applied the architecture of Collobert et al. (2011) to Chinese word segmentation and POS tagging, also he proposed a perceptron style algorithm to speed up the training process with negligible loss in performance. Pei et al. (2014) models tag-tag interactions, tagcharacter interactions and character-character interactions based on Zheng et al. (2013). Chen et al. (2015) proposed a gated recursive neural network (GRNN) to explicitly model the combinations of the characters for Chinese word segmentation task. Each neuron in GRNN can be regarded as a different combination of the input characters. Thus, the whole GRNN has an ability to simulate the design of the sophisticated features in traditional methods. Despite of their success, a limitation of them is that their performances are easily affected by the size of the context window. Intuitively, many words are difficult to segment based on the local information only. For example, the segmentation of the follow"
D15-1141,I05-3017,0,0.518308,"Missing"
D15-1141,D15-1280,1,0.0484058,"ent problem (Hochreiter and Schmidhuber, 1997). The LSTM provides a solution by incorporating memory units that allow the network to learn when to forget previous information and when to update the memory cells given new information. Thus, it is a natural choice to apply LSTM neural network to word segmentation task since the LSTM neural network can learn from data with long range temporal dependencies (memory) due to the considerable time lag between the inputs and their corresponding outputs. In addition, the LSTM has been applied successfully in many NLP tasks, such as text classification (Liu et al., 2015) and machine translation (Sutskever et al., 2014). The core of the LSTM model is a memory cell c encoding memory at every time step of what inputs have been observed up to this step (see Figure 2) . The behavior of the cell is controlled by three “gates”, namely input gate i, forget gate f and output gate o. The operations on gates are defined as element-wise multiplications, thus gate can either scale the input value if the gate is non-zero vector or omit input if the gate is zero vector. The output of output gate will be fed into the next time step t + 1 as previous hidden state and input of"
D15-1141,P14-1028,0,0.750867,"asks for their ability to minimize the effort in feature engineering (Collobert ∗ Corresponding author. et al., 2011; Socher et al., 2013; Turian et al., 2010; Mikolov et al., 2013b; Bengio et al., 2003). Collobert et al. (2011) developed the SENNA system that approaches or surpasses the state-of-theart systems on a variety of sequence labeling tasks for English. Zheng et al. (2013) applied the architecture of Collobert et al. (2011) to Chinese word segmentation and POS tagging, also he proposed a perceptron style algorithm to speed up the training process with negligible loss in performance. Pei et al. (2014) models tag-tag interactions, tagcharacter interactions and character-character interactions based on Zheng et al. (2013). Chen et al. (2015) proposed a gated recursive neural network (GRNN) to explicitly model the combinations of the characters for Chinese word segmentation task. Each neuron in GRNN can be regarded as a different combination of the input characters. Thus, the whole GRNN has an ability to simulate the design of the sophisticated features in traditional methods. Despite of their success, a limitation of them is that their performances are easily affected by the size of the cont"
D15-1141,C04-1081,0,0.829695,"tation, which adopts the long short-term memory (LSTM) neural network to keep the previous important information in memory cell and avoids the limit of window size of local context. Experiments on PKU, MSRA and CTB6 benchmark datasets show that our model outperforms the previous neural network models and state-of-the-art methods. 1 Introduction Word segmentation is a fundamental task for Chinese language processing. In recent years, Chinese word segmentation (CWS) has undergone great development. The popular method is to regard word segmentation task as a sequence labeling problem (Xue, 2003; Peng et al., 2004). The goal of sequence labeling is to assign labels to all elements in a sequence, which can be handled with supervised learning algorithms such as Maximum Entropy (ME) (Berger et al., 1996) and Conditional Random Fields (CRF) (Lafferty et al., 2001). However, the ability of these models is restricted by the design of features, and the number of features could be so large that the result models are too large for practical use and prone to overfit on training corpus. Recently, neural network models have increasingly used for NLP tasks for their ability to minimize the effort in feature engineer"
D15-1141,P13-1045,0,0.0413166,"o all elements in a sequence, which can be handled with supervised learning algorithms such as Maximum Entropy (ME) (Berger et al., 1996) and Conditional Random Fields (CRF) (Lafferty et al., 2001). However, the ability of these models is restricted by the design of features, and the number of features could be so large that the result models are too large for practical use and prone to overfit on training corpus. Recently, neural network models have increasingly used for NLP tasks for their ability to minimize the effort in feature engineering (Collobert ∗ Corresponding author. et al., 2011; Socher et al., 2013; Turian et al., 2010; Mikolov et al., 2013b; Bengio et al., 2003). Collobert et al. (2011) developed the SENNA system that approaches or surpasses the state-of-theart systems on a variety of sequence labeling tasks for English. Zheng et al. (2013) applied the architecture of Collobert et al. (2011) to Chinese word segmentation and POS tagging, also he proposed a perceptron style algorithm to speed up the training process with negligible loss in performance. Pei et al. (2014) models tag-tag interactions, tagcharacter interactions and character-character interactions based on Zheng et al. (2013"
D15-1141,D13-1061,0,0.443356,"d by the design of features, and the number of features could be so large that the result models are too large for practical use and prone to overfit on training corpus. Recently, neural network models have increasingly used for NLP tasks for their ability to minimize the effort in feature engineering (Collobert ∗ Corresponding author. et al., 2011; Socher et al., 2013; Turian et al., 2010; Mikolov et al., 2013b; Bengio et al., 2003). Collobert et al. (2011) developed the SENNA system that approaches or surpasses the state-of-theart systems on a variety of sequence labeling tasks for English. Zheng et al. (2013) applied the architecture of Collobert et al. (2011) to Chinese word segmentation and POS tagging, also he proposed a perceptron style algorithm to speed up the training process with negligible loss in performance. Pei et al. (2014) models tag-tag interactions, tagcharacter interactions and character-character interactions based on Zheng et al. (2013). Chen et al. (2015) proposed a gated recursive neural network (GRNN) to explicitly model the combinations of the characters for Chinese word segmentation task. Each neuron in GRNN can be regarded as a different combination of the input characters"
D15-1141,D11-1090,0,0.0275565,"Missing"
D15-1141,I05-3027,0,0.0366579,"Missing"
D15-1141,P10-1040,0,0.0273478,"equence, which can be handled with supervised learning algorithms such as Maximum Entropy (ME) (Berger et al., 1996) and Conditional Random Fields (CRF) (Lafferty et al., 2001). However, the ability of these models is restricted by the design of features, and the number of features could be so large that the result models are too large for practical use and prone to overfit on training corpus. Recently, neural network models have increasingly used for NLP tasks for their ability to minimize the effort in feature engineering (Collobert ∗ Corresponding author. et al., 2011; Socher et al., 2013; Turian et al., 2010; Mikolov et al., 2013b; Bengio et al., 2003). Collobert et al. (2011) developed the SENNA system that approaches or surpasses the state-of-theart systems on a variety of sequence labeling tasks for English. Zheng et al. (2013) applied the architecture of Collobert et al. (2011) to Chinese word segmentation and POS tagging, also he proposed a perceptron style algorithm to speed up the training process with negligible loss in performance. Pei et al. (2014) models tag-tag interactions, tagcharacter interactions and character-character interactions based on Zheng et al. (2013). Chen et al. (2015)"
D15-1141,O03-4002,0,0.931359,"word segmentation, which adopts the long short-term memory (LSTM) neural network to keep the previous important information in memory cell and avoids the limit of window size of local context. Experiments on PKU, MSRA and CTB6 benchmark datasets show that our model outperforms the previous neural network models and state-of-the-art methods. 1 Introduction Word segmentation is a fundamental task for Chinese language processing. In recent years, Chinese word segmentation (CWS) has undergone great development. The popular method is to regard word segmentation task as a sequence labeling problem (Xue, 2003; Peng et al., 2004). The goal of sequence labeling is to assign labels to all elements in a sequence, which can be handled with supervised learning algorithms such as Maximum Entropy (ME) (Berger et al., 1996) and Conditional Random Fields (CRF) (Lafferty et al., 2001). However, the ability of these models is restricted by the design of features, and the number of features could be so large that the result models are too large for practical use and prone to overfit on training corpus. Recently, neural network models have increasingly used for NLP tasks for their ability to minimize the effort"
D15-1141,P12-1083,0,0.049662,"Missing"
D15-1141,P07-1106,0,0.105943,"ir methods. models P +Pre-train (Zheng et al., 2013) (Pei et al., 2014) LSTM +bigram LSTM +Pre-train+bigram (Pei et al., 2014) LSTM PKU R MSRA P R F F P CTB6 R F 93.5 92.2 92.8 94.2 93.7 93.9 93.9* 93.4* 93.7* 94.4 93.6 94.0 95.2 94.6 94.9 94.2* 93.7* 94.0* 96.3 95.6 96.0 96.7 96.5 96.6 95.9 95.5 95.7 96.3 95.9 96.1 97.1 97.1 97.1 95.6 95.3 95.5 - 95.2 - 97.2 96.6 96.4 96.5 97.5 97.3 97.4 96.2 95.8 96.0 Table 5: Performances on three test sets with pre-trained and bigram character embeddings. The results with * symbol are from our implementations of their methods. Models (Tseng et al., 2005) (Zhang and Clark, 2007) (Sun and Xu, 2011) (Zhang et al., 2013) This work PKU MSRA CTB6 95.0 96.4 95.1 97.2 95.7 96.1 97.4 96.5 97.4 96.0 a lot. On PKU dataset, it takes about 3 days to train the model (last row of Table 5) using CPU (Intel(R) Xeon(R) CPU E5-2665 @ 2.40GHz) only. All implementation is based on Python. 6 Related Work Table 6: Comparison of our model with state-ofthe-art methods on three test sets. with previous neural models with pre-trained embedding and bigram embeddings. Table 6 lists the performances of our model as well as previous state-of-the-art systems. (Zhang and Clark, 2007) is a word-base"
D15-1141,D13-1031,0,0.061144,"Missing"
D15-1215,C14-1076,0,0.0133052,"te the dependency relations between the nodes; (c) is DAGGRNN for the nodes without given topological structure. Introduction Transition-based dependency parsing is a core task in natural language processing, which has been studied with considerable efforts in the NLP community. The traditional discriminative dependency parsing methods have achieved great success (Koo et al., 2008; He et al., 2013; Bohnet, 2010; Huang and Sagae, 2010; Zhang and Nivre, 2011; Martins et al., 2009; McDonald et al., 2005; Nivre et al., 2006; K¨ubler et al., 2009; Goldberg and Nivre, 2013; Choi and McCallum, 2013; Ballesteros and Bohnet, 2014). However, these methods are based on discrete features and suffer from the problems of data sparsity and feature engineering (Chen and Manning, 2014). Recently, distributed representations have been widely used in a variety of natural language processing (NLP) tasks (Collobert et al., 2011; Devlin et al., 2014; Socher et al., 2013; Turian et al., 2010; Mikolov et al., 2013b; Bengio et al., 2003). Specific to the transition-based parsing, the neural network based methods have also been increasingly focused on due to their ability to minimize the efforts in feature engineering and the boosted p"
D15-1215,D14-1082,0,0.530697,"rsing is a core task in natural language processing, which has been studied with considerable efforts in the NLP community. The traditional discriminative dependency parsing methods have achieved great success (Koo et al., 2008; He et al., 2013; Bohnet, 2010; Huang and Sagae, 2010; Zhang and Nivre, 2011; Martins et al., 2009; McDonald et al., 2005; Nivre et al., 2006; K¨ubler et al., 2009; Goldberg and Nivre, 2013; Choi and McCallum, 2013; Ballesteros and Bohnet, 2014). However, these methods are based on discrete features and suffer from the problems of data sparsity and feature engineering (Chen and Manning, 2014). Recently, distributed representations have been widely used in a variety of natural language processing (NLP) tasks (Collobert et al., 2011; Devlin et al., 2014; Socher et al., 2013; Turian et al., 2010; Mikolov et al., 2013b; Bengio et al., 2003). Specific to the transition-based parsing, the neural network based methods have also been increasingly focused on due to their ability to minimize the efforts in feature engineering and the boosted performance (Le and Zuidema, 2014; Stenetorp, 2013; Bansal et al., 2014; Chen and Manning, 2014; Zhu et al., 2015). However, most of the existing neura"
D15-1215,C14-1078,0,0.0286333,"Missing"
D15-1215,P15-1168,1,0.745625,"dren nodes into their parent node recursively according to the given tree structures in stack. Although the dependency relations have been built, it is still hard to apply the recursive neural network (as Eq. 3) directly for the uncertain number of children of each node in stack. By averaging operation on children nodes (Socher et al., 2014), the parent node cannot well capture the crucial features from the mixed information of its children nodes. Here, we propose tree structured gated recursive neural network (Tree-GRNN) incorporating the gate mechanism (Cho et al., 2014; Chung et al., 2014; Chen et al., 2015a; Chen et al., 2015b), which can selectively choose the (5) de where word embedding ew p.w ∈ R , pos embedt d e ding ep.t ∈ R and label embedding elp.l ∈ Rde are extracted from embedding matrices Ew , Et and El according to the indices of the corresponding word p.w, pos p.t and label p.l respectively. Specifically, in the case of unlabeled attachment parsing, we ignore the last term elp.l in Eq. 5. Thus, the dimensionality dn of vnp varies. In labeled attachment parsing case, we set a special token NULL to represent label p.l if not available (e.g. p is the node in stack or buffer). By given"
D15-1215,D15-1092,1,0.779126,"dren nodes into their parent node recursively according to the given tree structures in stack. Although the dependency relations have been built, it is still hard to apply the recursive neural network (as Eq. 3) directly for the uncertain number of children of each node in stack. By averaging operation on children nodes (Socher et al., 2014), the parent node cannot well capture the crucial features from the mixed information of its children nodes. Here, we propose tree structured gated recursive neural network (Tree-GRNN) incorporating the gate mechanism (Cho et al., 2014; Chung et al., 2014; Chen et al., 2015a; Chen et al., 2015b), which can selectively choose the (5) de where word embedding ew p.w ∈ R , pos embedt d e ding ep.t ∈ R and label embedding elp.l ∈ Rde are extracted from embedding matrices Ew , Et and El according to the indices of the corresponding word p.w, pos p.t and label p.l respectively. Specifically, in the case of unlabeled attachment parsing, we ignore the last term elp.l in Eq. 5. Thus, the dimensionality dn of vnp varies. In labeled attachment parsing case, we set a special token NULL to represent label p.l if not available (e.g. p is the node in stack or buffer). By given"
D15-1215,P13-1104,0,0.0308317,"Missing"
D15-1215,P14-1129,0,0.038985,"parsing methods have achieved great success (Koo et al., 2008; He et al., 2013; Bohnet, 2010; Huang and Sagae, 2010; Zhang and Nivre, 2011; Martins et al., 2009; McDonald et al., 2005; Nivre et al., 2006; K¨ubler et al., 2009; Goldberg and Nivre, 2013; Choi and McCallum, 2013; Ballesteros and Bohnet, 2014). However, these methods are based on discrete features and suffer from the problems of data sparsity and feature engineering (Chen and Manning, 2014). Recently, distributed representations have been widely used in a variety of natural language processing (NLP) tasks (Collobert et al., 2011; Devlin et al., 2014; Socher et al., 2013; Turian et al., 2010; Mikolov et al., 2013b; Bengio et al., 2003). Specific to the transition-based parsing, the neural network based methods have also been increasingly focused on due to their ability to minimize the efforts in feature engineering and the boosted performance (Le and Zuidema, 2014; Stenetorp, 2013; Bansal et al., 2014; Chen and Manning, 2014; Zhu et al., 2015). However, most of the existing neural network based methods still need some efforts in feature engineering. For example, most methods often select the first and second leftmost/rightmost children of"
D15-1215,P15-1033,0,0.0208946,"ng all words, POS tags and arc labels as dense vectors, and modeled their interactions with neural network to make predictions of actions. Their method only relies on dense features, and is not able to automatically learn the most useful feature conjunctions to predict the transition action. Compared with (Chen and Manning, 2014), our method can fully exploit the information of all the descendants of a node in stack with Tree-GRNN. Then DAG-GRNN automatically learns the complicated combination of all the features, while the traditional discrete feature based methods need manually design them. Dyer et al. (2015) improved the transition-based dependency parsing using stack long short term memory neural network and received significant improvement on performance. They focused on exploiting the long distance dependencies and information, while we aims to automatically model the complicated feature combination. 8 Conclusion In this paper, we pay attention to the syntactic and semantic composition of the dense features for transition-based dependency parsing. We propose two heterogeneous gated recursive neural networks, Tree-GRNN and DAG-GRNN. Each hidden neuron in two proposed GRNNs can be regarded as a"
D15-1215,Q13-1033,0,0.0168949,"or dependency tree, in which the dashed arcs indicate the dependency relations between the nodes; (c) is DAGGRNN for the nodes without given topological structure. Introduction Transition-based dependency parsing is a core task in natural language processing, which has been studied with considerable efforts in the NLP community. The traditional discriminative dependency parsing methods have achieved great success (Koo et al., 2008; He et al., 2013; Bohnet, 2010; Huang and Sagae, 2010; Zhang and Nivre, 2011; Martins et al., 2009; McDonald et al., 2005; Nivre et al., 2006; K¨ubler et al., 2009; Goldberg and Nivre, 2013; Choi and McCallum, 2013; Ballesteros and Bohnet, 2014). However, these methods are based on discrete features and suffer from the problems of data sparsity and feature engineering (Chen and Manning, 2014). Recently, distributed representations have been widely used in a variety of natural language processing (NLP) tasks (Collobert et al., 2011; Devlin et al., 2014; Socher et al., 2013; Turian et al., 2010; Mikolov et al., 2013b; Bengio et al., 2003). Specific to the transition-based parsing, the neural network based methods have also been increasingly focused on due to their ability to minim"
D15-1215,D13-1152,0,0.0540767,"Missing"
D15-1215,P10-1110,0,0.0238413,"DAG-GRNN Figure 1: Sketch of three recursive neural networks (RNN). (a) is the standard RNN for constituent tree; (b) is Tree-GRNN for dependency tree, in which the dashed arcs indicate the dependency relations between the nodes; (c) is DAGGRNN for the nodes without given topological structure. Introduction Transition-based dependency parsing is a core task in natural language processing, which has been studied with considerable efforts in the NLP community. The traditional discriminative dependency parsing methods have achieved great success (Koo et al., 2008; He et al., 2013; Bohnet, 2010; Huang and Sagae, 2010; Zhang and Nivre, 2011; Martins et al., 2009; McDonald et al., 2005; Nivre et al., 2006; K¨ubler et al., 2009; Goldberg and Nivre, 2013; Choi and McCallum, 2013; Ballesteros and Bohnet, 2014). However, these methods are based on discrete features and suffer from the problems of data sparsity and feature engineering (Chen and Manning, 2014). Recently, distributed representations have been widely used in a variety of natural language processing (NLP) tasks (Collobert et al., 2011; Devlin et al., 2014; Socher et al., 2013; Turian et al., 2010; Mikolov et al., 2013b; Bengio et al., 2003). Specifi"
D15-1215,W07-2416,0,0.0821264,"Missing"
D15-1215,P08-1068,0,0.046429,"posed model. 1 (a) Standard RNN (b) Tree-GRNN (c) DAG-GRNN Figure 1: Sketch of three recursive neural networks (RNN). (a) is the standard RNN for constituent tree; (b) is Tree-GRNN for dependency tree, in which the dashed arcs indicate the dependency relations between the nodes; (c) is DAGGRNN for the nodes without given topological structure. Introduction Transition-based dependency parsing is a core task in natural language processing, which has been studied with considerable efforts in the NLP community. The traditional discriminative dependency parsing methods have achieved great success (Koo et al., 2008; He et al., 2013; Bohnet, 2010; Huang and Sagae, 2010; Zhang and Nivre, 2011; Martins et al., 2009; McDonald et al., 2005; Nivre et al., 2006; K¨ubler et al., 2009; Goldberg and Nivre, 2013; Choi and McCallum, 2013; Ballesteros and Bohnet, 2014). However, these methods are based on discrete features and suffer from the problems of data sparsity and feature engineering (Chen and Manning, 2014). Recently, distributed representations have been widely used in a variety of natural language processing (NLP) tasks (Collobert et al., 2011; Devlin et al., 2014; Socher et al., 2013; Turian et al., 2010"
D15-1215,D14-1081,0,0.272393,"thods are based on discrete features and suffer from the problems of data sparsity and feature engineering (Chen and Manning, 2014). Recently, distributed representations have been widely used in a variety of natural language processing (NLP) tasks (Collobert et al., 2011; Devlin et al., 2014; Socher et al., 2013; Turian et al., 2010; Mikolov et al., 2013b; Bengio et al., 2003). Specific to the transition-based parsing, the neural network based methods have also been increasingly focused on due to their ability to minimize the efforts in feature engineering and the boosted performance (Le and Zuidema, 2014; Stenetorp, 2013; Bansal et al., 2014; Chen and Manning, 2014; Zhu et al., 2015). However, most of the existing neural network based methods still need some efforts in feature engineering. For example, most methods often select the first and second leftmost/rightmost children of the top nodes in stack, which could miss some valuable information hidden in the unchosen nodes. Besides, the features of the selected nodes are just simply concatenated and then fed into neural network. Since the concatenation operation is relatively simple, it is difficult to model the com1879 Proceedings of the 201"
D15-1215,P09-1039,0,0.0310972,"neural networks (RNN). (a) is the standard RNN for constituent tree; (b) is Tree-GRNN for dependency tree, in which the dashed arcs indicate the dependency relations between the nodes; (c) is DAGGRNN for the nodes without given topological structure. Introduction Transition-based dependency parsing is a core task in natural language processing, which has been studied with considerable efforts in the NLP community. The traditional discriminative dependency parsing methods have achieved great success (Koo et al., 2008; He et al., 2013; Bohnet, 2010; Huang and Sagae, 2010; Zhang and Nivre, 2011; Martins et al., 2009; McDonald et al., 2005; Nivre et al., 2006; K¨ubler et al., 2009; Goldberg and Nivre, 2013; Choi and McCallum, 2013; Ballesteros and Bohnet, 2014). However, these methods are based on discrete features and suffer from the problems of data sparsity and feature engineering (Chen and Manning, 2014). Recently, distributed representations have been widely used in a variety of natural language processing (NLP) tasks (Collobert et al., 2011; Devlin et al., 2014; Socher et al., 2013; Turian et al., 2010; Mikolov et al., 2013b; Bengio et al., 2003). Specific to the transition-based parsing, the neural"
D15-1215,E06-1011,0,0.11574,"Missing"
D15-1215,P05-1012,0,0.0883012,"(a) is the standard RNN for constituent tree; (b) is Tree-GRNN for dependency tree, in which the dashed arcs indicate the dependency relations between the nodes; (c) is DAGGRNN for the nodes without given topological structure. Introduction Transition-based dependency parsing is a core task in natural language processing, which has been studied with considerable efforts in the NLP community. The traditional discriminative dependency parsing methods have achieved great success (Koo et al., 2008; He et al., 2013; Bohnet, 2010; Huang and Sagae, 2010; Zhang and Nivre, 2011; Martins et al., 2009; McDonald et al., 2005; Nivre et al., 2006; K¨ubler et al., 2009; Goldberg and Nivre, 2013; Choi and McCallum, 2013; Ballesteros and Bohnet, 2014). However, these methods are based on discrete features and suffer from the problems of data sparsity and feature engineering (Chen and Manning, 2014). Recently, distributed representations have been widely used in a variety of natural language processing (NLP) tasks (Collobert et al., 2011; Devlin et al., 2014; Socher et al., 2013; Turian et al., 2010; Mikolov et al., 2013b; Bengio et al., 2003). Specific to the transition-based parsing, the neural network based methods"
D15-1215,P81-1022,0,0.57741,"Missing"
D15-1215,W04-0308,0,0.0348139,"Missing"
D15-1215,P13-1045,0,0.633824,"achieved great success (Koo et al., 2008; He et al., 2013; Bohnet, 2010; Huang and Sagae, 2010; Zhang and Nivre, 2011; Martins et al., 2009; McDonald et al., 2005; Nivre et al., 2006; K¨ubler et al., 2009; Goldberg and Nivre, 2013; Choi and McCallum, 2013; Ballesteros and Bohnet, 2014). However, these methods are based on discrete features and suffer from the problems of data sparsity and feature engineering (Chen and Manning, 2014). Recently, distributed representations have been widely used in a variety of natural language processing (NLP) tasks (Collobert et al., 2011; Devlin et al., 2014; Socher et al., 2013; Turian et al., 2010; Mikolov et al., 2013b; Bengio et al., 2003). Specific to the transition-based parsing, the neural network based methods have also been increasingly focused on due to their ability to minimize the efforts in feature engineering and the boosted performance (Le and Zuidema, 2014; Stenetorp, 2013; Bansal et al., 2014; Chen and Manning, 2014; Zhu et al., 2015). However, most of the existing neural network based methods still need some efforts in feature engineering. For example, most methods often select the first and second leftmost/rightmost children of the top nodes in sta"
D15-1215,Q14-1017,0,0.242651,"Association for Computational Linguistics. plicated feature combinations which can be manually designed in the traditional discrete feature based methods. To tackle these problems, we use two heterogeneous gated recursive neural networks, tree structured gated recursive neural network (TreeGRNN) and directed acyclic graph gated structured recursive neural network (DAG-GRNN), to model each configuration during transition based dependency parsing. The two proposed GRNNs introduce the gate mechanism (Chung et al., 2014) to improve the standard recursive neural network (RNN) (Socher et al., 2013; Socher et al., 2014), and can model the syntactic and semantic compositions of the nodes during parsing. Figure 1 gives a rough sketch for the standard RNN, Tree-GRNN and DAG-GRNN. Tree-GRNN is applied to the partial-constructed trees in stack, which have already been constructed according to the previous transition actions. DAG-GRNN is applied to model the feature composition of nodes in stack and buffer which have not been labeled their dependency relations yet. Intuitively, Tree-GRNN selects and merges features recursively from children nodes into their parent according to their dependency structures, while DA"
D15-1215,P10-1040,0,0.0247979,"s (Koo et al., 2008; He et al., 2013; Bohnet, 2010; Huang and Sagae, 2010; Zhang and Nivre, 2011; Martins et al., 2009; McDonald et al., 2005; Nivre et al., 2006; K¨ubler et al., 2009; Goldberg and Nivre, 2013; Choi and McCallum, 2013; Ballesteros and Bohnet, 2014). However, these methods are based on discrete features and suffer from the problems of data sparsity and feature engineering (Chen and Manning, 2014). Recently, distributed representations have been widely used in a variety of natural language processing (NLP) tasks (Collobert et al., 2011; Devlin et al., 2014; Socher et al., 2013; Turian et al., 2010; Mikolov et al., 2013b; Bengio et al., 2003). Specific to the transition-based parsing, the neural network based methods have also been increasingly focused on due to their ability to minimize the efforts in feature engineering and the boosted performance (Le and Zuidema, 2014; Stenetorp, 2013; Bansal et al., 2014; Chen and Manning, 2014; Zhu et al., 2015). However, most of the existing neural network based methods still need some efforts in feature engineering. For example, most methods often select the first and second leftmost/rightmost children of the top nodes in stack, which could miss"
D15-1215,D08-1059,0,0.0771397,"Missing"
D15-1215,P11-2033,0,0.022786,"tch of three recursive neural networks (RNN). (a) is the standard RNN for constituent tree; (b) is Tree-GRNN for dependency tree, in which the dashed arcs indicate the dependency relations between the nodes; (c) is DAGGRNN for the nodes without given topological structure. Introduction Transition-based dependency parsing is a core task in natural language processing, which has been studied with considerable efforts in the NLP community. The traditional discriminative dependency parsing methods have achieved great success (Koo et al., 2008; He et al., 2013; Bohnet, 2010; Huang and Sagae, 2010; Zhang and Nivre, 2011; Martins et al., 2009; McDonald et al., 2005; Nivre et al., 2006; K¨ubler et al., 2009; Goldberg and Nivre, 2013; Choi and McCallum, 2013; Ballesteros and Bohnet, 2014). However, these methods are based on discrete features and suffer from the problems of data sparsity and feature engineering (Chen and Manning, 2014). Recently, distributed representations have been widely used in a variety of natural language processing (NLP) tasks (Collobert et al., 2011; Devlin et al., 2014; Socher et al., 2013; Turian et al., 2010; Mikolov et al., 2013b; Bengio et al., 2003). Specific to the transition-bas"
D15-1215,P15-1112,1,0.903192,"rsity and feature engineering (Chen and Manning, 2014). Recently, distributed representations have been widely used in a variety of natural language processing (NLP) tasks (Collobert et al., 2011; Devlin et al., 2014; Socher et al., 2013; Turian et al., 2010; Mikolov et al., 2013b; Bengio et al., 2003). Specific to the transition-based parsing, the neural network based methods have also been increasingly focused on due to their ability to minimize the efforts in feature engineering and the boosted performance (Le and Zuidema, 2014; Stenetorp, 2013; Bansal et al., 2014; Chen and Manning, 2014; Zhu et al., 2015). However, most of the existing neural network based methods still need some efforts in feature engineering. For example, most methods often select the first and second leftmost/rightmost children of the top nodes in stack, which could miss some valuable information hidden in the unchosen nodes. Besides, the features of the selected nodes are just simply concatenated and then fed into neural network. Since the concatenation operation is relatively simple, it is difficult to model the com1879 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1879–1889"
D15-1215,nivre-etal-2006-maltparser,0,\N,Missing
D15-1215,D14-1179,0,\N,Missing
D15-1215,P14-2131,0,\N,Missing
D15-1280,D15-1141,1,0.0532292,"Missing"
D15-1280,P14-1062,0,0.475887,"s on four benchmark datasets show that our model outperforms the other neural models in text classification task. 1 Introduction Distributed representations of words have been widely used in many natural language processing (NLP) tasks (Collobert et al., 2011; Turian et al., 2010; Mikolov et al., 2013b; Bengio et al., 2003). Following this success, it is rising a substantial interest to learn the distributed representations of the continuous words, such as phrases, sentences, paragraphs and documents (Mitchell and Lapata, 2010; Socher et al., 2013; Mikolov et al., 2013b; Le and Mikolov, 2014; Kalchbrenner et al., 2014). The primary role of these models is to represent the variable-length sentence or document as a fixed-length vector. A good representation of the variable-length text should fully capture the semantics of natural language. Recently, the long short-term memory neural network (LSTM) (Hochreiter and Schmidhuber, 1997) has been applied successfully in many NLP tasks, such as spoken language understanding (Yao et al., 2014), sequence labeling (Chen et al., ∗ 2015) and machine translation (Sutskever et al., 2014). LSTM is an extension of the recurrent neural network (RNN) (Elman, 1990), which can c"
D15-1280,D14-1181,0,0.0823907,"Missing"
D15-1280,C02-1150,0,0.0317821,"level, and the last dataset is document-level. The detailed statistics about the four datasets are listed in Table 1. Each dataset is briefly described as follows. • SST-1 The movie reviews with five classes (negative, somewhat negative, neutral, somewhat positive, positive) in the Stanford Sentiment Treebank1 (Socher et al., 2013). • SST-2 The movie reviews with binary classes. It is also from the Stanford Sentiment Treebank. • QC The TREC questions dataset2 involves six different question types, e.g. whether the question is about a location, about a person or about some numeric information (Li and Roth, 2002). • IMDB The IMDB dataset3 consists of 100,000 movie reviews with binary classes (Maas et al., 2011). One key aspect of this dataset is that each movie review has several sentences. (16) where L is the average length of the corpus. Thus, the slowest group is activated at least twice. 5 Training In each of the experiments, the hidden layer at the last moment has a fully connected layer followed by a softmax non-linear layer that predicts the probability distribution over classes given the input sentence. The network is trained to minimise the cross-entropy of the predicted and true distribution"
D15-1280,P11-1015,0,0.0556068,"isted in Table 1. Each dataset is briefly described as follows. • SST-1 The movie reviews with five classes (negative, somewhat negative, neutral, somewhat positive, positive) in the Stanford Sentiment Treebank1 (Socher et al., 2013). • SST-2 The movie reviews with binary classes. It is also from the Stanford Sentiment Treebank. • QC The TREC questions dataset2 involves six different question types, e.g. whether the question is about a location, about a person or about some numeric information (Li and Roth, 2002). • IMDB The IMDB dataset3 consists of 100,000 movie reviews with binary classes (Maas et al., 2011). One key aspect of this dataset is that each movie review has several sentences. (16) where L is the average length of the corpus. Thus, the slowest group is activated at least twice. 5 Training In each of the experiments, the hidden layer at the last moment has a fully connected layer followed by a softmax non-linear layer that predicts the probability distribution over classes given the input sentence. The network is trained to minimise the cross-entropy of the predicted and true distributions; the objective includes an L2 regularization term over the parameters. The network is trained with"
D15-1280,D11-1014,0,0.43637,"ence models are sensitive to word order, but they have a bias towards the latest input words. This gives the RNN excellent performance at language modelling, but it is suboptimal for modeling the whole sentence, especially for the long texts. Le and Mikolov (2014) proposed a Paragraph Vector (PV) to learn continuous distributed vector representations for pieces of texts, which can be regarded as a long-term memory of sentences as opposed to the short-memory in RNN. Topological models Topological models compose the sentence representation following a given topological structure over the words (Socher et al., 2011a; Socher et al., 2012; Socher et al., 2013). Recursive neural network (RecNN) adopts a more general structure to encode sentence (Pollack, 1990; Socher et al., 2013). At every node in the tree the contexts at the left and right children of the node are combined by a classical layer. The weights of the layer are shared across all nodes in the tree. The layer computed at the top node gives a representation for the sentence. However, RecNN depends on external constituency parse trees provided by an external topological structure, such as parse tree. Convolutional models Convolutional neural netw"
D15-1280,D12-1110,0,0.868221,"ive to word order, but they have a bias towards the latest input words. This gives the RNN excellent performance at language modelling, but it is suboptimal for modeling the whole sentence, especially for the long texts. Le and Mikolov (2014) proposed a Paragraph Vector (PV) to learn continuous distributed vector representations for pieces of texts, which can be regarded as a long-term memory of sentences as opposed to the short-memory in RNN. Topological models Topological models compose the sentence representation following a given topological structure over the words (Socher et al., 2011a; Socher et al., 2012; Socher et al., 2013). Recursive neural network (RecNN) adopts a more general structure to encode sentence (Pollack, 1990; Socher et al., 2013). At every node in the tree the contexts at the left and right children of the node are combined by a classical layer. The weights of the layer are shared across all nodes in the tree. The layer computed at the top node gives a representation for the sentence. However, RecNN depends on external constituency parse trees provided by an external topological structure, such as parse tree. Convolutional models Convolutional neural network (CNN) is also used"
D15-1280,D13-1170,0,0.44174,"n model very long documents as well as short sentences. Experiments on four benchmark datasets show that our model outperforms the other neural models in text classification task. 1 Introduction Distributed representations of words have been widely used in many natural language processing (NLP) tasks (Collobert et al., 2011; Turian et al., 2010; Mikolov et al., 2013b; Bengio et al., 2003). Following this success, it is rising a substantial interest to learn the distributed representations of the continuous words, such as phrases, sentences, paragraphs and documents (Mitchell and Lapata, 2010; Socher et al., 2013; Mikolov et al., 2013b; Le and Mikolov, 2014; Kalchbrenner et al., 2014). The primary role of these models is to represent the variable-length sentence or document as a fixed-length vector. A good representation of the variable-length text should fully capture the semantics of natural language. Recently, the long short-term memory neural network (LSTM) (Hochreiter and Schmidhuber, 1997) has been applied successfully in many NLP tasks, such as spoken language understanding (Yao et al., 2014), sequence labeling (Chen et al., ∗ 2015) and machine translation (Sutskever et al., 2014). LSTM is an e"
D15-1280,P10-1040,0,0.0289762,"and documents. In this paper, we propose a multi-timescale long short-term memory (MT-LSTM) neural network to model long texts. MTLSTM partitions the hidden states of the standard LSTM into several groups. Each group is activated at different time periods. Thus, MT-LSTM can model very long documents as well as short sentences. Experiments on four benchmark datasets show that our model outperforms the other neural models in text classification task. 1 Introduction Distributed representations of words have been widely used in many natural language processing (NLP) tasks (Collobert et al., 2011; Turian et al., 2010; Mikolov et al., 2013b; Bengio et al., 2003). Following this success, it is rising a substantial interest to learn the distributed representations of the continuous words, such as phrases, sentences, paragraphs and documents (Mitchell and Lapata, 2010; Socher et al., 2013; Mikolov et al., 2013b; Le and Mikolov, 2014; Kalchbrenner et al., 2014). The primary role of these models is to represent the variable-length sentence or document as a fixed-length vector. A good representation of the variable-length text should fully capture the semantics of natural language. Recently, the long short-term"
D15-1280,P12-2018,0,0.0615111,"d update rule (Duchi et al., 2011). The back propagation of the error propagation is similar to LSTM as well. The only difference is that the error propagates only from groups that were executed at time step t. The error of nonactivated groups gets copied back in time (similarly to copying the activations of nodes not activated at the time step t during the corresponding forward pass), where it is added to the backpropagated error. 6 6.2 Competitor Models We compare our model with the following models: • NB-SVM and MNB. Naive Bayes SVM and Multinomial Naive Bayes with uni and bigram features (Wang and Manning, 2012). • NBOW The NBOW sums the word vectors and applies a non-linearity followed by a softmax classification layer. • RAE Recursive Autoencoders with pretrained word vectors from Wikipedia (Socher et al., 2011b). • MV-RNN Matrix-Vector Recursive Neural Network with parse trees (Socher et al., 2012). Experiments In this section, we investigate the empirical performances of our proposed MT-LSTM model on four benchmark datasets for sentence and document classification and then compare it to other competitor models. 1 http://nlp.stanford.edu/sentiment. http://cogcomp.cs.illinois.edu/Data/ QA/QC/. 3 ht"
D15-1280,D14-1179,0,\N,Missing
D16-1012,P07-1056,0,0.0643455,"Missing"
D16-1012,D14-1082,0,0.0202468,"More specifically, we augment neural model with an external memory, which is shared by several tasks. Experiments on two groups of text classification tasks show that our proposed architectures can improve the performance of a task with the help of other related tasks. 1 Introduction Neural network based models have been shown to achieved impressive results on various NLP tasks rivaling or in some cases surpassing traditional models, such as text classification (Kalchbrenner et al., 2014; Socher et al., 2013; Liu et al., 2015a), semantic matching (Hu et al., 2014; Liu et al., 2016a), parser (Chen and Manning, 2014) and machine translation (Bahdanau et al., 2014). Usually, due to the large number of parameters these neural models need a large-scale corpus. It is hard to train a deep neural model that generalizes well with size-limited data, while building the large scale resources for some NLP tasks is also a challenge. To overcome this problem, these models often involve an unsupervised pre-training phase. The final model is fine-tuned on specific task with respect ∗ Corresponding author. Multi-task learning is an approach to learn multiple related tasks simultaneously to significantly improve performan"
D16-1012,P15-1166,0,0.00687272,"sentation for input words and solved different traditional NLP tasks within one framework. However, only one lookup table is shared, and the other lookup tables and layers are task-specific. Liu et al. (2015b) developed a multi-task DNN for learning representations across multiple tasks. Their multi-task DNN approach combines tasks of query classification and ranking for web search. But the input of the model is bag-of-word representation, which loses the information of word order. More recently, several multi-task encoder125 decoder networks were also proposed for neural machine translation (Dong et al., 2015; Luong et al., 2015; Firat et al., 2016), which can make use of cross-lingual information. Unlike these works, in this paper we design two neural architectures with shared memory for multitask learning, which can store useful information across the tasks. Our architectures are relatively loosely coupled, and therefore more flexible to expand. With the help of shared memory, we can obtain better task-specific sentence representation by utilizing the knowledge obtained by other related tasks. 7 Conclusion and Future Work In this paper, we introduce two deep architectures for multi-task learning"
D16-1012,N16-1101,0,0.0452605,"Missing"
D16-1012,P14-1062,0,0.112434,"insufficient training data. In this paper, we propose two deep architectures which can be trained jointly on multiple related tasks. More specifically, we augment neural model with an external memory, which is shared by several tasks. Experiments on two groups of text classification tasks show that our proposed architectures can improve the performance of a task with the help of other related tasks. 1 Introduction Neural network based models have been shown to achieved impressive results on various NLP tasks rivaling or in some cases surpassing traditional models, such as text classification (Kalchbrenner et al., 2014; Socher et al., 2013; Liu et al., 2015a), semantic matching (Hu et al., 2014; Liu et al., 2016a), parser (Chen and Manning, 2014) and machine translation (Bahdanau et al., 2014). Usually, due to the large number of parameters these neural models need a large-scale corpus. It is hard to train a deep neural model that generalizes well with size-limited data, while building the large scale resources for some NLP tasks is also a challenge. To overcome this problem, these models often involve an unsupervised pre-training phase. The final model is fine-tuned on specific task with respect ∗ Correspo"
D16-1012,D14-1181,0,0.0121801,"Missing"
D16-1012,D15-1280,1,0.855459,"opose two deep architectures which can be trained jointly on multiple related tasks. More specifically, we augment neural model with an external memory, which is shared by several tasks. Experiments on two groups of text classification tasks show that our proposed architectures can improve the performance of a task with the help of other related tasks. 1 Introduction Neural network based models have been shown to achieved impressive results on various NLP tasks rivaling or in some cases surpassing traditional models, such as text classification (Kalchbrenner et al., 2014; Socher et al., 2013; Liu et al., 2015a), semantic matching (Hu et al., 2014; Liu et al., 2016a), parser (Chen and Manning, 2014) and machine translation (Bahdanau et al., 2014). Usually, due to the large number of parameters these neural models need a large-scale corpus. It is hard to train a deep neural model that generalizes well with size-limited data, while building the large scale resources for some NLP tasks is also a challenge. To overcome this problem, these models often involve an unsupervised pre-training phase. The final model is fine-tuned on specific task with respect ∗ Corresponding author. Multi-task learning is an"
D16-1012,N15-1092,0,0.0816419,"opose two deep architectures which can be trained jointly on multiple related tasks. More specifically, we augment neural model with an external memory, which is shared by several tasks. Experiments on two groups of text classification tasks show that our proposed architectures can improve the performance of a task with the help of other related tasks. 1 Introduction Neural network based models have been shown to achieved impressive results on various NLP tasks rivaling or in some cases surpassing traditional models, such as text classification (Kalchbrenner et al., 2014; Socher et al., 2013; Liu et al., 2015a), semantic matching (Hu et al., 2014; Liu et al., 2016a), parser (Chen and Manning, 2014) and machine translation (Bahdanau et al., 2014). Usually, due to the large number of parameters these neural models need a large-scale corpus. It is hard to train a deep neural model that generalizes well with size-limited data, while building the large scale resources for some NLP tasks is also a challenge. To overcome this problem, these models often involve an unsupervised pre-training phase. The final model is fine-tuned on specific task with respect ∗ Corresponding author. Multi-task learning is an"
D16-1012,P16-1098,1,0.518839,"y on multiple related tasks. More specifically, we augment neural model with an external memory, which is shared by several tasks. Experiments on two groups of text classification tasks show that our proposed architectures can improve the performance of a task with the help of other related tasks. 1 Introduction Neural network based models have been shown to achieved impressive results on various NLP tasks rivaling or in some cases surpassing traditional models, such as text classification (Kalchbrenner et al., 2014; Socher et al., 2013; Liu et al., 2015a), semantic matching (Hu et al., 2014; Liu et al., 2016a), parser (Chen and Manning, 2014) and machine translation (Bahdanau et al., 2014). Usually, due to the large number of parameters these neural models need a large-scale corpus. It is hard to train a deep neural model that generalizes well with size-limited data, while building the large scale resources for some NLP tasks is also a challenge. To overcome this problem, these models often involve an unsupervised pre-training phase. The final model is fine-tuned on specific task with respect ∗ Corresponding author. Multi-task learning is an approach to learn multiple related tasks simultaneously"
D16-1012,P11-1015,0,0.00422292,"veral related tasks. 5.1 Datasets The used multi-task datasets are briefly described as follows. The detailed statistics are listed in Table 1. Movie Reviews The movie reviews dataset consists of four sub-datasets about movie reviews. • SST-1 The movie reviews with five classes in the Stanford Sentiment Treebank1 (Socher et al., 2013). • SST-2 The movie reviews with binary classes. It is also from the Stanford Sentiment Treebank. • SUBJ The movie reviews with labels of subjective or objective (Pang and Lee, 2004). • IMDB The IMDB dataset2 consists of 100,000 movie reviews with binary classes (Maas et al., 2011). One key aspect of this dataset is that each movie review has several sentences. 1 http://nlp.stanford.edu/sentiment. http://ai.stanford.edu/˜amaas/data/ sentiment/ 2 Model LSTM Single Task ME-LSTM ARC-I ARC-II Multi-task MT-CNN MT-DNN NBOW RAE (Socher et al., 2011) MV-RNN (Socher et al., 2012) RNTN (Socher et al., 2013) DCNN (Kalchbrenner et al., 2014) CNN-multichannel (Kim, 2014) Tree-LSTM (Tai et al., 2015) SST-1 45.9 46.4 48.6 49.5 46.7 44.5 42.4 43.2 44.4 45.7 48.5 47.4 50.6 SST-2 85.8 85.5 87.0 87.8 86.1 84.0 80.5 82.4 82.9 85.4 86.8 88.1 86.9 SUBJ 91.6 91.0 93.8 95.0 92.2 90.1 91.3 93."
D16-1012,P04-1035,0,0.0123024,"irical performances of our proposed architectures on two multitask datasets. Each dataset contains several related tasks. 5.1 Datasets The used multi-task datasets are briefly described as follows. The detailed statistics are listed in Table 1. Movie Reviews The movie reviews dataset consists of four sub-datasets about movie reviews. • SST-1 The movie reviews with five classes in the Stanford Sentiment Treebank1 (Socher et al., 2013). • SST-2 The movie reviews with binary classes. It is also from the Stanford Sentiment Treebank. • SUBJ The movie reviews with labels of subjective or objective (Pang and Lee, 2004). • IMDB The IMDB dataset2 consists of 100,000 movie reviews with binary classes (Maas et al., 2011). One key aspect of this dataset is that each movie review has several sentences. 1 http://nlp.stanford.edu/sentiment. http://ai.stanford.edu/˜amaas/data/ sentiment/ 2 Model LSTM Single Task ME-LSTM ARC-I ARC-II Multi-task MT-CNN MT-DNN NBOW RAE (Socher et al., 2011) MV-RNN (Socher et al., 2012) RNTN (Socher et al., 2013) DCNN (Kalchbrenner et al., 2014) CNN-multichannel (Kim, 2014) Tree-LSTM (Tai et al., 2015) SST-1 45.9 46.4 48.6 49.5 46.7 44.5 42.4 43.2 44.4 45.7 48.5 47.4 50.6 SST-2 85.8 85."
D16-1012,D14-1162,0,0.111743,"08) with convolutional layer, in which lookup-tables are shared partially while other layers are task-specific. 3 https://www.cs.jhu.edu/˜mdredze/ datasets/sentiment/ 123 • MT-DNN: The model is proposed by Liu et al. (2015b) with bag-of-words input and multilayer perceptrons, in which a hidden layer is shared. 5.3 Hyperparameters and Training The networks are trained with backpropagation and the gradient-based optimization is performed using the Adagrad update rule (Duchi et al., 2011). The word embeddings for all of the models are initialized with the 100d GloVe vectors (840B token version, (Pennington et al., 2014)) and fine-tuned during training to improve the performance. The other parameters are initialized by randomly sampling from uniform distribution in [−0.1, 0.1]. The mini-batch size is set to 16. For each task, we take the hyperparameters which achieve the best performance on the development set via an small grid search over combinations of the initial learning rate [0.1, 0.01], l2 regularization [0.0, 5E−5, 1E−5]. For datasets without development set, we use 10-fold cross-validation (CV) instead. The final hyper-parameters are set as Table 2. 5.4 5.6 Multi-task Learning of Movie Reviews We fir"
D16-1012,D11-1014,0,0.0811253,"Missing"
D16-1012,D12-1110,0,0.0638676,"Missing"
D16-1012,D13-1170,0,0.356173,"In this paper, we propose two deep architectures which can be trained jointly on multiple related tasks. More specifically, we augment neural model with an external memory, which is shared by several tasks. Experiments on two groups of text classification tasks show that our proposed architectures can improve the performance of a task with the help of other related tasks. 1 Introduction Neural network based models have been shown to achieved impressive results on various NLP tasks rivaling or in some cases surpassing traditional models, such as text classification (Kalchbrenner et al., 2014; Socher et al., 2013; Liu et al., 2015a), semantic matching (Hu et al., 2014; Liu et al., 2016a), parser (Chen and Manning, 2014) and machine translation (Bahdanau et al., 2014). Usually, due to the large number of parameters these neural models need a large-scale corpus. It is hard to train a deep neural model that generalizes well with size-limited data, while building the large scale resources for some NLP tasks is also a challenge. To overcome this problem, these models often involve an unsupervised pre-training phase. The final model is fine-tuned on specific task with respect ∗ Corresponding author. Multi-t"
D16-1012,P15-1150,0,0.020304,"Missing"
D16-1012,P10-1040,0,0.0282603,"Missing"
D16-1069,O97-4005,0,0.254164,"he main reason that the dictionary may bring too much conflict. From the results of CRF and RNN, we can see that similar to the Chinese word segmentation task, methods using character dense representations can usually achieve better performance than character based methods. 4 Related Work Although dictionary can be manually constructed, it is a time-consuming work. Moreover, these manually constructed dictionaries are usually updated only occasionally. It would take months before it could be updated. Hence, automatic dictionary construction methods have also been investigated in recent years. Chang and Su (1997) proposed an unsupervised iterative approach for extracting out-of-vocabulary words from Chinese text corpora. Khoo (Khoo et al., 2002) introduced a method based on stepwise logistic regression to identify two-and three-character words in Chinese text. Jin and Wong (2002) incorporated local statistical information, global statistical information and contextual constraints to identify Chinese words. For collecting Thai unknown words, Haruechaiyasak et al. (2006) proposes a collaborative framework for achieving the task based on Web pages over the Internet. Except these unsupervised methods, the"
D16-1069,I13-1037,1,0.897472,"Missing"
D16-1069,P14-1062,0,0.014964,"haracters existing in the training data. We used LIBSVM to implement (Chang and Lin, 2011). Conditional Random Fields (CRFs) were proposed by Lafferty et al. (2001) to model sequence labeling tasks. According to the description given in §2.2, an NLP task can be converted into a sequence labeling problem. Hence, we used CRF to model characters as basic features and several combination templates of them. Compared to SVM, CRF takes both richer features and the labeling sequence into consideration. CRF++ 0.588 was used to do the experiments. Dynamic Convolutional Neural Network (DCNN), defined by Kalchbrenner et al. (2014), is used to model sentence semantics. The proposed method can handle input sequences of varying length, so we adopted their method by using the embeddings of characters as input. The toolkit we used in this work is provided by the authors9 . Recursive Autoencoder (RAE) (Socher et al., 2011), is a machine learning framework for representing variable sized words with a fixed length vector. In this work, we used greedy unsupervised RAE for modeling sequences of Chinese characters. The toolkit was provided by the authors 10 . Then, SVM was used to do the binary classification based on the generat"
D16-1069,P12-1055,0,0.0186041,"s the basic classification unit and are classified one by one. In these methods, dictionaries play important effect in constructing features and avoiding meaningless outputs. Various previous works have demonstrated the significant positive effectiveness of the external dictionary (Zhang et al., 2010). However, because these external dictionaries are usually static and preconstructed, one of the main drawbacks of these methods is that the words which are not included in the dictionaries cannot be well processed. This issue has also been mentioned by numerous previous works (Peng et al., 2004; Liu et al., 2012). Hence, understanding how Chinese words are constructed can benefit a variety of Chinese NLP tasks to avoid meaningless output. For example, to generate the abbreviation for a named entity, we can use a binary classifier to determine whether a character should be removed or retained. Both “国 航” and “中国国航” are appropriate abbreviations for “中 国 国 际 航 空 公 司(Air China)”. However “国 航 司” is not a Chinese word and cannot be understood by humans. 721 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 721–730, c Austin, Texas, November 1-5, 2016. 2016 Assoc"
D16-1069,W13-3512,0,0.0773684,"Missing"
D16-1069,C04-1081,0,0.0848875,"cters are treated as the basic classification unit and are classified one by one. In these methods, dictionaries play important effect in constructing features and avoiding meaningless outputs. Various previous works have demonstrated the significant positive effectiveness of the external dictionary (Zhang et al., 2010). However, because these external dictionaries are usually static and preconstructed, one of the main drawbacks of these methods is that the words which are not included in the dictionaries cannot be well processed. This issue has also been mentioned by numerous previous works (Peng et al., 2004; Liu et al., 2012). Hence, understanding how Chinese words are constructed can benefit a variety of Chinese NLP tasks to avoid meaningless output. For example, to generate the abbreviation for a named entity, we can use a binary classifier to determine whether a character should be removed or retained. Both “国 航” and “中国国航” are appropriate abbreviations for “中 国 国 际 航 空 公 司(Air China)”. However “国 航 司” is not a Chinese word and cannot be understood by humans. 721 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 721–730, c Austin, Texas, November 1-"
D16-1069,P03-1050,0,0.0396788,"r words in Chinese text. Jin and Wong (2002) incorporated local statistical information, global statistical information and contextual constraints to identify Chinese words. For collecting Thai unknown words, Haruechaiyasak et al. (2006) proposes a collaborative framework for achieving the task based on Web pages over the Internet. Except these unsupervised methods, there have been other approaches requiring additional information or selective input. Yarowsky and Wicentowski (2000) proposed to use labeled corpus to train a supervised method for transforming pasttense in English. Rogati et al. (2003) introduced a stemming model based on statistical machine 728 translation for Arabic. They used a parallel corpus to train the model. Luong et al. (2013) studied the problem of word representations for rare and complex words. They proposed to combine recursive neural networks and neural language models to build representations for morphologically complex words from their morphemes. Since English is usually considered limited in terms of morphology, their method can handle unseen words, whose representations could be constructed from vectors of known morphemes. However, most of the existing Chi"
D16-1069,D11-1014,0,0.125641,"Missing"
D16-1069,H05-1054,0,0.0457712,"for Chinese Named Entities Using Recurrent Neural Network with Dynamic Dictionary Qi Zhang, Jin Qian, Ya Guo, Yaqian Zhou, Xuanjing Huang Shanghai Key Laboratory of Data Science School of Computer Science, Fudan University Shanghai, P.R. China {qz, jqian12, yguo13, zhouyaqian, xjhuang}@fudan.edu.cn Abstract word segmenter produces erroneous output, the quality of these methods will be degraded as a direct result. Moreover, since the word segmenter may split the targets into two individual words, many methods adopted character-based methodologies, such as methods for named entity recognition (Wu et al., 2005), aspect-based opinion mining (Xu et al., 2014), and so on. Chinese named entities occur frequently in formal and informal environments. Various approaches have been formalized the problem as a sequence labelling task and utilize a character-based methodology, in which character is treated as the basic classification unit. One of the main drawbacks of these methods is that some of the generated abbreviations may not follow the conventional wisdom of Chinese. To address this problem, we propose a novel neural network architecture to perform task. It combines recurrent neural network (RNN) with"
D16-1069,D09-1159,1,0.849835,"Missing"
D16-1069,P14-1032,0,0.018155,"ral Network with Dynamic Dictionary Qi Zhang, Jin Qian, Ya Guo, Yaqian Zhou, Xuanjing Huang Shanghai Key Laboratory of Data Science School of Computer Science, Fudan University Shanghai, P.R. China {qz, jqian12, yguo13, zhouyaqian, xjhuang}@fudan.edu.cn Abstract word segmenter produces erroneous output, the quality of these methods will be degraded as a direct result. Moreover, since the word segmenter may split the targets into two individual words, many methods adopted character-based methodologies, such as methods for named entity recognition (Wu et al., 2005), aspect-based opinion mining (Xu et al., 2014), and so on. Chinese named entities occur frequently in formal and informal environments. Various approaches have been formalized the problem as a sequence labelling task and utilize a character-based methodology, in which character is treated as the basic classification unit. One of the main drawbacks of these methods is that some of the generated abbreviations may not follow the conventional wisdom of Chinese. To address this problem, we propose a novel neural network architecture to perform task. It combines recurrent neural network (RNN) with an architecture determining whether a given seq"
D16-1069,N09-2069,0,0.45292,"Missing"
D16-1069,P00-1027,0,0.0213643,"from Chinese text corpora. Khoo (Khoo et al., 2002) introduced a method based on stepwise logistic regression to identify two-and three-character words in Chinese text. Jin and Wong (2002) incorporated local statistical information, global statistical information and contextual constraints to identify Chinese words. For collecting Thai unknown words, Haruechaiyasak et al. (2006) proposes a collaborative framework for achieving the task based on Web pages over the Internet. Except these unsupervised methods, there have been other approaches requiring additional information or selective input. Yarowsky and Wicentowski (2000) proposed to use labeled corpus to train a supervised method for transforming pasttense in English. Rogati et al. (2003) introduced a stemming model based on statistical machine 728 translation for Arabic. They used a parallel corpus to train the model. Luong et al. (2013) studied the problem of word representations for rare and complex words. They proposed to combine recursive neural networks and neural language models to build representations for morphologically complex words from their morphemes. Since English is usually considered limited in terms of morphology, their method can handle uns"
D16-1069,C10-2167,0,0.0882137,"Missing"
D16-1080,C10-2042,0,0.036687,"n (Medelyan and Witten, 2006). In the unsupervised line of research, keyphrase extraction is formulated as a ranking problem. A well-known approach is the Term Frequency Inverse Document Frequency (TF-IDF) (Sparck Jones, 1972; Zhang et al., 2007; Lee and Kim, 2008). Measures like term frequencies (Wu and Giles, 2013; Rennie and Jaakkola, 2005; Kireyev, 2009), inverse document frequencies, topic proportions, etc. and knowledge of specific domain are applied to rank terms in documents which are aggregated to score the phrases. The ranking based on tf-idf has been shown to work well in practice (Hasan and Ng, 2010). Mihalcea et al. proposed the TextRank, which constructs keyphrases using the PageRank values obtained on a graph based ranking model for graphs extracted from texts (Mihalcea and Tarau, 2004). Liu et al. proposed to extract keyphrases by adopting a clustering-based approach, which ensures that the document is semantically covered by these keyphrases (Liu et al., 2009). Ali Mehri et al. put 843 forward a method for ranking the words in texts, which can also be used to classify the correlation range between word-type occurrences in a text, by using non-extensive statistical mechanics (Mehri an"
D16-1080,W03-1028,0,0.772505,"Missing"
D16-1080,N09-1060,0,0.0787219,"l. (2004) applied Bayesian decision theory for keyword extraction. Medelyan and Witten extended the KEA to KEA++, which uses semantic information on terms and phrases extracted from a domain specific thesaurus, thus enhances automatic keyphrase extraction (Medelyan and Witten, 2006). In the unsupervised line of research, keyphrase extraction is formulated as a ranking problem. A well-known approach is the Term Frequency Inverse Document Frequency (TF-IDF) (Sparck Jones, 1972; Zhang et al., 2007; Lee and Kim, 2008). Measures like term frequencies (Wu and Giles, 2013; Rennie and Jaakkola, 2005; Kireyev, 2009), inverse document frequencies, topic proportions, etc. and knowledge of specific domain are applied to rank terms in documents which are aggregated to score the phrases. The ranking based on tf-idf has been shown to work well in practice (Hasan and Ng, 2010). Mihalcea et al. proposed the TextRank, which constructs keyphrases using the PageRank values obtained on a graph based ranking model for graphs extracted from texts (Mihalcea and Tarau, 2004). Liu et al. proposed to extract keyphrases by adopting a clustering-based approach, which ensures that the document is semantically covered by thes"
D16-1080,D09-1027,0,0.416574,"ment frequencies, topic proportions, etc. and knowledge of specific domain are applied to rank terms in documents which are aggregated to score the phrases. The ranking based on tf-idf has been shown to work well in practice (Hasan and Ng, 2010). Mihalcea et al. proposed the TextRank, which constructs keyphrases using the PageRank values obtained on a graph based ranking model for graphs extracted from texts (Mihalcea and Tarau, 2004). Liu et al. proposed to extract keyphrases by adopting a clustering-based approach, which ensures that the document is semantically covered by these keyphrases (Liu et al., 2009). Ali Mehri et al. put 843 forward a method for ranking the words in texts, which can also be used to classify the correlation range between word-type occurrences in a text, by using non-extensive statistical mechanics (Mehri and Darooneh, 2011). Recurrent neural networks(RNNs) (Elman, 1990) has been applied to many sequential prediction tasks, which is an important class of naturally deep architecture. In NLP, RNNs deal with a sentence as a sequence of tokens and have been successfully applied to various tasks like spoken language understanding (Mesnil et al., 2013) and language modeling (Mik"
D16-1080,P15-2105,0,0.133585,"Missing"
D16-1080,C08-2021,0,0.0494406,"Missing"
D16-1080,W15-3606,0,0.057334,"Missing"
D16-1080,N13-1026,0,0.0598647,"tify candidate phrases from the text. Tang et al. (2004) applied Bayesian decision theory for keyword extraction. Medelyan and Witten extended the KEA to KEA++, which uses semantic information on terms and phrases extracted from a domain specific thesaurus, thus enhances automatic keyphrase extraction (Medelyan and Witten, 2006). In the unsupervised line of research, keyphrase extraction is formulated as a ranking problem. A well-known approach is the Term Frequency Inverse Document Frequency (TF-IDF) (Sparck Jones, 1972; Zhang et al., 2007; Lee and Kim, 2008). Measures like term frequencies (Wu and Giles, 2013; Rennie and Jaakkola, 2005; Kireyev, 2009), inverse document frequencies, topic proportions, etc. and knowledge of specific domain are applied to rank terms in documents which are aggregated to score the phrases. The ranking based on tf-idf has been shown to work well in practice (Hasan and Ng, 2010). Mihalcea et al. proposed the TextRank, which constructs keyphrases using the PageRank values obtained on a graph based ranking model for graphs extracted from texts (Mihalcea and Tarau, 2004). Liu et al. proposed to extract keyphrases by adopting a clustering-based approach, which ensures that t"
D16-1080,W04-3252,0,\N,Missing
D16-1080,P11-1039,0,\N,Missing
D16-1172,D15-1263,0,0.0184525,"ious methods have been investigated and explored over years (Wilson et al., 2005; Pang and Lee, 2008; Pak and Paroubek, 2010; Yessenalina et al., 2010; Moraes et al., 2013). Most of these methods depend on traditional machine learning algorithms, and are in need of effective handcrafted features. Recently, neural network based methods are prevalent due to their ability of learning discriminative features from data (Socher et al., 2013; Le and Mikolov, 2014; Tang et al., 2015a). Zhu et al. (2015) and Tai et al. (2015) integrate a tree-structured model into LSTM for better semantic composition; Bhatia et al. (2015) enhances document-level sentiment analysis by using extra discourse paring results. Most of these models work well on sentence-level or paragraph-level sentiment classification. When it comes to the document-level sentiment classification, a bottom-up hierarchical strategy is often adopted to alleviate the model complexity (Denil et al., 2014; Tang et al., 2015b; Li et al., 2015). 2.2 Memory Augmented Recurrent Models Although it is widely accepted that LSTM has more long-lasting memory units than RNNs, it still suffers from “forgetting” information which is too far away from the current poin"
D16-1172,D15-1092,1,0.396018,"evel sentiment analysis demands more on selecting and storing global sentiment message from long texts with noises and redundant local pattern. Simple RNNs are not powerful enough to handle the overflow and to pick up key sentiment messages from relatively far time-steps . Introduction Sentiment classification is one of the most widely used natural language processing techniques in many areas, such as E-commerce websites, online social networks, political orientation analyses (Wilson et al., 2009; O’Connor et al., 2010), etc. Recently, deep learning approaches (Socher et al., 2013; Kim, 2014; Chen et al., 2015; Liu et al., 2016) have gained encouraging results on sentiment classification, which frees researchers from handcrafted feature engineering. Among these methods, Recurrent Neural Networks (RNNs) are one of the most ∗ Corresponding author. Efforts have been made to solve such a scalability problem on long texts by extracting semantic information hierarchically (Tang et al., 2015a; Tai et al., 2015), which first obtain sentence representations and then combine them to generate high-level document embeddings. However, some of these solutions either rely on explicit a priori structural assumptio"
D16-1172,D14-1181,0,0.0272154,"document-level sentiment analysis demands more on selecting and storing global sentiment message from long texts with noises and redundant local pattern. Simple RNNs are not powerful enough to handle the overflow and to pick up key sentiment messages from relatively far time-steps . Introduction Sentiment classification is one of the most widely used natural language processing techniques in many areas, such as E-commerce websites, online social networks, political orientation analyses (Wilson et al., 2009; O’Connor et al., 2010), etc. Recently, deep learning approaches (Socher et al., 2013; Kim, 2014; Chen et al., 2015; Liu et al., 2016) have gained encouraging results on sentiment classification, which frees researchers from handcrafted feature engineering. Among these methods, Recurrent Neural Networks (RNNs) are one of the most ∗ Corresponding author. Efforts have been made to solve such a scalability problem on long texts by extracting semantic information hierarchically (Tang et al., 2015a; Tai et al., 2015), which first obtain sentence representations and then combine them to generate high-level document embeddings. However, some of these solutions either rely on explicit a priori s"
D16-1172,D15-1278,0,0.0147628,"criminative features from data (Socher et al., 2013; Le and Mikolov, 2014; Tang et al., 2015a). Zhu et al. (2015) and Tai et al. (2015) integrate a tree-structured model into LSTM for better semantic composition; Bhatia et al. (2015) enhances document-level sentiment analysis by using extra discourse paring results. Most of these models work well on sentence-level or paragraph-level sentiment classification. When it comes to the document-level sentiment classification, a bottom-up hierarchical strategy is often adopted to alleviate the model complexity (Denil et al., 2014; Tang et al., 2015b; Li et al., 2015). 2.2 Memory Augmented Recurrent Models Although it is widely accepted that LSTM has more long-lasting memory units than RNNs, it still suffers from “forgetting” information which is too far away from the current point (Le et al., 2015; Karpathy et al., 2015). Such a scalability problem of LSTMs is crucial to extend some previous sentence-level work to document-level sentiment analysis. Various models have been proposed to increase the ability of LSTMs to store long-range information (Le et al., 2015; Salehinejad, 2016) and two kinds of approaches gain attraction. One is to augment LSTM with a"
D16-1172,D15-1280,1,0.859919,"is. Various models have been proposed to increase the ability of LSTMs to store long-range information (Le et al., 2015; Salehinejad, 2016) and two kinds of approaches gain attraction. One is to augment LSTM with an external memory (Sukhbaatar et al., 2015; Monz, 2016), but they are of poor performance on time because of the huge external memory matrix. Unlike these methods, we fully exploit the potential of internal memory of LSTM by adjusting its forgetting rates. The other one tries to use multiple time-scales to distinguish different states (El Hihi and Bengio, 1995; Koutnik et al., 2014; Liu et al., 2015). They partition the hidden states into several groups and each group is activated and updated at different frequencies (e.g. one group updates every 2 time-step, the other updates every 4 time-step). In these methods, different memory groups are not fully interconnected, and the information is transmitted from faster groups to slower ones, or vice versa. However, the memory of slower groups are not updated at every step, which may lead to sentiment information loss and semantic inconsistency. In our proposed CLSTM, we assign different forgetting rates to memory groups. This novel strategy ena"
D16-1172,P07-1055,0,0.00836746,". Among these methods, Recurrent Neural Networks (RNNs) are one of the most ∗ Corresponding author. Efforts have been made to solve such a scalability problem on long texts by extracting semantic information hierarchically (Tang et al., 2015a; Tai et al., 2015), which first obtain sentence representations and then combine them to generate high-level document embeddings. However, some of these solutions either rely on explicit a priori structural assumptions or discard the order information within a sentence, which are vulnerable to sudden change or twists in texts especially a long-range one (McDonald et al., 2007; Mikolov et al., 2013). Recurrent models match people’s intuition of reading word by word and are capable to model the intrinsic relations between sentences. By keeping the word order, RNNs could extract the sentence representation implicitly and meanwhile analyze the semantic meaning of a whole document without any explicit boundary. Partially inspired by neural structure of human brain and computer system architecture, we present the Cached Long Short-Term Memory neural networks (CLSTM) to capture the long-range sentiment information. In the dual store memory model 1660 Proceedings of the 2"
D16-1172,pak-paroubek-2010-twitter,0,0.0214553,"me variants of LSTM which address the problem on storing the long-term information. 2.1 Document-level Sentiment Classification Document-level sentiment classification is a sticky task in sentiment analysis (Pang and Lee, 2008), which is to infer the sentiment polarity or intensity of a whole document. The most challenging part is that not every part of the document is equally informative for inferring the sentiment of the whole 1661 document (Pang and Lee, 2004; Yessenalina et al., 2010). Various methods have been investigated and explored over years (Wilson et al., 2005; Pang and Lee, 2008; Pak and Paroubek, 2010; Yessenalina et al., 2010; Moraes et al., 2013). Most of these methods depend on traditional machine learning algorithms, and are in need of effective handcrafted features. Recently, neural network based methods are prevalent due to their ability of learning discriminative features from data (Socher et al., 2013; Le and Mikolov, 2014; Tang et al., 2015a). Zhu et al. (2015) and Tai et al. (2015) integrate a tree-structured model into LSTM for better semantic composition; Bhatia et al. (2015) enhances document-level sentiment analysis by using extra discourse paring results. Most of these model"
D16-1172,P04-1035,0,0.0396153,"we briefly introduce related work in two areas: First, we discuss the existing documentlevel sentiment classification approaches; Second, we discuss some variants of LSTM which address the problem on storing the long-term information. 2.1 Document-level Sentiment Classification Document-level sentiment classification is a sticky task in sentiment analysis (Pang and Lee, 2008), which is to infer the sentiment polarity or intensity of a whole document. The most challenging part is that not every part of the document is equally informative for inferring the sentiment of the whole 1661 document (Pang and Lee, 2004; Yessenalina et al., 2010). Various methods have been investigated and explored over years (Wilson et al., 2005; Pang and Lee, 2008; Pak and Paroubek, 2010; Yessenalina et al., 2010; Moraes et al., 2013). Most of these methods depend on traditional machine learning algorithms, and are in need of effective handcrafted features. Recently, neural network based methods are prevalent due to their ability of learning discriminative features from data (Socher et al., 2013; Le and Mikolov, 2014; Tang et al., 2015a). Zhu et al. (2015) and Tai et al. (2015) integrate a tree-structured model into LSTM f"
D16-1172,D14-1162,0,0.112883,"Missing"
D16-1172,D13-1170,0,0.103268,"of information, while document-level sentiment analysis demands more on selecting and storing global sentiment message from long texts with noises and redundant local pattern. Simple RNNs are not powerful enough to handle the overflow and to pick up key sentiment messages from relatively far time-steps . Introduction Sentiment classification is one of the most widely used natural language processing techniques in many areas, such as E-commerce websites, online social networks, political orientation analyses (Wilson et al., 2009; O’Connor et al., 2010), etc. Recently, deep learning approaches (Socher et al., 2013; Kim, 2014; Chen et al., 2015; Liu et al., 2016) have gained encouraging results on sentiment classification, which frees researchers from handcrafted feature engineering. Among these methods, Recurrent Neural Networks (RNNs) are one of the most ∗ Corresponding author. Efforts have been made to solve such a scalability problem on long texts by extracting semantic information hierarchically (Tang et al., 2015a; Tai et al., 2015), which first obtain sentence representations and then combine them to generate high-level document embeddings. However, some of these solutions either rely on explicit"
D16-1172,P15-1150,0,0.244219,"as E-commerce websites, online social networks, political orientation analyses (Wilson et al., 2009; O’Connor et al., 2010), etc. Recently, deep learning approaches (Socher et al., 2013; Kim, 2014; Chen et al., 2015; Liu et al., 2016) have gained encouraging results on sentiment classification, which frees researchers from handcrafted feature engineering. Among these methods, Recurrent Neural Networks (RNNs) are one of the most ∗ Corresponding author. Efforts have been made to solve such a scalability problem on long texts by extracting semantic information hierarchically (Tang et al., 2015a; Tai et al., 2015), which first obtain sentence representations and then combine them to generate high-level document embeddings. However, some of these solutions either rely on explicit a priori structural assumptions or discard the order information within a sentence, which are vulnerable to sudden change or twists in texts especially a long-range one (McDonald et al., 2007; Mikolov et al., 2013). Recurrent models match people’s intuition of reading word by word and are capable to model the intrinsic relations between sentences. By keeping the word order, RNNs could extract the sentence representation implici"
D16-1172,H05-1044,0,0.0364848,"ication approaches; Second, we discuss some variants of LSTM which address the problem on storing the long-term information. 2.1 Document-level Sentiment Classification Document-level sentiment classification is a sticky task in sentiment analysis (Pang and Lee, 2008), which is to infer the sentiment polarity or intensity of a whole document. The most challenging part is that not every part of the document is equally informative for inferring the sentiment of the whole 1661 document (Pang and Lee, 2004; Yessenalina et al., 2010). Various methods have been investigated and explored over years (Wilson et al., 2005; Pang and Lee, 2008; Pak and Paroubek, 2010; Yessenalina et al., 2010; Moraes et al., 2013). Most of these methods depend on traditional machine learning algorithms, and are in need of effective handcrafted features. Recently, neural network based methods are prevalent due to their ability of learning discriminative features from data (Socher et al., 2013; Le and Mikolov, 2014; Tang et al., 2015a). Zhu et al. (2015) and Tai et al. (2015) integrate a tree-structured model into LSTM for better semantic composition; Bhatia et al. (2015) enhances document-level sentiment analysis by using extra d"
D16-1172,J09-3003,0,0.0213687,"ph-level sentiment analysis expects the model to extract features from limited source of information, while document-level sentiment analysis demands more on selecting and storing global sentiment message from long texts with noises and redundant local pattern. Simple RNNs are not powerful enough to handle the overflow and to pick up key sentiment messages from relatively far time-steps . Introduction Sentiment classification is one of the most widely used natural language processing techniques in many areas, such as E-commerce websites, online social networks, political orientation analyses (Wilson et al., 2009; O’Connor et al., 2010), etc. Recently, deep learning approaches (Socher et al., 2013; Kim, 2014; Chen et al., 2015; Liu et al., 2016) have gained encouraging results on sentiment classification, which frees researchers from handcrafted feature engineering. Among these methods, Recurrent Neural Networks (RNNs) are one of the most ∗ Corresponding author. Efforts have been made to solve such a scalability problem on long texts by extracting semantic information hierarchically (Tang et al., 2015a; Tai et al., 2015), which first obtain sentence representations and then combine them to generate hi"
D16-1172,D10-1102,0,0.0150866,"Missing"
D16-1176,D15-1075,0,0.0679935,"rately, then they are concatenated and fed to a MLP. • Attention LSTMs: An attentive LSTM to encode two sentences into a semantic space, which used in (Hermann et al., 2015; Rockt¨aschel et al., 2015). • Word-by-word Attention LSTMs: An improvement of attention LSTM by introducing wordby-word attention mechanism, which used in (Hermann et al., 2015; Rockt¨aschel et al., 2015). 6.3 Experiment-I: Recognizing Textual Entailment Recognizing textual entailment (RTE) is a task to determine the semantic relationship between two sentences. We use the Stanford Natural Language Inference Corpus (SNLI) (Bowman et al., 2015). This corpus contains 570K sentence pairs, and all of the sentences and labels stem from human annotators. SNLI is two orders of magnitude larger than all other existing RTE corpora. Therefore, the massive scale of SNLI allows us to train powerful neural networks such as our proposed architecture in this paper. 6.3.1 Results Table 2 shows the evaluation results on SNLI. The 3rd column of the table gives the number of parameters of different models without the word embeddings. Our proposed two C-LSTMs models with four stacked blocks outperform all the competitor models, which indicates that ou"
D16-1176,D15-1181,0,0.0294076,"teractions, such as ARCI(Hu et al., 2014), CNTN(Qiu and Huang, 2015) and so on. These models first encode two sequences with some basic (Neural Bag-of-words, BOW) or advanced (RNN, CNN) components of neural networks separately, and then compute the matching score based on the distributed vectors of two sentences. In this paradigm, two sentences have no interaction until arriving final phase. Semi-interaction Models Some improved methods focus on utilizing multi-granularity representation (word, phrase and sentence level), such as MultiGranCNN (Yin and Sch¨utze, 2015) and MultiPerspective CNN (He et al., 2015). Another kind of models use soft attention mechanism to obtain the representation of one sentence by depending on representation of another sentence, such as ABCNN (Yin et al., 2015), Attention LSTM(Rockt¨aschel et al., 2015; Hermann et al., 2015). These models can alleviate the weak interaction problem, but are still insufficient to model the contextualized interaction on the word as well as phrase level. Strong Interaction Models These models directly build an interaction space between two sentences and model the interaction at different positions, such as ARC-II (Hu et al., 2014), MV-LSTM"
D16-1176,P14-1062,0,0.0777921,"hitecture to model the strong interaction of sentence pair with two coupled-LSTMs. Specifically, we introduce two coupled ways to model the interdependences of two LSTMs, coupling the local contextualized interactions of two sentences. We then aggregate these interactions and use a dynamic pooling to select the most informative features. Experiments on two very large datasets demonstrate the efficacy of our proposed architectures. 1 Introduction Distributed representations of words or sentences have been widely used in many natural language processing (NLP) tasks, such as text classification (Kalchbrenner et al., 2014; Liu et al., 2015), question answering and machine translation (Sutskever et al., 2014) and so on. Among these tasks, a common problem is modelling the relevance/similarity of the sentence pair, which is also called text semantic matching. Recently, deep learning based models is rising a substantial interest in text semantic matching and have achieved some great progresses (Hu et al., 2014; Qiu and Huang, 2015; Wan et al., 2016). ∗ Corresponding author. According to the phases of interaction between two sentences, previous models can be classified into three categories. Weak interaction Model"
D16-1176,D15-1280,1,0.746506,"ng interaction of sentence pair with two coupled-LSTMs. Specifically, we introduce two coupled ways to model the interdependences of two LSTMs, coupling the local contextualized interactions of two sentences. We then aggregate these interactions and use a dynamic pooling to select the most informative features. Experiments on two very large datasets demonstrate the efficacy of our proposed architectures. 1 Introduction Distributed representations of words or sentences have been widely used in many natural language processing (NLP) tasks, such as text classification (Kalchbrenner et al., 2014; Liu et al., 2015), question answering and machine translation (Sutskever et al., 2014) and so on. Among these tasks, a common problem is modelling the relevance/similarity of the sentence pair, which is also called text semantic matching. Recently, deep learning based models is rising a substantial interest in text semantic matching and have achieved some great progresses (Hu et al., 2014; Qiu and Huang, 2015; Wan et al., 2016). ∗ Corresponding author. According to the phases of interaction between two sentences, previous models can be classified into three categories. Weak interaction Models Some early works"
D16-1176,P16-1098,1,0.558516,"t attention mechanism to obtain the representation of one sentence by depending on representation of another sentence, such as ABCNN (Yin et al., 2015), Attention LSTM(Rockt¨aschel et al., 2015; Hermann et al., 2015). These models can alleviate the weak interaction problem, but are still insufficient to model the contextualized interaction on the word as well as phrase level. Strong Interaction Models These models directly build an interaction space between two sentences and model the interaction at different positions, such as ARC-II (Hu et al., 2014), MV-LSTM (Wan et al., 2016) and DF-LSTMs(Liu et al., 2016). These models can easily capture the difference between semantic capacity of two sentences. In this paper, we propose a new deep neural network architecture to model the strong interactions 1703 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1703–1712, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics of two sentences. Different with modelling two sentences with separated LSTMs, we utilize two interdependent LSTMs, called coupled-LSTMs, to fully affect each other at different time steps. The output of coupled-LST"
D16-1176,D14-1162,0,0.0836116,"stochastic gradient descent with the diagonal variant of AdaGrad (Duchi et al., 2011). To prevent exploding gradients, we perform gradient clipping by scaling the gradient when the norm exceeds a threshold (Graves, 2013). 6 Experiment In this section, we investigate the empirical performances of our proposed model on two different text matching tasks: classification task (recognizing textual entailment) and ranking task (matching of question and answer). 6.1 Hyperparameters and Training The word embeddings for all of the models are initialized with the 100d GloVe vectors (840B token version, (Pennington et al., 2014)) and fine-tuned during training to improve the performance. The other parameters are initialized by randomly sampling from uniform distribution in [−0.1, 0.1]. For each task, we take the hyperparameters which achieve the best performance on the development set via an small grid search over combinations of the initial learning rate [0.05, 0.0005, 0.0001], l2 regularization [0.0, 5E−5, 1E−5, 1E−6] and the threshold value of gradient norm [5, 10, 100]. The final hyperparameters are set as Table 1. 6.2 Competitor Methods • Neural bag-of-words (NBOW): Each sequence as the sum of the embeddings of"
D16-1176,N15-1091,0,0.0461014,"Missing"
D17-1124,balahur-etal-2010-sentiment,0,0.11879,"Missing"
D17-1124,P16-1139,0,0.0146598,"0.01, 0.001], l2 regularization [0.0, 5E−5, 1E−5] The final hyper-parameters are as follows. The initial learning rate is 0.1. The regularization weight of the parameters is 1E−5. For all the sentences from the five datasets, we parse them with constituency parser (Klein and Manning, 2003) to obtain the trees for our and some competitor models. 5.2 We give some descriptions about the setting of our models and several baseline models. • CharLSTM: Character level LSTM. • TLSTM: Vanilla tree-based LSTM, proposed by Tai et al. (2015). • Cont-TLSTM: Context-dependent tree-based LSTM, introduced by Bowman et al. (2016). • iTLSTM-Lo: Proposed model with Look-Up idiomatic interpreter. • iTLSTM-Mo: Proposed model with Morphology-Sensitive interpreter. #Sentences Table 4: Key statistics for the idioms and sentences in iSent dataset. O(Original) denotes the idioms in dev/test sets are in original forms and have appeared in training set. M(Morphology) and L(Lexical) represent the morphology and lexical idiom variations respectively and they are unseen in training set. 600 500 400 300 200 100 0 5.3 0 5 10 15 20 25 30 35 40 45 50 55 Sentence length Figure 2: The distribution of the number of reviews over different"
D17-1124,W06-3506,0,0.0445032,"sentences such as the “Verb Phrases” and “Adverb Phrases”. For example, the sentence “More often than not, this mixed bag hit its mark” has a positive sentiment. Cont-TLSTM pays much more attention to the word “not” without realizing that it belongs to the collocation “more often than not”, which expresses neutral emotion. In comparison, our model regards this collocation as a whole with neutral sentiment, which is crucial for the final prediction. 6 Related Work Previous work related to idioms focused on their identification, which falls in two kinds of paradigms: idiom type classification (Gedigian et al., 2006; Shutova et al., 2010) and idiom token classification (Katz and Giesbrecht, 2006; Li and Sporleder, 2009; Fazly et al., 2009; Peng et al., 2014; Salton et al., 2016). Different with these work, we integrate idioms understanding into a real-world task and consider different peculiarities of idioms in an end-to-end trainable framework. Recently, there are some work exploring the compositionality of various types of phrases (Kartsaklis et al., 2012; Muraoka et al., 2014; Hermann, 2014; Hashimoto and Tsuruoka, 2016). Compared with these work, we focus on how to properly model idioms under the con"
D17-1124,P16-1020,0,0.0323403,"Missing"
D17-1124,P14-1062,0,0.225926,"semantic matching (Liu et al., 2016a,b), and machine translation (Cho et al., 2014). The key factor of these neural models is how to compose a phrase or sentence representation from its constitutive words. Typically, a shared compositional function is used to compose word vectors recursively until obtaining the representation of the phrase or sentence. The form ∗ Corresponding author. of compositional function involves many kinds of neural networks, such as recurrent neural networks (Hochreiter and Schmidhuber, 1997; Chung et al., 2014), convolutional neural networks (Collobert et al., 2011; Kalchbrenner et al., 2014), and recursive neural networks (Socher et al., 2013; Tai et al., 2015; Zhu et al., 2015). However, these methods show an obvious defect in representing idiomatic phrases, whose semantics are not literal compositions of the individual words. For example, “pulling my leg” is idiomatic, and its meaning cannot be directly derived from a literal combination of its contained words. Due to its importance, some previous work focuses on automatic identification of idioms (Katz and Giesbrecht, 2006; Li and Sporleder, 2009; Fazly et al., 2009; Peng et al., 2014; Salton et al., 2016). However, challenge"
D17-1124,C12-2054,0,0.0320652,"tion. 6 Related Work Previous work related to idioms focused on their identification, which falls in two kinds of paradigms: idiom type classification (Gedigian et al., 2006; Shutova et al., 2010) and idiom token classification (Katz and Giesbrecht, 2006; Li and Sporleder, 2009; Fazly et al., 2009; Peng et al., 2014; Salton et al., 2016). Different with these work, we integrate idioms understanding into a real-world task and consider different peculiarities of idioms in an end-to-end trainable framework. Recently, there are some work exploring the compositionality of various types of phrases (Kartsaklis et al., 2012; Muraoka et al., 2014; Hermann, 2014; Hashimoto and Tsuruoka, 2016). Compared with these work, we focus on how to properly model idioms under the context of sentence representations. More recently, Zhu et al. (2016) propose a DAG-structured LSTM to incorporate external semantics including non-compositional or holistically learned semantics. Its key characteristic is that a DAG needs be built in advance, which merges some detected n-grams as the noncompositional phrases based on external knowledge. Different from this work, we focus on how to integrate detection and understanding of idioms int"
D17-1124,P16-1160,0,0.0248845,"al memory M , which is a table and stores idiomatic information for each idiom as depicted in the top subfigure in Figure 1-(c). Formally, the idiomatic meaning for a phrase can be obtained as: h(i) = M[k] (6) where k denotes the index of the corresponding phrase p. Morphology-Sensitive Model Since most idioms enjoy certain flexibility in morphology, lexicon, syntax, the above model suffers from the problem of idiom variations. To remedy this, inspired by the compositional view of idioms (Nunberg et al., 1994) and recent success of characterbased models (Kim et al., 2016; Lample et al., 2016; Chung et al., 2016), we propose to use CharLSTM to directly encode the meaning of a phrase in an idiomatic space and generate an idiomatic representation, which is not contaminated by its literal meaning and sensitive to different variations. Formally, for each non-leaf node i and its corresponding phrase pi in a constituency tree, we apply charLSTM to phrase pi as depicted in the bottom subfigure in Figure 1-(c) and utilize the emitted hidden states rj to represent the idiomatic meaning of the phrase. rj = Char-LSTM(rj−1 , cj−1 , xj ) (7) where j = 1, 2, · · · , m and m represents the length of the input phrase"
D17-1124,W06-1203,0,0.281998,"eiter and Schmidhuber, 1997; Chung et al., 2014), convolutional neural networks (Collobert et al., 2011; Kalchbrenner et al., 2014), and recursive neural networks (Socher et al., 2013; Tai et al., 2015; Zhu et al., 2015). However, these methods show an obvious defect in representing idiomatic phrases, whose semantics are not literal compositions of the individual words. For example, “pulling my leg” is idiomatic, and its meaning cannot be directly derived from a literal combination of its contained words. Due to its importance, some previous work focuses on automatic identification of idioms (Katz and Giesbrecht, 2006; Li and Sporleder, 2009; Fazly et al., 2009; Peng et al., 2014; Salton et al., 2016). However, challenge remains to take idioms into account to improve neural based semantic representations of phrases or sentences. Motivated by the literal-first psycholinguistic hypothesis proposed by Bobrow and Bell (1973), in this paper, we propose an end-to-end neural model for idiom-aware distributed semantic representation, in which we adopt a neural architecture of recursive network (Socher et al., 2013; Tai et al., 2015; Zhu et al., 2015) to learn the compositional semantics over a constituent tree. Mo"
D17-1124,J09-1005,0,0.188751,"onvolutional neural networks (Collobert et al., 2011; Kalchbrenner et al., 2014), and recursive neural networks (Socher et al., 2013; Tai et al., 2015; Zhu et al., 2015). However, these methods show an obvious defect in representing idiomatic phrases, whose semantics are not literal compositions of the individual words. For example, “pulling my leg” is idiomatic, and its meaning cannot be directly derived from a literal combination of its contained words. Due to its importance, some previous work focuses on automatic identification of idioms (Katz and Giesbrecht, 2006; Li and Sporleder, 2009; Fazly et al., 2009; Peng et al., 2014; Salton et al., 2016). However, challenge remains to take idioms into account to improve neural based semantic representations of phrases or sentences. Motivated by the literal-first psycholinguistic hypothesis proposed by Bobrow and Bell (1973), in this paper, we propose an end-to-end neural model for idiom-aware distributed semantic representation, in which we adopt a neural architecture of recursive network (Socher et al., 2013; Tai et al., 2015; Zhu et al., 2015) to learn the compositional semantics over a constituent tree. More concretely, we introduce a neural idiom d"
D17-1124,D14-1181,0,0.0266757,"Missing"
D17-1124,P03-1054,0,0.0175655,"he lurch → in the big lurch on the same page → on different pages Table 3: Idiom variations at morphological and lexical level. Add. and Sub. refer to lexical addition and substitution respectively. Train Idiom Sent. 1247 9752 O 124 720 Dev M 21 200 L 40 100 O 124 1403 Test M 21 400 L 40 200 tions of the initial learning rate [0.1, 0.01, 0.001], l2 regularization [0.0, 5E−5, 1E−5] The final hyper-parameters are as follows. The initial learning rate is 0.1. The regularization weight of the parameters is 1E−5. For all the sentences from the five datasets, we parse them with constituency parser (Klein and Manning, 2003) to obtain the trees for our and some competitor models. 5.2 We give some descriptions about the setting of our models and several baseline models. • CharLSTM: Character level LSTM. • TLSTM: Vanilla tree-based LSTM, proposed by Tai et al. (2015). • Cont-TLSTM: Context-dependent tree-based LSTM, introduced by Bowman et al. (2016). • iTLSTM-Lo: Proposed model with Look-Up idiomatic interpreter. • iTLSTM-Mo: Proposed model with Morphology-Sensitive interpreter. #Sentences Table 4: Key statistics for the idioms and sentences in iSent dataset. O(Original) denotes the idioms in dev/test sets are in"
D17-1124,N16-1030,0,0.0328092,"rieved from an external memory M , which is a table and stores idiomatic information for each idiom as depicted in the top subfigure in Figure 1-(c). Formally, the idiomatic meaning for a phrase can be obtained as: h(i) = M[k] (6) where k denotes the index of the corresponding phrase p. Morphology-Sensitive Model Since most idioms enjoy certain flexibility in morphology, lexicon, syntax, the above model suffers from the problem of idiom variations. To remedy this, inspired by the compositional view of idioms (Nunberg et al., 1994) and recent success of characterbased models (Kim et al., 2016; Lample et al., 2016; Chung et al., 2016), we propose to use CharLSTM to directly encode the meaning of a phrase in an idiomatic space and generate an idiomatic representation, which is not contaminated by its literal meaning and sensitive to different variations. Formally, for each non-leaf node i and its corresponding phrase pi in a constituency tree, we apply charLSTM to phrase pi as depicted in the bottom subfigure in Figure 1-(c) and utilize the emitted hidden states rj to represent the idiomatic meaning of the phrase. rj = Char-LSTM(rj−1 , cj−1 , xj ) (7) where j = 1, 2, · · · , m and m represents the lengt"
D17-1124,D09-1033,0,0.0840053,"; Chung et al., 2014), convolutional neural networks (Collobert et al., 2011; Kalchbrenner et al., 2014), and recursive neural networks (Socher et al., 2013; Tai et al., 2015; Zhu et al., 2015). However, these methods show an obvious defect in representing idiomatic phrases, whose semantics are not literal compositions of the individual words. For example, “pulling my leg” is idiomatic, and its meaning cannot be directly derived from a literal combination of its contained words. Due to its importance, some previous work focuses on automatic identification of idioms (Katz and Giesbrecht, 2006; Li and Sporleder, 2009; Fazly et al., 2009; Peng et al., 2014; Salton et al., 2016). However, challenge remains to take idioms into account to improve neural based semantic representations of phrases or sentences. Motivated by the literal-first psycholinguistic hypothesis proposed by Bobrow and Bell (1973), in this paper, we propose an end-to-end neural model for idiom-aware distributed semantic representation, in which we adopt a neural architecture of recursive network (Socher et al., 2013; Tai et al., 2015; Zhu et al., 2015) to learn the compositional semantics over a constituent tree. More concretely, we introd"
D17-1124,C10-1113,0,0.0214074,"“Verb Phrases” and “Adverb Phrases”. For example, the sentence “More often than not, this mixed bag hit its mark” has a positive sentiment. Cont-TLSTM pays much more attention to the word “not” without realizing that it belongs to the collocation “more often than not”, which expresses neutral emotion. In comparison, our model regards this collocation as a whole with neutral sentiment, which is crucial for the final prediction. 6 Related Work Previous work related to idioms focused on their identification, which falls in two kinds of paradigms: idiom type classification (Gedigian et al., 2006; Shutova et al., 2010) and idiom token classification (Katz and Giesbrecht, 2006; Li and Sporleder, 2009; Fazly et al., 2009; Peng et al., 2014; Salton et al., 2016). Different with these work, we integrate idioms understanding into a real-world task and consider different peculiarities of idioms in an end-to-end trainable framework. Recently, there are some work exploring the compositionality of various types of phrases (Kartsaklis et al., 2012; Muraoka et al., 2014; Hermann, 2014; Hashimoto and Tsuruoka, 2016). Compared with these work, we focus on how to properly model idioms under the context of sentence repres"
D17-1124,P16-1098,1,0.837358,"ty of a phrase literally or idiomatically. To better evaluate our models, we also construct an idiom-enriched sentiment classification dataset with considerable scale and abundant peculiarities of idioms. The qualitative and quantitative experimental analyses demonstrate the efficacy of our models. The newly-introduced datasets are publicly available at http: //nlp.fudan.edu.cn/data/ 1 Introduction Currently, neural network models have achieved great success for many natural language processing (NLP) tasks , such as text classification (Zhao et al., 2015; Liu et al., 2017), semantic matching (Liu et al., 2016a,b), and machine translation (Cho et al., 2014). The key factor of these neural models is how to compose a phrase or sentence representation from its constitutive words. Typically, a shared compositional function is used to compose word vectors recursively until obtaining the representation of the phrase or sentence. The form ∗ Corresponding author. of compositional function involves many kinds of neural networks, such as recurrent neural networks (Hochreiter and Schmidhuber, 1997; Chung et al., 2014), convolutional neural networks (Collobert et al., 2011; Kalchbrenner et al., 2014), and recu"
D17-1124,D12-1110,0,0.149592,"Missing"
D17-1124,P17-1001,1,0.859383,"Missing"
D17-1124,D16-1176,1,0.851595,"ty of a phrase literally or idiomatically. To better evaluate our models, we also construct an idiom-enriched sentiment classification dataset with considerable scale and abundant peculiarities of idioms. The qualitative and quantitative experimental analyses demonstrate the efficacy of our models. The newly-introduced datasets are publicly available at http: //nlp.fudan.edu.cn/data/ 1 Introduction Currently, neural network models have achieved great success for many natural language processing (NLP) tasks , such as text classification (Zhao et al., 2015; Liu et al., 2017), semantic matching (Liu et al., 2016a,b), and machine translation (Cho et al., 2014). The key factor of these neural models is how to compose a phrase or sentence representation from its constitutive words. Typically, a shared compositional function is used to compose word vectors recursively until obtaining the representation of the phrase or sentence. The form ∗ Corresponding author. of compositional function involves many kinds of neural networks, such as recurrent neural networks (Hochreiter and Schmidhuber, 1997; Chung et al., 2014), convolutional neural networks (Collobert et al., 2011; Kalchbrenner et al., 2014), and recu"
D17-1124,Y14-1010,0,0.0550271,"Missing"
D17-1124,P04-1035,0,0.00622438,"lso evaluate our models on these datasets to make a comparison with many recent proposed models. Each dataset is briefly described as follows. • SST-1 The movie reviews with five classes (negative, somewhat negative, neutral, somewhat positive, positive) in the Stanford Sentiment Treebank1 (Socher et al., 2013). • SST-2 The movie reviews with binary classes. It is also derived from the Stanford Sentiment Treebank. • MR The movie reviews with two classes 2 (Pang and Lee, 2005). • SUBJ Subjectivity data set where the goal is to classify each instance (snippet) as being subjective or objective. (Pang and Lee, 2004) The detailed statistics about these four datasets are listed in Table 2. 4.2 Reasons for a New Dataset Differing from previous work, which evaluating idiom detection as a standalone task, we want to integrate idiom understanding into sentiment classification task. However, most of existing sentiment datasets do not cover enough idioms or related linguistic phenomenon. To better evaluate our models on idiom understanding task, we proposed an idiom-enriched sentiment classification dataset, in which each sentence contains at least one idiom. Additionally, considering most idioms have certain fl"
D17-1124,P05-1015,0,0.016112,"lassification We list four kinds of datasets which are most commonly used for sentiment classification in NLP community. Additionally, we also evaluate our models on these datasets to make a comparison with many recent proposed models. Each dataset is briefly described as follows. • SST-1 The movie reviews with five classes (negative, somewhat negative, neutral, somewhat positive, positive) in the Stanford Sentiment Treebank1 (Socher et al., 2013). • SST-2 The movie reviews with binary classes. It is also derived from the Stanford Sentiment Treebank. • MR The movie reviews with two classes 2 (Pang and Lee, 2005). • SUBJ Subjectivity data set where the goal is to classify each instance (snippet) as being subjective or objective. (Pang and Lee, 2004) The detailed statistics about these four datasets are listed in Table 2. 4.2 Reasons for a New Dataset Differing from previous work, which evaluating idiom detection as a standalone task, we want to integrate idiom understanding into sentiment classification task. However, most of existing sentiment datasets do not cover enough idioms or related linguistic phenomenon. To better evaluate our models on idiom understanding task, we proposed an idiom-enriched"
D17-1124,D14-1216,0,0.0930799,"networks (Collobert et al., 2011; Kalchbrenner et al., 2014), and recursive neural networks (Socher et al., 2013; Tai et al., 2015; Zhu et al., 2015). However, these methods show an obvious defect in representing idiomatic phrases, whose semantics are not literal compositions of the individual words. For example, “pulling my leg” is idiomatic, and its meaning cannot be directly derived from a literal combination of its contained words. Due to its importance, some previous work focuses on automatic identification of idioms (Katz and Giesbrecht, 2006; Li and Sporleder, 2009; Fazly et al., 2009; Peng et al., 2014; Salton et al., 2016). However, challenge remains to take idioms into account to improve neural based semantic representations of phrases or sentences. Motivated by the literal-first psycholinguistic hypothesis proposed by Bobrow and Bell (1973), in this paper, we propose an end-to-end neural model for idiom-aware distributed semantic representation, in which we adopt a neural architecture of recursive network (Socher et al., 2013; Tai et al., 2015; Zhu et al., 2015) to learn the compositional semantics over a constituent tree. More concretely, we introduce a neural idiom detector for each ph"
D17-1124,D14-1162,0,0.101534,": The distribution of the number of reviews over different lengths. 5.1 Experimental Settings Loss Function Given a sentence and its label, the output of neural network is the probabilities of the different classes. The parameters of the network are trained to minimise the cross-entropy of the predicted and true label distributions. To minimize the objective, we use stochastic gradient descent with the diagonal variant of AdaGrad (Duchi et al., 2011). Initialization and Hyperparameters In all of our experiments, the word embeddings for all of the models are initialized with the GloVe vectors (Pennington et al., 2014). The other parameters are initialized by randomly sampling from uniform distribution in [−0.1, 0.1]. For each task, we take the hyperparameters which achieve the best performance on the development set via a small grid search over combinaCompetitor Models Evaluation over Mainstream Datasets The experimental results are shown in Table 5. We can see Cont-TLSTM outperforms TLSTM on all four tasks, showing the importance of contextsensitive composition. Besides, both iTLSTM-Lo and iTLSTM-Mo achieve better results than TLSTM and Cont-LSTM, which indicates the effectiveness of our introduced module"
D17-1124,D11-1014,0,0.0732044,"Missing"
D17-1124,D13-1170,0,0.390752,"lation (Cho et al., 2014). The key factor of these neural models is how to compose a phrase or sentence representation from its constitutive words. Typically, a shared compositional function is used to compose word vectors recursively until obtaining the representation of the phrase or sentence. The form ∗ Corresponding author. of compositional function involves many kinds of neural networks, such as recurrent neural networks (Hochreiter and Schmidhuber, 1997; Chung et al., 2014), convolutional neural networks (Collobert et al., 2011; Kalchbrenner et al., 2014), and recursive neural networks (Socher et al., 2013; Tai et al., 2015; Zhu et al., 2015). However, these methods show an obvious defect in representing idiomatic phrases, whose semantics are not literal compositions of the individual words. For example, “pulling my leg” is idiomatic, and its meaning cannot be directly derived from a literal combination of its contained words. Due to its importance, some previous work focuses on automatic identification of idioms (Katz and Giesbrecht, 2006; Li and Sporleder, 2009; Fazly et al., 2009; Peng et al., 2014; Salton et al., 2016). However, challenge remains to take idioms into account to improve neura"
D17-1124,P15-1150,0,0.377141,"014). The key factor of these neural models is how to compose a phrase or sentence representation from its constitutive words. Typically, a shared compositional function is used to compose word vectors recursively until obtaining the representation of the phrase or sentence. The form ∗ Corresponding author. of compositional function involves many kinds of neural networks, such as recurrent neural networks (Hochreiter and Schmidhuber, 1997; Chung et al., 2014), convolutional neural networks (Collobert et al., 2011; Kalchbrenner et al., 2014), and recursive neural networks (Socher et al., 2013; Tai et al., 2015; Zhu et al., 2015). However, these methods show an obvious defect in representing idiomatic phrases, whose semantics are not literal compositions of the individual words. For example, “pulling my leg” is idiomatic, and its meaning cannot be directly derived from a literal combination of its contained words. Due to its importance, some previous work focuses on automatic identification of idioms (Katz and Giesbrecht, 2006; Li and Sporleder, 2009; Fazly et al., 2009; Peng et al., 2014; Salton et al., 2016). However, challenge remains to take idioms into account to improve neural based semantic r"
D17-1124,N16-1106,0,0.0186106,"ation (Katz and Giesbrecht, 2006; Li and Sporleder, 2009; Fazly et al., 2009; Peng et al., 2014; Salton et al., 2016). Different with these work, we integrate idioms understanding into a real-world task and consider different peculiarities of idioms in an end-to-end trainable framework. Recently, there are some work exploring the compositionality of various types of phrases (Kartsaklis et al., 2012; Muraoka et al., 2014; Hermann, 2014; Hashimoto and Tsuruoka, 2016). Compared with these work, we focus on how to properly model idioms under the context of sentence representations. More recently, Zhu et al. (2016) propose a DAG-structured LSTM to incorporate external semantics including non-compositional or holistically learned semantics. Its key characteristic is that a DAG needs be built in advance, which merges some detected n-grams as the noncompositional phrases based on external knowledge. Different from this work, we focus on how to integrate detection and understanding of idioms into a unified end-to-end model, in which an idiomatic detector is introduced to adaptively control the semantic compositionality. Particularly, in the whole process no extra information is given to tell which phrases s"
D17-1124,W14-1007,0,0.0191394,"ntation. • We integrate idioms understanding into a real-world NLP task instead of evaluating idiom detection as a standalone task. • We construct a new real-world dataset covering abundant idioms with original and variational forms. The elaborate qualitative and quantitative experimental analyses show the effectiveness of our models. 2 Linguistic Interpretation of Idioms Recently, idioms have raised eyebrows among linguists, psycholinguists, and lexicographers due to their pervasiveness in daily discourse and their fascinating properties in linguistics literature (Villavicencio et al., 2005; Salton et al., 2014). As peculiar linguistic constructions (Villavicencio et al., 2005; Salton et al., 2014), idioms have three following properties: Invisibility Idioms always disguise themselves as normal multi-words in sentences. It makes end-to-end training hard since we should detect idioms first, and then understand them. Idiomaticity Idioms are semantically opaque, whose meanings cannot be derived from their constituent words. Existing compositional distributed approaches fail due to the hypothesis that the meaning of any phrase can be composed of the meanings of its constituents. Flexibility While structu"
D17-1124,P16-1019,0,0.0635211,"et al., 2011; Kalchbrenner et al., 2014), and recursive neural networks (Socher et al., 2013; Tai et al., 2015; Zhu et al., 2015). However, these methods show an obvious defect in representing idiomatic phrases, whose semantics are not literal compositions of the individual words. For example, “pulling my leg” is idiomatic, and its meaning cannot be directly derived from a literal combination of its contained words. Due to its importance, some previous work focuses on automatic identification of idioms (Katz and Giesbrecht, 2006; Li and Sporleder, 2009; Fazly et al., 2009; Peng et al., 2014; Salton et al., 2016). However, challenge remains to take idioms into account to improve neural based semantic representations of phrases or sentences. Motivated by the literal-first psycholinguistic hypothesis proposed by Bobrow and Bell (1973), in this paper, we propose an end-to-end neural model for idiom-aware distributed semantic representation, in which we adopt a neural architecture of recursive network (Socher et al., 2013; Tai et al., 2015; Zhu et al., 2015) to learn the compositional semantics over a constituent tree. More concretely, we introduce a neural idiom detector for each phrase in a sentence to"
D17-1256,R13-1026,0,0.453604,"tics of the datasets used in this work are listed in Table 1. 2415 • Stanford POS Tagger: Stanford POS Tagger is a widely used tool for newswire domains (Toutanova et al., 2003). In this work, we train it using two different sets, the WSJ (sections 0-18) and a WSJ, IRC, and Twitter mixed corpus. We use StanfordWSJ and Stanford-MIX to represent them, respectively. • T-POS: T-Pos (Ritter et al., 2011) adopts the Conditional Random Fields and clustering algorithm to perform the task. It was trained from a mixture of hand-annotated tweets and existing POS-labeled data. • GATE Tagger: GATE tagger (Derczynski et al., 2013) is based on vote-constrained bootstrapping with unlabeled data. It combines cases where available taggers use different tagsets. • ARK Tagger: ARK tagger (Owoputi et al., 2013) is a system that reports the best accuracy on the RIT dataset. It uses unsupervised word clustering and a variety of lexical features. • bi-LSTM: Bidirectional Long Short-Term Memory (LSTM) networks have been widely used in a variety of sequence labeling tasks (Graves and Schmidhuber, 2005). In this work, we evaluate it at character level, word level, and combining them together. bi-LSTM (word level) uses one layer of"
D17-1256,P11-2008,0,0.0699662,"Missing"
D17-1256,N13-1097,0,0.0251818,"structured data from social media provides valuable information for a variety of applications such as stock prediction (Bollen et al., 2011), public health analysis (Wilson and Brownstein, 2009; Paul and Dredze, 2011), real-time event detection (Sakaki et al., 2010), and so on. The quality of these applications is highly impacted by the performance of natural language processing tasks. ∗ Corresponding author. u PRP Part-of-speech (POS) tagging is one of the most important natural language processing tasks. It has also been widely used in the social media analysis systems (Ritter et al., 2012; Lamb et al., 2013; Kiritchenko et al., 2014). Most stateof-the-art POS tagging approaches are based on supervised methods. Hence, they usually require a large amount of annotated data to train models. Many datasets have been constructed for POS tagging task. Because newswire articles are carefully edited, benchmarks usually use them for annotation (Marcus et al., 1993). However, usergenerated contents on social media are usually informal and contain many nonstandard lexical items. Moreover, the difference in domains between training data and evaluation data may heavily impact the performance of approaches base"
D17-1256,P15-1107,0,0.0393995,"Missing"
D17-1256,P16-1101,0,0.00689095,"tagging is an important preprocessing step and can provide valuable information for various natural language processing tasks. In recent years, deep learning algorithms have been successfully used for POS tagging. A number of approaches have been proposed and have achieved some progress. Santos and Guimaraes (2015) proposed using a character-based convolutional neural network to perform the POS tagging problem. Bi-LSTMs with word, character or unicode byte embedding were also introduced to achieve the POS tagging and named entity recognition tasks (Plank et al., 2016; Chiu and Nichols, 2015; Ma and Hovy, 2016). In this work, we study the problem from a domain adaption perspective. Inspired by these works, we also propose to use character-level methods to handle out-ofvocabulary words and bi-LSTMs to model the sequence relations. Adversarial networks were successfully used for image generation (Goodfellow et al., 2014; Dosovitskiy et al., 2015; Denton et al., 2015), domain adaption (Tzeng et al., 2014; Ganin et al., 2016), and semi-supervised learning (Denton et al., 2016). The key idea of adversarial networks for domain adaption is to construct invariant features by optimizing the feature extractor"
D17-1256,J93-2004,0,0.0607874,"al discriminator. In addition, we hypothesize that domain-specific features of target domain should be preserved in some degree. Hence, the proposed method adopts a sequence-to-sequence autoencoder to perform this task. Experimental results on three different datasets show that our method achieves better performance than state-of-the-art methods. 1 was talkin bout another time . nd i dnt VBD VBG IN DT NN . CC PRP VBP see u either ! VB PRP RB . Figure 1: An example of tagged Tweet, which contains nonstandard orthography, emoticon, and abbreviation. The tagset is defined similar as that of PTB (Marcus et al., 1993). Introduction During the last decade, social media have become extremely popular, on which billions of usergenerated contents are posted every day. Many users have been writing about their thoughts and lives on the go. The massive unstructured data from social media provides valuable information for a variety of applications such as stock prediction (Bollen et al., 2011), public health analysis (Wilson and Brownstein, 2009; Paul and Dredze, 2011), real-time event detection (Sakaki et al., 2010), and so on. The quality of these applications is highly impacted by the performance of natural lang"
D17-1256,N13-1039,0,0.0473268,"Missing"
D17-1256,Q17-1036,0,0.0580385,"Missing"
D17-1256,P16-2067,0,0.0287739,"ure knowledge. 5 Related Work Part-of-Speech tagging is an important preprocessing step and can provide valuable information for various natural language processing tasks. In recent years, deep learning algorithms have been successfully used for POS tagging. A number of approaches have been proposed and have achieved some progress. Santos and Guimaraes (2015) proposed using a character-based convolutional neural network to perform the POS tagging problem. Bi-LSTMs with word, character or unicode byte embedding were also introduced to achieve the POS tagging and named entity recognition tasks (Plank et al., 2016; Chiu and Nichols, 2015; Ma and Hovy, 2016). In this work, we study the problem from a domain adaption perspective. Inspired by these works, we also propose to use character-level methods to handle out-ofvocabulary words and bi-LSTMs to model the sequence relations. Adversarial networks were successfully used for image generation (Goodfellow et al., 2014; Dosovitskiy et al., 2015; Denton et al., 2015), domain adaption (Tzeng et al., 2014; Ganin et al., 2016), and semi-supervised learning (Denton et al., 2016). The key idea of adversarial networks for domain adaption is to construct invariant"
D17-1256,D11-1141,0,0.264053,"Missing"
D17-1256,W15-3904,0,0.0247453,"for proper nouns recognition, which fires on names from many sources, such as Freebase lists of celebrities, the Moby Words list of US Locations, proper names from Mark Kantrowitz’s name corpus and so on. So, our model is also competitive when lacking of manual feature knowledge. 5 Related Work Part-of-Speech tagging is an important preprocessing step and can provide valuable information for various natural language processing tasks. In recent years, deep learning algorithms have been successfully used for POS tagging. A number of approaches have been proposed and have achieved some progress. Santos and Guimaraes (2015) proposed using a character-based convolutional neural network to perform the POS tagging problem. Bi-LSTMs with word, character or unicode byte embedding were also introduced to achieve the POS tagging and named entity recognition tasks (Plank et al., 2016; Chiu and Nichols, 2015; Ma and Hovy, 2016). In this work, we study the problem from a domain adaption perspective. Inspired by these works, we also propose to use character-level methods to handle out-ofvocabulary words and bi-LSTMs to model the sequence relations. Adversarial networks were successfully used for image generation (Goodfello"
D17-1256,N03-1033,0,0.0622392,"g POS tagging approaches, we compare the proposed method with other approaches on three benchmarks: RIT-Twitter (Ritter et al., 2011), NPSCHAT (Forsyth, 2007), and ARKTwitter (Gimpel et al., 2011). Unlabeled in-domain data. For training the adversarial network, we need to use a dataset that has large scale unlabeled tweets. Hence, in this work, we construct large scale unlabeled data (UNL), from Twitter using its API. The detailed data statistics of the datasets used in this work are listed in Table 1. 2415 • Stanford POS Tagger: Stanford POS Tagger is a widely used tool for newswire domains (Toutanova et al., 2003). In this work, we train it using two different sets, the WSJ (sections 0-18) and a WSJ, IRC, and Twitter mixed corpus. We use StanfordWSJ and Stanford-MIX to represent them, respectively. • T-POS: T-Pos (Ritter et al., 2011) adopts the Conditional Random Fields and clustering algorithm to perform the task. It was trained from a mixture of hand-annotated tweets and existing POS-labeled data. • GATE Tagger: GATE tagger (Derczynski et al., 2013) is based on vote-constrained bootstrapping with unlabeled data. It combines cases where available taggers use different tagsets. • ARK Tagger: ARK tagge"
D18-1090,D15-1049,0,0.0541644,"of the future directions can be exploring other kinds of scoring actions than classification under the reinforcement learning framework. Text Quality Evaluation Traditionally, AES models are usually divided into three categories: classification, regression and ranking. Naive Bayes models are mostly used in classification tasks. Larkey (1998) use bagof-word features. Following that, Rudner and Liang (2002) develop a system based on multinomial Bernoulli Naive Bayes, using content and style features. E-rater (Attali and Burstein, 2004) is one of the earliest systems to adopt regression methods. Phandi et al. (2015) use correlated Bayesian Linear Ridge Regression (cBLRR) focusing on domain-adaptation tasks. Ranking models use linguistic features. Yannakoudakis et al. (2011) formulate AES as a pair-wise ranking problem by ranking the order of pair essays. Chen and He (2013) formulate AES into a list-wise ranking problem by considering the order relation among the whole essays. Argument quality evaluation is a task closely related to AES, which involves evaluation of argumentative texts with various grains (argumentlevel, post-level, etc.). Tan et al. (2016); Wei et al. (2016a); Wang et al. (2017) make use"
D18-1090,D13-1180,0,0.0930244,"king. Naive Bayes models are mostly used in classification tasks. Larkey (1998) use bagof-word features. Following that, Rudner and Liang (2002) develop a system based on multinomial Bernoulli Naive Bayes, using content and style features. E-rater (Attali and Burstein, 2004) is one of the earliest systems to adopt regression methods. Phandi et al. (2015) use correlated Bayesian Linear Ridge Regression (cBLRR) focusing on domain-adaptation tasks. Ranking models use linguistic features. Yannakoudakis et al. (2011) formulate AES as a pair-wise ranking problem by ranking the order of pair essays. Chen and He (2013) formulate AES into a list-wise ranking problem by considering the order relation among the whole essays. Argument quality evaluation is a task closely related to AES, which involves evaluation of argumentative texts with various grains (argumentlevel, post-level, etc.). Tan et al. (2016); Wei et al. (2016a); Wang et al. (2017) make use of linguistic features to evaluate the persuasiveness of arAcknowledgments Thanks for the constructive comments from anonymous reviewers. This work is partially supported by National Natural Science Foundation of China (Grant No. 61702106), Shanghai Science and"
D18-1090,D16-1115,0,0.0126277,"esults on benchmark datasets show the effectiveness of our framework. 1 Introduction In recent years, neural networks have been widely used to grade student essays automatically and achieve state-of-the-art performance. In particular, a distributed representation is learned for an essay with variant neural networks and a linear layer is then used to produce the final score. Existing researches focus on learning better essay representation using different neural networks, including long short-term memory (LSTM) network (Taghipour and Ng, 2016), hierarchical convolutional neural networks (CNN) (Dong and Zhang, 2016), hierarchical CNN-LSTM structure with attention mechanism (Dong et al., 2017), and SKIPFLOW LSTM (Tay et al., 2017). The major evaluation metric for AES is quadratic weighted kappa (QWK), which is also the official metric of Automated Student Assessment Prize1 (ASAP). It evaluates the scoring results by taking rating schema into consideration. Because QWK is not differentiable, it is hard to train systems via optimizing this metric directly. Alternatively, existing AES systems are typically trained to predict the score for a single essay and optimized using mean square error (MSE). The 2 Typi"
D18-1090,D16-1193,0,0.302353,"tic weighted kappa as guidance to optimize the scoring system. Experiment results on benchmark datasets show the effectiveness of our framework. 1 Introduction In recent years, neural networks have been widely used to grade student essays automatically and achieve state-of-the-art performance. In particular, a distributed representation is learned for an essay with variant neural networks and a linear layer is then used to produce the final score. Existing researches focus on learning better essay representation using different neural networks, including long short-term memory (LSTM) network (Taghipour and Ng, 2016), hierarchical convolutional neural networks (CNN) (Dong and Zhang, 2016), hierarchical CNN-LSTM structure with attention mechanism (Dong et al., 2017), and SKIPFLOW LSTM (Tay et al., 2017). The major evaluation metric for AES is quadratic weighted kappa (QWK), which is also the official metric of Automated Student Assessment Prize1 (ASAP). It evaluates the scoring results by taking rating schema into consideration. Because QWK is not differentiable, it is hard to train systems via optimizing this metric directly. Alternatively, existing AES systems are typically trained to predict the score f"
D18-1090,K17-1017,0,0.160871,"ion In recent years, neural networks have been widely used to grade student essays automatically and achieve state-of-the-art performance. In particular, a distributed representation is learned for an essay with variant neural networks and a linear layer is then used to produce the final score. Existing researches focus on learning better essay representation using different neural networks, including long short-term memory (LSTM) network (Taghipour and Ng, 2016), hierarchical convolutional neural networks (CNN) (Dong and Zhang, 2016), hierarchical CNN-LSTM structure with attention mechanism (Dong et al., 2017), and SKIPFLOW LSTM (Tay et al., 2017). The major evaluation metric for AES is quadratic weighted kappa (QWK), which is also the official metric of Automated Student Assessment Prize1 (ASAP). It evaluates the scoring results by taking rating schema into consideration. Because QWK is not differentiable, it is hard to train systems via optimizing this metric directly. Alternatively, existing AES systems are typically trained to predict the score for a single essay and optimized using mean square error (MSE). The 2 Typically, an essay scorer contains two components, namely, essay representation a"
D18-1090,C18-1314,1,0.81003,"ession-based scorer B0. The rating ranges for set 1,2,7,8 are much greater than set 3,4,5,6 (see Table 1). The performance difference between B1 and B0 decreases (from positive to negative) when the number of rating categories increases. This is because when the number of categories get larger, it requires much more parameters for the classification-based scorer to be well trained. Given N categories, the classification layer should output N probabilities for each category per essay, costing N times more parameters than regression-based scoring. 4 guments in online forums. Wei et al. (2016b); Ji et al. (2018) consider features from the perspectives of argumentation interaction between participants. Persing and Ng (2017) construct their model based on error types for argumentation. 4.2 Being able to optimize non-differentiable quality metrics, reinforcement learning has been widely used in natural language processing tasks such as machine translation (Bahdanau et al., 2016), image captioning (Rennie et al., 2016; Zhang et al., 2017) and text summarization (Ranzato et al., 2015). To the best of our knowledge, this paper is the first attempt to optimize the scorer by QWK that considers rating schema."
D18-1090,Q17-1016,0,0.0533373,"Missing"
D18-1090,P16-2032,1,0.834915,") compared with regression-based scorer B0. The rating ranges for set 1,2,7,8 are much greater than set 3,4,5,6 (see Table 1). The performance difference between B1 and B0 decreases (from positive to negative) when the number of rating categories increases. This is because when the number of categories get larger, it requires much more parameters for the classification-based scorer to be well trained. Given N categories, the classification layer should output N probabilities for each category per essay, costing N times more parameters than regression-based scoring. 4 guments in online forums. Wei et al. (2016b); Ji et al. (2018) consider features from the perspectives of argumentation interaction between participants. Persing and Ng (2017) construct their model based on error types for argumentation. 4.2 Being able to optimize non-differentiable quality metrics, reinforcement learning has been widely used in natural language processing tasks such as machine translation (Bahdanau et al., 2016), image captioning (Rennie et al., 2016; Zhang et al., 2017) and text summarization (Ranzato et al., 2015). To the best of our knowledge, this paper is the first attempt to optimize the scorer by QWK that cons"
D18-1090,W16-2820,1,0.86136,") compared with regression-based scorer B0. The rating ranges for set 1,2,7,8 are much greater than set 3,4,5,6 (see Table 1). The performance difference between B1 and B0 decreases (from positive to negative) when the number of rating categories increases. This is because when the number of categories get larger, it requires much more parameters for the classification-based scorer to be well trained. Given N categories, the classification layer should output N probabilities for each category per essay, costing N times more parameters than regression-based scoring. 4 guments in online forums. Wei et al. (2016b); Ji et al. (2018) consider features from the perspectives of argumentation interaction between participants. Persing and Ng (2017) construct their model based on error types for argumentation. 4.2 Being able to optimize non-differentiable quality metrics, reinforcement learning has been widely used in natural language processing tasks such as machine translation (Bahdanau et al., 2016), image captioning (Rennie et al., 2016; Zhang et al., 2017) and text summarization (Ranzato et al., 2015). To the best of our knowledge, this paper is the first attempt to optimize the scorer by QWK that cons"
D18-1090,P11-1019,0,0.16244,"uation Traditionally, AES models are usually divided into three categories: classification, regression and ranking. Naive Bayes models are mostly used in classification tasks. Larkey (1998) use bagof-word features. Following that, Rudner and Liang (2002) develop a system based on multinomial Bernoulli Naive Bayes, using content and style features. E-rater (Attali and Burstein, 2004) is one of the earliest systems to adopt regression methods. Phandi et al. (2015) use correlated Bayesian Linear Ridge Regression (cBLRR) focusing on domain-adaptation tasks. Ranking models use linguistic features. Yannakoudakis et al. (2011) formulate AES as a pair-wise ranking problem by ranking the order of pair essays. Chen and He (2013) formulate AES into a list-wise ranking problem by considering the order relation among the whole essays. Argument quality evaluation is a task closely related to AES, which involves evaluation of argumentative texts with various grains (argumentlevel, post-level, etc.). Tan et al. (2016); Wei et al. (2016a); Wang et al. (2017) make use of linguistic features to evaluate the persuasiveness of arAcknowledgments Thanks for the constructive comments from anonymous reviewers. This work is partially"
D18-1090,P17-1172,0,0.01471,"ize non-differentiable quality metrics, reinforcement learning has been widely used in natural language processing tasks such as machine translation (Bahdanau et al., 2016), image captioning (Rennie et al., 2016; Zhang et al., 2017) and text summarization (Ranzato et al., 2015). To the best of our knowledge, this paper is the first attempt to optimize the scorer by QWK that considers rating schema. Skip connections in RNNs are capable of capturing long-term dependencies in sequences. Vezhnevets et al. (2017) introduces dilated LSTM to allow its manager to operate at a low temporal resolution. Yu et al. (2017) propose a reinforcement learning method to let the network learn how long to skip. Related Work There are two lines of research related to our work including text quality evaluation and reinforcement learning for natural language processing. 4.1 Reinforcement Learning for Natural Language Processing 5 Conclusion and Future Work In this paper, we propose a reinforcement learning framework incorporating QWK metric as the reward to train the essay scoring system directly. A packed evaluation strategy is used for QWK computation and the scoring of each essay is treated as a single action. In part"
D18-1186,W17-5310,0,0.0245657,"Missing"
D18-1186,P14-1062,0,0.0560887,"twork (CIN). CIN can serve as an alternative module of attention mechanism. We first briefly introduce how the convolution works over text sequence, then describe the proposed model and its connection to attention model. 3.1 Convolution over Sequence Convolution is an effective operation in deep neural networks, which convolves the input with a set of filters to extract non-linear compositional features. Although originally designed for computer vision, convolutional models have subsequently shown to be effective for NLP and have achieved excellent performance in sentence modeling Kim (2014); Kalchbrenner et al. (2014), and other traditional NLP tasks Hu et al. (2014); Zeng et al. (2014); Gehring et al. (2017). Given a sentence representation X = d×m [x1 , x2 , · · · , xm ] ∈ R , a convolutional filter W (f ) ∈ Rd×kd , the convolution process is defined as where w, W1 , W2 and b are learnable parameters. 1577   x0i = f W (f ) [xi−[k/2] , · · · , xi+[k/2] ] + b(f ) , (8) where τ is the width of filter, FGN(·) is the filter generation network. A detailed implementation of FGN is presented in Section 3.4. Now we can convolve the two sentences with the generated filters. Convolutional Interaction Network Wx F"
D18-1186,D15-1075,0,0.152807,"Missing"
D18-1186,P17-1152,0,0.54434,"ing hypothesis. Generally, NLI is also related to many other NLP tasks under the paradigm of semantic matching of two texts, such as question answering Hu et al. (2014); Wan et al. (2016) and information retrieval Liu et al. (2015), and so on. An essential challenge is to capture the semantic relevance of two sentences. Due to the semantic gap (or lexical chasm) problem, natural language inference is still a challenging problem. Recently, deep learning is raising a substantial interest in natural language inference and has achieved some great progresses Hu et al. (2014); Parikh et al. (2016); Chen et al. (2017a). To model the complicated semantic relationship between two sentences, previous models heavily utilize various attention mechanism Bahdanau et al. ∗ † (2014); Vaswani et al. (2017) to build the interaction at different granularity (word, phrase and sentence level), such as ABCNN Yin et al. (2016), Attention LSTM Rockt¨aschel et al. (2015), bidirectional attention LSTM Chen et al. (2017a), and so on. While attention is very successful in natural language inference, its mechanism is quite simple and can be regarded as a weighted sum of the target vectors. This paradigm results in a lack of fl"
D18-1186,D14-1181,0,0.0220591,"Missing"
D18-1186,N15-1092,0,0.0149926,"ers and sizes, CIN can capture more complicated interaction patterns. Experiments on three very large datasets demonstrate CIN’s efficacy. 1 Introduction Natural language inference (NLI) is a pivotal and challenging natural language processing (NLP) task. The goal of NLI is to identify the logical relationship (entailment, neutral, or contradiction) between a premise and a corresponding hypothesis. Generally, NLI is also related to many other NLP tasks under the paradigm of semantic matching of two texts, such as question answering Hu et al. (2014); Wan et al. (2016) and information retrieval Liu et al. (2015), and so on. An essential challenge is to capture the semantic relevance of two sentences. Due to the semantic gap (or lexical chasm) problem, natural language inference is still a challenging problem. Recently, deep learning is raising a substantial interest in natural language inference and has achieved some great progresses Hu et al. (2014); Parikh et al. (2016); Chen et al. (2017a). To model the complicated semantic relationship between two sentences, previous models heavily utilize various attention mechanism Bahdanau et al. ∗ † (2014); Vaswani et al. (2017) to build the interaction at di"
D18-1186,W17-5307,0,0.259068,"ing hypothesis. Generally, NLI is also related to many other NLP tasks under the paradigm of semantic matching of two texts, such as question answering Hu et al. (2014); Wan et al. (2016) and information retrieval Liu et al. (2015), and so on. An essential challenge is to capture the semantic relevance of two sentences. Due to the semantic gap (or lexical chasm) problem, natural language inference is still a challenging problem. Recently, deep learning is raising a substantial interest in natural language inference and has achieved some great progresses Hu et al. (2014); Parikh et al. (2016); Chen et al. (2017a). To model the complicated semantic relationship between two sentences, previous models heavily utilize various attention mechanism Bahdanau et al. ∗ † (2014); Vaswani et al. (2017) to build the interaction at different granularity (word, phrase and sentence level), such as ABCNN Yin et al. (2016), Attention LSTM Rockt¨aschel et al. (2015), bidirectional attention LSTM Chen et al. (2017a), and so on. While attention is very successful in natural language inference, its mechanism is quite simple and can be regarded as a weighted sum of the target vectors. This paradigm results in a lack of fl"
D18-1186,E17-1002,0,0.0301943,"Missing"
D18-1186,D16-1244,0,0.0609064,"Missing"
D18-1186,D14-1162,0,0.080918,"layer or another stacked interaction layer. The interaction layers can be stacked for Nx times to capture the complicated matching information. 4.3 SNLI MultiNLI1 MultiNLI2 Quora J (θ) = − 1X log p(t(i) |x(i) , y (i) )+λ||θ||22 , (32) N i where θ represents all the connection weights. We use the Adam optimizer Kingma and Ba (2014) with an initial learning rate of 0.0004. Default L2 regularization λ is set to 10−6 . To avoid overfitting, dropout is applied after each fully connected, recurrent or convolutional layer. Initialization We take advantage of pre-trained word embeddings such as Glove Pennington et al. (2014) to transfer more knowledge from vast unlabeled data. For the words that don’t appear in Glove, we randomly initialize their embeddings from a normal distribution with mean 0.0 and standard deviation 0.1. The network weights are initialized with Xavier normalization Glorot and Bengio (2010) to maintain the variance of activations throughout the forward and backward passes. Biases are uniformly set to zero when the network is constructed. 5.1 Datasets To make quantitative evaluation, our model was evaluated on three well known datasets: Stanford Natural Language Inference dataset (SNLI), MultiN"
D18-1186,C16-1270,0,0.0387673,"Missing"
D18-1186,N16-1170,0,0.0201411,"ion exists between phrases {playing a violin vs. playing an instrument}, instead of the same words. The interaction layer connects playing in Premise to Hypothesis instrument, and connects playing in Hypothesis to Premise violin. Thus, the correlation between instrument in Hypothesis and violin in Premise are boosted, as we know these are important to reasoning. 6 efiting from the development of deep learning and the availability of large-scale annotated datasets, deep neural models have achieved great success. Rockt¨aschel et al. (2015) firstly use LSTM with attention for text matching task. Wang and Jiang (2016) use word-by-word attention to exploit the word-level match. Parikh et al. (2016) propose a new framework to model the relationship between two sentences using interact-compare-aggregate architecture. Chen et al. (2017a) incorporates the chain LSTM and tree LSTM jointly. Zhiguo Wang (2017) use self-attention mechanism to capture contextual information from the whole sentence. Unlike the above models, we use an alternative model to capture the complicate interaction information of two sentences. Another thread of work is the idea of using one network to generate the parameters of another networ"
D18-1186,Q16-1019,0,0.0461876,"Missing"
D18-1186,C14-1220,0,0.106859,"first briefly introduce how the convolution works over text sequence, then describe the proposed model and its connection to attention model. 3.1 Convolution over Sequence Convolution is an effective operation in deep neural networks, which convolves the input with a set of filters to extract non-linear compositional features. Although originally designed for computer vision, convolutional models have subsequently shown to be effective for NLP and have achieved excellent performance in sentence modeling Kim (2014); Kalchbrenner et al. (2014), and other traditional NLP tasks Hu et al. (2014); Zeng et al. (2014); Gehring et al. (2017). Given a sentence representation X = d×m [x1 , x2 , · · · , xm ] ∈ R , a convolutional filter W (f ) ∈ Rd×kd , the convolution process is defined as where w, W1 , W2 and b are learnable parameters. 1577   x0i = f W (f ) [xi−[k/2] , · · · , xi+[k/2] ] + b(f ) , (8) where τ is the width of filter, FGN(·) is the filter generation network. A detailed implementation of FGN is presented in Section 3.4. Now we can convolve the two sentences with the generated filters. Convolutional Interaction Network Wx Filter Generation Network ¯ = f (W (f ) ⊗ X) ∈ Rd×m , X y Y¯ = f (Wx(f"
D18-1275,R13-1026,0,0.0167791,"tter NPSChat ARK-Twitter #Train 10652 40497 26594 #Dev 2242 - #Test 2291 4500 7707 Table 1: The statistics of the datasets used in our experiments, where # represents the number of tokens in datasets. where zi is the one-hot vector of the POS tagging label corresponding to xi . zˆi is the output of the top softmax layer: zˆi = sof tmax(M LP (hi )). 3 Experimental Setup In this section, we will first detail the datasets we used. Then, we will describe several baseline methods, including a number of classic taggers and a series of deep learning sequence labeling methods. 3.1 Datasets Following (Derczynski et al., 2013), we use RIT-Twitter (Ritter et al., 2011) as our main dataset. The RIT-Twitter was split into training, development and evaluation sets (RIT-Train, RITDev, RIT-Test). The splitting method was shown in (Derczynski et al., 2013). In order to verify the validity of our model, we also tested it on two more datasets, NPSChat (Forsythand and Martell, 2007), and ARK-Twitter (Gimpel et al., 2011) using standard splits. The tag-sets of the RIT-Twitter and NPSChat are PTB-like, while that of the ARK-Twitter is specific. In order to use WSJ labeled data in experiments on ARKTwitter, we performed the map"
D18-1275,J93-2004,0,0.0606697,"-Twitter and NPSChat are PTB-like, while that of the ARK-Twitter is specific. In order to use WSJ labeled data in experiments on ARKTwitter, we performed the mapping from PTB tagsets to ARK tag-sets, according to the PTB POS Tagging Guidelines (Santorini, 1990) and ARK Guidelines1 . The mapping proceeded from fine to coarse. For pretraining the word embedding, we constructed a dataset containing 30 million tweets, from Twitter using its API. We introduced a newswire dataset containing 1173K tokens as the written language dataset, namely the Wall Street Journal (WSJ) from the Penn TreeBank v3 (Marcus et al., 1993). During training, we mixed each of RIT-Twitter, NPSChat and ARKTwitter with WSJ into three kinds of training data. The detailed data statistics of the above datasets used in this work are listed in Table 1. i 2543 1 http://www.ark.cs.cmu.edu/TweetNLP/ 3.2 3.3 Competitor Methods We applied several classic and state-of-the-art methods for comparison. In addition, we used a series of deep learning sequence labeling methods as baselines for comparison, as follows: Stanford POS Tagger is a widely used partof-speech taggers described in (Toutanova et al., 2003). It demonstrates the broad use of fea"
D18-1275,N13-1039,0,0.425412,"Missing"
D18-1275,D11-1141,0,0.21874,"594 #Dev 2242 - #Test 2291 4500 7707 Table 1: The statistics of the datasets used in our experiments, where # represents the number of tokens in datasets. where zi is the one-hot vector of the POS tagging label corresponding to xi . zˆi is the output of the top softmax layer: zˆi = sof tmax(M LP (hi )). 3 Experimental Setup In this section, we will first detail the datasets we used. Then, we will describe several baseline methods, including a number of classic taggers and a series of deep learning sequence labeling methods. 3.1 Datasets Following (Derczynski et al., 2013), we use RIT-Twitter (Ritter et al., 2011) as our main dataset. The RIT-Twitter was split into training, development and evaluation sets (RIT-Train, RITDev, RIT-Test). The splitting method was shown in (Derczynski et al., 2013). In order to verify the validity of our model, we also tested it on two more datasets, NPSChat (Forsythand and Martell, 2007), and ARK-Twitter (Gimpel et al., 2011) using standard splits. The tag-sets of the RIT-Twitter and NPSChat are PTB-like, while that of the ARK-Twitter is specific. In order to use WSJ labeled data in experiments on ARKTwitter, we performed the mapping from PTB tagsets to ARK tag-sets, acc"
D18-1275,N03-1033,0,0.0360337,"Missing"
D19-1096,N13-1006,0,0.385532,"rent domains and the baseline methods applied for comparison. We also detail the hyperparameter configuration of the proposed model. Our codes and datasets can be found at https: //github.com/RowitZou/LGN. 4.1 Data We conducted experiments on four Chinese NER datasets. (1) OntoNotes 4.0 (Weischedel et al., 2011): OntoNotes is a manually annotated multilingual corpus in the news domain that contains various text annotations, including Chinese named entity labels. Gold-standard segmentation is available. We only use Chinese documents (about 16k sentences) and process the data in the same way as Che et al. (2013). (2) MSRA (Levow, 2006): MSRA is also a dataset in the news domain and contains three types of named entities: LOC, PER, and ORG. Chinese word segmentation is available in the training set but not in the test set. (3) Weibo NER (Peng and Dredze, 2015): It consists of annotated NER messages drawn from the social media Sina Weibo2 . The corpus contains PER, ORG, GPE, and LOC for both named entity and nominal mention. (4) Resume NER (Zhang and Yang, 2018): It is composed of resumes collected from Sina Finance3 and is annotated with 8 types of named entities. Both Weibo and Resume datasets do not"
D19-1096,W06-0130,0,0.804173,"Missing"
D19-1096,D15-1141,1,0.763206,"Missing"
D19-1096,P15-1017,0,0.443298,"equence 河流 River Figure 1: Example of word character lattice with partial input. Because of the characteristic of chain structure, RNN-based methods must predict the label “度” using only previous partial sequences “印度 (India)”, which may suffer from word ambiguities without global sentence semantics. The task of named entity recognition (NER) involves determining entity boundaries and recognizing categories of named entities, which is a fundamental task in the field of natural language processing (NLP). NER plays an important role in many downstream NLP tasks, including information retrieval (Chen et al., 2015b), relation extraction (Bunescu and Mooney, 2005), question answering systems (Diefenbach et al., 2018), and other applications. Compared with English NER, Chinese named entities are more difficult to identify due to their uncertain boundaries, complex composition, and NE definitions within the nest (Duan and Zheng, 2011). One intuitive way to alleviate word boundary problems is to first perform word segmentation Equal contribution. M-LOC ? Yin Introduction ∗ B-LOC E-GPE B-GPE and then apply word sequence labeling (Yang et al., 2016; He and Sun, 2017). However, the rare gold-standard segmenta"
D19-1096,N19-1423,0,0.052104,"Missing"
D19-1096,li-etal-2014-comparison,0,0.377738,"Missing"
D19-1096,L16-1138,0,0.203827,"Missing"
D19-1096,D15-1104,0,0.0258868,"vely aggregating mechanism; 3) several experimental results demonstrate the effectiveness of the proposed method in different aspects. 2 2.1 Related Work Chinese NER with Lexicon. Some previous Chinese NER researches have compared word-based and character-based methods (Li et al., 2014) and show that due to the limited performance of the current Chinese word segmentation, character-based name taggers can outperform their word-based counterparts (He and Wang, 2008; Liu et al., 2010). Lexicon features have been widely used to better leverage word information for Chinese NER (Huang et al., 2015; Luo et al., 2015; Gui et al., 2019). Especially, Zhang and Yang (2018) proposed a lattice LSTM to model characters and potential words simultaneously. However, their lattice LSTM used a concatenation of independently trained left-toright and right-to-left LSTM to represent features, which was also limited (Devlin et al., 2018). In this work, we propose a novel character-based method that treats the named entities as a node classification task. The proposed method can utilize global information (both the left and the right context) (Dong et al., 2019) to tackle word ambiguities. 2.2 Graph Neural Networks on Te"
D19-1096,I08-4022,0,0.0863472,"on task; 2) the proposed model can capture global context information and local compositions to tackle Chinese word ambiguity problems through recursively aggregating mechanism; 3) several experimental results demonstrate the effectiveness of the proposed method in different aspects. 2 2.1 Related Work Chinese NER with Lexicon. Some previous Chinese NER researches have compared word-based and character-based methods (Li et al., 2014) and show that due to the limited performance of the current Chinese word segmentation, character-based name taggers can outperform their word-based counterparts (He and Wang, 2008; Liu et al., 2010). Lexicon features have been widely used to better leverage word information for Chinese NER (Huang et al., 2015; Luo et al., 2015; Gui et al., 2019). Especially, Zhang and Yang (2018) proposed a lattice LSTM to model characters and potential words simultaneously. However, their lattice LSTM used a concatenation of independently trained left-toright and right-to-left LSTM to represent features, which was also limited (Devlin et al., 2018). In this work, we propose a novel character-based method that treats the named entities as a node classification task. The proposed method"
D19-1096,D14-1181,0,0.00888242,"Missing"
D19-1096,W06-0115,0,0.819159,"methods applied for comparison. We also detail the hyperparameter configuration of the proposed model. Our codes and datasets can be found at https: //github.com/RowitZou/LGN. 4.1 Data We conducted experiments on four Chinese NER datasets. (1) OntoNotes 4.0 (Weischedel et al., 2011): OntoNotes is a manually annotated multilingual corpus in the news domain that contains various text annotations, including Chinese named entity labels. Gold-standard segmentation is available. We only use Chinese documents (about 16k sentences) and process the data in the same way as Che et al. (2013). (2) MSRA (Levow, 2006): MSRA is also a dataset in the news domain and contains three types of named entities: LOC, PER, and ORG. Chinese word segmentation is available in the training set but not in the test set. (3) Weibo NER (Peng and Dredze, 2015): It consists of annotated NER messages drawn from the social media Sina Weibo2 . The corpus contains PER, ORG, GPE, and LOC for both named entity and nominal mention. (4) Resume NER (Zhang and Yang, 2018): It is composed of resumes collected from Sina Finance3 and is annotated with 8 types of named entities. Both Weibo and Resume datasets do not contain the gold-standa"
D19-1096,W14-1609,0,0.0468043,"nest (Duan and Zheng, 2011). One intuitive way to alleviate word boundary problems is to first perform word segmentation Equal contribution. M-LOC ? Yin Introduction ∗ B-LOC E-GPE B-GPE and then apply word sequence labeling (Yang et al., 2016; He and Sun, 2017). However, the rare gold-standard segmentation in NER datasets and incorrectly segmented entity boundaries both negatively impact the identification of named entities (Peng and Dredze, 2015; He and Sun, 2016). Hence, character-level Chinese NER using lexicon features to better leverage word information has attracted research attention (Passos et al., 2014; Zhang and Yang, 2018). In particular, Zhang and Yang (2018) introduced a variant of a long short-term memory network (latticestructured LSTM) that encodes all potential words matching a sentence to exploit explicit word information, achieving state-of-the-art results. However, these methods are usually based on RNN or CRF to sequentially encode a sentence, while the underlying structure of language is not strictly sequential (Shen et al., 2019). As a result, these models would encounter serious word ambiguity problems (Mich et al., 2000). Especially in Chinese texts, the recognition of named"
D19-1096,D15-1064,0,0.488836,"ns. Compared with English NER, Chinese named entities are more difficult to identify due to their uncertain boundaries, complex composition, and NE definitions within the nest (Duan and Zheng, 2011). One intuitive way to alleviate word boundary problems is to first perform word segmentation Equal contribution. M-LOC ? Yin Introduction ∗ B-LOC E-GPE B-GPE and then apply word sequence labeling (Yang et al., 2016; He and Sun, 2017). However, the rare gold-standard segmentation in NER datasets and incorrectly segmented entity boundaries both negatively impact the identification of named entities (Peng and Dredze, 2015; He and Sun, 2016). Hence, character-level Chinese NER using lexicon features to better leverage word information has attracted research attention (Passos et al., 2014; Zhang and Yang, 2018). In particular, Zhang and Yang (2018) introduced a variant of a long short-term memory network (latticestructured LSTM) that encodes all potential words matching a sentence to exploit explicit word information, achieving state-of-the-art results. However, these methods are usually based on RNN or CRF to sequentially encode a sentence, while the underlying structure of language is not strictly sequential ("
D19-1096,W06-0126,0,0.781484,"Missing"
D19-1096,P18-1030,0,0.0335756,"Missing"
D19-1096,P18-1144,0,0.484748,", 2011). One intuitive way to alleviate word boundary problems is to first perform word segmentation Equal contribution. M-LOC ? Yin Introduction ∗ B-LOC E-GPE B-GPE and then apply word sequence labeling (Yang et al., 2016; He and Sun, 2017). However, the rare gold-standard segmentation in NER datasets and incorrectly segmented entity boundaries both negatively impact the identification of named entities (Peng and Dredze, 2015; He and Sun, 2016). Hence, character-level Chinese NER using lexicon features to better leverage word information has attracted research attention (Passos et al., 2014; Zhang and Yang, 2018). In particular, Zhang and Yang (2018) introduced a variant of a long short-term memory network (latticestructured LSTM) that encodes all potential words matching a sentence to exploit explicit word information, achieving state-of-the-art results. However, these methods are usually based on RNN or CRF to sequentially encode a sentence, while the underlying structure of language is not strictly sequential (Shen et al., 2019). As a result, these models would encounter serious word ambiguity problems (Mich et al., 2000). Especially in Chinese texts, the recognition of named entities with overlapp"
D19-1096,D18-1244,0,0.383117,"er, their lattice LSTM used a concatenation of independently trained left-toright and right-to-left LSTM to represent features, which was also limited (Devlin et al., 2018). In this work, we propose a novel character-based method that treats the named entities as a node classification task. The proposed method can utilize global information (both the left and the right context) (Dong et al., 2019) to tackle word ambiguities. 2.2 Graph Neural Networks on Texts Graph neural networks have been successfully applied to several text classification tasks (Veliˇckovi´c et al., 2017; Yao et al., 2018; Zhang et al., 2018b). Peng et al. (2018) proposed a GCNbased deep learning model for text classification. Zhang et al. (2018c) proposed using the dependency parse trees to construct a graph for relation extraction. Recently, multi-head attention mechanisms (Vaswani et al., 2017) have been widely used by graph neural networks during the fusion process (Zhang et al., 2018a; Lee et al., 2018), which can aggregate graph information by assigning different weights to neighboring nodes or associated edges. Given a set of vectors H ∈ ˆ ∈ R1×d , and a set of Rn×d , a query vector q trainable parameters W, this mechanism"
D19-1096,W03-1728,0,0.122763,"hese methods are based on character sequences. We applied the bidirectional LSTM (Hochreiter and Schmidhuber, 1997) and CNN (Kim, 2014) as classic baseline methods. Character-level methods + bichar + softword: Character bigrams are useful for capturing adjacent features and representing characters. We concatenated bigram embeddings with character embeddings to better leverage the bigram information. In addition, we added the segmentation information by incorporating segmentation label embeddings into the character representation. The BMES scheme is used for representing the word segmentation (Xue and Shen, 2003). Word-level methods: For the datasets with gold segmentation, we directly employed wordlevel NER methods to evaluate the performance, which are denoted as Gold seg. Otherwise, we first used open source segmentation toolkit6 to automatically segment the datasets. Then wordlevel NER methods are applied, which are denoted as Auto seg. The bi-directional LSTM and CNN are also applied as baselines. Word-level methods + char + bichar: For characters in the subsequence wb,e , we first used a bi-directional LSTM to learn their hidden states and bigram states. We then augmented the wordlevel methods w"
D19-1271,D15-1075,0,0.670997,"relationships between the two sentences include entailment (if the premise is true, then the hypothesis must be true), contradiction (if the premise is true, then the hypothesis must be false), and neutral (neither entailment nor contradiction). As a core task, conventional approaches have studied various aspects of the inference prob∗ † Equal contribution. Alphabetical order of the last name. Corresponding author. lem (MacCartney and Manning, 2008; Heilman and Smith, 2010). Thanks to the release of the largest publicly available corpus - the Stanford Natural Language Inference (SNLI) corpus (Bowman et al., 2015), neural network-based models have also been successfully used for this task (Parikh et al., 2016; Chen et al., 2016; Tay et al., 2018; Duan et al., 2018). These methods typically treat the premise sentence and the hypothesis sentence equally, learn an alignment of sub-phrases in both sentences symmetrically and in parallel, and fuse local information for making a global decision at the sentence level. They all frame the inference problem as a semantic matching task and ignore the reasoning process. However, different from a simple semantic matching task, reasoning should be asynchronous and f"
D19-1271,P16-1139,0,0.0218827,"d features such as syntactic information, n-gram overlapping and so on (Bowman et al., 2015; Heilman and Smith, 2010). Benefiting from the development of deep learning and the availability of large-scale annotated datasets (Bowman et al., 2015), neural networkbased models have also been successfully used for this task. And two categories of neural networkbased models have been developed for this problem. The first set of models is sentence encodingbased and aims to find vector representation for each sentence and classifies the relation by using the concatenation of two vector representation (Bowman et al., 2016; Nangia et al., 2017; Mou et al., 2015). However, this kind of framework ignores the interaction between two sentences. The other set of models uses the cross-sentence feature or inter-sentence attention from one sentence to another, and is hence referred to as a matching-aggregation framework. Parikh et al. (2016) use attention to decompose the problem into subproblems that can be solved separately, thus making it trivially parallelizable. Chen et al. (2016) propose a state-of-the-art model for the natural language inference (NLI) task. It is a sequential model that incorporates the chain LS"
D19-1271,P18-1224,0,0.5016,"mbedding, and syntactical features as the sentence representation. The word embedding is obtained by mapping token to high dimensional vector space by pre-trained word vector (300D Glove 840B), and the word embedding is updated during training. Character-level embedding could alleviate out-of-vocabulary (OOV) problems and capture helpful morphological information. As in (Kim et al., 2016; Lee et al., 2016), we filter the character embedding with 1D convolution kernel. The character convolutional feature maps are then max pooled over the time dimension for each token to obtain a vector. As in (Chen et al., 2018), the syntactical features consist of one-hot part-of-speech (POS) tagging feature and binary exact match (EM) feature. For one sentence, the EM value is activated if the same word is found in the other sentence. Next, ADIN adopts bidirectional Long ShortTerm Memory network (Bi-LSTM) (Graves and Schmidhuber, 2005) to model the internal temporal interaction on both directions of the sentences. Consider a premise sentence p and a hypothesis sentence q, we have got their multi-level features representation. Suppose the length of p and q are m and n respectively. These multi-level features represe"
D19-1271,N10-1145,0,0.254214,"intelligence. The goal of NLI is to predict whether a premise sentence can infer another hypothesis sentence. As illustrated in Table 1, logical relationships between the two sentences include entailment (if the premise is true, then the hypothesis must be true), contradiction (if the premise is true, then the hypothesis must be false), and neutral (neither entailment nor contradiction). As a core task, conventional approaches have studied various aspects of the inference prob∗ † Equal contribution. Alphabetical order of the last name. Corresponding author. lem (MacCartney and Manning, 2008; Heilman and Smith, 2010). Thanks to the release of the largest publicly available corpus - the Stanford Natural Language Inference (SNLI) corpus (Bowman et al., 2015), neural network-based models have also been successfully used for this task (Parikh et al., 2016; Chen et al., 2016; Tay et al., 2018; Duan et al., 2018). These methods typically treat the premise sentence and the hypothesis sentence equally, learn an alignment of sub-phrases in both sentences symmetrically and in parallel, and fuse local information for making a global decision at the sentence level. They all frame the inference problem as a semantic m"
D19-1271,P16-1160,0,0.020495,"lov et al., 2013), and fasttext (Joulin et al., 2016). It can also incorporate more syntactical and lexical information into the feature vector. For ADIN, we use a concatenation of word embedding, character embedding, and syntactical features as the sentence representation. The word embedding is obtained by mapping token to high dimensional vector space by pre-trained word vector (300D Glove 840B), and the word embedding is updated during training. Character-level embedding could alleviate out-of-vocabulary (OOV) problems and capture helpful morphological information. As in (Kim et al., 2016; Lee et al., 2016), we filter the character embedding with 1D convolution kernel. The character convolutional feature maps are then max pooled over the time dimension for each token to obtain a vector. As in (Chen et al., 2018), the syntactical features consist of one-hot part-of-speech (POS) tagging feature and binary exact match (EM) feature. For one sentence, the EM value is activated if the same word is found in the other sentence. Next, ADIN adopts bidirectional Long ShortTerm Memory network (Bi-LSTM) (Graves and Schmidhuber, 2005) to model the internal temporal interaction on both directions of the senten"
D19-1271,C08-1066,0,0.0462228,"e understanding and artificial intelligence. The goal of NLI is to predict whether a premise sentence can infer another hypothesis sentence. As illustrated in Table 1, logical relationships between the two sentences include entailment (if the premise is true, then the hypothesis must be true), contradiction (if the premise is true, then the hypothesis must be false), and neutral (neither entailment nor contradiction). As a core task, conventional approaches have studied various aspects of the inference prob∗ † Equal contribution. Alphabetical order of the last name. Corresponding author. lem (MacCartney and Manning, 2008; Heilman and Smith, 2010). Thanks to the release of the largest publicly available corpus - the Stanford Natural Language Inference (SNLI) corpus (Bowman et al., 2015), neural network-based models have also been successfully used for this task (Parikh et al., 2016; Chen et al., 2016; Tay et al., 2018; Duan et al., 2018). These methods typically treat the premise sentence and the hypothesis sentence equally, learn an alignment of sub-phrases in both sentences symmetrically and in parallel, and fuse local information for making a global decision at the sentence level. They all frame the inferen"
D19-1271,W17-5301,0,0.0162782,"ntactic information, n-gram overlapping and so on (Bowman et al., 2015; Heilman and Smith, 2010). Benefiting from the development of deep learning and the availability of large-scale annotated datasets (Bowman et al., 2015), neural networkbased models have also been successfully used for this task. And two categories of neural networkbased models have been developed for this problem. The first set of models is sentence encodingbased and aims to find vector representation for each sentence and classifies the relation by using the concatenation of two vector representation (Bowman et al., 2016; Nangia et al., 2017; Mou et al., 2015). However, this kind of framework ignores the interaction between two sentences. The other set of models uses the cross-sentence feature or inter-sentence attention from one sentence to another, and is hence referred to as a matching-aggregation framework. Parikh et al. (2016) use attention to decompose the problem into subproblems that can be solved separately, thus making it trivially parallelizable. Chen et al. (2016) propose a state-of-the-art model for the natural language inference (NLI) task. It is a sequential model that incorporates the chain LSTM and the tree LSTM"
D19-1271,D16-1244,0,0.164459,"Missing"
D19-1271,D14-1162,0,0.0816097,"or sentence b: ˜ b = (h ˜b , h ˜b , ..., h ˜bn ), H 1 2 ˆ b = LayerN orm(H ˜ b ), H (6) (7) Where LayerN orm(.) is layer normalization (Ba ˆ b is a 2D-tensor that has et al., 2016). The result H the same shape as Hb , and we refer to the whole inferential module as: The information representation layer converts each word or phrase in the sentences into a vector representation and constructs the representation matrix for the sentences. We combine the multi-level features as the sentence representation. Each token is represented as a vector by using the pre-trained word embedding such as GloVe (Pennington et al., 2014), word2Vec (Mikolov et al., 2013), and fasttext (Joulin et al., 2016). It can also incorporate more syntactical and lexical information into the feature vector. For ADIN, we use a concatenation of word embedding, character embedding, and syntactical features as the sentence representation. The word embedding is obtained by mapping token to high dimensional vector space by pre-trained word vector (300D Glove 840B), and the word embedding is updated during training. Character-level embedding could alleviate out-of-vocabulary (OOV) problems and capture helpful morphological information. As in (Ki"
D19-1271,D18-1185,0,0.522206,"Missing"
D19-1355,W16-5307,0,0.245323,"Missing"
D19-1355,D18-1170,0,0.796047,"ord expert supervised WSD methods perform better, they are less flexible than knowledge-based methods in the all-words WSD task (Raganato et al., 2017a). Recent neural-based methods are devoted to dealing with this problem. K˚ageb¨ack and Salomonsson (2016) present a supervised classifier based on Bi-LSTM, which shares parameters among all word types except the last layer. Raganato et al. (2017a) convert WSD task to a sequence labeling task, thus building a unified model for all polysemous words. However, neither of them can totally beat the best word expert supervised methods. More recently, Luo et al. (2018b) propose to leverage the gloss information from WordNet and model the semantic relationship between the context and gloss in an improved memory network. Similarly, Luo et al. (2018a) introduce a (hierarchical) co-attention mechanism to generate co-dependent representations for the context and gloss. Their attempts prove that incorporating gloss knowledge into supervised WSD approach is helpful, but they still have not achieved much improvement, because they may not make full use of gloss knowledge. In this paper, we focus on how to better leverage gloss information in a supervised neural WSD"
D19-1355,P18-1230,0,0.352773,"Missing"
D19-1355,Q14-1019,0,0.507136,"a particular context (Navigli, 2009). Previous WSD approaches can be grouped into two main categories: knowledge-based and supervised methods. Knowledge-based WSD methods rely on lexical resources like WordNet (Miller, 1995) and usually exploit two kinds of lexical knowledge. The gloss, which defines a word sense meaning, is first utilized in Lesk algorithm (Lesk, 1986) and then widely taken into account in many other approaches (Banerjee and Pedersen, 2002; Basile et al., 2014). Besides, structural properties of semantic graphs are mainly used in graph-based algorithms (Agirre et al., 2014; Moro et al., 2014). Traditional supervised WSD methods (Zhong and Ng, 2010; Shen et al., 2013; Iacobacci et al., ∗ Corresponding author. 2016) focus on extracting manually designed features and then train a dedicated classifier (word expert) for every target lemma. Although word expert supervised WSD methods perform better, they are less flexible than knowledge-based methods in the all-words WSD task (Raganato et al., 2017a). Recent neural-based methods are devoted to dealing with this problem. K˚ageb¨ack and Salomonsson (2016) present a supervised classifier based on Bi-LSTM, which shares parameters among all"
D19-1355,C14-1151,0,0.604972,"mental task and long-standing challenge in Natural Language Processing (NLP), which aims to find the exact sense of an ambiguous word in a particular context (Navigli, 2009). Previous WSD approaches can be grouped into two main categories: knowledge-based and supervised methods. Knowledge-based WSD methods rely on lexical resources like WordNet (Miller, 1995) and usually exploit two kinds of lexical knowledge. The gloss, which defines a word sense meaning, is first utilized in Lesk algorithm (Lesk, 1986) and then widely taken into account in many other approaches (Banerjee and Pedersen, 2002; Basile et al., 2014). Besides, structural properties of semantic graphs are mainly used in graph-based algorithms (Agirre et al., 2014; Moro et al., 2014). Traditional supervised WSD methods (Zhong and Ng, 2010; Shen et al., 2013; Iacobacci et al., ∗ Corresponding author. 2016) focus on extracting manually designed features and then train a dedicated classifier (word expert) for every target lemma. Although word expert supervised WSD methods perform better, they are less flexible than knowledge-based methods in the all-words WSD task (Raganato et al., 2017a). Recent neural-based methods are devoted to dealing wit"
D19-1355,P16-1085,0,0.402809,"optimal settings for our experiments. We keep the dropout probability at 0.1, set the number of epochs to 4. The initial learning rate is 2e-5, and the batch size is 64. 3.3 GlossBERT(Sent-CLS-WS) We use contextgloss pairs with weak supervision as input. We take the final hidden state of the first token [CLS] and add a classification layer (label ∈ {yes, no}), which weekly highlight the target word by the weak supervision. 3 3.1 Experiments Datasets The statistics of the WSD datasets are shown in Table 2. Training Dataset Following previous work (Luo et al., 2018a,b; Raganato et al., 2017a,b; Iacobacci et al., 2016; Zhong and Ng, 2010), we Settings Results Table 3 shows the performance of our method on the English all-words WSD benchmark datasets. We compare our approach with previous methods. The first block shows the MFS baseline, which selects the most frequent sense in the training corpus for each target word. The second block shows two knowledge-based systems. Leskext+emb (Basile et al., 2014) is a variant of Lesk algorithm (Lesk, 1986) by calculating the gloss-context overlap of the target word. Babelfy (Moro et al., 2014) is a unified graphbased approach which exploits the semantic network struct"
D19-1355,N18-1202,0,0.03381,"l the semantic relationship between the context and gloss in an improved memory network. Similarly, Luo et al. (2018a) introduce a (hierarchical) co-attention mechanism to generate co-dependent representations for the context and gloss. Their attempts prove that incorporating gloss knowledge into supervised WSD approach is helpful, but they still have not achieved much improvement, because they may not make full use of gloss knowledge. In this paper, we focus on how to better leverage gloss information in a supervised neural WSD system. Recently, the pre-trained language models, such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2018), have shown their effectiveness to alleviate the effort of feature engineering. Especially, BERT has achieved excellent results in question answering (QA) and natural language inference (NLI). We construct context-gloss pairs from glosses of all possible senses (in WordNet) of the target word, thus treating WSD task as a sentence-pair classification problem. We fine3509 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 3509–3514, c Hong Kong, Chi"
D19-1355,D17-1120,0,0.545467,"unt in many other approaches (Banerjee and Pedersen, 2002; Basile et al., 2014). Besides, structural properties of semantic graphs are mainly used in graph-based algorithms (Agirre et al., 2014; Moro et al., 2014). Traditional supervised WSD methods (Zhong and Ng, 2010; Shen et al., 2013; Iacobacci et al., ∗ Corresponding author. 2016) focus on extracting manually designed features and then train a dedicated classifier (word expert) for every target lemma. Although word expert supervised WSD methods perform better, they are less flexible than knowledge-based methods in the all-words WSD task (Raganato et al., 2017a). Recent neural-based methods are devoted to dealing with this problem. K˚ageb¨ack and Salomonsson (2016) present a supervised classifier based on Bi-LSTM, which shares parameters among all word types except the last layer. Raganato et al. (2017a) convert WSD task to a sequence labeling task, thus building a unified model for all polysemous words. However, neither of them can totally beat the best word expert supervised methods. More recently, Luo et al. (2018b) propose to leverage the gloss information from WordNet and model the semantic relationship between the context and gloss in an impr"
D19-1355,E17-1010,0,0.562265,"unt in many other approaches (Banerjee and Pedersen, 2002; Basile et al., 2014). Besides, structural properties of semantic graphs are mainly used in graph-based algorithms (Agirre et al., 2014; Moro et al., 2014). Traditional supervised WSD methods (Zhong and Ng, 2010; Shen et al., 2013; Iacobacci et al., ∗ Corresponding author. 2016) focus on extracting manually designed features and then train a dedicated classifier (word expert) for every target lemma. Although word expert supervised WSD methods perform better, they are less flexible than knowledge-based methods in the all-words WSD task (Raganato et al., 2017a). Recent neural-based methods are devoted to dealing with this problem. K˚ageb¨ack and Salomonsson (2016) present a supervised classifier based on Bi-LSTM, which shares parameters among all word types except the last layer. Raganato et al. (2017a) convert WSD task to a sequence labeling task, thus building a unified model for all polysemous words. However, neither of them can totally beat the best word expert supervised methods. More recently, Luo et al. (2018b) propose to leverage the gloss information from WordNet and model the semantic relationship between the context and gloss in an impr"
D19-1355,S13-1003,0,0.0758858,"d into two main categories: knowledge-based and supervised methods. Knowledge-based WSD methods rely on lexical resources like WordNet (Miller, 1995) and usually exploit two kinds of lexical knowledge. The gloss, which defines a word sense meaning, is first utilized in Lesk algorithm (Lesk, 1986) and then widely taken into account in many other approaches (Banerjee and Pedersen, 2002; Basile et al., 2014). Besides, structural properties of semantic graphs are mainly used in graph-based algorithms (Agirre et al., 2014; Moro et al., 2014). Traditional supervised WSD methods (Zhong and Ng, 2010; Shen et al., 2013; Iacobacci et al., ∗ Corresponding author. 2016) focus on extracting manually designed features and then train a dedicated classifier (word expert) for every target lemma. Although word expert supervised WSD methods perform better, they are less flexible than knowledge-based methods in the all-words WSD task (Raganato et al., 2017a). Recent neural-based methods are devoted to dealing with this problem. K˚ageb¨ack and Salomonsson (2016) present a supervised classifier based on Bi-LSTM, which shares parameters among all word types except the last layer. Raganato et al. (2017a) convert WSD task"
D19-1355,N19-1035,1,0.758242,"air classification problem. We describe our construction method with an example (See Table 1). There are four targets in this sentence, and here we take target word research as an example: Context-Gloss Pairs The sentence containing target words is denoted as context sentence. For each target word, we extract glosses of all N possible senses (here N = 4) of the target word (research) in WordNet to obtain the gloss sentence. [CLS] and [SEP] marks are added to the context-gloss pairs to make it suitable for the input of BERT model. A similar idea is also used in aspect-based sentiment analysis (Sun et al., 2019). Context-Gloss Pairs with Weak Supervision Based on the previous construction method, we add weak supervised signals to the context-gloss 3510 Dataset SemCor SE2 SE3 SE07 SE13 SE15 Total 226036 2282 1850 455 1644 1022 Noun 87002 1066 900 159 1644 531 Verb 88334 517 588 296 0 251 Adj 31753 445 350 0 0 160 Adv 18947 254 12 0 0 80 choose SemCor3.0 as training corpus, which is the largest corpus manually annotated with WordNet sense for WSD. Table 2: Statistics of the different parts of speech annotations in English all-words WSD datasets. pairs (see the highlighted part in Table 1). The signal i"
D19-1355,P10-4014,0,0.730179,"oaches can be grouped into two main categories: knowledge-based and supervised methods. Knowledge-based WSD methods rely on lexical resources like WordNet (Miller, 1995) and usually exploit two kinds of lexical knowledge. The gloss, which defines a word sense meaning, is first utilized in Lesk algorithm (Lesk, 1986) and then widely taken into account in many other approaches (Banerjee and Pedersen, 2002; Basile et al., 2014). Besides, structural properties of semantic graphs are mainly used in graph-based algorithms (Agirre et al., 2014; Moro et al., 2014). Traditional supervised WSD methods (Zhong and Ng, 2010; Shen et al., 2013; Iacobacci et al., ∗ Corresponding author. 2016) focus on extracting manually designed features and then train a dedicated classifier (word expert) for every target lemma. Although word expert supervised WSD methods perform better, they are less flexible than knowledge-based methods in the all-words WSD task (Raganato et al., 2017a). Recent neural-based methods are devoted to dealing with this problem. K˚ageb¨ack and Salomonsson (2016) present a supervised classifier based on Bi-LSTM, which shares parameters among all word types except the last layer. Raganato et al. (2017a"
D19-1355,J14-1003,0,\N,Missing
D19-5410,D18-1443,0,0.026602,"xample, we rethink the process of the model design based on the experience of the above analysis. We demonstrate that when we have a deep understanding of the characteristics of datasets, a simple approach can bring significant improvements to the existing stateof-the-art model. 1 Introduction Neural network-based models have achieved great success on summarization tasks (See et al., 2017; Celikyilmaz et al., 2018; Jadhav and Rajan, 2018). Current studies on summarization either explore the possibility of optimization in terms of networks’ structures (Zhou et al., 2018; Chen and Bansal, 2018; Gehrmann et al., 2018), the improvement in terms of training schemas (Wang et al., 2019; Narayan et al., 2018; Wu and Hu, 2018; Chen and Bansal, 2018), or the information fusion with large pre-trained knowledge (Peters et al., 2018; Devlin et al., 2018; Liu, 2019; Dong et al., 2019). More recently, Zhong et al. (2019) conducts a comprehensive analysis on why existing summarization systems perform so well from above three aspects. Despite their success, a relatively missing topic1 Main Contributions 1) For the summarization task itself, we diagnose the weakness of existing learning methods in terms of networks’ stru"
D19-5410,N18-1065,0,0.0832705,"Missing"
D19-5410,P17-1044,0,0.0353276,"ng of the characteristics for the datasets and the increasing development of above three learning methods. Architecture Designs Architecturally speaking, most of existing extractive summarization systems consists of three major modules: sentence encoder, document encoder and decoder. In this paper, our architectural choices vary with two types of document encoders: LSTM2 (Hochreiter and Schmidhuber, 1997) and Transformer (Vaswani et al., 2017) while we keep the sentence encoder (convolutional neural networks) and decoder (sequence labeling) unchanged3 . The base 2 We use the implementation of He et al. (2017). Since they do not show significant influence on our explored experiments. 3 81 model in all experiments refers to Transformer equipped with sequence labelling. extracted, the representation of the sentence consists of two components: position representation4 , which indicates the position of the sentence in the document; content representation, which contains the semantic information of the sentence. Therefore, we define the position and content information of the sentence as constituent factors, aiming to explore how the selected sentences in the test set relate to the training set in terms"
D19-5410,N18-1150,0,0.271935,"riors residing in datasets and model designs, analyzing how different properties of datasets influence the choices of model structure design and training methods. Finally, by taking a typical dataset as an example, we rethink the process of the model design based on the experience of the above analysis. We demonstrate that when we have a deep understanding of the characteristics of datasets, a simple approach can bring significant improvements to the existing stateof-the-art model. 1 Introduction Neural network-based models have achieved great success on summarization tasks (See et al., 2017; Celikyilmaz et al., 2018; Jadhav and Rajan, 2018). Current studies on summarization either explore the possibility of optimization in terms of networks’ structures (Zhou et al., 2018; Chen and Bansal, 2018; Gehrmann et al., 2018), the improvement in terms of training schemas (Wang et al., 2019; Narayan et al., 2018; Wu and Hu, 2018; Chen and Bansal, 2018), or the information fusion with large pre-trained knowledge (Peters et al., 2018; Devlin et al., 2018; Liu, 2019; Dong et al., 2019). More recently, Zhong et al. (2019) conducts a comprehensive analysis on why existing summarization systems perform so well from abov"
D19-5410,P18-1014,0,0.381999,"and model designs, analyzing how different properties of datasets influence the choices of model structure design and training methods. Finally, by taking a typical dataset as an example, we rethink the process of the model design based on the experience of the above analysis. We demonstrate that when we have a deep understanding of the characteristics of datasets, a simple approach can bring significant improvements to the existing stateof-the-art model. 1 Introduction Neural network-based models have achieved great success on summarization tasks (See et al., 2017; Celikyilmaz et al., 2018; Jadhav and Rajan, 2018). Current studies on summarization either explore the possibility of optimization in terms of networks’ structures (Zhou et al., 2018; Chen and Bansal, 2018; Gehrmann et al., 2018), the improvement in terms of training schemas (Wang et al., 2019; Narayan et al., 2018; Wu and Hu, 2018; Chen and Bansal, 2018), or the information fusion with large pre-trained knowledge (Peters et al., 2018; Devlin et al., 2018; Liu, 2019; Dong et al., 2019). More recently, Zhong et al. (2019) conducts a comprehensive analysis on why existing summarization systems perform so well from above three aspects. Despite"
D19-5410,D19-1327,0,0.230067,"al., 2018; Devlin et al., 2018; Liu, 2019; Dong et al., 2019). More recently, Zhong et al. (2019) conducts a comprehensive analysis on why existing summarization systems perform so well from above three aspects. Despite their success, a relatively missing topic1 Main Contributions 1) For the summarization task itself, we diagnose the weakness of existing learning methods in terms of networks’ structures, training schemas, and pre-trained knowledge. Some observations could instruct future researchers ∗ These three authors contributed equally. Corresponding author. 1 Concurrent with our work, (Jung et al., 2019) makes a similar analysis on datasets biases and presents three factors † which matter for the text summarization task. 80 Proceedings of the 2nd Workshop on New Frontiers in Summarization, pages 80–89 c Hong Kong, China, November 4, 2019. 2019 Association for Computational Linguistics Factors of Datasets Measures Model designs Constituent[4.1] Positional coverage rate [4.1.1] Content coverage rate [4.1.2] Architecture designs [6.2] Pre-trained strategies [6.2] Style [4.2] Density [4.2.1] Compression [4.2.2] Training schemas [6.1] Table 1: Organization structure of this paper: four measures pr"
D19-5410,P18-1063,0,0.309888,"typical dataset as an example, we rethink the process of the model design based on the experience of the above analysis. We demonstrate that when we have a deep understanding of the characteristics of datasets, a simple approach can bring significant improvements to the existing stateof-the-art model. 1 Introduction Neural network-based models have achieved great success on summarization tasks (See et al., 2017; Celikyilmaz et al., 2018; Jadhav and Rajan, 2018). Current studies on summarization either explore the possibility of optimization in terms of networks’ structures (Zhou et al., 2018; Chen and Bansal, 2018; Gehrmann et al., 2018), the improvement in terms of training schemas (Wang et al., 2019; Narayan et al., 2018; Wu and Hu, 2018; Chen and Bansal, 2018), or the information fusion with large pre-trained knowledge (Peters et al., 2018; Devlin et al., 2018; Liu, 2019; Dong et al., 2019). More recently, Zhong et al. (2019) conducts a comprehensive analysis on why existing summarization systems perform so well from above three aspects. Despite their success, a relatively missing topic1 Main Contributions 1) For the summarization task itself, we diagnose the weakness of existing learning methods in"
D19-5410,P16-1046,0,0.11915,"en a dataset D, different learning methods are trying to explain the data in diverse ways, which show different generalization behaviours. Existing learning methods for extractive summarization systems vary in architectures designs, pre-trained strategies and training schemas. Neural Extractive Summarization Recently, neural network-based models have achieved great success in extractive summarization. (Celikyilmaz et al., 2018; Jadhav and Rajan, 2018; Liu, 2019). Existing works on text summarization can roughly fall into one of three classes: exploring networks’ structures with suitable bias (Cheng and Lapata, 2016; Nallapati et al., 2017; Zhou et al., 2018); introducing new training schemas (Narayan et al., 2018; Wu and Hu, 2018; Chen and Bansal, 2018) and incorporating large pre-trained knowledge (Peters et al., 2018; Devlin et al., 2018; Liu, 2019; Dong et al., 2019). Instead of exploring the possibility for a new state-of-the-art along one of above three lines, in this paper, we aim to bridge the gap between the lack of understanding of the characteristics for the datasets and the increasing development of above three learning methods. Architecture Designs Architecturally speaking, most of existing"
D19-5410,N18-2097,0,0.106766,"Missing"
D19-5410,P18-1176,0,0.0310446,"e same dataset. So the results of compression in Table 4 are in line with our expectations, how the model represents long documents to get good performance in text summarization task remains a challenge (Celikyilmaz et al., 2018). Unlike the exploration of density, we attempt to understand how the model extracts sentences when faced with different compression samples. We utilize an attribution technique called Integrated Gradients (IG) (Sundararajan et al., 2017) to separate the position and content information of each sentence. The setting of input x and baseline x’ in this paper is close to Mudrakarta et al. (2018)8 , but it is worth noting that our base model adds positional embedding to each sentence, so input x and baseline x’ both have positional information. Table 5: Experiment aboutP DENSITY , Pct denotes the percentage of ψ(si , S) to si ∈D ψ(si , S). The first three sentences contain more salient information in samples with higher density. In order to comprehend this correlation, we conduct the following experiment. Given an article and summary pair, we assign a score ψ(si , S) to each sentence in article to indicate how much sailent information is contained in the sentence. ψ(si , S) = LCS(si ,"
D19-5410,K16-1028,0,0.0983355,"Missing"
D19-5410,P18-1061,0,0.359477,"nally, by taking a typical dataset as an example, we rethink the process of the model design based on the experience of the above analysis. We demonstrate that when we have a deep understanding of the characteristics of datasets, a simple approach can bring significant improvements to the existing stateof-the-art model. 1 Introduction Neural network-based models have achieved great success on summarization tasks (See et al., 2017; Celikyilmaz et al., 2018; Jadhav and Rajan, 2018). Current studies on summarization either explore the possibility of optimization in terms of networks’ structures (Zhou et al., 2018; Chen and Bansal, 2018; Gehrmann et al., 2018), the improvement in terms of training schemas (Wang et al., 2019; Narayan et al., 2018; Wu and Hu, 2018; Chen and Bansal, 2018), or the information fusion with large pre-trained knowledge (Peters et al., 2018; Devlin et al., 2018; Liu, 2019; Dong et al., 2019). More recently, Zhong et al. (2019) conducts a comprehensive analysis on why existing summarization systems perform so well from above three aspects. Despite their success, a relatively missing topic1 Main Contributions 1) For the summarization task itself, we diagnose the weakness of exist"
D19-5410,N18-1158,0,0.292258,"nalysis. We demonstrate that when we have a deep understanding of the characteristics of datasets, a simple approach can bring significant improvements to the existing stateof-the-art model. 1 Introduction Neural network-based models have achieved great success on summarization tasks (See et al., 2017; Celikyilmaz et al., 2018; Jadhav and Rajan, 2018). Current studies on summarization either explore the possibility of optimization in terms of networks’ structures (Zhou et al., 2018; Chen and Bansal, 2018; Gehrmann et al., 2018), the improvement in terms of training schemas (Wang et al., 2019; Narayan et al., 2018; Wu and Hu, 2018; Chen and Bansal, 2018), or the information fusion with large pre-trained knowledge (Peters et al., 2018; Devlin et al., 2018; Liu, 2019; Dong et al., 2019). More recently, Zhong et al. (2019) conducts a comprehensive analysis on why existing summarization systems perform so well from above three aspects. Despite their success, a relatively missing topic1 Main Contributions 1) For the summarization task itself, we diagnose the weakness of existing learning methods in terms of networks’ structures, training schemas, and pre-trained knowledge. Some observations could instruct f"
D19-5410,N18-1202,0,0.207059,"ing significant improvements to the existing stateof-the-art model. 1 Introduction Neural network-based models have achieved great success on summarization tasks (See et al., 2017; Celikyilmaz et al., 2018; Jadhav and Rajan, 2018). Current studies on summarization either explore the possibility of optimization in terms of networks’ structures (Zhou et al., 2018; Chen and Bansal, 2018; Gehrmann et al., 2018), the improvement in terms of training schemas (Wang et al., 2019; Narayan et al., 2018; Wu and Hu, 2018; Chen and Bansal, 2018), or the information fusion with large pre-trained knowledge (Peters et al., 2018; Devlin et al., 2018; Liu, 2019; Dong et al., 2019). More recently, Zhong et al. (2019) conducts a comprehensive analysis on why existing summarization systems perform so well from above three aspects. Despite their success, a relatively missing topic1 Main Contributions 1) For the summarization task itself, we diagnose the weakness of existing learning methods in terms of networks’ structures, training schemas, and pre-trained knowledge. Some observations could instruct future researchers ∗ These three authors contributed equally. Corresponding author. 1 Concurrent with our work, (Jung et al"
D19-5410,P17-1099,0,0.102021,"nnection between priors residing in datasets and model designs, analyzing how different properties of datasets influence the choices of model structure design and training methods. Finally, by taking a typical dataset as an example, we rethink the process of the model design based on the experience of the above analysis. We demonstrate that when we have a deep understanding of the characteristics of datasets, a simple approach can bring significant improvements to the existing stateof-the-art model. 1 Introduction Neural network-based models have achieved great success on summarization tasks (See et al., 2017; Celikyilmaz et al., 2018; Jadhav and Rajan, 2018). Current studies on summarization either explore the possibility of optimization in terms of networks’ structures (Zhou et al., 2018; Chen and Bansal, 2018; Gehrmann et al., 2018), the improvement in terms of training schemas (Wang et al., 2019; Narayan et al., 2018; Wu and Hu, 2018; Chen and Bansal, 2018), or the information fusion with large pre-trained knowledge (Peters et al., 2018; Devlin et al., 2018; Liu, 2019; Dong et al., 2019). More recently, Zhong et al. (2019) conducts a comprehensive analysis on why existing summarization systems"
D19-5410,P19-1100,1,0.717017,"l network-based models have achieved great success on summarization tasks (See et al., 2017; Celikyilmaz et al., 2018; Jadhav and Rajan, 2018). Current studies on summarization either explore the possibility of optimization in terms of networks’ structures (Zhou et al., 2018; Chen and Bansal, 2018; Gehrmann et al., 2018), the improvement in terms of training schemas (Wang et al., 2019; Narayan et al., 2018; Wu and Hu, 2018; Chen and Bansal, 2018), or the information fusion with large pre-trained knowledge (Peters et al., 2018; Devlin et al., 2018; Liu, 2019; Dong et al., 2019). More recently, Zhong et al. (2019) conducts a comprehensive analysis on why existing summarization systems perform so well from above three aspects. Despite their success, a relatively missing topic1 Main Contributions 1) For the summarization task itself, we diagnose the weakness of existing learning methods in terms of networks’ structures, training schemas, and pre-trained knowledge. Some observations could instruct future researchers ∗ These three authors contributed equally. Corresponding author. 1 Concurrent with our work, (Jung et al., 2019) makes a similar analysis on datasets biases and presents three factors † which"
H05-1076,P02-1006,0,0.129436,"Missing"
H05-1076,W00-1316,0,0.055911,"Missing"
H05-1076,P99-1042,0,0.315985,"Missing"
H05-1076,W00-0603,0,\N,Missing
H05-1076,W00-0601,0,\N,Missing
I05-1044,N03-2037,0,0.0614286,"Missing"
I05-4009,C02-1150,1,0.76429,"Missing"
I05-4009,J95-4004,0,0.060867,"Missing"
I11-1008,W03-1018,0,0.0306981,"l., 2001), semantic role labeling (Toutannova et al., 2005) and syntactic parsing (Finkel et al., 2008). CRFs outperform other models like Maximum Entropy Markov models (McCallum et al., 2000), because they overcome the problem of “label bias”. 65 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 65–74, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP 2001; Sutton and McCallum, 2006) and introduce the definition of some concepts and parameters. BFGS algorithm (Liu and Nocedal, 1989) and achieve better convergence than the method introduced by Kazama and Tsujii (2003). Stochastic gradient descent (SGD) methods are another kind of L1-regularized training methods. It is a very attractive framework for it often requires much less training time than the batch training algorithm in practice. Tsuruoka et al. (2009) presented a variant of SGD that can efficiently produce compact models with L1 regularization. The main idea is to keep track of the total penalty and the penalty each weight has applied, so that the penalization smooth away the noisy gradient. Although SGD method with cumulative penalty is very efficient, it sometimes fails to converge to the optimum"
I11-1008,P10-1052,0,0.0176354,"WL-QN algorithm and SGD algorithm on the same data sets. For the purpose of run-times comparison, we implemented all the algorithm in a quite similar way, especially in feature extraction and gradient computation. For example, we compute the forward/backward scores in logarithm domain instead of scaling 2 To ensure the objective function decrease a certain value and guarantee the convergence. 69 Table 1: Feature templates for Chinese word segmentation task. (1) ci−1 yi , ci yi , ci+1 yi (2) ci−1 ci yi , ci ci+1 yi , ci−1 ci+1 yi (3) yi−1 yi method, though the latter method was claimed faster (Lavergne et al., 2010). All experiments were performed on a server with Xeon 2.66GHz. 4.1 Chinese Word Segmentation Figure 1: Bakeoff 2005 Chinese word segmentation task: Objective function with fixed α. The first set of experiments used the Chinese word segmentation corpus from the Second International SIGHAN Bakeoff data sets (Emerson, 2005), provided by Peking University. The training data consists of 19,054 sentences, 1,109,947 Chinese words, 1,826,448 Chinese characters and the testing data consists of 1,944 sentences, 104,372 Chinese words, 172,733 Chinese characters. We separated 1,000 sentences from the tra"
I11-1008,I05-3017,0,0.0342785,"function decrease a certain value and guarantee the convergence. 69 Table 1: Feature templates for Chinese word segmentation task. (1) ci−1 yi , ci yi , ci+1 yi (2) ci−1 ci yi , ci ci+1 yi , ci−1 ci+1 yi (3) yi−1 yi method, though the latter method was claimed faster (Lavergne et al., 2010). All experiments were performed on a server with Xeon 2.66GHz. 4.1 Chinese Word Segmentation Figure 1: Bakeoff 2005 Chinese word segmentation task: Objective function with fixed α. The first set of experiments used the Chinese word segmentation corpus from the Second International SIGHAN Bakeoff data sets (Emerson, 2005), provided by Peking University. The training data consists of 19,054 sentences, 1,109,947 Chinese words, 1,826,448 Chinese characters and the testing data consists of 1,944 sentences, 104,372 Chinese words, 172,733 Chinese characters. We separated 1,000 sentences from the training data and use them as the heldout data. The test data was only used for the final accuracy report. The feature templates we used in this experiment were listed in Table 1, where ci denotes the ith Chinese character in an instance, yi denotes the ith label in the instance. Based on the work of Huang and Zhao (2007), i"
I11-1008,N04-1042,0,0.0447919,"very sparse, then the size of the model will be much smaller than that produced by L2 regularization. Compact models are more interpretable, generalizable and manageable, require less resources like memory and storage. It is very meaningful especially for the rapid development of mobile application nowadays, which suffer the scarcity of resources. In many NLP tasks, the feature sets can reach the magnitude of several million. Besides, L1 regularization method can implicitly perform the feature selection, and provide the result for further process such as iterative approach (Vail et al., 2007; Peng and McCallum, 2004). This task requires that we need to train the model as accurate as possible, as to converge to the optimum. The feature selection can be regarded as reliable and unbiased after such a process. Quasi-Newtion method was successfully and efficiently used in L1-regularized model by Andrew and Gao (2007). They presented an algorithm called Orthant-Wise Limited-memory Quasi-Newton (OWL-QN), which is based on LSparse learning framework, which is very popular in the field of nature language processing recently due to the advantages of efficiency and generalizability, can be applied to Conditional Ran"
I11-1008,P08-1109,0,0.0317698,"with less training time for L1 regularization. 1 Xuanjing Huang Fudan University Shanghai, China xjhuang@fudan.edu.cn Introduction Conditional Random Fields (CRFs) (Lafferty et al., 2001; Sutton and McCallum, 2006) are one of the most widely-used machine learning approach in the field of nature language processing, for their ability to handle large feature sets and structural dependency between output labels. The applications of CRFs cover a wide range of tasks such as part-of-speech (POS) tagging (Lafferty et al., 2001), semantic role labeling (Toutannova et al., 2005) and syntactic parsing (Finkel et al., 2008). CRFs outperform other models like Maximum Entropy Markov models (McCallum et al., 2000), because they overcome the problem of “label bias”. 65 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 65–74, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP 2001; Sutton and McCallum, 2006) and introduce the definition of some concepts and parameters. BFGS algorithm (Liu and Nocedal, 1989) and achieve better convergence than the method introduced by Kazama and Tsujii (2003). Stochastic gradient descent (SGD) methods are another kind of L1-regularized"
I11-1008,P07-1104,0,0.0252279,"probabilistic of labeling result for further use as pipeline or reranking. For all types of CRFs, the maximum-likelihood method can be applied for parameter estimation, which means training the model is done by maximizing the log-likelihood on the training data. To avoid overfitting the likelihood is often penalized with the regularization term. There were two common regularization methods named L1 and L2 regularization. L1 regularization, also called Laplace prior, penalizes the weight vector with its L1-norm. L2 regularization, also called Gaussian prior, uses L2-form. Based on the work of Gao et al. (2007), there is no significant difference between these two regularization methods in terms of accuracy. But L1 regularization has a major advantage that L1-regularized training can produce models, of which the feature weights can be very sparse, then the size of the model will be much smaller than that produced by L2 regularization. Compact models are more interpretable, generalizable and manageable, require less resources like memory and storage. It is very meaningful especially for the rapid development of mobile application nowadays, which suffer the scarcity of resources. In many NLP tasks, th"
I11-1008,P05-1073,0,0.0204701,"Missing"
I11-1008,P09-1054,0,0.452642,"Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 65–74, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP 2001; Sutton and McCallum, 2006) and introduce the definition of some concepts and parameters. BFGS algorithm (Liu and Nocedal, 1989) and achieve better convergence than the method introduced by Kazama and Tsujii (2003). Stochastic gradient descent (SGD) methods are another kind of L1-regularized training methods. It is a very attractive framework for it often requires much less training time than the batch training algorithm in practice. Tsuruoka et al. (2009) presented a variant of SGD that can efficiently produce compact models with L1 regularization. The main idea is to keep track of the total penalty and the penalty each weight has applied, so that the penalization smooth away the noisy gradient. Although SGD method with cumulative penalty is very efficient, it sometimes fails to converge to the optimum, because the training process is usually terminated at a certain number of iterations without explicit stop criteria as in quasi-Newton method. Another problem is that the training result of SGD method is very sensitive to the parameter settings"
I11-1008,I08-4010,0,0.0186301,"esulting model. The fourth column shows the F score of the Chinese word segment results, which is the harmonic mean of precision P (percentage of output Chinese words that exactly match the golden standard Chinese words) and recall R (percentage of golden standard Chinese words that returned by our system). In Table 3, the second column shows the number of passes performed in the training, in our method, this value includes the the number of 4.2 Name Entity Recognition The second set of experiments used the name entity recognition corpus from the Fourth International SIGHAN Bakeoff data sets (Jin and Chen, 2008), provided by Microsoft Research Asia. The training data consists of 23,182 sentences, 1,089,050 Chinese characters and the testing data consists of 4,636 sentences, 219,197 Chinese characters. We separated 1,000 sentences from the training data and use them as the heldout data. The training data is annotated with the “IOB” tags representing name entities including person, location and organization. The feature templates we used in this experiment were listed in Table 4. Notice we did not change the label representation made by the origin 71 Table 5: Fourth SIGHAN Bakeoff name entity recogniti"
I11-1019,W03-1028,0,0.367895,"pproach on a manually labeled corpus. Experimental results demonstrate that our approach achieves better performances compared with the state-of-the-art methods. 1 Introduction Keyphrase extraction is a long studied topic in natural language processing. A keyphrase, which consists a word or a group of words, is defined as a precise and concise expression of one or more documents. It has been widely used in various applications such as summarization, clustering, categorizing, browsing, and so on. In recent years, keyphrase extraction has received much attention (Witten et al., 1999; Zha, 2002; Hulth, 2003; Tomokiyo and Hurst, 2003; Chen et al., 2005; Medelyan et al., 2009; Liu et al., 2009). Keyphrases are usually manually chosen by 165 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 165–173, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP 2 Related Work that the keyphrases of news should satisfy the following properties: As mentioned in the previous section, most of current studies on keyphrase extraction can be roughly divided into two categories: supervised and unsupervised approaches. Unsupervised approaches usually select general sets"
I11-1019,W03-1805,0,0.865611,"manually labeled corpus. Experimental results demonstrate that our approach achieves better performances compared with the state-of-the-art methods. 1 Introduction Keyphrase extraction is a long studied topic in natural language processing. A keyphrase, which consists a word or a group of words, is defined as a precise and concise expression of one or more documents. It has been widely used in various applications such as summarization, clustering, categorizing, browsing, and so on. In recent years, keyphrase extraction has received much attention (Witten et al., 1999; Zha, 2002; Hulth, 2003; Tomokiyo and Hurst, 2003; Chen et al., 2005; Medelyan et al., 2009; Liu et al., 2009). Keyphrases are usually manually chosen by 165 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 165–173, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP 2 Related Work that the keyphrases of news should satisfy the following properties: As mentioned in the previous section, most of current studies on keyphrase extraction can be roughly divided into two categories: supervised and unsupervised approaches. Unsupervised approaches usually select general sets of candidates and use a ra"
I11-1019,D09-1027,0,0.730556,"pproach achieves better performances compared with the state-of-the-art methods. 1 Introduction Keyphrase extraction is a long studied topic in natural language processing. A keyphrase, which consists a word or a group of words, is defined as a precise and concise expression of one or more documents. It has been widely used in various applications such as summarization, clustering, categorizing, browsing, and so on. In recent years, keyphrase extraction has received much attention (Witten et al., 1999; Zha, 2002; Hulth, 2003; Tomokiyo and Hurst, 2003; Chen et al., 2005; Medelyan et al., 2009; Liu et al., 2009). Keyphrases are usually manually chosen by 165 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 165–173, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP 2 Related Work that the keyphrases of news should satisfy the following properties: As mentioned in the previous section, most of current studies on keyphrase extraction can be roughly divided into two categories: supervised and unsupervised approaches. Unsupervised approaches usually select general sets of candidates and use a ranking step to select the most important candidates. For examp"
I11-1019,P09-1039,0,0.075709,"Missing"
I11-1019,P10-1058,0,0.0295274,"integer linear programming (Alevras and Padberg, 2001) can be used to incorporate both local features and non-local features, which are difficult to handle with traditional algorithms, it has received much attention in various NLP problems in recent years. Roth and Yih (2005) extended CRF models by applying inference procedure based on ILP to naturally and efficiently support general constraint structures. They applied their model on semantic role labeling (SRL) task. Martin et al. (2009) formulated the problem of nonprojective dependency parsing as a polynomial-sized integer linear program. Woodsend and Lapata (2010) presented a joint content selection and compression model for singledocument summarization using an integer linear programming formulation. 3 maximize cT x subject to 0 ≤ xi ≤ 1 3.1 Objective Function x∈Z With thePBIP formulation, objective function cT x = k ck xk denotes the expected informative scores over all the words of a solution x. Maximizing the expected scores biases the words with highest ci values as keyphrases. Various features can be considered as the values c. In this work, we use two basic features TF·IDF and locality. They have also been widely used in existing keyphrase extra"
I11-1019,D09-1137,0,0.0507347,"demonstrate that our approach achieves better performances compared with the state-of-the-art methods. 1 Introduction Keyphrase extraction is a long studied topic in natural language processing. A keyphrase, which consists a word or a group of words, is defined as a precise and concise expression of one or more documents. It has been widely used in various applications such as summarization, clustering, categorizing, browsing, and so on. In recent years, keyphrase extraction has received much attention (Witten et al., 1999; Zha, 2002; Hulth, 2003; Tomokiyo and Hurst, 2003; Chen et al., 2005; Medelyan et al., 2009; Liu et al., 2009). Keyphrases are usually manually chosen by 165 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 165–173, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP 2 Related Work that the keyphrases of news should satisfy the following properties: As mentioned in the previous section, most of current studies on keyphrase extraction can be roughly divided into two categories: supervised and unsupervised approaches. Unsupervised approaches usually select general sets of candidates and use a ranking step to select the most important ca"
I11-1019,W04-3252,0,\N,Missing
I11-1112,P06-2042,0,0.0339718,"Missing"
I13-1014,P11-1032,0,0.0321282,"order to deceive readers. This kind of spammers are even more hazardous, since they are neither easily ignored nor identifiable by a human reader. Google Confucius CQA system also reported that best answer spammers may generate amounts of fake best answers, which could have a non-trivial impact on the quality of machine learning model (Si et al., 2010). With the increasing requirements, spammer detection has received considerable attentions, including e-mails(L.Gomes et al., 2007; C.Wu et al., 2005), web spammer (Cheng et al., 2011), review spammer (Lim et al., 2010; N.Jindal and B.Liu, 2008; ott et al., 2011), social media spammer (Zhu et al., 2012; Bosma et al., 2012; Wang, 2010). However, little work has been done about spammers on CQA sites. Filling this need is a challenging task. The existing approaches of spam detection can be roughly into two directions. The first direction usually relied on costly human-labeled training data for building spam classifiers based on textual features (Y.Liu et al., 2008; Y.Xie et al., Abstract As the popularity of Community Question Answering(CQA) increases, spamming activities also picked up in numbers and variety. On CQA sites, spammers often pretend to ask"
I13-1037,P06-2005,0,0.128411,"ocedure to identify formal-informal relations informal phrases in web corpora (Li and Yarowsky, 2008a) . They used search engine to extract contextual instances of the given an informal phrase, and ranked the candidate relation pairs using conditional log-linear model. Xie et al. (2011) proposed to extract Chinese abbreviations and their corresponding definitions based on anchor texts. They constructed a weighted URL-AnchorText bipartite graph from anchor texts and applied co-frequency based measures to quantify the relatedness between two anchor texts. Related Work For lexical normalisation, Aw et al. (2006) treated the lexical normalisation problem as a translation problem from the informal language to the formal English language and adapted a phrase-based method to do it. Han and Baldwin (2011) proposed a supervised method to detect ill-formed words and used morphophonemic similarity to generate correction candidates. Liu et al. (2012) proposed to use a broad coverage lexical normalization method consisting three key components enhanced letter transformation, visual priming, and string/phonetic similarity. Han et al. (2012) introduced a dictionary based method and an automatic normalisation-dic"
I13-1037,W04-1102,0,0.0386556,"this work, we only have one hidden predicate, drop, the global formulas incorporate correlations among different ground atoms of the drop predicate. We propose to use global formulas to force the abbreviations to contain at least 2 characters and to make sure that at least one character is deleted. The following formulas are implemented: 3.2 MLN for Abbreviation Generation In this work, we convert the abbreviation generation problem as a labeling task for every characters in entities. Predicate drop(i) indicates that the character at position i is omitted in the abbreviation. Previous works (Chang and Lai, 2004; Yang et al., 2009) show that Chinese named entities can be further segmented into words. Words also provide important information for abbreviation generation. Hence, in this work, we also segment named entities into words and propose an observed predict to connect words and characters. |character(i, c) ∧ drop(i) |all i |> 1 |character(i, c) ∧ ¬drop(i) |all i |> 2 Another constraint is that for the characters in some particular words should by dropped or kept simultaneously. So we add two formulas to model this: 3.2.1 Local Formulas character(i, c1) ∧ cwM ap(i, j) ∧ drop(i) ∧ The local formul"
I13-1037,D12-1039,0,0.0392449,"Missing"
I13-1037,D08-1068,0,0.0232957,"ogic, these hard logic formulas are softened and can be violated with some penalty (the weight of formula) in MLN. We use M to represent a MLN and {(ϕi , wi )} to represent formula ϕi and its weight wi . These weighted formulas define a probability distribution over sets of possible worlds. Let y denote a possible world, the p(y) is defined as follows (Richardson and Domingos, 2006):   ∑ ∑ 1 p(y) = exp  wi fcϕi (y) , Z nϕ Singla and Domingos (2006) modeled the entity resolution problem with MLN. They demonstrated the capability of MLN to seamlessly combine a number of previous approaches. Poon and Domingos (2008) proposed to use MLN for joint unsupervised coreference resolution. Yoshikawa et al. (2009) proposed to use Markov logic to incorporate both local features and global constraints that hold between temporal relations. Andrzejewski et al. (2011) introduced a framework for incorporating general domain knowledge, which is represented by First-Order Logic (FOL) rules, into LDA inference to produce topics shaped by both the data and the rules. (ϕi ,wi )∈M c∈C i where each c is a binding of free variable in ϕi to constraints; fcϕi (y) is a binary feature function that 322 also incorporate position in"
I13-1037,P11-1016,0,0.0590315,"Missing"
I13-1037,W08-2125,0,0.0190399,"he corresponding word. Table 2: Descriptions of observed predicates. 3 The Proposed Approach 2.2 Markov Logic Networks In this section, firstly, we briefly describe the Markov Logic Networks framework. Then, we present the first-order logic formulas including local formulas and global formulas we used in this work. Richardson and Domingos (2006) proposed Markov Logic Networks (MLN), which combines first-order logic and probabilistic graphical models. MLN framework has been adopted for several natural language processing tasks and achieved a certain level of success (Singla and Domingos, 2006; Riedel and Meza-Ruiz, 2008; Yoshikawa et al., 2009; Andrzejewski et al., 2011; Jiang et al., 2012; Huang et al., 2012). 3.1 Markov Logic Networks A MLN consists of a set of logic formulas that describe first-order knowledge base. Each formula consists of a set of first-order predicates, logical connectors and variables. Different with firstorder logic, these hard logic formulas are softened and can be violated with some penalty (the weight of formula) in MLN. We use M to represent a MLN and {(ϕi , wi )} to represent formula ϕi and its weight wi . These weighted formulas define a probability distribution over sets of po"
I13-1037,D08-1108,0,0.12808,"e a few of simple patterns to extract abbreviations from documents and search engine snippets with high precision as training data. Experimental results show that the proposed methods achieve better performance than state-of-the-art methods and can efficiently process large volumes of data. The remainder of the paper is organized as follows: In section 2, we review a number of related works and the state-of-the-art approaches in related areas. Section 3 presents the proposed method. Experimental results in test collections and analyses are shown in section 4. Section 5 concludes this paper. 2 Li and Yarowsky (2008b) proposed an unsupervised method extracting the relation between a full-form phrase and its abbreviation from monolingual corpora. They used data co-occurrence intuition to identify relations between abbreviation and full names. They also improved a statistical machine translation by incorporating the extracted relations into the baseline translation system. Based on the data co-occurrence phenomena, they introduced a bootstrapping procedure to identify formal-informal relations informal phrases in web corpora (Li and Yarowsky, 2008a) . They used search engine to extract contextual instances"
I13-1037,P08-1049,0,0.139412,"e a few of simple patterns to extract abbreviations from documents and search engine snippets with high precision as training data. Experimental results show that the proposed methods achieve better performance than state-of-the-art methods and can efficiently process large volumes of data. The remainder of the paper is organized as follows: In section 2, we review a number of related works and the state-of-the-art approaches in related areas. Section 3 presents the proposed method. Experimental results in test collections and analyses are shown in section 4. Section 5 concludes this paper. 2 Li and Yarowsky (2008b) proposed an unsupervised method extracting the relation between a full-form phrase and its abbreviation from monolingual corpora. They used data co-occurrence intuition to identify relations between abbreviation and full names. They also improved a statistical machine translation by incorporating the extracted relations into the baseline translation system. Based on the data co-occurrence phenomena, they introduced a bootstrapping procedure to identify formal-informal relations informal phrases in web corpora (Li and Yarowsky, 2008a) . They used search engine to extract contextual instances"
I13-1037,N09-2069,0,0.652514,"ities. Table 1 shows several examples of entities and their corresponding abbreviations. A few of approaches have been done on this task. Li and Yarowsky (Li and Yarowsky, 2008b) introduced an unsupervised method used to extract phrases and their abbreviation pair using parallel dataset and monolingual corpora. Xie et al. (2011) proposed to use weighted bipartite graph to extract definition and corresponding abbreviation pairs from anchor texts. Since these methods rely heavily on lexical/phonetic similarity, substitution of characters and portion may not be correctly identified through them. Yang et al. (2009) studied the Chinese entity name abbreviation problem. They formulated the abbreviation task as a sequence labeling problem and used the conditional random fields (CRFs) to model it. However the long distance and global constraint can not be easily modeled thorough CRFs. Normalizing named entity abbreviations to their standard forms is an important preprocessing task for question answering, entity retrieval, event detection, microblog processing, and many other applications. Along with the quick expansion of microblogs, this task has received more and more attentions in recent years. In this p"
I13-1037,P12-1109,0,0.035327,"Missing"
I13-1037,P09-1046,0,0.0221652,"2: Descriptions of observed predicates. 3 The Proposed Approach 2.2 Markov Logic Networks In this section, firstly, we briefly describe the Markov Logic Networks framework. Then, we present the first-order logic formulas including local formulas and global formulas we used in this work. Richardson and Domingos (2006) proposed Markov Logic Networks (MLN), which combines first-order logic and probabilistic graphical models. MLN framework has been adopted for several natural language processing tasks and achieved a certain level of success (Singla and Domingos, 2006; Riedel and Meza-Ruiz, 2008; Yoshikawa et al., 2009; Andrzejewski et al., 2011; Jiang et al., 2012; Huang et al., 2012). 3.1 Markov Logic Networks A MLN consists of a set of logic formulas that describe first-order knowledge base. Each formula consists of a set of first-order predicates, logical connectors and variables. Different with firstorder logic, these hard logic formulas are softened and can be violated with some penalty (the weight of formula) in MLN. We use M to represent a MLN and {(ϕi , wi )} to represent formula ϕi and its weight wi . These weighted formulas define a probability distribution over sets of possible worlds. Let y den"
I13-1037,I08-2127,0,0.181752,"Named entity normalization, abbreviation generation, and lexical normalization are related to this task. These problems have been recognized as important problems for various languages. Since different languages have their own peculiarities, many approaches have been proposed to handle variants of words (Aw et al., 2006; Liu et al., 2012; Han et al., 2012) and named entities (Yang et al., 2009; Xie et al., 2011; Li and Yarowsky, 2008b). Chang and Teng (2006) introduced an HMMbased single character recovery model to extract character level abbreviation pairs for textual corpus. Okazaki et al. (2008) also used discriminative approach for this task. They formalized the abbreviation recognition task as a binary classification problem and used Support Vector Machines to model it. Yang et al. (2012) also treated the abbreviation generation problem as a labeling task and used Conditional Random Fields (CRFs) to do it. They also proposed to re-rank candidates by a In this paper, we focused on named entity abbreviation generation problem and treated the problem as a labeling task. Due to the flexibilities of Markov Logic Networks on capturing local and global linguistic feature, we adopted it to"
I13-1037,P11-1038,0,\N,Missing
I13-1063,P11-1011,0,0.0611681,"tuents (i.e the“restaurant” is head search terms, “the best Italian ” and “near seattle washington” are modifier), then it would be able to route the query to a specialized search module (in this case restaurant search) and return the most relevant and essential answers rather than results that merely contain all these keywords. In no small part, the success of such approach relies on robust understanding of query intent. Most previous works in this area focus on query tagging problem, i.e., assigning semantic labels to query terms (Li et al., 2009; Manshadi and Li, 2009; Sarkas et al., 2010; Bendersky et al., 2011). Indeed, with the label information, a search engine is able to provide users with more relevant results. But previous works have not considered the issue for understanding the semantic intent of NL queries and their methods are not suitable for interpreting the semantic intent of this kind of complex queries. In this work, in order to enable search engines to understand natural language query, we focus on the problem of mapping NL queries from a particular search engine like Google maps, Bing maps etc, to their semantic intents representation. A key contribution of this work is that we forma"
I13-1063,J98-4004,0,0.085137,"occurs in the tree y. The example shown in Figure 2 clearly depicts the way features are mapped from a tree structure intent representation of an NL query. n 4 SVM1 s 1 CX : |{z} min kwk2 + ξi 2 n w i=1 s.t. ∀i, ξi ≥0 1 − ξi ∀i, ∀y∈Y yi : hw, δψi (y)i≥ 4(yi , y) (4) (5) The second approach to include loss function is to rescale the margin as a special case of the Hamming loss. The margin constraints in this setting take the following form: ∀i, ∀y∈Y yi : hw, δψi (y)i≥4(yi , y) − ξi 4.2.3 Loss function Typically, the correctness of a predicted parse tree is measured by its F1 score (see e.g. (Johnson, 1998)), the harmonic mean of precision of recall as calculated based on the overlap of nodes between the trees. In this work, we follow this loss function and introduce the standard zero-one classification loss as a baseline measure method. Let z and zi be two parse tree outputs and |z| and |zi |be the number of brackets in z and zi , re(6) This set of constraints yields an optimization probm lem, namely SVM4 . 1 The algorithm to solve the maximum-margin problem in structured learning problem is presented in detail in (Tsochantaridis et al., 2004). And 556 ; Algorithm 1 Algorithm of SSVM learning f"
I13-1063,P10-1136,0,0.0247245,"Business. In particular, Li et al. leverage clickthrough data and a database to automatically derive training data for learning a CRF-based tagger. Manshadi and Li develop a hybrid, generative grammar model for a similar task. Sarkas et al. consider an annotation as a mapping of a query to a table of structured data and attributes of this table, while Bendersky et al. mark up queries with annotations such as partof-speech tags, capitalization, and segmentation. There are relatively little published work on understanding the semantic intent of natural language query. Manshadi and Li (2009) and Li (2010) consider the semantic structure of queries. In particular, Li (2010) defines the semantic structure of noun-phrase queries as intent heads (attributes) coupled with some number of intent modifiers (attribute values), e.g., the query [alice in wonderland 2010 cast] is comprised of an intent head cast and two intent modifiers alice in wonderland and 2010. Our approach differs from the earlier work in that we investigate the natural language query intent understanding problem, and build a hierarchical representation for it. 2.2 Semantic Parsing For the purpose of enabling search engines to under"
I13-1063,P09-1097,0,\N,Missing
N19-1277,W16-1603,0,0.11003,"013a) and GloVe (Pennington et al., 2014), which are based on the distributional hypothesis (Harris, 1954): words that occur in the same contexts tend to have similar meanings. Earlier word embeddings often take a word as a basic unit, and they ignore compositionality of its sub-word information such as morphemes and character n-grams, and cannot competently handle the rare words. To improve the performance of word embeddings, sub-word information has been employed (Luong et al., 2013; Qiu ∗ Corresponding author. The source codes are available at https://github. com/HSLCY/VCWE 1 et al., 2014; Cao and Rei, 2016; Sun et al., 2016a; Wieting et al., 2016; Bojanowski et al., 2016). Compositionality is more critical for Chinese, since Chinese is a logographic writing system. In Chinese, each word typically consists of fewer characters and each character also contains richer semantic information. For example, Chinese character “休” (rest) is composed of the characters for “人” (person) and “木” (tree), with the intended idea of someone leaning against a tree, i.e., resting. Based on the linguistic features of Chinese, recent methods have used the character information to improve Chinese word embeddings. Thes"
N19-1277,W06-0115,0,0.0562965,"82.98 82.57 82.52 84.87 84.74 84.29 83.23 83.28 85.30 95.87 95.79 95.45 95.45 95.91 VCWE -CNN -LSTM 86.93 86.73 85.98 84.64 84.83 84.53 85.77 85.77 85.25 96.00 95.92 95.96 Model Table 3: Chinese NER and POS tagging results for different pretrained embeddings. The configurations are the same of the ones used in Table 1. Named Entity Recognition Task We evaluate our model on the named entity recognition task. We use an open source Chinese NER model to test our word embeddings on MSRA dataset14 . MSRA is a dataset for simplified Chinese NER. It comes from SIGHAN 2006 shared task for Chinese NER (Levow, 2006). We pretrain word embeddings from different models and feed them into the input layer as features. The key to the task is to extract named entities and their associated types. Better word embeddings could get a higher F1 score of NER. The results in Table 3 show that our model also outperforms baseline models in this task. The performance of CWE and GWE models are similar, both slightly lower than Skip-gram and CBOW models. The F1 score of the JWE model exceeds that of other baseline models and is similar to our model. When removing the CNN and image information, our LSTM with the self-attent"
N19-1277,P17-1188,0,0.394341,"information to improve Chinese word embeddings. These methods can be categorized into two kinds: 1) One kind of methods learn word embeddings with its constituent character (Chen et al., 2015), radical2 (Shi et al., 2015; Yin et al., 2016; Yu et al., 2017) or strokes3 (Cao et al., 2018). However, these methods usually use simple operations, such as averaging and n-gram, to model the inherent compositionality within a word, which is not enough to handle the complicated linguistic compositionality. 2) The other kind of methods learns word embeddings with the visual information of the character. Liu et al. (2017) learn character embedding based on its visual characteristics in the text classification task. Su and Lee (2017) also introduce a pixel-based model that learns character features from font images. However, their model is not shown to be better than word2vec model because it has little flexibility and fixed character features. Besides, most of these methods pay less attention to the non-compositionality. For example, the 2 the graphical component of Chinese, referring to https://en.wikipedia.org/wiki/Radical_ (Chinese_characters) 3 the basic pattern of Chinese characters, referring to https://"
N19-1277,W13-3512,0,0.347996,"ssing (NLP) tasks. Most of these NLP tasks also benefit from the pre-trained word embeddings, such as word2vec (Mikolov et al., 2013a) and GloVe (Pennington et al., 2014), which are based on the distributional hypothesis (Harris, 1954): words that occur in the same contexts tend to have similar meanings. Earlier word embeddings often take a word as a basic unit, and they ignore compositionality of its sub-word information such as morphemes and character n-grams, and cannot competently handle the rare words. To improve the performance of word embeddings, sub-word information has been employed (Luong et al., 2013; Qiu ∗ Corresponding author. The source codes are available at https://github. com/HSLCY/VCWE 1 et al., 2014; Cao and Rei, 2016; Sun et al., 2016a; Wieting et al., 2016; Bojanowski et al., 2016). Compositionality is more critical for Chinese, since Chinese is a logographic writing system. In Chinese, each word typically consists of fewer characters and each character also contains richer semantic information. For example, Chinese character “休” (rest) is composed of the characters for “人” (person) and “木” (tree), with the intended idea of someone leaning against a tree, i.e., resting. Based on"
N19-1277,D14-1162,0,0.093764,"ionality directly from the contextual information. Evaluations demonstrate the superior performance of our model on four tasks: word similarity, sentiment analysis, named entity recognition and part-of-speech tagging.1 1 Introduction Distributed representations of words, namely word embeddings, encode both semantic and syntactic information into a dense vector. Currently, word embeddings have been playing a pivotal role in many natural language processing (NLP) tasks. Most of these NLP tasks also benefit from the pre-trained word embeddings, such as word2vec (Mikolov et al., 2013a) and GloVe (Pennington et al., 2014), which are based on the distributional hypothesis (Harris, 1954): words that occur in the same contexts tend to have similar meanings. Earlier word embeddings often take a word as a basic unit, and they ignore compositionality of its sub-word information such as morphemes and character n-grams, and cannot competently handle the rare words. To improve the performance of word embeddings, sub-word information has been employed (Luong et al., 2013; Qiu ∗ Corresponding author. The source codes are available at https://github. com/HSLCY/VCWE 1 et al., 2014; Cao and Rei, 2016; Sun et al., 2016a; Wie"
N19-1277,C14-1015,0,0.016538,"nal probability estimation (Mikolov et al., 2013a). A different way to learn word embeddings is through factorization of word co-occurrence matrices such as GloVe embeddings (Pennington et al., 2014), which have been shown to be intrinsically linked to Skip-gram and negative sampling (Levy and Goldberg, 2014). The models mentioned above are popular and useful, but they regard individual words as atomic tokens, and the potentially useful internal structured information of words is ignored. To improve the performance of word embedding, sub-word information has been employed (Luong et al., 2013; Qiu et al., 2014; Cao and Rei, 2016; Sun et al., 2016a; Wieting et al., 2016; Bojanowski et al., 2016). These methods focus on alphabetic writing systems, but they are not directly applicable to logographic writing systems. For the alphabetic writing systems, research on Chinese word embedding has gradually emerged. These methods focus on the discovery of making full use of sub-word information. Chen et al. (2015) design a CWE model for jointly learning Chinese characters and word embeddings. Based on the CWE model, Yin et al. (2016) present a multi-granularity embedding (MGE) model, additionally using the em"
N19-1277,P15-2098,0,0.163698,"ng system. In Chinese, each word typically consists of fewer characters and each character also contains richer semantic information. For example, Chinese character “休” (rest) is composed of the characters for “人” (person) and “木” (tree), with the intended idea of someone leaning against a tree, i.e., resting. Based on the linguistic features of Chinese, recent methods have used the character information to improve Chinese word embeddings. These methods can be categorized into two kinds: 1) One kind of methods learn word embeddings with its constituent character (Chen et al., 2015), radical2 (Shi et al., 2015; Yin et al., 2016; Yu et al., 2017) or strokes3 (Cao et al., 2018). However, these methods usually use simple operations, such as averaging and n-gram, to model the inherent compositionality within a word, which is not enough to handle the complicated linguistic compositionality. 2) The other kind of methods learns word embeddings with the visual information of the character. Liu et al. (2017) learn character embedding based on its visual characteristics in the text classification task. Su and Lee (2017) also introduce a pixel-based model that learns character features from font images. Howev"
N19-1277,D17-1025,0,0.76979,"ethods learn word embeddings with its constituent character (Chen et al., 2015), radical2 (Shi et al., 2015; Yin et al., 2016; Yu et al., 2017) or strokes3 (Cao et al., 2018). However, these methods usually use simple operations, such as averaging and n-gram, to model the inherent compositionality within a word, which is not enough to handle the complicated linguistic compositionality. 2) The other kind of methods learns word embeddings with the visual information of the character. Liu et al. (2017) learn character embedding based on its visual characteristics in the text classification task. Su and Lee (2017) also introduce a pixel-based model that learns character features from font images. However, their model is not shown to be better than word2vec model because it has little flexibility and fixed character features. Besides, most of these methods pay less attention to the non-compositionality. For example, the 2 the graphical component of Chinese, referring to https://en.wikipedia.org/wiki/Radical_ (Chinese_characters) 3 the basic pattern of Chinese characters, referring to https://en.wikipedia.org/wiki/Stroke_ (CJKV_character) 2710 Proceedings of NAACL-HLT 2019, pages 2710–2719 c Minneapolis,"
N19-1277,D16-1157,0,0.123752,"14), which are based on the distributional hypothesis (Harris, 1954): words that occur in the same contexts tend to have similar meanings. Earlier word embeddings often take a word as a basic unit, and they ignore compositionality of its sub-word information such as morphemes and character n-grams, and cannot competently handle the rare words. To improve the performance of word embeddings, sub-word information has been employed (Luong et al., 2013; Qiu ∗ Corresponding author. The source codes are available at https://github. com/HSLCY/VCWE 1 et al., 2014; Cao and Rei, 2016; Sun et al., 2016a; Wieting et al., 2016; Bojanowski et al., 2016). Compositionality is more critical for Chinese, since Chinese is a logographic writing system. In Chinese, each word typically consists of fewer characters and each character also contains richer semantic information. For example, Chinese character “休” (rest) is composed of the characters for “人” (person) and “木” (tree), with the intended idea of someone leaning against a tree, i.e., resting. Based on the linguistic features of Chinese, recent methods have used the character information to improve Chinese word embeddings. These methods can be categorized into two kin"
N19-1277,N16-1119,0,0.0580809,"ojanowski et al., 2016). These methods focus on alphabetic writing systems, but they are not directly applicable to logographic writing systems. For the alphabetic writing systems, research on Chinese word embedding has gradually emerged. These methods focus on the discovery of making full use of sub-word information. Chen et al. (2015) design a CWE model for jointly learning Chinese characters and word embeddings. Based on the CWE model, Yin et al. (2016) present a multi-granularity embedding (MGE) model, additionally using the embeddings associated with radicals detected in the target word. Xu et al. (2016) propose a similarity-based characterenhanced word embedding (SCWE) model, exploiting the similarity between a word and its component characters with the semantic knowledge obtained from other languages. Shi et al. (2015) utilize radical information to improve Chinese word embeddings. Yu et al. (2017) introduce a joint learning word embedding (JWE) model and Cao et al. (2018) represent Chinese words as sequences of strokes and learn word embeddings with stroke n-grams information. From another perspective, Liu et al. (2017) provide a new way to automatically extract characterlevel features, cr"
N19-1277,D16-1100,0,0.0856302,"ese, each word typically consists of fewer characters and each character also contains richer semantic information. For example, Chinese character “休” (rest) is composed of the characters for “人” (person) and “木” (tree), with the intended idea of someone leaning against a tree, i.e., resting. Based on the linguistic features of Chinese, recent methods have used the character information to improve Chinese word embeddings. These methods can be categorized into two kinds: 1) One kind of methods learn word embeddings with its constituent character (Chen et al., 2015), radical2 (Shi et al., 2015; Yin et al., 2016; Yu et al., 2017) or strokes3 (Cao et al., 2018). However, these methods usually use simple operations, such as averaging and n-gram, to model the inherent compositionality within a word, which is not enough to handle the complicated linguistic compositionality. 2) The other kind of methods learns word embeddings with the visual information of the character. Liu et al. (2017) learn character embedding based on its visual characteristics in the text classification task. Su and Lee (2017) also introduce a pixel-based model that learns character features from font images. However, their model is"
N19-1277,D17-1027,0,0.593563,"ically consists of fewer characters and each character also contains richer semantic information. For example, Chinese character “休” (rest) is composed of the characters for “人” (person) and “木” (tree), with the intended idea of someone leaning against a tree, i.e., resting. Based on the linguistic features of Chinese, recent methods have used the character information to improve Chinese word embeddings. These methods can be categorized into two kinds: 1) One kind of methods learn word embeddings with its constituent character (Chen et al., 2015), radical2 (Shi et al., 2015; Yin et al., 2016; Yu et al., 2017) or strokes3 (Cao et al., 2018). However, these methods usually use simple operations, such as averaging and n-gram, to model the inherent compositionality within a word, which is not enough to handle the complicated linguistic compositionality. 2) The other kind of methods learns word embeddings with the visual information of the character. Liu et al. (2017) learn character embedding based on its visual characteristics in the text classification task. Su and Lee (2017) also introduce a pixel-based model that learns character features from font images. However, their model is not shown to be b"
P11-2105,P09-2042,1,0.678348,"Missing"
P13-2077,P11-1066,0,0.0275921,"atures, or ﬁnding textual clues using machine learning techniques; none of them ever consider questions and their answers as relational data but instead model them as independent information. Moreover, they only consider the answers of the current question, and ignore any previous knowledge that would be helpful to bridge the lexical and se mantic gap. In recent years, many methods have been proposed to solve the word mismatching problem between user questions and the questions in a QA archive(Blooma and Kurian, 2011), among which the translation-based (Riezler et al., 2007; Xue et al., 2008; Zhou et al., 2011) or syntactic-based approaches (Wang et al., 2009) methods have been proven to improve the performance of CQA retrieval. However, most of these approaches used Introduction Community-based (or collaborative) question answering(CQA) such as Yahoo! Answers1 and Baidu Zhidao2 has become a popular online service in recent years. Unlike traditional question answering (QA), information seekers can post their questions on a CQA website which are later answered by other users. However, with the increase of the CQA archive, there accumulate massive duplicate questions on CQA websites. One of the primar"
P13-2077,P13-4009,1,0.798255,"m . A:B √ A:B= √ A:A× B :B (8) While given a new question both with title and content parts, MContent is not a zero matrix and could be also employed in the question retrieval process. A simple strategy is to sum up the scores of two parts. 5 Experiments 5.1 Datasets We collected the resolved CQA triples from the “computer” category of Yahoo! Answers and Baidu Zhidao websites. We just selected the resolved questions that already have been given their best answers. The CQA triples are preprocessed with stopwords removal (Chinese sentences are segmented into words in advance by FudanNLP toolkit(Qiu et al., 2013)). In order to evaluate our retrieval system, we divide our dataset into two parts. The ﬁrst part is used as training dataset; the rest is used as test dataset for evaluation. The datasets are shown in Table 2. Figure 2: 3-mode SVD of CQA tensor To deal with such a huge sparse data set, we use singular value decomposition (SVD) implemented in Apache Mahout3 machine learning library, which is implemented on top of Apache Hadoop4 using the map/reduce paradigm and scalable to reasonably large data sets. 3 4 http://mahout.apache.org/ http://hadoop.apache.org 437 DataSet Baidu Zhidao Yahoo! Answers"
P13-2077,P07-1059,0,0.0332427,"mainly focus on generating redundant features, or ﬁnding textual clues using machine learning techniques; none of them ever consider questions and their answers as relational data but instead model them as independent information. Moreover, they only consider the answers of the current question, and ignore any previous knowledge that would be helpful to bridge the lexical and se mantic gap. In recent years, many methods have been proposed to solve the word mismatching problem between user questions and the questions in a QA archive(Blooma and Kurian, 2011), among which the translation-based (Riezler et al., 2007; Xue et al., 2008; Zhou et al., 2011) or syntactic-based approaches (Wang et al., 2009) methods have been proven to improve the performance of CQA retrieval. However, most of these approaches used Introduction Community-based (or collaborative) question answering(CQA) such as Yahoo! Answers1 and Baidu Zhidao2 has become a popular online service in recent years. Unlike traditional question answering (QA), information seekers can post their questions on a CQA website which are later answered by other users. However, with the increase of the CQA archive, there accumulate massive duplicate questi"
P13-2077,P08-1019,0,\N,Missing
P13-4009,W09-1201,0,0.0699738,"ped mainly for English and not optimized for Chinese. In order to customize an optimized system for Chinese language process, we implement an open source toolkit, FudanNLP5 , which is written in Java. Since most of the state-of-theart methods for NLP are based on statistical learning, the whole framework of our toolkit is established around statistics-based methods, supplemented by some rule-based methods. Therefore, the quality of training data is crucial for our toolkit. However, we ﬁnd that there are some drawbacks in currently most commonly used corpora, such as CTB (Xia, 2000) and CoNLL (Hajič et al., 2009) corpora. For example, in CTB corpus, the set of POS tags is relative small and some categories are derived from the perspective of English grammar. And in CoNLL corpus, the head words are often interrogative particles and punctuations, which are unidiomatic in Chinese. These drawbacks bring more challenges to further analyses, such as information extraction and semantic understanding. Therefore, we ﬁrst construct a corpus with a modiﬁed guideline, which is more in accordance with the common understanding for Chinese grammar. In addition to the basic Chinese NLP tasks The growing need for Chin"
P13-4009,W04-3236,0,0.0704609,": 5 TIME: 7 → → PER LOC → 1 → 1980 8. CS:COO means the coordinate complex sentence. Table 1: Example of the output representation of our toolkit location, organization and other proper name. Conversely, we merge the “VC” and “VE” into “VV” since there is no link verb in Chinese. Finally, we use a tag set with 39 categories in total. Since a POS tag is assigned to each word, not to each character, Chinese POS tagging has two ways: pipeline method or joint method. Currently, the joint method is more popular and eﬀective because it uses more ﬂexible features and can reduce the error propagation (Ng and Low, 2004). In our system, we implement both methods for POS tagging. Besides, we also use some knowledge to improve the performance, such as Chinese surname and the common suﬃxes of the names of locations and organizations. 2.3.3 tively more important, we use language models to capture the internal structures. Second, we merge the continuous NEs with some rulebased strategies. For example, we combine the continuous words “人民/NN 大会堂/NN” into “ 人民大会堂/LOC”. 2.3.4 Dependency parsing Our syntactic parser is currently a dependency parser, which is implemented with the shift-reduce deterministic algorithm bas"
P13-4009,C04-1081,0,0.0463487,"s are trained on our developed corpus. We also develop a visualization module to displaying the output. Table 1 shows the output representation of our toolkit. (4) 2.3.1 Chinese Word Segmentation Diﬀerent from English, Chinese sentences are written in a continuous sequence of characters without explicit delimiters such as the blank space. Since the meanings of most Chinese characters are not complete, words are the basic syntactic and semantic units. Therefore, it is indispensable step to segment the sentence into words in Chinese language processing. We use character-based sequence labeling (Peng et al., 2004) to ﬁnd the boundaries of words. Besides the carefully chosen features, we also use the meaning of character drawn from HowNet(Dong and Dong, 2006), which improves the performance greatly. Since unknown words detection is still one of main challenges of Chinese word segmentation. We implement a constrained Viterbi algorithm to allow users to add their own word dictionary. 2.2.1 Training In the training stage, we use the passiveaggressive algorithm to learn the model parameters. Passive-aggressive (PA) algorithm (Crammer et al., 2006) was proposed for normal multi-class classiﬁcation and can be"
P13-4009,W03-3023,0,0.0336701,"implement both methods for POS tagging. Besides, we also use some knowledge to improve the performance, such as Chinese surname and the common suﬃxes of the names of locations and organizations. 2.3.3 tively more important, we use language models to capture the internal structures. Second, we merge the continuous NEs with some rulebased strategies. For example, we combine the continuous words “人民/NN 大会堂/NN” into “ 人民大会堂/LOC”. 2.3.4 Dependency parsing Our syntactic parser is currently a dependency parser, which is implemented with the shift-reduce deterministic algorithm based on the work in (Yamada and Matsumoto, 2003). The syntactic structure of Chinese is more complex than that of English, and semantic meaning is more dominant than syntax in Chinese sentences. So we select the dependency parser to avoid the minutiae in syntactic constituents and wish to pay more attention to the subsequent semantic analysis. Since the structure of the Chinese language is quite different from that of English, we use more eﬀective features according to the characteristics of Chinese sentences. The common used corpus for Chinese dependency parsing is CoNLL corpus (Hajič et al., 2009). However, there are some illogical cases"
P15-1112,P14-2131,0,0.0326639,"applied on many Corresponding author. red,JJ bike,NN Figure 1: Illustration of a RCNN unit. Introduction ∗ red bike,NN natural language processing (NLP) tasks, such as syntax (Turian et al., 2010; Mikolov et al., 2010; Collobert et al., 2011; Chen and Manning, 2014) and semantics (Huang et al., 2012; Mikolov et al., 2013). Distributed representations are to represent words (or phrase) by the dense, low-dimensional and real-valued vectors, which help address the curse of dimensionality and have better generalization than discrete representations. For dependency parsing, Chen et al. (2014) and Bansal et al. (2014) used the dense vectors (embeddings) to represent words or features and found these representations are complementary to the traditional discrete feature representation. However, these two methods only focus on the dense representations (embeddings) of words or features. These embeddings are pre-trained and keep unchanged in the training phase of parsing model, which cannot be optimized for the specific tasks. Besides, it is also important to represent the (unseen) phrases with dense vector in dependency parsing. Since the dependency tree is also in recursive structure, it is intuitive to use"
P15-1112,P05-1022,0,0.309939,"yi } (9) d∈yˆi where κ is a discount parameter and d represents the nodes in trees. Given a set of training dependency parses D, the final training objective is to minimize the loss function J(Θ), plus a l2 -regulation term: J(Θ) = 1 |D| X (xi ,yi )∈D ri (Θ) + λ kΘk22 , 2 (10) where ri (Θ) = max ( 0, st (xi , yˆi , Θ) yˆi ∈Y (xi ) + ∆(yi , yˆi ) − st (xi , yi , Θ) ) . (11) By minimizing this object, the score of the correct tree yi is increased and the score of the highest scoring incorrect tree yˆi is decreased. Re-rankers Re-ranking k-best lists was introduced by Collins and Koo (2005) and Charniak and Johnson (2005). They used discriminative methods to re-rank the constituent parsing. In the dependency parsing, Sangati et al. (2009) used a third-order generative model for re-ranking k-best lists of base parser. Hayashi et al. (2013) used a discriminative forest re-ranking algorithm for dependency parsing. These re-ranking models achieved a substantial raise on the parsing performances. Given T (x), the set of k-best trees of a sentence x from a base parser, we use the popular mixture re-ranking strategy (Hayashi et al., 2013; Le and Mikolov, 2014), which is a combination of the our model and the base par"
P15-1112,D14-1082,0,0.718651,"ign of features. The number of features could be so large that the result models are too complicated for practical use and prone to overfit on training corpus due to data sparseness. Recently, many methods are proposed to learn various distributed representations on both syntax and semantics levels. These distributed representations have been extensively applied on many Corresponding author. red,JJ bike,NN Figure 1: Illustration of a RCNN unit. Introduction ∗ red bike,NN natural language processing (NLP) tasks, such as syntax (Turian et al., 2010; Mikolov et al., 2010; Collobert et al., 2011; Chen and Manning, 2014) and semantics (Huang et al., 2012; Mikolov et al., 2013). Distributed representations are to represent words (or phrase) by the dense, low-dimensional and real-valued vectors, which help address the curse of dimensionality and have better generalization than discrete representations. For dependency parsing, Chen et al. (2014) and Bansal et al. (2014) used the dense vectors (embeddings) to represent words or features and found these representations are complementary to the traditional discrete feature representation. However, these two methods only focus on the dense representations (embedding"
P15-1112,C14-1078,0,0.0254698,"Missing"
P15-1112,J05-1003,0,0.0345127,". X ∆(yi , yˆi ) = κ1{d ∈ / yi } (9) d∈yˆi where κ is a discount parameter and d represents the nodes in trees. Given a set of training dependency parses D, the final training objective is to minimize the loss function J(Θ), plus a l2 -regulation term: J(Θ) = 1 |D| X (xi ,yi )∈D ri (Θ) + λ kΘk22 , 2 (10) where ri (Θ) = max ( 0, st (xi , yˆi , Θ) yˆi ∈Y (xi ) + ∆(yi , yˆi ) − st (xi , yi , Θ) ) . (11) By minimizing this object, the score of the correct tree yi is increased and the score of the highest scoring incorrect tree yˆi is decreased. Re-rankers Re-ranking k-best lists was introduced by Collins and Koo (2005) and Charniak and Johnson (2005). They used discriminative methods to re-rank the constituent parsing. In the dependency parsing, Sangati et al. (2009) used a third-order generative model for re-ranking k-best lists of base parser. Hayashi et al. (2013) used a discriminative forest re-ranking algorithm for dependency parsing. These re-ranking models achieved a substantial raise on the parsing performances. Given T (x), the set of k-best trees of a sentence x from a base parser, we use the popular mixture re-ranking strategy (Hayashi et al., 2013; Le and Mikolov, 2014), which is a combination o"
P15-1112,J03-4003,0,0.0595577,"s that need to be trained. This score will be used to find the highest scoring tree. For more details on how standard RNN can be used for parsing, see (Socher et al., 2011). Costa et al. (2003) applied recursive neural networks to re-rank possible phrase attachments in an incremental constituency parser. Their work is the first to show that RNNs can capture enough information to make the correct parsing decisions. Menchetti et al. (2005) used RNNs to re-rank different constituency parses. For their results on full sentence parsing, they re-ranked candidate trees created by the Collins parser (Collins, 2003). 3 ··· (h,ci) d Distance Embedding ··· wh Word Embedding x 1 x2 ··· xK Phrase Representations of Children Figure 3: Architecture of a RCNN unit. Recursive Convolutional Neural Network The dependency grammar is a widely used syntactic structure, which directly reflects relationships among the words in a sentence. In a dependency tree, all nodes are terminal (words) and each node may have more than two children. Therefore, the standard RNN architecture is not suitable for dependency grammar since it is based on the binary tree. In this section, we propose a more general architecture, called rec"
P15-1112,Q13-1012,0,0.238233,"(Θ) = 1 |D| X (xi ,yi )∈D ri (Θ) + λ kΘk22 , 2 (10) where ri (Θ) = max ( 0, st (xi , yˆi , Θ) yˆi ∈Y (xi ) + ∆(yi , yˆi ) − st (xi , yi , Θ) ) . (11) By minimizing this object, the score of the correct tree yi is increased and the score of the highest scoring incorrect tree yˆi is decreased. Re-rankers Re-ranking k-best lists was introduced by Collins and Koo (2005) and Charniak and Johnson (2005). They used discriminative methods to re-rank the constituent parsing. In the dependency parsing, Sangati et al. (2009) used a third-order generative model for re-ranking k-best lists of base parser. Hayashi et al. (2013) used a discriminative forest re-ranking algorithm for dependency parsing. These re-ranking models achieved a substantial raise on the parsing performances. Given T (x), the set of k-best trees of a sentence x from a base parser, we use the popular mixture re-ranking strategy (Hayashi et al., 2013; Le and Mikolov, 2014), which is a combination of the our model and the base parser. yˆi = arg max αst (xi , y, Θ) + (1 − α)sb (xi , y) y∈T (xi ) (14) where α ∈ [0, 1] is a hyperparameter; st (xi , y, Θ) and sb (xi , y) are the scores given by RCNN and the base parser respectively. To apply RCNN into"
P15-1112,P10-1110,0,0.21165,"Missing"
P15-1112,P12-1092,0,0.0322838,"could be so large that the result models are too complicated for practical use and prone to overfit on training corpus due to data sparseness. Recently, many methods are proposed to learn various distributed representations on both syntax and semantics levels. These distributed representations have been extensively applied on many Corresponding author. red,JJ bike,NN Figure 1: Illustration of a RCNN unit. Introduction ∗ red bike,NN natural language processing (NLP) tasks, such as syntax (Turian et al., 2010; Mikolov et al., 2010; Collobert et al., 2011; Chen and Manning, 2014) and semantics (Huang et al., 2012; Mikolov et al., 2013). Distributed representations are to represent words (or phrase) by the dense, low-dimensional and real-valued vectors, which help address the curse of dimensionality and have better generalization than discrete representations. For dependency parsing, Chen et al. (2014) and Bansal et al. (2014) used the dense vectors (embeddings) to represent words or features and found these representations are complementary to the traditional discrete feature representation. However, these two methods only focus on the dense representations (embeddings) of words or features. These emb"
P15-1112,D14-1081,0,0.477752,"ifferences as follows. 1) They first summed up all child nodes into a dense vector vc and then composed subtree representation from vc and vector parent node. In contrast, our model first combine the parent and each child and then choose the most informative features with a pooling layer. 2) We represent the relative position of each child and its parent with distributed representation (position embeddings), which is very useful for convolutional layer. Figure 7 shows an example of DTRNN to illustrates how RCNN represents phrases as continuous vectors. Specific to the re-ranking model, Le and Zuidema (2014) proposed a generative re-ranking model with Inside-Outside Recursive Neural Network (IORNN), which can process trees both bottom-up and top-down. However, IORNN works in generative way and just estimates the probability of a given tree, so IORNN cannot fully utilize the incorrect trees in k-best candidate results. Besides, IORNN treats dependency tree as a sequence, which can be regarded as a generalization of simple recurrent neural network (SRNN) (Elman, 1990). Unlike IORNN, our proposed RCNN is a discriminative model and can optimize the re-ranking strategy for a particular base 1166 parse"
P15-1112,P05-1012,0,0.101813,"ork, we introduce the convolution and pooling layers, which can model a variety of compositions by the feature maps and choose the most informative compositions by the pooling layers. Based on RCNN, we use a discriminative model to re-rank a k-best list of candidate dependency parsing trees. The experiments show that RCNN is very effective to improve the state-of-the-art dependency parsing on both English and Chinese datasets. 1 Convolution a bike,NN a,Det Feature-based discriminative supervised models have achieved much progress in dependency parsing (Nivre, 2004; Yamada and Matsumoto, 2003; McDonald et al., 2005), which typically use millions of discrete binary features generated from a limited size training data. However, the ability of these models is restricted by the design of features. The number of features could be so large that the result models are too complicated for practical use and prone to overfit on training corpus due to data sparseness. Recently, many methods are proposed to learn various distributed representations on both syntax and semantics levels. These distributed representations have been extensively applied on many Corresponding author. red,JJ bike,NN Figure 1: Illustration of"
P15-1112,W04-0308,0,0.0420055,"t with the original recursive neural network, we introduce the convolution and pooling layers, which can model a variety of compositions by the feature maps and choose the most informative compositions by the pooling layers. Based on RCNN, we use a discriminative model to re-rank a k-best list of candidate dependency parsing trees. The experiments show that RCNN is very effective to improve the state-of-the-art dependency parsing on both English and Chinese datasets. 1 Convolution a bike,NN a,Det Feature-based discriminative supervised models have achieved much progress in dependency parsing (Nivre, 2004; Yamada and Matsumoto, 2003; McDonald et al., 2005), which typically use millions of discrete binary features generated from a limited size training data. However, the ability of these models is restricted by the design of features. The number of features could be so large that the result models are too complicated for practical use and prone to overfit on training corpus due to data sparseness. Recently, many methods are proposed to learn various distributed representations on both syntax and semantics levels. These distributed representations have been extensively applied on many Correspond"
P15-1112,P13-1045,0,0.449946,"nd these representations are complementary to the traditional discrete feature representation. However, these two methods only focus on the dense representations (embeddings) of words or features. These embeddings are pre-trained and keep unchanged in the training phase of parsing model, which cannot be optimized for the specific tasks. Besides, it is also important to represent the (unseen) phrases with dense vector in dependency parsing. Since the dependency tree is also in recursive structure, it is intuitive to use the recursive neural network (RNN), which is used for constituent parsing (Socher et al., 2013a). However, recursive neural network can only process the binary combination and is not suitable for dependency parsing, since a parent node may have two or more child nodes in dependency tree. In this work, we address the problem to rep1159 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 1159–1168, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics resent all level nodes (words or phrases) with dense representations in a dependency tree. We"
P15-1112,D13-1170,0,0.432116,"nd these representations are complementary to the traditional discrete feature representation. However, these two methods only focus on the dense representations (embeddings) of words or features. These embeddings are pre-trained and keep unchanged in the training phase of parsing model, which cannot be optimized for the specific tasks. Besides, it is also important to represent the (unseen) phrases with dense vector in dependency parsing. Since the dependency tree is also in recursive structure, it is intuitive to use the recursive neural network (RNN), which is used for constituent parsing (Socher et al., 2013a). However, recursive neural network can only process the binary combination and is not suitable for dependency parsing, since a parent node may have two or more child nodes in dependency tree. In this work, we address the problem to rep1159 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 1159–1168, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics resent all level nodes (words or phrases) with dense representations in a dependency tree. We"
P15-1112,P10-1040,0,0.111381,"data. However, the ability of these models is restricted by the design of features. The number of features could be so large that the result models are too complicated for practical use and prone to overfit on training corpus due to data sparseness. Recently, many methods are proposed to learn various distributed representations on both syntax and semantics levels. These distributed representations have been extensively applied on many Corresponding author. red,JJ bike,NN Figure 1: Illustration of a RCNN unit. Introduction ∗ red bike,NN natural language processing (NLP) tasks, such as syntax (Turian et al., 2010; Mikolov et al., 2010; Collobert et al., 2011; Chen and Manning, 2014) and semantics (Huang et al., 2012; Mikolov et al., 2013). Distributed representations are to represent words (or phrase) by the dense, low-dimensional and real-valued vectors, which help address the curse of dimensionality and have better generalization than discrete representations. For dependency parsing, Chen et al. (2014) and Bansal et al. (2014) used the dense vectors (embeddings) to represent words or features and found these representations are complementary to the traditional discrete feature representation. Howeve"
P15-1112,W03-3023,0,0.0642293,"iginal recursive neural network, we introduce the convolution and pooling layers, which can model a variety of compositions by the feature maps and choose the most informative compositions by the pooling layers. Based on RCNN, we use a discriminative model to re-rank a k-best list of candidate dependency parsing trees. The experiments show that RCNN is very effective to improve the state-of-the-art dependency parsing on both English and Chinese datasets. 1 Convolution a bike,NN a,Det Feature-based discriminative supervised models have achieved much progress in dependency parsing (Nivre, 2004; Yamada and Matsumoto, 2003; McDonald et al., 2005), which typically use millions of discrete binary features generated from a limited size training data. However, the ability of these models is restricted by the design of features. The number of features could be so large that the result models are too complicated for practical use and prone to overfit on training corpus due to data sparseness. Recently, many methods are proposed to learn various distributed representations on both syntax and semantics levels. These distributed representations have been extensively applied on many Corresponding author. red,JJ bike,NN F"
P15-1112,D08-1059,0,0.442944,"Missing"
P15-1112,P11-2033,0,0.154309,"Missing"
P15-1112,W09-3839,0,0.0238199,"s D, the final training objective is to minimize the loss function J(Θ), plus a l2 -regulation term: J(Θ) = 1 |D| X (xi ,yi )∈D ri (Θ) + λ kΘk22 , 2 (10) where ri (Θ) = max ( 0, st (xi , yˆi , Θ) yˆi ∈Y (xi ) + ∆(yi , yˆi ) − st (xi , yi , Θ) ) . (11) By minimizing this object, the score of the correct tree yi is increased and the score of the highest scoring incorrect tree yˆi is decreased. Re-rankers Re-ranking k-best lists was introduced by Collins and Koo (2005) and Charniak and Johnson (2005). They used discriminative methods to re-rank the constituent parsing. In the dependency parsing, Sangati et al. (2009) used a third-order generative model for re-ranking k-best lists of base parser. Hayashi et al. (2013) used a discriminative forest re-ranking algorithm for dependency parsing. These re-ranking models achieved a substantial raise on the parsing performances. Given T (x), the set of k-best trees of a sentence x from a base parser, we use the popular mixture re-ranking strategy (Hayashi et al., 2013; Le and Mikolov, 2014), which is a combination of the our model and the base parser. yˆi = arg max αst (xi , y, Θ) + (1 − α)sb (xi , y) y∈T (xi ) (14) where α ∈ [0, 1] is a hyperparameter; st (xi , y"
P15-1112,Q14-1017,0,\N,Missing
P15-1168,W14-4012,0,0.0337609,"Missing"
P15-1168,D14-1179,0,0.0110592,"Missing"
P15-1168,D11-1090,0,0.0803899,"with them. This difference indicates that our model has well modeled the combinations of the characters and do not need much help of the feature engineering. The detailed results are shown in Table 4 (4-th and 6-th rows). Table 5 shows the comparisons of our model with the state-of-the-art systems on F-value. The model proposed by Zhang and Clark (2007) is a word-based segmentation method, which exploit features of complete words, while remains of the list are all character-based word segmenters, whose features are mostly extracted from the context characters. Moreover, some systems (such as Sun and Xu (2011) and Zhang et al. (2013)) also exploit kinds of extra information such as the unlabeled data or other knowledge. Although our model only uses simple bigram features, it outperforms the previous state-of-the-art methods which use more complex features. 1750 models P 92.8 93.7 96.0 (Zheng et al., 2013) (Pei et al., 2014) GRNN PKU R 92.0 93.4 95.7 F 92.4 93.5 95.9 MSRA R F 93.6 93.3 94.2 94.4 96.1 96.2 P 92.9 94.6 96.3 P 94.0* 94.4* 95.4 CTB6 R 93.1* 93.4* 95.2 F 93.6* 93.9* 95.3 Table 3: Performances on PKU, MSRA and CTB6 test sets with random initialized character embeddings. models +Pre-train"
P15-1168,I05-3017,0,0.516571,"Missing"
P15-1168,I05-3027,0,0.383422,"Missing"
P15-1168,P14-1028,0,0.732501,"he links denote the information flow, where the solid edges denote the acceptation of the combinations while the dashed edges means rejection of that. As shown in the right figure, we receive a score vector for tagging target character “地” by incorporating all the combination information. Introduction ∗ 雨 Rainy Recently, neural network models have been increasingly focused on for their ability to minimize the effort in feature engineering. Collobert et al. (2011) developed a general neural network architecture for sequence labeling tasks. Following this work, many methods (Zheng et al., 2013; Pei et al., 2014; Qi et al., 2014) applied the neural network to Chinese word segmentation and achieved a performance that approaches the state-of-the-art methods. However, these neural models just concatenate the embeddings of the context characters, and feed them into neural network. Since the concatenation operation is relatively simple, it is difficult to model the complicated features as the traditional discrete feature based models. Although the complicated interactions of inputs can be modeled by the deep neural network, the previous neural model shows that the deep model cannot outperform the one with"
P15-1168,O03-4002,0,0.922691,"Missing"
P15-1168,P13-4009,1,0.863934,"Missing"
P15-1168,P12-1083,0,0.0153951,"Missing"
P15-1168,P07-1106,0,0.77742,"ly use the simply bigram feature embeddings initialized by the average of two embeddings of consecutive characters element-wisely. Although the model of Pei et al. (2014) greatly benefits from the bigram feature embeddings, our model just obtains a small improvement with them. This difference indicates that our model has well modeled the combinations of the characters and do not need much help of the feature engineering. The detailed results are shown in Table 4 (4-th and 6-th rows). Table 5 shows the comparisons of our model with the state-of-the-art systems on F-value. The model proposed by Zhang and Clark (2007) is a word-based segmentation method, which exploit features of complete words, while remains of the list are all character-based word segmenters, whose features are mostly extracted from the context characters. Moreover, some systems (such as Sun and Xu (2011) and Zhang et al. (2013)) also exploit kinds of extra information such as the unlabeled data or other knowledge. Although our model only uses simple bigram features, it outperforms the previous state-of-the-art methods which use more complex features. 1750 models P 92.8 93.7 96.0 (Zheng et al., 2013) (Pei et al., 2014) GRNN PKU R 92.0 93"
P15-1168,D13-1031,0,0.108722,"rence indicates that our model has well modeled the combinations of the characters and do not need much help of the feature engineering. The detailed results are shown in Table 4 (4-th and 6-th rows). Table 5 shows the comparisons of our model with the state-of-the-art systems on F-value. The model proposed by Zhang and Clark (2007) is a word-based segmentation method, which exploit features of complete words, while remains of the list are all character-based word segmenters, whose features are mostly extracted from the context characters. Moreover, some systems (such as Sun and Xu (2011) and Zhang et al. (2013)) also exploit kinds of extra information such as the unlabeled data or other knowledge. Although our model only uses simple bigram features, it outperforms the previous state-of-the-art methods which use more complex features. 1750 models P 92.8 93.7 96.0 (Zheng et al., 2013) (Pei et al., 2014) GRNN PKU R 92.0 93.4 95.7 F 92.4 93.5 95.9 MSRA R F 93.6 93.3 94.2 94.4 96.1 96.2 P 92.9 94.6 96.3 P 94.0* 94.4* 95.4 CTB6 R 93.1* 93.4* 95.2 F 93.6* 93.9* 95.3 Table 3: Performances on PKU, MSRA and CTB6 test sets with random initialized character embeddings. models +Pre-train (Zheng et al., 2013) (Pe"
P15-1168,D13-1061,0,0.783938,"ons. Specifically, the links denote the information flow, where the solid edges denote the acceptation of the combinations while the dashed edges means rejection of that. As shown in the right figure, we receive a score vector for tagging target character “地” by incorporating all the combination information. Introduction ∗ 雨 Rainy Recently, neural network models have been increasingly focused on for their ability to minimize the effort in feature engineering. Collobert et al. (2011) developed a general neural network architecture for sequence labeling tasks. Following this work, many methods (Zheng et al., 2013; Pei et al., 2014; Qi et al., 2014) applied the neural network to Chinese word segmentation and achieved a performance that approaches the state-of-the-art methods. However, these neural models just concatenate the embeddings of the context characters, and feed them into neural network. Since the concatenation operation is relatively simple, it is difficult to model the complicated features as the traditional discrete feature based models. Although the complicated interactions of inputs can be modeled by the deep neural network, the previous neural model shows that the deep model cannot outpe"
P15-1168,P13-1045,0,0.181222,"ag j ∈ T (Collobert et al., 2011). 3 …… (1) where W1 ∈ RH2 ×H1 , b1 ∈ RH2 , hi ∈ RH2 . H2 is the number of hidden units in Layer 2. Here, w, H1 and H2 are hyper-parameters chosen on development set. Then, a similar linear transformation is performed without non-linear function followed: f (t|ci−w1 :i+w2 ) = W2 hi + b2 , xi …… Concatenate …… hi = g(W1 ai + b1 ), E MB S Recursive Neural Network A recursive neural network (RNN) is a kind of deep neural network created by applying the same set of weights recursively over a given structure(such as parsing tree) in topological order (Pollack, 1990; Socher et al., 2013a). In the simplest case, children nodes are combined into their parent node using a weight matrix W that is shared across the whole network, followed by a non-linear function g(·). Specifically, if hL and hR are d-dimensional vector representations of left and right children nodes respectively, 3.2 Gated Recursive Neural Network The RNN need a topological structure to model a sequence, such as a syntactic tree. In this paper, we use a directed acyclic graph (DAG), as showing in Figure 3, to model the combinations of the input characters, in which two consecutive nodes in the lower layer are c"
P16-1098,D15-1181,0,0.0282344,"nterest in text semantic matching and has achieved some great progresses (Hu et al., 2014; Qiu and Huang, 2015; Wan et al., 2016). According to their interaction ways, previous models can be classified into three categories: Weak interaction Models Some early works focus on sentence level interactions, such as ARCI(Hu et al., 2014), CNTN(Qiu and Huang, 2015) ∗ Corresponding author Strong Interaction Models Some models build the interaction at different granularity (word, phrase and sentence level), such as ARC-II (Hu et al., 2014), MultiGranCNN (Yin and Sch¨utze, 2015), Multi-Perspective CNN (He et al., 2015), MV-LSTM (Wan et al., 2016), MatchPyramid (Pang et al., 2016). The final matching score depends on these different levels of interactions. In this paper, we adopt a deep fusion strategy to model the strong interactions of two sentences. Given two texts x1:m and y1:n , we define a matching vector hi,j to represent the interaction of the subsequences x1:i and y1:j . hi,j depends on the matching vectors hs,t on previous interactions 1 ≤ s &lt; i and 1 ≤ t &lt; j. Thus, text matching can be regarded as modelling the interaction of two texts in a recursive matching way. Following this idea, we propose d"
P16-1098,D15-1075,0,0.0196488,"-by-word Attention LSTMs: An improved strategy of attention LSTMs, which introduces word-by-word attention mechanism and is proposed by (Rockt¨aschel et al., 2015). k 100 Train 77.9 Test 75.1 100 83.7 80.9 −0.4 −0.2 0 0.2 0.4 −0.2 100 84.8 77.6 running 100 83.2 82.3 pet 100 83.7 83.5 being 100 85.2 84.6 0.6 0.8 A young family enjoys feeling ocean waves lap at their feet by another Experiment-I: Recognizing Textual Entailment Recognizing textual entailment (RTE) is a task to determine the semantic relationship between two sentences. We use the Stanford Natural Language Inference Corpus (SNLI) (Bowman et al., 2015). This corpus contains 570K sentence pairs, and all of the sentences and labels stem from human annotators. SNLI is two orders of magnitude larger than all other existing RTE corpora. Therefore, the massive scale of SNLI allows us to train powerful neural networks such as our proposed architecture in this paper. Results Table 2 shows the evaluation results on SNLI. The 2nd column of the table gives the number of hidden states. From experimental results, we have several experimental findings. The results of DF-LSTMs outperform all the competitor models with the same number of hidden states whil"
P16-1098,D16-1053,0,0.0572037,"Missing"
P16-1098,D15-1280,1,0.535996,"ed, the input gate controls how much each unit is updated, and the output gate controls the exposure of the internal memory state. The update of each LSTM unit can be written precisely as (ht , ct ) = LSTM(ht−1 , ct−1 , xt ). (5) Here, the function LSTM(·, ·, ·) is a shorthand for Eq. (2-4). LSTM can map the input sequence of arbitrary length to a fixed-sized vector, and has been successfully applied to a wide range of NLP tasks, such as machine translation (Sutskever et al., 2014), language modelling (Sutskever et al., 2011), text matching (Rockt¨aschel et al., 2015) and text classification (Liu et al., 2015). 4 Deep Fusion LSTMs for Recursively Semantic Matching To deal with two sentences, one straightforward method is to model them with two separate LSTMs. However, this method is difficult to model local interactions of two sentences. Following the recursive matching strategy, we propose a neural model of deep fusion LSTMs (DF-LSTMs), which consists of two interdependent LSTMs to capture the inter- and intrainteractions between two sequences. Figure 2 gives an illustration of DF-LSTMs unit. To facilitate our model, we firstly give some definitions. Given two sequences X = x1 , x2 , · · · , xn an"
P16-1098,D14-1162,0,0.0877353,"Missing"
P16-1098,N15-1091,0,0.0620213,"Missing"
P16-1098,N16-1036,0,\N,Missing
P16-1140,W13-3520,0,0.0595384,"Missing"
P16-1140,P14-2133,0,0.0977274,"s achieved huge successes in many NLP applications. Despite its popularity, character-based approach also comes out as an equal competitor (Santos and Zadrozny, 2014; Kim et al., 2016; Ling et al., 2015b; Ling et al., 2015a; Faruqui et al., 2016; Ballesteros et al., 2015) . Moreover, questions arise when we consider what these models could capture from linguistic cues under the perspective of cross-language typological diversity, as is argued by Bender (2009). Despite previous efforts in empirically interpreting word embedding and exploring the intrinsic/extrinsic factors in learning process (Andreas and Klein, 2014; Lai et al., 2015; K¨ohn, 2015; Melamud et al., 2016), it remains unknown whether embedding models are really immune to the structural variance of languages. ∗ Corresponding author. c) How do the neurons of a model respond to linguistic features? Can we explain the utility of context and form by analyzing neuron activation pattern? 2 Experiment Design To study the proposed questions above, we design four series of experiments to comprehensively compare context-based and character-based word representations on different languages, covering syntactic, morphological and semantic properties. The"
P16-1140,D15-1041,0,0.0268622,"odel? Does a model behave similarly towards phylogenetically-related languages? b) Is word form a more efficient predictor of a certain grammatical function than word context for specific languages? Introduction Word representation is a core issue in natural language processing. Context-based word representation, which is inspired by Harris (1954), has achieved huge successes in many NLP applications. Despite its popularity, character-based approach also comes out as an equal competitor (Santos and Zadrozny, 2014; Kim et al., 2016; Ling et al., 2015b; Ling et al., 2015a; Faruqui et al., 2016; Ballesteros et al., 2015) . Moreover, questions arise when we consider what these models could capture from linguistic cues under the perspective of cross-language typological diversity, as is argued by Bender (2009). Despite previous efforts in empirically interpreting word embedding and exploring the intrinsic/extrinsic factors in learning process (Andreas and Klein, 2014; Lai et al., 2015; K¨ohn, 2015; Melamud et al., 2016), it remains unknown whether embedding models are really immune to the structural variance of languages. ∗ Corresponding author. c) How do the neurons of a model respond to linguistic features? C"
P16-1140,W09-0106,0,0.0107638,"oduction Word representation is a core issue in natural language processing. Context-based word representation, which is inspired by Harris (1954), has achieved huge successes in many NLP applications. Despite its popularity, character-based approach also comes out as an equal competitor (Santos and Zadrozny, 2014; Kim et al., 2016; Ling et al., 2015b; Ling et al., 2015a; Faruqui et al., 2016; Ballesteros et al., 2015) . Moreover, questions arise when we consider what these models could capture from linguistic cues under the perspective of cross-language typological diversity, as is argued by Bender (2009). Despite previous efforts in empirically interpreting word embedding and exploring the intrinsic/extrinsic factors in learning process (Andreas and Klein, 2014; Lai et al., 2015; K¨ohn, 2015; Melamud et al., 2016), it remains unknown whether embedding models are really immune to the structural variance of languages. ∗ Corresponding author. c) How do the neurons of a model respond to linguistic features? Can we explain the utility of context and form by analyzing neuron activation pattern? 2 Experiment Design To study the proposed questions above, we design four series of experiments to compre"
P16-1140,J81-4005,0,0.784035,"Missing"
P16-1140,de-marneffe-etal-2014-universal,0,0.0338399,"Missing"
P16-1140,P15-2076,0,0.03866,"r word-based model captures both semantic information and syntactic information, although the latter is not displayed as explicitly as the former. utility of pure word form and novelly point out the cross-language differences in word representation, which have been overlooked by huge amount of monolingual/bilingual research on well-studied languages. 6 7 Related works There have been a lot of research on interpreting or relating word embedding with linguistic features. Yogatama et al. (2014) projects word embedding into a sparse vector. They found some linguistically interpretable dimensions. Faruqui and Dyer (2015) use linguistic features to build word vector. Their results show that these representation of word meaning can also achieve good performance in the analogy and similarity tasks. These work can be regarded as the foreshadowing of our experiment paradigm that mapping dense vector to a sparse linguistic property space. Besides, a lot of study focus on empirical comparison of different word embedding model. Melamud et al. (2016) investigates the influence of context type and vector dimension on word embedding. Their main finding is that concatenating two different types of embeddings can still im"
P16-1140,N16-1077,0,0.0151527,"nce a word embedding model? Does a model behave similarly towards phylogenetically-related languages? b) Is word form a more efficient predictor of a certain grammatical function than word context for specific languages? Introduction Word representation is a core issue in natural language processing. Context-based word representation, which is inspired by Harris (1954), has achieved huge successes in many NLP applications. Despite its popularity, character-based approach also comes out as an equal competitor (Santos and Zadrozny, 2014; Kim et al., 2016; Ling et al., 2015b; Ling et al., 2015a; Faruqui et al., 2016; Ballesteros et al., 2015) . Moreover, questions arise when we consider what these models could capture from linguistic cues under the perspective of cross-language typological diversity, as is argued by Bender (2009). Despite previous efforts in empirically interpreting word embedding and exploring the intrinsic/extrinsic factors in learning process (Andreas and Klein, 2014; Lai et al., 2015; K¨ohn, 2015; Melamud et al., 2016), it remains unknown whether embedding models are really immune to the structural variance of languages. ∗ Corresponding author. c) How do the neurons of a model respon"
P16-1140,D15-1246,0,0.0819388,"Missing"
P16-1140,D15-1176,0,0.0323133,"Missing"
P16-1140,N16-1118,0,0.0491996,"ite its popularity, character-based approach also comes out as an equal competitor (Santos and Zadrozny, 2014; Kim et al., 2016; Ling et al., 2015b; Ling et al., 2015a; Faruqui et al., 2016; Ballesteros et al., 2015) . Moreover, questions arise when we consider what these models could capture from linguistic cues under the perspective of cross-language typological diversity, as is argued by Bender (2009). Despite previous efforts in empirically interpreting word embedding and exploring the intrinsic/extrinsic factors in learning process (Andreas and Klein, 2014; Lai et al., 2015; K¨ohn, 2015; Melamud et al., 2016), it remains unknown whether embedding models are really immune to the structural variance of languages. ∗ Corresponding author. c) How do the neurons of a model respond to linguistic features? Can we explain the utility of context and form by analyzing neuron activation pattern? 2 Experiment Design To study the proposed questions above, we design four series of experiments to comprehensively compare context-based and character-based word representations on different languages, covering syntactic, morphological and semantic properties. The basic paradigm is to decode interpretable linguistic f"
P16-1140,D15-1243,0,0.024935,"es, a lot of study focus on empirical comparison of different word embedding model. Melamud et al. (2016) investigates the influence of context type and vector dimension on word embedding. Their main finding is that concatenating two different types of embeddings can still improve performance even if the utility of dimensionality has run out. Andreas and Klein (2014) assess the potential syntactic information encoded in word embeddings by directly apply word embeddings to parser and they concluded that embeddings add redundant information to what the conventional parser has already extracted. Tsvetkov et al. (2015) propose a method to evaluate word embeddings through the alignment of distributional vectors and linguistic word vectors. However, the method still lacks a direct and comprehensive investigation of the utility of form, context and language typological diversity. This is exactly our novelty and contribution. It is worth noticing that K¨ohn (2015) evaluates multilingual word embedding and compares skip-gram, language model and other competitive embedding models. They show that dependencybased skip-gram embedding is effective, even at low dimension. Although K¨ohn (2015) work involves different"
P16-1163,D15-1262,0,0.816312,"on becomes much more challenging when such connectives are missing. In fact, such implicit discourse relations Intuitively, (good, wrong) and (good, ruined), seem to be the most informative word pairs, and it is likely that they will trigger a contrast relation. Therefore, we can see that another main disadvantage of using word pairs is the lack of contextual information, and using n-gram pairs will again suffer from data sparsity problem. Recently, the distributed word representations (Bengio et al., 2006; Mikolov et al., 2013) have shown an advantage when dealing with data sparsity problem (Braud and Denis, 2015), and many deep learning based models are generating substantial interests in text semantic matching and have achieved some significant progresses (Hu 1726 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1726–1735, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics Psyllium's not a good crop. You get a rain at the wrong time and the crop is ruined. Bidirectional LSTM Pooling Layer MLP f Gated Relevance Network Bilinear Tensor Single Layer Network Figure 1: The processing framework of the proposed approach. et al.,"
P16-1163,Q15-1024,0,0.374564,"for testing and the other sections for validation (Prasad et al., 2008). For comparison with the previous work (Pitler et al., 2009; Zhou et al., 2010; Park 1729 Table 1: The unbalanced sample distribution of PDTB. Relation Comparison Contingency Expansion Temporal Train 1894 3281 6792 665 Dev 401 628 1253 93 Table 2: Hyperparameters for our model in the experiment. Word Embedding size Initial learning rate Minibatch size Pooling Size Number of tensor slice Test 146 276 556 68 segments representation. The rest of the method is the same as Word-NTN. and Cardie, 2012; Rutherford and Xue, 2014; Ji and Eisenstein, 2015), we train four binary classifiers to identify each of the top level relations, the EntRel relations are merged with Expansion relations. For each classifier, we use an equal number of positive and negative samples as training data, because each of the relations except Expansion is infrequent (Pitler et al., 2009) as what shows in Table 1. The negative samples were chosen randomly from training sections 2-20. 3.2 3.2.1 • Word+GRN: We use the gated relevance network proposed in this paper to capture the semantic interaction scores between every word embedding pair of the two text segments. The"
P16-1163,P13-2013,0,0.594359,"ng@fudan.edu.cn Abstract outnumber explicit relations in naturally occurring text, and identify those relations have been shown to be the performance bottleneck of an end-to-end discourse parser (Lin et al., 2014). Most of the existing researches used rich linguistic features and supervised learning methods to achieve the task (Soricut and Marcu, 2003; Pitler et al., 2009; Rutherford and Xue, 2014). Among their works, word pairs are heavily used as an important feature, since word pairs like (warm,cold) might directly trigger a contrast relation. However, because of the data sparsity problem (McKeown and Biran, 2013) and the lack of metrics to measure the semantic relation between those pairs, which is so-called the semantic gap problem (Zhao and Grosky, 2002), the classifiers based on word pairs in the previous studies did not work well. Moreover, some text segment pairs are more complicated, it is hard to determine the relation held between them using only word pairs. Consider the following sentence pair with a casual relation as an example: Word pairs, which are one of the most easily accessible features between two text segments, have been proven to be very useful for detecting the discourse relations"
P16-1163,W12-1614,0,0.186442,"an implicit marker. Since explicit relations are easy to identify (Pitler et al., 2008), existing methods achieved good performance on the relations with explicit maker. In recent years, researchers mainly focused on implicit relations. For easily comparing with other methods, in this work, we also use PDTB as the training and testing corpus. As we mentioned above, various approaches have been proposed to do the task. Pitler et al. (2009) proposed to train four binary classifiers using word pairs as well as other rich linguistic features to automatically identify the top-level PDTB relations. Park and Cardie (2012) achieved a higher performance by optimizing the feature set. McKeown and Biran (2013) aims at solving the data sparsity problem, and they extended the work of Pitler et al. (2009) by aggregating word pairs. Rutherford and Xue (2014) used Brown clusters and coreferential patterns as new features and improved the baseline a lot. Braud and Denis (2015) compared different word representations for implicit relation classification. The word pairs feature have been studied by all of the work above, showing its importance on discourse relation. We follow their work, and incorporate word embedding to"
P16-1163,P09-1077,0,0.173917,"e with Gated Relevance Network Jifan Chen, Qi Zhang, Pengfei Liu, Xipeng Qiu, Xuanjing Huang Shanghai Key Laboratory of Data Science School of Computer Science, Fudan University 825 Zhangheng Road, Shanghai, China jfchen14,qz,pfliu14,xpqiu,xjhuang@fudan.edu.cn Abstract outnumber explicit relations in naturally occurring text, and identify those relations have been shown to be the performance bottleneck of an end-to-end discourse parser (Lin et al., 2014). Most of the existing researches used rich linguistic features and supervised learning methods to achieve the task (Soricut and Marcu, 2003; Pitler et al., 2009; Rutherford and Xue, 2014). Among their works, word pairs are heavily used as an important feature, since word pairs like (warm,cold) might directly trigger a contrast relation. However, because of the data sparsity problem (McKeown and Biran, 2013) and the lack of metrics to measure the semantic relation between those pairs, which is so-called the semantic gap problem (Zhao and Grosky, 2002), the classifiers based on word pairs in the previous studies did not work well. Moreover, some text segment pairs are more complicated, it is hard to determine the relation held between them using only w"
P16-1163,prasad-etal-2008-penn,0,0.92201,"ur model is trained end to end by BackPropagation and Adagrad. The main contribution of this paper can be summarized as follows: • We use word embeddings to replace the original words in the text segments to overcome data sparsity problem. In order to preserve the contextual information, we further encode the text segment to its positional representation through a recurrent neural network. • To deal with the semantic gap problem, we adopt a gated relevance network to capture the semantic interaction between the intermediate representations of the text segments. • Experimental results on PDTB (Prasad et al., 2008) show that the proposed method can achieve better performance in recognizing discourse level relations in all of the relations than the previous methods. 2 The Proposed Method The architecture of our proposed method is shown in figure 1. In the following of this section, we will illustrate the details of the proposed framework. 2.1 Embedding Layer To model the sentences with neural model, we firstly need to transform the one-hot representation of word into the distributed representation. All words of two text segments X and Y will be mapped into low dimensional vector representations, which ar"
P16-1163,E14-1068,0,0.467992,"e Network Jifan Chen, Qi Zhang, Pengfei Liu, Xipeng Qiu, Xuanjing Huang Shanghai Key Laboratory of Data Science School of Computer Science, Fudan University 825 Zhangheng Road, Shanghai, China jfchen14,qz,pfliu14,xpqiu,xjhuang@fudan.edu.cn Abstract outnumber explicit relations in naturally occurring text, and identify those relations have been shown to be the performance bottleneck of an end-to-end discourse parser (Lin et al., 2014). Most of the existing researches used rich linguistic features and supervised learning methods to achieve the task (Soricut and Marcu, 2003; Pitler et al., 2009; Rutherford and Xue, 2014). Among their works, word pairs are heavily used as an important feature, since word pairs like (warm,cold) might directly trigger a contrast relation. However, because of the data sparsity problem (McKeown and Biran, 2013) and the lack of metrics to measure the semantic relation between those pairs, which is so-called the semantic gap problem (Zhao and Grosky, 2002), the classifiers based on word pairs in the previous studies did not work well. Moreover, some text segment pairs are more complicated, it is hard to determine the relation held between them using only word pairs. Consider the fol"
P16-1163,N03-1030,0,0.090673,"on via a Deep Architecture with Gated Relevance Network Jifan Chen, Qi Zhang, Pengfei Liu, Xipeng Qiu, Xuanjing Huang Shanghai Key Laboratory of Data Science School of Computer Science, Fudan University 825 Zhangheng Road, Shanghai, China jfchen14,qz,pfliu14,xpqiu,xjhuang@fudan.edu.cn Abstract outnumber explicit relations in naturally occurring text, and identify those relations have been shown to be the performance bottleneck of an end-to-end discourse parser (Lin et al., 2014). Most of the existing researches used rich linguistic features and supervised learning methods to achieve the task (Soricut and Marcu, 2003; Pitler et al., 2009; Rutherford and Xue, 2014). Among their works, word pairs are heavily used as an important feature, since word pairs like (warm,cold) might directly trigger a contrast relation. However, because of the data sparsity problem (McKeown and Biran, 2013) and the lack of metrics to measure the semantic relation between those pairs, which is so-called the semantic gap problem (Zhao and Grosky, 2002), the classifiers based on word pairs in the previous studies did not work well. Moreover, some text segment pairs are more complicated, it is hard to determine the relation held betw"
P16-1163,P10-1040,0,0.0338307,"d+NTN: We use the neural tensor defined in (8) to capture the semantic interaction scores between every word embedding pair, the rest of the method is the same as our proposed method. • LSTM+NTN: We use two single LSTM to generate the positional text segments representation. The rest of the method is the same as Word-NTN. • BLSTM+NTN: We use two single bidirectional LSTM to generate the positional text nw = 50 ρ = 0.01 m = 32 (p, q) = (3, 3) r=2 3.2.2 Parameter Setting For the initialization of the word embeddings used in our model, we use the 50-dimensional pre-trained embeddings provided by Turian et al. (2010), and the embeddings are fixed during training. We only preserve the top 10,000 words according to its frequency of occurrence in the training data, all the text segments are padded to have the same length of 50, the intermediate representations of LSTM are also set to 50. The other parameters are initialized by randomly sampling from uniform distribution in [-0.1,0.1]. For other hyperparameters of our proposed model, we take those hyperparameters that achieved best performance on the development set, and keep the same parameters for other competitors. The final hyper-parameters are show in Ta"
P16-1163,K15-2001,0,0.0985963,"ation could help to determine which part of the two sentence should be focused when identifying their relation. 4 Related Work Discourse relations, which link clauses in text, are used to represent the overall text structure. Many downstream NLP tasks such as text summarization, question answering, and textual entailment can benefit from the task. Along with the increasing requirement, many works have been constructed to automatically identify these relations from different aspects (Pitler et al., 2008; Pitler et al., 2009; Zhou et al., 2010; McKeown and Biran, 2013; Rutherford and Xue, 2014; Xue et al., 2015). For training and comparing the performance of different methods, the Penn Discourse Treebank (PDTB) 2.0, which is large annotated discourse corpuses, were released in 2008 (Prasad et al., 2008). The annotation methodology of it follows the lexically grounded, predicate-argument approach. In PDTB, the discourse relations were predefined by Webber (2004). PDTB-styled discourse relations hold in only a local contextual window, and these relations are organized hierarchically. Also, every relation in PDTB has either an explicit or an implicit marker. Since explicit relations are easy to identify"
P16-1163,C10-2172,0,0.697668,"τ for parameter θτ,i . 3 3.1 Experiment Dataset The dataset we used in this work is Penn Discourse Treebank 2.0 (Prasad et al., 2008), which is one of the largest available annotated corpora of discourse relations. It contains 40,600 relations, which are manually annotated from the same 2,312 Wall Street Journal (WSJ) articles as the Penn Treebank. We follow the recommended section partition of PDTB 2.0, which is to use sections 2-20 for training, sections 21-22 for testing and the other sections for validation (Prasad et al., 2008). For comparison with the previous work (Pitler et al., 2009; Zhou et al., 2010; Park 1729 Table 1: The unbalanced sample distribution of PDTB. Relation Comparison Contingency Expansion Temporal Train 1894 3281 6792 665 Dev 401 628 1253 93 Table 2: Hyperparameters for our model in the experiment. Word Embedding size Initial learning rate Minibatch size Pooling Size Number of tensor slice Test 146 276 556 68 segments representation. The rest of the method is the same as Word-NTN. and Cardie, 2012; Rutherford and Xue, 2014; Ji and Eisenstein, 2015), we train four binary classifiers to identify each of the top level relations, the EntRel relations are merged with Expansion"
P16-1163,miltsakaki-etal-2004-penn,0,\N,Missing
P16-1163,C08-2022,0,\N,Missing
P16-1206,J96-1002,0,0.168189,"ntation is a fundamental task for Chinese language processing. In recent years, Chinese word segmentation (CWS) has undergone great development, which is, to some degree, driven by evaluation conferences of CWS, such as SIGHAN Bakeoffs (Emerson, 2005; Levow, 2006; Jin and Chen, 2008; Zhao and Liu, 2010). The current state-of-the-art methods regard word segmentation as a sequence labeling problem (Xue, 2003; Peng et al., 2004). The goal of sequence labeling is to assign labels to all elements in a sequence, which can be handled with supervised learning algorithms, such as maximum entropy (ME) (Berger et al., 1996), conditional random fields (CRF) (Lafferty et al., 2001) and Perceptron (Collins, 2002). ∗ Corresponding author. We release the word difficulty of the popular word segmentation datasets at http://nlp.fudan.edu.cn/data/ . 1 Benefiting from the public datasets and feature engineering, Chinese word segmentation achieves quite high precision after years of intensive research. To evaluate a word segmenter, the standard metric consists of precision p, recall r, and an evenly-weighted F-score f1 . However, with the successive improvement of performance, state-of-the-art segmenters are hard to be dis"
P16-1206,W02-1001,0,0.0813381,"Missing"
P16-1206,I05-3017,0,0.214398,"e very skewed word distribution at different levels of difficulty1 . The performance on a real evaluation shows that the proposed metric gives more reasonable and distinguishable scores and correlates well with human judgement. In addition, the proposed metric can be easily extended to evaluate other sequence labelling based NLP tasks. 1 Introduction Word segmentation is a fundamental task for Chinese language processing. In recent years, Chinese word segmentation (CWS) has undergone great development, which is, to some degree, driven by evaluation conferences of CWS, such as SIGHAN Bakeoffs (Emerson, 2005; Levow, 2006; Jin and Chen, 2008; Zhao and Liu, 2010). The current state-of-the-art methods regard word segmentation as a sequence labeling problem (Xue, 2003; Peng et al., 2004). The goal of sequence labeling is to assign labels to all elements in a sequence, which can be handled with supervised learning algorithms, such as maximum entropy (ME) (Berger et al., 1996), conditional random fields (CRF) (Lafferty et al., 2001) and Perceptron (Collins, 2002). ∗ Corresponding author. We release the word difficulty of the popular word segmentation datasets at http://nlp.fudan.edu.cn/data/ . 1 Benefi"
P16-1206,N12-1016,0,0.05536,"Missing"
P16-1206,P13-1167,0,0.0503991,"Missing"
P16-1206,P15-1174,0,0.0621592,"Missing"
P16-1206,D15-1013,0,0.0452988,"Missing"
P16-1206,I08-4010,0,0.0249064,"ion at different levels of difficulty1 . The performance on a real evaluation shows that the proposed metric gives more reasonable and distinguishable scores and correlates well with human judgement. In addition, the proposed metric can be easily extended to evaluate other sequence labelling based NLP tasks. 1 Introduction Word segmentation is a fundamental task for Chinese language processing. In recent years, Chinese word segmentation (CWS) has undergone great development, which is, to some degree, driven by evaluation conferences of CWS, such as SIGHAN Bakeoffs (Emerson, 2005; Levow, 2006; Jin and Chen, 2008; Zhao and Liu, 2010). The current state-of-the-art methods regard word segmentation as a sequence labeling problem (Xue, 2003; Peng et al., 2004). The goal of sequence labeling is to assign labels to all elements in a sequence, which can be handled with supervised learning algorithms, such as maximum entropy (ME) (Berger et al., 1996), conditional random fields (CRF) (Lafferty et al., 2001) and Perceptron (Collins, 2002). ∗ Corresponding author. We release the word difficulty of the popular word segmentation datasets at http://nlp.fudan.edu.cn/data/ . 1 Benefiting from the public datasets and"
P16-1206,H94-1018,0,0.356068,"6 shows that the performances of different models with our proposed evaluation metric are significantly correlated in two parallel tests. 0.6 5.4 Visualization of the Weight 0.4 0.3 0.4 0.5 0.6 0.7 fb on parallel test 1 Figure 6: Correlation between the evaluation results fb of two parallel testsets with the proposed metrics on a collection of models. The Pearson correlation is 0.9961, p = 0.000. 5.3 the proposed evaluation metric by comparing the evaluation results with human judgements. The evaluation results with our new metric correlated with human intuition well. Validity and Reliability Jones (1994) concluded some important criteria for the evaluation metrics of NLP system. It is very important to check the validity and reliability of a new metric. Previous section has displayed the validity of As is known, there might be some annotation inconsistency in the dataset. We find that most of the cases with high weight are really valuable difficult test cases, such as the visualized sentences from WB dataset in Figure 7. In the first sentence, the word ‘BMW 族’ (NOUN.People who take bus, metro and then walk to the destination) is an OOV word and contains English characters. The weight of this"
P16-1206,W06-0115,0,0.0338114,"ord distribution at different levels of difficulty1 . The performance on a real evaluation shows that the proposed metric gives more reasonable and distinguishable scores and correlates well with human judgement. In addition, the proposed metric can be easily extended to evaluate other sequence labelling based NLP tasks. 1 Introduction Word segmentation is a fundamental task for Chinese language processing. In recent years, Chinese word segmentation (CWS) has undergone great development, which is, to some degree, driven by evaluation conferences of CWS, such as SIGHAN Bakeoffs (Emerson, 2005; Levow, 2006; Jin and Chen, 2008; Zhao and Liu, 2010). The current state-of-the-art methods regard word segmentation as a sequence labeling problem (Xue, 2003; Peng et al., 2004). The goal of sequence labeling is to assign labels to all elements in a sequence, which can be handled with supervised learning algorithms, such as maximum entropy (ME) (Berger et al., 1996), conditional random fields (CRF) (Lafferty et al., 2001) and Perceptron (Collins, 2002). ∗ Corresponding author. We release the word difficulty of the popular word segmentation datasets at http://nlp.fudan.edu.cn/data/ . 1 Benefiting from the"
P16-1206,P02-1040,0,0.102989,"Missing"
P16-1206,C04-1081,0,0.0543738,"able scores and correlates well with human judgement. In addition, the proposed metric can be easily extended to evaluate other sequence labelling based NLP tasks. 1 Introduction Word segmentation is a fundamental task for Chinese language processing. In recent years, Chinese word segmentation (CWS) has undergone great development, which is, to some degree, driven by evaluation conferences of CWS, such as SIGHAN Bakeoffs (Emerson, 2005; Levow, 2006; Jin and Chen, 2008; Zhao and Liu, 2010). The current state-of-the-art methods regard word segmentation as a sequence labeling problem (Xue, 2003; Peng et al., 2004). The goal of sequence labeling is to assign labels to all elements in a sequence, which can be handled with supervised learning algorithms, such as maximum entropy (ME) (Berger et al., 1996), conditional random fields (CRF) (Lafferty et al., 2001) and Perceptron (Collins, 2002). ∗ Corresponding author. We release the word difficulty of the popular word segmentation datasets at http://nlp.fudan.edu.cn/data/ . 1 Benefiting from the public datasets and feature engineering, Chinese word segmentation achieves quite high precision after years of intensive research. To evaluate a word segmenter, the"
P16-1206,J02-1002,0,0.0540452,"Missing"
P16-1206,O03-4002,0,0.0971998,"distinguishable scores and correlates well with human judgement. In addition, the proposed metric can be easily extended to evaluate other sequence labelling based NLP tasks. 1 Introduction Word segmentation is a fundamental task for Chinese language processing. In recent years, Chinese word segmentation (CWS) has undergone great development, which is, to some degree, driven by evaluation conferences of CWS, such as SIGHAN Bakeoffs (Emerson, 2005; Levow, 2006; Jin and Chen, 2008; Zhao and Liu, 2010). The current state-of-the-art methods regard word segmentation as a sequence labeling problem (Xue, 2003; Peng et al., 2004). The goal of sequence labeling is to assign labels to all elements in a sequence, which can be handled with supervised learning algorithms, such as maximum entropy (ME) (Berger et al., 1996), conditional random fields (CRF) (Lafferty et al., 2001) and Perceptron (Collins, 2002). ∗ Corresponding author. We release the word difficulty of the popular word segmentation datasets at http://nlp.fudan.edu.cn/data/ . 1 Benefiting from the public datasets and feature engineering, Chinese word segmentation achieves quite high precision after years of intensive research. To evaluate a"
P16-1206,W10-4126,0,\N,Missing
P17-1001,P07-1056,0,0.397818,"Missing"
P17-1001,P14-1062,0,0.00708944,"text sequence x = {x1 , x2 , · · · , xT }, we first use a lookup layer to get the vector representation (embeddings) xi of the each word xi . The output at the last moment hT can be regarded as the representation of the whole sequence, which has a fully connected layer followed by a softmax non-linear layer that predicts the probability distribution over classes. There are many neural sentence models, which can be used for text modelling, involving recurrent neural networks (Sutskever et al., 2014; Chung et al., 2014; Liu et al., 2015a), convolutional neural networks (Collobert et al., 2011; Kalchbrenner et al., 2014), and recursive neural networks (Socher et al., 2013). Here we adopt recurrent neural network with long short-term memory (LSTM) due to their superior performance in various NLP tasks (Liu et al., 2016a; Lin et al., 2017). ˆ = softmax(WhT + b) y (5) ˆ is prediction probabilities, W is the where y weight which needs to be learned, b is a bias term. Given a corpus with N training samples (xi , yi ), the parameters of the network are trained to minimise the cross-entropy of the predicted and true distributions. Long Short-term Memory Long short-term memory network (LSTM) (Hochreiter and Schmidhub"
P17-1001,P16-1098,1,0.203704,"opose an adversarial multi-task framework, in which the shared and private feature spaces are inIntroduction Multi-task learning is an effective approach to improve the performance of a single task with the help of other related tasks. Recently, neuralbased models for multi-task learning have become very popular, ranging from computer vision (Misra et al., 2016; Zhang et al., 2014) to natural language processing (Collobert and Weston, 2008; Luong et al., 2015), since they provide a convenient way of combining information from multiple tasks. However, most existing work on multi-task learning (Liu et al., 2016c,b) attempts to divide the features of different tasks into private and shared spaces, merely based on whether parameters of 1 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1–10 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1001 forget gate ft , an output gate ot , a memory cell ct and a hidden state ht . d is the number of the LSTM units. The elements of the gating vectors it , ft and ot are in [0, 1]. The LSTM is precisely specified as follows. herently disjoint b"
P17-1001,D15-1280,1,0.52053,"urrent Models for Text Classification Text Classification with LSTM Given a text sequence x = {x1 , x2 , · · · , xT }, we first use a lookup layer to get the vector representation (embeddings) xi of the each word xi . The output at the last moment hT can be regarded as the representation of the whole sequence, which has a fully connected layer followed by a softmax non-linear layer that predicts the probability distribution over classes. There are many neural sentence models, which can be used for text modelling, involving recurrent neural networks (Sutskever et al., 2014; Chung et al., 2014; Liu et al., 2015a), convolutional neural networks (Collobert et al., 2011; Kalchbrenner et al., 2014), and recursive neural networks (Socher et al., 2013). Here we adopt recurrent neural network with long short-term memory (LSTM) due to their superior performance in various NLP tasks (Liu et al., 2016a; Lin et al., 2017). ˆ = softmax(WhT + b) y (5) ˆ is prediction probabilities, W is the where y weight which needs to be learned, b is a bias term. Given a corpus with N training samples (xi , yi ), the parameters of the network are trained to minimise the cross-entropy of the predicted and true distributions. L"
P17-1001,D16-1012,1,0.450341,"Missing"
P17-1001,N15-1092,0,0.0605154,"urrent Models for Text Classification Text Classification with LSTM Given a text sequence x = {x1 , x2 , · · · , xT }, we first use a lookup layer to get the vector representation (embeddings) xi of the each word xi . The output at the last moment hT can be regarded as the representation of the whole sequence, which has a fully connected layer followed by a softmax non-linear layer that predicts the probability distribution over classes. There are many neural sentence models, which can be used for text modelling, involving recurrent neural networks (Sutskever et al., 2014; Chung et al., 2014; Liu et al., 2015a), convolutional neural networks (Collobert et al., 2011; Kalchbrenner et al., 2014), and recursive neural networks (Socher et al., 2013). Here we adopt recurrent neural network with long short-term memory (LSTM) due to their superior performance in various NLP tasks (Liu et al., 2016a; Lin et al., 2017). ˆ = softmax(WhT + b) y (5) ˆ is prediction probabilities, W is the where y weight which needs to be learned, b is a bias term. Given a corpus with N training samples (xi , yi ), the parameters of the network are trained to minimise the cross-entropy of the predicted and true distributions. L"
P17-1001,P11-1015,0,0.0560746,"Missing"
P17-1001,P05-1015,0,0.33778,"Missing"
P17-1001,D14-1162,0,0.098307,"Missing"
P17-1001,D13-1170,0,0.00557314,"lookup layer to get the vector representation (embeddings) xi of the each word xi . The output at the last moment hT can be regarded as the representation of the whole sequence, which has a fully connected layer followed by a softmax non-linear layer that predicts the probability distribution over classes. There are many neural sentence models, which can be used for text modelling, involving recurrent neural networks (Sutskever et al., 2014; Chung et al., 2014; Liu et al., 2015a), convolutional neural networks (Collobert et al., 2011; Kalchbrenner et al., 2014), and recursive neural networks (Socher et al., 2013). Here we adopt recurrent neural network with long short-term memory (LSTM) due to their superior performance in various NLP tasks (Liu et al., 2016a; Lin et al., 2017). ˆ = softmax(WhT + b) y (5) ˆ is prediction probabilities, W is the where y weight which needs to be learned, b is a bias term. Given a corpus with N training samples (xi , yi ), the parameters of the network are trained to minimise the cross-entropy of the predicted and true distributions. Long Short-term Memory Long short-term memory network (LSTM) (Hochreiter and Schmidhuber, 1997) is a type of recurrent neural network (RNN)"
P17-1110,D16-1070,0,0.0244839,"for Chinese word segmentation or part-of-speech tagging (Jiang et al., 2009; Sun and Wan, 2012; Qiu et al., 2013; Li et al., 2015, 2016). These methods adopted stacking or multi-task architectures and showed that heterogeneous corpora can help each other. However, most of these model adopt the shallow linear classifier with discrete features, which makes it difficult to design the shared feature spaces, usually resulting in a complex model. Fortunately, recent deep neural models provide a convenient way to share information among multiple tasks (Collobert and Weston, 2008; Luong et al., 2015; Chen et al., 2016). In this paper, we propose an adversarial multicriteria learning for CWS by integrating shared knowledge from multiple segmentation criteria. Specifically, we regard each segmentation criterion as a single task and propose three different shared-private models under the framework of multi-task learning (Caruana, 1997; Ben-David and Schuller, 2003), where a shared layer is used to extract the criteria-invariant features, and a private layer is used to extract the criteria-specific features. Inspired by the success of adversarial strategy on domain adaption (Ajakan et al., 2014; Ganin et al., 2"
P17-1110,P09-1059,0,0.0124772,"tence “姚明进 入总决赛 (YaoMing reaches the final)”, the two commonly-used corpora, PKU’s People’s Daily (PKU) (Yu et al., 2001) and Penn Chinese Treebank (CTB) (Fei, 2000), use different segmentation criteria. In a sense, it is a waste of resources if we fail to fully exploit these corpora. ∗ Corresponding author. https://github.com/FudanNLP reaches 进入 进入 the final 总决赛 总 决赛 Table 1: Illustration of the different segmentation criteria. Introduction 1 Yao Ming 姚明 姚 明 Recently, some efforts have been made to exploit heterogeneous annotation data for Chinese word segmentation or part-of-speech tagging (Jiang et al., 2009; Sun and Wan, 2012; Qiu et al., 2013; Li et al., 2015, 2016). These methods adopted stacking or multi-task architectures and showed that heterogeneous corpora can help each other. However, most of these model adopt the shallow linear classifier with discrete features, which makes it difficult to design the shared feature spaces, usually resulting in a complex model. Fortunately, recent deep neural models provide a convenient way to share information among multiple tasks (Collobert and Weston, 2008; Luong et al., 2015; Chen et al., 2016). In this paper, we propose an adversarial multicriteria"
P17-1110,I08-4010,0,0.329481,"ters following uniform distribution at (−0.05, 0.05). We simply map traditional Chinese characters to simplified Chinese, and optimize on the same character embedding matrix across datasets, which is pre-trained on Chinese Wikipedia corpus, using word2vec toolkit (Mikolov et al., 2013). Following previous work (Chen et al., 2015b; Pei et al., 2014), all experiments including baseline results are using pre-trained character embedding with bigram feature. Datasets To evaluate our proposed architecture, we experiment on eight prevalent CWS datasets from SIGHAN2005 (Emerson, 2005) and SIGHAN2008 (Jin and Chen, 2008). Table 2 gives the details of the eight datasets. Among these datasets, AS, CITYU and CKIP are traditional Chinese, while the 6.3 Overall Results Table 3 shows the experiment results of the proposed models on test sets of eight CWS datasets, which has three blocks. (1) In the first block, we can see that the performance is boosted by using Bi-LSTM, and the 1197 Sighan05 Datasets MSRA AS PKU Sighan08 CTB CKIP CITYU NCC SXU Train Test Train Test Train Test Train Test Train Test Train Test Train Test Train Test Words 2.4M 0.1M 5.4M 0.1M 1.1M 0.2M 0.6M 0.1M 0.7M 0.1M 1.1M 0.2M 0.5M 0.1M 0.5M 0.1M"
P17-1110,P15-1168,1,0.876477,"ayer Chinese word segmentation task is usually regarded as a character based sequence labeling problem. Specifically, each character in a sentence is labeled as one of L = {B, M, E, S}, indicating the begin, middle, end of a word, or a word with single character. There are lots of prevalent methods to solve sequence labeling problem such as maximum entropy Markov model (MEMM), conditional random fields (CRF), etc. Recently, neural networks are widely applied to Chinese word segmentation task for their ability to minimize the effort in feature engineering (Zheng et al., 2013; Pei et al., 2014; Chen et al., 2015a,b). Specifically, given a sequence with n characters X = {x1 , . . . , xn }, the aim of CWS task is to figure out the ground truth of labels Y ∗ = {y1∗ , . . . , yn∗ }: Y ∗ = arg max p(Y |X), Inference Layer (1) Y ∈Ln where L = {B, M, E, S}. The general architecture of neural CWS could be characterized by three components: (1) a character embedding layer; (2) feature layers consisting of several classical neural networks and (3) a tag inference layer. The role of feature layers is to extract features, which could be either convolution neural network or recurrent neural network. In this In ne"
P17-1110,D15-1141,1,0.885499,"Missing"
P17-1110,P15-1172,0,0.0239503,"monly-used corpora, PKU’s People’s Daily (PKU) (Yu et al., 2001) and Penn Chinese Treebank (CTB) (Fei, 2000), use different segmentation criteria. In a sense, it is a waste of resources if we fail to fully exploit these corpora. ∗ Corresponding author. https://github.com/FudanNLP reaches 进入 进入 the final 总决赛 总 决赛 Table 1: Illustration of the different segmentation criteria. Introduction 1 Yao Ming 姚明 姚 明 Recently, some efforts have been made to exploit heterogeneous annotation data for Chinese word segmentation or part-of-speech tagging (Jiang et al., 2009; Sun and Wan, 2012; Qiu et al., 2013; Li et al., 2015, 2016). These methods adopted stacking or multi-task architectures and showed that heterogeneous corpora can help each other. However, most of these model adopt the shallow linear classifier with discrete features, which makes it difficult to design the shared feature spaces, usually resulting in a complex model. Fortunately, recent deep neural models provide a convenient way to share information among multiple tasks (Collobert and Weston, 2008; Luong et al., 2015; Chen et al., 2016). In this paper, we propose an adversarial multicriteria learning for CWS by integrating shared knowledge from"
P17-1110,I05-3017,0,0.16536,"zation, we randomize all parameters following uniform distribution at (−0.05, 0.05). We simply map traditional Chinese characters to simplified Chinese, and optimize on the same character embedding matrix across datasets, which is pre-trained on Chinese Wikipedia corpus, using word2vec toolkit (Mikolov et al., 2013). Following previous work (Chen et al., 2015b; Pei et al., 2014), all experiments including baseline results are using pre-trained character embedding with bigram feature. Datasets To evaluate our proposed architecture, we experiment on eight prevalent CWS datasets from SIGHAN2005 (Emerson, 2005) and SIGHAN2008 (Jin and Chen, 2008). Table 2 gives the details of the eight datasets. Among these datasets, AS, CITYU and CKIP are traditional Chinese, while the 6.3 Overall Results Table 3 shows the experiment results of the proposed models on test sets of eight CWS datasets, which has three blocks. (1) In the first block, we can see that the performance is boosted by using Bi-LSTM, and the 1197 Sighan05 Datasets MSRA AS PKU Sighan08 CTB CKIP CITYU NCC SXU Train Test Train Test Train Test Train Test Train Test Train Test Train Test Train Test Words 2.4M 0.1M 5.4M 0.1M 1.1M 0.2M 0.6M 0.1M 0.7"
P17-1110,D16-1072,0,0.080826,"Missing"
P17-1110,D16-1012,1,0.0679631,"Missing"
P17-1110,P14-1028,0,0.139815,"S. 2.1 Embedding layer Chinese word segmentation task is usually regarded as a character based sequence labeling problem. Specifically, each character in a sentence is labeled as one of L = {B, M, E, S}, indicating the begin, middle, end of a word, or a word with single character. There are lots of prevalent methods to solve sequence labeling problem such as maximum entropy Markov model (MEMM), conditional random fields (CRF), etc. Recently, neural networks are widely applied to Chinese word segmentation task for their ability to minimize the effort in feature engineering (Zheng et al., 2013; Pei et al., 2014; Chen et al., 2015a,b). Specifically, given a sequence with n characters X = {x1 , . . . , xn }, the aim of CWS task is to figure out the ground truth of labels Y ∗ = {y1∗ , . . . , yn∗ }: Y ∗ = arg max p(Y |X), Inference Layer (1) Y ∈Ln where L = {B, M, E, S}. The general architecture of neural CWS could be characterized by three components: (1) a character embedding layer; (2) feature layers consisting of several classical neural networks and (3) a tag inference layer. The role of feature layers is to extract features, which could be either convolution neural network or recurrent neural net"
P17-1110,D13-1062,1,0.685854,"nal)”, the two commonly-used corpora, PKU’s People’s Daily (PKU) (Yu et al., 2001) and Penn Chinese Treebank (CTB) (Fei, 2000), use different segmentation criteria. In a sense, it is a waste of resources if we fail to fully exploit these corpora. ∗ Corresponding author. https://github.com/FudanNLP reaches 进入 进入 the final 总决赛 总 决赛 Table 1: Illustration of the different segmentation criteria. Introduction 1 Yao Ming 姚明 姚 明 Recently, some efforts have been made to exploit heterogeneous annotation data for Chinese word segmentation or part-of-speech tagging (Jiang et al., 2009; Sun and Wan, 2012; Qiu et al., 2013; Li et al., 2015, 2016). These methods adopted stacking or multi-task architectures and showed that heterogeneous corpora can help each other. However, most of these model adopt the shallow linear classifier with discrete features, which makes it difficult to design the shared feature spaces, usually resulting in a complex model. Fortunately, recent deep neural models provide a convenient way to share information among multiple tasks (Collobert and Weston, 2008; Luong et al., 2015; Chen et al., 2016). In this paper, we propose an adversarial multicriteria learning for CWS by integrating share"
P17-1110,P12-1025,0,0.0454498,"Ming reaches the final)”, the two commonly-used corpora, PKU’s People’s Daily (PKU) (Yu et al., 2001) and Penn Chinese Treebank (CTB) (Fei, 2000), use different segmentation criteria. In a sense, it is a waste of resources if we fail to fully exploit these corpora. ∗ Corresponding author. https://github.com/FudanNLP reaches 进入 进入 the final 总决赛 总 决赛 Table 1: Illustration of the different segmentation criteria. Introduction 1 Yao Ming 姚明 姚 明 Recently, some efforts have been made to exploit heterogeneous annotation data for Chinese word segmentation or part-of-speech tagging (Jiang et al., 2009; Sun and Wan, 2012; Qiu et al., 2013; Li et al., 2015, 2016). These methods adopted stacking or multi-task architectures and showed that heterogeneous corpora can help each other. However, most of these model adopt the shallow linear classifier with discrete features, which makes it difficult to design the shared feature spaces, usually resulting in a complex model. Fortunately, recent deep neural models provide a convenient way to share information among multiple tasks (Collobert and Weston, 2008; Luong et al., 2015; Chen et al., 2016). In this paper, we propose an adversarial multicriteria learning for CWS by"
P17-1110,D13-1061,0,0.075779,"l architecture of CWS. 2.1 Embedding layer Chinese word segmentation task is usually regarded as a character based sequence labeling problem. Specifically, each character in a sentence is labeled as one of L = {B, M, E, S}, indicating the begin, middle, end of a word, or a word with single character. There are lots of prevalent methods to solve sequence labeling problem such as maximum entropy Markov model (MEMM), conditional random fields (CRF), etc. Recently, neural networks are widely applied to Chinese word segmentation task for their ability to minimize the effort in feature engineering (Zheng et al., 2013; Pei et al., 2014; Chen et al., 2015a,b). Specifically, given a sequence with n characters X = {x1 , . . . , xn }, the aim of CWS task is to figure out the ground truth of labels Y ∗ = {y1∗ , . . . , yn∗ }: Y ∗ = arg max p(Y |X), Inference Layer (1) Y ∈Ln where L = {B, M, E, S}. The general architecture of neural CWS could be characterized by three components: (1) a character embedding layer; (2) feature layers consisting of several classical neural networks and (3) a tag inference layer. The role of feature layers is to extract features, which could be either convolution neural network or re"
P18-1233,P07-1056,0,0.20628,"the target labels are used. The entire procedure is described in Algorithm 1. 3.4 i=1 lt X (7) where α, γ, and λ are weights that control the interaction of the loss terms. L(θ) means that loss, L, is optimized on the parameters θ during training. And Lc denotes the classification loss on the domain invariant representation, which 4 lt 1X −Yti log Ft (Yti |Et (Lit )) lt (10) i=1 Experiment 4.1 Dataset Domain adaptation for sentiment classification has been widely studied in the NLP community. The major experiments were performed on the benchmark made of reviews of Amazon products gathered by Blitzer et al. (2007). This data set1 contains Amazon product reviews from four different domains: Books, DVD, Electronics, and Kitchen appliances from Amazon.com. Each review was originally associated with a rating of 15 stars. For simplicity, we are only concerned with whether or not a review is positive (higher than 3 stars) or negative (3 stars or lower). Reviews are encoded in 5,000 dimensional tf-idf feature vectors of bag-of-words unigrams and bigrams. From this data, we constructed 12 cross-domain binary classification tasks. Each domain adaptation task consists of 2,000 labeled source examples, 2509 1 htt"
P18-2033,D17-1237,1,0.889592,"Missing"
P18-2033,N07-2038,0,0.0554368,"the turn information. In terms of the representation vector of symptoms, it’s dimension is equal to the number of all symptoms, whose elements for positive symptoms are 1, negative symptoms are -1, notsure symptoms are −2 and not-mentioned sympUser Simulator At the beginning of each dialogue session, a user simulator samples a user goal from the experiment dataset. At each turn t, the user takes an action au,t according to the current user state su,t and the previous agent action at−1 , and transits into the next user state su,t+1 . In practice, the user state su is factored into an agenda A (Schatzmann et al., 2007) and a goal G, noted as su = (A, G). During the course of the dialogue, the goal G ensures that the user behaves in a consistent, goal-oriented manner. And the agenda contains a list of symptoms and their status (whether or not they are requested) to track the progress of the conversation. Every dialogue session is initiated by the user 203 4 toms are 0. Each state s ∈ S is the concatenation of these four vectors. Actions A. An action a ∈ A is composed of a dialogue act (e.g., inform, request, deny and confirm) and a slot (i.e., normalized symptoms or a special slot disease). In addition, than"
P18-2033,I17-1074,0,0.222406,"agnose tests, vital signs and medical image. And it is collected accumulatively following a diagnostic procedure in clinic, which involves interactions between patients and doctors and some complicated medical tests. Therefore, it is very expensive to collect EHRs for different diseases. How to collect the information from patient automatically remains the challenge for automatic diagnosis. Recently, due to its promising potentials and alluring commercial values, research about taskoriented dialogue system (DS) has attracted increasing attention in different domains, including ticket booking (Li et al., 2017; Peng et al., 2017a), online shopping (Yan et al., 2017) and restaurant searching (Wen et al., 2017). We believe that applying DS in the medical domain has great potential to reduce the cost of collecting data from patients. However, there is a gap to fill for applying DS in disease identification. There are basically two major challenges. First, the lack of annotated medical dialogue dataset. Second, no available DS framework for disease identification. By addressing these two problems, we make the first move to build a dialogue system facilitating automatic information collection and diagno"
P18-2033,E17-1042,0,0.0993595,"Missing"
P19-1100,P18-3015,0,0.612259,"ering problem? Understanding the above questions can not only help us to choose suitable architectures in different application scenarios, but motivate us to move forward to more powerful frameworks. External Transferable Knowledge and Learning schemas Clearly, the improvement in accuracy and performance is not merely because of the shift from feature engineering to structure engineering, but the flexible ways to incorporate external knowledge (Mikolov et al., 2013; Peters et al., 2018a; Devlin et al., 2018) and learning schemas to introduce extra instructive constraints (Paulus et al., 2017; Arumae and Liu, 2018). For this part, we make some first steps toward answers to the following questions: 1) Which type of pre-trained models (supervised or unsupervised pre-training) is more friendly to the summarization task? 2) When architectures are explored exhaustively, can we push the state-of-the-art results to a new level by introducing external transferable knowledge or changing another learning schema? To make a comprehensive study of above an1049 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1049–1058 c Florence, Italy, July 28 - August 2, 2019. 2019 Ass"
P19-1100,N18-1150,0,0.0446013,"ow neural extractive summarization systems could benefit from different types of model architectures, transferable knowledge and learning schemas. Additionally, we find an effective way to improve current frameworks and achieve the state-ofthe-art result on CNN/DailyMail by a large margin based on our observations and analyses. Hopefully, our work could provide more clues for future research on extractive summarization. Source code will be available on Github1 . 1 Introduction Recent years has seen remarkable success in the use of deep neural networks for text summarization (See et al., 2017; Celikyilmaz et al., 2018; Jadhav and Rajan, 2018). So far, most research utilizing the neural network for text summarization has revolved around architecture engineering (Zhou et al., 2018; Chen and Bansal, 2018; Gehrmann et al., 2018). Despite their success, it remains poorly understood why they perform well and what their shortcomings are, which limits our ability to design better architectures. The rapid development of neural architectures calls for a detailed empirical study of analyzing and understanding existing models. In this paper, we primarily focus on extractive summarization since they are computationally"
P19-1100,P18-1063,0,0.35113,"mprove current frameworks and achieve the state-ofthe-art result on CNN/DailyMail by a large margin based on our observations and analyses. Hopefully, our work could provide more clues for future research on extractive summarization. Source code will be available on Github1 . 1 Introduction Recent years has seen remarkable success in the use of deep neural networks for text summarization (See et al., 2017; Celikyilmaz et al., 2018; Jadhav and Rajan, 2018). So far, most research utilizing the neural network for text summarization has revolved around architecture engineering (Zhou et al., 2018; Chen and Bansal, 2018; Gehrmann et al., 2018). Despite their success, it remains poorly understood why they perform well and what their shortcomings are, which limits our ability to design better architectures. The rapid development of neural architectures calls for a detailed empirical study of analyzing and understanding existing models. In this paper, we primarily focus on extractive summarization since they are computationally efficient, and can generate grammatically and coherent summaries (Nallapati et al., 2017). and seek to ∗ These two authors contributed equally. Corresponding author. 1 https://github.com"
P19-1100,P16-1046,0,0.224131,"able knowledge influence extractive summarization and a more popular neural architecture, Transformer. Besides, we come to inconsistent conclusions when analyzing the auto-regressive decoder. More importantly, our paper also shows how existing systems can be improved, and we have achieved a state-of-the-art performance on CNN/DailyMail. Extractive Summarization Most of recent work attempt to explore different neural components or their combinations to build an end-to-end learning model. Specifically, these work instantiate their encoder-decoder framework by choosing recurrent neural networks (Cheng and Lapata, 2016; Nallapati et al., 2017; Zhou et al., 2018) as encoder, auto-regressive decoder (Chen and 1050 Bansal, 2018; Jadhav and Rajan, 2018; Zhou et al., 2018) or non auto-regressive decoder (Isonuma et al., 2017; Narayan et al., 2018; Arumae and Liu, 2018) as decoder, based on pre-trained word representations (Mikolov et al., 2013; Pennington et al., 2014). However, how to use Transformer in extractive summarization is still a missing issue. In addition, some work uses reinforcement learning technique (Narayan et al., 2018; Wu and Hu, 2018; Chen and Bansal, 2018), which can provide more direct optim"
P19-1100,D18-1409,0,0.714632,"Missing"
P19-1100,D18-1443,0,0.128821,"ks and achieve the state-ofthe-art result on CNN/DailyMail by a large margin based on our observations and analyses. Hopefully, our work could provide more clues for future research on extractive summarization. Source code will be available on Github1 . 1 Introduction Recent years has seen remarkable success in the use of deep neural networks for text summarization (See et al., 2017; Celikyilmaz et al., 2018; Jadhav and Rajan, 2018). So far, most research utilizing the neural network for text summarization has revolved around architecture engineering (Zhou et al., 2018; Chen and Bansal, 2018; Gehrmann et al., 2018). Despite their success, it remains poorly understood why they perform well and what their shortcomings are, which limits our ability to design better architectures. The rapid development of neural architectures calls for a detailed empirical study of analyzing and understanding existing models. In this paper, we primarily focus on extractive summarization since they are computationally efficient, and can generate grammatically and coherent summaries (Nallapati et al., 2017). and seek to ∗ These two authors contributed equally. Corresponding author. 1 https://github.com/fastnlp/fastNLP † Archi"
P19-1100,N18-1065,0,0.119485,"Missing"
P19-1100,D17-1223,0,0.0153301,"y, our paper also shows how existing systems can be improved, and we have achieved a state-of-the-art performance on CNN/DailyMail. Extractive Summarization Most of recent work attempt to explore different neural components or their combinations to build an end-to-end learning model. Specifically, these work instantiate their encoder-decoder framework by choosing recurrent neural networks (Cheng and Lapata, 2016; Nallapati et al., 2017; Zhou et al., 2018) as encoder, auto-regressive decoder (Chen and 1050 Bansal, 2018; Jadhav and Rajan, 2018; Zhou et al., 2018) or non auto-regressive decoder (Isonuma et al., 2017; Narayan et al., 2018; Arumae and Liu, 2018) as decoder, based on pre-trained word representations (Mikolov et al., 2013; Pennington et al., 2014). However, how to use Transformer in extractive summarization is still a missing issue. In addition, some work uses reinforcement learning technique (Narayan et al., 2018; Wu and Hu, 2018; Chen and Bansal, 2018), which can provide more direct optimization goals. Although above work improves the performance of summarization system from different perspectives, yet a comprehensive study remains missing. 3 A Testbed for Text Summarization To analyze neu"
P19-1100,P18-1014,0,0.461649,"rization systems could benefit from different types of model architectures, transferable knowledge and learning schemas. Additionally, we find an effective way to improve current frameworks and achieve the state-ofthe-art result on CNN/DailyMail by a large margin based on our observations and analyses. Hopefully, our work could provide more clues for future research on extractive summarization. Source code will be available on Github1 . 1 Introduction Recent years has seen remarkable success in the use of deep neural networks for text summarization (See et al., 2017; Celikyilmaz et al., 2018; Jadhav and Rajan, 2018). So far, most research utilizing the neural network for text summarization has revolved around architecture engineering (Zhou et al., 2018; Chen and Bansal, 2018; Gehrmann et al., 2018). Despite their success, it remains poorly understood why they perform well and what their shortcomings are, which limits our ability to design better architectures. The rapid development of neural architectures calls for a detailed empirical study of analyzing and understanding existing models. In this paper, we primarily focus on extractive summarization since they are computationally efficient, and can gener"
P19-1100,D18-1208,0,0.308389,"g a right direction. 2 Related Work The work is connected to the following threads of work of NLP research. Task-oriented Neural Networks Interpreting Without knowing the internal working mechanism of the neural network, it is easy for us to get into a hobble when the performance of a task has reached the bottleneck. More recently, Peters et al. (2018b) investigate how different learning frameworks influence the properties of learned contextualized representations. Different from this work, in this paper, we focus on dissecting the neural models for text summarization. A similar work to us is Kedzie et al. (2018), which studies how deep learning models perform context selection in terms of several typical summarization architectures, and domains. Compared with this work, we make a more comprehensive study and give more different analytic aspects. For example, we additionally investigate how transferable knowledge influence extractive summarization and a more popular neural architecture, Transformer. Besides, we come to inconsistent conclusions when analyzing the auto-regressive decoder. More importantly, our paper also shows how existing systems can be improved, and we have achieved a state-of-the-art"
P19-1100,D14-1181,0,0.0110666,"Missing"
P19-1100,P16-1098,1,0.838798,"te the LSTM-based structure and the Transformer structure, both of which have proven to be effective and achieved the state-of-the-art results in many other NLP tasks. Notably, to let the model make the best of its structural bias, stacking deep layers is allowed. LSTM Layer Long short-term memory network (LSTM) was proposed by (Hochreiter and Schmidhuber, 1997) to specifically address this issue of learning long-term dependencies, which has proven to be effective in a wide range of NLP tasks, such as text classification (Liu et al., 2017, 2016b), semantic matching (Rockt¨aschel et al., 2015; Liu et al., 2016a), text summarization (Rush et al., 2015) and machine translation (Sutskever et al., 2014). Transformer Layer Transformer (Vaswani et al., 2017) is essentially a feed-forward selfattention architecture, which achieves pairwise interaction by attention mechanism. Recently, Transformer has achieved great success in many other NLP tasks (Vaswani et al., 2017; Dai et al., 2018), and it is appealing to know how this neural module performs on text summarization task. 3.2.3 Decoder Decoder is used to extract a subset of sentences from the original document based on contextualized representations: s1"
P19-1100,P17-1001,1,0.855402,"zed representations s1 , · · · , sn . To achieve this goal, we investigate the LSTM-based structure and the Transformer structure, both of which have proven to be effective and achieved the state-of-the-art results in many other NLP tasks. Notably, to let the model make the best of its structural bias, stacking deep layers is allowed. LSTM Layer Long short-term memory network (LSTM) was proposed by (Hochreiter and Schmidhuber, 1997) to specifically address this issue of learning long-term dependencies, which has proven to be effective in a wide range of NLP tasks, such as text classification (Liu et al., 2017, 2016b), semantic matching (Rockt¨aschel et al., 2015; Liu et al., 2016a), text summarization (Rush et al., 2015) and machine translation (Sutskever et al., 2014). Transformer Layer Transformer (Vaswani et al., 2017) is essentially a feed-forward selfattention architecture, which achieves pairwise interaction by attention mechanism. Recently, Transformer has achieved great success in many other NLP tasks (Vaswani et al., 2017; Dai et al., 2018), and it is appealing to know how this neural module performs on text summarization task. 3.2.3 Decoder Decoder is used to extract a subset of sentence"
P19-1100,K16-1028,0,0.137143,"Missing"
P19-1100,N18-1158,0,0.164021,"s how existing systems can be improved, and we have achieved a state-of-the-art performance on CNN/DailyMail. Extractive Summarization Most of recent work attempt to explore different neural components or their combinations to build an end-to-end learning model. Specifically, these work instantiate their encoder-decoder framework by choosing recurrent neural networks (Cheng and Lapata, 2016; Nallapati et al., 2017; Zhou et al., 2018) as encoder, auto-regressive decoder (Chen and 1050 Bansal, 2018; Jadhav and Rajan, 2018; Zhou et al., 2018) or non auto-regressive decoder (Isonuma et al., 2017; Narayan et al., 2018; Arumae and Liu, 2018) as decoder, based on pre-trained word representations (Mikolov et al., 2013; Pennington et al., 2014). However, how to use Transformer in extractive summarization is still a missing issue. In addition, some work uses reinforcement learning technique (Narayan et al., 2018; Wu and Hu, 2018; Chen and Bansal, 2018), which can provide more direct optimization goals. Although above work improves the performance of summarization system from different perspectives, yet a comprehensive study remains missing. 3 A Testbed for Text Summarization To analyze neural summarization syst"
P19-1100,D14-1162,0,0.0931397,"Enc. represent decoder and encoder respectively. Sup. denotes supervised learning and NEWS. means supervised pre-training knowledge. alytical perspectives, we first build a testbed for summarization system, in which training and testing environment will be constructed. In the training environment, we design different summarization models to analyze how they influence the performance. Specifically, these models differ in the types of architectures (Encoders: CNN, LSTM, Transformer (Vaswani et al., 2017); Decoders: auto-regressive2 , non auto-regressive), external transferable knowledge (GloVe (Pennington et al., 2014), BERT (Devlin et al., 2018), N EWSROOM (Grusky et al., 2018)) and different learning schemas (supervised learning and reinforcement learning). To peer into the internal working mechanism of above testing cases, we provide sufficient evaluation scenarios in the testing environment. Concretely, we present a multi-domain test, sentence shuffling test, and analyze models by different metrics: repetition, sentence length, and position bias, which we additionally developed to provide a better understanding of the characteristics of different datasets. Empirically, our main observations are summariz"
P19-1100,N18-1202,0,0.151942,"for a detailed empirical study of analyzing and understanding existing models. In this paper, we primarily focus on extractive summarization since they are computationally efficient, and can generate grammatically and coherent summaries (Nallapati et al., 2017). and seek to ∗ These two authors contributed equally. Corresponding author. 1 https://github.com/fastnlp/fastNLP † Architectures Architecturally, the better performance usually comes at the cost of our understanding of the system. To date, we know little about the functionality of each neural component and the differences between them (Peters et al., 2018b), which raises the following typical questions: 1) How does the choice of different neural architectures (CNN, RNN, Transformer) influence the performance of the summarization system? 2) Which part of components matters for specific dataset? 3) Do current models suffer from the over-engineering problem? Understanding the above questions can not only help us to choose suitable architectures in different application scenarios, but motivate us to move forward to more powerful frameworks. External Transferable Knowledge and Learning schemas Clearly, the improvement in accuracy and performance is"
P19-1100,D15-1044,0,0.131669,"sformer structure, both of which have proven to be effective and achieved the state-of-the-art results in many other NLP tasks. Notably, to let the model make the best of its structural bias, stacking deep layers is allowed. LSTM Layer Long short-term memory network (LSTM) was proposed by (Hochreiter and Schmidhuber, 1997) to specifically address this issue of learning long-term dependencies, which has proven to be effective in a wide range of NLP tasks, such as text classification (Liu et al., 2017, 2016b), semantic matching (Rockt¨aschel et al., 2015; Liu et al., 2016a), text summarization (Rush et al., 2015) and machine translation (Sutskever et al., 2014). Transformer Layer Transformer (Vaswani et al., 2017) is essentially a feed-forward selfattention architecture, which achieves pairwise interaction by attention mechanism. Recently, Transformer has achieved great success in many other NLP tasks (Vaswani et al., 2017; Dai et al., 2018), and it is appealing to know how this neural module performs on text summarization task. 3.2.3 Decoder Decoder is used to extract a subset of sentences from the original document based on contextualized representations: s1 , · · · , sn . Most existing architecture"
P19-1100,P17-1099,0,0.722919,"etter understand how neural extractive summarization systems could benefit from different types of model architectures, transferable knowledge and learning schemas. Additionally, we find an effective way to improve current frameworks and achieve the state-ofthe-art result on CNN/DailyMail by a large margin based on our observations and analyses. Hopefully, our work could provide more clues for future research on extractive summarization. Source code will be available on Github1 . 1 Introduction Recent years has seen remarkable success in the use of deep neural networks for text summarization (See et al., 2017; Celikyilmaz et al., 2018; Jadhav and Rajan, 2018). So far, most research utilizing the neural network for text summarization has revolved around architecture engineering (Zhou et al., 2018; Chen and Bansal, 2018; Gehrmann et al., 2018). Despite their success, it remains poorly understood why they perform well and what their shortcomings are, which limits our ability to design better architectures. The rapid development of neural architectures calls for a detailed empirical study of analyzing and understanding existing models. In this paper, we primarily focus on extractive summarization sinc"
P19-1100,P18-1061,0,0.289883,"effective way to improve current frameworks and achieve the state-ofthe-art result on CNN/DailyMail by a large margin based on our observations and analyses. Hopefully, our work could provide more clues for future research on extractive summarization. Source code will be available on Github1 . 1 Introduction Recent years has seen remarkable success in the use of deep neural networks for text summarization (See et al., 2017; Celikyilmaz et al., 2018; Jadhav and Rajan, 2018). So far, most research utilizing the neural network for text summarization has revolved around architecture engineering (Zhou et al., 2018; Chen and Bansal, 2018; Gehrmann et al., 2018). Despite their success, it remains poorly understood why they perform well and what their shortcomings are, which limits our ability to design better architectures. The rapid development of neural architectures calls for a detailed empirical study of analyzing and understanding existing models. In this paper, we primarily focus on extractive summarization since they are computationally efficient, and can generate grammatically and coherent summaries (Nallapati et al., 2017). and seek to ∗ These two authors contributed equally. Corresponding autho"
P19-1100,W04-1013,0,\N,Missing
P19-1100,D18-1179,0,\N,Missing
P19-1231,W03-2201,0,0.0333383,"naries and makes our method generalize well with quite simple dictionaries. Empirical studies on four public NER datasets demonstrate the effectiveness of our proposed method. We have published the source code at https:// github.com/v-mipeng/LexiconNER. 1 Joe Figure 1: Data labeling example for person names using our constructed dictionary. Named Entity Recognition (NER) is concerned with identifying named entities, such as person, location, product and organization names in unstructured text. It is a fundamental component in many natural language processing tasks such as machine translation (Babych and Hartley, 2003), knowledge base construction (Riedel et al., 2013; Shen et al., 2012), automatic question answering (Bordes et al., 2015), search (Zhu et al., 2005), etc. In this field, supervised methods, ranging from the typical graph models (Zhou and Su, 2002; McCallum et al., 2000; McCallum and Li, 2003; Settles, 2004) to current popular neural-networkbased models (Chiu and Nichols, 2016; Lample et al., 2016; Gridach, 2017; Liu et al., 2018; Zhang Equal contribution. Anna Bobick was managed by weight legend Joe Frazier Introduction ∗ David and Yang, 2018), have achieved great success. However, these supe"
P19-1231,Q16-1026,0,0.0814987,"rned with identifying named entities, such as person, location, product and organization names in unstructured text. It is a fundamental component in many natural language processing tasks such as machine translation (Babych and Hartley, 2003), knowledge base construction (Riedel et al., 2013; Shen et al., 2012), automatic question answering (Bordes et al., 2015), search (Zhu et al., 2005), etc. In this field, supervised methods, ranging from the typical graph models (Zhou and Su, 2002; McCallum et al., 2000; McCallum and Li, 2003; Settles, 2004) to current popular neural-networkbased models (Chiu and Nichols, 2016; Lample et al., 2016; Gridach, 2017; Liu et al., 2018; Zhang Equal contribution. Anna Bobick was managed by weight legend Joe Frazier Introduction ∗ David and Yang, 2018), have achieved great success. However, these supervised methods often require large scale fine-grained annotations (label every word of a sentence) to generalize well. This makes it hard to apply them to label-few domains, e.g., bio/medical domains (Del˙eger et al., 2016). In this work, we explore the way to perform NER using only unlabeled data and named entity dictionaries, which are relatively easier to obtain compared wi"
P19-1231,W16-3002,0,0.0272937,"Missing"
P19-1231,P05-1045,0,0.0546107,"Missing"
P19-1231,D14-1181,0,0.00326128,"summarized in Alg. 1. In our experiments, we intuitively set the context size k = 4. 3.4 Build PU Learning Classifier In this work, we use a neural-network-based architecture to implement the classifier f , and this architecture is shared by different entity types. Word Representation. Context-independent word representation consists of three part of features, i.e., the character sequence representation ec (w), the word embedding ew (w), and some human designed features on the word-face eh (w). For the character-level representation ec (w) of w, we use the one-layer convolution network model (Kim, 2014) on its character sequence {c1 , c2 , · · · , cm } ∈ Vc , where Vc is the character vocabulary. Each character c is represented using where Wc denotes a character embedding lookup table. The one-layer convolution network is then applied to {v(c1 ), v(c2 ), · · · , v(cm )} to obtain ec (w). For the word-level representation ew (w) of w, we introduce an unique dense vector for w, which is initialized with Stanford’s GloVe word embeddings1 (Pennington et al., 2014) and finetuned during model training. For the human designed features eh (w) of w, we introduce a set of binary feature indicators. Th"
P19-1231,N16-1030,0,0.0223614,"med entities, such as person, location, product and organization names in unstructured text. It is a fundamental component in many natural language processing tasks such as machine translation (Babych and Hartley, 2003), knowledge base construction (Riedel et al., 2013; Shen et al., 2012), automatic question answering (Bordes et al., 2015), search (Zhu et al., 2005), etc. In this field, supervised methods, ranging from the typical graph models (Zhou and Su, 2002; McCallum et al., 2000; McCallum and Li, 2003; Settles, 2004) to current popular neural-networkbased models (Chiu and Nichols, 2016; Lample et al., 2016; Gridach, 2017; Liu et al., 2018; Zhang Equal contribution. Anna Bobick was managed by weight legend Joe Frazier Introduction ∗ David and Yang, 2018), have achieved great success. However, these supervised methods often require large scale fine-grained annotations (label every word of a sentence) to generalize well. This makes it hard to apply them to label-few domains, e.g., bio/medical domains (Del˙eger et al., 2016). In this work, we explore the way to perform NER using only unlabeled data and named entity dictionaries, which are relatively easier to obtain compared with labeled data. A na"
P19-1231,W02-2024,0,0.17193,"s ) and recall. (Huang et al., 2015) a neural-network-based method as the BiLSTM baseline, but additionally introducing a CRF layer. 4.2 PER Datasets CoNLL (en). CoNLL2003 NER Shared Task Dataset in English (Tjong Kim Sang and De Meulder, 2003) collected from Reuters News. It is annotated by four types: PER, LOC, ORG, and MISC. We used the official split training set for model training, and testb for testing in our experiments, which contains 203K and 46K tokens, respectively. In addition, there are about 456k additional unlabeled tokens. CoNLL (sp). CoNLL2002 Spanish NER Shared Task Dataset (Sang and Erik, 2002) collected from Spanish EFE News Agency. It is also annotated by PER, LOC, ORG, and MISC types. The training and test data sets contain 273k and 53k lines, respectively. MUC. Message Understanding Conference 7 released by Chinchor (1998) for NER. It has about 190K tokens in the training set and 64K tokens in the testing set. For the sake of homogeneity, we perform entity detection on PER, LOC, and ORG in this study. Twitter. Twitter is a dataset collected from Twitter and released by Zhang et al. (2018). It contains 4,000 tweets for training and 3,257 tweets for testing. Every tweet contains b"
P19-1231,W03-0419,0,0.608779,"Missing"
P19-1231,W03-0430,0,0.294989,"erson names using our constructed dictionary. Named Entity Recognition (NER) is concerned with identifying named entities, such as person, location, product and organization names in unstructured text. It is a fundamental component in many natural language processing tasks such as machine translation (Babych and Hartley, 2003), knowledge base construction (Riedel et al., 2013; Shen et al., 2012), automatic question answering (Bordes et al., 2015), search (Zhu et al., 2005), etc. In this field, supervised methods, ranging from the typical graph models (Zhou and Su, 2002; McCallum et al., 2000; McCallum and Li, 2003; Settles, 2004) to current popular neural-networkbased models (Chiu and Nichols, 2016; Lample et al., 2016; Gridach, 2017; Liu et al., 2018; Zhang Equal contribution. Anna Bobick was managed by weight legend Joe Frazier Introduction ∗ David and Yang, 2018), have achieved great success. However, these supervised methods often require large scale fine-grained annotations (label every word of a sentence) to generalize well. This makes it hard to apply them to label-few domains, e.g., bio/medical domains (Del˙eger et al., 2016). In this work, we explore the way to perform NER using only unlabeled"
P19-1231,D14-1162,0,0.0793453,"uman designed features on the word-face eh (w). For the character-level representation ec (w) of w, we use the one-layer convolution network model (Kim, 2014) on its character sequence {c1 , c2 , · · · , cm } ∈ Vc , where Vc is the character vocabulary. Each character c is represented using where Wc denotes a character embedding lookup table. The one-layer convolution network is then applied to {v(c1 ), v(c2 ), · · · , v(cm )} to obtain ec (w). For the word-level representation ew (w) of w, we introduce an unique dense vector for w, which is initialized with Stanford’s GloVe word embeddings1 (Pennington et al., 2014) and finetuned during model training. For the human designed features eh (w) of w, we introduce a set of binary feature indicators. These indicators are designed on options proposed by Collobert et al. (2011): allCaps, upperInitial, lowercase, mixedCaps, noinfo. If any feature is activated, its corresponding indicator is set to 1, otherwise 0. This way, it can keep the capitalization information erased during lookup of the word embedding. The final word presentation independent to its context e(w) ∈ Rkw of w, is obtained by concatenating these three part of features: e(w) = [ec (w) ⊕ ew (w) ⊕"
P19-1231,N13-1008,0,0.0463058,"imple dictionaries. Empirical studies on four public NER datasets demonstrate the effectiveness of our proposed method. We have published the source code at https:// github.com/v-mipeng/LexiconNER. 1 Joe Figure 1: Data labeling example for person names using our constructed dictionary. Named Entity Recognition (NER) is concerned with identifying named entities, such as person, location, product and organization names in unstructured text. It is a fundamental component in many natural language processing tasks such as machine translation (Babych and Hartley, 2003), knowledge base construction (Riedel et al., 2013; Shen et al., 2012), automatic question answering (Bordes et al., 2015), search (Zhu et al., 2005), etc. In this field, supervised methods, ranging from the typical graph models (Zhou and Su, 2002; McCallum et al., 2000; McCallum and Li, 2003; Settles, 2004) to current popular neural-networkbased models (Chiu and Nichols, 2016; Lample et al., 2016; Gridach, 2017; Liu et al., 2018; Zhang Equal contribution. Anna Bobick was managed by weight legend Joe Frazier Introduction ∗ David and Yang, 2018), have achieved great success. However, these supervised methods often require large scale fine-grai"
P19-1231,C18-1183,0,0.108261,"ge scale fine-grained annotations (label every word of a sentence) to generalize well. This makes it hard to apply them to label-few domains, e.g., bio/medical domains (Del˙eger et al., 2016). In this work, we explore the way to perform NER using only unlabeled data and named entity dictionaries, which are relatively easier to obtain compared with labeled data. A natural practice to perform the task is to scan through the query text using the dictionary and treat terms matched with a list of entries of the dictionary as the entities (Nadeau et al., 2006; Gerner et al., 2010; Liu et al., 2015; Yang et al., 2018). However, this practice requires very high quality named entity dictionaries that cover most of entities, otherwise it will fail with poor performance. As shown in Figure 1, the constructed dictionary of person names only labels one entity within the query text, which contains two entities “Bobick” and “Joe Frazier”, and it only labels one word “Joe” out of the two-word entity “Joe Frazier”. To address this problem, an intuitive solution is to further perform supervised or semi-supervised learning using the dictionary labeled data. However, since it does not guarantee that the dictionary cove"
P19-1231,P18-1144,0,0.0850307,"Missing"
P19-1231,P02-1060,0,0.407134,"Joe Figure 1: Data labeling example for person names using our constructed dictionary. Named Entity Recognition (NER) is concerned with identifying named entities, such as person, location, product and organization names in unstructured text. It is a fundamental component in many natural language processing tasks such as machine translation (Babych and Hartley, 2003), knowledge base construction (Riedel et al., 2013; Shen et al., 2012), automatic question answering (Bordes et al., 2015), search (Zhu et al., 2005), etc. In this field, supervised methods, ranging from the typical graph models (Zhou and Su, 2002; McCallum et al., 2000; McCallum and Li, 2003; Settles, 2004) to current popular neural-networkbased models (Chiu and Nichols, 2016; Lample et al., 2016; Gridach, 2017; Liu et al., 2018; Zhang Equal contribution. Anna Bobick was managed by weight legend Joe Frazier Introduction ∗ David and Yang, 2018), have achieved great success. However, these supervised methods often require large scale fine-grained annotations (label every word of a sentence) to generalize well. This makes it hard to apply them to label-few domains, e.g., bio/medical domains (Del˙eger et al., 2016). In this work, we explo"
P19-1231,W04-1221,0,\N,Missing
P19-1231,O03-4002,0,\N,Missing
P19-1359,D14-1179,0,0.00554472,"ating emotional expressions. In early representative work (Polzin and Waibel, 2000; Skowron, 2010), manually prepared rules are used to choose the responses associated with a specific emotion from a conversation corpus. Those rules need to be written by well-trained experts, which makes it hard to extend to deal with complex, nuanced emotions, especially for large corpora. Recurrent neural networks (RNNs) and their applications in the sequence-to-sequence framework have been empirically proven to be quite successful in structured prediction such as machine translation (Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2014), summarization (Rush et al., 2015), or image caption generation (Vinyals et al., 2015). This framework was also applied to build a chatbot, designed to simulate how a human would behave as an interactive agent (Vinyals and Le, 2015). In earlier attempts to develop chatbots by the seq2seq framework, many efforts have been made to avoid generating dull sentences (such as “tell me more”, and “go on”) in their responses. Very recently, a little attention has been given to generate responses with the specific properties like sentiments, tenses, or emotions. Hu et al. (2017)"
P19-1359,P17-1059,0,0.072286,"o applied to build a chatbot, designed to simulate how a human would behave as an interactive agent (Vinyals and Le, 2015). In earlier attempts to develop chatbots by the seq2seq framework, many efforts have been made to avoid generating dull sentences (such as “tell me more”, and “go on”) in their responses. Very recently, a little attention has been given to generate responses with the specific properties like sentiments, tenses, or emotions. Hu et al. (2017) proposed a text generative model based on variational autoencoders (VAEs) to produce sentences presenting a given sentiment or tense. Ghosh et al. (2017) presented a RNN-based language model to generate emotional sentences conditioned on their affect categories. This study focused on the text generations only, but not in the case of conversations. Zhou and Wang (2018) collected a large corpus of Twitter conversations including emojis (ideograms and smileys used in electronic messages) first, and then used emojis to express emotions in the generated texts by trying several variants of conditional VAEs. Zhou et al. (2018) proposed an emotional chat machine (ECM) based on the seq2seq framework, which is more closely related to this study. They di"
P19-1359,D16-1116,0,0.0356795,"Missing"
P19-1359,N16-1014,0,0.643841,"“joy”), where pw denotes the probability being assigned to this category while nw denotes the opposite. Given a sentence s that is a sequence of n words, and the estimated emotion is simply calculated as zˆs = Pn pwprobability i nwi i=1 ( n , n ). If sentence s presents the emotion, it is labeled as a two-dimensional emotion vector z = (1, 0); if not z = (0, 1). Each word is initialized by small random values, and trained by minimizing the cross-entropy error in form of All All Table 3: Statistics of emotion-labeled STC dataset. Method Lexicon-based RNN LSTM Bi-LSTM Diverse Decoding Algorithm Li et al. (2016c) found that most responses in the N -best results produced by the traditional beam search are much similar, and thus we propose a diverse decoding algorithm to foster diversity in the response generation. We force the head words of N -candidates should be different, and then the model continues to generate a response by a greedy decoding strategy after such head words are determined. Finally, we choose the response with the highest emotion score from the best N candidates. The candidates are scored by the emotion classifier trained in advance on a dataset annotated automatically (see Section"
P19-1359,P16-1094,0,0.0630415,"Missing"
P19-1359,D17-1230,0,0.0592005,"deliberately select the desired “emotional” responses from a conversation corpus. Those rules were written by persons with expertise after careful investigation in the corpus, which makes it hard to express complex, various emotions, and difficult to scale well to large datasets. Most recently, a sequence to sequence (seq2seq) learning framework with recurrent neural networks (RNNs) has been successfully used to build conversational agents (also known as chatbots) (Sutskever et al., 2014; Sordoni et al., 2015; Shang et al., 2015; Vinyals and Le, 2015; Serban et al., 2016a,b; Wen et al., 2016; Li et al., 2017; Shen et al., 2018) due to their capability to bridge arbitrary time lags. Such framework was also tried to address the problem of emotional expression in a chatbot, called emotional chat machine (ECM) by Zhou el al (2018). However, the authors reported that ECM tends to express the emotion category (say “joy” or “neutral”) with much more training samples than others, although it is explicitly asked to express another (“anger” for example). It suffers from exploring the overwhelming samples belonging to a certain emotion category. Language plays an important role in emotion because it support"
P19-1359,D16-1230,0,0.19342,"Missing"
P19-1359,D15-1166,0,0.0150308,"2, ..., N and s0 = hM . Emb(yj−1 ) is the word embedding of yj−1 , and [·; ·] denotes an operation that concatenates the feature vectors 3687 separated with semicolons. The emotion vector ej is calculated as a weighted sum of embeddings of words in Vez with the given category z: ej = X ajk · Emb(wkz ) k exp(cjk ) ajk = PTz t=1 exp(cjt ) (3) cjk = Sigmoid(α> hM + β > sj−1 + γ > Emb(wkz )) where wkz denotes the k-th word in Vez , Tz is the number of words for the emotion category z, and α, β and γ are trainable parameters. We compute attention scores using the global attention model proposed by Luong et al. (2015). For each emotional word wkz in Vez , the attention score ajk at the time step j is determined by three parts: the previous hidden state sj−1 of the decoder, the encoded representation hM of the input post, and the embedding Emb(wkz ) of the k-th word in Vez . Therefore, given the partial generated response and the input post, the more relevant an emotional word is, the more influence it will have on the emotion feature vector at the current time step. In this way, such lexicon-based attention gives higher probability to the emotional words that are more relevant to the current context. In or"
P19-1359,P02-1040,0,0.103078,"Missing"
P19-1359,D14-1162,0,0.0819701,"the six emotion labels, and thus we obtained the emotion-labeled conversation dataset. Finally we randomly split the emotionlabeled STC dataset into training/validation/test sets with the ratio of 9:0.5:0.5. The detailed statistics are shown in Table 3. 4.2 Training Details We implemented our EmoDS in Tensorflow5 . Specifically, we used one layer of bidirectional LSTM for encoder and another uni-directional LSTM for decoder, with the size of LSTM hidden state set as 256 in both the encoder and decoder. The dimension of word embedding was set to 100, which was initialized with Glove embedding (Pennington et al., 2014). Many empirical results show that such pre-trained word representations can enhance the supervised models on a variety of NLP tasks (Zheng et al., 2013; Zheng, 2017; Feng and Zheng, 2018). The generic vocab3 Available at http://tcci.ccf.org.cn/conference/2013/ Available at http://tcci.ccf.org.cn/conference/2014/ 5 Available at https://www.tensorflow.org/ ulary was built based on the most frequent 30, 000 words, and the emotion lexicon for each category was constructed by our semi-supervised method with size set to 200. All the remaining words were replaced by a special token <UNK>. Parameters"
P19-1359,D15-1044,0,0.0158443,"(Polzin and Waibel, 2000; Skowron, 2010), manually prepared rules are used to choose the responses associated with a specific emotion from a conversation corpus. Those rules need to be written by well-trained experts, which makes it hard to extend to deal with complex, nuanced emotions, especially for large corpora. Recurrent neural networks (RNNs) and their applications in the sequence-to-sequence framework have been empirically proven to be quite successful in structured prediction such as machine translation (Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2014), summarization (Rush et al., 2015), or image caption generation (Vinyals et al., 2015). This framework was also applied to build a chatbot, designed to simulate how a human would behave as an interactive agent (Vinyals and Le, 2015). In earlier attempts to develop chatbots by the seq2seq framework, many efforts have been made to avoid generating dull sentences (such as “tell me more”, and “go on”) in their responses. Very recently, a little attention has been given to generate responses with the specific properties like sentiments, tenses, or emotions. Hu et al. (2017) proposed a text generative model based on variational auto"
P19-1359,P15-1152,0,0.0703371,"Missing"
P19-1359,D18-1463,0,0.0143865,"ct the desired “emotional” responses from a conversation corpus. Those rules were written by persons with expertise after careful investigation in the corpus, which makes it hard to express complex, various emotions, and difficult to scale well to large datasets. Most recently, a sequence to sequence (seq2seq) learning framework with recurrent neural networks (RNNs) has been successfully used to build conversational agents (also known as chatbots) (Sutskever et al., 2014; Sordoni et al., 2015; Shang et al., 2015; Vinyals and Le, 2015; Serban et al., 2016a,b; Wen et al., 2016; Li et al., 2017; Shen et al., 2018) due to their capability to bridge arbitrary time lags. Such framework was also tried to address the problem of emotional expression in a chatbot, called emotional chat machine (ECM) by Zhou el al (2018). However, the authors reported that ECM tends to express the emotion category (say “joy” or “neutral”) with much more training samples than others, although it is explicitly asked to express another (“anger” for example). It suffers from exploring the overwhelming samples belonging to a certain emotion category. Language plays an important role in emotion because it supports the conceptual kno"
P19-1359,P16-2036,0,0.0194826,"term guides the generation process and guarantees that a specific emotion is appropriately expressed in the generated responses. 3.5 Emotion Lexicon Construction In this section, we describe how to construct the required emotion lexicon in semi-supervised manner from a corpus consisting of the sentences annotated with their emotion categories. The meaning of words is rated on a number of different bipolar adjective scales. For example, scales might range from “strong” to “weak”. We only collect the words rated as “strong” for each emotion category and put into the emotion lexicon. Inspired by Vo and Zhang (2016), each word is represented as w = (pw , nw ) for an emotion category (i.e. “joy”), where pw denotes the probability being assigned to this category while nw denotes the opposite. Given a sentence s that is a sequence of n words, and the estimated emotion is simply calculated as zˆs = Pn pwprobability i nwi i=1 ( n , n ). If sentence s presents the emotion, it is labeled as a two-dimensional emotion vector z = (1, 0); if not z = (0, 1). Each word is initialized by small random values, and trained by minimizing the cross-entropy error in form of All All Table 3: Statistics of emotion-labeled STC"
P19-1359,D17-1173,1,0.706575,"with the ratio of 9:0.5:0.5. The detailed statistics are shown in Table 3. 4.2 Training Details We implemented our EmoDS in Tensorflow5 . Specifically, we used one layer of bidirectional LSTM for encoder and another uni-directional LSTM for decoder, with the size of LSTM hidden state set as 256 in both the encoder and decoder. The dimension of word embedding was set to 100, which was initialized with Glove embedding (Pennington et al., 2014). Many empirical results show that such pre-trained word representations can enhance the supervised models on a variety of NLP tasks (Zheng et al., 2013; Zheng, 2017; Feng and Zheng, 2018). The generic vocab3 Available at http://tcci.ccf.org.cn/conference/2013/ Available at http://tcci.ccf.org.cn/conference/2014/ 5 Available at https://www.tensorflow.org/ ulary was built based on the most frequent 30, 000 words, and the emotion lexicon for each category was constructed by our semi-supervised method with size set to 200. All the remaining words were replaced by a special token <UNK>. Parameters were randomly initialized by the uniform distribution within [−3.0/n, 3.0/n], where n denotes the dimension of parameters. The size of diverse decoding was set to 2"
P19-1359,D13-1061,1,0.745076,"validation/test sets with the ratio of 9:0.5:0.5. The detailed statistics are shown in Table 3. 4.2 Training Details We implemented our EmoDS in Tensorflow5 . Specifically, we used one layer of bidirectional LSTM for encoder and another uni-directional LSTM for decoder, with the size of LSTM hidden state set as 256 in both the encoder and decoder. The dimension of word embedding was set to 100, which was initialized with Glove embedding (Pennington et al., 2014). Many empirical results show that such pre-trained word representations can enhance the supervised models on a variety of NLP tasks (Zheng et al., 2013; Zheng, 2017; Feng and Zheng, 2018). The generic vocab3 Available at http://tcci.ccf.org.cn/conference/2013/ Available at http://tcci.ccf.org.cn/conference/2014/ 5 Available at https://www.tensorflow.org/ ulary was built based on the most frequent 30, 000 words, and the emotion lexicon for each category was constructed by our semi-supervised method with size set to 200. All the remaining words were replaced by a special token <UNK>. Parameters were randomly initialized by the uniform distribution within [−3.0/n, 3.0/n], where n denotes the dimension of parameters. The size of diverse decoding"
P19-1359,P18-1104,0,0.0719895,"e to avoid generating dull sentences (such as “tell me more”, and “go on”) in their responses. Very recently, a little attention has been given to generate responses with the specific properties like sentiments, tenses, or emotions. Hu et al. (2017) proposed a text generative model based on variational autoencoders (VAEs) to produce sentences presenting a given sentiment or tense. Ghosh et al. (2017) presented a RNN-based language model to generate emotional sentences conditioned on their affect categories. This study focused on the text generations only, but not in the case of conversations. Zhou and Wang (2018) collected a large corpus of Twitter conversations including emojis (ideograms and smileys used in electronic messages) first, and then used emojis to express emotions in the generated texts by trying several variants of conditional VAEs. Zhou et al. (2018) proposed an emotional chat machine (ECM) based on the seq2seq framework, which is more closely related to this study. They disentangle the emotion factors from texts in the form of embeddings. When ECM is asked to express a specific emotion in the response, the corresponding emotion embedding is consumed by the machine until each element of"
P19-1601,P19-1285,0,0.0288274,"tence. Besides, without referring the original text, RNN-based decoder is also hard to preserve the content. The generation quality for long text is also uncontrollable. In this paper, we address the above concerns of disentangled models for style transfer. Different from them, we propose Style Transformer, which takes Transformer (Vaswani et al., 2017) as the basic block. Transformer is a fully-connected selfattention neural architecture, which has achieved many exciting results on natural language processing (NLP) tasks, such as machine translation (Vaswani et al., 2017), language modeling (Dai et al., 2019), text classification (Devlin et al., 2018). Different from RNNs, Transformer uses stacked self-attention and point-wise, fully connected layers for both the encoder and decoder. Moreover, Transformer decoder fetches the information from the encoder part via attention mechanism, compared to a fixed size vector used by RNNs. With the strong ability of Transformer, our model can transfer the style of a sentence while better preserving its meaning. The difference between our model and the previous model is shown in Figure 1. Our contributions are summarized as follows: • We introduce a novel trai"
P19-1601,D18-1002,0,0.0215771,". These methods focus on how to disentangle the content and style in the latent space. The latent representation needs better preserve the meaning of the text while reducing its stylistic properties. Due to lacking paired sentence, an adversarial loss (Goodfellow et al., 2014) is used in the latent space to discourage encoding style information in the latent representation. Although the disentangled latent representation brings better interpretability, in this paper, we address the following concerns for these models. 1) It is difficult to judge the quality of disentanglement. As reported in (Elazar and Goldberg, 2018; Lample et al., 2019), the style information can be still recovered from the latent representation even the model has trained adversarially. Therefore, it is not easy to disentangle the stylistic property from the semantics of a sentence. 2) Disentanglement is also unnecessary. Lample et al. (2019) reported that a good decoder can generate the text with the desired style from an entangled latent representation by “overwriting” the original style. 3) Due to the limited capacity of vector representation, the latent representation is hard to capture the rich semantic information, especially for"
P19-1601,W11-2123,0,0.036442,"4 N/A N/A N/A 18 ControlledGen (Hu et al., 2017) CrossAlignment (Shen et al., 2017) MultiDecoder (Fu et al., 2018) CycleRL(Xu et al., 2018) 88.9 76.3 49.9 88.0 14.3 4.3 9.2 2.8 45.7 13.2 37.9 7.2 201 90 127 204 93.9 N/A N/A 97.6 62.1 N/A N/A 4.9 58 N/A N/A 246 Ours (Conditional) Ours (Multi-Class) 93.6 87.6 17.1 20.3 45.3 54.9 78 50 86.8 79.7 66.2 70.5 38 29 Table 2: Automatic evaluation results on Yelp and IMDb datset respectively. Fluency Fluency is measured by the perplexity of the transferred sentence, and we trained a 5-gram language model on the training set of two datasets using KenLM (Heafield, 2011). 4.2.2 Human Evaluation Due to the lack of parallel data in style transfer area, automatic metrics are insufficient to evaluate the quality of the transferred sentence. Therefore we also conduct human evaluation experiments on two datasets. We randomly select 100 source sentences (50 for each sentiment) from each test set for human evaluation. For each review, one source input and three anonymous transferred samples are shown to a reviewer. And the reviewer is asked to choose the best sentence for style control, content preservation, and fluency respectively. • Which sentence has the most opp"
P19-1601,D19-1306,0,0.169694,"Missing"
P19-1601,E17-2068,0,0.0350339,"respectively. 4.2 Evaluation A goal transferred sentence should be a fluent, content-complete one with target style. To evaluate the performance of the different model, following previous works, we compared three different dimensions of generated samples: 1) Style control, 2) Content preservation and 3) Fluency. 4.2.1 Automatic Evaluation Style Control We measure style control automatically by evaluating the target sentiment accuracy of transferred sentences. For an accurate evaluation of style control, we trained two sentiment classifiers on the training set of Yelp and IMDb using fastText (Joulin et al., 2017). Content Preservation To measure content preservation, we calculate the BLEU score (Papineni et al., 2002) between the transferred sentence and its source input using NLTK. A higher BLEU score indicates the transferred sentence can achieve better content preservation by retaining more words from the source sentence. If a human reference is available, we will calculate the BLEU score between the transferred sentence and corresponding reference as well. Two BLEU score metrics are referred to as self -BLEU and ref -BLEU 6002 Yelp Model ACC IMDb ref -BLEU self -BLEU PPL ACC self -BLEU PPL Input C"
P19-1601,D18-1549,0,0.059816,"Missing"
P19-1601,N18-1169,0,0.257719,"ethods (Fu et al., 2018; John et al., 2018; Zhang et al., 2018a,b) has been proposed based on standard encoder-decoder architecture. Although, learning a latent representation will make the model more interpretable and easy to manipulate, the model which is assumed a fixed size latent representation cannot utilize the information from the source sentence anymore. On the other hand, there are also some approaches without manipulating latent representation are proposed recently. Xu et al. (2018) propose a cycled reinforcement learning method for unpaired sentiment-to-sentiment translation task. Li et al. (2018) propose a three-stage method. Their model first extracts content words by deleting phrases a strong attribute value, then retrieves new phrases associated with the target attribute, and finally uses a neural model to combine these into a final output. Lample et al. (2019) reduce text style transfer to unsupervised machine translation problem (Lample et al., 2018). They employ Denoising Auto-encoders (Vincent et al., 2008) and back-translation (Sennrich et al., 2016) to build a translation style between different styles. However, both lines of the previous models make few attempts to utilize t"
P19-1601,D15-1166,0,0.0606196,"o unsupervised machine translation problem (Lample et al., 2018). They employ Denoising Auto-encoders (Vincent et al., 2008) and back-translation (Sennrich et al., 2016) to build a translation style between different styles. However, both lines of the previous models make few attempts to utilize the attention mechanism to refer the long-term history or the source sentence, except Lample et al. (2019). In many NLP tasks, especially for text generation, attention mechanism has been proved to be an essential technique to enable the model to capture the longterm dependency (Bahdanau et al., 2014; Luong et al., 2015; Vaswani et al., 2017). In this paper, we follow the second line of work and propose a novel method which makes no assumption about the latent representation of source sentence and takes the proven self-attention network, Transformer, as a basic module to train a style transfer system. 5998 x z Encoder Decoder Finally, we will combine the Style Transformer network and discriminator network via an overall learning algorithm in section 3.5 to train our style transfer system. y s (a) Disentangled Style Transfer x Transformer y 3.3 s (b) Style Transformer Figure 1: General illustration of previou"
P19-1601,P11-1015,0,0.175537,"Missing"
P19-1601,P02-1040,0,0.10373,"arget style. To evaluate the performance of the different model, following previous works, we compared three different dimensions of generated samples: 1) Style control, 2) Content preservation and 3) Fluency. 4.2.1 Automatic Evaluation Style Control We measure style control automatically by evaluating the target sentiment accuracy of transferred sentences. For an accurate evaluation of style control, we trained two sentiment classifiers on the training set of Yelp and IMDb using fastText (Joulin et al., 2017). Content Preservation To measure content preservation, we calculate the BLEU score (Papineni et al., 2002) between the transferred sentence and its source input using NLTK. A higher BLEU score indicates the transferred sentence can achieve better content preservation by retaining more words from the source sentence. If a human reference is available, we will calculate the BLEU score between the transferred sentence and corresponding reference as well. Two BLEU score metrics are referred to as self -BLEU and ref -BLEU 6002 Yelp Model ACC IMDb ref -BLEU self -BLEU PPL ACC self -BLEU PPL Input Copy 3.3 23 100 11 5.2 100 5 RetrieveOnly (Li et al., 2018) TemplateBased (Li et al., 2018) DeleteOnly (Li e"
P19-1601,P18-1080,0,0.117337,"uction Text style transfer is the task of changing the stylistic properties (e.g., sentiment) of the text while retaining the style-independent content within the context. Since the definition of the text style is vague, it is difficult to construct paired sentences with the same content and differing styles. Therefore, the studies of text style transfer focus on the unpaired transfer. Recently, neural networks have become the dominant methods in text style transfer. Most of the previous methods (Hu et al., 2017; Shen et al., 2017; Fu et al., 2018; Carlson et al., 2017; Zhang et al., 2018b,a; Prabhumoye et al., 2018; Jin et al., 2019; Melnyk et al., 2017; dos Santos et al., 2018) formulate the style transfer problem into the “encoder-decoder” framework. The encoder maps the text into a style-independent latent ∗ 1 Corresponding author https://github.com/fastnlp/fastNLP representation (vector representation), and the decoder generates a new text with the same content but a different style from the disentangled latent representation plus a style variable. These methods focus on how to disentangle the content and style in the latent space. The latent representation needs better preserve the meaning of the t"
P19-1601,P18-2031,0,0.121373,"perties (e.g., sentiment) of the text while retaining the style-independent content within the context. Since the definition of the text style is vague, it is difficult to construct paired sentences with the same content and differing styles. Therefore, the studies of text style transfer focus on the unpaired transfer. Recently, neural networks have become the dominant methods in text style transfer. Most of the previous methods (Hu et al., 2017; Shen et al., 2017; Fu et al., 2018; Carlson et al., 2017; Zhang et al., 2018b,a; Prabhumoye et al., 2018; Jin et al., 2019; Melnyk et al., 2017; dos Santos et al., 2018) formulate the style transfer problem into the “encoder-decoder” framework. The encoder maps the text into a style-independent latent ∗ 1 Corresponding author https://github.com/fastnlp/fastNLP representation (vector representation), and the decoder generates a new text with the same content but a different style from the disentangled latent representation plus a style variable. These methods focus on how to disentangle the content and style in the latent space. The latent representation needs better preserve the meaning of the text while reducing its stylistic properties. Due to lacking paire"
P19-1601,P16-1009,0,0.0406867,"oposed recently. Xu et al. (2018) propose a cycled reinforcement learning method for unpaired sentiment-to-sentiment translation task. Li et al. (2018) propose a three-stage method. Their model first extracts content words by deleting phrases a strong attribute value, then retrieves new phrases associated with the target attribute, and finally uses a neural model to combine these into a final output. Lample et al. (2019) reduce text style transfer to unsupervised machine translation problem (Lample et al., 2018). They employ Denoising Auto-encoders (Vincent et al., 2008) and back-translation (Sennrich et al., 2016) to build a translation style between different styles. However, both lines of the previous models make few attempts to utilize the attention mechanism to refer the long-term history or the source sentence, except Lample et al. (2019). In many NLP tasks, especially for text generation, attention mechanism has been proved to be an essential technique to enable the model to capture the longterm dependency (Bahdanau et al., 2014; Luong et al., 2015; Vaswani et al., 2017). In this paper, we follow the second line of work and propose a novel method which makes no assumption about the latent represe"
P19-1601,P18-1090,0,0.177901,"nd holistic attribute discriminators for the effective imposition of semantic structures. Following their work, many methods (Fu et al., 2018; John et al., 2018; Zhang et al., 2018a,b) has been proposed based on standard encoder-decoder architecture. Although, learning a latent representation will make the model more interpretable and easy to manipulate, the model which is assumed a fixed size latent representation cannot utilize the information from the source sentence anymore. On the other hand, there are also some approaches without manipulating latent representation are proposed recently. Xu et al. (2018) propose a cycled reinforcement learning method for unpaired sentiment-to-sentiment translation task. Li et al. (2018) propose a three-stage method. Their model first extracts content words by deleting phrases a strong attribute value, then retrieves new phrases associated with the target attribute, and finally uses a neural model to combine these into a final output. Lample et al. (2019) reduce text style transfer to unsupervised machine translation problem (Lample et al., 2018). They employ Denoising Auto-encoders (Vincent et al., 2008) and back-translation (Sennrich et al., 2016) to build a"
P19-1601,N18-1138,0,0.186298,"c attribute discriminators for the effective imposition of semantic structures. Following their work, many methods (Fu et al., 2018; John et al., 2018; Zhang et al., 2018a,b) has been proposed based on standard encoder-decoder architecture. Although, learning a latent representation will make the model more interpretable and easy to manipulate, the model which is assumed a fixed size latent representation cannot utilize the information from the source sentence anymore. On the other hand, there are also some approaches without manipulating latent representation are proposed recently. Xu et al. (2018) propose a cycled reinforcement learning method for unpaired sentiment-to-sentiment translation task. Li et al. (2018) propose a three-stage method. Their model first extracts content words by deleting phrases a strong attribute value, then retrieves new phrases associated with the target attribute, and finally uses a neural model to combine these into a final output. Lample et al. (2019) reduce text style transfer to unsupervised machine translation problem (Lample et al., 2018). They employ Denoising Auto-encoders (Vincent et al., 2008) and back-translation (Sennrich et al., 2016) to build a"
P19-1601,N19-1423,0,\N,Missing
P19-1652,W05-0909,0,0.10726,"NIC+WC uses the hard word constraints (WC). NIC+WC+RL is trained with reinforcement learning using the image-grounded vocabulary as the action space. - NIC+WC+WA employs the soft word-aware (WA) mechanism on top of NIC+WC. NIC+WC+WA+RL is trained with reinforcement learning using the image-grounded vocabulary as the action space. - NIC+WC(GT) utilizes the ground-truth vocabufi as the word constraints instead of Wi . lary W This is an oracle. 3.4 Overall Performance We report scores of several widely used metrics for image captioning evaluation, including BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), ROUGE (Lin and Hovy, 2003) and CIDEr-D (Vedantam et al., 2015). The overall performance is shown in Table 1. Several findings stand out: - Both NIC+WC and NIC+WC+RL perform better than their counter-part models NIC and NIC+RL across all metrics. This shows the effectiveness of using the word constraint mechanism for reducing irrelevant words for a given image. - Both NIC+WC+WA and NIC+WC+WA+RL outperform NIC+WC and NIC+WC+RL respectively. This shows that the word-aware mechanism effectively guides the generator to better capturing the semantics of a given image. - Compared to NIC+WC and NIC+"
P19-1652,P15-2017,0,0.022324,"baseline model that takes visual features as input and employs a sinEffectiveness of generating novel captions Novel caption generation is crucial for automatic image captioning because retrieval-based models that simply retrieve existing captions from the training set often produce less human results though they can achieve high scores in terms of auFigure 5: Mean CIDEr-D scores with standard derivation of |Wi |= 64 versus |Wi |= |V |, for NIC+WC+WA+RL (3 seeds) on the validation set. Xaxis: the number of training iterations (2×104 ), Y-axis: CIDEr-D scores. 6520 tomatic evaluation metrics (Devlin et al., 2015b). The worst case of N-gram problem is that the model directly generated the same frequent captions in the training set (Devlin et al., 2015a). Thus the capability of generating novel captions for an image that is not seen in the training set indicates that the generator is able to understand a given image better instead of simply generating frequent Ngrams found in the training set. In this experiment, we consider captions generated by models that are not seen in the training set as novel captions. We show the ratio of novel captions generated by different models in Figure 6. Our proposed mo"
P19-1652,C18-1150,1,0.691852,"ord (2015). Visual question answering (Antol et al., 2015; Goyal et al., 2017) aims to provide an answer to a question related to a given image. Existing architectures designed for VQA (Malinowski et al., 2015) utilize an RNN to encode the question, and a CNN to encode the image. Most efforts are made to align the visual and text information for generating the answer. Visual question generation is a relatively new task that generates natural questions about an image (Mostafazadeh et al., 2016). Approaches have been explored to generate diverse questions (Tang et al., 2017; Zhang et al., 2017; Fan et al., 2018a) and questions with a specific property (Fan et al., 2018b). Instead of using high-level visual features extracted from the image for text generation, some researchers explore identifying fine-grained information from the image, i.e. objects and attributes, to guide the process of text generation. Traditionally, template-based approaches are used to compose the caption (Farhadi et al., 2010; Kulkarni et al., 2013; Lin et al., 2015). After that, different attention mechanisms are proposed to align visual information and text for better generation (You et al., 2016; Lu et al., 2017; Anderson e"
P19-1652,N03-1020,0,0.0542235,"Missing"
P19-1652,P16-1170,0,0.0191424,"al., 2016; Lu et al., 2017; Anderson et al., 2018) employ CNN to extract visual features and RNN to generate captions word by word (2015). Visual question answering (Antol et al., 2015; Goyal et al., 2017) aims to provide an answer to a question related to a given image. Existing architectures designed for VQA (Malinowski et al., 2015) utilize an RNN to encode the question, and a CNN to encode the image. Most efforts are made to align the visual and text information for generating the answer. Visual question generation is a relatively new task that generates natural questions about an image (Mostafazadeh et al., 2016). Approaches have been explored to generate diverse questions (Tang et al., 2017; Zhang et al., 2017; Fan et al., 2018a) and questions with a specific property (Fan et al., 2018b). Instead of using high-level visual features extracted from the image for text generation, some researchers explore identifying fine-grained information from the image, i.e. objects and attributes, to guide the process of text generation. Traditionally, template-based approaches are used to compose the caption (Farhadi et al., 2010; Kulkarni et al., 2013; Lin et al., 2015). After that, different attention mechanisms"
P19-1652,P02-1040,0,0.110523,"with the two-layer LSTM. 6518 - NIC+WC uses the hard word constraints (WC). NIC+WC+RL is trained with reinforcement learning using the image-grounded vocabulary as the action space. - NIC+WC+WA employs the soft word-aware (WA) mechanism on top of NIC+WC. NIC+WC+WA+RL is trained with reinforcement learning using the image-grounded vocabulary as the action space. - NIC+WC(GT) utilizes the ground-truth vocabufi as the word constraints instead of Wi . lary W This is an oracle. 3.4 Overall Performance We report scores of several widely used metrics for image captioning evaluation, including BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), ROUGE (Lin and Hovy, 2003) and CIDEr-D (Vedantam et al., 2015). The overall performance is shown in Table 1. Several findings stand out: - Both NIC+WC and NIC+WC+RL perform better than their counter-part models NIC and NIC+RL across all metrics. This shows the effectiveness of using the word constraint mechanism for reducing irrelevant words for a given image. - Both NIC+WC+WA and NIC+WC+WA+RL outperform NIC+WC and NIC+WC+RL respectively. This shows that the word-aware mechanism effectively guides the generator to better capturing the semantics of a given i"
P19-1652,D14-1162,0,0.0831222,"for the training, validation and test sets is 29,000, 1,000 and 1,000, respectively. Each image contains 5 human annotated captions. We split the dataset following the process described in (Karpathy and Fei-Fei, 2015). 3.2 Implementation Details For image representation, we rescale the image to 224 × 224 and use ResNet-152 (He et al., 2016) pre-trained on ImageNet (Russakovsky et al., 2015) to extract features of dimension 2,048. The mini-batch size is 64. The dimensions of LSTM hidden unit and the word embedding are 512 and 300, respectively, and the word embedding is initialized with GloVe (Pennington et al., 2014)2 which is pretrained on Wikipedia 2014 and Gigaword 5. We prune the vocabulary by dropping words appear less than five times. For the generator, We train the model with cross-entropy using Adam (Kingma and Ba, 2014) with an initial learning rate 1 × 10−3 which decreases by a factor of 0.8 every 2 × 104 iterations. Then we train the generator with reinforcement learning but without 2 http://nlp.stanford.edu/data/glove. 6B.zip hard constraints using Adam with an initial learning rate 5 × 10−5 which decreases by a factor of 0.8 every 3 × 104 iterations. Finally, we train the generator with reinf"
P19-1652,D17-1090,0,0.0241213,"and RNN to generate captions word by word (2015). Visual question answering (Antol et al., 2015; Goyal et al., 2017) aims to provide an answer to a question related to a given image. Existing architectures designed for VQA (Malinowski et al., 2015) utilize an RNN to encode the question, and a CNN to encode the image. Most efforts are made to align the visual and text information for generating the answer. Visual question generation is a relatively new task that generates natural questions about an image (Mostafazadeh et al., 2016). Approaches have been explored to generate diverse questions (Tang et al., 2017; Zhang et al., 2017; Fan et al., 2018a) and questions with a specific property (Fan et al., 2018b). Instead of using high-level visual features extracted from the image for text generation, some researchers explore identifying fine-grained information from the image, i.e. objects and attributes, to guide the process of text generation. Traditionally, template-based approaches are used to compose the caption (Farhadi et al., 2010; Kulkarni et al., 2013; Lin et al., 2015). After that, different attention mechanisms are proposed to align visual information and text for better generation (You et"
W10-3005,W09-1304,0,0.0613034,"osterior probability. They only employ bag-of-words features. On the public biomedical dataset1 , their experiments achieve the performance of 0.76 in BEP (break even point). Although they also introduced more linguistic features, such as part-of-speech (POS), lemma and bigram (Medlock, 2008), there are no significant improvements. In Ganter and Strube (2009), the same task on Wikipedia is presented. In their system, score of a sentence is defined as a normalized tangent value of the sum of scores over all words in the sentence. Shallow linguistic features are introduced in their experiments. Morante and Daelemans (2009) present their research on identifying hedge cues and their scopes. Their system consists of several classifiers and works in two phases, first identifying the hedge cues in a sentence and secondly finding the full scope for each hedge cue. In the first phase, they use IGTREE algorithm to train a classifier with 3 categories. In the second phase, three different classifiers are trained to find the first token and last token of in-sentence scope and finally combined into a meta classifier. The experiments shown that their system achieves an F1 of nearly 0.85 of identifying hedge cues in the abs"
W10-3005,W08-0606,0,0.0614431,"and used average perceptron as the training algorithm. The rest of the paper is organized as follows. In Section 2, a brief review of related works is presented. Then, we describe our method in Section 3. Experiments and results are presented in the section 4. Finally, the conclusion will be presented in Section 5. Introduction Detecting hedged information in biomedical literatures has received considerable interest in the biomedical natural language processing (NLP) community recently. Hedge information indicates that authors do not or cannot back up their opinions or statements with facts (Szarvas et al., 2008), which exists in many natural language texts, such as webpages or blogs, as well as biomedical literatures. For many NLP applications, such as question answering and information extraction, the information extracted from hedge sentences would be harmful to their final performances. Therefore, the hedge or speculative information should be detected in advance, and dealt with different approaches or discarded directly. In CoNLL-2010 Shared Task (Farkas et al., 2010), there are two different level subtasks: detecting sentences containing uncertainty and identifying the in-sentence scopes of hedg"
W10-3005,W02-1001,0,0.0267214,"ptron (Duda et al., 2001). It adjusts parameters w when a misclassification occurs. Although this framework is very simple, it has been shown that the algorithm converges in a finite number of iterations if the data is linearly separable. Moreover, much less training time is required in practice than the batch learning methods, such as support vector machine (SVM) or conditional maximum entropy (CME). Here we employ a variant perceptron algorithm to train the model, which is commonly named average perceptron since it averages parameters w across iterations. This algorithm is first proposed in Collins (2002). Many experiments of Hedge detection with average perceptron Detecting uncertain sentences The first subtask is to identify sentences containing uncertainty information. In particular, 1 http://www.benmedlock.co.uk/ hedgeclassif.html 2 http://www.inf.u-szeged.hu/rgai/ bioscope 33 NLP problems demonstrate better generalization performance than non averaged parameters. More theoretical proofs can be found in Collins (2002). Different from the standard average perceptron algorithm, we slightly modify the average strategy. The reason to this modification is that the original algorithm is slow sin"
W10-3005,N03-1033,0,0.00986148,"ts and our experimental results when developing the system. Our system architecture is shown in Figure 4, which consists of the following modules. 1. corpus preprocess module, which employs a tokenizer to normalize the corpus; 2. sentence detection module, which uses a binary sentence-level classifier to determine whether a sentence contains uncertainty information; 4.2 3. hedge cues detection module, which identifies which words in a sentence are the hedge cues, we train a binary word-level classifier; Corpus preprocess The sentence are processed with a maximumentropy part-of-speech tagger4 (Toutanova et al., 2003), in which a rule-based tokenzier is used to separate punctuations or other symbols from regular words. Moreover, we train a first-order projective dependency parser with MSTParser5 (McDonald et al., 2005) on the standard WSJ training corpus, which is converted from constituent trees to dependency trees by several heuristic rules6 . 4. cue scope recognition module, which recognizes the corresponding scope for each hedge cue by another word-level classifier. Our experimental results are obtained on the training datasets by 10-fold cross validation. The maximum iteration number for training the"
W10-3005,W10-3001,0,0.036671,"Missing"
W10-3005,P09-2044,0,0.114565,"ock and Briscoe (2007) report their approach based on weakly supervised learning. In their method, a statistical model is initially derived from a seed corpus, and then iteratively modified by augmenting the training dataset with unlabeled samples according the posterior probability. They only employ bag-of-words features. On the public biomedical dataset1 , their experiments achieve the performance of 0.76 in BEP (break even point). Although they also introduced more linguistic features, such as part-of-speech (POS), lemma and bigram (Medlock, 2008), there are no significant improvements. In Ganter and Strube (2009), the same task on Wikipedia is presented. In their system, score of a sentence is defined as a normalized tangent value of the sum of scores over all words in the sentence. Shallow linguistic features are introduced in their experiments. Morante and Daelemans (2009) present their research on identifying hedge cues and their scopes. Their system consists of several classifiers and works in two phases, first identifying the hedge cues in a sentence and secondly finding the full scope for each hedge cue. In the first phase, they use IGTREE algorithm to train a classifier with 3 categories. In th"
W10-3005,P05-1012,0,0.224884,"trained to predict whether each Identifying hedge cues and their scopes Our approach for the second subtask consists of two phases: (1) identifying hedge cues in a sentence, then (2) recognizing their corresponding scopes. 3.2.1 Identifying hedge cues Hedge cues are the most important clues for determining whether a sentence contains uncertain 34 word-cue pair in a sentence is in the scope of the hedge cue. Besides base context features used in the previous phase, we introduce additional syntactic dependency features. These features are generated by a first-order projective dependency parser (McDonald et al., 2005), and listed in Figure 3. The scopes of hedge cues are always covering a consecutive block of words including the hedge cue itself. The ideal method should recognize only one consecutive block for each hedge cue. However, our classifier cannot work so well. Therefore, we apply a simple strategy to process the output of the classifier. The simple strategy is to find a maximum consecutive sequence which covers the hedge cue. If a sentence is considered to contain several hedge cues, we simply combine the consecutive sequences, which have at least one common word, to a large block and assign it t"
W10-3005,P07-1125,0,0.101397,"ference on Computational Natural Language Learning: Shared Task, pages 32–39, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics this subtask is a binary classification problem at sentence-level. We define the score of sentence as the confidence that the sentence contains uncertainty information. The score can be decomposed as the sum of the scores of all words in the sentence, X X wT φ(xi , y) s(xi , y) = S(x, y) = tational linguistic perspective in recent years. In this section, we give a brief review on the related works. For speculative sentences detection, Medlock and Briscoe (2007) report their approach based on weakly supervised learning. In their method, a statistical model is initially derived from a seed corpus, and then iteratively modified by augmenting the training dataset with unlabeled samples according the posterior probability. They only employ bag-of-words features. On the public biomedical dataset1 , their experiments achieve the performance of 0.76 in BEP (break even point). Although they also introduced more linguistic features, such as part-of-speech (POS), lemma and bigram (Medlock, 2008), there are no significant improvements. In Ganter and Strube (200"
W10-4132,W06-1615,0,0.132654,"Missing"
W10-4132,C04-1081,0,0.0309809,"Science Fudan University Fudan University Fudan University Shanghai, China Shanghai, China Shanghai, China wjgao616@gmail.com xpqiu@fudan.edu.cn xjhuang@fudan.edu.cn Abstract In this paper, we describe our system1 for CIPS-SIGHAN-2010 bake-off task of Chinese word segmentation, which focused on the cross-domain performance of Chinese word segmentation algorithms. We use the online passive-aggressive algorithm with domain invariant information for cross-domain Chinese word segmentation. 1 Introduction In recent years, Chinese word segmentation (CWS) has undergone great development (Xue, 2003; Peng et al., 2004). The popular method is to regard word segmentation as a sequence labeling problems. The goal of sequence labeling is to assign labels to all elements of a sequence. Due to the exponential size of the output space, sequence labeling problems tend to be more challenging than the conventional classification problems. Many algorithms have been proposed and the progress has been encouraging, such as SVMstruct (Tsochantaridis et al., 2004), conditional random fields (CRF) (Lafferty et al., 2001), maximum margin Markov networks (M3N) (Taskar et al., 2003) and so on. After years of intensive research"
W10-4132,O03-4002,0,0.113539,"of Computer Science Fudan University Fudan University Fudan University Shanghai, China Shanghai, China Shanghai, China wjgao616@gmail.com xpqiu@fudan.edu.cn xjhuang@fudan.edu.cn Abstract In this paper, we describe our system1 for CIPS-SIGHAN-2010 bake-off task of Chinese word segmentation, which focused on the cross-domain performance of Chinese word segmentation algorithms. We use the online passive-aggressive algorithm with domain invariant information for cross-domain Chinese word segmentation. 1 Introduction In recent years, Chinese word segmentation (CWS) has undergone great development (Xue, 2003; Peng et al., 2004). The popular method is to regard word segmentation as a sequence labeling problems. The goal of sequence labeling is to assign labels to all elements of a sequence. Due to the exponential size of the output space, sequence labeling problems tend to be more challenging than the conventional classification problems. Many algorithms have been proposed and the progress has been encouraging, such as SVMstruct (Tsochantaridis et al., 2004), conditional random fields (CRF) (Lafferty et al., 2001), maximum margin Markov networks (M3N) (Taskar et al., 2003) and so on. After years o"
W10-4132,P95-1026,0,0.233427,"he intrinsic geometry of inputs revealed by unlabeled data points and we derive a maximum-margin formulation of semi-supervised learning for structured variables. Self-training (Zhu, 2005) is also a popular technology. In self-training a classifier is first trained with the small amount of labeled data. The classifier is then used to classify the unlabeled data. Typically the most confident unlabeled points, together with their predicted labels, are added to the training set. The classifier is re-trained and the procedure repeated. Note the classifier uses its own predictions to teach itself. Yarowsky (1995) uses self-training for word sense disambiguation, e.g. deciding whether the word plant means a living organism or a factory in a given context. Zhao and Kit (2008) integrated unsupervised segmentation and CRF learning for Chinese word segmentation and named entity recognition. They found word accessory variance (Feng et al., 2004) is useful to CWS. 3 Online Passive-Aggressive Algorithm Sequence labeling, the task of assigning labels y = y1 , . . . , yL to an input sequence x = x1 , . . . , xL . Give a sample (x, y), we define the feature is Φ(x, y). Thus, we can label x with a score function,"
W10-4132,I08-4017,0,0.295213,"s. Self-training (Zhu, 2005) is also a popular technology. In self-training a classifier is first trained with the small amount of labeled data. The classifier is then used to classify the unlabeled data. Typically the most confident unlabeled points, together with their predicted labels, are added to the training set. The classifier is re-trained and the procedure repeated. Note the classifier uses its own predictions to teach itself. Yarowsky (1995) uses self-training for word sense disambiguation, e.g. deciding whether the word plant means a living organism or a factory in a given context. Zhao and Kit (2008) integrated unsupervised segmentation and CRF learning for Chinese word segmentation and named entity recognition. They found word accessory variance (Feng et al., 2004) is useful to CWS. 3 Online Passive-Aggressive Algorithm Sequence labeling, the task of assigning labels y = y1 , . . . , yL to an input sequence x = x1 , . . . , xL . Give a sample (x, y), we define the feature is Φ(x, y). Thus, we can label x with a score function, ˆ = arg max F (w, Φ(x, z)), y (1) z where w is the parameter of function F (·). The score function of our algorithm is linear function. ˆ is denoted as the Given a"
W10-4132,J04-1004,0,\N,Missing
W10-4165,E06-1018,0,0.0184146,"e clustering methods. Many clustering methods have been used in the task of word sense induction. For example, kmeans and agglomerative clustering (Schütze, 1998). sIB (Sequential Information Bottleneck) a variation of Information Bottleneck is applied in (Niu et al., 2007). In (Dorow and Widdows, 2003) Graph-based clustering algorithm is employed that in a graph a node represents a noun and two nodes have an edge between them if they co-occur in list more than a given number of times. A generative model based on LDA is proposed in (Brody and Lapata, 2009). In our method, we use the triplets (Bordag, 2006) and their intersections from the Internet to construct the feature vectors then sIB is used as the clustering method. 3 Feature Selection Our method select the features of the words similar to (Bordag, 2006) is also using the triplets. In Chinese there are no natural separators between the words as English, so the ﬁrst step in Chinese language processing is often the Chinese word segmentation. In our system we use the FudanNLP toolkit1 to split the words. At the ﬁrst stage, we split the instance of the target word and ﬁlter out the numbers, English words and stop words from it. So we get a se"
W10-4165,E03-1020,0,0.0622769,"s, it can be as small as only two words before and after the target words. It may be the sentence where the target word is in. Or it will be 20 surrounding words on either side of the target words and even more words. After every instance of the target word is represented as a feature vector, it will be the input of the clustering methods. Many clustering methods have been used in the task of word sense induction. For example, kmeans and agglomerative clustering (Schütze, 1998). sIB (Sequential Information Bottleneck) a variation of Information Bottleneck is applied in (Niu et al., 2007). In (Dorow and Widdows, 2003) Graph-based clustering algorithm is employed that in a graph a node represents a noun and two nodes have an edge between them if they co-occur in list more than a given number of times. A generative model based on LDA is proposed in (Brody and Lapata, 2009). In our method, we use the triplets (Bordag, 2006) and their intersections from the Internet to construct the feature vectors then sIB is used as the clustering method. 3 Feature Selection Our method select the features of the words similar to (Bordag, 2006) is also using the triplets. In Chinese there are no natural separators between the"
W10-4165,S07-1037,0,0.157504,"organized as following: ﬁrstly we introduce the related works. Then we talk about the methods in features selection and clustering. The method of evaluation and the result of our system is following. At last we discuss the improvement and the weakness of our system. 2 Related Works Sense induction is typically treated as a clustering problem, by considering their cooccurring contexts, the instances of a target word are partitioned into classes. Previous methods have used the ﬁrst or second order co-occurrence (Pedersen and Bruce, 1997; Schütze, 1998), parts of speech, and local collocations (Niu et al., 2007). The size of context window is also various, it can be as small as only two words before and after the target words. It may be the sentence where the target word is in. Or it will be 20 surrounding words on either side of the target words and even more words. After every instance of the target word is represented as a feature vector, it will be the input of the clustering methods. Many clustering methods have been used in the task of word sense induction. For example, kmeans and agglomerative clustering (Schütze, 1998). sIB (Sequential Information Bottleneck) a variation of Information Bottle"
W10-4165,W97-0322,0,0.0516194,"e sIB clustering algorithm and at last discriminated the word senses. This paper is organized as following: ﬁrstly we introduce the related works. Then we talk about the methods in features selection and clustering. The method of evaluation and the result of our system is following. At last we discuss the improvement and the weakness of our system. 2 Related Works Sense induction is typically treated as a clustering problem, by considering their cooccurring contexts, the instances of a target word are partitioned into classes. Previous methods have used the ﬁrst or second order co-occurrence (Pedersen and Bruce, 1997; Schütze, 1998), parts of speech, and local collocations (Niu et al., 2007). The size of context window is also various, it can be as small as only two words before and after the target words. It may be the sentence where the target word is in. Or it will be 20 surrounding words on either side of the target words and even more words. After every instance of the target word is represented as a feature vector, it will be the input of the clustering methods. Many clustering methods have been used in the task of word sense induction. For example, kmeans and agglomerative clustering (Schütze, 1998"
W10-4165,J98-1004,0,0.175097,"and at last discriminated the word senses. This paper is organized as following: ﬁrstly we introduce the related works. Then we talk about the methods in features selection and clustering. The method of evaluation and the result of our system is following. At last we discuss the improvement and the weakness of our system. 2 Related Works Sense induction is typically treated as a clustering problem, by considering their cooccurring contexts, the instances of a target word are partitioned into classes. Previous methods have used the ﬁrst or second order co-occurrence (Pedersen and Bruce, 1997; Schütze, 1998), parts of speech, and local collocations (Niu et al., 2007). The size of context window is also various, it can be as small as only two words before and after the target words. It may be the sentence where the target word is in. Or it will be 20 surrounding words on either side of the target words and even more words. After every instance of the target word is represented as a feature vector, it will be the input of the clustering methods. Many clustering methods have been used in the task of word sense induction. For example, kmeans and agglomerative clustering (Schütze, 1998). sIB (Sequenti"
W10-4165,E09-1013,0,\N,Missing
W14-6810,C10-1003,0,0.215589,"ase strategy, where they first build customized text wrappers based on the input seeds in order to exact candidate entities from web pages. Then a graph-based random walk approach is used to rank candidate entities based on their closeness to the seeds on the graph. The third method is set expansion by iterative similarity aggregation (He and Xin, 2011), in which a set of given seed entities is expanded into a more complete set. All these methods are entity expansion from monolingual data sources. Another meaningful work is the bilingual lexicon extraction (Fung and McKeown, 1997; Rapp, 1999; Andrade et al., 2010; Fiˇser et al., 2011; Daille and Morin, 2005; Vulic et al., 2011; Andrade et al., 2011; Bo et al., 2011). Most of the This paper proposes a novel two-stage method for bilingual product name dictionary construction from comparable corpora. In previous work, some researchers study the problem of expanding a set of given seed entities into a more complete set by discovering other entities that also belong to the same concept, it just solves the problem about expansion of entity set in a monolingual language, but the expansion of bilingual entity is really blank problem from comparable corpora. A"
W14-6810,W11-1203,0,0.0529281,"in order to exact candidate entities from web pages. Then a graph-based random walk approach is used to rank candidate entities based on their closeness to the seeds on the graph. The third method is set expansion by iterative similarity aggregation (He and Xin, 2011), in which a set of given seed entities is expanded into a more complete set. All these methods are entity expansion from monolingual data sources. Another meaningful work is the bilingual lexicon extraction (Fung and McKeown, 1997; Rapp, 1999; Andrade et al., 2010; Fiˇser et al., 2011; Daille and Morin, 2005; Vulic et al., 2011; Andrade et al., 2011; Bo et al., 2011). Most of the This paper proposes a novel two-stage method for bilingual product name dictionary construction from comparable corpora. In previous work, some researchers study the problem of expanding a set of given seed entities into a more complete set by discovering other entities that also belong to the same concept, it just solves the problem about expansion of entity set in a monolingual language, but the expansion of bilingual entity is really blank problem from comparable corpora. A typical example is to use/Honda- X”as seed entity, and derive other entities(e.g.,/For"
W14-6810,I05-1062,0,0.138327,"ized text wrappers based on the input seeds in order to exact candidate entities from web pages. Then a graph-based random walk approach is used to rank candidate entities based on their closeness to the seeds on the graph. The third method is set expansion by iterative similarity aggregation (He and Xin, 2011), in which a set of given seed entities is expanded into a more complete set. All these methods are entity expansion from monolingual data sources. Another meaningful work is the bilingual lexicon extraction (Fung and McKeown, 1997; Rapp, 1999; Andrade et al., 2010; Fiˇser et al., 2011; Daille and Morin, 2005; Vulic et al., 2011; Andrade et al., 2011; Bo et al., 2011). Most of the This paper proposes a novel two-stage method for bilingual product name dictionary construction from comparable corpora. In previous work, some researchers study the problem of expanding a set of given seed entities into a more complete set by discovering other entities that also belong to the same concept, it just solves the problem about expansion of entity set in a monolingual language, but the expansion of bilingual entity is really blank problem from comparable corpora. A typical example is to use/Honda- X”as seed e"
W14-6810,W11-1204,0,0.0368204,"Missing"
W14-6810,W97-0119,0,0.45754,"Cohen, 2009), which adoptes a two-phase strategy, where they first build customized text wrappers based on the input seeds in order to exact candidate entities from web pages. Then a graph-based random walk approach is used to rank candidate entities based on their closeness to the seeds on the graph. The third method is set expansion by iterative similarity aggregation (He and Xin, 2011), in which a set of given seed entities is expanded into a more complete set. All these methods are entity expansion from monolingual data sources. Another meaningful work is the bilingual lexicon extraction (Fung and McKeown, 1997; Rapp, 1999; Andrade et al., 2010; Fiˇser et al., 2011; Daille and Morin, 2005; Vulic et al., 2011; Andrade et al., 2011; Bo et al., 2011). Most of the This paper proposes a novel two-stage method for bilingual product name dictionary construction from comparable corpora. In previous work, some researchers study the problem of expanding a set of given seed entities into a more complete set by discovering other entities that also belong to the same concept, it just solves the problem about expansion of entity set in a monolingual language, but the expansion of bilingual entity is really blank"
W14-6810,J03-1002,0,0.00364773,"only used to complete the task. By the tool of machine translation, two different language context of entity is mapped to the same language space. Many studies on machine translation use GIZA++ as their underlying word-by-word alignment system. Machine translation systems have also benefited from such alignment, performing it at the character level (AbdulJaleel and Larkey, 2003), (Virga and Khudanpur, 2003), (Gao et al., 2005). GIZA++ is a statistical machine translation toolkit freely available for research purposes. The extended version of this toolkit is called GIZA++ and was developed by (Och and Ney, 2003). We employ the word-based translation model to perform context alignment, we get the alignment probability between the context pattern Looking Up Context of Every Entity With the bootstrapping algorithm, we get the set of semantic category entity in English and Chinese comparable corpora. For every entity, we look up their context, and use the method of string matching in the corpora. We use 3 forward and backward window of word as context, That is what we call the context. The context and their corresponding entities have great relevance. As an example, it is easier for us to find some words"
W14-6810,P99-1067,0,0.648922,"tes a two-phase strategy, where they first build customized text wrappers based on the input seeds in order to exact candidate entities from web pages. Then a graph-based random walk approach is used to rank candidate entities based on their closeness to the seeds on the graph. The third method is set expansion by iterative similarity aggregation (He and Xin, 2011), in which a set of given seed entities is expanded into a more complete set. All these methods are entity expansion from monolingual data sources. Another meaningful work is the bilingual lexicon extraction (Fung and McKeown, 1997; Rapp, 1999; Andrade et al., 2010; Fiˇser et al., 2011; Daille and Morin, 2005; Vulic et al., 2011; Andrade et al., 2011; Bo et al., 2011). Most of the This paper proposes a novel two-stage method for bilingual product name dictionary construction from comparable corpora. In previous work, some researchers study the problem of expanding a set of given seed entities into a more complete set by discovering other entities that also belong to the same concept, it just solves the problem about expansion of entity set in a monolingual language, but the expansion of bilingual entity is really blank problem from"
W14-6810,W06-2919,0,0.0232672,"nsive list of the same semantic entity, while some seed entities of the same concept are given as input. The second problem is to find bilingual entity translation from comparable corpora. Related Work There is a significant body of related work in the broad space of information extraction and named entity extraction. We will only summarize work most relevant to set expansion and bilingual entity extraction due to the limit of space. Google sets does set expansion using propriety algorithms which are not publicly available. (He and Xin, 2011) expand seeds by iterative similarity aggregation. (Talukdar et al., 2006) studied the problem of set expansion of open text, which proposes to automatically identify triggerwords which indicate patterns in a bootstrapping manner. (Ghahramani and Heller, 2005) used the method of Bayesian inference to solve the problem of set expansion. In comparison, our approach expands bilingual entity seeds set by using bootstrapping algorithms ,which learn entity candidates and their corresponding patterns iteratively. Our goal is to find the same semantic concept set . (Fung and McKeown, 1997) present a statistical word feature that is said to the word relation matrix, which ca"
W14-6810,D12-1003,0,0.0541733,"ating domainspecific bilingual lexicon from comparable corpora. (Vulic et al., 2011) investigate the algorithm of bilingual topic models, which finds translations of terms in comparable corpora by using knowledge from word-topic distributions. (Andrade et al., 2011) propose to perform a linear transformation of the context vectors, the new word translations are found by context similarity. (Bo et al., 2011) introduce a clustering-based approach for enhancing corpus comparability which exploits the homogeneity feature of the corpus, and preserves most of the vocabulary of the original corpus. (Tamura et al., 2012) proposes a novel method for lexicon extraction that extracts translation pairs from comparable corpora by using graph- based label propagation. All the methods mentioned above may potentially extract entities translation pairs when context of entities are similarity. We are also based on this assumption, but we are different from the previous models where we use machine translation model to map the context of entity to the same language space, which can improve performance and illustrate robustness. 3 Bootstrapping for Entity Set Expansion 4.1 Growing Seed Dictionary We focus on the problem o"
W14-6810,W02-1028,0,0.0670251,"problem of how to grow the seed dictionary and discovering new product names from user reviews. In this section, we use the seed entity to automatically generate semantic lexicons. For the specific case of brand discovery, this initial list used to generate semantic lexicons must contain only names that are unambiguously. We hence remove ambiguous names or phrases that belong to multiple entity types from the dataset, and only choose those entities as entity seed that it owns definite semantic. We used a weakly supervised bootstrapping algorithm that automatically generates semantic lexicons (Thelen and Riloff, 2002). Bootstrapping algorithms hypothesizes the semantic class of entity by gathering collective evidence about semantic associations from extraction pattern context. For our representation of extraction patterns, we used the AutoSlog system (Riloff, 1996), AutoSlog’s extraction patterns represent linguistic expressions that extract a noun phrase in one of three syntactic roles: subject, direct object, or prepositional phrase object. Before bootstrapping begins, we run AutoSlog exhaustively over the corpus to generate an extraction pattern for every noun phrase that appears. In these, noun or noun"
W14-6810,P11-2084,0,0.0747762,"Missing"
W14-6810,D09-1156,0,0.0494844,"Missing"
W14-6810,W03-1730,0,0.057428,"h are collected from www.buzzilions.com and www.carreview.com. For the source language of Chinese, the product dataset contains 8432 reviews which are collected from www.Amazon.cn and www.xche.com.cn. For our experiment,we use a Oxford English-Chinese bilingual dictionary to match similarity semantic reviewer sentence, any two of them are used as comparable corpus,the copora are non-parallel, but loosely compara in term of its content. Though the scale of Chinese corpora is large, most of the reviews are short texts and there are a lot noise in the content. For Chinese, we use the ICTLAS 3.0 (Zhang et al., 2003) toolkit to conduct word segmentation over sentences. To evaluate the effectiveness of our algorithms, we select two semantic entity sets in camera domain and car domain as seeds, where set expansion experiments are conducted . We select these two categories because (1) they are from different domains; and (2) they have different degree of 6.2 Example Output Table 2 lists the top 20 ranked results produced by two stage algorithm for the two domains that we experiment with. In each domain, those terms in boldface are the input seeds. The underlined terms are the results that do not belong to th"
W14-6810,P11-2083,0,\N,Missing
W97-0115,J96-1001,0,\N,Missing
Y06-1034,W02-1011,0,0.0134809,"h these relations defined by the dependency grammars, we find there are three instances about the relation for opinion mining as Figure 2. target S- words same ancestor target S- words A B S- words target C Fig. 2. Three instances of dependency sub-trees for opinion mining S-words = subjective words. The dashed means the nodes may not be related directly. i ) The subjective words are in the children of target as A in Figure 2. In such instance, the sentiment expressions are in the modifiers of targets, which include adjective, noun, participle phrase, and attributive clause, etc. For example, (5) There&apos;s a great movie! 259 Dependency relations: {(there, &apos;s), (&apos;s ,null), (a, movie), (great,movie), (movie,&apos;s)}. (6) Lumumba is a story that deserves to be told. Dependency relations: {(Lumumba,is), (is, null), (a, story), (story, is), (that, story), (deserves, that), (to ,deserves), (be, to), (told, be)}. In the brackets, the second words are the parents. null means the word in the bracket is the root of the dependency tree. In (5), movie is the target and great is the modifier. In (6), story is the target and deserves is the related subjective word. ii ) The target and the subjective word"
Y06-1034,P05-1015,0,0.0605414,"Missing"
Y06-1034,H05-1043,0,0.0767559,"ocedure of searching in the dependency tree of a sentence. The result of our experiments shows that word dependency relation based methods is more flexible and effective than some previous surface patterns based methods. Keywords: Opinion Mining, Dependency Grammars, Sentiment Analysis, Dependency of Words. 1 Introduction Opinion mining is also called sentiment analysis. It is to analyze whether an author expresses a favorable or unfavorable sentiment for a specific target. Now, it has been extensively used in text filtering, public opinion tracking, customer relation management, etc. [8],[9],[12],[19]. For example, during mining the opinion about a product of customers, we need to find what the customers remark on. It means we should not only extract the sentiment expressions, but also find the targets. Therefore, it needs to mine the relation between sentiment expression and target [9]. This paper will focus on it. The rest of the paper is arranged as following. Section 2 introduces some previous work. Section 3 provides how to gain subjective words. Section 4 introduces the dependency relation between words. Section 5 and section 6 put forward our method on mining the relation. Expe"
Y06-1034,P06-2104,0,0.0223501,"the film terms. Both = the number of sentences that contain both the subjective words and the film terms. Total (table 3) = total number of sentences of the corpus. 7 Experiment and Analysis 7.1 Experiment We use the polarity dataset 2.0 in our experiment2. It includes 1000 positive and 1000 negative reviews on films. In our experiment, classification of the sentiment orientation will not be done. We will test the recall and the precision of the extracted pairs. Some of the English parsers, such as Stanford Parser and MiniPAR achieve a high precision to extract word dependency relations [7], [13]. We use the Stanford parser (Version of 2005.7) in our experiment. 2 http://www.cs.cornell.edu/people/pabo/movie-review-data 261 As all the characters in the corpus are in lowercase, which increases difficulty to do name entity extraction. Therefore, the proper nouns, such as titles of films are not included in the targets. Only some popular film terms, such as story, dialogue, plot, script, etc. are included. We totally collect 86 popular film terms from the corpus and online film glossary3. Table 3 shows the statistics of the sentences that contains subjective words and film terms4 of the c"
Y06-1034,W03-1017,0,0.087427,"n between sentiment expression and target [9]. This paper will focus on it. The rest of the paper is arranged as following. Section 2 introduces some previous work. Section 3 provides how to gain subjective words. Section 4 introduces the dependency relation between words. Section 5 and section 6 put forward our method on mining the relation. Experiment and result are discussed in the last section. 2 Previous Work There is some previous work about opinion mining. Some previous work usually depend on the position between words [8],[9],[12],[17]. In some of the publications, n-gram was used [1],[15]. Turney (2002) utilized some phrase patterns such as RB/RBR/RBS+JJ+NN/NNS; JJ+NN/NNS+ Anything; RB+VB+ Anything. These patterns are composed of subjective adjectives, nouns, adverbs, etc. Nasukawa (2003) uses some patterns as V+Obj: admire somebody, JJ+Obj: crude oil. 257 V is the subjective verbs, JJ represents subjective adjectives. Obj represents the target. Nasukawa further defined some transfer verbs in the patterns, such as get, feel, etc, which may help to expression sentiment. Yi et al. (2003) utilized some patterns based on BNP. Popescu and Etzioni (2005) defined 10 kinds of rules to"
Y06-1034,P02-1053,0,0.00362837,"nd the targets. Therefore, it needs to mine the relation between sentiment expression and target [9]. This paper will focus on it. The rest of the paper is arranged as following. Section 2 introduces some previous work. Section 3 provides how to gain subjective words. Section 4 introduces the dependency relation between words. Section 5 and section 6 put forward our method on mining the relation. Experiment and result are discussed in the last section. 2 Previous Work There is some previous work about opinion mining. Some previous work usually depend on the position between words [8],[9],[12],[17]. In some of the publications, n-gram was used [1],[15]. Turney (2002) utilized some phrase patterns such as RB/RBR/RBS+JJ+NN/NNS; JJ+NN/NNS+ Anything; RB+VB+ Anything. These patterns are composed of subjective adjectives, nouns, adverbs, etc. Nasukawa (2003) uses some patterns as V+Obj: admire somebody, JJ+Obj: crude oil. 257 V is the subjective verbs, JJ represents subjective adjectives. Obj represents the target. Nasukawa further defined some transfer verbs in the patterns, such as get, feel, etc, which may help to expression sentiment. Yi et al. (2003) utilized some patterns based on BNP."
Y06-1034,H05-2017,0,\N,Missing
