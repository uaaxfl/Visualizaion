P13-1097,Probabilistic Sense Sentiment Similarity through Hidden Emotions,2013,19,6,3,0,12657,mitra mohtarami,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"Sentiment Similarity of word pairs reflects the distance between the words regarding their underlying sentiments. This paper aims to infer the sentiment similarity between word pairs with respect to their senses. To achieve this aim, we propose a probabilistic emotionbased approach that is built on a hidden emotional model. The model aims to predict a vector of basic human emotions for each sense of the words. The resultant emotional vectors are then employed to infer the sentiment similarity of word pairs. We apply the proposed approach to address two main NLP tasks, namely, Indirect yes/no Question Answer Pairs inference and Sentiment Orientation prediction. Extensive experiments demonstrate the effectiveness of the proposed approach."
D13-1163,Lexical Chain Based Cohesion Models for Document-Level Statistical Machine Translation,2013,26,31,4,0,3236,deyi xiong,Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,0,"Lexical chains provide a representation of the lexical cohesion structure of a text. In this paper, we propose two lexical chain based cohesion models to incorporate lexical cohesion into document-level statistical machine translation: 1) a count cohesion model that rewards a hypothesis whenever a chain word occurs in the hypothesis, 2) and a probability cohesion model that further takes chain word translation probabilities into account. We compute lexical chains for each source document to be translated and generate target lexical chains based on the computed source chains via maximum entropy classifiers. We then use the generated target chains to provide constraints for word selection in document-level machine translation through the two proposed lexical chain based cohesion models. We verify the effectiveness of the two models using a hierarchical phrase-based translation system. Experiments on large-scale training data show that they can substantially improve translation quality in terms of BLEU and that the probability cohesion model outperforms previous models based on lexical cohesion devices."
Y12-1061,Improved Constituent Context Model with Features,2012,32,3,3,1,41991,yun huang,"Proceedings of the 26th Pacific Asia Conference on Language, Information, and Computation",0,"The Constituent-Context Model (CCM) achieves promising results for unsupervised grammar induction. However, its performance drops for longer sentences. In this paper, we describe a general feature-based model for CCM, in which linguistic knowledge can be easily integrated as features. Features take the log-linear form with local normalization, so the Expectation-Maximization (EM) algorithm is still applicable to estimate model parameters. The l1-norm is used to control the model complexity, leading to sparse and compact grammar. We also propose to use a separated development to perform model selection and an additional test set to evaluate the performance. Under this framework, we could automatically choose suitable model parameters rather than setting them empirically. Experiments on the English treebank demonstrate that the feature-based model achieves comparable performance on short sentences but significant improvement on longer sentences."
D12-1026,N-gram-based Tense Models for Statistical Machine Translation,2012,17,16,3,0,9183,zhengxian gong,Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,0,"Tense is a small element to a sentence, however, error tense can raise odd grammars and result in misunderstanding. Recently, tense has drawn attention in many natural language processing applications. However, most of current Statistical Machine Translation (SMT) systems mainly depend on translation model and language model. They never consider and make full use of tense information. In this paper, we propose n-gram-based tense models for SMT and successfully integrate them into a state-of-the-art phrase-based SMT system via two additional features. Experimental results on the NIST Chinese-English translation task show that our proposed tense models are very effective, contributing performance improvement by 0.62 BLUE points over a strong baseline."
P11-2094,Nonparametric {B}ayesian Machine Transliteration with Synchronous {A}daptor {G}rammars,2011,20,12,3,1,41991,yun huang,Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,0,"Machine transliteration is defined as automatic phonetic translation of names across languages. In this paper, we propose synchronous adaptor grammar, a novel nonpara-metric Bayesian learning approach, for machine transliteration. This model provides a general framework without heuristic or restriction to automatically learn syllable equivalents between languages. The proposed model outperforms the state-of-the-art EM-based model in the English to Chinese transliteration task."
I11-1012,A Unified Event Coreference Resolution by Integrating Multiple Resolvers,2011,19,25,4,1,36709,bin chen,Proceedings of 5th International Joint Conference on Natural Language Processing,0,"Event coreference is an important and complicated task in cascaded event template extraction and other natural language processing tasks. Despite its importance, it was merely discussed in previous studies. In this paper, we present a globally optimized coreference resolution system dedicated to various sophisticated event coreference phenomena. Seven resolvers for both event and object coreference cases are utilized, which include three new resolvers for event coreference resolution. Three enhancements are further proposed at both mention pair detection and chain formation levels. First, the object coreference resolvers are used to effectively reduce the false positive cases for event coreference. Second, A revised instance selection scheme is proposed to improve link level mention-pair model performances. Last but not least, an efficient and globally optimized graph partitioning model is employed for coreference chain formation using spectral partitioning which allows the incorporation of pronoun coreference information. The three techniques contribute to a significant improvement of 8.54% in B 3 F-score for event coreference resolution on OntoNotes 2.0 corpus."
I11-1063,A {W}ikipedia-{LDA} Model for Entity Linking with Batch Size Changing Instance Selection,2011,26,12,3,1,8103,wei zhang,Proceedings of 5th International Joint Conference on Natural Language Processing,0,"Entity linking maps name mentions in context to entries in a knowledge base through resolving the name variations and ambiguities. In this paper, we propose two advancements for entity linking. First, a Wikipedia-LDA method is proposed to model the contexts as the probability distributions over Wikipedia categories, which allows the context similarity being measured in a semantic space instead of literal term space used by other studies for the disambiguation. Furthermore, to automate the training instance annotation without compromising the accuracy, an instance selection strategy is proposed to select an informative, representative and diverse subset from an auto-generated dataset. During the iterative selection process, the batch sizes at each iteration change according to the variance of classifierxe2x80x99s confidence or accuracy between batches in sequence, which not only makes the selection insensitive to the initial batch size, but also leads to a better performance. The above two advancements give significant improvements to entity linking individually. Collectively they lead the highest performance on KBP-10 task. Being a generic approach, the batch size changing method can also benefit active learning for other tasks."
P10-1032,Exploring Syntactic Structural Features for Sub-Tree Alignment Using Bilingual Tree Kernels,2010,19,18,3,1,33268,jun sun,Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,1,We propose Bilingual Tree Kernels (BTKs) to capture the structural similarities across a pair of syntactic translational equivalences and apply BTKs to sub-tree alignment along with some plain features. Our study reveals that the structural features embedded in a bilingual parse tree pair are very effective for sub-tree alignment and the bilingual tree kernels can well capture such features. The experimental results show that our approach achieves a significant improvement on both gold standard tree bank and automatically parsed tree pairs against a heuristic similarity based method. We further apply the sub-tree alignment in machine translation with two methods. It is suggested that the subtree alignment benefits both phrase and syntax based systems by relaxing the constraint of the word alignment.
P10-1073,Kernel Based Discourse Relation Recognition with Temporal Ordering Information,2010,19,35,3,0,9118,wenting wang,Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,1,"Syntactic knowledge is important for discourse relation recognition. Yet only heuristically selected flat paths and 2-level production rules have been used to incorporate such information so far. In this paper we propose using tree kernel based approach to automatically mine the syntactic information from the parse trees for discourse analysis, applying kernel function to the tree structures directly. These structural syntactic features, together with other normal flat features are incorporated into our composite kernel to capture diverse knowledge for simultaneous discourse identification and classification for both explicit and implicit relations. The experiment shows tree kernel approach is able to give statistical significant improvements over flat syntactic path feature. We also illustrate that tree kernel approach covers more structure information than the production rules, which allows tree kernel to further incorporate information from a higher dimension space for possible better discrimination. Besides, we further propose to leverage on temporal ordering information to constrain the interpretation of discourse relation, which also demonstrate statistical significant improvements for discourse relation recognition on PDTB 2.0 for both explicit and implicit as well."
D10-1085,Resolving Event Noun Phrases to Their Verbal Mentions,2010,22,10,3,1,36709,bin chen,Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,0,"Event Anaphora Resolution is an important task for cascaded event template extraction and other NLP study. Previous study only touched on event pronoun resolution. In this paper, we provide the first systematic study to resolve event noun phrases to their verbal mentions crossing long distances. Our study shows various lexical, syntactic and positional features are needed for event noun phrase resolution and most of them, such as morphology relation, synonym and etc, are different from those features used for conventional noun phrase resolution. Syntactic structural information in the parse tree modeled with tree kernel is combined with the above diverse flat features using a composite kernel, which shows more than 10% F-score improvement over the flat features baseline. In addition, we employed a twin-candidate based model to capture the pair-wise candidate preference knowledge, which further demonstrates a statistically significant improvement. All the above contributes to an encouraging performance of 61.36% F-score on OntoNotes corpus."
C10-2172,Predicting Discourse Connectives for Implicit Discourse Relation Recognition,2010,18,78,6,0,46509,zhimin zhou,Coling 2010: Posters,0,Existing works indicate that the absence of explicit discourse connectives makes it difficult to recognize implicit discourse relations. In this paper we attempt to overcome this difficulty for implicit relation recognition by automatically inserting discourse connectives between arguments with the use of a language model. Then we propose two algorithms to leverage the information of these predicted connectives. One is to use these predicted implicit connectives as additional features in a supervised model. The other is to perform implicit relation recognition based only on these predicted connectives. Results on Penn Discourse Treebank 2.0 show that predicted discourse connectives help implicit relation recognition and the first algorithm can achieve an absolute average f-score improvement of 3% over a state of the art baseline system.
C10-1022,A Twin-Candidate Based Approach for Event Pronoun Resolution using Composite Kernel,2010,26,10,3,1,36709,bin chen,Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010),0,"Event Anaphora Resolution is an important task for cascaded event template extraction and other NLP study. In this paper, we provide a first systematic study of resolving pronouns to their event verb antecedents for general purpose. First, we explore various positional, lexical and syntactic features useful for the event pronoun resolution. We further explore tree kernel to model structural information embedded in syntactic parses. A composite kernel is then used to combine the above diverse information. In addition, we employed a twin-candidate based preferences learning model to capture the pair wise candidates' preference knowledge. Besides we also look into the incorporation of the negative training instances with anaphoric pronouns whose antecedents are not verbs. Although these negative training instances are not used in previous study on anaphora resolution, our study shows that they are very useful for the final resolution through random sampling strategy. Our experiments demonstrate that it's meaningful to keep certain training data as development data to help SVM select a more accurate hyper plane which provides significant improvement over the default setting with all training data."
C10-1118,Discriminative Induction of Sub-Tree Alignment using Limited Labeled Data,2010,21,5,3,1,33268,jun sun,Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010),0,"We employ Maximum Entropy model to conduct sub-tree alignment between bilingual phrasal structure trees. Various lexical and structural knowledge is explored to measure the syntactic similarity across Chinese-English bilingual tree pairs. In the experiment, we evaluate the sub-tree alignment using both gold standard tree bank and the automatically parsed corpus with manually annotated sub-tree alignment. Compared with a heuristic similarity based method, the proposed method significantly improves the performance with only limited sub-tree aligned data. To examine its effectiveness for multilingual applications, we further attempt different approaches to apply the sub-tree alignment in both phrase and syntax based SMT systems. We then compare the performance with that of the widely used word alignment. Experimental results on benchmark data show that sub-tree alignment benefits both systems by relaxing the constraint of the word alignment."
C10-1145,Entity Linking Leveraging Automatically Generated Annotation,2010,20,76,3,1,8103,wei zhang,Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010),0,"Entity linking refers entity mentions in a document to their representations in a knowledge base (KB). In this paper, we propose to use additional information sources from Wikipedia to find more name variations for entity linking task. In addition, as manually creating a training corpus for entity linking is laborintensive and costly, we present a novel method to automatically generate a large scale corpus annotation for ambiguous mentions leveraging on their unambiguous synonyms in the document collection. Then, a binary classifier is trained to filter out KB entities that are not similar to current mentions. This classifier not only can effectively reduce the ambiguities to the existing entities in KB, but also be very useful to highlight the new entities to KB for the further population. Furthermore, we also leverage on the Wikipedia documents to provide additional information which is not available in our generated corpus through a domain adaption approach which provides further performance improvements. The experiment results show that our proposed method outperforms the state-of-the-art approaches."
W09-3721,An Ordering of Terms Based on Semantic Relatedness,2009,54,6,3,0,46843,peter wittek,Proceedings of the Eight International Conference on Computational Semantics,0,"Term selection methods typically employ a statistical measure to filter or weight terms. Term expansion for IR may also depend on statistics, or use some other, non-metric method based on a lexical resource. At the same time, a wide range of semantic similarity measures have been developed to support natural language processing tasks such as word sense disambiguation. This paper combines the two approaches and proposes an algorithm that provides a semantic order of terms based on a semantic relatedness measure. This semantic order can be exploited by term weighting and term expansion methods."
W09-1123,Improving Text Classification by a Sense Spectrum Approach to Term Expansion,2009,31,9,3,0,46843,peter wittek,Proceedings of the Thirteenth Conference on Computational Natural Language Learning ({C}o{NLL}-2009),0,"Experimenting with different mathematical objects for text representation is an important step of building text classification models. In order to be efficient, such objects of a formal model, like vectors, have to reasonably reproduce language-related phenomena such as word meaning inherent in index terms. We introduce an algorithm for sense-based semantic ordering of index terms which approximates Cruse's description of a sense spectrum. Following semantic ordering, text classification by support vector machines can benefit from semantic smoothing kernels that regard semantic relations among index terms while computing document similarity. Adding expansion terms to the vector representation can also improve effectiveness. This paper proposes a new kernel which discounts less important expansion terms based on lexical relatedness."
P09-1020,Forest-based Tree Sequence to String Translation Model,2009,23,28,5,0,11711,hui zhang,Proceedings of the Joint Conference of the 47th Annual Meeting of the {ACL} and the 4th International Joint Conference on Natural Language Processing of the {AFNLP},1,"This paper proposes a forest-based tree sequence to string translation model for syntax-based statistical machine translation, which automatically learns tree sequence to string translation rules from word-aligned source-side-parsed bilingual texts. The proposed model leverages on the strengths of both tree sequence-based and forest-based translation models. Therefore, it can not only utilize forest structure that compactly encodes exponential number of parse trees but also capture nonsyntactic translation equivalences with linguistically structured information through tree sequence. This makes our model potentially more robust to parse errors and structure divergence. Experimental results on the NIST MT-2003 Chinese-English translation task show that our method statistically significantly outperforms the four baseline systems."
P09-1103,A non-contiguous Tree Sequence Alignment-based Model for Statistical Machine Translation,2009,24,12,3,1,33268,jun sun,Proceedings of the Joint Conference of the 47th Annual Meeting of the {ACL} and the 4th International Joint Conference on Natural Language Processing of the {AFNLP},1,"The tree sequence based translation model allows the violation of syntactic boundaries in a rule to capture non-syntactic phrases, where a tree sequence is a contiguous sequence of subtrees. This paper goes further to present a translation model based on non-contiguous tree sequence alignment, where a non-contiguous tree sequence is a sequence of sub-trees and gaps. Compared with the contiguous tree sequence-based model, the proposed model can well handle non-contiguous phrases with any large gaps by means of non-contiguous tree sequence alignment. An algorithm targeting the noncontiguous constituent decoding is also proposed. Experimental results on the NIST MT-05 Chinese-English translation task show that the proposed model statistically significantly outperforms the baseline systems."
D09-1108,Fast Translation Rule Matching for Syntax-based Statistical Machine Translation,2009,21,10,4,0,11711,hui zhang,Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,0,"In a linguistically-motivated syntax-based translation system, the entire translation process is normally carried out in two steps, translation rule matching and target sentence decoding using the matched rules. Both steps are very time-consuming due to the tremendous number of translation rules, the exhaustive search in translation rule matching and the complex nature of the translation task itself. In this paper, we propose a hyper-tree-based fast algorithm for translation rule matching. Experimental results on the NIST MT-2003 Chinese-English translation task show that our algorithm is at least 19 times faster in rule matching and is able to help to save 57% of overall translation time over previous methods when using large fragment translation rules."
D09-1161,K-Best Combination of Syntactic Parsers,2009,25,44,3,0,11711,hui zhang,Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,0,"In this paper, we propose a linear model-based general framework to combine k-best parse outputs from multiple parsers. The proposed framework leverages on the strengths of previous system combination and re-ranking techniques in parsing by integrating them into a linear model. As a result, it is able to fully utilize both the logarithm of the probability of each k-best parse tree from each individual parser and any additional useful features. For feature weight tuning, we compare the simulated-annealing algorithm and the perceptron algorithm. Our experiments are carried out on both the Chinese and English Penn Treebank syntactic parsing task by combining two state-of-the-art parsing models, a head-driven lexicalized model and a latent-annotation-based un-lexicalized model. Experimental results show that our F-Scores of 85.45 on Chinese and 92.62 on English outperform the previously best-reported systems by 1.21 and 0.52, respectively."
P08-1064,A Tree Sequence Alignment-based Tree-to-Tree Translation Model,2008,34,104,5,0.485213,3694,min zhang,Proceedings of ACL-08: HLT,1,"This paper presents a translation model that is based on tree sequence alignment, where a tree sequence refers to a single sequence of subtrees that covers a phrase. The model leverages on the strengths of both phrase-based and linguistically syntax-based method. It automatically learns aligned tree sequence pairs with mapping probabilities from word-aligned biparsed parallel texts. Compared with previous models, it not only captures non-syntactic phrases and discontinuous phrases with linguistically structured features, but also supports multi-level structure reordering of tree typology with larger span. This gives our model stronger expressive power than other reported models. Experimental results on the NIST MT-2005 Chinese-English translation task show that our method statistically significantly outperforms the baseline systems."
P08-1096,An Entity-Mention Model for Coreference Resolution with Inductive Logic Programming,2008,18,62,4,1,47859,xiaofeng yang,Proceedings of ACL-08: HLT,1,"The traditional mention-pair model for coreference resolution cannot capture information beyond mention pairs for both learning and testing. To deal with this problem, we present an expressive entity-mention model that performs coreference resolution at an entity level. The model adopts the Inductive Logic Programming (ILP) algorithm, which provides a relational way to organize different knowledge of entities and mentions. The solution can explicitly express relations between an entity and the contained mentions, and automatically learn first-order rules important for coreference decision. The evaluation on the ACE data set shows that the ILP based entity-mention model is effective for the coreference resolution task."
J08-3002,A Twin-Candidate Model for Learning-Based Anaphora Resolution,2008,36,36,3,1,47859,xiaofeng yang,Computational Linguistics,0,"The traditional single-candidate learning model for anaphora resolution considers the antecedent candidates of an anaphor in isolation, and thus cannot effectively capture the preference relationships between competing candidates for its learning and resolution. To deal with this problem, we propose a twin-candidate model for anaphora resolution. The main idea behind the model is to recast anaphora resolution as a preference classification problem. Specifically, the model learns a classifier that determines the preference between competing candidates, and, during resolution, chooses the antecedent of a given anaphor based on the ranking of the candidates. We present in detail the framework of the twin-candidate model for anaphora resolution. Further, we explore how to deploy the model in the more complicated coreference resolution task. We evaluate the twin-candidate model in different domains using the Automatic Content Extraction data sets. The experimental results indicate that our twin-candidate model is superior to the single-candidate model for the task of pronominal anaphora resolution. For the task of coreference resolution, it also performs equally well, or better."
I08-2109,Fast Computing Grammar-driven Convolution Tree Kernel for Semantic Role Labeling,2008,11,0,4,0.833333,1017,wanxiang che,Proceedings of the Third International Joint Conference on Natural Language Processing: Volume-{II},0,"Grammar-driven convolution tree kernel (GTK) has shown promising results for semantic role labeling (SRL). However, the time complexity of computing the GTK is exponential in theory. In order to speed up the computing process, we design two fast grammar-driven convolution tree kernel (FGTK) algorithms, which can compute the GTK in polynomial time. Experimental results on the CoNLL-2005 SRL data show that our two FGTK algorithms are much faster than the GTK."
I08-1008,Name Origin Recognition Using Maximum Entropy Model and Diverse Features,2008,6,2,5,0.485213,3694,min zhang,Proceedings of the Third International Joint Conference on Natural Language Processing: Volume-{I},0,"Name origin recognition is to identify the source language of a personal or location name. Some early work used either rulebased or statistical methods with single knowledge source. In this paper, we cast the name origin recognition as a multi-class classification problem and approach the problem using Maximum Entropy method. In doing so, we investigate the use of different features, including phonetic rules, ngram statistics and character position information for name origin recognition. Experiments on a publicly available personal name database show that the proposed approach achieves an overall accuracy of 98.44% for names written in English and 98.10% for names written in Chinese, which are significantly and consistently better than those in reported work."
C08-1016,Other-Anaphora Resolution in Biomedical Texts with Automatically Mined Patterns,2008,10,3,4,1,36709,bin chen,Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008),0,"This paper proposes an other-anaphora resolution approach in bio-medical texts. It utilizes automatically mined patterns to discover the semantic relation between an anaphor and a candidate antecedent. The knowledge from lexical patterns is incorporated in a machine learning framework to perform anaphora resolution. The experiments show that machine learning approach combined with the auto-mined knowledge is effective for other-anaphora resolution in the biomedical domain. Our system with auto-mined patterns gives an accuracy of 56.5%., yielding 16.2% improvement against the baseline system without pattern features, and 9% improvement against the system using manually designed patterns."
P07-1026,A Grammar-driven Convolution Tree Kernel for Semantic Role Classification,2007,27,25,4,0.485213,3694,min zhang,Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,1,"Convolution tree kernel has shown promising results in semantic role classification. However, it only carries out hard matching, which may lead to over-fitting and less accurate similarity measure. To remove the constraint, this paper proposes a grammardriven convolution tree kernel for semantic role classification by introducing more linguistic knowledge into the standard tree kernel. The proposed grammar-driven tree kernel displays two advantages over the previous one: 1) grammar-driven approximate substructure matching and 2) grammardriven approximate tree node matching. The two improvements enable the grammardriven tree kernel explore more linguistically motivated structure features than the previous one. Experiments on the CoNLL-2005 SRL shared task show that the grammardriven tree kernel significantly outperforms the previous non-grammar-driven one in SRL. Moreover, we present a composite kernel to integrate feature-based and tree kernel-based methods. Experimental results show that the composite kernel outperforms the previously best-reported methods."
2007.mtsummit-papers.71,A tree-to-tree alignment-based model for statistical machine translation,2007,-1,-1,6,0.485213,3694,min zhang,Proceedings of Machine Translation Summit XI: Papers,0,None
W06-1649,Partially Supervised Sense Disambiguation by Learning Sense Number from Tagged and Untagged Corpora,2006,18,0,3,1,9439,zhengyu niu,Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing,0,"Supervised and semi-supervised sense disambiguation methods will mis-tag the instances of a target word if the senses of these instances are not defined in sense inventories or there are no tagged instances for these senses in training data. Here we used a model order identification method to avoid the misclassification of the instances with undefined senses by discovering new senses from mixed data (tagged and untagged corpora). This algorithm tries to obtain a natural partition of the mixed data by maximizing a stability criterion defined on the classification result from an extended label propagation algorithm over all the possible values of the number of senses (or sense number, model order). Experimental results on SENSEVAL-3 data indicate that it outperforms SVM, a one-class partially supervised classification algorithm, and a clustering based model order identification algorithm when the tagged data is incomplete."
W06-1667,Unsupervised Relation Disambiguation with Order Identification Capabilities,2006,16,3,3,1,49775,jinxiu chen,Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing,0,"We present an unsupervised learning approach to disambiguate various relations between name entities by use of various lexical and syntactic features from the contexts. It works by calculating eigen-vectors of an adjacency graph's Laplacian to recover a submanifold of data from a high dimensionality space and then performing cluster number estimation on the eigenvectors. This method can address two difficulties encoutered in Hasegawa et al. (2004)'s hierarchical clustering: no consideration of manifold structure in data, and requirement to provide cluster number by users. Experiment results on ACE corpora show that this spectral clustering based approach outperforms Hasegawa et al. (2004)'s hierarchical clustering method and a plain k-means clustering method."
P06-2012,Unsupervised Relation Disambiguation Using Spectral Clustering,2006,16,14,3,1,49775,jinxiu chen,Proceedings of the {COLING}/{ACL} 2006 Main Conference Poster Sessions,0,This paper presents an unsupervised learning approach to disambiguate various relations between name entities by use of various lexical and syntactic features from the contexts. It works by calculating eigen-vectors of an adjacency graph's Laplacian to recover a submanifold of data from a high dimensionality space and then performing cluster number estimation on the eigenvectors. Experiment results on ACE corpora show that this spectral clustering based approach outperforms the other clustering methods.
P06-1006,Kernel-Based Pronoun Resolution with Structured Syntactic Knowledge,2006,20,64,3,1,47859,xiaofeng yang,Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,1,"Syntactic knowledge is important for pronoun resolution. Traditionally, the syntactic information for pronoun resolution is represented in terms of features that have to be selected and defined heuristically. In the paper, we propose a kernel-based method that can automatically mine the syntactic information from the parse trees for pronoun resolution. Specifically, we utilize the parse trees directly as a structured feature and apply kernel functions to this feature, as well as other normal features, to learn the resolution classifier. In this way, our approach avoids the efforts of decoding the parse trees into the set of flat syntactic features. The experimental results show that our approach can bring significant performance improvement and is reliably effective for the pronoun resolution task."
P06-1017,Relation Extraction Using Label Propagation Based Semi-Supervised Learning,2006,17,69,3,1,49775,jinxiu chen,Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,1,"Shortage of manually labeled data is an obstacle to supervised relation extraction methods. In this paper we investigate a graph based semi-supervised learning algorithm, a label propagation (LP) algorithm, for relation extraction. It represents labeled and unlabeled examples and their distances as the nodes and the weights of edges of a graph, and tries to obtain a labeling function to satisfy two constraints: 1) it should be fixed on the labeled nodes, 2) it should be smooth on the whole graph. Experiment results on the ACE corpus showed that this LP algorithm achieves better performance than SVM when only very few labeled examples are available, and it also performs better than bootstrapping for the relation extraction task."
N06-2007,Semi-supervised Relation Extraction with Label Propagation,2006,12,9,3,1,49775,jinxiu chen,"Proceedings of the Human Language Technology Conference of the {NAACL}, Companion Volume: Short Papers",0,"To overcome the problem of not having enough manually labeled relation instances for supervised relation extraction methods, in this paper we propose a label propagation (LP) based semi-supervised learning algorithm for relation extraction task to learn from both labeled and unlabeled data. Evaluation on the ACE corpus showed when only a few labeled examples are available, our LP based relation extraction can achieve better performance than SVM and another bootstrapping method."
P05-1021,Improving Pronoun Resolution Using Statistics-Based Semantic Compatibility Information,2005,17,41,3,0.5,47859,xiaofeng yang,Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics ({ACL}{'}05),1,"In this paper we focus on how to improve pronoun resolution using the statistics-based semantic compatibility information. We investigate two unexplored issues that influence the effectiveness of such information: statistics source and learning framework. Specifically, we for the first time propose to utilize the web and the twin-candidate model, in addition to the previous combination of the corpus and the single-candidate model, to compute and apply the semantic information. Our study shows that the semantic compatibility obtained from the web can be effectively incorporated in the twin-candidate learning model and significantly improve the resolution of neutral pronouns."
P05-1049,Word Sense Disambiguation Using Label Propagation Based Semi-Supervised Learning,2005,36,93,3,1,9439,zhengyu niu,Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics ({ACL}{'}05),1,"Shortage of manually sense-tagged data is an obstacle to supervised word sense disambiguation methods. In this paper we investigate a label propagation based semi-supervised learning algorithm for WSD, which combines labeled and unlabeled data in learning process to fully realize a global consistency assumption: similar examples should have similar labels. Our experimental results on benchmark corpora indicate that it consistently outperforms SVM when only very few labeled examples are available, and its performance is also better than monolingual bootstrapping, and comparable to bilingual bootstrapping."
I05-2045,Unsupervised Feature Selection for Relation Extraction,2005,14,52,3,1,49775,jinxiu chen,Companion Volume to the Proceedings of Conference including Posters/Demos and tutorial abstracts,0,"This paper presents an unsupervised relation extraction algorithm, which induces relations between entity pairs by grouping them into a xe2x80x9cnaturalxe2x80x9d number of clusters based on the similarity of their contexts. Stability-based criterion is used to automatically estimate the number of clusters. For removing noisy feature words in clustering procedure, feature selection is conducted by optimizing a trace based criterion subject to some constraint in an unsupervised manner. After relation clustering procedure, we employ a discriminative category matching (DCM) to find typical and discriminative words to represent different relations. Experimental results show the effectiveness of our algorithm."
I05-1034,Discovering Relations Between Named Entities from a Large Raw Corpus Using Tree Similarity-Based Clustering,2005,17,59,5,0.441354,3694,min zhang,Second International Joint Conference on Natural Language Processing: Full Papers,0,"We propose a tree-similarity-based unsupervised learning method to extract relations between Named Entities from a large raw corpus. Our method regards relation extraction as a clustering problem on shallow parse trees. First, we modify previous tree kernels on relation extraction to estimate the similarity between parse trees more efficiently. Then, the similarity between parse trees is used in a hierarchical clustering algorithm to group entity pairs into different clusters. Finally, each cluster is labeled by an indicative word and unreliable clusters are pruned out. Evaluation on the New York Times (1995) corpus shows that our method outperforms the only previous work by 5 in F-measure. It also shows that our method performs well on both high-frequent and less-frequent entity pairs. To the best of our knowledge, this is the first work to use a tree similarity metric in relation clustering."
I05-1035,Automatic Relation Extraction with Model Order Selection and Discriminative Label Identification,2005,16,17,3,1,49775,jinxiu chen,Second International Joint Conference on Natural Language Processing: Full Papers,0,"In this paper, we study the problem of unsupervised relation extraction based on model order identification and discriminative feature analysis. The model order identification is achieved by stability-based clustering and used to infer the number of the relation types between entity pairs automatically. The discriminative feature analysis is used to find discriminative feature words to name the relation types. Experiments on ACE corpus show that the method is promising."
I05-1063,A Twin-Candidate Model of Coreference Resolution with Non-Anaphor Identification Capability,2005,10,11,3,0.5,47859,xiaofeng yang,Second International Joint Conference on Natural Language Processing: Full Papers,0,"Although effective for antecedent determination, the traditional twin-candidate model can not prevent the invalid resolution of non-anaphors without additional measures. In this paper we propose a modified learning framework for the twin-candidate model. In the new framework, we make use of non-anaphors to create a special class of training instances, which leads to a classifier capable of identifying the cases of non-anaphors during resolution. In this way, the twin-candidate model itself could avoid the resolution of non-anaphors, and thus could be directly deployed to coreference resolution. The evaluation done on newswire domain shows that the twin-candidate based system with our modified framework achieves better and more reliable performance than those with other solutions."
H05-1114,A Semi-Supervised Feature Clustering Algorithm with Application to Word Sense Disambiguation,2005,32,6,3,1,9439,zhengyu niu,Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing,0,"In this paper we investigate an application of feature clustering for word sense disambiguation, and propose a semisupervised feature clustering algorithm. Compared with other feature clustering methods (ex. supervised feature clustering), it can infer the distribution of class labels over (unseen) features unavailable in training data (labeled data) by the use of the distribution of class labels over (seen) features available in training data. Thus, it can deal with both seen and unseen features in feature clustering process. Our experimental results show that feature clustering can aggressively reduce the dimensionality of feature space, while still maintaining state of the art sense disambiguation accuracy. Furthermore, when combined with a semi-supervised WSD algorithm, semi-supervised feature clustering outperforms other dimensionality reduction techniques, which indicates that using unlabeled data in learning process helps to improve the performance of feature clustering and sense disambiguation."
C04-1033,An {NP}-Cluster Based Approach to Coreference Resolution,2004,17,62,4,1,47859,xiaofeng yang,{COLING} 2004: Proceedings of the 20th International Conference on Computational Linguistics,0,"Traditionally, coreference resolution is done by mining the reference relationships between NP pairs. However, an individual NP usually lacks adequate description information of its referred entity. In this paper, we propose a supervised learning-based approach which does coreference resolution by exploring the relationships between NPs and coreferential clusters. Compared with individual NPs, coreferential clusters could provide richer information of the entities for better rules learning and reference determination. The evaluation done on MEDLINE data set shows that our approach outperforms the baseline NP-NP based approach in both recall and precision."
P03-1023,Coreference Resolution Using Competition Learning Approach,2003,15,130,4,1,47859,xiaofeng yang,Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,1,"In this paper we propose a competition learning approach to coreference resolution. Traditionally, supervised machine learning approaches adopt the single-candidate model. Nevertheless the preference relationship between the antecedent candidates cannot be determined accurately in this model. By contrast, our approach adopts a twin-candidate learning model. Such a model can present the competition criterion for antecedent candidates reliably, and ensure that the most preferred candidate is selected. Furthermore, our approach applies a candidate filter to reduce the computational cost and data noises during training and resolution. The experimental results on MUC-6 and MUC-7 data set show that our approach can outperform those based on the single-candidate model."
O93-2003,Developing a {C}hinese Module in {UNITRAN},1993,0,0,4,0,54504,zhibiao wu,{ROCLING} 1993 Short Papers,0,None
