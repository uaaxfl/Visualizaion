2007.iwslt-1.17,P96-1021,0,0.0581114,"Missing"
2007.iwslt-1.17,N03-1017,0,0.00951,"Missing"
2007.iwslt-1.17,P02-1038,0,0.132631,"Missing"
2007.iwslt-1.17,koen-2004-pharaoh,0,\N,Missing
2007.iwslt-1.17,I05-1007,1,\N,Missing
2007.iwslt-1.17,P06-1077,1,\N,Missing
2007.iwslt-1.17,P06-1066,1,\N,Missing
2007.iwslt-1.17,P00-1056,0,\N,Missing
2007.iwslt-1.17,P03-1021,0,\N,Missing
2008.iwslt-evaluation.7,P06-1077,1,0.887612,"el. This year, we participated in three tracks (two translation directions): 1. BTEC task, Chinese-English direction; 2. Challenge task, Chinese-English direction; 3. Challenge task, English-Chinese direction. This paper is structured as follows. Section 2 describes the four SMT systems and the combiner; Section 3 gives the experimental results, and Section 4 is the conclusion. - 52 - Figure 1: A pair of linked source forest and target string. The solid lines denote hyperedges and the dashed lines denote word alignments. 2. SMT Systems 2.1. Silenus Deriving from the tree-to-string system Lynx [1, 2], Silenus [3, 4] uses packed forests instead of 1-best parse trees in both training and decoding. A packed parse forest is a compact representation of all derivations (i.e., parse trees) for a given sentence under a context-free grammar [5]. A forest can be formally defined as a tuple hV, E, v¯, Ri, where V is a finite set of nodes, E is a finite set of hyperedges, v¯ ∈ V is a distinguished node that denotes the goal item in parsing, R is the set of weights. For a given sentence w1:l = w1 . . . wl , each node v ∈ V is in the form if Xi,j , which denotes the recognition of non-terminal X spanni"
2008.iwslt-evaluation.7,P07-1089,1,0.885881,"el. This year, we participated in three tracks (two translation directions): 1. BTEC task, Chinese-English direction; 2. Challenge task, Chinese-English direction; 3. Challenge task, English-Chinese direction. This paper is structured as follows. Section 2 describes the four SMT systems and the combiner; Section 3 gives the experimental results, and Section 4 is the conclusion. - 52 - Figure 1: A pair of linked source forest and target string. The solid lines denote hyperedges and the dashed lines denote word alignments. 2. SMT Systems 2.1. Silenus Deriving from the tree-to-string system Lynx [1, 2], Silenus [3, 4] uses packed forests instead of 1-best parse trees in both training and decoding. A packed parse forest is a compact representation of all derivations (i.e., parse trees) for a given sentence under a context-free grammar [5]. A forest can be formally defined as a tuple hV, E, v¯, Ri, where V is a finite set of nodes, E is a finite set of hyperedges, v¯ ∈ V is a distinguished node that denotes the goal item in parsing, R is the set of weights. For a given sentence w1:l = w1 . . . wl , each node v ∈ V is in the form if Xi,j , which denotes the recognition of non-terminal X spanni"
2008.iwslt-evaluation.7,P08-1023,1,0.802664,"e participated in three tracks (two translation directions): 1. BTEC task, Chinese-English direction; 2. Challenge task, Chinese-English direction; 3. Challenge task, English-Chinese direction. This paper is structured as follows. Section 2 describes the four SMT systems and the combiner; Section 3 gives the experimental results, and Section 4 is the conclusion. - 52 - Figure 1: A pair of linked source forest and target string. The solid lines denote hyperedges and the dashed lines denote word alignments. 2. SMT Systems 2.1. Silenus Deriving from the tree-to-string system Lynx [1, 2], Silenus [3, 4] uses packed forests instead of 1-best parse trees in both training and decoding. A packed parse forest is a compact representation of all derivations (i.e., parse trees) for a given sentence under a context-free grammar [5]. A forest can be formally defined as a tuple hV, E, v¯, Ri, where V is a finite set of nodes, E is a finite set of hyperedges, v¯ ∈ V is a distinguished node that denotes the goal item in parsing, R is the set of weights. For a given sentence w1:l = w1 . . . wl , each node v ∈ V is in the form if Xi,j , which denotes the recognition of non-terminal X spanning from position"
2008.iwslt-evaluation.7,D08-1022,1,0.818999,"e participated in three tracks (two translation directions): 1. BTEC task, Chinese-English direction; 2. Challenge task, Chinese-English direction; 3. Challenge task, English-Chinese direction. This paper is structured as follows. Section 2 describes the four SMT systems and the combiner; Section 3 gives the experimental results, and Section 4 is the conclusion. - 52 - Figure 1: A pair of linked source forest and target string. The solid lines denote hyperedges and the dashed lines denote word alignments. 2. SMT Systems 2.1. Silenus Deriving from the tree-to-string system Lynx [1, 2], Silenus [3, 4] uses packed forests instead of 1-best parse trees in both training and decoding. A packed parse forest is a compact representation of all derivations (i.e., parse trees) for a given sentence under a context-free grammar [5]. A forest can be formally defined as a tuple hV, E, v¯, Ri, where V is a finite set of nodes, E is a finite set of hyperedges, v¯ ∈ V is a distinguished node that denotes the goal item in parsing, R is the set of weights. For a given sentence w1:l = w1 . . . wl , each node v ∈ V is in the form if Xi,j , which denotes the recognition of non-terminal X spanning from position"
2008.iwslt-evaluation.7,P89-1018,0,0.0164582,"ws. Section 2 describes the four SMT systems and the combiner; Section 3 gives the experimental results, and Section 4 is the conclusion. - 52 - Figure 1: A pair of linked source forest and target string. The solid lines denote hyperedges and the dashed lines denote word alignments. 2. SMT Systems 2.1. Silenus Deriving from the tree-to-string system Lynx [1, 2], Silenus [3, 4] uses packed forests instead of 1-best parse trees in both training and decoding. A packed parse forest is a compact representation of all derivations (i.e., parse trees) for a given sentence under a context-free grammar [5]. A forest can be formally defined as a tuple hV, E, v¯, Ri, where V is a finite set of nodes, E is a finite set of hyperedges, v¯ ∈ V is a distinguished node that denotes the goal item in parsing, R is the set of weights. For a given sentence w1:l = w1 . . . wl , each node v ∈ V is in the form if Xi,j , which denotes the recognition of non-terminal X spanning from position i through j (that is, wi+1 . . . wj ). Each hyperedge e ∈ E is a triple e = hT (e), h(e), f (e)i, where h(e) ∈ V is its head, T (e) ∈ V ∗ is a vector of tail nodes, and f (e) is a weight function from R|T (e) |to R. Figure"
2008.iwslt-evaluation.7,N04-1035,0,0.105473,"the outside probability of its root, the inside probabilities of its leaf nodes, and the probabilities of hyperedges involved in the fragment: Y Y αβ(t) = α(root(t)) × P (e) × β(v) (2) (1) where plm (s) is the language model score, |d |is the number of rules in a derivation, and |s |is the number of target words produced. The derivation probability P r(d|T ) is the product of probabilities of translation rules involved in d: Y P r(d|T ) = P r(r) (5) d∈D r∈d Table 1 gives a derivation for the example forest-string pair. To learn tree-to-string rules from annotated training data, we follow GHKM [6] to first identify minimal rules and then obtain composed rules. Like in tree-based extraction, we extract rules from a packed forest F in two steps: frontier set computation (where to cut) and fragmentation (how to cut). It turns out that the exact formulation developed for frontier set in tree-based case can be applied to a forest without change. The fragmentation step, however, becomes much more complicated since we now face a choice of multiple hyperedges at each node. We develop a breadth-first search algorithm for extracting tree-to-string rules from packed forests. The basic idea is to"
2008.iwslt-evaluation.7,P02-1038,0,0.152821,"ves(t) where α(·) and β(·) are the outside and inside probabilities of nodes, root(·) returns the root of a tree fragment and leaves(·) returns the leaf nodes of a tree fragment. Now, the fractional count of a rule r is simply lines denote word alignments. Each hyperedge is associated with a probability, which we omit in Figure 1 for clarity. In a forest, a node usually has multiple incoming hyperedges. For example, the source node IP0,6 has two incoming hyperedges: c(r) = αβ(lhs(r)) αβ(¯ v) (3) where v¯ denotes the root of the forest. We extend the simple model in Eq. 1 to a log-linear model [7]: dˆ = argmax P r(d|T )λ1 × plm (s)λ2 × eλ3 |d |× eλ4 |s| e1 = h(NP-B0,1 , VP1,6 ), IP0,6 , 0.6i e2 = h(NP0,3 , VP-B3,6 ), IP0,6 , 0.4i (4) d∈D Silenus searches for the best derivation (a sequence of translation rules) dˆ that converts a source tree T in the packed forest into a target-language string s: dˆ = argmax P r(d|T ) should be penalized accordingly and should have fractional counts instead of unit count. We penalize a rule r by the posterior probability of the corresponding tree fragment t = lhs(r), which can be computed as the product of the outside probability of its root, the insid"
2008.iwslt-evaluation.7,J07-2003,0,0.096572,"ach tree has its own probability (that is, product of hyperedge probabilities). As a result, a rule extracted from non 1-best parse - 53 - where each P r(r) can be decomposed into the product of six probabilities: P r(r) = p(r|lhs(r))λ5 × p(r|rhs(r))λ6 ×p(r|root(lhs(r)))λ7 ×plex (lhs(r)|rhs(r))λ8 ×plex (rhs(r)|lhs(r))λ9 ×p(T )λ10 (6) where the first three terms are conditional probabilities based on fractional counts, plex (·) denotes lexical weighting, and p(T ) denotes the probability of the matched source tree T . To search for 1-best derivation, the decoder employs the cube pruning method [8] that approximately intersects the translation forest with language model. Basically, cube pruning works bottom up in a forest, keeping at most k +LM items at each node, and uses the best-first expansion idea from the Algorithm 2 of Liang Huang [9] to speed up the computation. K-best derivations can also be easily obtained by applying Algorithm 3 of Liang Huang [9]. 2.2. Bruin Bruin [10] is a formally syntax-based system that implements a maximum entropy based reordering model on BTG [11] Proceedings of IWSLT 2008, Hawaii - U.S.A. rules. Bruin employs the following three BTG rules to direct tr"
2008.iwslt-evaluation.7,W05-1506,0,0.0292297,"))λ6 ×p(r|root(lhs(r)))λ7 ×plex (lhs(r)|rhs(r))λ8 ×plex (rhs(r)|lhs(r))λ9 ×p(T )λ10 (6) where the first three terms are conditional probabilities based on fractional counts, plex (·) denotes lexical weighting, and p(T ) denotes the probability of the matched source tree T . To search for 1-best derivation, the decoder employs the cube pruning method [8] that approximately intersects the translation forest with language model. Basically, cube pruning works bottom up in a forest, keeping at most k +LM items at each node, and uses the best-first expansion idea from the Algorithm 2 of Liang Huang [9] to speed up the computation. K-best derivations can also be easily obtained by applying Algorithm 3 of Liang Huang [9]. 2.2. Bruin Bruin [10] is a formally syntax-based system that implements a maximum entropy based reordering model on BTG [11] Proceedings of IWSLT 2008, Hawaii - U.S.A. rules. Bruin employs the following three BTG rules to direct translation: [] A → (A1 , A2 ) b3 b4 (7) target hi A → (A1 , A2 ) A → (x, y) (8) (9) b2 b The first two rules are used to merge two neighboring blocks into one larger block either in a monotonic or an inverted order. A block is a pair of source and t"
2008.iwslt-evaluation.7,P06-1066,1,0.895245,"ties based on fractional counts, plex (·) denotes lexical weighting, and p(T ) denotes the probability of the matched source tree T . To search for 1-best derivation, the decoder employs the cube pruning method [8] that approximately intersects the translation forest with language model. Basically, cube pruning works bottom up in a forest, keeping at most k +LM items at each node, and uses the best-first expansion idea from the Algorithm 2 of Liang Huang [9] to speed up the computation. K-best derivations can also be easily obtained by applying Algorithm 3 of Liang Huang [9]. 2.2. Bruin Bruin [10] is a formally syntax-based system that implements a maximum entropy based reordering model on BTG [11] Proceedings of IWSLT 2008, Hawaii - U.S.A. rules. Bruin employs the following three BTG rules to direct translation: [] A → (A1 , A2 ) b3 b4 (7) target hi A → (A1 , A2 ) A → (x, y) (8) (9) b2 b The first two rules are used to merge two neighboring blocks into one larger block either in a monotonic or an inverted order. A block is a pair of source and target contiguous sequences of words. The last rule translates a source phrase x into a target phrase y and generate a block A. In the followin"
2008.iwslt-evaluation.7,J97-3002,0,0.019739,"of the matched source tree T . To search for 1-best derivation, the decoder employs the cube pruning method [8] that approximately intersects the translation forest with language model. Basically, cube pruning works bottom up in a forest, keeping at most k +LM items at each node, and uses the best-first expansion idea from the Algorithm 2 of Liang Huang [9] to speed up the computation. K-best derivations can also be easily obtained by applying Algorithm 3 of Liang Huang [9]. 2.2. Bruin Bruin [10] is a formally syntax-based system that implements a maximum entropy based reordering model on BTG [11] Proceedings of IWSLT 2008, Hawaii - U.S.A. rules. Bruin employs the following three BTG rules to direct translation: [] A → (A1 , A2 ) b3 b4 (7) target hi A → (A1 , A2 ) A → (x, y) (8) (9) b2 b The first two rules are used to merge two neighboring blocks into one larger block either in a monotonic or an inverted order. A block is a pair of source and target contiguous sequences of words. The last rule translates a source phrase x into a target phrase y and generate a block A. In the following, we will define the model by separating different features (including the language model) from the ru"
2008.iwslt-evaluation.7,P08-2041,1,0.861149,"asures how precisely a feature f predicts a class c: IGR(f, c) = E(c) − E(c|f ) Ef (13) where E(·) is an entropy and E(·|·) is a conditional entropy. Surprisingly, the IGR for boundary words (0.2637) is very close to that of blocks (0.2655), suggesting that boundary words do provide sufficient information for predicting reordering. Based on CKY algorithm, the decoder finds the best derivation that produces the input sentence and its translation. To speed up the computation, Bruin also makes use of cube pruning. The lazy Algorithm 3 [9] are used for n-best list generation. 2.3. Mencius Mencius [12] is a phrase-based system that is very similar to Moses [13]. The major difference is that we introduce Proceedings of IWSLT 2008, Hawaii - U.S.A. similarity-based partial matching for bilingual phrases to alleviate data sparseness problem. J Given two source phrases fˆ1J and fˆ0 1 , their matching similarity is given by J SIM(fˆ1J , fˆ0 1 ) = PJ j=1 δ(fj , fj0 ) J 2.5. System Combination (14) where  0 δ(f, f ) = 1 if f = f 0 0 otherwise than . We find that large number of items will often be enumerated under this condition in our experiments. To tackle this problem, we further limit the num"
2008.iwslt-evaluation.7,P07-2045,0,0.00671916,"c) = E(c) − E(c|f ) Ef (13) where E(·) is an entropy and E(·|·) is a conditional entropy. Surprisingly, the IGR for boundary words (0.2637) is very close to that of blocks (0.2655), suggesting that boundary words do provide sufficient information for predicting reordering. Based on CKY algorithm, the decoder finds the best derivation that produces the input sentence and its translation. To speed up the computation, Bruin also makes use of cube pruning. The lazy Algorithm 3 [9] are used for n-best list generation. 2.3. Mencius Mencius [12] is a phrase-based system that is very similar to Moses [13]. The major difference is that we introduce Proceedings of IWSLT 2008, Hawaii - U.S.A. similarity-based partial matching for bilingual phrases to alleviate data sparseness problem. J Given two source phrases fˆ1J and fˆ0 1 , their matching similarity is given by J SIM(fˆ1J , fˆ0 1 ) = PJ j=1 δ(fj , fj0 ) J 2.5. System Combination (14) where  0 δ(f, f ) = 1 if f = f 0 0 otherwise than . We find that large number of items will often be enumerated under this condition in our experiments. To tackle this problem, we further limit the number of items taken from the heap. (15) We combine the output"
2008.iwslt-evaluation.7,D07-1105,0,0.0588403,". similarity-based partial matching for bilingual phrases to alleviate data sparseness problem. J Given two source phrases fˆ1J and fˆ0 1 , their matching similarity is given by J SIM(fˆ1J , fˆ0 1 ) = PJ j=1 δ(fj , fj0 ) J 2.5. System Combination (14) where  0 δ(f, f ) = 1 if f = f 0 0 otherwise than . We find that large number of items will often be enumerated under this condition in our experiments. To tackle this problem, we further limit the number of items taken from the heap. (15) We combine the outputs of single SMT systems at sentence level, similarly to the work by Macherey and Och [14]. Global linear models are used as a framework for reranking a merged n-best list: yˆ = argmax f (x, y) · W (17) y∈GEN(x) Note that we only consider two source phrases that have the same length. To make partially matching more reliable, we further restrict that they share with the same parts-ofspeech sequence. Our hope is that similar bilingual phrases can be used to create translation templates if one source phrase cannot find translations in the phrase table. For example, suppose that we cannot find translations for a source phrase “yu zuotian dida taiguo” in a phrase table, in which we find"
2008.iwslt-evaluation.7,P03-1021,0,0.031464,"se table, in which we find a similar source phrase “yu zuowan dida bulage” with its translation “arrived in Prague last evening”. According to the alignment information, we obtain a translation template: where x is a source sentence, y is a translation, f (x, y) is a feature vector, W is a weight vector, and GEN(x) is the set of possible candidate translations. There types of features are used: (1) relative BLEU scores against 1-best translations from other candidates, (2) language model scores, and (3) length of the translation. The feature weights are tuned using minimum-error-rate training [15]. In this year’s evaluation, each single SMT system generated 200-best list translations, which were merged and served as the input to the combiner. hyu X1 dida X2 , arrived in X2 X1 i 3. Experimental Results Then, the unmatched source substrings “zuotian” and “taiguo” can be translated into “yesterday” and “Thailand”, respectively. As a result, the translation for “yu zuotian dida taiguo” is “arrived in Thailand yesterday”. Given a source sentence, the decoder firstly search for all possible translation options from the phrase table by exact matching. For source phrases which have no translat"
2008.iwslt-evaluation.7,I05-1007,1,0.800287,"ran GIZA++ and used the “growdiagfinal” heuristic to get many-to-many word alignments. We observe that in a sentence some phrases are more likely to appear at the beginning, while other phrases are more likely to be located at the end. Inspired by the literature in language modeling, we mark the beginning and ending of word aligned sentences with two tags, “hsi” and “h/si”, to capture such reordering information. The sentences to be translated will also be annotated with the two tags, which will be removed after decoding. To get packed forests for Silenus, we used the Chinese parser modified [17] by Haitao Mi and the English parser [18] modified by Liang Huang to produce entire parse forests. Then, we ran the Python scripts [19] provided by Liang Huang to output packed forests. To prune the packed forests, Huang [19] uses inside and outside probabilities to compute the distance of the best derivation that traverses a hyperedge away from the globally best derivation. A hyperedge will be pruned if the difference is greater than a threshold. Nodes with all incoming hyperedges pruned are also pruned. 3.3. Results Table 2 presents the BLEU scores (case-sensitive, with punctuations) of our"
2008.iwslt-evaluation.7,P05-1022,0,0.131775,"heuristic to get many-to-many word alignments. We observe that in a sentence some phrases are more likely to appear at the beginning, while other phrases are more likely to be located at the end. Inspired by the literature in language modeling, we mark the beginning and ending of word aligned sentences with two tags, “hsi” and “h/si”, to capture such reordering information. The sentences to be translated will also be annotated with the two tags, which will be removed after decoding. To get packed forests for Silenus, we used the Chinese parser modified [17] by Haitao Mi and the English parser [18] modified by Liang Huang to produce entire parse forests. Then, we ran the Python scripts [19] provided by Liang Huang to output packed forests. To prune the packed forests, Huang [19] uses inside and outside probabilities to compute the distance of the best derivation that traverses a hyperedge away from the globally best derivation. A hyperedge will be pruned if the difference is greater than a threshold. Nodes with all incoming hyperedges pruned are also pruned. 3.3. Results Table 2 presents the BLEU scores (case-sensitive, with punctuations) of our five systems achieved on the IWSLT 2007 C"
2008.iwslt-evaluation.7,P08-1067,0,0.0287062,"more likely to appear at the beginning, while other phrases are more likely to be located at the end. Inspired by the literature in language modeling, we mark the beginning and ending of word aligned sentences with two tags, “hsi” and “h/si”, to capture such reordering information. The sentences to be translated will also be annotated with the two tags, which will be removed after decoding. To get packed forests for Silenus, we used the Chinese parser modified [17] by Haitao Mi and the English parser [18] modified by Liang Huang to produce entire parse forests. Then, we ran the Python scripts [19] provided by Liang Huang to output packed forests. To prune the packed forests, Huang [19] uses inside and outside probabilities to compute the distance of the best derivation that traverses a hyperedge away from the globally best derivation. A hyperedge will be pruned if the difference is greater than a threshold. Nodes with all incoming hyperedges pruned are also pruned. 3.3. Results Table 2 presents the BLEU scores (case-sensitive, with punctuations) of our five systems achieved on the IWSLT 2007 Chinese-English development set. Prior to the evaluation, we used the development sets from 200"
2010.iwslt-evaluation.8,I05-1007,1,\N,Missing
2010.iwslt-evaluation.8,C10-1135,1,\N,Missing
2010.iwslt-evaluation.8,N04-1023,0,\N,Missing
2010.iwslt-evaluation.8,D08-1022,1,\N,Missing
2010.iwslt-evaluation.8,D09-1108,0,\N,Missing
2010.iwslt-evaluation.8,P10-1146,0,\N,Missing
2010.iwslt-evaluation.8,W06-3110,0,\N,Missing
2010.iwslt-evaluation.8,P07-1089,1,\N,Missing
2010.iwslt-evaluation.8,P08-1023,1,\N,Missing
2010.iwslt-evaluation.8,P06-1077,1,\N,Missing
2010.iwslt-evaluation.8,P06-1066,1,\N,Missing
2010.iwslt-evaluation.8,P07-2045,0,\N,Missing
2010.iwslt-evaluation.8,P05-1022,0,\N,Missing
2010.iwslt-evaluation.8,P08-1067,0,\N,Missing
2010.iwslt-evaluation.8,P05-1033,0,\N,Missing
2010.iwslt-evaluation.8,P05-1066,0,\N,Missing
2010.iwslt-evaluation.8,2009.iwslt-evaluation.8,1,\N,Missing
2010.iwslt-evaluation.8,2006.iwslt-papers.4,0,\N,Missing
2011.mtsummit-papers.3,P08-1024,0,0.0179087,"and negative examples respectively, MIRA will make the margin between them as large as possible and λ will no smaller than 2. 2 Related Work Many people have tried to improve MERT in different aspects, such as to improve its stability (Foster and Kuhn, 2009), to improve its performance 49 (Duh and Kirchhoff, 2008), to extent the search space (Macherey et al., 2008; Kumar et al., 2009; Chatterjee and Cancedda, 2010), and to improve the optimization algorithm itself (Lambert and Banchs, 2006; Cer et al., 2008; Gonz´alez-Rubio et al., 2009). Some even replace it completely (Turian et al., 2007; Blunsom et al., 2008). Some people try other objectives during the decoding phase (Kumar and Byrne, 2004; Tromble et al., 2008; Li et al., 2009; DeNero et al., 2009), while others change it in training phase (Zens et al., 2007; He and Way, 2009; He and Way, 2010). There is research that introduces other objectives during tuning (Chiang et al., 2008; Li and Eisner, 2009; Pauls et al., 2009; Hopkins and May, 2011), but these objectives are different from the MRC objective presented in this paper. 3 Maximum Rank Correlation Training 3.1 The Training Paradigm Using the N-best derivation of a decoder to approximate its"
2011.mtsummit-papers.3,W08-0304,0,0.013488,", 2009). In MIRA (Chiang et al., 2008), if we choose candidates whose SBLEU are 0.5 and 0.2 as positive and negative examples respectively, MIRA will make the margin between them as large as possible and λ will no smaller than 2. 2 Related Work Many people have tried to improve MERT in different aspects, such as to improve its stability (Foster and Kuhn, 2009), to improve its performance 49 (Duh and Kirchhoff, 2008), to extent the search space (Macherey et al., 2008; Kumar et al., 2009; Chatterjee and Cancedda, 2010), and to improve the optimization algorithm itself (Lambert and Banchs, 2006; Cer et al., 2008; Gonz´alez-Rubio et al., 2009). Some even replace it completely (Turian et al., 2007; Blunsom et al., 2008). Some people try other objectives during the decoding phase (Kumar and Byrne, 2004; Tromble et al., 2008; Li et al., 2009; DeNero et al., 2009), while others change it in training phase (Zens et al., 2007; He and Way, 2009; He and Way, 2010). There is research that introduces other objectives during tuning (Chiang et al., 2008; Li and Eisner, 2009; Pauls et al., 2009; Hopkins and May, 2011), but these objectives are different from the MRC objective presented in this paper. 3 Maximum Ran"
2011.mtsummit-papers.3,D10-1059,0,0.0118331,"each (c)andicate is given by  p(ci ) = exp (γ · score(c))/ i exp (γ · score(ci )) with γ = 1 (Li and Eisner, 2009). In MIRA (Chiang et al., 2008), if we choose candidates whose SBLEU are 0.5 and 0.2 as positive and negative examples respectively, MIRA will make the margin between them as large as possible and λ will no smaller than 2. 2 Related Work Many people have tried to improve MERT in different aspects, such as to improve its stability (Foster and Kuhn, 2009), to improve its performance 49 (Duh and Kirchhoff, 2008), to extent the search space (Macherey et al., 2008; Kumar et al., 2009; Chatterjee and Cancedda, 2010), and to improve the optimization algorithm itself (Lambert and Banchs, 2006; Cer et al., 2008; Gonz´alez-Rubio et al., 2009). Some even replace it completely (Turian et al., 2007; Blunsom et al., 2008). Some people try other objectives during the decoding phase (Kumar and Byrne, 2004; Tromble et al., 2008; Li et al., 2009; DeNero et al., 2009), while others change it in training phase (Zens et al., 2007; He and Way, 2009; He and Way, 2010). There is research that introduces other objectives during tuning (Chiang et al., 2008; Li and Eisner, 2009; Pauls et al., 2009; Hopkins and May, 2011), bu"
2011.mtsummit-papers.3,D08-1024,0,0.257247,"n such scenarios. The ﬂuctuation in quality can even be statistically perceivable when the number of features is larger than 25 or 30 in practice; on the other hand, Smith (2006) ﬁnds that, MERT relies heavily on the behavior of parameters on the error surface, which is likely to be affected by random variances in the N-best list, and also lead to less generalizable results especially when the development set and the test set are not from exactly the same domain. Both the former and the latter shortcomings have been studied in recent research. e.g. The Margin Infused Relaxed Algorithm (MIRA: (Chiang et al., 2008; Chiang et al., 2009)) is shown to be capable of handling tens of thousands of features in training, while Cer et. al (2008) try to overcome irregularities on the error surface of MERT. In this paper, we focus on improving the generalizability of MERT by introduce a new objective function MRC, which restricts the permutation of the whole N-best list. In the MERT paradigm, the tuning objective is based on the 1-best error surface of the N-best list of an in-domain test set. As MERT actually optimizes parameters for the 1-best for a particular domain, the resulting parameters become domain spec"
2011.mtsummit-papers.3,N09-1025,0,0.01381,"ﬂuctuation in quality can even be statistically perceivable when the number of features is larger than 25 or 30 in practice; on the other hand, Smith (2006) ﬁnds that, MERT relies heavily on the behavior of parameters on the error surface, which is likely to be affected by random variances in the N-best list, and also lead to less generalizable results especially when the development set and the test set are not from exactly the same domain. Both the former and the latter shortcomings have been studied in recent research. e.g. The Margin Infused Relaxed Algorithm (MIRA: (Chiang et al., 2008; Chiang et al., 2009)) is shown to be capable of handling tens of thousands of features in training, while Cer et. al (2008) try to overcome irregularities on the error surface of MERT. In this paper, we focus on improving the generalizability of MERT by introduce a new objective function MRC, which restricts the permutation of the whole N-best list. In the MERT paradigm, the tuning objective is based on the 1-best error surface of the N-best list of an in-domain test set. As MERT actually optimizes parameters for the 1-best for a particular domain, the resulting parameters become domain speciﬁc, and does not gene"
2011.mtsummit-papers.3,P05-1033,0,0.0319721,"te that our method can bring steady improvement for cross-domain translation. Given the fact that purely in-domain data is rarely found in the real world use cases, our method’s ability to generalize to unknown domains is desirable in real world translation tasks. 6 Conclusion 6.1 Conclusions and Future Work The most important characteristic of our method is that it is easily extensible. We therefore plan to experiment with more new optimizing objectives and other optimizing algorithms, to exploit more features in translation, and to extend our method to other formalisms, such as hierachical (Chiang, 2005; Chiang, 2007) or syntax-based (Galley et al., 2006; Liu et al., 2006) translation. 7 Acknowledgement Daqi Zheng, Yang Liu and Qun Liu are supported by National Natural Science Foundation of China Contract 60903138, 90920004, and 60736014. Yifan He is supported by Science Foundation Ireland (Grant No 07/CE/I1142). This work is part funded under FP7 of the EC within the EuroMatrix+ project (Grant No 231720). We thank Jennifer Foster and Joachim Wagner for their valuable suggestions. References In this paper we presented Maximum Ranking Correlation Training (MRCT) for tuning MT systems which wa"
2011.mtsummit-papers.3,J07-2003,0,0.0396422,"thod can bring steady improvement for cross-domain translation. Given the fact that purely in-domain data is rarely found in the real world use cases, our method’s ability to generalize to unknown domains is desirable in real world translation tasks. 6 Conclusion 6.1 Conclusions and Future Work The most important characteristic of our method is that it is easily extensible. We therefore plan to experiment with more new optimizing objectives and other optimizing algorithms, to exploit more features in translation, and to extend our method to other formalisms, such as hierachical (Chiang, 2005; Chiang, 2007) or syntax-based (Galley et al., 2006; Liu et al., 2006) translation. 7 Acknowledgement Daqi Zheng, Yang Liu and Qun Liu are supported by National Natural Science Foundation of China Contract 60903138, 90920004, and 60736014. Yifan He is supported by Science Foundation Ireland (Grant No 07/CE/I1142). This work is part funded under FP7 of the EC within the EuroMatrix+ project (Grant No 231720). We thank Jennifer Foster and Joachim Wagner for their valuable suggestions. References In this paper we presented Maximum Ranking Correlation Training (MRCT) for tuning MT systems which was different fro"
2011.mtsummit-papers.3,P09-1064,0,0.0119997,"y people have tried to improve MERT in different aspects, such as to improve its stability (Foster and Kuhn, 2009), to improve its performance 49 (Duh and Kirchhoff, 2008), to extent the search space (Macherey et al., 2008; Kumar et al., 2009; Chatterjee and Cancedda, 2010), and to improve the optimization algorithm itself (Lambert and Banchs, 2006; Cer et al., 2008; Gonz´alez-Rubio et al., 2009). Some even replace it completely (Turian et al., 2007; Blunsom et al., 2008). Some people try other objectives during the decoding phase (Kumar and Byrne, 2004; Tromble et al., 2008; Li et al., 2009; DeNero et al., 2009), while others change it in training phase (Zens et al., 2007; He and Way, 2009; He and Way, 2010). There is research that introduces other objectives during tuning (Chiang et al., 2008; Li and Eisner, 2009; Pauls et al., 2009; Hopkins and May, 2011), but these objectives are different from the MRC objective presented in this paper. 3 Maximum Rank Correlation Training 3.1 The Training Paradigm Using the N-best derivation of a decoder to approximate its search space, we ﬁnd the optimal set of paˆ (Fig 1), which maximizes the weighted rameters λ sum of correlation on a set with M sentences, as i"
2011.mtsummit-papers.3,P08-2010,0,0.02105,"= 1.37 via assuming the objective of Min-Risk is the expectation of BLEU, and the probability of each (c)andicate is given by  p(ci ) = exp (γ · score(c))/ i exp (γ · score(ci )) with γ = 1 (Li and Eisner, 2009). In MIRA (Chiang et al., 2008), if we choose candidates whose SBLEU are 0.5 and 0.2 as positive and negative examples respectively, MIRA will make the margin between them as large as possible and λ will no smaller than 2. 2 Related Work Many people have tried to improve MERT in different aspects, such as to improve its stability (Foster and Kuhn, 2009), to improve its performance 49 (Duh and Kirchhoff, 2008), to extent the search space (Macherey et al., 2008; Kumar et al., 2009; Chatterjee and Cancedda, 2010), and to improve the optimization algorithm itself (Lambert and Banchs, 2006; Cer et al., 2008; Gonz´alez-Rubio et al., 2009). Some even replace it completely (Turian et al., 2007; Blunsom et al., 2008). Some people try other objectives during the decoding phase (Kumar and Byrne, 2004; Tromble et al., 2008; Li et al., 2009; DeNero et al., 2009), while others change it in training phase (Zens et al., 2007; He and Way, 2009; He and Way, 2010). There is research that introduces other objectives"
2011.mtsummit-papers.3,W09-0439,0,0.0115063,"core is perfectly the same as their SBLEU. We obtain λ = 1.37 via assuming the objective of Min-Risk is the expectation of BLEU, and the probability of each (c)andicate is given by  p(ci ) = exp (γ · score(c))/ i exp (γ · score(ci )) with γ = 1 (Li and Eisner, 2009). In MIRA (Chiang et al., 2008), if we choose candidates whose SBLEU are 0.5 and 0.2 as positive and negative examples respectively, MIRA will make the margin between them as large as possible and λ will no smaller than 2. 2 Related Work Many people have tried to improve MERT in different aspects, such as to improve its stability (Foster and Kuhn, 2009), to improve its performance 49 (Duh and Kirchhoff, 2008), to extent the search space (Macherey et al., 2008; Kumar et al., 2009; Chatterjee and Cancedda, 2010), and to improve the optimization algorithm itself (Lambert and Banchs, 2006; Cer et al., 2008; Gonz´alez-Rubio et al., 2009). Some even replace it completely (Turian et al., 2007; Blunsom et al., 2008). Some people try other objectives during the decoding phase (Kumar and Byrne, 2004; Tromble et al., 2008; Li et al., 2009; DeNero et al., 2009), while others change it in training phase (Zens et al., 2007; He and Way, 2009; He and Way, 2"
2011.mtsummit-papers.3,P06-1121,0,0.041395,"nt for cross-domain translation. Given the fact that purely in-domain data is rarely found in the real world use cases, our method’s ability to generalize to unknown domains is desirable in real world translation tasks. 6 Conclusion 6.1 Conclusions and Future Work The most important characteristic of our method is that it is easily extensible. We therefore plan to experiment with more new optimizing objectives and other optimizing algorithms, to exploit more features in translation, and to extend our method to other formalisms, such as hierachical (Chiang, 2005; Chiang, 2007) or syntax-based (Galley et al., 2006; Liu et al., 2006) translation. 7 Acknowledgement Daqi Zheng, Yang Liu and Qun Liu are supported by National Natural Science Foundation of China Contract 60903138, 90920004, and 60736014. Yifan He is supported by Science Foundation Ireland (Grant No 07/CE/I1142). This work is part funded under FP7 of the EC within the EuroMatrix+ project (Grant No 231720). We thank Jennifer Foster and Joachim Wagner for their valuable suggestions. References In this paper we presented Maximum Ranking Correlation Training (MRCT) for tuning MT systems which was different from the MER. We optimized to maximize t"
2011.mtsummit-papers.3,2009.mtsummit-posters.8,1,0.85082,"ability (Foster and Kuhn, 2009), to improve its performance 49 (Duh and Kirchhoff, 2008), to extent the search space (Macherey et al., 2008; Kumar et al., 2009; Chatterjee and Cancedda, 2010), and to improve the optimization algorithm itself (Lambert and Banchs, 2006; Cer et al., 2008; Gonz´alez-Rubio et al., 2009). Some even replace it completely (Turian et al., 2007; Blunsom et al., 2008). Some people try other objectives during the decoding phase (Kumar and Byrne, 2004; Tromble et al., 2008; Li et al., 2009; DeNero et al., 2009), while others change it in training phase (Zens et al., 2007; He and Way, 2009; He and Way, 2010). There is research that introduces other objectives during tuning (Chiang et al., 2008; Li and Eisner, 2009; Pauls et al., 2009; Hopkins and May, 2011), but these objectives are different from the MRC objective presented in this paper. 3 Maximum Rank Correlation Training 3.1 The Training Paradigm Using the N-best derivation of a decoder to approximate its search space, we ﬁnd the optimal set of paˆ (Fig 1), which maximizes the weighted rameters λ sum of correlation on a set with M sentences, as in Eq. (1). ˆ = arg max( λ λ M  wi · Corri (λ)) (1) i=1 where wi is the weight"
2011.mtsummit-papers.3,D11-1125,0,0.0152855,"terjee and Cancedda, 2010), and to improve the optimization algorithm itself (Lambert and Banchs, 2006; Cer et al., 2008; Gonz´alez-Rubio et al., 2009). Some even replace it completely (Turian et al., 2007; Blunsom et al., 2008). Some people try other objectives during the decoding phase (Kumar and Byrne, 2004; Tromble et al., 2008; Li et al., 2009; DeNero et al., 2009), while others change it in training phase (Zens et al., 2007; He and Way, 2009; He and Way, 2010). There is research that introduces other objectives during tuning (Chiang et al., 2008; Li and Eisner, 2009; Pauls et al., 2009; Hopkins and May, 2011), but these objectives are different from the MRC objective presented in this paper. 3 Maximum Rank Correlation Training 3.1 The Training Paradigm Using the N-best derivation of a decoder to approximate its search space, we ﬁnd the optimal set of paˆ (Fig 1), which maximizes the weighted rameters λ sum of correlation on a set with M sentences, as in Eq. (1). ˆ = arg max( λ λ M  wi · Corri (λ)) (1) i=1 where wi is the weight of the i–th sentence, and Corri (λ) is the correlation between the model scores and the translation quality of the translation candidates approximated by SBLEU, as in Eq."
2011.mtsummit-papers.3,N03-1017,0,0.0047482,"hared translation task. The training data is the Europarl v3b release (Koehn, 2005). The language model corpus is the English part of monolingual language model training data provided by the organizers of WMT From the data sets provided by WMT, we use dev2006 as the tuning set for λ, test2005 as tuning set for α, and test2006, test2007 and test2008 as in-domain test sets. We use newstest2008, newstest2009 and We use Moses 2 as the baseline decoder, perform word alignment using the GIZA++ (Och and Ney, 2003) implementation of IBM Model 4, and extract phrases using the grow-diag-ﬁnal heuristic (Koehn et al., 2003). We train a 4-gram language model using the SRILM language modeling toolkit (Stolcke, 2002). We use the MERT implemented by Bertoldi et al., (2009), which is included in the Moses package. BLEU is calculated by the mteval script provided by NIST 3 . Statistical signiﬁcance of test results is computed by Koehn’s boosting tool (Koehn, 2004). We preprocess the data using the toolkits provided by WMT08 organizers, train and tune Moses following the WMT baseline description, with 100-best list, using up to 15 tuning iterations, and ﬁnally arriving at a model with 14 default features. 4.2 The Basel"
2011.mtsummit-papers.3,W04-3250,0,0.0707191,"08 as in-domain test sets. We use newstest2008, newstest2009 and We use Moses 2 as the baseline decoder, perform word alignment using the GIZA++ (Och and Ney, 2003) implementation of IBM Model 4, and extract phrases using the grow-diag-ﬁnal heuristic (Koehn et al., 2003). We train a 4-gram language model using the SRILM language modeling toolkit (Stolcke, 2002). We use the MERT implemented by Bertoldi et al., (2009), which is included in the Moses package. BLEU is calculated by the mteval script provided by NIST 3 . Statistical signiﬁcance of test results is computed by Koehn’s boosting tool (Koehn, 2004). We preprocess the data using the toolkits provided by WMT08 organizers, train and tune Moses following the WMT baseline description, with 100-best list, using up to 15 tuning iterations, and ﬁnally arriving at a model with 14 default features. 4.2 The Baseline System We tune 3 times on the tuning set and pick the experiment whose parameters achieve the highest BLEU score on the test05 as a baseline. We then use this parameter to decode all other sets, and obtain results 2 checkout from svn with version 3625 ftp://jaguar.ncsl.nist.gov/mt/ resources/mteval-v11b.pl 3 1 http://www.statmt.org/wmt"
2011.mtsummit-papers.3,2005.mtsummit-papers.11,0,0.0501157,"t-of-Domain test set, MRC not only performs better than MER, but also beat the interpolation’s perfomance. Notice: the directly decoding method outperform the reranking one. 32.75 reranking decoding baseline 32.7 newstest2010 as out-of-domain test sets. BLEU 32.65 4.1.2 Program 32.6 32.55 32.5 32.45 0 0.2 0.4 α 0.6 0.8 1 Figure 4: Test06: Another in-domain test set to demonstrate the advantage of interpolation of objectives 4.1 Experimental setting 4.1.1 Data We use the French–English parallel data provided by the WMT08 1 shared translation task. The training data is the Europarl v3b release (Koehn, 2005). The language model corpus is the English part of monolingual language model training data provided by the organizers of WMT From the data sets provided by WMT, we use dev2006 as the tuning set for λ, test2005 as tuning set for α, and test2006, test2007 and test2008 as in-domain test sets. We use newstest2008, newstest2009 and We use Moses 2 as the baseline decoder, perform word alignment using the GIZA++ (Och and Ney, 2003) implementation of IBM Model 4, and extract phrases using the grow-diag-ﬁnal heuristic (Koehn et al., 2003). We train a 4-gram language model using the SRILM language mode"
2011.mtsummit-papers.3,N04-1022,0,0.0380525,"e as possible and λ will no smaller than 2. 2 Related Work Many people have tried to improve MERT in different aspects, such as to improve its stability (Foster and Kuhn, 2009), to improve its performance 49 (Duh and Kirchhoff, 2008), to extent the search space (Macherey et al., 2008; Kumar et al., 2009; Chatterjee and Cancedda, 2010), and to improve the optimization algorithm itself (Lambert and Banchs, 2006; Cer et al., 2008; Gonz´alez-Rubio et al., 2009). Some even replace it completely (Turian et al., 2007; Blunsom et al., 2008). Some people try other objectives during the decoding phase (Kumar and Byrne, 2004; Tromble et al., 2008; Li et al., 2009; DeNero et al., 2009), while others change it in training phase (Zens et al., 2007; He and Way, 2009; He and Way, 2010). There is research that introduces other objectives during tuning (Chiang et al., 2008; Li and Eisner, 2009; Pauls et al., 2009; Hopkins and May, 2011), but these objectives are different from the MRC objective presented in this paper. 3 Maximum Rank Correlation Training 3.1 The Training Paradigm Using the N-best derivation of a decoder to approximate its search space, we ﬁnd the optimal set of paˆ (Fig 1), which maximizes the weighted"
2011.mtsummit-papers.3,P09-1019,0,0.012328,"the probability of each (c)andicate is given by  p(ci ) = exp (γ · score(c))/ i exp (γ · score(ci )) with γ = 1 (Li and Eisner, 2009). In MIRA (Chiang et al., 2008), if we choose candidates whose SBLEU are 0.5 and 0.2 as positive and negative examples respectively, MIRA will make the margin between them as large as possible and λ will no smaller than 2. 2 Related Work Many people have tried to improve MERT in different aspects, such as to improve its stability (Foster and Kuhn, 2009), to improve its performance 49 (Duh and Kirchhoff, 2008), to extent the search space (Macherey et al., 2008; Kumar et al., 2009; Chatterjee and Cancedda, 2010), and to improve the optimization algorithm itself (Lambert and Banchs, 2006; Cer et al., 2008; Gonz´alez-Rubio et al., 2009). Some even replace it completely (Turian et al., 2007; Blunsom et al., 2008). Some people try other objectives during the decoding phase (Kumar and Byrne, 2004; Tromble et al., 2008; Li et al., 2009; DeNero et al., 2009), while others change it in training phase (Zens et al., 2007; He and Way, 2009; He and Way, 2010). There is research that introduces other objectives during tuning (Chiang et al., 2008; Li and Eisner, 2009; Pauls et al.,"
2011.mtsummit-papers.3,2006.iwslt-papers.5,0,0.0181767,"with γ = 1 (Li and Eisner, 2009). In MIRA (Chiang et al., 2008), if we choose candidates whose SBLEU are 0.5 and 0.2 as positive and negative examples respectively, MIRA will make the margin between them as large as possible and λ will no smaller than 2. 2 Related Work Many people have tried to improve MERT in different aspects, such as to improve its stability (Foster and Kuhn, 2009), to improve its performance 49 (Duh and Kirchhoff, 2008), to extent the search space (Macherey et al., 2008; Kumar et al., 2009; Chatterjee and Cancedda, 2010), and to improve the optimization algorithm itself (Lambert and Banchs, 2006; Cer et al., 2008; Gonz´alez-Rubio et al., 2009). Some even replace it completely (Turian et al., 2007; Blunsom et al., 2008). Some people try other objectives during the decoding phase (Kumar and Byrne, 2004; Tromble et al., 2008; Li et al., 2009; DeNero et al., 2009), while others change it in training phase (Zens et al., 2007; He and Way, 2009; He and Way, 2010). There is research that introduces other objectives during tuning (Chiang et al., 2008; Li and Eisner, 2009; Pauls et al., 2009; Hopkins and May, 2011), but these objectives are different from the MRC objective presented in this pa"
2011.mtsummit-papers.3,D09-1005,0,0.0130764,"I (NSGA-II) (Deb et al., 2002), a genetic algorithm to perform multi-objective optimization. For example, in Figure 1, MERT chooses the middle point of two cross points. By contrast, MRC tries to maximize the rank correlation between SBLEU and the model score. and will adjust λ into the open interval (1.5, 2), in which the order of candidates’ model score is perfectly the same as their SBLEU. We obtain λ = 1.37 via assuming the objective of Min-Risk is the expectation of BLEU, and the probability of each (c)andicate is given by  p(ci ) = exp (γ · score(c))/ i exp (γ · score(ci )) with γ = 1 (Li and Eisner, 2009). In MIRA (Chiang et al., 2008), if we choose candidates whose SBLEU are 0.5 and 0.2 as positive and negative examples respectively, MIRA will make the margin between them as large as possible and λ will no smaller than 2. 2 Related Work Many people have tried to improve MERT in different aspects, such as to improve its stability (Foster and Kuhn, 2009), to improve its performance 49 (Duh and Kirchhoff, 2008), to extent the search space (Macherey et al., 2008; Kumar et al., 2009; Chatterjee and Cancedda, 2010), and to improve the optimization algorithm itself (Lambert and Banchs, 2006; Cer et"
2011.mtsummit-papers.3,P09-1067,0,0.0130105,"Related Work Many people have tried to improve MERT in different aspects, such as to improve its stability (Foster and Kuhn, 2009), to improve its performance 49 (Duh and Kirchhoff, 2008), to extent the search space (Macherey et al., 2008; Kumar et al., 2009; Chatterjee and Cancedda, 2010), and to improve the optimization algorithm itself (Lambert and Banchs, 2006; Cer et al., 2008; Gonz´alez-Rubio et al., 2009). Some even replace it completely (Turian et al., 2007; Blunsom et al., 2008). Some people try other objectives during the decoding phase (Kumar and Byrne, 2004; Tromble et al., 2008; Li et al., 2009; DeNero et al., 2009), while others change it in training phase (Zens et al., 2007; He and Way, 2009; He and Way, 2010). There is research that introduces other objectives during tuning (Chiang et al., 2008; Li and Eisner, 2009; Pauls et al., 2009; Hopkins and May, 2011), but these objectives are different from the MRC objective presented in this paper. 3 Maximum Rank Correlation Training 3.1 The Training Paradigm Using the N-best derivation of a decoder to approximate its search space, we ﬁnd the optimal set of paˆ (Fig 1), which maximizes the weighted rameters λ sum of correlation on a set"
2011.mtsummit-papers.3,P06-1077,1,0.815113,"ranslation. Given the fact that purely in-domain data is rarely found in the real world use cases, our method’s ability to generalize to unknown domains is desirable in real world translation tasks. 6 Conclusion 6.1 Conclusions and Future Work The most important characteristic of our method is that it is easily extensible. We therefore plan to experiment with more new optimizing objectives and other optimizing algorithms, to exploit more features in translation, and to extend our method to other formalisms, such as hierachical (Chiang, 2005; Chiang, 2007) or syntax-based (Galley et al., 2006; Liu et al., 2006) translation. 7 Acknowledgement Daqi Zheng, Yang Liu and Qun Liu are supported by National Natural Science Foundation of China Contract 60903138, 90920004, and 60736014. Yifan He is supported by Science Foundation Ireland (Grant No 07/CE/I1142). This work is part funded under FP7 of the EC within the EuroMatrix+ project (Grant No 231720). We thank Jennifer Foster and Joachim Wagner for their valuable suggestions. References In this paper we presented Maximum Ranking Correlation Training (MRCT) for tuning MT systems which was different from the MER. We optimized to maximize the correlation betw"
2011.mtsummit-papers.3,D08-1076,0,0.0299773,"xpectation of BLEU, and the probability of each (c)andicate is given by  p(ci ) = exp (γ · score(c))/ i exp (γ · score(ci )) with γ = 1 (Li and Eisner, 2009). In MIRA (Chiang et al., 2008), if we choose candidates whose SBLEU are 0.5 and 0.2 as positive and negative examples respectively, MIRA will make the margin between them as large as possible and λ will no smaller than 2. 2 Related Work Many people have tried to improve MERT in different aspects, such as to improve its stability (Foster and Kuhn, 2009), to improve its performance 49 (Duh and Kirchhoff, 2008), to extent the search space (Macherey et al., 2008; Kumar et al., 2009; Chatterjee and Cancedda, 2010), and to improve the optimization algorithm itself (Lambert and Banchs, 2006; Cer et al., 2008; Gonz´alez-Rubio et al., 2009). Some even replace it completely (Turian et al., 2007; Blunsom et al., 2008). Some people try other objectives during the decoding phase (Kumar and Byrne, 2004; Tromble et al., 2008; Li et al., 2009; DeNero et al., 2009), while others change it in training phase (Zens et al., 2007; He and Way, 2009; He and Way, 2010). There is research that introduces other objectives during tuning (Chiang et al., 2008; Li and Eisner,"
2011.mtsummit-papers.3,P02-1038,0,0.0660037,"d signiﬁcantly outperforms MERT on out-of-domain data sets, and performs marginally better than MERT on in-domain data sets, which validates the usefulness of MRC on both domain speciﬁc and general domain data. Min−Risk MIRA <− MRC−> Model Score 3 2.5 0.5 2 1.5 1 −0.5 0.3 SBLEU=0.3 y=x SBLEU=0.2 y=−x+3 SBLEU=0.5 y=−0.5*x+3 0 0.5 0.2 1 λ 1.5 2 2.5 Figure 1: Made-up Example: The 3 sloping lines represent all 3 candidates in N-best list. Their SBLEU (BLEU = SBLEU when only one sentence) and funtions are in the legend of ﬁgure. 1 Introduction Searching for the optimal parameters in linear models (Och and Ney, 2002) of Statistical Machine Translation (SMT) has been a major challenge to the MT community. The most widely used approach todate is Minimum-Error-Rate Training (MERT:(Och, 2003)), which tries to ﬁnd the parameters that optimize the translation quality of the 1-best translation candidate, using the N-best list as an approximation of the decoder’s search space. In spite of its usefulness and high adoption, MERT suffers from shortcomings that the MT community is becoming aware of. On the one hand, 48 MERT is not designed for models with rich features and therefore leads to translations of unstable"
2011.mtsummit-papers.3,J03-1002,0,0.00260608,"ives 4.1 Experimental setting 4.1.1 Data We use the French–English parallel data provided by the WMT08 1 shared translation task. The training data is the Europarl v3b release (Koehn, 2005). The language model corpus is the English part of monolingual language model training data provided by the organizers of WMT From the data sets provided by WMT, we use dev2006 as the tuning set for λ, test2005 as tuning set for α, and test2006, test2007 and test2008 as in-domain test sets. We use newstest2008, newstest2009 and We use Moses 2 as the baseline decoder, perform word alignment using the GIZA++ (Och and Ney, 2003) implementation of IBM Model 4, and extract phrases using the grow-diag-ﬁnal heuristic (Koehn et al., 2003). We train a 4-gram language model using the SRILM language modeling toolkit (Stolcke, 2002). We use the MERT implemented by Bertoldi et al., (2009), which is included in the Moses package. BLEU is calculated by the mteval script provided by NIST 3 . Statistical signiﬁcance of test results is computed by Koehn’s boosting tool (Koehn, 2004). We preprocess the data using the toolkits provided by WMT08 organizers, train and tune Moses following the WMT baseline description, with 100-best lis"
2011.mtsummit-papers.3,P03-1021,0,0.0481218,"ﬁc and general domain data. Min−Risk MIRA <− MRC−> Model Score 3 2.5 0.5 2 1.5 1 −0.5 0.3 SBLEU=0.3 y=x SBLEU=0.2 y=−x+3 SBLEU=0.5 y=−0.5*x+3 0 0.5 0.2 1 λ 1.5 2 2.5 Figure 1: Made-up Example: The 3 sloping lines represent all 3 candidates in N-best list. Their SBLEU (BLEU = SBLEU when only one sentence) and funtions are in the legend of ﬁgure. 1 Introduction Searching for the optimal parameters in linear models (Och and Ney, 2002) of Statistical Machine Translation (SMT) has been a major challenge to the MT community. The most widely used approach todate is Minimum-Error-Rate Training (MERT:(Och, 2003)), which tries to ﬁnd the parameters that optimize the translation quality of the 1-best translation candidate, using the N-best list as an approximation of the decoder’s search space. In spite of its usefulness and high adoption, MERT suffers from shortcomings that the MT community is becoming aware of. On the one hand, 48 MERT is not designed for models with rich features and therefore leads to translations of unstable quality in such scenarios. The ﬂuctuation in quality can even be statistically perceivable when the number of features is larger than 25 or 30 in practice; on the other hand,"
2011.mtsummit-papers.3,P02-1040,0,0.0947049,"optimal set of paˆ (Fig 1), which maximizes the weighted rameters λ sum of correlation on a set with M sentences, as in Eq. (1). ˆ = arg max( λ λ M  wi · Corri (λ)) (1) i=1 where wi is the weight of the i–th sentence, and Corri (λ) is the correlation between the model scores and the translation quality of the translation candidates approximated by SBLEU, as in Eq. (2). N Corri (λ) = Corr(ΦN 1 (λ), SBLEU (e1 ))) (2) where e is the i–th sentence, eN 1 is the N-best deriva(λ) are the model scores tion of the decoder and ΦN 1 using parameters λ. We calculate SBLEU by for eN 1 applying the BLEU (Papineni et al., 2002) formulation directly to single sentence. There exist many coefﬁcients to measure the correlation between model scores and SBLEU scores. In our implementation, we use the Spearman’s ρ ranking correlation, as in Eq. (3)  (xi − x ¯)(yi − y¯) (3) ρ =  i  ¯)2 i (yi − y¯)2 i (xi − x 3.2 Combination of MRC and MER Training Inspired by Chiang et al. (2008), we also explore the possibility of combining the rank correlation with evaluation metric score as an alternative to the MER and the MRC objectives. Given a set of features λ, we perform a straightforward linear combination as in Eq. (4) ˆ = ar"
2011.mtsummit-papers.3,D09-1147,0,0.0187952,"r et al., 2009; Chatterjee and Cancedda, 2010), and to improve the optimization algorithm itself (Lambert and Banchs, 2006; Cer et al., 2008; Gonz´alez-Rubio et al., 2009). Some even replace it completely (Turian et al., 2007; Blunsom et al., 2008). Some people try other objectives during the decoding phase (Kumar and Byrne, 2004; Tromble et al., 2008; Li et al., 2009; DeNero et al., 2009), while others change it in training phase (Zens et al., 2007; He and Way, 2009; He and Way, 2010). There is research that introduces other objectives during tuning (Chiang et al., 2008; Li and Eisner, 2009; Pauls et al., 2009; Hopkins and May, 2011), but these objectives are different from the MRC objective presented in this paper. 3 Maximum Rank Correlation Training 3.1 The Training Paradigm Using the N-best derivation of a decoder to approximate its search space, we ﬁnd the optimal set of paˆ (Fig 1), which maximizes the weighted rameters λ sum of correlation on a set with M sentences, as in Eq. (1). ˆ = arg max( λ λ M  wi · Corri (λ)) (1) i=1 where wi is the weight of the i–th sentence, and Corri (λ) is the correlation between the model scores and the translation quality of the translation candidates approxima"
2011.mtsummit-papers.3,P06-2101,0,0.0506745,"Missing"
2011.mtsummit-papers.3,D08-1065,0,0.0153584,"l no smaller than 2. 2 Related Work Many people have tried to improve MERT in different aspects, such as to improve its stability (Foster and Kuhn, 2009), to improve its performance 49 (Duh and Kirchhoff, 2008), to extent the search space (Macherey et al., 2008; Kumar et al., 2009; Chatterjee and Cancedda, 2010), and to improve the optimization algorithm itself (Lambert and Banchs, 2006; Cer et al., 2008; Gonz´alez-Rubio et al., 2009). Some even replace it completely (Turian et al., 2007; Blunsom et al., 2008). Some people try other objectives during the decoding phase (Kumar and Byrne, 2004; Tromble et al., 2008; Li et al., 2009; DeNero et al., 2009), while others change it in training phase (Zens et al., 2007; He and Way, 2009; He and Way, 2010). There is research that introduces other objectives during tuning (Chiang et al., 2008; Li and Eisner, 2009; Pauls et al., 2009; Hopkins and May, 2011), but these objectives are different from the MRC objective presented in this paper. 3 Maximum Rank Correlation Training 3.1 The Training Paradigm Using the N-best derivation of a decoder to approximate its search space, we ﬁnd the optimal set of paˆ (Fig 1), which maximizes the weighted rameters λ sum of corr"
2011.mtsummit-papers.3,D07-1055,0,0.0177524,"s to improve its stability (Foster and Kuhn, 2009), to improve its performance 49 (Duh and Kirchhoff, 2008), to extent the search space (Macherey et al., 2008; Kumar et al., 2009; Chatterjee and Cancedda, 2010), and to improve the optimization algorithm itself (Lambert and Banchs, 2006; Cer et al., 2008; Gonz´alez-Rubio et al., 2009). Some even replace it completely (Turian et al., 2007; Blunsom et al., 2008). Some people try other objectives during the decoding phase (Kumar and Byrne, 2004; Tromble et al., 2008; Li et al., 2009; DeNero et al., 2009), while others change it in training phase (Zens et al., 2007; He and Way, 2009; He and Way, 2010). There is research that introduces other objectives during tuning (Chiang et al., 2008; Li and Eisner, 2009; Pauls et al., 2009; Hopkins and May, 2011), but these objectives are different from the MRC objective presented in this paper. 3 Maximum Rank Correlation Training 3.1 The Training Paradigm Using the N-best derivation of a decoder to approximate its search space, we ﬁnd the optimal set of paˆ (Fig 1), which maximizes the weighted rameters λ sum of correlation on a set with M sentences, as in Eq. (1). ˆ = arg max( λ λ M  wi · Corri (λ)) (1) i=1 where"
2020.acl-main.278,N13-1073,0,0.179842,"Missing"
2020.acl-main.278,P16-1162,0,0.225361,"tion of translation errors. While TER only labels the mis-translation (“S”) and over-translation (“I”) errors, we describe a simple heuristic method to annotate the undertranslation error by mapping the label “D” from the ground-truth sequence to the generated sequence. 4 Miscalibration in NMT Data and Setup We carried out experiments on three different language pairs, including WAT17 English-Japanese (En-Jp), WMT14 EnglishGerman (En-De), and WMT17 Chinese-English (Zh-En). The training datasets consist of 1.9M, 4.5M, and 20.6M sentence pairs respectively. We employed Byte pair encoding (BPE) (Sennrich et al., 2016) with 32K merge operations for all the three language pairs. We used BLEU (Papineni et al., 2001) to evaluate the NMT models. We used the TER toolkit (Snover et al., 2006) to label whether the tokens in NMT outputs are correctly translated. Normalization was not used, and the maximum shift distance was set to 50. The NMT model that we used in our experiments is Transformer (Vaswani et al., 2017). We used base model as default, which consists of a 6-layer encoder and a 6-layer decoder and the hidden size is 512. The model parameters are optimized by Adam (Kingma and Ba, 2015), with β1 = 0.9, β2"
2020.acl-main.278,2006.amta-papers.25,0,0.502853,"019) and Kumar and Sarawagi (2019) studied the calibration of NMT in the training setting, and found that NMT trained with label smoothing (Szegedy et al., 2016) is well-calibrated. We believe that this setting would cover up a central problem of NMT, the exposure bias (Ranzato et al., 2015) – the training-inference discrepancy caused by teacher forcing in the training of auto-regressive models. In response to this problem, this work focuses on the calibration of NMT in inference, which can better reflect the generative capacity of NMT models. To this end, we use translation error rate (TER) (Snover et al., 2006) to automatically annotate the correctness of generated tokens, which makes it feasible to evaluate calibration in infer3070 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3070–3079 c July 5 - 10, 2020. 2020 Association for Computational Linguistics ence. Experimental results on several datasets across language pairs show that even trained with label smoothing, NMT models still suffer from miscalibration errors in inference. Figure 1 shows an example. While modern neural networks on classification tasks have been found to be miscalibrated in the"
2020.acl-main.278,D15-1182,0,0.06029,"Missing"
2020.acl-main.278,2001.mtsummit-papers.68,0,0.0152736,"(“I”) errors, we describe a simple heuristic method to annotate the undertranslation error by mapping the label “D” from the ground-truth sequence to the generated sequence. 4 Miscalibration in NMT Data and Setup We carried out experiments on three different language pairs, including WAT17 English-Japanese (En-Jp), WMT14 EnglishGerman (En-De), and WMT17 Chinese-English (Zh-En). The training datasets consist of 1.9M, 4.5M, and 20.6M sentence pairs respectively. We employed Byte pair encoding (BPE) (Sennrich et al., 2016) with 32K merge operations for all the three language pairs. We used BLEU (Papineni et al., 2001) to evaluate the NMT models. We used the TER toolkit (Snover et al., 2006) to label whether the tokens in NMT outputs are correctly translated. Normalization was not used, and the maximum shift distance was set to 50. The NMT model that we used in our experiments is Transformer (Vaswani et al., 2017). We used base model as default, which consists of a 6-layer encoder and a 6-layer decoder and the hidden size is 512. The model parameters are optimized by Adam (Kingma and Ba, 2015), with β1 = 0.9, β2 = 0.98 and  = 10−9 . We used the same warmup strategy for learning rate as Vaswani et al. (2017"
2020.acl-main.278,P19-1176,0,0.132887,"bration. Szegedy et al. (2016) propose the label smoothing technique which can effectively reduce the calibration error. Ding et al. (2019) extend label smoothing to adaptive label regularization. Calibration on Structured Prediction Different from classification tasks, most natural language processing (NLP) tasks deal with complex structures (Kuleshov and Liang, 2015). Nguyen and O’Connor (2015) verified the finding of NiculescuMizil and Caruana (2005) in NLP tasks on loglinear structured models. For NMT, some works directed their attention to the uncertainty in prediction (Ott et al., 2018; Wang et al., 2019), Kumar and Sarawagi (2019) studied the calibration of several NMT models and found that the end of a sentence is severely miscalibrated. M¨uller et al. (2019) investigated the effect of label smoothing, finding that NMT models are well-calibrated in training. Different from previous works, we are interested in the calibration of NMT models in inference, given that the training and inference are discrepant for standard NMT models (Vaswani et al., 2017). 3 3.1 Definitions of Calibration Neural Machine Translation Training In machine translation task, an NMT model F : x → y maximizes the probabi"
2020.acl-main.278,D19-1073,1,0.935975,"bration. Szegedy et al. (2016) propose the label smoothing technique which can effectively reduce the calibration error. Ding et al. (2019) extend label smoothing to adaptive label regularization. Calibration on Structured Prediction Different from classification tasks, most natural language processing (NLP) tasks deal with complex structures (Kuleshov and Liang, 2015). Nguyen and O’Connor (2015) verified the finding of NiculescuMizil and Caruana (2005) in NLP tasks on loglinear structured models. For NMT, some works directed their attention to the uncertainty in prediction (Ott et al., 2018; Wang et al., 2019), Kumar and Sarawagi (2019) studied the calibration of several NMT models and found that the end of a sentence is severely miscalibrated. M¨uller et al. (2019) investigated the effect of label smoothing, finding that NMT models are well-calibrated in training. Different from previous works, we are interested in the calibration of NMT models in inference, given that the training and inference are discrepant for standard NMT models (Vaswani et al., 2017). 3 3.1 Definitions of Calibration Neural Machine Translation Training In machine translation task, an NMT model F : x → y maximizes the probabi"
2020.acl-main.278,D18-1396,0,0.0500661,"Missing"
2020.acl-main.278,P02-1040,0,\N,Missing
2020.amta-research.11,J93-2003,0,0.191098,"n languages automatically, is an important task in natural language processing and artificial intelligence communities. With the availability of bilingual machine-readable texts, data-driven approaches to machine translation have gained wide popularity since the 1990s (Hutchins and Lovtskii, 2000). Recent several years have witnessed the rapid development of end-to-end neural machine translation (NMT) (Sutskever et al., 2014; Bahdanau et al., 2014; Vaswani et al., 2017). Capable of learning representations from data, NMT has quickly replaced conventional statistical machine translation (SMT) (Brown et al., 1993; Koehn et al., 2003; Chiang, 2005) to become the new de facto method in practical MT systems (Wu et al., 2016). This paper introduces THUMT, an open-source NMT toolkit targeting both academia and industry. THUMT originally developed with Theano (Theano Development Team, 2016) and begins its launch in June 2017. With the emerging of new deep learning frameworks, THUMT added TensorFlow (Abadi et al., 2016) implementation in October 2017 and PyTorch (Paszke et al., 2019) implementation in August 2019. The current status of the three implementations are as follows: • THUMT-Theano (Zhang et al., 2"
2020.amta-research.11,P16-1185,1,0.836159,"arning frameworks, THUMT added TensorFlow (Abadi et al., 2016) implementation in October 2017 and PyTorch (Paszke et al., 2019) implementation in August 2019. The current status of the three implementations are as follows: • THUMT-Theano (Zhang et al., 2017): the original project developed with Theano, which is no longer updated because MLA put an end to Theano. It implemented the standard attention-based model (RNNsearch) (Bahdanau et al., 2014), minimum risk training (MRT) (Shen et al., 2015) for optimizing model parameters with respect to evaluation metrics, semi-supervised training (SST) (Cheng et al., 2016) for exploiting monolingual ∗Corresponding author. Proceedings of the 14th Conference of the Association for Machine Translation in the Americas October 6 - 9, 2020, Volume 1: MT Research Track Page 116 Features Models Criterions Optimizers LRP Gradient Aggregation Distributed Training Mixed-Precision TensorBoard Theano RNNsearch MLE, MRT, SST SGD, Adadelta, Adam Yes No No No No TensorFlow Seq2Seq, RNNsearch, Transformer MLE Adam Yes Yes Yes Yes Yes PyTorch Transformer MLE SGD, Adadelta, Adam No Yes Yes Yes Yes Table 1: Available features in different implementations. corpora to learn bi-direc"
2020.amta-research.11,P05-1033,0,0.306244,"nt task in natural language processing and artificial intelligence communities. With the availability of bilingual machine-readable texts, data-driven approaches to machine translation have gained wide popularity since the 1990s (Hutchins and Lovtskii, 2000). Recent several years have witnessed the rapid development of end-to-end neural machine translation (NMT) (Sutskever et al., 2014; Bahdanau et al., 2014; Vaswani et al., 2017). Capable of learning representations from data, NMT has quickly replaced conventional statistical machine translation (SMT) (Brown et al., 1993; Koehn et al., 2003; Chiang, 2005) to become the new de facto method in practical MT systems (Wu et al., 2016). This paper introduces THUMT, an open-source NMT toolkit targeting both academia and industry. THUMT originally developed with Theano (Theano Development Team, 2016) and begins its launch in June 2017. With the emerging of new deep learning frameworks, THUMT added TensorFlow (Abadi et al., 2016) implementation in October 2017 and PyTorch (Paszke et al., 2019) implementation in August 2019. The current status of the three implementations are as follows: • THUMT-Theano (Zhang et al., 2017): the original project develope"
2020.amta-research.11,P17-1106,1,0.849312,"4th Conference of the Association for Machine Translation in the Americas October 6 - 9, 2020, Volume 1: MT Research Track Page 116 Features Models Criterions Optimizers LRP Gradient Aggregation Distributed Training Mixed-Precision TensorBoard Theano RNNsearch MLE, MRT, SST SGD, Adadelta, Adam Yes No No No No TensorFlow Seq2Seq, RNNsearch, Transformer MLE Adam Yes Yes Yes Yes Yes PyTorch Transformer MLE SGD, Adadelta, Adam No Yes Yes Yes Yes Table 1: Available features in different implementations. corpora to learn bi-directional translation models, and layer-wise relevance propagation (LRP) (Ding et al., 2017) for visualizing and analyzing RNNsearch. • THUMT-TensorFlow: an implementation focuses on performance. It implemented the sequence-to-sequence model (Seq2Seq) (Sutskever et al., 2014), the standard attentionbased model (RNNsearch) (Bahdanau et al., 2014), the Transformer model (Transformer) (Vaswani et al., 2017), and LRP visualization for RNNsearch and Transformer. It also added new features such as multi-GPU training, distributed training, ensemble inference, and TensorBoard visualization. • THUMT-PyTorch: a new implementation developed with PyTorch, which is more flexible and easier to use"
2020.amta-research.11,N03-1017,0,0.194988,"cally, is an important task in natural language processing and artificial intelligence communities. With the availability of bilingual machine-readable texts, data-driven approaches to machine translation have gained wide popularity since the 1990s (Hutchins and Lovtskii, 2000). Recent several years have witnessed the rapid development of end-to-end neural machine translation (NMT) (Sutskever et al., 2014; Bahdanau et al., 2014; Vaswani et al., 2017). Capable of learning representations from data, NMT has quickly replaced conventional statistical machine translation (SMT) (Brown et al., 1993; Koehn et al., 2003; Chiang, 2005) to become the new de facto method in practical MT systems (Wu et al., 2016). This paper introduces THUMT, an open-source NMT toolkit targeting both academia and industry. THUMT originally developed with Theano (Theano Development Team, 2016) and begins its launch in June 2017. With the emerging of new deep learning frameworks, THUMT added TensorFlow (Abadi et al., 2016) implementation in October 2017 and PyTorch (Paszke et al., 2019) implementation in August 2019. The current status of the three implementations are as follows: • THUMT-Theano (Zhang et al., 2017): the original p"
2020.amta-research.11,P02-1040,0,0.110845,"d interpret neural machine translation models (Ding et al., 2017). Figure 1 shows an example of visualizing the Transformer model. Although the transformer model uses attention mechanisms extensively, it is hard to determine the most representative head. For LRP, it is possible to calculate the global relevance between source words and target words, which helps analyze the internal workings of NMT. Please refer to Ding et al. (2017) for more details. 3 Experiments 3.1 Setup We evaluate THUMT on English-German and Chinese-English translation tasks. The evaluation metric is case-sensitive BLEU (Papineni et al., 2002). Following Vaswani et al. (2017), translations are generated via beam-search with a beam size of 4 and a length penalty of 0.6. For English-German, we use the WMT14 training corpus which contains 4.5M sentence pairs with 103M English words and 96M German words. We also use a shared sourcetarget vocabulary of about 37000 tokens encoded by BPE (Sennrich et al., 2016). We use newstest2014 as the test set. For Chinese-English translation, we use the training corpus provided by WMT18 2 . The corpus consists of 24M sentence pairs with 509M Chinese words and 576M English words. We use 32K BPE operat"
2020.amta-research.11,N18-2074,0,0.0261219,"and uses another RNN to generate translation conditioned on the representation. RNNsearch exploits variable representation with attention mechanism and achieves significant improvements over Seq2Seq model. Transformer uses deep self-attention layers instead of RNN layers in both encoder and decoder. It achieves the best performance among the three models. 1 https://github.com/THUNLP-MT/THUMT Proceedings of the 14th Conference of the Association for Machine Translation in the Americas October 6 - 9, 2020, Volume 1: MT Research Track Page 117 THUMT also implemented relative position embedding (Shaw et al., 2018) in its Transformer implementation. Instead of adding absolute positions to its input, this approach considers relative positions in the self-attention mechanism. We offer two options in our Tensorflow implementation, one can turn on or off the relative position embedding and set the desired maximum relative distance. Note that we only implement relative position embedding in selfattention but not in encoder-decoder attention. 2.2 Training THUMT supports both single machine multi-GPU training and multi-machine distributed training. THUMT-TensorFlow uses the Horovod (Sergeev and Del Balso, 2018"
2020.amta-research.11,P17-4012,0,0.0344285,"wn et al., 1993; Koehn et al., 2003; Chiang, 2005) to become the new de facto method in practical MT systems (Wu et al., 2016). This paper introduces THUMT, an open-source NMT toolkit targeting both academia and industry. THUMT originally developed with Theano (Theano Development Team, 2016) and begins its launch in June 2017. With the emerging of new deep learning frameworks, THUMT added TensorFlow (Abadi et al., 2016) implementation in October 2017 and PyTorch (Paszke et al., 2019) implementation in August 2019. The current status of the three implementations are as follows: • THUMT-Theano (Zhang et al., 2017): the original project developed with Theano, which is no longer updated because MLA put an end to Theano. It implemented the standard attention-based model (RNNsearch) (Bahdanau et al., 2014), minimum risk training (MRT) (Shen et al., 2015) for optimizing model parameters with respect to evaluation metrics, semi-supervised training (SST) (Cheng et al., 2016) for exploiting monolingual ∗Corresponding author. Proceedings of the 14th Conference of the Association for Machine Translation in the Americas October 6 - 9, 2020, Volume 1: MT Research Track Page 116 Features Models Criterions Optimizer"
2020.deelio-1.9,2020.emnlp-main.54,0,0.184937,"Missing"
2020.deelio-1.9,2021.ccl-1.108,0,0.135312,"Missing"
2020.deelio-1.9,D18-2029,0,0.0270846,"Pretrained KG embeddings based on ConceptNet via TransE (Bordes et al., 2013; Zhou et al., 2018). Figure 2: Illustration of the proposed model incorporating external KGs for SocialIQA. • Pretrained word embeddings retrofitted by ConceptNet (Speer et al., 2017), where its training adjusts a word’s embeddings to be close to those of its neighbors in the graph. 4 Experiments and Results 4.1 Experimental Setup We use Hugging Face’s transformers toolkit2 and train our models on the 33k SocialIQA training • Encoded tuple-converted text with templates and pretrained universal sentence encoder (USE) (Cer et al., 2018), a Transformer-based 2 76 https://huggingface.co/transformers/ 4.3 instances, running hyper-parameters search over the learning rate in {5e − 6, 1e − 5, 2.5e − 5}, and the effective batch size (number of GPUs × batch size per GPU × gradient accumulation steps) in {8, 16, 32} for the proposed models and baselines respectively, and report their best performance on the dev set. We set the maximum returned tuples of each instance to k = 30. 4.2 To demonstrate the effective utilization of external KGs, we now investigate performance in the limited training data regime. We fine-tuned our model on 5"
2020.deelio-1.9,N19-1423,0,0.0289326,"ng-Yun Chang1 , Yang Liu2 , Karthik Gopalakrishnan2 , Behnam Hedayatnia2 , ¨ 2∗ Pei Zhou3 , Dilek Hakkani-Tur 1 Academia Sinica, Taiwan; 2 Alexa AI, Amazon, USA; 3 USC, USA r06922168@ntu.edu.tw, {yangliud,karthgop,behnam,hakkanit}@amazon.com Abstract achieved impressive performance by fine-tuning T5 (Raffel et al., 2019). Recently there is an increasing effort to utilize external knowledge bases to incorporate commonsense information underlying the text (Shwartz et al., 2020; Mitra et al., 2019; Ji et al., 2020a,b). While most prior work on SocialIQA utilized large pretrained language models (Devlin et al., 2019; Liu et al., 2019; Radford et al., 2018, 2019; Raffel et al., 2019), we argue that such a challenging task requires commonsense reasoning of social events, and simply fine-tuning the model to fit the task is insufficient. We believe it would be beneficial if the model can learn from knowledge-rich resources such as ConceptNet (Liu and Singh, 2004), and thus have a broader and deeper understanding of the information not present in the provided context and answer candidates. In this paper, we propose two approaches tailored to large pretrained language models to utilize existing knowledge graph"
2020.deelio-1.9,S12-1052,0,0.033482,"ly infuse such KGs into pretrained language models. We demonstrate our proposed methods perform well on SocialIQA, a social commonsense reasoning task, in both limited and full training data regimes. 1 Introduction Empowering machines with commonsense has become a hot topic recently. Past research efforts for this problem include the construction of various data sets and models. Several commonsense data sets have been commonly used in past work to develop machines’ commonsense capability (Talmor et al., 2019; Huang et al., 2019; Zellers et al., 2019; Sap et al., 2019b; Sakaguchi et al., 2019; Gordon et al., 2012; Rajani et al., 2019). In particular, SocialIQA (Sap et al., 2019b) is a multiple-choice QA data set for probing machine’s emotional and social intelligence in a variety of everyday situations, which is the data set used in this study. To improve the modeling approaches for the SocialIQA and other commonsense tasks, Shwartz et al. (2020) and Bosselut and Choi (2019) focused on zero-shot setting using pretrained language models. Khashabi et al. (2020) reformulated the multi-choice setup used in most data sets as a generation task and 2 Problem Formulation and Baseline In SocialIQA, given a con"
2020.deelio-1.9,P19-1487,0,0.019547,"to pretrained language models. We demonstrate our proposed methods perform well on SocialIQA, a social commonsense reasoning task, in both limited and full training data regimes. 1 Introduction Empowering machines with commonsense has become a hot topic recently. Past research efforts for this problem include the construction of various data sets and models. Several commonsense data sets have been commonly used in past work to develop machines’ commonsense capability (Talmor et al., 2019; Huang et al., 2019; Zellers et al., 2019; Sap et al., 2019b; Sakaguchi et al., 2019; Gordon et al., 2012; Rajani et al., 2019). In particular, SocialIQA (Sap et al., 2019b) is a multiple-choice QA data set for probing machine’s emotional and social intelligence in a variety of everyday situations, which is the data set used in this study. To improve the modeling approaches for the SocialIQA and other commonsense tasks, Shwartz et al. (2020) and Bosselut and Choi (2019) focused on zero-shot setting using pretrained language models. Khashabi et al. (2020) reformulated the multi-choice setup used in most data sets as a generation task and 2 Problem Formulation and Baseline In SocialIQA, given a context C of an event and"
2020.deelio-1.9,2020.acl-main.740,0,0.0394901,"Missing"
2020.deelio-1.9,D19-1243,0,0.0428778,"Missing"
2020.deelio-1.9,2020.aacl-main.28,0,0.253534,"Missing"
2020.deelio-1.9,D19-1454,0,0.0570248,"Missing"
2020.deelio-1.9,2020.emnlp-main.373,0,0.211563,"Missing"
2020.deelio-1.9,N19-1421,0,0.0267776,"ionships. Thus, towards general commonsense learning, we propose two approaches to implicitly and explicitly infuse such KGs into pretrained language models. We demonstrate our proposed methods perform well on SocialIQA, a social commonsense reasoning task, in both limited and full training data regimes. 1 Introduction Empowering machines with commonsense has become a hot topic recently. Past research efforts for this problem include the construction of various data sets and models. Several commonsense data sets have been commonly used in past work to develop machines’ commonsense capability (Talmor et al., 2019; Huang et al., 2019; Zellers et al., 2019; Sap et al., 2019b; Sakaguchi et al., 2019; Gordon et al., 2012; Rajani et al., 2019). In particular, SocialIQA (Sap et al., 2019b) is a multiple-choice QA data set for probing machine’s emotional and social intelligence in a variety of everyday situations, which is the data set used in this study. To improve the modeling approaches for the SocialIQA and other commonsense tasks, Shwartz et al. (2020) and Bosselut and Choi (2019) focused on zero-shot setting using pretrained language models. Khashabi et al. (2020) reformulated the multi-choice setup us"
2020.deelio-1.9,S18-1120,0,0.0281737,"assification and semantic similarity. In the second approach, we treat the retrieved tuples as items in a cached external knowledge base (KB), which dynamically changes based on every input instance. The model can then decide the importance of each item and leverage them accordingly. Then we transform these embeddings of the topk tuples using a linear transformation that is learned during training, and then concatenate all of them to form the knowledge representation HKG ∈ Rk×d . KG Attentive Representations Motivated by previous work on question answering (Seo et al., 2017; Zhu et al., 2018; Wang et al., 2018; Huang et al., 2019), which uses attention among different segments of the input, here we treat the knowledge tuples as a new segment. Specifically, we concatenate the top-k retrieved tuples and map them into the space of RoBERTa’s final hidden representations as an additional segment, and then attend to it using RoBERTa’s last hidden representation to generate a new KG-attentive sentence representation. Formally, let d be the hidden dimension, l be the sequence length of the input, HR ∈ Rl×d be RoBERTa’s final hidden representation for the SocialIQA input sequence for a given candidate, and"
2020.deelio-1.9,P19-1472,0,0.0232918,"se learning, we propose two approaches to implicitly and explicitly infuse such KGs into pretrained language models. We demonstrate our proposed methods perform well on SocialIQA, a social commonsense reasoning task, in both limited and full training data regimes. 1 Introduction Empowering machines with commonsense has become a hot topic recently. Past research efforts for this problem include the construction of various data sets and models. Several commonsense data sets have been commonly used in past work to develop machines’ commonsense capability (Talmor et al., 2019; Huang et al., 2019; Zellers et al., 2019; Sap et al., 2019b; Sakaguchi et al., 2019; Gordon et al., 2012; Rajani et al., 2019). In particular, SocialIQA (Sap et al., 2019b) is a multiple-choice QA data set for probing machine’s emotional and social intelligence in a variety of everyday situations, which is the data set used in this study. To improve the modeling approaches for the SocialIQA and other commonsense tasks, Shwartz et al. (2020) and Bosselut and Choi (2019) focused on zero-shot setting using pretrained language models. Khashabi et al. (2020) reformulated the multi-choice setup used in most data sets as a generation task"
2020.emnlp-main.42,W18-6318,0,0.357737,"ating that if the correct decoding step and layer are chosen, attention weights in vanilla Transformer are sufficient for generating accurate word alignment interpretation. • We further propose S HIFT-AET , which extracts alignments from an additional alignment module. The module is tightly integrated into vanilla Transformer and trained with supervision from symmetrized S HIFT-ATT alignments. S HIFT-AET does not affect the translation accuracy and significantly outperforms GIZA++ by 1.4-4.8 AER points in our experiments. • We compare our methods with NAIVE -ATT on dictionary-guided decoding (Alkhouli et al., 2018), an alignment-related downstream task. Both methods consistently outperform NAIVE -ATT, demonstrating the effectiveness of our methods in such alignment-related NLP tasks. To alleviate this problem, some researchers modify the transformer architecture by adding alignment modules that predict the to-be-aligned target token (Zenkel et al., 2019, 2020) or modify the training loss by designing an alignment loss computed with full target sentence (Garg et al., 2019; Zenkel et al., 2020). Others argue that using only attention weights is insufficient for generating clean word alignment and propose"
2020.emnlp-main.42,W17-4711,0,0.0449857,"alignments especially for sentence beginning compared to NAIVE -ATT. 5 Related Work Alignment induction from RNNSearch (Bahdanau et al., 2015) has been explored by a number of works. Bahdanau et al. (2015) are the first to show word alignment example using attention in RNNSearch. Ghader and Monz (2017) further demonstrate that the RNN-based NMT system achieves comparable alignment performance to that of GIZA++. Alignment has also been used to improve NMT performance, especially in low resource settings, by supervising the attention mechanisms of RNNSearch (Chen et al., 2016; Liu et al., 2016; Alkhouli and Ney, 2017). There is also a number of other studies that induce word alignment from Transformer. Li et al. (2019); Ding et al. (2019) claim that attention may not capture word alignment in Transformer, and propose to induce word alignment with prediction difference (Li et al., 2019) or gradient-based measures (Ding et al., 2019). Zenkel et al. (2019) modify the Transformer architecture for better align573 !   !  ""  !  "" ""   !  !  $ # # $  ! #$ # $ !  "" "" # $ #$  ""  $ # ""  (a) Naive-Att (b) Shift-Att Figure 4: AER on the test set v.s. BLEU on the validation set on the de→en translation, eva"
2020.emnlp-main.42,J93-2003,0,0.192205,"weiß ich . das weiß ich Source: das weiß ich . . Dec. input: <bos> i understand this . Dec. output: i understand this . <eos> Figure 1: An example to compare our method S HIFTATT and the baseline NAIVE -ATT. The left is an attention map from the third decoder layer of the vanilla Transformer and the right are the induced alignments. S HIFT-ATT induces alignments for target word yi at decoding step i + 1 when yi is the decoder input, while NAIVE -ATT at step i when yi is the decoder output. Introduction The task of word alignment is to find lexicon translation equivalents from parallel corpus (Brown et al., 1993). It is one of the fundamental tasks in natural language processing (NLP) and is widely studied by the community (Dyer et al., 2013; Brown et al., 1993; Liu and Sun, 2015). Word alignments are useful in many scenarios, such as error analysis (Ding et al., 2017; Li et al., 2019), the introduction of coverage and fertility models (Tu et al., 2016), inserting external constraints in interactive machine translation (Hasler et al., 2018; ∗ Corresponding author. Part of the work was done when Yun was in Huawei Noah’s Ark Lab. 1 Code can be found at https://github.com/ sufe-nlp/transformer-alignment."
2020.emnlp-main.42,2016.amta-researchers.10,0,0.06603,"airs than GIZA++, and extract better alignments especially for sentence beginning compared to NAIVE -ATT. 5 Related Work Alignment induction from RNNSearch (Bahdanau et al., 2015) has been explored by a number of works. Bahdanau et al. (2015) are the first to show word alignment example using attention in RNNSearch. Ghader and Monz (2017) further demonstrate that the RNN-based NMT system achieves comparable alignment performance to that of GIZA++. Alignment has also been used to improve NMT performance, especially in low resource settings, by supervising the attention mechanisms of RNNSearch (Chen et al., 2016; Liu et al., 2016; Alkhouli and Ney, 2017). There is also a number of other studies that induce word alignment from Transformer. Li et al. (2019); Ding et al. (2019) claim that attention may not capture word alignment in Transformer, and propose to induce word alignment with prediction difference (Li et al., 2019) or gradient-based measures (Ding et al., 2019). Zenkel et al. (2019) modify the Transformer architecture for better align573 !   !  ""  !  "" ""   !  !  $ # # $  ! #$ # $ !  "" "" # $ #$  ""  $ # ""  (a) Naive-Att (b) Shift-Att Figure 4: AER on the test set v.s. BLEU on the v"
2020.emnlp-main.42,W93-0301,0,0.721085,"by the community (Dyer et al., 2013; Brown et al., 1993; Liu and Sun, 2015). Word alignments are useful in many scenarios, such as error analysis (Ding et al., 2017; Li et al., 2019), the introduction of coverage and fertility models (Tu et al., 2016), inserting external constraints in interactive machine translation (Hasler et al., 2018; ∗ Corresponding author. Part of the work was done when Yun was in Huawei Noah’s Ark Lab. 1 Code can be found at https://github.com/ sufe-nlp/transformer-alignment. Chen et al., 2020) and providing guidance for human translators in computer-aided translation (Dagan et al., 1993). Word alignment is part of the pipeline in statistical machine translation (Koehn et al., 2003, SMT), but is not necessarily needed for neural machine translation (Bahdanau et al., 2015, NMT). The attention mechanism in NMT does not functionally play the role of word alignments between the source and the target, at least not in the same way as its analog in SMT. It is hard to interpret the attention activations and extract meaningful word alignments especially from Transformer (Garg et al., 2019). As a result, the most widely used word alignment tools are still external statistical models suc"
2020.emnlp-main.42,W19-5201,0,0.145507,"rd to interpret the attention activations and extract meaningful word alignments especially from Transformer (Garg et al., 2019). As a result, the most widely used word alignment tools are still external statistical models such as FAST-A LIGN (Dyer et al., 2013) and GIZA++ (Brown et al., 1993; Och and Ney, 2003). 566 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 566–576, c November 16–20, 2020. 2020 Association for Computational Linguistics Recently, there is a resurgence of interest in the community to study word alignments for the Transformer (Ding et al., 2019; Li et al., 2019). One simple solution is NAIVE -ATT, which induces word alignments from the attention weights between the encoder and decoder. The next target word is aligned with the source word that has the maximum attention weight, as shown in Fig. 1. However, such schedule only captures noisy word alignments (Ding et al., 2019; Garg et al., 2019). One of the major problems is that it induces alignment before observing the to-be-aligned target token (Peter et al., 2017; Ding et al., 2019). Suppose for the same source sentence, there are two alternative translations that diverge at decodin"
2020.emnlp-main.42,P17-1106,1,0.862585,"yer of the vanilla Transformer and the right are the induced alignments. S HIFT-ATT induces alignments for target word yi at decoding step i + 1 when yi is the decoder input, while NAIVE -ATT at step i when yi is the decoder output. Introduction The task of word alignment is to find lexicon translation equivalents from parallel corpus (Brown et al., 1993). It is one of the fundamental tasks in natural language processing (NLP) and is widely studied by the community (Dyer et al., 2013; Brown et al., 1993; Liu and Sun, 2015). Word alignments are useful in many scenarios, such as error analysis (Ding et al., 2017; Li et al., 2019), the introduction of coverage and fertility models (Tu et al., 2016), inserting external constraints in interactive machine translation (Hasler et al., 2018; ∗ Corresponding author. Part of the work was done when Yun was in Huawei Noah’s Ark Lab. 1 Code can be found at https://github.com/ sufe-nlp/transformer-alignment. Chen et al., 2020) and providing guidance for human translators in computer-aided translation (Dagan et al., 1993). Word alignment is part of the pipeline in statistical machine translation (Koehn et al., 2003, SMT), but is not necessarily needed for neural m"
2020.emnlp-main.42,N13-1073,0,0.863936,"1: An example to compare our method S HIFTATT and the baseline NAIVE -ATT. The left is an attention map from the third decoder layer of the vanilla Transformer and the right are the induced alignments. S HIFT-ATT induces alignments for target word yi at decoding step i + 1 when yi is the decoder input, while NAIVE -ATT at step i when yi is the decoder output. Introduction The task of word alignment is to find lexicon translation equivalents from parallel corpus (Brown et al., 1993). It is one of the fundamental tasks in natural language processing (NLP) and is widely studied by the community (Dyer et al., 2013; Brown et al., 1993; Liu and Sun, 2015). Word alignments are useful in many scenarios, such as error analysis (Ding et al., 2017; Li et al., 2019), the introduction of coverage and fertility models (Tu et al., 2016), inserting external constraints in interactive machine translation (Hasler et al., 2018; ∗ Corresponding author. Part of the work was done when Yun was in Huawei Noah’s Ark Lab. 1 Code can be found at https://github.com/ sufe-nlp/transformer-alignment. Chen et al., 2020) and providing guidance for human translators in computer-aided translation (Dagan et al., 1993). Word alignment"
2020.emnlp-main.42,D19-1453,0,0.0693459,". Chen et al., 2020) and providing guidance for human translators in computer-aided translation (Dagan et al., 1993). Word alignment is part of the pipeline in statistical machine translation (Koehn et al., 2003, SMT), but is not necessarily needed for neural machine translation (Bahdanau et al., 2015, NMT). The attention mechanism in NMT does not functionally play the role of word alignments between the source and the target, at least not in the same way as its analog in SMT. It is hard to interpret the attention activations and extract meaningful word alignments especially from Transformer (Garg et al., 2019). As a result, the most widely used word alignment tools are still external statistical models such as FAST-A LIGN (Dyer et al., 2013) and GIZA++ (Brown et al., 1993; Och and Ney, 2003). 566 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 566–576, c November 16–20, 2020. 2020 Association for Computational Linguistics Recently, there is a resurgence of interest in the community to study word alignments for the Transformer (Ding et al., 2019; Li et al., 2019). One simple solution is NAIVE -ATT, which induces word alignments from the attention weights"
2020.emnlp-main.42,I17-1004,0,0.0428756,"Missing"
2020.emnlp-main.42,N18-2081,0,0.0646567,"Missing"
2020.emnlp-main.42,P17-1141,1,0.848262,"over NAIVE -ATT by 1.1 and 1.5 BLEU scores on de→en and en→de translations, respectively. The results suggest the effectiveness of our methods in application to alignment-related NLP tasks. Downstream Task Results In addition to AER, we compare the performance of NAIVE -ATT, S HIFT-ATT and S HIFT-AET on dictionary-guided machine translation (Song et al., 2020), which is an alignment-based downstream task. Given source and target constraint pairs from dictionary, the NMT model is encouraged to translate with provided constraints via word alignments (Alkhouli et al., 2018; Hasler et al., 2018; Hokamp and Liu, 2017; Song et al., 2020). More specifically, at each decoding step, the last token of the candidate translation will be revised with target constraint if it is aligned to the corresponding source constraint according to the alignment induction method. To simulate the process of looking up dictionary, we follow Hasler et al. (2018) and extract the pre-specified constraints from the test set and its reference according to the golden word alignments. We exclude stop words, and sample up to 3 dictionary constraints per sentence. Each dic1 4.4 Analysis Layer Selection Criterion To test whether the laye"
2020.emnlp-main.42,W04-3250,0,0.0731991,"02E18, LDC2003E07, LDC2003E14, LDC2004T07, LDC2004T08 and LDC2005T06 Task de→en en→de NAIVE -ATT 33.7 26.5 S HIFT-ATT 34.3∗ 26.8 (a) Validation AER for Layer Selection S HIFT-AET 34.8∗ 28.0∗ de→en en→de 1 2 3 4 5 6 Table 5: Comparison of dictionary-guided decoding with different alignment methods. We report BLEU scores on the test set. Without dictionary-guided decoding, we obtain 32.3 and 24.2 BLEU on de→en and en→de translations respectively. “*” indicates the result is significantly better than that of NAIVE -ATT (p<0.05). All significance tests are measured by paired bootstrap resampling (Koehn, 2004) set. The test set includes 450 parallel sentence pairs with manually labelled word alignments.9 We use jieba10 for Chinese text segmentation and follow the settings in Section 4.1 for data pre-processing and model training. The results are shown in Table 4. It presents that both S HIFT-ATT and S HIFTAET outperform NAIVE -ATT to a large margin. When comparing the symmetrized alignment performance with GIZA++, S HIFT-AET performs better, while S HIFT-ATT is worse. The experimental results are roughly consistent with the observations on other language pairs, demonstrating the effectiveness of ou"
2020.emnlp-main.42,2005.iwslt-1.8,0,0.49688,"VE -ATT uses zil . l We argue using zi+1 is better. First, at bottom layl ers, we hypothesize that zi+1 could better represent the decoder input yi than output yi+1 . Therefore we l can use zi+1 with small l to represent yi . Second, l zi+1 is computed after observing yi , indicating that S HIFT-ATT is able to adapt the alignment induction with the to-be-aligned target token. Our proposed method involves inducing alignments from source-to-target and target-to-source vanilla Transformer models. Following Zenkel et al. (2019), we merge bidirectional alignments using the grow diagonal heuristic (Koehn et al., 2005). Layer Selection Criterion To select the best layer lb to induce alignments, we propose a surrogate layer selection criterion without manually labelled word alignments. Experiments show that this criterion correlates well with the AER metric. Given parallel sentence pairs hx, yi, we train a source-to-target model θx→y and a target-to-source model θy→x . We assume that the word alignments extracted from these two models should agree with each other (Cheng et al., 2016). Therefore, we evaluate the quality of the alignments by computing the AER score on the validation set with the source-to-targ"
2020.emnlp-main.42,N03-1017,0,0.342534,"Missing"
2020.emnlp-main.42,N16-1082,0,0.294355,"former iwslt de en model configuration following Ding et al. (2019). We train the models with a batch size of 36K tokens and set the maximum updates as 50K and 10K for 2 We simply normalize rows corresponding to target tokens ˆ that are aligned to at least one source token of A. 3 https://www-i6.informatik.rwth-aachen. de/goldAlignment/ 4 http://web.eecs.umich.edu/˜mihalcea/ wpt/index.html 5 https://github.com/pytorch/fairseq Method Inter. FAST-A LIGN (Dyer et al., 2013) GIZA++ (Brown et al., 1993) - NAIVE -ATT (Garg et al., 2019) NAIVE -ATT-LA (Garg et al., 2019) S HIFT-ATT-LA S MOOTH G RAD (Li et al., 2016) SD-S MOOTH G RAD (Ding et al., 2019) PD (Li et al., 2019) A DD SGD (Zenkel et al., 2019) M TL -F ULLC (Garg et al., 2019) Y Y Y Y Y Y N N M TL -F ULLC -GZ (Garg et al., 2019) N S HIFT-ATT S HIFT-AET Y N de-en de→en en→de bidir Statistical Methods Y 28.5 30.4 25.7 Y 18.8 19.6 17.8 Neural Methods N 33.3 36.5 28.1 N 40.9 50.8 39.8 N 54.7 46.2 45.5 N 36.4 45.8 30.3 N 36.4 43.0 29.0 N 38.1 44.8 34.4 N 26.6 30.4 21.2 Y - 20.2 Statistical + Neural Methods Y - 16.0 Our Neural Methods N 20.9 25.7 17.9 N 15.8 19.2 15.4 Fullc fr→en fr-en en→fr bidir ro→en ro-en en→ro bidir 16.3 7.1 17.1 7.2 12.1 6.1 33."
2020.emnlp-main.42,P19-1124,0,0.573565,"Transformer and the right are the induced alignments. S HIFT-ATT induces alignments for target word yi at decoding step i + 1 when yi is the decoder input, while NAIVE -ATT at step i when yi is the decoder output. Introduction The task of word alignment is to find lexicon translation equivalents from parallel corpus (Brown et al., 1993). It is one of the fundamental tasks in natural language processing (NLP) and is widely studied by the community (Dyer et al., 2013; Brown et al., 1993; Liu and Sun, 2015). Word alignments are useful in many scenarios, such as error analysis (Ding et al., 2017; Li et al., 2019), the introduction of coverage and fertility models (Tu et al., 2016), inserting external constraints in interactive machine translation (Hasler et al., 2018; ∗ Corresponding author. Part of the work was done when Yun was in Huawei Noah’s Ark Lab. 1 Code can be found at https://github.com/ sufe-nlp/transformer-alignment. Chen et al., 2020) and providing guidance for human translators in computer-aided translation (Dagan et al., 1993). Word alignment is part of the pipeline in statistical machine translation (Koehn et al., 2003, SMT), but is not necessarily needed for neural machine translation"
2020.emnlp-main.42,C16-1291,0,0.026905,"nd extract better alignments especially for sentence beginning compared to NAIVE -ATT. 5 Related Work Alignment induction from RNNSearch (Bahdanau et al., 2015) has been explored by a number of works. Bahdanau et al. (2015) are the first to show word alignment example using attention in RNNSearch. Ghader and Monz (2017) further demonstrate that the RNN-based NMT system achieves comparable alignment performance to that of GIZA++. Alignment has also been used to improve NMT performance, especially in low resource settings, by supervising the attention mechanisms of RNNSearch (Chen et al., 2016; Liu et al., 2016; Alkhouli and Ney, 2017). There is also a number of other studies that induce word alignment from Transformer. Li et al. (2019); Ding et al. (2019) claim that attention may not capture word alignment in Transformer, and propose to induce word alignment with prediction difference (Li et al., 2019) or gradient-based measures (Ding et al., 2019). Zenkel et al. (2019) modify the Transformer architecture for better align573 !   !  ""  !  "" ""   !  !  $ # # $  ! #$ # $ !  "" "" # $ #$  ""  $ # ""  (a) Naive-Att (b) Shift-Att Figure 4: AER on the test set v.s. BLEU on the validation set on t"
2020.emnlp-main.42,P00-1056,0,0.832681,"er model. The column Fullc denotes whether full target sentence is used to extract alignments at test time. The lower AER, the better. We mark best symmetrized interpretation results of vanilla Transformer with underlines, and best symmetrized results among all with boldface. Transformer and AET respectively. The last checkpoint of AET is used for evaluation. All models are trained in both translation directions and symmetrized with grow-diag (Koehn et al., 2005) using the script from Zenkel et al. (2019).6 Evaluation We evaluate the alignment quality of our methods with Alignment Error Rate (Och and Ney, 2000, AER). Since word alignments are useful for many downstream tasks as discussed in Section 1, we also evaluate our methods on dictionaryguided decoding, a downstream task of alignment induction, with the metric BLEU (Papineni et al., 2002). More details are in Section 4.3. Baselines We compare our methods with two statistical baselines FAST-A LIGN and GIZA++ and nine other baselines: • NAIVE -ATT (Garg et al., 2019): the approach we discuss in Section 2.2, which induces alignments from the attention weights of the penultimate layer of the Transformer. • NAIVE -ATT-LA (Garg et al., 2019): the N"
2020.emnlp-main.42,J03-1002,0,0.136128,"lation (Koehn et al., 2003, SMT), but is not necessarily needed for neural machine translation (Bahdanau et al., 2015, NMT). The attention mechanism in NMT does not functionally play the role of word alignments between the source and the target, at least not in the same way as its analog in SMT. It is hard to interpret the attention activations and extract meaningful word alignments especially from Transformer (Garg et al., 2019). As a result, the most widely used word alignment tools are still external statistical models such as FAST-A LIGN (Dyer et al., 2013) and GIZA++ (Brown et al., 1993; Och and Ney, 2003). 566 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 566–576, c November 16–20, 2020. 2020 Association for Computational Linguistics Recently, there is a resurgence of interest in the community to study word alignments for the Transformer (Ding et al., 2019; Li et al., 2019). One simple solution is NAIVE -ATT, which induces word alignments from the attention weights between the encoder and decoder. The next target word is aligned with the source word that has the maximum attention weight, as shown in Fig. 1. However, such schedule only captures no"
2020.emnlp-main.42,P02-1040,0,0.106675,"mmetrized results among all with boldface. Transformer and AET respectively. The last checkpoint of AET is used for evaluation. All models are trained in both translation directions and symmetrized with grow-diag (Koehn et al., 2005) using the script from Zenkel et al. (2019).6 Evaluation We evaluate the alignment quality of our methods with Alignment Error Rate (Och and Ney, 2000, AER). Since word alignments are useful for many downstream tasks as discussed in Section 1, we also evaluate our methods on dictionaryguided decoding, a downstream task of alignment induction, with the metric BLEU (Papineni et al., 2002). More details are in Section 4.3. Baselines We compare our methods with two statistical baselines FAST-A LIGN and GIZA++ and nine other baselines: • NAIVE -ATT (Garg et al., 2019): the approach we discuss in Section 2.2, which induces alignments from the attention weights of the penultimate layer of the Transformer. • NAIVE -ATT-LA (Garg et al., 2019): the NAIVE ATT method without layer selection. It induces alignments from attention weights averaged across all layers. • S HIFT-ATT-LA: S HIFT-ATT method without layer selection. It induces alignments from attention weights averaged across all"
2020.emnlp-main.42,P16-1162,0,0.234929,". Once the alignment module is trained, we extract alignment scores S from it given a parallel sentence pair and induce alignments A using Eq. 3. 4 Experiments 4.1 Settings Dataset We follow previous work (Zenkel et al., 2019, 2020) in data setup and conduct experiments on publicly available datasets for German-English (de-en)3 , Romanian-English (ro-en) and FrenchEnglish (fr-en)4 . Since no validation set is provided, we follow Ding et al. (2019) to set the last 1,000 sentences of the training data before preprocessing as validation set. We learn a joint source and target Byte-Pair-Encoding (Sennrich et al., 2016) with 10k merge operations. Table 1 shows the detailed data statistics. > (hGK zilb GQ 1 X n) n )(˜ √ Si−1 = softmax( ), N n dk (5) Q K d ×d model k where Gn ,Gn ∈ R are the key and query projection matrices for the n-th head, N is the number of attention heads and dk = dmodel /N . Since we only care about the attention weights, the value-related parameters and computation are omitted in this module. To train the alignment module, we use the symmetrized S HIFT-ATT alignments extracted from 569 NMT Systems We implement the Transformer with fairseq-py5 and use the transformer iwslt de en model c"
2020.emnlp-main.42,P16-1008,1,0.866093,"ces alignments for target word yi at decoding step i + 1 when yi is the decoder input, while NAIVE -ATT at step i when yi is the decoder output. Introduction The task of word alignment is to find lexicon translation equivalents from parallel corpus (Brown et al., 1993). It is one of the fundamental tasks in natural language processing (NLP) and is widely studied by the community (Dyer et al., 2013; Brown et al., 1993; Liu and Sun, 2015). Word alignments are useful in many scenarios, such as error analysis (Ding et al., 2017; Li et al., 2019), the introduction of coverage and fertility models (Tu et al., 2016), inserting external constraints in interactive machine translation (Hasler et al., 2018; ∗ Corresponding author. Part of the work was done when Yun was in Huawei Noah’s Ark Lab. 1 Code can be found at https://github.com/ sufe-nlp/transformer-alignment. Chen et al., 2020) and providing guidance for human translators in computer-aided translation (Dagan et al., 1993). Word alignment is part of the pipeline in statistical machine translation (Koehn et al., 2003, SMT), but is not necessarily needed for neural machine translation (Bahdanau et al., 2015, NMT). The attention mechanism in NMT does no"
2020.emnlp-main.42,2020.acl-main.146,0,0.193692,"1.4-4.8 AER points in our experiments. • We compare our methods with NAIVE -ATT on dictionary-guided decoding (Alkhouli et al., 2018), an alignment-related downstream task. Both methods consistently outperform NAIVE -ATT, demonstrating the effectiveness of our methods in such alignment-related NLP tasks. To alleviate this problem, some researchers modify the transformer architecture by adding alignment modules that predict the to-be-aligned target token (Zenkel et al., 2019, 2020) or modify the training loss by designing an alignment loss computed with full target sentence (Garg et al., 2019; Zenkel et al., 2020). Others argue that using only attention weights is insufficient for generating clean word alignment and propose to induce alignments with feature importance measures, such as leaveone-out measures (Li et al., 2019) and gradientbased measures (Ding et al., 2019). However, all previous work induces alignment for target word yi at step i, when yi is the decoder output. Let x = {x1 , ..., x|x |} and y = {y1 , ..., y|y |} be source and target sentences. Neural machine translation models the target sentence given the source sentence as p(y|x; θ): In this work, we propose to induce alignment for tar"
2020.emnlp-main.42,D13-1176,0,\N,Missing
2020.emnlp-main.42,C96-2141,0,\N,Missing
2020.emnlp-main.42,P05-1033,0,\N,Missing
2020.findings-emnlp.132,K18-2017,0,0.0205116,"Missing"
2020.inlg-1.46,D18-1431,0,0.018451,"onal generation and weighted decoding. Conditional generation modifies the input to the model to condition on control parameters. Previous works proposed conditioning response generators on latent (Serban et al., 2016, 2017; Shen et al., 2017; Zhao et al., 2017) or discrete attributes, including dialog acts (Sankar and Ravi, 2019), sentiment (Sankar and Ravi, 2019), speaker identifiers (Li et al., 2016a), lexical features (See et al., 2019) or topics (Serban et al., 2017). Weighted decoding (See et al., 2019) instead uses token-level features that are controllable (Ghazvininejad et al., 2017; Baheti et al., 2018) and supplements the scores from the decoder model output with these features. Our work focuses on conditional generation methods with sentence-level control, as described in more detail in Section 4. There is also previous work on controlling attributes such as question asking at the dialog level. See et al. (2019) initialized the generation of turns of a dialog with a fixed distribution that specified what percentage of generated turns should include questions during the dialog. However this does not allow for flexible control where the number of questions may need to vary depending on the c"
2020.inlg-1.46,P17-4012,0,0.0314902,"ted is the same or different as compared to the knowledge sentence selected for the previous turn xj . Based on this information a certain subset of the transitions defined for dialog act planning are used to predict the dialog acts for the next response. 3.3.2 Knowledge-independent DA Planning The prediction of the dialog acts is done independently of the selected knowledge in four ways: 1. Simple DA planning: We define a set of transitions that determine the set of DAs for the next response based solely on the previous dialog act. 2. Seq2Seq Model for DA planning: Using the OpenNMT library (Klein et al., 2017), we train a sequence-to-sequence model based on bi-directional LSTMs with Luong attention (Luong et al., 2015) to estimate the DAs of the current turn given the dialog context Dj . During training, each dialog act label is a separate token in the vocabulary and has its own embedding vector. Both the dialog act and word embeddings are initialized randomly and learned during training. 3. PropQ DA planning: For comparison to previous work we use the method in (See et al., 2019) which initializes the distribution of questions to be asked at the beginning of the conversation. The work finds that t"
2020.inlg-1.46,P16-1094,0,0.0996359,"Missing"
2020.inlg-1.46,D16-1127,0,0.646489,"l., 2017). As seen in Figure 1, candidate A is a typical generic response given the dialog context. In order to deal with this problem, previous work proposed grounding generated responses on knowledge sentences related to the dialog context (Ghazvininejad et al., 2018; Yavuz et al., 2019; Zhou et al., 2018; Dinan et al., 2018; Gopalakrishnan et al., 2019). To improve the diversity of generated responses, others proposed conditioning response generation on latent (Serban et al., 2016, 2017; Shen et al., 2017; Zhao et al., 2017; Xing et al., 2016) or discrete attributes (Sankar and Ravi, 2019; Li et al., 2016a; See et al., 2019; Serban et al., 2017). These discrete attributes are typically presented to the decoder at the turn level, and are not associated with a specific segment of the output. Another issue with seq2seq approaches is that, due to the lack of explicit control mechanisms, the style of these responses does not always match with what would be suggested by user experience experts. For example, the generated response may not acknowledge what the user just said, or may 412 Proceedings of The 13th International Conference on Natural Language Generation, pages 412–421, c Dublin, Ireland, 1"
2020.inlg-1.46,W02-0109,0,0.236445,"ted to their turns in the conversation. However there is no fine-grained annotation of which knowledge sentence or sentences were used for a turn, hence we create ground-truth knowledge annotations as a corpus post-processing step. To obtain the knowledge annotation for each turn we use Equation 1 to compute similarity between xj+1 and km . To obtain the knowledge annotation for each sentence within a turn, we tokenize the turn into individual sentences. For each sentence we use the same equation to compute similarity between i snj+1 and km . For sentence-tokenization we use the NLTK library (Loper and Bird, 2002). We decide whether or not the turn or sentences within a turn should be linked to a knowledge sentence by manually setting a threshold value on the similarity score between the knowledge and turn or sentences within a turn. We use the same threshold, 0.2, as described in Section 3.2. 5.2.2 Annotating Dialog Acts We obtain the dialog acts for each sentence by running an off-the-shelf SVM dialog act tagger4 (Mezza et al., 2018) which takes in as input the current sentence to predict one of 11 dialog acts listed in Table 1. We also experimented with using 4 https://github.com/ColingPaper2018/dia"
2020.inlg-1.46,D15-1166,0,0.122866,"Missing"
2020.inlg-1.46,N18-5020,0,0.0275398,"nes the style of the response in the form of DAs to be realized. We have two forms of DA planning methods: Knowledge-dependent DA planning and Knowledge-independent DA planning. Figure 2 depicts the architecture of PD-NRG. Figure 2: Policy-driven neural response generation. There has been previous work on task-oriented systems that proposed explicit content and sentence planning (Walker et al., 2007) to further control the content and order of sentences within the generated response. Previous work for open-domain dialog systems also followed a similar method for content and sentence planning. Fang et al. (2018), Ahmadvand et al. (2018), Fulda et al. (2018), Pichl (2018), Cervone et al. (2017), Yu et al. (2019) and Bowden et al. (2019) extracted multiple features such as topic, intent, entities, and sentiment to send to a dialog policy model to plan the structure and content of the response. However, these previous works generated responses from a set of templates that are usually repetitive for open-domain conversations. Our work focuses on neural generative models for response generation in opendomain dialog systems. The closest work to ours in terms of learning a dialog policy for open-domain dial"
2020.inlg-1.46,P17-4008,0,0.0171471,"two main approaches, conditional generation and weighted decoding. Conditional generation modifies the input to the model to condition on control parameters. Previous works proposed conditioning response generators on latent (Serban et al., 2016, 2017; Shen et al., 2017; Zhao et al., 2017) or discrete attributes, including dialog acts (Sankar and Ravi, 2019), sentiment (Sankar and Ravi, 2019), speaker identifiers (Li et al., 2016a), lexical features (See et al., 2019) or topics (Serban et al., 2017). Weighted decoding (See et al., 2019) instead uses token-level features that are controllable (Ghazvininejad et al., 2017; Baheti et al., 2018) and supplements the scores from the decoder model output with these features. Our work focuses on conditional generation methods with sentence-level control, as described in more detail in Section 4. There is also previous work on controlling attributes such as question asking at the dialog level. See et al. (2019) initialized the generation of turns of a dialog with a fixed distribution that specified what percentage of generated turns should include questions during the dialog. However this does not allow for flexible control where the number of questions may need to v"
2020.inlg-1.46,W19-5901,0,0.21303,"ive responses (Wei et al., 2017). As seen in Figure 1, candidate A is a typical generic response given the dialog context. In order to deal with this problem, previous work proposed grounding generated responses on knowledge sentences related to the dialog context (Ghazvininejad et al., 2018; Yavuz et al., 2019; Zhou et al., 2018; Dinan et al., 2018; Gopalakrishnan et al., 2019). To improve the diversity of generated responses, others proposed conditioning response generation on latent (Serban et al., 2016, 2017; Shen et al., 2017; Zhao et al., 2017; Xing et al., 2016) or discrete attributes (Sankar and Ravi, 2019; Li et al., 2016a; See et al., 2019; Serban et al., 2017). These discrete attributes are typically presented to the decoder at the turn level, and are not associated with a specific segment of the output. Another issue with seq2seq approaches is that, due to the lack of explicit control mechanisms, the style of these responses does not always match with what would be suggested by user experience experts. For example, the generated response may not acknowledge what the user just said, or may 412 Proceedings of The 13th International Conference on Natural Language Generation, pages 412–421, c D"
2020.inlg-1.46,N19-1170,0,0.0820829,"in Figure 1, candidate A is a typical generic response given the dialog context. In order to deal with this problem, previous work proposed grounding generated responses on knowledge sentences related to the dialog context (Ghazvininejad et al., 2018; Yavuz et al., 2019; Zhou et al., 2018; Dinan et al., 2018; Gopalakrishnan et al., 2019). To improve the diversity of generated responses, others proposed conditioning response generation on latent (Serban et al., 2016, 2017; Shen et al., 2017; Zhao et al., 2017; Xing et al., 2016) or discrete attributes (Sankar and Ravi, 2019; Li et al., 2016a; See et al., 2019; Serban et al., 2017). These discrete attributes are typically presented to the decoder at the turn level, and are not associated with a specific segment of the output. Another issue with seq2seq approaches is that, due to the lack of explicit control mechanisms, the style of these responses does not always match with what would be suggested by user experience experts. For example, the generated response may not acknowledge what the user just said, or may 412 Proceedings of The 13th International Conference on Natural Language Generation, pages 412–421, c Dublin, Ireland, 15-18 December, 2020"
2020.inlg-1.46,W19-5917,1,0.901903,"og systems have typically been modeled using end-to-end approaches, more specifically encoder-decoder architectures (Sordoni et al., 2015; Serban et al., 2017, 2016; Vinyals and Le, 2015). These seq2seq models are commonly trained on a maximum likelihood objective, which leads to repetitive and uninformative responses (Wei et al., 2017). As seen in Figure 1, candidate A is a typical generic response given the dialog context. In order to deal with this problem, previous work proposed grounding generated responses on knowledge sentences related to the dialog context (Ghazvininejad et al., 2018; Yavuz et al., 2019; Zhou et al., 2018; Dinan et al., 2018; Gopalakrishnan et al., 2019). To improve the diversity of generated responses, others proposed conditioning response generation on latent (Serban et al., 2016, 2017; Shen et al., 2017; Zhao et al., 2017; Xing et al., 2016) or discrete attributes (Sankar and Ravi, 2019; Li et al., 2016a; See et al., 2019; Serban et al., 2017). These discrete attributes are typically presented to the decoder at the turn level, and are not associated with a specific segment of the output. Another issue with seq2seq approaches is that, due to the lack of explicit control me"
2020.inlg-1.46,2020.acl-main.183,0,0.0233133,"ntrol where the number of questions may need to vary depending on the course of the dialog. Therefore, we focus on learning a dialog policy model that automatically learns the style of the response based on the dialog context. Similar to previous work for response generation we ground our generated responses on knowledge. Ghazvininejad et al. (2018), Yavuz et al. (2019), and Zhou et al. (2018) used end-to-end memory networks, copy mechanisms and static graph attention mechanisms respectively to incorporate knowledge. Dinan et al. (2018), Gopalakrishnan et al. (2019), and (Roller et al., 2020; Smith et al., 2020) used memory networks based on transformer architectures (Vaswani et al., 2017) to encode knowledge sentences and dialog history to decode a response. 413 dialog policy has components that predict the individual elements of the action plan: knowledge selection and dialog act planning. Knowledge selection determines the knowledge to be integrated in the response by finding sentences from a knowledge document corpus that are relevant to the dialog context. Dialog act (DA) planning determines the style of the response in the form of DAs to be realized. We have two forms of DA planning methods: Kn"
2020.inlg-1.46,N15-1020,0,0.0857211,"Missing"
2020.inlg-1.46,D19-3014,0,0.0186653,"ds: Knowledge-dependent DA planning and Knowledge-independent DA planning. Figure 2 depicts the architecture of PD-NRG. Figure 2: Policy-driven neural response generation. There has been previous work on task-oriented systems that proposed explicit content and sentence planning (Walker et al., 2007) to further control the content and order of sentences within the generated response. Previous work for open-domain dialog systems also followed a similar method for content and sentence planning. Fang et al. (2018), Ahmadvand et al. (2018), Fulda et al. (2018), Pichl (2018), Cervone et al. (2017), Yu et al. (2019) and Bowden et al. (2019) extracted multiple features such as topic, intent, entities, and sentiment to send to a dialog policy model to plan the structure and content of the response. However, these previous works generated responses from a set of templates that are usually repetitive for open-domain conversations. Our work focuses on neural generative models for response generation in opendomain dialog systems. The closest work to ours in terms of learning a dialog policy for open-domain dialog is (Xu et al., 2018) who designed a policy network to predict dialog acts and fed those acts into"
2020.inlg-1.46,P17-1061,0,0.0764659,"likelihood objective, which leads to repetitive and uninformative responses (Wei et al., 2017). As seen in Figure 1, candidate A is a typical generic response given the dialog context. In order to deal with this problem, previous work proposed grounding generated responses on knowledge sentences related to the dialog context (Ghazvininejad et al., 2018; Yavuz et al., 2019; Zhou et al., 2018; Dinan et al., 2018; Gopalakrishnan et al., 2019). To improve the diversity of generated responses, others proposed conditioning response generation on latent (Serban et al., 2016, 2017; Shen et al., 2017; Zhao et al., 2017; Xing et al., 2016) or discrete attributes (Sankar and Ravi, 2019; Li et al., 2016a; See et al., 2019; Serban et al., 2017). These discrete attributes are typically presented to the decoder at the turn level, and are not associated with a specific segment of the output. Another issue with seq2seq approaches is that, due to the lack of explicit control mechanisms, the style of these responses does not always match with what would be suggested by user experience experts. For example, the generated response may not acknowledge what the user just said, or may 412 Proceedings of The 13th Internati"
2020.lrec-1.648,N19-1078,0,0.0156419,"ts are initially tokenized using a custom rule-based tokenizer with a large number of postprocessing rules based on spot-checking of common mistakes in the data. For example, the tokenizer handles typical sequences of expressions for phone numbers and opening times used in travel guides from Wikivoyage, which out-of-the-box tokenizers often split incorrectly. For POS tagging we train an ensemble model. This model takes 4 models’ tag predictions as input, fits an XGBoost model (Chen and Guestrin, 2016) to them and then predicts the final tag of the tokens. The 4 models we use here are Flair’s (Akbik et al., 2019) default POS tagging model which is trained on OntoNotes, Flair re-trained on GUM, StanfordNLP (Qi et al., 2018) pretrained on EWT, and StanfordNLP re-trained on GUM.3 Since we need training data for both re-training the models and training the ensemble model, we split GUM’s train set into 5 folds. Each time, we use 4 of the folds to re-train Flair and StanfordNLP and then make predictions on the remaining fold. The predictions on all these 5 folds, together with Flair OntoNotes and StanfordNLP EWT predictions, constitute the training data for the ensemble model. At test time, the four base mo"
2020.lrec-1.648,asheghi-etal-2014-designing,0,0.0254717,"tuned on a smaller gold dataset Testing these applications is outside the scope of the current paper, though several papers suggest that large scale ‘silver quality’ data can be better for training tools than smaller gold standard resources for POS tagging (Schulz and Ketschik, 2019), parsing (Toutanova, 2005) using information from parse banks (Charniak et al., 2000), discourse relation classification (Marcu and Echihabi, 2002), and other tasks. Moreover, due to its genre diversity, this corpus can be useful for genre studies as well as automatic text type identification in Web corpora (e.g. Asheghi et al. (2014), 5267 Rehm et al. (2008), and Dalan and Sharoff (2016)). We plan to pursue several of these approaches in future work (see Section 5). Our main contributions in this paper consist in (1) presenting a genre-balanced, richly-annotated web corpus which is more than double (and in some cases 20 times) the size of the largest similar gold annotated resources; (2) evaluating tailored NLP tools which have been trained specifically for this task and comparing them to off-the-shelf tools; and (3) demonstrating the added value of rich annotations and ensembling of multiple concurrent tools, which impro"
2020.lrec-1.648,D16-1239,0,0.0285539,"e correct (cf. Derczynski et al. (2012)), or use other similar approaches such as tri-training (cf. Zhou and Li (2005)) 1. The study of systematic genre variation is limited to surface textual properties and cannot use complex annotations, unless it is confined to very limited data • Human-in-the-loop/crowdsourcing – outputting sentences with low NLP tool confidence for expert correction or crowd worker annotations, or using a hybrid annotation setup in which several models are used, and the tags that the models agree on are only reviewed by annotators and the rest are annotated from scratch (Berzak et al., 2016) 2. For large resources, NLP quality is low since opportunistically-acquired content often deviates substantially from the language found in training data used by out-of-the-box tools 3. Large open data sets for training NLP tools on complex phenomena (e.g. coreference resolution, discourse parsing) are unavailable1 In this paper, we attempt to walk a middle path, combining some of the best features of corpus data harvested from the web – size, open licenses, lexical diversity – and data collected using a carefully defined sampling frame (cf. Hundt (2008)), which allows for more interpretable"
2020.lrec-1.648,W16-2611,0,0.0256897,"ations is outside the scope of the current paper, though several papers suggest that large scale ‘silver quality’ data can be better for training tools than smaller gold standard resources for POS tagging (Schulz and Ketschik, 2019), parsing (Toutanova, 2005) using information from parse banks (Charniak et al., 2000), discourse relation classification (Marcu and Echihabi, 2002), and other tasks. Moreover, due to its genre diversity, this corpus can be useful for genre studies as well as automatic text type identification in Web corpora (e.g. Asheghi et al. (2014), 5267 Rehm et al. (2008), and Dalan and Sharoff (2016)). We plan to pursue several of these approaches in future work (see Section 5). Our main contributions in this paper consist in (1) presenting a genre-balanced, richly-annotated web corpus which is more than double (and in some cases 20 times) the size of the largest similar gold annotated resources; (2) evaluating tailored NLP tools which have been trained specifically for this task and comparing them to off-the-shelf tools; and (3) demonstrating the added value of rich annotations and ensembling of multiple concurrent tools, which improve the accuracy of each task by incorporating informati"
2020.lrec-1.648,P10-2013,0,0.0994841,"Missing"
2020.lrec-1.648,P14-1002,0,0.0274335,"mbination of the knowledge-driven system, xrenner (Zeldes and Zhang, 2016), for in-vocabulary items and on neural sequence labeling using large trainable contextualized BERT embeddings (Devlin et al., 2018) for ambiguous or out of vocabulary entities. Coreference resolution is done based on linguistic features using xrenner’s XGBoost implementation which is more robust than state of the art lexicalized neural coreference architectures for our data (see Section 4 for a comparison of systems). Discourse Parsing For discourse parsing we use a modified version of the freely available DPLP parser (Ji and Eisenstein, 2014), which is known to perform at or close to SOTA for English (Morey et al., 2017) on a number of metrics. Since we have access to additional annotation layers, we featurize structural markup (paragraphs, headings, lists, etc.) and add parse-based sentence types and the genre as features in order to boost out-of-the-box performance. The parser operates on automatically segmented discourse units generated by ToNy (Muller et al., 2019), a discourse segmenter trained specifically on the GUM data. The resulting RST trees are constrained to use the parsed sentence splits as maximal units: no elementa"
2020.lrec-1.648,D19-1588,0,0.0258622,"Missing"
2020.lrec-1.648,D16-1180,0,0.0151032,"w StanfordNLP compares to other available libraries, such as Spacy (https://spacy.io/). While we do not have up to date numbers for Spacy, which was not featured in the recent CoNLL shared task on Universal Dependencies parsing, the most recent numbers reported in (Zeldes and Simonson, 2016) do not suggest that it would outperform StanfordNLP. 5269 and POS tags from the previous components, outperforming StanfordNLP’s base accuracy (see Section 4). No ensembling was done at this stage, though we are considering the application of parser interpolation or ensembling to this task in future work (Kuncoro et al., 2016). Coreference and Entity Resolution Following GUM’s annotation scheme, we attempt to classify all entity mentions in the corpus, including nested non-named and pronominal entities, requiring coreference resolution. Since our gold standard training data is limited in size, we rely on a combination of the knowledge-driven system, xrenner (Zeldes and Zhang, 2016), for in-vocabulary items and on neural sequence labeling using large trainable contextualized BERT embeddings (Devlin et al., 2018) for ambiguous or out of vocabulary entities. Coreference resolution is done based on linguistic features"
2020.lrec-1.648,N18-2108,0,0.0670917,"Missing"
2020.lrec-1.648,P02-1047,0,0.015351,"on words and are only available for purchase from the LDC. • Pre-training – the error-prone but large automatically produced corpus can be used for pre-training a model which can later be fine-tuned on a smaller gold dataset Testing these applications is outside the scope of the current paper, though several papers suggest that large scale ‘silver quality’ data can be better for training tools than smaller gold standard resources for POS tagging (Schulz and Ketschik, 2019), parsing (Toutanova, 2005) using information from parse banks (Charniak et al., 2000), discourse relation classification (Marcu and Echihabi, 2002), and other tasks. Moreover, due to its genre diversity, this corpus can be useful for genre studies as well as automatic text type identification in Web corpora (e.g. Asheghi et al. (2014), 5267 Rehm et al. (2008), and Dalan and Sharoff (2016)). We plan to pursue several of these approaches in future work (see Section 5). Our main contributions in this paper consist in (1) presenting a genre-balanced, richly-annotated web corpus which is more than double (and in some cases 20 times) the size of the largest similar gold annotated resources; (2) evaluating tailored NLP tools which have been tra"
2020.lrec-1.648,D17-1136,0,0.0162404,"cabulary items and on neural sequence labeling using large trainable contextualized BERT embeddings (Devlin et al., 2018) for ambiguous or out of vocabulary entities. Coreference resolution is done based on linguistic features using xrenner’s XGBoost implementation which is more robust than state of the art lexicalized neural coreference architectures for our data (see Section 4 for a comparison of systems). Discourse Parsing For discourse parsing we use a modified version of the freely available DPLP parser (Ji and Eisenstein, 2014), which is known to perform at or close to SOTA for English (Morey et al., 2017) on a number of metrics. Since we have access to additional annotation layers, we featurize structural markup (paragraphs, headings, lists, etc.) and add parse-based sentence types and the genre as features in order to boost out-of-the-box performance. The parser operates on automatically segmented discourse units generated by ToNy (Muller et al., 2019), a discourse segmenter trained specifically on the GUM data. The resulting RST trees are constrained to use the parsed sentence splits as maximal units: no elementary discourse unit (EDU) is permitted to be larger than one sentence, and heading"
2020.lrec-1.648,W19-2715,0,0.0180313,"for our data (see Section 4 for a comparison of systems). Discourse Parsing For discourse parsing we use a modified version of the freely available DPLP parser (Ji and Eisenstein, 2014), which is known to perform at or close to SOTA for English (Morey et al., 2017) on a number of metrics. Since we have access to additional annotation layers, we featurize structural markup (paragraphs, headings, lists, etc.) and add parse-based sentence types and the genre as features in order to boost out-of-the-box performance. The parser operates on automatically segmented discourse units generated by ToNy (Muller et al., 2019), a discourse segmenter trained specifically on the GUM data. The resulting RST trees are constrained to use the parsed sentence splits as maximal units: no elementary discourse unit (EDU) is permitted to be larger than one sentence, and headings, captions, and speaker “turns” (in Reddit) are guaranteed to be split from their environment thanks to the structural markup. We also feed our system the predicted discourse function labels from a Flair sentence classifier trained on RST-DT and out-of-the-box sentiment and subjectivity scores using TextBlob’s4 pretrained model as features. 4. Evaluati"
2020.lrec-1.648,rehm-etal-2008-towards,0,0.0527644,"set Testing these applications is outside the scope of the current paper, though several papers suggest that large scale ‘silver quality’ data can be better for training tools than smaller gold standard resources for POS tagging (Schulz and Ketschik, 2019), parsing (Toutanova, 2005) using information from parse banks (Charniak et al., 2000), discourse relation classification (Marcu and Echihabi, 2002), and other tasks. Moreover, due to its genre diversity, this corpus can be useful for genre studies as well as automatic text type identification in Web corpora (e.g. Asheghi et al. (2014), 5267 Rehm et al. (2008), and Dalan and Sharoff (2016)). We plan to pursue several of these approaches in future work (see Section 5). Our main contributions in this paper consist in (1) presenting a genre-balanced, richly-annotated web corpus which is more than double (and in some cases 20 times) the size of the largest similar gold annotated resources; (2) evaluating tailored NLP tools which have been trained specifically for this task and comparing them to off-the-shelf tools; and (3) demonstrating the added value of rich annotations and ensembling of multiple concurrent tools, which improve the accuracy of each t"
2020.lrec-1.648,W19-2717,1,0.353582,"models and training the ensemble model, we split GUM’s train set into 5 folds. Each time, we use 4 of the folds to re-train Flair and StanfordNLP and then make predictions on the remaining fold. The predictions on all these 5 folds, together with Flair OntoNotes and StanfordNLP EWT predictions, constitute the training data for the ensemble model. At test time, the four base models are run in parallel and their outputs are fed into the ensemble classifier in a pipeline. This approach improves the accuracy of the POS tagger, which in turn gets used by the machine learning sentence splitter from Yu et al. (2019), which was retrained on our genres and incorporated a number of features, including POS tags, as well as the dependency parser (see below). Sentence boundaries from the sentencer splitter are also used as a basis for syntactic parsing, sentence type annotation, and maximal units for discourse parsing (see below). Dependency Parsing Universal Dependency parses and morphological features are extracted using StanfordNLP, which was retrained on our genres using the latest GUM data, and configured to use the high-accuracy tokenization 3 One reviewer has asked how StanfordNLP compares to other avai"
2020.lrec-1.648,W16-1709,1,0.828201,"rsing, sentence type annotation, and maximal units for discourse parsing (see below). Dependency Parsing Universal Dependency parses and morphological features are extracted using StanfordNLP, which was retrained on our genres using the latest GUM data, and configured to use the high-accuracy tokenization 3 One reviewer has asked how StanfordNLP compares to other available libraries, such as Spacy (https://spacy.io/). While we do not have up to date numbers for Spacy, which was not featured in the recent CoNLL shared task on Universal Dependencies parsing, the most recent numbers reported in (Zeldes and Simonson, 2016) do not suggest that it would outperform StanfordNLP. 5269 and POS tags from the previous components, outperforming StanfordNLP’s base accuracy (see Section 4). No ensembling was done at this stage, though we are considering the application of parser interpolation or ensembling to this task in future work (Kuncoro et al., 2016). Coreference and Entity Resolution Following GUM’s annotation scheme, we attempt to classify all entity mentions in the corpus, including nested non-named and pronominal entities, requiring coreference resolution. Since our gold standard training data is limited in size"
2020.lrec-1.648,W16-0713,1,0.872119,"ags from the previous components, outperforming StanfordNLP’s base accuracy (see Section 4). No ensembling was done at this stage, though we are considering the application of parser interpolation or ensembling to this task in future work (Kuncoro et al., 2016). Coreference and Entity Resolution Following GUM’s annotation scheme, we attempt to classify all entity mentions in the corpus, including nested non-named and pronominal entities, requiring coreference resolution. Since our gold standard training data is limited in size, we rely on a combination of the knowledge-driven system, xrenner (Zeldes and Zhang, 2016), for in-vocabulary items and on neural sequence labeling using large trainable contextualized BERT embeddings (Devlin et al., 2018) for ambiguous or out of vocabulary entities. Coreference resolution is done based on linguistic features using xrenner’s XGBoost implementation which is more robust than state of the art lexicalized neural coreference architectures for our data (see Section 4 for a comparison of systems). Discourse Parsing For discourse parsing we use a modified version of the freely available DPLP parser (Ji and Eisenstein, 2014), which is known to perform at or close to SOTA fo"
2020.lrec-1.648,K18-2001,0,0.0528279,"Missing"
2020.lrec-1.733,W07-1604,0,0.0518592,"a Mandarin translation of The Little Prince, we achieve high inter-annotator agreement and analyze semantic correspondences of adposition tokens in bitext. Keywords: adpositions, supersenses, Mandarin Chinese, corpus, annotation 1. Introduction Adpositions (i.e. prepositions and postpositions) include some of the most frequent words in languages like Chinese and English, and help convey a myriad of semantic relations of space, time, causality, possession, and other domains of meaning. They are also a persistent thorn in the side of second language learners owing to their extreme idiosyncrasy (Chodorow et al., 2007; Lorincz and Gordon, 2012). For instance, the English word in has no exact parallel in another language; rather, for purposes of translation, its many different usages cluster differently depending on the second language. Semantically annotated corpora of adpositions in multiple languages, including parallel data, would facilitate broader empirical study of adposition variation than is possible today, and could also contribute to NLP applications such as machine translation (Li et al., 2005; Agirre et al., 2009; Shilon et al., 2012; Weller et al., 2014, 2015; Hashemi and Hwa, 2014; Popovi´c,"
2020.lrec-1.733,hashemi-hwa-2014-comparison,0,0.0250065,"iosyncrasy (Chodorow et al., 2007; Lorincz and Gordon, 2012). For instance, the English word in has no exact parallel in another language; rather, for purposes of translation, its many different usages cluster differently depending on the second language. Semantically annotated corpora of adpositions in multiple languages, including parallel data, would facilitate broader empirical study of adposition variation than is possible today, and could also contribute to NLP applications such as machine translation (Li et al., 2005; Agirre et al., 2009; Shilon et al., 2012; Weller et al., 2014, 2015; Hashemi and Hwa, 2014; Popovi´c, 2017) and grammatical error correction (Chodorow et al., 2007; Tetreault and Chodorow, 2008; De Felice and Pulman, 2008; Hermet and Alain, 2009; Huang et al., 2016; Graën and Schneider, 2017). This paper describes the first corpus with broad-coverage annotation of adpositions in Chinese. For this corpus we have adapted Schneider et al.’s (2018) Semantic Network of Adposition and Case Supersenses annotation scheme (SNACS; see §2.2) to Chinese.1 Though other languages were taken into consideration in designing SNACS, no serious annotation effort has been undertaken to confirm empiric"
2020.lrec-1.733,2009.eamt-1.9,0,0.0890222,"Missing"
2020.lrec-1.733,W13-2322,1,0.88058,"with the main predicate of the clause and introduce an NP argument to the clause (Li and Thompson, 1974) as in (4). These tokens are referred to as coverbs. In some cases, coverbs can also occur as the main predicate. For example, the coverb zài heads the predicate phrase in (5). (4) (5) t¯a zài:L OCUS xuéshù 3 SG P:at academia shàng:T OPIC↝L OCUS yˇousuˇozuòwéi. LC :on-top-of successful ‘He succeeded in academia.’ 4.1. Preprocessing We use the same Chinese translation of The Little Prince as the Chinese AMR corpus (Li et al., 2016), which is also sentence-aligned with the English AMR corpus (Banarescu et al., 2013). These bitext annotations in multiple languages and annotation semantic frameworks can facilitate crossframework comparisons. Prior to supersense annotation, we conducted the following preprocessing steps in order to identify the adposition targets that merit supersense annotation. nˇı yào de yáng jiù zài lˇımiàn. 2 SG want DE sheep RES at inside ‘The sheep you wanted is in the box.’ (zh_lpp_1943.92) In this project, we only annotate coverbs when they do not function as the main predicate in the sentence, echoing the view that coverbs modify events introduced by the predicates, rather than es"
2020.lrec-1.733,W09-2110,0,0.0111112,"poses of translation, its many different usages cluster differently depending on the second language. Semantically annotated corpora of adpositions in multiple languages, including parallel data, would facilitate broader empirical study of adposition variation than is possible today, and could also contribute to NLP applications such as machine translation (Li et al., 2005; Agirre et al., 2009; Shilon et al., 2012; Weller et al., 2014, 2015; Hashemi and Hwa, 2014; Popovi´c, 2017) and grammatical error correction (Chodorow et al., 2007; Tetreault and Chodorow, 2008; De Felice and Pulman, 2008; Hermet and Alain, 2009; Huang et al., 2016; Graën and Schneider, 2017). This paper describes the first corpus with broad-coverage annotation of adpositions in Chinese. For this corpus we have adapted Schneider et al.’s (2018) Semantic Network of Adposition and Case Supersenses annotation scheme (SNACS; see §2.2) to Chinese.1 Though other languages were taken into consideration in designing SNACS, no serious annotation effort has been undertaken to confirm empirically that it generalizes to other languages. After developing new guidelines for syntactic phenomena in Chinese (§3), we apply the SNACS supersenses to a t"
2020.lrec-1.733,C16-1085,0,0.0224686,"s many different usages cluster differently depending on the second language. Semantically annotated corpora of adpositions in multiple languages, including parallel data, would facilitate broader empirical study of adposition variation than is possible today, and could also contribute to NLP applications such as machine translation (Li et al., 2005; Agirre et al., 2009; Shilon et al., 2012; Weller et al., 2014, 2015; Hashemi and Hwa, 2014; Popovi´c, 2017) and grammatical error correction (Chodorow et al., 2007; Tetreault and Chodorow, 2008; De Felice and Pulman, 2008; Hermet and Alain, 2009; Huang et al., 2016; Graën and Schneider, 2017). This paper describes the first corpus with broad-coverage annotation of adpositions in Chinese. For this corpus we have adapted Schneider et al.’s (2018) Semantic Network of Adposition and Case Supersenses annotation scheme (SNACS; see §2.2) to Chinese.1 Though other languages were taken into consideration in designing SNACS, no serious annotation effort has been undertaken to confirm empirically that it generalizes to other languages. After developing new guidelines for syntactic phenomena in Chinese (§3), we apply the SNACS supersenses to a translation of The Li"
2020.lrec-1.733,S17-1022,1,0.640534,"Missing"
2020.lrec-1.733,W16-1702,0,0.339033,"st adaptation of SNACS for Chinese by annotating an entire Chinese translation of The Little Prince. 2.1. Chinese Adpositions and Roles In the computational literature for Chinese, apart from some focused studies (e.g., Yang and Kuo (1998) on logicalsemantic representation of temporal adpositions), there has been little work addressing adpositions specifically. Most previous semantic projects for Mandarin Chinese focused on content words and did not directly annotate the semantic relations signaled by functions words such as prepositions (Xue et al., 2014; Hao et al., 2007; You and Liu, 2005; Li et al., 2016). For example, in Chinese PropBank, Xue (2008) argued that the head word and its part of speech are clearly informative for labeling the semantic role of a phrase, but the preposition is not always the most informative element. Li et al. (2003) annotated the Tsinghua Corpus (Zhang, 1999) from People’s Daily where the content words were 5986 3 https://github.com/nert-nlp/Chinese-SNACS/ selected as the headwords, i.e., the object is the headword of the prepositional phrase. In these prepositional phrases, the nominal headwords were labeled with one of the 59 semantic relations (e.g. Location, Lo"
2020.lrec-1.733,W03-1712,0,0.329789,"ogicalsemantic representation of temporal adpositions), there has been little work addressing adpositions specifically. Most previous semantic projects for Mandarin Chinese focused on content words and did not directly annotate the semantic relations signaled by functions words such as prepositions (Xue et al., 2014; Hao et al., 2007; You and Liu, 2005; Li et al., 2016). For example, in Chinese PropBank, Xue (2008) argued that the head word and its part of speech are clearly informative for labeling the semantic role of a phrase, but the preposition is not always the most informative element. Li et al. (2003) annotated the Tsinghua Corpus (Zhang, 1999) from People’s Daily where the content words were 5986 3 https://github.com/nert-nlp/Chinese-SNACS/ selected as the headwords, i.e., the object is the headword of the prepositional phrase. In these prepositional phrases, the nominal headwords were labeled with one of the 59 semantic relations (e.g. Location, LocationIni, Kernel word) whereas the prepositions and postpositions were respectively labeled with syntactic relations Preposition and LocationPreposition.4 Similarly, in Semantic Dependency Relations (SDR, Che et al. 2012, 2016), prepositions a"
2020.lrec-1.733,P14-1120,0,0.165666,"Petit Prince by Antoine de St. Exupéry, published in 1943 and subsequently translated into numerous languages. 2 corpus, and compare to adposition behavior in a separate English corpus (see §5). We also examine the predictions of a part-of-speech tagger in relation to our criteria for annotation targets (§6). The annotated corpus and the Chinese guidelines for SNACS will be made freely available online.3 2. Related Work To date, most wide-coverage semantic annotation of prepositions has been dictionary-based, taking a word sense disambiguation perspective (Litkowski and Hargraves, 2005, 2007; Litkowski, 2014). Schneider et al. (2015) proposed a supersense-based (unlexicalized) semantic annotation scheme which would be applied to all tokens of prepositions in English text. We adopt a revised version of the approach, known as SNACS (see §2.2). Previous SNACS annotation efforts have been mostly focused on English— particularly STREUSLE (Schneider et al., 2016, 2018), the semantically annotated corpus of reviews from the English Web Treebank (EWT; Bies et al., 2012). We present the first adaptation of SNACS for Chinese by annotating an entire Chinese translation of The Little Prince. 2.1. Chinese Adpo"
2020.lrec-1.733,K18-2016,0,0.0405455,"Missing"
2020.lrec-1.733,W16-1712,1,0.855068,"guidelines for SNACS will be made freely available online.3 2. Related Work To date, most wide-coverage semantic annotation of prepositions has been dictionary-based, taking a word sense disambiguation perspective (Litkowski and Hargraves, 2005, 2007; Litkowski, 2014). Schneider et al. (2015) proposed a supersense-based (unlexicalized) semantic annotation scheme which would be applied to all tokens of prepositions in English text. We adopt a revised version of the approach, known as SNACS (see §2.2). Previous SNACS annotation efforts have been mostly focused on English— particularly STREUSLE (Schneider et al., 2016, 2018), the semantically annotated corpus of reviews from the English Web Treebank (EWT; Bies et al., 2012). We present the first adaptation of SNACS for Chinese by annotating an entire Chinese translation of The Little Prince. 2.1. Chinese Adpositions and Roles In the computational literature for Chinese, apart from some focused studies (e.g., Yang and Kuo (1998) on logicalsemantic representation of temporal adpositions), there has been little work addressing adpositions specifically. Most previous semantic projects for Mandarin Chinese focused on content words and did not directly annotate"
2020.lrec-1.733,P18-1018,1,0.830137,"reover, there is a dearth of annotated corpora for investigating the cross-linguistic variation of adposition semantics, or for building multilingual disambiguation systems. This paper presents a corpus in which all adpositions have been semantically annotated in Mandarin Chinese; to the best of our knowledge, this is the first Chinese corpus to be broadly annotated with adposition semantics. Our approach adapts a framework that defined a general set of supersenses according to ostensibly language-independent semantic criteria, though its development focused primarily on English prepositions (Schneider et al., 2018). We find that the supersense categories are well-suited to Chinese adpositions despite syntactic differences from English. On a Mandarin translation of The Little Prince, we achieve high inter-annotator agreement and analyze semantic correspondences of adposition tokens in bitext. Keywords: adpositions, supersenses, Mandarin Chinese, corpus, annotation 1. Introduction Adpositions (i.e. prepositions and postpositions) include some of the most frequent words in languages like Chinese and English, and help convey a myriad of semantic relations of space, time, causality, possession, and other dom"
2020.lrec-1.733,N15-1177,1,0.916719,"Missing"
2020.lrec-1.733,W15-1612,1,0.846978,"toine de St. Exupéry, published in 1943 and subsequently translated into numerous languages. 2 corpus, and compare to adposition behavior in a separate English corpus (see §5). We also examine the predictions of a part-of-speech tagger in relation to our criteria for annotation targets (§6). The annotated corpus and the Chinese guidelines for SNACS will be made freely available online.3 2. Related Work To date, most wide-coverage semantic annotation of prepositions has been dictionary-based, taking a word sense disambiguation perspective (Litkowski and Hargraves, 2005, 2007; Litkowski, 2014). Schneider et al. (2015) proposed a supersense-based (unlexicalized) semantic annotation scheme which would be applied to all tokens of prepositions in English text. We adopt a revised version of the approach, known as SNACS (see §2.2). Previous SNACS annotation efforts have been mostly focused on English— particularly STREUSLE (Schneider et al., 2016, 2018), the semantically annotated corpus of reviews from the English Web Treebank (EWT; Bies et al., 2012). We present the first adaptation of SNACS for Chinese by annotating an entire Chinese translation of The Little Prince. 2.1. Chinese Adpositions and Roles In the"
2020.lrec-1.733,S07-1005,0,0.147385,"Missing"
2020.lrec-1.733,W12-0514,0,0.0309314,"cond language learners owing to their extreme idiosyncrasy (Chodorow et al., 2007; Lorincz and Gordon, 2012). For instance, the English word in has no exact parallel in another language; rather, for purposes of translation, its many different usages cluster differently depending on the second language. Semantically annotated corpora of adpositions in multiple languages, including parallel data, would facilitate broader empirical study of adposition variation than is possible today, and could also contribute to NLP applications such as machine translation (Li et al., 2005; Agirre et al., 2009; Shilon et al., 2012; Weller et al., 2014, 2015; Hashemi and Hwa, 2014; Popovi´c, 2017) and grammatical error correction (Chodorow et al., 2007; Tetreault and Chodorow, 2008; De Felice and Pulman, 2008; Hermet and Alain, 2009; Huang et al., 2016; Graën and Schneider, 2017). This paper describes the first corpus with broad-coverage annotation of adpositions in Chinese. For this corpus we have adapted Schneider et al.’s (2018) Semantic Network of Adposition and Case Supersenses annotation scheme (SNACS; see §2.2) to Chinese.1 Though other languages were taken into consideration in designing SNACS, no serious annota"
2020.lrec-1.733,N04-1032,0,0.11466,"fit the needs of the scene in actual language use. (3) I care about:S TIMULUS↝T OPIC you. For instance, (3) blends the domains of emotion (principally 4 Though named LocationPreposition in Li et al. (2003), these adpositions actually occur postnominally, equivalent to localizers in this paper. 5 Throughout this paper, adposition tokens under discussion are bolded and labeled. Causer Agent StartTime EndTime Co-Agent Theme Configuration Identity Species Gestalt Possessor Frequency Co-Theme Duration Topic Characteristic Stimulus Possession Experiencer PartPortion Interval Locus Source Goal Path Sun and Jurafsky (2004) compared PropBank parsing performance on Chinese and English, and showed that four Chinese prepositions (zài, yú, bˇı, and duì) are among the top 20 lexicalized syntactic head words in Chinese PropBank, bridging the connections between verbs and their arguments. The high frequency of prepositions as head words in PropBank reflects their importance in context. However, very few annotation scheme attempted to directly label the semantics of these adposition words. Chinese Knowledge Information Processing Group (CKIP) (1993) is the most relevant adposition annotation effort, categorizing Chinese"
2020.lrec-1.733,L16-1262,0,0.134227,"Missing"
2020.lrec-1.733,C08-1109,0,0.0251851,"has no exact parallel in another language; rather, for purposes of translation, its many different usages cluster differently depending on the second language. Semantically annotated corpora of adpositions in multiple languages, including parallel data, would facilitate broader empirical study of adposition variation than is possible today, and could also contribute to NLP applications such as machine translation (Li et al., 2005; Agirre et al., 2009; Shilon et al., 2012; Weller et al., 2014, 2015; Hashemi and Hwa, 2014; Popovi´c, 2017) and grammatical error correction (Chodorow et al., 2007; Tetreault and Chodorow, 2008; De Felice and Pulman, 2008; Hermet and Alain, 2009; Huang et al., 2016; Graën and Schneider, 2017). This paper describes the first corpus with broad-coverage annotation of adpositions in Chinese. For this corpus we have adapted Schneider et al.’s (2018) Semantic Network of Adposition and Case Supersenses annotation scheme (SNACS; see §2.2) to Chinese.1 Though other languages were taken into consideration in designing SNACS, no serious annotation effort has been undertaken to confirm empirically that it generalizes to other languages. After developing new guidelines for syntactic phenomena in"
2020.lrec-1.733,W15-4923,0,0.0309798,"Missing"
2020.lrec-1.733,2014.amta-researchers.21,0,0.0298506,"s owing to their extreme idiosyncrasy (Chodorow et al., 2007; Lorincz and Gordon, 2012). For instance, the English word in has no exact parallel in another language; rather, for purposes of translation, its many different usages cluster differently depending on the second language. Semantically annotated corpora of adpositions in multiple languages, including parallel data, would facilitate broader empirical study of adposition variation than is possible today, and could also contribute to NLP applications such as machine translation (Li et al., 2005; Agirre et al., 2009; Shilon et al., 2012; Weller et al., 2014, 2015; Hashemi and Hwa, 2014; Popovi´c, 2017) and grammatical error correction (Chodorow et al., 2007; Tetreault and Chodorow, 2008; De Felice and Pulman, 2008; Hermet and Alain, 2009; Huang et al., 2016; Graën and Schneider, 2017). This paper describes the first corpus with broad-coverage annotation of adpositions in Chinese. For this corpus we have adapted Schneider et al.’s (2018) Semantic Network of Adposition and Case Supersenses annotation scheme (SNACS; see §2.2) to Chinese.1 Though other languages were taken into consideration in designing SNACS, no serious annotation effort has been"
2020.lrec-1.733,J08-2004,0,0.0466704,"entire Chinese translation of The Little Prince. 2.1. Chinese Adpositions and Roles In the computational literature for Chinese, apart from some focused studies (e.g., Yang and Kuo (1998) on logicalsemantic representation of temporal adpositions), there has been little work addressing adpositions specifically. Most previous semantic projects for Mandarin Chinese focused on content words and did not directly annotate the semantic relations signaled by functions words such as prepositions (Xue et al., 2014; Hao et al., 2007; You and Liu, 2005; Li et al., 2016). For example, in Chinese PropBank, Xue (2008) argued that the head word and its part of speech are clearly informative for labeling the semantic role of a phrase, but the preposition is not always the most informative element. Li et al. (2003) annotated the Tsinghua Corpus (Zhang, 1999) from People’s Daily where the content words were 5986 3 https://github.com/nert-nlp/Chinese-SNACS/ selected as the headwords, i.e., the object is the headword of the prepositional phrase. In these prepositional phrases, the nominal headwords were labeled with one of the 59 semantic relations (e.g. Location, LocationIni, Kernel word) whereas the prepositio"
2020.lrec-1.733,xue-etal-2014-interlingua,0,0.0669534,"Missing"
2020.lrec-1.733,Y98-1003,0,0.164563,"would be applied to all tokens of prepositions in English text. We adopt a revised version of the approach, known as SNACS (see §2.2). Previous SNACS annotation efforts have been mostly focused on English— particularly STREUSLE (Schneider et al., 2016, 2018), the semantically annotated corpus of reviews from the English Web Treebank (EWT; Bies et al., 2012). We present the first adaptation of SNACS for Chinese by annotating an entire Chinese translation of The Little Prince. 2.1. Chinese Adpositions and Roles In the computational literature for Chinese, apart from some focused studies (e.g., Yang and Kuo (1998) on logicalsemantic representation of temporal adpositions), there has been little work addressing adpositions specifically. Most previous semantic projects for Mandarin Chinese focused on content words and did not directly annotate the semantic relations signaled by functions words such as prepositions (Xue et al., 2014; Hao et al., 2007; You and Liu, 2005; Li et al., 2016). For example, in Chinese PropBank, Xue (2008) argued that the head word and its part of speech are clearly informative for labeling the semantic role of a phrase, but the preposition is not always the most informative elem"
2020.lrec-1.733,C08-1022,0,\N,Missing
2020.lrec-1.733,W17-0303,0,\N,Missing
2020.lrec-1.733,S16-1167,0,\N,Missing
2020.lrec-1.733,S12-1050,0,\N,Missing
2020.nlptea-1.9,N19-1423,0,0.0456449,"Missing"
2020.nlptea-1.9,D19-1435,0,0.010946,"characterlevel 5-gram language model (denoted by L in the following) is trained based on 30 million Chinese sentences. Then L is used to select the k most appropriate candidate words from a large set of candidates, which is extracted from the CGED training data, to replace the words in the original sentences according to the error type and position in the detection phase. Finally, the candidates along with those generated by Seq2Seq models and GECToR models are all sent to the post-processing module to obtain the final output. GECToR Models. Similar to the Parallel Iterative Edit (PIE) model (Awasthi et al., 2019), GECToR (Omelianchuk et al., 2020) treats the GEC task as a sequential labeling problem. The core of the approach is the design of special output tags, which indicate the differences between source sentences and target sentences. In order to obtain the tags, minimal edits of the characters are firstly extracted based on the modified Levenshtein distance. Then the edits are converted to the following tags: 4.4 Post-processing Post-processing Outputs of GED Models. Considering that one input token is allowed to be labeled as multiple error types depending on the actual situation, we propose to"
2020.nlptea-1.9,W18-3707,0,0.10084,"and third in the TOP1 correction track, also with the highest precision. Ours are among the top 4 to 6 in other tracks, except for FPR where we rank 12. And our system achieves the highest precisions among the top 10 submissions at IDENTIFICATION and POSITION tracks. The rest of this paper is organized as follows: A brief description of the CGED shared task is given in Section 2, followed by an overview of prior work in Section 3. Section 4 introduces our system in detail, and Section 5 demonstrates the experimental results. Finally, Section 6 concludes this paper. 2 2017; Liao et al., 2017; Fu et al., 2018b; Zhang et al., 2018; Li et al., 2018). Performance of these approaches are usually highly dependent on the handcrafted features fed into the LSTM layer. Yang et al. (2017) extracted features including characters, character-level bi-gram, Part-of-Speech (POS), POS scores, adjacent and dependent word collocations. Later in 2018, the feature sets were further enlarged by incorporating new features like word segmentation and Gaussian exact Point-wise Mutual Information (ePMI, Fu et al., 2018b). Grammatical Error Correction. Unlike the GED tasks, GEC tasks has been mostly treated as the machine t"
2020.nlptea-1.9,P18-1097,0,0.0240115,"s, adjacent and dependent word collocations. Later in 2018, the feature sets were further enlarged by incorporating new features like word segmentation and Gaussian exact Point-wise Mutual Information (ePMI, Fu et al., 2018b). Grammatical Error Correction. Unlike the GED tasks, GEC tasks has been mostly treated as the machine translation problem. To the best of our knowledge, the multi-layer convolutional neural network accompanied by a large language model (Chollampatt and Ng, 2018) is considered as the first Neural Machine Translation (NMT)-like approach to handle GEC tasks in English. Then Ge et al. (2018) and Grundkiewicz and JunczysDowmunt (2018); Fu et al. (2018b) proposed to use recurrent neural networks, while recent work (Junczys-Dowmunt et al., 2018; Grundkiewicz et al., 2019; Lichtarge et al., 2019; Fu et al., 2018a) made use of the Transformer (Vaswani et al., 2017). Specially, GECToR (Omelianchuk et al., 2020), which considered the English GEC task as a sequential labeling problem, has obtained competitive results to previous GEC systems. Task Description Generally, the CGED shared task classifies grammatical errors found in Chinese writings into four different classes, i.e., redundan"
2020.nlptea-1.9,W19-4406,0,0.0117888,"the TOP1 correction track, also with the highest precision. Ours are among the top 4 to 6 in other tracks, except for FPR where we rank 12. And our system achieves the highest precisions among the top 10 submissions at IDENTIFICATION and POSITION tracks. 1 Introduction With the rapid growth of online education platforms and the advance of natural language processing (NLP) techniques, recent years have seen an increased interest in automatic Grammatical Error Diagnosis (GED) and Grammatical Error Correction (GEC). Shared tasks such as CoNLL-2013, CoNLL-2014 and BEA-2019 (Ng et al., 2013, 2014; Bryant et al., 2019) were held to correct grammatical errors in essays written by learners of ∗ Equal contribution. 67 Proceedings of the 6th Workshop on Natural Language Processing Techniques for Educational Applications, pages 67–77 c Suzhou, China, December 4, 2020. 2020 Association for Computational Linguistics tasks. Our system performs well in the correction tracks: measured in F1 score, we rank first, with the highest precision, in the TOP3 correction track and third in the TOP1 correction track, also with the highest precision. Ours are among the top 4 to 6 in other tracks, except for FPR where we rank 12"
2020.nlptea-1.9,N18-2046,0,0.0502059,"Missing"
2020.nlptea-1.9,W19-4423,0,0.0152158,"P (y|X) = M 1 X Pk (y|X; θk ) . M Grammatical Error Correction (2) k=1 70 Input Output Input Output 因 (Y¯ın O 我 ( Wˇo O 为 w`ei O 不 b`u B-S ， , O 可 kˇe I-S 雾 w`u B-S 以 yˇı I-S 烟 y¯an I-S 找 zhˇao O 刺 c`ı O 到 d`ao O 激 j¯ı O 了 le B-M 就 ji`u O 在 z`ai B-S 对 du`ı B-W 哪 nˇa O 人 r´en I-W 里 lˇı O 体 tˇı I-W 我 wˇo B-R 会 hu`ı I-W 会 hu`ı I-R 有 yˇou O 买 mˇai O 危 w¯ei O 菜 c`ai O 害 h`ai O 。 .) O 。 .) O Table 2: Examples of BIO tags used in basic GED models. Sequences in the bracket are the corresponding transliterations. 2016; See et al., 2017) and subsequently revamped to handle GEC tasks (Zhao et al., 2019; Choe et al., 2019). Unlike the normal Transformers, copyaugmented Transformers are able to copy units (e.g. characters, sub-words, or words) from the source sentence, since the final probability distribution of a unit is the combination of a generative distribution and a copy distribution, balanced by a factor αcopy ∈ [0, 1]. With a larger copy factor, the output units tend to copy from the source rather than generating their own, and vice versa. data. It is also consistent with intuition since there exist commonly confused words or characters in Chinese. To make use of this observation, we propose a candidate"
2020.nlptea-1.9,W19-4427,0,0.0122574,"Point-wise Mutual Information (ePMI, Fu et al., 2018b). Grammatical Error Correction. Unlike the GED tasks, GEC tasks has been mostly treated as the machine translation problem. To the best of our knowledge, the multi-layer convolutional neural network accompanied by a large language model (Chollampatt and Ng, 2018) is considered as the first Neural Machine Translation (NMT)-like approach to handle GEC tasks in English. Then Ge et al. (2018) and Grundkiewicz and JunczysDowmunt (2018); Fu et al. (2018b) proposed to use recurrent neural networks, while recent work (Junczys-Dowmunt et al., 2018; Grundkiewicz et al., 2019; Lichtarge et al., 2019; Fu et al., 2018a) made use of the Transformer (Vaswani et al., 2017). Specially, GECToR (Omelianchuk et al., 2020), which considered the English GEC task as a sequential labeling problem, has obtained competitive results to previous GEC systems. Task Description Generally, the CGED shared task classifies grammatical errors found in Chinese writings into four different classes, i.e., redundant words (R), missing words (M), word selection errors (S), word ordering errors (W). Table 1 gives some examples of the errors, which are sampled from CGED 2020 training data. It s"
2020.nlptea-1.9,P16-1154,0,0.0609872,"Missing"
2020.nlptea-1.9,N18-1055,0,0.0217933,"gmentation and Gaussian exact Point-wise Mutual Information (ePMI, Fu et al., 2018b). Grammatical Error Correction. Unlike the GED tasks, GEC tasks has been mostly treated as the machine translation problem. To the best of our knowledge, the multi-layer convolutional neural network accompanied by a large language model (Chollampatt and Ng, 2018) is considered as the first Neural Machine Translation (NMT)-like approach to handle GEC tasks in English. Then Ge et al. (2018) and Grundkiewicz and JunczysDowmunt (2018); Fu et al. (2018b) proposed to use recurrent neural networks, while recent work (Junczys-Dowmunt et al., 2018; Grundkiewicz et al., 2019; Lichtarge et al., 2019; Fu et al., 2018a) made use of the Transformer (Vaswani et al., 2017). Specially, GECToR (Omelianchuk et al., 2020), which considered the English GEC task as a sequential labeling problem, has obtained competitive results to previous GEC systems. Task Description Generally, the CGED shared task classifies grammatical errors found in Chinese writings into four different classes, i.e., redundant words (R), missing words (M), word selection errors (S), word ordering errors (W). Table 1 gives some examples of the errors, which are sampled from CG"
2020.nlptea-1.9,P19-1601,0,0.0123828,"ce step. 4.3 As shown in Figure 1, the GEC framework consists of Seq2Seq GEC models, GECToR models, and a candidates generation module. Seq2Seq GEC Models. This work explores two kinds of Seq2Seq GEC models: one is the regular Transformer model (Vaswani et al., 2017), and the other is the copy augmented Transformer model (Zhao et al., 2019). The attention-based Transformer is the most widely used sequence transduction model in Natural Language Processing (NLP) area that are capable of a broad spectrum of tasks (Vaswani et al., 2017; Lample et al., 2018; Yang et al., 2019; Devlin et al., 2018; Dai et al., 2019), including machine translation, text style transfer, reading comprehension, etc. Transformers employ Seq2Seq structures that are usually built up by stacking encoder and decoder layers. Encoder layers consist of a multi-head self-attention layer followed by a position-wise feed-forward layer, while decoder layers consist of a multi-head self-attention layer, a multi-head cross-attention layer and a positionwise feed-forward layer. Residual connections and layer normalizations are used to improve the performance of deep Transformers. The copy-augmented Transformer was originally proposed for t"
2020.nlptea-1.9,J82-2005,0,0.644966,"Missing"
2020.nlptea-1.9,W13-3601,0,0.0290006,"ion track and third in the TOP1 correction track, also with the highest precision. Ours are among the top 4 to 6 in other tracks, except for FPR where we rank 12. And our system achieves the highest precisions among the top 10 submissions at IDENTIFICATION and POSITION tracks. 1 Introduction With the rapid growth of online education platforms and the advance of natural language processing (NLP) techniques, recent years have seen an increased interest in automatic Grammatical Error Diagnosis (GED) and Grammatical Error Correction (GEC). Shared tasks such as CoNLL-2013, CoNLL-2014 and BEA-2019 (Ng et al., 2013, 2014; Bryant et al., 2019) were held to correct grammatical errors in essays written by learners of ∗ Equal contribution. 67 Proceedings of the 6th Workshop on Natural Language Processing Techniques for Educational Applications, pages 67–77 c Suzhou, China, December 4, 2020. 2020 Association for Computational Linguistics tasks. Our system performs well in the correction tracks: measured in F1 score, we rank first, with the highest precision, in the TOP3 correction track and third in the TOP1 correction track, also with the highest precision. Ours are among the top 4 to 6 in other tracks, exc"
2020.nlptea-1.9,2020.bea-1.16,0,0.179069,"d as the machine translation problem. To the best of our knowledge, the multi-layer convolutional neural network accompanied by a large language model (Chollampatt and Ng, 2018) is considered as the first Neural Machine Translation (NMT)-like approach to handle GEC tasks in English. Then Ge et al. (2018) and Grundkiewicz and JunczysDowmunt (2018); Fu et al. (2018b) proposed to use recurrent neural networks, while recent work (Junczys-Dowmunt et al., 2018; Grundkiewicz et al., 2019; Lichtarge et al., 2019; Fu et al., 2018a) made use of the Transformer (Vaswani et al., 2017). Specially, GECToR (Omelianchuk et al., 2020), which considered the English GEC task as a sequential labeling problem, has obtained competitive results to previous GEC systems. Task Description Generally, the CGED shared task classifies grammatical errors found in Chinese writings into four different classes, i.e., redundant words (R), missing words (M), word selection errors (S), word ordering errors (W). Table 1 gives some examples of the errors, which are sampled from CGED 2020 training data. It should be noted that various error types may occur more than once in one sentence. System performance is measured at the following levels: •"
2020.nlptea-1.9,W18-3706,0,0.0400285,"Missing"
2020.nlptea-1.9,W15-4401,0,0.0210487,"le English has only 26 in total; Chinese uses tones to indicate various meanings, while English uses them to express emotions; Chinese emphasizes the meaning of expressions, usually resulting in short sentences without complex structure often seen in English. Due to the large number of complex characters and flexible sentence structures, Chinese is considered one of the most difficult languages in the world to learn. Under this circumstance, the workshop on Natural Language Processing Techniques for Educational Applications (NLP-TEA) has been organizing shared tasks for CGED (Yu et al., 2014; Lee et al., 2015, 2016; Rao et al., 2017, 2018) to help learners of Chinese as a Foreign Language (CFL) since 2014. The shared tasks provide common test conditions for researchers from both industry and academia. We believe they are very beneficial to advancing CGED technology. This paper introduces our work on this year’s CGED shared task. The task requires both error detection and correction, and we use a hybrid system to handle both. It uses as building blocks models designed for various NLP tasks, including BERTbased sequence labeling models, Seq2Seq, and GECToR. We tune our models to lean towards optimiz"
2020.nlptea-1.9,I17-4001,0,0.0194836,"n total; Chinese uses tones to indicate various meanings, while English uses them to express emotions; Chinese emphasizes the meaning of expressions, usually resulting in short sentences without complex structure often seen in English. Due to the large number of complex characters and flexible sentence structures, Chinese is considered one of the most difficult languages in the world to learn. Under this circumstance, the workshop on Natural Language Processing Techniques for Educational Applications (NLP-TEA) has been organizing shared tasks for CGED (Yu et al., 2014; Lee et al., 2015, 2016; Rao et al., 2017, 2018) to help learners of Chinese as a Foreign Language (CFL) since 2014. The shared tasks provide common test conditions for researchers from both industry and academia. We believe they are very beneficial to advancing CGED technology. This paper introduces our work on this year’s CGED shared task. The task requires both error detection and correction, and we use a hybrid system to handle both. It uses as building blocks models designed for various NLP tasks, including BERTbased sequence labeling models, Seq2Seq, and GECToR. We tune our models to lean towards optimizing precision, which we"
2020.nlptea-1.9,W18-3708,0,0.0180366,"k, also with the highest precision. Ours are among the top 4 to 6 in other tracks, except for FPR where we rank 12. And our system achieves the highest precisions among the top 10 submissions at IDENTIFICATION and POSITION tracks. The rest of this paper is organized as follows: A brief description of the CGED shared task is given in Section 2, followed by an overview of prior work in Section 3. Section 4 introduces our system in detail, and Section 5 demonstrates the experimental results. Finally, Section 6 concludes this paper. 2 2017; Liao et al., 2017; Fu et al., 2018b; Zhang et al., 2018; Li et al., 2018). Performance of these approaches are usually highly dependent on the handcrafted features fed into the LSTM layer. Yang et al. (2017) extracted features including characters, character-level bi-gram, Part-of-Speech (POS), POS scores, adjacent and dependent word collocations. Later in 2018, the feature sets were further enlarged by incorporating new features like word segmentation and Gaussian exact Point-wise Mutual Information (ePMI, Fu et al., 2018b). Grammatical Error Correction. Unlike the GED tasks, GEC tasks has been mostly treated as the machine translation problem. To the best of our"
2020.nlptea-1.9,P17-1099,0,0.0538172,"Missing"
2020.nlptea-1.9,I17-4011,0,0.0219123,"P3 correction track and third in the TOP1 correction track, also with the highest precision. Ours are among the top 4 to 6 in other tracks, except for FPR where we rank 12. And our system achieves the highest precisions among the top 10 submissions at IDENTIFICATION and POSITION tracks. The rest of this paper is organized as follows: A brief description of the CGED shared task is given in Section 2, followed by an overview of prior work in Section 3. Section 4 introduces our system in detail, and Section 5 demonstrates the experimental results. Finally, Section 6 concludes this paper. 2 2017; Liao et al., 2017; Fu et al., 2018b; Zhang et al., 2018; Li et al., 2018). Performance of these approaches are usually highly dependent on the handcrafted features fed into the LSTM layer. Yang et al. (2017) extracted features including characters, character-level bi-gram, Part-of-Speech (POS), POS scores, adjacent and dependent word collocations. Later in 2018, the feature sets were further enlarged by incorporating new features like word segmentation and Gaussian exact Point-wise Mutual Information (ePMI, Fu et al., 2018b). Grammatical Error Correction. Unlike the GED tasks, GEC tasks has been mostly treated"
2020.nlptea-1.9,N19-1333,0,0.0147968,"on (ePMI, Fu et al., 2018b). Grammatical Error Correction. Unlike the GED tasks, GEC tasks has been mostly treated as the machine translation problem. To the best of our knowledge, the multi-layer convolutional neural network accompanied by a large language model (Chollampatt and Ng, 2018) is considered as the first Neural Machine Translation (NMT)-like approach to handle GEC tasks in English. Then Ge et al. (2018) and Grundkiewicz and JunczysDowmunt (2018); Fu et al. (2018b) proposed to use recurrent neural networks, while recent work (Junczys-Dowmunt et al., 2018; Grundkiewicz et al., 2019; Lichtarge et al., 2019; Fu et al., 2018a) made use of the Transformer (Vaswani et al., 2017). Specially, GECToR (Omelianchuk et al., 2020), which considered the English GEC task as a sequential labeling problem, has obtained competitive results to previous GEC systems. Task Description Generally, the CGED shared task classifies grammatical errors found in Chinese writings into four different classes, i.e., redundant words (R), missing words (M), word selection errors (S), word ordering errors (W). Table 1 gives some examples of the errors, which are sampled from CGED 2020 training data. It should be noted that vari"
2020.nlptea-1.9,W19-4415,0,0.022785,"sentence 我和妈妈不像别的母女。 (Wˇo h´e m¯a ma b`u xi`ang bi´e de mˇu nuˇ¨.) 我同意后者的主张。 (Wˇo t´ong y`ı h`ou zhˇe de zhˇu zh¯ang.) 上周我的车被刮了。 (Sh`ang zh¯ou wˇo de ch¯e b`ei gu¯a le.) 我还是在学校上班。 (Wˇo h´ai sh`ı z`ai xu´e xi`ao sh`ang b¯an.) Table 1: Example sentences with corresponding errors. Sequences in the bracket are the corresponding transliterations. Basic GED Models Ensemble Models … … Distribution Ensemble Voting Mechanism GED framework Inputs Postprocessing Final Outputs Seq2seq Models GECToR Models Candidate Generation GEC framework Figure 1: The overall architecture of the developed system. 2019; Xu et al., 2019), the synthetic data generation process in this work operates on two different levels, i.e., word-level and character-level. The error probabilities of deletion and insertion are sampled from a normal distribution with mean 0.015 and standard deviation 0.2, while the error probability of substitution is sampled from a normal distribution with mean 0.075 and standard deviation 0.2. Word-level. At this level, error-free sentences are firstly segmented into words using the selfdeveloped tokenizer. Then the following wordlevel errors are randomly added to the error-free sentences. Character-level."
2020.nlptea-1.9,I17-4006,0,0.0119552,"chieves the highest precisions among the top 10 submissions at IDENTIFICATION and POSITION tracks. The rest of this paper is organized as follows: A brief description of the CGED shared task is given in Section 2, followed by an overview of prior work in Section 3. Section 4 introduces our system in detail, and Section 5 demonstrates the experimental results. Finally, Section 6 concludes this paper. 2 2017; Liao et al., 2017; Fu et al., 2018b; Zhang et al., 2018; Li et al., 2018). Performance of these approaches are usually highly dependent on the handcrafted features fed into the LSTM layer. Yang et al. (2017) extracted features including characters, character-level bi-gram, Part-of-Speech (POS), POS scores, adjacent and dependent word collocations. Later in 2018, the feature sets were further enlarged by incorporating new features like word segmentation and Gaussian exact Point-wise Mutual Information (ePMI, Fu et al., 2018b). Grammatical Error Correction. Unlike the GED tasks, GEC tasks has been mostly treated as the machine translation problem. To the best of our knowledge, the multi-layer convolutional neural network accompanied by a large language model (Chollampatt and Ng, 2018) is considered"
2020.nlptea-1.9,W14-1701,0,0.0537811,"Missing"
2020.nlptea-1.9,W18-3726,0,0.0195634,"TOP1 correction track, also with the highest precision. Ours are among the top 4 to 6 in other tracks, except for FPR where we rank 12. And our system achieves the highest precisions among the top 10 submissions at IDENTIFICATION and POSITION tracks. The rest of this paper is organized as follows: A brief description of the CGED shared task is given in Section 2, followed by an overview of prior work in Section 3. Section 4 introduces our system in detail, and Section 5 demonstrates the experimental results. Finally, Section 6 concludes this paper. 2 2017; Liao et al., 2017; Fu et al., 2018b; Zhang et al., 2018; Li et al., 2018). Performance of these approaches are usually highly dependent on the handcrafted features fed into the LSTM layer. Yang et al. (2017) extracted features including characters, character-level bi-gram, Part-of-Speech (POS), POS scores, adjacent and dependent word collocations. Later in 2018, the feature sets were further enlarged by incorporating new features like word segmentation and Gaussian exact Point-wise Mutual Information (ePMI, Fu et al., 2018b). Grammatical Error Correction. Unlike the GED tasks, GEC tasks has been mostly treated as the machine translation problem. T"
2020.nlptea-1.9,N19-1014,0,0.19057,"pecific error occurs. For example, triples (5, 5, R) and (2, 3, W) are expected for S and W errors shown in Table 1. • Correction-level. At this level, developed systems are required to provide up to 3 potential correction candidates for S or M errors. 3 Methodology 4.1 Synthetic Data Generation. Pre-training on synthetic data is crucial for the present GEC and GED tasks since the parallel training data are still extremely scarce. It is found that the proposed basic GED models, Seq2Seq GEC models and GECToR models also benefit from synthetic data. Following previous work on English GEC tasks (Zhao et al., 2019; Grundkiewicz et al., Related Work Grammatical Error Diagnosis. GED tasks are usually treated as a kind of sequential labeling problem. The common solution to this problem is utilizing the Long Short-Term Memory (LSTM) - Conditional Random Fields (CRF) model (Yang et al., 68 Error type R M S W Erroneous sentence 我和妈妈是不像别的母女。 (Wˇo h´e m¯a ma sh`ı b`u xi`ang bi´e de mˇu nuˇ¨.) 我同意后者主张。 (Wˇo t´ong y`ı h`ou zhˇe zhˇu zh¯ang.) 上周我的车刮疼啊。 (Sh`ang zh¯ou wˇo de ch¯e gu¯a t´eng a.) 我是还在学校上班。 (Wˇo sh`ı h´ai z`ai xu´e xi`ao sh`ang b¯an.) Correct sentence 我和妈妈不像别的母女。 (Wˇo h´e m¯a ma b`u xi`ang bi´e de mˇu"
2020.nlptea-1.9,W16-4907,0,0.0198832,"ttention layer, a multi-head cross-attention layer and a positionwise feed-forward layer. Residual connections and layer normalizations are used to improve the performance of deep Transformers. The copy-augmented Transformer was originally proposed for text summarization tasks (Gu et al., (1) Here, X denotes the input sequence, hi is the final hidden state of BERT, W and b are model parameters. The tag with the largest conditional probability will be chosen as the final output corresponding to the input token xi . Distribution Ensemble. Top results are usually achieved by ensemble techniques (Zheng et al., 2016; Fu et al., 2018b), and this work also benefits from model ensemble approaches. Specifically, we assume that there are M different basic GED models {m1 , m2 , ..., mM }. Then for each input sequence X = (x1 , x2 , ..., xn ), we have M output sequences {Y1 , Y2 , ..., YM }. The distribution ensemble based on M different models can be written by: P (y|X) = M 1 X Pk (y|X; θk ) . M Grammatical Error Correction (2) k=1 70 Input Output Input Output 因 (Y¯ın O 我 ( Wˇo O 为 w`ei O 不 b`u B-S ， , O 可 kˇe I-S 雾 w`u B-S 以 yˇı I-S 烟 y¯an I-S 找 zhˇao O 刺 c`ı O 到 d`ao O 激 j¯ı O 了 le B-M 就 ji`u O 在 z`ai B-S 对"
2020.sigdial-1.35,D18-1547,0,0.043355,"mation by knowledge access, but also maintain natural conversation. For example, the responses at t = {4, 8} paraphrase written sentences into a colloquial style, the responses at t = {4, 16} acknowledge before giving a statements, the responses at t = {8, 12} ask a follow-up question to the user. (a) Positions for augmentation (b) User utterances (c) System responses Figure 3: Crowdsourcing user interfaces for MultiWOZ data augmentation with knowledge access turns 4 Split Train Valid Test Total Data To address the proposed research problems, we collected an augmented version of MultiWOZ 2.1 (Budzianowski et al., 2018; Eric et al., 2019) with out-of-API-coverage turns grounded on external knowledge sources beyond the original database entries. This was incrementally done by the following three crowdsourcing tasks. First, crowd workers were given a dialogue sampled from the original MultiWOZ 2.1 conversations and asked to indicate an appropriate position to insert a new turn about a selected subject from external knowledge categories (Figure 3a). This task aims to collect user behaviors about when to ask a knowledge-seeking question for a given subject. It corresponds to the knowledge-seeking turn detection"
2020.sigdial-1.35,N19-1423,0,0.0509188,"nsupervised anomaly detection algorithm, Local Outlier Factor (LOF) (Breunig et al., 2000). The algorithm compares the local densities between a given input instance and its nearest neighbors. If the input has a significantly lower density than the neighbors, it is considered an anomaly. We built a knowledge-seeking turn detector with the LOF implementation in PyOD (Zhao et al., 2019) with its default configurations. The system includes all the user utterances in the original MultiWOZ 2.1 training set. Every utterance in both training and test sets was encoded by the uncased pre-trained BERT (Devlin et al., 2019) model. If training data is available for the knowledgeseeking turn detection, the most straightforward solution will be training a binary classifier in a supervised manner. In this experiment, we fine-tuned the uncased pre-trained BERT (Devlin et al., 2019) model on the training data in Section 4. The model takes each single user utterance ut as an input and generates the utterance representation as the final layer output for [CLS] which is a special token in the beginning of the input sequence. We added a single layer feedforward network on top of the utterance embeddings, which was trained"
2020.sigdial-1.35,P18-1138,0,0.0236476,"al language response to the user by natural language generation (Perera and Nand, 2017). On the other hand, social conversational systems typically follow an end-to-end approach, and aim to generate target responses based on the previous conversation context (Ritter et al., 2011; Vinyals and Le, 2015; Serban et al., 2017). Ghazvininejad et al. (2018) proposed an extension to these models that grounds the responses on unstructured, textual knowledge, by using end-to-end memory networks where an attention over the knowledge relevant to the conversation context is estimated. Along similar lines, Liu et al. (2018) used pattern matching, named entity recognition and linking to find facts relevant to the current dialogue and other related entities from a knowledge base. Zhou et al. (2018) proposed both static and dynamic graph attention mechanisms for knowledge selection and response generation, respectively, using knowledge graphs. More recently, Dinan et al. (2018) and Gopalakrishnan et al. (2019) both have publicly released large conversational data sets, where knowledge sentences related to each conversation turn are annotated. Our proposed task, data, and baseline models in this work differ from the"
2020.sigdial-1.35,D16-1264,0,0.0114454,"ased large conversational data sets, where knowledge sentences related to each conversation turn are annotated. Our proposed task, data, and baseline models in this work differ from these studies in the following aspects: we target task-oriented conversations with more clear goals and explicit dialogue states than social conversations; and we aim to incorporate task-specific domain knowledge instead of commonsense knowledge. 279 The other line of related work is machine reading comprehension which aims to answer questions given unstructured text (Richardson et al., 2013; Hermann et al., 2015; Rajpurkar et al., 2016) and has later been extended to conversational question answering (Choi et al., 2018; Reddy et al., 2019). In our work, the document required to generate a response needs to be identified according to the conversation context. The responses are also different in that, rather than plain answers to factual questions, we aim to form factually accurate responses that seamlessly blend into the conversation. 3 turns at t = {3, 7, 11, 15}, while f1 (Ut |K) = 0 for the other user turns at t = {1, 5, 9, 13}. 3.2 Once a given user turn at t is determined as a knowledge-seeking turn by f1 (Ut |K), it mov"
2020.sigdial-1.35,Q19-1016,0,0.0323172,"d. Our proposed task, data, and baseline models in this work differ from these studies in the following aspects: we target task-oriented conversations with more clear goals and explicit dialogue states than social conversations; and we aim to incorporate task-specific domain knowledge instead of commonsense knowledge. 279 The other line of related work is machine reading comprehension which aims to answer questions given unstructured text (Richardson et al., 2013; Hermann et al., 2015; Rajpurkar et al., 2016) and has later been extended to conversational question answering (Choi et al., 2018; Reddy et al., 2019). In our work, the document required to generate a response needs to be identified according to the conversation context. The responses are also different in that, rather than plain answers to factual questions, we aim to form factually accurate responses that seamlessly blend into the conversation. 3 turns at t = {3, 7, 11, 15}, while f1 (Ut |K) = 0 for the other user turns at t = {1, 5, 9, 13}. 3.2 Once a given user turn at t is determined as a knowledge-seeking turn by f1 (Ut |K), it moves forward with Knowledge Selection to sort out the relevant knowledge snippets. This task takes each pai"
2020.sigdial-1.35,D13-1020,0,0.0140398,"akrishnan et al. (2019) both have publicly released large conversational data sets, where knowledge sentences related to each conversation turn are annotated. Our proposed task, data, and baseline models in this work differ from these studies in the following aspects: we target task-oriented conversations with more clear goals and explicit dialogue states than social conversations; and we aim to incorporate task-specific domain knowledge instead of commonsense knowledge. 279 The other line of related work is machine reading comprehension which aims to answer questions given unstructured text (Richardson et al., 2013; Hermann et al., 2015; Rajpurkar et al., 2016) and has later been extended to conversational question answering (Choi et al., 2018; Reddy et al., 2019). In our work, the document required to generate a response needs to be identified according to the conversation context. The responses are also different in that, rather than plain answers to factual questions, we aim to form factually accurate responses that seamlessly blend into the conversation. 3 turns at t = {3, 7, 11, 15}, while f1 (Ut |K) = 0 for the other user turns at t = {1, 5, 9, 13}. 3.2 Once a given user turn at t is determined as"
2020.sigdial-1.35,D11-1054,0,0.330889,"ictionless task-oriented scenarios, where the flow of the conversation does not break when users have requests that are out of the coverage of APIs/DB but potentially are already available in external knowledge sources. Inspired by recent studies on knowledge-grounded conversational modeling (Zhou et al., 2018; Dinan et al., 2018; Galley et al., 2019; Gopalakrishnan et al., 2019), our proposed task aims to develop end-to-end dialogue systems to understand relevant domain knowledge, and generate system responses with the selected knowledge. Different from previous work on social conversations (Ritter et al., 2011; Vinyals and Le, 2015; Serban et al., 2017), this task addresses task-oriented conversations grounded on fine-grained domain-level or entity-level knowledge sources related to given dialogue contexts. Figure 1 shows an example conversation with unstructured knowledge access. The user utterances at turns t = {3, 7} and t = {11, 15} request the policy details about bringing pets and making payments, respectively, which are out of the coverage of the structured domain APIs. On the other hand, the relevant knowledge contents can be found from the external sources as in the rightmost column which"
2021.acl-long.369,W16-2206,0,0.0191707,"Figure 7 shows the results. We divide target tokens into four categories: 1. cPcA: correct prediction & correct alignment; 2. wPcA: wrong prediction & correct alignment; 3. cPwA: correct prediction & wrong alignment; 4. wPwA: wrong prediction & wrong alignment. Compared with other methods, M ASK -A LIGN significantly reduces the alignment errors caused by wrong predictions (wPwA). In addition, the number of the tokens with correct prediction but wrong Related Work Our work is closely related to unsupervised neural word alignment. While early unsupervised neural aligners (Tamura et al., 2014; Alkhouli et al., 2016; Peter et al., 2017) failed to outperform their statistical counterparts such as FAST-A LIGN (Dyer et al., 2013) and GIZA++ (Och and Ney, 2003), recent studies have made significant progress by inducing alignments from NMT models (Garg et al., 2019; Zenkel et al., 2019, 2020; Chen et al., 2020). Our work differs from prior studies in that we design a novel self-supervised model that is capable of utilizing more target context than NMT-based models to generate high quality alignments without using guided training. Our work is also inspired by the success of conditional masked language models ("
2021.acl-long.369,D16-1162,0,0.0186491,"hosen to be aligned to “Tokyo”. Word alignment is an important task of finding the correspondence between words in a sentence pair (Brown et al., 1993) and used to be a key component of statistical machine translation (SMT) (Koehn et al., 2003; Dyer et al., 2013). Although word alignment is no longer explicitly modeled in neural machine translation (NMT) (Bahdanau et al., 2015; Vaswani et al., 2017), it is often leveraged to analyze NMT models (Tu et al., 2016; Ding et al., 2017). Word alignment is also used in many other scenarios such as imposing lexical constraints on the decoding process (Arthur et al., 2016; Hasler ∗ I was born in Induced alignment link: Tokio - Tokyo Introduction Corresponding author Code can be found at https://github.com/THUNLP-MT/ Mask-Align. 1 Target : et al., 2018), improving automatic post-editing (Pal et al., 2017) , and providing guidance for translators in computer-aided translation (Dagan et al., 1993). Compared with statistical methods, neural methods can learn representations end-to-end from raw data and have been successfully applied to supervised word alignment (Yang et al., 2013; Tamura et al., 2014). For unsupervised word alignment, however, previous neural meth"
2021.acl-long.369,J93-2003,0,0.206312,"Missing"
2021.acl-long.369,2016.amta-researchers.10,0,0.0225016,"d Forward h4 ✕L L✕ Static-KV Attention EncoderLayer w1 + p1 w2 + p2 w3 + p3 w4 + p4 w1+p1 p1 w2+p2 p2 w3+p3 p3 w4+p4 p4 Figure 2: The architecture of M ASK -A LIGN. property of NMT systems (Sutskever et al., 2014), they only leverage part of the target context. This inevitably brings noisy alignments when the prediction is ambiguous. Consider the target sentence in Figure 1. When predicting “Tokyo”, an NMT system may generate “1968” because future context is not observed, leading to a wrong alignment link (“1968”, “Tokyo”). Second, they have to incorporate an additional guided alignment loss (Chen et al., 2016) to outperform GIZA++. This loss requires pseudo alignments of the full training data to guide the training of the model. Although these pseudo alignments can be utilized to partially alleviate the problem of ignoring future context, they are computationally expensive to obtain. In this paper, we propose a self-supervised model specifically designed for the word alignment task, namely M ASK -A LIGN. Our model parallelly masks out each target token and recovers it conditioned on the source and other target tokens. Figure 1 shows an example where the target token “Tokyo” is masked out and re-pre"
2021.acl-long.369,2020.emnlp-main.42,1,0.838872,"1993). Compared with statistical methods, neural methods can learn representations end-to-end from raw data and have been successfully applied to supervised word alignment (Yang et al., 2013; Tamura et al., 2014). For unsupervised word alignment, however, previous neural methods fail to significantly exceed their statistical counterparts such as FAST-A LIGN (Dyer et al., 2013) and GIZA++ (Och and Ney, 2003). Recently, there is a surge of interest in NMT-based alignment methods which take alignments as a by-product of NMT systems (Li et al., 2019; Garg et al., 2019; Zenkel et al., 2019, 2020; Chen et al., 2020). Using attention weights or feature importance measures to induce alignments for to-be-predicted target tokens, these methods outperform unsupervised statistical aligners like GIZA++ on a variety of language pairs. Although NMT-based unsupervised aligners have proven to be effective, they suffer from two major limitations. First, due to the autoregressive 4781 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 4781–4791 August 1–6, 2021. ©2021 Association for Computational Li"
2021.acl-long.369,W19-4828,0,0.0171693,"as collectors. As a result of such effect, many target tokens (e.g., the 4783 vanilla attention not 1.0 true 1.0 falsch falsch leaky attention not 0.4 0.6 true 0.2 0.8 [NULL] falsch falsch 0.5 0.5 not true 0.2 0.4 0.4 [NULL] not true Figure 4: An illustrative example of the attention weights from two directional models using vanilla and leaky attention. Leaky attention provides a leak position “[NULL]” to collect extra attention weights. two “in”s in Figure 3) will be incorrectly aligned to the collectors according to the attention weights. This phenomenon has been studied in previous works (Clark et al., 2019; Kobayashi et al., 2020). Kobayashi et al. (2020) show that the norms of the value vectors for the collectors are usually small, making their influence on attention outputs actually limited. We conjecture that this phenomenon is due to the incapability of NMT-based aligners to deal with tokens that have no counterparts on the other side because there is no empty (NULL) token that is widely used in statistical aligners (Brown et al., 1993; Och and Ney, 2003). We propose to explicitly model the NULL token with an attention variant, namely leaky attention. As shown in Figure 4, when calculating"
2021.acl-long.369,W19-5201,0,0.0239257,"and Ly→x are NLL losses, α and β are hyperparameters. 2.3 Inference When extracting alignments, we compute an alignment score Sij for yi and xj as the harmonic mean ij ji of attention weights Wx→y and Wy→x from two directional models: Sij = ij ji 2 Wx→y Wy→x ij ji Wx→y + Wy→x (18) We use the harmonic mean because we assume a ij ji large Sij requires both Wx→y and Wy→x to be large. Word alignments can be induced from the alignment score matrix as follows:  1 if Sij ≥ τ Aij = (19) 0 otherwise (Zenkel et al., 2019, 2020) and used the preprocessing scripts from Zenkel et al. (2019)4 . Following Ding et al. (2019), we take the last 1000 sentences of the training data for these three datasets as validation sets. We used a joint source and target Byte Pair Encoding (BPE) (Sennrich et al., 2016) with 40k merge operations. During training, we filtered out sentences with the length of 1 to ensure the validity of the masking process. 3.2 We implemented our model based on the Transformer architecture (Vaswani et al., 2017). The encoder consists of 6 standard Transformer encoder layers. The decoder is composed of 6 layers, each of which contains static-KV attention while only the last layer is equipped with le"
2021.acl-long.369,P17-1106,1,0.697574,"and other target tokens. Then, the source token “Tokio” that contributes most to recovering the masked word (highlighted in red) is chosen to be aligned to “Tokyo”. Word alignment is an important task of finding the correspondence between words in a sentence pair (Brown et al., 1993) and used to be a key component of statistical machine translation (SMT) (Koehn et al., 2003; Dyer et al., 2013). Although word alignment is no longer explicitly modeled in neural machine translation (NMT) (Bahdanau et al., 2015; Vaswani et al., 2017), it is often leveraged to analyze NMT models (Tu et al., 2016; Ding et al., 2017). Word alignment is also used in many other scenarios such as imposing lexical constraints on the decoding process (Arthur et al., 2016; Hasler ∗ I was born in Induced alignment link: Tokio - Tokyo Introduction Corresponding author Code can be found at https://github.com/THUNLP-MT/ Mask-Align. 1 Target : et al., 2018), improving automatic post-editing (Pal et al., 2017) , and providing guidance for translators in computer-aided translation (Dagan et al., 1993). Compared with statistical methods, neural methods can learn representations end-to-end from raw data and have been successfully applie"
2021.acl-long.369,N13-1073,0,0.68427,"-art results.1 1 Source : Ich wurde 1968 in Tokio geboren . ___ in 1968 . Figure 1: An example of inducing an alignment link for target token “Tokyo” in M ASK -A LIGN. First, we mask out “Tokyo” and predict it with source and other target tokens. Then, the source token “Tokio” that contributes most to recovering the masked word (highlighted in red) is chosen to be aligned to “Tokyo”. Word alignment is an important task of finding the correspondence between words in a sentence pair (Brown et al., 1993) and used to be a key component of statistical machine translation (SMT) (Koehn et al., 2003; Dyer et al., 2013). Although word alignment is no longer explicitly modeled in neural machine translation (NMT) (Bahdanau et al., 2015; Vaswani et al., 2017), it is often leveraged to analyze NMT models (Tu et al., 2016; Ding et al., 2017). Word alignment is also used in many other scenarios such as imposing lexical constraints on the decoding process (Arthur et al., 2016; Hasler ∗ I was born in Induced alignment link: Tokio - Tokyo Introduction Corresponding author Code can be found at https://github.com/THUNLP-MT/ Mask-Align. 1 Target : et al., 2018), improving automatic post-editing (Pal et al., 2017) , and"
2021.acl-long.369,D19-1453,0,0.339124,"s in computer-aided translation (Dagan et al., 1993). Compared with statistical methods, neural methods can learn representations end-to-end from raw data and have been successfully applied to supervised word alignment (Yang et al., 2013; Tamura et al., 2014). For unsupervised word alignment, however, previous neural methods fail to significantly exceed their statistical counterparts such as FAST-A LIGN (Dyer et al., 2013) and GIZA++ (Och and Ney, 2003). Recently, there is a surge of interest in NMT-based alignment methods which take alignments as a by-product of NMT systems (Li et al., 2019; Garg et al., 2019; Zenkel et al., 2019, 2020; Chen et al., 2020). Using attention weights or feature importance measures to induce alignments for to-be-predicted target tokens, these methods outperform unsupervised statistical aligners like GIZA++ on a variety of language pairs. Although NMT-based unsupervised aligners have proven to be effective, they suffer from two major limitations. First, due to the autoregressive 4781 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 4781–4791 August 1–"
2021.acl-long.369,D19-1633,0,0.0345485,"et al., 2017) failed to outperform their statistical counterparts such as FAST-A LIGN (Dyer et al., 2013) and GIZA++ (Och and Ney, 2003), recent studies have made significant progress by inducing alignments from NMT models (Garg et al., 2019; Zenkel et al., 2019, 2020; Chen et al., 2020). Our work differs from prior studies in that we design a novel self-supervised model that is capable of utilizing more target context than NMT-based models to generate high quality alignments without using guided training. Our work is also inspired by the success of conditional masked language models (CMLMs) (Ghazvininejad et al., 2019), which have been applied to non-autoregressive machine translation. The CMLM can leverage both previous and future context on the target side for sequence-to-sequence tasks with the masking mechanism. Kasai et al. (2020) extend it with a disentangled context Transformer that predicts every target token conditioned on arbitrary context. By taking the characteristics of word alignment into consideration, we propose to use static-KV attention to achieve masking and aligning in parallel. To the best of our knowledge, this is the first work that incorporates a CMLM into alignment models. 4788 5 Co"
2021.acl-long.369,N18-2081,0,0.0337361,"Missing"
2021.acl-long.369,P00-1056,0,0.726347,"last layer is equipped with leaky attention. We set the embedding size to 512, the hidden size to 1024, and attention heads to 4. The input and output embeddings are shared for the decoder. We trained the models with a batch size of 36K tokens. We used early stopping based on the prediction accuracy on the validation sets. We tuned the hyperparameters via grid search on the ChineseEnglish validation set as it contains gold word alignments. In all of our experiments, we set λ = 0.05 (Eq. (16)), α = 5, β = 1 (Eq. (17)) and τ = 0.2 (Eq. (19)). The evaluation metric is Alignment Error Rate (AER) (Och and Ney, 2000). 3.3 3.1 Baselines We introduce the following unsupervised neural baselines besides two statistical baselines FASTA LIGN and GIZA++: where τ is a threshold. 3 Settings Experiments Datasets We conducted our experiments on four public datasets: German-English (De-En), English-French (En-Fr), Romanian-English (Ro-En) and ChineseEnglish (Zh-En). The Chinese-English training set is from the LDC corpus that consists of 1.2M sentence pairs. For validation and testing, we used the Chinese-English alignment dataset from Liu et al. (2005)3 , which contains 450 sentence pairs for validation and 450 for"
2021.acl-long.369,J03-1002,0,0.286326,"t https://github.com/THUNLP-MT/ Mask-Align. 1 Target : et al., 2018), improving automatic post-editing (Pal et al., 2017) , and providing guidance for translators in computer-aided translation (Dagan et al., 1993). Compared with statistical methods, neural methods can learn representations end-to-end from raw data and have been successfully applied to supervised word alignment (Yang et al., 2013; Tamura et al., 2014). For unsupervised word alignment, however, previous neural methods fail to significantly exceed their statistical counterparts such as FAST-A LIGN (Dyer et al., 2013) and GIZA++ (Och and Ney, 2003). Recently, there is a surge of interest in NMT-based alignment methods which take alignments as a by-product of NMT systems (Li et al., 2019; Garg et al., 2019; Zenkel et al., 2019, 2020; Chen et al., 2020). Using attention weights or feature importance measures to induce alignments for to-be-predicted target tokens, these methods outperform unsupervised statistical aligners like GIZA++ on a variety of language pairs. Although NMT-based unsupervised aligners have proven to be effective, they suffer from two major limitations. First, due to the autoregressive 4781 Proceedings of the 59th Annua"
2021.acl-long.369,E17-2056,0,0.0257607,"Missing"
2021.acl-long.369,N03-1017,0,0.200635,"Missing"
2021.acl-long.369,P19-1124,0,0.0313147,"ce for translators in computer-aided translation (Dagan et al., 1993). Compared with statistical methods, neural methods can learn representations end-to-end from raw data and have been successfully applied to supervised word alignment (Yang et al., 2013; Tamura et al., 2014). For unsupervised word alignment, however, previous neural methods fail to significantly exceed their statistical counterparts such as FAST-A LIGN (Dyer et al., 2013) and GIZA++ (Och and Ney, 2003). Recently, there is a surge of interest in NMT-based alignment methods which take alignments as a by-product of NMT systems (Li et al., 2019; Garg et al., 2019; Zenkel et al., 2019, 2020; Chen et al., 2020). Using attention weights or feature importance measures to induce alignments for to-be-predicted target tokens, these methods outperform unsupervised statistical aligners like GIZA++ on a variety of language pairs. Although NMT-based unsupervised aligners have proven to be effective, they suffer from two major limitations. First, due to the autoregressive 4781 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages"
2021.acl-long.369,P16-1162,0,0.0936632,"ention weights Wx→y and Wy→x from two directional models: Sij = ij ji 2 Wx→y Wy→x ij ji Wx→y + Wy→x (18) We use the harmonic mean because we assume a ij ji large Sij requires both Wx→y and Wy→x to be large. Word alignments can be induced from the alignment score matrix as follows:  1 if Sij ≥ τ Aij = (19) 0 otherwise (Zenkel et al., 2019, 2020) and used the preprocessing scripts from Zenkel et al. (2019)4 . Following Ding et al. (2019), we take the last 1000 sentences of the training data for these three datasets as validation sets. We used a joint source and target Byte Pair Encoding (BPE) (Sennrich et al., 2016) with 40k merge operations. During training, we filtered out sentences with the length of 1 to ensure the validity of the masking process. 3.2 We implemented our model based on the Transformer architecture (Vaswani et al., 2017). The encoder consists of 6 standard Transformer encoder layers. The decoder is composed of 6 layers, each of which contains static-KV attention while only the last layer is equipped with leaky attention. We set the embedding size to 512, the hidden size to 1024, and attention heads to 4. The input and output embeddings are shared for the decoder. We trained the models"
2021.acl-long.369,N06-1014,0,0.179926,"2, we will show that leaky attention is also helpful for applying agreement-based training on two directional models. We remove the cross-attention in all but the last decoder layer. This makes the interaction between the source and target restricted in the last layer. Our experiments demonstrate that this modification improves alignment results with fewer model parameters. 2.2 Training To better utilize the attention weights from two directions, we apply an agreement loss in the training process to improve the symmetry of our model, which has proven effective in statistical alignment models (Liang et al., 2006; Liu et al., 2015). Given a parallel sentence pair hx, yi, we can obtain the attention weights from two different directions, denoted as W x→y and W y→x . As alignment is bijective, W x→y is supposed to be equal to the transpose of W y→x . We encourage this kind of symmetry through an agreement loss:   La = MSE W x→y , W &gt; y→x (14) where MSE represents the mean squared error. For vanilla attention, La is hardly small because of the normalization constraint. As shown in Figure 4, due to the use of softmax activation, the minimal value of La is 0.25 for vanilla attention. Using leaky attentio"
2021.acl-long.369,D15-1210,1,0.831448,"leaky attention is also helpful for applying agreement-based training on two directional models. We remove the cross-attention in all but the last decoder layer. This makes the interaction between the source and target restricted in the last layer. Our experiments demonstrate that this modification improves alignment results with fewer model parameters. 2.2 Training To better utilize the attention weights from two directions, we apply an agreement loss in the training process to improve the symmetry of our model, which has proven effective in statistical alignment models (Liang et al., 2006; Liu et al., 2015). Given a parallel sentence pair hx, yi, we can obtain the attention weights from two different directions, denoted as W x→y and W y→x . As alignment is bijective, W x→y is supposed to be equal to the transpose of W y→x . We encourage this kind of symmetry through an agreement loss:   La = MSE W x→y , W &gt; y→x (14) where MSE represents the mean squared error. For vanilla attention, La is hardly small because of the normalization constraint. As shown in Figure 4, due to the use of softmax activation, the minimal value of La is 0.25 for vanilla attention. Using leaky attention, our approach can"
2021.acl-long.369,P05-1057,1,0.63668,"Eq. (19)). The evaluation metric is Alignment Error Rate (AER) (Och and Ney, 2000). 3.3 3.1 Baselines We introduce the following unsupervised neural baselines besides two statistical baselines FASTA LIGN and GIZA++: where τ is a threshold. 3 Settings Experiments Datasets We conducted our experiments on four public datasets: German-English (De-En), English-French (En-Fr), Romanian-English (Ro-En) and ChineseEnglish (Zh-En). The Chinese-English training set is from the LDC corpus that consists of 1.2M sentence pairs. For validation and testing, we used the Chinese-English alignment dataset from Liu et al. (2005)3 , which contains 450 sentence pairs for validation and 450 for testing. For other three language pairs, we followed the experimental setup in 3 http://nlp.csai.tsinghua.edu.cn/∼ly/systems/ TsinghuaAligner/TsinghuaAligner.html • NAIVE -ATT (Garg et al., 2019): a method that induces alignments from cross-attention weights of the best (usually penultimate) decoder layer in a vanilla Tranformer. • NAIVE -ATT-L AST: same as NAIVE -ATT except that only the last decoder layer performs cross-attention. • A DD SGD (Zenkel et al., 2019): a method that adds an extra alignment layer to repredict the to-"
2021.acl-long.369,P04-1066,0,0.166114,"uery inputs and hidden states for yi in the l-th layer, respectively. h0i is initialized with pi . We name this variant of attention the static-KV attention. By static-KV, we mean the keys and values are unchanged across different layers in our approach. Our model replaces all selfattention in the decoder with static-KV attention. Leaky Attention Extracting alignments from vanilla cross-attention often suffers from the high attention weights on some specific source tokens such as periods, [EOS], or other high frequency tokens (see Figure 3). This is similar to the “garbage collectors” effect (Moore, 2004) in statistical aligners, where a source token is aligned to too many target tokens. Hereinafter, we will refer to these tokens as collectors. As a result of such effect, many target tokens (e.g., the 4783 vanilla attention not 1.0 true 1.0 falsch falsch leaky attention not 0.4 0.6 true 0.2 0.8 [NULL] falsch falsch 0.5 0.5 not true 0.2 0.4 0.4 [NULL] not true Figure 4: An illustrative example of the attention weights from two directional models using vanilla and leaky attention. Leaky attention provides a leak position “[NULL]” to collect extra attention weights. two “in”s in Figure 3) will be"
2021.acl-long.369,P14-1138,0,0.0587573,"os such as imposing lexical constraints on the decoding process (Arthur et al., 2016; Hasler ∗ I was born in Induced alignment link: Tokio - Tokyo Introduction Corresponding author Code can be found at https://github.com/THUNLP-MT/ Mask-Align. 1 Target : et al., 2018), improving automatic post-editing (Pal et al., 2017) , and providing guidance for translators in computer-aided translation (Dagan et al., 1993). Compared with statistical methods, neural methods can learn representations end-to-end from raw data and have been successfully applied to supervised word alignment (Yang et al., 2013; Tamura et al., 2014). For unsupervised word alignment, however, previous neural methods fail to significantly exceed their statistical counterparts such as FAST-A LIGN (Dyer et al., 2013) and GIZA++ (Och and Ney, 2003). Recently, there is a surge of interest in NMT-based alignment methods which take alignments as a by-product of NMT systems (Li et al., 2019; Garg et al., 2019; Zenkel et al., 2019, 2020; Chen et al., 2020). Using attention weights or feature importance measures to induce alignments for to-be-predicted target tokens, these methods outperform unsupervised statistical aligners like GIZA++ on a variet"
2021.acl-long.369,P16-1008,1,0.809924,"ct it with source and other target tokens. Then, the source token “Tokio” that contributes most to recovering the masked word (highlighted in red) is chosen to be aligned to “Tokyo”. Word alignment is an important task of finding the correspondence between words in a sentence pair (Brown et al., 1993) and used to be a key component of statistical machine translation (SMT) (Koehn et al., 2003; Dyer et al., 2013). Although word alignment is no longer explicitly modeled in neural machine translation (NMT) (Bahdanau et al., 2015; Vaswani et al., 2017), it is often leveraged to analyze NMT models (Tu et al., 2016; Ding et al., 2017). Word alignment is also used in many other scenarios such as imposing lexical constraints on the decoding process (Arthur et al., 2016; Hasler ∗ I was born in Induced alignment link: Tokio - Tokyo Introduction Corresponding author Code can be found at https://github.com/THUNLP-MT/ Mask-Align. 1 Target : et al., 2018), improving automatic post-editing (Pal et al., 2017) , and providing guidance for translators in computer-aided translation (Dagan et al., 1993). Compared with statistical methods, neural methods can learn representations end-to-end from raw data and have been"
2021.acl-long.369,W19-4808,0,0.0275838,"encoder outputs. 2 We use a normal distribution with a mean of 0 and a small 2 A similar attention implementation can be found in https://github.com/pytorch/fairseq/blob/master/fairseq/ modules/multihead attention.py. deviation to initialize kNULL and vNULL to ensure that their initial norms are rather small. When extracting alignments, we only consider the attention matrix without the leak position. Note that leaky attention is different from adding a special token in the source sequence, which will share the same high attention weights with the existing collector instead of calibrating it (Vig and Belinkov, 2019). Our parameterized method is more flexible than Leaky-Softmax (Sabour et al., 2017) which adds an extra dimension with the value of zero to the routing logits. In Section 2.2, we will show that leaky attention is also helpful for applying agreement-based training on two directional models. We remove the cross-attention in all but the last decoder layer. This makes the interaction between the source and target restricted in the last layer. Our experiments demonstrate that this modification improves alignment results with fewer model parameters. 2.2 Training To better utilize the attention weig"
2021.acl-long.369,P13-1017,0,0.0217326,"many other scenarios such as imposing lexical constraints on the decoding process (Arthur et al., 2016; Hasler ∗ I was born in Induced alignment link: Tokio - Tokyo Introduction Corresponding author Code can be found at https://github.com/THUNLP-MT/ Mask-Align. 1 Target : et al., 2018), improving automatic post-editing (Pal et al., 2017) , and providing guidance for translators in computer-aided translation (Dagan et al., 1993). Compared with statistical methods, neural methods can learn representations end-to-end from raw data and have been successfully applied to supervised word alignment (Yang et al., 2013; Tamura et al., 2014). For unsupervised word alignment, however, previous neural methods fail to significantly exceed their statistical counterparts such as FAST-A LIGN (Dyer et al., 2013) and GIZA++ (Och and Ney, 2003). Recently, there is a surge of interest in NMT-based alignment methods which take alignments as a by-product of NMT systems (Li et al., 2019; Garg et al., 2019; Zenkel et al., 2019, 2020; Chen et al., 2020). Using attention weights or feature importance measures to induce alignments for to-be-predicted target tokens, these methods outperform unsupervised statistical aligners l"
2021.acl-long.369,2020.acl-main.146,0,0.0709404,"forms cross-attention. • A DD SGD (Zenkel et al., 2019): a method that adds an extra alignment layer to repredict the to-be-aligned target token. • M TL -F ULLC (Garg et al., 2019): a method that supervises an attention head with symmetrized NAIVE -ATT alignments in a multitask learning framework. 4 4785 https://github.com/lilt/alignment-scripts Method Guided De-En En-Fr Ro-En Zh-En FAST-A LIGN (Dyer et al., 2013) GIZA++ (Och and Ney, 2003) N N 25.7 17.8 12.1 6.1 31.8 26.0 18.5 NAIVE -ATT (Garg et al., 2019) NAIVE -ATT-L AST A DD SGD (Zenkel et al., 2019) M TL -F ULLC (Garg et al., 2019) BAO (Zenkel et al., 2020) S HIFT-ATT (Chen et al., 2020) N N N N N N 31.9 28.4 21.2 20.2 17.9 17.9 18.5 17.7 10.0 7.7 8.4 6.6 32.9 32.4 27.6 26.0 24.1 23.9 28.9 26.4 20.2 M TL -F ULLC -GZ (Garg et al., 2019) BAO-G UIDED (Zenkel et al., 2020) S HIFT-AET (Chen et al., 2020) Y Y Y 16.0 16.3 15.4 4.6 5.0 4.7 23.1 23.4 21.2 17.2 M ASK -A LIGN N 14.4 4.4 19.5 13.8 Table 1: Alignment Error Rate (AER) scores on four datasets for different alignment methods. The lower AER, the better. “Guided” denotes whether the guided alignment loss is used during training. All results are symmetrized. We highlight the best results for each"
2021.acl-long.446,W17-4772,0,0.0214025,"ation strategy” proposed by Libovick`y et al. (2018) and our method. 5742 Models TEST16 Pretraining TEST17 TER BLEU TER BLEU — 22.89 — 23.08 65.57 D UAL B ERT (Correia and Martins, 2019) D UAL BART (Correia and Martins, 2019) mBERT mBART 18.88 18.26 71.61 72.65 19.03 18.41 70.66 72.08 TRICE mBART 17.41M? 73.43M? 17.75M? 72.70M? — — 17.81 17.45 72.79 73.51 18.10 17.77 71.72 72.98 D UAL B ERT (Correia and Martins, 2019) D UAL BART (Correia and Martins, 2019) mBERT mBART 16.91 16.40 74.29 74.74 17.26 17.26 73.42 73.56 TRICE mBART 16.09M? 75.39M? 16.91M? 74.09M? extremely low-resource F ORCEDATT (Berard et al., 2017) high-resource D UALT RANS (Junczys-Dowmunt and Grundkiewicz, 2018) L2C OPY (Huang et al., 2019) Table 2: Results on the automatic post-editing task (extremely low- and high-resource). “D UAL BART”: a method to leverage pretrained Seq2Seq models adapted from “D UAL B ERT”. Please refer to Appendix A.3 for detailed descriptions of baselines and the same below. “M”: significantly better than “D UAL B ERT” (p < 0.01). “?”: significantly better than “D UAL BART” (p < 0.01). Models Source Pretraining TEST14 M ULTI R NN (Zoph and Knight, 2016) D UALT RANS (Junczys-Dowmunt and Grundkiewicz, 2018) (De"
2021.acl-long.446,W17-4773,0,0.124791,"that are complementary have proven to be helpful for many sequence generation tasks such as question answering (Antol et al., ∗ Multi-source SG AutoEncoding Seq2Seq Introduction Corresponding author: Yang Liu The source code is available at https://github. com/THUNLP-MT/TRICE 1 Single-source SG 2015), machine translation (Huang et al., 2016), and speech recognition (Dupont and Luettin, 2000). In natural language processing, multiple textual inputs have also been shown to be valuable for sequence generation tasks such as multi-source translation (Zoph and Knight, 2016), automatic postediting (Chatterjee et al., 2017), multi-document summarization (Haghighi and Vanderwende, 2009), system combination for NMT (Huang et al., 2020), and document-level machine translation (Wang et al., 2017). We refer to this kind of tasks as multi-source sequence generation (MSG). Unfortunately, MSG tasks face a severe challenge: there are no sufficient data to train MSG models. For example, multi-source translation requires parallel corpora involving multiple languages, which are usually restricted in quantity and coverage. Recently, as pretraining language models that take advantage of massive unlabeled data have proven to i"
2021.acl-long.446,W19-5402,0,0.0160071,"ult of the separated cross-attention sublayer, and the parameters of separated cross-attentions to leverage each source are shared. Finally, a feedforward network is the last sublayer of a decoder layer. In this way, the decoder in our framework can better handle representations of multiple sources. 3 Experiments 3.1 Setup Datasets We evaluated our framework on three MSG tasks: (1) automatic post-editing (APE), (2) multi-source translation, and (3) document-level translation. For the APE task, following Correia and Martins (2019), we used the data from the WMT17 APE task (English-German SMT) (Chatterjee et al., 2019). The dataset contains 23K dual-source examples (e.g., hEnglish source sentence, German translation, German post-editi) for training in an extremely low-resource setting. We also followed Correia and Martins (2019) to adopt pseudo data (Junczys-Dowmunt and Grundkiewicz, 2016; Negri et al., 2018), which contains about 8M pseudo training examples, to evaluate our framework in a high-resource setting. We adopted the dev16 for development and used test16 and test17 for testing. For the multi-source translation task, following Zoph and Knight (2016), we used a subset of the WMT14 news dataset (Boja"
2021.acl-long.446,2020.acl-main.747,0,0.0976652,"Missing"
2021.acl-long.446,P19-1292,0,0.0786151,"retrained selfattention layers to capture cross-source information is not sufficient. Therefore, we propose a two-stage finetuning method to alleviate the pretrain-finetune discrepancy and introduce a novel MSG model with a fine encoder to learn better representations in MSG tasks. Experiments show that our approach achieves new state-of-the-art results on the WMT17 APE task and multi-source translation task using the WMT14 test set. When adapted to documentlevel translation, our framework outperforms strong baselines significantly.1 1 B ERT- FUSED D UAL B ERT (e.g., BERT) (Zhu et al., 2019) (Correia and Martins, 2019) (e.g., BART) M BART-T RANS (Liu et al., 2020) this work Table 1: Comparison of various approaches to transferring pretrained models to single-source and multisource sequence generation tasks. Different from prior studies, this work aims at transferring pretrained sequence-to-sequence models to multi-source sequence generation tasks. Thanks to the continuous representations widely used across text, speech, and image, neural networks that accept multiple sources as input have gained increasing attention in the community (Ive et al., 2019; Dupont and Luettin, 2000). For example, multi-modal inpu"
2021.acl-long.446,N19-1423,0,0.66529,"ion for NMT (Huang et al., 2020), and document-level machine translation (Wang et al., 2017). We refer to this kind of tasks as multi-source sequence generation (MSG). Unfortunately, MSG tasks face a severe challenge: there are no sufficient data to train MSG models. For example, multi-source translation requires parallel corpora involving multiple languages, which are usually restricted in quantity and coverage. Recently, as pretraining language models that take advantage of massive unlabeled data have proven to improve natural language understanding (NLU) and generation tasks substantially (Devlin et al., 2019; Liu et al., 2019; Lewis et al., 2020), a number of researchers have proposed to leverage pretrained language models to enhance MSG tasks (Correia and Martins, 2019; Lee et al., 2020; Lee, 2020). For example, Correia and Martins (2019) show that pretrained autoencoding (AE) models 5738 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 5738–5750 August 1–6, 2021. ©2021 Association for Computational Linguistics Pretraining on unlabeled data for SSG Single-source finetuning on"
2021.acl-long.446,W16-2360,0,0.0304034,"n tasks. Thanks to the continuous representations widely used across text, speech, and image, neural networks that accept multiple sources as input have gained increasing attention in the community (Ive et al., 2019; Dupont and Luettin, 2000). For example, multi-modal inputs that are complementary have proven to be helpful for many sequence generation tasks such as question answering (Antol et al., ∗ Multi-source SG AutoEncoding Seq2Seq Introduction Corresponding author: Yang Liu The source code is available at https://github. com/THUNLP-MT/TRICE 1 Single-source SG 2015), machine translation (Huang et al., 2016), and speech recognition (Dupont and Luettin, 2000). In natural language processing, multiple textual inputs have also been shown to be valuable for sequence generation tasks such as multi-source translation (Zoph and Knight, 2016), automatic postediting (Chatterjee et al., 2017), multi-document summarization (Haghighi and Vanderwende, 2009), system combination for NMT (Huang et al., 2020), and document-level machine translation (Wang et al., 2017). We refer to this kind of tasks as multi-source sequence generation (MSG). Unfortunately, MSG tasks face a severe challenge: there are no sufficien"
2021.acl-long.446,D19-1634,1,0.930675,"ng TEST17 TER BLEU TER BLEU — 22.89 — 23.08 65.57 D UAL B ERT (Correia and Martins, 2019) D UAL BART (Correia and Martins, 2019) mBERT mBART 18.88 18.26 71.61 72.65 19.03 18.41 70.66 72.08 TRICE mBART 17.41M? 73.43M? 17.75M? 72.70M? — — 17.81 17.45 72.79 73.51 18.10 17.77 71.72 72.98 D UAL B ERT (Correia and Martins, 2019) D UAL BART (Correia and Martins, 2019) mBERT mBART 16.91 16.40 74.29 74.74 17.26 17.26 73.42 73.56 TRICE mBART 16.09M? 75.39M? 16.91M? 74.09M? extremely low-resource F ORCEDATT (Berard et al., 2017) high-resource D UALT RANS (Junczys-Dowmunt and Grundkiewicz, 2018) L2C OPY (Huang et al., 2019) Table 2: Results on the automatic post-editing task (extremely low- and high-resource). “D UAL BART”: a method to leverage pretrained Seq2Seq models adapted from “D UAL B ERT”. Please refer to Appendix A.3 for detailed descriptions of baselines and the same below. “M”: significantly better than “D UAL B ERT” (p < 0.01). “?”: significantly better than “D UAL BART” (p < 0.01). Models Source Pretraining TEST14 M ULTI R NN (Zoph and Knight, 2016) D UALT RANS (Junczys-Dowmunt and Grundkiewicz, 2018) (De, Fr) — — 30.0 37.0 (De, Fr) M BART-T RANS (Liu et al., 2020) De mBART M BART-T RANS (Liu et al."
2021.acl-long.446,P19-1653,0,0.0244876,"ED D UAL B ERT (e.g., BERT) (Zhu et al., 2019) (Correia and Martins, 2019) (e.g., BART) M BART-T RANS (Liu et al., 2020) this work Table 1: Comparison of various approaches to transferring pretrained models to single-source and multisource sequence generation tasks. Different from prior studies, this work aims at transferring pretrained sequence-to-sequence models to multi-source sequence generation tasks. Thanks to the continuous representations widely used across text, speech, and image, neural networks that accept multiple sources as input have gained increasing attention in the community (Ive et al., 2019; Dupont and Luettin, 2000). For example, multi-modal inputs that are complementary have proven to be helpful for many sequence generation tasks such as question answering (Antol et al., ∗ Multi-source SG AutoEncoding Seq2Seq Introduction Corresponding author: Yang Liu The source code is available at https://github. com/THUNLP-MT/TRICE 1 Single-source SG 2015), machine translation (Huang et al., 2016), and speech recognition (Dupont and Luettin, 2000). In natural language processing, multiple textual inputs have also been shown to be valuable for sequence generation tasks such as multi-source"
2021.acl-long.446,W16-2378,0,0.0232202,"le representations of multiple sources. 3 Experiments 3.1 Setup Datasets We evaluated our framework on three MSG tasks: (1) automatic post-editing (APE), (2) multi-source translation, and (3) document-level translation. For the APE task, following Correia and Martins (2019), we used the data from the WMT17 APE task (English-German SMT) (Chatterjee et al., 2019). The dataset contains 23K dual-source examples (e.g., hEnglish source sentence, German translation, German post-editi) for training in an extremely low-resource setting. We also followed Correia and Martins (2019) to adopt pseudo data (Junczys-Dowmunt and Grundkiewicz, 2016; Negri et al., 2018), which contains about 8M pseudo training examples, to evaluate our framework in a high-resource setting. We adopted the dev16 for development and used test16 and test17 for testing. For the multi-source translation task, following Zoph and Knight (2016), we used a subset of the WMT14 news dataset (Bojar et al., 2014), 3 There is little difference between the “parallel attention combination strategy” proposed by Libovick`y et al. (2018) and our method. 5742 Models TEST16 Pretraining TEST17 TER BLEU TER BLEU — 22.89 — 23.08 65.57 D UAL B ERT (Correia and Martins, 2019) D UA"
2021.acl-long.446,W18-6467,0,0.0195983,"018) and our method. 5742 Models TEST16 Pretraining TEST17 TER BLEU TER BLEU — 22.89 — 23.08 65.57 D UAL B ERT (Correia and Martins, 2019) D UAL BART (Correia and Martins, 2019) mBERT mBART 18.88 18.26 71.61 72.65 19.03 18.41 70.66 72.08 TRICE mBART 17.41M? 73.43M? 17.75M? 72.70M? — — 17.81 17.45 72.79 73.51 18.10 17.77 71.72 72.98 D UAL B ERT (Correia and Martins, 2019) D UAL BART (Correia and Martins, 2019) mBERT mBART 16.91 16.40 74.29 74.74 17.26 17.26 73.42 73.56 TRICE mBART 16.09M? 75.39M? 16.91M? 74.09M? extremely low-resource F ORCEDATT (Berard et al., 2017) high-resource D UALT RANS (Junczys-Dowmunt and Grundkiewicz, 2018) L2C OPY (Huang et al., 2019) Table 2: Results on the automatic post-editing task (extremely low- and high-resource). “D UAL BART”: a method to leverage pretrained Seq2Seq models adapted from “D UAL B ERT”. Please refer to Appendix A.3 for detailed descriptions of baselines and the same below. “M”: significantly better than “D UAL B ERT” (p < 0.01). “?”: significantly better than “D UAL BART” (p < 0.01). Models Source Pretraining TEST14 M ULTI R NN (Zoph and Knight, 2016) D UALT RANS (Junczys-Dowmunt and Grundkiewicz, 2018) (De, Fr) — — 30.0 37.0 (De, Fr) M BART-T RANS (Liu et al., 2020) De mB"
2021.acl-long.446,W04-3250,0,0.0195956,"ains 250K tokens. We used minibatch sizes of 256, 1,024, 4,096, and 16,384 tokens for extremely low-, low-, medium-, and high-resource settings, respectively. We used the development set to tune the hyper-parameters and select the best model. In inference, the beamsize was set to 4. Please refer to Appendix A.1 for more details. Evaluation Metrics We used case-sensitive BLEU (multi-bleu.perl) and TER for automatic post-editing. For multi-source translation and document-level translation, S ACRE BLEU5 (Post, 2018) and METEOR6 was adopted for evaluation. We used the paired bootstrap resampling (Koehn, 2004) for statistical significance tests. 3.2 Main Results Table 2 shows the results on the automatic postediting task. Our framework outperforms previous methods without pretraining (i.e., F ORCEDATT, 4 A dual-source example can be obtained by matching two single-source examples. 5 The signature is “BLEU+case.mixed+numrefs.1+smooth .exp+tok.13a+version.1.4.14”. 6 https://www.cs.cmu.edu/˜alavie/ METEOR/ Variants None FFN adapter (Guo et al., 2020) Fine encoder (Nf Fine encoder (Nf Fine encoder (Nf Fine encoder (Nf = 1) w/o CA = 1) = 2) = 3) #Para. BLEU 0M 100.7M 73.65 73.71 12.5M 16.8M 33.6M 50.4M"
2021.acl-long.446,2020.wmt-1.81,0,0.136117,"re challenge: there are no sufficient data to train MSG models. For example, multi-source translation requires parallel corpora involving multiple languages, which are usually restricted in quantity and coverage. Recently, as pretraining language models that take advantage of massive unlabeled data have proven to improve natural language understanding (NLU) and generation tasks substantially (Devlin et al., 2019; Liu et al., 2019; Lewis et al., 2020), a number of researchers have proposed to leverage pretrained language models to enhance MSG tasks (Correia and Martins, 2019; Lee et al., 2020; Lee, 2020). For example, Correia and Martins (2019) show that pretrained autoencoding (AE) models 5738 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 5738–5750 August 1–6, 2021. ©2021 Association for Computational Linguistics Pretraining on unlabeled data for SSG Single-source finetuning on labeled data for SSG A B C Decoder Decoder Decoder Encoder Encoder A! B! C C Decoder Decoder Encoder Encoder Encoder C! A B Transfer Multi-source finetuning on labeled data for MSG C Transfer Dec"
2021.acl-long.446,2020.wmt-1.82,0,0.103471,"tasks face a severe challenge: there are no sufficient data to train MSG models. For example, multi-source translation requires parallel corpora involving multiple languages, which are usually restricted in quantity and coverage. Recently, as pretraining language models that take advantage of massive unlabeled data have proven to improve natural language understanding (NLU) and generation tasks substantially (Devlin et al., 2019; Liu et al., 2019; Lewis et al., 2020), a number of researchers have proposed to leverage pretrained language models to enhance MSG tasks (Correia and Martins, 2019; Lee et al., 2020; Lee, 2020). For example, Correia and Martins (2019) show that pretrained autoencoding (AE) models 5738 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 5738–5750 August 1–6, 2021. ©2021 Association for Computational Linguistics Pretraining on unlabeled data for SSG Single-source finetuning on labeled data for SSG A B C Decoder Decoder Decoder Encoder Encoder A! B! C C Decoder Decoder Encoder Encoder Encoder C! A B Transfer Multi-source finetuning on labeled data for MSG C"
2021.acl-long.446,2020.acl-main.703,0,0.544642,"ocument-level machine translation (Wang et al., 2017). We refer to this kind of tasks as multi-source sequence generation (MSG). Unfortunately, MSG tasks face a severe challenge: there are no sufficient data to train MSG models. For example, multi-source translation requires parallel corpora involving multiple languages, which are usually restricted in quantity and coverage. Recently, as pretraining language models that take advantage of massive unlabeled data have proven to improve natural language understanding (NLU) and generation tasks substantially (Devlin et al., 2019; Liu et al., 2019; Lewis et al., 2020), a number of researchers have proposed to leverage pretrained language models to enhance MSG tasks (Correia and Martins, 2019; Lee et al., 2020; Lee, 2020). For example, Correia and Martins (2019) show that pretrained autoencoding (AE) models 5738 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 5738–5750 August 1–6, 2021. ©2021 Association for Computational Linguistics Pretraining on unlabeled data for SSG Single-source finetuning on labeled data for SSG A B C Decoder Deco"
2021.acl-long.446,N09-1041,0,0.395835,"ny sequence generation tasks such as question answering (Antol et al., ∗ Multi-source SG AutoEncoding Seq2Seq Introduction Corresponding author: Yang Liu The source code is available at https://github. com/THUNLP-MT/TRICE 1 Single-source SG 2015), machine translation (Huang et al., 2016), and speech recognition (Dupont and Luettin, 2000). In natural language processing, multiple textual inputs have also been shown to be valuable for sequence generation tasks such as multi-source translation (Zoph and Knight, 2016), automatic postediting (Chatterjee et al., 2017), multi-document summarization (Haghighi and Vanderwende, 2009), system combination for NMT (Huang et al., 2020), and document-level machine translation (Wang et al., 2017). We refer to this kind of tasks as multi-source sequence generation (MSG). Unfortunately, MSG tasks face a severe challenge: there are no sufficient data to train MSG models. For example, multi-source translation requires parallel corpora involving multiple languages, which are usually restricted in quantity and coverage. Recently, as pretraining language models that take advantage of massive unlabeled data have proven to improve natural language understanding (NLU) and generation task"
2021.acl-long.446,W18-6326,0,0.0468509,"Missing"
2021.acl-long.446,2020.tacl-1.47,0,0.292357,"information is not sufficient. Therefore, we propose a two-stage finetuning method to alleviate the pretrain-finetune discrepancy and introduce a novel MSG model with a fine encoder to learn better representations in MSG tasks. Experiments show that our approach achieves new state-of-the-art results on the WMT17 APE task and multi-source translation task using the WMT14 test set. When adapted to documentlevel translation, our framework outperforms strong baselines significantly.1 1 B ERT- FUSED D UAL B ERT (e.g., BERT) (Zhu et al., 2019) (Correia and Martins, 2019) (e.g., BART) M BART-T RANS (Liu et al., 2020) this work Table 1: Comparison of various approaches to transferring pretrained models to single-source and multisource sequence generation tasks. Different from prior studies, this work aims at transferring pretrained sequence-to-sequence models to multi-source sequence generation tasks. Thanks to the continuous representations widely used across text, speech, and image, neural networks that accept multiple sources as input have gained increasing attention in the community (Ive et al., 2019; Dupont and Luettin, 2000). For example, multi-modal inputs that are complementary have proven to be he"
2021.acl-long.446,2021.ccl-1.108,0,0.0777017,"Missing"
2021.acl-long.446,D17-1301,0,0.152128,"Corresponding author: Yang Liu The source code is available at https://github. com/THUNLP-MT/TRICE 1 Single-source SG 2015), machine translation (Huang et al., 2016), and speech recognition (Dupont and Luettin, 2000). In natural language processing, multiple textual inputs have also been shown to be valuable for sequence generation tasks such as multi-source translation (Zoph and Knight, 2016), automatic postediting (Chatterjee et al., 2017), multi-document summarization (Haghighi and Vanderwende, 2009), system combination for NMT (Huang et al., 2020), and document-level machine translation (Wang et al., 2017). We refer to this kind of tasks as multi-source sequence generation (MSG). Unfortunately, MSG tasks face a severe challenge: there are no sufficient data to train MSG models. For example, multi-source translation requires parallel corpora involving multiple languages, which are usually restricted in quantity and coverage. Recently, as pretraining language models that take advantage of massive unlabeled data have proven to improve natural language understanding (NLU) and generation tasks substantially (Devlin et al., 2019; Liu et al., 2019; Lewis et al., 2020), a number of researchers have pro"
2021.acl-long.446,N19-1313,0,0.0486746,"Missing"
2021.acl-long.446,L18-1004,0,0.105585,"3 Experiments 3.1 Setup Datasets We evaluated our framework on three MSG tasks: (1) automatic post-editing (APE), (2) multi-source translation, and (3) document-level translation. For the APE task, following Correia and Martins (2019), we used the data from the WMT17 APE task (English-German SMT) (Chatterjee et al., 2019). The dataset contains 23K dual-source examples (e.g., hEnglish source sentence, German translation, German post-editi) for training in an extremely low-resource setting. We also followed Correia and Martins (2019) to adopt pseudo data (Junczys-Dowmunt and Grundkiewicz, 2016; Negri et al., 2018), which contains about 8M pseudo training examples, to evaluate our framework in a high-resource setting. We adopted the dev16 for development and used test16 and test17 for testing. For the multi-source translation task, following Zoph and Knight (2016), we used a subset of the WMT14 news dataset (Bojar et al., 2014), 3 There is little difference between the “parallel attention combination strategy” proposed by Libovick`y et al. (2018) and our method. 5742 Models TEST16 Pretraining TEST17 TER BLEU TER BLEU — 22.89 — 23.08 65.57 D UAL B ERT (Correia and Martins, 2019) D UAL BART (Correia and M"
2021.acl-long.446,W18-6319,0,0.0122834,"and the number of heads are the same as mBART. We adopted the vocabulary of mBART, which contains 250K tokens. We used minibatch sizes of 256, 1,024, 4,096, and 16,384 tokens for extremely low-, low-, medium-, and high-resource settings, respectively. We used the development set to tune the hyper-parameters and select the best model. In inference, the beamsize was set to 4. Please refer to Appendix A.1 for more details. Evaluation Metrics We used case-sensitive BLEU (multi-bleu.perl) and TER for automatic post-editing. For multi-source translation and document-level translation, S ACRE BLEU5 (Post, 2018) and METEOR6 was adopted for evaluation. We used the paired bootstrap resampling (Koehn, 2004) for statistical significance tests. 3.2 Main Results Table 2 shows the results on the automatic postediting task. Our framework outperforms previous methods without pretraining (i.e., F ORCEDATT, 4 A dual-source example can be obtained by matching two single-source examples. 5 The signature is “BLEU+case.mixed+numrefs.1+smooth .exp+tok.13a+version.1.4.14”. 6 https://www.cs.cmu.edu/˜alavie/ METEOR/ Variants None FFN adapter (Guo et al., 2020) Fine encoder (Nf Fine encoder (Nf Fine encoder (Nf Fine enc"
2021.acl-long.446,D19-1164,0,0.0419344,"Missing"
2021.acl-long.446,D18-1049,1,0.845923,"the fine encoder could capture finer cross-source information, which helps correct translation errors. 4 4.1 Related Work Multi-source Sequence Generation Multi-source sequence generation includes multisource translation (Zoph and Knight, 2016), automatic post-editing (Chatterjee et al., 2017), multidocument summarization (Haghighi and Vanderwende, 2009), system combination for NMT (Huang et al., 2020), and document-level machine translation (Wang et al., 2017), etc. For these tasks, researchers usually leverage multi-encoder architectures to achieve better performance (Zoph and Knight, 2016; Zhang et al., 2018; Huang et al., 2019). To address the data scarcity problem in MSG, some researchers generate pseudo corpora (Negri et al., 2018; Nishimura et al., 2020) to augment the corpus size while others try to make use of pretrained autoencoding models (e.g., BERT 5745 Normal Models M BART-T RANS M BART-T RANS D UAL BART TRICE (De) (Fr) Randomized Fr Randomized De BLEU METEOR BLEU METEOR BLEU METEOR 31.8 34.8 40.2 41.5 33.9 37.9 38.9 39.8 — — 11.3 13.5 — — 13.1 15.0 — — 24.9 23.0 — — 26.4 23.9 Table 8: Adversarial evaluation on the multi-source translation task. “Randomized Fr/De” denotes that the Fr/D"
2021.acl-long.446,N16-1004,0,0.41941,"Luettin, 2000). For example, multi-modal inputs that are complementary have proven to be helpful for many sequence generation tasks such as question answering (Antol et al., ∗ Multi-source SG AutoEncoding Seq2Seq Introduction Corresponding author: Yang Liu The source code is available at https://github. com/THUNLP-MT/TRICE 1 Single-source SG 2015), machine translation (Huang et al., 2016), and speech recognition (Dupont and Luettin, 2000). In natural language processing, multiple textual inputs have also been shown to be valuable for sequence generation tasks such as multi-source translation (Zoph and Knight, 2016), automatic postediting (Chatterjee et al., 2017), multi-document summarization (Haghighi and Vanderwende, 2009), system combination for NMT (Huang et al., 2020), and document-level machine translation (Wang et al., 2017). We refer to this kind of tasks as multi-source sequence generation (MSG). Unfortunately, MSG tasks face a severe challenge: there are no sufficient data to train MSG models. For example, multi-source translation requires parallel corpora involving multiple languages, which are usually restricted in quantity and coverage. Recently, as pretraining language models that take adv"
2021.bionlp-1.23,2020.coling-main.24,1,0.905505,"ferential Diagnosis) of the patient, distinguish it from other diseases and exclude the possible diagnosis of other diseases 治疗计划 (Treatment Plan) Class Patient’s name, gender, age, reason for admission, time of admission Medical examinations to be done in the next step, and preliminary treatment plan Table 1: The major five parts of information contained in one First Course Record in Chinese EMRs. 2 Related Work NLP for medical text has draw many attentions in the recent years (Xue et al., 2012; Xu et al., 2015; Li et al., 2019; Tian et al., 2019, 2020a; Song et al., 2020; Wang et al., 2020; Chen et al., 2020b), especially for the EMR texts. Among different tasks to process Chinese EMR texts, CWS and medical concept recognition are two fundamental ones that draw much attentions from previous studies. Due to the dramatic performance drop when applying the model trained from open source corpus on the medical field, previous studies (Xu et al., 2014, 2015; Li et al., 2015; Zhang et al., 2016; He et al., 2017) always construct Chinese medical datasets themselves and test their models on the datasets. However, most constructed datasets used for CWS are relatively small, where there are only roughly 100"
2021.bionlp-1.23,D15-1141,0,0.0256855,"nd can be used to describe the body, abnormalities, etc. E.g., “胃 肠型感冒” (gastrointestinal cold) where “胃肠型” (gastrointestinal) are Qualitative medical concepts. 4 Methods A good text representation is highly important in achieving a promising performance in many NLP tasks (Song et al., 2017; Liu and Lapata, 2018; Song and Shi, 2018). Therefore, we select several well-known models for CWS and medical concept recognition tasks and test them on ACEMR corpus. 4.1 CWS for Chinese EMR For CWS, we follow the convention in previous CWS studies (Sun and Xu, 2011; Song et al., 2012; Song and Xia, 2013; Chen et al., 2015; Zhang et al., 2016; Qiu et al., 2019) to regard it as a sequence labeling task with the “BIES” scheme. We select four well-know models, namely, BiLSTM, BERT (Devlin et al., 2019), ZEN (Diao et al., 2020), and WMSeg (Tian et al., 2020d) with softmax and CRF decoder. Herein, BERT and ZEN are pre-trained language models that have achieved state-of-the-art performance in many NLP tasks (Liang et al., 2020; Tian et al., 2020c; Yu et al., 2020; Nie et al., 2020; Luoma and Pyysalo, 2020; Chen et al., 2020a; Helwe et al., 2020; Tian et al., 2021a,b). WMSeg is CWS model that leverages key-value memor"
2021.bionlp-1.23,2020.emnlp-main.112,1,0.905694,"ferential Diagnosis) of the patient, distinguish it from other diseases and exclude the possible diagnosis of other diseases 治疗计划 (Treatment Plan) Class Patient’s name, gender, age, reason for admission, time of admission Medical examinations to be done in the next step, and preliminary treatment plan Table 1: The major five parts of information contained in one First Course Record in Chinese EMRs. 2 Related Work NLP for medical text has draw many attentions in the recent years (Xue et al., 2012; Xu et al., 2015; Li et al., 2019; Tian et al., 2019, 2020a; Song et al., 2020; Wang et al., 2020; Chen et al., 2020b), especially for the EMR texts. Among different tasks to process Chinese EMR texts, CWS and medical concept recognition are two fundamental ones that draw much attentions from previous studies. Due to the dramatic performance drop when applying the model trained from open source corpus on the medical field, previous studies (Xu et al., 2014, 2015; Li et al., 2015; Zhang et al., 2016; He et al., 2017) always construct Chinese medical datasets themselves and test their models on the datasets. However, most constructed datasets used for CWS are relatively small, where there are only roughly 100"
2021.bionlp-1.23,N19-1423,0,0.0194832,"text representation is highly important in achieving a promising performance in many NLP tasks (Song et al., 2017; Liu and Lapata, 2018; Song and Shi, 2018). Therefore, we select several well-known models for CWS and medical concept recognition tasks and test them on ACEMR corpus. 4.1 CWS for Chinese EMR For CWS, we follow the convention in previous CWS studies (Sun and Xu, 2011; Song et al., 2012; Song and Xia, 2013; Chen et al., 2015; Zhang et al., 2016; Qiu et al., 2019) to regard it as a sequence labeling task with the “BIES” scheme. We select four well-know models, namely, BiLSTM, BERT (Devlin et al., 2019), ZEN (Diao et al., 2020), and WMSeg (Tian et al., 2020d) with softmax and CRF decoder. Herein, BERT and ZEN are pre-trained language models that have achieved state-of-the-art performance in many NLP tasks (Liang et al., 2020; Tian et al., 2020c; Yu et al., 2020; Nie et al., 2020; Luoma and Pyysalo, 2020; Chen et al., 2020a; Helwe et al., 2020; Tian et al., 2021a,b). WMSeg is CWS model that leverages key-value memory networks (KVMN) (Miller et al., 2016) to incorporate wordhood information to improve model performance, which achieves state-of-the-art performance on many CWS benchmark datasets"
2021.bionlp-1.23,2020.findings-emnlp.425,1,0.793346,"hly important in achieving a promising performance in many NLP tasks (Song et al., 2017; Liu and Lapata, 2018; Song and Shi, 2018). Therefore, we select several well-known models for CWS and medical concept recognition tasks and test them on ACEMR corpus. 4.1 CWS for Chinese EMR For CWS, we follow the convention in previous CWS studies (Sun and Xu, 2011; Song et al., 2012; Song and Xia, 2013; Chen et al., 2015; Zhang et al., 2016; Qiu et al., 2019) to regard it as a sequence labeling task with the “BIES” scheme. We select four well-know models, namely, BiLSTM, BERT (Devlin et al., 2019), ZEN (Diao et al., 2020), and WMSeg (Tian et al., 2020d) with softmax and CRF decoder. Herein, BERT and ZEN are pre-trained language models that have achieved state-of-the-art performance in many NLP tasks (Liang et al., 2020; Tian et al., 2020c; Yu et al., 2020; Nie et al., 2020; Luoma and Pyysalo, 2020; Chen et al., 2020a; Helwe et al., 2020; Tian et al., 2021a,b). WMSeg is CWS model that leverages key-value memory networks (KVMN) (Miller et al., 2016) to incorporate wordhood information to improve model performance, which achieves state-of-the-art performance on many CWS benchmark datasets. 4.2 Medical Concept Rec"
2021.bionlp-1.23,2020.wanlp-1.5,0,0.0410023,"ious CWS studies (Sun and Xu, 2011; Song et al., 2012; Song and Xia, 2013; Chen et al., 2015; Zhang et al., 2016; Qiu et al., 2019) to regard it as a sequence labeling task with the “BIES” scheme. We select four well-know models, namely, BiLSTM, BERT (Devlin et al., 2019), ZEN (Diao et al., 2020), and WMSeg (Tian et al., 2020d) with softmax and CRF decoder. Herein, BERT and ZEN are pre-trained language models that have achieved state-of-the-art performance in many NLP tasks (Liang et al., 2020; Tian et al., 2020c; Yu et al., 2020; Nie et al., 2020; Luoma and Pyysalo, 2020; Chen et al., 2020a; Helwe et al., 2020; Tian et al., 2021a,b). WMSeg is CWS model that leverages key-value memory networks (KVMN) (Miller et al., 2016) to incorporate wordhood information to improve model performance, which achieves state-of-the-art performance on many CWS benchmark datasets. 4.2 Medical Concept Recognition Similarly, for medical concept recognition, we regard it as a character-based sequence labeling task and perform it in a similar way with named entity 215 If a Chinese word is translated into multiple English words, we use “*” in the English translation to mark its boundary in Table 4. E.g., “3天” is translated"
2021.bionlp-1.23,Q18-1005,1,0.837874,"otated medical sentence in ACEMR with the corresponding English translations. The abbreviations of tags are used for annotation. ment behaviors. E.g., “予” (given), “入院治疗” (admission to hospital for treatment). Qualitative emphasizes a qualitative description of something, rather than a direct measurement and can be used to describe the body, abnormalities, etc. E.g., “胃 肠型感冒” (gastrointestinal cold) where “胃肠型” (gastrointestinal) are Qualitative medical concepts. 4 Methods A good text representation is highly important in achieving a promising performance in many NLP tasks (Song et al., 2017; Liu and Lapata, 2018; Song and Shi, 2018). Therefore, we select several well-known models for CWS and medical concept recognition tasks and test them on ACEMR corpus. 4.1 CWS for Chinese EMR For CWS, we follow the convention in previous CWS studies (Sun and Xu, 2011; Song et al., 2012; Song and Xia, 2013; Chen et al., 2015; Zhang et al., 2016; Qiu et al., 2019) to regard it as a sequence labeling task with the “BIES” scheme. We select four well-know models, namely, BiLSTM, BERT (Devlin et al., 2019), ZEN (Diao et al., 2020), and WMSeg (Tian et al., 2020d) with softmax and CRF decoder. Herein, BERT and ZEN are pre"
2021.bionlp-1.23,2020.coling-main.78,0,0.0400041,"EMR For CWS, we follow the convention in previous CWS studies (Sun and Xu, 2011; Song et al., 2012; Song and Xia, 2013; Chen et al., 2015; Zhang et al., 2016; Qiu et al., 2019) to regard it as a sequence labeling task with the “BIES” scheme. We select four well-know models, namely, BiLSTM, BERT (Devlin et al., 2019), ZEN (Diao et al., 2020), and WMSeg (Tian et al., 2020d) with softmax and CRF decoder. Herein, BERT and ZEN are pre-trained language models that have achieved state-of-the-art performance in many NLP tasks (Liang et al., 2020; Tian et al., 2020c; Yu et al., 2020; Nie et al., 2020; Luoma and Pyysalo, 2020; Chen et al., 2020a; Helwe et al., 2020; Tian et al., 2021a,b). WMSeg is CWS model that leverages key-value memory networks (KVMN) (Miller et al., 2016) to incorporate wordhood information to improve model performance, which achieves state-of-the-art performance on many CWS benchmark datasets. 4.2 Medical Concept Recognition Similarly, for medical concept recognition, we regard it as a character-based sequence labeling task and perform it in a similar way with named entity 215 If a Chinese word is translated into multiple English words, we use “*” in the English translation to mark its bounda"
2021.bionlp-1.23,P14-5010,0,0.00252111,"b-class “BP”, and thus the tags for the three characters are “B-BP”, “I-BP”, and “E-BP”, respectively. We try BiLSTM, BERT, ZEN, as well as TwASP (Tian et al., 2020b) with the CRF decoder for medical concept recognition. TwASP is a model that leverages the auto-generated syntactic information (e.g., the POS tags (POS), the dependency relations (Dep.), and the syntactic constituents (Syn.)) through a two-way attention mechanism to improve model performance for sequence labeling tasks. To obtain the syntactic information of the input sentence required by TwASP, we use Stanford CoreNLP Toolkits (Manning et al., 2014) to obtain the POS tags, the dependency tree, and the constituent syntax tree. Figure 1 shows an example sentence (with English translation) and the three types of the auto-generated syntactic information. 5 BiLSTM + CRF + Tencent Embedding ZEN + CRF + KVMN 99.01 98.99 99.03 Table 6: CWS performance for different composition of training data where +CRF, +KVMN, +Tencent Embedding represent the use of CRF layer, memory network (WMSeg) and Tencent Embedding respectively. character embeddings from Tencent Embedding4 (Song et al., 2018), with the training epoch, batch size, and learning rate set to"
2021.bionlp-1.23,D16-1147,0,0.0238089,"6; Qiu et al., 2019) to regard it as a sequence labeling task with the “BIES” scheme. We select four well-know models, namely, BiLSTM, BERT (Devlin et al., 2019), ZEN (Diao et al., 2020), and WMSeg (Tian et al., 2020d) with softmax and CRF decoder. Herein, BERT and ZEN are pre-trained language models that have achieved state-of-the-art performance in many NLP tasks (Liang et al., 2020; Tian et al., 2020c; Yu et al., 2020; Nie et al., 2020; Luoma and Pyysalo, 2020; Chen et al., 2020a; Helwe et al., 2020; Tian et al., 2021a,b). WMSeg is CWS model that leverages key-value memory networks (KVMN) (Miller et al., 2016) to incorporate wordhood information to improve model performance, which achieves state-of-the-art performance on many CWS benchmark datasets. 4.2 Medical Concept Recognition Similarly, for medical concept recognition, we regard it as a character-based sequence labeling task and perform it in a similar way with named entity 215 If a Chinese word is translated into multiple English words, we use “*” in the English translation to mark its boundary in Table 4. E.g., “3天” is translated into “*3 days*”. Methods Prec. Recall F1 76.85 77.22 CTB Only WMSeg *ZEN is the base model 77.60 CTB+ACEMR Figure"
2021.bionlp-1.23,2020.emnlp-main.107,1,0.762718,"1 CWS for Chinese EMR For CWS, we follow the convention in previous CWS studies (Sun and Xu, 2011; Song et al., 2012; Song and Xia, 2013; Chen et al., 2015; Zhang et al., 2016; Qiu et al., 2019) to regard it as a sequence labeling task with the “BIES” scheme. We select four well-know models, namely, BiLSTM, BERT (Devlin et al., 2019), ZEN (Diao et al., 2020), and WMSeg (Tian et al., 2020d) with softmax and CRF decoder. Herein, BERT and ZEN are pre-trained language models that have achieved state-of-the-art performance in many NLP tasks (Liang et al., 2020; Tian et al., 2020c; Yu et al., 2020; Nie et al., 2020; Luoma and Pyysalo, 2020; Chen et al., 2020a; Helwe et al., 2020; Tian et al., 2021a,b). WMSeg is CWS model that leverages key-value memory networks (KVMN) (Miller et al., 2016) to incorporate wordhood information to improve model performance, which achieves state-of-the-art performance on many CWS benchmark datasets. 4.2 Medical Concept Recognition Similarly, for medical concept recognition, we regard it as a character-based sequence labeling task and perform it in a similar way with named entity 215 If a Chinese word is translated into multiple English words, we use “*” in the English trans"
2021.bionlp-1.23,C12-2116,1,0.75437,"ing, rather than a direct measurement and can be used to describe the body, abnormalities, etc. E.g., “胃 肠型感冒” (gastrointestinal cold) where “胃肠型” (gastrointestinal) are Qualitative medical concepts. 4 Methods A good text representation is highly important in achieving a promising performance in many NLP tasks (Song et al., 2017; Liu and Lapata, 2018; Song and Shi, 2018). Therefore, we select several well-known models for CWS and medical concept recognition tasks and test them on ACEMR corpus. 4.1 CWS for Chinese EMR For CWS, we follow the convention in previous CWS studies (Sun and Xu, 2011; Song et al., 2012; Song and Xia, 2013; Chen et al., 2015; Zhang et al., 2016; Qiu et al., 2019) to regard it as a sequence labeling task with the “BIES” scheme. We select four well-know models, namely, BiLSTM, BERT (Devlin et al., 2019), ZEN (Diao et al., 2020), and WMSeg (Tian et al., 2020d) with softmax and CRF decoder. Herein, BERT and ZEN are pre-trained language models that have achieved state-of-the-art performance in many NLP tasks (Liang et al., 2020; Tian et al., 2020c; Yu et al., 2020; Nie et al., 2020; Luoma and Pyysalo, 2020; Chen et al., 2020a; Helwe et al., 2020; Tian et al., 2021a,b). WMSeg is C"
2021.bionlp-1.23,K17-1016,1,0.832577,": An example of annotated medical sentence in ACEMR with the corresponding English translations. The abbreviations of tags are used for annotation. ment behaviors. E.g., “予” (given), “入院治疗” (admission to hospital for treatment). Qualitative emphasizes a qualitative description of something, rather than a direct measurement and can be used to describe the body, abnormalities, etc. E.g., “胃 肠型感冒” (gastrointestinal cold) where “胃肠型” (gastrointestinal) are Qualitative medical concepts. 4 Methods A good text representation is highly important in achieving a promising performance in many NLP tasks (Song et al., 2017; Liu and Lapata, 2018; Song and Shi, 2018). Therefore, we select several well-known models for CWS and medical concept recognition tasks and test them on ACEMR corpus. 4.1 CWS for Chinese EMR For CWS, we follow the convention in previous CWS studies (Sun and Xu, 2011; Song et al., 2012; Song and Xia, 2013; Chen et al., 2015; Zhang et al., 2016; Qiu et al., 2019) to regard it as a sequence labeling task with the “BIES” scheme. We select four well-know models, namely, BiLSTM, BERT (Devlin et al., 2019), ZEN (Diao et al., 2020), and WMSeg (Tian et al., 2020d) with softmax and CRF decoder. Herein"
2021.bionlp-1.23,N18-2028,1,0.842704,"ntence required by TwASP, we use Stanford CoreNLP Toolkits (Manning et al., 2014) to obtain the POS tags, the dependency tree, and the constituent syntax tree. Figure 1 shows an example sentence (with English translation) and the three types of the auto-generated syntactic information. 5 BiLSTM + CRF + Tencent Embedding ZEN + CRF + KVMN 99.01 98.99 99.03 Table 6: CWS performance for different composition of training data where +CRF, +KVMN, +Tencent Embedding represent the use of CRF layer, memory network (WMSeg) and Tencent Embedding respectively. character embeddings from Tencent Embedding4 (Song et al., 2018), with the training epoch, batch size, and learning rate set to 50, 32, and 0.001, respectively. For BERT, ZEN, and WMSeg, we use the official settings (e.g., 768 dimensional hidden vectors with 12 multi-head self-attentions for BERT), where the number of training epoch is 50, the batch size is 16, and the learning rate is 1e-5. The experimental results of CWS are presented in Table 6 with three different settings (i.e., CTB Only, CTB+ACEMR, and ACEMR Only). The CTB Only setting displays the results of WMSeg model (with ZEN encoder) when it is trained on CTB6 only and evaluated on the ACEMR te"
2021.bionlp-1.23,2020.coling-main.63,1,0.774205,"断 According to the main complaint (Differential Diagnosis) of the patient, distinguish it from other diseases and exclude the possible diagnosis of other diseases 治疗计划 (Treatment Plan) Class Patient’s name, gender, age, reason for admission, time of admission Medical examinations to be done in the next step, and preliminary treatment plan Table 1: The major five parts of information contained in one First Course Record in Chinese EMRs. 2 Related Work NLP for medical text has draw many attentions in the recent years (Xue et al., 2012; Xu et al., 2015; Li et al., 2019; Tian et al., 2019, 2020a; Song et al., 2020; Wang et al., 2020; Chen et al., 2020b), especially for the EMR texts. Among different tasks to process Chinese EMR texts, CWS and medical concept recognition are two fundamental ones that draw much attentions from previous studies. Due to the dramatic performance drop when applying the model trained from open source corpus on the medical field, previous studies (Xu et al., 2014, 2015; Li et al., 2015; Zhang et al., 2016; He et al., 2017) always construct Chinese medical datasets themselves and test their models on the datasets. However, most constructed datasets used for CWS are relatively s"
2021.bionlp-1.23,W19-5027,1,0.820897,"several disease names 鉴别诊断 According to the main complaint (Differential Diagnosis) of the patient, distinguish it from other diseases and exclude the possible diagnosis of other diseases 治疗计划 (Treatment Plan) Class Patient’s name, gender, age, reason for admission, time of admission Medical examinations to be done in the next step, and preliminary treatment plan Table 1: The major five parts of information contained in one First Course Record in Chinese EMRs. 2 Related Work NLP for medical text has draw many attentions in the recent years (Xue et al., 2012; Xu et al., 2015; Li et al., 2019; Tian et al., 2019, 2020a; Song et al., 2020; Wang et al., 2020; Chen et al., 2020b), especially for the EMR texts. Among different tasks to process Chinese EMR texts, CWS and medical concept recognition are two fundamental ones that draw much attentions from previous studies. Due to the dramatic performance drop when applying the model trained from open source corpus on the medical field, previous studies (Xu et al., 2014, 2015; Li et al., 2015; Zhang et al., 2016; He et al., 2017) always construct Chinese medical datasets themselves and test their models on the datasets. However, most constructed datasets use"
2021.bionlp-1.23,2020.findings-emnlp.378,1,0.816592,"omising performance in many NLP tasks (Song et al., 2017; Liu and Lapata, 2018; Song and Shi, 2018). Therefore, we select several well-known models for CWS and medical concept recognition tasks and test them on ACEMR corpus. 4.1 CWS for Chinese EMR For CWS, we follow the convention in previous CWS studies (Sun and Xu, 2011; Song et al., 2012; Song and Xia, 2013; Chen et al., 2015; Zhang et al., 2016; Qiu et al., 2019) to regard it as a sequence labeling task with the “BIES” scheme. We select four well-know models, namely, BiLSTM, BERT (Devlin et al., 2019), ZEN (Diao et al., 2020), and WMSeg (Tian et al., 2020d) with softmax and CRF decoder. Herein, BERT and ZEN are pre-trained language models that have achieved state-of-the-art performance in many NLP tasks (Liang et al., 2020; Tian et al., 2020c; Yu et al., 2020; Nie et al., 2020; Luoma and Pyysalo, 2020; Chen et al., 2020a; Helwe et al., 2020; Tian et al., 2021a,b). WMSeg is CWS model that leverages key-value memory networks (KVMN) (Miller et al., 2016) to incorporate wordhood information to improve model performance, which achieves state-of-the-art performance on many CWS benchmark datasets. 4.2 Medical Concept Recognition Similarly, for medica"
2021.bionlp-1.23,2020.acl-main.735,1,0.904769,"omising performance in many NLP tasks (Song et al., 2017; Liu and Lapata, 2018; Song and Shi, 2018). Therefore, we select several well-known models for CWS and medical concept recognition tasks and test them on ACEMR corpus. 4.1 CWS for Chinese EMR For CWS, we follow the convention in previous CWS studies (Sun and Xu, 2011; Song et al., 2012; Song and Xia, 2013; Chen et al., 2015; Zhang et al., 2016; Qiu et al., 2019) to regard it as a sequence labeling task with the “BIES” scheme. We select four well-know models, namely, BiLSTM, BERT (Devlin et al., 2019), ZEN (Diao et al., 2020), and WMSeg (Tian et al., 2020d) with softmax and CRF decoder. Herein, BERT and ZEN are pre-trained language models that have achieved state-of-the-art performance in many NLP tasks (Liang et al., 2020; Tian et al., 2020c; Yu et al., 2020; Nie et al., 2020; Luoma and Pyysalo, 2020; Chen et al., 2020a; Helwe et al., 2020; Tian et al., 2021a,b). WMSeg is CWS model that leverages key-value memory networks (KVMN) (Miller et al., 2016) to incorporate wordhood information to improve model performance, which achieves state-of-the-art performance on many CWS benchmark datasets. 4.2 Medical Concept Recognition Similarly, for medica"
2021.bionlp-1.23,2020.coling-main.187,1,0.904792,"omising performance in many NLP tasks (Song et al., 2017; Liu and Lapata, 2018; Song and Shi, 2018). Therefore, we select several well-known models for CWS and medical concept recognition tasks and test them on ACEMR corpus. 4.1 CWS for Chinese EMR For CWS, we follow the convention in previous CWS studies (Sun and Xu, 2011; Song et al., 2012; Song and Xia, 2013; Chen et al., 2015; Zhang et al., 2016; Qiu et al., 2019) to regard it as a sequence labeling task with the “BIES” scheme. We select four well-know models, namely, BiLSTM, BERT (Devlin et al., 2019), ZEN (Diao et al., 2020), and WMSeg (Tian et al., 2020d) with softmax and CRF decoder. Herein, BERT and ZEN are pre-trained language models that have achieved state-of-the-art performance in many NLP tasks (Liang et al., 2020; Tian et al., 2020c; Yu et al., 2020; Nie et al., 2020; Luoma and Pyysalo, 2020; Chen et al., 2020a; Helwe et al., 2020; Tian et al., 2021a,b). WMSeg is CWS model that leverages key-value memory networks (KVMN) (Miller et al., 2016) to incorporate wordhood information to improve model performance, which achieves state-of-the-art performance on many CWS benchmark datasets. 4.2 Medical Concept Recognition Similarly, for medica"
2021.bionlp-1.23,2020.acl-main.734,1,0.904701,"omising performance in many NLP tasks (Song et al., 2017; Liu and Lapata, 2018; Song and Shi, 2018). Therefore, we select several well-known models for CWS and medical concept recognition tasks and test them on ACEMR corpus. 4.1 CWS for Chinese EMR For CWS, we follow the convention in previous CWS studies (Sun and Xu, 2011; Song et al., 2012; Song and Xia, 2013; Chen et al., 2015; Zhang et al., 2016; Qiu et al., 2019) to regard it as a sequence labeling task with the “BIES” scheme. We select four well-know models, namely, BiLSTM, BERT (Devlin et al., 2019), ZEN (Diao et al., 2020), and WMSeg (Tian et al., 2020d) with softmax and CRF decoder. Herein, BERT and ZEN are pre-trained language models that have achieved state-of-the-art performance in many NLP tasks (Liang et al., 2020; Tian et al., 2020c; Yu et al., 2020; Nie et al., 2020; Luoma and Pyysalo, 2020; Chen et al., 2020a; Helwe et al., 2020; Tian et al., 2021a,b). WMSeg is CWS model that leverages key-value memory networks (KVMN) (Miller et al., 2016) to incorporate wordhood information to improve model performance, which achieves state-of-the-art performance on many CWS benchmark datasets. 4.2 Medical Concept Recognition Similarly, for medica"
2021.bionlp-1.23,I13-1071,1,0.786645,"direct measurement and can be used to describe the body, abnormalities, etc. E.g., “胃 肠型感冒” (gastrointestinal cold) where “胃肠型” (gastrointestinal) are Qualitative medical concepts. 4 Methods A good text representation is highly important in achieving a promising performance in many NLP tasks (Song et al., 2017; Liu and Lapata, 2018; Song and Shi, 2018). Therefore, we select several well-known models for CWS and medical concept recognition tasks and test them on ACEMR corpus. 4.1 CWS for Chinese EMR For CWS, we follow the convention in previous CWS studies (Sun and Xu, 2011; Song et al., 2012; Song and Xia, 2013; Chen et al., 2015; Zhang et al., 2016; Qiu et al., 2019) to regard it as a sequence labeling task with the “BIES” scheme. We select four well-know models, namely, BiLSTM, BERT (Devlin et al., 2019), ZEN (Diao et al., 2020), and WMSeg (Tian et al., 2020d) with softmax and CRF decoder. Herein, BERT and ZEN are pre-trained language models that have achieved state-of-the-art performance in many NLP tasks (Liang et al., 2020; Tian et al., 2020c; Yu et al., 2020; Nie et al., 2020; Luoma and Pyysalo, 2020; Chen et al., 2020a; Helwe et al., 2020; Tian et al., 2021a,b). WMSeg is CWS model that levera"
2021.bionlp-1.23,2020.nlpmc-1.3,1,0.735847,"main complaint (Differential Diagnosis) of the patient, distinguish it from other diseases and exclude the possible diagnosis of other diseases 治疗计划 (Treatment Plan) Class Patient’s name, gender, age, reason for admission, time of admission Medical examinations to be done in the next step, and preliminary treatment plan Table 1: The major five parts of information contained in one First Course Record in Chinese EMRs. 2 Related Work NLP for medical text has draw many attentions in the recent years (Xue et al., 2012; Xu et al., 2015; Li et al., 2019; Tian et al., 2019, 2020a; Song et al., 2020; Wang et al., 2020; Chen et al., 2020b), especially for the EMR texts. Among different tasks to process Chinese EMR texts, CWS and medical concept recognition are two fundamental ones that draw much attentions from previous studies. Due to the dramatic performance drop when applying the model trained from open source corpus on the medical field, previous studies (Xu et al., 2014, 2015; Li et al., 2015; Zhang et al., 2016; He et al., 2017) always construct Chinese medical datasets themselves and test their models on the datasets. However, most constructed datasets used for CWS are relatively small, where there a"
2021.bionlp-1.23,D11-1090,0,0.0428177,"cription of something, rather than a direct measurement and can be used to describe the body, abnormalities, etc. E.g., “胃 肠型感冒” (gastrointestinal cold) where “胃肠型” (gastrointestinal) are Qualitative medical concepts. 4 Methods A good text representation is highly important in achieving a promising performance in many NLP tasks (Song et al., 2017; Liu and Lapata, 2018; Song and Shi, 2018). Therefore, we select several well-known models for CWS and medical concept recognition tasks and test them on ACEMR corpus. 4.1 CWS for Chinese EMR For CWS, we follow the convention in previous CWS studies (Sun and Xu, 2011; Song et al., 2012; Song and Xia, 2013; Chen et al., 2015; Zhang et al., 2016; Qiu et al., 2019) to regard it as a sequence labeling task with the “BIES” scheme. We select four well-know models, namely, BiLSTM, BERT (Devlin et al., 2019), ZEN (Diao et al., 2020), and WMSeg (Tian et al., 2020d) with softmax and CRF decoder. Herein, BERT and ZEN are pre-trained language models that have achieved state-of-the-art performance in many NLP tasks (Liang et al., 2020; Tian et al., 2020c; Yu et al., 2020; Nie et al., 2020; Luoma and Pyysalo, 2020; Chen et al., 2020a; Helwe et al., 2020; Tian et al., 2"
2021.bionlp-1.23,2021.naacl-main.231,1,0.755948,"n and Xu, 2011; Song et al., 2012; Song and Xia, 2013; Chen et al., 2015; Zhang et al., 2016; Qiu et al., 2019) to regard it as a sequence labeling task with the “BIES” scheme. We select four well-know models, namely, BiLSTM, BERT (Devlin et al., 2019), ZEN (Diao et al., 2020), and WMSeg (Tian et al., 2020d) with softmax and CRF decoder. Herein, BERT and ZEN are pre-trained language models that have achieved state-of-the-art performance in many NLP tasks (Liang et al., 2020; Tian et al., 2020c; Yu et al., 2020; Nie et al., 2020; Luoma and Pyysalo, 2020; Chen et al., 2020a; Helwe et al., 2020; Tian et al., 2021a,b). WMSeg is CWS model that leverages key-value memory networks (KVMN) (Miller et al., 2016) to incorporate wordhood information to improve model performance, which achieves state-of-the-art performance on many CWS benchmark datasets. 4.2 Medical Concept Recognition Similarly, for medical concept recognition, we regard it as a character-based sequence labeling task and perform it in a similar way with named entity 215 If a Chinese word is translated into multiple English words, we use “*” in the English translation to mark its boundary in Table 4. E.g., “3天” is translated into “*3 days*”. Me"
2021.bionlp-1.23,2021.eacl-main.326,1,0.727122,"n and Xu, 2011; Song et al., 2012; Song and Xia, 2013; Chen et al., 2015; Zhang et al., 2016; Qiu et al., 2019) to regard it as a sequence labeling task with the “BIES” scheme. We select four well-know models, namely, BiLSTM, BERT (Devlin et al., 2019), ZEN (Diao et al., 2020), and WMSeg (Tian et al., 2020d) with softmax and CRF decoder. Herein, BERT and ZEN are pre-trained language models that have achieved state-of-the-art performance in many NLP tasks (Liang et al., 2020; Tian et al., 2020c; Yu et al., 2020; Nie et al., 2020; Luoma and Pyysalo, 2020; Chen et al., 2020a; Helwe et al., 2020; Tian et al., 2021a,b). WMSeg is CWS model that leverages key-value memory networks (KVMN) (Miller et al., 2016) to incorporate wordhood information to improve model performance, which achieves state-of-the-art performance on many CWS benchmark datasets. 4.2 Medical Concept Recognition Similarly, for medical concept recognition, we regard it as a character-based sequence labeling task and perform it in a similar way with named entity 215 If a Chinese word is translated into multiple English words, we use “*” in the English translation to mark its boundary in Table 4. E.g., “3天” is translated into “*3 days*”. Me"
2021.bionlp-1.23,C18-1307,0,0.0182227,"ersity of Hong Kong (Shenzhen), ♥ University of Washington ♣ PingHu Hospital of Shenzhen University 4 Shenzhen Hospital of Shanghai University of Traditional Chinese Medicine ♦ Shenzhen Research Institute of Big Data ♠ yangliu5@link.cuhk.edu.cn ♥ yhtian@uw.edu ♣ wusong@szu.edu.cn ♦ wanxiang@sribd.cn ♠ {changtsunghui,songyan}@cuhk.edu.cn Abstract Chinese word segmentation (CWS) and medical concept recognition are two important and related Chinese word segmentation (CWS) and medtasks for Chinese MLP, which received much attenical concept recognition are two fundamental tion in previous studies (Xing et al., 2018; Wang tasks to process Chinese electronic medical et al., 2019). The first task (i.e., CWS) aims to records (EMRs) and play important roles in segment Chinese text (i.e., character sequence) into downstream tasks for understanding Chinese EMRs. One challenge to these tasks is the words, which is a necessary step for MLP because lack of medical domain datasets with highthe meaning of many medical terms cannot be simquality annotations, especially medical-related ply inferred by its component characters. For examtags that reveal the characteristics of Chinese ple, it is hard to infer the meanin"
2021.bionlp-1.23,2020.acl-main.577,0,0.019368,"ACEMR corpus. 4.1 CWS for Chinese EMR For CWS, we follow the convention in previous CWS studies (Sun and Xu, 2011; Song et al., 2012; Song and Xia, 2013; Chen et al., 2015; Zhang et al., 2016; Qiu et al., 2019) to regard it as a sequence labeling task with the “BIES” scheme. We select four well-know models, namely, BiLSTM, BERT (Devlin et al., 2019), ZEN (Diao et al., 2020), and WMSeg (Tian et al., 2020d) with softmax and CRF decoder. Herein, BERT and ZEN are pre-trained language models that have achieved state-of-the-art performance in many NLP tasks (Liang et al., 2020; Tian et al., 2020c; Yu et al., 2020; Nie et al., 2020; Luoma and Pyysalo, 2020; Chen et al., 2020a; Helwe et al., 2020; Tian et al., 2021a,b). WMSeg is CWS model that leverages key-value memory networks (KVMN) (Miller et al., 2016) to incorporate wordhood information to improve model performance, which achieves state-of-the-art performance on many CWS benchmark datasets. 4.2 Medical Concept Recognition Similarly, for medical concept recognition, we regard it as a character-based sequence labeling task and perform it in a similar way with named entity 215 If a Chinese word is translated into multiple English words, we use “*” in"
2021.disrpt-1.1,W11-0401,0,0.0375854,"140 332 267 50 50 documents 2,162 197 164 units 3,040 19,268 21,789 12,588 4,202 5,855 3,429 2,343 5,537 41,542 3,351 744 744 units 26,048 8,748 1,660 segStyle EDU EDU EDU EDU EDU EDU EDU EDU EDU EDU EDU EDU EDU segStyle Conn Conn Conn relations 2,164 13,897 16,002 9,580 2,533 4,100 2,185 1,608 4,148 28,868 2,240 439 439 relations 43,920 2,451 3,657 relTypes 26 23 17 16 29 17 18 32 32 22 28 24 26 relTypes 23 23 9 discontinuous No Yes Yes No Yes Yes Yes No Yes Yes Yes Yes Yes discontinuous Yes Yes Yes Table 2: Datasets in the DISRPT 2021 Shared Task. − spa.rst.rststb - RST Spanish Treebank (da Cunha et al., 2011). − spa.rst.sctb - RST Spanish-Chinese Treebank (Spanish) (Cao et al., 2018). − tur.pdtb.tdb - Turkish Discourse Bank (Zeyrek and Webber, 2008; Zeyrek and Kurfalı, 2017). − zho.pdtb.cdtb - Chinese Discourse Treebank (Zhou et al., 2014). − zho.rst.sctb - RST Spanish-Chinese Treebank (Chinese) (Cao et al., 2018). A script included in the shared task repository was provided in order to reconstruct the data, which requires users to have access to the original LDC releases of the underlying corpora in the case of PDTB, RST-DT, and CDTB or the original Turkish texts available from Middle East Techni"
2021.disrpt-1.1,N19-1423,0,0.0235502,"Missing"
2021.disrpt-1.1,L16-1432,0,0.0583497,"Missing"
2021.disrpt-1.1,2021.iwpt-1.19,0,0.0378038,"obtain superior results (see Ezzabady et al. 2021), possibly because the single-corpus training results are already so strong. models, which may also be a reason for higher scores on Chinese and especially Turkish, which may be less well-represented by multilingual LMs. Finally we note that in the plain text scenario, all systems except for DisCoDisCo used noncontextualized tools for sentence splitting and/or automatic parsing (SegFormers: CoreNLP; disCut: stanza; TMVM: SpaCy); DisCoDisCo used the tranformer-based sentence splitter from the AMALGUM corpus (Gessler et al., 2020) and DiaParser (Attardi et al., 2021), both with language-specific transformers, with the result that plain text numbers are very close to gold numbers for DisCoDisCo (and in fact insignificantly better for plain Connective Detection: 91.49 on average vs. 91.22 for gold treebanked data). This echoes results from the 2019 task (see Yu et al. 2019) which showed the crucial importance of high quality preprocessing in general, and sentence splitting in particular. Among the Transformer-based systems, although DisCoDisCo performs best overall, other systems score highest on particular datasets in different scenarios, with SegFormers a"
2021.disrpt-1.1,E17-1028,1,0.830073,"ation of the Shared Task included for the first time a track on discourse relation classification across three formalisms: RST, SDRT, and PDTB. In this paper we review the data included in the Shared Task, which covers nearly 3 million manually annotated tokens from 16 datasets in 11 languages, survey and compare submitted systems and report on system performance on each task for both annotated and plain-tokenized versions of the data. 1 Introduction Building on rapid progress in NLP for discourse parsing in the past decade (e.g. Iruskieta et al. 2013, Zhou et al. 2014, Afantenos et al. 2012, Braud et al. 2017, Wang and Lan 2015, Li et al. 2016, Perret et al. 2016), the past two years since the DISRPT 2019 Shared Task (Zeldes et al., 2019) have seen unprecedented performance on benchmark datasets for discourse parsing (e.g. Guz and Carenini 2020; Liu et al. 2020; Kurfali 2020; Zhang et al. 2021b). Following the 2019 Shared Task, our focus in 2021 remained on the most established frameworks for discourse relation treebanking: Rhetorical Structure Theory (Mann and Thompson, 1988), the Penn Discourse Treebank’s framework (Prasad et al., 2014), and Segmented Discourse Representation Theory (Asher, 1993"
2021.disrpt-1.1,2021.disrpt-1.3,1,0.785514,"one English case (training on GUM and testing on RST-DT, leading to a 5 point gain from 75 to 80), and the two small Spanish and Chinese datasets (a 3 point gain from 66 to 69 for Spanish when training on the larger Spanish RST STB, and a more modest but surprising 2 point gain from 72 to 74 when training on Basque and testing on Chinese, see Dönicke 2021). The latter two cases are probably owing to the small size of the target corpora. The disCut paper takes a related approach and trains on multiple corpora from the same language or language family, but does not obtain superior results (see Ezzabady et al. 2021), possibly because the single-corpus training results are already so strong. models, which may also be a reason for higher scores on Chinese and especially Turkish, which may be less well-represented by multilingual LMs. Finally we note that in the plain text scenario, all systems except for DisCoDisCo used noncontextualized tools for sentence splitting and/or automatic parsing (SegFormers: CoreNLP; disCut: stanza; TMVM: SpaCy); DisCoDisCo used the tranformer-based sentence splitter from the AMALGUM corpus (Gessler et al., 2020) and DiaParser (Attardi et al., 2021), both with language-specific"
2021.disrpt-1.1,W18-4917,1,0.905534,"Missing"
2021.disrpt-1.1,2021.disrpt-1.6,1,0.700524,"is meant to approach the case of labelling an unlabelled discourse dependency graph. In terms of encoding features, DisCoDisCo uses a sentence pair classifier architecture feeding the underlying transformer embeddings in the form of the BERT classification ([CLS]) token embeddings, while injecting categorical features at two levels: using pseudo tokens, encoded as usual by the transformer, which indicate the relations direction; and using an inserted pseudo token next to the CLS token, which encodes all sequence-level categorical features, such as genre, gold speaker information and more (see Gessler et al. 2021). These features are then read and trained on by the transformer block alongside the word embeddings. The DiscRel system takes a two level approach to relation classification, using two stacked oneversus-rest Random Forest classifiers, which first attempt to distinguish the coarse class of discourse For Connective Detection, DisCoDisCo’s lead becomes slightly larger, about 3 points above SegFormers and almost 7 points above disCut, perhaps due to a combination of features (which give a minor boost of just a few points across datasets in their paper, see Gessler et al. 2021), the use of a CRF l"
2021.disrpt-1.1,2020.lrec-1.648,1,0.731887,"age or language family, but does not obtain superior results (see Ezzabady et al. 2021), possibly because the single-corpus training results are already so strong. models, which may also be a reason for higher scores on Chinese and especially Turkish, which may be less well-represented by multilingual LMs. Finally we note that in the plain text scenario, all systems except for DisCoDisCo used noncontextualized tools for sentence splitting and/or automatic parsing (SegFormers: CoreNLP; disCut: stanza; TMVM: SpaCy); DisCoDisCo used the tranformer-based sentence splitter from the AMALGUM corpus (Gessler et al., 2020) and DiaParser (Attardi et al., 2021), both with language-specific transformers, with the result that plain text numbers are very close to gold numbers for DisCoDisCo (and in fact insignificantly better for plain Connective Detection: 91.49 on average vs. 91.22 for gold treebanked data). This echoes results from the 2019 task (see Yu et al. 2019) which showed the crucial importance of high quality preprocessing in general, and sentence splitting in particular. Among the Transformer-based systems, although DisCoDisCo performs best overall, other systems score highest on particular datasets in d"
2021.disrpt-1.1,2020.codi-1.17,0,0.12301,"getown.edu mikel.iruskieta@ehu.eus Philippe Muller IRIT Toulouse Chloé Braud IRIT-CNRS-ANITI Toulouse Sonia Badene Linagora-IRIT philippe.muller@irit.fr chloe.braud@irit.fr sbadene@linagora.com Abstract task: discourse relation classification across frameworks. Although work on relation classification is not new, work has generally been separated into several somewhat independent strands of work: relation classification with explicit connectives (see Kido and Aizawa 2016; Nie et al. 2019), implicit relation classification (Wang and Lan, 2016; Kim et al., 2020), and full discourse constituent (Guz and Carenini, 2020; Zhang et al., 2021b) or dependency parsing (Morey et al., 2018). We believe that these strands of research can come together and benefit from data across frameworks, but only if we are able to formulate a common denominator which poses the problem of discourse relation recognition in similar terms, independently of the underlying theoretical assumptions. In addition to the new relation classification task, we also updated and expanded the two tasks from 2019 and added a new surprise dataset (and language) which became available in the interim: the Persian RST Corpus (Shahmohammadi et al., 20"
2021.disrpt-1.1,W01-1605,0,0.309662,"Missing"
2021.disrpt-1.1,N16-1013,0,0.0234056,"track on discourse relation classification across three formalisms: RST, SDRT, and PDTB. In this paper we review the data included in the Shared Task, which covers nearly 3 million manually annotated tokens from 16 datasets in 11 languages, survey and compare submitted systems and report on system performance on each task for both annotated and plain-tokenized versions of the data. 1 Introduction Building on rapid progress in NLP for discourse parsing in the past decade (e.g. Iruskieta et al. 2013, Zhou et al. 2014, Afantenos et al. 2012, Braud et al. 2017, Wang and Lan 2015, Li et al. 2016, Perret et al. 2016), the past two years since the DISRPT 2019 Shared Task (Zeldes et al., 2019) have seen unprecedented performance on benchmark datasets for discourse parsing (e.g. Guz and Carenini 2020; Liu et al. 2020; Kurfali 2020; Zhang et al. 2021b). Following the 2019 Shared Task, our focus in 2021 remained on the most established frameworks for discourse relation treebanking: Rhetorical Structure Theory (Mann and Thompson, 1988), the Penn Discourse Treebank’s framework (Prasad et al., 2014), and Segmented Discourse Representation Theory (Asher, 1993). With the progress achieved on discourse segmentation"
2021.disrpt-1.1,K16-2018,0,0.155925,"eldes Georgetown University Yang Janet Liu Georgetown University Mikel Iruskieta University of the Basque Country az364@georgetown.edu yl879@georgetown.edu mikel.iruskieta@ehu.eus Philippe Muller IRIT Toulouse Chloé Braud IRIT-CNRS-ANITI Toulouse Sonia Badene Linagora-IRIT philippe.muller@irit.fr chloe.braud@irit.fr sbadene@linagora.com Abstract task: discourse relation classification across frameworks. Although work on relation classification is not new, work has generally been separated into several somewhat independent strands of work: relation classification with explicit connectives (see Kido and Aizawa 2016; Nie et al. 2019), implicit relation classification (Wang and Lan, 2016; Kim et al., 2020), and full discourse constituent (Guz and Carenini, 2020; Zhang et al., 2021b) or dependency parsing (Morey et al., 2018). We believe that these strands of research can come together and benefit from data across frameworks, but only if we are able to formulate a common denominator which poses the problem of discourse relation recognition in similar terms, independently of the underlying theoretical assumptions. In addition to the new relation classification task, we also updated and expanded the two task"
2021.disrpt-1.1,prasad-etal-2008-penn,0,0.114698,"al gains in labeled accuracy when combined with the approaches tested in this Shared Task. Relation Classification The main results for the relation classification task are given in Table 6. Since the relation classification task is new, it is difficult to evaluate the quality of the scores obtained in the results. For some datasets, such as PDTB, existing scores have been previously reported on the same underlying data, but generally in different settings, where implicit and explicit relations were scored separately. To make matters worse, most previous work on PDTB uses the older version 2 (Prasad et al., 2008), making scores again completely non-comparable. Previous scores on PDTB-V2 (level 2 relations, the same hierarchical level used for the Shared Task) with explicitly specified connectives have reached scores above 90 for a while (Kido and Aizawa, 2016); more recently, results on PDTB-V3 implicit relation classification have reached an accuracy of 64.83 (Kim et al., 2020). By comparison, we see a best score of 74.44 (DisCoDisCo) on all PDTB-V3 relations (including implicit and explicit), but the task is both easier and harder, since in both cases no connective is specified (for uniformity with"
2021.disrpt-1.1,2020.acl-main.480,0,0.176467,"the Basque Country az364@georgetown.edu yl879@georgetown.edu mikel.iruskieta@ehu.eus Philippe Muller IRIT Toulouse Chloé Braud IRIT-CNRS-ANITI Toulouse Sonia Badene Linagora-IRIT philippe.muller@irit.fr chloe.braud@irit.fr sbadene@linagora.com Abstract task: discourse relation classification across frameworks. Although work on relation classification is not new, work has generally been separated into several somewhat independent strands of work: relation classification with explicit connectives (see Kido and Aizawa 2016; Nie et al. 2019), implicit relation classification (Wang and Lan, 2016; Kim et al., 2020), and full discourse constituent (Guz and Carenini, 2020; Zhang et al., 2021b) or dependency parsing (Morey et al., 2018). We believe that these strands of research can come together and benefit from data across frameworks, but only if we are able to formulate a common denominator which poses the problem of discourse relation recognition in similar terms, independently of the underlying theoretical assumptions. In addition to the new relation classification task, we also updated and expanded the two tasks from 2019 and added a new surprise dataset (and language) which became available in the i"
2021.disrpt-1.1,J14-4007,0,0.108043,"ruskieta et al. 2013, Zhou et al. 2014, Afantenos et al. 2012, Braud et al. 2017, Wang and Lan 2015, Li et al. 2016, Perret et al. 2016), the past two years since the DISRPT 2019 Shared Task (Zeldes et al., 2019) have seen unprecedented performance on benchmark datasets for discourse parsing (e.g. Guz and Carenini 2020; Liu et al. 2020; Kurfali 2020; Zhang et al. 2021b). Following the 2019 Shared Task, our focus in 2021 remained on the most established frameworks for discourse relation treebanking: Rhetorical Structure Theory (Mann and Thompson, 1988), the Penn Discourse Treebank’s framework (Prasad et al., 2014), and Segmented Discourse Representation Theory (Asher, 1993). With the progress achieved on discourse segmentation and connective detection since 2019, this year we decided to extend the competition to a new 1 https://github.com/hadiveisi/ PersianRST 2 https://github.com/disrpt/ sharedtask2021 ∗ Discourse Relation Parsing and Treebanking (DISRPT 2021) was held in conjunction with CODI at EMNLP 2021 in the Dominican Republic and Online (https://sites. google.com/georgetown.edu/disrpt2021). 1 Proceedings of the 2nd Shared Task on Discourse Relation Parsing and Treebanking (DISRPT 2021), pages 1"
2021.disrpt-1.1,redeker-etal-2012-multi,0,0.0807605,"Missing"
2021.disrpt-1.1,D16-1035,0,0.0261747,"the first time a track on discourse relation classification across three formalisms: RST, SDRT, and PDTB. In this paper we review the data included in the Shared Task, which covers nearly 3 million manually annotated tokens from 16 datasets in 11 languages, survey and compare submitted systems and report on system performance on each task for both annotated and plain-tokenized versions of the data. 1 Introduction Building on rapid progress in NLP for discourse parsing in the past decade (e.g. Iruskieta et al. 2013, Zhou et al. 2014, Afantenos et al. 2012, Braud et al. 2017, Wang and Lan 2015, Li et al. 2016, Perret et al. 2016), the past two years since the DISRPT 2019 Shared Task (Zeldes et al., 2019) have seen unprecedented performance on benchmark datasets for discourse parsing (e.g. Guz and Carenini 2020; Liu et al. 2020; Kurfali 2020; Zhang et al. 2021b). Following the 2019 Shared Task, our focus in 2021 remained on the most established frameworks for discourse relation treebanking: Rhetorical Structure Theory (Mann and Thompson, 1988), the Penn Discourse Treebank’s framework (Prasad et al., 2014), and Segmented Discourse Representation Theory (Asher, 1993). With the progress achieved on di"
2021.disrpt-1.1,2020.emnlp-main.365,0,0.0133432,"y segmentation system using per-language transformers and making use of grammatical features, which in the plain text scenario were induced using a SOTA Transformerbased parser and sentence splitter as well. Relation Classification Although both systems tackling the Relation Classification task use word embeddings, their approaches are rather different: while the best system, DisCoDisCo, relies on a similar approach to their segmentation task entry, which encodes each sequence using a transformer, the DiscRel system uses whole sentence embeddings from the Sentence-Transformers library (SBERT, Reimers and Gurevych 2020) to compute Euclidean distance between discourse units, as part of a two-level feature-based Random Forest classifier. Both systems also use the provided discourse relation direction, which is meant to approach the case of labelling an unlabelled discourse dependency graph. In terms of encoding features, DisCoDisCo uses a sentence pair classifier architecture feeding the underlying transformer embeddings in the form of the BERT classification ([CLS]) token embeddings, while injecting categorical features at two levels: using pseudo tokens, encoded as usual by the transformer, which indicate th"
2021.disrpt-1.1,stede-neumann-2014-potsdam,0,0.0619789,"Missing"
2021.disrpt-1.1,2021.ccl-1.108,0,0.0186206,"Missing"
2021.disrpt-1.1,W17-3604,0,0.0412999,"Missing"
2021.disrpt-1.1,J18-2001,1,0.853464,"oé Braud IRIT-CNRS-ANITI Toulouse Sonia Badene Linagora-IRIT philippe.muller@irit.fr chloe.braud@irit.fr sbadene@linagora.com Abstract task: discourse relation classification across frameworks. Although work on relation classification is not new, work has generally been separated into several somewhat independent strands of work: relation classification with explicit connectives (see Kido and Aizawa 2016; Nie et al. 2019), implicit relation classification (Wang and Lan, 2016; Kim et al., 2020), and full discourse constituent (Guz and Carenini, 2020; Zhang et al., 2021b) or dependency parsing (Morey et al., 2018). We believe that these strands of research can come together and benefit from data across frameworks, but only if we are able to formulate a common denominator which poses the problem of discourse relation recognition in similar terms, independently of the underlying theoretical assumptions. In addition to the new relation classification task, we also updated and expanded the two tasks from 2019 and added a new surprise dataset (and language) which became available in the interim: the Persian RST Corpus (Shahmohammadi et al., 2021).1 While it remains impossible to place some data openly onlin"
2021.disrpt-1.1,2021.disrpt-1.5,0,0.0322926,"of a CRF layer and the combination of both contextualized and static word embeddings as well as character embeddings, which provided a variety of views on the data and allowed for both fine tuning and fixed generalization in the test set. Choice of language models is also an important factor, and here again disCut and SegFormers rely on multilingual LMs, while DisCoDisCo uses individual per-language 8 relation (for example collapsing relations such as EVALUATION , INTERPRETATION , JUSTIFY ), and then sub-classify the coarse classes into the final target labels using the second classifier (see Varachkina and Pannach 2021). For large datasets, the first classifier is trained on the train set, while the second is trained using the development data, but for smaller datasets, training data is duplicated along with the development set for the second classifier. DisCoDisCo could be used in a two stage approach such as the one taken by DiscRel. 7 Conclusion We conducted the second DISRPT shared task for Discourse Relation Parsing and Treebanking across frameworks, resulting in a number of new state of the art scores on benchmark datasets, as well as the publication of a new multilingual benchmark for discourse parsin"
2021.disrpt-1.1,P19-1442,0,0.0209862,"rsity Yang Janet Liu Georgetown University Mikel Iruskieta University of the Basque Country az364@georgetown.edu yl879@georgetown.edu mikel.iruskieta@ehu.eus Philippe Muller IRIT Toulouse Chloé Braud IRIT-CNRS-ANITI Toulouse Sonia Badene Linagora-IRIT philippe.muller@irit.fr chloe.braud@irit.fr sbadene@linagora.com Abstract task: discourse relation classification across frameworks. Although work on relation classification is not new, work has generally been separated into several somewhat independent strands of work: relation classification with explicit connectives (see Kido and Aizawa 2016; Nie et al. 2019), implicit relation classification (Wang and Lan, 2016; Kim et al., 2020), and full discourse constituent (Guz and Carenini, 2020; Zhang et al., 2021b) or dependency parsing (Morey et al., 2018). We believe that these strands of research can come together and benefit from data across frameworks, but only if we are able to formulate a common denominator which poses the problem of discourse relation recognition in similar terms, independently of the underlying theoretical assumptions. In addition to the new relation classification task, we also updated and expanded the two tasks from 2019 and ad"
2021.disrpt-1.1,K15-2002,0,0.0179648,"Task included for the first time a track on discourse relation classification across three formalisms: RST, SDRT, and PDTB. In this paper we review the data included in the Shared Task, which covers nearly 3 million manually annotated tokens from 16 datasets in 11 languages, survey and compare submitted systems and report on system performance on each task for both annotated and plain-tokenized versions of the data. 1 Introduction Building on rapid progress in NLP for discourse parsing in the past decade (e.g. Iruskieta et al. 2013, Zhou et al. 2014, Afantenos et al. 2012, Braud et al. 2017, Wang and Lan 2015, Li et al. 2016, Perret et al. 2016), the past two years since the DISRPT 2019 Shared Task (Zeldes et al., 2019) have seen unprecedented performance on benchmark datasets for discourse parsing (e.g. Guz and Carenini 2020; Liu et al. 2020; Kurfali 2020; Zhang et al. 2021b). Following the 2019 Shared Task, our focus in 2021 remained on the most established frameworks for discourse relation treebanking: Rhetorical Structure Theory (Mann and Thompson, 1988), the Penn Discourse Treebank’s framework (Prasad et al., 2014), and Segmented Discourse Representation Theory (Asher, 1993). With the progres"
2021.disrpt-1.1,K16-2004,0,0.024196,"skieta University of the Basque Country az364@georgetown.edu yl879@georgetown.edu mikel.iruskieta@ehu.eus Philippe Muller IRIT Toulouse Chloé Braud IRIT-CNRS-ANITI Toulouse Sonia Badene Linagora-IRIT philippe.muller@irit.fr chloe.braud@irit.fr sbadene@linagora.com Abstract task: discourse relation classification across frameworks. Although work on relation classification is not new, work has generally been separated into several somewhat independent strands of work: relation classification with explicit connectives (see Kido and Aizawa 2016; Nie et al. 2019), implicit relation classification (Wang and Lan, 2016; Kim et al., 2020), and full discourse constituent (Guz and Carenini, 2020; Zhang et al., 2021b) or dependency parsing (Morey et al., 2018). We believe that these strands of research can come together and benefit from data across frameworks, but only if we are able to formulate a common denominator which poses the problem of discourse relation recognition in similar terms, independently of the underlying theoretical assumptions. In addition to the new relation classification task, we also updated and expanded the two tasks from 2019 and added a new surprise dataset (and language) which became"
2021.disrpt-1.1,W19-2717,1,0.85001,"o, all systems except for DisCoDisCo used noncontextualized tools for sentence splitting and/or automatic parsing (SegFormers: CoreNLP; disCut: stanza; TMVM: SpaCy); DisCoDisCo used the tranformer-based sentence splitter from the AMALGUM corpus (Gessler et al., 2020) and DiaParser (Attardi et al., 2021), both with language-specific transformers, with the result that plain text numbers are very close to gold numbers for DisCoDisCo (and in fact insignificantly better for plain Connective Detection: 91.49 on average vs. 91.22 for gold treebanked data). This echoes results from the 2019 task (see Yu et al. 2019) which showed the crucial importance of high quality preprocessing in general, and sentence splitting in particular. Among the Transformer-based systems, although DisCoDisCo performs best overall, other systems score highest on particular datasets in different scenarios, with SegFormers attaining a new SOTA score on RST-DT from gold trees (97.09) next to DisCoDisCo’s new SOTA score on the same data without gold trees (96.35 in the plain text scenario), and disCut showing impressively strong performance on German (95.61 from gold trees, and an even better 95.63 in the plain text scenario). Over"
2021.disrpt-1.1,W19-2713,1,0.767456,"DRT, and PDTB. In this paper we review the data included in the Shared Task, which covers nearly 3 million manually annotated tokens from 16 datasets in 11 languages, survey and compare submitted systems and report on system performance on each task for both annotated and plain-tokenized versions of the data. 1 Introduction Building on rapid progress in NLP for discourse parsing in the past decade (e.g. Iruskieta et al. 2013, Zhou et al. 2014, Afantenos et al. 2012, Braud et al. 2017, Wang and Lan 2015, Li et al. 2016, Perret et al. 2016), the past two years since the DISRPT 2019 Shared Task (Zeldes et al., 2019) have seen unprecedented performance on benchmark datasets for discourse parsing (e.g. Guz and Carenini 2020; Liu et al. 2020; Kurfali 2020; Zhang et al. 2021b). Following the 2019 Shared Task, our focus in 2021 remained on the most established frameworks for discourse relation treebanking: Rhetorical Structure Theory (Mann and Thompson, 1988), the Penn Discourse Treebank’s framework (Prasad et al., 2014), and Segmented Discourse Representation Theory (Asher, 1993). With the progress achieved on discourse segmentation and connective detection since 2019, this year we decided to extend the comp"
2021.disrpt-1.1,W17-0809,0,0.0343718,"EDU EDU EDU EDU EDU EDU EDU EDU EDU EDU EDU EDU segStyle Conn Conn Conn relations 2,164 13,897 16,002 9,580 2,533 4,100 2,185 1,608 4,148 28,868 2,240 439 439 relations 43,920 2,451 3,657 relTypes 26 23 17 16 29 17 18 32 32 22 28 24 26 relTypes 23 23 9 discontinuous No Yes Yes No Yes Yes Yes No Yes Yes Yes Yes Yes discontinuous Yes Yes Yes Table 2: Datasets in the DISRPT 2021 Shared Task. − spa.rst.rststb - RST Spanish Treebank (da Cunha et al., 2011). − spa.rst.sctb - RST Spanish-Chinese Treebank (Spanish) (Cao et al., 2018). − tur.pdtb.tdb - Turkish Discourse Bank (Zeyrek and Webber, 2008; Zeyrek and Kurfalı, 2017). − zho.pdtb.cdtb - Chinese Discourse Treebank (Zhou et al., 2014). − zho.rst.sctb - RST Spanish-Chinese Treebank (Chinese) (Cao et al., 2018). A script included in the shared task repository was provided in order to reconstruct the data, which requires users to have access to the original LDC releases of the underlying corpora in the case of PDTB, RST-DT, and CDTB or the original Turkish texts available from Middle East Technical University (METU) by request. Missing data for GUM is downloadable directly by running the script and consenting to non-commercial use conditions. The short names fo"
2021.disrpt-1.1,I08-7009,0,0.0665657,"8,748 1,660 segStyle EDU EDU EDU EDU EDU EDU EDU EDU EDU EDU EDU EDU EDU segStyle Conn Conn Conn relations 2,164 13,897 16,002 9,580 2,533 4,100 2,185 1,608 4,148 28,868 2,240 439 439 relations 43,920 2,451 3,657 relTypes 26 23 17 16 29 17 18 32 32 22 28 24 26 relTypes 23 23 9 discontinuous No Yes Yes No Yes Yes Yes No Yes Yes Yes Yes Yes discontinuous Yes Yes Yes Table 2: Datasets in the DISRPT 2021 Shared Task. − spa.rst.rststb - RST Spanish Treebank (da Cunha et al., 2011). − spa.rst.sctb - RST Spanish-Chinese Treebank (Spanish) (Cao et al., 2018). − tur.pdtb.tdb - Turkish Discourse Bank (Zeyrek and Webber, 2008; Zeyrek and Kurfalı, 2017). − zho.pdtb.cdtb - Chinese Discourse Treebank (Zhou et al., 2014). − zho.rst.sctb - RST Spanish-Chinese Treebank (Chinese) (Cao et al., 2018). A script included in the shared task repository was provided in order to reconstruct the data, which requires users to have access to the original LDC releases of the underlying corpora in the case of PDTB, RST-DT, and CDTB or the original Turkish texts available from Middle East Technical University (METU) by request. Missing data for GUM is downloadable directly by running the script and consenting to non-commercial use con"
2021.disrpt-1.1,2021.acl-long.449,0,0.357093,"ta@ehu.eus Philippe Muller IRIT Toulouse Chloé Braud IRIT-CNRS-ANITI Toulouse Sonia Badene Linagora-IRIT philippe.muller@irit.fr chloe.braud@irit.fr sbadene@linagora.com Abstract task: discourse relation classification across frameworks. Although work on relation classification is not new, work has generally been separated into several somewhat independent strands of work: relation classification with explicit connectives (see Kido and Aizawa 2016; Nie et al. 2019), implicit relation classification (Wang and Lan, 2016; Kim et al., 2020), and full discourse constituent (Guz and Carenini, 2020; Zhang et al., 2021b) or dependency parsing (Morey et al., 2018). We believe that these strands of research can come together and benefit from data across frameworks, but only if we are able to formulate a common denominator which poses the problem of discourse relation recognition in similar terms, independently of the underlying theoretical assumptions. In addition to the new relation classification task, we also updated and expanded the two tasks from 2019 and added a new surprise dataset (and language) which became available in the interim: the Persian RST Corpus (Shahmohammadi et al., 2021).1 While it remai"
2021.disrpt-1.1,2021.acl-long.305,0,0.196304,"ta@ehu.eus Philippe Muller IRIT Toulouse Chloé Braud IRIT-CNRS-ANITI Toulouse Sonia Badene Linagora-IRIT philippe.muller@irit.fr chloe.braud@irit.fr sbadene@linagora.com Abstract task: discourse relation classification across frameworks. Although work on relation classification is not new, work has generally been separated into several somewhat independent strands of work: relation classification with explicit connectives (see Kido and Aizawa 2016; Nie et al. 2019), implicit relation classification (Wang and Lan, 2016; Kim et al., 2020), and full discourse constituent (Guz and Carenini, 2020; Zhang et al., 2021b) or dependency parsing (Morey et al., 2018). We believe that these strands of research can come together and benefit from data across frameworks, but only if we are able to formulate a common denominator which poses the problem of discourse relation recognition in similar terms, independently of the underlying theoretical assumptions. In addition to the new relation classification task, we also updated and expanded the two tasks from 2019 and added a new surprise dataset (and language) which became available in the interim: the Persian RST Corpus (Shahmohammadi et al., 2021).1 While it remai"
2021.ecnlp-1.19,2020.emnlp-main.506,0,0.0237342,"constraints on the model to include certain attribute words in the product summarization. Zhu et al. (2021) integrated information extraction and graph attention network into transformer-based seq2seq framework. To identify and correct the unfaithful summaries, Wang et al. (2020) proposed to use a question answering framework to check the faithfulness of the summary while Dong et al. (2020) built a factual correction model that leverages knowledge learned from question answering models. Kryscinski et al. (2020) trained a BERT-based model to classify whether the summary is factual consistent. Cao et al. (2020) and Zhu et al. (2021) developed factual corrector based on BART (Lewis et al., 2020) and UniLM (Dong et al., 2019), as a post-processor to rectify factual errors from the upstream summarization model. They corrupted the reference summaries with artificial errors and used them as the negative samples for training the correctors. In our work, we also generate corrupted summaries as the negative counterparts of the target summaries. The difference is that, instead of building a separate corrector model, we directly engineer the training objective of the summarization model. By leveraging contras"
2021.ecnlp-1.19,2020.emnlp-main.749,0,0.0610333,"Missing"
2021.ecnlp-1.19,P19-1213,0,0.0416566,"Missing"
2021.ecnlp-1.19,D19-1051,0,0.049657,"Missing"
2021.ecnlp-1.19,2020.emnlp-main.750,0,0.0286844,"constrained certain tokens to require them to be present in the summary. Similarly, Yuan et al. (2020) add constraints on the model to include certain attribute words in the product summarization. Zhu et al. (2021) integrated information extraction and graph attention network into transformer-based seq2seq framework. To identify and correct the unfaithful summaries, Wang et al. (2020) proposed to use a question answering framework to check the faithfulness of the summary while Dong et al. (2020) built a factual correction model that leverages knowledge learned from question answering models. Kryscinski et al. (2020) trained a BERT-based model to classify whether the summary is factual consistent. Cao et al. (2020) and Zhu et al. (2021) developed factual corrector based on BART (Lewis et al., 2020) and UniLM (Dong et al., 2019), as a post-processor to rectify factual errors from the upstream summarization model. They corrupted the reference summaries with artificial errors and used them as the negative samples for training the correctors. In our work, we also generate corrupted summaries as the negative counterparts of the target summaries. The difference is that, instead of building a separate corrector"
2021.ecnlp-1.19,2020.acl-main.703,0,0.449812,"ided where the information is concentrated on the product issues while other irrelevant contents are filtered out. Such summary allows sellers to quickly capture and comprehend the problems, and thus they can address buyer dissatisfaction more efficiently. The problem of generating summaries from customer feedback is modeled as a text summarization task (Nallapati et al., 2016; Allahyari et al., 2017; Gao et al., 2020) in the natural language processing (NLP) domain. Abstractive summarization models with transformer-based architecture have achieved success in a variety of summarization tasks (Lewis et al., 2020; Raffel et al., 2020; Zhang et al., 2020; Bao et al., 2020). Hence, we harnessed the recent state-of-the-art (SOTA) abstractive summarization models, BART (Lewis et al., 2020) and T5 (Raffel et al., 2020), and fine tuned the models for our specific summarization task. We aim to utilize summarization models to produce the summary that can correctly describe the product issues presented in customer feedback. However, from human evalu158 Proceedings of the 4th Workshop on e-Commerce and NLP (ECNLP 4), pages 158–163 August 5, 2021. ©2021 Association for Computational Linguistics ation results, we"
2021.ecnlp-1.19,C18-1121,0,0.0185044,"First, The proposed approaches with corrupted summary generation and contrastive loss augmentation do not pose requirements on the achitecture of the summarization model. Thus, they can be applied to any abstraction-based summarization model to improve the model faithfulness. Second, we test the proposed approaches on SOTA summarization algorithms such as BART and T5. Our approaches show large benefits in reducing the common factual errors in customer-feedback summarization. 2 Related Work so that the summary generation is conditioned on both the source document and extracted key information. Li et al. (2018) incorporated the entailment knowledge by utilizing entailment-aware encoder and decoder. With using the textual entailment, Falke et al. (2019) re-ranked the candidates summaries to select the summary that’s better aligned with the source document. Dou et al. (2020) studied different external signals, including key sentences, keywords and relations, and used them in addition to the input text to guide the summary generation. Mao et al. (2020) constrained certain tokens to require them to be present in the summary. Similarly, Yuan et al. (2020) add constraints on the model to include certain a"
2021.ecnlp-1.19,K16-1028,0,0.0326304,"e product issues. Such redundant information requires extra efforts for sellers to fully understand the customers major concerns, and sometimes even causes confusion. To reduce the redundancy, a concise summary of customer feedback can be provided where the information is concentrated on the product issues while other irrelevant contents are filtered out. Such summary allows sellers to quickly capture and comprehend the problems, and thus they can address buyer dissatisfaction more efficiently. The problem of generating summaries from customer feedback is modeled as a text summarization task (Nallapati et al., 2016; Allahyari et al., 2017; Gao et al., 2020) in the natural language processing (NLP) domain. Abstractive summarization models with transformer-based architecture have achieved success in a variety of summarization tasks (Lewis et al., 2020; Raffel et al., 2020; Zhang et al., 2020; Bao et al., 2020). Hence, we harnessed the recent state-of-the-art (SOTA) abstractive summarization models, BART (Lewis et al., 2020) and T5 (Raffel et al., 2020), and fine tuned the models for our specific summarization task. We aim to utilize summarization models to produce the summary that can correctly describe t"
2021.ecnlp-1.19,2020.acl-main.450,0,0.0185809,"ligned with the source document. Dou et al. (2020) studied different external signals, including key sentences, keywords and relations, and used them in addition to the input text to guide the summary generation. Mao et al. (2020) constrained certain tokens to require them to be present in the summary. Similarly, Yuan et al. (2020) add constraints on the model to include certain attribute words in the product summarization. Zhu et al. (2021) integrated information extraction and graph attention network into transformer-based seq2seq framework. To identify and correct the unfaithful summaries, Wang et al. (2020) proposed to use a question answering framework to check the faithfulness of the summary while Dong et al. (2020) built a factual correction model that leverages knowledge learned from question answering models. Kryscinski et al. (2020) trained a BERT-based model to classify whether the summary is factual consistent. Cao et al. (2020) and Zhu et al. (2021) developed factual corrector based on BART (Lewis et al., 2020) and UniLM (Dong et al., 2019), as a post-processor to rectify factual errors from the upstream summarization model. They corrupted the reference summaries with artificial errors"
2021.ecnlp-1.19,2020.coling-main.502,0,0.0303822,"the source document and extracted key information. Li et al. (2018) incorporated the entailment knowledge by utilizing entailment-aware encoder and decoder. With using the textual entailment, Falke et al. (2019) re-ranked the candidates summaries to select the summary that’s better aligned with the source document. Dou et al. (2020) studied different external signals, including key sentences, keywords and relations, and used them in addition to the input text to guide the summary generation. Mao et al. (2020) constrained certain tokens to require them to be present in the summary. Similarly, Yuan et al. (2020) add constraints on the model to include certain attribute words in the product summarization. Zhu et al. (2021) integrated information extraction and graph attention network into transformer-based seq2seq framework. To identify and correct the unfaithful summaries, Wang et al. (2020) proposed to use a question answering framework to check the faithfulness of the summary while Dong et al. (2020) built a factual correction model that leverages knowledge learned from question answering models. Kryscinski et al. (2020) trained a BERT-based model to classify whether the summary is factual consiste"
2021.emnlp-main.151,W17-1601,0,0.02263,"c S(v, c) = ∥ v ∥∥ c∥ (1) s.t. v = image encoder(v) c = text encoder(c) The image search system outputs a set of top-K retrieved images RK (c) with the highest similarity scores. In this work, we assume that when evaluating on test data, ∀c ∈ C, the text query c is written in gender-neutral language. 2.2 Measuring Gender Bias The situations of image search results are complex: there might be no people, one person, or more than one person in the images. Let g(v) ∈ {male, female, neutral} represent the gender attribute of an image v. Note that in this study gender refers to biological sex Larson, 2017. We use the following rules to determine g(v): g(v) = male when there are only men in the image, g(v) = female when there are only women in the image, otherwise g(v) = neutral. Portraits in image search results with different gender attributes often receive unequal exposure. Inspired by Kay et al. (2015) and Zhao et al. (2017), we measure gender bias in image search by comparing the proportions of masculine and feminine images in search results. Given the set of retrieved images RK (c), we count the images depicting males and females Nmale = 1[g(v) = male], ∑ v∈RK (c) Nfemale = 1[g(v) = femal"
2021.emnlp-main.151,N19-1062,0,0.013635,"emonstrates to learn a fairer model from scratch. Meanwhile, the association between the word embeddings of the clip algorithm can be used for lightweight deoccupation and gendered concepts correlates with ployment of pre-trained representation models with the imbalanced distribution of gender in text coraccessible gender information. pora. There are also a series of debiasing techniques in this area. Bolukbasi et al. (2016) propose Broader Impact to surgically alter the embedding space by identifying the gender subspace from gendered word The algorithmic processes behind modern search pairs. Manzini et al. (2019) extend the bias com- engines, with extensive use of machine learning ponent removal approach to the setting where the algorithms, have great power to determine users’ 2003 access to information (Eslami et al., 2015). Our research provides evidence that unintentionally using image search models trained either on in-domain image retrieval data sets or massive corpora across the internet may lead to unequal inclusiveness between males and females in image search results, even when the search terms are gender-neutral. This inequity can and do have significant impacts on shaping and exaggerating g"
2021.emnlp-main.151,Q14-1006,0,0.739519,"s. We diagnose two typical image search models, the specialized model trained on in-domain datasets and the generalized representation model pretrained on massive image and text data across the internet. Both models suffer from severe gender bias. Therefore, we introduce two novel debiasing approaches: an in-processing fair sampling method to address the gender imbalance issue for training models, and a postprocessing feature clipping method base on mutual information to debias multimodal representations of pre-trained models. Extensive experiments on MS-COCO (Lin et al., 2014) and Flickr30K (Young et al., 2014) benchmarks show that our methods significantly reduce the gender bias in image search models. 1 Introduction 2015). Mitigating gender bias in image search is imperative for social good. In this paper, we formally develop a framework for quantifying gender bias in image search results, where text queries in English1 are made genderneutral, and gender-balanced search images are expected for models to retrieve. To evaluate model fairness, we use the normalized difference between masculine and feminine images in the retrieved results to represent gender bias. We diagnose the gender bias of two pr"
2021.emnlp-main.151,D17-1323,0,0.194346,"ender Bias The situations of image search results are complex: there might be no people, one person, or more than one person in the images. Let g(v) ∈ {male, female, neutral} represent the gender attribute of an image v. Note that in this study gender refers to biological sex Larson, 2017. We use the following rules to determine g(v): g(v) = male when there are only men in the image, g(v) = female when there are only women in the image, otherwise g(v) = neutral. Portraits in image search results with different gender attributes often receive unequal exposure. Inspired by Kay et al. (2015) and Zhao et al. (2017), we measure gender bias in image search by comparing the proportions of masculine and feminine images in search results. Given the set of retrieved images RK (c), we count the images depicting males and females Nmale = 1[g(v) = male], ∑ v∈RK (c) Nfemale = 1[g(v) = female], ∑ v∈RK (c) and define the gender bias metric as: ⎧ ⎪ if Nmale + Nfemale = 0 ⎪0, ∆K (c) = ⎨ Nmale −Nfemale ⎪ ⎪ ⎩ Nmale +Nfemale , otherwise (2) We don’t take absolute values for measuring the direction of skewness, i.e., if ∆K (c) > 0 it skews towards males. Note that a similar definition of male gender bias NmaleN+N in Zhao"
2021.emnlp-main.151,N18-2003,0,0.0156443,"). In this work, the fair sampling strategy designed for the contrastive learning framework could be considered as an in-processing treatment, while the clip algorithm is in the post-processing regime that features an information-theoretical clipping procedure. Our contribution highlights new challenges of reducing gender bias in a multimodal task and specializes new in-processing and postprocessing ideas in the domain of image search. sensitive attribute is non-binary. Data augmentation approaches remove the implicit bias in the training corpora and train the models on the balanced datasets (Zhao et al., 2018). Our work complements this line of research by examining gender bias induced by multimodal models in image search results. Our focus on gender bias in the gender-neutral language would offer new insights for a less explored topic to the community. Gender Bias in Online Search Systems Our work is also closely connected to studies in the HCI community showing the gender inequality in online image search results. Kay et al. (2015) articulate the gender bias in occupational image search results affect people’s perceptions of the prevalence of men and women in each occupation. Kay et al. (2015) co"
2021.emnlp-main.158,P17-1110,0,0.0129712,"ormalizes the CWS task as a sequence labeling problem, almost all methods transfer the CWS results into the sequence labels. As a sequence labeling task, the CRF-based model can achieve a competitive performance with multiple features (Peng et al., 2004; Tseng et al., 2005; Zhao and Kit, 2008; Zhao et al., 2010). However, the effect of each method is determined by the quality of manual features. To reduce the influence of feature engineering, neural CWS methods have been studied and further progress has been made (Zheng et al., 2013; Pei et al., 2014; Chen et al., 2015a,b; Cai and Zhao, 2016; Chen et al., 2017; Cai et al., 2017). Neural methods gradually replaces traditional machine learning methods. Ma et al. (2018) propose the basic LSTM model that is the same with Chen et al. (2015b). But the former study could achieve SOTA performance through tuning the hyper-parameters. Some studies leverage the rich pre-trained embeddings to improve the performance for neural CWS methods (Zhou et al., 2017; Yang et al., 2017, 2019). To alleviate the issue of OOV words for CWS, some researches have been studied for cross-domain CWS. Zhang et al. (2018) incorporate the domain dictionary into the neural network,"
2021.emnlp-main.158,N19-1423,0,0.15531,"ith Chen et al. (2015b). But the former study could achieve SOTA performance through tuning the hyper-parameters. Some studies leverage the rich pre-trained embeddings to improve the performance for neural CWS methods (Zhou et al., 2017; Yang et al., 2017, 2019). To alleviate the issue of OOV words for CWS, some researches have been studied for cross-domain CWS. Zhang et al. (2018) incorporate the domain dictionary into the neural network, and Zhao et al. (2018) utilize the unlabeled data to enhance the ability to recognize OOV words. With the development of pre-trained language models (PLM) (Devlin et al., 2019), CWS methods also make further progress. Previous SOTA methods effectively achieve good performance for CWS (Meng et al., 2019; Huang et al., 2020; Duan and Zhao, 2020), and they take the advantages of PLMs rather than the pure models themselves. The redundant components get slight improvements that are not as much as the PLMs learning paradigm. 3 Method The overall process of our method is shown in algorithm 1: First, we train a word segmentation model and use it to generate segmentation results. Then, according to the segmentation results, the masked sentence is generated based on certain s"
2021.emnlp-main.158,P16-1039,0,0.0204645,"e Xue (2003) first formalizes the CWS task as a sequence labeling problem, almost all methods transfer the CWS results into the sequence labels. As a sequence labeling task, the CRF-based model can achieve a competitive performance with multiple features (Peng et al., 2004; Tseng et al., 2005; Zhao and Kit, 2008; Zhao et al., 2010). However, the effect of each method is determined by the quality of manual features. To reduce the influence of feature engineering, neural CWS methods have been studied and further progress has been made (Zheng et al., 2013; Pei et al., 2014; Chen et al., 2015a,b; Cai and Zhao, 2016; Chen et al., 2017; Cai et al., 2017). Neural methods gradually replaces traditional machine learning methods. Ma et al. (2018) propose the basic LSTM model that is the same with Chen et al. (2015b). But the former study could achieve SOTA performance through tuning the hyper-parameters. Some studies leverage the rich pre-trained embeddings to improve the performance for neural CWS methods (Zhou et al., 2017; Yang et al., 2017, 2019). To alleviate the issue of OOV words for CWS, some researches have been studied for cross-domain CWS. Zhang et al. (2018) incorporate the domain dictionary into"
2021.emnlp-main.158,P17-2096,0,0.0779089,"rning methods and (2) neural network methods. In statistical machine learning meth2068 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 2068–2077 c November 7–11, 2021. 2021 Association for Computational Linguistics ods, Conditional Random Fields (CRF) is the most effective model for the sequence labeling problem (Zhao and Kit, 2008; Zhao et al., 2010). However, the performance of the CRF model depends on the quality of the hand-crafted features. To minimize the effects of different hand-crafted features, neural network methods (Chen et al., 2015b; Cai et al., 2017; Ma et al., 2018) have been widely used. On the other hand, these supervised learning methods are usually limited by the training data. Recent SOTA approaches utilize the pre-trained models (PTM) to improve the quality of CWS (Tian et al., 2020; Huang et al., 2020). However, the CWS methods based on the PTM only utilize the large-scale annotated data to finetune the parameters. It omits much-generated information of the training step. Besides, the annotated data has some incorrect labels due to lexical diversity in Chinese, therefore the robustness of methods is quite important for the CWS. I"
2021.emnlp-main.158,P15-1168,0,0.104971,"tistical machine learning methods and (2) neural network methods. In statistical machine learning meth2068 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 2068–2077 c November 7–11, 2021. 2021 Association for Computational Linguistics ods, Conditional Random Fields (CRF) is the most effective model for the sequence labeling problem (Zhao and Kit, 2008; Zhao et al., 2010). However, the performance of the CRF model depends on the quality of the hand-crafted features. To minimize the effects of different hand-crafted features, neural network methods (Chen et al., 2015b; Cai et al., 2017; Ma et al., 2018) have been widely used. On the other hand, these supervised learning methods are usually limited by the training data. Recent SOTA approaches utilize the pre-trained models (PTM) to improve the quality of CWS (Tian et al., 2020; Huang et al., 2020). However, the CWS methods based on the PTM only utilize the large-scale annotated data to finetune the parameters. It omits much-generated information of the training step. Besides, the annotated data has some incorrect labels due to lexical diversity in Chinese, therefore the robustness of methods is quite impor"
2021.emnlp-main.158,D15-1141,0,0.104748,"tistical machine learning methods and (2) neural network methods. In statistical machine learning meth2068 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 2068–2077 c November 7–11, 2021. 2021 Association for Computational Linguistics ods, Conditional Random Fields (CRF) is the most effective model for the sequence labeling problem (Zhao and Kit, 2008; Zhao et al., 2010). However, the performance of the CRF model depends on the quality of the hand-crafted features. To minimize the effects of different hand-crafted features, neural network methods (Chen et al., 2015b; Cai et al., 2017; Ma et al., 2018) have been widely used. On the other hand, these supervised learning methods are usually limited by the training data. Recent SOTA approaches utilize the pre-trained models (PTM) to improve the quality of CWS (Tian et al., 2020; Huang et al., 2020). However, the CWS methods based on the PTM only utilize the large-scale annotated data to finetune the parameters. It omits much-generated information of the training step. Besides, the annotated data has some incorrect labels due to lexical diversity in Chinese, therefore the robustness of methods is quite impor"
2021.emnlp-main.158,2020.emnlp-main.317,0,0.0194672,"o improve the performance for neural CWS methods (Zhou et al., 2017; Yang et al., 2017, 2019). To alleviate the issue of OOV words for CWS, some researches have been studied for cross-domain CWS. Zhang et al. (2018) incorporate the domain dictionary into the neural network, and Zhao et al. (2018) utilize the unlabeled data to enhance the ability to recognize OOV words. With the development of pre-trained language models (PLM) (Devlin et al., 2019), CWS methods also make further progress. Previous SOTA methods effectively achieve good performance for CWS (Meng et al., 2019; Huang et al., 2020; Duan and Zhao, 2020), and they take the advantages of PLMs rather than the pure models themselves. The redundant components get slight improvements that are not as much as the PLMs learning paradigm. 3 Method The overall process of our method is shown in algorithm 1: First, we train a word segmentation model and use it to generate segmentation results. Then, according to the segmentation results, the masked sentence is generated based on certain strategies, and an MLM is trained with the masked sentence. Afterward, we mask the sentences in the training 2069 (t) Figure 2: The architecture of our model. D, D(t) and"
2021.emnlp-main.158,I05-3017,0,0.178287,"Missing"
2021.emnlp-main.158,2020.emnlp-main.318,1,0.713502,"s, Conditional Random Fields (CRF) is the most effective model for the sequence labeling problem (Zhao and Kit, 2008; Zhao et al., 2010). However, the performance of the CRF model depends on the quality of the hand-crafted features. To minimize the effects of different hand-crafted features, neural network methods (Chen et al., 2015b; Cai et al., 2017; Ma et al., 2018) have been widely used. On the other hand, these supervised learning methods are usually limited by the training data. Recent SOTA approaches utilize the pre-trained models (PTM) to improve the quality of CWS (Tian et al., 2020; Huang et al., 2020). However, the CWS methods based on the PTM only utilize the large-scale annotated data to finetune the parameters. It omits much-generated information of the training step. Besides, the annotated data has some incorrect labels due to lexical diversity in Chinese, therefore the robustness of methods is quite important for the CWS. In this work, we propose a self-supervised CWS approach to enhance the performance of CWS model. In addition, we also investigate on the crossdomain and low-quality datasets to analyze the robustness of CWS models. As depicted in Figure 1, our model consists of two p"
2021.emnlp-main.158,I17-1019,0,0.028262,"Missing"
2021.emnlp-main.158,I08-4010,0,0.0784791,"Missing"
2021.emnlp-main.158,D18-1529,0,0.0255183,"Missing"
2021.emnlp-main.158,P14-1028,0,0.0259696,"methods and characterbased methods. Since Xue (2003) first formalizes the CWS task as a sequence labeling problem, almost all methods transfer the CWS results into the sequence labels. As a sequence labeling task, the CRF-based model can achieve a competitive performance with multiple features (Peng et al., 2004; Tseng et al., 2005; Zhao and Kit, 2008; Zhao et al., 2010). However, the effect of each method is determined by the quality of manual features. To reduce the influence of feature engineering, neural CWS methods have been studied and further progress has been made (Zheng et al., 2013; Pei et al., 2014; Chen et al., 2015a,b; Cai and Zhao, 2016; Chen et al., 2017; Cai et al., 2017). Neural methods gradually replaces traditional machine learning methods. Ma et al. (2018) propose the basic LSTM model that is the same with Chen et al. (2015b). But the former study could achieve SOTA performance through tuning the hyper-parameters. Some studies leverage the rich pre-trained embeddings to improve the performance for neural CWS methods (Zhou et al., 2017; Yang et al., 2017, 2019). To alleviate the issue of OOV words for CWS, some researches have been studied for cross-domain CWS. Zhang et al. (201"
2021.emnlp-main.158,C04-1081,0,0.229756,"ms previous methods with different criteria training, and our proposed method also improves the robustness of the model. 2 Related Work Chinese word segmentation (CWS) has been studied for several years as an essential Chinese NLP task. CWS methods are divided into two streams of approaches: word-based methods and characterbased methods. Since Xue (2003) first formalizes the CWS task as a sequence labeling problem, almost all methods transfer the CWS results into the sequence labels. As a sequence labeling task, the CRF-based model can achieve a competitive performance with multiple features (Peng et al., 2004; Tseng et al., 2005; Zhao and Kit, 2008; Zhao et al., 2010). However, the effect of each method is determined by the quality of manual features. To reduce the influence of feature engineering, neural CWS methods have been studied and further progress has been made (Zheng et al., 2013; Pei et al., 2014; Chen et al., 2015a,b; Cai and Zhao, 2016; Chen et al., 2017; Cai et al., 2017). Neural methods gradually replaces traditional machine learning methods. Ma et al. (2018) propose the basic LSTM model that is the same with Chen et al. (2015b). But the former study could achieve SOTA performance th"
2021.emnlp-main.158,P16-1159,1,0.740175,"igate on the crossdomain and low-quality datasets to analyze the robustness of CWS models. As depicted in Figure 1, our model consists of two parts: segmenter and predictor. We leverage the Transformer encoder as a word segmenter. We exploit the revised masked language model (MLM) as a predictor to improve the segmentation model. We generate masked sequences with respect to the segmentation results. Then we exploit MLM to predict the masked part and evaluate the quality of the segmentation based on the quality of the predictions. We leverage an improved version of minimum risk training (MRT) (Shen et al., 2016) to enhance the segmentation. Our contributions are as follows: • We propose a self-supervised method for CWS, which uses the predictions of revised MLM to assist the word segmentation model. • We present an improved version of MRT by adding regularization terms to boost the performance of the word segmentation model. • Experimental results show that our approach outperforms previous methods with different criteria training, and our proposed method also improves the robustness of the model. 2 Related Work Chinese word segmentation (CWS) has been studied for several years as an essential Chines"
2021.emnlp-main.158,2020.acl-main.734,0,0.0325726,"onal Linguistics ods, Conditional Random Fields (CRF) is the most effective model for the sequence labeling problem (Zhao and Kit, 2008; Zhao et al., 2010). However, the performance of the CRF model depends on the quality of the hand-crafted features. To minimize the effects of different hand-crafted features, neural network methods (Chen et al., 2015b; Cai et al., 2017; Ma et al., 2018) have been widely used. On the other hand, these supervised learning methods are usually limited by the training data. Recent SOTA approaches utilize the pre-trained models (PTM) to improve the quality of CWS (Tian et al., 2020; Huang et al., 2020). However, the CWS methods based on the PTM only utilize the large-scale annotated data to finetune the parameters. It omits much-generated information of the training step. Besides, the annotated data has some incorrect labels due to lexical diversity in Chinese, therefore the robustness of methods is quite important for the CWS. In this work, we propose a self-supervised CWS approach to enhance the performance of CWS model. In addition, we also investigate on the crossdomain and low-quality datasets to analyze the robustness of CWS models. As depicted in Figure 1, our mo"
2021.emnlp-main.158,I08-4017,0,0.30877,"ome neural methods try to incorporate external resources to achieve good performance for in-domain and cross-domain CWS (Zhou et al., 2017; Zhang et al., 2018). The previous methods fall into two categories: (1) the statistical machine learning methods and (2) neural network methods. In statistical machine learning meth2068 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 2068–2077 c November 7–11, 2021. 2021 Association for Computational Linguistics ods, Conditional Random Fields (CRF) is the most effective model for the sequence labeling problem (Zhao and Kit, 2008; Zhao et al., 2010). However, the performance of the CRF model depends on the quality of the hand-crafted features. To minimize the effects of different hand-crafted features, neural network methods (Chen et al., 2015b; Cai et al., 2017; Ma et al., 2018) have been widely used. On the other hand, these supervised learning methods are usually limited by the training data. Recent SOTA approaches utilize the pre-trained models (PTM) to improve the quality of CWS (Tian et al., 2020; Huang et al., 2020). However, the CWS methods based on the PTM only utilize the large-scale annotated data to finetu"
2021.emnlp-main.158,D13-1061,0,0.0814908,"Missing"
2021.emnlp-main.158,D17-1079,0,0.0770808,"curately represent semantic information of Chinese NLP tasks. Besides, the length of the sentence is shortened by word segmentation. The shorter length of a sentence is effective for the deep learning method in some cases. Recently, good performance for CWS has already been achieved in large-scale annotated corpora as reported by related research (Huang and Zhao, 2007; Zhao et al., 2019). Most methods start with data-driven to improve the performance for CWS. For instance, some neural methods try to incorporate external resources to achieve good performance for in-domain and cross-domain CWS (Zhou et al., 2017; Zhang et al., 2018). The previous methods fall into two categories: (1) the statistical machine learning methods and (2) neural network methods. In statistical machine learning meth2068 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 2068–2077 c November 7–11, 2021. 2021 Association for Computational Linguistics ods, Conditional Random Fields (CRF) is the most effective model for the sequence labeling problem (Zhao and Kit, 2008; Zhao et al., 2010). However, the performance of the CRF model depends on the quality of the hand-crafted features. To"
2021.emnlp-main.158,I05-3027,0,0.0875816,"with different criteria training, and our proposed method also improves the robustness of the model. 2 Related Work Chinese word segmentation (CWS) has been studied for several years as an essential Chinese NLP task. CWS methods are divided into two streams of approaches: word-based methods and characterbased methods. Since Xue (2003) first formalizes the CWS task as a sequence labeling problem, almost all methods transfer the CWS results into the sequence labels. As a sequence labeling task, the CRF-based model can achieve a competitive performance with multiple features (Peng et al., 2004; Tseng et al., 2005; Zhao and Kit, 2008; Zhao et al., 2010). However, the effect of each method is determined by the quality of manual features. To reduce the influence of feature engineering, neural CWS methods have been studied and further progress has been made (Zheng et al., 2013; Pei et al., 2014; Chen et al., 2015a,b; Cai and Zhao, 2016; Chen et al., 2017; Cai et al., 2017). Neural methods gradually replaces traditional machine learning methods. Ma et al. (2018) propose the basic LSTM model that is the same with Chen et al. (2015b). But the former study could achieve SOTA performance through tuning the hyp"
2021.emnlp-main.158,O03-4002,0,0.299583,"the predictions of revised MLM to assist the word segmentation model. • We present an improved version of MRT by adding regularization terms to boost the performance of the word segmentation model. • Experimental results show that our approach outperforms previous methods with different criteria training, and our proposed method also improves the robustness of the model. 2 Related Work Chinese word segmentation (CWS) has been studied for several years as an essential Chinese NLP task. CWS methods are divided into two streams of approaches: word-based methods and characterbased methods. Since Xue (2003) first formalizes the CWS task as a sequence labeling problem, almost all methods transfer the CWS results into the sequence labels. As a sequence labeling task, the CRF-based model can achieve a competitive performance with multiple features (Peng et al., 2004; Tseng et al., 2005; Zhao and Kit, 2008; Zhao et al., 2010). However, the effect of each method is determined by the quality of manual features. To reduce the influence of feature engineering, neural CWS methods have been studied and further progress has been made (Zheng et al., 2013; Pei et al., 2014; Chen et al., 2015a,b; Cai and Zhao"
2021.emnlp-main.158,P17-1078,0,0.0163971,"influence of feature engineering, neural CWS methods have been studied and further progress has been made (Zheng et al., 2013; Pei et al., 2014; Chen et al., 2015a,b; Cai and Zhao, 2016; Chen et al., 2017; Cai et al., 2017). Neural methods gradually replaces traditional machine learning methods. Ma et al. (2018) propose the basic LSTM model that is the same with Chen et al. (2015b). But the former study could achieve SOTA performance through tuning the hyper-parameters. Some studies leverage the rich pre-trained embeddings to improve the performance for neural CWS methods (Zhou et al., 2017; Yang et al., 2017, 2019). To alleviate the issue of OOV words for CWS, some researches have been studied for cross-domain CWS. Zhang et al. (2018) incorporate the domain dictionary into the neural network, and Zhao et al. (2018) utilize the unlabeled data to enhance the ability to recognize OOV words. With the development of pre-trained language models (PLM) (Devlin et al., 2019), CWS methods also make further progress. Previous SOTA methods effectively achieve good performance for CWS (Meng et al., 2019; Huang et al., 2020; Duan and Zhao, 2020), and they take the advantages of PLMs rather than the pure models"
2021.emnlp-main.158,N19-1278,0,0.0324817,"Missing"
2021.emnlp-main.158,E14-1062,0,0.0698456,"Missing"
2021.emnlp-main.158,Y98-1020,0,0.762655,"Missing"
2021.emnlp-main.267,J09-1002,0,0.0616764,"ntroduction In recent years, neural approaches (Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015; Vaswani et al., 2017) have significantly improved the quality of machine translation (MT). Despite their apparent success, neural machine translation (NMT) systems still inevitably generate erroneous translations in real-world scenarios (Bentivogli et al., 2016; Castilho et al., 2017), especially for low-resource language pairs. Therefore, the evaluation of translation quality plays an important role in some applications of MT. For example, in computer-assisted translation (CAT) (Barrachina et al., 2009), the evaluation of translation quality can significantly reduce human efforts for postediting (Specia, 2011). ∗ Corresponding author Code can be found at https://github.com/ THUNLP-MT/SelfSupervisedQE. 1 Quality estimation (QE) of MT aims to evaluate the quality of the outputs of an MT system without references. Training QE models often requires massive parallel data, which are composed of authentic source sentences and machine-translated target sentences with quality annotations produced by manual evaluation or human post-editing (Moura et al., 2020; Hu et al., 2020; Ranasinghe et al., 2020)"
2021.emnlp-main.267,D16-1025,0,0.0268183,"ence- and word-level QE, which performs quality estimation by recovering the masked target words. Experimental results show that our method outperforms previous unsupervised methods on several QE tasks in different language pairs and domains.1 1 Introduction In recent years, neural approaches (Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015; Vaswani et al., 2017) have significantly improved the quality of machine translation (MT). Despite their apparent success, neural machine translation (NMT) systems still inevitably generate erroneous translations in real-world scenarios (Bentivogli et al., 2016; Castilho et al., 2017), especially for low-resource language pairs. Therefore, the evaluation of translation quality plays an important role in some applications of MT. For example, in computer-assisted translation (CAT) (Barrachina et al., 2009), the evaluation of translation quality can significantly reduce human efforts for postediting (Specia, 2011). ∗ Corresponding author Code can be found at https://github.com/ THUNLP-MT/SelfSupervisedQE. 1 Quality estimation (QE) of MT aims to evaluate the quality of the outputs of an MT system without references. Training QE models often requires mas"
2021.emnlp-main.267,2021.acl-long.369,1,0.705842,"process. Moreover, our work utilizes different techniques like WWM (Cui et al., 2019) and MC Dropout (Gal and Ghahramani, 2016) to further improve the performance. 5.2 Masked Language Models Recently, pretrained masked language models (MLMs) (Devlin et al., 2019) have been widely used in various NLP tasks including natural language understanding (Wang et al., 2019) and machine reading comprehension (Xu et al., 2019). The idea of MLM is also used in other complex NLP tasks. For example, Ghazvininejad et al. (2019) introduce a conditional masked language model (CMLM) for non-autoregressive NMT. Chen et al. (2021) and Zhang and van Genabith (2021) present MLM objectives to improve neural word alignment models. MLM objectives are also used in the training process of supervised QE (Kim et al., 2019; Rubino and Sumita, 2020; Cui et al., 2021). To 3329 the best of our knowledge, our work is the first to utilize MLM objectives for QE under unsupervised settings. Our work is also similar to translation language modeling (TLM) (Conneau and Lample, 2019). However, TLM is a multilingual pretraining schema designed for fine-tuning on various multilingual downstream tasks, while our work finetunes a multilingual"
2021.emnlp-main.267,2020.acl-main.747,0,0.03445,"ty Estimation for Machine Translation QE aims to evaluate the quality of machinetranslated sentences without references, which has been studied mainly under supervised settings. Specia et al. (2013) propose a feature-based QE method using various manually designed features and traditional machine learning models. With the recent prevalence of deep learning, various neural methods for QE have been proposed (Kim et al., 2017; Ive et al., 2018; Fan et al., 2019). Recently, with the development of pretraining, multilingual pretrained language models (Devlin et al., 2019; Conneau and Lample, 2019; Conneau et al., 2020) are also utilized in QE (Kim et al., 2019; Kepler et al., 2019; Moura et al., 2020; Ranasinghe et al., 2020; Rubino and Sumita, 2020; Zhang and van Genabith, 2020; Lee, 2020). Due to the data scarcity problem in QE, several studies have endeavored to construct unsupervised QE models. For example, Etchegoyhen et al. (2018) build unsupervised QE models using lexical translation tables and language models. Zhang et al. (2020) utilize lexical similarities based on word vectors. Zhou et al. (2020) propose an enhanced version of Zhang et al. (2020), which also utilizes explicit cross-lingual patter"
2021.emnlp-main.267,D19-1633,0,0.0211716,"ctly utilizing the conditional probabilities given by the model and does not require any further fine-tuning process. Moreover, our work utilizes different techniques like WWM (Cui et al., 2019) and MC Dropout (Gal and Ghahramani, 2016) to further improve the performance. 5.2 Masked Language Models Recently, pretrained masked language models (MLMs) (Devlin et al., 2019) have been widely used in various NLP tasks including natural language understanding (Wang et al., 2019) and machine reading comprehension (Xu et al., 2019). The idea of MLM is also used in other complex NLP tasks. For example, Ghazvininejad et al. (2019) introduce a conditional masked language model (CMLM) for non-autoregressive NMT. Chen et al. (2021) and Zhang and van Genabith (2021) present MLM objectives to improve neural word alignment models. MLM objectives are also used in the training process of supervised QE (Kim et al., 2019; Rubino and Sumita, 2020; Cui et al., 2021). To 3329 the best of our knowledge, our work is the first to utilize MLM objectives for QE under unsupervised settings. Our work is also similar to translation language modeling (TLM) (Conneau and Lample, 2019). However, TLM is a multilingual pretraining schema designe"
2021.emnlp-main.267,C18-1266,0,0.0214412,"t MC Dropout. 5 Related Work Our work is closely related to two lines of research: (1) quality estimation for machine translation, and (2) masked language models. 5.1 Quality Estimation for Machine Translation QE aims to evaluate the quality of machinetranslated sentences without references, which has been studied mainly under supervised settings. Specia et al. (2013) propose a feature-based QE method using various manually designed features and traditional machine learning models. With the recent prevalence of deep learning, various neural methods for QE have been proposed (Kim et al., 2017; Ive et al., 2018; Fan et al., 2019). Recently, with the development of pretraining, multilingual pretrained language models (Devlin et al., 2019; Conneau and Lample, 2019; Conneau et al., 2020) are also utilized in QE (Kim et al., 2019; Kepler et al., 2019; Moura et al., 2020; Ranasinghe et al., 2020; Rubino and Sumita, 2020; Zhang and van Genabith, 2020; Lee, 2020). Due to the data scarcity problem in QE, several studies have endeavored to construct unsupervised QE models. For example, Etchegoyhen et al. (2018) build unsupervised QE models using lexical translation tables and language models. Zhang et al. (2"
2021.emnlp-main.267,W19-5406,0,0.131787,"1 0.233 0.263 0.265 SyntheticQE-MLM 0.386 0.368 0.318 0.309 0.204 0.284 0.181 0.208 Ours 0.504 0.463 0.381 0.383 0.242 0.435 0.318 0.338 Results of Ensemble Unsupervised Models SyntheticQE-MT Ensemble 0.488 0.428 0.360 0.339 0.212 0.246 0.274 0.297 SyntheticQE-MLM Ensemble 0.407 0.379 0.318 0.307 0.210 0.299 0.185 0.216 0.508 0.460 0.373 0.362 0.247 0.317 0.262 0.286 SyntheticQE-MT+MLM Ours Ensemble 0.518 0.462 0.395 0.385 0.248 0.453 0.318 0.359 Method Table 3: Comparison with SyntheticQE (Tuan et al., 2021) on the WMT 2019 sentence- and word-level development and test sets. “*”: we followed Kepler et al. (2019) and implemented the supervised models by fine-tuning the multilingual BERT (Devlin et al., 2019). For the implementation details of the supervised models, please refer to Appendix A.4. Dataset SMT NMT Method SyntheticQE-MT SyntheticQE-MLM Ours SyntheticQE-MT SyntheticQE-MLM Ours Sent 0.469 0.416 0.560 0.526 0.424 0.590 Word 0.417 0.298 0.425 0.444 0.320 0.476 Table 4: Comparison with SyntheticQE (Tuan et al., 2021) on the WMT 2018 En-Lv test sets. En-Lv SMT NMT uMQE 0.385 0.550 0.176 0.221 BERTScore BERTScore++ 0.213 0.155 0.540 0.580 NMT-QE Ours 0.560 0.590 Method En-De En-Ru NMT NMT 0.375 0"
2021.emnlp-main.267,W17-4763,0,0.0200064,"counterpart without MC Dropout. 5 Related Work Our work is closely related to two lines of research: (1) quality estimation for machine translation, and (2) masked language models. 5.1 Quality Estimation for Machine Translation QE aims to evaluate the quality of machinetranslated sentences without references, which has been studied mainly under supervised settings. Specia et al. (2013) propose a feature-based QE method using various manually designed features and traditional machine learning models. With the recent prevalence of deep learning, various neural methods for QE have been proposed (Kim et al., 2017; Ive et al., 2018; Fan et al., 2019). Recently, with the development of pretraining, multilingual pretrained language models (Devlin et al., 2019; Conneau and Lample, 2019; Conneau et al., 2020) are also utilized in QE (Kim et al., 2019; Kepler et al., 2019; Moura et al., 2020; Ranasinghe et al., 2020; Rubino and Sumita, 2020; Zhang and van Genabith, 2020; Lee, 2020). Due to the data scarcity problem in QE, several studies have endeavored to construct unsupervised QE models. For example, Etchegoyhen et al. (2018) build unsupervised QE models using lexical translation tables and language model"
2021.emnlp-main.267,W19-5407,0,0.103634,"o evaluate the quality of machinetranslated sentences without references, which has been studied mainly under supervised settings. Specia et al. (2013) propose a feature-based QE method using various manually designed features and traditional machine learning models. With the recent prevalence of deep learning, various neural methods for QE have been proposed (Kim et al., 2017; Ive et al., 2018; Fan et al., 2019). Recently, with the development of pretraining, multilingual pretrained language models (Devlin et al., 2019; Conneau and Lample, 2019; Conneau et al., 2020) are also utilized in QE (Kim et al., 2019; Kepler et al., 2019; Moura et al., 2020; Ranasinghe et al., 2020; Rubino and Sumita, 2020; Zhang and van Genabith, 2020; Lee, 2020). Due to the data scarcity problem in QE, several studies have endeavored to construct unsupervised QE models. For example, Etchegoyhen et al. (2018) build unsupervised QE models using lexical translation tables and language models. Zhang et al. (2020) utilize lexical similarities based on word vectors. Zhou et al. (2020) propose an enhanced version of Zhang et al. (2020), which also utilizes explicit cross-lingual patterns obtained from word alignments and multi"
2021.emnlp-main.267,2020.wmt-1.122,0,0.151216,"Barrachina et al., 2009), the evaluation of translation quality can significantly reduce human efforts for postediting (Specia, 2011). ∗ Corresponding author Code can be found at https://github.com/ THUNLP-MT/SelfSupervisedQE. 1 Quality estimation (QE) of MT aims to evaluate the quality of the outputs of an MT system without references. Training QE models often requires massive parallel data, which are composed of authentic source sentences and machine-translated target sentences with quality annotations produced by manual evaluation or human post-editing (Moura et al., 2020; Hu et al., 2020; Ranasinghe et al., 2020). As obtaining such annotated data is time-consuming and labor-intensive in practice, unsupervised QE has received increasing attention (Popovi´c, 2012; Etchegoyhen et al., 2018; Zhang et al., 2020; Zhou et al., 2020; Fomicheva et al., 2020; Tuan et al., 2021). Most of the aforementioned methods use various features to conduct unsupervised QE (Popovi´c, 2012; Etchegoyhen et al., 2018; Zhang et al., 2020; Zhou et al., 2020; Fomicheva et al., 2020). These methods are simple and effective but limited to sentence-level tasks. Compared with sentencelevel QE, word-level QE can provide more finegrain"
2021.emnlp-main.267,2020.coling-main.385,0,0.10135,"been studied mainly under supervised settings. Specia et al. (2013) propose a feature-based QE method using various manually designed features and traditional machine learning models. With the recent prevalence of deep learning, various neural methods for QE have been proposed (Kim et al., 2017; Ive et al., 2018; Fan et al., 2019). Recently, with the development of pretraining, multilingual pretrained language models (Devlin et al., 2019; Conneau and Lample, 2019; Conneau et al., 2020) are also utilized in QE (Kim et al., 2019; Kepler et al., 2019; Moura et al., 2020; Ranasinghe et al., 2020; Rubino and Sumita, 2020; Zhang and van Genabith, 2020; Lee, 2020). Due to the data scarcity problem in QE, several studies have endeavored to construct unsupervised QE models. For example, Etchegoyhen et al. (2018) build unsupervised QE models using lexical translation tables and language models. Zhang et al. (2020) utilize lexical similarities based on word vectors. Zhou et al. (2020) propose an enhanced version of Zhang et al. (2020), which also utilizes explicit cross-lingual patterns obtained from word alignments and multilingual MLMs. Fomicheva et al. (2020) use different features extracted from NMT models. Tua"
2021.emnlp-main.267,P16-1162,0,0.110972,"Missing"
2021.emnlp-main.267,2011.eamt-1.12,0,0.0344057,"ni et al., 2017) have significantly improved the quality of machine translation (MT). Despite their apparent success, neural machine translation (NMT) systems still inevitably generate erroneous translations in real-world scenarios (Bentivogli et al., 2016; Castilho et al., 2017), especially for low-resource language pairs. Therefore, the evaluation of translation quality plays an important role in some applications of MT. For example, in computer-assisted translation (CAT) (Barrachina et al., 2009), the evaluation of translation quality can significantly reduce human efforts for postediting (Specia, 2011). ∗ Corresponding author Code can be found at https://github.com/ THUNLP-MT/SelfSupervisedQE. 1 Quality estimation (QE) of MT aims to evaluate the quality of the outputs of an MT system without references. Training QE models often requires massive parallel data, which are composed of authentic source sentences and machine-translated target sentences with quality annotations produced by manual evaluation or human post-editing (Moura et al., 2020; Hu et al., 2020; Ranasinghe et al., 2020). As obtaining such annotated data is time-consuming and labor-intensive in practice, unsupervised QE has rec"
2021.emnlp-main.267,P13-4014,0,0.0716555,"Missing"
2021.emnlp-main.267,P07-2045,0,0.00803237,"16 IT domain translation task, the WMT 2017 QE task, and the WMT 2018 APE task, as well as the Openoffice and KDE4 corpora available in OPUS3 (Tiedemann, 2012). For En-Ru, we used the in-domain parallel data collected by OPUS, including ada83, GNOME, KDE4, OpenOffice, PHP and Ubuntu. To further validate our method’s performance in different domains, we also conducted experiments on the WMT 2018 En-Lv QE task, which is in the biomedical domain. We used the EMEA corpus (which is also available in OPUS) as training data. Sentences were tokenized and truecased using the scripts provided by Moses (Koehn et al., 2007). We also deduplicated the sentences in the training datasets. Table 2 shows the statistics of these datasets. 2 Although some of the training data have quality annotations, we did not use these annotations in the experiments. 3 https://opus.nlpl.eu/ 3325 Year Language Pair Domain 2018 En-Lv Biomedical 2019 En-De En-Ru IT IT System SMT NMT NMT NMT Train 313K 365K 217K Dev 1.00K 1.00K 1.00K 1.00K Test 1.32K 1.45K 1.02K 1.02K Table 2: Statistics of the training, development and test datasets in our experiments. Baselines Implementation Details We mainly compared our method with SyntheticQE (Tuan"
2021.emnlp-main.267,2020.wmt-1.118,0,0.0365847,"t al. (2013) propose a feature-based QE method using various manually designed features and traditional machine learning models. With the recent prevalence of deep learning, various neural methods for QE have been proposed (Kim et al., 2017; Ive et al., 2018; Fan et al., 2019). Recently, with the development of pretraining, multilingual pretrained language models (Devlin et al., 2019; Conneau and Lample, 2019; Conneau et al., 2020) are also utilized in QE (Kim et al., 2019; Kepler et al., 2019; Moura et al., 2020; Ranasinghe et al., 2020; Rubino and Sumita, 2020; Zhang and van Genabith, 2020; Lee, 2020). Due to the data scarcity problem in QE, several studies have endeavored to construct unsupervised QE models. For example, Etchegoyhen et al. (2018) build unsupervised QE models using lexical translation tables and language models. Zhang et al. (2020) utilize lexical similarities based on word vectors. Zhou et al. (2020) propose an enhanced version of Zhang et al. (2020), which also utilizes explicit cross-lingual patterns obtained from word alignments and multilingual MLMs. Fomicheva et al. (2020) use different features extracted from NMT models. Tuan et al. (2021) train unsupervised QE mode"
2021.emnlp-main.267,D15-1166,0,0.0610192,"lop unsupervised QE methods. However, very few of them can be applied to both sentence- and word-level QE tasks, and they may suffer from noises in the synthetic data. To reduce the negative impact of noises, we propose a self-supervised method for both sentence- and word-level QE, which performs quality estimation by recovering the masked target words. Experimental results show that our method outperforms previous unsupervised methods on several QE tasks in different language pairs and domains.1 1 Introduction In recent years, neural approaches (Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015; Vaswani et al., 2017) have significantly improved the quality of machine translation (MT). Despite their apparent success, neural machine translation (NMT) systems still inevitably generate erroneous translations in real-world scenarios (Bentivogli et al., 2016; Castilho et al., 2017), especially for low-resource language pairs. Therefore, the evaluation of translation quality plays an important role in some applications of MT. For example, in computer-assisted translation (CAT) (Barrachina et al., 2009), the evaluation of translation quality can significantly reduce human efforts for posted"
2021.emnlp-main.267,2020.amta-research.11,1,0.822326,"Missing"
2021.emnlp-main.267,2020.wmt-1.119,0,0.0456977,"Missing"
2021.emnlp-main.267,tiedemann-2012-parallel,0,0.0352985,"nts Setup Data and Preprocessing We mainly conducted experiments on the WMT 2019 QE tasks, which consist of tasks in two different language pairs (En-De and En-Ru). Both tasks are in the IT domain. Since our experiments were conducted in an unsupervised setting, we used parallel corpora without quality annotations as training data2 . Specifically, for En-De, we used indomain parallel data from various sources, including the training data from the WMT 2016 IT domain translation task, the WMT 2017 QE task, and the WMT 2018 APE task, as well as the Openoffice and KDE4 corpora available in OPUS3 (Tiedemann, 2012). For En-Ru, we used the in-domain parallel data collected by OPUS, including ada83, GNOME, KDE4, OpenOffice, PHP and Ubuntu. To further validate our method’s performance in different domains, we also conducted experiments on the WMT 2018 En-Lv QE task, which is in the biomedical domain. We used the EMEA corpus (which is also available in OPUS) as training data. Sentences were tokenized and truecased using the scripts provided by Moses (Koehn et al., 2007). We also deduplicated the sentences in the training datasets. Table 2 shows the statistics of these datasets. 2 Although some of the traini"
2021.emnlp-main.267,2021.eacl-main.50,0,0.253522,"evaluate the quality of the outputs of an MT system without references. Training QE models often requires massive parallel data, which are composed of authentic source sentences and machine-translated target sentences with quality annotations produced by manual evaluation or human post-editing (Moura et al., 2020; Hu et al., 2020; Ranasinghe et al., 2020). As obtaining such annotated data is time-consuming and labor-intensive in practice, unsupervised QE has received increasing attention (Popovi´c, 2012; Etchegoyhen et al., 2018; Zhang et al., 2020; Zhou et al., 2020; Fomicheva et al., 2020; Tuan et al., 2021). Most of the aforementioned methods use various features to conduct unsupervised QE (Popovi´c, 2012; Etchegoyhen et al., 2018; Zhang et al., 2020; Zhou et al., 2020; Fomicheva et al., 2020). These methods are simple and effective but limited to sentence-level tasks. Compared with sentencelevel QE, word-level QE can provide more finegrained quality information (Fan et al., 2019), and thus it can better assist post-editing in CAT when combined with sentence-level QE. Recently, Tuan et al. (2021) use synthetic data to train unsupervised QE models, which can be applied for both sentence- and word"
2021.emnlp-main.267,2020.wmt-1.125,0,0.573067,"QE. 1 Quality estimation (QE) of MT aims to evaluate the quality of the outputs of an MT system without references. Training QE models often requires massive parallel data, which are composed of authentic source sentences and machine-translated target sentences with quality annotations produced by manual evaluation or human post-editing (Moura et al., 2020; Hu et al., 2020; Ranasinghe et al., 2020). As obtaining such annotated data is time-consuming and labor-intensive in practice, unsupervised QE has received increasing attention (Popovi´c, 2012; Etchegoyhen et al., 2018; Zhang et al., 2020; Zhou et al., 2020; Fomicheva et al., 2020; Tuan et al., 2021). Most of the aforementioned methods use various features to conduct unsupervised QE (Popovi´c, 2012; Etchegoyhen et al., 2018; Zhang et al., 2020; Zhou et al., 2020; Fomicheva et al., 2020). These methods are simple and effective but limited to sentence-level tasks. Compared with sentencelevel QE, word-level QE can provide more finegrained quality information (Fan et al., 2019), and thus it can better assist post-editing in CAT when combined with sentence-level QE. Recently, Tuan et al. (2021) use synthetic data to train unsupervised QE models, whic"
2021.emnlp-main.267,2020.emnlp-demos.6,0,0.0767023,"Missing"
2021.emnlp-main.267,N19-1242,0,0.0271217,"o be finetuned on labeled training data, while our work conducts unsupervised QE by directly utilizing the conditional probabilities given by the model and does not require any further fine-tuning process. Moreover, our work utilizes different techniques like WWM (Cui et al., 2019) and MC Dropout (Gal and Ghahramani, 2016) to further improve the performance. 5.2 Masked Language Models Recently, pretrained masked language models (MLMs) (Devlin et al., 2019) have been widely used in various NLP tasks including natural language understanding (Wang et al., 2019) and machine reading comprehension (Xu et al., 2019). The idea of MLM is also used in other complex NLP tasks. For example, Ghazvininejad et al. (2019) introduce a conditional masked language model (CMLM) for non-autoregressive NMT. Chen et al. (2021) and Zhang and van Genabith (2021) present MLM objectives to improve neural word alignment models. MLM objectives are also used in the training process of supervised QE (Kim et al., 2019; Rubino and Sumita, 2020; Cui et al., 2021). To 3329 the best of our knowledge, our work is the first to utilize MLM objectives for QE under unsupervised settings. Our work is also similar to translation language m"
2021.emnlp-main.267,2020.emnlp-main.205,0,0.0577838,"Missing"
2021.emnlp-main.267,2021.acl-long.24,0,0.0846356,"Missing"
2021.emnlp-main.480,W19-2305,0,0.0632397,"Missing"
2021.emnlp-main.480,P05-1074,0,0.400455,"Missing"
2021.emnlp-main.480,P19-1602,0,0.0190198,"produce better paraphrases. More recently, retrievalaugmented generation methods have also been investigated (Hashimoto et al., 2018; Kazemnejad et al., 2020; Lewis et al., 2020) for paraphrase generation and achieved promising performance. Unsupervised Paraphrase Generation. Due to the burdensome labeling cost of supervised counterparts, unsupervised paraphrasing methods have drawn increasing research attention in the community. Methods based on variational autoencoders (VAE) are first proposed to generate paraphrases by sampling sentences from the learned latent space (Bowman et al., 2016; Bao et al., 2019; Fu et al., 2019), while the generated sentences are commonly less controllable. To tackle this issue, CGMH (Miao et al., 2019) uses MetropolisHastings sampling to add constraints on the decoder at inference time. Furthermore, researchers try to improve the generation performance in terms of semantic similarity, expression diversity, and language fluency by using simulated annealing (Liu et al., 2019), syntactic control (Huang and Chang, 2021), or dynamic blocking (Niu et al., 2020). In addition, pre-trained translation models have been explored to generate paraphrases via backtranslation (Wi"
2021.emnlp-main.480,P14-1133,0,0.0401777,"damental NLP task tend to be noisy and are not equally informative for that restates text input in a different surface form building the generation model (Ren et al., 2018; Li while preserving its semantic meaning. It serves et al., 2019a; Yoon et al., 2020). Hence, selecting as a cornerstone in a wide spectrum of NLP appli- valuable parallel sentences from weakly-labeled cations, such as question answering (Dong et al., data is vital for solving the studied problem; and 2017), machine translation (Resnik et al., 2010), (iii) the state-of-the-art paraphrasing methods are and semantic parsing (Berant and Liang, 2014). predominantly built upon traditional Seq2Seq modWith the recent advances of neural sequence-to- els, while the necessity of learning from scratch sequence (Seq2Seq) architecture in the field of lan- largely magnifies the learning difficulty when dealguage generation, a growing amount of literature ing with scarce or noisy training data (Guu et al., has also applied Seq2Seq models to the sentential 2018). Thus it is imperative to seek a more roparaphrasing task. bust and knowledge-intensive backbone for learnDespite their promising results, collecting large ing with weakly-labeled paraphrases"
2021.emnlp-main.480,K16-1002,0,0.267179,"angliud}@amazon.com Abstract edge. Therefore, the performance of supervised methods could be largely limited in real-world sceParaphrase generation is a longstanding NLP narios. Due to this problem, unsupervised paratask that has diverse applications for downphrase generation has recently received increasing stream NLP tasks. However, the effectiveattention, but the development is still in its infancy. ness of existing efforts predominantly relies on Generally, sampling-based or editing-based aplarge amounts of golden labeled data. Though unsupervised endeavors have been proposed to proaches (Bowman et al., 2016; Miao et al., 2019) address this issue, they may fail to generate fail to incorporate valuable supervised knowledge, meaningful paraphrases due to the lack of suresulting in less coherent and controllable generpervision signals. In this work, we go beyond ated paraphrases (Liu et al., 2019). In this work, the existing paradigms and propose a novel we propose going beyond the existing learning approach to generate high-quality paraphrases paradigms and investigate a novel research probwith weak supervision data. Specifically, we lem – weakly-supervised paraphrase generation, tackle the weakly-"
2021.emnlp-main.480,P19-1599,0,0.0170064,"leveraging meta-learning to enhance pre-trained language model on paraphrase generation with costless weak supervision data. • We conduct extensive experiments to illustrate the superiority of our approach over both supervised and unsupervised state-of-the-art methods on the task of paraphrase generation. ground-truth parallel sentences are available during the training time. Among supervised efforts, Residual LSTM (Prakash et al., 2016) is one of the earliest works based on neural networks. Later on, Li et al. (2018) propose to make use of deep reinforcement learning and Iyyer et al. (2018); Chen et al. (2019a) leverage syntactic structures to produce better paraphrases. More recently, retrievalaugmented generation methods have also been investigated (Hashimoto et al., 2018; Kazemnejad et al., 2020; Lewis et al., 2020) for paraphrase generation and achieved promising performance. Unsupervised Paraphrase Generation. Due to the burdensome labeling cost of supervised counterparts, unsupervised paraphrasing methods have drawn increasing research attention in the community. Methods based on variational autoencoders (VAE) are first proposed to generate paraphrases by sampling sentences from the learned"
2021.emnlp-main.480,C04-1051,0,0.333399,"Missing"
2021.emnlp-main.480,D17-1091,0,0.0471389,"Missing"
2021.emnlp-main.480,Q18-1031,0,0.0608302,"Missing"
2021.emnlp-main.480,2021.eacl-main.88,0,0.0439054,"based on variational autoencoders (VAE) are first proposed to generate paraphrases by sampling sentences from the learned latent space (Bowman et al., 2016; Bao et al., 2019; Fu et al., 2019), while the generated sentences are commonly less controllable. To tackle this issue, CGMH (Miao et al., 2019) uses MetropolisHastings sampling to add constraints on the decoder at inference time. Furthermore, researchers try to improve the generation performance in terms of semantic similarity, expression diversity, and language fluency by using simulated annealing (Liu et al., 2019), syntactic control (Huang and Chang, 2021), or dynamic blocking (Niu et al., 2020). In addition, pre-trained translation models have been explored to generate paraphrases via backtranslation (Wieting et al., 2017; Guo et al., 2021). But still, those methods can hardly achieve comparable results with supervised approaches. Learning with Weak Supervision. The profound success of machine learning systems largely benefits from abundant labeled data, however, their performance has been shown to degrade noticeably in the presence of inaccurate supervision signals (Hendrycks et al., 2018), especially in an ad2 Related Work versary environmen"
2021.emnlp-main.480,N18-1170,0,0.0180463,"is a new attempt of leveraging meta-learning to enhance pre-trained language model on paraphrase generation with costless weak supervision data. • We conduct extensive experiments to illustrate the superiority of our approach over both supervised and unsupervised state-of-the-art methods on the task of paraphrase generation. ground-truth parallel sentences are available during the training time. Among supervised efforts, Residual LSTM (Prakash et al., 2016) is one of the earliest works based on neural networks. Later on, Li et al. (2018) propose to make use of deep reinforcement learning and Iyyer et al. (2018); Chen et al. (2019a) leverage syntactic structures to produce better paraphrases. More recently, retrievalaugmented generation methods have also been investigated (Hashimoto et al., 2018; Kazemnejad et al., 2020; Lewis et al., 2020) for paraphrase generation and achieved promising performance. Unsupervised Paraphrase Generation. Due to the burdensome labeling cost of supervised counterparts, unsupervised paraphrasing methods have drawn increasing research attention in the community. Methods based on variational autoencoders (VAE) are first proposed to generate paraphrases by sampling sentence"
2021.emnlp-main.480,2020.acl-main.535,0,0.187524,"ty of our approach over both supervised and unsupervised state-of-the-art methods on the task of paraphrase generation. ground-truth parallel sentences are available during the training time. Among supervised efforts, Residual LSTM (Prakash et al., 2016) is one of the earliest works based on neural networks. Later on, Li et al. (2018) propose to make use of deep reinforcement learning and Iyyer et al. (2018); Chen et al. (2019a) leverage syntactic structures to produce better paraphrases. More recently, retrievalaugmented generation methods have also been investigated (Hashimoto et al., 2018; Kazemnejad et al., 2020; Lewis et al., 2020) for paraphrase generation and achieved promising performance. Unsupervised Paraphrase Generation. Due to the burdensome labeling cost of supervised counterparts, unsupervised paraphrasing methods have drawn increasing research attention in the community. Methods based on variational autoencoders (VAE) are first proposed to generate paraphrases by sampling sentences from the learned latent space (Bowman et al., 2016; Bao et al., 2019; Fu et al., 2019), while the generated sentences are commonly less controllable. To tackle this issue, CGMH (Miao et al., 2019) uses Metropol"
2021.emnlp-main.480,D17-1126,0,0.0193606,"39.21 10.91 13.45 15.90 18.87 40.65 45.18 15.62 19.17 and conduct the comparison respectively. Specifically, we use the following datasets to compare with supervised methods: • Quora-S: is the Quora question pair dataset which contains 260K non-parallel sentence pairs and 140K parallel paraphrases. Here we denote the version used by supervised methods as QuoraS. We follow the same setting in Li et al. (2018); Kazemnejad et al. (2020) and randomly sample 100K, 30K, 3K parallel sentences for training, test, and validation, respectively. • Twitter: is the twitter URL paraphrasing corpus built by Lan et al. (2017). Following the setting in Li et al. (2018); Kazemnejad et al. (2020), we sample 110K instances from about 670K automatically labeled data as our training set and two non-overlapping subsets of 5K and 1K instances from the human-annotated data for the test and validation sets, respectively. ROUGE-2 The detailed dataset statistics are summarized in Table 1. Notably, although all the datasets have ground-truth paraphrases, our approach does not use them in the training set, which is as same as unsupervised methods (Siddique et al., 2020). We only allow the model to access the parallel sentences"
2021.emnlp-main.480,2020.acl-main.703,0,0.0532987,"Missing"
2021.emnlp-main.480,D18-1421,0,0.0803577,"asing under a low-resource setting. • We develop a framework LTSL, which is a new attempt of leveraging meta-learning to enhance pre-trained language model on paraphrase generation with costless weak supervision data. • We conduct extensive experiments to illustrate the superiority of our approach over both supervised and unsupervised state-of-the-art methods on the task of paraphrase generation. ground-truth parallel sentences are available during the training time. Among supervised efforts, Residual LSTM (Prakash et al., 2016) is one of the earliest works based on neural networks. Later on, Li et al. (2018) propose to make use of deep reinforcement learning and Iyyer et al. (2018); Chen et al. (2019a) leverage syntactic structures to produce better paraphrases. More recently, retrievalaugmented generation methods have also been investigated (Hashimoto et al., 2018; Kazemnejad et al., 2020; Lewis et al., 2020) for paraphrase generation and achieved promising performance. Unsupervised Paraphrase Generation. Due to the burdensome labeling cost of supervised counterparts, unsupervised paraphrasing methods have drawn increasing research attention in the community. Methods based on variational autoenc"
2021.emnlp-main.480,P19-1332,0,0.0313347,"Missing"
2021.emnlp-main.480,2020.findings-emnlp.334,0,0.0307923,"the-art methods usually exploit a small clean labelled dataset that is allowed under the low resource setting (Mirzasoleiman et al., 2020). For instance, Gold Loss Correction (Hendrycks et al., 2018) uses a clean validation set to recover the label corruption matrix to re-train the predictor model with corrected labels. Learning to Reweight (Ren et al., 2018) proposes a single gradient descent step guided with validation set performance to reweight the training batch. Learning with noisy/weak supervision has drawn increasing attention in the NLP community (Qin et al., 2018; Feng et al., 2018; Ren et al., 2020), but it is seldomly investigated in the filed of paraphrase generation. In this work, we propose a new meta-learning framework that is capable of selecting valuable instances from abundant retrieved weakly-labeled sentence pairs. the following subsections, we will introduce how to solve the main challenges with the proposed pseudo paraphrase expansion module and the metalearning framework LTSL. 3.1 Pseudo Paraphrase Expansion To enable weakly-supervised paraphrase generation, we first propose a plug-and-play pseudo paraphrase expansion module. Essentially, the function of this module is to ob"
2021.emnlp-main.480,W04-1013,0,0.0276027,"nstances from the human-annotated data for the test and validation sets, respectively. ROUGE-2 The detailed dataset statistics are summarized in Table 1. Notably, although all the datasets have ground-truth paraphrases, our approach does not use them in the training set, which is as same as unsupervised methods (Siddique et al., 2020). We only allow the model to access the parallel sentences in the validation set during the learning process. Specifically, when comparing with supervised baselines, we follow the previous works and adopt BLEU-n (Papineni et al., 2002) (up to n-grams), and ROUGE (Lin, 2004) scores as evaluation metrics; similarly, we use iBLEU (Sun and Zhou, 2012), BLEU (Post, 2018) and ROUGE scores for comparing with unsupervised methods. Compared Methods. To show the superiority of our approach, we first include both widely used and state-of-the-art paraphrase generation methods as our baselines. In general, those methods can be divided into two categories: (1) suTo compare our approach with unsupervised efforts, pervised methods that are trained with all the we adopt another two benchmark datasets: parallel sentences in the training corpus, in• Quora-U: is the version of Quor"
2021.emnlp-main.480,J83-1001,0,0.688171,"Missing"
2021.emnlp-main.480,D10-1013,0,0.0366677,"1 Introduction verse paraphrases; (ii) weakly-labeled paraphrases Paraphrase generation is a fundamental NLP task tend to be noisy and are not equally informative for that restates text input in a different surface form building the generation model (Ren et al., 2018; Li while preserving its semantic meaning. It serves et al., 2019a; Yoon et al., 2020). Hence, selecting as a cornerstone in a wide spectrum of NLP appli- valuable parallel sentences from weakly-labeled cations, such as question answering (Dong et al., data is vital for solving the studied problem; and 2017), machine translation (Resnik et al., 2010), (iii) the state-of-the-art paraphrasing methods are and semantic parsing (Berant and Liang, 2014). predominantly built upon traditional Seq2Seq modWith the recent advances of neural sequence-to- els, while the necessity of learning from scratch sequence (Seq2Seq) architecture in the field of lan- largely magnifies the learning difficulty when dealguage generation, a growing amount of literature ing with scarce or noisy training data (Guu et al., has also applied Seq2Seq models to the sentential 2018). Thus it is imperative to seek a more roparaphrasing task. bust and knowledge-intensive back"
2021.emnlp-main.480,P12-2008,0,0.0195979,"sets, respectively. ROUGE-2 The detailed dataset statistics are summarized in Table 1. Notably, although all the datasets have ground-truth paraphrases, our approach does not use them in the training set, which is as same as unsupervised methods (Siddique et al., 2020). We only allow the model to access the parallel sentences in the validation set during the learning process. Specifically, when comparing with supervised baselines, we follow the previous works and adopt BLEU-n (Papineni et al., 2002) (up to n-grams), and ROUGE (Lin, 2004) scores as evaluation metrics; similarly, we use iBLEU (Sun and Zhou, 2012), BLEU (Post, 2018) and ROUGE scores for comparing with unsupervised methods. Compared Methods. To show the superiority of our approach, we first include both widely used and state-of-the-art paraphrase generation methods as our baselines. In general, those methods can be divided into two categories: (1) suTo compare our approach with unsupervised efforts, pervised methods that are trained with all the we adopt another two benchmark datasets: parallel sentences in the training corpus, in• Quora-U: is the version of Quora dataset used by cluding Residual LSTM (Prakash et al., 2016), unsupervise"
2021.emnlp-main.480,D17-1026,0,0.019169,"19; Fu et al., 2019), while the generated sentences are commonly less controllable. To tackle this issue, CGMH (Miao et al., 2019) uses MetropolisHastings sampling to add constraints on the decoder at inference time. Furthermore, researchers try to improve the generation performance in terms of semantic similarity, expression diversity, and language fluency by using simulated annealing (Liu et al., 2019), syntactic control (Huang and Chang, 2021), or dynamic blocking (Niu et al., 2020). In addition, pre-trained translation models have been explored to generate paraphrases via backtranslation (Wieting et al., 2017; Guo et al., 2021). But still, those methods can hardly achieve comparable results with supervised approaches. Learning with Weak Supervision. The profound success of machine learning systems largely benefits from abundant labeled data, however, their performance has been shown to degrade noticeably in the presence of inaccurate supervision signals (Hendrycks et al., 2018), especially in an ad2 Related Work versary environment (Reed et al., 2014). As one Supervised Paraphrase Generation. With the of the central problems in weak supervision, learnfast development of deep learning techniques, n"
2021.emnlp-main.480,P02-1040,0,0.11004,"set and two non-overlapping subsets of 5K and 1K instances from the human-annotated data for the test and validation sets, respectively. ROUGE-2 The detailed dataset statistics are summarized in Table 1. Notably, although all the datasets have ground-truth paraphrases, our approach does not use them in the training set, which is as same as unsupervised methods (Siddique et al., 2020). We only allow the model to access the parallel sentences in the validation set during the learning process. Specifically, when comparing with supervised baselines, we follow the previous works and adopt BLEU-n (Papineni et al., 2002) (up to n-grams), and ROUGE (Lin, 2004) scores as evaluation metrics; similarly, we use iBLEU (Sun and Zhou, 2012), BLEU (Post, 2018) and ROUGE scores for comparing with unsupervised methods. Compared Methods. To show the superiority of our approach, we first include both widely used and state-of-the-art paraphrase generation methods as our baselines. In general, those methods can be divided into two categories: (1) suTo compare our approach with unsupervised efforts, pervised methods that are trained with all the we adopt another two benchmark datasets: parallel sentences in the training corp"
2021.emnlp-main.480,W18-6319,0,0.0119426,"2 The detailed dataset statistics are summarized in Table 1. Notably, although all the datasets have ground-truth paraphrases, our approach does not use them in the training set, which is as same as unsupervised methods (Siddique et al., 2020). We only allow the model to access the parallel sentences in the validation set during the learning process. Specifically, when comparing with supervised baselines, we follow the previous works and adopt BLEU-n (Papineni et al., 2002) (up to n-grams), and ROUGE (Lin, 2004) scores as evaluation metrics; similarly, we use iBLEU (Sun and Zhou, 2012), BLEU (Post, 2018) and ROUGE scores for comparing with unsupervised methods. Compared Methods. To show the superiority of our approach, we first include both widely used and state-of-the-art paraphrase generation methods as our baselines. In general, those methods can be divided into two categories: (1) suTo compare our approach with unsupervised efforts, pervised methods that are trained with all the we adopt another two benchmark datasets: parallel sentences in the training corpus, in• Quora-U: is the version of Quora dataset used by cluding Residual LSTM (Prakash et al., 2016), unsupervised paraphrasing meth"
2021.emnlp-main.480,C16-1275,0,0.130642,"pervised paraphrase generation, which sheds light on the research of sentential paraphrasing under a low-resource setting. • We develop a framework LTSL, which is a new attempt of leveraging meta-learning to enhance pre-trained language model on paraphrase generation with costless weak supervision data. • We conduct extensive experiments to illustrate the superiority of our approach over both supervised and unsupervised state-of-the-art methods on the task of paraphrase generation. ground-truth parallel sentences are available during the training time. Among supervised efforts, Residual LSTM (Prakash et al., 2016) is one of the earliest works based on neural networks. Later on, Li et al. (2018) propose to make use of deep reinforcement learning and Iyyer et al. (2018); Chen et al. (2019a) leverage syntactic structures to produce better paraphrases. More recently, retrievalaugmented generation methods have also been investigated (Hashimoto et al., 2018; Kazemnejad et al., 2020; Lewis et al., 2020) for paraphrase generation and achieved promising performance. Unsupervised Paraphrase Generation. Due to the burdensome labeling cost of supervised counterparts, unsupervised paraphrasing methods have drawn in"
2021.emnlp-main.480,P18-1199,0,0.0270418,"al., 2020). In general, the stateof-the-art methods usually exploit a small clean labelled dataset that is allowed under the low resource setting (Mirzasoleiman et al., 2020). For instance, Gold Loss Correction (Hendrycks et al., 2018) uses a clean validation set to recover the label corruption matrix to re-train the predictor model with corrected labels. Learning to Reweight (Ren et al., 2018) proposes a single gradient descent step guided with validation set performance to reweight the training batch. Learning with noisy/weak supervision has drawn increasing attention in the NLP community (Qin et al., 2018; Feng et al., 2018; Ren et al., 2020), but it is seldomly investigated in the filed of paraphrase generation. In this work, we propose a new meta-learning framework that is capable of selecting valuable instances from abundant retrieved weakly-labeled sentence pairs. the following subsections, we will introduce how to solve the main challenges with the proposed pseudo paraphrase expansion module and the metalearning framework LTSL. 3.1 Pseudo Paraphrase Expansion To enable weakly-supervised paraphrase generation, we first propose a plug-and-play pseudo paraphrase expansion module. Essentially"
2021.emnlp-main.481,2020.acl-main.282,0,0.177682,"he top 50 codes. Mullenbach et al. (2018) proposed CAML that applies separate attention for each label, which generates better label-specific representations for label prediction. They also used the label descriptions to regularize the model (called DL-CAML) in an attempt to improve the prediction of rare labels. To improve the classification performance, Xie et al. (2019) used the multi-scale convolutional attention while Li and Yu (2020) employed multi-filter convolution to learn text patterns of different lengths. Furthermore, to incorporate the inner relationship of the labels, HyperCore (Cao et al., 2020) integrated a hyperbolic representation learning method and a graph convolutional network, and Lu et al. (2020) utilized multi-graph knowledge aggregation. Vu et al. (2020) proposed to combine Bi-LSTM and an extension of structured self-attention mechanism for ICD code prediction. 3 EffectiveCAN Model Figure 1: The architecture of EffectiveCAN. attention component that selects the most important text features and generates label-specific representations for each label, and an output layer that produces the final predictions. The model structure is primarily designed for generating better predi"
2021.emnlp-main.481,P19-4007,0,0.024669,"and recall on the MIMIC data. For comparison purposes, we report F1 and other previously used metrics on both MIMIC-III and the non-English datasets, but the emphasis should be on Micro F1. 5 Results the average of five runs with different random seeds for parameter initialization. We also investigate the interpretability of the model. 5.2 Results on MIMIC-III Results on Dutch and French On the Dutch and French datasets, we establish two baselines. The first is MultiResCNN (Li and Yu, 2020), which is the best performing model on MIMIC-III that is publicly available. The second is XLM-RoBERTa (Conneau et al., 2019), a multi-lingual transformer model.1 XLM-RoBERTa and related models achieve excellent performance on well-known benchmarks such as GLUE (Wang et al., 2018), however they are not well established on the task of long-document, multi-label classification. Table 5 presents our results. Of the models we considered, only EffectiveCAN can be trained on the full label set (i.e. 144 codes for Dutch, 940 codes for French): XLMRoBERTa and MultiResCNN run out of 16GB GPU memory. As such, we resort to comparison with the baselines on only the top-50 codes. XLMRoBERTa yields poor results for both Dutch and"
2021.emnlp-main.481,D18-1485,0,0.0184004,"ural network architectures to learn ing, where a patient encounter containing multi- the semantic embeddings of the document texts. ple records are assigned with appropriate medical For example, XML-CNN proposed by Liu et al. 5941 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 5941–5953 c November 7–11, 2021. 2021 Association for Computational Linguistics (2017) employs a 1-dimension convolutional network along with dynamic pooling to learn the text representation. RNN-based sequence-to-sequence models, such as SGM (Yang et al., 2018) and SU4MLC (Lin et al., 2018) use an encoder to encode the information of the input text and a decoder to generate the predicted labels. AttentionXML proposed by You et al. (2019a) leverages the BiLSTM and label-aware attention to capture the most relevant texts for each label. As a follow-up, MAGNET (Pal et al., 2020) incorporates graph neural network to capture the attentive dependency structure among the labels. More recently, transformers such as the X-transformer (Chang et al., 2020) have also been introduced. X-transformer tackles MLDC in three steps: label clustering, transformer classification and label ranking. T"
2021.emnlp-main.481,2021.ccl-1.108,0,0.0724469,"Missing"
2021.emnlp-main.481,2020.emnlp-main.235,0,0.032491,"nerates better label-specific representations for label prediction. They also used the label descriptions to regularize the model (called DL-CAML) in an attempt to improve the prediction of rare labels. To improve the classification performance, Xie et al. (2019) used the multi-scale convolutional attention while Li and Yu (2020) employed multi-filter convolution to learn text patterns of different lengths. Furthermore, to incorporate the inner relationship of the labels, HyperCore (Cao et al., 2020) integrated a hyperbolic representation learning method and a graph convolutional network, and Lu et al. (2020) utilized multi-graph knowledge aggregation. Vu et al. (2020) proposed to combine Bi-LSTM and an extension of structured self-attention mechanism for ICD code prediction. 3 EffectiveCAN Model Figure 1: The architecture of EffectiveCAN. attention component that selects the most important text features and generates label-specific representations for each label, and an output layer that produces the final predictions. The model structure is primarily designed for generating better predictions on multi-label classification tasks from three aspects: (1) generating meaningful representations for in"
2021.emnlp-main.481,N18-1100,0,0.20185,"ode system with over 90,000 codes) and long documents are challenging for ML models. In addition, coding requires extracting useful information from specific locations across the entire encounter to support the assigned codes. Consequently, effective models with the capability of handling these challenges will have an immense impact in the medical domain by helping to reduce coding cost, improve coding accuracy and increase customer satisfaction. Deep learning methods have been demonstrated to produce the state-of-the-art outcomes on benchmark MLDC and medical coding tasks (You et al., 2019a; Mullenbach et al., 2018; Chang et al., 2020), but demands remain for more effective and accurate solutions. In this paper, we propose EffectiveCAN, an effective convolution attention network for MLDC. Our models try to strike a careful balance of simplicity and effective over-parameterization such that we can effectively model long documents and capture nuanced aspects of the whole document texts. Such a model is particularly useful for addressing the challenges of automatic medical coding. We evaluate our models on the widely used MIMIC-III dataset (Johnson et al., 2016), and attain state-of-the-art results across"
2021.emnlp-main.481,W18-5446,0,0.0874108,"Missing"
2021.emnlp-main.481,C18-1330,0,0.0281998,"c medical cod- volve various neural network architectures to learn ing, where a patient encounter containing multi- the semantic embeddings of the document texts. ple records are assigned with appropriate medical For example, XML-CNN proposed by Liu et al. 5941 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 5941–5953 c November 7–11, 2021. 2021 Association for Computational Linguistics (2017) employs a 1-dimension convolutional network along with dynamic pooling to learn the text representation. RNN-based sequence-to-sequence models, such as SGM (Yang et al., 2018) and SU4MLC (Lin et al., 2018) use an encoder to encode the information of the input text and a decoder to generate the predicted labels. AttentionXML proposed by You et al. (2019a) leverages the BiLSTM and label-aware attention to capture the most relevant texts for each label. As a follow-up, MAGNET (Pal et al., 2020) incorporates graph neural network to capture the attentive dependency structure among the labels. More recently, transformers such as the X-transformer (Chang et al., 2020) have also been introduced. X-transformer tackles MLDC in three steps: label clustering, transformer class"
2021.emnlp-main.481,P19-1205,0,0.355949,"(viz. the ICD-10 code system with over 90,000 codes) and long documents are challenging for ML models. In addition, coding requires extracting useful information from specific locations across the entire encounter to support the assigned codes. Consequently, effective models with the capability of handling these challenges will have an immense impact in the medical domain by helping to reduce coding cost, improve coding accuracy and increase customer satisfaction. Deep learning methods have been demonstrated to produce the state-of-the-art outcomes on benchmark MLDC and medical coding tasks (You et al., 2019a; Mullenbach et al., 2018; Chang et al., 2020), but demands remain for more effective and accurate solutions. In this paper, we propose EffectiveCAN, an effective convolution attention network for MLDC. Our models try to strike a careful balance of simplicity and effective over-parameterization such that we can effectively model long documents and capture nuanced aspects of the whole document texts. Such a model is particularly useful for addressing the challenges of automatic medical coding. We evaluate our models on the widely used MIMIC-III dataset (Johnson et al., 2016), and attain state-"
2021.eval4nlp-1.11,Q18-1008,0,0.0480223,"Missing"
2021.eval4nlp-1.11,N19-1210,0,0.284248,"textual word embeddings lose this information, flattening word embeddings with permutation-based staall instances of the same term into a single word tistical tests. We use the false discovery rate embedding. procedure to address the large number of hyIn this paper, we focus on the problem of robust pothesis tests being conducted simultaneously. We demonstrate the performance of this apsemantic change detection in scenarios where there proach in simulation where it achieves consisis either limited data or low frequency terms of tently high precision by suppressing false posiinterest (e.g. Del Tredici et al. (2019)). Our aptives. We additionally analyze real-world data proach is based on contextual word embeddings, from SemEval-2020 Task 1 and the Liverpool such as those produced by BERT (Devlin et al., FC subreddit corpus. We show that by taking 2019), and permutation-based statistical tests. Consample variation into account, we can improve textual word embeddings have several advantages the robustness of individual semantic shift estimates without degrading overall performance. over non-contextual embeddings for inferring semantic shift when there is limited data. First, we 1 Introduction can leverage"
2021.eval4nlp-1.11,N19-1423,0,0.0144572,"e enumerated), we first use n = 103 . If the (2020a), we fine-tune a pre-trained BERT model for p-value is < 0.05, we increase n to 104 . Finally, each data set, combining both time periods. After 5 fine-tuning, we use the following procedure to gen- if the p-value is < 0.005, we increase n to 10 . 1 P-values with a value of 0.0 are reported as n . erate word representations: we feed sentences of up to 512 tokens into BERT and extract contextual For example, from the Liverpool FC data set, the embeddings for each token in the sequence. Fol- word shovel appears 5 and 35 times in the 2011lowing Devlin et al. (2019), we extract embeddings 2013 and 2017 corpora, respectively. The number  by summing the last 4 encoder layers in the model. of combinations, 40 , is too large to enumerate 5 As BERT uses byte-pair input encoding, not all exhaustively, forcing us to use random sampling. tokens correspond to individual words (Kudo and Figure 1 shows the sampling distribution together 106 500 Frequency 400 300 200 100 0 0.00 0.05 0.10 0.15 Cosine Distance Figure 1: Histogram of the sampling distribution for shovel from the Liverpool FC data set. The red dashed line at x = 0.104 is the observed cosine distance be"
2021.eval4nlp-1.11,P19-1044,0,0.0164637,"ings from the previous time step (Kim et al., 2014). Subsequent methods improved performance by training independent embedding models for each corpus (Kulkarni et al., 2015; Hamilton et al., 2016b). Embeddings are invariant under rotation and therefore need to be aligned by solving the orthogonal Procrustes problem (Hamilton et al., 3 Data 2016b). This alignment step can be avoided altogether by, for example, comparing word neighbor- Table 1 lists the data sets used in this article. The hoods (Hamilton et al., 2016a) or using temporal Liverpool FC corpus collects data from the Livreferencing (Dubossarsky et al., 2019). erpool Football Club subreddit from the Reddit Semantic shift detection with contextual word online discussion forum. The corpus is in English embeddings is becoming increasingly popular. Hu and split into two time periods from 2011-2013 and et al. (2019) used BERT embeddings to define 2017 (Del Tredici et al., 2019). SemEval-2020 Task exemplar representations for pre-defined word 1 was created to benchmark semantic change detecsenses to track usage over time. Several meth- tion methods using two subtasks: binary classifica105 Target words Liverpool FC SemEval-2020 English SemEval-2020 Germa"
2021.eval4nlp-1.11,D16-1229,0,0.0941599,"e used for fine-tuning to perform semantic change detection using more limited data (Martinc et al., 2020a). Lastly, researchers have started to experiment with ensembling multiple types of word embeddings and distance metrics to improve overall performance (Kutuzov and Giulianelli, 2020; Martinc et al., 2020b). Recently, there have been several benchmarking studies of semantic change detection methods using simulated data (Shoemark et al., 2019; Schlechtweg et al., 2019) and manually annotated data sets (Schlechtweg et al., 2020). These studies found that variations on the method proposed by Hamilton et al. (2016b) performed best, however, methods based on contextual word embeddings were either absent or the study was based on data where contextual information was partially lost due to shuffling the order of sentences in the corpus (Schlechtweg et al., 2020). In this paper, we use contextual word embeddings to identify statistically significant semantic shifts. Statistical significance has not been the subject of much investigation in the semantic shift literature. Indeed, the only approach we are aware of was proposed by Kulkarni et al. (2015), which used bootstrapping to perform change-point detecti"
2021.eval4nlp-1.11,P16-1141,0,0.11045,"e used for fine-tuning to perform semantic change detection using more limited data (Martinc et al., 2020a). Lastly, researchers have started to experiment with ensembling multiple types of word embeddings and distance metrics to improve overall performance (Kutuzov and Giulianelli, 2020; Martinc et al., 2020b). Recently, there have been several benchmarking studies of semantic change detection methods using simulated data (Shoemark et al., 2019; Schlechtweg et al., 2019) and manually annotated data sets (Schlechtweg et al., 2020). These studies found that variations on the method proposed by Hamilton et al. (2016b) performed best, however, methods based on contextual word embeddings were either absent or the study was based on data where contextual information was partially lost due to shuffling the order of sentences in the corpus (Schlechtweg et al., 2020). In this paper, we use contextual word embeddings to identify statistically significant semantic shifts. Statistical significance has not been the subject of much investigation in the semantic shift literature. Indeed, the only approach we are aware of was proposed by Kulkarni et al. (2015), which used bootstrapping to perform change-point detecti"
2021.eval4nlp-1.11,P19-1379,0,0.0359244,"Missing"
2021.eval4nlp-1.11,W14-2517,0,0.0283933,"e developed to detect semantic change, including dynamic topic models (Blei and Lafferty, 2006), word co-occurrence statistics (Gulordava and Baroni, 2011) and graphbased methods (Mitra et al., 2014). Methods for semantic change detection based on word embeddings exploit their distributional properties to identify words whose relative position in the embedding space has changed over time, implying a concordant change in meaning (Kutuzov et al., 2018). The earliest work in this area was based on continuous training, initializing each embedding model with embeddings from the previous time step (Kim et al., 2014). Subsequent methods improved performance by training independent embedding models for each corpus (Kulkarni et al., 2015; Hamilton et al., 2016b). Embeddings are invariant under rotation and therefore need to be aligned by solving the orthogonal Procrustes problem (Hamilton et al., 3 Data 2016b). This alignment step can be avoided altogether by, for example, comparing word neighbor- Table 1 lists the data sets used in this article. The hoods (Hamilton et al., 2016a) or using temporal Liverpool FC corpus collects data from the Livreferencing (Dubossarsky et al., 2019). erpool Football Club sub"
2021.eval4nlp-1.11,D18-2012,0,0.0323782,"Missing"
2021.eval4nlp-1.11,2020.acl-main.365,0,0.0211824,"and Comparison of NLP Systems (Eval4NLP 2021), pages 104–113 c November 10, 2021. 2021 Association for Computational Linguistics improves precision and scales to estimating the uncertainty for all words in a vocabulary. • We evaluate the impact of statistical testing on overall performance using manually annotated data sets in multiple languages. In a majority of cases, our approach improves performance, resulting in higher Spearman correlations between estimated semantic shifts and annotations. 2 Related Work ods have side-stepped the need for known word senses by clustering BERT embeddings (Giulianelli et al., 2020; Martinc et al., 2020b) and shown that clustering-based approaches can scale to the whole vocabulary (Montariol et al., 2021). Another benefit of using contextual word embeddings is that pre-trained models are widely available for many different languages. These models can be used for fine-tuning to perform semantic change detection using more limited data (Martinc et al., 2020a). Lastly, researchers have started to experiment with ensembling multiple types of word embeddings and distance metrics to improve overall performance (Kutuzov and Giulianelli, 2020; Martinc et al., 2020b). Recently,"
2021.eval4nlp-1.11,2020.semeval-1.14,0,0.019474,"rd senses by clustering BERT embeddings (Giulianelli et al., 2020; Martinc et al., 2020b) and shown that clustering-based approaches can scale to the whole vocabulary (Montariol et al., 2021). Another benefit of using contextual word embeddings is that pre-trained models are widely available for many different languages. These models can be used for fine-tuning to perform semantic change detection using more limited data (Martinc et al., 2020a). Lastly, researchers have started to experiment with ensembling multiple types of word embeddings and distance metrics to improve overall performance (Kutuzov and Giulianelli, 2020; Martinc et al., 2020b). Recently, there have been several benchmarking studies of semantic change detection methods using simulated data (Shoemark et al., 2019; Schlechtweg et al., 2019) and manually annotated data sets (Schlechtweg et al., 2020). These studies found that variations on the method proposed by Hamilton et al. (2016b) performed best, however, methods based on contextual word embeddings were either absent or the study was based on data where contextual information was partially lost due to shuffling the order of sentences in the corpus (Schlechtweg et al., 2020). In this paper,"
2021.eval4nlp-1.11,W11-2508,0,0.010254,"example, to analyze historical word usage (Hamilton et al., 2016a) and to identify statistical laws of language change (Hamilton et al., 2016b). More recently, there has been increased interest in detecting short-term meaning change, such as novel slang terms, in Amazon reviews (Kulkarni et al., 2015), Twitter data (Shoemark et al., 2019) and specialist online communities (Del Tredici et al., 2019). Prior to the wide-spread use of word embeddings, numerous methods were developed to detect semantic change, including dynamic topic models (Blei and Lafferty, 2006), word co-occurrence statistics (Gulordava and Baroni, 2011) and graphbased methods (Mitra et al., 2014). Methods for semantic change detection based on word embeddings exploit their distributional properties to identify words whose relative position in the embedding space has changed over time, implying a concordant change in meaning (Kutuzov et al., 2018). The earliest work in this area was based on continuous training, initializing each embedding model with embeddings from the previous time step (Kim et al., 2014). Subsequent methods improved performance by training independent embedding models for each corpus (Kulkarni et al., 2015; Hamilton et al."
2021.eval4nlp-1.11,2020.lrec-1.592,0,0.548762,"ems (Eval4NLP 2021), pages 104–113 c November 10, 2021. 2021 Association for Computational Linguistics improves precision and scales to estimating the uncertainty for all words in a vocabulary. • We evaluate the impact of statistical testing on overall performance using manually annotated data sets in multiple languages. In a majority of cases, our approach improves performance, resulting in higher Spearman correlations between estimated semantic shifts and annotations. 2 Related Work ods have side-stepped the need for known word senses by clustering BERT embeddings (Giulianelli et al., 2020; Martinc et al., 2020b) and shown that clustering-based approaches can scale to the whole vocabulary (Montariol et al., 2021). Another benefit of using contextual word embeddings is that pre-trained models are widely available for many different languages. These models can be used for fine-tuning to perform semantic change detection using more limited data (Martinc et al., 2020a). Lastly, researchers have started to experiment with ensembling multiple types of word embeddings and distance metrics to improve overall performance (Kutuzov and Giulianelli, 2020; Martinc et al., 2020b). Recently, there have been severa"
2021.eval4nlp-1.11,2020.semeval-1.6,0,0.0841206,"Missing"
2021.eval4nlp-1.11,P14-1096,0,0.0676021,"Missing"
2021.eval4nlp-1.11,2021.naacl-main.369,0,0.0494247,"s improves precision and scales to estimating the uncertainty for all words in a vocabulary. • We evaluate the impact of statistical testing on overall performance using manually annotated data sets in multiple languages. In a majority of cases, our approach improves performance, resulting in higher Spearman correlations between estimated semantic shifts and annotations. 2 Related Work ods have side-stepped the need for known word senses by clustering BERT embeddings (Giulianelli et al., 2020; Martinc et al., 2020b) and shown that clustering-based approaches can scale to the whole vocabulary (Montariol et al., 2021). Another benefit of using contextual word embeddings is that pre-trained models are widely available for many different languages. These models can be used for fine-tuning to perform semantic change detection using more limited data (Martinc et al., 2020a). Lastly, researchers have started to experiment with ensembling multiple types of word embeddings and distance metrics to improve overall performance (Kutuzov and Giulianelli, 2020; Martinc et al., 2020b). Recently, there have been several benchmarking studies of semantic change detection methods using simulated data (Shoemark et al., 2019;"
2021.eval4nlp-1.11,D19-1007,0,0.0340054,"Missing"
2021.eval4nlp-1.11,N18-1190,0,0.0210414,"ing in semantic change detection. Our contributions are as follows: acterize the uncertainty of the estimated semantic shift for each individual word. This is especially • We show how to apply statistical significance problematic because semantic change detection tests to any semantic change detection method is usually based on word embeddings (Kutuzov based on contextual word embeddings. To et al., 2018) and recently it has been observed that our knowledge, this is the first paper to use their stability can vary widely across term frequenstatistical testing in the context of individual cies (Wendlandt et al., 2018; Antoniak and Mimno, words in semantic change detection. 2018), implying that many semantic shift estimates • We show in simulation that using permutation are erroneously inflated or underestimated. Prior tests while controlling the false discovery rate 104 Proceedings of the 2nd Workshop on Evaluation and Comparison of NLP Systems (Eval4NLP 2021), pages 104–113 c November 10, 2021. 2021 Association for Computational Linguistics improves precision and scales to estimating the uncertainty for all words in a vocabulary. • We evaluate the impact of statistical testing on overall performance usin"
2021.fever-1.6,D14-1059,0,0.0311482,"rmation, including fake news and online rumors, have also been growing and spreading widely over the past several years. Vosoughi et al. (2018) shows that false news travels even faster, deeper and broader than the truth. To prevent harm from this false information, automatically verifying the truthfulness of textual contents is becoming an urgent need for our society. In this work, we study fact verification with the goal of automatically assessing the veracity of a textual claim given supporting evidence. Most existing methods consider fact verification as a natural language inference task (Angeli and Manning, 2014). Usually, these systems concatenate claim and its supporting evidence sentences, and then feed them into a classification model (Nie et al., 2019). Alternatively, previous studies construct graph structures based on claim and evidence, 50 Proceedings of the Fourth Workshop on Fact Extraction and VERification (FEVER) at EMNLP 2021, pages 50–59 November 10, 2021. ©2021 Association for Computational Linguistics 2 Claim #1: Assassin’s Creed has only ever been released on a Microsoft and Sony platform. Gold Evidence: The main video game series consists of nine games , developed by Ubisoft, release"
2021.fever-1.6,2020.findings-emnlp.313,0,0.479362,"chell, 2014), and different fact checking datasets usually provide different evidence sources. Vlachos and Riedel (2014) propose a fact verification dataset by collecting 106 labeled political claims and providing the journalists’ analysis material as the evidence. Ferreira and Vlachos (2016) construct the Emergent dataset containing 300 labeled rumors and 2,595 associated news articles, collected and labelled by journalists. L IAR (Wang, 2017) is a dataset for fake news detection. It contains 2.8K labeled short statements from the web, with detailed analysis and source documents as evidence. Chen et al. (2020c) build TABFACT, a dataset collecting Wikipedia tables as the evidence for human-labeled statements. Recently, the FEVER shared task 1.0 (Thorne et al., 2018) attracts attention from the research community. It is a challenge that requires participants to develop automatic fact verification systems to check the truthfulness of human-generated claims by extracted evidence from Wikipedia. Many systems were proposed for this challenging task. Nie et al. (2019) design a Neural Semantic Matching Network that takes the concatenation of all evidence sentences as input. They also propose a two-hop evi"
2021.fever-1.6,P18-1224,0,0.0206852,"em in Section 4.1 and then introduce our model for incorporating entity knowledge in Section 4.2. Finally, we present our complete solution to the FEVER Challenge including document retrieval, evidence selection and entity description collection in Section 4.3. Modeling External Knowledge in NLP The usage of external knowledge, like WordNet, Wikipedia and knowledge graph, has benefited many natural language processing tasks including natural language inference and fact verification. Jijkoun et al. (2005) uses WordNet to measure word similarity to obtain a better textual entailment recognizer. Chen et al. (2018) proposes a neural network model for natural language inference equipped with several external knowledge. Wang et al. (2019) finds that utilizing ConceptNet as an external knowledge source can benefit entailment model in scientific domain. Chen et al. (2020b) proposes WIKINLI, a large-scale naturally annotated dataset constructed from Wikipedia category graph. And they show that model pretrained on this dataset can achieve better performance on downstream natural language entailment tasks. 3 Method 4.1 Problem Formulation Given the input claim and its retrieved evidence sentences, our approach"
2021.fever-1.6,P17-1152,0,0.0312499,"ins 185,445 claims generated by altering sentences extracted from Wikipedia. To verify a claim in FEVER, a model typically follows a three-step pipeline framework, i.e. document retrieval, sentence selection and claim verification. In document retrieval, a system matches the claim to Wikipedia articles by extracted named entities and phrases using a search engine built on Wikipedia. In sentence selection, a system ranks the sentences from retrieved articles by their similarity scores against the claim. The similarity score can be calculated by a trainable regression model, like Enhanced LSTM (Chen et al., 2017), or pretrained language models like BERT and RoBERTa (Devlin et al., 2019; Liu et al., 2019). In claim verification, a system classifies the truthfulness of the claim based on top-ranked sentences from the previous step, also known as the evidence sentences. Like most participants in this 4.2 Model Architecture The general architecture of our fact verification model is shown in Figure 1. It is a classification neural network based on RoBERTa (Liu et al., 2019), a Transformer-based model (Vaswani et al., 2017) pretrained on large corpora with a masked language modeling objective. We concatenat"
2021.fever-1.6,N16-1138,0,0.0136683,"S X) is a series of proprietary graphical operating systems developed and marketed by Apple Inc. Verdict: REFUTED 2.1 Related Work Fact Verification Fact checking is a challenging task aiming to automatically verify the truthfulness of claims. A claim can be a plain text or a triple of (subject, predicate, object) (Nakashole and Mitchell, 2014), and different fact checking datasets usually provide different evidence sources. Vlachos and Riedel (2014) propose a fact verification dataset by collecting 106 labeled political claims and providing the journalists’ analysis material as the evidence. Ferreira and Vlachos (2016) construct the Emergent dataset containing 300 labeled rumors and 2,595 associated news articles, collected and labelled by journalists. L IAR (Wang, 2017) is a dataset for fake news detection. It contains 2.8K labeled short statements from the web, with detailed analysis and source documents as evidence. Chen et al. (2020c) build TABFACT, a dataset collecting Wikipedia tables as the evidence for human-labeled statements. Recently, the FEVER shared task 1.0 (Thorne et al., 2018) attracts attention from the research community. It is a challenge that requires participants to develop automatic fa"
2021.fever-1.6,W18-2501,0,0.0135236,"ntity knowledge encoder. (6) where hl−1 ei is the i-th output of the (l − 1)-th layer of Te , and Wel−1 ∈ Rde ×dm , bl−1 ∈ Rdm are e weight and bias. Then the fact verification module Tm conducts ˆ l−1 }, along ˆ el−1 = {h unidirectional attention to H ei l . with its self-attention, to produce the output Hm 4.3 In this section, we introduce our complete solution to the FEVER Challenge of fact verification. Document Retrieval We adopt the same document retrieval module as in (Hanselowski et al., 2018; Liu et al., 2020). For a given claim, it first utilizes the constituency parser in AllenNLP (Gardner et al., 2018) to extract all phrases which potentially indicate entities. Then it uses these phrases as queries to find relevant Wikipedia pages through the online MediaWiki API. Then the highestranked results are retrieved and further filtered by a set of rules. ˜ l =LN(H l−1 H m m l Hm l−1 l−1 ˆ l−1 + Att(Hm , [Hm , He ])) l l ˜ m + FFN(H ˜ m )), =LN(H Complete Solution (7) (8) where [∗, ∗] indicates the element-wise concatenation of two lists of vectors. And the entity knowledge encoder Te carries out its self-attention as in standard Transformer models. Evidence Selection We use the evidence selection"
2021.fever-1.6,N18-1074,0,0.0300416,"Missing"
2021.fever-1.6,W18-5516,0,0.30705,"a. Many systems were proposed for this challenging task. Nie et al. (2019) design a Neural Semantic Matching Network that takes the concatenation of all evidence sentences as input. They also propose a two-hop evidence enhancement process where they apply sentence selection twice to retrieve more related evidence sentences. Stammbach and Neumann (2019) propose a two-staged selection process with two different retrieval models for selecting evidence sentences. Yoneda et al. (2018) infer the veracity of each claim-evidence pair and make final prediction by aggregating multiple predicted labels. Hanselowski et al. (2018) encode each claim-evidence pair separately, and use a pooling function to aggregate features for prediction. Zhou et al. (2019) formulates claim verification as a graph reasoning task and propose a new model with graph neural networks. Liu et al. (2020) regards sentences as the nodes of a graph and uses Kernel Graph Attention Network (KGAT) to aggregate information. Zhong et al. (2020) further constructs a semantic-level graph for input claim and evidence and perform reasoning over this graph with pretrained XLNet model (Yang et al., 2019). Similar to our work, some previous systems also Clai"
2021.fever-1.6,2021.ccl-1.108,0,0.0432071,"Missing"
2021.fever-1.6,W14-2508,0,0.0161163,"platforms . Entity Knowledge: Wii U: The Wii U is a home video game console developed by Nintendo as the successor to the Wii. OS X: macOS (previously Mac OS X and later OS X) is a series of proprietary graphical operating systems developed and marketed by Apple Inc. Verdict: REFUTED 2.1 Related Work Fact Verification Fact checking is a challenging task aiming to automatically verify the truthfulness of claims. A claim can be a plain text or a triple of (subject, predicate, object) (Nakashole and Mitchell, 2014), and different fact checking datasets usually provide different evidence sources. Vlachos and Riedel (2014) propose a fact verification dataset by collecting 106 labeled political claims and providing the journalists’ analysis material as the evidence. Ferreira and Vlachos (2016) construct the Emergent dataset containing 300 labeled rumors and 2,595 associated news articles, collected and labelled by journalists. L IAR (Wang, 2017) is a dataset for fake news detection. It contains 2.8K labeled short statements from the web, with detailed analysis and source documents as evidence. Chen et al. (2020c) build TABFACT, a dataset collecting Wikipedia tables as the evidence for human-labeled statements. R"
2021.fever-1.6,2020.acl-main.655,0,0.181378,"ce selection twice to retrieve more related evidence sentences. Stammbach and Neumann (2019) propose a two-staged selection process with two different retrieval models for selecting evidence sentences. Yoneda et al. (2018) infer the veracity of each claim-evidence pair and make final prediction by aggregating multiple predicted labels. Hanselowski et al. (2018) encode each claim-evidence pair separately, and use a pooling function to aggregate features for prediction. Zhou et al. (2019) formulates claim verification as a graph reasoning task and propose a new model with graph neural networks. Liu et al. (2020) regards sentences as the nodes of a graph and uses Kernel Graph Attention Network (KGAT) to aggregate information. Zhong et al. (2020) further constructs a semantic-level graph for input claim and evidence and perform reasoning over this graph with pretrained XLNet model (Yang et al., 2019). Similar to our work, some previous systems also Claim #2: Beastie Boys was formed in Australia. Gold Evidence: The Beastie Boys were an American hip hop group from New York City, formed in 1981. Entity Knowledge: New York City: New York City (NYC), often called simply New York, is the most populous city i"
2021.fever-1.6,P14-1095,0,0.0296596,"ed by Ubisoft, released on PlayStation 3, PlayStation 4, Xbox 360, Xbox One, Wii U, Microsoft Windows, and OS X platforms . Entity Knowledge: Wii U: The Wii U is a home video game console developed by Nintendo as the successor to the Wii. OS X: macOS (previously Mac OS X and later OS X) is a series of proprietary graphical operating systems developed and marketed by Apple Inc. Verdict: REFUTED 2.1 Related Work Fact Verification Fact checking is a challenging task aiming to automatically verify the truthfulness of claims. A claim can be a plain text or a triple of (subject, predicate, object) (Nakashole and Mitchell, 2014), and different fact checking datasets usually provide different evidence sources. Vlachos and Riedel (2014) propose a fact verification dataset by collecting 106 labeled political claims and providing the journalists’ analysis material as the evidence. Ferreira and Vlachos (2016) construct the Emergent dataset containing 300 labeled rumors and 2,595 associated news articles, collected and labelled by journalists. L IAR (Wang, 2017) is a dataset for fake news detection. It contains 2.8K labeled short statements from the web, with detailed analysis and source documents as evidence. Chen et al."
2021.fever-1.6,P17-2067,0,0.017745,"llenging task aiming to automatically verify the truthfulness of claims. A claim can be a plain text or a triple of (subject, predicate, object) (Nakashole and Mitchell, 2014), and different fact checking datasets usually provide different evidence sources. Vlachos and Riedel (2014) propose a fact verification dataset by collecting 106 labeled political claims and providing the journalists’ analysis material as the evidence. Ferreira and Vlachos (2016) construct the Emergent dataset containing 300 labeled rumors and 2,595 associated news articles, collected and labelled by journalists. L IAR (Wang, 2017) is a dataset for fake news detection. It contains 2.8K labeled short statements from the web, with detailed analysis and source documents as evidence. Chen et al. (2020c) build TABFACT, a dataset collecting Wikipedia tables as the evidence for human-labeled statements. Recently, the FEVER shared task 1.0 (Thorne et al., 2018) attracts attention from the research community. It is a challenge that requires participants to develop automatic fact verification systems to check the truthfulness of human-generated claims by extracted evidence from Wikipedia. Many systems were proposed for this chall"
2021.fever-1.6,W18-5519,0,0.0225779,"verification models with external entity knowledge. • We design an entity knowledge encoder module and employ unidirectional attention to effectively incorporate entity descriptions. • Empirical results show that our approach achieves competitive performance on the FEVER dataset, and ablation study shows that incorporating entity knowledge is useful for fact verification. 51 focus on using entity information for fact verification. Taniguchi et al. (2018) first extract entities from the claim and propose to use a simple entity-linking system based on text match to retrieve evidence documents. Nooralahzadeh and Øvrelid (2018) select evidence documents by finding article titles which contain the entities and noun phrases of the claim. 2.2 challenge, we adopt existing approaches for document retrieval and sentence selection, while mainly focusing on the claim verification model. 4 In this section, we first formalize the fact verification problem in Section 4.1 and then introduce our model for incorporating entity knowledge in Section 4.2. Finally, we present our complete solution to the FEVER Challenge including document retrieval, evidence selection and entity description collection in Section 4.3. Modeling Externa"
2021.fever-1.6,D19-6616,0,0.0122313,"he FEVER shared task 1.0 (Thorne et al., 2018) attracts attention from the research community. It is a challenge that requires participants to develop automatic fact verification systems to check the truthfulness of human-generated claims by extracted evidence from Wikipedia. Many systems were proposed for this challenging task. Nie et al. (2019) design a Neural Semantic Matching Network that takes the concatenation of all evidence sentences as input. They also propose a two-hop evidence enhancement process where they apply sentence selection twice to retrieve more related evidence sentences. Stammbach and Neumann (2019) propose a two-staged selection process with two different retrieval models for selecting evidence sentences. Yoneda et al. (2018) infer the veracity of each claim-evidence pair and make final prediction by aggregating multiple predicted labels. Hanselowski et al. (2018) encode each claim-evidence pair separately, and use a pooling function to aggregate features for prediction. Zhou et al. (2019) formulates claim verification as a graph reasoning task and propose a new model with graph neural networks. Liu et al. (2020) regards sentences as the nodes of a graph and uses Kernel Graph Attention"
2021.fever-1.6,W18-5520,0,0.0217201,"tively improve performance. We then provide a detailed error analysis for our system. In summary, we list our contributions as follows. • We propose to enhance fact verification models with external entity knowledge. • We design an entity knowledge encoder module and employ unidirectional attention to effectively incorporate entity descriptions. • Empirical results show that our approach achieves competitive performance on the FEVER dataset, and ablation study shows that incorporating entity knowledge is useful for fact verification. 51 focus on using entity information for fact verification. Taniguchi et al. (2018) first extract entities from the claim and propose to use a simple entity-linking system based on text match to retrieve evidence documents. Nooralahzadeh and Øvrelid (2018) select evidence documents by finding article titles which contain the entities and noun phrases of the claim. 2.2 challenge, we adopt existing approaches for document retrieval and sentence selection, while mainly focusing on the claim verification model. 4 In this section, we first formalize the fact verification problem in Section 4.1 and then introduce our model for incorporating entity knowledge in Section 4.2. Finally"
2021.fever-1.6,D18-1010,0,0.0122872,"e result is attended by the previous fact verification module. In the follows, we will first introduce the main fact verification module and then the entity knowledge encoder. 4.2.1 (4) (3) where Wc and bc are weight and bias. We adopt the cross entropy loss for claim truthfulness classification against the ground-truth label yc . Since the input evidence sentences are obtained by an upstream retrieval module, some of them may be irrelevant to the claim. Therefore, as an auxiliary training task, we also predict the relatedness of each evidence sentence, which has been shown to be effective in Yin and Roth (2018). 1 RoBERTa does not use segmentation embeddings in pretraining, but we found it is useful in finetuning. 53 Figure 1: Architecture of our fact verification system enhanced with entity knowledge. The left part is the fact verification module based on RoBERTa and the right part is the entity encoder based on distilled RoBERTa. The dotted green arrows indicate unidirectional attention mechanism which the fact verification module uses to access outputs from the entity encoder. The final loss of our model is the combination of claim loss and auxiliary evidence loss. Then, the fact verification mod"
2021.fever-1.6,W18-5515,0,0.0646998,"s to develop automatic fact verification systems to check the truthfulness of human-generated claims by extracted evidence from Wikipedia. Many systems were proposed for this challenging task. Nie et al. (2019) design a Neural Semantic Matching Network that takes the concatenation of all evidence sentences as input. They also propose a two-hop evidence enhancement process where they apply sentence selection twice to retrieve more related evidence sentences. Stammbach and Neumann (2019) propose a two-staged selection process with two different retrieval models for selecting evidence sentences. Yoneda et al. (2018) infer the veracity of each claim-evidence pair and make final prediction by aggregating multiple predicted labels. Hanselowski et al. (2018) encode each claim-evidence pair separately, and use a pooling function to aggregate features for prediction. Zhou et al. (2019) formulates claim verification as a graph reasoning task and propose a new model with graph neural networks. Liu et al. (2020) regards sentences as the nodes of a graph and uses Kernel Graph Attention Network (KGAT) to aggregate information. Zhong et al. (2020) further constructs a semantic-level graph for input claim and evidenc"
2021.fever-1.6,2020.acl-main.549,0,0.0928959,"ith two different retrieval models for selecting evidence sentences. Yoneda et al. (2018) infer the veracity of each claim-evidence pair and make final prediction by aggregating multiple predicted labels. Hanselowski et al. (2018) encode each claim-evidence pair separately, and use a pooling function to aggregate features for prediction. Zhou et al. (2019) formulates claim verification as a graph reasoning task and propose a new model with graph neural networks. Liu et al. (2020) regards sentences as the nodes of a graph and uses Kernel Graph Attention Network (KGAT) to aggregate information. Zhong et al. (2020) further constructs a semantic-level graph for input claim and evidence and perform reasoning over this graph with pretrained XLNet model (Yang et al., 2019). Similar to our work, some previous systems also Claim #2: Beastie Boys was formed in Australia. Gold Evidence: The Beastie Boys were an American hip hop group from New York City, formed in 1981. Entity Knowledge: New York City: New York City (NYC), often called simply New York, is the most populous city in the United States. Verdict: REFUTED Table 1: Two motivating examples for fact checking and the FEVER task. Identifying the truthfulne"
2021.fever-1.6,P19-1085,0,0.0619557,"catenation of all evidence sentences as input. They also propose a two-hop evidence enhancement process where they apply sentence selection twice to retrieve more related evidence sentences. Stammbach and Neumann (2019) propose a two-staged selection process with two different retrieval models for selecting evidence sentences. Yoneda et al. (2018) infer the veracity of each claim-evidence pair and make final prediction by aggregating multiple predicted labels. Hanselowski et al. (2018) encode each claim-evidence pair separately, and use a pooling function to aggregate features for prediction. Zhou et al. (2019) formulates claim verification as a graph reasoning task and propose a new model with graph neural networks. Liu et al. (2020) regards sentences as the nodes of a graph and uses Kernel Graph Attention Network (KGAT) to aggregate information. Zhong et al. (2020) further constructs a semantic-level graph for input claim and evidence and perform reasoning over this graph with pretrained XLNet model (Yang et al., 2019). Similar to our work, some previous systems also Claim #2: Beastie Boys was formed in Australia. Gold Evidence: The Beastie Boys were an American hip hop group from New York City, f"
2021.findings-acl.102,2020.coling-main.232,0,0.0226238,"oncepts in a knowledge graph, which can dynamically provide multi-hop relations between any pair of concepts. Several studies have utilized knowledge descriptions for different tasks. Yu et al. (2020a) uses description text from Wikipedia for knowledgetext co-pretraining. Xie et al. (2016) encodes the semantics of entity descriptions in knowledge graphs to improve the performance on knowledge graph completion and entity classification. Chen et al. (2018) co-trains the knowledge graph embeddings and entity description representation for cross-lingual entity alignment. Concurrent with our work, Chen et al. (2020) also insert knowledge descriptions into commonsense question answering. Compared with our work, the proposed method in Chen et al. (2020) is much more complex, e.g. involving training additional rankers on retrieved text, while our result outperforms Chen et al. on CommonsenseQA. 3 Method 3.1 Knowledge Retrieval Problem formulation. In this paper, we focus on the following QA task: given a commonsense question q, select the correct answer from several choices c1 , ..., cn . In most cases, the question does not contain any mentions of the answer. Therefore, external knowledge sources can be us"
2021.findings-acl.102,2020.emnlp-main.99,0,0.0430306,"2020). While massive pre-trained models (Devlin et al., 2018; Liu et al., 2019) are effective in language understanding, they lack modules to explicitly handle knowledge and commonsense. Also, structured data like knowledge graph is much more efficient in representing commonsense compared with unstructured text. Therefore, there have been multiple ∗ Equal contribution methods coupling language models with various forms of knowledge graphs (KG) for commonsense reasoning, including knowledge bases (Sap et al., 2019; Yu et al., 2020b), relational paths (Lin et al., 2019), graph relation network (Feng et al., 2020) and heterogeneous graph (Lv et al., 2020). These methods combine the merits of language modeling and structural knowledge information and improve the performance of commonsense reasoning and question answering. However, there is still a non-negligible gap between the performance of these models and humans. One reason is that, although a KG can encode topological information between the concepts, it lacks rich context information. For instance, for a graph node for the entity “Mona Lisa”, the graph depicts its relations to multiple other entities. But given this neighborhood information, it is"
2021.findings-acl.102,2021.ccl-1.108,0,0.0339687,"Missing"
2021.findings-acl.102,P19-1615,0,0.0224894,"Missing"
2021.findings-acl.102,D19-6003,0,0.0534514,"Missing"
2021.findings-acl.102,D18-1260,0,0.0258512,"is a parameter vector. The relevance score between the question and the choice is then s = softmax(v T b), where b ∈ Rd is a parameter vector and the softmax is computed over all choices for the cross-entropy loss function. The architecture of our model DEKCOR and the construction of input is shown in Fig. 1. 4 Experiments 4.1 Datasets and baselines 3.3 Reasoning 2 Table 1: Statistics of CommonsenseQA (CSQA) and OpenBookQA (OBQA). We evaluate our model on two benchmark datasets of multiple-choice questions for commonsense question answering: CommonsenseQA (Talmor et al., 2018) and OpenBookQA (Mihaylov et al., 2018). CommonsenseQA creates questions from ConceptNet entities and relations; OpenBookQA probes elementary science knowledge from a book of 1,326 facts. The statistics of the datasets is provided in Table 1. For OpenBookQA, we follow prior approaches (Wang et al., 2020) to append top 1203 Table 2: Accuracy on the test set of CommonsenseQA. Methods BERT+OMCS RoBERTa RoBERTa+HyKAS XLNet+DREAM RoBERTa+KE RoBERTa+KEDGN XLNet+GraphReason ALBERT RoBERTa+MHGRN ALBERT+PG-Full T5 ALBERT+KRD UnifiedQA ALBERT+KCR DEKCOR (ours) Single 62.5 72.1 73.2 73.3 75.3 75.4 75.6 78.1 78.4 79.1 79.5 80.7 Ensemble 72.5 7"
2021.findings-acl.102,2020.findings-emnlp.369,0,0.102067,"els. The resulting model achieves state-of-the-art result in the CommonsenseQA dataset and the best result among non-generative models in OpenBookQA. 1 Introduction One critical aspect of human intelligence is the ability to reason over everyday matters based on observation and knowledge. This capability is usually shared by most people as a foundation for communication and interaction with the world. Therefore, commonsense reasoning has emerged as an important task in natural language understanding, with various datasets and models proposed in this area (Ma et al., 2019; Talmor et al., 2018; Wang et al., 2020; Lv et al., 2020). While massive pre-trained models (Devlin et al., 2018; Liu et al., 2019) are effective in language understanding, they lack modules to explicitly handle knowledge and commonsense. Also, structured data like knowledge graph is much more efficient in representing commonsense compared with unstructured text. Therefore, there have been multiple ∗ Equal contribution methods coupling language models with various forms of knowledge graphs (KG) for commonsense reasoning, including knowledge bases (Sap et al., 2019; Yu et al., 2020b), relational paths (Lin et al., 2019), graph relat"
2021.findings-acl.160,W19-5206,0,0.0730147,"tagged AlterBT AlterBT-tagged NIST06 45.94 43.89 46.79 49.07+† 49.40+† NIST02 45.82 44.79 47.11 48.77+† 49.04+† NIST03 45.35 44.40 46.49 48.36+† 48.37+† NIST04 46.88 46.24 47.73 49.51+† 49.10+† NIST05 45.43 45.45 47.17 49.94+† 49.64+† NIST08 36.98 36.45 38.41 40.95+† 40.56+† All 44.40 43.57 45.47 47.68+† 47.49+† Table 1: BLEU scores on the NIST Chinese-English task with 10M additional synthetic corpus. “Base” means only authentic data is used. “BT” corresponds to the back-translation method (Sennrich et al., 2016a). “BT-tagged” corresponds to the tagged BT technique proposed by Caswell et al. (2019). “AlterBT” means alternated training on authentic data and synthetic data using “BT” in each alternation. “AlterBT-tagged” means alternated training on authentic data and synthetic data using “BT-tagged” in each alternation. “+” means significantly better than BT (p &lt; 0.01).“†” means significantly better than BT-tagged (p &lt; 0.01). Scale Base BT BT-tagged AlterBT AlterBT-tagged 1M 34.16 37.36 37.65 38.20+† 37.98+† 4.5M 34.16 36.30 37.42 38.53+† 39.19+† Table 2: BLEU scores on the IWSLT14 GermanEnglish task with 1M and 4.5M additional synthetic corpus. “+” means significantly better than BT (p"
2021.findings-acl.160,P16-1185,1,0.556489,"lps to direct the NMT model parameters towards points with higher BLEU scores and leads to consistent translation performance improvement. 1 Introduction While recent years have witnessed the rapid development of Neural Machine Translation (NMT) (Sutskever et al., 2014; Bahdanau et al., 2015; Gehring et al., 2017; Vaswani et al., 2017), it heavily relies on large-scale, high-quality bilingual corpora. Due to the expense and scarcity of authentic corpora, synthetic data has played a significant role in boosting translation quality (He et al., 2016; Sennrich et al., 2016a; Zhang and Zong, 2016; Cheng et al., 2016; Fadaee et al., 2017). Existing approaches to synthesizing data in NMT focus on leveraging monolingual data in the training process. Among them, back-translation (BT) (Sennrich et al., 2016a) has been widely used ∗ Corresponding author: Yang Liu to generate synthetic bilingual corpora by using a trained target-to-source NMT model to translate large-scale target-side monolingual corpora. Such synthetic data can be used to improve source-totarget NMT models. Despite the effectiveness of back-translation, the synthetic data inevitably contains noise and erroneous translations. As a matter of fac"
2021.findings-acl.160,D18-1045,0,0.0746995,"ely used ∗ Corresponding author: Yang Liu to generate synthetic bilingual corpora by using a trained target-to-source NMT model to translate large-scale target-side monolingual corpora. Such synthetic data can be used to improve source-totarget NMT models. Despite the effectiveness of back-translation, the synthetic data inevitably contains noise and erroneous translations. As a matter of fact, it has been widely observed that while BT is capable of benefiting NMT models by using relatively small-scale synthetic data, further increasing the quantity often deteriorates translation performance (Edunov et al., 2018; Wu et al., 2019; Caswell et al., 2019). This problem has attracted increasing attention in the NMT community (Edunov et al., 2018; Wang et al., 2019). One direction to alleviate the problem is to add noise or a special tag on the source side of synthetic data, which enables NMT models to distinguish between authentic and synthetic data (Edunov et al., 2018; Caswell et al., 2019). Another direction is to filter or evaluate the synthetic data by calculating confidence over corpora, making NMT models better exploit synthetic data (Imamura et al., 2018; Wang et al., 2019). While these methods ha"
2021.findings-acl.160,P17-2090,0,0.026019,"T model parameters towards points with higher BLEU scores and leads to consistent translation performance improvement. 1 Introduction While recent years have witnessed the rapid development of Neural Machine Translation (NMT) (Sutskever et al., 2014; Bahdanau et al., 2015; Gehring et al., 2017; Vaswani et al., 2017), it heavily relies on large-scale, high-quality bilingual corpora. Due to the expense and scarcity of authentic corpora, synthetic data has played a significant role in boosting translation quality (He et al., 2016; Sennrich et al., 2016a; Zhang and Zong, 2016; Cheng et al., 2016; Fadaee et al., 2017). Existing approaches to synthesizing data in NMT focus on leveraging monolingual data in the training process. Among them, back-translation (BT) (Sennrich et al., 2016a) has been widely used ∗ Corresponding author: Yang Liu to generate synthetic bilingual corpora by using a trained target-to-source NMT model to translate large-scale target-side monolingual corpora. Such synthetic data can be used to improve source-totarget NMT models. Despite the effectiveness of back-translation, the synthetic data inevitably contains noise and erroneous translations. As a matter of fact, it has been widely"
2021.findings-acl.160,W18-2703,0,0.0617932,"(t) and θˆa successively, which from θˆs to θˆa , θˆs finally leads to a better point with a higher BLEU score. enables authentic data to further redirect the model into a better point with a higher BLEU score. 4 Related Work Our work is based on back-translation (BT), an approach to leverage monolingual data by an additional target-to-source system. BT was proved to be effective in neural machine translation (NMT) systems (Sennrich et al., 2016a). Despite its effectiveness, BT is limited by the accuracy of synthetic data. Noise and translation errors hinder the boosting of model performance (Hoang et al., 2018). The 1831 negative results become more evident when more synthetic data is mixed into training data (Caswell et al., 2019; Wu et al., 2019). Considerable studies have focused on the accuracy problem in synthetic data and further extended back-translation. Imamura et al. (2018) demonstrate that generating source sentences via sampling increases the diversity of synthetic data and benefits the BT system. Edunov et al. (2018) further propose a noisy beam search method to generate more diversified source-side data. Caswell et al. (2019) add a reserved token to synthetic source-side sentences in o"
2021.findings-acl.160,W18-2707,0,0.0762232,"often deteriorates translation performance (Edunov et al., 2018; Wu et al., 2019; Caswell et al., 2019). This problem has attracted increasing attention in the NMT community (Edunov et al., 2018; Wang et al., 2019). One direction to alleviate the problem is to add noise or a special tag on the source side of synthetic data, which enables NMT models to distinguish between authentic and synthetic data (Edunov et al., 2018; Caswell et al., 2019). Another direction is to filter or evaluate the synthetic data by calculating confidence over corpora, making NMT models better exploit synthetic data (Imamura et al., 2018; Wang et al., 2019). While these methods have outperformed the conventional BT approach, NMT models still suffer from a performance degradation as the size of synthetic data keeps increasing. Hence, how to better take advantage of limited authentic data and abundant synthetic data still remains a grand challenge. In this work, we propose alternated training with synthetic and authentic data for neural machine translation. The basic idea is to alternate synthetic and authentic corpora iteratively during training. Compared with previous work, we introduce authentic data as guidance to prevent t"
2021.findings-acl.160,P07-2045,0,0.016436,"c bilingual corpus and 10M English-side sentences from WMT17 Chinese-English training set as our monolingual corpus for back-translation. NIST06 was used as the validation set. We use NIST02, 03, 04, 05 and 08 datasets as test sets. For the German-English task, we selected the dataset of IWSLT14 German-English task, which contains 16k parallel sentence pairs for training. We further extracted 4.5M English-side sentences from WMT14 German-English training set as monolingual dataset. We segmented Chinese sentences by THULAC (Sun et al., 2016) and tokenized English and German sentences by Moses (Koehn et al., 2007). The vocabulary was built by Byte Pair Encoding (BPE) (Sennrich et al., 2016b) with 32k merge operations. We used Transformer (Vaswani et al., 2017) implemented in THUMT (Tan et al., 2020) with standard hyperparameters as a base model. We used Adam optimizer (Kingma and Ba, 2015) with β1 = 0.9, β2 = 0.98 and  = 10−9 with the maximum learning rate = 7 × 10−4 . We applied early-stopping to verify convergence of each single S/A-step. If the validation BLEU failed ti exceed the highest score during the certain S/A-step after 10K training iterations, we consider the model converged and alternated"
2021.findings-acl.160,P16-1009,0,0.316106,"visualization, we find that authentic data helps to direct the NMT model parameters towards points with higher BLEU scores and leads to consistent translation performance improvement. 1 Introduction While recent years have witnessed the rapid development of Neural Machine Translation (NMT) (Sutskever et al., 2014; Bahdanau et al., 2015; Gehring et al., 2017; Vaswani et al., 2017), it heavily relies on large-scale, high-quality bilingual corpora. Due to the expense and scarcity of authentic corpora, synthetic data has played a significant role in boosting translation quality (He et al., 2016; Sennrich et al., 2016a; Zhang and Zong, 2016; Cheng et al., 2016; Fadaee et al., 2017). Existing approaches to synthesizing data in NMT focus on leveraging monolingual data in the training process. Among them, back-translation (BT) (Sennrich et al., 2016a) has been widely used ∗ Corresponding author: Yang Liu to generate synthetic bilingual corpora by using a trained target-to-source NMT model to translate large-scale target-side monolingual corpora. Such synthetic data can be used to improve source-totarget NMT models. Despite the effectiveness of back-translation, the synthetic data inevitably contains noise and"
2021.findings-acl.160,D19-1073,1,0.896517,"target-side monolingual corpora. Such synthetic data can be used to improve source-totarget NMT models. Despite the effectiveness of back-translation, the synthetic data inevitably contains noise and erroneous translations. As a matter of fact, it has been widely observed that while BT is capable of benefiting NMT models by using relatively small-scale synthetic data, further increasing the quantity often deteriorates translation performance (Edunov et al., 2018; Wu et al., 2019; Caswell et al., 2019). This problem has attracted increasing attention in the NMT community (Edunov et al., 2018; Wang et al., 2019). One direction to alleviate the problem is to add noise or a special tag on the source side of synthetic data, which enables NMT models to distinguish between authentic and synthetic data (Edunov et al., 2018; Caswell et al., 2019). Another direction is to filter or evaluate the synthetic data by calculating confidence over corpora, making NMT models better exploit synthetic data (Imamura et al., 2018; Wang et al., 2019). While these methods have outperformed the conventional BT approach, NMT models still suffer from a performance degradation as the size of synthetic data keeps increasing. He"
2021.findings-acl.160,D19-1430,0,0.0728883,"ing author: Yang Liu to generate synthetic bilingual corpora by using a trained target-to-source NMT model to translate large-scale target-side monolingual corpora. Such synthetic data can be used to improve source-totarget NMT models. Despite the effectiveness of back-translation, the synthetic data inevitably contains noise and erroneous translations. As a matter of fact, it has been widely observed that while BT is capable of benefiting NMT models by using relatively small-scale synthetic data, further increasing the quantity often deteriorates translation performance (Edunov et al., 2018; Wu et al., 2019; Caswell et al., 2019). This problem has attracted increasing attention in the NMT community (Edunov et al., 2018; Wang et al., 2019). One direction to alleviate the problem is to add noise or a special tag on the source side of synthetic data, which enables NMT models to distinguish between authentic and synthetic data (Edunov et al., 2018; Caswell et al., 2019). Another direction is to filter or evaluate the synthetic data by calculating confidence over corpora, making NMT models better exploit synthetic data (Imamura et al., 2018; Wang et al., 2019). While these methods have outperformed t"
2021.findings-acl.160,D16-1160,0,0.0201458,"that authentic data helps to direct the NMT model parameters towards points with higher BLEU scores and leads to consistent translation performance improvement. 1 Introduction While recent years have witnessed the rapid development of Neural Machine Translation (NMT) (Sutskever et al., 2014; Bahdanau et al., 2015; Gehring et al., 2017; Vaswani et al., 2017), it heavily relies on large-scale, high-quality bilingual corpora. Due to the expense and scarcity of authentic corpora, synthetic data has played a significant role in boosting translation quality (He et al., 2016; Sennrich et al., 2016a; Zhang and Zong, 2016; Cheng et al., 2016; Fadaee et al., 2017). Existing approaches to synthesizing data in NMT focus on leveraging monolingual data in the training process. Among them, back-translation (BT) (Sennrich et al., 2016a) has been widely used ∗ Corresponding author: Yang Liu to generate synthetic bilingual corpora by using a trained target-to-source NMT model to translate large-scale target-side monolingual corpora. Such synthetic data can be used to improve source-totarget NMT models. Despite the effectiveness of back-translation, the synthetic data inevitably contains noise and erroneous translations"
2021.findings-acl.160,P16-1162,0,0.687765,"visualization, we find that authentic data helps to direct the NMT model parameters towards points with higher BLEU scores and leads to consistent translation performance improvement. 1 Introduction While recent years have witnessed the rapid development of Neural Machine Translation (NMT) (Sutskever et al., 2014; Bahdanau et al., 2015; Gehring et al., 2017; Vaswani et al., 2017), it heavily relies on large-scale, high-quality bilingual corpora. Due to the expense and scarcity of authentic corpora, synthetic data has played a significant role in boosting translation quality (He et al., 2016; Sennrich et al., 2016a; Zhang and Zong, 2016; Cheng et al., 2016; Fadaee et al., 2017). Existing approaches to synthesizing data in NMT focus on leveraging monolingual data in the training process. Among them, back-translation (BT) (Sennrich et al., 2016a) has been widely used ∗ Corresponding author: Yang Liu to generate synthetic bilingual corpora by using a trained target-to-source NMT model to translate large-scale target-side monolingual corpora. Such synthetic data can be used to improve source-totarget NMT models. Despite the effectiveness of back-translation, the synthetic data inevitably contains noise and"
2021.findings-acl.160,2020.amta-research.11,1,0.433437,"03, 04, 05 and 08 datasets as test sets. For the German-English task, we selected the dataset of IWSLT14 German-English task, which contains 16k parallel sentence pairs for training. We further extracted 4.5M English-side sentences from WMT14 German-English training set as monolingual dataset. We segmented Chinese sentences by THULAC (Sun et al., 2016) and tokenized English and German sentences by Moses (Koehn et al., 2007). The vocabulary was built by Byte Pair Encoding (BPE) (Sennrich et al., 2016b) with 32k merge operations. We used Transformer (Vaswani et al., 2017) implemented in THUMT (Tan et al., 2020) with standard hyperparameters as a base model. We used Adam optimizer (Kingma and Ba, 2015) with β1 = 0.9, β2 = 0.98 and  = 10−9 with the maximum learning rate = 7 × 10−4 . We applied early-stopping to verify convergence of each single S/A-step. If the validation BLEU failed ti exceed the highest score during the certain S/A-step after 10K training iterations, we consider the model converged and alternated the training set. For the whole training process, we set the maximum training iterations as 250k for ChineseEnglish task and 150k for German-English task. 3.2 Results Figure 1 shows the co"
2021.findings-acl.269,W05-0909,0,0.108147,"ng At fine-tuning stage, we use trainable retriever to score sentences from candidate set Z and select top k sentence as additional context Z. Similar to pre-training, RE-T5 takes the concatenation of 3 Experiments Experiments Settings Dataset CommonGen is a benchmark dataset designed to diagnose whether a model has the ability of generative commonsense reasoning (Lin et al., 2020). This dataset contains 32,651/993/1,497 concept sets for training/development/test, and the numbers of corresponding sentences are 67,389/4,018/7,644. We use BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), METEOR (Banerjee and Lavie, 2005), CIDEr (Vedantam et al., 2015) and SPICE (Anderson et al., 2016) as evaluation metrics. Because SPICE correlates the most with human evaluation (Lin et al., 2020), we take SPICE as the primary metric. External Corpora To be consistent with the distribution of the CommonGen dataset, we use VATEX (Wang et al., 2019), Activity (Krishna et al., 2017), SNLI (Bowman et al., 2015) and MNLI (Williams et al., 2018) as external corpora. We sample 500k sentences from these corpora to construct our pre-training dataset. Meanwhile, these datasets are also used as our sentence pool for the retrieval module"
2021.findings-acl.269,D15-1075,0,0.0465705,"0). This dataset contains 32,651/993/1,497 concept sets for training/development/test, and the numbers of corresponding sentences are 67,389/4,018/7,644. We use BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), METEOR (Banerjee and Lavie, 2005), CIDEr (Vedantam et al., 2015) and SPICE (Anderson et al., 2016) as evaluation metrics. Because SPICE correlates the most with human evaluation (Lin et al., 2020), we take SPICE as the primary metric. External Corpora To be consistent with the distribution of the CommonGen dataset, we use VATEX (Wang et al., 2019), Activity (Krishna et al., 2017), SNLI (Bowman et al., 2015) and MNLI (Williams et al., 2018) as external corpora. We sample 500k sentences from these corpora to construct our pre-training dataset. Meanwhile, these datasets are also used as our sentence pool for the retrieval module. For both the pre-training and fine-tuning, all sentences that appear in the CommonGen targets are not used as retrieval sentences candidates. Baselines We compare RE-T5 with several baseline systems. GPT-2, BERT-Gen, UniLM, BART, and T5 are pre-trained language models tested in 2 https://inklab.usc.edu/CommonGen/ leaderboard.html 3058 Concept Set: trailer shirt side sit ro"
2021.findings-acl.269,N19-1423,0,0.0164203,"ver in this pre-training stage. Formally, given a concept set X = {x1 , x2 , . . . , xn }, where xi represents the ith concept and n is the number of concepts, our goal is to generate a natural language output of tokens Y = {y1 , y2 , . . . , ym }, which describes a common scenario in our daily life, using all given concepts in X. Trainable Retriever In order to retrieve more useful sentences from the sentence candidate set, we design a trainable retriever, which predicts scores to rank these candidates, and then select top-k sentences as additional context. The scorer is built based on BERT (Devlin et al., 2019), a pre-trained language model usually used for language understanding. Given a concept set X and a candidate sentence zi , our trainable retriever first concatenate them into a text input: [CLS]X[SEP]zi [SEP] where [CLS] and [SEP] are special symbols in BERT. We pass this into BERT, which generates an output vector for each input token. We take the output vector corresponding to [CLS] which is used as the aggregated representation of the input sequence (denoted c ) into a linear layer with sigmoid activation to obtain the binary classification output yc . yc = σ(W c c + bc ) (1) where W c is"
2021.findings-acl.269,2020.acl-main.703,0,0.407612,"on to obtain the binary classification output yc . yc = σ(W c c + bc ) (1) where W c is a projection matrix and bc is a bias. To train this retriever, for each concept set in CommonGen training set, we use its paired sentence as a positive example and we randomly sample another sentence, also from the training set, as a negative example. Then, we adopt cross entropy loss for this binary classification. The top-k scored sentences with the highest scores will be selected as the auxiliary input Z. 3057 Model GPT-2 (Radford et al., 2019) BERT-Gen (Bao et al., 2020) UniLM (Dong et al., 2019) BART (Lewis et al., 2020) T5-base (Raffel et al., 2020) T5-large (Raffel et al., 2020) EKI-BART (Fan et al., 2020) KG-BART (Liu et al., 2021) CALM(T5-base) (Zhou et al., 2021) RE-T5 (ours) BLEU-4 26.833 23.468 30.616 31.827 18.546 31.962 35.945 33.867 40.863 CIDEr 12.187 12.606 14.889 13.976 9.399 15.128 16.999 16.927 17.663 SPICE 23.567 24.822 27.429 27.995 19.871 28.855 29.583 29.634 31.079 SPICE(v1.0) 25.90 27.30 30.20 30.60 22.00 31.60 32.40 32.70 33.00 34.30 Table 2: Test results on CommonGen benchmark. All results except CALM are based on the latest human references(v1.1). v1.0 indicates evaluation with old eval"
2021.findings-acl.269,N19-1421,0,0.0198534,"th rainbow across the lake. Several canoes parked in the grass on the shore of a lake. Table 1: Two concept sets and their gold corresponding sentences from CommonGen dataset. Introduction The understanding of commonsense knowledge in human language has been acknowledged as a critical component for artificial intelligence systems. In recent years, many new tasks and datasets are proposed to assess NLP model’s ability of commonsense reasoning (Yu et al., 2020). SWAG (Zellers et al., 2018) is a task of inferring the upcoming event based on a partial description using commonsense. CommonsenseQA (Talmor et al., 2019) is a commonsense question answering dataset built from ConceptNet. Recently, Lin et al. (2020) propose CommonGen, a new challenge for evaluating model’s ability of generative commonsense reasoning. CommonGen requires the system to construct a plausible sentence based on several concepts related to an everyday scenario. Two examples for ∗ Work done during internship at Microsoft. The code and data are available at https://github. com/HanNight/RE-T5 1 this task are shown in Table 1. The task is challenging because the system needs to organize provided concepts into the most plausible scenario,"
2021.findings-acl.269,2020.findings-emnlp.165,0,0.49875,"o concept sets and their gold corresponding sentences from CommonGen dataset. Introduction The understanding of commonsense knowledge in human language has been acknowledged as a critical component for artificial intelligence systems. In recent years, many new tasks and datasets are proposed to assess NLP model’s ability of commonsense reasoning (Yu et al., 2020). SWAG (Zellers et al., 2018) is a task of inferring the upcoming event based on a partial description using commonsense. CommonsenseQA (Talmor et al., 2019) is a commonsense question answering dataset built from ConceptNet. Recently, Lin et al. (2020) propose CommonGen, a new challenge for evaluating model’s ability of generative commonsense reasoning. CommonGen requires the system to construct a plausible sentence based on several concepts related to an everyday scenario. Two examples for ∗ Work done during internship at Microsoft. The code and data are available at https://github. com/HanNight/RE-T5 1 this task are shown in Table 1. The task is challenging because the system needs to organize provided concepts into the most plausible scenario, avoid violation of commonsense, and ensure the generated sentence is grammatically correct. Exi"
2021.findings-acl.269,W04-1013,0,0.0572427,"utput. 2.3 Fine-tuning At fine-tuning stage, we use trainable retriever to score sentences from candidate set Z and select top k sentence as additional context Z. Similar to pre-training, RE-T5 takes the concatenation of 3 Experiments Experiments Settings Dataset CommonGen is a benchmark dataset designed to diagnose whether a model has the ability of generative commonsense reasoning (Lin et al., 2020). This dataset contains 32,651/993/1,497 concept sets for training/development/test, and the numbers of corresponding sentences are 67,389/4,018/7,644. We use BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), METEOR (Banerjee and Lavie, 2005), CIDEr (Vedantam et al., 2015) and SPICE (Anderson et al., 2016) as evaluation metrics. Because SPICE correlates the most with human evaluation (Lin et al., 2020), we take SPICE as the primary metric. External Corpora To be consistent with the distribution of the CommonGen dataset, we use VATEX (Wang et al., 2019), Activity (Krishna et al., 2017), SNLI (Bowman et al., 2015) and MNLI (Williams et al., 2018) as external corpora. We sample 500k sentences from these corpora to construct our pre-training dataset. Meanwhile, these datasets are also used as our sen"
2021.findings-acl.269,P02-1040,0,0.113717,"and the original sentence as output. 2.3 Fine-tuning At fine-tuning stage, we use trainable retriever to score sentences from candidate set Z and select top k sentence as additional context Z. Similar to pre-training, RE-T5 takes the concatenation of 3 Experiments Experiments Settings Dataset CommonGen is a benchmark dataset designed to diagnose whether a model has the ability of generative commonsense reasoning (Lin et al., 2020). This dataset contains 32,651/993/1,497 concept sets for training/development/test, and the numbers of corresponding sentences are 67,389/4,018/7,644. We use BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), METEOR (Banerjee and Lavie, 2005), CIDEr (Vedantam et al., 2015) and SPICE (Anderson et al., 2016) as evaluation metrics. Because SPICE correlates the most with human evaluation (Lin et al., 2020), we take SPICE as the primary metric. External Corpora To be consistent with the distribution of the CommonGen dataset, we use VATEX (Wang et al., 2019), Activity (Krishna et al., 2017), SNLI (Bowman et al., 2015) and MNLI (Williams et al., 2018) as external corpora. We sample 500k sentences from these corpora to construct our pre-training dataset. Meanwhile, these datasets are a"
2021.findings-acl.269,D19-1250,0,0.0463338,"Missing"
2021.findings-acl.269,N18-1101,0,0.0297806,"1/993/1,497 concept sets for training/development/test, and the numbers of corresponding sentences are 67,389/4,018/7,644. We use BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), METEOR (Banerjee and Lavie, 2005), CIDEr (Vedantam et al., 2015) and SPICE (Anderson et al., 2016) as evaluation metrics. Because SPICE correlates the most with human evaluation (Lin et al., 2020), we take SPICE as the primary metric. External Corpora To be consistent with the distribution of the CommonGen dataset, we use VATEX (Wang et al., 2019), Activity (Krishna et al., 2017), SNLI (Bowman et al., 2015) and MNLI (Williams et al., 2018) as external corpora. We sample 500k sentences from these corpora to construct our pre-training dataset. Meanwhile, these datasets are also used as our sentence pool for the retrieval module. For both the pre-training and fine-tuning, all sentences that appear in the CommonGen targets are not used as retrieval sentences candidates. Baselines We compare RE-T5 with several baseline systems. GPT-2, BERT-Gen, UniLM, BART, and T5 are pre-trained language models tested in 2 https://inklab.usc.edu/CommonGen/ leaderboard.html 3058 Concept Set: trailer shirt side sit road T5: A man sits on the side of"
2021.findings-acl.269,D18-1009,0,0.0225585,"tate-of-the-art results.1 1 Concept Set #2: lake, shore, canoe Gold Target Sentences: Canoe on a shore of lake. Canoe on shore with rainbow across the lake. Several canoes parked in the grass on the shore of a lake. Table 1: Two concept sets and their gold corresponding sentences from CommonGen dataset. Introduction The understanding of commonsense knowledge in human language has been acknowledged as a critical component for artificial intelligence systems. In recent years, many new tasks and datasets are proposed to assess NLP model’s ability of commonsense reasoning (Yu et al., 2020). SWAG (Zellers et al., 2018) is a task of inferring the upcoming event based on a partial description using commonsense. CommonsenseQA (Talmor et al., 2019) is a commonsense question answering dataset built from ConceptNet. Recently, Lin et al. (2020) propose CommonGen, a new challenge for evaluating model’s ability of generative commonsense reasoning. CommonGen requires the system to construct a plausible sentence based on several concepts related to an everyday scenario. Two examples for ∗ Work done during internship at Microsoft. The code and data are available at https://github. com/HanNight/RE-T5 1 this task are sho"
2021.findings-acl.422,W19-5206,0,0.108278,"e bias problem through explicitly distinguishing between the source- and target-original training data, which consistently improve the performance over strong baselines on six WMT20 translation tasks. Complementary to the translationese effect, language coverage bias provides another explanation for the performance drop caused by back-translation (Marie et al., 2020). We also apply our approach to both back- and forward-translation and find that mitigating the language coverage bias can improve the performance of both the two representative data augmentation methods and their tagged variants (Caswell et al., 2019). company best family back policeAmerican work show US well later children country men women decision officer meeting Abstract game {wangshuo2018, zxtan, sms, liuyang2011}@tsinghua.edu.cn &gt; {zptu, shumingshi}@tencent.com despite ? next used (b) German-Original Figure 1: Example of language coverage bias illustrated by word clouds that are plotted at the English side of sentence pairs in the En-De test sets from WMT10 to WMT18. The test sets consist of Englishoriginal and German-original sentence pairs. Introduction In recent years, there has been a growing interest in investigating the effect"
2021.findings-acl.422,C18-1111,0,0.0181722,"performance drop of backtranslation, as well as the different performances between tagged forward-translation and tagged back-translation (Caswell et al., 2019). In addition, we show that our approach is also beneficial for data augmentation approaches, which can further improve the translation performance over both back-translation and forward-translation. 5.3 Domain Adaptation Since high-quality and domain-specific parallel data is usually scarce or even unavailable, domain adaptation approaches are generally employed for translation in low-resource domains by leveraging out-of-domain data (Chu and Wang, 2018). Languages can be also regarded as different domains, since articles in different languages cover different topics (Bogoychev and Sennrich, 2019). Starting from this intuition, we distinguish examples with different original languages with tagging (Aharoni et al., 2019) and fine-tuning (Luong and Manning, 2015), which are commonly-used in domain adaptation and multi-lingual translation tasks. Our work also benefits domain adaptation: distinguishing original languages in general domain 4785 data consistently improves translation performance of NMT models in several specific domains (Table 16 i"
2021.findings-acl.422,D18-1045,0,0.0469535,"this section, we aim to provide some insights where monolingual data augmentation improves translation performance, and investigate whether our approach can further improve model performance in this scenario that potentially suffers more from the language coverage bias problem. For fair comparison across language pairs, we augment NMT models with the same English monolingual corpus as described in Section 2. We down-sample the large-scale monolingual corpus to the same amount as that of the bilingual corpus in each language pair, in order to rule out the effect of the scale of synthetic data (Edunov et al., 2018; Fadaee and Monz, 2018). We use backtranslation (Sennrich et al., 2016a) to augment the English monolingual data for the task of translating from another language to English (“X⇒En”), and use forward-translation for the task in the opposite translation direction (“En⇒X”). Table 10 lists the results, where several observations can be made. Explaining Data Augmentation with Language Coverage Bias Concerning the monolingual data augmentation methods (Rows 3-4), the vanilla 4783 En-Zh Method En-Ja En-De Average ⇒ ⇐ ⇒ ⇐ ⇒ ⇐ WMT20 Systems Shi et al. (2020) Zhang et al. (2020) Molchanov (2020) 38.6"
2021.findings-acl.422,N19-1388,0,0.0767514,"validation sets. the monolingual data augmentation scenario (Section 4.2), where the language coverage bias problem is more severe due to the newly introduced dataset in source or target language. 4.1 Bilingual Data Utilization In this section, we aim to improve bilingual data utilization through explicitly distinguishing between the source- and target-original training data. Methodology We distinguish original languages with two simple and effective methods: • Bias-Tagging: Tagging is a commonly-used approach to distinguishing between different types of examples, such as different languages (Aharoni et al., 2019; Riley et al., 2020) and synthetic vs authentic examples (Caswell et al., 2019). In this work, we attach a special tag to the source side of each target-original example, which enables NMT models to distinguish it from the source-original ones in training. • Fine-Tuning: Fine-tuning (Luong and Manning, 2015) is a useful method to help knowledge transmit among data from different distributions. We pre-train NMT models on the full training data that consists of both the source- and targetoriginal data, and then fine-tune them on only the source-original data. For fair comparison, the total trai"
2021.findings-acl.422,P07-2045,0,0.0080886,"Missing"
2021.findings-acl.422,2021.eacl-main.130,0,0.0136043,"(2020) show that the source-side translationese texts can potentially lead to distortions in automatic and human evaluations. Accordingly, the WMT competition starts to use only source-original test sets for most translation directions since 2019. Our study reconfirms the necessity of distinguishing the source- and target-original examples and takes one step further to distinguish examples in training data. Complementary to previous works, we investigate the effect of language coverage bias on machine translation, which is related to the content bias rather than the language style difference. Shen et al. (2021) also reveal the context mismatch between texts from different original languages. To alleviate this problem, they proposed to combine back- and forward-translation by introducing additional monolingual data, while we focus on better exploiting bilingual data by distinguishing the original languages, which is also helpful for back- and forward-translation. Lembersky et al. (2011, 2012) propose to adapt machine translation systems to generate texts that are more similar to human-translations, while Riley et al. (2020) propose to model human-translated texts and original texts as separate langua"
2021.findings-acl.422,2020.wmt-1.30,0,0.0343531,"Missing"
2021.findings-acl.422,D11-1034,0,0.080019,"y crabs are the most well-known image spokesmen of Bacheng. Both It is the best-known icon of Bacheng. Table 5: An example of the outputs of NMT models trained on different sets of data. Using the targetoriginal data tends to omit content words. naturally arises: can target-original bilingual data improve the fluency of NMT models? To answer the above question, we measure the fluency of outputs with language models trained on the monolingual data as described in Section 2. Previous study finds that different perplexities could be caused by specific contents rather than structural differences (Lembersky et al., 2011). Specifically, some source-original contents are of low frequency in the target-language monolingual data (e.g., “Bacheng” in Table 5), thus the language model trained on the target-language monolingual data tends to assign higher perplexities to outputs containing more source-original content words. To rule out this possibility and check whether the outputs are structurally different, we follow Lembersky et al. (2011) to abstract away from the contentspecific features of the outputs to measure their fluency at the syntactic level. Table 6 shows the results. Although using only the source-ori"
2021.findings-acl.422,W19-6627,0,0.115311,"t the English side of sentence pairs in the En-De test sets from WMT10 to WMT18. The test sets consist of Englishoriginal and German-original sentence pairs. Introduction In recent years, there has been a growing interest in investigating the effect of original languages in parallel data on neural machine translation (Barrault et al., 2020; Edunov et al., 2020; Marie et al., 2020). Several studies have shown that targetoriginal test examples1 can lead to distortions in automatic and human evaluations, which should be omitted from machine translation test sets (Barrault et al., 2019; Zhang and Toral, 2019; Graham 1 Target-original test examples are sentence pairs that are translated from the target language into the source language. et al., 2020). Another branch of studies report that target-original test data leads to discrepant conclusions: back-translation only benefits the translation of target-original test data while harms that of source-original test data (Edunov et al., 2020; Marie et al., 2020). They attribute these phenomena to the reason that human-translated texts (i.e., translationese) exhibit formal and stylistic differences that set them apart from the texts originally written i"
2021.findings-acl.422,E12-1026,0,0.0480962,"Missing"
2021.findings-acl.422,2015.iwslt-evaluation.11,0,0.170579,"ly distinguishing between the source- and target-original training data. Methodology We distinguish original languages with two simple and effective methods: • Bias-Tagging: Tagging is a commonly-used approach to distinguishing between different types of examples, such as different languages (Aharoni et al., 2019; Riley et al., 2020) and synthetic vs authentic examples (Caswell et al., 2019). In this work, we attach a special tag to the source side of each target-original example, which enables NMT models to distinguish it from the source-original ones in training. • Fine-Tuning: Fine-tuning (Luong and Manning, 2015) is a useful method to help knowledge transmit among data from different distributions. We pre-train NMT models on the full training data that consists of both the source- and targetoriginal data, and then fine-tune them on only the source-original data. For fair comparison, the total training steps of the pre-training and fine-tuning stages are the same as the baseline. Analysis Recent studies have shown that generating human-translation like texts as opposed to original texts can improve the BLEU score (Riley et al., 2020). To validate that the improvement is partially from alleviating the c"
2021.findings-acl.422,2020.acl-main.532,0,0.193192,"the training data, and find that using only the source-original data achieves comparable performance with using full training data. Based on these observations, we further propose two simple and effective approaches to alleviate the language coverage bias problem through explicitly distinguishing between the source- and target-original training data, which consistently improve the performance over strong baselines on six WMT20 translation tasks. Complementary to the translationese effect, language coverage bias provides another explanation for the performance drop caused by back-translation (Marie et al., 2020). We also apply our approach to both back- and forward-translation and find that mitigating the language coverage bias can improve the performance of both the two representative data augmentation methods and their tagged variants (Caswell et al., 2019). company best family back policeAmerican work show US well later children country men women decision officer meeting Abstract game {wangshuo2018, zxtan, sms, liuyang2011}@tsinghua.edu.cn &gt; {zptu, shumingshi}@tencent.com despite ? next used (b) German-Original Figure 1: Example of language coverage bias illustrated by word clouds that are plotted"
2021.findings-acl.422,W19-5208,0,0.392807,"plotted at the English side of sentence pairs in the En-De test sets from WMT10 to WMT18. The test sets consist of Englishoriginal and German-original sentence pairs. Introduction In recent years, there has been a growing interest in investigating the effect of original languages in parallel data on neural machine translation (Barrault et al., 2020; Edunov et al., 2020; Marie et al., 2020). Several studies have shown that targetoriginal test examples1 can lead to distortions in automatic and human evaluations, which should be omitted from machine translation test sets (Barrault et al., 2019; Zhang and Toral, 2019; Graham 1 Target-original test examples are sentence pairs that are translated from the target language into the source language. et al., 2020). Another branch of studies report that target-original test data leads to discrepant conclusions: back-translation only benefits the translation of target-original test data while harms that of source-original test data (Edunov et al., 2020; Marie et al., 2020). They attribute these phenomena to the reason that human-translated texts (i.e., translationese) exhibit formal and stylistic differences that set them apart from the texts originally written i"
2021.findings-acl.422,2020.wmt-1.25,0,0.0648905,"Missing"
2021.findings-acl.422,N19-4007,0,0.0685171,"ly, if the language coverage bias exists, the vocabulary distributions of the source- and target-original data should differ greatly from each other, since the covered issues tend to have different frequencies between them (D’Alessio and Allen, 2000). We use the Jensen-Shannon (JS) divergence (Lin, 1991) to measure the difference between two vocabulary En⇒Zh En⇐Zh Origin noun verb adj noun verb adj Target Source Both 67.6 69.7 69.9 52.0 54.0 54.1 64.3 66.2 65.9 53.8 61.8 61.2 38.0 44.1 43.8 57.0 63.9 63.4 Table 4: Translation adequacy of different types of content words measured by F-measure (Neubig et al., 2019). The results are reported on the validation sets. distributions p and q:   p+q p+q 1 KL(p|| ) + KL(q|| ) , JS (p||q) = 2 2 2 where KL(·||·) is the KL divergence (Kullback and Leibler, 1951) of two distributions. Table 2 shows the JS divergence of the vocabulary distributions between the source- and targetoriginal data. We also divide the words into content words and functions words based on their POS tags, since content words are more related to the language coverage bias, while the function words are more related to the stylistic and structural differences between the translationese and or"
2021.findings-acl.422,W19-5333,0,0.0130493,"each direction. We used newstest2020 as the test sets for all the six tasks. We reported the Sacre BLEU (Post, 2018), as recommended by WMT20. Model We used the Transformer-Big (Vaswani et al., 2017) model, which consists of a 6-layer encoder and a 6-layer decoder, and the hidden size is 1024. Recent studies showed that training on large batches can further boost model performance (Ott et al., 2018; Wu et al., 2018). Accordingly, we followed their settings to train models with batches of approximately 460k tokens. Please refer to the Appendix for more details about model training. We followed Ng et al. (2019) to use the Transformer-Big decoder as our language models, which are used to detect the original language and measure translation fluency. Language models are also trained with large batches (Ott et al., 2018). 3 Observing Language Coverage Bias In this study, we first establish the existence of language coverage bias (Section 3.2), and show how the bias affects NMT performance (Section 3.3). To this end, we propose an automatic method to detect the original language of each training example (Section 3.1), which is often not available in large-scale parallel corpora (Riley et al., 2020). 3.1"
2021.findings-acl.422,W18-6301,0,0.0117812,". For En⇔De and En⇔Zh, we used newstest2019 as the validation sets. For En⇔Ja, we split the official validation set released by WMT20 into two parts by the original language and only used the corresponding part for each direction. We used newstest2020 as the test sets for all the six tasks. We reported the Sacre BLEU (Post, 2018), as recommended by WMT20. Model We used the Transformer-Big (Vaswani et al., 2017) model, which consists of a 6-layer encoder and a 6-layer decoder, and the hidden size is 1024. Recent studies showed that training on large batches can further boost model performance (Ott et al., 2018; Wu et al., 2018). Accordingly, we followed their settings to train models with batches of approximately 460k tokens. Please refer to the Appendix for more details about model training. We followed Ng et al. (2019) to use the Transformer-Big decoder as our language models, which are used to detect the original language and measure translation fluency. Language models are also trained with large batches (Ott et al., 2018). 3 Observing Language Coverage Bias In this study, we first establish the existence of language coverage bias (Section 3.2), and show how the bias affects NMT performance (Se"
2021.findings-acl.422,W18-6319,0,0.0201539,"nce pairs for En⇔De, En⇔Zh, and En⇔Ja, respectively. We used the monolingual data that is publicly available in WMT20 to train the proposed original language detection model (Section 3.1) and data augmentation (Section 4.2). The Appendix lists details about the data preprocessing. For En⇔De and En⇔Zh, we used newstest2019 as the validation sets. For En⇔Ja, we split the official validation set released by WMT20 into two parts by the original language and only used the corresponding part for each direction. We used newstest2020 as the test sets for all the six tasks. We reported the Sacre BLEU (Post, 2018), as recommended by WMT20. Model We used the Transformer-Big (Vaswani et al., 2017) model, which consists of a 6-layer encoder and a 6-layer decoder, and the hidden size is 1024. Recent studies showed that training on large batches can further boost model performance (Ott et al., 2018; Wu et al., 2018). Accordingly, we followed their settings to train models with batches of approximately 460k tokens. Please refer to the Appendix for more details about model training. We followed Ng et al. (2019) to use the Transformer-Big decoder as our language models, which are used to detect the original la"
2021.findings-acl.422,2020.findings-emnlp.276,0,0.057633,"Missing"
2021.findings-acl.422,2020.acl-main.691,0,0.33119,"followed Ng et al. (2019) to use the Transformer-Big decoder as our language models, which are used to detect the original language and measure translation fluency. Language models are also trained with large batches (Ott et al., 2018). 3 Observing Language Coverage Bias In this study, we first establish the existence of language coverage bias (Section 3.2), and show how the bias affects NMT performance (Section 3.3). To this end, we propose an automatic method to detect the original language of each training example (Section 3.1), which is often not available in large-scale parallel corpora (Riley et al., 2020). 3.1 Detecting Original Languages Detection Method Intuitively, we use a largescale monolingual dataset to estimate the distribution of the contents covered by each language. For each training example, we compare its similarities 4779 WMT20 WMT20 En=&gt;Zh En=&gt;Zh En-Zh En-Ja En-De FT Ours 83.6 84.4 83.7 91.5 86.6 88.7 WMT20 WMT20 Zh=&gt;En Zh=&gt;En 37 37 28 28 36.5 36.5 32 32 Table 1: F1 scores of detecting original languages in the test sets. “FT” denotes the forward translation classifier proposed by Riley et al. (2020). 33.2 33.2 31.0 31.0 P (Ds )P (hx, yi|Ds ) , P (hx, yi) P (Dt )P (hx, yi|Dt ) P"
2021.findings-acl.422,P16-1009,0,0.0613289,"augmentation improves translation performance, and investigate whether our approach can further improve model performance in this scenario that potentially suffers more from the language coverage bias problem. For fair comparison across language pairs, we augment NMT models with the same English monolingual corpus as described in Section 2. We down-sample the large-scale monolingual corpus to the same amount as that of the bilingual corpus in each language pair, in order to rule out the effect of the scale of synthetic data (Edunov et al., 2018; Fadaee and Monz, 2018). We use backtranslation (Sennrich et al., 2016a) to augment the English monolingual data for the task of translating from another language to English (“X⇒En”), and use forward-translation for the task in the opposite translation direction (“En⇒X”). Table 10 lists the results, where several observations can be made. Explaining Data Augmentation with Language Coverage Bias Concerning the monolingual data augmentation methods (Rows 3-4), the vanilla 4783 En-Zh Method En-Ja En-De Average ⇒ ⇐ ⇒ ⇐ ⇒ ⇐ WMT20 Systems Shi et al. (2020) Zhang et al. (2020) Molchanov (2020) 38.6 40.8 - 28.8 - 34.8 - 20.4 - 31.9 39.6 - Our Implemented Systems Baselin"
2021.findings-acl.422,P16-1162,0,0.0390796,"augmentation improves translation performance, and investigate whether our approach can further improve model performance in this scenario that potentially suffers more from the language coverage bias problem. For fair comparison across language pairs, we augment NMT models with the same English monolingual corpus as described in Section 2. We down-sample the large-scale monolingual corpus to the same amount as that of the bilingual corpus in each language pair, in order to rule out the effect of the scale of synthetic data (Edunov et al., 2018; Fadaee and Monz, 2018). We use backtranslation (Sennrich et al., 2016a) to augment the English monolingual data for the task of translating from another language to English (“X⇒En”), and use forward-translation for the task in the opposite translation direction (“En⇒X”). Table 10 lists the results, where several observations can be made. Explaining Data Augmentation with Language Coverage Bias Concerning the monolingual data augmentation methods (Rows 3-4), the vanilla 4783 En-Zh Method En-Ja En-De Average ⇒ ⇐ ⇒ ⇐ ⇒ ⇐ WMT20 Systems Shi et al. (2020) Zhang et al. (2020) Molchanov (2020) 38.6 40.8 - 28.8 - 34.8 - 20.4 - 31.9 39.6 - Our Implemented Systems Baselin"
2021.findings-acl.449,2020.acl-main.463,0,0.0179136,"rization is the task of automatically generating a concise, salient, coherent and fluent summary of a given set of documents (Radev et al., 2002). Thanks to the advance in neural network models and the availability of large-scale labeled datasets, recent research has achieved promising progress on summarizing monologic texts such as news articles (Paulus et al., 2018; Gehrmann et al., 2018; Liu and Lapata, 2019; Liu et al., 2020), patents (Pilault et al., 2020) and academic papers (Koncel-Kedziorski et al., 2019). However, dialogue, as an important channel for achieving communicative intents (Bender and Koller, 2020), has received significantly less attention from the summarization research community. One main reason is the paucity of a suitable summarization dataset built on dialogue texts. Most existing research uses the AMI meeting corpus (Carletta et al., 2005), which consists of 137 dialogues obtained from virtual multi-party meeting recordings. However, research on the corpus is limited to its Summary from SAMSum: Ryan is in Italy while Leo is working hard and wishing he could win the lottery. Figure 1: An example from D IALOG S UM dataset compared with an example from SAMSum dataset. small scale. S"
2021.findings-acl.449,D18-1547,0,0.0330492,"Missing"
2021.findings-acl.449,2020.emnlp-main.336,0,0.0905716,"ey important relations between main events, and identifying discourse relations and using proper phrases to express them in summaries can be challenging for summarization systems (Xu et al., 2020). Take Figure 1 (a) for example, the human annotated summary connects two main events (underlined) using “since” to express their causal relation explicitly. However, the causal relation between those two events are not explicitly expressed in the dialogue, and the distance between them is long. Multiple turns usually correspond to more complicated discourse structure and relation. Also, similar with Chen and Yang (2020), we find that model performance decreases when the number of dialogue turns grows (See Appendix B). To better evaluate model ability to disambiguate discourse relations in D IALOG S UM, we first collect discourse connectives from Penn Discourse Treebank (Miltsakaki et al., 2004), and check whether these connectives are included in summaries in the testset. If the three reference summaries of a dialogue all contain connectives, we assume that the dialogues have strong discourse signals. We choose 70 dialogues from D IALOG S UM in this way. We then ask linguists who specialize in discourse to e"
2021.findings-acl.449,2020.acl-main.130,1,0.893686,"Missing"
2021.findings-acl.449,D18-1443,0,0.0674089,"))))))) Leo: Yeah. They seem nice. (‘A`) Ryan: That&apos;s all???? I need more reactions!!!!!!!!!! Leo: I&apos;m tied to this office and working like a slave. AM I SUPPOSED TO SAY ""I AM SO JEALOUS!!!!!!!!""?! ! ! … Introduction Text summarization is the task of automatically generating a concise, salient, coherent and fluent summary of a given set of documents (Radev et al., 2002). Thanks to the advance in neural network models and the availability of large-scale labeled datasets, recent research has achieved promising progress on summarizing monologic texts such as news articles (Paulus et al., 2018; Gehrmann et al., 2018; Liu and Lapata, 2019; Liu et al., 2020), patents (Pilault et al., 2020) and academic papers (Koncel-Kedziorski et al., 2019). However, dialogue, as an important channel for achieving communicative intents (Bender and Koller, 2020), has received significantly less attention from the summarization research community. One main reason is the paucity of a suitable summarization dataset built on dialogue texts. Most existing research uses the AMI meeting corpus (Carletta et al., 2005), which consists of 137 dialogues obtained from virtual multi-party meeting recordings. However, research on the co"
2021.findings-acl.449,D19-5409,0,0.264245,"eceived significantly less attention from the summarization research community. One main reason is the paucity of a suitable summarization dataset built on dialogue texts. Most existing research uses the AMI meeting corpus (Carletta et al., 2005), which consists of 137 dialogues obtained from virtual multi-party meeting recordings. However, research on the corpus is limited to its Summary from SAMSum: Ryan is in Italy while Leo is working hard and wishing he could win the lottery. Figure 1: An example from D IALOG S UM dataset compared with an example from SAMSum dataset. small scale. SAMSum (Gliwa et al., 2019) is a recently released written online dialogue summarization dataset, which contains 16k online chats with corresponding summaries. However, it focuses on conversations via messenger apps, which are rather short (around 94 tokens per conversation) and their language style and topics also differ from spoken daily dialogues. A comparison between the real-life scenario dialogue and online chat is shown in Figure 1. Online-chat messages contain unique tokens (e.g., “BTW”), emoticons (e.g., “:)”) and emojis (e.g., “ ”). In contrast, daily conversations have a different 5062 Findings of the Associa"
2021.findings-acl.449,J95-2003,0,0.661715,"Missing"
2021.findings-acl.449,N19-1238,0,0.081358,"Missing"
2021.findings-acl.449,D19-1051,0,0.0754933,"Missing"
2021.findings-acl.449,2020.emnlp-main.750,0,0.227527,"Missing"
2021.findings-acl.449,2020.acl-main.703,0,0.0336097,"rameters for news summarization3 , but changing the minimum length to 15. We train the 6-layer Transformer model with Adam (Kingma and Ba, 2014) for 100,000 steps. Copy attention mechanism is applied and the dropout rate is set to 0.1. U NI LM V 2 U NI LM V 2 (Bao et al., 2020) is a recently released pretrained language model for autoencoding and partially autoregressive language modeling. Here we use U NI LM V 2 BASE as a strong abstractive model. For dialogue summarization, we train the model with Adam for 100,000 steps with 2,000 warmup steps and learning rate is set to 1.5e−5 . BART BART (Lewis et al., 2020) is an encoderdecoder Transformer model pretrained on a large corpus using a denoising autoencoder task. We use the large version of BART and finetune it with 5,000 training steps/200 warmup steps for dialogue summarization. Learning rate is set to 3e−5 . 3.2 Results Table 5 presents the experimental results. In general, we find that non-pretrained abstractive models outperform LEAD (Table 4), and the best results are achieved by pretrained models, despite the fact that BART LARGE and U NI LM V 2 BASE are pretrained on monologic texts. Extractive Summary vs Abstractive Summary Transformer give"
2021.findings-acl.449,I17-1099,0,0.036943,"chats, but real-life dialogues contain business negotiation (Figure 1(a)). Intuitively, automatically summarizing such dialogues can help a business find common needs or complaints from customers. With the rise of personal assisting chatbots, summarizing dialogues from different aspects of daily life can also be useful for personal record management and other applications. We introduce Real-Life Scenario Dialogue Summarization (D IALOG S UM), a large-scale summarization dataset for dialogues. Dialogue data for D IALOG S UM are collected from three public dialogue corpora, namely Dailydialog (Li et al., 2017), DREAM (Sun et al., 2019) and MuTual (Cui et al., 2020), as well as an English speaking practice website. These datasets contain face-to-face spoken dialogues that cover a wide range of daily-life topics, including schooling, work, medication, shopping, leisure, travel. Most conversations take place between friends, colleagues, and between service providers and customers. We clean and preprocess the dialogue data into a unified format, and ask annotators to summarize them from an observer perspective. Topics are also manually labeled for each dialogue. An example of D IALOG S UM is shown in F"
2021.findings-acl.449,W04-1013,0,0.143348,"10.13 29.11 27.52 6.78 27.31 24.15 6.25 22.73 EXT- ORACLE R1 50.38 55.12 52.08 29.79 44.60 37.90 R2 28.55 30.55 31.5 8.81 17.37 13.88 RL 46.58 51.24 46.72 22.65 39.38 34.04 Table 4: Corpora statistics and extractive methods on CNN/DailyMail, NY Times, XSum, SAMSum and D IALOG S UM. Part of results is from Narayan et al. (2018). All results are computed on test sets. For D IALOG S UM, the results are the average of multi-reference results. test set, we provide three summaries written and checked by different annotators. For each test dialogue, we compare its and compute their pair-wise ROUGE (Lin, 2004) scores. Table 3 reports their averaged F1 scores of ROUGE-1 (R1), ROUGE2 (R2) and ROUGE-L (RL). We see R2 is relatively low while RL is high, which suggests that annotators’ usage of language is variable, but the main content and logical order are mostly the same. 2.4 Characteristics of D IALOG S UM We empirically compare D IALOG S UM with existing news summarization datasets and SAMSum. CNN/DailyMail (Hermann et al., 2015), NY Times (Sandhaus, 2008) and XSum (Narayan et al., 2018) are large-scale summarization datasets from the news domain, written in a monologic structure. XSum is a dataset"
2021.findings-acl.449,D19-1387,1,0.819956,"seem nice. (‘A`) Ryan: That&apos;s all???? I need more reactions!!!!!!!!!! Leo: I&apos;m tied to this office and working like a slave. AM I SUPPOSED TO SAY ""I AM SO JEALOUS!!!!!!!!""?! ! ! … Introduction Text summarization is the task of automatically generating a concise, salient, coherent and fluent summary of a given set of documents (Radev et al., 2002). Thanks to the advance in neural network models and the availability of large-scale labeled datasets, recent research has achieved promising progress on summarizing monologic texts such as news articles (Paulus et al., 2018; Gehrmann et al., 2018; Liu and Lapata, 2019; Liu et al., 2020), patents (Pilault et al., 2020) and academic papers (Koncel-Kedziorski et al., 2019). However, dialogue, as an important channel for achieving communicative intents (Bender and Koller, 2020), has received significantly less attention from the summarization research community. One main reason is the paucity of a suitable summarization dataset built on dialogue texts. Most existing research uses the AMI meeting corpus (Carletta et al., 2005), which consists of 137 dialogues obtained from virtual multi-party meeting recordings. However, research on the corpus is limited to its"
2021.findings-acl.449,2021.naacl-main.56,1,0.838473,"Missing"
2021.findings-acl.449,miltsakaki-etal-2004-penn,0,0.138161,"vents (underlined) using “since” to express their causal relation explicitly. However, the causal relation between those two events are not explicitly expressed in the dialogue, and the distance between them is long. Multiple turns usually correspond to more complicated discourse structure and relation. Also, similar with Chen and Yang (2020), we find that model performance decreases when the number of dialogue turns grows (See Appendix B). To better evaluate model ability to disambiguate discourse relations in D IALOG S UM, we first collect discourse connectives from Penn Discourse Treebank (Miltsakaki et al., 2004), and check whether these connectives are included in summaries in the testset. If the three reference summaries of a dialogue all contain connectives, we assume that the dialogues have strong discourse signals. We choose 70 dialogues from D IALOG S UM in this way. We then ask linguists who specialize in discourse to evaluate model outputs and give scores from {−1, 0, 1}, where 1 means that the generated descriptions of main events are reasonable and contain correct discourse connectives, 0 means that the descriptions are good but contain no discourse connectives and −1 means that the descript"
2021.findings-acl.449,D18-1206,0,0.0638485,"Missing"
2021.findings-acl.449,D14-1162,0,0.0845504,"Missing"
2021.findings-acl.449,2020.emnlp-main.748,0,0.531908,"reactions!!!!!!!!!! Leo: I&apos;m tied to this office and working like a slave. AM I SUPPOSED TO SAY ""I AM SO JEALOUS!!!!!!!!""?! ! ! … Introduction Text summarization is the task of automatically generating a concise, salient, coherent and fluent summary of a given set of documents (Radev et al., 2002). Thanks to the advance in neural network models and the availability of large-scale labeled datasets, recent research has achieved promising progress on summarizing monologic texts such as news articles (Paulus et al., 2018; Gehrmann et al., 2018; Liu and Lapata, 2019; Liu et al., 2020), patents (Pilault et al., 2020) and academic papers (Koncel-Kedziorski et al., 2019). However, dialogue, as an important channel for achieving communicative intents (Bender and Koller, 2020), has received significantly less attention from the summarization research community. One main reason is the paucity of a suitable summarization dataset built on dialogue texts. Most existing research uses the AMI meeting corpus (Carletta et al., 2005), which consists of 137 dialogues obtained from virtual multi-party meeting recordings. However, research on the corpus is limited to its Summary from SAMSum: Ryan is in Italy while Leo is"
2021.findings-acl.449,D19-1462,0,0.0692126,"Missing"
2021.findings-acl.449,J02-4001,0,0.397685,"l with. 1 Summary from DIALOGSUM: #Person_1# and #Person_2# agree to sign an agreement since #Person_1# could speed up the delivery as #Person_2# hopes. (b) Dialogue from SAMSum: … Leo: BTW what are those pics? Ryan: Pics from Italy!!! :):):):))))))))) Leo: Yeah. They seem nice. (‘A`) Ryan: That&apos;s all???? I need more reactions!!!!!!!!!! Leo: I&apos;m tied to this office and working like a slave. AM I SUPPOSED TO SAY ""I AM SO JEALOUS!!!!!!!!""?! ! ! … Introduction Text summarization is the task of automatically generating a concise, salient, coherent and fluent summary of a given set of documents (Radev et al., 2002). Thanks to the advance in neural network models and the availability of large-scale labeled datasets, recent research has achieved promising progress on summarizing monologic texts such as news articles (Paulus et al., 2018; Gehrmann et al., 2018; Liu and Lapata, 2019; Liu et al., 2020), patents (Pilault et al., 2020) and academic papers (Koncel-Kedziorski et al., 2019). However, dialogue, as an important channel for achieving communicative intents (Bender and Koller, 2020), has received significantly less attention from the summarization research community. One main reason is the paucity of"
2021.findings-acl.449,Q19-1014,0,0.120783,"Missing"
2021.findings-acl.449,J05-2005,0,0.642475,"Missing"
2021.findings-acl.449,2020.acl-main.451,0,0.106567,"lower than human. Model-generated summaries have the Table 8: Human evaluation on discourse relations, with corresponding ROUGE scores on the sub-test set. Avg. stands for the averaged score here. highest scores on Fluency, while lowest on Consistency. It suggests that although model-generated summaries are grammatical and fluent, they still contain factual errors. Discourse Relation Reasonable summaries should convey important relations between main events, and identifying discourse relations and using proper phrases to express them in summaries can be challenging for summarization systems (Xu et al., 2020). Take Figure 1 (a) for example, the human annotated summary connects two main events (underlined) using “since” to express their causal relation explicitly. However, the causal relation between those two events are not explicitly expressed in the dialogue, and the distance between them is long. Multiple turns usually correspond to more complicated discourse structure and relation. Also, similar with Chen and Yang (2020), we find that model performance decreases when the number of dialogue turns grows (See Appendix B). To better evaluate model ability to disambiguate discourse relations in D I"
2021.findings-acl.449,P18-1205,0,0.022492,"be detected from the conversation. Annotators are also asked to write a short (around 3 tokens) topic for each dialogue. Appendix A shows the list of topics. Data Cleaning and Pre-Processing We delete non-English characters, correct typos and grammatical errors, and further filter out duplicated data based on text similarity. After deduplicating, proportions of the data sources are summarized in Table 2. Because of different data processing methods and annotation procedures, original dialogues in DailyDialog, DREAM and MuTual are in different formats. We follow previous work (Li et al., 2017; Zhang et al., 2018; Budzianowski et al., 2018; Dinan et al., 2019) and preprocess them into a biturn dialogue flow, merging continuous turns of the same speaker into one utterance. Also, we add tags (e.g. #Person 1# and #Person 2# in Figure 1(a)) before each dialogue turn, to distinguish speakers. The final D IALOG S UM dataset contains 13,460 dialogues, which are divided into training (12,460), validation (500) and test (500) sets. 2.2 Annotation We ask annotators to write dialogue summaries based on following criteria: the summary should (1) convey the most salient information of the dialogue and; (2) be brie"
2021.findings-emnlp.263,D14-1067,0,0.0241671,"embed the high-dimension and of different negative instances differs greatly. Reusually discrete features of entities/relations into calling the training loss in previous KRL methods a low-dimension vector space. These learned rep- (such as the margin-based (Chechik et al., 2009) resentations, by encoding the underlying semantic and logistic-based (Gutmann and Hyvärinen, 2010) relationships among entities/relations, are able to loss), it coequally compares each positive instance facilitate various downstream tasks, such as ques- with only one negative instance at each training tion answering (Bordes et al., 2014), recommen- iteration. In this way, it not only restrains the indation (Wang et al., 2019b) and relation extrac- teraction between positive-negative instances, but tion (Bastos et al., 2021) to name some. As a basic also overlooks the different weights of different research topic, KRL has always attracted many at- negative samples to each positive instance, which, tentions of researchers in relevant domains. in general, would lead to bias and slow training ∗ Corresponding author. convergence. Taking the triple (Kobe Bryant, na3061 Findings of the Association for Computational Linguistics: EMNL"
2021.findings-emnlp.263,N18-1133,0,0.0268155,"∈G exp(S(z j , c)/τ ) (13) For better readability, we illustrate the flowchart of our method in Algorithm 1. 4 4.1 Experiments Setup Datasets. We evaluate our C 3 models on two standard link prediction datasets: FB15k237 (Toutanova and Chen, 2015) that is created from FB15K (Bordes et al., 2013) and WN18RR (Dettmers et al., 2018) which is a subset of WN18 (Miller, 1995). Baselines. We compare our C 3 with the following previous state-of-the-art KRL methods: TransE (Bordes et al., 2013), DistMult (Yang et al., 2014), ComplEx (Trouillon et al., 2016), R-GCN (Schlichtkrull et al., 2018), KBGAN (Cai and Wang, 2018), ConvE (Dettmers (11) et al., 2018), SACN (Shang et al., 2019), HypER (Balaževi´c et al., 2019a), RotatE (Sun 3065 WN18RR FB15k-237 MRR H@10 H@3 H@1 MRR H@10 H@3 H@1 TransE (Bordes et al., 2013) DistMult (Yang et al., 2014) ComplEx (Trouillon et al., 2016) R-GCN (Schlichtkrull et al., 2018) KBGAN (Cai and Wang, 2018) ConvE (Dettmers et al., 2018) SACN (Shang et al., 2019) HypER (Balaževi´c et al., 2019a) RotatE (Sun et al., 2019) ConvR (Jiang et al., 2019) VR-GCN (Ye et al., 2019) TuckER (Balaževi´c et al., 2019b) COMPGCN (Vashishth et al., 2019) SANS (Ahrabian et al., 2020) .226 .43 .44 .214"
2021.findings-emnlp.354,N19-1423,0,0.0775104,"Missing"
2021.findings-emnlp.354,2020.emnlp-main.638,0,0.0425731,"data as input of GPT-3 by constructing an artificial development set. One concurrent with our work, Yoo et al. (2021) consider distilling knowledge from GPT-3 with synthetic data. In their work, the synthetic dataset size is always the same as the original training dataset size. Unlike the most recent works on GPT-3, we treat GPT-3 as a new source of labeler and focus on analyzing the cost of running GPT-3, which is not free according to OpenAI API. This work is complementary to many other methods based on human labeling, such as few-shot learning (Yin, 2020), active learning (Settles, 2009; Dor et al., 2020) and transfer learning (Ruder et al., 2019). Dual supervision. Our method is also related to dual supervision (Attenberg et al., 2010), which combines two types of labels (one cheap and one expensive) to train a model. Dual supervision typically considers different labeling tasks for humans, for example labeling words or documents (Melville and Sindhwani, 2009), natural language understanding or generation (Su et al., 2019), cardinal or ordinal labels (Xu et al., 2020); here, we consider the same task for different-cost labelers. Labeling oracles with different costs for the same task have als"
2021.findings-emnlp.354,2020.coling-main.564,0,0.0796803,"Missing"
2021.findings-emnlp.354,2021.ccl-1.108,0,0.0712828,"Missing"
2021.findings-emnlp.354,W09-1907,0,0.0556008,"of labeler and focus on analyzing the cost of running GPT-3, which is not free according to OpenAI API. This work is complementary to many other methods based on human labeling, such as few-shot learning (Yin, 2020), active learning (Settles, 2009; Dor et al., 2020) and transfer learning (Ruder et al., 2019). Dual supervision. Our method is also related to dual supervision (Attenberg et al., 2010), which combines two types of labels (one cheap and one expensive) to train a model. Dual supervision typically considers different labeling tasks for humans, for example labeling words or documents (Melville and Sindhwani, 2009), natural language understanding or generation (Su et al., 2019), cardinal or ordinal labels (Xu et al., 2020); here, we consider the same task for different-cost labelers. Labeling oracles with different costs for the same task have also been considered in other areas. Proactive learning (Donmez and Carbonell, 2008) considers active learning with multiple oracles with varied label quality and cost, and oracles can also abstain from labeling an example (“unknown” label). Multifidelity optimization (Song et al., 2019) considers optimizing an underlying function (e.g., development accuracy of a"
2021.findings-emnlp.354,D18-1206,0,0.0204454,"ets. diction, Entailment, or Neutral. RTE (Dagan et al., 2005) is a 2-way text entailment: Entailment or We use the original test set for evaluation if it is Not-Entailment. AGNews (Zhang et al., 2015) is available, and use development set otherwise. to identify the topic from World, Sports, Business, NLG tasks We apply our labeling strategies to and Technology. DBPedia (Zhang et al., 2015) natural language generation tasks, two on sum- provides a different topic pool: Company, School, marization and one on question generation task. Artist, Athlete, Politician, Transportation, Building, XSum (Narayan et al., 2018) is from BBC articles, Nature, Village, Animal, Plant, Album, Film, or 4199 Figure 4: GPT-3 labeling performance. We feed un-labeled data to GPT-3 with different shot settings and fine-tune Transformer models on the corresponding labeled data. The dot lines are the raw GPT-3 performance with various shots. Lines in the same color use the same number of shots in GPT-3. The cost of GPT3-Label cannot further increase when all training data (up to 5,120 instances) has been labeled. Book. 3.2 Settings Model structure For GPT-3 labeling API, we select the largest version Davinci4 . Our in-house NLG"
2021.findings-emnlp.354,D16-1264,0,0.0150481,"those with the lowest scores to be re-labeled by humans. All the budget for human labeling is dedicated to this relabeling. In our experiments, the number of data to label depends on the budget assigned to either GPT-3 or human, and we will show different strategies to split the budget. Finally, the relabeled data and other GPT-3 labeled data are fed into in-house models for fine-tuning. each of which contains an expert-written summary. Gigaword (Rush et al., 2015) also comes from news articles, and the task is to summarize the first sentence in the article by generating its headline. SQuAD (Rajpurkar et al., 2016) is Stanford Question Answering dataset, and our task is to generate a question given a paragraph and an answer. NLU tasks We leverage the following classification tasks. SST-2 (Socher et al., 2013) is a binary sentiment classification task from Stanford 3 Experiments Sentiment Treebank. TREC (Socher et al., 2013) is to identify an answer type of a question from 3.1 Datasets We employ 3 natural language generation (NLG) Number, Location, Person, Description, Entity, or tasks and 6 natural language understanding (NLU) Abbreviation. CB (De Marneffe et al., 2019) is a 3-way textual entailment tas"
2021.findings-emnlp.354,N19-5004,0,0.0287775,"n artificial development set. One concurrent with our work, Yoo et al. (2021) consider distilling knowledge from GPT-3 with synthetic data. In their work, the synthetic dataset size is always the same as the original training dataset size. Unlike the most recent works on GPT-3, we treat GPT-3 as a new source of labeler and focus on analyzing the cost of running GPT-3, which is not free according to OpenAI API. This work is complementary to many other methods based on human labeling, such as few-shot learning (Yin, 2020), active learning (Settles, 2009; Dor et al., 2020) and transfer learning (Ruder et al., 2019). Dual supervision. Our method is also related to dual supervision (Attenberg et al., 2010), which combines two types of labels (one cheap and one expensive) to train a model. Dual supervision typically considers different labeling tasks for humans, for example labeling words or documents (Melville and Sindhwani, 2009), natural language understanding or generation (Su et al., 2019), cardinal or ordinal labels (Xu et al., 2020); here, we consider the same task for different-cost labelers. Labeling oracles with different costs for the same task have also been considered in other areas. Proactive"
2021.findings-emnlp.354,D15-1044,0,0.0254016,"when all training data (up to 5,120 instances) has been labeled. data. Then, we rank all the labels based on the confidence score (logit) and select those with the lowest scores to be re-labeled by humans. All the budget for human labeling is dedicated to this relabeling. In our experiments, the number of data to label depends on the budget assigned to either GPT-3 or human, and we will show different strategies to split the budget. Finally, the relabeled data and other GPT-3 labeled data are fed into in-house models for fine-tuning. each of which contains an expert-written summary. Gigaword (Rush et al., 2015) also comes from news articles, and the task is to summarize the first sentence in the article by generating its headline. SQuAD (Rajpurkar et al., 2016) is Stanford Question Answering dataset, and our task is to generate a question given a paragraph and an answer. NLU tasks We leverage the following classification tasks. SST-2 (Socher et al., 2013) is a binary sentiment classification task from Stanford 3 Experiments Sentiment Treebank. TREC (Socher et al., 2013) is to identify an answer type of a question from 3.1 Datasets We employ 3 natural language generation (NLG) Number, Location, Perso"
2021.findings-emnlp.354,D13-1170,0,0.00311574,"ed to either GPT-3 or human, and we will show different strategies to split the budget. Finally, the relabeled data and other GPT-3 labeled data are fed into in-house models for fine-tuning. each of which contains an expert-written summary. Gigaword (Rush et al., 2015) also comes from news articles, and the task is to summarize the first sentence in the article by generating its headline. SQuAD (Rajpurkar et al., 2016) is Stanford Question Answering dataset, and our task is to generate a question given a paragraph and an answer. NLU tasks We leverage the following classification tasks. SST-2 (Socher et al., 2013) is a binary sentiment classification task from Stanford 3 Experiments Sentiment Treebank. TREC (Socher et al., 2013) is to identify an answer type of a question from 3.1 Datasets We employ 3 natural language generation (NLG) Number, Location, Person, Description, Entity, or tasks and 6 natural language understanding (NLU) Abbreviation. CB (De Marneffe et al., 2019) is a 3-way textual entailment task to classify a sentasks for evaluation. We sample up to 5.1K cases from the training data for labeling. We simulate hu- tence pair of premise and hypothesis into Contraman labeling by using the lab"
2021.findings-emnlp.354,P19-1545,0,0.022262,"e according to OpenAI API. This work is complementary to many other methods based on human labeling, such as few-shot learning (Yin, 2020), active learning (Settles, 2009; Dor et al., 2020) and transfer learning (Ruder et al., 2019). Dual supervision. Our method is also related to dual supervision (Attenberg et al., 2010), which combines two types of labels (one cheap and one expensive) to train a model. Dual supervision typically considers different labeling tasks for humans, for example labeling words or documents (Melville and Sindhwani, 2009), natural language understanding or generation (Su et al., 2019), cardinal or ordinal labels (Xu et al., 2020); here, we consider the same task for different-cost labelers. Labeling oracles with different costs for the same task have also been considered in other areas. Proactive learning (Donmez and Carbonell, 2008) considers active learning with multiple oracles with varied label quality and cost, and oracles can also abstain from labeling an example (“unknown” label). Multifidelity optimization (Song et al., 2019) considers optimizing an underlying function (e.g., development accuracy of a neural network) by querying approximations of different precisio"
2021.findings-emnlp.354,P95-1026,0,0.625576,"Missing"
2021.findings-emnlp.354,2021.findings-emnlp.192,0,0.0337547,"formance v.s. label4201 ing cost of this strategy is quite similar to Figure 4. Thus we leave it in Appendix B for reference. 4 Related Work GPT-3 Overview. With the success of large pretrained language modeling GPT-3 (Brown et al., 2020) on few-shot learning, more works have been done to improve GPT-3. Zhao et al. (2021) propose to remove the model bias before using GPT-3, which not only increases the accuracy but also reduces the variance. Lu et al. (2021) work on how to order the few labeled data as input of GPT-3 by constructing an artificial development set. One concurrent with our work, Yoo et al. (2021) consider distilling knowledge from GPT-3 with synthetic data. In their work, the synthetic dataset size is always the same as the original training dataset size. Unlike the most recent works on GPT-3, we treat GPT-3 as a new source of labeler and focus on analyzing the cost of running GPT-3, which is not free according to OpenAI API. This work is complementary to many other methods based on human labeling, such as few-shot learning (Yin, 2020), active learning (Settles, 2009; Dor et al., 2020) and transfer learning (Ruder et al., 2019). Dual supervision. Our method is also related to dual sup"
2021.findings-emnlp.78,2020.coling-main.155,0,0.0366559,"SD. We first construct a large-scale Chinese lexical sample WSD dataset with word-formations. Then, we propose a model FormBERT to explicitly incorporate word-formations into sense disambiguation. To further enhance generalizability, we design a word-formation predictor module in case word-formation annotations are unavailable. Experimental results show that our method brings substantial performance improvement over strong baselines. 1 1 Introduction Word sense disambiguation (WSD) aims to identify the sense of a polysemous word in a specific context, which benefits multiple downstream tasks (Hou et al., 2020). With copious senseannotated data (Raganato et al., 2017), neural WSD methods achieve superior performance by leveraging definitional and relational features in knowledge bases (KB) (Luo et al., 2018a; Huang et al., 2019; Bevilacqua and Navigli, 2020). In parataxis languages like Chinese, word meanings are highly correlated with word-formations (Li et al., 2018), which have not been explored in WSD thus far. Specifically, word-formations designate how characters interact to construct meanings. As shown in Figure 1, “征文1 "" with the Modifier-Head formation means solicited paper, where “征"" (soli"
2021.findings-emnlp.78,D19-1355,0,0.21438,"ralizability, we design a word-formation predictor module in case word-formation annotations are unavailable. Experimental results show that our method brings substantial performance improvement over strong baselines. 1 1 Introduction Word sense disambiguation (WSD) aims to identify the sense of a polysemous word in a specific context, which benefits multiple downstream tasks (Hou et al., 2020). With copious senseannotated data (Raganato et al., 2017), neural WSD methods achieve superior performance by leveraging definitional and relational features in knowledge bases (KB) (Luo et al., 2018a; Huang et al., 2019; Bevilacqua and Navigli, 2020). In parataxis languages like Chinese, word meanings are highly correlated with word-formations (Li et al., 2018), which have not been explored in WSD thus far. Specifically, word-formations designate how characters interact to construct meanings. As shown in Figure 1, “征文1 "" with the Modifier-Head formation means solicited paper, where “征"" (solicit) modifies “文"" (paper); “征文2 "" with the VerbObject formation means solicit paper, where “征"" Word-Formation Modifier-Head …接收征文… … accept solicited paper … …开始征文… 征收的文章 solicited paper 征: 征收 solicit 文: 文章 paper Definiti"
2021.findings-emnlp.78,S07-1004,0,0.0663487,"Missing"
2021.findings-emnlp.78,P19-1568,0,0.0366864,"Missing"
2021.findings-emnlp.78,P18-2023,0,0.151903,"ethod brings substantial performance improvement over strong baselines. 1 1 Introduction Word sense disambiguation (WSD) aims to identify the sense of a polysemous word in a specific context, which benefits multiple downstream tasks (Hou et al., 2020). With copious senseannotated data (Raganato et al., 2017), neural WSD methods achieve superior performance by leveraging definitional and relational features in knowledge bases (KB) (Luo et al., 2018a; Huang et al., 2019; Bevilacqua and Navigli, 2020). In parataxis languages like Chinese, word meanings are highly correlated with word-formations (Li et al., 2018), which have not been explored in WSD thus far. Specifically, word-formations designate how characters interact to construct meanings. As shown in Figure 1, “征文1 "" with the Modifier-Head formation means solicited paper, where “征"" (solicit) modifies “文"" (paper); “征文2 "" with the VerbObject formation means solicit paper, where “征"" Word-Formation Modifier-Head …接收征文… … accept solicited paper … …开始征文… 征收的文章 solicited paper 征: 征收 solicit 文: 文章 paper Definition Verb-Object … start to solicit paper … 征收文章 solicit paper Figure 1: The contexts indicate that the word “征 文"" holds two senses constructed by"
2021.findings-emnlp.78,D18-1170,1,0.929226,"urther enhance generalizability, we design a word-formation predictor module in case word-formation annotations are unavailable. Experimental results show that our method brings substantial performance improvement over strong baselines. 1 1 Introduction Word sense disambiguation (WSD) aims to identify the sense of a polysemous word in a specific context, which benefits multiple downstream tasks (Hou et al., 2020). With copious senseannotated data (Raganato et al., 2017), neural WSD methods achieve superior performance by leveraging definitional and relational features in knowledge bases (KB) (Luo et al., 2018a; Huang et al., 2019; Bevilacqua and Navigli, 2020). In parataxis languages like Chinese, word meanings are highly correlated with word-formations (Li et al., 2018), which have not been explored in WSD thus far. Specifically, word-formations designate how characters interact to construct meanings. As shown in Figure 1, “征文1 "" with the Modifier-Head formation means solicited paper, where “征"" (solicit) modifies “文"" (paper); “征文2 "" with the VerbObject formation means solicit paper, where “征"" Word-Formation Modifier-Head …接收征文… … accept solicited paper … …开始征文… 征收的文章 solicited paper 征: 征收 solicit"
2021.findings-emnlp.78,P18-1230,1,0.905863,"urther enhance generalizability, we design a word-formation predictor module in case word-formation annotations are unavailable. Experimental results show that our method brings substantial performance improvement over strong baselines. 1 1 Introduction Word sense disambiguation (WSD) aims to identify the sense of a polysemous word in a specific context, which benefits multiple downstream tasks (Hou et al., 2020). With copious senseannotated data (Raganato et al., 2017), neural WSD methods achieve superior performance by leveraging definitional and relational features in knowledge bases (KB) (Luo et al., 2018a; Huang et al., 2019; Bevilacqua and Navigli, 2020). In parataxis languages like Chinese, word meanings are highly correlated with word-formations (Li et al., 2018), which have not been explored in WSD thus far. Specifically, word-formations designate how characters interact to construct meanings. As shown in Figure 1, “征文1 "" with the Modifier-Head formation means solicited paper, where “征"" (solicit) modifies “文"" (paper); “征文2 "" with the VerbObject formation means solicit paper, where “征"" Word-Formation Modifier-Head …接收征文… … accept solicited paper … …开始征文… 征收的文章 solicited paper 征: 征收 solicit"
2021.findings-emnlp.78,W04-0847,0,0.116833,"Missing"
2021.findings-emnlp.78,E17-1010,0,0.0220478,"sample WSD dataset with word-formations. Then, we propose a model FormBERT to explicitly incorporate word-formations into sense disambiguation. To further enhance generalizability, we design a word-formation predictor module in case word-formation annotations are unavailable. Experimental results show that our method brings substantial performance improvement over strong baselines. 1 1 Introduction Word sense disambiguation (WSD) aims to identify the sense of a polysemous word in a specific context, which benefits multiple downstream tasks (Hou et al., 2020). With copious senseannotated data (Raganato et al., 2017), neural WSD methods achieve superior performance by leveraging definitional and relational features in knowledge bases (KB) (Luo et al., 2018a; Huang et al., 2019; Bevilacqua and Navigli, 2020). In parataxis languages like Chinese, word meanings are highly correlated with word-formations (Li et al., 2018), which have not been explored in WSD thus far. Specifically, word-formations designate how characters interact to construct meanings. As shown in Figure 1, “征文1 "" with the Modifier-Head formation means solicited paper, where “征"" (solicit) modifies “文"" (paper); “征文2 "" with the VerbObject form"
2021.findings-emnlp.78,W13-4302,0,0.0799539,"Missing"
2021.findings-emnlp.78,2021.naacl-main.437,1,0.780753,"; Jin et al., 2007; Agirre et al., 2009; Hou et al., 2020) are small in vocabulary size (less than 100 words except for Agirre et al., 2009), and it is uneasy to combine these datasets to enlarge their size, since they differ in format, sense inventory and construction guidelines. Word-Formation knowledge: Instead of combining roots and affixes, Chinese words are constructed by characters using word-formations (Zhu et al., 2019). Word-formations have shown to be effective in multiple tasks like learning embeddings for parataxis languages (Park et al., 2018; Li et al., 2018; Lin and Liu, 2019; Zheng et al., 2021a,b). However, these works lack a clear distinction among different word-formations which require manual annotations. 3 The FiCLS Dataset The construction of FiCLS includes two phases: collecting a base dataset and annotating wordformations. Each FiCLS entry consists of (1) a word, (2) a sense definition, (3) a word-formation, and (4) a context sentence. 3.1 Chinese WSD Dataset We first construct a Chinese lexical sample WSD base dataset. We build the sense inventory based on the 5th edition of the Contemporary Chinese Dictionary (CCD) published by the Commercial Press,2 one of the most influe"
2021.findings-emnlp.78,2021.ccl-1.36,1,0.722417,"; Jin et al., 2007; Agirre et al., 2009; Hou et al., 2020) are small in vocabulary size (less than 100 words except for Agirre et al., 2009), and it is uneasy to combine these datasets to enlarge their size, since they differ in format, sense inventory and construction guidelines. Word-Formation knowledge: Instead of combining roots and affixes, Chinese words are constructed by characters using word-formations (Zhu et al., 2019). Word-formations have shown to be effective in multiple tasks like learning embeddings for parataxis languages (Park et al., 2018; Li et al., 2018; Lin and Liu, 2019; Zheng et al., 2021a,b). However, these works lack a clear distinction among different word-formations which require manual annotations. 3 The FiCLS Dataset The construction of FiCLS includes two phases: collecting a base dataset and annotating wordformations. Each FiCLS entry consists of (1) a word, (2) a sense definition, (3) a word-formation, and (4) a context sentence. 3.1 Chinese WSD Dataset We first construct a Chinese lexical sample WSD base dataset. We build the sense inventory based on the 5th edition of the Contemporary Chinese Dictionary (CCD) published by the Commercial Press,2 one of the most influe"
2021.findings-emnlp.78,N19-1097,0,0.0631183,"Missing"
2021.findings-emnlp.91,W05-0909,0,0.346537,"multiple regions, the target question and answer are only related tightly with only part of them. As the latent variable contains information of the target QAP, we use it to make the model concentrate on those related regions by scaling their representations. To improve the joint model’s consistency, we align the attentions of the question decoder and the answer decoder to make them focus on similar regions. A remaining issue of VQAPG is how to measure the consistency automatically in addition to manual inspection. Traditional popular metrics, such as BLEU (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005), only tell the overlapping degree between generated QAPs and the target ones, being insufficient to indicate the consistency. Targeting this issue, we devise a consistency evaluator trained with an adversarial strategy. We assume the samples in the dataset as consistent and construct inconsistent samples through shuffling images, answers, questions, or all of them. This evaluator will give a high score if the generated QAP is consistent with the image and a low score if not. proposed methods: both the region representation scaling mechanism and the attention alignment mechanism improve consis"
2021.findings-emnlp.91,P17-1123,0,0.0184862,"a consistency evaluator. iii) We incorporate variational inference into models and propose a series of techniques, including region representation scale and attention concentration to improve diversity and consistency. iv) We conduct comprehensive experiments on two large scale datasets. The results show the effectiveness of our approach to generate diverse and consistency QAPs and benefit other applications. 2 Related Works Visual Question Generation is an interesting task emerged in recent years. Question generation is firstly studied on text (Heilman and Smith, 2010; Labutov et al., 2015; Du et al., 2017; Zhou et al., 2018; Sun et al., 2018; Ma et al., 2020; Kim et al., 2019). While related studies on images has received little attention. Existing methods in this field are typically based on learning algorithms (Mostafazadeh et al., 2016; Zhang et al., 2017). Such methods are often incorporated with the variational process (Jain et al., 2017; Krishna et al., 2019). Visual question generation is also conducted together with visual question answering (Li et al., 2018; Sun et al., 2020). Visual Question Answering has received more We conduct experiments on two datasets, interest thanks to availa"
2021.findings-emnlp.91,P15-1086,0,0.0313247,"models. We also design a consistency evaluator. iii) We incorporate variational inference into models and propose a series of techniques, including region representation scale and attention concentration to improve diversity and consistency. iv) We conduct comprehensive experiments on two large scale datasets. The results show the effectiveness of our approach to generate diverse and consistency QAPs and benefit other applications. 2 Related Works Visual Question Generation is an interesting task emerged in recent years. Question generation is firstly studied on text (Heilman and Smith, 2010; Labutov et al., 2015; Du et al., 2017; Zhou et al., 2018; Sun et al., 2018; Ma et al., 2020; Kim et al., 2019). While related studies on images has received little attention. Existing methods in this field are typically based on learning algorithms (Mostafazadeh et al., 2016; Zhang et al., 2017). Such methods are often incorporated with the variational process (Jain et al., 2017; Krishna et al., 2019). Visual question generation is also conducted together with visual question answering (Li et al., 2018; Sun et al., 2020). Visual Question Answering has received more We conduct experiments on two datasets, interest"
2021.findings-emnlp.91,2020.acl-main.20,0,0.0265571,"solid gray line indicates the influence works for estimating likelihood. T represents “X <sep> Y ."" the true data distribution P (Q, A|I) so that we can sample questions and answers from it. Because VQAPG is a one-to-many task, we also expect this model to support sampling of diverse and consistent QAPs. The VQG and VQA task Image Answer Image Question is unexplored so far. Some works explore this task in text using techniques such as pipeline (Subramanian et al., 2018), multi-agent system (Wang et al., 2019), hierarchical variational model (Subramanian et al., 2018) or coreference knowledge (Lee et al., 2020). Su et al. (2021) also proposes a model for QAP generation from video. However, such QAP generation works assume answers are selected from the spans of input context (Subramanian et al., 2018; Lee et al., 2020; Wang et al., 2019) or the given candidates (Su et al., 2021). As answers could not be extracted directly from images and there are no candidate ones, the above methods can not be simply applied to the image. 3 VQAPG Task The VQAPG task Question Image Answer Question Answer Figure 4: Comparison between VQG&VQA and VQAPG. Unlike VQG or VQA, which rely on answer or question additionally f"
2021.findings-emnlp.91,N16-1014,0,0.0340896,"ousand samples from the train set as our development set. For the Visual-7w, we take no extra operations. Table 2 shows the statistics of these two datasets2 . (5) 2 1057 We use K to represent thousand. Dataset Images (K) QAPs (K) VQA2.0 Visual-7w 10.0/6.1/4.8 14.4/5.7/8.6 70.6/10.0/36.9 69.8/28.0/42.0 Table 2: The statistics of two datasets (Train/Dev/Test). 5.2 Evaluation Metrics In the following experiments, we evaluate our models with both automatic metrics and manual inspection. We mainly focus on the diversity and consistency of generated results. On the diversity side, we use Distinct (Li et al., 2016), a common metric for diversity, to measures the ratio of unique n-grams in the text. We adopt Distinct-4 (denoted as D) to evaluate the generation by concatenating the question and answer together. We also report the number of unique QAPs of the generation result (denoted as N). On the consistency side, we use our consistency evaluation model (Section 4.6) to indicate whether the QAP is consistent with the image. If the output score is greater than 0.5, we consider the result is consistent, otherwise inconsistent. We report two metrics for consistency, the percentage of consistent QAPs (denot"
2021.findings-emnlp.91,P16-1170,0,0.0181853,"onduct comprehensive experiments on two large scale datasets. The results show the effectiveness of our approach to generate diverse and consistency QAPs and benefit other applications. 2 Related Works Visual Question Generation is an interesting task emerged in recent years. Question generation is firstly studied on text (Heilman and Smith, 2010; Labutov et al., 2015; Du et al., 2017; Zhou et al., 2018; Sun et al., 2018; Ma et al., 2020; Kim et al., 2019). While related studies on images has received little attention. Existing methods in this field are typically based on learning algorithms (Mostafazadeh et al., 2016; Zhang et al., 2017). Such methods are often incorporated with the variational process (Jain et al., 2017; Krishna et al., 2019). Visual question generation is also conducted together with visual question answering (Li et al., 2018; Sun et al., 2020). Visual Question Answering has received more We conduct experiments on two datasets, interest thanks to available public datasets such as VQA2.0 (Goyal et al., 2017) and VisuVQA2.0 (Goyal et al., 2017) and Visual-7w (Zhu et al., 2016), in both of which each image has mul- alGenome (Krishna et al., 2017). Through finetuning on large pre-trained mo"
2021.findings-emnlp.91,P02-1040,0,0.109176,"serve that if we grid an image into multiple regions, the target question and answer are only related tightly with only part of them. As the latent variable contains information of the target QAP, we use it to make the model concentrate on those related regions by scaling their representations. To improve the joint model’s consistency, we align the attentions of the question decoder and the answer decoder to make them focus on similar regions. A remaining issue of VQAPG is how to measure the consistency automatically in addition to manual inspection. Traditional popular metrics, such as BLEU (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005), only tell the overlapping degree between generated QAPs and the target ones, being insufficient to indicate the consistency. Targeting this issue, we devise a consistency evaluator trained with an adversarial strategy. We assume the samples in the dataset as consistent and construct inconsistent samples through shuffling images, answers, questions, or all of them. This evaluator will give a high score if the generated QAP is consistent with the image and a low score if not. proposed methods: both the region representation scaling mechanism and the attent"
2021.findings-emnlp.91,D18-1434,0,0.0145804,"uestion. Therefore, keepstream applications such as child education (Wang ing the consistency is difficult because it requires et al., 2018), visual dialog (Das et al., 2017), generthe generation model to simultaneously guarantee ating verification code for websites, visual question the correctness of both questions and answers. generation (VQG) (Krishna et al., 2019) and visual There are two related tasks with VQAPG, but question answering (VQA) (Wu et al., 2016). For neither serves as suitable prior art. The first is example, training VQA models requires large scale VQG (Zhang et al., 2017; Patro et al., 2018; Krlabelled data, which is usually labour intensive and ishna et al., 2019), which produces questions given expensive to construct. Meanwhile, bias still exists answers or other knowledge. Another similar task in large QA datasets (Goyal et al., 2017), including is VQA that aims to answer given questions (Goyal domain coverage, question and answer types, and et al., 2017; Zhou et al., 2020; Su et al., 2020; Lu ∗ Corresponding author. et al., 2019). They can be viewed as two subtasks 1053 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 1053–1066 November 7–11, 2021"
2021.findings-emnlp.91,2020.acl-main.15,0,0.126033,"al., 2020; Su et al., 2020; Lu ∗ Corresponding author. et al., 2019). They can be viewed as two subtasks 1053 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 1053–1066 November 7–11, 2021. ©2021 Association for Computational Linguistics of VQAPG. However, simply combining the two is not an ideal substitute for VQAPG since they condition another’s output for learning. We study several generation paradigms to perform VQAPG. The first is a pipeline model that generates questions and answers one after another. Next, inspired by the non-autoregressive text generation (Ren et al., 2020), we propose a joint model that generates questions and answers in parallel. To reduce the model size, we also propose a sequential model that concatenates the two targets into one. We integrate latent variable(s) into these models through variational inference (Kingma and Welling, 2014) to improve diversity. If fed with different latent variables sampled from the prior distribution, the model can generate various QAPs. Besides, we observe that if we grid an image into multiple regions, the target question and answer are only related tightly with only part of them. As the latent variable conta"
2021.findings-emnlp.91,W18-2609,0,0.0148713,"tes the influence works for estimating both prior and posterior. The blue dashed line indicates the influence works only for estimating posterior. The solid gray line indicates the influence works for estimating likelihood. T represents “X <sep> Y ."" the true data distribution P (Q, A|I) so that we can sample questions and answers from it. Because VQAPG is a one-to-many task, we also expect this model to support sampling of diverse and consistent QAPs. The VQG and VQA task Image Answer Image Question is unexplored so far. Some works explore this task in text using techniques such as pipeline (Subramanian et al., 2018), multi-agent system (Wang et al., 2019), hierarchical variational model (Subramanian et al., 2018) or coreference knowledge (Lee et al., 2020). Su et al. (2021) also proposes a model for QAP generation from video. However, such QAP generation works assume answers are selected from the spans of input context (Subramanian et al., 2018; Lee et al., 2020; Wang et al., 2019) or the given candidates (Su et al., 2021). As answers could not be extracted directly from images and there are no candidate ones, the above methods can not be simply applied to the image. 3 VQAPG Task The VQAPG task Question"
2021.findings-emnlp.91,D18-1427,0,0.0162873,"corporate variational inference into models and propose a series of techniques, including region representation scale and attention concentration to improve diversity and consistency. iv) We conduct comprehensive experiments on two large scale datasets. The results show the effectiveness of our approach to generate diverse and consistency QAPs and benefit other applications. 2 Related Works Visual Question Generation is an interesting task emerged in recent years. Question generation is firstly studied on text (Heilman and Smith, 2010; Labutov et al., 2015; Du et al., 2017; Zhou et al., 2018; Sun et al., 2018; Ma et al., 2020; Kim et al., 2019). While related studies on images has received little attention. Existing methods in this field are typically based on learning algorithms (Mostafazadeh et al., 2016; Zhang et al., 2017). Such methods are often incorporated with the variational process (Jain et al., 2017; Krishna et al., 2019). Visual question generation is also conducted together with visual question answering (Li et al., 2018; Sun et al., 2020). Visual Question Answering has received more We conduct experiments on two datasets, interest thanks to available public datasets such as VQA2.0 (G"
2021.findings-emnlp.91,P19-1427,0,0.0235737,"el strategy to scale the region representation. The core idea is that since the target QAP is tightly related to only a few regions in the image and its information is included in the latent variable, we can scale the region representations to highlight those related and weaken those unrelated before decoding. Specifically, we assign a weight to the representations of each region: hi = wi hi h i wi = min 1, R × Softmax(M(Z)> hi ) . Consistency Evaluator Consistency evaluation is critical in our work. However, there is no existing automatic metrics. Inspired by the work in semantic evaluation (Wieting et al., 2019), we devise an evaluation model to measure the consistency of generated QAPs with given image: s = Sigmoid ◦ M ◦ f (hQ ⊕ hA , hI ). Both the pipeline model and the joint model are redundant since they require separate modules to generate QAPs. And errors introduced by the pipeline or separate training could result in inconsistency. Therefore, we propose a sequential model with just one decoder as shown in Figure 2. The sequential model concatenates question and answer as an integral sentence and inserts a reserved token “<sep>"" between them. This sentence is denoted as T . Similar to the pipel"
2021.inlg-1.33,2021.findings-acl.449,1,0.400734,"ss on summarizing monologic texts, such as news articles (Liu and Lapata, 2019; Gehrmann et al., 2018), patents (Pilault et al., 2020) and academic papers (Koncel-Kedziorski et al., 2019). However, dialogue, as an important channel for achieving communicative intents, differs from monologic texts in nature and has received significantly less attention from the summarization research community. A major reason is the paucity of suitable dialogue summarization datasets. To cope with this problem, we build a large scale labeled summarization dataset for real-life scenario dialogues, D IALOG S UM (Chen et al., 2021). An example from D IALOG S UM is shown in Figure 1. Compared with existing dialogue summariztaion datasets (Carletta et al., 2005; Gliwa et al., 2019; Zhong et al., 2021; Zhu et al., 2021), D IALOG S UM is useful for training neural models and is staying in the spoken domain as opposed to the written chat domain. Also, it contains diverse task-oriented dialogues that cover a wide range of daily-life topics. Summarizing those dialogues is useful for both business (e.g. help a business find common needs) and personal uses (e.g. track important events as 308 Proceedings of the 14th International"
2021.inlg-1.33,2020.acl-main.130,1,0.828823,"mmary2 to Summary3 Average to not only summarize what speakers are saying, but also what they are doing. 3 Task Description The task for participants is to provide a model that generates a summary given the input dialogue text. Both automatic and manual evaluation will be conducted to measure model performance. 3.1 Data The participant of DialogSum Challenge can start immediately, as the D IALOG S UM dataset has been already public 1 . We collect 13,460 dialogue data for D IALOG S UM from three public dialogue corpora, namely Dailydialog (Li et al., 2017), DREAM (Sun et al., 2019) and MuTual (Cui et al., 2020), as well as an English speaking practice website. In term of size, D IALOG S UM is comparable with SAMSum while its average dialogue length is much longer than SAMSum, which comforts the purpose of summarization and is thus more challenging. The dialogue data cover a wide range of daily-life topics, including diverse task-oriented scenarios. We ask annotators to summarize the dialogue from an observer’s perspective. To ensure the annotation quality, each summary has been checked twice by different people, where the reward and punishment mechanism is included. We also sample and check the data"
2021.inlg-1.33,D18-1443,0,0.0119614,"s unique challenges in dialogue summarization, we will manually evaluate system-generated summaries from multiple aspects designed for dialogue summarization, including coreference information, discourse relation, intent identification and objective description. An example is shown in Figure 1, where the summary describes main events in a business conversation. 2 Motivation Thanks to the advance in neural network models, and availability of large scale labeled datasets, recent research has achieved promising progress on summarizing monologic texts, such as news articles (Liu and Lapata, 2019; Gehrmann et al., 2018), patents (Pilault et al., 2020) and academic papers (Koncel-Kedziorski et al., 2019). However, dialogue, as an important channel for achieving communicative intents, differs from monologic texts in nature and has received significantly less attention from the summarization research community. A major reason is the paucity of suitable dialogue summarization datasets. To cope with this problem, we build a large scale labeled summarization dataset for real-life scenario dialogues, D IALOG S UM (Chen et al., 2021). An example from D IALOG S UM is shown in Figure 1. Compared with existing dialogue"
2021.inlg-1.33,D19-5409,0,0.0174606,"pers (Koncel-Kedziorski et al., 2019). However, dialogue, as an important channel for achieving communicative intents, differs from monologic texts in nature and has received significantly less attention from the summarization research community. A major reason is the paucity of suitable dialogue summarization datasets. To cope with this problem, we build a large scale labeled summarization dataset for real-life scenario dialogues, D IALOG S UM (Chen et al., 2021). An example from D IALOG S UM is shown in Figure 1. Compared with existing dialogue summariztaion datasets (Carletta et al., 2005; Gliwa et al., 2019; Zhong et al., 2021; Zhu et al., 2021), D IALOG S UM is useful for training neural models and is staying in the spoken domain as opposed to the written chat domain. Also, it contains diverse task-oriented dialogues that cover a wide range of daily-life topics. Summarizing those dialogues is useful for both business (e.g. help a business find common needs) and personal uses (e.g. track important events as 308 Proceedings of the 14th International Conference on Natural Language Generation (INLG), pages 308–313, Aberdeen, Scotland, UK, 20-24 September 2021. ©2021 Association for Computational Li"
2021.inlg-1.33,J95-2003,0,0.883796,"gue. Dialogue information flow is intuitively reflected in the dialogue discourse structures (Wolf and Gibson, 2005), where two utterances can be closely related even there is a large distance between them. Such a phenomenon is common in procedures and negotiations. For example, in Figure 1, the penultimate utterance “... we’ll draft the agreement... and sign it...” is actually replying to the third utterance “What’s the answer?”, between which the utterances can be viewed as negotiation 309 process and conditions. Also, frequent coreference and ellipsis make dialogue difficult to understand (Grosz et al., 1995; Quan et al., 2019). For example, to generate “wrong” in the summary in Figure 2, the model needs to understand “I think you have added someone else’s (laundry service on my bill)”, where “my bill” refers to “#Person 2#’s bill”. These linguistic phenomena make dialogues difficult to encode using ordinary representation learning techonologies (Chen et al., 2021). Second, compared with monologic summarization, dialogues are summarized from an observer’s perspective, which requires summary to be objective. For example, in Figure 3, #Person 1#’s statements are actually awaiting to be confirmed. H"
2021.inlg-1.33,N19-1238,0,0.0357933,"Missing"
2021.inlg-1.33,D19-1051,0,0.0518932,"Missing"
2021.inlg-1.33,2020.emnlp-main.750,0,0.0552237,"Missing"
2021.inlg-1.33,I17-1099,0,0.0233617,"ed Summary Summary1 to Summary2 Summary1 to Summary3 Summary2 to Summary3 Average to not only summarize what speakers are saying, but also what they are doing. 3 Task Description The task for participants is to provide a model that generates a summary given the input dialogue text. Both automatic and manual evaluation will be conducted to measure model performance. 3.1 Data The participant of DialogSum Challenge can start immediately, as the D IALOG S UM dataset has been already public 1 . We collect 13,460 dialogue data for D IALOG S UM from three public dialogue corpora, namely Dailydialog (Li et al., 2017), DREAM (Sun et al., 2019) and MuTual (Cui et al., 2020), as well as an English speaking practice website. In term of size, D IALOG S UM is comparable with SAMSum while its average dialogue length is much longer than SAMSum, which comforts the purpose of summarization and is thus more challenging. The dialogue data cover a wide range of daily-life topics, including diverse task-oriented scenarios. We ask annotators to summarize the dialogue from an observer’s perspective. To ensure the annotation quality, each summary has been checked twice by different people, where the reward and punishment"
2021.inlg-1.33,W04-1013,0,0.0804982,"#Person1# gives #Person_2# a wrong bill at first then corrects it. Figure 3: Selected case from D IALOG S UM dataset. Figure 2: Selected case from D IALOG S UM dataset. personal assistants). Empirical study and analysis demonstrate challenges in real-life scenario dialogue summarization (Chen et al., 2021). To highlight the challenges in dialogue summarization, we propose real-life scenario dialogue summarization challenge, DialogSum Challenge, to encourage researchers to investigate such problems. The evaluation for dialogue summarization contains both automatic evaluation, i.e. ROUGE score (Lin, 2004) and BERTScore (Zhang et al., 2019), and human evaluation from multiple aspects to address corresponding challenges (c.f. Section 2.1 and Section 3.3.2). For human evaluation, we anonymize the submitted models, and evaluate them on corresponding hidden sub-test sets to ensure the fairness. 2.1 Unique Challenges in D IALOG S UM Although dialogue summarization is in line with the philosophy of monologue summarization, we find some unique challenges in dialogue summarization. First, because of special linguistic phenomena, the dialogue on the source side differs from monologue. Dialogue informati"
2021.inlg-1.33,P19-1500,1,0.846327,"particular, to address unique challenges in dialogue summarization, we will manually evaluate system-generated summaries from multiple aspects designed for dialogue summarization, including coreference information, discourse relation, intent identification and objective description. An example is shown in Figure 1, where the summary describes main events in a business conversation. 2 Motivation Thanks to the advance in neural network models, and availability of large scale labeled datasets, recent research has achieved promising progress on summarizing monologic texts, such as news articles (Liu and Lapata, 2019; Gehrmann et al., 2018), patents (Pilault et al., 2020) and academic papers (Koncel-Kedziorski et al., 2019). However, dialogue, as an important channel for achieving communicative intents, differs from monologic texts in nature and has received significantly less attention from the summarization research community. A major reason is the paucity of suitable dialogue summarization datasets. To cope with this problem, we build a large scale labeled summarization dataset for real-life scenario dialogues, D IALOG S UM (Chen et al., 2021). An example from D IALOG S UM is shown in Figure 1. Compare"
2021.inlg-1.33,2020.emnlp-main.748,0,0.0156833,"ummarization, we will manually evaluate system-generated summaries from multiple aspects designed for dialogue summarization, including coreference information, discourse relation, intent identification and objective description. An example is shown in Figure 1, where the summary describes main events in a business conversation. 2 Motivation Thanks to the advance in neural network models, and availability of large scale labeled datasets, recent research has achieved promising progress on summarizing monologic texts, such as news articles (Liu and Lapata, 2019; Gehrmann et al., 2018), patents (Pilault et al., 2020) and academic papers (Koncel-Kedziorski et al., 2019). However, dialogue, as an important channel for achieving communicative intents, differs from monologic texts in nature and has received significantly less attention from the summarization research community. A major reason is the paucity of suitable dialogue summarization datasets. To cope with this problem, we build a large scale labeled summarization dataset for real-life scenario dialogues, D IALOG S UM (Chen et al., 2021). An example from D IALOG S UM is shown in Figure 1. Compared with existing dialogue summariztaion datasets (Carlett"
2021.inlg-1.33,D19-1462,0,0.0201156,"ation flow is intuitively reflected in the dialogue discourse structures (Wolf and Gibson, 2005), where two utterances can be closely related even there is a large distance between them. Such a phenomenon is common in procedures and negotiations. For example, in Figure 1, the penultimate utterance “... we’ll draft the agreement... and sign it...” is actually replying to the third utterance “What’s the answer?”, between which the utterances can be viewed as negotiation 309 process and conditions. Also, frequent coreference and ellipsis make dialogue difficult to understand (Grosz et al., 1995; Quan et al., 2019). For example, to generate “wrong” in the summary in Figure 2, the model needs to understand “I think you have added someone else’s (laundry service on my bill)”, where “my bill” refers to “#Person 2#’s bill”. These linguistic phenomena make dialogues difficult to encode using ordinary representation learning techonologies (Chen et al., 2021). Second, compared with monologic summarization, dialogues are summarized from an observer’s perspective, which requires summary to be objective. For example, in Figure 3, #Person 1#’s statements are actually awaiting to be confirmed. Human annotators iden"
2021.inlg-1.33,Q19-1014,0,0.0280322,"mmary2 Summary1 to Summary3 Summary2 to Summary3 Average to not only summarize what speakers are saying, but also what they are doing. 3 Task Description The task for participants is to provide a model that generates a summary given the input dialogue text. Both automatic and manual evaluation will be conducted to measure model performance. 3.1 Data The participant of DialogSum Challenge can start immediately, as the D IALOG S UM dataset has been already public 1 . We collect 13,460 dialogue data for D IALOG S UM from three public dialogue corpora, namely Dailydialog (Li et al., 2017), DREAM (Sun et al., 2019) and MuTual (Cui et al., 2020), as well as an English speaking practice website. In term of size, D IALOG S UM is comparable with SAMSum while its average dialogue length is much longer than SAMSum, which comforts the purpose of summarization and is thus more challenging. The dialogue data cover a wide range of daily-life topics, including diverse task-oriented scenarios. We ask annotators to summarize the dialogue from an observer’s perspective. To ensure the annotation quality, each summary has been checked twice by different people, where the reward and punishment mechanism is included. We"
2021.inlg-1.33,2020.inlg-1.30,0,0.0959482,"Missing"
2021.inlg-1.33,W18-6538,0,0.0601786,"Missing"
2021.inlg-1.33,J05-2005,0,0.0696102,"aspects to address corresponding challenges (c.f. Section 2.1 and Section 3.3.2). For human evaluation, we anonymize the submitted models, and evaluate them on corresponding hidden sub-test sets to ensure the fairness. 2.1 Unique Challenges in D IALOG S UM Although dialogue summarization is in line with the philosophy of monologue summarization, we find some unique challenges in dialogue summarization. First, because of special linguistic phenomena, the dialogue on the source side differs from monologue. Dialogue information flow is intuitively reflected in the dialogue discourse structures (Wolf and Gibson, 2005), where two utterances can be closely related even there is a large distance between them. Such a phenomenon is common in procedures and negotiations. For example, in Figure 1, the penultimate utterance “... we’ll draft the agreement... and sign it...” is actually replying to the third utterance “What’s the answer?”, between which the utterances can be viewed as negotiation 309 process and conditions. Also, frequent coreference and ellipsis make dialogue difficult to understand (Grosz et al., 1995; Quan et al., 2019). For example, to generate “wrong” in the summary in Figure 2, the model needs"
2021.inlg-1.33,2020.acl-main.451,0,0.0413089,"n annotators to follow Chen et al. (2021)’s criteria and rate the summary on a 50 randomly selected sub-testset. However, system-generated summaries usually focus on the consequence of a dialogue, and fail to correctly identify interlocutors’ intents. Therefore, we will conduct human evaluation on intent identification on the 50 randomly selected sub-testset following Chen et al. (2021). Discourse Relation Coherent summaries convey important relations between main events, and identifying discourse relations and using proper phrases to express them can be challenging for summarization systems (Xu et al., 2020). However, causally related events are usually not explicitly expressed, and the distance between them is long due to the unique dialogue discourse structure (Grosz et al., 1995). To quantify such challenge, we will conduct human evaluation on discourse relation following (Chen et al., 2021) on the discourse sub-testset. Objective Description In addition to the above evaluation aspects, we also find that models tend to take all interlocutors’ contents as ground truth while failing to reason whether their statements are just subjective assumptions or even defended to be fake. Therefore, we will"
2021.inlg-1.33,2021.naacl-main.472,1,0.814203,"Missing"
2021.inlg-1.33,2021.naacl-main.474,1,0.653972,"owever, dialogue, as an important channel for achieving communicative intents, differs from monologic texts in nature and has received significantly less attention from the summarization research community. A major reason is the paucity of suitable dialogue summarization datasets. To cope with this problem, we build a large scale labeled summarization dataset for real-life scenario dialogues, D IALOG S UM (Chen et al., 2021). An example from D IALOG S UM is shown in Figure 1. Compared with existing dialogue summariztaion datasets (Carletta et al., 2005; Gliwa et al., 2019; Zhong et al., 2021; Zhu et al., 2021), D IALOG S UM is useful for training neural models and is staying in the spoken domain as opposed to the written chat domain. Also, it contains diverse task-oriented dialogues that cover a wide range of daily-life topics. Summarizing those dialogues is useful for both business (e.g. help a business find common needs) and personal uses (e.g. track important events as 308 Proceedings of the 14th International Conference on Natural Language Generation (INLG), pages 308–313, Aberdeen, Scotland, UK, 20-24 September 2021. ©2021 Association for Computational Linguistics Dialogue from DIALOGSUM: #Per"
2021.inlg-1.9,P84-1044,0,0.71927,"Missing"
2021.inlg-1.9,D18-1076,0,0.0267875,"dge sentences per dialogue turn, and demonstrates significantly more knowledge diversity. Using WOW++, we then benchmark a number of different knowledge ranking algorithms using both standard information retrieval automatic measures as well as extrinsic human evaluation on generated responses. Our results indicate that neural rerankers using WOW++ are able to outperform other algorithms such as traditional IR baselines and neural models trained using the original WOW data. 2 took a tremendous leap forward with the introduction of standard datasets of knowledge-grounded dialogues. The work of (Zhou et al., 2018; Dinan et al., 2019; Gopalakrishnan et al., 2019) produced such corpora with upwards of 10K dialogues and up to 10s of dialogue turns leveraging knowledge from diverse sources such as Wikipedia, and the Washington Post. While certainly a step forward, these datasets introduced some unreasonable data constraints that aren’t apt to the knowledge setting such as either no explicitly-annotated knowledge snippets or only a single one, making training of robust knowledge selection systems very difficult. Since the introduction of these corpora, numerous groups have tackled the knowledge selection p"
2021.inlg-1.9,2020.eval4nlp-1.1,0,0.0118077,"ational systems (Ram et al., 2018; Khatri et al., 2018; Gabriel et al., 2020). In one line of work, a number of industry research groups have demonstrated that large quantities of chat data coupled with the latest highcapacity Transformer-based models can produce particularly engaging and convincing conversational experiences (Adiwardana et al., 2020; Roller et al., 2020). While these models produce impressive outputs, they consciously shirk any explicit knowledge-selection mechanisms. Any knowledgeable appearance in their outputs tends to be a consequence of facts memorized in training data (Lux et al., 2020). In addition, the models have a tendency to generate facts that may be factually inaccurate, referred to as factual hallucination. Knowledge selection in open-domain systems 3 WOW++ The WOW++ dataset we describe below is an augmented dataset based on the Wizard of Wikipedia (WOW) corpus (Dinan et al., 2019). The WOW corpus consists of 22,311 dialogues containing 201,999 turns. The dialogues are comprised of two interlocutors who engage in chit chat on a given topic where one interlocutor is a knowledgeable expert in the topic. The expert, or wizard, is provided access to knowledge snippets fr"
2021.naacl-industry.4,C18-1139,0,0.151896,"e context information. • In an end2end evaluation, we demonstrate that incorporating ER information improves quality of neural response generation models in open domain conversations. 2 2.1 2.2 Neural Entity Linking NEL typically involves two tasks: recognizing named entities in a given text and then disamgibuating the entity mentions according to the knowledge base (KB). Researchers have shown great success in NER with the help of Convolutional Neural Networks (CNNs), Bidirectional Recurrent Neural Networks (Bi-RNNs), and attention mechanisms along with a CRF decoder (Chiu and Nichols, 2016; Akbik et al., 2018; Ghaddar and Langlais, 2018; Jiang et al., 2019; Baevski et al., 2019; Yamada et al., 2020). Deep neural networks (DNNs) are also dominant in entity resolution tasks. They are used to calculate the semantic similarity between the recognized entity mentions and the entities in the KB (Yamada et al., 2016; Ganea and Hofmann, 2017; Sil et al., 2018; Raiman and Raiman, 2018). However, previous NEL work has mainly focused on news or formal documents, which is different from open-domain dialogues in many aspects. Sentences in open-domain dialogues are more informal, making it more difficult to reco"
2021.naacl-industry.4,D19-1539,0,0.0448174,"Missing"
2021.naacl-industry.4,D19-1367,0,0.0213049,"n, we demonstrate that incorporating ER information improves quality of neural response generation models in open domain conversations. 2 2.1 2.2 Neural Entity Linking NEL typically involves two tasks: recognizing named entities in a given text and then disamgibuating the entity mentions according to the knowledge base (KB). Researchers have shown great success in NER with the help of Convolutional Neural Networks (CNNs), Bidirectional Recurrent Neural Networks (Bi-RNNs), and attention mechanisms along with a CRF decoder (Chiu and Nichols, 2016; Akbik et al., 2018; Ghaddar and Langlais, 2018; Jiang et al., 2019; Baevski et al., 2019; Yamada et al., 2020). Deep neural networks (DNNs) are also dominant in entity resolution tasks. They are used to calculate the semantic similarity between the recognized entity mentions and the entities in the KB (Yamada et al., 2016; Ganea and Hofmann, 2017; Sil et al., 2018; Raiman and Raiman, 2018). However, previous NEL work has mainly focused on news or formal documents, which is different from open-domain dialogues in many aspects. Sentences in open-domain dialogues are more informal, making it more difficult to recognize and disambiguate entities. In addition, si"
2021.naacl-industry.4,Q17-1010,0,0.0127932,"Missing"
2021.naacl-industry.4,D17-1230,0,0.223866,"i.e., NER and ER standalone performance, we conduct an extrinsic evaluation where NER and ER results are integrated in a knowledge grounded neural response generation model in an open domain conversation system and response quality is evaluated. Our major contributions can Introduction Building an informative open-domain conversational agent that can naturally interact with humans has been one of recent scientific research topics. Inspired by the development of neural networks, neural generation based conversation systems have made great progress (Sutskever et al., 2014; Vinyals and Le, 2015; Li et al., 2017; Wolf et al., 2019a; Zhou et al., 2020). However, one issue in such approaches is that the neural models often produce universal and less informative responses (Huang et al., 2020). To address this issue, previous work proposed to incorporate external information into the response generation models, such as topics (Xing et al., 2017) and emotions (Zhou et al., 2018a). One line of research investigates the use of external knowledge to enrich the information of the responses (Ghazvininejad et al., 2018; Young et al., 2018; Dinan et al., 2018; Gopalakrishnan et al., 2019; Meng et al., 2020). ∗ T"
2021.naacl-industry.4,Q16-1026,0,0.133604,"rt models that do not use context information. • In an end2end evaluation, we demonstrate that incorporating ER information improves quality of neural response generation models in open domain conversations. 2 2.1 2.2 Neural Entity Linking NEL typically involves two tasks: recognizing named entities in a given text and then disamgibuating the entity mentions according to the knowledge base (KB). Researchers have shown great success in NER with the help of Convolutional Neural Networks (CNNs), Bidirectional Recurrent Neural Networks (Bi-RNNs), and attention mechanisms along with a CRF decoder (Chiu and Nichols, 2016; Akbik et al., 2018; Ghaddar and Langlais, 2018; Jiang et al., 2019; Baevski et al., 2019; Yamada et al., 2020). Deep neural networks (DNNs) are also dominant in entity resolution tasks. They are used to calculate the semantic similarity between the recognized entity mentions and the entities in the KB (Yamada et al., 2016; Ganea and Hofmann, 2017; Sil et al., 2018; Raiman and Raiman, 2018). However, previous NEL work has mainly focused on news or formal documents, which is different from open-domain dialogues in many aspects. Sentences in open-domain dialogues are more informal, making it mo"
2021.naacl-industry.4,D17-1277,0,0.0589877,"Missing"
2021.naacl-industry.4,C18-1161,0,0.0244133,"n. • In an end2end evaluation, we demonstrate that incorporating ER information improves quality of neural response generation models in open domain conversations. 2 2.1 2.2 Neural Entity Linking NEL typically involves two tasks: recognizing named entities in a given text and then disamgibuating the entity mentions according to the knowledge base (KB). Researchers have shown great success in NER with the help of Convolutional Neural Networks (CNNs), Bidirectional Recurrent Neural Networks (Bi-RNNs), and attention mechanisms along with a CRF decoder (Chiu and Nichols, 2016; Akbik et al., 2018; Ghaddar and Langlais, 2018; Jiang et al., 2019; Baevski et al., 2019; Yamada et al., 2020). Deep neural networks (DNNs) are also dominant in entity resolution tasks. They are used to calculate the semantic similarity between the recognized entity mentions and the entities in the KB (Yamada et al., 2016; Ganea and Hofmann, 2017; Sil et al., 2018; Raiman and Raiman, 2018). However, previous NEL work has mainly focused on news or formal documents, which is different from open-domain dialogues in many aspects. Sentences in open-domain dialogues are more informal, making it more difficult to recognize and disambiguate entit"
2021.naacl-industry.4,P19-1001,0,0.0128277,"n, since conversations are multi-turn, the semantic information in the current utterance is ambiguous and context needs to be considered. In this paper, we investigate NEL in open-domain conversational data and propose context-aware NER and ER models. Related Work Open-domain Conversation System Inspired by the availability of conversational data and the prosperity of neural networks, building open-domain conversation systems by data-driven approaches has achieved great progress. Previous methods can be roughly divided into two categories, retrieval-based (Zhang et al., 2018; Wu et al., 2019; Tao et al., 2019) and generation-based (Vinyals and Le, 2015; Li et al., 2017; Asghar et al., 2018; Tao et al., 2018). Chen et al. (2017) point out that conventional sequence-to-sequence methods tend to generate trivial responses that lack information and diversity. To address this issue, a line of research proposes to incorporate external knowledge into the generation process. Most of the work in this line retrieves knowledge based on a search or retrieval step first, and followed by further reranking of retrieved relevant knowledge snippets (Ghazvininejad et al., 2018; Young et al., 2018; Zhou et al., 27 Sup"
2021.naacl-industry.4,2020.cl-1.2,0,0.026393,"Missing"
2021.naacl-industry.4,J19-1005,0,0.0208717,"ities. In addition, since conversations are multi-turn, the semantic information in the current utterance is ambiguous and context needs to be considered. In this paper, we investigate NEL in open-domain conversational data and propose context-aware NER and ER models. Related Work Open-domain Conversation System Inspired by the availability of conversational data and the prosperity of neural networks, building open-domain conversation systems by data-driven approaches has achieved great progress. Previous methods can be roughly divided into two categories, retrieval-based (Zhang et al., 2018; Wu et al., 2019; Tao et al., 2019) and generation-based (Vinyals and Le, 2015; Li et al., 2017; Asghar et al., 2018; Tao et al., 2018). Chen et al. (2017) point out that conventional sequence-to-sequence methods tend to generate trivial responses that lack information and diversity. To address this issue, a line of research proposes to incorporate external knowledge into the generation process. Most of the work in this line retrieves knowledge based on a search or retrieval step first, and followed by further reranking of retrieved relevant knowledge snippets (Ghazvininejad et al., 2018; Young et al., 2018;"
2021.naacl-industry.4,2020.emnlp-main.523,0,0.0271871,"formation improves quality of neural response generation models in open domain conversations. 2 2.1 2.2 Neural Entity Linking NEL typically involves two tasks: recognizing named entities in a given text and then disamgibuating the entity mentions according to the knowledge base (KB). Researchers have shown great success in NER with the help of Convolutional Neural Networks (CNNs), Bidirectional Recurrent Neural Networks (Bi-RNNs), and attention mechanisms along with a CRF decoder (Chiu and Nichols, 2016; Akbik et al., 2018; Ghaddar and Langlais, 2018; Jiang et al., 2019; Baevski et al., 2019; Yamada et al., 2020). Deep neural networks (DNNs) are also dominant in entity resolution tasks. They are used to calculate the semantic similarity between the recognized entity mentions and the entities in the KB (Yamada et al., 2016; Ganea and Hofmann, 2017; Sil et al., 2018; Raiman and Raiman, 2018). However, previous NEL work has mainly focused on news or formal documents, which is different from open-domain dialogues in many aspects. Sentences in open-domain dialogues are more informal, making it more difficult to recognize and disambiguate entities. In addition, since conversations are multi-turn, the semant"
2021.naacl-industry.4,K16-1025,0,0.0268942,"samgibuating the entity mentions according to the knowledge base (KB). Researchers have shown great success in NER with the help of Convolutional Neural Networks (CNNs), Bidirectional Recurrent Neural Networks (Bi-RNNs), and attention mechanisms along with a CRF decoder (Chiu and Nichols, 2016; Akbik et al., 2018; Ghaddar and Langlais, 2018; Jiang et al., 2019; Baevski et al., 2019; Yamada et al., 2020). Deep neural networks (DNNs) are also dominant in entity resolution tasks. They are used to calculate the semantic similarity between the recognized entity mentions and the entities in the KB (Yamada et al., 2016; Ganea and Hofmann, 2017; Sil et al., 2018; Raiman and Raiman, 2018). However, previous NEL work has mainly focused on news or formal documents, which is different from open-domain dialogues in many aspects. Sentences in open-domain dialogues are more informal, making it more difficult to recognize and disambiguate entities. In addition, since conversations are multi-turn, the semantic information in the current utterance is ambiguous and context needs to be considered. In this paper, we investigate NEL in open-domain conversational data and propose context-aware NER and ER models. Related Wo"
2021.naacl-industry.4,C18-1317,0,0.0260671,"and disambiguate entities. In addition, since conversations are multi-turn, the semantic information in the current utterance is ambiguous and context needs to be considered. In this paper, we investigate NEL in open-domain conversational data and propose context-aware NER and ER models. Related Work Open-domain Conversation System Inspired by the availability of conversational data and the prosperity of neural networks, building open-domain conversation systems by data-driven approaches has achieved great progress. Previous methods can be roughly divided into two categories, retrieval-based (Zhang et al., 2018; Wu et al., 2019; Tao et al., 2019) and generation-based (Vinyals and Le, 2015; Li et al., 2017; Asghar et al., 2018; Tao et al., 2018). Chen et al. (2017) point out that conventional sequence-to-sequence methods tend to generate trivial responses that lack information and diversity. To address this issue, a line of research proposes to incorporate external knowledge into the generation process. Most of the work in this line retrieves knowledge based on a search or retrieval step first, and followed by further reranking of retrieved relevant knowledge snippets (Ghazvininejad et al., 2018; You"
2021.naacl-industry.4,2020.emnlp-main.272,0,0.0426753,"1, 2021. ©2021 Association for Computational Linguistics Figure 1: An example dialog illustrating the pipeline of NER, ER, and response generation. The bold sentence in the utterances is the current utterance and the previous utterances are the context. The current utterance and its context are fed to the NER module to identify the entity mentions. Then the ER module takes the entity mentions and all the sentences as input to resolve the entity. The response generation module produces an output based on the knowledge entity information and the dialog input. 2018b; Gopalakrishnan et al., 2019; Zhao et al., 2020). In our work, we propose neural entity recognition and linking to identify and resolve entities more accurately in order to obtain more relevant knowledge for knowledge grounded response generation. be summarized as follows: • We propose neural network based contextaware models for NER and ER respectively in open domain conversations. • Experimental results on different conversation datasets show that our proposed context-aware NER and ER models outperform other stateof-the-art models that do not use context information. • In an end2end evaluation, we demonstrate that incorporating ER informa"
2021.naacl-main.437,Q17-1010,0,0.0227941,"und truth deﬁnition are Hyper-parameters: We tune hyper-parameters to achieve the best BLEU score on the validation set. covered by the predicted deﬁnition. The overall We use Adam (Kingma and Ba, 2015) with an ini- metric measures the overall quality of the predicted deﬁnition, referencing the ground-truth deﬁnition. tial learning rate of 10−3 as the optimizer. We set We randomly select 100 entries from the test set, hidden size to 300, batch size to 64 and dropout rate to 0.2. Word embeddings are 300-dimensional, and hire three raters to rate the predicted deﬁnitions pretrained by fastText (Bojanowski et al., 2017). on a scale of 1 to 5, where each entry includes (1) the source word, (2) the ground-truth deﬁnition, We train for up to 50 epochs, and early stop the and (3) the predicted deﬁnition to the raters. We training process once the performance does not show in Table 5 the detailed guideline for raters on improve for 10 consecutive epochs. We run our each point. experiments on a single NVIDIA GeForce GTX 2080Ti GPU with 11 GB memory. The inter-rater kappa (Fleiss and Cohen, 1973) is 0.65 for coverage and 0.66 for overall. We average Baselines: We compare with two reproducible baselines that have a"
2021.naacl-main.437,P18-2043,0,0.0608041,"Missing"
2021.naacl-main.437,N19-1350,0,0.0917766,"ically Fuses different features through a gating mechanism, and generaTes word definitions. Experimental results show that our method is both effective and robust. 1 1 Introduction Deﬁnition Generation (DG) aims at automatically generating an explanatory text for a word. This task is of practical importance to assist dictionary construction, especially in highly productive languages like Chinese (Yang et al., 2020). Most existing methods take the source word as an indecomposable lexico-semantic unit, using features like word embedding (Noraset et al., 2017) and context (Gadetsky et al., 2018; Ishiwatari et al., 2019). Recently, Yang et al. (2020) and Li et al. (2020) achieve improvement by decomposing the word meaning into different semantic components. In decomposing the word meaning, the word formation process is an intuitive and informative way that has not been explored in DG by far. For parataxis languages like Chinese, a word is formed by formation components, i.e., morphemes, and Word 䕾冲 Word Definition 䕾冲䕼共䖃冰ȼ (White flower.) Formation Rule Modifier-Head 䕾冲 䕼䕼☯冰尸ȼ (Vainly spend.) Morphemes: Definitions 䕾1: 䕼共䖃 (white) 冲1: 冰㘴 (flower) Adverb-Verb 䕾2: 䕼䕼☯ (vainly) 冲2: 冰尸 (spend) Figure 1: Word"
2021.naacl-main.437,2020.acl-main.65,0,0.333516,"and generaTes word definitions. Experimental results show that our method is both effective and robust. 1 1 Introduction Deﬁnition Generation (DG) aims at automatically generating an explanatory text for a word. This task is of practical importance to assist dictionary construction, especially in highly productive languages like Chinese (Yang et al., 2020). Most existing methods take the source word as an indecomposable lexico-semantic unit, using features like word embedding (Noraset et al., 2017) and context (Gadetsky et al., 2018; Ishiwatari et al., 2019). Recently, Yang et al. (2020) and Li et al. (2020) achieve improvement by decomposing the word meaning into different semantic components. In decomposing the word meaning, the word formation process is an intuitive and informative way that has not been explored in DG by far. For parataxis languages like Chinese, a word is formed by formation components, i.e., morphemes, and Word 䕾冲 Word Definition 䕾冲䕼共䖃冰ȼ (White flower.) Formation Rule Modifier-Head 䕾冲 䕼䕼☯冰尸ȼ (Vainly spend.) Morphemes: Definitions 䕾1: 䕼共䖃 (white) 冲1: 冰㘴 (flower) Adverb-Verb 䕾2: 䕼䕼☯ (vainly) 冲2: 冰尸 (spend) Figure 1: Word formation process for the polysemous &quot;白花&quot;. With mor"
2021.naacl-main.437,P18-2023,0,0.23046,"Missing"
2021.naacl-main.437,D19-1357,0,0.0366765,"Missing"
2021.naacl-main.437,N19-1097,0,0.152168,"–5531 June 6–11, 2021. ©2021 Association for Computational Linguistics Recent methods attempt to decompose the word meaning by using HowNet sememes (Yang et al., 2020) or modeling latent variables (Li et al., 2020). Semantic Components: To systematically deﬁne words, linguists decompose the word meaning into semantic components (Wierzbicka, 1996). Following this idea, HowNet (Dong and Dong, 2006) uses manually-created sememes to describe the semantic aspects of words. Recent studies also show that leveraging subword information produces better embeddings (Park et al., 2018; Lin and Liu, 2019; Zhu et al., 2019), but these methods lack a clear distinction among different formation rules. 3 Word Formation Process in Chinese It is linguistically motivated to explore the word formation process to better understand words. Instead of combining roots and afﬁxes, Chinese words are formed by characters in a parataxis way (Li et al., 2018). Here, we introduce two formation features and construct a formation-informed dataset. 3.1 Formation components and rules Chinese formation components are morphemes, deﬁned as the smallest meaning-bearing units (Zhu, 1982). Morphemes are unambiguous in representing word mea"
2021.naacl-main.472,N18-1150,1,0.850619,"ting models struggle in solving this task, highlighting the challenges the models face when generating query-based meeting summaries. We are releasing our dataset and baselines to support additional research in queryfocused meeting summarization. its strong variants and different training settings. 3) By human evaluation, we further pose the challenges of the new task, including the impact of different query types and factuality errors. 2 2.1 Related Work Text Summarization Most prior work in text summarization (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016; See et al., 2017; Celikyilmaz et al., 2018; Chen and Bansal, 2018; Zhong et al., 2019a; Xu and Durrett, 2019; Liu and Lapata, 2019; Lebanoff et al., 2019; Cho et al., 2019; Zhong et al., 2020; Wang et al., 2020; Xu et al., 2019; Jia et al., 2020) investigate how to generate better summaries on news article data, such as CNN/DailyMail (Hermann et al., 2015), Newsroom (Grusky et al., 2018), etc. Scientific paper summarization is another important branch (Cohan et al., 2018; Yasunaga et al., 2019; An et al., 2021). Our paper mainly focuses on meeting summarization, a more challenging task compared to news summarization. With the burst of"
2021.naacl-main.472,P18-1063,0,0.0367563,"lving this task, highlighting the challenges the models face when generating query-based meeting summaries. We are releasing our dataset and baselines to support additional research in queryfocused meeting summarization. its strong variants and different training settings. 3) By human evaluation, we further pose the challenges of the new task, including the impact of different query types and factuality errors. 2 2.1 Related Work Text Summarization Most prior work in text summarization (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016; See et al., 2017; Celikyilmaz et al., 2018; Chen and Bansal, 2018; Zhong et al., 2019a; Xu and Durrett, 2019; Liu and Lapata, 2019; Lebanoff et al., 2019; Cho et al., 2019; Zhong et al., 2020; Wang et al., 2020; Xu et al., 2019; Jia et al., 2020) investigate how to generate better summaries on news article data, such as CNN/DailyMail (Hermann et al., 2015), Newsroom (Grusky et al., 2018), etc. Scientific paper summarization is another important branch (Cohan et al., 2018; Yasunaga et al., 2019; An et al., 2021). Our paper mainly focuses on meeting summarization, a more challenging task compared to news summarization. With the burst of demand for meeting sum"
2021.naacl-main.472,2020.findings-emnlp.329,1,0.691909,"nsive experiments on ing strategy to generate meeting summaries. Wang 5906 and Cardie (2013) attempt to make use of decisions, action items and progress to generate the whole meeting summaries. Oya et al. (2014) leverages the relationship between summaries and the meeting transcripts to extract templates and generate summaries with the guidance of the templates. Shang et al. (2018) utilize multi-sentence compression techniques to generate summaries under an unsupervised setting. Li et al. (2019) attempt to incorporate multi-modal information to facilitate the meeting summarization. Zhu et al. (2020) propose a model which builds a hierarchical structure on word-level and turn-level information and uses news summary data to alleviate the inadequacy of meeting data. Unlike previous works, instead of merely generating summaries for the complete meeting, we propose a novel task where we focus on summarizing multi-granularity contents which cater to different people’s need for the entire meetings, and help people comprehensively understand meetings. 3 Data Construction In this section, we show how we collected meeting data from three different domains: academic meetings, product meetings, and"
2021.naacl-main.472,N12-1041,0,0.66189,"2006; Otterbacher et al., 2009; Wang et al., 2016; Litvak and Vanetik, 2017; Nema et al., 2017; Baumel et al., 2018; Ishigaki et al., 2020; Kulkarni et al., 2020; Laskar et al., 2020). However, the models focus on news (Dang, 2005, 2006), debate (Nema et al., 2017), and Wikipedia (Zhu et al., 2019). Meeting is also a genre of discourses where query-based summarization could be applied, but to our best knowledge, there are no works studying this direction. 2.3 Meeting Summarization Meeting summarization has attracted a lot of interOverall, our contributions are listed as follows: est recently (Chen and Metze, 2012; Wang and 1) We propose a new task, query-based multi- Cardie, 2013; Mehdad et al., 2013; Oya et al., domain meeting summarization, and build a new 2014; Shang et al., 2018; Li et al., 2019; Zhu et al., benchmark QMSum with a hierarchical annotation 2020; Koay et al., 2020). Specifically, Mehdad structure. 2) We design a locate-then-summarize et al. (2013) leverage entailment graphs and rankmodel and conduct comprehensive experiments on ing strategy to generate meeting summaries. Wang 5906 and Cardie (2013) attempt to make use of decisions, action items and progress to generate the whole meet"
2021.naacl-main.472,P19-1098,0,0.0151867,"are releasing our dataset and baselines to support additional research in queryfocused meeting summarization. its strong variants and different training settings. 3) By human evaluation, we further pose the challenges of the new task, including the impact of different query types and factuality errors. 2 2.1 Related Work Text Summarization Most prior work in text summarization (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016; See et al., 2017; Celikyilmaz et al., 2018; Chen and Bansal, 2018; Zhong et al., 2019a; Xu and Durrett, 2019; Liu and Lapata, 2019; Lebanoff et al., 2019; Cho et al., 2019; Zhong et al., 2020; Wang et al., 2020; Xu et al., 2019; Jia et al., 2020) investigate how to generate better summaries on news article data, such as CNN/DailyMail (Hermann et al., 2015), Newsroom (Grusky et al., 2018), etc. Scientific paper summarization is another important branch (Cohan et al., 2018; Yasunaga et al., 2019; An et al., 2021). Our paper mainly focuses on meeting summarization, a more challenging task compared to news summarization. With the burst of demand for meeting summarization, this task attracts more and more interests from academia (Wang and Cardie, 2013; Oya et al., 2"
2021.naacl-main.472,N16-1012,0,0.0336034,"s and analysis from different perspectives reveal that the existing models struggle in solving this task, highlighting the challenges the models face when generating query-based meeting summaries. We are releasing our dataset and baselines to support additional research in queryfocused meeting summarization. its strong variants and different training settings. 3) By human evaluation, we further pose the challenges of the new task, including the impact of different query types and factuality errors. 2 2.1 Related Work Text Summarization Most prior work in text summarization (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016; See et al., 2017; Celikyilmaz et al., 2018; Chen and Bansal, 2018; Zhong et al., 2019a; Xu and Durrett, 2019; Liu and Lapata, 2019; Lebanoff et al., 2019; Cho et al., 2019; Zhong et al., 2020; Wang et al., 2020; Xu et al., 2019; Jia et al., 2020) investigate how to generate better summaries on news article data, such as CNN/DailyMail (Hermann et al., 2015), Newsroom (Grusky et al., 2018), etc. Scientific paper summarization is another important branch (Cohan et al., 2018; Yasunaga et al., 2019; An et al., 2021). Our paper mainly focuses on meeting summarization, a mor"
2021.naacl-main.472,N18-2097,0,0.0198273,". 2 2.1 Related Work Text Summarization Most prior work in text summarization (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016; See et al., 2017; Celikyilmaz et al., 2018; Chen and Bansal, 2018; Zhong et al., 2019a; Xu and Durrett, 2019; Liu and Lapata, 2019; Lebanoff et al., 2019; Cho et al., 2019; Zhong et al., 2020; Wang et al., 2020; Xu et al., 2019; Jia et al., 2020) investigate how to generate better summaries on news article data, such as CNN/DailyMail (Hermann et al., 2015), Newsroom (Grusky et al., 2018), etc. Scientific paper summarization is another important branch (Cohan et al., 2018; Yasunaga et al., 2019; An et al., 2021). Our paper mainly focuses on meeting summarization, a more challenging task compared to news summarization. With the burst of demand for meeting summarization, this task attracts more and more interests from academia (Wang and Cardie, 2013; Oya et al., 2014; Shang et al., 2018; Zhu et al., 2020) and becomes an emerging branch of text summarization area. 2.2 Query-based Summarization Query-based summarization aims to generate a brief summary according to a source document and a given query. There are works studying this task (Daumé III and Marcu, 2006;"
2021.naacl-main.472,W06-0707,0,0.0593589,"Missing"
2021.naacl-main.472,P06-1039,0,0.208583,"Missing"
2021.naacl-main.472,N18-1065,0,0.039943,"Missing"
2021.naacl-main.472,2020.emnlp-main.295,0,0.0149003,"queryfocused meeting summarization. its strong variants and different training settings. 3) By human evaluation, we further pose the challenges of the new task, including the impact of different query types and factuality errors. 2 2.1 Related Work Text Summarization Most prior work in text summarization (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016; See et al., 2017; Celikyilmaz et al., 2018; Chen and Bansal, 2018; Zhong et al., 2019a; Xu and Durrett, 2019; Liu and Lapata, 2019; Lebanoff et al., 2019; Cho et al., 2019; Zhong et al., 2020; Wang et al., 2020; Xu et al., 2019; Jia et al., 2020) investigate how to generate better summaries on news article data, such as CNN/DailyMail (Hermann et al., 2015), Newsroom (Grusky et al., 2018), etc. Scientific paper summarization is another important branch (Cohan et al., 2018; Yasunaga et al., 2019; An et al., 2021). Our paper mainly focuses on meeting summarization, a more challenging task compared to news summarization. With the burst of demand for meeting summarization, this task attracts more and more interests from academia (Wang and Cardie, 2013; Oya et al., 2014; Shang et al., 2018; Zhu et al., 2020) and becomes an emerging branch o"
2021.naacl-main.472,W17-1004,0,0.108697,"of the Association for Computational Linguistics: Human Language Technologies, pages 5905–5921 June 6–11, 2021. ©2021 Association for Computational Linguistics standing out, while others may be more interested in what other attendees thought about different elements of the design. It is challenging to compress or compose a short summary that contains all the salient information. Alternatively, summarization systems should adopt a more flexible and interactive approach that allows people to express their interests and caters to their diverse intents when generating summaries (Dang, 2005, 2006; Litvak and Vanetik, 2017; Baumel et al., 2018). With comprehensive consideration of the multigranularity meeting contents, we propose a new task, query-based meeting summarization. To enable research in this area, we also create a highquality multi-domain summarization dataset. In this task, as shown in Figure 1, given a query and a meeting transcript, a model is required to generate the corresponding summary. The query-based approach is a flexible setup that enables the system to satisfy different intents and different levels of granularity. Besides the annotated queries and corresponding gold summaries at different"
2021.naacl-main.472,D18-1208,0,0.0649839,"Missing"
2021.naacl-main.472,D19-1387,1,0.849152,"generating query-based meeting summaries. We are releasing our dataset and baselines to support additional research in queryfocused meeting summarization. its strong variants and different training settings. 3) By human evaluation, we further pose the challenges of the new task, including the impact of different query types and factuality errors. 2 2.1 Related Work Text Summarization Most prior work in text summarization (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016; See et al., 2017; Celikyilmaz et al., 2018; Chen and Bansal, 2018; Zhong et al., 2019a; Xu and Durrett, 2019; Liu and Lapata, 2019; Lebanoff et al., 2019; Cho et al., 2019; Zhong et al., 2020; Wang et al., 2020; Xu et al., 2019; Jia et al., 2020) investigate how to generate better summaries on news article data, such as CNN/DailyMail (Hermann et al., 2015), Newsroom (Grusky et al., 2018), etc. Scientific paper summarization is another important branch (Cohan et al., 2018; Yasunaga et al., 2019; An et al., 2021). Our paper mainly focuses on meeting summarization, a more challenging task compared to news summarization. With the burst of demand for meeting summarization, this task attracts more and more interests from acade"
2021.naacl-main.472,D14-1181,0,0.00351051,", Pointer Network will point to the start turn and the end turn for each query. It is worth noting that one query can correspond to multiple spans in our dataset, so we always extract three spans as the corresponding text for each query when we use Pointer Network as Locator in the experiments. In addition, we design a hierarchical rankingbased model structure as the Locator. As shown in Figure 3, we first input the tokens in each turn to a feature-based BERT to obtain the word embedding, where feature-based means we fix the parameters of BERT, so it is actually an embedding layer. Next, CNN (Kim, 2014) is applied as a turn-level encoder to capture the local features such as bigram, 5 Experiments trigram and so on in each turn. Here we do not use Transformer because previous work (Kedzie et al., In this section, we introduce the implementation details, effectiveness of Locator, experimental results 2018) shows that this component does not matter and multi-domain experiments on QMSum. too much for the final performance. We combine different features to represent the utterance ui in 5.1 Implementation Details each turn, and concatenate the speaker embedding si as the turn-level representation:"
2021.naacl-main.472,2020.coling-main.499,0,0.0176046,"dia (Zhu et al., 2019). Meeting is also a genre of discourses where query-based summarization could be applied, but to our best knowledge, there are no works studying this direction. 2.3 Meeting Summarization Meeting summarization has attracted a lot of interOverall, our contributions are listed as follows: est recently (Chen and Metze, 2012; Wang and 1) We propose a new task, query-based multi- Cardie, 2013; Mehdad et al., 2013; Oya et al., domain meeting summarization, and build a new 2014; Shang et al., 2018; Li et al., 2019; Zhu et al., benchmark QMSum with a hierarchical annotation 2020; Koay et al., 2020). Specifically, Mehdad structure. 2) We design a locate-then-summarize et al. (2013) leverage entailment graphs and rankmodel and conduct comprehensive experiments on ing strategy to generate meeting summaries. Wang 5906 and Cardie (2013) attempt to make use of decisions, action items and progress to generate the whole meeting summaries. Oya et al. (2014) leverages the relationship between summaries and the meeting transcripts to extract templates and generate summaries with the guidance of the templates. Shang et al. (2018) utilize multi-sentence compression techniques to generate summaries u"
2021.naacl-main.472,P19-1209,0,0.0161457,"meeting summaries. We are releasing our dataset and baselines to support additional research in queryfocused meeting summarization. its strong variants and different training settings. 3) By human evaluation, we further pose the challenges of the new task, including the impact of different query types and factuality errors. 2 2.1 Related Work Text Summarization Most prior work in text summarization (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016; See et al., 2017; Celikyilmaz et al., 2018; Chen and Bansal, 2018; Zhong et al., 2019a; Xu and Durrett, 2019; Liu and Lapata, 2019; Lebanoff et al., 2019; Cho et al., 2019; Zhong et al., 2020; Wang et al., 2020; Xu et al., 2019; Jia et al., 2020) investigate how to generate better summaries on news article data, such as CNN/DailyMail (Hermann et al., 2015), Newsroom (Grusky et al., 2018), etc. Scientific paper summarization is another important branch (Cohan et al., 2018; Yasunaga et al., 2019; An et al., 2021). Our paper mainly focuses on meeting summarization, a more challenging task compared to news summarization. With the burst of demand for meeting summarization, this task attracts more and more interests from academia (Wang and Cardie, 2"
2021.naacl-main.472,2020.acl-main.703,0,0.0293968,"our goal in the second stage is to summarize the selected text spans based on the query. We instantiate our Summarizer with the current powerful abstractive models to explore whether the query-based meeting summarization task on our dataset is challenging. To be more specific, we choose the following three models: Pointer-Generator Network (See et al., 2017) is a popular sequence-to-sequence model with copy mechanism and coverage loss, and it acts as a baseline system in many generation tasks. The input to Pointer-Generator Network (PGNet) is: “&lt;s&gt; Query &lt;/s&gt; Relevant Text Spans &lt;/s&gt;”. BART (Lewis et al., 2020) is a denoising pretrained model for language generation, translation and comprehension. It has achieved new state-ofthe-art results on many generation tasks, including summarization and abstractive question answering. The input to BART is the same as PGNet. HMNet (Zhu et al., 2020) is the state-of-the-art meeting summarization model. It contains a hierarchical structure to process long meeting transcripts and a role vector to depict the difference among speakers. Besides, a cross-domain pretraining process is also included in this strong model. We add a turn representing the query at the begi"
2021.naacl-main.472,P19-1210,0,0.211278,"cussing user interface? User Interface Designer said the remote should perform standard features right out-of-the-box ...... Turn 121: User Interface Designer: The idea of having a remote is you have different keys and different structures. ...... Turn 162: Project Manager: Sure. Let&apos;s push forward the interface design. ...... Turn 316: Project Manager: Thanks. Have a nice day! Figure 1: Examples of query-based meeting summarization task. Users are interested in different facets of the meeting. In this task, a model is required to summarize the contents that users are interested in and query. Li et al., 2019; Zhu et al., 2020) is a task where summarization models are leveraged to generate summaries of entire meetings based on meeting transcripts. The resulting summaries distill the core contents of a meeting that helps people efficiently catch up to meetings. Most existing work and datasets on meeting summarization (Janin et al., 2003; Carletta et al., 2005) 1 Introduction pose the problem as a single document summarizaMeetings remain the go-to tool for collaboration, tion task where a single summary is generated for the whole meeting. Unlike news articles where with 11 million meetings taking pl"
2021.naacl-main.472,W13-2117,0,0.0344771,"2017; Baumel et al., 2018; Ishigaki et al., 2020; Kulkarni et al., 2020; Laskar et al., 2020). However, the models focus on news (Dang, 2005, 2006), debate (Nema et al., 2017), and Wikipedia (Zhu et al., 2019). Meeting is also a genre of discourses where query-based summarization could be applied, but to our best knowledge, there are no works studying this direction. 2.3 Meeting Summarization Meeting summarization has attracted a lot of interOverall, our contributions are listed as follows: est recently (Chen and Metze, 2012; Wang and 1) We propose a new task, query-based multi- Cardie, 2013; Mehdad et al., 2013; Oya et al., domain meeting summarization, and build a new 2014; Shang et al., 2018; Li et al., 2019; Zhu et al., benchmark QMSum with a hierarchical annotation 2020; Koay et al., 2020). Specifically, Mehdad structure. 2) We design a locate-then-summarize et al. (2013) leverage entailment graphs and rankmodel and conduct comprehensive experiments on ing strategy to generate meeting summaries. Wang 5906 and Cardie (2013) attempt to make use of decisions, action items and progress to generate the whole meeting summaries. Oya et al. (2014) leverages the relationship between summaries and the mee"
2021.naacl-main.472,P17-1098,0,0.0183837,"meeting summarization, a more challenging task compared to news summarization. With the burst of demand for meeting summarization, this task attracts more and more interests from academia (Wang and Cardie, 2013; Oya et al., 2014; Shang et al., 2018; Zhu et al., 2020) and becomes an emerging branch of text summarization area. 2.2 Query-based Summarization Query-based summarization aims to generate a brief summary according to a source document and a given query. There are works studying this task (Daumé III and Marcu, 2006; Otterbacher et al., 2009; Wang et al., 2016; Litvak and Vanetik, 2017; Nema et al., 2017; Baumel et al., 2018; Ishigaki et al., 2020; Kulkarni et al., 2020; Laskar et al., 2020). However, the models focus on news (Dang, 2005, 2006), debate (Nema et al., 2017), and Wikipedia (Zhu et al., 2019). Meeting is also a genre of discourses where query-based summarization could be applied, but to our best knowledge, there are no works studying this direction. 2.3 Meeting Summarization Meeting summarization has attracted a lot of interOverall, our contributions are listed as follows: est recently (Chen and Metze, 2012; Wang and 1) We propose a new task, query-based multi- Cardie, 2013; Mehd"
2021.naacl-main.472,W14-4407,0,0.133484,"et al., 2019; Zhong et al., 2020; Wang et al., 2020; Xu et al., 2019; Jia et al., 2020) investigate how to generate better summaries on news article data, such as CNN/DailyMail (Hermann et al., 2015), Newsroom (Grusky et al., 2018), etc. Scientific paper summarization is another important branch (Cohan et al., 2018; Yasunaga et al., 2019; An et al., 2021). Our paper mainly focuses on meeting summarization, a more challenging task compared to news summarization. With the burst of demand for meeting summarization, this task attracts more and more interests from academia (Wang and Cardie, 2013; Oya et al., 2014; Shang et al., 2018; Zhu et al., 2020) and becomes an emerging branch of text summarization area. 2.2 Query-based Summarization Query-based summarization aims to generate a brief summary according to a source document and a given query. There are works studying this task (Daumé III and Marcu, 2006; Otterbacher et al., 2009; Wang et al., 2016; Litvak and Vanetik, 2017; Nema et al., 2017; Baumel et al., 2018; Ishigaki et al., 2020; Kulkarni et al., 2020; Laskar et al., 2020). However, the models focus on news (Dang, 2005, 2006), debate (Nema et al., 2017), and Wikipedia (Zhu et al., 2019). Meet"
2021.naacl-main.472,D15-1044,0,0.0238088,"n QMSum. Our results and analysis from different perspectives reveal that the existing models struggle in solving this task, highlighting the challenges the models face when generating query-based meeting summaries. We are releasing our dataset and baselines to support additional research in queryfocused meeting summarization. its strong variants and different training settings. 3) By human evaluation, we further pose the challenges of the new task, including the impact of different query types and factuality errors. 2 2.1 Related Work Text Summarization Most prior work in text summarization (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016; See et al., 2017; Celikyilmaz et al., 2018; Chen and Bansal, 2018; Zhong et al., 2019a; Xu and Durrett, 2019; Liu and Lapata, 2019; Lebanoff et al., 2019; Cho et al., 2019; Zhong et al., 2020; Wang et al., 2020; Xu et al., 2019; Jia et al., 2020) investigate how to generate better summaries on news article data, such as CNN/DailyMail (Hermann et al., 2015), Newsroom (Grusky et al., 2018), etc. Scientific paper summarization is another important branch (Cohan et al., 2018; Yasunaga et al., 2019; An et al., 2021). Our paper mainly focuses on meeting"
2021.naacl-main.472,P17-1099,0,0.400999,"veal that the existing models struggle in solving this task, highlighting the challenges the models face when generating query-based meeting summaries. We are releasing our dataset and baselines to support additional research in queryfocused meeting summarization. its strong variants and different training settings. 3) By human evaluation, we further pose the challenges of the new task, including the impact of different query types and factuality errors. 2 2.1 Related Work Text Summarization Most prior work in text summarization (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016; See et al., 2017; Celikyilmaz et al., 2018; Chen and Bansal, 2018; Zhong et al., 2019a; Xu and Durrett, 2019; Liu and Lapata, 2019; Lebanoff et al., 2019; Cho et al., 2019; Zhong et al., 2020; Wang et al., 2020; Xu et al., 2019; Jia et al., 2020) investigate how to generate better summaries on news article data, such as CNN/DailyMail (Hermann et al., 2015), Newsroom (Grusky et al., 2018), etc. Scientific paper summarization is another important branch (Cohan et al., 2018; Yasunaga et al., 2019; An et al., 2021). Our paper mainly focuses on meeting summarization, a more challenging task compared to news summar"
2021.naacl-main.472,D19-1324,0,0.0115444,"the models face when generating query-based meeting summaries. We are releasing our dataset and baselines to support additional research in queryfocused meeting summarization. its strong variants and different training settings. 3) By human evaluation, we further pose the challenges of the new task, including the impact of different query types and factuality errors. 2 2.1 Related Work Text Summarization Most prior work in text summarization (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016; See et al., 2017; Celikyilmaz et al., 2018; Chen and Bansal, 2018; Zhong et al., 2019a; Xu and Durrett, 2019; Liu and Lapata, 2019; Lebanoff et al., 2019; Cho et al., 2019; Zhong et al., 2020; Wang et al., 2020; Xu et al., 2019; Jia et al., 2020) investigate how to generate better summaries on news article data, such as CNN/DailyMail (Hermann et al., 2015), Newsroom (Grusky et al., 2018), etc. Scientific paper summarization is another important branch (Cohan et al., 2018; Yasunaga et al., 2019; An et al., 2021). Our paper mainly focuses on meeting summarization, a more challenging task compared to news summarization. With the burst of demand for meeting summarization, this task attracts more and mor"
2021.naacl-main.472,P18-1062,0,0.35554,"ing a toll on our productivity and well- decisions (Wang and Cardie, 2013). This poses the being (Spataro, 2020). The proliferation of meet- question of whether a single paragraph is enough to summarize the content of an entire meeting? ings makes it hard to stay on top of this sheer Figure 1 shows an example of a meeting about volume of information and increases the need for automated methods for accessing key informa- “remote control design”. The discussions in the tion exchanged during them. Meeting summariza- meeting are multi-faceted and hence different users tion (Wang and Cardie, 2013; Shang et al., 2018; might be interested in different facets. For exam∗ ple, someone may be interested in learning about These two authors contributed equally. The order of authorship decided by the flip of a coin. the new trends that may lead to the new product 5905 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 5905–5921 June 6–11, 2021. ©2021 Association for Computational Linguistics standing out, while others may be more interested in what other attendees thought about different elements of the design. It i"
2021.naacl-main.472,2020.acl-main.553,1,0.826971,"s to support additional research in queryfocused meeting summarization. its strong variants and different training settings. 3) By human evaluation, we further pose the challenges of the new task, including the impact of different query types and factuality errors. 2 2.1 Related Work Text Summarization Most prior work in text summarization (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016; See et al., 2017; Celikyilmaz et al., 2018; Chen and Bansal, 2018; Zhong et al., 2019a; Xu and Durrett, 2019; Liu and Lapata, 2019; Lebanoff et al., 2019; Cho et al., 2019; Zhong et al., 2020; Wang et al., 2020; Xu et al., 2019; Jia et al., 2020) investigate how to generate better summaries on news article data, such as CNN/DailyMail (Hermann et al., 2015), Newsroom (Grusky et al., 2018), etc. Scientific paper summarization is another important branch (Cohan et al., 2018; Yasunaga et al., 2019; An et al., 2021). Our paper mainly focuses on meeting summarization, a more challenging task compared to news summarization. With the burst of demand for meeting summarization, this task attracts more and more interests from academia (Wang and Cardie, 2013; Oya et al., 2014; Shang et al., 2018; Zhu et al., 20"
2021.naacl-main.472,P13-1137,0,0.0731651,"Missing"
2021.naacl-main.472,2020.acl-main.552,1,0.832276,"dataset and baselines to support additional research in queryfocused meeting summarization. its strong variants and different training settings. 3) By human evaluation, we further pose the challenges of the new task, including the impact of different query types and factuality errors. 2 2.1 Related Work Text Summarization Most prior work in text summarization (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016; See et al., 2017; Celikyilmaz et al., 2018; Chen and Bansal, 2018; Zhong et al., 2019a; Xu and Durrett, 2019; Liu and Lapata, 2019; Lebanoff et al., 2019; Cho et al., 2019; Zhong et al., 2020; Wang et al., 2020; Xu et al., 2019; Jia et al., 2020) investigate how to generate better summaries on news article data, such as CNN/DailyMail (Hermann et al., 2015), Newsroom (Grusky et al., 2018), etc. Scientific paper summarization is another important branch (Cohan et al., 2018; Yasunaga et al., 2019; An et al., 2021). Our paper mainly focuses on meeting summarization, a more challenging task compared to news summarization. With the burst of demand for meeting summarization, this task attracts more and more interests from academia (Wang and Cardie, 2013; Oya et al., 2014; Shang et al., 2"
2021.naacl-main.472,P19-1100,1,0.888302,"ghting the challenges the models face when generating query-based meeting summaries. We are releasing our dataset and baselines to support additional research in queryfocused meeting summarization. its strong variants and different training settings. 3) By human evaluation, we further pose the challenges of the new task, including the impact of different query types and factuality errors. 2 2.1 Related Work Text Summarization Most prior work in text summarization (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016; See et al., 2017; Celikyilmaz et al., 2018; Chen and Bansal, 2018; Zhong et al., 2019a; Xu and Durrett, 2019; Liu and Lapata, 2019; Lebanoff et al., 2019; Cho et al., 2019; Zhong et al., 2020; Wang et al., 2020; Xu et al., 2019; Jia et al., 2020) investigate how to generate better summaries on news article data, such as CNN/DailyMail (Hermann et al., 2015), Newsroom (Grusky et al., 2018), etc. Scientific paper summarization is another important branch (Cohan et al., 2018; Yasunaga et al., 2019; An et al., 2021). Our paper mainly focuses on meeting summarization, a more challenging task compared to news summarization. With the burst of demand for meeting summarization, this tas"
2021.naacl-main.472,D19-5410,1,0.872427,"Missing"
2021.naacl-main.472,2020.findings-emnlp.19,0,0.764092,"rface? User Interface Designer said the remote should perform standard features right out-of-the-box ...... Turn 121: User Interface Designer: The idea of having a remote is you have different keys and different structures. ...... Turn 162: Project Manager: Sure. Let&apos;s push forward the interface design. ...... Turn 316: Project Manager: Thanks. Have a nice day! Figure 1: Examples of query-based meeting summarization task. Users are interested in different facets of the meeting. In this task, a model is required to summarize the contents that users are interested in and query. Li et al., 2019; Zhu et al., 2020) is a task where summarization models are leveraged to generate summaries of entire meetings based on meeting transcripts. The resulting summaries distill the core contents of a meeting that helps people efficiently catch up to meetings. Most existing work and datasets on meeting summarization (Janin et al., 2003; Carletta et al., 2005) 1 Introduction pose the problem as a single document summarizaMeetings remain the go-to tool for collaboration, tion task where a single summary is generated for the whole meeting. Unlike news articles where with 11 million meetings taking place each day people"
2021.naacl-main.56,N18-1150,0,0.0166175,"mated by matching the student’s predictions to the teacher. generator network which allows to copy words from the source text, and a coverage mechanism Let T and S denote teacher and student models, which keeps track of words that have been sum- respectively. Let fT and fS be functions of the marized. Other work develops abstractive mod- teacher and student. The models are typically neuels trained end-to-end with reinforcement learning ral networks and function f can be in principle debased on multiple encoders and hierarchical atten- fined using the output of any network layer (e.g., a tion (Celikyilmaz et al., 2018) or a coverage mech- hidden or softmax layer). Knowledge distillation anism where the decoder attends over previously methods are commonly expressed as minimizing 693 an objective function over training set X : X LKD = l(fT (xi ), fS (xi )) (1) xi ∈X where l() is a loss function that penalizes the difference between the teacher and the student. Specific instantiations of this general framework include minimizing the teacher/student difference based on output logits, intermediate hidden representations, attention maps, and derivatives of the loss to the input (Ba and Caruana, 2014; Romero et al"
2021.naacl-main.56,P19-1595,0,0.011335,"r (see the discussion in the next section). Knowledge distillation has been also shown to improve results for various NLP tasks. Tan et al. (2019) use it to transfer knowledge from BERT to smaller models, helping them approach or exceed the quality of much larger pretrained neural networks. Aside from distilling large models into smaller ones (Kim and Rush, 2016; Mou et al., 2016) or ensembles of models into single models (Kuncoro et al., 2016; Liu et al., 2019), knowledge distillation has been further used in multi-task learning, e.g., to teach a multi-task student from single-task teachers (Clark et al., 2019). 3 Self-Knowledge Distillation for Text Summarization Self-knowledge distillation refers to the special case where the teacher and student have identical neural network architectures. Surprisingly, perhaps, it has been consistently observed (Furlanello et al., 2018; Yang et al., 2019; Ahn et al., 2019; Liu et al., 2020) that students trained with self-knowledge distillation outperform their teachers by significant margins in several computer vision and language modeling tasks. Recent efforts have also focused on understanding why this happens, e.g., by observing that knowledge transferred by"
2021.naacl-main.56,N19-1423,0,0.0254685,"lthough the majority of summarization systems are composed of LSTM units, Narayan et al. (2018) and (PerezBeltrachini et al., 2019) propose abstractive models based on convolutional neural networks. Pretrained language models have recently emerged as a key technology for achieving impressive gains in abstractive summarization (Liu and Lapata, 2019; Lewis et al., 2020; Song et al., 2019). These models first pretrain a language model with self-supervised objectives on large corpora and then fine-tune it on summarization datasets. Liu and Lapata (2019) combine a pretrained encoder based on BERT (Devlin et al., 2019) with a randomly initialized decoder, demonstrating substantial gains on summarization performance. Song et al. (2019) pretrain an encoder-decoder framework to reconstruct (masked) fragments within a sentence and then fine-tune it on summarization datasets. In the same vein, Lewis et al. (2020) present BART, an encoder-decoder Transformer (Vaswani et al., 2017), pretrained by reconstructing a text corrupted with several arbitrary noising functions. Bao et al. (2020) design U NILMv2, a Transformer-based neural network pretrained as a pseudo-masked language model. Qi et al. (2020) introduce thei"
2021.naacl-main.56,P18-1195,0,0.0218311,"interpreting the teacher’s knowledge as importance weighting (Furlanello et al., 2018), by showing that early-stopping is crucial (Dong et al., 2019), and by studying how selfdistillation modifies regularization (Mobahi et al., 2020). For text summarization, we argue that selfknowledge distillation can potentially alleviate problems in conventional maximum likelihood training. Summarization models are typically trained on single reference document-summary pairs, however considering a single summary as the only correct reference during maximum likelihood training can harm model generalization (Elbayad et al., 2018) and is counter-intuitive. There can be multiple valid summaries for a source input (Harman and Over, 2004; Nenkova, 2006) and even the single reference summaries available are not entirely goldstandard due to the inherent noise in the automatic construction of large-scale summarization datasets (Kry´sci´nski et al., 2019). With self-knowledge distillation, teacher outputs provide softened distributions of the reference summaries, which can be viewed as an enrichment of the single reference setting and a reweighting of gold summaries to prevent the student from becoming overconfident in its pr"
2021.naacl-main.56,D18-1443,0,0.0393716,"ther with knowledge distillation improve model generalization and performance. We present experiments on several summarization benchmarks (Narayan et al., 2018; PerezBeltrachini et al., 2019; Hermann et al., 2015) covering single- and multi-document summarization settings as well as different types of summaries (e.g., verbose or more telegraphic). Across datasets, the proposed framework boosts the performance of pretrained and non-pretrained abstractive summarizers, achieving new state-of-the-art results. 2 2.1 Background Neural Abstractive Summarization generated words (Paulus et al., 2018). Gehrmann et al. (2018) follow a bottom-up approach where a content selector first determines which phrases in a source document should be part of the summary, and a copy mechanism is applied only to preselected phrases during decoding. Although the majority of summarization systems are composed of LSTM units, Narayan et al. (2018) and (PerezBeltrachini et al., 2019) propose abstractive models based on convolutional neural networks. Pretrained language models have recently emerged as a key technology for achieving impressive gains in abstractive summarization (Liu and Lapata, 2019; Lewis et al., 2020; Song et al., 2"
2021.naacl-main.56,N18-1065,0,0.0265958,"Missing"
2021.naacl-main.56,R19-1050,0,0.0623232,"Missing"
2021.naacl-main.56,W04-1003,0,0.150298,"rly-stopping is crucial (Dong et al., 2019), and by studying how selfdistillation modifies regularization (Mobahi et al., 2020). For text summarization, we argue that selfknowledge distillation can potentially alleviate problems in conventional maximum likelihood training. Summarization models are typically trained on single reference document-summary pairs, however considering a single summary as the only correct reference during maximum likelihood training can harm model generalization (Elbayad et al., 2018) and is counter-intuitive. There can be multiple valid summaries for a source input (Harman and Over, 2004; Nenkova, 2006) and even the single reference summaries available are not entirely goldstandard due to the inherent noise in the automatic construction of large-scale summarization datasets (Kry´sci´nski et al., 2019). With self-knowledge distillation, teacher outputs provide softened distributions of the reference summaries, which can be viewed as an enrichment of the single reference setting and a reweighting of gold summaries to prevent the student from becoming overconfident in its predictions. The standard objective for an abstractive summarization model is negative log likelihood: LNLL"
2021.naacl-main.56,P84-1044,0,0.399948,"Missing"
2021.naacl-main.56,D16-1139,0,0.187887,"ination (Song et al., 2018; Maynez et al., 2020), majority sequence-to-sequence models which are and struggling to identify which content units are trained in an end-to-end fashion with a maximum likelihood estimation loss (See et al., 2017; Celiky- salient (Tan et al., 2017). ilmaz et al., 2018; Paulus et al., 2018; Gehrmann In this paper, we propose to alleviate these probet al., 2018). lems by turning to knowledge distillation (Bucilu Despite promising results, there are specific char- et al., 2006; Ba and Caruana, 2014; Hinton et al., acteristics of the summarization task which ren- 2015; Kim and Rush, 2016). Knowledge distillader it ill-suited to standard sequence-to-sequence tion transfers knowledge from a larger “teacher” training. For instance, maximum-likelihood train- network to a smaller “student” model by training ing on single reference datasets might not be opti- the student to imitate the teacher’s outputs (in admal for summarization which is subject to a great dition to learning from the training data set). In 1 “born-again networks”, (Furlanello et al., 2018) the Our code is available at https://github.com/ nlpyang/NoisySumm. teacher and student have the same neural archi692 Proceedi"
2021.naacl-main.56,D19-1051,0,0.0744586,"Missing"
2021.naacl-main.56,D16-1180,0,0.109748,"lanello et al., 2018; Hahn and Choi, 2019) also sheds light on leveraging knowledge distillation for training a highperforming student model with the same size as the teacher (see the discussion in the next section). Knowledge distillation has been also shown to improve results for various NLP tasks. Tan et al. (2019) use it to transfer knowledge from BERT to smaller models, helping them approach or exceed the quality of much larger pretrained neural networks. Aside from distilling large models into smaller ones (Kim and Rush, 2016; Mou et al., 2016) or ensembles of models into single models (Kuncoro et al., 2016; Liu et al., 2019), knowledge distillation has been further used in multi-task learning, e.g., to teach a multi-task student from single-task teachers (Clark et al., 2019). 3 Self-Knowledge Distillation for Text Summarization Self-knowledge distillation refers to the special case where the teacher and student have identical neural network architectures. Surprisingly, perhaps, it has been consistently observed (Furlanello et al., 2018; Yang et al., 2019; Ahn et al., 2019; Liu et al., 2020) that students trained with self-knowledge distillation outperform their teachers by significant margins i"
2021.naacl-main.56,2020.acl-main.703,0,0.348304,"al., 2018). Gehrmann et al. (2018) follow a bottom-up approach where a content selector first determines which phrases in a source document should be part of the summary, and a copy mechanism is applied only to preselected phrases during decoding. Although the majority of summarization systems are composed of LSTM units, Narayan et al. (2018) and (PerezBeltrachini et al., 2019) propose abstractive models based on convolutional neural networks. Pretrained language models have recently emerged as a key technology for achieving impressive gains in abstractive summarization (Liu and Lapata, 2019; Lewis et al., 2020; Song et al., 2019). These models first pretrain a language model with self-supervised objectives on large corpora and then fine-tune it on summarization datasets. Liu and Lapata (2019) combine a pretrained encoder based on BERT (Devlin et al., 2019) with a randomly initialized decoder, demonstrating substantial gains on summarization performance. Song et al. (2019) pretrain an encoder-decoder framework to reconstruct (masked) fragments within a sentence and then fine-tune it on summarization datasets. In the same vein, Lewis et al. (2020) present BART, an encoder-decoder Transformer (Vaswani"
2021.naacl-main.56,W04-1013,0,0.0372504,"d on noisy data (+Noisy S). The second and third blocks in Table 1 include the results of pretrained models. To make comparisons fairer, we separate LARGE- (second block) from BASE-size (third block) pretrained models based on parameter size (shown within parentheses). With regard to LARGE-size models, we re5 Results port the results of three very strong summarization systems finetuned with U NI LMLARGE (Bao 5.1 Automatic Evaluation et al., 2020), BARTLARGE (Lewis et al., 2020), and We evaluated summarization quality automatically T511B (Raffel et al., 2019). Our BASE-size models using ROUGE (Lin, 2004). We report unigram and include BERTS UMBASE (Liu and Lapata, 2019), a 697 Models T RANSFORMER A BS +Noisy SKD U NI LMv2BASE +Noisy SKD CNN/DailyMail 20.8 21.4 23.7 24.8 XSum 32.7 33.6 38.7 39.9 Table 3: Factual correctness on CNN/DailyMail and XSum test set. +Noisy SKD are students trained on noisy signals and noisy data. CNN/DailyMail U NI LMv2BASE +Noisy SKD Succinct 0.47 0.53 Inform 0.40 0.60 Fluent 0.54 0.46 XSum U NI LMv2BASE +Noisy SKD Succinct 0.46 0.54 Inform 0.36 0.64 Fluent 0.53 0.47 WikiCatSum U NI LMv2BASE +Noisy SKD Company 0.62 0.38 Film 0.47 0.53 Animal 0.45 0.55 summarizer bas"
2021.naacl-main.56,2020.acl-main.537,0,0.0237099,"ls into smaller ones (Kim and Rush, 2016; Mou et al., 2016) or ensembles of models into single models (Kuncoro et al., 2016; Liu et al., 2019), knowledge distillation has been further used in multi-task learning, e.g., to teach a multi-task student from single-task teachers (Clark et al., 2019). 3 Self-Knowledge Distillation for Text Summarization Self-knowledge distillation refers to the special case where the teacher and student have identical neural network architectures. Surprisingly, perhaps, it has been consistently observed (Furlanello et al., 2018; Yang et al., 2019; Ahn et al., 2019; Liu et al., 2020) that students trained with self-knowledge distillation outperform their teachers by significant margins in several computer vision and language modeling tasks. Recent efforts have also focused on understanding why this happens, e.g., by observing that knowledge transferred by the teacher is localized mainly in higher layers and does not affect early (feature extraction) layers much (Gotmare et al., 2019), by interpreting the teacher’s knowledge as importance weighting (Furlanello et al., 2018), by showing that early-stopping is crucial (Dong et al., 2019), and by studying how selfdistillation"
2021.naacl-main.56,P19-1441,0,0.0194071,"Hahn and Choi, 2019) also sheds light on leveraging knowledge distillation for training a highperforming student model with the same size as the teacher (see the discussion in the next section). Knowledge distillation has been also shown to improve results for various NLP tasks. Tan et al. (2019) use it to transfer knowledge from BERT to smaller models, helping them approach or exceed the quality of much larger pretrained neural networks. Aside from distilling large models into smaller ones (Kim and Rush, 2016; Mou et al., 2016) or ensembles of models into single models (Kuncoro et al., 2016; Liu et al., 2019), knowledge distillation has been further used in multi-task learning, e.g., to teach a multi-task student from single-task teachers (Clark et al., 2019). 3 Self-Knowledge Distillation for Text Summarization Self-knowledge distillation refers to the special case where the teacher and student have identical neural network architectures. Surprisingly, perhaps, it has been consistently observed (Furlanello et al., 2018; Yang et al., 2019; Ahn et al., 2019; Liu et al., 2020) that students trained with self-knowledge distillation outperform their teachers by significant margins in several computer"
2021.naacl-main.56,D19-1387,1,0.813833,"ated words (Paulus et al., 2018). Gehrmann et al. (2018) follow a bottom-up approach where a content selector first determines which phrases in a source document should be part of the summary, and a copy mechanism is applied only to preselected phrases during decoding. Although the majority of summarization systems are composed of LSTM units, Narayan et al. (2018) and (PerezBeltrachini et al., 2019) propose abstractive models based on convolutional neural networks. Pretrained language models have recently emerged as a key technology for achieving impressive gains in abstractive summarization (Liu and Lapata, 2019; Lewis et al., 2020; Song et al., 2019). These models first pretrain a language model with self-supervised objectives on large corpora and then fine-tune it on summarization datasets. Liu and Lapata (2019) combine a pretrained encoder based on BERT (Devlin et al., 2019) with a randomly initialized decoder, demonstrating substantial gains on summarization performance. Song et al. (2019) pretrain an encoder-decoder framework to reconstruct (masked) fragments within a sentence and then fine-tune it on summarization datasets. In the same vein, Lewis et al. (2020) present BART, an encoder-decoder"
2021.naacl-main.56,P14-5010,0,0.00321573,"blicly released software. datasets (XSum and WikiCatSum) were created automatically following various assumptions about the correspondence of purported summaries to the source input. CNN/DailyMail contains news articles and associated highlights, i.e., a few bullet points written by journalists which give a brief overview of the article. We used the standard splits of Hermann et al. (2015) for training, validation, and testing (90,266/1,220/1,093 CNN documents and 196,961/12,148/10,397 DailyMail documents). We did not anonymize entities. Sentences were split with the Stanford CoreNLP toolkit (Manning et al., 2014) and the dataset was pre-processed following See et al. (2017). Input documents were truncated to 512 tokens. 2018). The target summary is the lead section of a Wikipedia article, and the source input are webpages related to this article. WikiCatSum (PerezBeltrachini et al., 2019) represents three domains from the original Wikisum dataset under the assumption that these vary in terms of the topics the summaries discuss and their linguistic characteristics. Aside from the summaries, the dataset contains the input webpages whose length is truncated to the first 800 tokens. WikiCatSum contains 62"
2021.naacl-main.56,2020.acl-main.173,0,0.0184305,"datasets (Sandhaus, 2008; et al., 2018; Perez-Beltrachini et al., 2019), multidocument summarization datasets are created by Hermann et al., 2015; Grusky et al., 2018; Narayan viewing lead sections in Wikipedia articles as sumet al., 2018) containing hundreds of thousands of document-summary pairs has driven the develop- maries of documents cited therein. The inherent ment of neural architectures for summarization. noise in the data collection process further hampers training with models often being prone to halluSeveral approaches have been proposed, in the vast cination (Song et al., 2018; Maynez et al., 2020), majority sequence-to-sequence models which are and struggling to identify which content units are trained in an end-to-end fashion with a maximum likelihood estimation loss (See et al., 2017; Celiky- salient (Tan et al., 2017). ilmaz et al., 2018; Paulus et al., 2018; Gehrmann In this paper, we propose to alleviate these probet al., 2018). lems by turning to knowledge distillation (Bucilu Despite promising results, there are specific char- et al., 2006; Ba and Caruana, 2014; Hinton et al., acteristics of the summarization task which ren- 2015; Kim and Rush, 2016). Knowledge distillader it il"
2021.naacl-main.56,K16-1028,0,0.028725,"am prediction. Neural approaches to abstractive summarization conceptualize the task as a sequence-to-sequence problem, where the encoder maps the sequence of tokens in the source document x = [x1 , ..., xn ] to a sequence of continuous representations 2.2 Knowledge Distillation z = [z1 , ..., zn ], and the decoder autoregressively generates the target summary y = (y1 , ..., ym ) Knowledge Distillation refers to a class of methods token-by-token, hence modeling the conditional for training a new smaller student network by learnprobability p(y1 , ..., ym |x1 , ..., xn ). Rush et al. (2015) and Nallapati et al. (2016) ing from a teacher network (in addition to learning were among the first to apply the neural encoder- from the training data). It is generally assumed that the teacher has been previously trained, and the padecoder architecture to text summarization. See et al. (2017) enhance this model with a pointer- rameters for the student are estimated by matching the student’s predictions to the teacher. generator network which allows to copy words from the source text, and a coverage mechanism Let T and S denote teacher and student models, which keeps track of words that have been sum- respectively. Le"
2021.naacl-main.56,D18-1206,1,0.898344,"multiple target referularize training. Furthermore, to better model ences, it is unrealistic to expect that multi-reference uncertainty during training, we introduce muldatasets can be created at scale for neural network tiple noise signals for both teacher and student models. We demonstrate experimentally on training. In fact, most popular benchmarks are colthree benchmarks that our framework boosts lated opportunistically, based on summaries which the performance of both pretrained and nononly loosely correspond to the source input. pretrained summarizers achieving state-of-theFor example, Narayan et al. (2018) create a art results.1 dataset by pairing the first sentence of a news article 1 Introduction with the rest of the document under the assumpAutomatic summarization has enjoyed renewed in- tion that the introductory sentence expresses the gist of the article. Grusky et al. (2018) pair artiterest in recent years, thanks to the popularity of cles with metadata available in HTML pages under neural network models and their ability to learn continuous representations without recourse to pre- the assumption that HTML tags (e.g., description) denote summary-like content. In other work (Liu processing"
2021.naacl-main.56,P19-1504,1,0.903175,"st of the document under the assumpAutomatic summarization has enjoyed renewed in- tion that the introductory sentence expresses the gist of the article. Grusky et al. (2018) pair artiterest in recent years, thanks to the popularity of cles with metadata available in HTML pages under neural network models and their ability to learn continuous representations without recourse to pre- the assumption that HTML tags (e.g., description) denote summary-like content. In other work (Liu processing tools or linguistic annotations. The availability of large-scale datasets (Sandhaus, 2008; et al., 2018; Perez-Beltrachini et al., 2019), multidocument summarization datasets are created by Hermann et al., 2015; Grusky et al., 2018; Narayan viewing lead sections in Wikipedia articles as sumet al., 2018) containing hundreds of thousands of document-summary pairs has driven the develop- maries of documents cited therein. The inherent ment of neural architectures for summarization. noise in the data collection process further hampers training with models often being prone to halluSeveral approaches have been proposed, in the vast cination (Song et al., 2018; Maynez et al., 2020), majority sequence-to-sequence models which are and"
2021.naacl-main.56,2020.findings-emnlp.217,0,0.0107355,"d on BERT (Devlin et al., 2019) with a randomly initialized decoder, demonstrating substantial gains on summarization performance. Song et al. (2019) pretrain an encoder-decoder framework to reconstruct (masked) fragments within a sentence and then fine-tune it on summarization datasets. In the same vein, Lewis et al. (2020) present BART, an encoder-decoder Transformer (Vaswani et al., 2017), pretrained by reconstructing a text corrupted with several arbitrary noising functions. Bao et al. (2020) design U NILMv2, a Transformer-based neural network pretrained as a pseudo-masked language model. Qi et al. (2020) introduce their own novel self-supervised task based on future n-gram prediction. Neural approaches to abstractive summarization conceptualize the task as a sequence-to-sequence problem, where the encoder maps the sequence of tokens in the source document x = [x1 , ..., xn ] to a sequence of continuous representations 2.2 Knowledge Distillation z = [z1 , ..., zn ], and the decoder autoregressively generates the target summary y = (y1 , ..., ym ) Knowledge Distillation refers to a class of methods token-by-token, hence modeling the conditional for training a new smaller student network by lear"
2021.naacl-main.56,D15-1044,0,0.0409884,"sk based on future n-gram prediction. Neural approaches to abstractive summarization conceptualize the task as a sequence-to-sequence problem, where the encoder maps the sequence of tokens in the source document x = [x1 , ..., xn ] to a sequence of continuous representations 2.2 Knowledge Distillation z = [z1 , ..., zn ], and the decoder autoregressively generates the target summary y = (y1 , ..., ym ) Knowledge Distillation refers to a class of methods token-by-token, hence modeling the conditional for training a new smaller student network by learnprobability p(y1 , ..., ym |x1 , ..., xn ). Rush et al. (2015) and Nallapati et al. (2016) ing from a teacher network (in addition to learning were among the first to apply the neural encoder- from the training data). It is generally assumed that the teacher has been previously trained, and the padecoder architecture to text summarization. See et al. (2017) enhance this model with a pointer- rameters for the student are estimated by matching the student’s predictions to the teacher. generator network which allows to copy words from the source text, and a coverage mechanism Let T and S denote teacher and student models, which keeps track of words that hav"
2021.naacl-main.56,P17-1099,0,0.634007,"ns in Wikipedia articles as sumet al., 2018) containing hundreds of thousands of document-summary pairs has driven the develop- maries of documents cited therein. The inherent ment of neural architectures for summarization. noise in the data collection process further hampers training with models often being prone to halluSeveral approaches have been proposed, in the vast cination (Song et al., 2018; Maynez et al., 2020), majority sequence-to-sequence models which are and struggling to identify which content units are trained in an end-to-end fashion with a maximum likelihood estimation loss (See et al., 2017; Celiky- salient (Tan et al., 2017). ilmaz et al., 2018; Paulus et al., 2018; Gehrmann In this paper, we propose to alleviate these probet al., 2018). lems by turning to knowledge distillation (Bucilu Despite promising results, there are specific char- et al., 2006; Ba and Caruana, 2014; Hinton et al., acteristics of the summarization task which ren- 2015; Kim and Rush, 2016). Knowledge distillader it ill-suited to standard sequence-to-sequence tion transfers knowledge from a larger “teacher” training. For instance, maximum-likelihood train- network to a smaller “student” model by training in"
2021.naacl-main.56,C18-1146,0,0.018774,"lity of large-scale datasets (Sandhaus, 2008; et al., 2018; Perez-Beltrachini et al., 2019), multidocument summarization datasets are created by Hermann et al., 2015; Grusky et al., 2018; Narayan viewing lead sections in Wikipedia articles as sumet al., 2018) containing hundreds of thousands of document-summary pairs has driven the develop- maries of documents cited therein. The inherent ment of neural architectures for summarization. noise in the data collection process further hampers training with models often being prone to halluSeveral approaches have been proposed, in the vast cination (Song et al., 2018; Maynez et al., 2020), majority sequence-to-sequence models which are and struggling to identify which content units are trained in an end-to-end fashion with a maximum likelihood estimation loss (See et al., 2017; Celiky- salient (Tan et al., 2017). ilmaz et al., 2018; Paulus et al., 2018; Gehrmann In this paper, we propose to alleviate these probet al., 2018). lems by turning to knowledge distillation (Bucilu Despite promising results, there are specific char- et al., 2006; Ba and Caruana, 2014; Hinton et al., acteristics of the summarization task which ren- 2015; Kim and Rush, 2016). Knowl"
2021.naacl-main.56,P17-1108,0,0.0575179,"Missing"
2021.nlp4convai-1.23,2021.deelio-1.2,0,0.0385273,"plicit background knowledge I and then produce a response R given both U and I. Extending most neural RG models that treat this as a conditional language modeling problem, we aim to learn the conditional probability distribution P (I, R|U ) by training on human dialogues. 2.1 Data Preparation Knowledge Schema We consider ConceptNet (Speer et al., 2017) as our knowledge schema, which is a large-scale crowdsourced commonsense knowledge base consisting of triples such as “buy, RelatedTo, money”. We have explored several other sources such as LMs trained to generate knowledge (Hwang et al., 2021; Becker et al., 2021) but observe much noise while aligning knowledge to dialogue turns. Dialogues We use dialogue datasets from Zhou ∗ et al. (2021) as they propose “common senseWork done while Pei Zhou was an intern at Amazon Alexa AI focused dialogues” by filtering three existing di251 Proceedings of the Third Workshop on Natural Language Processing for Conversational AI, pages 251–253 November 10, 2021. ©2021 Association for Computational Linguistics alogue datasets DailyDialog (Li et al., 2017), EmpatheticDialogues (Rashkin et al., 2019), MuTual (Cui et al., 2020) using ConceptNet triples, and also crowdsourc"
2021.nlp4convai-1.23,2020.emnlp-main.373,0,0.0374913,"Missing"
2021.nlp4convai-1.23,2020.acl-main.130,0,0.0242908,"to generate knowledge (Hwang et al., 2021; Becker et al., 2021) but observe much noise while aligning knowledge to dialogue turns. Dialogues We use dialogue datasets from Zhou ∗ et al. (2021) as they propose “common senseWork done while Pei Zhou was an intern at Amazon Alexa AI focused dialogues” by filtering three existing di251 Proceedings of the Third Workshop on Natural Language Processing for Conversational AI, pages 251–253 November 10, 2021. ©2021 Association for Computational Linguistics alogue datasets DailyDialog (Li et al., 2017), EmpatheticDialogues (Rashkin et al., 2019), MuTual (Cui et al., 2020) using ConceptNet triples, and also crowdsourced SocialIQA-prompted dialogues (Zhou et al., 2021). We extract dialogues that each has at least one matched triple from ConceptNet in one of its consecutive turn pairs, resulting in around 31k dialogues and 159k turns we can use for training instances (excluding the first turn). The total number of turn pairs that have at least one matched triple is around 57k. 2.2 Training Setup Each of our training instance is a sequence of tokens with three components: a dialog history U , implicit knowledge (empty or non-empty) I, and a response R. We enclose"
2021.nlp4convai-1.23,2020.acl-main.703,0,0.0211956,"Think Before You Speak: Learning to Generate Implicit Knowledge for Response Generation by Self-Talk Pei Zhou1∗ Behnam Hedayatnia2 Karthik Gopalakrishnan2 Seokhwan Kim2 Jay Pujara1 Xiang Ren1 Yang Liu2 Dilek Hakkani-Tur2 1 Department of Computer Science, University of Southern California 2 Amazon Alexa AI {peiz,jpujara,xiangren}@usc.edu, {behnam,karthgop,seokhwk,yangliud,hakkanit}@amazon.com 1 Introduction End-to-end response generation (RG) models based on pre-trained transformer-based (Vaswani et al., 2017) language models (LM) have shown to produce human-like responses (Zhang et al., 2020; Lewis et al., 2020; Roller et al., 2020). Humans, however, not only “just utter the right sentence”, but also contribute to the common ground consisting of mutual beliefs and common knowledge “whose truth is taken for granted as part of the background of the conversation” (Stalnaker, 1978; Clark and Schaefer, 1989; Clark and Brennan, 1991). For example, consider the utterance “I need to buy some flowers for my wife”, a potential appropriate response is “Perhaps you’d be interested in red roses”. To produce this response, the participant needs to understand relevant background knowledge such as “rose is a type o"
2021.nlp4convai-1.23,2020.acl-demos.30,0,0.0959559,"Missing"
2021.nlp4convai-1.23,2021.sigdial-1.13,1,0.882617,"RG models that first generate implicit commonsense inferences before producing a response given previous utterances. Inspired by inquiry-based discovery learning (Bruner, 1961) and the self-talk procedure (Shwartz et al., 2020), we encourage the RG model to talk with itself to elicit implicit knowledge before making a response. Collecting training data is challenging for our purpose since commonsense knowledge is by definition often omitted in conversations. We use ConceptNet triples (Speer et al., 2017) as the knowledge schema to represent knowledge and align common sense-focused dialogues (Zhou et al., 2021) with knowledge to train RG models. We conduct extensive human evaluation on different variants of our training procedure and find that models that generate implicit background knowledge before responding produce more grammatical, coherent, and engaging responses compared to RG models that directly generate responses. Further analysis shows that models can sometimes even learn to distinguish when it is necessary to self-talk to generate implicit knowledge, i.e., be aware of potential knowledge gaps in dialogues. We hope our findings encourage more future studies on making RG models better emul"
2021.nlp4convai-1.23,I17-1099,0,0.0227344,"o, money”. We have explored several other sources such as LMs trained to generate knowledge (Hwang et al., 2021; Becker et al., 2021) but observe much noise while aligning knowledge to dialogue turns. Dialogues We use dialogue datasets from Zhou ∗ et al. (2021) as they propose “common senseWork done while Pei Zhou was an intern at Amazon Alexa AI focused dialogues” by filtering three existing di251 Proceedings of the Third Workshop on Natural Language Processing for Conversational AI, pages 251–253 November 10, 2021. ©2021 Association for Computational Linguistics alogue datasets DailyDialog (Li et al., 2017), EmpatheticDialogues (Rashkin et al., 2019), MuTual (Cui et al., 2020) using ConceptNet triples, and also crowdsourced SocialIQA-prompted dialogues (Zhou et al., 2021). We extract dialogues that each has at least one matched triple from ConceptNet in one of its consecutive turn pairs, resulting in around 31k dialogues and 159k turns we can use for training instances (excluding the first turn). The total number of turn pairs that have at least one matched triple is around 57k. 2.2 Training Setup Each of our training instance is a sequence of tokens with three components: a dialog history U , i"
2021.nlp4convai-1.23,P19-1534,0,0.0205292,"er sources such as LMs trained to generate knowledge (Hwang et al., 2021; Becker et al., 2021) but observe much noise while aligning knowledge to dialogue turns. Dialogues We use dialogue datasets from Zhou ∗ et al. (2021) as they propose “common senseWork done while Pei Zhou was an intern at Amazon Alexa AI focused dialogues” by filtering three existing di251 Proceedings of the Third Workshop on Natural Language Processing for Conversational AI, pages 251–253 November 10, 2021. ©2021 Association for Computational Linguistics alogue datasets DailyDialog (Li et al., 2017), EmpatheticDialogues (Rashkin et al., 2019), MuTual (Cui et al., 2020) using ConceptNet triples, and also crowdsourced SocialIQA-prompted dialogues (Zhou et al., 2021). We extract dialogues that each has at least one matched triple from ConceptNet in one of its consecutive turn pairs, resulting in around 31k dialogues and 159k turns we can use for training instances (excluding the first turn). The total number of turn pairs that have at least one matched triple is around 57k. 2.2 Training Setup Each of our training instance is a sequence of tokens with three components: a dialog history U , implicit knowledge (empty or non-empty) I, an"
2021.nlp4convai-1.27,N19-1423,0,0.0486235,"Missing"
2021.nlp4convai-1.27,W17-5506,0,0.0485847,"Missing"
2021.nlp4convai-1.27,2020.nlp4convai-1.10,1,0.894996,"Missing"
2021.nlp4convai-1.27,2021.emnlp-main.552,0,0.0698861,"Missing"
2021.nlp4convai-1.27,2020.acl-main.456,1,0.868985,"Missing"
2021.nlp4convai-1.27,2021.dialdoc-1.16,1,0.839459,"Missing"
2021.nlp4convai-1.27,D19-1410,0,0.0172199,"ng turn detection benchmark dataset. Pos: knowledge-seeking turns; Neg: non-knowledge-seeking turns. 4.2 Baselines and Settings The baselines are 1) the best performing model in the DSTC9 Track 1 competition (Kim et al., 2021), which is a fine-tuned RoBERTa-Large model (Liu et al., 2019) on the training set. 2) Fine-tuned RoBERTa-Large-NLI (obtained by finetuning RoBERTa-Large on SNLI and MultiNLI datasets) and DistilBERT-Base-NLI-STSB (obtained by fine-tuning DistilBERT-Base on SNLI, MultiNLI, and STS-B datasets) on the training set. The sentence encoder E we used is DistilBERTBase-NLI-STSB (Reimers and Gurevych, 2019).4 Domains Examples Attraction Attraction Is it necessary to buy tickets in advance? How long it could take to see it all ? 4 hours it would be enough? Hotel Is there a minimum check in age? Hotel How much do you charge for parking? Restaurant Would there be room for a stroller with a sleeping baby during dinner? Restaurant Can I order crab cakes take out for eight servings ? Table 3: Examples of newly collected user questions in the contrast set. These user queries collected from real users are much more diverse than those in the benchmark test set of DSTC9 Track 1 dataset. The threshold η is"
2021.nlp4convai-1.27,2020.coling-main.611,1,0.811966,"Missing"
2021.nlp4convai-1.27,P17-1062,0,0.042073,"Missing"
2021.nlp4convai-1.27,2020.sigdial-1.35,1,0.842901,"Missing"
2021.nlp4convai-1.27,2021.ccl-1.108,0,0.0773871,"Missing"
2021.nlp4convai-1.27,2020.acl-main.654,1,0.769199,"Missing"
2021.nlp4convai-1.27,P19-1336,1,0.842344,"Missing"
2021.scil-1.52,N19-1078,0,0.0172652,"contain basic structural markup, including headings, paragraphs, position of figures and captions, bulleted lists, speaker information, and textual highlightings. This information is scraped directly from the source documents and is useful for subsequent layers, such as sentence splitting and discourse annotations. Tokenization and Tagging Documents are initially tokenized using a rule-based tokenizer with postprocessing tailored to our genres. For POS tagging we train an ensemble model that takes 4 models’ tag predictions as input to predict final tags. The 4 models we use here are Flair’s (Akbik et al., 2019) and StanfordNLP’s (Qi et al., 2018) models trained on OntoNotes (Hovy et al., 2006) and GUM. Sentence boundaries are added using an ensembled sentence splitter using XGBoost. Dependency Parsing Universal Dependency parses and morphological features are extracted using StanfordNLP, retrained on our genres using GUM, and configured to use the tokenization and stacked POS tag predictions from the previous components, rather than rely on its own POS predictions. We use the standard StanfordNLP English model as a benchmark. The high accuracy POS 434 Proceedings of the Society for Computation in Li"
2021.scil-1.52,N06-2015,0,0.121631,"and captions, bulleted lists, speaker information, and textual highlightings. This information is scraped directly from the source documents and is useful for subsequent layers, such as sentence splitting and discourse annotations. Tokenization and Tagging Documents are initially tokenized using a rule-based tokenizer with postprocessing tailored to our genres. For POS tagging we train an ensemble model that takes 4 models’ tag predictions as input to predict final tags. The 4 models we use here are Flair’s (Akbik et al., 2019) and StanfordNLP’s (Qi et al., 2018) models trained on OntoNotes (Hovy et al., 2006) and GUM. Sentence boundaries are added using an ensembled sentence splitter using XGBoost. Dependency Parsing Universal Dependency parses and morphological features are extracted using StanfordNLP, retrained on our genres using GUM, and configured to use the tokenization and stacked POS tag predictions from the previous components, rather than rely on its own POS predictions. We use the standard StanfordNLP English model as a benchmark. The high accuracy POS 434 Proceedings of the Society for Computation in Linguistics (SCiL) 2021, pages 434-437. Held on-line February 14-19, 2021 Tasks Tokeni"
2021.scil-1.52,P14-1002,0,0.0674176,"Missing"
2021.scil-1.52,W19-2715,0,0.0457993,"Missing"
2021.scil-1.52,K18-2016,0,0.0189621,"ding headings, paragraphs, position of figures and captions, bulleted lists, speaker information, and textual highlightings. This information is scraped directly from the source documents and is useful for subsequent layers, such as sentence splitting and discourse annotations. Tokenization and Tagging Documents are initially tokenized using a rule-based tokenizer with postprocessing tailored to our genres. For POS tagging we train an ensemble model that takes 4 models’ tag predictions as input to predict final tags. The 4 models we use here are Flair’s (Akbik et al., 2019) and StanfordNLP’s (Qi et al., 2018) models trained on OntoNotes (Hovy et al., 2006) and GUM. Sentence boundaries are added using an ensembled sentence splitter using XGBoost. Dependency Parsing Universal Dependency parses and morphological features are extracted using StanfordNLP, retrained on our genres using GUM, and configured to use the tokenization and stacked POS tag predictions from the previous components, rather than rely on its own POS predictions. We use the standard StanfordNLP English model as a benchmark. The high accuracy POS 434 Proceedings of the Society for Computation in Linguistics (SCiL) 2021, pages 434-437"
2021.scil-1.52,2020.tacl-1.39,0,0.0205302,"Missing"
2021.scil-1.52,W16-0713,1,0.866914,"Missing"
2021.sigdial-1.13,I17-1099,0,0.126315,"Missing"
2021.sigdial-1.13,2020.findings-emnlp.165,1,0.852259,"he best of our knowledge, there is no study or largescale multi-turn data for analyzing whether modelgenerated responses present the ability to communicate with commonsense knowledge or reasoning. Lack of real-life interactive setting for Commonsense Reasoning Benchmarks Current commonsense reasoning (CSR) benchmarks mostly target models’ ability to choose a right answer from several candidates given a question. We argue that this is a highly artificial scenario as models do not get options to choose from in real-life, and often they need to generate utterances. Recent work such as CommonGen (Lin et al., 2020) has started to explore generative settings to examine commonsense in natural language processing (NLP) models. This line of work, however, is still far from real use cases as it does not consider a real-life interaction task setup such as conversations. Thus we argue that existing commonsense benchmarks in NLP are not enough to train a language agent that produces smooth interpersonal communications, nor evaluate whether models have such capabilities. 3 2 2.1 Task Introduction and Motivations Commonsense-Focused Dialogue Response Generation We study commonsense-focused response generation for"
2021.sigdial-1.13,D16-1230,0,0.0300229,"n collected with different focuses such as incorporating knowledge (Gopalakrishnan et al., 2019; Dinan et al., 2018), empathy (Rashkin et al., 2019), task completion (Budzianowski et al., 2018), consistency (Nie et al., 2020), personality (Zhang et al., 2018) and reasoning (Cui et al., 2020) within dialog systems. There has also been work on combining a variety of Dialog Response Evaluation Due to the diverse responses that a dialog system can output, referenced automatic metrics (such as BLEU, ROUGE, Perplexity) do not correlate well with human judgement of these systems (Deriu et al., 2020; Liu et al., 2016). As a result, human evaluation has become the de-facto standard to evaluate dialog systems. However human evaluation is costly. Recently model-based metrics have been proposed with good correlation with human annotations (Zhang et al., 2019; Sellam et al., 2020; Mehri and Eskenazi, 2020b,a; Tao et al., 2018; Lowe et al., 2017). Most metrics focus on evaluating the coherence or appropriatness of a response with respect to its dialog context. (Mehri and Eskenazi, 2020a) identified 18 different dialog qualities such as interesting and topic depth. However none of these metrics evaluate the commo"
2021.sigdial-1.13,2020.acl-main.130,0,0.168291,"ed language models, GPT2 (Radford et al., 2019), and further train it on our training data sets. Specifically, the model is trained in a multitask fashion that minimizes the LM loss as well as the multiple choice loss following Wolf et al. (2019), and generates responses for a given dialog history. We consider the follow three types of training data setups. • Existing data sets, including DailyDialog (Li et al., 2017) (DD), EmpatheticDialogues (Rashkin et al., 2019)(ED), and Topical-Chat (Gopalakrishnan et al., 2019), a knowledge-grounded open-domain dataset with around 11k dialogues. MuTual (Cui et al., 2020) is not included since it is designed for response selection. • As described in Section 3.1, we use ConceptNet to search for potential triples in response turns and filter three dialogue datasets, DD, ED, and MuTual. We combine the three filtered dialogues from these datasets to form our training data, named ‘filter existing’ (FE, total around 21K dialogues). • The third category includes our collected dialogues using SocialIQA contexts. This is used along with the FE data above: FE and all of the 25k collected dialogues (FE+new crowdsourced), and FE plus the 11K filtered dialogues of our coll"
2021.sigdial-1.13,P17-1103,0,0.0204431,"so been work on combining a variety of Dialog Response Evaluation Due to the diverse responses that a dialog system can output, referenced automatic metrics (such as BLEU, ROUGE, Perplexity) do not correlate well with human judgement of these systems (Deriu et al., 2020; Liu et al., 2016). As a result, human evaluation has become the de-facto standard to evaluate dialog systems. However human evaluation is costly. Recently model-based metrics have been proposed with good correlation with human annotations (Zhang et al., 2019; Sellam et al., 2020; Mehri and Eskenazi, 2020b,a; Tao et al., 2018; Lowe et al., 2017). Most metrics focus on evaluating the coherence or appropriatness of a response with respect to its dialog context. (Mehri and Eskenazi, 2020a) identified 18 different dialog qualities such as interesting and topic depth. However none of these metrics evaluate the commonsense of a response, which is the focus of this work. 7 Conclusion We present our empirical study on commonsense in dialogue response generation. To obtain data for commonsense-focused analysis in open domain response generation, we use two strategies: filtering existing dialogue data using a commonsense knowledge graph Concep"
2021.sigdial-1.13,2020.acl-main.704,0,0.0162514,"and reasoning (Cui et al., 2020) within dialog systems. There has also been work on combining a variety of Dialog Response Evaluation Due to the diverse responses that a dialog system can output, referenced automatic metrics (such as BLEU, ROUGE, Perplexity) do not correlate well with human judgement of these systems (Deriu et al., 2020; Liu et al., 2016). As a result, human evaluation has become the de-facto standard to evaluate dialog systems. However human evaluation is costly. Recently model-based metrics have been proposed with good correlation with human annotations (Zhang et al., 2019; Sellam et al., 2020; Mehri and Eskenazi, 2020b,a; Tao et al., 2018; Lowe et al., 2017). Most metrics focus on evaluating the coherence or appropriatness of a response with respect to its dialog context. (Mehri and Eskenazi, 2020a) identified 18 different dialog qualities such as interesting and topic depth. However none of these metrics evaluate the commonsense of a response, which is the focus of this work. 7 Conclusion We present our empirical study on commonsense in dialogue response generation. To obtain data for commonsense-focused analysis in open domain response generation, we use two strategies: filterin"
2021.sigdial-1.13,2020.acl-main.64,0,0.112351,"s that contain ConceptNet triples as discussed earlier. In addition, it shows that though overall increasing training data size benefits model performance, the quality of data plays a more important role. We Proposed Commonsense Automatic Evaluation Results We now examine the correlation of our proposed automatic metric (MLP regressor) with human scores on the testing portion of our annotations. We cross-validate on the collected dialogues with 0.8/0.1/0.1 proportions. For comparison, we consider three baselines: our MLP with only symbolic features, our MLP with only neural features, and FED (Mehri and Eskenazi, 2020a), which uses DialoGPT to score how likely the next turn after the response expresses confusion. It requires no training nor human references, and has been shown to correlate with humans judgements on different criteria (commonsense not included). Table 4 shows the Spearman’s correlation of the system computed scores and human annotation scores using all the annotated data in a cross-validation setup. We can see that our simple MLP-based regressor reaches the highest spearman’s correlation with human scores, outperforming other baselines significantly. However, such a correlation result still"
2021.sigdial-1.13,P19-1534,0,0.429257,"odels, and show reasonable correlation with human evaluation of responses’ commonsense quality. 1 1 Introduction Open-domain dialogue response generation (RG) models aim to provide human-like natural language responses given dialogue histories (Chen et al., 2017). To improve generated response quality, many studies have been conducted to develop knowledge-grounded RG (Ghazvininejad et al., ∗ Work done while Pei Zhou was an intern at Amazon Alexa AI 1 Data and code will be released soon. 2018; Gopalakrishnan et al., 2019), personalized dialogue agents (Zhang et al., 2018), empathetic response (Rashkin et al., 2019), etc. For all the above-mentioned directions for RG, large-scale dialogue data geared towards the specific goals is crucial, since most current state-of-the-art neural RG models require training on appropriate and large data. Therefore several datasets have been collected to support such research efforts such as knowledge-grounded dialogues (Ghazvininejad et al., 2018; Gopalakrishnan et al., 2019), PersonaChat (Zhang et al., 2018), and EmpatheticDialogues (Rashkin et al., 2019). Producing natural and logically-coherent responses given dialogue contexts involves making commonsense inferences d"
2021.sigdial-1.13,2020.tacl-1.37,0,0.380271,"; Bisk et al., 2020; Sap et al., 2019b) test a model’s ability to choose the correct option given a context and a question; pre-trained language models have reached high performance on these benchmarks after fine-tuning. There have been many benchmarks that focus on reasoning abilities in multiple tasks such as reading comprehension (Huang et al., 2019; Yu et al., 2020), dialogue systems (Cui et al., 2020), and natural language inference (Williams et al., 2018), which involve inferences on language. Recent work also aims to probe models in these tasks to see if reasoning is actually achieved (Richardson and Sabharwal, 2020; Richardson et al., 2020; Zhou et al., 2020). In this study we tackle the response generation problem in dialogues, with a focus on collecting commonsense rich dialog data and evaluating commonsense quality of model responses. 6.2 Open Domain Dialogue Response Generation Recently open domain dialog systems have been modeled using end-to-end approaches, more specifically encoder-decoder architectures (Sordoni et al., 2015; Serban et al., 2017, 2016; Vinyals and Le, 2015). Recent work focused on finetuning large pre-trained transformer models (Radford et al., 2019; Zhang et al., 2020) on dialog"
2021.sigdial-1.13,D19-1454,0,0.0742573,"Missing"
2021.sigdial-1.13,2020.acl-tutorials.7,0,0.340866,"not consider a real-life interaction task setup such as conversations. Thus we argue that existing commonsense benchmarks in NLP are not enough to train a language agent that produces smooth interpersonal communications, nor evaluate whether models have such capabilities. 3 2 2.1 Task Introduction and Motivations Commonsense-Focused Dialogue Response Generation We study commonsense-focused response generation for dialogues. Commonsense can be defined as “the basic level of practical knowledge and reasoning concerning everyday situations and events that are commonly shared among most people” (Sap et al., 2020). Dialogue response generation is the task of generating a response turn r in a conversational setting given previous history turns h. Thus by combining these two together, we want to examine models’ ability to produce responses that make sense or is plausible in terms of commonsense. Motivations Commonsense Focused Dialogue Collection To collect more commonsense focused dialogues for response generation model training and evaluation, our effort is along two directions: filtering existing data to collect dialogues with responses that consist of commonsense (Section 3.1), and curating new data"
2021.sigdial-1.13,N15-1020,0,0.164442,"Missing"
2021.sigdial-1.13,N19-1421,0,0.0459046,"tion drop when considering either symbolic or neural features alone in our model, indicating that they might each capture a different 127 aspect for evaluating commonsense. datasets to exhibit multiple attributes (Roller et al., 2020). Metrics Spearman’s Correlation p-Value FED Symbolic Ours Neural All features -0.00797 0.12336 0.06176 0.20789 0.80569 1.27E-08 0.00450 4.53E-22 6.3 Table 4: Spearman’s correlation and p-values for different automatic metrics with human scores. 6 6.1 Related Work Commonsense Reasoning The majority of recent commonsense reasoning benchmarks (Zellers et al., 2018; Talmor et al., 2019; Bisk et al., 2020; Sap et al., 2019b) test a model’s ability to choose the correct option given a context and a question; pre-trained language models have reached high performance on these benchmarks after fine-tuning. There have been many benchmarks that focus on reasoning abilities in multiple tasks such as reading comprehension (Huang et al., 2019; Yu et al., 2020), dialogue systems (Cui et al., 2020), and natural language inference (Williams et al., 2018), which involve inferences on language. Recent work also aims to probe models in these tasks to see if reasoning is actually achieved ("
2021.sigdial-1.13,N18-1101,0,0.0298111,"ith human scores. 6 6.1 Related Work Commonsense Reasoning The majority of recent commonsense reasoning benchmarks (Zellers et al., 2018; Talmor et al., 2019; Bisk et al., 2020; Sap et al., 2019b) test a model’s ability to choose the correct option given a context and a question; pre-trained language models have reached high performance on these benchmarks after fine-tuning. There have been many benchmarks that focus on reasoning abilities in multiple tasks such as reading comprehension (Huang et al., 2019; Yu et al., 2020), dialogue systems (Cui et al., 2020), and natural language inference (Williams et al., 2018), which involve inferences on language. Recent work also aims to probe models in these tasks to see if reasoning is actually achieved (Richardson and Sabharwal, 2020; Richardson et al., 2020; Zhou et al., 2020). In this study we tackle the response generation problem in dialogues, with a focus on collecting commonsense rich dialog data and evaluating commonsense quality of model responses. 6.2 Open Domain Dialogue Response Generation Recently open domain dialog systems have been modeled using end-to-end approaches, more specifically encoder-decoder architectures (Sordoni et al., 2015; Serban e"
2021.sigdial-1.13,D18-1009,0,0.0457767,"ere is a large correlation drop when considering either symbolic or neural features alone in our model, indicating that they might each capture a different 127 aspect for evaluating commonsense. datasets to exhibit multiple attributes (Roller et al., 2020). Metrics Spearman’s Correlation p-Value FED Symbolic Ours Neural All features -0.00797 0.12336 0.06176 0.20789 0.80569 1.27E-08 0.00450 4.53E-22 6.3 Table 4: Spearman’s correlation and p-values for different automatic metrics with human scores. 6 6.1 Related Work Commonsense Reasoning The majority of recent commonsense reasoning benchmarks (Zellers et al., 2018; Talmor et al., 2019; Bisk et al., 2020; Sap et al., 2019b) test a model’s ability to choose the correct option given a context and a question; pre-trained language models have reached high performance on these benchmarks after fine-tuning. There have been many benchmarks that focus on reasoning abilities in multiple tasks such as reading comprehension (Huang et al., 2019; Yu et al., 2020), dialogue systems (Cui et al., 2020), and natural language inference (Williams et al., 2018), which involve inferences on language. Recent work also aims to probe models in these tasks to see if reasoning i"
2021.sigdial-1.13,P18-1205,0,0.202871,"ptNet and pretrained language and dialog models, and show reasonable correlation with human evaluation of responses’ commonsense quality. 1 1 Introduction Open-domain dialogue response generation (RG) models aim to provide human-like natural language responses given dialogue histories (Chen et al., 2017). To improve generated response quality, many studies have been conducted to develop knowledge-grounded RG (Ghazvininejad et al., ∗ Work done while Pei Zhou was an intern at Amazon Alexa AI 1 Data and code will be released soon. 2018; Gopalakrishnan et al., 2019), personalized dialogue agents (Zhang et al., 2018), empathetic response (Rashkin et al., 2019), etc. For all the above-mentioned directions for RG, large-scale dialogue data geared towards the specific goals is crucial, since most current state-of-the-art neural RG models require training on appropriate and large data. Therefore several datasets have been collected to support such research efforts such as knowledge-grounded dialogues (Ghazvininejad et al., 2018; Gopalakrishnan et al., 2019), PersonaChat (Zhang et al., 2018), and EmpatheticDialogues (Rashkin et al., 2019). Producing natural and logically-coherent responses given dialogue conte"
2021.sigdial-1.13,2020.acl-demos.30,0,0.416946,"rom ConceptNet. The triple identifying process is the same as our filtering process described earlier (Section 3.1). That is, we first identify a set of concepts in the response turn and query ConceptNet for potential triples and match those with the other concepts appearing in the dialogue history. Two-hop triples are searched in a similar manner, with the only difference being that the number of potential triples will be much larger. We also include the length of the response as an additional feature. As for neural features, we use the scores from a dialogue-focused language model DialoGPT (Zhang et al., 2020) on both the response itself and the dialogue history concatenated with the response. The score from DialoGPT can be considered as the plausibility of the sentence. We train this MLP model using the human evaluation scores for different responses. 5 5.1 Results and Analysis Automatic Evaluation Results Table 2 shows results according to automatic metrics on our 4.6K testing dialogues. We find that perplexity scores for the GPT2 model trained on filtered existing dialogue data (FE), or plus new collected data (FE+Crowdsourced), are much lower than that just trained on existing datasets as is. T"
bies-etal-2006-linguistic,A00-2018,0,\N,Missing
bies-etal-2006-linguistic,graff-bird-2000-many,0,\N,Missing
bies-etal-2006-linguistic,N01-1016,0,\N,Missing
bies-etal-2006-linguistic,W02-1007,0,\N,Missing
bies-etal-2006-linguistic,N04-4032,0,\N,Missing
bies-etal-2006-linguistic,J03-4003,0,\N,Missing
bies-etal-2006-linguistic,N06-1024,1,\N,Missing
bies-etal-2006-linguistic,P04-1005,0,\N,Missing
bies-etal-2006-linguistic,cieri-etal-2004-fisher,0,\N,Missing
C10-1080,D08-1023,0,0.0148063,"al parser instead of a syntax-based language model. More importantly, we integrate translation models and parsing models in a discriminative framework where they can interact with each other directly. Our work also has connections to joint parsing (Smith and Smith, 2004; Burkett and Klein, 2008) and bilingually-constrained monolingual parsing (Huang et al., 2009a) because we use another language to resolve ambiguity for one language. However, while both joint parsing and bilinguallyconstrained monolingual parsing rely on the target sentence, our approach only takes a source sentence as input. Blunsom and Osborne (2008) incorporate the source-side parse trees into their probabilistic SCFG framework and treat every source-parse PCFG rule as an individual feature. The difference is that they parse the test set before decoding so as to exploit the source syntactic information to guide translation. More recently, Chiang (2010) has shown that (“exact”) tree-to-tree translation as parsing achieves comparable performance with Hiero (Chiang, 2007) using much fewer rules. Xiao et al. (2010) integrate tokenization and translation into a single step and improve the performance of tokenization and translation significan"
C10-1080,P04-1083,0,0.0797349,"Missing"
C10-1080,P08-1023,1,0.938919,"1: Tree-based decoding: (a) separate parsing and translation versus (b) joint parsing and translation. 1 Introduction Recent several years have witnessed the rapid development of syntax-based translation models (Chiang, 2007; Galley et al., 2006; Shen et al., 2008; Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006; Eisner, 2003; Zhang et al., 2008; Chiang, 2010), which incorporate formal or linguistic syntax into translation process. Depending on whether modeling the linguistic syntax of source language or not, we divide them into two categories: string-based and tree-based models. 1 1 Mi et al. (2008) also distinguish between string-based and tree-based models but depending on the type of input. String-based models include string-to-string (Chiang, 2007) and string-to-tree (Galley et al., 2006; Shen et al., 2008). Regardless of the syntactic information on the source side, they treat decoding as a parsing problem: the decoder parses a source-language sentence using the source projection of a synchronous grammar while building the target sub-translations in parallel. Tree-based models include tree-to-string (Liu et al., 2006; Huang et al., 2006) and tree-to-tree (Quirk et al., 2005; Eisner,"
C10-1080,D08-1092,0,0.0134633,"ine translation. Recently, Shen et al. (2008) have shown that dependency language model is beneficial for capturing long-distance relations between target words. As our approach adds parsing models to the source side where the source sentence is fixed during decoding, our decoder does parse the source sentence like a monolingual parser instead of a syntax-based language model. More importantly, we integrate translation models and parsing models in a discriminative framework where they can interact with each other directly. Our work also has connections to joint parsing (Smith and Smith, 2004; Burkett and Klein, 2008) and bilingually-constrained monolingual parsing (Huang et al., 2009a) because we use another language to resolve ambiguity for one language. However, while both joint parsing and bilinguallyconstrained monolingual parsing rely on the target sentence, our approach only takes a source sentence as input. Blunsom and Osborne (2008) incorporate the source-side parse trees into their probabilistic SCFG framework and treat every source-parse PCFG rule as an individual feature. The difference is that they parse the test set before decoding so as to exploit the source syntactic information to guide tr"
C10-1080,J03-1002,0,0.00572434,"Missing"
C10-1080,2003.mtsummit-papers.6,0,0.0515247,"enote only using translation features. Table 5 shows the results. Translation features were used for all configurations. Without parsing models, the F1 score is 62.7. Adding Collins’ Model 1 results in much larger gains than adding PCFG. With all parsing models integrated, our joint decoder achieves an F1 score of 80.6 on the test set. Although lower than the F1 score of the in-house parser that produces the noisy training data, this result is still very promising because the tree-to-string rules that construct trees in the decoding process are learned from noisy training data. 4 Related Work Charniak et al. (2003) firstly associate lexicalized parsing model with syntax-based translation. They first run a string-to-tree decoder (Yamada and Knight, 2001) to produce an English parse forest and then use a lexicalized parsing model to select the best translation from the forest. As the parsing model operates on the target side, it actually serves as a syntax-based language model for machine translation. Recently, Shen et al. (2008) have shown that dependency language model is beneficial for capturing long-distance relations between target words. As our approach adds parsing models to the source side where t"
C10-1080,P03-1021,0,0.0354318,"probabilities of a natural rule unchanged and set those of a virtual rule to 1. 4 After binarizing tree-to-string rules into SCFG rules that have at most two non-terminals, we can use the CKY algorithm to parse a source sentence and produce its translation simultaneously as described in (Chiang, 2007; Galley et al., 2006). 2.2 Adding Parsing Models As our decoder produces “genuine” parse trees during decoding, we can integrate parsing models as features together with translation features such as the tree-to-string model, n-gram language model, and word penalty into a discriminative framework (Och, 2003). We expect that parsing and translation could interact with each other: parsing offers linguistically motivated reordering to translation and translation helps parsing resolve ambiguity. IP(x1 :NPB VP(x2 :PP x3 :VPB))→x1 x3 x2 2.2.1 PCFG We use the probabilistic context-free grammar (PCFG) as the first parsing feature in our decoder. Given a PCFG, the probability for a tree is the IP → hT 1 , T 1 i (1) product of probabilities for the rules that it contains. That is, if a tree π is a context-free derivaT → hNPB 1 PP 2 VPB 3 , NPB 1 VPB 3 PP 2 i (2) tion that involves K rules of the form αk →"
C10-1080,J07-2003,0,0.685036,"two-step SCFG derivation. For example, consider the first rule in Table 1: We call rules the tree roots of which are virtual non-terminals virtual rules and others natural rules. For example, the rule (1) is a natural rule and the rules (3) and (4) are virtual rules. We follow Huang et al. (2009b) to keep the probabilities of a natural rule unchanged and set those of a virtual rule to 1. 4 After binarizing tree-to-string rules into SCFG rules that have at most two non-terminals, we can use the CKY algorithm to parse a source sentence and produce its translation simultaneously as described in (Chiang, 2007; Galley et al., 2006). 2.2 Adding Parsing Models As our decoder produces “genuine” parse trees during decoding, we can integrate parsing models as features together with translation features such as the tree-to-string model, n-gram language model, and word penalty into a discriminative framework (Och, 2003). We expect that parsing and translation could interact with each other: parsing offers linguistically motivated reordering to translation and translation helps parsing resolve ambiguity. IP(x1 :NPB VP(x2 :PP x3 :VPB))→x1 x3 x2 2.2.1 PCFG We use the probabilistic context-free grammar (PCFG)"
C10-1080,N07-1051,0,0.0884195,"Missing"
C10-1080,P10-1146,0,0.0342491,"ed monolingual parsing (Huang et al., 2009a) because we use another language to resolve ambiguity for one language. However, while both joint parsing and bilinguallyconstrained monolingual parsing rely on the target sentence, our approach only takes a source sentence as input. Blunsom and Osborne (2008) incorporate the source-side parse trees into their probabilistic SCFG framework and treat every source-parse PCFG rule as an individual feature. The difference is that they parse the test set before decoding so as to exploit the source syntactic information to guide translation. More recently, Chiang (2010) has shown that (“exact”) tree-to-tree translation as parsing achieves comparable performance with Hiero (Chiang, 2007) using much fewer rules. Xiao et al. (2010) integrate tokenization and translation into a single step and improve the performance of tokenization and translation significantly. 5 Conclusion We have presented a framework for joint parsing and translation by casting tree-to-string translation as a parsing problem. While tree-to-string rules construct parse trees on the source side and translations on the target side simultaneously, parsing models can be integrated to improve bot"
C10-1080,W06-1608,0,0.0239566,", the output on the target side can be taken as a translation. Compared with its string-based counterparts, tree-based decoding is simpler and faster: there is no need for synchronous binarization (Huang et al., 2009b; Zhang et al., 2006) and tree parsing generally runs in linear time (Huang et al., 2006). While separating parsing and translation makes tree-based decoding simple and efficient, its search space is limited by the number of parse trees offered by parser. Studies reveal that treebased systems are prone to produce degenerate translations due to the propagation of parsing mistakes (Quirk and Corston-Oliver, 2006). This problem can be alleviated by offering more alternatives to the pipeline. An elegant solution is to replace 1-best trees with packed forests that encode exponentially many trees (Mi et al., 2008; Liu et al., 2009). Mi et al. (2008) present an efficient algorithm to match tree-to-string rules against packed forests that encode millions of trees. They prove that offering more alternatives to tree parsing improves translation performance substantially. In this paper, we take a further step towards the direction of offering multiple parses to translation by proposing joint parsing and transl"
C10-1080,J03-4003,0,0.366819,"the original tree from virtual rules. We first construct the tree on the left by substituting the trees of the rules (1), (3), and (4) and then restore the original tree on the right via T. Now, we can calculate the PCFG probability of the original tree. 6 In practice, we pre-compute this PCFG probability and store it in the rule (1) to reduce computational overhead. 2.2.2 Although widely used in natural language processing, PCFGs are often criticized for the lack of lexicalization, which is very important to capture the lexical dependencies between words. Therefore, we use Collins’ Model 1 (Collins, 2003), a simple and effective lexicalized parsing model, as the second parsing feature in our decoder. Following Collins (2003), we first lexicalize a tree by associating a headword h with each nonterminal. Figure 4 gives the lexicalized tree corresponding to Figure 2. The left-hand side of a rule in a lexicalized PCFG is P (h) and the right-hand side has the form: Ln (ln ) . . . L1 (l1 )H(h)R1 (τ1 ) . . . Rm (τm ) (11) where H is the head-child that inherits the headword h from its parent P , L1 . . . Ln and R1 . . . Rm are left and right modifiers of H, and l1 . . . ln and τ1 . . . τm are the cor"
C10-1080,P05-1034,0,0.105504,"Missing"
C10-1080,P03-2041,0,0.171565,"xing) AS(le) x1 :NPB)→held a x1 NPB(NN(huitan))→meeting Table 1: Tree-to-string rules extracted from Figure 2. ¬! huitan Sharon Figure 2: A training example that consists of a Chinese parse, an English sentence, and the word alignment between them. a set of tree-to-string rules obtained from Figure 2. The source side of a rule is a tree fragment and the target side is a string. We use x to denote non-terminals and the associated subscripts indicate the correspondence between non-terminals on both sides. Conventionally, decoding for tree-to-string translation is cast as a tree parsing problem (Eisner, 2003). The tree parsing algorithm visits each node in the input source tree in a top-down order and tries to match each translation rule against the local sub-tree rooted at the node. For example, the first rule in Table 1 matches a sub-tree rooted at IP0,6 in Figure 2. The descendent nodes of IP0,6 (i.e., NPB0,1 , PP1,3 , and VPB3,6 ) can be further matched by other rules in Table 1. The matching procedure runs recursively until the entire tree is covered. Finally, the output on the target side can be taken as a translation. Compared with its string-based counterparts, tree-based decoding is simpl"
C10-1080,P08-1066,0,0.286792,"we cast tree-to-string decoding as a monolingual parsing problem (Melamed, 2004; Chiang, 2007; Galley et al., 2006): the decoder takes a source-language string as input and parses it using the source-projection of SCFG while building the corresponding sub-translations simultaneously. 708 For example, given the Chinese sentence bushi yu sha long juxing le huitan in Figure 2, the derivation in Table 1 explains how a Chinese tree, an English string, and the word alignment between them are generated synchronously. Unlike the string-based systems as described in (Chiang, 2007; Galley et al., 2006; Shen et al., 2008), we exploit the linguistic syntax on the source side explicitly. Therefore, the source parse trees produced by our decoder are meaningful from a linguistic point of view. As tree-to-string rules usually have multiple non-terminals that make decoding complexity generally exponential, synchronous binarization (Huang et al., 2009b; Zhang et al., 2006) is a key technique for applying the CKY algorithm to parsing with tree-to-string rules. 2 Huang et al. (2009b) factor each tree-to-string rule into two SCFG rules: one from the root nonterminal to the subtree, and the other from the subtree to the"
C10-1080,N04-1035,0,0.286019,"Missing"
C10-1080,P06-1121,0,0.053641,"derivation. For example, consider the first rule in Table 1: We call rules the tree roots of which are virtual non-terminals virtual rules and others natural rules. For example, the rule (1) is a natural rule and the rules (3) and (4) are virtual rules. We follow Huang et al. (2009b) to keep the probabilities of a natural rule unchanged and set those of a virtual rule to 1. 4 After binarizing tree-to-string rules into SCFG rules that have at most two non-terminals, we can use the CKY algorithm to parse a source sentence and produce its translation simultaneously as described in (Chiang, 2007; Galley et al., 2006). 2.2 Adding Parsing Models As our decoder produces “genuine” parse trees during decoding, we can integrate parsing models as features together with translation features such as the tree-to-string model, n-gram language model, and word penalty into a discriminative framework (Och, 2003). We expect that parsing and translation could interact with each other: parsing offers linguistically motivated reordering to translation and translation helps parsing resolve ambiguity. IP(x1 :NPB VP(x2 :PP x3 :VPB))→x1 x3 x2 2.2.1 PCFG We use the probabilistic context-free grammar (PCFG) as the first parsing"
C10-1080,2006.amta-papers.8,0,0.223162,"Missing"
C10-1080,D09-1127,1,0.886349,"Missing"
C10-1080,J09-4009,0,0.209095,"rce tree in a top-down order and tries to match each translation rule against the local sub-tree rooted at the node. For example, the first rule in Table 1 matches a sub-tree rooted at IP0,6 in Figure 2. The descendent nodes of IP0,6 (i.e., NPB0,1 , PP1,3 , and VPB3,6 ) can be further matched by other rules in Table 1. The matching procedure runs recursively until the entire tree is covered. Finally, the output on the target side can be taken as a translation. Compared with its string-based counterparts, tree-based decoding is simpler and faster: there is no need for synchronous binarization (Huang et al., 2009b; Zhang et al., 2006) and tree parsing generally runs in linear time (Huang et al., 2006). While separating parsing and translation makes tree-based decoding simple and efficient, its search space is limited by the number of parse trees offered by parser. Studies reveal that treebased systems are prone to produce degenerate translations due to the propagation of parsing mistakes (Quirk and Corston-Oliver, 2006). This problem can be alleviated by offering more alternatives to the pipeline. An elegant solution is to replace 1-best trees with packed forests that encode exponentially many trees ("
C10-1080,P08-1067,0,0.0662005,"Missing"
C10-1080,P06-1077,1,0.949632,"Missing"
C10-1080,P09-1063,1,0.874797,"Missing"
C10-1080,W04-3207,0,0.0208115,"language model for machine translation. Recently, Shen et al. (2008) have shown that dependency language model is beneficial for capturing long-distance relations between target words. As our approach adds parsing models to the source side where the source sentence is fixed during decoding, our decoder does parse the source sentence like a monolingual parser instead of a syntax-based language model. More importantly, we integrate translation models and parsing models in a discriminative framework where they can interact with each other directly. Our work also has connections to joint parsing (Smith and Smith, 2004; Burkett and Klein, 2008) and bilingually-constrained monolingual parsing (Huang et al., 2009a) because we use another language to resolve ambiguity for one language. However, while both joint parsing and bilinguallyconstrained monolingual parsing rely on the target sentence, our approach only takes a source sentence as input. Blunsom and Osborne (2008) incorporate the source-side parse trees into their probabilistic SCFG framework and treat every source-parse PCFG rule as an individual feature. The difference is that they parse the test set before decoding so as to exploit the source syntact"
C10-1080,C10-1135,1,0.716482,"inguallyconstrained monolingual parsing rely on the target sentence, our approach only takes a source sentence as input. Blunsom and Osborne (2008) incorporate the source-side parse trees into their probabilistic SCFG framework and treat every source-parse PCFG rule as an individual feature. The difference is that they parse the test set before decoding so as to exploit the source syntactic information to guide translation. More recently, Chiang (2010) has shown that (“exact”) tree-to-tree translation as parsing achieves comparable performance with Hiero (Chiang, 2007) using much fewer rules. Xiao et al. (2010) integrate tokenization and translation into a single step and improve the performance of tokenization and translation significantly. 5 Conclusion We have presented a framework for joint parsing and translation by casting tree-to-string translation as a parsing problem. While tree-to-string rules construct parse trees on the source side and translations on the target side simultaneously, parsing models can be integrated to improve both translation and parsing quality. This work can be considered as a final step towards the continuum of tree-to-string translation: from single tree to forest and"
C10-1080,I05-1007,1,0.854995,"Missing"
C10-1080,P01-1067,0,0.116813,"dels, the F1 score is 62.7. Adding Collins’ Model 1 results in much larger gains than adding PCFG. With all parsing models integrated, our joint decoder achieves an F1 score of 80.6 on the test set. Although lower than the F1 score of the in-house parser that produces the noisy training data, this result is still very promising because the tree-to-string rules that construct trees in the decoding process are learned from noisy training data. 4 Related Work Charniak et al. (2003) firstly associate lexicalized parsing model with syntax-based translation. They first run a string-to-tree decoder (Yamada and Knight, 2001) to produce an English parse forest and then use a lexicalized parsing model to select the best translation from the forest. As the parsing model operates on the target side, it actually serves as a syntax-based language model for machine translation. Recently, Shen et al. (2008) have shown that dependency language model is beneficial for capturing long-distance relations between target words. As our approach adds parsing models to the source side where the source sentence is fixed during decoding, our decoder does parse the source sentence like a monolingual parser instead of a syntax-based l"
C10-1080,N06-1033,0,0.170021,"n order and tries to match each translation rule against the local sub-tree rooted at the node. For example, the first rule in Table 1 matches a sub-tree rooted at IP0,6 in Figure 2. The descendent nodes of IP0,6 (i.e., NPB0,1 , PP1,3 , and VPB3,6 ) can be further matched by other rules in Table 1. The matching procedure runs recursively until the entire tree is covered. Finally, the output on the target side can be taken as a translation. Compared with its string-based counterparts, tree-based decoding is simpler and faster: there is no need for synchronous binarization (Huang et al., 2009b; Zhang et al., 2006) and tree parsing generally runs in linear time (Huang et al., 2006). While separating parsing and translation makes tree-based decoding simple and efficient, its search space is limited by the number of parse trees offered by parser. Studies reveal that treebased systems are prone to produce degenerate translations due to the propagation of parsing mistakes (Quirk and Corston-Oliver, 2006). This problem can be alleviated by offering more alternatives to the pipeline. An elegant solution is to replace 1-best trees with packed forests that encode exponentially many trees (Mi et al., 2008; Liu e"
C10-1080,P08-1064,0,0.101116,"Missing"
C10-1080,D08-1060,0,0.0349263,"(5) Then, the rule (2) can be further binarized into k=1...K two rules that have at most two non-terminals: For example, the probability for the tree in FigT → hNPB 1 PP-VPB 2 , NPB 1 PP-VPB 2 i (3) ure 2 is PP-VPB → hPP 1 VPB 2 , VPB 2 PP 1 i (4) P(π) = Ppcf g (IP → NPB VP) × where PP-VPB is an intermediate virtual nonPpcf g (NPB → NR) × terminal. Ppcf g (NR → bushi) × 2 But CKY is not the only choice. The Earley algorithm We use a specific non-terminal, say, T, to uniquely identify the left-hand side subtree and produce two SCFG rules: 3 can also be used to parse with tree-to-string rules (Zhao and Al-Onaizan, 2008). As the Earley algorithm binarizes multinonterminal rules implicitly, there is no need for synchronous binarization. 3 It might look strange that the node VP disappears. This node is actually stored in the monolithic node T. Please refer to page 573 of (Huang et al., 2009b) for more details about how to convert tree-to-string rules to SCFG rules. ... (6) 4 This makes the scores of hypotheses in the same chart cell hardly comparable because some hypotheses are covered by a natural non-terminal and others covered by a virtual non-terminal. To alleviate this problem, we follow Huang et al. (2009"
C10-1080,D08-1022,0,\N,Missing
C10-1123,P05-1067,0,0.0626954,"string-to-dependency system obtains significant improvements ranging from 1.36 to 1.46 BLEU points over the tree-based baseline on the NIST 2004/2005/2006 Chinese-English test sets. 1 Introduction Dependency grammars have become increasingly popular in syntax-based statistical machine translation (SMT). One important advantage of dependency grammars is that they directly capture the dependencies between words, which are key to resolving most parsing ambiguities. As a result, incorporating dependency trees proves to be effective in improving statistical machine translation (Quirk et al., 2005; Ding and Palmer, 2005; Shen et al., 2008). However, most dependency-based translation systems suffer from a major drawback: they only use 1-best dependency trees for rule extraction, dependency language model training, and decoding, which potentially introduces translation mistakes due to the propagation of parsing errors (Quirk and Corston-Oliver, 2006). While the treelet system (Quirk et al., 2005) takes a dependency tree as input, the string-to-dependency system (Shen et al., 2008) decodes on a sourcelanguage string. However, as we will show, the string-to-dependency system still commits to using degenerate rul"
C10-1123,P08-1115,0,0.0433796,"endency system (Shen et al., 2008) decodes on a sourcelanguage string. However, as we will show, the string-to-dependency system still commits to using degenerate rules and dependency language models learned from noisy 1-best trees. HILab Convergence Technology Center C&I Business SKTelecom yshwang@sktelecom.com To alleviate this problem, an obvious solution is to offer more alternatives. Recent studies have shown that SMT systems can benefit from widening the annotation pipeline: using packed forests instead of 1-best trees (Mi and Huang, 2008), word lattices instead of 1-best segmentations (Dyer et al., 2008), and weighted alignment matrices instead of 1-best alignments (Liu et al., 2009). Along the same direction, we propose a structure called dependency forest, which encodes exponentially many dependency trees compactly, for dependency-based translation systems. In this paper, we develop two new algorithms for extracting string-to-dependency rules and for training dependency language models, respectively. We show that using the rules and dependency language models learned from dependency forests leads to consistent and significant improvements over that of using 1-best trees on the NIST 2004/200"
C10-1123,N04-1035,0,0.0493122,".append(r) keep k-best dependency structures for v It is difficult to assign a probability to each hyperedge. The current method is arbitrary, and we will improve it in the future. 4 Forest-based Rule Extraction In tree-based rule extraction, one just needs to first enumerate all bilingual phrases that are consistent with word alignment and then check whether the dependency structures over the target phrases are well-formed. However, this algorithm fails to work in the forest scenario because there are usually exponentially many well-formed structures over a target phrase. The GHKM algorithm (Galley et al., 2004), which is originally developed for extracting treeto-string rules from 1-best trees, has been successfully extended to packed forests recently (Mi and Huang, 2008). The algorithm distinguishes between minimal and composed rules. Although there are exponentially many composed rules, the number of minimal rules extracted from each node is rather limited (e.g., one or zero). Therefore, one can obtain promising composed rules by combining minimal rules. Unfortunately, the GHKM algorithm cannot be applied to extracting string-to-dependency rules from dependency forests. This is because the GHKM al"
C10-1123,W05-1506,0,0.411678,"i denotes that he0,1 , boy2,4 , and with4,7 are dependants (from left to right) of saw0,7 . More formally, a dependency forest is a pair hV, Ei, where V is a set of nodes, and E is a set of hyperedges. For a given sentence w1:l = w1 . . . wl , each node v ∈ V is in the form of wi,j , which denotes that w dominates the substring from positions i through j (i.e., wi+1 . . . wj ). Each hyperedge e ∈ E is a pair htails(e), head(e)i, where head(e) ∈ V is the head and tails(e) ∈ V are its dependants. A dependency forest has a structure of a hypergraph such as packed forest (Klein and Manning, 2001; Huang and Chiang, 2005). However, while each hyperedge in a packed forest naturally treats the corresponding PCFG rule probability as its weight, it is challenging to make dependency forest to be a weighted hypergraph because dependency parsers usually only output a score, which can be either positive or negative, for each edge in a dependency tree rather than a hyperedge in a 1094 saw0,7 e1 he0,1 Algorithm 1 Forest-based Initial Phrase Extraction e2 boy2,4 e3 boy2,7 e4 a2,3 with4,7 e5 he ta saw a boy kandao yige dai with 1: 2: 3: telescope5,7 4: e6 5: 6: a5,6 7: 8: a telescope 9: 10: 11: wangyuanjing de nanhai Figu"
C10-1123,D09-1127,1,0.908687,"Missing"
C10-1123,W01-1812,0,0.060847,"y2,4 , with4,7 ), saw0,7 i denotes that he0,1 , boy2,4 , and with4,7 are dependants (from left to right) of saw0,7 . More formally, a dependency forest is a pair hV, Ei, where V is a set of nodes, and E is a set of hyperedges. For a given sentence w1:l = w1 . . . wl , each node v ∈ V is in the form of wi,j , which denotes that w dominates the substring from positions i through j (i.e., wi+1 . . . wj ). Each hyperedge e ∈ E is a pair htails(e), head(e)i, where head(e) ∈ V is the head and tails(e) ∈ V are its dependants. A dependency forest has a structure of a hypergraph such as packed forest (Klein and Manning, 2001; Huang and Chiang, 2005). However, while each hyperedge in a packed forest naturally treats the corresponding PCFG rule probability as its weight, it is challenging to make dependency forest to be a weighted hypergraph because dependency parsers usually only output a score, which can be either positive or negative, for each edge in a dependency tree rather than a hyperedge in a 1094 saw0,7 e1 he0,1 Algorithm 1 Forest-based Initial Phrase Extraction e2 boy2,4 e3 boy2,7 e4 a2,3 with4,7 e5 he ta saw a boy kandao yige dai with 1: 2: 3: telescope5,7 4: e6 5: 6: a5,6 7: 8: a telescope 9: 10: 11: wa"
C10-1123,D09-1106,1,0.801307,"we will show, the string-to-dependency system still commits to using degenerate rules and dependency language models learned from noisy 1-best trees. HILab Convergence Technology Center C&I Business SKTelecom yshwang@sktelecom.com To alleviate this problem, an obvious solution is to offer more alternatives. Recent studies have shown that SMT systems can benefit from widening the annotation pipeline: using packed forests instead of 1-best trees (Mi and Huang, 2008), word lattices instead of 1-best segmentations (Dyer et al., 2008), and weighted alignment matrices instead of 1-best alignments (Liu et al., 2009). Along the same direction, we propose a structure called dependency forest, which encodes exponentially many dependency trees compactly, for dependency-based translation systems. In this paper, we develop two new algorithms for extracting string-to-dependency rules and for training dependency language models, respectively. We show that using the rules and dependency language models learned from dependency forests leads to consistent and significant improvements over that of using 1-best trees on the NIST 2004/2005/2006 Chinese-English test sets. 2 Background Figure 1 shows a dependency tree o"
C10-1123,D08-1022,0,0.607088,"rk et al., 2005) takes a dependency tree as input, the string-to-dependency system (Shen et al., 2008) decodes on a sourcelanguage string. However, as we will show, the string-to-dependency system still commits to using degenerate rules and dependency language models learned from noisy 1-best trees. HILab Convergence Technology Center C&I Business SKTelecom yshwang@sktelecom.com To alleviate this problem, an obvious solution is to offer more alternatives. Recent studies have shown that SMT systems can benefit from widening the annotation pipeline: using packed forests instead of 1-best trees (Mi and Huang, 2008), word lattices instead of 1-best segmentations (Dyer et al., 2008), and weighted alignment matrices instead of 1-best alignments (Liu et al., 2009). Along the same direction, we propose a structure called dependency forest, which encodes exponentially many dependency trees compactly, for dependency-based translation systems. In this paper, we develop two new algorithms for extracting string-to-dependency rules and for training dependency language models, respectively. We show that using the rules and dependency language models learned from dependency forests leads to consistent and significan"
C10-1123,P00-1056,0,0.135647,"Each n-gram (e.g., “boy-as-head a”) is assigned the same fractional count of the hyperedge it belongs to. We also tried training dependency language model as in (Shen et al., 2008), which means all hyperedges were on equal footing without regarding probabilities. However, the performance is about 0.8 point lower in BLEU. One possbile reason is that hyperedges with probabilities could distinguish high quality structures better. 6 Experiments 6.1 Results on the Chinese-English Task We used the FBIS corpus (6.9M Chinese words + 8.9M English words) as our bilingual training corpus. We ran GIZA++ (Och and Ney, 2000) to obtain word alignments. We trained a 4-gram language model on the Xinhua portion of GIGAWORD corpus using the SRI Language Modeling Toolkit (Stolcke, 2002) with modified Kneser-Ney smoothing (Kneser and Ney, 1995). We optimized feature weights using the minimum error rate training algorithm (Och and Ney, 2002) on the NIST 2002 test set. We evaluated the translation quality using case-insensitive BLEU metric (Papineni et al., 2002) on the NIST 2004/2005/2006 test sets. To obtain dependency trees and forests, we parsed the English sentences of the FBIS corpus using a shift-reduce dependency"
C10-1123,P02-1038,0,0.220964,"Missing"
C10-1123,J04-4002,0,0.0457485,"dency structure di..j is fixed on head h, where h ∈ / [i, j], or fixed for short, if and only if it meets the following conditions telescope a (b) (c) and the word alignments between them. To facilitate identifying the correspondence between the English and Chinese words, we also gives the English sentence. Extracting string-to-dependency rules from aligned string-dependency pairs is similar to extracting SCFG (Chiang, 2007) except that the target side of a rule is a well-formed structure. For example, we can first extract a string-todependency rule that is consistent with the word alignment (Och and Ney, 2004): with ((a) telescope) → dai wangyuanjing de Then a smaller rule (a) telescope → wangyuanjing can be subtracted to obtain a rule with one nonterminal: • dh ∈ / [i, j] with (X1 ) → dai X1 de • ∀k ∈ [i, j] and k 6= h, dk ∈ [i, j] • ∀k ∈ / [i, j], dk = h or dk ∈ / [i, j] Definition 2. A dependency structure di..j is floating with children C, for a non-empty set C ⊆ {i, ..., j}, or floating for short, if and only if it meets the following conditions • ∃h ∈ / [i, j], s.t.∀k ∈ C, dk = h • ∀k ∈ [i, j] and k ∈ / C, dk ∈ [i, j] • ∀k ∈ / [i, j], dk ∈ / [i, j] A dependency structure is well-formed if and"
C10-1123,P02-1040,0,0.0903191,"Missing"
C10-1123,W06-1608,0,0.0253466,"age of dependency grammars is that they directly capture the dependencies between words, which are key to resolving most parsing ambiguities. As a result, incorporating dependency trees proves to be effective in improving statistical machine translation (Quirk et al., 2005; Ding and Palmer, 2005; Shen et al., 2008). However, most dependency-based translation systems suffer from a major drawback: they only use 1-best dependency trees for rule extraction, dependency language model training, and decoding, which potentially introduces translation mistakes due to the propagation of parsing errors (Quirk and Corston-Oliver, 2006). While the treelet system (Quirk et al., 2005) takes a dependency tree as input, the string-to-dependency system (Shen et al., 2008) decodes on a sourcelanguage string. However, as we will show, the string-to-dependency system still commits to using degenerate rules and dependency language models learned from noisy 1-best trees. HILab Convergence Technology Center C&I Business SKTelecom yshwang@sktelecom.com To alleviate this problem, an obvious solution is to offer more alternatives. Recent studies have shown that SMT systems can benefit from widening the annotation pipeline: using packed fo"
C10-1123,P05-1034,0,0.308079,"s. Our forest-based string-to-dependency system obtains significant improvements ranging from 1.36 to 1.46 BLEU points over the tree-based baseline on the NIST 2004/2005/2006 Chinese-English test sets. 1 Introduction Dependency grammars have become increasingly popular in syntax-based statistical machine translation (SMT). One important advantage of dependency grammars is that they directly capture the dependencies between words, which are key to resolving most parsing ambiguities. As a result, incorporating dependency trees proves to be effective in improving statistical machine translation (Quirk et al., 2005; Ding and Palmer, 2005; Shen et al., 2008). However, most dependency-based translation systems suffer from a major drawback: they only use 1-best dependency trees for rule extraction, dependency language model training, and decoding, which potentially introduces translation mistakes due to the propagation of parsing errors (Quirk and Corston-Oliver, 2006). While the treelet system (Quirk et al., 2005) takes a dependency tree as input, the string-to-dependency system (Shen et al., 2008) decodes on a sourcelanguage string. However, as we will show, the string-to-dependency system still commits"
C10-1123,P08-1066,0,0.574261,"stem obtains significant improvements ranging from 1.36 to 1.46 BLEU points over the tree-based baseline on the NIST 2004/2005/2006 Chinese-English test sets. 1 Introduction Dependency grammars have become increasingly popular in syntax-based statistical machine translation (SMT). One important advantage of dependency grammars is that they directly capture the dependencies between words, which are key to resolving most parsing ambiguities. As a result, incorporating dependency trees proves to be effective in improving statistical machine translation (Quirk et al., 2005; Ding and Palmer, 2005; Shen et al., 2008). However, most dependency-based translation systems suffer from a major drawback: they only use 1-best dependency trees for rule extraction, dependency language model training, and decoding, which potentially introduces translation mistakes due to the propagation of parsing errors (Quirk and Corston-Oliver, 2006). While the treelet system (Quirk et al., 2005) takes a dependency tree as input, the string-to-dependency system (Shen et al., 2008) decodes on a sourcelanguage string. However, as we will show, the string-to-dependency system still commits to using degenerate rules and dependency la"
C10-1123,D07-1078,0,0.013782,"ructures of a head can be constructed from those of its dependants. For example, in Figure 4, as the fixed structure rooted at telescope 5,7 is (a) telescope we can obtain a fixed structure rooted for the node with4,7 by attaching the fixed structure of its dependant to the node (EnumFixed in line 4). Figure 2(b) shows the resulting fixed structure. Similarly, the floating structure for the node saw0,7 can be obtained by concatenating the fixed structures of its dependants boy2,4 and with4,7 (EnumFloating in line 5). Figure 2(c) shows the resulting fixed structure. The algorithm is similar to Wang et al. (2007), which binarize each constituent node to create some intermediate nodes that correspond to the floating structures. Therefore, we can find k-best fixed and floating structures for a node in a dependency forest by manipulating the fixed structures of its dependants. Then we can extract string-to-dependency rules if the dependency structures are consistent with the word alignment. How to judge a well-formed structure extracted from a node is better than others? We follow Mi and Huang (2008) to assign a fractional count to each well-formed structure. Given a tree fragment t, we use the inside-ou"
C10-1123,P08-1023,1,\N,Missing
C10-1123,P03-1041,0,\N,Missing
C10-1123,P08-1067,0,\N,Missing
C10-1123,W06-1606,0,\N,Missing
C10-1123,P05-1033,0,\N,Missing
C10-1123,P08-1010,0,\N,Missing
C10-1123,J07-2003,0,\N,Missing
C10-1123,2008.amta-papers.18,0,\N,Missing
C10-1135,W08-0336,0,0.0683447,"nput sentence and then obtain a parse tree or forest of the sentence. As shown in Figure 1(a), we refer to this pipeline as separate tokenization and translation because they are divided into single steps. As tokenization for many languages is usually ambiguous, SMT systems that separate tokenization and translation suffer from a major drawback: tokenization errors potentially introduce translation mistakes. As some languages such as Chinese have no spaces in their writing systems, how to segment sentences into appropriate words has a direct impact on translation performance (Xu et al., 2005; Chang et al., 2008; Zhang et al., 2008). In addition, although agglutinative languages such as Korean incorporate spaces between “words”, which consist of multiple morphemes, the granularity is too coarse and makes the training data 1200 Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1200–1208, Beijing, August 2010 considerably sparse. Studies reveal that segmenting “words” into morphemes effectively improves translating morphologically rich languages (Oflazer, 2008). More importantly, a tokenization close to a gold standard does not necessarily leads to bette"
C10-1135,J07-2003,0,0.702362,"enizer, our joint decoder achieves significant improvements over monolingual Chinese tokenizers. translation (a) source string tokenization tokenize+translate target translation (b) Figure 1: (a) Separate tokenization and translation and (b) joint tokenization and translation. 1 Introduction Tokenization plays an important role in statistical machine translation (SMT) because tokenizing a source-language sentence is always the first step in SMT systems. Based on the type of input, Mi and Huang (2008) distinguish between two categories of SMT systems : string-based systems (Koehn et al., 2003; Chiang, 2007; Galley et al., 2006; Shen et al., 2008) that take a string as input and tree-based systems (Liu et al., 2006; Mi et al., 2008) that take a tree as input. Note that a treebased system still needs to first tokenize the input sentence and then obtain a parse tree or forest of the sentence. As shown in Figure 1(a), we refer to this pipeline as separate tokenization and translation because they are divided into single steps. As tokenization for many languages is usually ambiguous, SMT systems that separate tokenization and translation suffer from a major drawback: tokenization errors potentially"
C10-1135,D09-1075,0,0.119698,"is no gold standard data for tokenization, we do not use ME and LM tokenization features here. However, our joint method can still significantly (p < 0.05) improve the performance by about +0.3 points. This also reflects the importance of optimizing granularity for morphological complex languages. Related Work Methods have been proposed to optimize tokenization for word alignment. For example, word alignment can be simplified by packing (Ma et al., 2007) several consecutive words together. Word alignment and tokenization can also be optimized by maximizing the likelihood of bilingual corpus (Chung and Gildea, 2009; Xu et al., 2008). In fact, these work are orthogonal to our joint method, since they focus on training step while we are concerned of decoding. We believe we can further the performance by combining these two kinds of work. Our work also has connections to multilingual tokenization (Snyder and Barzilay, 2008). While they have verified that tokenization can be improved by multilingual learning, our work shows that we can also improve tokenization by collaborating with translation task in a supervised way. More recently, Liu and Liu (2010) also shows the effect of joint method. They integrate"
C10-1135,P08-1115,0,0.42896,"t 2010 considerably sparse. Studies reveal that segmenting “words” into morphemes effectively improves translating morphologically rich languages (Oflazer, 2008). More importantly, a tokenization close to a gold standard does not necessarily leads to better translation quality (Chang et al., 2008; Zhang et al., 2008). Therefore, it is necessary to offer more tokenizations to SMT systems to alleviate the tokenization error propagation problem. Recently, many researchers have shown that replacing 1-best tokenizations with lattices improves translation performance significantly (Xu et al., 2005; Dyer et al., 2008; Dyer, 2009). We take a next step towards the direction of offering more tokenizations to SMT systems by proposing joint tokenization and translation. As shown in Figure 1(b), our approach tokenizes and translates jointly to find a tokenization and a translation for a source-language string simultaneously. We integrate translation and tokenization models into a discriminative framework (Och and Ney, 2002), within which tokenization and translation models interact with each other. Experiments show that joint tokenization and translation outperforms its separate counterparts (1best tokenization"
C10-1135,N09-1046,0,0.0470927,"sparse. Studies reveal that segmenting “words” into morphemes effectively improves translating morphologically rich languages (Oflazer, 2008). More importantly, a tokenization close to a gold standard does not necessarily leads to better translation quality (Chang et al., 2008; Zhang et al., 2008). Therefore, it is necessary to offer more tokenizations to SMT systems to alleviate the tokenization error propagation problem. Recently, many researchers have shown that replacing 1-best tokenizations with lattices improves translation performance significantly (Xu et al., 2005; Dyer et al., 2008; Dyer, 2009). We take a next step towards the direction of offering more tokenizations to SMT systems by proposing joint tokenization and translation. As shown in Figure 1(b), our approach tokenizes and translates jointly to find a tokenization and a translation for a source-language string simultaneously. We integrate translation and tokenization models into a discriminative framework (Och and Ney, 2002), within which tokenization and translation models interact with each other. Experiments show that joint tokenization and translation outperforms its separate counterparts (1best tokenizations and lattice"
C10-1135,P06-1121,0,0.0317509,"int decoder achieves significant improvements over monolingual Chinese tokenizers. translation (a) source string tokenization tokenize+translate target translation (b) Figure 1: (a) Separate tokenization and translation and (b) joint tokenization and translation. 1 Introduction Tokenization plays an important role in statistical machine translation (SMT) because tokenizing a source-language sentence is always the first step in SMT systems. Based on the type of input, Mi and Huang (2008) distinguish between two categories of SMT systems : string-based systems (Koehn et al., 2003; Chiang, 2007; Galley et al., 2006; Shen et al., 2008) that take a string as input and tree-based systems (Liu et al., 2006; Mi et al., 2008) that take a tree as input. Note that a treebased system still needs to first tokenize the input sentence and then obtain a parse tree or forest of the sentence. As shown in Figure 1(a), we refer to this pipeline as separate tokenization and translation because they are divided into single steps. As tokenization for many languages is usually ambiguous, SMT systems that separate tokenization and translation suffer from a major drawback: tokenization errors potentially introduce translation"
C10-1135,P08-1102,1,0.87833,"Missing"
C10-1135,N03-1017,0,0.0193365,"erestingly, as a tokenizer, our joint decoder achieves significant improvements over monolingual Chinese tokenizers. translation (a) source string tokenization tokenize+translate target translation (b) Figure 1: (a) Separate tokenization and translation and (b) joint tokenization and translation. 1 Introduction Tokenization plays an important role in statistical machine translation (SMT) because tokenizing a source-language sentence is always the first step in SMT systems. Based on the type of input, Mi and Huang (2008) distinguish between two categories of SMT systems : string-based systems (Koehn et al., 2003; Chiang, 2007; Galley et al., 2006; Shen et al., 2008) that take a string as input and tree-based systems (Liu et al., 2006; Mi et al., 2008) that take a tree as input. Note that a treebased system still needs to first tokenize the input sentence and then obtain a parse tree or forest of the sentence. As shown in Figure 1(a), we refer to this pipeline as separate tokenization and translation because they are divided into single steps. As tokenization for many languages is usually ambiguous, SMT systems that separate tokenization and translation suffer from a major drawback: tokenization error"
C10-1135,W04-3250,0,0.0218819,"05 33.06 33.22 30.91 33.95 33.76 34.69 34.56 34.17 34.88** Speed 2.48 2.55 2.34 3.83 6.79 17.66 17.37 17.23 17.52 Table 2: Comparison of Separate and Joint methods in terms of BLEU and speed (second per sentence). Columns Train and Test represents the tokenization methods for training and testing respectively. Column TFs stands for whether the 8 √ tokenization features is used ( ) or not (×). ICT, SF and ME are segmenter names for preprocessing. All means combined corpus processed by the three segmenters. Lattice represent the system implemented as Dyer et al., (2008). ** means significantly (Koehn, 2004) better than Lattice (p < 0.01). 4 Experiments In this section, we try to answer the following questions: 1. Does the joint method outperform conventional methods that separate tokenization from decoding. (Section 4.1) 2. How about the tokenization performance of the joint decoder? (Section 4.2) 4.1 Translation Evaluation We use the SCFG model (Chiang, 2007) for our experiments. We firstly work on the ChineseEnglish translation task. The bilingual training data contains 1.5M sentence pairs coming from LDC data.1 The monolingual data for training English language model includes Xinhua portion o"
C10-1135,C10-1080,1,0.716304,"by maximizing the likelihood of bilingual corpus (Chung and Gildea, 2009; Xu et al., 2008). In fact, these work are orthogonal to our joint method, since they focus on training step while we are concerned of decoding. We believe we can further the performance by combining these two kinds of work. Our work also has connections to multilingual tokenization (Snyder and Barzilay, 2008). While they have verified that tokenization can be improved by multilingual learning, our work shows that we can also improve tokenization by collaborating with translation task in a supervised way. More recently, Liu and Liu (2010) also shows the effect of joint method. They integrate parsing and translation into a single step and improve the performance of translation significantly. 6 Conclusion We have presented a novel method for joint tokenization and translation which directly combines the tokenization model into the decoding phase. Allowing tokenization and translation to collaborate with each other, tokenization can be optimized for translation, while translation also makes contribution to tokenization performance under a supervised way. We believe that our approach can be applied to other string-based model such"
C10-1135,P06-1077,1,0.83937,"on (a) source string tokenization tokenize+translate target translation (b) Figure 1: (a) Separate tokenization and translation and (b) joint tokenization and translation. 1 Introduction Tokenization plays an important role in statistical machine translation (SMT) because tokenizing a source-language sentence is always the first step in SMT systems. Based on the type of input, Mi and Huang (2008) distinguish between two categories of SMT systems : string-based systems (Koehn et al., 2003; Chiang, 2007; Galley et al., 2006; Shen et al., 2008) that take a string as input and tree-based systems (Liu et al., 2006; Mi et al., 2008) that take a tree as input. Note that a treebased system still needs to first tokenize the input sentence and then obtain a parse tree or forest of the sentence. As shown in Figure 1(a), we refer to this pipeline as separate tokenization and translation because they are divided into single steps. As tokenization for many languages is usually ambiguous, SMT systems that separate tokenization and translation suffer from a major drawback: tokenization errors potentially introduce translation mistakes. As some languages such as Chinese have no spaces in their writing systems, how"
C10-1135,P07-1039,0,0.0879531,"odel includes Xinhua portion of the GIGAWORD corpus, which contains 238M English words. We use the NIST evaluation sets of 2002 (MT02) as our development data set, and sets of 2004(MT04) and 2005(MT05) as test sets. We use the corpus derived from the People’s Daily (Renmin Ribao) in Feb. to Jun. 1998 containing 6M words for training LM and ME tokenization models. Translation Part. We used GIZA++ (Och and Ney, 2003) to perform word alignment in both directions, and grow-diag-final-and (Koehn et al., 2003) to generate symmetric word alignment. We extracted the SCFG rules as describing in Chiang (2007). The language model were trained by the 1 including LDC2002E18, LDC2003E07, LDC2003E14, Hansards portion of LDC2004T07, LDC2004T08 and LDC2005T06 SRILM toolkit (Stolcke, 2002).2 Case insensitive NIST BLEU (Papineni et al., 2002) was used to measure translation performance. Tokenization Part. We used the toolkit implemented by Zhang (2004) to train the ME model. Three Chinese word segmenters were used for comparing: ICTCLAS (ICT) developed by institute of Computing Technology Chinese Academy of Sciences (Zhang et al., 2003); SF developed at Stanford University (Huihsin et al., 2005) and ME whi"
C10-1135,P08-1023,1,0.879426,"ng tokenization tokenize+translate target translation (b) Figure 1: (a) Separate tokenization and translation and (b) joint tokenization and translation. 1 Introduction Tokenization plays an important role in statistical machine translation (SMT) because tokenizing a source-language sentence is always the first step in SMT systems. Based on the type of input, Mi and Huang (2008) distinguish between two categories of SMT systems : string-based systems (Koehn et al., 2003; Chiang, 2007; Galley et al., 2006; Shen et al., 2008) that take a string as input and tree-based systems (Liu et al., 2006; Mi et al., 2008) that take a tree as input. Note that a treebased system still needs to first tokenize the input sentence and then obtain a parse tree or forest of the sentence. As shown in Figure 1(a), we refer to this pipeline as separate tokenization and translation because they are divided into single steps. As tokenization for many languages is usually ambiguous, SMT systems that separate tokenization and translation suffer from a major drawback: tokenization errors potentially introduce translation mistakes. As some languages such as Chinese have no spaces in their writing systems, how to segment senten"
C10-1135,W04-3236,0,0.0287871,"okenization and translation could collaborate with each other. Tokenization offers translation with good tokenized results, while translation helps tokenization to eliminate ambiguity. Formally, the probability of a derivation D is represented as P (D) ∝ Y λi φi (D) (1) i Since our model is still a string-based model, the CKY algorithm and cube pruning are still applicable for our model to find the derivation with max score. 3.2 Adding Tokenization Features Maximum Entropy model (ME). We first introduce ME model feature for tokenization by casting it as a labeling problem (Xue and Shen, 2003; Ng and Low, 2004). We label a character with the following 4 types: • b: the begin of a word • m: the middle of a word • e: the end of a word • s: a single-character word Taking the tokenization you-wang of the string you wang for example, we first create a label sequence b e for the tokenization you-wang and then calculate the probability of tokenization by P (you-wang |you wang) = P (b e |you wang) = P (b |you, you wang) × P (e |wang, you wang) Given a tokenization w1L with L words for a character sequence cn1 , we firstly create labels l1n for every characters and then calculate the probability by where φi"
C10-1135,P02-1038,0,0.0512261,"okenization error propagation problem. Recently, many researchers have shown that replacing 1-best tokenizations with lattices improves translation performance significantly (Xu et al., 2005; Dyer et al., 2008; Dyer, 2009). We take a next step towards the direction of offering more tokenizations to SMT systems by proposing joint tokenization and translation. As shown in Figure 1(b), our approach tokenizes and translates jointly to find a tokenization and a translation for a source-language string simultaneously. We integrate translation and tokenization models into a discriminative framework (Och and Ney, 2002), within which tokenization and translation models interact with each other. Experiments show that joint tokenization and translation outperforms its separate counterparts (1best tokenizations and lattices) significantly on the NIST 2004 and 2005 Chinese-English test sets. Our joint decoder also reports positive results on Korean-Chinese translation. As a tokenizer, our joint decoder achieves significantly better tokenization accuracy than three monolingual Chinese tokenizers. 2 Separate Tokenization and Translation Tokenization is to split a string of characters into meaningful elements, whic"
C10-1135,J03-1002,0,0.00401364,"xperiments. We firstly work on the ChineseEnglish translation task. The bilingual training data contains 1.5M sentence pairs coming from LDC data.1 The monolingual data for training English language model includes Xinhua portion of the GIGAWORD corpus, which contains 238M English words. We use the NIST evaluation sets of 2002 (MT02) as our development data set, and sets of 2004(MT04) and 2005(MT05) as test sets. We use the corpus derived from the People’s Daily (Renmin Ribao) in Feb. to Jun. 1998 containing 6M words for training LM and ME tokenization models. Translation Part. We used GIZA++ (Och and Ney, 2003) to perform word alignment in both directions, and grow-diag-final-and (Koehn et al., 2003) to generate symmetric word alignment. We extracted the SCFG rules as describing in Chiang (2007). The language model were trained by the 1 including LDC2002E18, LDC2003E07, LDC2003E14, Hansards portion of LDC2004T07, LDC2004T08 and LDC2005T06 SRILM toolkit (Stolcke, 2002).2 Case insensitive NIST BLEU (Papineni et al., 2002) was used to measure translation performance. Tokenization Part. We used the toolkit implemented by Zhang (2004) to train the ME model. Three Chinese word segmenters were used for com"
C10-1135,P03-1021,0,0.0602288,"Missing"
C10-1135,P02-1040,0,0.079414,"Missing"
C10-1135,P08-1066,0,0.0259864,"significant improvements over monolingual Chinese tokenizers. translation (a) source string tokenization tokenize+translate target translation (b) Figure 1: (a) Separate tokenization and translation and (b) joint tokenization and translation. 1 Introduction Tokenization plays an important role in statistical machine translation (SMT) because tokenizing a source-language sentence is always the first step in SMT systems. Based on the type of input, Mi and Huang (2008) distinguish between two categories of SMT systems : string-based systems (Koehn et al., 2003; Chiang, 2007; Galley et al., 2006; Shen et al., 2008) that take a string as input and tree-based systems (Liu et al., 2006; Mi et al., 2008) that take a tree as input. Note that a treebased system still needs to first tokenize the input sentence and then obtain a parse tree or forest of the sentence. As shown in Figure 1(a), we refer to this pipeline as separate tokenization and translation because they are divided into single steps. As tokenization for many languages is usually ambiguous, SMT systems that separate tokenization and translation suffer from a major drawback: tokenization errors potentially introduce translation mistakes. As some l"
C10-1135,P08-1084,0,0.0280376,"Work Methods have been proposed to optimize tokenization for word alignment. For example, word alignment can be simplified by packing (Ma et al., 2007) several consecutive words together. Word alignment and tokenization can also be optimized by maximizing the likelihood of bilingual corpus (Chung and Gildea, 2009; Xu et al., 2008). In fact, these work are orthogonal to our joint method, since they focus on training step while we are concerned of decoding. We believe we can further the performance by combining these two kinds of work. Our work also has connections to multilingual tokenization (Snyder and Barzilay, 2008). While they have verified that tokenization can be improved by multilingual learning, our work shows that we can also improve tokenization by collaborating with translation task in a supervised way. More recently, Liu and Liu (2010) also shows the effect of joint method. They integrate parsing and translation into a single step and improve the performance of translation significantly. 6 Conclusion We have presented a novel method for joint tokenization and translation which directly combines the tokenization model into the decoding phase. Allowing tokenization and translation to collaborate w"
C10-1135,2005.iwslt-1.18,0,0.0338026,"st tokenize the input sentence and then obtain a parse tree or forest of the sentence. As shown in Figure 1(a), we refer to this pipeline as separate tokenization and translation because they are divided into single steps. As tokenization for many languages is usually ambiguous, SMT systems that separate tokenization and translation suffer from a major drawback: tokenization errors potentially introduce translation mistakes. As some languages such as Chinese have no spaces in their writing systems, how to segment sentences into appropriate words has a direct impact on translation performance (Xu et al., 2005; Chang et al., 2008; Zhang et al., 2008). In addition, although agglutinative languages such as Korean incorporate spaces between “words”, which consist of multiple morphemes, the granularity is too coarse and makes the training data 1200 Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1200–1208, Beijing, August 2010 considerably sparse. Studies reveal that segmenting “words” into morphemes effectively improves translating morphologically rich languages (Oflazer, 2008). More importantly, a tokenization close to a gold standard does not necess"
C10-1135,C08-1128,0,0.0174115,"a for tokenization, we do not use ME and LM tokenization features here. However, our joint method can still significantly (p < 0.05) improve the performance by about +0.3 points. This also reflects the importance of optimizing granularity for morphological complex languages. Related Work Methods have been proposed to optimize tokenization for word alignment. For example, word alignment can be simplified by packing (Ma et al., 2007) several consecutive words together. Word alignment and tokenization can also be optimized by maximizing the likelihood of bilingual corpus (Chung and Gildea, 2009; Xu et al., 2008). In fact, these work are orthogonal to our joint method, since they focus on training step while we are concerned of decoding. We believe we can further the performance by combining these two kinds of work. Our work also has connections to multilingual tokenization (Snyder and Barzilay, 2008). While they have verified that tokenization can be improved by multilingual learning, our work shows that we can also improve tokenization by collaborating with translation task in a supervised way. More recently, Liu and Liu (2010) also shows the effect of joint method. They integrate parsing and transl"
C10-1135,W03-1728,0,0.0308134,"amework. We expect tokenization and translation could collaborate with each other. Tokenization offers translation with good tokenized results, while translation helps tokenization to eliminate ambiguity. Formally, the probability of a derivation D is represented as P (D) ∝ Y λi φi (D) (1) i Since our model is still a string-based model, the CKY algorithm and cube pruning are still applicable for our model to find the derivation with max score. 3.2 Adding Tokenization Features Maximum Entropy model (ME). We first introduce ME model feature for tokenization by casting it as a labeling problem (Xue and Shen, 2003; Ng and Low, 2004). We label a character with the following 4 types: • b: the begin of a word • m: the middle of a word • e: the end of a word • s: a single-character word Taking the tokenization you-wang of the string you wang for example, we first create a label sequence b e for the tokenization you-wang and then calculate the probability of tokenization by P (you-wang |you wang) = P (b e |you wang) = P (b |you, you wang) × P (e |wang, you wang) Given a tokenization w1L with L words for a character sequence cn1 , we firstly create labels l1n for every characters and then calculate the proba"
C10-1135,W03-1730,1,0.832219,"Missing"
C10-1135,W08-0335,0,0.180017,"en obtain a parse tree or forest of the sentence. As shown in Figure 1(a), we refer to this pipeline as separate tokenization and translation because they are divided into single steps. As tokenization for many languages is usually ambiguous, SMT systems that separate tokenization and translation suffer from a major drawback: tokenization errors potentially introduce translation mistakes. As some languages such as Chinese have no spaces in their writing systems, how to segment sentences into appropriate words has a direct impact on translation performance (Xu et al., 2005; Chang et al., 2008; Zhang et al., 2008). In addition, although agglutinative languages such as Korean incorporate spaces between “words”, which consist of multiple morphemes, the granularity is too coarse and makes the training data 1200 Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1200–1208, Beijing, August 2010 considerably sparse. Studies reveal that segmenting “words” into morphemes effectively improves translating morphologically rich languages (Oflazer, 2008). More importantly, a tokenization close to a gold standard does not necessarily leads to better translation quality"
C10-1135,D08-1022,0,\N,Missing
C10-1135,I05-3027,0,\N,Missing
C10-2033,P05-1066,0,0.0967175,"s, including beam size, phrase table limit and so on, were the same, so the performance was compared fairly. First, we will show the performance of shiftreduce algorithm on three data sets with large training data sets (Section 4.1). Then, we will analyze the performance elaborately in terms of accuracy, speed and search ability with a smaller 289 training data set (Section 4.2). All experiments were done on Chinese-to-English translation tasks and all results are reported with case insensitive BLEU score. Statistical significance were computed using the sign-test described in Collins et al. (Collins et al., 2005). 4.1 Performance Evaluation NIST06 30.24 30.27 30.35 —— 30.47 NIST08 25.08 23.80 26.23** 25.09 26.67** speed 4.827 1.501 4.335 3.856 4.126 NIST05 35.80 35.03 36.56** 35.84 36.42** speed 7.142 1.811 6.276 5.008 5.432 (a) We did three experiments to compare the performance of the shift-reduce decoder, Moses and the decoder with ITG constraints using cover vector (denoted as CV). 2 The shift-reduce decoder decoded with two sets of parameters: one was tuned by itself (denoted as SR) and the other was tuned by Moses (denoted as SR-same), using MERT (Och, 2003). Two searching algorithms of Moses ar"
C10-2033,D08-1089,0,0.198885,"ransitions LShift and RShift push [i, j] into St , they check whether [i, j] is adjacent to the top block of St . If so, they change the top block into the merged block directly. In practical implementation, in order to further restrict search space, distortion limit is applied besides ITG constraints: a source phrase can be covered next only when it is ITG-legal and its distortion does not exceed distortion limit. The distortion d is calculated by d = |starti − endi−1 − 1|, where starti is the start position of the current phrase and endi−1 is the last position of the last translated phrase. Galley and Manning (2008) present a hierarchical phrase reordering model aimed at improving non-local reorderings. Via the hierarchical mergence of two blocks, the orientation of long distance words can be computed. Their shift-reduce algorithm does not import ITG constraints and admits the translation violating ITG constraints. Zens et al. (2004) introduce a left-toright decoding algorithm with ITG constraints on the alignment template system (Och et al., 1999). Their algorithm processes candidate source phrases one by one through the whole search space and checks if the candidate phrase complies with ITG constraints"
C10-2033,W05-1506,0,0.0420627,"models easily. Xiong et al. (2006) propose a BTG-based model, which uses the context to determine the orientation of two adjacent spans. It employs the cube-time CYK algorithm. 4 3 Related Work St [0] [0][5] [0][5][1] [0][5][1][3] [0][5][1][3][4] [0][5][1][3, 4] [0][5][1][3, 4][2] [0][5][1][2, 4] [0][5][1, 4] [0][1, 5] [0, 5] [0, 5][6] [0, 6] Experiments We compare the shift-reduce decoder with the state-of-the-art decoder Moses (Koehn et al., 2007). The shift-reduce decoder was implemented by modifying the normal search algorithm of Moses to our shift-reduce algorithm, without cube pruning (Huang and Chiang, 2005). We retained the features of Moses: four translation features, three lexical reordering features (straight, inverted and discontinuous), linear distortion, phrase penalty, word penalty and language model, without importing any new feature. The decoding configurations used by all the decoders, including beam size, phrase table limit and so on, were the same, so the performance was compared fairly. First, we will show the performance of shiftreduce algorithm on three data sets with large training data sets (Section 4.1). Then, we will analyze the performance elaborately in terms of accuracy, sp"
C10-2033,J99-4005,0,0.793313,"resent a shift-reduce decoding algorithm that can generate ITG-legal translation from left to right in linear time. This algorithm runs in a reduce-eager style and is suited to phrase-based models. Using the state-ofthe-art decoder Moses as the baseline, experiment results show that the shift-reduce algorithm can significantly improve both the accuracy and the speed on different test sets. 1 Introduction In statistical machine translation, for the diversity of natural languages, the word order of source and target language may differ and searching through all possible translations is NP-hard (Knight, 1999). So some measures have to be taken to reduce search space: either using a search algorithm with pruning technique or restricting possible reorderings. Currently, beam search is widely used (Tillmann and Ney, 2003; Koehn, 2004) to reduce search space. However, the pruning technique adopted by this algorithm is not risk-free. As a result, the best partial translation may be ruled out during pruning. The more aggressive the pruning is, the more likely the best translation escapes. There should be a tradeoff between the speed and the accuracy. If some heuristic knowledge is employed to guide the"
C10-2033,P07-2045,0,0.011696,"ple in Figure 2. The top nine transitions correspond to Figure 3 (a), ... , Figure 3 (i), respectively. the help of ITG structure, it can be extended to syntax-based models easily. Xiong et al. (2006) propose a BTG-based model, which uses the context to determine the orientation of two adjacent spans. It employs the cube-time CYK algorithm. 4 3 Related Work St [0] [0][5] [0][5][1] [0][5][1][3] [0][5][1][3][4] [0][5][1][3, 4] [0][5][1][3, 4][2] [0][5][1][2, 4] [0][5][1, 4] [0][1, 5] [0, 5] [0, 5][6] [0, 6] Experiments We compare the shift-reduce decoder with the state-of-the-art decoder Moses (Koehn et al., 2007). The shift-reduce decoder was implemented by modifying the normal search algorithm of Moses to our shift-reduce algorithm, without cube pruning (Huang and Chiang, 2005). We retained the features of Moses: four translation features, three lexical reordering features (straight, inverted and discontinuous), linear distortion, phrase penalty, word penalty and language model, without importing any new feature. The decoding configurations used by all the decoders, including beam size, phrase table limit and so on, were the same, so the performance was compared fairly. First, we will show the perfor"
C10-2033,koen-2004-pharaoh,0,0.0590687,"der Moses as the baseline, experiment results show that the shift-reduce algorithm can significantly improve both the accuracy and the speed on different test sets. 1 Introduction In statistical machine translation, for the diversity of natural languages, the word order of source and target language may differ and searching through all possible translations is NP-hard (Knight, 1999). So some measures have to be taken to reduce search space: either using a search algorithm with pruning technique or restricting possible reorderings. Currently, beam search is widely used (Tillmann and Ney, 2003; Koehn, 2004) to reduce search space. However, the pruning technique adopted by this algorithm is not risk-free. As a result, the best partial translation may be ruled out during pruning. The more aggressive the pruning is, the more likely the best translation escapes. There should be a tradeoff between the speed and the accuracy. If some heuristic knowledge is employed to guide the search, the search algorithm can discard some implausible hypotheses in advance and focus on more possible ones. Inversion Transduction Grammars (ITGs) permit a minimal extra degree of ordering flexibility and are particularly"
C10-2033,W99-0604,0,0.0555584,"by d = |starti − endi−1 − 1|, where starti is the start position of the current phrase and endi−1 is the last position of the last translated phrase. Galley and Manning (2008) present a hierarchical phrase reordering model aimed at improving non-local reorderings. Via the hierarchical mergence of two blocks, the orientation of long distance words can be computed. Their shift-reduce algorithm does not import ITG constraints and admits the translation violating ITG constraints. Zens et al. (2004) introduce a left-toright decoding algorithm with ITG constraints on the alignment template system (Och et al., 1999). Their algorithm processes candidate source phrases one by one through the whole search space and checks if the candidate phrase complies with ITG constraints. Besides, their algorithm checks validity via cover vector and does not formalize ITG structure. The shift-reduce decoding algorithm holds ITG structure via three stacks. As a result, it can offer ITG-legal spans directly and decode faster. Furthermore, with Sl ∅ [1, 4] ∅ [2] [2] [2] ∅ ∅ ∅ ∅ ∅ ∅ ∅ Sr [1, 6] [6] [2, 4][6] [4][6] [6] [6] [6] [6] [6] [6] [6] ∅ ∅ Figure 5: Transition sequence for the example in Figure 2. The top nine transi"
C10-2033,P03-1021,0,0.0707834,"scribed in Collins et al. (Collins et al., 2005). 4.1 Performance Evaluation NIST06 30.24 30.27 30.35 —— 30.47 NIST08 25.08 23.80 26.23** 25.09 26.67** speed 4.827 1.501 4.335 3.856 4.126 NIST05 35.80 35.03 36.56** 35.84 36.42** speed 7.142 1.811 6.276 5.008 5.432 (a) We did three experiments to compare the performance of the shift-reduce decoder, Moses and the decoder with ITG constraints using cover vector (denoted as CV). 2 The shift-reduce decoder decoded with two sets of parameters: one was tuned by itself (denoted as SR) and the other was tuned by Moses (denoted as SR-same), using MERT (Och, 2003). Two searching algorithms of Moses are considered: one is the normal search algorithm without cubing pruning (denoted as Moses), the other is the search algorithm with cube pruning (denoted as Moses-cb). For all the decoders, the distortion limit was set to 6, the nbest size was set to 100 and the phrase table limit was 50. In the first experiment, the development set is part of NIST MT06 data set including 862 sentences, the test set is NIST MT08 data set and the training data set contains 5 million sentence pairs. We used a 5-gram language model which were trained on the Xinhua and AFP port"
C10-2033,J03-1005,0,0.0606595,"the state-ofthe-art decoder Moses as the baseline, experiment results show that the shift-reduce algorithm can significantly improve both the accuracy and the speed on different test sets. 1 Introduction In statistical machine translation, for the diversity of natural languages, the word order of source and target language may differ and searching through all possible translations is NP-hard (Knight, 1999). So some measures have to be taken to reduce search space: either using a search algorithm with pruning technique or restricting possible reorderings. Currently, beam search is widely used (Tillmann and Ney, 2003; Koehn, 2004) to reduce search space. However, the pruning technique adopted by this algorithm is not risk-free. As a result, the best partial translation may be ruled out during pruning. The more aggressive the pruning is, the more likely the best translation escapes. There should be a tradeoff between the speed and the accuracy. If some heuristic knowledge is employed to guide the search, the search algorithm can discard some implausible hypotheses in advance and focus on more possible ones. Inversion Transduction Grammars (ITGs) permit a minimal extra degree of ordering flexibility and are"
C10-2033,P96-1021,0,0.0604209,"pted by this algorithm is not risk-free. As a result, the best partial translation may be ruled out during pruning. The more aggressive the pruning is, the more likely the best translation escapes. There should be a tradeoff between the speed and the accuracy. If some heuristic knowledge is employed to guide the search, the search algorithm can discard some implausible hypotheses in advance and focus on more possible ones. Inversion Transduction Grammars (ITGs) permit a minimal extra degree of ordering flexibility and are particularly well suited to modeling ordering shifts between languages (Wu, 1996; Wu, 1997). They can well balance the needed flexibility against complexity constraints. Recently, ITG has been successfully applied to statistical machine translation (Zens and Ney, 2003; Zens et al., 2004; Xiong et al., 2006). However, ITG generally employs the expensive CYK parsing algorithm which runs in cube time. In addition, the CYK algorithm can not calculate language model exactly in the process of decoding, as it can not catch the full history context of the left words in a hypothesis. In this paper, we introduce a shift-reduce decoding algorithm with ITG constraints which runs in a"
C10-2033,J97-3002,0,0.701493,"is algorithm is not risk-free. As a result, the best partial translation may be ruled out during pruning. The more aggressive the pruning is, the more likely the best translation escapes. There should be a tradeoff between the speed and the accuracy. If some heuristic knowledge is employed to guide the search, the search algorithm can discard some implausible hypotheses in advance and focus on more possible ones. Inversion Transduction Grammars (ITGs) permit a minimal extra degree of ordering flexibility and are particularly well suited to modeling ordering shifts between languages (Wu, 1996; Wu, 1997). They can well balance the needed flexibility against complexity constraints. Recently, ITG has been successfully applied to statistical machine translation (Zens and Ney, 2003; Zens et al., 2004; Xiong et al., 2006). However, ITG generally employs the expensive CYK parsing algorithm which runs in cube time. In addition, the CYK algorithm can not calculate language model exactly in the process of decoding, as it can not catch the full history context of the left words in a hypothesis. In this paper, we introduce a shift-reduce decoding algorithm with ITG constraints which runs in a left-to-ri"
C10-2033,P06-1066,1,0.934889,"radeoff between the speed and the accuracy. If some heuristic knowledge is employed to guide the search, the search algorithm can discard some implausible hypotheses in advance and focus on more possible ones. Inversion Transduction Grammars (ITGs) permit a minimal extra degree of ordering flexibility and are particularly well suited to modeling ordering shifts between languages (Wu, 1996; Wu, 1997). They can well balance the needed flexibility against complexity constraints. Recently, ITG has been successfully applied to statistical machine translation (Zens and Ney, 2003; Zens et al., 2004; Xiong et al., 2006). However, ITG generally employs the expensive CYK parsing algorithm which runs in cube time. In addition, the CYK algorithm can not calculate language model exactly in the process of decoding, as it can not catch the full history context of the left words in a hypothesis. In this paper, we introduce a shift-reduce decoding algorithm with ITG constraints which runs in a left-to-right manner. This algorithm parses source words in the order of their corresponding translations on the target side. In the meantime, it gives all candidate ITG-legal reorderings. The shift-reduce algorithm is differen"
C10-2033,P03-1019,0,0.245335,"ranslation escapes. There should be a tradeoff between the speed and the accuracy. If some heuristic knowledge is employed to guide the search, the search algorithm can discard some implausible hypotheses in advance and focus on more possible ones. Inversion Transduction Grammars (ITGs) permit a minimal extra degree of ordering flexibility and are particularly well suited to modeling ordering shifts between languages (Wu, 1996; Wu, 1997). They can well balance the needed flexibility against complexity constraints. Recently, ITG has been successfully applied to statistical machine translation (Zens and Ney, 2003; Zens et al., 2004; Xiong et al., 2006). However, ITG generally employs the expensive CYK parsing algorithm which runs in cube time. In addition, the CYK algorithm can not calculate language model exactly in the process of decoding, as it can not catch the full history context of the left words in a hypothesis. In this paper, we introduce a shift-reduce decoding algorithm with ITG constraints which runs in a left-to-right manner. This algorithm parses source words in the order of their corresponding translations on the target side. In the meantime, it gives all candidate ITG-legal reorderings"
C10-2033,C04-1030,0,\N,Missing
C10-2059,P06-1109,0,0.0640572,", the projected parser leads to translation performance comparable with using a supervised parser trained on thousands of annotated trees. 1 Introduction In recent years, supervised constituent parsing has been well studied and achieves the state-of-the-art for many resource-rich languages (Collins, 1999; Charniak, 2000; Petrov et al., 2006). Because of the cost and difficulty in treebank construction, researchers have also investigated the utilization of unannotated text, including the unsupervised parsing which totally uses unannotated data (Klein and Manning, 2002; Klein and Manning, 2004; Bod, 2006; Seginer, 2007), and the semisupervised parsing which uses both annotated and unannotated data (Sarkar, 2001; Steedman et al., 2003; McClosky et al., 2006). Because of the higher complexity and lower performance of unsupervised methods, as well as In this paper we propose for constituent projection a stepwise but totally automatic strategy, which performs constituent projection on the basis of dependency projection, and then use a constraint EM optimization algorithm to optimized the initially projected trees. Given a word-aligned bilingual corpus with source sentences parsed, we first projec"
C10-2059,W08-2102,0,0.020283,"target language than a supervised-trained parser dose. 3.1 Boost an Traditional Parser We first establish a unified framework for the enhanced parser where a projected parser is adopted to guide the parsing procedure of the baseline parser. For a given target sentence S, the enhanced parser selected the best parse T˜ among the set of candidates Ω(S) according to two evaluation functions, given by the baseline parser B and the projected guide parser G, respectively. T˜ = argmax P (T |B) × P (T |G)λ (4) T ∈Ω(S) These two evaluation functions can be integrated deeply into the decoding procedure (Carreras et al., 2008; Zhang and Clark, 2008; Huang, 2008), or can be integrated at a shallow level in a reranking manner (Collins, 2000; Charniak and Johnson, 2005). For simplicity and generability, we adopt the reranking strategy. In k-best reranking, Ω(S) is simply a set of candidate parses, denoted as {T1 , T2 , ..., Tk }, and we use the single parse of the guide parser, TG , to re-evaluate these candidates. Formula 4 can be redefined as T˜(TG ) = argmax w · f (T, TG ) (5) T ∈Ω(S) Here, f (T, TG ) and w represent a high dimensional feature representation and a corresponding weight vector, respectively. The fir"
C10-2059,P06-1043,0,0.0541073,"troduction In recent years, supervised constituent parsing has been well studied and achieves the state-of-the-art for many resource-rich languages (Collins, 1999; Charniak, 2000; Petrov et al., 2006). Because of the cost and difficulty in treebank construction, researchers have also investigated the utilization of unannotated text, including the unsupervised parsing which totally uses unannotated data (Klein and Manning, 2002; Klein and Manning, 2004; Bod, 2006; Seginer, 2007), and the semisupervised parsing which uses both annotated and unannotated data (Sarkar, 2001; Steedman et al., 2003; McClosky et al., 2006). Because of the higher complexity and lower performance of unsupervised methods, as well as In this paper we propose for constituent projection a stepwise but totally automatic strategy, which performs constituent projection on the basis of dependency projection, and then use a constraint EM optimization algorithm to optimized the initially projected trees. Given a word-aligned bilingual corpus with source sentences parsed, we first project the dependency structures of these constituent trees to the target sentences using a dynamic programming algorithm, then we generate a set of candidate co"
C10-2059,A00-2018,0,0.0555991,"PCFG-style parsing procedure is then used to search for the most probable projected constituent tree. Experiments show that, the parser trained on the projected treebank can significantly boost a state-of-the-art supervised parser. When integrated into a tree-based machine translation system, the projected parser leads to translation performance comparable with using a supervised parser trained on thousands of annotated trees. 1 Introduction In recent years, supervised constituent parsing has been well studied and achieves the state-of-the-art for many resource-rich languages (Collins, 1999; Charniak, 2000; Petrov et al., 2006). Because of the cost and difficulty in treebank construction, researchers have also investigated the utilization of unannotated text, including the unsupervised parsing which totally uses unannotated data (Klein and Manning, 2002; Klein and Manning, 2004; Bod, 2006; Seginer, 2007), and the semisupervised parsing which uses both annotated and unannotated data (Sarkar, 2001; Steedman et al., 2003; McClosky et al., 2006). Because of the higher complexity and lower performance of unsupervised methods, as well as In this paper we propose for constituent projection a stepwise"
C10-2059,H05-1066,0,0.0407709,"tituent projection which is conducted on projected dependent structures, and finally show the constraint EM procedure for constituent optimization. 2.1 Algorithm 1 Dependency projection. (1) Pe can then be obtained by simple accumulation across all possible situations of correspondence Pe (x y y|DE , A) X Ax,x′ × Ay,y′ × δ(x′ , y ′ |DE ) = 1≤x′ ,y ′ ≤|E| where δ(x′ , y ′ |DE ) is a 0-1 function that equals 1 only if the dependent relation x′ y y ′ holds in DE . The search procedure needed by the argmax operation in equation 1 can be effectively solved by the Chu-Liu-Edmonds algorithm used in (McDonald et al., 2005). In this work, however, we adopt a more general and simple dynamic programming algorithm as shown in Algorithm 1, in order to facilitate the possible expansions. In practice, the cube-pruning strategy (Huang and Chiang, 2005) is used to speed up the enumeration of derivations (loops started by line 4 and 5). 2.2 Constituent Projection The PCFG-style parsing procedure searches for the most probable projected constituent tree in a shrunken search space determined by the projected dependency structure and the target constituent tree. The shrunken search space can be built as following. First, we"
C10-2059,W02-1001,0,0.100295,"able to cover this span, and attach a asterisk at the tail of this non-terminal. Here is an example of the guide features f100 (T, TG ) = V P ∈ T ◦ P P ∗ ∈ TG It represents that a V P in the candidate parse corresponds to a segment of a P P in the projected parse. The quantity of its weight w100 indicates how probably a span can be predicated as V P if the span corresponds to a partial P P in the projected parse. We adopt the perceptron algorithm to train the reranker. To reduce overfitting and produce a more stable weight vector, we also use a refinement strategy called averaged parameters (Collins, 2002). 3.2 Using in Machine Translation Researchers have achieved promising improvements in tree-based machine translation (Liu et al., 2006; Huang et al., 2006). Such models use a parsed tree as input and converts it into a target tree or string. Given a source language sentence, first we use a traditional source language parser to parse the sentence to obtain the syntax tree T , and then use the translation decoder to search for ˜ where a derivation d is a sethe best derivation d, quence of transformations that converts the source tree into the target language string d˜ = argmax P (d|T ) (6) d∈D"
C10-2059,P09-1042,0,0.204465,"Missing"
C10-2059,W05-1506,0,0.0283388,"accumulation across all possible situations of correspondence Pe (x y y|DE , A) X Ax,x′ × Ay,y′ × δ(x′ , y ′ |DE ) = 1≤x′ ,y ′ ≤|E| where δ(x′ , y ′ |DE ) is a 0-1 function that equals 1 only if the dependent relation x′ y y ′ holds in DE . The search procedure needed by the argmax operation in equation 1 can be effectively solved by the Chu-Liu-Edmonds algorithm used in (McDonald et al., 2005). In this work, however, we adopt a more general and simple dynamic programming algorithm as shown in Algorithm 1, in order to facilitate the possible expansions. In practice, the cube-pruning strategy (Huang and Chiang, 2005) is used to speed up the enumeration of derivations (loops started by line 4 and 5). 2.2 Constituent Projection The PCFG-style parsing procedure searches for the most probable projected constituent tree in a shrunken search space determined by the projected dependency structure and the target constituent tree. The shrunken search space can be built as following. First, we generates the candidate constituents of the source tree and the candidate spans of the target sentence, so as to enumerate the candidate constituents of the target sentence. Then we compute the consistent degree for 517 has L"
C10-2059,2006.amta-papers.8,0,0.0222814,"P ∗ ∈ TG It represents that a V P in the candidate parse corresponds to a segment of a P P in the projected parse. The quantity of its weight w100 indicates how probably a span can be predicated as V P if the span corresponds to a partial P P in the projected parse. We adopt the perceptron algorithm to train the reranker. To reduce overfitting and produce a more stable weight vector, we also use a refinement strategy called averaged parameters (Collins, 2002). 3.2 Using in Machine Translation Researchers have achieved promising improvements in tree-based machine translation (Liu et al., 2006; Huang et al., 2006). Such models use a parsed tree as input and converts it into a target tree or string. Given a source language sentence, first we use a traditional source language parser to parse the sentence to obtain the syntax tree T , and then use the translation decoder to search for ˜ where a derivation d is a sethe best derivation d, quence of transformations that converts the source tree into the target language string d˜ = argmax P (d|T ) (6) d∈D 1 Using non-terminals as features brings no improvement in the reranking experiments, so as to examine the impact of the projected parser. 520 Here D is the"
C10-2059,P08-1067,0,0.0165313,"er dose. 3.1 Boost an Traditional Parser We first establish a unified framework for the enhanced parser where a projected parser is adopted to guide the parsing procedure of the baseline parser. For a given target sentence S, the enhanced parser selected the best parse T˜ among the set of candidates Ω(S) according to two evaluation functions, given by the baseline parser B and the projected guide parser G, respectively. T˜ = argmax P (T |B) × P (T |G)λ (4) T ∈Ω(S) These two evaluation functions can be integrated deeply into the decoding procedure (Carreras et al., 2008; Zhang and Clark, 2008; Huang, 2008), or can be integrated at a shallow level in a reranking manner (Collins, 2000; Charniak and Johnson, 2005). For simplicity and generability, we adopt the reranking strategy. In k-best reranking, Ω(S) is simply a set of candidate parses, denoted as {T1 , T2 , ..., Tk }, and we use the single parse of the guide parser, TG , to re-evaluate these candidates. Formula 4 can be redefined as T˜(TG ) = argmax w · f (T, TG ) (5) T ∈Ω(S) Here, f (T, TG ) and w represent a high dimensional feature representation and a corresponding weight vector, respectively. The first feature f1 (T, TG ) = logP (T |B)"
C10-2059,P02-1050,0,0.0558792,"Missing"
C10-2059,P02-1017,0,0.0419431,"rated into a tree-based machine translation system, the projected parser leads to translation performance comparable with using a supervised parser trained on thousands of annotated trees. 1 Introduction In recent years, supervised constituent parsing has been well studied and achieves the state-of-the-art for many resource-rich languages (Collins, 1999; Charniak, 2000; Petrov et al., 2006). Because of the cost and difficulty in treebank construction, researchers have also investigated the utilization of unannotated text, including the unsupervised parsing which totally uses unannotated data (Klein and Manning, 2002; Klein and Manning, 2004; Bod, 2006; Seginer, 2007), and the semisupervised parsing which uses both annotated and unannotated data (Sarkar, 2001; Steedman et al., 2003; McClosky et al., 2006). Because of the higher complexity and lower performance of unsupervised methods, as well as In this paper we propose for constituent projection a stepwise but totally automatic strategy, which performs constituent projection on the basis of dependency projection, and then use a constraint EM optimization algorithm to optimized the initially projected trees. Given a word-aligned bilingual corpus with sour"
C10-2059,P04-1061,0,0.0442519,"achine translation system, the projected parser leads to translation performance comparable with using a supervised parser trained on thousands of annotated trees. 1 Introduction In recent years, supervised constituent parsing has been well studied and achieves the state-of-the-art for many resource-rich languages (Collins, 1999; Charniak, 2000; Petrov et al., 2006). Because of the cost and difficulty in treebank construction, researchers have also investigated the utilization of unannotated text, including the unsupervised parsing which totally uses unannotated data (Klein and Manning, 2002; Klein and Manning, 2004; Bod, 2006; Seginer, 2007), and the semisupervised parsing which uses both annotated and unannotated data (Sarkar, 2001; Steedman et al., 2003; McClosky et al., 2006). Because of the higher complexity and lower performance of unsupervised methods, as well as In this paper we propose for constituent projection a stepwise but totally automatic strategy, which performs constituent projection on the basis of dependency projection, and then use a constraint EM optimization algorithm to optimized the initially projected trees. Given a word-aligned bilingual corpus with source sentences parsed, we f"
C10-2059,P06-1077,1,0.947086,"lts validate the effectiveness of our approach. On the Chinese-English FBIS corpus, we project the English parses produced by the Charniak parser across to the Chinese sen516 Coling 2010: Poster Volume, pages 516–524, Beijing, August 2010 tences. A berkeley parser trained on this projected treebank can effectively boost the supervised parsers trained on bunches of CTB trees. Especially, the supervised parser trained on the smaller CTB 1.0 benefits a significant F-measure increment of more than 1 point from the projected parser. When using the projected parser in a treebased translation model (Liu et al., 2006), we achieve translation performance comparable with using a state-of-the-art supervised parser trained on thousands of CTB trees. This surprising result gives us an inspiration that better translation would be achieved by combining both projected parsing and supervised parsing into a hybrid parsing schema. 2 Stepwise Constituent Projection Dependency Projection For dependency projection we adopt a dynamic programming algorithm, which searches the most probable projected target dependency structure according to the source dependency structure and the word alignment. In order to mitigate the ef"
C10-2059,D09-1106,1,0.833412,"n 40 words, and evaluate the translation quality by the case-sensitive BLEU-4 metric (Papineni et al., 2002) with 4 references. 4.1 Constituent Projection We perform constituent projection from English to Chinese on the FBIS corpus, which contains 239K sentence pairs with about 6.9M/8.9M words in Chinese/English. The English sentences are parsed by the Charniak Parser and the dependency structures are extracted from these parses according to the head-finding rules of (Yamada and Matsumoto, 2003). The word alignment matrixes are obtained by combining the 10-best results of GIZA++ according to (Liu et al., 2009). We first project the dependency structures from English to Chinese according to section 2.1, and then project the constituent structures according to section 2.2. We define an assessment criteria to evaluate the confidence of the final projected constituent tree p c = n P (DF |DE , A) × P (TF |TE , A) where n is the word count of a Chinese sentence in our experiments. A series of projected ChiThres c 0.5 0.4 0.3 0.2 0.1 #Resrv 12.6K 17.8K 27.2K 45.1K 87.0K Cons-F1 23.9 23.9 25.4 26.6 27.8 Span-F1 32.7 33.4 35.7 38.0 40.4 Table 1: Performances of the projected parsers on the CTB test set. #Re"
C10-2059,P00-1056,0,0.0564898,"with using a state-of-the-art supervised parser trained on thousands of CTB trees. This surprising result gives us an inspiration that better translation would be achieved by combining both projected parsing and supervised parsing into a hybrid parsing schema. 2 Stepwise Constituent Projection Dependency Projection For dependency projection we adopt a dynamic programming algorithm, which searches the most probable projected target dependency structure according to the source dependency structure and the word alignment. In order to mitigate the effect of word alignment errors, multiple GIZA++ (Och and Ney, 2000) results are combined into a compact representation called alignment matrix. Given a source sentence with m words, represented as E1:m , and a target sentence with n words, represented as F1:n , their word alignment matrix A is an m × n matrix, where each element Ai,j denotes the probability of the source word Ei aligned to the target word Fj . Using P (DF |DE , A) to denote the probability of the projected target dependency structure DF conditioned on the source dependency structure DE and the alignment matrix A, the projection algorithm aims to find D˜F = argmax P (DF |DE , A) DF 1: 2: 3: 4:"
C10-2059,P03-1021,0,0.0185498,"Missing"
C10-2059,P02-1040,0,0.080704,"cted parser to parse the input sentence for the subsequent translation decoding procedure. 4 Experiments In this section, we first invalidate the effect of constituent projection by evaluating a parser trained on the projected treebank. Then we investigate two applications of the projected parser: boosting an traditional supervised-trained parser, and integration in a tree-based machine translation system. Following the previous works, we depict the parsing performance by F-score on sentences with no more than 40 words, and evaluate the translation quality by the case-sensitive BLEU-4 metric (Papineni et al., 2002) with 4 references. 4.1 Constituent Projection We perform constituent projection from English to Chinese on the FBIS corpus, which contains 239K sentence pairs with about 6.9M/8.9M words in Chinese/English. The English sentences are parsed by the Charniak Parser and the dependency structures are extracted from these parses according to the head-finding rules of (Yamada and Matsumoto, 2003). The word alignment matrixes are obtained by combining the 10-best results of GIZA++ according to (Liu et al., 2009). We first project the dependency structures from English to Chinese according to section 2"
C10-2059,P06-1055,0,0.075058,"ing procedure is then used to search for the most probable projected constituent tree. Experiments show that, the parser trained on the projected treebank can significantly boost a state-of-the-art supervised parser. When integrated into a tree-based machine translation system, the projected parser leads to translation performance comparable with using a supervised parser trained on thousands of annotated trees. 1 Introduction In recent years, supervised constituent parsing has been well studied and achieves the state-of-the-art for many resource-rich languages (Collins, 1999; Charniak, 2000; Petrov et al., 2006). Because of the cost and difficulty in treebank construction, researchers have also investigated the utilization of unannotated text, including the unsupervised parsing which totally uses unannotated data (Klein and Manning, 2002; Klein and Manning, 2004; Bod, 2006; Seginer, 2007), and the semisupervised parsing which uses both annotated and unannotated data (Sarkar, 2001; Steedman et al., 2003; McClosky et al., 2006). Because of the higher complexity and lower performance of unsupervised methods, as well as In this paper we propose for constituent projection a stepwise but totally automatic"
C10-2059,N01-1023,0,0.160907,"on thousands of annotated trees. 1 Introduction In recent years, supervised constituent parsing has been well studied and achieves the state-of-the-art for many resource-rich languages (Collins, 1999; Charniak, 2000; Petrov et al., 2006). Because of the cost and difficulty in treebank construction, researchers have also investigated the utilization of unannotated text, including the unsupervised parsing which totally uses unannotated data (Klein and Manning, 2002; Klein and Manning, 2004; Bod, 2006; Seginer, 2007), and the semisupervised parsing which uses both annotated and unannotated data (Sarkar, 2001; Steedman et al., 2003; McClosky et al., 2006). Because of the higher complexity and lower performance of unsupervised methods, as well as In this paper we propose for constituent projection a stepwise but totally automatic strategy, which performs constituent projection on the basis of dependency projection, and then use a constraint EM optimization algorithm to optimized the initially projected trees. Given a word-aligned bilingual corpus with source sentences parsed, we first project the dependency structures of these constituent trees to the target sentences using a dynamic programming al"
C10-2059,P07-1049,0,0.0499131,"cted parser leads to translation performance comparable with using a supervised parser trained on thousands of annotated trees. 1 Introduction In recent years, supervised constituent parsing has been well studied and achieves the state-of-the-art for many resource-rich languages (Collins, 1999; Charniak, 2000; Petrov et al., 2006). Because of the cost and difficulty in treebank construction, researchers have also investigated the utilization of unannotated text, including the unsupervised parsing which totally uses unannotated data (Klein and Manning, 2002; Klein and Manning, 2004; Bod, 2006; Seginer, 2007), and the semisupervised parsing which uses both annotated and unannotated data (Sarkar, 2001; Steedman et al., 2003; McClosky et al., 2006). Because of the higher complexity and lower performance of unsupervised methods, as well as In this paper we propose for constituent projection a stepwise but totally automatic strategy, which performs constituent projection on the basis of dependency projection, and then use a constraint EM optimization algorithm to optimized the initially projected trees. Given a word-aligned bilingual corpus with source sentences parsed, we first project the dependency"
C10-2059,D09-1086,0,0.0410601,"fective Constituent Projection across Languages Wenbin Jiang and Yajuan Lu¨ and Yang Liu and Qun Liu Key Laboratory of Intelligent Information Processing Institute of Computing Technology Chinese Academy of Sciences {jiangwenbin, lvyajuan, yliu, liuqun}@ict.ac.cn Abstract the need of reliable priori knowledge in semisupervised methods, it seems promising to project the syntax structures from a resource-rich language to a resource-scarce one across a bilingual corpus. Lots of researches have so far been devoted to dependency projection (Hwa et al., 2002; Hwa et al., 2005; Ganchev et al., 2009; Smith and Eisner, 2009). While for constituent projection there is few progress. This is due to the fact that the constituent syntax describes the language structure in a more detailed way, and the degree of isomorphism between constituent structures appears much lower. We describe an effective constituent projection strategy, where constituent projection is performed on the basis of dependency projection. Especially, a novel measurement is proposed to evaluate the candidate projected constituents for a target language sentence, and a PCFG-style parsing procedure is then used to search for the most probable projecte"
C10-2059,E03-1008,0,0.0901041,"f annotated trees. 1 Introduction In recent years, supervised constituent parsing has been well studied and achieves the state-of-the-art for many resource-rich languages (Collins, 1999; Charniak, 2000; Petrov et al., 2006). Because of the cost and difficulty in treebank construction, researchers have also investigated the utilization of unannotated text, including the unsupervised parsing which totally uses unannotated data (Klein and Manning, 2002; Klein and Manning, 2004; Bod, 2006; Seginer, 2007), and the semisupervised parsing which uses both annotated and unannotated data (Sarkar, 2001; Steedman et al., 2003; McClosky et al., 2006). Because of the higher complexity and lower performance of unsupervised methods, as well as In this paper we propose for constituent projection a stepwise but totally automatic strategy, which performs constituent projection on the basis of dependency projection, and then use a constraint EM optimization algorithm to optimized the initially projected trees. Given a word-aligned bilingual corpus with source sentences parsed, we first project the dependency structures of these constituent trees to the target sentences using a dynamic programming algorithm, then we genera"
C10-2059,I05-1007,1,0.916773,"the CTB annotation standard, but it needs to be validated by the following experiments. 4.2 CTB 1.0 baseline boosted parser 10000 Scale of treebank (log) Figure 1: Log-likelihood of the 87K-projected treebank after each EM interation. Cons-F1 27.8 22.8 CTB 5.0 1000 EM iteration Train Set Original 87K Optimized 87K 88 86 84 82 80 78 76 74 72 70 Boost an Traditional Parser The projected parser is used to help the reranking of the k-best parses produced by another state-ofthe-art parser, which is called the baseline parser for convenience. In our experiments we choose the revised Chinese parser (Xiong et al., 2005) Figure 2: Boosting performance of the projected parser on a series of baseline parsers that are trained on treebanks of different scales. based on Collins model 2 (Collins, 1999) as the baseline parser.2 The baseline parser is respectively trained on CTB 1.0 and CTB 5.0. For both corpora we follow the traditional corpus splitting: chapters 271-300 for testing, chapters 301-325 for development, and else for training. Experimental results are shown in Table 3. We find that both projected parsers bring significant improvement to the baseline parsers. Especially the later, although performs worse"
C10-2059,W03-3023,0,0.0392877,"ne translation system. Following the previous works, we depict the parsing performance by F-score on sentences with no more than 40 words, and evaluate the translation quality by the case-sensitive BLEU-4 metric (Papineni et al., 2002) with 4 references. 4.1 Constituent Projection We perform constituent projection from English to Chinese on the FBIS corpus, which contains 239K sentence pairs with about 6.9M/8.9M words in Chinese/English. The English sentences are parsed by the Charniak Parser and the dependency structures are extracted from these parses according to the head-finding rules of (Yamada and Matsumoto, 2003). The word alignment matrixes are obtained by combining the 10-best results of GIZA++ according to (Liu et al., 2009). We first project the dependency structures from English to Chinese according to section 2.1, and then project the constituent structures according to section 2.2. We define an assessment criteria to evaluate the confidence of the final projected constituent tree p c = n P (DF |DE , A) × P (TF |TE , A) where n is the word count of a Chinese sentence in our experiments. A series of projected ChiThres c 0.5 0.4 0.3 0.2 0.1 #Resrv 12.6K 17.8K 27.2K 45.1K 87.0K Cons-F1 23.9 23.9 25"
C10-2059,D08-1059,0,0.0220778,"supervised-trained parser dose. 3.1 Boost an Traditional Parser We first establish a unified framework for the enhanced parser where a projected parser is adopted to guide the parsing procedure of the baseline parser. For a given target sentence S, the enhanced parser selected the best parse T˜ among the set of candidates Ω(S) according to two evaluation functions, given by the baseline parser B and the projected guide parser G, respectively. T˜ = argmax P (T |B) × P (T |G)λ (4) T ∈Ω(S) These two evaluation functions can be integrated deeply into the decoding procedure (Carreras et al., 2008; Zhang and Clark, 2008; Huang, 2008), or can be integrated at a shallow level in a reranking manner (Collins, 2000; Charniak and Johnson, 2005). For simplicity and generability, we adopt the reranking strategy. In k-best reranking, Ω(S) is simply a set of candidate parses, denoted as {T1 , T2 , ..., Tk }, and we use the single parse of the guide parser, TG , to re-evaluate these candidates. Formula 4 can be redefined as T˜(TG ) = argmax w · f (T, TG ) (5) T ∈Ω(S) Here, f (T, TG ) and w represent a high dimensional feature representation and a corresponding weight vector, respectively. The first feature f1 (T, TG )"
C10-2059,J03-4003,0,\N,Missing
C10-2059,P05-1022,0,\N,Missing
C10-2136,P05-1067,0,0.108695,"ing of the target-side syntax that directly influences the translation quality. As a result, how to obtain better translation by exploiting target syntactic knowledge is somehow neglected. Thus, we argue that it is important to model the target-side syntax in BTG-based translation. Recently, modeling syntactic information on the target side has progressed significantly. Depending on the type of output, these models can be divided into two categories: the constituentoutput systems (Galley et al., 2006; Zhang et al., 2008; Liu et al., 2009) and dependencyoutput systems (Eisner, 2003; Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Shen et al., 2008). Compared with the constituent-output systems, the dependency-output systems provide a simpler platform to capture the target-side syntactic information, while also having the best interlingual phrasal cohesion properties (Fox, 2002). Typically, Shen et al. (2008) propose a string-todependency model, which integrates the targetside well-formed dependency structure into translation rules. With the dependency structure, this system employs a dependency language model (LM) to exploit long distance word relations, and achieves a significant improvement over"
C10-2136,P03-2041,0,0.165157,"while ignoring the modeling of the target-side syntax that directly influences the translation quality. As a result, how to obtain better translation by exploiting target syntactic knowledge is somehow neglected. Thus, we argue that it is important to model the target-side syntax in BTG-based translation. Recently, modeling syntactic information on the target side has progressed significantly. Depending on the type of output, these models can be divided into two categories: the constituentoutput systems (Galley et al., 2006; Zhang et al., 2008; Liu et al., 2009) and dependencyoutput systems (Eisner, 2003; Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Shen et al., 2008). Compared with the constituent-output systems, the dependency-output systems provide a simpler platform to capture the target-side syntactic information, while also having the best interlingual phrasal cohesion properties (Fox, 2002). Typically, Shen et al. (2008) propose a string-todependency model, which integrates the targetside well-formed dependency structure into translation rules. With the dependency structure, this system employs a dependency language model (LM) to exploit long distance word relations, and achie"
C10-2136,W02-1039,0,0.0891427,"cently, modeling syntactic information on the target side has progressed significantly. Depending on the type of output, these models can be divided into two categories: the constituentoutput systems (Galley et al., 2006; Zhang et al., 2008; Liu et al., 2009) and dependencyoutput systems (Eisner, 2003; Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Shen et al., 2008). Compared with the constituent-output systems, the dependency-output systems provide a simpler platform to capture the target-side syntactic information, while also having the best interlingual phrasal cohesion properties (Fox, 2002). Typically, Shen et al. (2008) propose a string-todependency model, which integrates the targetside well-formed dependency structure into translation rules. With the dependency structure, this system employs a dependency language model (LM) to exploit long distance word relations, and achieves a significant improvement over the hierarchical phrase-based system (Chiang, 2007). So 1 A block is a bilingual phrase without maximum length limitation. 1185 Coling 2010: Poster Volume, pages 1185–1193, Beijing, August 2010 we think it will be a promising way to integrate the target-side dependency str"
C10-2136,P06-1121,0,0.0348927,"9). However, these methods mainly focus on the utilization of source syntactic knowledge, while ignoring the modeling of the target-side syntax that directly influences the translation quality. As a result, how to obtain better translation by exploiting target syntactic knowledge is somehow neglected. Thus, we argue that it is important to model the target-side syntax in BTG-based translation. Recently, modeling syntactic information on the target side has progressed significantly. Depending on the type of output, these models can be divided into two categories: the constituentoutput systems (Galley et al., 2006; Zhang et al., 2008; Liu et al., 2009) and dependencyoutput systems (Eisner, 2003; Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Shen et al., 2008). Compared with the constituent-output systems, the dependency-output systems provide a simpler platform to capture the target-side syntactic information, while also having the best interlingual phrasal cohesion properties (Fox, 2002). Typically, Shen et al. (2008) propose a string-todependency model, which integrates the targetside well-formed dependency structure into translation rules. With the dependency structure, this system employs a"
C10-2136,P07-1090,0,0.0199902,"cted increasing attention in statistical machine translation (SMT). In flat reordering model (Wu, 1996; Zens et al., 2004), which assigns constant reordering probabilities depending on the language pairs, BTG constraint proves to be very effective for reducing the search space of phrase reordering. To pursue a better method to predict the order between two neighboring blocks1 , Xiong et al. (2006) present an enhanced BTG with a maximum entropy (ME) based reordering model. Along this line, source-side syntactic knowledge is introduced into the reordering model to improve BTG-based translation (Setiawan et al., 2007; Zhang et al., 2007; Xiong et al., 2008; Zhang and Li, 2009). However, these methods mainly focus on the utilization of source syntactic knowledge, while ignoring the modeling of the target-side syntax that directly influences the translation quality. As a result, how to obtain better translation by exploiting target syntactic knowledge is somehow neglected. Thus, we argue that it is important to model the target-side syntax in BTG-based translation. Recently, modeling syntactic information on the target side has progressed significantly. Depending on the type of output, these models can be d"
C10-2136,P08-1066,0,0.446303,"influences the translation quality. As a result, how to obtain better translation by exploiting target syntactic knowledge is somehow neglected. Thus, we argue that it is important to model the target-side syntax in BTG-based translation. Recently, modeling syntactic information on the target side has progressed significantly. Depending on the type of output, these models can be divided into two categories: the constituentoutput systems (Galley et al., 2006; Zhang et al., 2008; Liu et al., 2009) and dependencyoutput systems (Eisner, 2003; Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Shen et al., 2008). Compared with the constituent-output systems, the dependency-output systems provide a simpler platform to capture the target-side syntactic information, while also having the best interlingual phrasal cohesion properties (Fox, 2002). Typically, Shen et al. (2008) propose a string-todependency model, which integrates the targetside well-formed dependency structure into translation rules. With the dependency structure, this system employs a dependency language model (LM) to exploit long distance word relations, and achieves a significant improvement over the hierarchical phrase-based system (C"
C10-2136,C10-1123,1,0.783262,"Missing"
C10-2136,P96-1021,0,0.0727476,"w that our system achieves significant improvements over the baseline system on various test sets even with fewer phrases. 1 Introduction Bracketing transduction grammar (BTG) (Wu, 1995) is an important subclass of synchronous context free grammar, which employs a special synchronous rewriting mechanism to parse parallel sentence of both languages. Due to the prominent advantages such as the simplicity of grammar and the good coverage of syntactic diversities in different language pairs, BTG has attracted increasing attention in statistical machine translation (SMT). In flat reordering model (Wu, 1996; Zens et al., 2004), which assigns constant reordering probabilities depending on the language pairs, BTG constraint proves to be very effective for reducing the search space of phrase reordering. To pursue a better method to predict the order between two neighboring blocks1 , Xiong et al. (2006) present an enhanced BTG with a maximum entropy (ME) based reordering model. Along this line, source-side syntactic knowledge is introduced into the reordering model to improve BTG-based translation (Setiawan et al., 2007; Zhang et al., 2007; Xiong et al., 2008; Zhang and Li, 2009). However, these met"
C10-2136,P06-1066,1,0.905951,"writing mechanism to parse parallel sentence of both languages. Due to the prominent advantages such as the simplicity of grammar and the good coverage of syntactic diversities in different language pairs, BTG has attracted increasing attention in statistical machine translation (SMT). In flat reordering model (Wu, 1996; Zens et al., 2004), which assigns constant reordering probabilities depending on the language pairs, BTG constraint proves to be very effective for reducing the search space of phrase reordering. To pursue a better method to predict the order between two neighboring blocks1 , Xiong et al. (2006) present an enhanced BTG with a maximum entropy (ME) based reordering model. Along this line, source-side syntactic knowledge is introduced into the reordering model to improve BTG-based translation (Setiawan et al., 2007; Zhang et al., 2007; Xiong et al., 2008; Zhang and Li, 2009). However, these methods mainly focus on the utilization of source syntactic knowledge, while ignoring the modeling of the target-side syntax that directly influences the translation quality. As a result, how to obtain better translation by exploiting target syntactic knowledge is somehow neglected. Thus, we argue th"
C10-2136,C08-1127,0,0.0148475,"chine translation (SMT). In flat reordering model (Wu, 1996; Zens et al., 2004), which assigns constant reordering probabilities depending on the language pairs, BTG constraint proves to be very effective for reducing the search space of phrase reordering. To pursue a better method to predict the order between two neighboring blocks1 , Xiong et al. (2006) present an enhanced BTG with a maximum entropy (ME) based reordering model. Along this line, source-side syntactic knowledge is introduced into the reordering model to improve BTG-based translation (Setiawan et al., 2007; Zhang et al., 2007; Xiong et al., 2008; Zhang and Li, 2009). However, these methods mainly focus on the utilization of source syntactic knowledge, while ignoring the modeling of the target-side syntax that directly influences the translation quality. As a result, how to obtain better translation by exploiting target syntactic knowledge is somehow neglected. Thus, we argue that it is important to model the target-side syntax in BTG-based translation. Recently, modeling syntactic information on the target side has progressed significantly. Depending on the type of output, these models can be divided into two categories: the constitu"
C10-2136,D09-1127,1,0.839344,"DIK Figure 5: Dependency combination of the illformed dependency structure dl with the right well-formed dependency structure dr . G denotes gap and the dotted line denotes the substitution of the gap G with dr . P (wL |wh -as-head) ... · P (wln |wln−1 , wln−2 ) JE Setup The training corpus1 comes from LDC with 1.54M bilingual sentences (41M Chinese words and 48M English words). We run GIZA++ (Och and Ney, 2000) to obtain word alignments with the heuristic method “grow-diag-final-and”. Then we parse the English sentences to generate a string-to-dependency word-aligned corpus using the parser (Huang et al., 2009). From this corpus, we extract bilingual phrases with dependency structure. Here, the maximum length of the source phrase is set to 7. For the n-gram LM, we use SRILM Toolkits (Stolcke, 2002) to train a 4-gram LM on the Xinhua portion of the Gigaword corpus. For the dependency LM, we train different 3-gram dependency LMs at word level and POS level separately on the English side of the training corpus. During the process of bilingual phrase extraction, we collect the neighboring blocks without 1 The training corpus consists of six LDC corpora: LDC2002E18, LDC2003E07, LDC2003E14, Hansards part"
C10-2136,D09-1073,0,0.0134639,"MT). In flat reordering model (Wu, 1996; Zens et al., 2004), which assigns constant reordering probabilities depending on the language pairs, BTG constraint proves to be very effective for reducing the search space of phrase reordering. To pursue a better method to predict the order between two neighboring blocks1 , Xiong et al. (2006) present an enhanced BTG with a maximum entropy (ME) based reordering model. Along this line, source-side syntactic knowledge is introduced into the reordering model to improve BTG-based translation (Setiawan et al., 2007; Zhang et al., 2007; Xiong et al., 2008; Zhang and Li, 2009). However, these methods mainly focus on the utilization of source syntactic knowledge, while ignoring the modeling of the target-side syntax that directly influences the translation quality. As a result, how to obtain better translation by exploiting target syntactic knowledge is somehow neglected. Thus, we argue that it is important to model the target-side syntax in BTG-based translation. Recently, modeling syntactic information on the target side has progressed significantly. Depending on the type of output, these models can be divided into two categories: the constituentoutput systems (Ga"
C10-2136,C04-1090,0,0.414998,"g the modeling of the target-side syntax that directly influences the translation quality. As a result, how to obtain better translation by exploiting target syntactic knowledge is somehow neglected. Thus, we argue that it is important to model the target-side syntax in BTG-based translation. Recently, modeling syntactic information on the target side has progressed significantly. Depending on the type of output, these models can be divided into two categories: the constituentoutput systems (Galley et al., 2006; Zhang et al., 2008; Liu et al., 2009) and dependencyoutput systems (Eisner, 2003; Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Shen et al., 2008). Compared with the constituent-output systems, the dependency-output systems provide a simpler platform to capture the target-side syntactic information, while also having the best interlingual phrasal cohesion properties (Fox, 2002). Typically, Shen et al. (2008) propose a string-todependency model, which integrates the targetside well-formed dependency structure into translation rules. With the dependency structure, this system employs a dependency language model (LM) to exploit long distance word relations, and achieves a signi"
C10-2136,zhang-etal-2004-interpreting,0,0.068922,"Missing"
C10-2136,P09-1063,1,0.918036,"Missing"
C10-2136,P00-1056,0,0.0838278,"s-head) · P (wl2 |wl1 , wh -as-head) 6 where ‘-as-head’ is used to distinguish the head word from child word in the language model. In like manner, P (wR |wh -as-head) has a similar calculation method. KDIK Figure 5: Dependency combination of the illformed dependency structure dl with the right well-formed dependency structure dr . G denotes gap and the dotted line denotes the substitution of the gap G with dr . P (wL |wh -as-head) ... · P (wln |wln−1 , wln−2 ) JE Setup The training corpus1 comes from LDC with 1.54M bilingual sentences (41M Chinese words and 48M English words). We run GIZA++ (Och and Ney, 2000) to obtain word alignments with the heuristic method “grow-diag-final-and”. Then we parse the English sentences to generate a string-to-dependency word-aligned corpus using the parser (Huang et al., 2009). From this corpus, we extract bilingual phrases with dependency structure. Here, the maximum length of the source phrase is set to 7. For the n-gram LM, we use SRILM Toolkits (Stolcke, 2002) to train a 4-gram LM on the Xinhua portion of the Gigaword corpus. For the dependency LM, we train different 3-gram dependency LMs at word level and POS level separately on the English side of the trainin"
C10-2136,D07-1056,0,0.0175994,"on in statistical machine translation (SMT). In flat reordering model (Wu, 1996; Zens et al., 2004), which assigns constant reordering probabilities depending on the language pairs, BTG constraint proves to be very effective for reducing the search space of phrase reordering. To pursue a better method to predict the order between two neighboring blocks1 , Xiong et al. (2006) present an enhanced BTG with a maximum entropy (ME) based reordering model. Along this line, source-side syntactic knowledge is introduced into the reordering model to improve BTG-based translation (Setiawan et al., 2007; Zhang et al., 2007; Xiong et al., 2008; Zhang and Li, 2009). However, these methods mainly focus on the utilization of source syntactic knowledge, while ignoring the modeling of the target-side syntax that directly influences the translation quality. As a result, how to obtain better translation by exploiting target syntactic knowledge is somehow neglected. Thus, we argue that it is important to model the target-side syntax in BTG-based translation. Recently, modeling syntactic information on the target side has progressed significantly. Depending on the type of output, these models can be divided into two cate"
C10-2136,P03-1021,0,0.041434,"Missing"
C10-2136,P08-1064,0,0.0141864,"thods mainly focus on the utilization of source syntactic knowledge, while ignoring the modeling of the target-side syntax that directly influences the translation quality. As a result, how to obtain better translation by exploiting target syntactic knowledge is somehow neglected. Thus, we argue that it is important to model the target-side syntax in BTG-based translation. Recently, modeling syntactic information on the target side has progressed significantly. Depending on the type of output, these models can be divided into two categories: the constituentoutput systems (Galley et al., 2006; Zhang et al., 2008; Liu et al., 2009) and dependencyoutput systems (Eisner, 2003; Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Shen et al., 2008). Compared with the constituent-output systems, the dependency-output systems provide a simpler platform to capture the target-side syntactic information, while also having the best interlingual phrasal cohesion properties (Fox, 2002). Typically, Shen et al. (2008) propose a string-todependency model, which integrates the targetside well-formed dependency structure into translation rules. With the dependency structure, this system employs a dependency language"
C10-2136,P02-1040,0,0.0785539,"Missing"
C10-2136,P05-1034,0,0.0710455,"syntax that directly influences the translation quality. As a result, how to obtain better translation by exploiting target syntactic knowledge is somehow neglected. Thus, we argue that it is important to model the target-side syntax in BTG-based translation. Recently, modeling syntactic information on the target side has progressed significantly. Depending on the type of output, these models can be divided into two categories: the constituentoutput systems (Galley et al., 2006; Zhang et al., 2008; Liu et al., 2009) and dependencyoutput systems (Eisner, 2003; Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Shen et al., 2008). Compared with the constituent-output systems, the dependency-output systems provide a simpler platform to capture the target-side syntactic information, while also having the best interlingual phrasal cohesion properties (Fox, 2002). Typically, Shen et al. (2008) propose a string-todependency model, which integrates the targetside well-formed dependency structure into translation rules. With the dependency structure, this system employs a dependency language model (LM) to exploit long distance word relations, and achieves a significant improvement over the hierarchical ph"
C10-2136,N03-1017,0,\N,Missing
C10-2136,J07-2003,0,\N,Missing
C12-1097,P06-2005,0,0.627216,"Missing"
C12-1097,C10-2022,0,0.319386,"Missing"
C12-1097,W09-2010,0,0.781758,"Missing"
C12-1097,P11-1038,0,0.490235,"Missing"
C12-1097,C08-1056,0,0.033691,"Missing"
C12-1097,P12-1109,0,0.303833,"Missing"
C12-1097,P11-2013,1,0.899605,"Missing"
C12-1097,J03-1002,0,0.0083563,"Missing"
C12-1097,I11-1109,1,0.797792,"Missing"
C12-1097,D11-1141,0,0.0987585,"Missing"
C12-1097,P02-1019,0,0.11623,"Missing"
C12-1097,N09-2069,0,0.0689033,"Missing"
C12-1097,P07-2045,0,\N,Missing
C12-1176,P09-1088,0,0.136299,"r (Galley et al., 2006; Liu et al., 2006; Chiang, 2007). Most systems induce synchronous grammars (including phrases) from parallel corpora using a heuristic two-step pipeline. This pipeline first aligns a parallel corpus at the word level with heuristic word alignment combination strategies (e.g., grow-diag-final-and) (Koehn et al., 2003), and then extracts translation rules from word-aligned sentence pairs. It is working well in practice and therefore widely adopted. However, such a pipeline artificially brings an undesirable disconnection between translation model and word alignment model (Blunsom et al., 2009; DeNero and Klein, 2010). Recently, researchers have resorted to principled probabilistic formulations. Various generative models are proposed to learn translation rules directly from sentence pairs without word alignments (Marcu and Wong, 2002; Cherry and Lin, 2007; Zhang et al., 2008; DeNero et al., 2008; Blunsom et al., 2009; Cohn and Blunsom, 2009; Neubig et al., 2011; Levenberg et al., 2012). Due to the independency assumptions in such generative models, it is hard to extend them to incorporate arbitrary features, especially word alignment information as used in the traditional two-step"
C12-1176,N10-1015,0,0.0540957,"hat our improvement comes from the theoretical advantage of our approach, and that such advantage does exist even on large-scale data. 7 Related Work Because of the great importance of synchronous grammars to SMT systems, researchers have proposed various methods in order to improve the quality of grammars. In addition to the generative model introduced in Section 1, researchers also have made efforts on word alignment and grammar rescoring. The first effort is to improve word alignment by considering phrase/syntax information (May and Knight, 2007; DeNero and Klein, 2010; Pauls et al., 2010; Burkett et al., 2010; Riesa et al., 2011). Such approaches also use discriminative framework to combine word alignment and syntactic alignment information. In this way, they prefer word alignments that are consistent with syntactic structure alignments. However, labeled word alignment data are required in order to learn the discriminative model. 2895 Researchers also try to rescore the weights of translation rules. They rescore the weights of rules extracted from the two-step pipeline, by using the similar latent log-linear model as us (Blunsom et al., 2008; Kääriäinen, 2009), or incorporating various features us"
C12-1176,W07-0403,0,0.245791,"nt combination strategies (e.g., grow-diag-final-and) (Koehn et al., 2003), and then extracts translation rules from word-aligned sentence pairs. It is working well in practice and therefore widely adopted. However, such a pipeline artificially brings an undesirable disconnection between translation model and word alignment model (Blunsom et al., 2009; DeNero and Klein, 2010). Recently, researchers have resorted to principled probabilistic formulations. Various generative models are proposed to learn translation rules directly from sentence pairs without word alignments (Marcu and Wong, 2002; Cherry and Lin, 2007; Zhang et al., 2008; DeNero et al., 2008; Blunsom et al., 2009; Cohn and Blunsom, 2009; Neubig et al., 2011; Levenberg et al., 2012). Due to the independency assumptions in such generative models, it is hard to extend them to incorporate arbitrary features, especially word alignment information as used in the traditional two-step pipeline. As a result, despite theoretical advantages of these models over the two-step pipeline, in practice they can only produce comparable translation quality with the two-step pipeline in some scenarios, and usually even worse results. Yet another alternative fo"
C12-1176,P11-2031,0,0.116176,"Missing"
C12-1176,D11-1005,0,0.0253585,"ted from the two-step pipeline, by using the similar latent log-linear model as us (Blunsom et al., 2008; Kääriäinen, 2009), or incorporating various features using labeled data (Huang and Xiang, 2010). Such methods still need to run the heuristic two-step pipeline to extract the grammar, while our method can directly learn the grammar and correspondent weights. Our work also has a connection to the research direction that exploits resources-rich languages to construct similar tools for resource-poor languages. This can be done with parallel data (Pauls et al., 2010) or without parallel data (Cohen et al., 2011). Saers et al. (2009) also propose a cubic biparsing algorithm based on beam pruning. They apply this algorithm for generative model-based ITG grammar induction. Conclusion and Future Work We have presented a global log-linear model for synchronous grammar induction, and have also proposed efficient training and inference algorithms. In addition to the theoretical advantage, we also achieve significant improvements over the traditional heuristic two-stage pipeline on both medium-scale and large-scale training data. In the future, we hope to find efficient algorithms that are capable of incorpo"
C12-1176,D09-1037,0,0.376558,"extracts translation rules from word-aligned sentence pairs. It is working well in practice and therefore widely adopted. However, such a pipeline artificially brings an undesirable disconnection between translation model and word alignment model (Blunsom et al., 2009; DeNero and Klein, 2010). Recently, researchers have resorted to principled probabilistic formulations. Various generative models are proposed to learn translation rules directly from sentence pairs without word alignments (Marcu and Wong, 2002; Cherry and Lin, 2007; Zhang et al., 2008; DeNero et al., 2008; Blunsom et al., 2009; Cohn and Blunsom, 2009; Neubig et al., 2011; Levenberg et al., 2012). Due to the independency assumptions in such generative models, it is hard to extend them to incorporate arbitrary features, especially word alignment information as used in the traditional two-step pipeline. As a result, despite theoretical advantages of these models over the two-step pipeline, in practice they can only produce comparable translation quality with the two-step pipeline in some scenarios, and usually even worse results. Yet another alternative for synchronous grammar induction is unsupervised discriminative model. Unsupervised disc"
C12-1176,J97-3002,0,0.532366,"Missing"
C12-1176,D08-1033,0,0.164223,"-final-and) (Koehn et al., 2003), and then extracts translation rules from word-aligned sentence pairs. It is working well in practice and therefore widely adopted. However, such a pipeline artificially brings an undesirable disconnection between translation model and word alignment model (Blunsom et al., 2009; DeNero and Klein, 2010). Recently, researchers have resorted to principled probabilistic formulations. Various generative models are proposed to learn translation rules directly from sentence pairs without word alignments (Marcu and Wong, 2002; Cherry and Lin, 2007; Zhang et al., 2008; DeNero et al., 2008; Blunsom et al., 2009; Cohn and Blunsom, 2009; Neubig et al., 2011; Levenberg et al., 2012). Due to the independency assumptions in such generative models, it is hard to extend them to incorporate arbitrary features, especially word alignment information as used in the traditional two-step pipeline. As a result, despite theoretical advantages of these models over the two-step pipeline, in practice they can only produce comparable translation quality with the two-step pipeline in some scenarios, and usually even worse results. Yet another alternative for synchronous grammar induction is unsupe"
C12-1176,P10-1147,0,0.0800555,"; Liu et al., 2006; Chiang, 2007). Most systems induce synchronous grammars (including phrases) from parallel corpora using a heuristic two-step pipeline. This pipeline first aligns a parallel corpus at the word level with heuristic word alignment combination strategies (e.g., grow-diag-final-and) (Koehn et al., 2003), and then extracts translation rules from word-aligned sentence pairs. It is working well in practice and therefore widely adopted. However, such a pipeline artificially brings an undesirable disconnection between translation model and word alignment model (Blunsom et al., 2009; DeNero and Klein, 2010). Recently, researchers have resorted to principled probabilistic formulations. Various generative models are proposed to learn translation rules directly from sentence pairs without word alignments (Marcu and Wong, 2002; Cherry and Lin, 2007; Zhang et al., 2008; DeNero et al., 2008; Blunsom et al., 2009; Cohn and Blunsom, 2009; Neubig et al., 2011; Levenberg et al., 2012). Due to the independency assumptions in such generative models, it is hard to extend them to incorporate arbitrary features, especially word alignment information as used in the traditional two-step pipeline. As a result, de"
C12-1176,N10-1033,0,0.0163797,"sorted candidate lists for every substructure, and create cubes that represent the potential combinations of these candidates (see Fig. 2(a)). In this way, we are able to create k-best hyperedges by cube pruning. More specifically, we maintain charts for source words and source spans respectively: • s-chart for each source word. The cell char t[s, i] in s-chart stores a list of candidate alignments for the i-th source word. A candidate alignment is represented by a list of target index. The vertical dimension in Figure 2(a) is an instance of char t[s, 3]. 1 http://leon.bottou.org/projects/sgd Dyer (2010) has shown that two monolingual parses can be more efficient than one synchronous parse, due to the sparsity of pre-fixed translation rules. Such rules are extracted by the heuristic two-step pipeline. In contrast, there are no pre-fixed translation rules in our case. 2 2888 5 ,3 , 5 0, 2 ,2 , 0, 2 zhiyi3 [0,3] [0,5] r3 1 2 [0,2] zhiyi 3 [2,5] zhiyi 3 4 1 2 Figure 2: Construct hyperedge from a cube. (a) Cube X 0,2 zhiyi3 for source span (0,3). The vertical direction represents the candidate list of zhiyi3 , while the horizontal direction is a list of nodes that share the same source span (0,2)"
C12-1176,P11-1042,0,0.215487,"ules discovered in the training corpus by our algorithm. sr c() denotes the source side of a rule. G() is the set of rules in a hypergraph. The neighbor grammar contains those rules that are discovered during training (rather than all potential rules), and whose source sides occur in the synchronous hypergraph. Sine the size of NG is typical fairly small, the parsing of our source hypergraph becomes tractable in practice. The definition of neighbor source hypergraph is inspired by contrastive estimation (Smith and Eisner, 2005). Similar shrinkage of discriminative neighborhood is also used in Dyer et al. (2011a). Notably, our approximation is consistent with the purpose of synchronous grammar induction for SMT. In SMT, the goal of grammar induction is for translation rather than synchronous parsing. Our source hypergraph corresponds to the potential translation space during SMT decoding. Thus, we expect such approximation to be suitable for SMT. 3.3 Training Algorithm Based on the synchronous hypergraph and source hypergraph introduced above, we optimize L in an online style as shown in Algorithm 1. For each sentence pair, we first use cube-pruning based biparsing (Sec. 4) to construct a synchronou"
C12-1176,W11-2139,0,0.468141,"ules discovered in the training corpus by our algorithm. sr c() denotes the source side of a rule. G() is the set of rules in a hypergraph. The neighbor grammar contains those rules that are discovered during training (rather than all potential rules), and whose source sides occur in the synchronous hypergraph. Sine the size of NG is typical fairly small, the parsing of our source hypergraph becomes tractable in practice. The definition of neighbor source hypergraph is inspired by contrastive estimation (Smith and Eisner, 2005). Similar shrinkage of discriminative neighborhood is also used in Dyer et al. (2011a). Notably, our approximation is consistent with the purpose of synchronous grammar induction for SMT. In SMT, the goal of grammar induction is for translation rather than synchronous parsing. Our source hypergraph corresponds to the potential translation space during SMT decoding. Thus, we expect such approximation to be suitable for SMT. 3.3 Training Algorithm Based on the synchronous hypergraph and source hypergraph introduced above, we optimize L in an online style as shown in Algorithm 1. For each sentence pair, we first use cube-pruning based biparsing (Sec. 4) to construct a synchronou"
C12-1176,P06-1121,0,0.117782,"Missing"
C12-1176,C10-1056,0,0.0569973,"11). Such approaches also use discriminative framework to combine word alignment and syntactic alignment information. In this way, they prefer word alignments that are consistent with syntactic structure alignments. However, labeled word alignment data are required in order to learn the discriminative model. 2895 Researchers also try to rescore the weights of translation rules. They rescore the weights of rules extracted from the two-step pipeline, by using the similar latent log-linear model as us (Blunsom et al., 2008; Kääriäinen, 2009), or incorporating various features using labeled data (Huang and Xiang, 2010). Such methods still need to run the heuristic two-step pipeline to extract the grammar, while our method can directly learn the grammar and correspondent weights. Our work also has a connection to the research direction that exploits resources-rich languages to construct similar tools for resource-poor languages. This can be done with parallel data (Pauls et al., 2010) or without parallel data (Cohen et al., 2011). Saers et al. (2009) also propose a cubic biparsing algorithm based on beam pruning. They apply this algorithm for generative model-based ITG grammar induction. Conclusion and Futur"
C12-1176,P08-1067,0,0.116367,"Missing"
C12-1176,D09-1107,0,0.449941,"nto t, and d is one such derivation. Given a source sentence s, the conditional probability of a derivation d and the corresponding translation t is: P exp i λi H i (d, t, s) p(d, t|s) = (2) Z(s) P where H i (d, t, s) = r∈d hi (r, s) is feature function. We assume H i decomposes with derivation d in terms of local feature function hi , which is related to a rule r and a source sentence s. λi is the correspondent feature weight. Z(s) is the partition function: X X X Z(s) = exp λi H i (d, t, s) (3) t d∈△(t,s) i Such a discriminative latent variable model is not new to SMT (Blunsom et al., 2008; Kääriäinen, 2009; Xiao et al., 2011). However, we are distinguished from previous work by applying this model to synchronous grammar induction. The purpose of the latent variable model in such previous work is to do max-translation decoding and training (Blunsom et al., 2008), or to eliminate the gap between heuristic extraction and decoding (Kääriäinen, 2009), instead of grammar induction as synchronous rules are still extracted by the heuristic two-step pipeline. In contrast, our interest lies in using latent variable model to learn synchronous grammar directly from sentence pairs. Overall, to the best of o"
C12-1176,W01-1812,0,0.0529357,"tion E p(d|t,s) [H i ] of a parameter given a sentence pair, and the second one E p(d,t|s) [H i ] is the expectation of a parameter given the source sentence. In the following sections, we first introduce how to use hypergraph to compute these expectations by synchronous hypergraph and source hypergraph respectively (Sec. 3.1). Then, we discuss the intractability of the exact training, and achieve tractable training by approximation (Sec. 3.2). Finally, we describe the training algorithm in detail to explain how the rules is induced (Sec. 3.3). 3.1 Inference with Hypergraph We use hypergraph (Klein and Manning, 2001) to compactly represent the space of derivations. Based on hypergraphs, it’s straightforward to calculate the two expectations in Eq. (5). The first expectation E p(d|t,s) [H i ] is the expected value when observing both source sentence s and target sentence t. The second expectation E p(d,t|s) [H i ] is a similar function, but only the source sentence is observed. Thus, in order to calculate the first expectation, we construct a synchronous hypergraph to represent all derivations of a sentence pair. Similarly, for the second expectation, we use a source hypergraph to represent all derivations"
C12-1176,W04-3250,0,0.12891,"reports the average score on the three test set. |G′ | MT03 MT04 MT05 Avg. Moses-Chart 45.0M 32.93 34.73 31.24 32.97 Baseline Baseline-expand 13.2M 46.2M 32.36 33.04 34.51 35.13 31.86 32.08 32.91 33.42 UDSGI Dense UDSGI Dense+Sparse 13.1M 13.1M 33.46 33.58 35.43 36.27 32.74 33.05 33.87 34.30 System Table 3: Evaluation of translation quality in terms of BLEU. Moses-Chart is the running of hierarchical phrased-model in Moses. Baseline-expand uses a similar extraction constraint to Moses-Chart and the same decoder as Baseline. The improvement of UDSGI over Baseline is statistically significant (Koehn, 2004) (p < 0.01). We can not directly evaluate the quality of grammar, since there is not a golden grammar. However, as grammar is used to generate target translations, it’s reasonable to decide the quality of a grammar by testing what best translations it can produce. Therefore, we compare the oracle translation result of the grammar in baseline and the grammar in UDSGI, with four reference translations given.4 As shown in Table 2, our grammar achieves a much higher oracle BLEU (+8.3 BLEU points) than the baseline grammar. This suggests that the grammar induced by our method is able to generate be"
C12-1176,P07-2045,0,0.00773561,"four reference translations given.4 As shown in Table 2, our grammar achieves a much higher oracle BLEU (+8.3 BLEU points) than the baseline grammar. This suggests that the grammar induced by our method is able to generate better translations than the grammar extracted by the traditional two-step pipeline. 6.3 Translation Results Table 3 compares the translation performance of our approach and the baseline on the test sets. When extracting rules as Chiang (2007), the baseline produces an average BLEU of 32.91. We also run Moses-Chart, the implementation of hierarchical phrased-model in Moses (Koehn et al., 2007), on our data with its default settings. As shown in the table, our Baseline is comparable with Moses-Chart. However, Moses-Chart extracts much more rules, because it uses a different extraction constraint from Chiang (2007). The differences mainly include edges of initial phrases can be unaligned and minimum size of source part of sub-phrases is 2. We also relax the Baseline by applying these two options, and call such setting as Baselineexpand. Baseline-expand outperforms Baseline by +0.51 BLEU with 3.5 times of rules. As our UDSGI method learns a similar size of rules with the Baseline’s, w"
C12-1176,N03-1017,0,0.512013,"with millions of features that contain rule level, word level and translation boundary information, we significantly outperform a competitive hierarchical phrased-based baseline system by +1.4 BLEU on average on three NIST test sets. KEYWORDS: synchronous grammar induction, discriminative model, unsupervised learning, machine translation. Proceedings of COLING 2012: Technical Papers, pages 2883–2898, COLING 2012, Mumbai, December 2012. 2883 1 Introduction In the last decade, statistical machine translation (SMT) has been advanced by expanding the basic unit of translation from word to phrase (Koehn et al., 2003) and grammar (Galley et al., 2006; Liu et al., 2006; Chiang, 2007). Most systems induce synchronous grammars (including phrases) from parallel corpora using a heuristic two-step pipeline. This pipeline first aligns a parallel corpus at the word level with heuristic word alignment combination strategies (e.g., grow-diag-final-and) (Koehn et al., 2003), and then extracts translation rules from word-aligned sentence pairs. It is working well in practice and therefore widely adopted. However, such a pipeline artificially brings an undesirable disconnection between translation model and word alignm"
C12-1176,D12-1021,0,0.600119,"sentence pairs. It is working well in practice and therefore widely adopted. However, such a pipeline artificially brings an undesirable disconnection between translation model and word alignment model (Blunsom et al., 2009; DeNero and Klein, 2010). Recently, researchers have resorted to principled probabilistic formulations. Various generative models are proposed to learn translation rules directly from sentence pairs without word alignments (Marcu and Wong, 2002; Cherry and Lin, 2007; Zhang et al., 2008; DeNero et al., 2008; Blunsom et al., 2009; Cohn and Blunsom, 2009; Neubig et al., 2011; Levenberg et al., 2012). Due to the independency assumptions in such generative models, it is hard to extend them to incorporate arbitrary features, especially word alignment information as used in the traditional two-step pipeline. As a result, despite theoretical advantages of these models over the two-step pipeline, in practice they can only produce comparable translation quality with the two-step pipeline in some scenarios, and usually even worse results. Yet another alternative for synchronous grammar induction is unsupervised discriminative model. Unsupervised discriminative model can directly learn synchronou"
C12-1176,P09-1067,0,0.0479859,"is the set of hyperedges. Each hyperedge e ∈ E connects a set of antecedent nodes to a single consequent node. And each hyperedge corresponds to an SCFG rule r. In a synchronous hypergraph, a node v ∈ V is in the form X i, j,k,l , which denotes the nonterminal X spanning from i to j (that is si+1 ...s j ) in the source sentence, and from k to l in the target sentence. In a source hypergraph, each node v ∈ V is in the form X i, j , which spans from i to j in the source sentence. Based on these hypergraphs, we compute the two expectations by applying the inside-outside algorithm as described in Li et al. (2009). The computation complexity is linear to the size of hypergraph O(|E|). More exactly, O(|E|) denotes O(|s|3 |t|3 ) for synchronous hypergraph, and O(|G||s|3 ) for source hypergraph. Here, G denotes all potential synchronous grammars. 3.2 Tractable Estimation by Approximation However, the size of potential SCFGs G is extremely large given a vocabulary Ω, resulting in a large number of hyperedges in source hypergraph. See the rule r2 in Figure 1. In reality, there are many potential translation rules that share the same source side “shaoshu X ” as r2 , but with different target side. Suppose n"
C12-1176,P06-1077,1,0.815726,"d level and translation boundary information, we significantly outperform a competitive hierarchical phrased-based baseline system by +1.4 BLEU on average on three NIST test sets. KEYWORDS: synchronous grammar induction, discriminative model, unsupervised learning, machine translation. Proceedings of COLING 2012: Technical Papers, pages 2883–2898, COLING 2012, Mumbai, December 2012. 2883 1 Introduction In the last decade, statistical machine translation (SMT) has been advanced by expanding the basic unit of translation from word to phrase (Koehn et al., 2003) and grammar (Galley et al., 2006; Liu et al., 2006; Chiang, 2007). Most systems induce synchronous grammars (including phrases) from parallel corpora using a heuristic two-step pipeline. This pipeline first aligns a parallel corpus at the word level with heuristic word alignment combination strategies (e.g., grow-diag-final-and) (Koehn et al., 2003), and then extracts translation rules from word-aligned sentence pairs. It is working well in practice and therefore widely adopted. However, such a pipeline artificially brings an undesirable disconnection between translation model and word alignment model (Blunsom et al., 2009; DeNero and Klein,"
C12-1176,W02-1018,0,0.0903456,"heuristic word alignment combination strategies (e.g., grow-diag-final-and) (Koehn et al., 2003), and then extracts translation rules from word-aligned sentence pairs. It is working well in practice and therefore widely adopted. However, such a pipeline artificially brings an undesirable disconnection between translation model and word alignment model (Blunsom et al., 2009; DeNero and Klein, 2010). Recently, researchers have resorted to principled probabilistic formulations. Various generative models are proposed to learn translation rules directly from sentence pairs without word alignments (Marcu and Wong, 2002; Cherry and Lin, 2007; Zhang et al., 2008; DeNero et al., 2008; Blunsom et al., 2009; Cohn and Blunsom, 2009; Neubig et al., 2011; Levenberg et al., 2012). Due to the independency assumptions in such generative models, it is hard to extend them to incorporate arbitrary features, especially word alignment information as used in the traditional two-step pipeline. As a result, despite theoretical advantages of these models over the two-step pipeline, in practice they can only produce comparable translation quality with the two-step pipeline in some scenarios, and usually even worse results. Yet"
C12-1176,D07-1038,0,0.220266,"Missing"
C12-1176,P11-1064,0,0.359975,"es from word-aligned sentence pairs. It is working well in practice and therefore widely adopted. However, such a pipeline artificially brings an undesirable disconnection between translation model and word alignment model (Blunsom et al., 2009; DeNero and Klein, 2010). Recently, researchers have resorted to principled probabilistic formulations. Various generative models are proposed to learn translation rules directly from sentence pairs without word alignments (Marcu and Wong, 2002; Cherry and Lin, 2007; Zhang et al., 2008; DeNero et al., 2008; Blunsom et al., 2009; Cohn and Blunsom, 2009; Neubig et al., 2011; Levenberg et al., 2012). Due to the independency assumptions in such generative models, it is hard to extend them to incorporate arbitrary features, especially word alignment information as used in the traditional two-step pipeline. As a result, despite theoretical advantages of these models over the two-step pipeline, in practice they can only produce comparable translation quality with the two-step pipeline in some scenarios, and usually even worse results. Yet another alternative for synchronous grammar induction is unsupervised discriminative model. Unsupervised discriminative model can"
C12-1176,P03-1021,0,0.0658986,"for MERT, and test the translation performance on the NIST 2003-2005 (MT03-05) evaluation sets. Case-insensitive NIST BLEU-4 (Papineni et al., 2002) is used to measure translation performance. We used a bilingual corpus that contains 200K sentence pairs of up to length 40 from the LDC data. 3 There are 8.8M words in the 200K data. We trained a 5-grams language model by the SRILM toolkit (Stolcke, 2002). The monolingual data for language model training includes the Xinhua portion of the GIGAWORD corpus and the English side of the entire LDC data, which contains 432 million words. We used MERT (Och, 2003) to optimize the feature weights for decoding by maximizing BLEU. Since the instability of MERT has a substantial impact on results, we follow Clark et al. (2011) to report the average scores of 3 independent runs. 6.2 Grammar Analysis and Oracle Results The synchronous grammar generated by UDSGI has 13.1 millions rules. The number of these rules is comparable with that of grammar extracted by the traditional pipeline, which has 13.2 millions rules. However, the two grammars are quite different as shown in Table 1. Our grammar is more reusable than the baseline’s, because it contains less sour"
C12-1176,J03-1002,0,0.00589028,"The experiments are aimed at measuring the quality and effectiveness of grammar induced by our method. We present the performance of our unsupervised discriminate synchronous grammar induction (UDSGI) using two groups of features during decoding. We test the translation performance of UDSGI and the baseline on the same decoder. Baseline The baseline system is an in-house implementation of hierarchical phrase-based translation system(Chiang, 2007). The grammar is extracted from word-aligned corpus by traditional two-step pipeline. Symmetric word alignments were created by first running GIZA++ (Och and Ney, 2003) in both directions and then applying refinement rule “grow-diag-finaland” (Koehn et al., 2003). The system uses 8 dense features including: forward and backward translation probabilities; forward and backward lexical weights; language model; 3 penalties for word count, extracted rule count, and glue rule count. UDSGI Dense This configuration uses the same feature set as the baseline. Our log-linear model for grammar induction does not contain the forward and backward translation probabilities. However, we still compute the forward and backward translation probabilities for UDSGI by normalizin"
C12-1176,P02-1040,0,0.0874883,"ize these sparse features with the dense features by minimum error rate training (MERT), we group features of the same type into one coarse ""summary feature"", and get three such features including: rule, word-pair and phraseboundary features. In this way, we rescale the weights of the three ""summary features"" with the 8 dense features by MERT. Such approach is similar to Dyer et al. (2011b). 6.1 Data We used the NIST evaluation set of 2002 (MT02) as our development set for MERT, and test the translation performance on the NIST 2003-2005 (MT03-05) evaluation sets. Case-insensitive NIST BLEU-4 (Papineni et al., 2002) is used to measure translation performance. We used a bilingual corpus that contains 200K sentence pairs of up to length 40 from the LDC data. 3 There are 8.8M words in the 200K data. We trained a 5-grams language model by the SRILM toolkit (Stolcke, 2002). The monolingual data for language model training includes the Xinhua portion of the GIGAWORD corpus and the English side of the entire LDC data, which contains 432 million words. We used MERT (Och, 2003) to optimize the feature weights for decoding by maximizing BLEU. Since the instability of MERT has a substantial impact on results, we fo"
C12-1176,N10-1014,0,0.0886082,"refore, we believe that our improvement comes from the theoretical advantage of our approach, and that such advantage does exist even on large-scale data. 7 Related Work Because of the great importance of synchronous grammars to SMT systems, researchers have proposed various methods in order to improve the quality of grammars. In addition to the generative model introduced in Section 1, researchers also have made efforts on word alignment and grammar rescoring. The first effort is to improve word alignment by considering phrase/syntax information (May and Knight, 2007; DeNero and Klein, 2010; Pauls et al., 2010; Burkett et al., 2010; Riesa et al., 2011). Such approaches also use discriminative framework to combine word alignment and syntactic alignment information. In this way, they prefer word alignments that are consistent with syntactic structure alignments. However, labeled word alignment data are required in order to learn the discriminative model. 2895 Researchers also try to rescore the weights of translation rules. They rescore the weights of rules extracted from the two-step pipeline, by using the similar latent log-linear model as us (Blunsom et al., 2008; Kääriäinen, 2009), or incorporati"
C12-1176,D11-1046,0,0.0596119,"mes from the theoretical advantage of our approach, and that such advantage does exist even on large-scale data. 7 Related Work Because of the great importance of synchronous grammars to SMT systems, researchers have proposed various methods in order to improve the quality of grammars. In addition to the generative model introduced in Section 1, researchers also have made efforts on word alignment and grammar rescoring. The first effort is to improve word alignment by considering phrase/syntax information (May and Knight, 2007; DeNero and Klein, 2010; Pauls et al., 2010; Burkett et al., 2010; Riesa et al., 2011). Such approaches also use discriminative framework to combine word alignment and syntactic alignment information. In this way, they prefer word alignments that are consistent with syntactic structure alignments. However, labeled word alignment data are required in order to learn the discriminative model. 2895 Researchers also try to rescore the weights of translation rules. They rescore the weights of rules extracted from the two-step pipeline, by using the similar latent log-linear model as us (Blunsom et al., 2008; Kääriäinen, 2009), or incorporating various features using labeled data (Hua"
C12-1176,W09-3804,0,0.0647723,"pipeline, by using the similar latent log-linear model as us (Blunsom et al., 2008; Kääriäinen, 2009), or incorporating various features using labeled data (Huang and Xiang, 2010). Such methods still need to run the heuristic two-step pipeline to extract the grammar, while our method can directly learn the grammar and correspondent weights. Our work also has a connection to the research direction that exploits resources-rich languages to construct similar tools for resource-poor languages. This can be done with parallel data (Pauls et al., 2010) or without parallel data (Cohen et al., 2011). Saers et al. (2009) also propose a cubic biparsing algorithm based on beam pruning. They apply this algorithm for generative model-based ITG grammar induction. Conclusion and Future Work We have presented a global log-linear model for synchronous grammar induction, and have also proposed efficient training and inference algorithms. In addition to the theoretical advantage, we also achieve significant improvements over the traditional heuristic two-stage pipeline on both medium-scale and large-scale training data. In the future, we hope to find efficient algorithms that are capable of incorporating contextual fea"
C12-1176,P05-1044,0,0.289551,"nerative model. However, the advantage over generative model is that it is able to easily incorporate word alignment information which has been proved useful in the two-step pipeline. In this paper, we propose a global log-linear model (Sec. 2) for the induction of synchronous context free grammar (SCFG) (Chiang, 2007). The log-linear model is able to incorporate arbitrary features. Furthermore, it is trained from sentence pairs without word alignments in an unsupervised fashion. In particular: • We approximate the exact conditional log-likelihood objective inspired by contrastive estimation (Smith and Eisner, 2005) as the optimization of the exact objective is very expensive. The key idea is to estimate parameters via synchronous hypergraphs of sentence pairs and neighbor source hypergraphs of source sentences (Sec. 3). • Synchronous parsing is often impractical in large-scale learning applications due to its high complexity O(n6 ). We address this challenge by proposing a novel and efficient O(n3 ) cube pruning based synchronous parsing algorithm (Sec. 4). • Aiming to enhance the ability to predict whether a translation derivation is good or not, we incorporate a variety of fine-grained features into o"
C12-1176,P09-1054,0,0.0315396,"SE(NG, s) ⊲ generate neighbor source hypergraph λ ← λ + η × ∂∂ λL (H1 , H2 ) ⊲ η is learning rate return G′ , λ and store them in G′ . After that, we create the neighbor source hypergraph H2 by exhaustive bottom-up chart parsing using the neighbor grammar NG (line 6). Finally, we calculate the gradient by these two hypergraphs and update the feature weights (line 8). When the algorithm is complete, we learn a grammar G′ and also the feature weights λ of the model. We implement an stochastic gradient descent (SGD) recommended by Bottou.1 We schedule the learning rate η by an exponential decay (Tsuruoka et al., 2009). We set the regularization strength, initial learning rate and the base of exponential decay as 1.0, 0.2, 0.9 respectively. We choose these values by maximizing the translation performance measured by BLEU on the NIST 2002 development set with a subset of our training data including 20k sentence pairs. 4 Cube Pruning-based Synchronous Parsing The approximation makes the training algorithm tractable. However, there is still one problem: how to efficiently construct the synchronous hypergraph? Exhaustive synchronous parsing requires O(|s|3 |t|3 ) time.2 To overcome this challenge, we propose a"
C12-1176,D11-1081,1,0.868474,"e such derivation. Given a source sentence s, the conditional probability of a derivation d and the corresponding translation t is: P exp i λi H i (d, t, s) p(d, t|s) = (2) Z(s) P where H i (d, t, s) = r∈d hi (r, s) is feature function. We assume H i decomposes with derivation d in terms of local feature function hi , which is related to a rule r and a source sentence s. λi is the correspondent feature weight. Z(s) is the partition function: X X X Z(s) = exp λi H i (d, t, s) (3) t d∈△(t,s) i Such a discriminative latent variable model is not new to SMT (Blunsom et al., 2008; Kääriäinen, 2009; Xiao et al., 2011). However, we are distinguished from previous work by applying this model to synchronous grammar induction. The purpose of the latent variable model in such previous work is to do max-translation decoding and training (Blunsom et al., 2008), or to eliminate the gap between heuristic extraction and decoding (Kääriäinen, 2009), instead of grammar induction as synchronous rules are still extracted by the heuristic two-step pipeline. In contrast, our interest lies in using latent variable model to learn synchronous grammar directly from sentence pairs. Overall, to the best of our knowledge, this i"
C12-1176,N10-1016,1,0.945845,"parameters via synchronous hypergraphs of sentence pairs and neighbor source hypergraphs of source sentences (Sec. 3). • Synchronous parsing is often impractical in large-scale learning applications due to its high complexity O(n6 ). We address this challenge by proposing a novel and efficient O(n3 ) cube pruning based synchronous parsing algorithm (Sec. 4). • Aiming to enhance the ability to predict whether a translation derivation is good or not, we incorporate a variety of fine-grained features into our model, including rule level features, word level features and phrase boundary features (Xiong et al., 2010) (Sec. 5). We evaluate our approach on the NIST Chinese-English translation task. According to the analysis of grammar (Sec. 6.2), our induced grammar is more reusable, and is able to generate better (+8.3 BLEU points) oracle translations than the grammar of the baseline. Meanwhile, in the end-to-end machine translation experiments, our approach outperforms the two-step pipeline by +1.4 BLEU points (Sec. 6.3). 2884 2 Global Log-linear Model We propose a log-linear model to induce SCFG rules for hierarchical phrase-based translation (Chiang, 2007) which transforms a source sentence s into a tar"
C12-1176,P08-1012,0,0.0560722,"ies (e.g., grow-diag-final-and) (Koehn et al., 2003), and then extracts translation rules from word-aligned sentence pairs. It is working well in practice and therefore widely adopted. However, such a pipeline artificially brings an undesirable disconnection between translation model and word alignment model (Blunsom et al., 2009; DeNero and Klein, 2010). Recently, researchers have resorted to principled probabilistic formulations. Various generative models are proposed to learn translation rules directly from sentence pairs without word alignments (Marcu and Wong, 2002; Cherry and Lin, 2007; Zhang et al., 2008; DeNero et al., 2008; Blunsom et al., 2009; Cohn and Blunsom, 2009; Neubig et al., 2011; Levenberg et al., 2012). Due to the independency assumptions in such generative models, it is hard to extend them to incorporate arbitrary features, especially word alignment information as used in the traditional two-step pipeline. As a result, despite theoretical advantages of these models over the two-step pipeline, in practice they can only produce comparable translation quality with the two-step pipeline in some scenarios, and usually even worse results. Yet another alternative for synchronous gramma"
C12-1176,2009.eamt-smart.4,0,\N,Missing
C12-1176,J07-2003,0,\N,Missing
C12-1176,P08-1024,0,\N,Missing
C12-2066,P06-1002,0,0.0173256,"ignment plays an important role in statistical machine translation (SMT) as it indicates the correspondence between two languages. The parameter estimation of many SMT models rely heavily on word alignment. Och and Ney (2004) firstly introduce alignment consistency to identify equivalent phrase pairs. Simple and effective, rule extraction algorithms based on word alignment have also been extended to hierarchial phrase-based (Chiang, 2007) and syntax-based (Galley et al., 2004) SMT systems successfully. Studies reveal that word alignment has a profound effect on the performance of SMT systems (Ayan and Dorr, 2006; Fraser and Marcu, 2007). One major challenge in word alignment is modeling the permutations of words between source and target sentences. Due to the diversity of natural languages, the word orders of source and target sentences are usually quite different, especially for distantly-related language pairs such as Chinese and English. While most word alignment approaches either use distortion models (Brown et al., 1993; Vogel and Ney, 1996) or features (Taskar et al., 2005; Moore, 2005; Moore et al., 2006; Liu et al., 2010c) to capture reordering of words, inversion transduction grammar (ITG) ("
C12-2066,J93-2003,0,0.0666932,"based (Chiang, 2007) and syntax-based (Galley et al., 2004) SMT systems successfully. Studies reveal that word alignment has a profound effect on the performance of SMT systems (Ayan and Dorr, 2006; Fraser and Marcu, 2007). One major challenge in word alignment is modeling the permutations of words between source and target sentences. Due to the diversity of natural languages, the word orders of source and target sentences are usually quite different, especially for distantly-related language pairs such as Chinese and English. While most word alignment approaches either use distortion models (Brown et al., 1993; Vogel and Ney, 1996) or features (Taskar et al., 2005; Moore, 2005; Moore et al., 2006; Liu et al., 2010c) to capture reordering of words, inversion transduction grammar (ITG) (Wu, 1997) provides a syntactically motivated solution. ITG is a synchronous grammar of which a derivation explains how a source sentence and a target sentence are generated synchronously. By recursively merging blocks (i.e., consecutive word sequences) either in a monotone order or an inverted order, ITG constrains the search space of distortion in a way that proves to be effective in both alignment (Zhang and Gildea,"
C12-2066,J07-3002,0,0.0414709,"Missing"
C12-2066,N04-1035,0,0.0518495,"L2 : cŠéà, ‡•=Š{, Î|¢. Proceedings of COLING 2012: Posters, pages 673–682, COLING 2012, Mumbai, December 2012. 673 1 Introduction Word alignment plays an important role in statistical machine translation (SMT) as it indicates the correspondence between two languages. The parameter estimation of many SMT models rely heavily on word alignment. Och and Ney (2004) firstly introduce alignment consistency to identify equivalent phrase pairs. Simple and effective, rule extraction algorithms based on word alignment have also been extended to hierarchial phrase-based (Chiang, 2007) and syntax-based (Galley et al., 2004) SMT systems successfully. Studies reveal that word alignment has a profound effect on the performance of SMT systems (Ayan and Dorr, 2006; Fraser and Marcu, 2007). One major challenge in word alignment is modeling the permutations of words between source and target sentences. Due to the diversity of natural languages, the word orders of source and target sentences are usually quite different, especially for distantly-related language pairs such as Chinese and English. While most word alignment approaches either use distortion models (Brown et al., 1993; Vogel and Ney, 1996) or features (Taska"
C12-2066,P09-1104,0,0.216333,"ey, 1996) or features (Taskar et al., 2005; Moore, 2005; Moore et al., 2006; Liu et al., 2010c) to capture reordering of words, inversion transduction grammar (ITG) (Wu, 1997) provides a syntactically motivated solution. ITG is a synchronous grammar of which a derivation explains how a source sentence and a target sentence are generated synchronously. By recursively merging blocks (i.e., consecutive word sequences) either in a monotone order or an inverted order, ITG constrains the search space of distortion in a way that proves to be effective in both alignment (Zhang and Gildea, 2005, 2006; Haghighi et al., 2009; Liu et al., 2010a,b) and translation (Zens and Ney, 2003; Xiong et al., 2006) benchmark tests. Although ITG only requires O(n6 ) time for finding Viterbi alignment, which is a significant improvement over the intractable search problem faced by most alignment models (Brown et al., 1993; Moore et al., 2006; Liu et al., 2010c), the degree of the polynomial is still too high for practical use. For example, the maximal sentence length of bilingual corpus is often set to 100 words in Moses (Koehn et al., 2007), a state-of-the-art SMT system. Synchronous parsing of such long sentences can be prohi"
C12-2066,P07-2045,0,0.00833936,"in a way that proves to be effective in both alignment (Zhang and Gildea, 2005, 2006; Haghighi et al., 2009; Liu et al., 2010a,b) and translation (Zens and Ney, 2003; Xiong et al., 2006) benchmark tests. Although ITG only requires O(n6 ) time for finding Viterbi alignment, which is a significant improvement over the intractable search problem faced by most alignment models (Brown et al., 1993; Moore et al., 2006; Liu et al., 2010c), the degree of the polynomial is still too high for practical use. For example, the maximal sentence length of bilingual corpus is often set to 100 words in Moses (Koehn et al., 2007), a state-of-the-art SMT system. Synchronous parsing of such long sentences can be prohibitively slow, making ITG alignment methods hard to deal with large scale real-world data. To alleviate this problem, many pruning methods have been proposed to reduce the computational complexity of synchronous parsing by pruning less promising cells. Zhang and Gildea (2005) introduce a tic-tac-toe pruning method based on IBM model 1 probabilities. Haghighi et al. (2009) use posterior predictions from simpler alignment models for identifying degenerate cells. Liu et al. (2010a) propose a discriminative fra"
C12-2066,W09-0424,0,0.0189591,"ich link will result in an ITG alignment before calling the ITG(a) procedure. We leave this for future work. 3.2 Translation Evaluation For the translation evaluation, we used 138K sentence pairs that have at most 40 words from the FBIS corpus as the training set, NIST 2002 dataset as the development set, and NIST 2005 dataset as the test set. As the biparsing algorithm runs too slow on the training data, we only compared our algorithm with biparsing+pruning in terms of average time per sentence pair and BLEU. Moses (Koehn et al., 2007) (a state-of-the-art phrase-based SMT system) and Joshua (Li et al., 2009) (a state-of-the-art hierarchial phrase-based SMT system) are used in our experiments. Both of them are used with default settings, except that word alignments are produced by “biparsing+pruning” and “beam search” respectively rather than GIZA++. Table 2 shows the average aligning time as well as the BLEU scores obtained by Moses and Joshua. Our system runs 20 times faster than the baseline without significant loss in translation quality. algorithm biparsing+pruning beam search setting t = 10−5 b = 10 average time (s) 7.57 0.35 Moses 23.86 23.95 Joshua 23.77 23.38 Table 2: Comparison of averag"
C12-2066,P10-1033,0,0.0772598,"rd alignment has a profound effect on the performance of SMT systems (Ayan and Dorr, 2006; Fraser and Marcu, 2007). One major challenge in word alignment is modeling the permutations of words between source and target sentences. Due to the diversity of natural languages, the word orders of source and target sentences are usually quite different, especially for distantly-related language pairs such as Chinese and English. While most word alignment approaches either use distortion models (Brown et al., 1993; Vogel and Ney, 1996) or features (Taskar et al., 2005; Moore, 2005; Moore et al., 2006; Liu et al., 2010c) to capture reordering of words, inversion transduction grammar (ITG) (Wu, 1997) provides a syntactically motivated solution. ITG is a synchronous grammar of which a derivation explains how a source sentence and a target sentence are generated synchronously. By recursively merging blocks (i.e., consecutive word sequences) either in a monotone order or an inverted order, ITG constrains the search space of distortion in a way that proves to be effective in both alignment (Zhang and Gildea, 2005, 2006; Haghighi et al., 2009; Liu et al., 2010a,b) and translation (Zens and Ney, 2003; Xiong et al."
C12-2066,C10-2084,0,0.10331,"rd alignment has a profound effect on the performance of SMT systems (Ayan and Dorr, 2006; Fraser and Marcu, 2007). One major challenge in word alignment is modeling the permutations of words between source and target sentences. Due to the diversity of natural languages, the word orders of source and target sentences are usually quite different, especially for distantly-related language pairs such as Chinese and English. While most word alignment approaches either use distortion models (Brown et al., 1993; Vogel and Ney, 1996) or features (Taskar et al., 2005; Moore, 2005; Moore et al., 2006; Liu et al., 2010c) to capture reordering of words, inversion transduction grammar (ITG) (Wu, 1997) provides a syntactically motivated solution. ITG is a synchronous grammar of which a derivation explains how a source sentence and a target sentence are generated synchronously. By recursively merging blocks (i.e., consecutive word sequences) either in a monotone order or an inverted order, ITG constrains the search space of distortion in a way that proves to be effective in both alignment (Zhang and Gildea, 2005, 2006; Haghighi et al., 2009; Liu et al., 2010a,b) and translation (Zens and Ney, 2003; Xiong et al."
C12-2066,P05-1057,1,0.80375,"ligning time and model score over various sentence lengths. 2. biparsing + pruning: the bilingual parsing algorithm with tic-tac-toe pruning (Zhang and Gildea, 2005). For simplicity, we used the IBM model 4 translation probabilities trained on the FBIS corpus (6.5M+8.4M words) to approximate ITG lexical probabilities in the following experiments: p( f , e) ≈ pm4 ( f |e) × pm4 (e |f )/2, p( f , ε) ≈ pm4 ( f |ε), p(ε, e) ≈ pm4 (e|ε). 3.1 Alignment Evaluation For the alignment evaluation, we selected 461 sentence pairs that contain at most 50 words on both sides from the hand-aligned dataset of (Liu et al., 2005). The three ITG alignment methods are compared in terms of average time per sentence pair, average model score per sentence pair, and AER. The results are shown in Table 1. Although achieving the best model score and AER, the biparsing algorithm runs too slow: 126.164 seconds per sentence pair on average. This is impractical for dealing with large scale real-world data that usually contains millions of sentence pairs. The tic-tac-toe pruning method (biparsing + pruning) does increase the speed by two orders of magnitude (3.571 seconds per sentence pair), which confirms the effectiveness of cel"
C12-2066,J10-3002,1,0.283909,"rd alignment has a profound effect on the performance of SMT systems (Ayan and Dorr, 2006; Fraser and Marcu, 2007). One major challenge in word alignment is modeling the permutations of words between source and target sentences. Due to the diversity of natural languages, the word orders of source and target sentences are usually quite different, especially for distantly-related language pairs such as Chinese and English. While most word alignment approaches either use distortion models (Brown et al., 1993; Vogel and Ney, 1996) or features (Taskar et al., 2005; Moore, 2005; Moore et al., 2006; Liu et al., 2010c) to capture reordering of words, inversion transduction grammar (ITG) (Wu, 1997) provides a syntactically motivated solution. ITG is a synchronous grammar of which a derivation explains how a source sentence and a target sentence are generated synchronously. By recursively merging blocks (i.e., consecutive word sequences) either in a monotone order or an inverted order, ITG constrains the search space of distortion in a way that proves to be effective in both alignment (Zhang and Gildea, 2005, 2006; Haghighi et al., 2009; Liu et al., 2010a,b) and translation (Zens and Ney, 2003; Xiong et al."
C12-2066,H05-1011,0,0.0257162,"cessfully. Studies reveal that word alignment has a profound effect on the performance of SMT systems (Ayan and Dorr, 2006; Fraser and Marcu, 2007). One major challenge in word alignment is modeling the permutations of words between source and target sentences. Due to the diversity of natural languages, the word orders of source and target sentences are usually quite different, especially for distantly-related language pairs such as Chinese and English. While most word alignment approaches either use distortion models (Brown et al., 1993; Vogel and Ney, 1996) or features (Taskar et al., 2005; Moore, 2005; Moore et al., 2006; Liu et al., 2010c) to capture reordering of words, inversion transduction grammar (ITG) (Wu, 1997) provides a syntactically motivated solution. ITG is a synchronous grammar of which a derivation explains how a source sentence and a target sentence are generated synchronously. By recursively merging blocks (i.e., consecutive word sequences) either in a monotone order or an inverted order, ITG constrains the search space of distortion in a way that proves to be effective in both alignment (Zhang and Gildea, 2005, 2006; Haghighi et al., 2009; Liu et al., 2010a,b) and transla"
C12-2066,P06-1065,0,0.020514,"udies reveal that word alignment has a profound effect on the performance of SMT systems (Ayan and Dorr, 2006; Fraser and Marcu, 2007). One major challenge in word alignment is modeling the permutations of words between source and target sentences. Due to the diversity of natural languages, the word orders of source and target sentences are usually quite different, especially for distantly-related language pairs such as Chinese and English. While most word alignment approaches either use distortion models (Brown et al., 1993; Vogel and Ney, 1996) or features (Taskar et al., 2005; Moore, 2005; Moore et al., 2006; Liu et al., 2010c) to capture reordering of words, inversion transduction grammar (ITG) (Wu, 1997) provides a syntactically motivated solution. ITG is a synchronous grammar of which a derivation explains how a source sentence and a target sentence are generated synchronously. By recursively merging blocks (i.e., consecutive word sequences) either in a monotone order or an inverted order, ITG constrains the search space of distortion in a way that proves to be effective in both alignment (Zhang and Gildea, 2005, 2006; Haghighi et al., 2009; Liu et al., 2010a,b) and translation (Zens and Ney,"
C12-2066,J04-4002,0,0.077746,"© ÛŽ{Œ±3õ‘ªžmS|¢ Viterbiéà§ÙOŽE,Ý•, p§J±?n•é õ•éf ý¢êâ""•d§·‚JÑ˜«{ük Î|¢Ž{""TŽ{±˜éà•å :§`kÀJ•Ð ë‚V éà¥§†–Ã{Jp .VÇ•Ž""3Ç=êâþ ¢ (JL§·‚ Ž{'¦^}{Eâ VŠ©ÛŽ{¯˜‡êþ?§Óž ± éà Ú€È Ÿþ"" KEYWORDS: word alignment, inversion transduction grammar, beam search. KEYWORDS IN L2 : cŠéà, ‡•=Š{, Î|¢. Proceedings of COLING 2012: Posters, pages 673–682, COLING 2012, Mumbai, December 2012. 673 1 Introduction Word alignment plays an important role in statistical machine translation (SMT) as it indicates the correspondence between two languages. The parameter estimation of many SMT models rely heavily on word alignment. Och and Ney (2004) firstly introduce alignment consistency to identify equivalent phrase pairs. Simple and effective, rule extraction algorithms based on word alignment have also been extended to hierarchial phrase-based (Chiang, 2007) and syntax-based (Galley et al., 2004) SMT systems successfully. Studies reveal that word alignment has a profound effect on the performance of SMT systems (Ayan and Dorr, 2006; Fraser and Marcu, 2007). One major challenge in word alignment is modeling the permutations of words between source and target sentences. Due to the diversity of natural languages, the word orders of sour"
C12-2066,H05-1010,0,0.0310368,"2004) SMT systems successfully. Studies reveal that word alignment has a profound effect on the performance of SMT systems (Ayan and Dorr, 2006; Fraser and Marcu, 2007). One major challenge in word alignment is modeling the permutations of words between source and target sentences. Due to the diversity of natural languages, the word orders of source and target sentences are usually quite different, especially for distantly-related language pairs such as Chinese and English. While most word alignment approaches either use distortion models (Brown et al., 1993; Vogel and Ney, 1996) or features (Taskar et al., 2005; Moore, 2005; Moore et al., 2006; Liu et al., 2010c) to capture reordering of words, inversion transduction grammar (ITG) (Wu, 1997) provides a syntactically motivated solution. ITG is a synchronous grammar of which a derivation explains how a source sentence and a target sentence are generated synchronously. By recursively merging blocks (i.e., consecutive word sequences) either in a monotone order or an inverted order, ITG constrains the search space of distortion in a way that proves to be effective in both alignment (Zhang and Gildea, 2005, 2006; Haghighi et al., 2009; Liu et al., 2010a,b"
C12-2066,C96-2141,0,0.252895,"and syntax-based (Galley et al., 2004) SMT systems successfully. Studies reveal that word alignment has a profound effect on the performance of SMT systems (Ayan and Dorr, 2006; Fraser and Marcu, 2007). One major challenge in word alignment is modeling the permutations of words between source and target sentences. Due to the diversity of natural languages, the word orders of source and target sentences are usually quite different, especially for distantly-related language pairs such as Chinese and English. While most word alignment approaches either use distortion models (Brown et al., 1993; Vogel and Ney, 1996) or features (Taskar et al., 2005; Moore, 2005; Moore et al., 2006; Liu et al., 2010c) to capture reordering of words, inversion transduction grammar (ITG) (Wu, 1997) provides a syntactically motivated solution. ITG is a synchronous grammar of which a derivation explains how a source sentence and a target sentence are generated synchronously. By recursively merging blocks (i.e., consecutive word sequences) either in a monotone order or an inverted order, ITG constrains the search space of distortion in a way that proves to be effective in both alignment (Zhang and Gildea, 2005, 2006; Haghighi"
C12-2066,J97-3002,0,0.791372,"; Fraser and Marcu, 2007). One major challenge in word alignment is modeling the permutations of words between source and target sentences. Due to the diversity of natural languages, the word orders of source and target sentences are usually quite different, especially for distantly-related language pairs such as Chinese and English. While most word alignment approaches either use distortion models (Brown et al., 1993; Vogel and Ney, 1996) or features (Taskar et al., 2005; Moore, 2005; Moore et al., 2006; Liu et al., 2010c) to capture reordering of words, inversion transduction grammar (ITG) (Wu, 1997) provides a syntactically motivated solution. ITG is a synchronous grammar of which a derivation explains how a source sentence and a target sentence are generated synchronously. By recursively merging blocks (i.e., consecutive word sequences) either in a monotone order or an inverted order, ITG constrains the search space of distortion in a way that proves to be effective in both alignment (Zhang and Gildea, 2005, 2006; Haghighi et al., 2009; Liu et al., 2010a,b) and translation (Zens and Ney, 2003; Xiong et al., 2006) benchmark tests. Although ITG only requires O(n6 ) time for finding Viterb"
C12-2066,P06-1066,0,0.171229,"et al., 2010c) to capture reordering of words, inversion transduction grammar (ITG) (Wu, 1997) provides a syntactically motivated solution. ITG is a synchronous grammar of which a derivation explains how a source sentence and a target sentence are generated synchronously. By recursively merging blocks (i.e., consecutive word sequences) either in a monotone order or an inverted order, ITG constrains the search space of distortion in a way that proves to be effective in both alignment (Zhang and Gildea, 2005, 2006; Haghighi et al., 2009; Liu et al., 2010a,b) and translation (Zens and Ney, 2003; Xiong et al., 2006) benchmark tests. Although ITG only requires O(n6 ) time for finding Viterbi alignment, which is a significant improvement over the intractable search problem faced by most alignment models (Brown et al., 1993; Moore et al., 2006; Liu et al., 2010c), the degree of the polynomial is still too high for practical use. For example, the maximal sentence length of bilingual corpus is often set to 100 words in Moses (Koehn et al., 2007), a state-of-the-art SMT system. Synchronous parsing of such long sentences can be prohibitively slow, making ITG alignment methods hard to deal with large scale real-"
C12-2066,P03-1019,0,0.0390805,"e et al., 2006; Liu et al., 2010c) to capture reordering of words, inversion transduction grammar (ITG) (Wu, 1997) provides a syntactically motivated solution. ITG is a synchronous grammar of which a derivation explains how a source sentence and a target sentence are generated synchronously. By recursively merging blocks (i.e., consecutive word sequences) either in a monotone order or an inverted order, ITG constrains the search space of distortion in a way that proves to be effective in both alignment (Zhang and Gildea, 2005, 2006; Haghighi et al., 2009; Liu et al., 2010a,b) and translation (Zens and Ney, 2003; Xiong et al., 2006) benchmark tests. Although ITG only requires O(n6 ) time for finding Viterbi alignment, which is a significant improvement over the intractable search problem faced by most alignment models (Brown et al., 1993; Moore et al., 2006; Liu et al., 2010c), the degree of the polynomial is still too high for practical use. For example, the maximal sentence length of bilingual corpus is often set to 100 words in Moses (Koehn et al., 2007), a state-of-the-art SMT system. Synchronous parsing of such long sentences can be prohibitively slow, making ITG alignment methods hard to deal w"
C12-2066,P05-1059,0,0.374016,"rown et al., 1993; Vogel and Ney, 1996) or features (Taskar et al., 2005; Moore, 2005; Moore et al., 2006; Liu et al., 2010c) to capture reordering of words, inversion transduction grammar (ITG) (Wu, 1997) provides a syntactically motivated solution. ITG is a synchronous grammar of which a derivation explains how a source sentence and a target sentence are generated synchronously. By recursively merging blocks (i.e., consecutive word sequences) either in a monotone order or an inverted order, ITG constrains the search space of distortion in a way that proves to be effective in both alignment (Zhang and Gildea, 2005, 2006; Haghighi et al., 2009; Liu et al., 2010a,b) and translation (Zens and Ney, 2003; Xiong et al., 2006) benchmark tests. Although ITG only requires O(n6 ) time for finding Viterbi alignment, which is a significant improvement over the intractable search problem faced by most alignment models (Brown et al., 1993; Moore et al., 2006; Liu et al., 2010c), the degree of the polynomial is still too high for practical use. For example, the maximal sentence length of bilingual corpus is often set to 100 words in Moses (Koehn et al., 2007), a state-of-the-art SMT system. Synchronous parsing of suc"
C12-2066,W06-1627,0,0.0409849,"Missing"
C12-2066,J07-2003,0,\N,Missing
C12-2073,W06-1615,0,0.270605,"Missing"
C12-2073,W03-0407,0,0.0270842,"Missing"
C12-2073,P07-1034,0,0.281063,"orums, and internet literature, which are written in a different genre, and for which little manual annotation is available. In this paper, we choose internet literature as the target domain, and study domain adaptation for joint segmentation and POS-tagging. We consider the single model approach of Zhang and Clark (2010), trained using the CTB, as our baseline system, and apply self-training and character clustering to improve its performance on our test data from an internet novel. Much work has been done on domain adaptation for POS-tagging (Blitzer et al., 2006; Daumé III and Marcu, 2006; Jiang and Zhai, 2007). However, relatively little attention has been paid to the domain adaptation for joint segmentation and POS-tagging. Among the range of methods that have been developed for domain adaptation, self-training and character clustering are applicable to a comparatively large number of baseline supervised model types, including feature-based probability models and large-margin discriminative models, and are fairly straightforward to implement. We focus on unsupervised domain adaptation, using fully unannotated data in the target-domain. We evaluate our system on a set of manually annotated target-d"
C12-2073,P08-1102,0,0.0165596,"Missing"
C12-2073,C08-1049,0,0.0242137,"Missing"
C12-2073,P08-1068,0,0.0113675,"in Saerens et al. (2002), EM is applied to the target-domain only, and the source data is used for an initial estimation. In this paper, we apply the standard self-training process, but with target-domain data point selection (Rehbein, 2011; Søgaard, 2011). 3 Character clustering Word/character clustering is an unsupervised approach that groups similar words/characters according to their context. Clusters can be used as features instead of the original words/characters for the reduction of data sparsity. Word clustering has been applied to many NLP problems (Miller et al., 2004; Liang, 2005; Koo et al., 2008). For our domain adaptation problem, clusters are created from large unannotated targetdomain data, and applied as features in our joint segmentor and POS-tagger during both training and testing. The weights of the cluster features are estimated during training using source-domain data. During testing, they can help to alleviate the out-of-vocabulary (OOV) problem in the target-domain when a rare input has not been seen in the training data but belongs to a known cluster. We use Liang’s implementation (Liang, 2005) of the bottom-up agglomerative Brown algorithm (Brown et al., 1992) to generate"
C12-2073,P09-1058,0,0.0222645,"tion (Liang, 2005) of the bottom-up agglomerative Brown algorithm (Brown et al., 1992) to generate character clusters, choosing the numbers of clusters according to development experiments. 4 Experiments Software We use ZPar (Zhang and Clark, 2010, 2011) as the baseline system1 . The system uses a single discriminative model for joint segmentation and tagging, trained using the generalized perceptron algorithm. Standard beam search is applied to ensure efficient decoding. Source-domain data We use the CTB 5 for source-domain training, making the same training, development and test sections as Kruengkrai et al. (2009) (Table 1). Target-domain data We collect the target-domain data from a Chinese Internet novel “Jade dynasty”2 (also known as “Zhuxian”) by Ding Xiao. The first 18 chapters (927K words in 25413 sentences) have been collected. Section 1 of chapter 6 is used as the development data, and 1 2 www.sourceforge.net/project/zpar; version 0.4 An electronic version of the book is free for download from the Internet. 747 Data set chap. IDs # of sen. # of words Training 1-5, 8-18 6.1 7.2 25413 927405 159 226 5077 5173 Development Test Table 2: Target-domain training, development and test data. BE FA DC BD"
C12-2073,N06-1020,0,0.0202212,"Missing"
C12-2073,N04-1043,0,0.00851174,"ta is increased at each iteration; in Saerens et al. (2002), EM is applied to the target-domain only, and the source data is used for an initial estimation. In this paper, we apply the standard self-training process, but with target-domain data point selection (Rehbein, 2011; Søgaard, 2011). 3 Character clustering Word/character clustering is an unsupervised approach that groups similar words/characters according to their context. Clusters can be used as features instead of the original words/characters for the reduction of data sparsity. Word clustering has been applied to many NLP problems (Miller et al., 2004; Liang, 2005; Koo et al., 2008). For our domain adaptation problem, clusters are created from large unannotated targetdomain data, and applied as features in our joint segmentor and POS-tagger during both training and testing. The weights of the cluster features are estimated during training using source-domain data. During testing, they can help to alleviate the out-of-vocabulary (OOV) problem in the target-domain when a rare input has not been seen in the training data but belongs to a known cluster. We use Liang’s implementation (Liang, 2005) of the bottom-up agglomerative Brown algorithm"
C12-2073,W04-3236,0,0.0298088,"Missing"
C12-2073,W11-3808,0,0.0116869,"nd unlabeled data, the supervised training algorithm, and additional reranking and filtering of output predictions. Modifications can be made to the standard self-training process for domain adaptation to address the difference in source and target distributions (Margolis, 2011). In Tan et al. (2009), the weights on the target-domain data is increased at each iteration; in Saerens et al. (2002), EM is applied to the target-domain only, and the source data is used for an initial estimation. In this paper, we apply the standard self-training process, but with target-domain data point selection (Rehbein, 2011; Søgaard, 2011). 3 Character clustering Word/character clustering is an unsupervised approach that groups similar words/characters according to their context. Clusters can be used as features instead of the original words/characters for the reduction of data sparsity. Word clustering has been applied to many NLP problems (Miller et al., 2004; Liang, 2005; Koo et al., 2008). For our domain adaptation problem, clusters are created from large unannotated targetdomain data, and applied as features in our joint segmentor and POS-tagger during both training and testing. The weights of the cluster f"
C12-2073,P07-1078,0,0.0158414,"Missing"
C12-2073,N03-1027,0,0.0319633,"s reported. Clark et al. (2003) apply self-training to POS -tagging and achieve minor improvements. Steedman et al. (2003) report that self-training can either slightly improve or significantly harm the parsing accuracy. McClosky et al. (2006) achieves improved parsing accuracies using self-training, and Reichart and Rappoport (2007) has obtained significant improvement on small datasets with lexicalized parser. In this paper, we focus on the use of self-training for unsupervised domain adaptation. Selftraining has been applied to the domain adaptation of several NLP tasks, including parsing (Roark and Bacchiani, 2003; Sagae, 2010), POS-tagging (Jiang and Zhai, 2007) and crosslanguage text classification (Shi et al., 2010). It improves system performance on the target domain by simultaneously modelling annotated source-domain data and unannotated targetdomain data in the training process. Theoretically, self-training has a strong relationship with the EM algorithm, where tagging unlabeled data corresponds to the expectation step, and supervised parameter estimation corresponds to the maximization step. There are various factors that affects the effectiveness of self-training, such as the difference in the"
C12-2073,W10-2606,0,0.0144402,"003) apply self-training to POS -tagging and achieve minor improvements. Steedman et al. (2003) report that self-training can either slightly improve or significantly harm the parsing accuracy. McClosky et al. (2006) achieves improved parsing accuracies using self-training, and Reichart and Rappoport (2007) has obtained significant improvement on small datasets with lexicalized parser. In this paper, we focus on the use of self-training for unsupervised domain adaptation. Selftraining has been applied to the domain adaptation of several NLP tasks, including parsing (Roark and Bacchiani, 2003; Sagae, 2010), POS-tagging (Jiang and Zhai, 2007) and crosslanguage text classification (Shi et al., 2010). It improves system performance on the target domain by simultaneously modelling annotated source-domain data and unannotated targetdomain data in the training process. Theoretically, self-training has a strong relationship with the EM algorithm, where tagging unlabeled data corresponds to the expectation step, and supervised parameter estimation corresponds to the maximization step. There are various factors that affects the effectiveness of self-training, such as the difference in the distributions"
C12-2073,D10-1103,0,0.0195218,"(2003) report that self-training can either slightly improve or significantly harm the parsing accuracy. McClosky et al. (2006) achieves improved parsing accuracies using self-training, and Reichart and Rappoport (2007) has obtained significant improvement on small datasets with lexicalized parser. In this paper, we focus on the use of self-training for unsupervised domain adaptation. Selftraining has been applied to the domain adaptation of several NLP tasks, including parsing (Roark and Bacchiani, 2003; Sagae, 2010), POS-tagging (Jiang and Zhai, 2007) and crosslanguage text classification (Shi et al., 2010). It improves system performance on the target domain by simultaneously modelling annotated source-domain data and unannotated targetdomain data in the training process. Theoretically, self-training has a strong relationship with the EM algorithm, where tagging unlabeled data corresponds to the expectation step, and supervised parameter estimation corresponds to the maximization step. There are various factors that affects the effectiveness of self-training, such as the difference in the distributions of 746 Data set chap. IDs # of sen. # of words Training 1-270, 400-931, 1001-1151 301-325 271"
C12-2073,P11-2120,0,0.0172639,"ta, the supervised training algorithm, and additional reranking and filtering of output predictions. Modifications can be made to the standard self-training process for domain adaptation to address the difference in source and target distributions (Margolis, 2011). In Tan et al. (2009), the weights on the target-domain data is increased at each iteration; in Saerens et al. (2002), EM is applied to the target-domain only, and the source data is used for an initial estimation. In this paper, we apply the standard self-training process, but with target-domain data point selection (Rehbein, 2011; Søgaard, 2011). 3 Character clustering Word/character clustering is an unsupervised approach that groups similar words/characters according to their context. Clusters can be used as features instead of the original words/characters for the reduction of data sparsity. Word clustering has been applied to many NLP problems (Miller et al., 2004; Liang, 2005; Koo et al., 2008). For our domain adaptation problem, clusters are created from large unannotated targetdomain data, and applied as features in our joint segmentor and POS-tagger during both training and testing. The weights of the cluster features are esti"
C12-2073,E03-1008,0,0.0151779,"Missing"
C12-2073,P11-1139,0,0.0669048,"Missing"
C12-2073,C10-2168,1,0.876311,"Missing"
C12-2073,P08-1101,1,0.83525,"Missing"
C12-2073,D10-1082,1,0.730777,"督学习，聚类，自学习 Proceedings of COLING 2012: Posters, pages 745–754, COLING 2012, Mumbai, December 2012. 745 1 Introduction Joint segmentation and POS-tagging can improve upon a pipelined baseline by reducing error propagation and accommodating features that represent combined word and POS information. Three general approaches have been taken to perform joint inference, namely two-stage ensemble methods (Jiang et al., 2008a; Sun, 2011), reranking (Jiang et al., 2008b; Shi and Wang, 2007) and single joint models with heuristic search (Ng and Low, 2004; Zhang and Clark, 2008; Kruengkrai et al., 2009; Zhang and Clark, 2010), leading to improved accuracies on the Chinese Treebank data. All these methods rely on supervised learning, and are expected to perform worse when the test domain shifts from CTB to blogs, computer forums, and internet literature, which are written in a different genre, and for which little manual annotation is available. In this paper, we choose internet literature as the target domain, and study domain adaptation for joint segmentation and POS-tagging. We consider the single model approach of Zhang and Clark (2010), trained using the CTB, as our baseline system, and apply self-training and"
C12-2073,J11-1005,1,0.851308,"Missing"
C12-2073,C04-1081,0,\N,Missing
C12-2073,P04-1015,0,\N,Missing
C12-2073,D10-1056,0,\N,Missing
C12-2073,P08-1085,0,\N,Missing
C12-2073,D12-1075,0,\N,Missing
C12-2073,J94-2001,0,\N,Missing
C12-2073,I11-1035,0,\N,Missing
C12-2073,J92-4003,0,\N,Missing
C12-2073,P13-1076,0,\N,Missing
C12-2073,P09-1057,0,\N,Missing
C12-2073,petrov-etal-2012-universal,0,\N,Missing
C12-2073,Q13-1001,0,\N,Missing
C12-2073,P13-1057,0,\N,Missing
C12-2073,N13-1014,0,\N,Missing
C12-2073,P12-1110,0,\N,Missing
C12-2073,I05-3025,0,\N,Missing
C12-2073,D10-1017,0,\N,Missing
C12-2073,P07-1094,0,\N,Missing
C12-2073,P07-1033,0,\N,Missing
C12-2122,P06-1002,0,0.0193855,"nother alignment by Vigne (a3 ). where S and P are sets of sure and possible links in a hand-aligned reference alignment respectively, and A is a candidate alignment. Note that S is a subset of P: S ⊆ P. As there is no reference alignment that is hand-aligned by human experts in our work, we cannot distinguish sure links from possible links. Therefore, we regard all links to be sure links: S = P. With this, the AER score is calculated by: AER(ai , a j ) = 1 − (2 × |ai ∩ a j |)/(|ai |+ |a j |) (3) CPER Although widely used, AER is criticized for correlating poorly with translation performance (Ayan and Dorr, 2006; Fraser and Marcu, 2007). Therefore, Ayan and Dorr (2006) have proposed constituent phrase error rate (CPER) for evaluating word alignments at the phrase level instead of the alignment level. CPER can be computed as: C P ER(ai , a j ) = 1 − (2 × |Pai ∩ Pa j |)/(|Pai |+ |Pa j |) (4) where Pa denotes the set of phrases that are consistent with a given alignment a. Compared with AER, CPER penalizes dissimilar alignment links more heavily. As a dissimilar link reduces the number of intersected links of two alignments by 1 in AER, it might lead to more than one different phrase pair added to or re"
C12-2122,H05-1009,0,0.0216,"stem combination, the process which integrates fragment outputs from multiple systems, has produced substantial improvements in many natural language processing tasks, including parsing (Henderson and Brill, 1999; Sagae and Lavie, 2006; Fossum and Knight, 2009), word segmentation (Sun and Wan, 2012) and machine translation (Rosti et al., 2007; He et al., 1 These alignments have equivalent qualities compared to a true gold standard (see in Table 2). 1250 2008; Feng et al., 2009), just to name a few. Alignment combination has also been explored previously (Och and Ney, 2003; Koehn et al., 2003; Ayan et al., 2005; DeNero and Macherey, 2011). We draw inspiration from (Och and Ney, 2003; Koehn et al., 2003) but our technique differs from previous work in that (1) they require exactly two bidirectional alignments while our approach can use an arbitrary number of alignments; (2) we take into account the occurrences of potential links, which turns out to be important. Previous research has demonstrated that compact representations can produce improved results by offering more alternatives, e.g. using forests over 1-best trees (Mi and Huang, 2008; Tu et al., 2010), word lattices over 1-best segmentations (D"
C12-2122,P06-1009,0,0.0532677,"Missing"
C12-2122,J93-2003,0,0.0478199,"Missing"
C12-2122,P05-1033,0,0.441175,"robabilities. Instead of extracting phrase pairs that respect the word alignment, Tu et al. (2011) enumerate all potential phrase pairs and calculate their fractional counts. As they soften the alignment consistency constraint, there exists a massive number of phrase pairs extracted from the training corpus. To maintain a reasonable phrase table size, they discard any phrase pair that has a fractional count lower than a threshold t. For further details, see (Tu et al., 2011). 4 Experiments 4.1 Setup We carry out our experiments using a reimplementation of the hierarchical phrase-based system (Chiang, 2005) on the NIST Chinese-English translation tasks. Our training data contains 1.5M sentence pairs from LDC dataset.4 We train a 4-gram language model on the Xinhua portion of the GIGAWORD corpus using the SRI Language Toolkit (Stolcke, 2002) with modified Kneser-Ney Smoothing (Kneser and Ney, 1995). We use minimum error rate training (Och, 2003) to optimize the feature weights on the MT02 testset, and test on the MT03/04/05 testsets. For evaluation, case-insensitive NIST BLEU (Papineni et al., 2002) is used to measure translation performance. Three alignment models are chosen for our experiments"
C12-2122,P05-1066,0,0.166329,"Missing"
C12-2122,P11-1043,0,0.0377862,"he process which integrates fragment outputs from multiple systems, has produced substantial improvements in many natural language processing tasks, including parsing (Henderson and Brill, 1999; Sagae and Lavie, 2006; Fossum and Knight, 2009), word segmentation (Sun and Wan, 2012) and machine translation (Rosti et al., 2007; He et al., 1 These alignments have equivalent qualities compared to a true gold standard (see in Table 2). 1250 2008; Feng et al., 2009), just to name a few. Alignment combination has also been explored previously (Och and Ney, 2003; Koehn et al., 2003; Ayan et al., 2005; DeNero and Macherey, 2011). We draw inspiration from (Och and Ney, 2003; Koehn et al., 2003) but our technique differs from previous work in that (1) they require exactly two bidirectional alignments while our approach can use an arbitrary number of alignments; (2) we take into account the occurrences of potential links, which turns out to be important. Previous research has demonstrated that compact representations can produce improved results by offering more alternatives, e.g. using forests over 1-best trees (Mi and Huang, 2008; Tu et al., 2010), word lattices over 1-best segmentations (Dyer et al., 2008), and weigh"
C12-2122,C10-1036,0,0.01903,"2 3.1.2 Loss Functions The loss function L (ai , a j ) is used to measure the quality of alignments. Here we introduce a set of metrics for the evaluation of alignments at both alignment and phrase levels. AER Alignment error rate (Och and Ney, 2003) has been used as the official evaluation criterion in most alignment shared tasks (Liu et al., 2009). AER scores are given by: 2 AER(S, P, A) = 1 − (|A ∩ S |+ |A ∩ P|)/(|A |+ |S|) (2) Alignment probabilities can be set empirically based on (expected overall) performance (Fossum and Knight, 2009), or uniformly without any bias (Xiao et al., 2010; Duan et al., 2010). We tried a few other settings and found them to be less effective. 1251 the development of China ’s economy (a) zhongguo de jingji fazhan zhongguo de jingji fazhan zhongguo de jingji fazhan the development of China ’s economy the development of China ’s economy (b) (c) Figure 1: (a) Alignment of a sentence pair generated by GIZA++ (a1 ), (b) alignment of the same sentence by Berkeley aligner (a2 ), (c) another alignment by Vigne (a3 ). where S and P are sets of sure and possible links in a hand-aligned reference alignment respectively, and A is a candidate alignment. Note that S is a subset"
C12-2122,P08-1115,0,0.0280219,"5; DeNero and Macherey, 2011). We draw inspiration from (Och and Ney, 2003; Koehn et al., 2003) but our technique differs from previous work in that (1) they require exactly two bidirectional alignments while our approach can use an arbitrary number of alignments; (2) we take into account the occurrences of potential links, which turns out to be important. Previous research has demonstrated that compact representations can produce improved results by offering more alternatives, e.g. using forests over 1-best trees (Mi and Huang, 2008; Tu et al., 2010), word lattices over 1-best segmentations (Dyer et al., 2008), and weighted alignment matrices (WAMs) over 1-best alignments (Liu et al., 2009; Tu et al., 2011). Instead of using k-best alignments from the same model, as in (Liu et al., 2009; Tu et al., 2011), here we construct WAMs from multiple alignments generated by different models (including MBRbased and refined models). As the alignment probabilities are generally incomparable between different alignment models, we propose a novel calculation of link probabilities in WAMs. 3 Approach 3.1 Alignment Selection Alignment selection refers to selecting one alignment from multiple alignments using minim"
C12-2122,D09-1115,1,0.846211,"). In our work, we select an alignment within the MBR framework using a number of loss functions at both alignment and phrase levels. System combination, the process which integrates fragment outputs from multiple systems, has produced substantial improvements in many natural language processing tasks, including parsing (Henderson and Brill, 1999; Sagae and Lavie, 2006; Fossum and Knight, 2009), word segmentation (Sun and Wan, 2012) and machine translation (Rosti et al., 2007; He et al., 1 These alignments have equivalent qualities compared to a true gold standard (see in Table 2). 1250 2008; Feng et al., 2009), just to name a few. Alignment combination has also been explored previously (Och and Ney, 2003; Koehn et al., 2003; Ayan et al., 2005; DeNero and Macherey, 2011). We draw inspiration from (Och and Ney, 2003; Koehn et al., 2003) but our technique differs from previous work in that (1) they require exactly two bidirectional alignments while our approach can use an arbitrary number of alignments; (2) we take into account the occurrences of potential links, which turns out to be important. Previous research has demonstrated that compact representations can produce improved results by offering mo"
C12-2122,N09-2064,0,0.13425,"Diverse loss functions have been described by using different evaluation criteria for loss calculation, e.g. edit distance and sentence-level BLEU in SMT (Kumar and Byrne, 2004; Tromble et al., 2008; González-Rubio et al., 2011). In our work, we select an alignment within the MBR framework using a number of loss functions at both alignment and phrase levels. System combination, the process which integrates fragment outputs from multiple systems, has produced substantial improvements in many natural language processing tasks, including parsing (Henderson and Brill, 1999; Sagae and Lavie, 2006; Fossum and Knight, 2009), word segmentation (Sun and Wan, 2012) and machine translation (Rosti et al., 2007; He et al., 1 These alignments have equivalent qualities compared to a true gold standard (see in Table 2). 1250 2008; Feng et al., 2009), just to name a few. Alignment combination has also been explored previously (Och and Ney, 2003; Koehn et al., 2003; Ayan et al., 2005; DeNero and Macherey, 2011). We draw inspiration from (Och and Ney, 2003; Koehn et al., 2003) but our technique differs from previous work in that (1) they require exactly two bidirectional alignments while our approach can use an arbitrary nu"
C12-2122,J07-3002,0,0.0988242,"igne (a3 ). where S and P are sets of sure and possible links in a hand-aligned reference alignment respectively, and A is a candidate alignment. Note that S is a subset of P: S ⊆ P. As there is no reference alignment that is hand-aligned by human experts in our work, we cannot distinguish sure links from possible links. Therefore, we regard all links to be sure links: S = P. With this, the AER score is calculated by: AER(ai , a j ) = 1 − (2 × |ai ∩ a j |)/(|ai |+ |a j |) (3) CPER Although widely used, AER is criticized for correlating poorly with translation performance (Ayan and Dorr, 2006; Fraser and Marcu, 2007). Therefore, Ayan and Dorr (2006) have proposed constituent phrase error rate (CPER) for evaluating word alignments at the phrase level instead of the alignment level. CPER can be computed as: C P ER(ai , a j ) = 1 − (2 × |Pai ∩ Pa j |)/(|Pai |+ |Pa j |) (4) where Pa denotes the set of phrases that are consistent with a given alignment a. Compared with AER, CPER penalizes dissimilar alignment links more heavily. As a dissimilar link reduces the number of intersected links of two alignments by 1 in AER, it might lead to more than one different phrase pair added to or removed from the set of phr"
C12-2122,P06-1121,0,0.10519,"Missing"
C12-2122,P11-1127,0,0.0185151,"s high quality alignment choices, that can be exploited by alignment compaction. 2 Related Work Our research builds on previous work in the field of minimum Bayes risk (MBR) decision, system combination and model compaction. MBR decision aims to find the candidate hypothesis that has the least expected loss under a probability model when the true reference is not known (Brickel and Doksum, 1977). Diverse loss functions have been described by using different evaluation criteria for loss calculation, e.g. edit distance and sentence-level BLEU in SMT (Kumar and Byrne, 2004; Tromble et al., 2008; González-Rubio et al., 2011). In our work, we select an alignment within the MBR framework using a number of loss functions at both alignment and phrase levels. System combination, the process which integrates fragment outputs from multiple systems, has produced substantial improvements in many natural language processing tasks, including parsing (Henderson and Brill, 1999; Sagae and Lavie, 2006; Fossum and Knight, 2009), word segmentation (Sun and Wan, 2012) and machine translation (Rosti et al., 2007; He et al., 1 These alignments have equivalent qualities compared to a true gold standard (see in Table 2). 1250 2008; F"
C12-2122,D08-1011,0,0.0397351,"Missing"
C12-2122,W99-0623,0,0.0194602,"eference is not known (Brickel and Doksum, 1977). Diverse loss functions have been described by using different evaluation criteria for loss calculation, e.g. edit distance and sentence-level BLEU in SMT (Kumar and Byrne, 2004; Tromble et al., 2008; González-Rubio et al., 2011). In our work, we select an alignment within the MBR framework using a number of loss functions at both alignment and phrase levels. System combination, the process which integrates fragment outputs from multiple systems, has produced substantial improvements in many natural language processing tasks, including parsing (Henderson and Brill, 1999; Sagae and Lavie, 2006; Fossum and Knight, 2009), word segmentation (Sun and Wan, 2012) and machine translation (Rosti et al., 2007; He et al., 1 These alignments have equivalent qualities compared to a true gold standard (see in Table 2). 1250 2008; Feng et al., 2009), just to name a few. Alignment combination has also been explored previously (Och and Ney, 2003; Koehn et al., 2003; Ayan et al., 2005; DeNero and Macherey, 2011). We draw inspiration from (Och and Ney, 2003; Koehn et al., 2003) but our technique differs from previous work in that (1) they require exactly two bidirectional alig"
C12-2122,2006.amta-papers.8,0,0.0607293,"Missing"
C12-2122,N03-1017,0,0.0694144,"nd phrase levels. System combination, the process which integrates fragment outputs from multiple systems, has produced substantial improvements in many natural language processing tasks, including parsing (Henderson and Brill, 1999; Sagae and Lavie, 2006; Fossum and Knight, 2009), word segmentation (Sun and Wan, 2012) and machine translation (Rosti et al., 2007; He et al., 1 These alignments have equivalent qualities compared to a true gold standard (see in Table 2). 1250 2008; Feng et al., 2009), just to name a few. Alignment combination has also been explored previously (Och and Ney, 2003; Koehn et al., 2003; Ayan et al., 2005; DeNero and Macherey, 2011). We draw inspiration from (Och and Ney, 2003; Koehn et al., 2003) but our technique differs from previous work in that (1) they require exactly two bidirectional alignments while our approach can use an arbitrary number of alignments; (2) we take into account the occurrences of potential links, which turns out to be important. Previous research has demonstrated that compact representations can produce improved results by offering more alternatives, e.g. using forests over 1-best trees (Mi and Huang, 2008; Tu et al., 2010), word lattices over 1-be"
C12-2122,N04-1022,0,0.0650294,"ell together: alignment refinement e.g. offers high quality alignment choices, that can be exploited by alignment compaction. 2 Related Work Our research builds on previous work in the field of minimum Bayes risk (MBR) decision, system combination and model compaction. MBR decision aims to find the candidate hypothesis that has the least expected loss under a probability model when the true reference is not known (Brickel and Doksum, 1977). Diverse loss functions have been described by using different evaluation criteria for loss calculation, e.g. edit distance and sentence-level BLEU in SMT (Kumar and Byrne, 2004; Tromble et al., 2008; González-Rubio et al., 2011). In our work, we select an alignment within the MBR framework using a number of loss functions at both alignment and phrase levels. System combination, the process which integrates fragment outputs from multiple systems, has produced substantial improvements in many natural language processing tasks, including parsing (Henderson and Brill, 1999; Sagae and Lavie, 2006; Fossum and Knight, 2009), word segmentation (Sun and Wan, 2012) and machine translation (Rosti et al., 2007; He et al., 1 These alignments have equivalent qualities compared to"
C12-2122,N06-1014,0,0.0889621,"ce pairs from LDC dataset.4 We train a 4-gram language model on the Xinhua portion of the GIGAWORD corpus using the SRI Language Toolkit (Stolcke, 2002) with modified Kneser-Ney Smoothing (Kneser and Ney, 1995). We use minimum error rate training (Och, 2003) to optimize the feature weights on the MT02 testset, and test on the MT03/04/05 testsets. For evaluation, case-insensitive NIST BLEU (Papineni et al., 2002) is used to measure translation performance. Three alignment models are chosen for our experiments with default settings: GIZA++ (Och and Ney, 2003), the unsupervised Berkeley aligner (Liang et al., 2006), and the linear modeling alignment Vigne (Liu et al., 2010). We use the three baseline alignments to select MBR alignments and to generate a refined alignment. We use all three baseline alignments, as well as all of the MBR and refined alignments in the WAM-based compaction approach. When extracting rules from WAM, we follow (Tu et al., 2011) to set the pruning threshold t=0.5. 3 In practice, alignment compaction encodes both baseline alignments and the new alignments in Section 3.1 and 3.2. 4 The corpus includes LDC2002E18, LDC2003E07, LDC2003E14, Hansards portion of LDC2004T07, LDC2004T08 a"
C12-2122,P06-1077,1,0.905513,"Missing"
C12-2122,J10-3002,1,0.857717,"n the Xinhua portion of the GIGAWORD corpus using the SRI Language Toolkit (Stolcke, 2002) with modified Kneser-Ney Smoothing (Kneser and Ney, 1995). We use minimum error rate training (Och, 2003) to optimize the feature weights on the MT02 testset, and test on the MT03/04/05 testsets. For evaluation, case-insensitive NIST BLEU (Papineni et al., 2002) is used to measure translation performance. Three alignment models are chosen for our experiments with default settings: GIZA++ (Och and Ney, 2003), the unsupervised Berkeley aligner (Liang et al., 2006), and the linear modeling alignment Vigne (Liu et al., 2010). We use the three baseline alignments to select MBR alignments and to generate a refined alignment. We use all three baseline alignments, as well as all of the MBR and refined alignments in the WAM-based compaction approach. When extracting rules from WAM, we follow (Tu et al., 2011) to set the pruning threshold t=0.5. 3 In practice, alignment compaction encodes both baseline alignments and the new alignments in Section 3.1 and 3.2. 4 The corpus includes LDC2002E18, LDC2003E07, LDC2003E14, Hansards portion of LDC2004T07, LDC2004T08 and LDC2005T06. 1254 Alignments GIZA++ Berkeley Vigne Selecti"
C12-2122,D09-1106,1,0.959517,"t al., 2003) but our technique differs from previous work in that (1) they require exactly two bidirectional alignments while our approach can use an arbitrary number of alignments; (2) we take into account the occurrences of potential links, which turns out to be important. Previous research has demonstrated that compact representations can produce improved results by offering more alternatives, e.g. using forests over 1-best trees (Mi and Huang, 2008; Tu et al., 2010), word lattices over 1-best segmentations (Dyer et al., 2008), and weighted alignment matrices (WAMs) over 1-best alignments (Liu et al., 2009; Tu et al., 2011). Instead of using k-best alignments from the same model, as in (Liu et al., 2009; Tu et al., 2011), here we construct WAMs from multiple alignments generated by different models (including MBRbased and refined models). As the alignment probabilities are generally incomparable between different alignment models, we propose a novel calculation of link probabilities in WAMs. 3 Approach 3.1 Alignment Selection Alignment selection refers to selecting one alignment from multiple alignments using minimum Bayes risk. If the reference alignment a was known, we could measure each alig"
C12-2122,D08-1022,0,0.0291746,"en explored previously (Och and Ney, 2003; Koehn et al., 2003; Ayan et al., 2005; DeNero and Macherey, 2011). We draw inspiration from (Och and Ney, 2003; Koehn et al., 2003) but our technique differs from previous work in that (1) they require exactly two bidirectional alignments while our approach can use an arbitrary number of alignments; (2) we take into account the occurrences of potential links, which turns out to be important. Previous research has demonstrated that compact representations can produce improved results by offering more alternatives, e.g. using forests over 1-best trees (Mi and Huang, 2008; Tu et al., 2010), word lattices over 1-best segmentations (Dyer et al., 2008), and weighted alignment matrices (WAMs) over 1-best alignments (Liu et al., 2009; Tu et al., 2011). Instead of using k-best alignments from the same model, as in (Liu et al., 2009; Tu et al., 2011), here we construct WAMs from multiple alignments generated by different models (including MBRbased and refined models). As the alignment probabilities are generally incomparable between different alignment models, we propose a novel calculation of link probabilities in WAMs. 3 Approach 3.1 Alignment Selection Alignment s"
C12-2122,P06-1065,0,0.0455271,"Missing"
C12-2122,P03-1021,0,0.0759237,"size, they discard any phrase pair that has a fractional count lower than a threshold t. For further details, see (Tu et al., 2011). 4 Experiments 4.1 Setup We carry out our experiments using a reimplementation of the hierarchical phrase-based system (Chiang, 2005) on the NIST Chinese-English translation tasks. Our training data contains 1.5M sentence pairs from LDC dataset.4 We train a 4-gram language model on the Xinhua portion of the GIGAWORD corpus using the SRI Language Toolkit (Stolcke, 2002) with modified Kneser-Ney Smoothing (Kneser and Ney, 1995). We use minimum error rate training (Och, 2003) to optimize the feature weights on the MT02 testset, and test on the MT03/04/05 testsets. For evaluation, case-insensitive NIST BLEU (Papineni et al., 2002) is used to measure translation performance. Three alignment models are chosen for our experiments with default settings: GIZA++ (Och and Ney, 2003), the unsupervised Berkeley aligner (Liang et al., 2006), and the linear modeling alignment Vigne (Liu et al., 2010). We use the three baseline alignments to select MBR alignments and to generate a refined alignment. We use all three baseline alignments, as well as all of the MBR and refined al"
C12-2122,J03-1002,0,0.167522,"at both alignment and phrase levels. System combination, the process which integrates fragment outputs from multiple systems, has produced substantial improvements in many natural language processing tasks, including parsing (Henderson and Brill, 1999; Sagae and Lavie, 2006; Fossum and Knight, 2009), word segmentation (Sun and Wan, 2012) and machine translation (Rosti et al., 2007; He et al., 1 These alignments have equivalent qualities compared to a true gold standard (see in Table 2). 1250 2008; Feng et al., 2009), just to name a few. Alignment combination has also been explored previously (Och and Ney, 2003; Koehn et al., 2003; Ayan et al., 2005; DeNero and Macherey, 2011). We draw inspiration from (Och and Ney, 2003; Koehn et al., 2003) but our technique differs from previous work in that (1) they require exactly two bidirectional alignments while our approach can use an arbitrary number of alignments; (2) we take into account the occurrences of potential links, which turns out to be important. Previous research has demonstrated that compact representations can produce improved results by offering more alternatives, e.g. using forests over 1-best trees (Mi and Huang, 2008; Tu et al., 2010), wor"
C12-2122,J04-4002,0,0.308906,"Missing"
C12-2122,P02-1040,0,0.0852613,"ts 4.1 Setup We carry out our experiments using a reimplementation of the hierarchical phrase-based system (Chiang, 2005) on the NIST Chinese-English translation tasks. Our training data contains 1.5M sentence pairs from LDC dataset.4 We train a 4-gram language model on the Xinhua portion of the GIGAWORD corpus using the SRI Language Toolkit (Stolcke, 2002) with modified Kneser-Ney Smoothing (Kneser and Ney, 1995). We use minimum error rate training (Och, 2003) to optimize the feature weights on the MT02 testset, and test on the MT03/04/05 testsets. For evaluation, case-insensitive NIST BLEU (Papineni et al., 2002) is used to measure translation performance. Three alignment models are chosen for our experiments with default settings: GIZA++ (Och and Ney, 2003), the unsupervised Berkeley aligner (Liang et al., 2006), and the linear modeling alignment Vigne (Liu et al., 2010). We use the three baseline alignments to select MBR alignments and to generate a refined alignment. We use all three baseline alignments, as well as all of the MBR and refined alignments in the WAM-based compaction approach. When extracting rules from WAM, we follow (Tu et al., 2011) to set the pruning threshold t=0.5. 3 In practice,"
C12-2122,N07-1029,0,0.0897098,"s calculation, e.g. edit distance and sentence-level BLEU in SMT (Kumar and Byrne, 2004; Tromble et al., 2008; González-Rubio et al., 2011). In our work, we select an alignment within the MBR framework using a number of loss functions at both alignment and phrase levels. System combination, the process which integrates fragment outputs from multiple systems, has produced substantial improvements in many natural language processing tasks, including parsing (Henderson and Brill, 1999; Sagae and Lavie, 2006; Fossum and Knight, 2009), word segmentation (Sun and Wan, 2012) and machine translation (Rosti et al., 2007; He et al., 1 These alignments have equivalent qualities compared to a true gold standard (see in Table 2). 1250 2008; Feng et al., 2009), just to name a few. Alignment combination has also been explored previously (Och and Ney, 2003; Koehn et al., 2003; Ayan et al., 2005; DeNero and Macherey, 2011). We draw inspiration from (Och and Ney, 2003; Koehn et al., 2003) but our technique differs from previous work in that (1) they require exactly two bidirectional alignments while our approach can use an arbitrary number of alignments; (2) we take into account the occurrences of potential links, wh"
C12-2122,N06-2033,0,0.0331853,"kel and Doksum, 1977). Diverse loss functions have been described by using different evaluation criteria for loss calculation, e.g. edit distance and sentence-level BLEU in SMT (Kumar and Byrne, 2004; Tromble et al., 2008; González-Rubio et al., 2011). In our work, we select an alignment within the MBR framework using a number of loss functions at both alignment and phrase levels. System combination, the process which integrates fragment outputs from multiple systems, has produced substantial improvements in many natural language processing tasks, including parsing (Henderson and Brill, 1999; Sagae and Lavie, 2006; Fossum and Knight, 2009), word segmentation (Sun and Wan, 2012) and machine translation (Rosti et al., 2007; He et al., 1 These alignments have equivalent qualities compared to a true gold standard (see in Table 2). 1250 2008; Feng et al., 2009), just to name a few. Alignment combination has also been explored previously (Och and Ney, 2003; Koehn et al., 2003; Ayan et al., 2005; DeNero and Macherey, 2011). We draw inspiration from (Och and Ney, 2003; Koehn et al., 2003) but our technique differs from previous work in that (1) they require exactly two bidirectional alignments while our approa"
C12-2122,P08-1066,0,0.102941,"Missing"
C12-2122,P12-1025,0,0.0250082,"using different evaluation criteria for loss calculation, e.g. edit distance and sentence-level BLEU in SMT (Kumar and Byrne, 2004; Tromble et al., 2008; González-Rubio et al., 2011). In our work, we select an alignment within the MBR framework using a number of loss functions at both alignment and phrase levels. System combination, the process which integrates fragment outputs from multiple systems, has produced substantial improvements in many natural language processing tasks, including parsing (Henderson and Brill, 1999; Sagae and Lavie, 2006; Fossum and Knight, 2009), word segmentation (Sun and Wan, 2012) and machine translation (Rosti et al., 2007; He et al., 1 These alignments have equivalent qualities compared to a true gold standard (see in Table 2). 1250 2008; Feng et al., 2009), just to name a few. Alignment combination has also been explored previously (Och and Ney, 2003; Koehn et al., 2003; Ayan et al., 2005; DeNero and Macherey, 2011). We draw inspiration from (Och and Ney, 2003; Koehn et al., 2003) but our technique differs from previous work in that (1) they require exactly two bidirectional alignments while our approach can use an arbitrary number of alignments; (2) we take into ac"
C12-2122,H05-1010,0,0.073942,"Missing"
C12-2122,D08-1065,0,0.016802,"refinement e.g. offers high quality alignment choices, that can be exploited by alignment compaction. 2 Related Work Our research builds on previous work in the field of minimum Bayes risk (MBR) decision, system combination and model compaction. MBR decision aims to find the candidate hypothesis that has the least expected loss under a probability model when the true reference is not known (Brickel and Doksum, 1977). Diverse loss functions have been described by using different evaluation criteria for loss calculation, e.g. edit distance and sentence-level BLEU in SMT (Kumar and Byrne, 2004; Tromble et al., 2008; González-Rubio et al., 2011). In our work, we select an alignment within the MBR framework using a number of loss functions at both alignment and phrase levels. System combination, the process which integrates fragment outputs from multiple systems, has produced substantial improvements in many natural language processing tasks, including parsing (Henderson and Brill, 1999; Sagae and Lavie, 2006; Fossum and Knight, 2009), word segmentation (Sun and Wan, 2012) and machine translation (Rosti et al., 2007; He et al., 1 These alignments have equivalent qualities compared to a true gold standard"
C12-2122,C10-1123,1,0.860759,"ly (Och and Ney, 2003; Koehn et al., 2003; Ayan et al., 2005; DeNero and Macherey, 2011). We draw inspiration from (Och and Ney, 2003; Koehn et al., 2003) but our technique differs from previous work in that (1) they require exactly two bidirectional alignments while our approach can use an arbitrary number of alignments; (2) we take into account the occurrences of potential links, which turns out to be important. Previous research has demonstrated that compact representations can produce improved results by offering more alternatives, e.g. using forests over 1-best trees (Mi and Huang, 2008; Tu et al., 2010), word lattices over 1-best segmentations (Dyer et al., 2008), and weighted alignment matrices (WAMs) over 1-best alignments (Liu et al., 2009; Tu et al., 2011). Instead of using k-best alignments from the same model, as in (Liu et al., 2009; Tu et al., 2011), here we construct WAMs from multiple alignments generated by different models (including MBRbased and refined models). As the alignment probabilities are generally incomparable between different alignment models, we propose a novel calculation of link probabilities in WAMs. 3 Approach 3.1 Alignment Selection Alignment selection refers to"
C12-2122,I11-1145,1,0.883984,"ur technique differs from previous work in that (1) they require exactly two bidirectional alignments while our approach can use an arbitrary number of alignments; (2) we take into account the occurrences of potential links, which turns out to be important. Previous research has demonstrated that compact representations can produce improved results by offering more alternatives, e.g. using forests over 1-best trees (Mi and Huang, 2008; Tu et al., 2010), word lattices over 1-best segmentations (Dyer et al., 2008), and weighted alignment matrices (WAMs) over 1-best alignments (Liu et al., 2009; Tu et al., 2011). Instead of using k-best alignments from the same model, as in (Liu et al., 2009; Tu et al., 2011), here we construct WAMs from multiple alignments generated by different models (including MBRbased and refined models). As the alignment probabilities are generally incomparable between different alignment models, we propose a novel calculation of link probabilities in WAMs. 3 Approach 3.1 Alignment Selection Alignment selection refers to selecting one alignment from multiple alignments using minimum Bayes risk. If the reference alignment a was known, we could measure each alignment ai using the"
C12-2122,C96-2141,0,0.530719,"Missing"
C12-2122,C10-2154,0,0.0147392,"ave the same value. 2 3.1.2 Loss Functions The loss function L (ai , a j ) is used to measure the quality of alignments. Here we introduce a set of metrics for the evaluation of alignments at both alignment and phrase levels. AER Alignment error rate (Och and Ney, 2003) has been used as the official evaluation criterion in most alignment shared tasks (Liu et al., 2009). AER scores are given by: 2 AER(S, P, A) = 1 − (|A ∩ S |+ |A ∩ P|)/(|A |+ |S|) (2) Alignment probabilities can be set empirically based on (expected overall) performance (Fossum and Knight, 2009), or uniformly without any bias (Xiao et al., 2010; Duan et al., 2010). We tried a few other settings and found them to be less effective. 1251 the development of China ’s economy (a) zhongguo de jingji fazhan zhongguo de jingji fazhan zhongguo de jingji fazhan the development of China ’s economy the development of China ’s economy (b) (c) Figure 1: (a) Alignment of a sentence pair generated by GIZA++ (a1 ), (b) alignment of the same sentence by Berkeley aligner (a2 ), (c) another alignment by Vigne (a3 ). where S and P are sets of sure and possible links in a hand-aligned reference alignment respectively, and A is a candidate alignment. Note"
C12-2122,P06-1066,1,0.854268,"Missing"
C12-3040,P01-1004,0,0.830801,"Missing"
C12-3040,C00-1006,0,0.507065,"Missing"
C12-3040,C10-1054,0,0.0422361,"Missing"
C12-3040,P07-2045,0,0.0140445,"Missing"
C12-3040,N03-1017,0,0.00994328,"Missing"
C12-3040,W02-1018,0,0.0922608,"Missing"
C12-3040,H05-1086,0,0.0278739,"Missing"
C12-3040,P03-1021,0,0.0414804,"Missing"
C12-3040,P02-1038,0,0.445065,"Missing"
C14-1179,P06-1067,0,0.0207309,"Missing"
C14-1179,P12-1050,0,0.195704,"Missing"
C14-1179,N13-1003,0,0.320689,"tion (Koehn et al., 2003; Och and Ney, 2004). While local reordering can be directly memorized in phrases, modeling reordering at a phrase level still remains a major challenge: it can be cast as a travelling salesman problem and proves to be NP-complete (Knight, 1999; Zaslavskiy et al., 2009). The past decade has witnessed the rapid development of phrase reordering models (e.g., (Och et al., 2004; Tillman, 2004; Zens et al., 2004; Xiong et al., 2006; Al-Onaizan and Papineni, 2006; Koehn et al., 2007; Galley and Manning, 2008; Feng et al., 2010; Green et al., 2010; Bisazza and Federico, 2012; Cherry, 2013), just to name a few). Among them, lexicalized reordering models (Tillman, 2004; Koehn et al., 2007; Galley and Manning, 2008) have been widely used in practical phrase-based systems. Unlike the distance-based reordering model (Koehn et al., 2003) that only penalizes phrase displacements in terms of the degree of nonmonotonicity, lexicalized reordering models introduce reordering probabilities conditioned on the words of each phrase pair. They often distinguish between three orientations with respect to the previous phrase pair: monotone, swap, and discontinuous. As lexicalized reordering mode"
C14-1179,P11-1105,0,0.0610031,"performance. We leave this for future work. 1904 by june 1 within the agencies group all together as detention center take care of old late 2011 and complete by end 1998 june 18, 2001 and for other or other economic range of services to economy is required to but is willing to said his visit is to is making use of Figure 4: Phrase clusters as calculated by the Euclidean distance in the vector space. English phrases that have similar reordering probability distributions rather than similar semantic similarity fall into one cluster. Along another line, n-gram-based models (Mari`no et al., 2006; Durrani et al., 2011; Durrani et al., 2013) treat translation as Markov chains over minimal translation units (Mari`no et al., 2006; Durrani et al., 2013) or operations (Durrani et al., 2011) directly. Although naturally leveraging both the source and target side contexts, these approaches still face the data sparsity problem. Our work is closely related to Li et al. (2013). The major difference is that Li et al. (2013) need to compute vector space representation for variable-sized blocks ranging from words to sentences on the fly both in training and decoding. In contrast, we only need to compute vectors for phr"
C14-1179,P13-2071,0,0.211141,"this for future work. 1904 by june 1 within the agencies group all together as detention center take care of old late 2011 and complete by end 1998 june 18, 2001 and for other or other economic range of services to economy is required to but is willing to said his visit is to is making use of Figure 4: Phrase clusters as calculated by the Euclidean distance in the vector space. English phrases that have similar reordering probability distributions rather than similar semantic similarity fall into one cluster. Along another line, n-gram-based models (Mari`no et al., 2006; Durrani et al., 2011; Durrani et al., 2013) treat translation as Markov chains over minimal translation units (Mari`no et al., 2006; Durrani et al., 2013) or operations (Durrani et al., 2011) directly. Although naturally leveraging both the source and target side contexts, these approaches still face the data sparsity problem. Our work is closely related to Li et al. (2013). The major difference is that Li et al. (2013) need to compute vector space representation for variable-sized blocks ranging from words to sentences on the fly both in training and decoding. In contrast, we only need to compute vectors for phrases with up to 7 words"
C14-1179,C10-2033,1,0.935129,"Missing"
C14-1179,D08-1089,0,0.228527,"zed reordering models. 1 Introduction Reordering plays a crucial role in phrase-based translation (Koehn et al., 2003; Och and Ney, 2004). While local reordering can be directly memorized in phrases, modeling reordering at a phrase level still remains a major challenge: it can be cast as a travelling salesman problem and proves to be NP-complete (Knight, 1999; Zaslavskiy et al., 2009). The past decade has witnessed the rapid development of phrase reordering models (e.g., (Och et al., 2004; Tillman, 2004; Zens et al., 2004; Xiong et al., 2006; Al-Onaizan and Papineni, 2006; Koehn et al., 2007; Galley and Manning, 2008; Feng et al., 2010; Green et al., 2010; Bisazza and Federico, 2012; Cherry, 2013), just to name a few). Among them, lexicalized reordering models (Tillman, 2004; Koehn et al., 2007; Galley and Manning, 2008) have been widely used in practical phrase-based systems. Unlike the distance-based reordering model (Koehn et al., 2003) that only penalizes phrase displacements in terms of the degree of nonmonotonicity, lexicalized reordering models introduce reordering probabilities conditioned on the words of each phrase pair. They often distinguish between three orientations with respect to the previ"
C14-1179,N10-1129,0,0.238196,"model with those trained using word2vec (Mikolov et al., 2013). The recursive autoencoders and the classifier are retrained. The performance of the neural reordering model trained in this way drops significantly, which confirms our analysis. 5 Related Work Reordering as classification is a common way to alleviate the data sparsity problem. Xiong et al. (2006) use a maximum entropy model to predict whether to merge two blocks in a straight or an inverted order in their ITG decoder. Nguyen et al. (2009) build a similar model for hierarchical phrase reordering models (Galley and Manning, 2008). Green et al. (2010) and Yahyaei and Monz (2010) predict finergrained distance bins instead. Another direction is to learn sparse reordering features and create more flexible distributions (Cherry, 2013). Although these models are effective, feature engineering is a major challenge. In contrast, our neural reordering model is capable of learning features automatically. 6 The reason why the BLEU scores oscillate slightly on the training set is that classification accuracy is not directly correlated with BLEU scores. Optimizing the neural reordering model directly with respect to BLEU score may further improve the"
C14-1179,W11-2123,0,0.00870943,"s directly integrated in decoding, making the decoder to only explore in a much smaller search space. 4 Therefore, we use Moses to generate search graphs and then use hypergraph reranking (Huang and Chiang, 2007; Huang, 2008) to find most probable derivations using the neural reordering model. 4 Experiments 4.1 Data Preparation We evaluate our reordering model on Chinese-English translation. The training corpus consists of 1.23M sentence pairs with 32.1M Chinese words and 35.4M English words. A 4-gram language model was trained on the Xinhua portion of the English GIGAWORD corpus using KenLM (Heafield, 2011), which contains 398.6M words. We used the NIST 2006 MT Chinese-English dataset as the development set, and NIST 2002-2005, 2008 MT Chinese-English datasets as the test sets. Case-insensitive BLEU is used 2 In practice, as suggested by Socher et al. (2011b), we feed the four average vectors of the vectors present in each recursive autoencoders to the softmax layer. Taking “resident population” as an example, there are three vectors in the binary ∑ tree used by the corresponding recursive autoencoder, denoted as x ˆ1 , x ˆ2 and x ˆ3 . The average vector is computed as x ¯ = 13 3i=1 x ˆi . 3 As"
C14-1179,P07-1019,0,0.013884,"error backpropagation algorithm (Rumelhart et al., 1986) to compute the derivatives. 3.3 Decoding As the vector space representation of a phrase is calculated based on all the words in the phrase, using the neural reordering model complicates the conditions for risk-free hypothesis recombination (Koehn et al., 2003). Therefore, many hypotheses are not likely to be recombined if the neural reordering model is directly integrated in decoding, making the decoder to only explore in a much smaller search space. 4 Therefore, we use Moses to generate search graphs and then use hypergraph reranking (Huang and Chiang, 2007; Huang, 2008) to find most probable derivations using the neural reordering model. 4 Experiments 4.1 Data Preparation We evaluate our reordering model on Chinese-English translation. The training corpus consists of 1.23M sentence pairs with 32.1M Chinese words and 35.4M English words. A 4-gram language model was trained on the Xinhua portion of the English GIGAWORD corpus using KenLM (Heafield, 2011), which contains 398.6M words. We used the NIST 2006 MT Chinese-English dataset as the development set, and NIST 2002-2005, 2008 MT Chinese-English datasets as the test sets. Case-insensitive BLEU"
C14-1179,P08-1067,0,0.00956818,"lgorithm (Rumelhart et al., 1986) to compute the derivatives. 3.3 Decoding As the vector space representation of a phrase is calculated based on all the words in the phrase, using the neural reordering model complicates the conditions for risk-free hypothesis recombination (Koehn et al., 2003). Therefore, many hypotheses are not likely to be recombined if the neural reordering model is directly integrated in decoding, making the decoder to only explore in a much smaller search space. 4 Therefore, we use Moses to generate search graphs and then use hypergraph reranking (Huang and Chiang, 2007; Huang, 2008) to find most probable derivations using the neural reordering model. 4 Experiments 4.1 Data Preparation We evaluate our reordering model on Chinese-English translation. The training corpus consists of 1.23M sentence pairs with 32.1M Chinese words and 35.4M English words. A 4-gram language model was trained on the Xinhua portion of the English GIGAWORD corpus using KenLM (Heafield, 2011), which contains 398.6M words. We used the NIST 2006 MT Chinese-English dataset as the development set, and NIST 2002-2005, 2008 MT Chinese-English datasets as the test sets. Case-insensitive BLEU is used 2 In"
C14-1179,2010.eamt-1.28,0,0.0153363,"reordering probabilities for long phrase pairs that are usually observed only once in the training data. On the contrary, short phrase pairs that occur in the training data for many times tend to be ambiguous. For example, as shown in Figure 1, a Chinese-English phrase pair ⟨“yingyun”, “business”⟩ is observed to have different orientations in different contexts. It is unreasonable to use fixed reordering probability distributions in decoding as the surrounding contexts keep changing. Previous study shows that considering more contexts into reordering modeling improves translation performance (Khalilov and Simaan, 2010). Therefore, we need a more powerful mechanism to include more contexts, resolve the reordering ambiguity, and reduce the data sparsity. 3 A Neural Reordering Model 3.1 The Model Intuitively, conditioning reordering probabilities on the words of both the current and previous phrase pairs will significantly reduce both reordering ambiguity and context insensitivity. The new reordering model is given by P (o|f , e, a) ≈ n ∏ P (oi |f˜ai , e˜i , f˜ai−1 , e˜i−1 , ai−1 , ai ) (4) i=1 where ⟨f˜ai−1 , e˜i−1 ⟩ is the previous phrase pair. Including the previous phrase pairs improves the context sensiti"
C14-1179,J99-4005,0,0.0691019,"Missing"
C14-1179,N03-1017,0,0.318751,"tions reordering probabilities on the words of both the current and previous phrase pairs. Including the words of previous phrase pairs significantly improves context sensitivity and reduces reordering ambiguity. To alleviate the data sparsity problem, we build one classifier for all phrase pairs, which are represented as continuous space vectors. Experiments on the NIST Chinese-English datasets show that our neural reordering model achieves significant improvements over state-of-the-art lexicalized reordering models. 1 Introduction Reordering plays a crucial role in phrase-based translation (Koehn et al., 2003; Och and Ney, 2004). While local reordering can be directly memorized in phrases, modeling reordering at a phrase level still remains a major challenge: it can be cast as a travelling salesman problem and proves to be NP-complete (Knight, 1999; Zaslavskiy et al., 2009). The past decade has witnessed the rapid development of phrase reordering models (e.g., (Och et al., 2004; Tillman, 2004; Zens et al., 2004; Xiong et al., 2006; Al-Onaizan and Papineni, 2006; Koehn et al., 2007; Galley and Manning, 2008; Feng et al., 2010; Green et al., 2010; Bisazza and Federico, 2012; Cherry, 2013), just to n"
C14-1179,P07-2045,0,0.119238,"-of-the-art lexicalized reordering models. 1 Introduction Reordering plays a crucial role in phrase-based translation (Koehn et al., 2003; Och and Ney, 2004). While local reordering can be directly memorized in phrases, modeling reordering at a phrase level still remains a major challenge: it can be cast as a travelling salesman problem and proves to be NP-complete (Knight, 1999; Zaslavskiy et al., 2009). The past decade has witnessed the rapid development of phrase reordering models (e.g., (Och et al., 2004; Tillman, 2004; Zens et al., 2004; Xiong et al., 2006; Al-Onaizan and Papineni, 2006; Koehn et al., 2007; Galley and Manning, 2008; Feng et al., 2010; Green et al., 2010; Bisazza and Federico, 2012; Cherry, 2013), just to name a few). Among them, lexicalized reordering models (Tillman, 2004; Koehn et al., 2007; Galley and Manning, 2008) have been widely used in practical phrase-based systems. Unlike the distance-based reordering model (Koehn et al., 2003) that only penalizes phrase displacements in terms of the degree of nonmonotonicity, lexicalized reordering models introduce reordering probabilities conditioned on the words of each phrase pair. They often distinguish between three orientations"
C14-1179,D13-1054,1,0.864535,"propose a neural reordering classifier that takes the current and previous phrase pairs as input. The neural network consists of four recursive autoencoders and a softmax layer. The input of the classifier are the previous phrase pair and the current phrase pair. Four recursive autoencoders are used to transform the four phrases (i.e., f˜ai , e˜i , f˜ai−1 , e˜i−1 ) into vectors. Then, these vectors are fed to the softmax layer to predict reordering orientations. Note that the recursive autoencoders for the same language share with the same parameters. Our neural network is similar to that of Li et al. (2013). The major difference is that Li et al. (2013) need to compute vector space representation for variable-sized blocks ranging from words to sentences on the fly both in training and decoding. In contrast, we only need to compute vectors for phrases with up to 7 words in the training phase, which makes our approach simpler and more scalable to large data. 1900 Formally, given the previous phrase pair ⟨f˜ai−1 , e˜i−1 ⟩, the current phrase pair ⟨f˜i , e˜i ⟩ and the orientation oi , the reordering probability is computed as P (oi |f˜ai , e˜i , f˜ai−1 , e˜i−1 , ai−1 , ai ) = g(W o c(f˜ai , e˜i , f˜"
C14-1179,J06-4004,0,0.0801973,"Missing"
C14-1179,N13-1090,0,0.0101041,"alized models in predicting long-distance reordering. 1903 # examples 100,000 200,000 300,000 400,000 500,000 3,000,000 BLEU 25 24 23 22 neural lexicalized 2 4 6 Distortion Limit 8 Figure 3: BLEU with various distortion limits. Vectors ours word2vec MT06 31.03 30.44 MT02 33.03 32.28 Accuracy 83.55 84.40 84.55 84.95 85.25 85.55 BLEU 30.92 31.03 31.01 30.93 31.27 31.03 Table 3: Effect of training corpus size. MT03 32.48 32.00 MT04 32.52 32.07 MT05 31.11 30.24 MT08 25.20 24.54 Table 4: Comparison of neural reordering models trained based on word vectors produced by our model (ours) and word2vec (Mikolov et al., 2013). 4.5 The Effect of Training Corpus Size Table 3 shows the classification accuracy and translation performance with various number of randomly sampled reordering examples for training the neural classifier. The classification accuracy and translation performance generally rise as the number of reordering example increases.6 Surprisingly, both the classification accuracy and translation performance of using 500,000 reordering examples are close to using 3,000,000 reordering examples, suggesting that a relatively small amount of reordering examples are enough for training a robust classifier. 4."
C14-1179,2009.mtsummit-papers.10,0,0.0894341,"Missing"
C14-1179,J04-4002,0,0.0403147,"Missing"
C14-1179,P03-1021,0,0.0215854,"nd decoding. In contrast, we only need to compute vectors for phrases with up to 7 words in the training phase, which makes our approach simpler and more scalable to large data. 1900 Formally, given the previous phrase pair ⟨f˜ai−1 , e˜i−1 ⟩, the current phrase pair ⟨f˜i , e˜i ⟩ and the orientation oi , the reordering probability is computed as P (oi |f˜ai , e˜i , f˜ai−1 , e˜i−1 , ai−1 , ai ) = g(W o c(f˜ai , e˜i , f˜ai−1 , e˜i−1 ) + bo ), (7) where W o is a weight matrix, bo is a bias vector, c(f˜ai , e˜i , f˜ai−1 , e˜i−1 ) is the concatenation of the vectors of the four phrases. 2 Following Och (2003), we use a linear model in our decoder with conventional features (e.g., translation probabilities and n-gram language model). The neural reordering model is incorporated into the discriminative framework as an additional feature. 3.2 Training Training the neural reordering model involves minimizing the following two kinds of errors: • Reconstruction error: It measures how well the computed vector space representations represent the input vectors. It is defined as the average reconstruction error of all the parent nodes in the trees formed during computing the vector space representation for a"
C14-1179,D11-1014,0,0.731154,"dering as classification. Instead of maintaining a reordering probability distribution for each phrase pair, we build a reordering classifier for all phrase pairs (Xiong et al., 2006; Li et al., 2013). This significantly reduces data sparsity by considering all occurrences of extracted phrase pairs as training examples. We find that 500, 000 reordering examples suffice to train a robust classifier (Section 4.5). 2. Continuous space representation. Instead of using a symbolic representation of phrases, we use a continuous space representation that treats a phrase as a dense real-valued vector (Socher et al., 2011b; Li et al., 2013). Consider two phrases “in London” and “in Centara Grand”. It is usually easy to predict the orientations of “in London” because it might be observed in the training data for many times. This is not the case for “in Centara Grand” as it might occur only once. However, if the two phrases happen to have very similar continuous space representations, “in Centara Grand” is likely to have a similar reordering probability distribution with “in London”. To generate vector space representation for phrases, we follow Socher et al. (2011a) to use recursive autoencoders. Given two word"
C14-1179,N04-4026,0,0.195174,"ts show that our neural reordering model achieves significant improvements over state-of-the-art lexicalized reordering models. 1 Introduction Reordering plays a crucial role in phrase-based translation (Koehn et al., 2003; Och and Ney, 2004). While local reordering can be directly memorized in phrases, modeling reordering at a phrase level still remains a major challenge: it can be cast as a travelling salesman problem and proves to be NP-complete (Knight, 1999; Zaslavskiy et al., 2009). The past decade has witnessed the rapid development of phrase reordering models (e.g., (Och et al., 2004; Tillman, 2004; Zens et al., 2004; Xiong et al., 2006; Al-Onaizan and Papineni, 2006; Koehn et al., 2007; Galley and Manning, 2008; Feng et al., 2010; Green et al., 2010; Bisazza and Federico, 2012; Cherry, 2013), just to name a few). Among them, lexicalized reordering models (Tillman, 2004; Koehn et al., 2007; Galley and Manning, 2008) have been widely used in practical phrase-based systems. Unlike the distance-based reordering model (Koehn et al., 2003) that only penalizes phrase displacements in terms of the degree of nonmonotonicity, lexicalized reordering models introduce reordering probabilities condi"
C14-1179,P06-1066,0,0.0967446,"orientations but are totally unrelated in terms of meaning. As a result, the vector representations of words trained using unlabeled data hardly helps in training the neural reordering model. Table 4 shows the results when we replace the word vectors of our model with those trained using word2vec (Mikolov et al., 2013). The recursive autoencoders and the classifier are retrained. The performance of the neural reordering model trained in this way drops significantly, which confirms our analysis. 5 Related Work Reordering as classification is a common way to alleviate the data sparsity problem. Xiong et al. (2006) use a maximum entropy model to predict whether to merge two blocks in a straight or an inverted order in their ITG decoder. Nguyen et al. (2009) build a similar model for hierarchical phrase reordering models (Galley and Manning, 2008). Green et al. (2010) and Yahyaei and Monz (2010) predict finergrained distance bins instead. Another direction is to learn sparse reordering features and create more flexible distributions (Cherry, 2013). Although these models are effective, feature engineering is a major challenge. In contrast, our neural reordering model is capable of learning features automa"
C14-1179,2010.iwslt-papers.19,0,0.177035,"d using word2vec (Mikolov et al., 2013). The recursive autoencoders and the classifier are retrained. The performance of the neural reordering model trained in this way drops significantly, which confirms our analysis. 5 Related Work Reordering as classification is a common way to alleviate the data sparsity problem. Xiong et al. (2006) use a maximum entropy model to predict whether to merge two blocks in a straight or an inverted order in their ITG decoder. Nguyen et al. (2009) build a similar model for hierarchical phrase reordering models (Galley and Manning, 2008). Green et al. (2010) and Yahyaei and Monz (2010) predict finergrained distance bins instead. Another direction is to learn sparse reordering features and create more flexible distributions (Cherry, 2013). Although these models are effective, feature engineering is a major challenge. In contrast, our neural reordering model is capable of learning features automatically. 6 The reason why the BLEU scores oscillate slightly on the training set is that classification accuracy is not directly correlated with BLEU scores. Optimizing the neural reordering model directly with respect to BLEU score may further improve the performance. We leave this f"
C14-1179,P09-1038,0,0.0516363,"Missing"
C14-1179,C04-1030,0,0.228504,"Missing"
C14-1179,N04-1021,0,\N,Missing
C14-1192,C00-1006,0,0.225443,"corpora. Experiments show that our translation retrieval system significantly outperforms a state-of-the-art parallel corpus mining system. 2 Related Work Our work is inspired by three research topics: retrieving translation candidates from parallel corpus, using lattice to compactly represent exponentially many alternatives, and using lattice as query in information retrieval. 1. Translation Retrieval using Parallel Corpus. The idea of retrieving translation candidates from existing texts originated in example-based and memory-based translation (Sato and Nagao, 1990; Nirenburg et al., 1993; Baldwin and Tanaka, 2000; Baldwin, 2001). As these early efforts use a parallel corpus (e.g., translation records that are pairs of source-language and target-language strings), they focus on calculating the similarity between two source-language strings. In contrast, we evaluate the translational equivalence of a given source string and a target string in a large monolingual corpus. 2. Lattice in Machine Translation. Lattices have been widely used in machine translation: considering Chinese word segmentation alternatives (Xu et al., 2005), speech recognition candidates (Matsoukas et al., 2007), SCFG (Dyer et al., 20"
C14-1192,P01-1004,0,0.203375,"that our translation retrieval system significantly outperforms a state-of-the-art parallel corpus mining system. 2 Related Work Our work is inspired by three research topics: retrieving translation candidates from parallel corpus, using lattice to compactly represent exponentially many alternatives, and using lattice as query in information retrieval. 1. Translation Retrieval using Parallel Corpus. The idea of retrieving translation candidates from existing texts originated in example-based and memory-based translation (Sato and Nagao, 1990; Nirenburg et al., 1993; Baldwin and Tanaka, 2000; Baldwin, 2001). As these early efforts use a parallel corpus (e.g., translation records that are pairs of source-language and target-language strings), they focus on calculating the similarity between two source-language strings. In contrast, we evaluate the translational equivalence of a given source string and a target string in a large monolingual corpus. 2. Lattice in Machine Translation. Lattices have been widely used in machine translation: considering Chinese word segmentation alternatives (Xu et al., 2005), speech recognition candidates (Matsoukas et al., 2007), SCFG (Dyer et al., 2008) and so on in"
C14-1192,P08-1115,0,0.025281,"nd Tanaka, 2000; Baldwin, 2001). As these early efforts use a parallel corpus (e.g., translation records that are pairs of source-language and target-language strings), they focus on calculating the similarity between two source-language strings. In contrast, we evaluate the translational equivalence of a given source string and a target string in a large monolingual corpus. 2. Lattice in Machine Translation. Lattices have been widely used in machine translation: considering Chinese word segmentation alternatives (Xu et al., 2005), speech recognition candidates (Matsoukas et al., 2007), SCFG (Dyer et al., 2008) and so on in the decoding process, minimum bayes risk decoding (Tromble et al., 2008), minimum error rate training (Macherey et al., 2008), system combination (Feng et al., 2009), just to name a few. In this work, we are interested in how to use a lattice that encodes exponentially many translation candidates as a single query to retrieve similar target sentences via an information retrieval system. 3. Query Lattice in Information Retrieval. The use of lattices in information retrieval dates back to Moore (1958). Most current lattice-based IR systems often treat lattices as conceptional hiera"
C14-1192,D09-1115,1,0.845273,"Missing"
C14-1192,C12-3040,1,0.946269,"y used in example-based and memory-based translation systems (Sato and Nagao, 1990; Nirenburg et al., 1993; Baldwin and Tanaka, 2000; Baldwin, 2001). Often, the document set is a list of translation records that are pairs of source-language and target-language strings. Given an input source string, the retrieval system returns a translation record of maximum similarity to the input on the source side. Although these methods prove to be effective in example-based and memory-based translation systems, they heavily rely on parallel corpora that are limited both in size and domain. More recently, Liu et al. (2012) have proposed a new translation retrieval architecture that depends only on monolingual corpora. Given an input source string, their system retrieves translation candidates from a set of target-language sentences. This can be done by combining machine translation (MT) and information retrieval (IR): machine translation is used to transform the input source string to a coarse translation, which serves as a query to retrieve the most probable translation in the monolingual corpus. Therefore, it is possible for translation retrieval to have access to a huge volume of monolingual corpora that are"
C14-1192,D08-1076,0,0.0329923,"age and target-language strings), they focus on calculating the similarity between two source-language strings. In contrast, we evaluate the translational equivalence of a given source string and a target string in a large monolingual corpus. 2. Lattice in Machine Translation. Lattices have been widely used in machine translation: considering Chinese word segmentation alternatives (Xu et al., 2005), speech recognition candidates (Matsoukas et al., 2007), SCFG (Dyer et al., 2008) and so on in the decoding process, minimum bayes risk decoding (Tromble et al., 2008), minimum error rate training (Macherey et al., 2008), system combination (Feng et al., 2009), just to name a few. In this work, we are interested in how to use a lattice that encodes exponentially many translation candidates as a single query to retrieve similar target sentences via an information retrieval system. 3. Query Lattice in Information Retrieval. The use of lattices in information retrieval dates back to Moore (1958). Most current lattice-based IR systems often treat lattices as conceptional hierarchies or thesauri in formal concept analysis (Priss, 2000; Cheung and Vogel, 2005). In spoken document retrieval, however, lattices are us"
C14-1192,J05-4003,0,0.0321661,"w that using lattices clearly outperforms using 1-best and n-best lists. The margins are larger on the out-of-domain test set. 4.2 Evaluation on Parallel Corpus Mining In this section, we evaluate translation retrieval on the parallel corpus mining task: extracting a parallel corpus from a comparable corpus. 4.2.1 Experimental Setup The comparable corpus for extracting parallel sentences contains news articles published by Xinhua News Agency from 1995 to 2010. Table 4 shows the detailed statistics. There are 1.2M Chinese and 1.7M English articles. We re-implemented the method as described in (Munteanu and Marcu, 2005) as the baseline system. 2038 language Chinese English articles 1.2M 1.7M sentences 18.5M 17.8M words 441.2M 440.2M vocabulary 2.1M 3.4M Table 4: The Xinhua News Comparable Corpus from 1995 to 2010 Munteanu and Marcu (2005) English words Chinese words BLEU 5.00M 4.12M 22.84 10.00M 8.20M 25.10 15.00M 12.26M 25.41 20.00M 16.30M 25.56 this work English words Chinese Words 5.00M 3.98M 10.00M 8.17M 15.00M 12.49M 20.00M 16.90M BLEU 25.44 26.62 26.49 26.87 Table 5: Comparison of BLEU scores using parallel corpora extracted by the baseline and our system. Given a comparable corpus (see Table 4), both"
C14-1192,1993.tmi-1.4,0,0.801428,"entences from comparable corpora. Experiments show that our translation retrieval system significantly outperforms a state-of-the-art parallel corpus mining system. 2 Related Work Our work is inspired by three research topics: retrieving translation candidates from parallel corpus, using lattice to compactly represent exponentially many alternatives, and using lattice as query in information retrieval. 1. Translation Retrieval using Parallel Corpus. The idea of retrieving translation candidates from existing texts originated in example-based and memory-based translation (Sato and Nagao, 1990; Nirenburg et al., 1993; Baldwin and Tanaka, 2000; Baldwin, 2001). As these early efforts use a parallel corpus (e.g., translation records that are pairs of source-language and target-language strings), they focus on calculating the similarity between two source-language strings. In contrast, we evaluate the translational equivalence of a given source string and a target string in a large monolingual corpus. 2. Lattice in Machine Translation. Lattices have been widely used in machine translation: considering Chinese word segmentation alternatives (Xu et al., 2005), speech recognition candidates (Matsoukas et al., 20"
C14-1192,P02-1038,0,0.108194,"entence e is the translation of a sourcelanguage sentence f . As suggested by Liu et al. (2012), it can be decomposed into two sub-models by 2032 introducing a coarse translation q as a hidden variable: X P (q, e|f ) P (e|f ) = (1) q∈Q(f ) X = P (q|f ) × P (e|q, f ) (2) q∈Q(f ) where P (q|f ) is a translation sub-model, P (e|q, f ) is a retrieval sub-model, and Q(f ) is the set of all possible translations of the sentence f . Note that q actually serves as a query to the retrieval sub-model. To take advantage of various translation and retrieval information sources, we use a log-linear model (Och and Ney, 2002) to define the conditional probability of a query q and a target sentence e conditioned on a source sentence f parameterized by a real-valued vector θ: exp(θ · h(q, e, f )) P 0 0 q0 ∈Q(f ) e0 ∈E exp(θ · h(q , e , f )) P (q, e|f ; θ) = P where h(·) is a vector of feature functions and θ is the corresponding feature weight vector. Accordingly, the decision rule for the latent variable model is given by ( ) X ˆ = arg max e exp(θ · h(q, e, f )) e∈E (3) (4) q∈Q(f ) As there are exponentially many queries, it is efficient to approximate the summation over all possible queries by using maximization i"
C14-1192,P03-1021,0,0.527429,"“with”}. This set serves as a coarse single query and the retrieval system returns a list of target sentences that contain these words: 2035 Training Dev in-domain out-of-domain Test in-domain out-of-domain query document query document query document query document Chinese 1.21M 5K 5K 5K 5K English 1.21M 2.23M 2.23M 2.23M 2.23M Table 1: The datasets for the retrieval evaluation. The training set is used to train the phrase-based translation model and language model for Moses (Koehn et al., 2007). The development set is used to optimize feature weights using the minimum-error-rate algorithm (Och, 2003). A development set consists of a query set and a document set. The test set is used to evaluate the retrieval accuracy. To examine the effect of domains on retrieval performance, we used two development and test sets: indomain and out-domain. President Bush gave a talk at a meeting Bush held a meeting with Sharon Sharon and Bush attended a meeting held at London Note that as a retrieval system usually ignores the structural dependencies in text, the retrieved sentences (scored by retrieval features) are relevant but not necessarily translations of the input. Therefore, we can match each retri"
C14-1192,P02-1040,0,0.0905267,"variants of translation retrieval: 1-best list, n-best list, and lattice. For query lattice, we further distinguish between search graph and translation option graph. They are generated by Moses with the default setting. We use both translation and retrieval features in the experiments. The translation features include phrase translation probabilities, phrase penalty, distance-based and lexicalized reordering models, language models, and word penalty. Besides the conventional IR features such as term frequency and inverse document frequency, we use five additional featured derived from BLEU (Papineni et al., 2002): the n-gram matching precisions between query and retrieved target sentence (n = 1, 2, 3, 4) and brevity penalty. These features impose structural constraints on retrieval and ensure translation closeness of retrieved target sentences. The minimum-error-rate algorithm supports a variety of loss functions. The loss function we used in our experiment is 1−P@n. Note that using translation option graph as query lattice does not include language models and distance-based lexicalized reordering models as features. 4.1.2 Evaluation Results Table 2 shows the results on the in-domain test set. The “#"
C14-1192,N04-1017,0,0.0407263,"ponentially many translation candidates as a single query to retrieve similar target sentences via an information retrieval system. 3. Query Lattice in Information Retrieval. The use of lattices in information retrieval dates back to Moore (1958). Most current lattice-based IR systems often treat lattices as conceptional hierarchies or thesauri in formal concept analysis (Priss, 2000; Cheung and Vogel, 2005). In spoken document retrieval, however, lattices are used as a compact representation of multiple speech recognition transcripts to estimate the expected counts of words in each document (Saraclar and Sproat, 2004; Zhou et al., 2006; Chia et al., 2010). Our work is significantly different from previous work that uses the bag-of-words model because translation retrieval must take structure and dependencies in text into account to ensure translational equivalence. 3 3.1 Query Lattice for Translation Retrieval Translation Retrieval Let f be a source-language string, E be a set of target-language strings, the problem is how to find the ˆ from E. Note that E is a monolingual corpus rather than a parallel corpus. most probable translation e Therefore, string matching on the source side (Sato and Nagao, 1990;"
C14-1192,C90-3044,0,0.400766,"extracting parallel sentences from comparable corpora. Experiments show that our translation retrieval system significantly outperforms a state-of-the-art parallel corpus mining system. 2 Related Work Our work is inspired by three research topics: retrieving translation candidates from parallel corpus, using lattice to compactly represent exponentially many alternatives, and using lattice as query in information retrieval. 1. Translation Retrieval using Parallel Corpus. The idea of retrieving translation candidates from existing texts originated in example-based and memory-based translation (Sato and Nagao, 1990; Nirenburg et al., 1993; Baldwin and Tanaka, 2000; Baldwin, 2001). As these early efforts use a parallel corpus (e.g., translation records that are pairs of source-language and target-language strings), they focus on calculating the similarity between two source-language strings. In contrast, we evaluate the translational equivalence of a given source string and a target string in a large monolingual corpus. 2. Lattice in Machine Translation. Lattices have been widely used in machine translation: considering Chinese word segmentation alternatives (Xu et al., 2005), speech recognition candidat"
C14-1192,D08-1065,0,0.0259949,", translation records that are pairs of source-language and target-language strings), they focus on calculating the similarity between two source-language strings. In contrast, we evaluate the translational equivalence of a given source string and a target string in a large monolingual corpus. 2. Lattice in Machine Translation. Lattices have been widely used in machine translation: considering Chinese word segmentation alternatives (Xu et al., 2005), speech recognition candidates (Matsoukas et al., 2007), SCFG (Dyer et al., 2008) and so on in the decoding process, minimum bayes risk decoding (Tromble et al., 2008), minimum error rate training (Macherey et al., 2008), system combination (Feng et al., 2009), just to name a few. In this work, we are interested in how to use a lattice that encodes exponentially many translation candidates as a single query to retrieve similar target sentences via an information retrieval system. 3. Query Lattice in Information Retrieval. The use of lattices in information retrieval dates back to Moore (1958). Most current lattice-based IR systems often treat lattices as conceptional hierarchies or thesauri in formal concept analysis (Priss, 2000; Cheung and Vogel, 2005). I"
C14-1192,2005.iwslt-1.18,1,0.777298,"emory-based translation (Sato and Nagao, 1990; Nirenburg et al., 1993; Baldwin and Tanaka, 2000; Baldwin, 2001). As these early efforts use a parallel corpus (e.g., translation records that are pairs of source-language and target-language strings), they focus on calculating the similarity between two source-language strings. In contrast, we evaluate the translational equivalence of a given source string and a target string in a large monolingual corpus. 2. Lattice in Machine Translation. Lattices have been widely used in machine translation: considering Chinese word segmentation alternatives (Xu et al., 2005), speech recognition candidates (Matsoukas et al., 2007), SCFG (Dyer et al., 2008) and so on in the decoding process, minimum bayes risk decoding (Tromble et al., 2008), minimum error rate training (Macherey et al., 2008), system combination (Feng et al., 2009), just to name a few. In this work, we are interested in how to use a lattice that encodes exponentially many translation candidates as a single query to retrieve similar target sentences via an information retrieval system. 3. Query Lattice in Information Retrieval. The use of lattices in information retrieval dates back to Moore (1958)"
C14-1192,N06-1053,0,0.0305477,"n candidates as a single query to retrieve similar target sentences via an information retrieval system. 3. Query Lattice in Information Retrieval. The use of lattices in information retrieval dates back to Moore (1958). Most current lattice-based IR systems often treat lattices as conceptional hierarchies or thesauri in formal concept analysis (Priss, 2000; Cheung and Vogel, 2005). In spoken document retrieval, however, lattices are used as a compact representation of multiple speech recognition transcripts to estimate the expected counts of words in each document (Saraclar and Sproat, 2004; Zhou et al., 2006; Chia et al., 2010). Our work is significantly different from previous work that uses the bag-of-words model because translation retrieval must take structure and dependencies in text into account to ensure translational equivalence. 3 3.1 Query Lattice for Translation Retrieval Translation Retrieval Let f be a source-language string, E be a set of target-language strings, the problem is how to find the ˆ from E. Note that E is a monolingual corpus rather than a parallel corpus. most probable translation e Therefore, string matching on the source side (Sato and Nagao, 1990; Nirenburg et al.,"
C14-1192,P07-2045,0,\N,Missing
C14-1199,P05-1045,0,0.102615,"or each entity mention. The top list types are considered in our methods. Experiments in Section 4.3 are conducted on top k {k ∈ 1, 2, 3} type/types in the obtained ranked list. And they are combined with a greedy method similar to that in the substitution method explained above. 4 Experiments 4.1 Settings We use the same data sets as (Riedel et al., 2010) and (Hoffmann et al., 2011), where NYTimes sentences in the years 2005-2006 are used as training corpus Σtrain for distant supervision and sentences in 2007 are used as testing corpus Σpredict . The data was first tagged with an NER system (Finkel et al., 2005) and consecutive words with the same tag are extracted as entity mentions. And then, entity mentions Etrain in training corpus are aligned to facts ∆ in Freebase as training examples to train the models. We integrate our fine-grained entity type constraint with MULTIR, an existing multi-instance multilabel extracting model in (Hoffmann et al., 2011). Following their setttings, we conduct experiments on aggregated extraction and sentential extraction to show the effect of fine-grained entity type constraints. • Aggregated extraction: Aggregated extraction is corpus-level extraction. When given"
C14-1199,P11-1055,0,0.557259,"es mentioned above, recently, a more promising approach named distantly supervised relation extraction (or distant supervision for relation extraction) (Mintz et al., 2009) has become popular. Instead of manual labeling, it automatically generates training data by aligning facts in existing knowledge bases to text. However, the paradigm of distant supervision also causes new problems of noisy training data both in positive training instances and negative training instances. To overcome the false positive problem caused by the distant supervision assumption, researches in (Riedel et al., 2010)(Hoffmann et al., 2011)(Surdeanu et al., 2012) proposed multi-instance models to model noisy positive training data, where they assumed that at least one sentence in those containing an entity pair is truly positive. Takamatsu et al. (Takamatsu et al., 2012) claimed that the at-least-one assumption in multi-instance models would fail when there was only one sentence containing both entities. They proposed a method to learn and filter noisy pattern features from training instances to overcome the false positive problem. Researchers (Xu et al., 2013)(Zhang et al., 2013)(Ritter and Etzioni, 2013) tried to address the p"
C14-1199,N13-1095,0,0.0324603,"without the mentioned assumptions. Their work predicted negative patterns using a generative model and remove labeled data containing negative patterns to reducing noise in labeled data. 2114 Besides the problem of false positive training examples caused by distant supervision. There were a bunch of researches trying to solve the problem of false negative training examples caused by incomplete knowledge bases. Zhang (Zhang et al., 2013) made heuristic rules to filter the false negative training examples. And Xu (Xu et al., 2013) tried to overcom this problem by pseudo-relevance feedback. Min (Min et al., 2013) improved MIML in (Surdeanu et al., 2012) by adding a new layer in their 3-layer graphic model to model the incomplete knowledge base. Ritter (Ritter and Etzioni, 2013) employed similar intuition with (Xu et al., 2013) that they thought rear entities missing in the database would be often mentioned in the text. They proposed a latent-variable approach to model it and showed its improvement over aggregate and sentential extraction. 6 Conclusion In this paper, we propose a novel approach to explore the fine-grained entity type constraints for distantly supervised relation extraction. We leverage"
C14-1199,P09-1113,0,0.516899,"om sentences containing them. It can potentially benefit many applications, such as knowledge base construction, question answering (Ravichandran and Hovy, 2002), textual entailment (Szpektor et al., 2005), etc. Traditional supervised approaches for relation extraction (Zhou et al., 2005)(Zhou et al., 2007) need to manually label training data, which is expensive and limits the ability to scale up. Due to the shortcoming of supervised approaches mentioned above, recently, a more promising approach named distantly supervised relation extraction (or distant supervision for relation extraction) (Mintz et al., 2009) has become popular. Instead of manual labeling, it automatically generates training data by aligning facts in existing knowledge bases to text. However, the paradigm of distant supervision also causes new problems of noisy training data both in positive training instances and negative training instances. To overcome the false positive problem caused by the distant supervision assumption, researches in (Riedel et al., 2010)(Hoffmann et al., 2011)(Surdeanu et al., 2012) proposed multi-instance models to model noisy positive training data, where they assumed that at least one sentence in those c"
C14-1199,P02-1006,0,0.0380121,"we propose a novel method to explore fine-grained entity type constraints, and we study a series of methods to integrate the constraints with the relation extracting model. Experimental results show that our methods achieve better precision/recall curves in sentential extraction with smoother curves in aggregated extraction which mean more stable models. 1 Introduction Relation Extraction is the task of extracting semantic relations between a pair of entities from sentences containing them. It can potentially benefit many applications, such as knowledge base construction, question answering (Ravichandran and Hovy, 2002), textual entailment (Szpektor et al., 2005), etc. Traditional supervised approaches for relation extraction (Zhou et al., 2005)(Zhou et al., 2007) need to manually label training data, which is expensive and limits the ability to scale up. Due to the shortcoming of supervised approaches mentioned above, recently, a more promising approach named distantly supervised relation extraction (or distant supervision for relation extraction) (Mintz et al., 2009) has become popular. Instead of manual labeling, it automatically generates training data by aligning facts in existing knowledge bases to tex"
C14-1199,Q13-1030,0,0.129219,"in (Riedel et al., 2010)(Hoffmann et al., 2011)(Surdeanu et al., 2012) proposed multi-instance models to model noisy positive training data, where they assumed that at least one sentence in those containing an entity pair is truly positive. Takamatsu et al. (Takamatsu et al., 2012) claimed that the at-least-one assumption in multi-instance models would fail when there was only one sentence containing both entities. They proposed a method to learn and filter noisy pattern features from training instances to overcome the false positive problem. Researchers (Xu et al., 2013)(Zhang et al., 2013)(Ritter and Etzioni, 2013) tried to address the problem of false negative training data caused by the incomplete knowledge base. Xu el al. (Xu et al., 2013) used the pseudorelevance feedback method trying to find out the false negative instances and add them into positive training instances. Zhang et al. (Zhang et al., 2013) employed some rules to select negative training instances carefully, hoping not to include the false negative instances. And Ritter et al. (Ritter and Etzioni, 2013) used hidden variables to model the missing data in databases based on a graphical model. The training data generation process for all"
C14-1199,D12-1042,0,0.372137,"ently, a more promising approach named distantly supervised relation extraction (or distant supervision for relation extraction) (Mintz et al., 2009) has become popular. Instead of manual labeling, it automatically generates training data by aligning facts in existing knowledge bases to text. However, the paradigm of distant supervision also causes new problems of noisy training data both in positive training instances and negative training instances. To overcome the false positive problem caused by the distant supervision assumption, researches in (Riedel et al., 2010)(Hoffmann et al., 2011)(Surdeanu et al., 2012) proposed multi-instance models to model noisy positive training data, where they assumed that at least one sentence in those containing an entity pair is truly positive. Takamatsu et al. (Takamatsu et al., 2012) claimed that the at-least-one assumption in multi-instance models would fail when there was only one sentence containing both entities. They proposed a method to learn and filter noisy pattern features from training instances to overcome the false positive problem. Researchers (Xu et al., 2013)(Zhang et al., 2013)(Ritter and Etzioni, 2013) tried to address the problem of false negativ"
C14-1199,P12-1076,0,0.211384,"lly generates training data by aligning facts in existing knowledge bases to text. However, the paradigm of distant supervision also causes new problems of noisy training data both in positive training instances and negative training instances. To overcome the false positive problem caused by the distant supervision assumption, researches in (Riedel et al., 2010)(Hoffmann et al., 2011)(Surdeanu et al., 2012) proposed multi-instance models to model noisy positive training data, where they assumed that at least one sentence in those containing an entity pair is truly positive. Takamatsu et al. (Takamatsu et al., 2012) claimed that the at-least-one assumption in multi-instance models would fail when there was only one sentence containing both entities. They proposed a method to learn and filter noisy pattern features from training instances to overcome the false positive problem. Researchers (Xu et al., 2013)(Zhang et al., 2013)(Ritter and Etzioni, 2013) tried to address the problem of false negative training data caused by the incomplete knowledge base. Xu el al. (Xu et al., 2013) used the pseudorelevance feedback method trying to find out the false negative instances and add them into positive training in"
C14-1199,P10-1013,0,0.0263185,"n which aims to automatically generate labeled data by aligning with data in knowledge bases. It is introduced by Craven and Kumlien (Craven et al., 1999) who used the Yeast Protein Database to generate labeled data and trained a naive-Bayes extractor. Bellare and McCallum (Bellare and McCallum, 2007) used BibTex records as the source of distant supervision. The KYLIN system in (Wu and Weld, 2007) used article titles and infoboxes of Wikipedia to label sentences and trained a CRF extractor aiming to generate infoboxes automatically. The Open IE systems TEXTRUNNER (Yates et al., 2007) and WOE (Wu and Weld, 2010) trained their extractors with the automatic labeled data from Penn Treebank and Wikipedia infoboxes respectively. Mintz (Mintz et al., 2009) first introduced their work that performed distant supervision for relation extraction. It used Freebase as the knowledge base to align sentences in Wikipedia as training data and trained a logistic regression classifier to extract relations between entities.Distant supervision supplied a method to generate training data automatically, however it also bring the problem of noisy labeling. After their work, a variety of methods focused to solve this proble"
C14-1199,P13-2117,0,0.063904,"Missing"
C14-1199,N07-4013,0,0.0132693,"hods in information extraction which aims to automatically generate labeled data by aligning with data in knowledge bases. It is introduced by Craven and Kumlien (Craven et al., 1999) who used the Yeast Protein Database to generate labeled data and trained a naive-Bayes extractor. Bellare and McCallum (Bellare and McCallum, 2007) used BibTex records as the source of distant supervision. The KYLIN system in (Wu and Weld, 2007) used article titles and infoboxes of Wikipedia to label sentences and trained a CRF extractor aiming to generate infoboxes automatically. The Open IE systems TEXTRUNNER (Yates et al., 2007) and WOE (Wu and Weld, 2010) trained their extractors with the automatic labeled data from Penn Treebank and Wikipedia infoboxes respectively. Mintz (Mintz et al., 2009) first introduced their work that performed distant supervision for relation extraction. It used Freebase as the knowledge base to align sentences in Wikipedia as training data and trained a logistic regression classifier to extract relations between entities.Distant supervision supplied a method to generate training data automatically, however it also bring the problem of noisy labeling. After their work, a variety of methods"
C14-1199,P13-2141,0,0.064606,"sumption, researches in (Riedel et al., 2010)(Hoffmann et al., 2011)(Surdeanu et al., 2012) proposed multi-instance models to model noisy positive training data, where they assumed that at least one sentence in those containing an entity pair is truly positive. Takamatsu et al. (Takamatsu et al., 2012) claimed that the at-least-one assumption in multi-instance models would fail when there was only one sentence containing both entities. They proposed a method to learn and filter noisy pattern features from training instances to overcome the false positive problem. Researchers (Xu et al., 2013)(Zhang et al., 2013)(Ritter and Etzioni, 2013) tried to address the problem of false negative training data caused by the incomplete knowledge base. Xu el al. (Xu et al., 2013) used the pseudorelevance feedback method trying to find out the false negative instances and add them into positive training instances. Zhang et al. (Zhang et al., 2013) employed some rules to select negative training instances carefully, hoping not to include the false negative instances. And Ritter et al. (Ritter and Etzioni, 2013) used hidden variables to model the missing data in databases based on a graphical model. The training data"
C14-1199,P05-1053,0,0.175111,"with the relation extracting model. Experimental results show that our methods achieve better precision/recall curves in sentential extraction with smoother curves in aggregated extraction which mean more stable models. 1 Introduction Relation Extraction is the task of extracting semantic relations between a pair of entities from sentences containing them. It can potentially benefit many applications, such as knowledge base construction, question answering (Ravichandran and Hovy, 2002), textual entailment (Szpektor et al., 2005), etc. Traditional supervised approaches for relation extraction (Zhou et al., 2005)(Zhou et al., 2007) need to manually label training data, which is expensive and limits the ability to scale up. Due to the shortcoming of supervised approaches mentioned above, recently, a more promising approach named distantly supervised relation extraction (or distant supervision for relation extraction) (Mintz et al., 2009) has become popular. Instead of manual labeling, it automatically generates training data by aligning facts in existing knowledge bases to text. However, the paradigm of distant supervision also causes new problems of noisy training data both in positive training instan"
C14-1199,D07-1076,0,0.0304933,"xtracting model. Experimental results show that our methods achieve better precision/recall curves in sentential extraction with smoother curves in aggregated extraction which mean more stable models. 1 Introduction Relation Extraction is the task of extracting semantic relations between a pair of entities from sentences containing them. It can potentially benefit many applications, such as knowledge base construction, question answering (Ravichandran and Hovy, 2002), textual entailment (Szpektor et al., 2005), etc. Traditional supervised approaches for relation extraction (Zhou et al., 2005)(Zhou et al., 2007) need to manually label training data, which is expensive and limits the ability to scale up. Due to the shortcoming of supervised approaches mentioned above, recently, a more promising approach named distantly supervised relation extraction (or distant supervision for relation extraction) (Mintz et al., 2009) has become popular. Instead of manual labeling, it automatically generates training data by aligning facts in existing knowledge bases to text. However, the paradigm of distant supervision also causes new problems of noisy training data both in positive training instances and negative tr"
C14-1199,W04-3206,0,\N,Missing
C16-1054,C12-1029,0,0.0193289,"d rewrite rules in summarization. Galanis et al. (2012) used ILP to jointly maximize the importance of the sentences and their diversity in the summary. In this work, we leverage the unsupervised ILP framework from Gillick et al. (2009) as our summarization system and incorporate post information to help boost summarization performance. Sentence compression techniques are widely used in summarization in order to generate abstractive summaries. Previous research has shown the effectiveness of sentence compression for automatic document summarization (Knight and Marcu, 2000; Zajic et al., 2007; Chali and Hasan, 2012; Wang et al., 2013). The compressed summaries can be generated through a pipeline approach that combines a generic sentence compression model with a summary sentence pre-selection or post-selection step. In addition, joint summarization and sentence compression method attracts lots of attention these years. (Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011; Li et al., 2014) are typical work in this area. Their focus is to leverage the ILP technique to jointly select and compress sentences for multi-document summarization. In our work, we consider posts as summary related information and"
C16-1054,W08-1105,0,0.0909924,"Missing"
C16-1054,C12-1056,0,0.0179621,"t al. (2009) revised it to conceptbased ILP, which is similar to the Budgeted Maximal Coverage problem in (Khuller et al., 1999). Then other optimization methods have been used in summarization (Lin and Bilmes, 2010; Davis et al., 2012; Li et al., 2015b; Li et al., 2015a). In the concept-based ILP summarization methods, how to determine the concepts and measure their weights are the two key factors impacting the system performance. Woodsend and Lapata (2012) utilized ILP to jointly optimize different aspects including content selection, surface realization, and rewrite rules in summarization. Galanis et al. (2012) used ILP to jointly maximize the importance of the sentences and their diversity in the summary. In this work, we leverage the unsupervised ILP framework from Gillick et al. (2009) as our summarization system and incorporate post information to help boost summarization performance. Sentence compression techniques are widely used in summarization in order to generate abstractive summaries. Previous research has shown the effectiveness of sentence compression for automatic document summarization (Knight and Marcu, 2000; Zajic et al., 2007; Chali and Hasan, 2012; Wang et al., 2013). The compress"
C16-1054,D14-1076,1,0.850198,"n summarization in order to generate abstractive summaries. Previous research has shown the effectiveness of sentence compression for automatic document summarization (Knight and Marcu, 2000; Zajic et al., 2007; Chali and Hasan, 2012; Wang et al., 2013). The compressed summaries can be generated through a pipeline approach that combines a generic sentence compression model with a summary sentence pre-selection or post-selection step. In addition, joint summarization and sentence compression method attracts lots of attention these years. (Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011; Li et al., 2014) are typical work in this area. Their focus is to leverage the ILP technique to jointly select and compress sentences for multi-document summarization. In our work, we consider posts as summary related information and then use them for joint sentence compression and summarization. Although there is little work about generating summaries by considering extra information on Facebook data, there is some similar work done on Twitter or other resources. Unsupervised method was tried for summarization by (Wong et al., 2008). (Phelan et al., 2011) used tweets to recommend news articles based on user"
C16-1054,N15-1145,1,0.827258,"related to the following aspects: ILP based summarization method, dependency tree based sentence compression by considering extra information, and mining social media for document summarization. Recently optimization methods have been widely used in extractive summarization. McDonald (2007) first introduced sentence level ILP for summarization. Later Gillick et al. (2009) revised it to conceptbased ILP, which is similar to the Budgeted Maximal Coverage problem in (Khuller et al., 1999). Then other optimization methods have been used in summarization (Lin and Bilmes, 2010; Davis et al., 2012; Li et al., 2015b; Li et al., 2015a). In the concept-based ILP summarization methods, how to determine the concepts and measure their weights are the two key factors impacting the system performance. Woodsend and Lapata (2012) utilized ILP to jointly optimize different aspects including content selection, surface realization, and rewrite rules in summarization. Galanis et al. (2012) used ILP to jointly maximize the importance of the sentences and their diversity in the summary. In this work, we leverage the unsupervised ILP framework from Gillick et al. (2009) as our summarization system and incorporate post"
C16-1054,N15-1079,1,0.837783,"related to the following aspects: ILP based summarization method, dependency tree based sentence compression by considering extra information, and mining social media for document summarization. Recently optimization methods have been widely used in extractive summarization. McDonald (2007) first introduced sentence level ILP for summarization. Later Gillick et al. (2009) revised it to conceptbased ILP, which is similar to the Budgeted Maximal Coverage problem in (Khuller et al., 1999). Then other optimization methods have been used in summarization (Lin and Bilmes, 2010; Davis et al., 2012; Li et al., 2015b; Li et al., 2015a). In the concept-based ILP summarization methods, how to determine the concepts and measure their weights are the two key factors impacting the system performance. Woodsend and Lapata (2012) utilized ILP to jointly optimize different aspects including content selection, surface realization, and rewrite rules in summarization. Galanis et al. (2012) used ILP to jointly maximize the importance of the sentences and their diversity in the summary. In this work, we leverage the unsupervised ILP framework from Gillick et al. (2009) as our summarization system and incorporate post"
C16-1054,N10-1134,0,0.0299569,"s kind. 2 Related Work Our work is closely related to the following aspects: ILP based summarization method, dependency tree based sentence compression by considering extra information, and mining social media for document summarization. Recently optimization methods have been widely used in extractive summarization. McDonald (2007) first introduced sentence level ILP for summarization. Later Gillick et al. (2009) revised it to conceptbased ILP, which is similar to the Budgeted Maximal Coverage problem in (Khuller et al., 1999). Then other optimization methods have been used in summarization (Lin and Bilmes, 2010; Davis et al., 2012; Li et al., 2015b; Li et al., 2015a). In the concept-based ILP summarization methods, how to determine the concepts and measure their weights are the two key factors impacting the system performance. Woodsend and Lapata (2012) utilized ILP to jointly optimize different aspects including content selection, surface realization, and rewrite rules in summarization. Galanis et al. (2012) used ILP to jointly maximize the importance of the sentences and their diversity in the summary. In this work, we leverage the unsupervised ILP framework from Gillick et al. (2009) as our summa"
C16-1054,W04-1013,0,0.0084025,"Missing"
C16-1054,W09-1801,0,0.0273222,"mance. Sentence compression techniques are widely used in summarization in order to generate abstractive summaries. Previous research has shown the effectiveness of sentence compression for automatic document summarization (Knight and Marcu, 2000; Zajic et al., 2007; Chali and Hasan, 2012; Wang et al., 2013). The compressed summaries can be generated through a pipeline approach that combines a generic sentence compression model with a summary sentence pre-selection or post-selection step. In addition, joint summarization and sentence compression method attracts lots of attention these years. (Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011; Li et al., 2014) are typical work in this area. Their focus is to leverage the ILP technique to jointly select and compress sentences for multi-document summarization. In our work, we consider posts as summary related information and then use them for joint sentence compression and summarization. Although there is little work about generating summaries by considering extra information on Facebook data, there is some similar work done on Twitter or other resources. Unsupervised method was tried for summarization by (Wong et al., 2008). (Phelan et al., 2011) used"
C16-1054,D07-1047,0,0.024065,"Unsupervised method was tried for summarization by (Wong et al., 2008). (Phelan et al., 2011) used tweets to recommend news articles based on user preferences. (Gao et al., 2012) produced cross-media news summaries by capturing the ˇ complementary information from both sides. Kothari et al. (2013) and Stajner et al. (2013) investigated detecting news comments from Twitter for extending news information provided. Wei and Gao (2014) derived external features based on a collection of relevant tweets to assist the ranking of the original sentences for highlight generation. In addition to tweets, Svore et al. (2007) leveraged Wikipedia and query log of search engines to help document summarization. Tsukamoto et al. (2015) proposed a method for efficiently collecting posts that are only implicitly related to an announcement post, taking into account retweets on Twitter in particular. Our work involves the two aspects when using post information: one is that we utilize post information to help choose sentences from new articles and compress them to form a summary, and the other is that we directly use sentences from the posts as the summary. 3 Corpus Construction For our work, we manually collected popular"
C16-1054,P13-1136,0,0.0210716,"arization. Galanis et al. (2012) used ILP to jointly maximize the importance of the sentences and their diversity in the summary. In this work, we leverage the unsupervised ILP framework from Gillick et al. (2009) as our summarization system and incorporate post information to help boost summarization performance. Sentence compression techniques are widely used in summarization in order to generate abstractive summaries. Previous research has shown the effectiveness of sentence compression for automatic document summarization (Knight and Marcu, 2000; Zajic et al., 2007; Chali and Hasan, 2012; Wang et al., 2013). The compressed summaries can be generated through a pipeline approach that combines a generic sentence compression model with a summary sentence pre-selection or post-selection step. In addition, joint summarization and sentence compression method attracts lots of attention these years. (Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011; Li et al., 2014) are typical work in this area. Their focus is to leverage the ILP technique to jointly select and compress sentences for multi-document summarization. In our work, we consider posts as summary related information and then use them for j"
C16-1054,C14-1083,1,0.847348,"mmarization. Although there is little work about generating summaries by considering extra information on Facebook data, there is some similar work done on Twitter or other resources. Unsupervised method was tried for summarization by (Wong et al., 2008). (Phelan et al., 2011) used tweets to recommend news articles based on user preferences. (Gao et al., 2012) produced cross-media news summaries by capturing the ˇ complementary information from both sides. Kothari et al. (2013) and Stajner et al. (2013) investigated detecting news comments from Twitter for extending news information provided. Wei and Gao (2014) derived external features based on a collection of relevant tweets to assist the ranking of the original sentences for highlight generation. In addition to tweets, Svore et al. (2007) leveraged Wikipedia and query log of search engines to help document summarization. Tsukamoto et al. (2015) proposed a method for efficiently collecting posts that are only implicitly related to an announcement post, taking into account retweets on Twitter in particular. Our work involves the two aspects when using post information: one is that we utilize post information to help choose sentences from new articl"
C16-1054,C08-1124,0,0.0362832,"ttention these years. (Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011; Li et al., 2014) are typical work in this area. Their focus is to leverage the ILP technique to jointly select and compress sentences for multi-document summarization. In our work, we consider posts as summary related information and then use them for joint sentence compression and summarization. Although there is little work about generating summaries by considering extra information on Facebook data, there is some similar work done on Twitter or other resources. Unsupervised method was tried for summarization by (Wong et al., 2008). (Phelan et al., 2011) used tweets to recommend news articles based on user preferences. (Gao et al., 2012) produced cross-media news summaries by capturing the ˇ complementary information from both sides. Kothari et al. (2013) and Stajner et al. (2013) investigated detecting news comments from Twitter for extending news information provided. Wei and Gao (2014) derived external features based on a collection of relevant tweets to assist the ranking of the original sentences for highlight generation. In addition to tweets, Svore et al. (2007) leveraged Wikipedia and query log of search engines"
C16-1054,D12-1022,0,0.0161237,"ently optimization methods have been widely used in extractive summarization. McDonald (2007) first introduced sentence level ILP for summarization. Later Gillick et al. (2009) revised it to conceptbased ILP, which is similar to the Budgeted Maximal Coverage problem in (Khuller et al., 1999). Then other optimization methods have been used in summarization (Lin and Bilmes, 2010; Davis et al., 2012; Li et al., 2015b; Li et al., 2015a). In the concept-based ILP summarization methods, how to determine the concepts and measure their weights are the two key factors impacting the system performance. Woodsend and Lapata (2012) utilized ILP to jointly optimize different aspects including content selection, surface realization, and rewrite rules in summarization. Galanis et al. (2012) used ILP to jointly maximize the importance of the sentences and their diversity in the summary. In this work, we leverage the unsupervised ILP framework from Gillick et al. (2009) as our summarization system and incorporate post information to help boost summarization performance. Sentence compression techniques are widely used in summarization in order to generate abstractive summaries. Previous research has shown the effectiveness of"
C16-1054,P11-1049,0,\N,Missing
C16-1300,C96-1005,0,0.0570209,"ong et al., 2015; Coulmance et al., 2015), making them unsuitable for bilingual lexicon induction. Although a few exceptions exist (Mikolov et al., 2013b; Faruqui and Dyer, 2014; Lu et al., 2015; Vuli´c and Moens, 2015; Shi et al., 2015; Gouws and Søgaard, 2015; Wick et al., 2016; Ammar et al., 2016), they lack a mechanism to deal with the multiple alternative translation prevalent cross-lingually. The multiple alternative translation across languages is rooted in the polysemy of words within languages. In the monolingual setting, word sense disambiguation stands with a long line of research (Agirre and Rigau, 1996, inter alia). Since the advent of word representation learning, there have been some attempts to learn multiple vectors for a word, each dedicated to a single sense of the word, and therefore known as “sense embedding”. Existing sense embeddings can be roughly divided into two categories, depending on whether external resources are utilized. For those that do not rely on external resources, their main idea is to employ unsupervised methods like clustering to differentiate between multiple senses (Reisinger and Mooney, 2010; Huang et al., 2012; Neelakantan et al., 2014; Tian et al., 2014; Li a"
C16-1300,P15-1072,0,0.0132745,"har et al., 2015; Rothe and Sch¨utze, 2015), or use the resources to obtain a word sense disambiguation system, and then use it to disambiguate words, so that word representation learning methods can be applied (Chen et al., 2014; Iacobacci et al., 2015). An exception is the work of (Guo et al., 2014). Their external resource is parallel data. They observe that different senses of a word usually have different translations, so disambiguation can be thus achieved. However, no prior research has shown how to connect sense embeddings cross-lingually, unless multilingual lexical ontologies exist (Camacho-Collados et al., 2015). For bilingual lexicon induction, where only non-parallel data and a seed lexicon are available, it is unclear whether sense embeddings can address multiple alternative translation. Our work complements (Zhang et al., 2016): Their work applies the Earth Mover’s Distance to the post-processing of fixed bilingual word embeddings to retrieve word translation, while ours strives to 3195 train better bilingual word embeddings with the EMD. In addition, we also explore the feasibility of using the EMD for bilingual lexicon induction from non-parallel data. In computer vision, there have been a few"
C16-1300,D14-1110,1,0.82059,"esources are utilized. For those that do not rely on external resources, their main idea is to employ unsupervised methods like clustering to differentiate between multiple senses (Reisinger and Mooney, 2010; Huang et al., 2012; Neelakantan et al., 2014; Tian et al., 2014; Li and Jurafsky, 2015). For those that do, they typically retrofit existing word vectors to sense inventories (Jauhar et al., 2015; Rothe and Sch¨utze, 2015), or use the resources to obtain a word sense disambiguation system, and then use it to disambiguate words, so that word representation learning methods can be applied (Chen et al., 2014; Iacobacci et al., 2015). An exception is the work of (Guo et al., 2014). Their external resource is parallel data. They observe that different senses of a word usually have different translations, so disambiguation can be thus achieved. However, no prior research has shown how to connect sense embeddings cross-lingually, unless multilingual lexical ontologies exist (Camacho-Collados et al., 2015). For bilingual lexicon induction, where only non-parallel data and a seed lexicon are available, it is unclear whether sense embeddings can address multiple alternative translation. Our work complem"
C16-1300,D15-1131,0,0.102346,"lish translations of three (romanized) Chinese words by the tested systems. The correct translations are in bold. The number of translations in each cell varies because it is automatically determined by the EMD program. 7 Related Work Following its monolingual counterpart (Mikolov et al., 2013c, inter alia), bilingual word representation learning has attracted considerable attention. However, most of the works require parallel data as the cross-lingual signal (Zou et al., 2013; Chandar A P et al., 2014; Hermann and Blunsom, 2014; Koˇcisk´y et al., 2014; Gouws et al., 2015; Luong et al., 2015; Coulmance et al., 2015), making them unsuitable for bilingual lexicon induction. Although a few exceptions exist (Mikolov et al., 2013b; Faruqui and Dyer, 2014; Lu et al., 2015; Vuli´c and Moens, 2015; Shi et al., 2015; Gouws and Søgaard, 2015; Wick et al., 2016; Ammar et al., 2016), they lack a mechanism to deal with the multiple alternative translation prevalent cross-lingually. The multiple alternative translation across languages is rooted in the polysemy of words within languages. In the monolingual setting, word sense disambiguation stands with a long line of research (Agirre and Rigau, 1996, inter alia). Sinc"
C16-1300,E14-1049,0,0.0413951,"ons in each cell varies because it is automatically determined by the EMD program. 7 Related Work Following its monolingual counterpart (Mikolov et al., 2013c, inter alia), bilingual word representation learning has attracted considerable attention. However, most of the works require parallel data as the cross-lingual signal (Zou et al., 2013; Chandar A P et al., 2014; Hermann and Blunsom, 2014; Koˇcisk´y et al., 2014; Gouws et al., 2015; Luong et al., 2015; Coulmance et al., 2015), making them unsuitable for bilingual lexicon induction. Although a few exceptions exist (Mikolov et al., 2013b; Faruqui and Dyer, 2014; Lu et al., 2015; Vuli´c and Moens, 2015; Shi et al., 2015; Gouws and Søgaard, 2015; Wick et al., 2016; Ammar et al., 2016), they lack a mechanism to deal with the multiple alternative translation prevalent cross-lingually. The multiple alternative translation across languages is rooted in the polysemy of words within languages. In the monolingual setting, word sense disambiguation stands with a long line of research (Agirre and Rigau, 1996, inter alia). Since the advent of word representation learning, there have been some attempts to learn multiple vectors for a word, each dedicated to a si"
C16-1300,W04-3208,0,0.15272,"Ney, 2003; Levow et al., 2005; T¨ackstr¨om et al., 2013, inter alia). As building bilingual lexica from parallel corpora has been solved by word alignment (Och and Ney, 2003), researchers have turned their attention to non-parallel corpora. Accompanied by a small seed lexicon, non-parallel corpora are usually the only resources available in resource-scarce languages and domains, making the task of bilingual lexicon induction both important and challenging. A variety of statistical methods have been proposed to induce bilingual lexica from non-parallel data (Rapp, 1999; Koehn and Knight, 2002; Fung and Cheung, 2004; Gaussier et al., 2004; Haghighi et al., 2008; Ravi and Knight, 2011; Vuli´c et al., 2011; Vuli´c and Moens, 2013a; Vuli´c and Moens, 2013b; Dong et al., 2015). With the surge of word embeddings trained by neural networks, recent approaches that learn bilingual word representations from non-parallel data for bilingual lexicon induction have also shown promise (Mikolov et al., 2013b; Vuli´c and Moens, 2015). However, none of the existing methods explicitly considers multiple alternative translation, i.e., the phenomenon that one source language word may have multiple possible translations in t"
C16-1300,P04-1067,0,0.779039,", 2005; T¨ackstr¨om et al., 2013, inter alia). As building bilingual lexica from parallel corpora has been solved by word alignment (Och and Ney, 2003), researchers have turned their attention to non-parallel corpora. Accompanied by a small seed lexicon, non-parallel corpora are usually the only resources available in resource-scarce languages and domains, making the task of bilingual lexicon induction both important and challenging. A variety of statistical methods have been proposed to induce bilingual lexica from non-parallel data (Rapp, 1999; Koehn and Knight, 2002; Fung and Cheung, 2004; Gaussier et al., 2004; Haghighi et al., 2008; Ravi and Knight, 2011; Vuli´c et al., 2011; Vuli´c and Moens, 2013a; Vuli´c and Moens, 2013b; Dong et al., 2015). With the surge of word embeddings trained by neural networks, recent approaches that learn bilingual word representations from non-parallel data for bilingual lexicon induction have also shown promise (Mikolov et al., 2013b; Vuli´c and Moens, 2015). However, none of the existing methods explicitly considers multiple alternative translation, i.e., the phenomenon that one source language word may have multiple possible translations in the target language. For"
C16-1300,N15-1157,0,0.0811635,"7 Related Work Following its monolingual counterpart (Mikolov et al., 2013c, inter alia), bilingual word representation learning has attracted considerable attention. However, most of the works require parallel data as the cross-lingual signal (Zou et al., 2013; Chandar A P et al., 2014; Hermann and Blunsom, 2014; Koˇcisk´y et al., 2014; Gouws et al., 2015; Luong et al., 2015; Coulmance et al., 2015), making them unsuitable for bilingual lexicon induction. Although a few exceptions exist (Mikolov et al., 2013b; Faruqui and Dyer, 2014; Lu et al., 2015; Vuli´c and Moens, 2015; Shi et al., 2015; Gouws and Søgaard, 2015; Wick et al., 2016; Ammar et al., 2016), they lack a mechanism to deal with the multiple alternative translation prevalent cross-lingually. The multiple alternative translation across languages is rooted in the polysemy of words within languages. In the monolingual setting, word sense disambiguation stands with a long line of research (Agirre and Rigau, 1996, inter alia). Since the advent of word representation learning, there have been some attempts to learn multiple vectors for a word, each dedicated to a single sense of the word, and therefore known as “sense embedding”. Existing sense emb"
C16-1300,C14-1048,0,0.0154618,"their main idea is to employ unsupervised methods like clustering to differentiate between multiple senses (Reisinger and Mooney, 2010; Huang et al., 2012; Neelakantan et al., 2014; Tian et al., 2014; Li and Jurafsky, 2015). For those that do, they typically retrofit existing word vectors to sense inventories (Jauhar et al., 2015; Rothe and Sch¨utze, 2015), or use the resources to obtain a word sense disambiguation system, and then use it to disambiguate words, so that word representation learning methods can be applied (Chen et al., 2014; Iacobacci et al., 2015). An exception is the work of (Guo et al., 2014). Their external resource is parallel data. They observe that different senses of a word usually have different translations, so disambiguation can be thus achieved. However, no prior research has shown how to connect sense embeddings cross-lingually, unless multilingual lexical ontologies exist (Camacho-Collados et al., 2015). For bilingual lexicon induction, where only non-parallel data and a seed lexicon are available, it is unclear whether sense embeddings can address multiple alternative translation. Our work complements (Zhang et al., 2016): Their work applies the Earth Mover’s Distance"
C16-1300,P08-1088,0,0.0443622,"al., 2013, inter alia). As building bilingual lexica from parallel corpora has been solved by word alignment (Och and Ney, 2003), researchers have turned their attention to non-parallel corpora. Accompanied by a small seed lexicon, non-parallel corpora are usually the only resources available in resource-scarce languages and domains, making the task of bilingual lexicon induction both important and challenging. A variety of statistical methods have been proposed to induce bilingual lexica from non-parallel data (Rapp, 1999; Koehn and Knight, 2002; Fung and Cheung, 2004; Gaussier et al., 2004; Haghighi et al., 2008; Ravi and Knight, 2011; Vuli´c et al., 2011; Vuli´c and Moens, 2013a; Vuli´c and Moens, 2013b; Dong et al., 2015). With the surge of word embeddings trained by neural networks, recent approaches that learn bilingual word representations from non-parallel data for bilingual lexicon induction have also shown promise (Mikolov et al., 2013b; Vuli´c and Moens, 2015). However, none of the existing methods explicitly considers multiple alternative translation, i.e., the phenomenon that one source language word may have multiple possible translations in the target language. For example, the (romanize"
C16-1300,P12-1092,0,0.0357477,"biguation stands with a long line of research (Agirre and Rigau, 1996, inter alia). Since the advent of word representation learning, there have been some attempts to learn multiple vectors for a word, each dedicated to a single sense of the word, and therefore known as “sense embedding”. Existing sense embeddings can be roughly divided into two categories, depending on whether external resources are utilized. For those that do not rely on external resources, their main idea is to employ unsupervised methods like clustering to differentiate between multiple senses (Reisinger and Mooney, 2010; Huang et al., 2012; Neelakantan et al., 2014; Tian et al., 2014; Li and Jurafsky, 2015). For those that do, they typically retrofit existing word vectors to sense inventories (Jauhar et al., 2015; Rothe and Sch¨utze, 2015), or use the resources to obtain a word sense disambiguation system, and then use it to disambiguate words, so that word representation learning methods can be applied (Chen et al., 2014; Iacobacci et al., 2015). An exception is the work of (Guo et al., 2014). Their external resource is parallel data. They observe that different senses of a word usually have different translations, so disambig"
C16-1300,P15-1010,0,0.0124639,"ed. For those that do not rely on external resources, their main idea is to employ unsupervised methods like clustering to differentiate between multiple senses (Reisinger and Mooney, 2010; Huang et al., 2012; Neelakantan et al., 2014; Tian et al., 2014; Li and Jurafsky, 2015). For those that do, they typically retrofit existing word vectors to sense inventories (Jauhar et al., 2015; Rothe and Sch¨utze, 2015), or use the resources to obtain a word sense disambiguation system, and then use it to disambiguate words, so that word representation learning methods can be applied (Chen et al., 2014; Iacobacci et al., 2015). An exception is the work of (Guo et al., 2014). Their external resource is parallel data. They observe that different senses of a word usually have different translations, so disambiguation can be thus achieved. However, no prior research has shown how to connect sense embeddings cross-lingually, unless multilingual lexical ontologies exist (Camacho-Collados et al., 2015). For bilingual lexicon induction, where only non-parallel data and a seed lexicon are available, it is unclear whether sense embeddings can address multiple alternative translation. Our work complements (Zhang et al., 2016)"
C16-1300,N15-1070,0,0.0124216,"tiple vectors for a word, each dedicated to a single sense of the word, and therefore known as “sense embedding”. Existing sense embeddings can be roughly divided into two categories, depending on whether external resources are utilized. For those that do not rely on external resources, their main idea is to employ unsupervised methods like clustering to differentiate between multiple senses (Reisinger and Mooney, 2010; Huang et al., 2012; Neelakantan et al., 2014; Tian et al., 2014; Li and Jurafsky, 2015). For those that do, they typically retrofit existing word vectors to sense inventories (Jauhar et al., 2015; Rothe and Sch¨utze, 2015), or use the resources to obtain a word sense disambiguation system, and then use it to disambiguate words, so that word representation learning methods can be applied (Chen et al., 2014; Iacobacci et al., 2015). An exception is the work of (Guo et al., 2014). Their external resource is parallel data. They observe that different senses of a word usually have different translations, so disambiguation can be thus achieved. However, no prior research has shown how to connect sense embeddings cross-lingually, unless multilingual lexical ontologies exist (Camacho-Collados"
C16-1300,W02-0902,0,0.217772,"ocessing tasks (Och and Ney, 2003; Levow et al., 2005; T¨ackstr¨om et al., 2013, inter alia). As building bilingual lexica from parallel corpora has been solved by word alignment (Och and Ney, 2003), researchers have turned their attention to non-parallel corpora. Accompanied by a small seed lexicon, non-parallel corpora are usually the only resources available in resource-scarce languages and domains, making the task of bilingual lexicon induction both important and challenging. A variety of statistical methods have been proposed to induce bilingual lexica from non-parallel data (Rapp, 1999; Koehn and Knight, 2002; Fung and Cheung, 2004; Gaussier et al., 2004; Haghighi et al., 2008; Ravi and Knight, 2011; Vuli´c et al., 2011; Vuli´c and Moens, 2013a; Vuli´c and Moens, 2013b; Dong et al., 2015). With the surge of word embeddings trained by neural networks, recent approaches that learn bilingual word representations from non-parallel data for bilingual lexicon induction have also shown promise (Mikolov et al., 2013b; Vuli´c and Moens, 2015). However, none of the existing methods explicitly considers multiple alternative translation, i.e., the phenomenon that one source language word may have multiple pos"
C16-1300,P14-2037,0,0.18134,"Missing"
C16-1300,Q15-1016,0,0.0151673,"of the EMD regularization. We are inclined to take small and many gradient ascent steps, so we fix M = 10, 000 and tune λe on the validation set. Finally, the learning rate is decayed linearly at the end of each epoch. 4.2 Adding Context Vectors In the previous section, we have presented our model with word vectors W S and W T as the parameters. In reality, each word is associated with a context vector as well (Mikolov et al., 2013c). While the usual representation of a word for evaluation is simply a word vector, some authors have suggested adding the context vector (Pennington et al., 2014; Levy et al., 2015). Previously this means a simple postprocessing step during evaluation, but in our setting we can bring the trick to training. Specifically, using Euclidean distance as the ground distance, we would have parametrized Cts in the EMD term (4) as Cts = WtT − WsS . (7) Considering the context vectors U S and U T , we now reformulate the ground distance as   Cts = WtT + UtT − WsS + UsS . (8) This modification affects both steps in the alternating optimization procedure. In addition, the seed term also encourages corresponding context vectors to be close. 5 Experimental Setup 5.1 Data In our exper"
C16-1300,D15-1200,0,0.0122641,"1996, inter alia). Since the advent of word representation learning, there have been some attempts to learn multiple vectors for a word, each dedicated to a single sense of the word, and therefore known as “sense embedding”. Existing sense embeddings can be roughly divided into two categories, depending on whether external resources are utilized. For those that do not rely on external resources, their main idea is to employ unsupervised methods like clustering to differentiate between multiple senses (Reisinger and Mooney, 2010; Huang et al., 2012; Neelakantan et al., 2014; Tian et al., 2014; Li and Jurafsky, 2015). For those that do, they typically retrofit existing word vectors to sense inventories (Jauhar et al., 2015; Rothe and Sch¨utze, 2015), or use the resources to obtain a word sense disambiguation system, and then use it to disambiguate words, so that word representation learning methods can be applied (Chen et al., 2014; Iacobacci et al., 2015). An exception is the work of (Guo et al., 2014). Their external resource is parallel data. They observe that different senses of a word usually have different translations, so disambiguation can be thus achieved. However, no prior research has shown how"
C16-1300,N15-1028,0,0.109226,"because it is automatically determined by the EMD program. 7 Related Work Following its monolingual counterpart (Mikolov et al., 2013c, inter alia), bilingual word representation learning has attracted considerable attention. However, most of the works require parallel data as the cross-lingual signal (Zou et al., 2013; Chandar A P et al., 2014; Hermann and Blunsom, 2014; Koˇcisk´y et al., 2014; Gouws et al., 2015; Luong et al., 2015; Coulmance et al., 2015), making them unsuitable for bilingual lexicon induction. Although a few exceptions exist (Mikolov et al., 2013b; Faruqui and Dyer, 2014; Lu et al., 2015; Vuli´c and Moens, 2015; Shi et al., 2015; Gouws and Søgaard, 2015; Wick et al., 2016; Ammar et al., 2016), they lack a mechanism to deal with the multiple alternative translation prevalent cross-lingually. The multiple alternative translation across languages is rooted in the polysemy of words within languages. In the monolingual setting, word sense disambiguation stands with a long line of research (Agirre and Rigau, 1996, inter alia). Since the advent of word representation learning, there have been some attempts to learn multiple vectors for a word, each dedicated to a single sense of the"
C16-1300,W15-1521,0,0.0519279,"ountain Table 5: English translations of three (romanized) Chinese words by the tested systems. The correct translations are in bold. The number of translations in each cell varies because it is automatically determined by the EMD program. 7 Related Work Following its monolingual counterpart (Mikolov et al., 2013c, inter alia), bilingual word representation learning has attracted considerable attention. However, most of the works require parallel data as the cross-lingual signal (Zou et al., 2013; Chandar A P et al., 2014; Hermann and Blunsom, 2014; Koˇcisk´y et al., 2014; Gouws et al., 2015; Luong et al., 2015; Coulmance et al., 2015), making them unsuitable for bilingual lexicon induction. Although a few exceptions exist (Mikolov et al., 2013b; Faruqui and Dyer, 2014; Lu et al., 2015; Vuli´c and Moens, 2015; Shi et al., 2015; Gouws and Søgaard, 2015; Wick et al., 2016; Ammar et al., 2016), they lack a mechanism to deal with the multiple alternative translation prevalent cross-lingually. The multiple alternative translation across languages is rooted in the polysemy of words within languages. In the monolingual setting, word sense disambiguation stands with a long line of research (Agirre and Rigau"
C16-1300,D14-1113,0,0.0242166,"h a long line of research (Agirre and Rigau, 1996, inter alia). Since the advent of word representation learning, there have been some attempts to learn multiple vectors for a word, each dedicated to a single sense of the word, and therefore known as “sense embedding”. Existing sense embeddings can be roughly divided into two categories, depending on whether external resources are utilized. For those that do not rely on external resources, their main idea is to employ unsupervised methods like clustering to differentiate between multiple senses (Reisinger and Mooney, 2010; Huang et al., 2012; Neelakantan et al., 2014; Tian et al., 2014; Li and Jurafsky, 2015). For those that do, they typically retrofit existing word vectors to sense inventories (Jauhar et al., 2015; Rothe and Sch¨utze, 2015), or use the resources to obtain a word sense disambiguation system, and then use it to disambiguate words, so that word representation learning methods can be applied (Chen et al., 2014; Iacobacci et al., 2015). An exception is the work of (Guo et al., 2014). Their external resource is parallel data. They observe that different senses of a word usually have different translations, so disambiguation can be thus achieve"
C16-1300,J03-1002,0,0.0055145,"e into the training of bilingual word embeddings. In this way, we take advantage of its capability to handle multiple alternative word translations in a natural form of regularization. Our approach shows significant and consistent improvements across four language pairs. We also demonstrate that our approach is particularly preferable in resource-scarce settings as it only requires a minimal seed lexicon. 1 Introduction Bilingual lexica provide word-level semantic equivalence information across languages, and prove to be valuable for a range of cross-lingual natural language processing tasks (Och and Ney, 2003; Levow et al., 2005; T¨ackstr¨om et al., 2013, inter alia). As building bilingual lexica from parallel corpora has been solved by word alignment (Och and Ney, 2003), researchers have turned their attention to non-parallel corpora. Accompanied by a small seed lexicon, non-parallel corpora are usually the only resources available in resource-scarce languages and domains, making the task of bilingual lexicon induction both important and challenging. A variety of statistical methods have been proposed to induce bilingual lexica from non-parallel data (Rapp, 1999; Koehn and Knight, 2002; Fung and"
C16-1300,D14-1162,0,0.0780849,"ntly affect the strength of the EMD regularization. We are inclined to take small and many gradient ascent steps, so we fix M = 10, 000 and tune λe on the validation set. Finally, the learning rate is decayed linearly at the end of each epoch. 4.2 Adding Context Vectors In the previous section, we have presented our model with word vectors W S and W T as the parameters. In reality, each word is associated with a context vector as well (Mikolov et al., 2013c). While the usual representation of a word for evaluation is simply a word vector, some authors have suggested adding the context vector (Pennington et al., 2014; Levy et al., 2015). Previously this means a simple postprocessing step during evaluation, but in our setting we can bring the trick to training. Specifically, using Euclidean distance as the ground distance, we would have parametrized Cts in the EMD term (4) as Cts = WtT − WsS . (7) Considering the context vectors U S and U T , we now reformulate the ground distance as   Cts = WtT + UtT − WsS + UsS . (8) This modification affects both steps in the alternating optimization procedure. In addition, the seed term also encourages corresponding context vectors to be close. 5 Experimental Setup 5"
C16-1300,P99-1067,0,0.445284,"language processing tasks (Och and Ney, 2003; Levow et al., 2005; T¨ackstr¨om et al., 2013, inter alia). As building bilingual lexica from parallel corpora has been solved by word alignment (Och and Ney, 2003), researchers have turned their attention to non-parallel corpora. Accompanied by a small seed lexicon, non-parallel corpora are usually the only resources available in resource-scarce languages and domains, making the task of bilingual lexicon induction both important and challenging. A variety of statistical methods have been proposed to induce bilingual lexica from non-parallel data (Rapp, 1999; Koehn and Knight, 2002; Fung and Cheung, 2004; Gaussier et al., 2004; Haghighi et al., 2008; Ravi and Knight, 2011; Vuli´c et al., 2011; Vuli´c and Moens, 2013a; Vuli´c and Moens, 2013b; Dong et al., 2015). With the surge of word embeddings trained by neural networks, recent approaches that learn bilingual word representations from non-parallel data for bilingual lexicon induction have also shown promise (Mikolov et al., 2013b; Vuli´c and Moens, 2015). However, none of the existing methods explicitly considers multiple alternative translation, i.e., the phenomenon that one source language wo"
C16-1300,P11-1002,0,0.0276315,"As building bilingual lexica from parallel corpora has been solved by word alignment (Och and Ney, 2003), researchers have turned their attention to non-parallel corpora. Accompanied by a small seed lexicon, non-parallel corpora are usually the only resources available in resource-scarce languages and domains, making the task of bilingual lexicon induction both important and challenging. A variety of statistical methods have been proposed to induce bilingual lexica from non-parallel data (Rapp, 1999; Koehn and Knight, 2002; Fung and Cheung, 2004; Gaussier et al., 2004; Haghighi et al., 2008; Ravi and Knight, 2011; Vuli´c et al., 2011; Vuli´c and Moens, 2013a; Vuli´c and Moens, 2013b; Dong et al., 2015). With the surge of word embeddings trained by neural networks, recent approaches that learn bilingual word representations from non-parallel data for bilingual lexicon induction have also shown promise (Mikolov et al., 2013b; Vuli´c and Moens, 2015). However, none of the existing methods explicitly considers multiple alternative translation, i.e., the phenomenon that one source language word may have multiple possible translations in the target language. For example, the (romanized) Chinese word “qiche”"
C16-1300,N10-1013,0,0.01923,"al setting, word sense disambiguation stands with a long line of research (Agirre and Rigau, 1996, inter alia). Since the advent of word representation learning, there have been some attempts to learn multiple vectors for a word, each dedicated to a single sense of the word, and therefore known as “sense embedding”. Existing sense embeddings can be roughly divided into two categories, depending on whether external resources are utilized. For those that do not rely on external resources, their main idea is to employ unsupervised methods like clustering to differentiate between multiple senses (Reisinger and Mooney, 2010; Huang et al., 2012; Neelakantan et al., 2014; Tian et al., 2014; Li and Jurafsky, 2015). For those that do, they typically retrofit existing word vectors to sense inventories (Jauhar et al., 2015; Rothe and Sch¨utze, 2015), or use the resources to obtain a word sense disambiguation system, and then use it to disambiguate words, so that word representation learning methods can be applied (Chen et al., 2014; Iacobacci et al., 2015). An exception is the work of (Guo et al., 2014). Their external resource is parallel data. They observe that different senses of a word usually have different trans"
C16-1300,P15-1173,0,0.0234585,"Missing"
C16-1300,P15-2093,1,0.836973,"the EMD program. 7 Related Work Following its monolingual counterpart (Mikolov et al., 2013c, inter alia), bilingual word representation learning has attracted considerable attention. However, most of the works require parallel data as the cross-lingual signal (Zou et al., 2013; Chandar A P et al., 2014; Hermann and Blunsom, 2014; Koˇcisk´y et al., 2014; Gouws et al., 2015; Luong et al., 2015; Coulmance et al., 2015), making them unsuitable for bilingual lexicon induction. Although a few exceptions exist (Mikolov et al., 2013b; Faruqui and Dyer, 2014; Lu et al., 2015; Vuli´c and Moens, 2015; Shi et al., 2015; Gouws and Søgaard, 2015; Wick et al., 2016; Ammar et al., 2016), they lack a mechanism to deal with the multiple alternative translation prevalent cross-lingually. The multiple alternative translation across languages is rooted in the polysemy of words within languages. In the monolingual setting, word sense disambiguation stands with a long line of research (Agirre and Rigau, 1996, inter alia). Since the advent of word representation learning, there have been some attempts to learn multiple vectors for a word, each dedicated to a single sense of the word, and therefore known as “sense embed"
C16-1300,Q13-1001,0,0.124303,"Missing"
C16-1300,C14-1016,0,0.0130171,"(Agirre and Rigau, 1996, inter alia). Since the advent of word representation learning, there have been some attempts to learn multiple vectors for a word, each dedicated to a single sense of the word, and therefore known as “sense embedding”. Existing sense embeddings can be roughly divided into two categories, depending on whether external resources are utilized. For those that do not rely on external resources, their main idea is to employ unsupervised methods like clustering to differentiate between multiple senses (Reisinger and Mooney, 2010; Huang et al., 2012; Neelakantan et al., 2014; Tian et al., 2014; Li and Jurafsky, 2015). For those that do, they typically retrofit existing word vectors to sense inventories (Jauhar et al., 2015; Rothe and Sch¨utze, 2015), or use the resources to obtain a word sense disambiguation system, and then use it to disambiguate words, so that word representation learning methods can be applied (Chen et al., 2014; Iacobacci et al., 2015). An exception is the work of (Guo et al., 2014). Their external resource is parallel data. They observe that different senses of a word usually have different translations, so disambiguation can be thus achieved. However, no prio"
C16-1300,N13-1011,0,0.48435,"Missing"
C16-1300,D13-1168,0,0.151132,"Missing"
C16-1300,P15-2118,0,0.149214,"Missing"
C16-1300,P11-2084,0,0.188188,"Missing"
C16-1300,D13-1141,0,0.0966579,"e competitor customer luxury middle part Ours car automobile auto foundation interior brick onwards building hill mountain Table 5: English translations of three (romanized) Chinese words by the tested systems. The correct translations are in bold. The number of translations in each cell varies because it is automatically determined by the EMD program. 7 Related Work Following its monolingual counterpart (Mikolov et al., 2013c, inter alia), bilingual word representation learning has attracted considerable attention. However, most of the works require parallel data as the cross-lingual signal (Zou et al., 2013; Chandar A P et al., 2014; Hermann and Blunsom, 2014; Koˇcisk´y et al., 2014; Gouws et al., 2015; Luong et al., 2015; Coulmance et al., 2015), making them unsuitable for bilingual lexicon induction. Although a few exceptions exist (Mikolov et al., 2013b; Faruqui and Dyer, 2014; Lu et al., 2015; Vuli´c and Moens, 2015; Shi et al., 2015; Gouws and Søgaard, 2015; Wick et al., 2016; Ammar et al., 2016), they lack a mechanism to deal with the multiple alternative translation prevalent cross-lingually. The multiple alternative translation across languages is rooted in the polysemy of words within l"
C18-1150,W05-0909,0,0.240213,", and we compress it to a 300-dimension vector using 2-layer fully-connected layer. The upper settings are the same for all neural network models. We set batch size, rollout size, D1-step, G1-step, G2-step and G3-step as 64, 16, 5, 1, 2 and 1, respectively. γ for the training of D2 is set to 2.0 empirically. 3.4 Automatic Evaluation There is no direct evaluation metric to determine whether a question is natural or not. We thus use several relevance scores for the automatic evaluation following the setting of existing researches, including Corpus BLEU-4, BLEU-4 (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), ROUGE (Lin and Hovy, 2003) and CIDEr (Vedantam et al., 2015). The overall experiment results in terms of five relevance scores are shown in Table 1. We have some findings as follows: 1 The original dataset contains three parts, namely, MS-COCO, Flickr and Bing, VQG-MSCOCO. However, images from Flickr and Bing are quite different from those in Visual Question Answering (VQA) dataset (Antol et al., 2015), and this makes it difficult to use questions from VQA as negative samples to train D2 . 1768 Model BLEU-4 KNN Img2Seq Img2Seqpre−train MIXER-BLEU-4 ReinforceD1 ReinforceD2 ReinforceD1 +D2 37."
C18-1150,D14-1179,0,0.0420663,"Missing"
C18-1150,P17-1123,0,0.0666076,"is paper locates in the research filed of question generation and reinforcement learning for sequence generation. We will focus on related works from these two domains. Question Generation Question generation has been researched for years from textual input (Rus et al., 2010; Heilman, 2011). Researchers start from rule-based method that extracts key aspects from the input text and then insert these aspects into human generated templates for interrogative sentence generation (Heilman, 2011). Recently, sequence-to-sequence model is utilized for question generation in description-question pairs (Du et al., 2017; Tang et al., 2017; Serban et al., 2016). Although these models generate better performance, the characteristics of question is still ignored. On the other hand, research about visual question generation is much less (Ren et al., 2015; Vijayakumar et al., 2018; 1770 Mostafazadeh et al., 2016; Shijie et al., 2017). Diversity as another important characteristic of question also draws much attention. Li et al. (2016) proposed to use Maximum Mutual Information (MMI) as the objective function for result diversification. Vijayakumar et al. (2018) proposed a diverse beam search for generated multipl"
C18-1150,N16-1014,0,0.0721589,"Missing"
C18-1150,N03-1020,0,0.326693,"nsion vector using 2-layer fully-connected layer. The upper settings are the same for all neural network models. We set batch size, rollout size, D1-step, G1-step, G2-step and G3-step as 64, 16, 5, 1, 2 and 1, respectively. γ for the training of D2 is set to 2.0 empirically. 3.4 Automatic Evaluation There is no direct evaluation metric to determine whether a question is natural or not. We thus use several relevance scores for the automatic evaluation following the setting of existing researches, including Corpus BLEU-4, BLEU-4 (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), ROUGE (Lin and Hovy, 2003) and CIDEr (Vedantam et al., 2015). The overall experiment results in terms of five relevance scores are shown in Table 1. We have some findings as follows: 1 The original dataset contains three parts, namely, MS-COCO, Flickr and Bing, VQG-MSCOCO. However, images from Flickr and Bing are quite different from those in Visual Question Answering (VQA) dataset (Antol et al., 2015), and this makes it difficult to use questions from VQA as negative samples to train D2 . 1768 Model BLEU-4 KNN Img2Seq Img2Seqpre−train MIXER-BLEU-4 ReinforceD1 ReinforceD2 ReinforceD1 +D2 37.062 36.744 37.522 41.674 38."
C18-1150,P16-1170,0,0.0939854,"Missing"
C18-1150,P02-1040,0,0.100776,"riginal dimension of fc7 is 4096, and we compress it to a 300-dimension vector using 2-layer fully-connected layer. The upper settings are the same for all neural network models. We set batch size, rollout size, D1-step, G1-step, G2-step and G3-step as 64, 16, 5, 1, 2 and 1, respectively. γ for the training of D2 is set to 2.0 empirically. 3.4 Automatic Evaluation There is no direct evaluation metric to determine whether a question is natural or not. We thus use several relevance scores for the automatic evaluation following the setting of existing researches, including Corpus BLEU-4, BLEU-4 (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), ROUGE (Lin and Hovy, 2003) and CIDEr (Vedantam et al., 2015). The overall experiment results in terms of five relevance scores are shown in Table 1. We have some findings as follows: 1 The original dataset contains three parts, namely, MS-COCO, Flickr and Bing, VQG-MSCOCO. However, images from Flickr and Bing are quite different from those in Visual Question Answering (VQA) dataset (Antol et al., 2015), and this makes it difficult to use questions from VQA as negative samples to train D2 . 1768 Model BLEU-4 KNN Img2Seq Img2Seqpre−train MIXER-BLEU-4 Reinforc"
C18-1150,D14-1162,0,0.0820416,"rial learning network to better train the generator under the reinforcement learning framework. - ReinforceD2 : it uses the score of D2 as the reward to guide the training of the generator. This model is comparable to MIXER-BLEU-4 because both models utilize a static way to produce reward (BLEU score with ground truth questions in MIXER-BLEU-4 and classification confidence in ReinforceD2 ) - ReinforceD1 +D2 : this is our proposed model. 3.3 Training Details For the generator, we use GRU cell and the number of cells is 512; the dimension of word embedding is 300 and is pre-trained using GloVe (Pennington et al., 2014). The image feature, fc7 is the output of the 7th fully-connected layer in VGGNet. The original dimension of fc7 is 4096, and we compress it to a 300-dimension vector using 2-layer fully-connected layer. The upper settings are the same for all neural network models. We set batch size, rollout size, D1-step, G1-step, G2-step and G3-step as 64, 16, 5, 1, 2 and 1, respectively. γ for the training of D2 is set to 2.0 empirically. 3.4 Automatic Evaluation There is no direct evaluation metric to determine whether a question is natural or not. We thus use several relevance scores for the automatic ev"
C18-1150,W10-4234,0,0.035598,"iddle-level. This indicates that optimizing a single evaluation metric is not sufficient enough for generating high quality natural questions. - The gap between ground-truth questions and machine generated questions is still large. This indicates that there is still a large room for question generation system to improve. 4 Related Works This paper locates in the research filed of question generation and reinforcement learning for sequence generation. We will focus on related works from these two domains. Question Generation Question generation has been researched for years from textual input (Rus et al., 2010; Heilman, 2011). Researchers start from rule-based method that extracts key aspects from the input text and then insert these aspects into human generated templates for interrogative sentence generation (Heilman, 2011). Recently, sequence-to-sequence model is utilized for question generation in description-question pairs (Du et al., 2017; Tang et al., 2017; Serban et al., 2016). Although these models generate better performance, the characteristics of question is still ignored. On the other hand, research about visual question generation is much less (Ren et al., 2015; Vijayakumar et al., 201"
C18-1150,P16-1056,0,0.080342,"Missing"
C18-1150,D17-1090,0,0.0513244,"in the research filed of question generation and reinforcement learning for sequence generation. We will focus on related works from these two domains. Question Generation Question generation has been researched for years from textual input (Rus et al., 2010; Heilman, 2011). Researchers start from rule-based method that extracts key aspects from the input text and then insert these aspects into human generated templates for interrogative sentence generation (Heilman, 2011). Recently, sequence-to-sequence model is utilized for question generation in description-question pairs (Du et al., 2017; Tang et al., 2017; Serban et al., 2016). Although these models generate better performance, the characteristics of question is still ignored. On the other hand, research about visual question generation is much less (Ren et al., 2015; Vijayakumar et al., 2018; 1770 Mostafazadeh et al., 2016; Shijie et al., 2017). Diversity as another important characteristic of question also draws much attention. Li et al. (2016) proposed to use Maximum Mutual Information (MMI) as the objective function for result diversification. Vijayakumar et al. (2018) proposed a diverse beam search for generated multiple questions. Fan et"
C18-1314,N16-1165,0,0.0352315,"Missing"
C18-1314,D16-1053,0,0.0182811,"lignment matrix based on two inputs, which can model complex interactions between the two inputs. Xiong et al. (2016) present a co-attention encoder to focus on relevant parts of the representations of the question and document and use a dynamic pointing decoder to locate the answer. Cui et al. (2016) propose a two-way attention mechanism to encode the passage and question mutually and induce attended attention for final answer predictions. Self-attention mechanism is an attention mechanism aiming at aligning the sequence with itself, which has been successfully used in a variety of tasks. In Cheng et al. (2016), both encoder and decoder are modeled as LSTMs with self-attention for extractive summarization of documents. In Lin et al. (2017), the authors conduct a self-attention over the hidden states of a BiLSTM to extract the sentence embedding. Instead of sentence vector, they use a 2-D matrix to represent the embedding, with each row of the matrix attending on a different part of the sentence. In this work, we employ a co-attention mechanism to capture the interactions between the original post and the reply on argument level. What’s more, we use a self-attention mechanism to obtain the argument r"
C18-1314,W14-4012,0,0.0128942,"Missing"
C18-1314,K17-1017,0,0.0774691,"Matrix Alignment Matrix Post to reply argument attention + Weights GRU GRU GRU u OP Argument Vector Original Post Softmax r1R r1O P uOP GRU r r ... r Co-Attention Network Attention Post argument to reply argument attention R m OP 2 n {U*i }i=1 Co-Attention Network r ... R 2 r1O P Softmax Aggregation Network r X feat O* Attention Pooling R 1 r1R Original Post Reply ... r2R rmR Reply Figure 2: Overall architecture of the proposed model. The left part is the main framework of this work. The right part is the detailed structure of the co-attention network. 2.1 Argument Representation Inspired by Dong et al. (2017), we employ a hierarchical architecture to obtain two different representations for each single argument. For simplicity, we consider each sentence as an argument. Representation based on internal words given an argument with words w1 , w2 , ..., wT , we first map each word to a dense vector obtaining x1 , x2 , ..., xT correspondingly. We then employ a convolution layer to incorporate the context information on word level. zi = f (Wz · [xi : xi+hw −1 ] + bz ) (1) where Wz and bz are weight matrix and bias vector. hw is the window size in the convolution layer and zi is the feature representati"
C18-1314,C14-1089,0,0.0293406,"results on a publicly available dataset show that the proposed model significantly outperforms some state-of-the-art methods for persuasiveness evaluation. Further analysis reveals that attention weights computed in our model are able to extract interactive argument pairs from the original post and the reply. 1 Introduction Computational argumentation is a growing field in natural language processing. Existing research covers argument unit detection (Al-Khatib et al., 2016), argument structure prediction (Peldszus and Stede, 2015; Stab and Gurevych, 2014), argumentation scheme classification (Feng et al., 2014), etc. Recently, the automatic assessment of argumentation quality has started gaining attention. It can be analyzed at two levels, namely monological argumentation and dialogical argumentation. Monological argumentation refers to a composition of arguments on a certain issue (Wachsmuth et al., 2017). A typical example of quality evaluation for monological argumentation is automated essay scoring, which aims to process argumentative essays without human interference (Taghipour and Ng, 2016). It takes an essay as the input and outputs a numeric score, considering features of content, grammar, d"
C18-1314,D16-1129,0,0.350129,"nt, grammar, discourse structure and lexical richness (Burstein et al., 2013). Most of the efforts are made on the exploration for better document representation. Dialogical argumentation refers to a series of interactive arguments related to a given topic, involving argument retraction, view exchange, and so on (Besnard et al., 2014). With the popularity of online debating forums like convinceme 1 , debatepedia 2 and change my view (CMV) 3 , researchers pay increasing attention to evaluating the quality of debating or persuasive content (Tan et al., 2016; Wei and Liu, 2016; Wei et al., 2016; Habernal and Gurevych, 2016a). Although some similarity features of content between the reply and the original post are used to evaluate the quality of the given reply, they are computed on word level without considering the exchange of opinions on the basis of arguments. An example of dialogical argumentation is shown in Figure 1. There are three posts, one is original and the other two are replies. The repliers post to change the view of the original poster. And the positive reply is deemed to be more persuasive than the negative reply. We have three observations. First, viewpoints of the original poster and repliers"
C18-1314,P16-1150,0,0.458692,"nt, grammar, discourse structure and lexical richness (Burstein et al., 2013). Most of the efforts are made on the exploration for better document representation. Dialogical argumentation refers to a series of interactive arguments related to a given topic, involving argument retraction, view exchange, and so on (Besnard et al., 2014). With the popularity of online debating forums like convinceme 1 , debatepedia 2 and change my view (CMV) 3 , researchers pay increasing attention to evaluating the quality of debating or persuasive content (Tan et al., 2016; Wei and Liu, 2016; Wei et al., 2016; Habernal and Gurevych, 2016a). Although some similarity features of content between the reply and the original post are used to evaluate the quality of the given reply, they are computed on word level without considering the exchange of opinions on the basis of arguments. An example of dialogical argumentation is shown in Figure 1. There are three posts, one is original and the other two are replies. The repliers post to change the view of the original poster. And the positive reply is deemed to be more persuasive than the negative reply. We have three observations. First, viewpoints of the original poster and repliers"
C18-1314,P15-1107,0,0.0352229,"xt similarity. The interactions among argument pairs are ignored. In this work, we evaluate the quality of debate comments through the interactions among them on argument level. 5.2 Attention mechanism Attention mechanism allows models to focus on specific parts of inputs at each step of a task. Moreover, attention mechanism has been proved to be significantly effective in natural language processing tasks. Co-attention mechanism has recently attracted lots of research interest in the fields of machine translation (Bahdanau et al., 2014), question answering (Wu et al., 2017), text generation (Li et al., 2015), etc. It is computed as an alignment matrix based on two inputs, which can model complex interactions between the two inputs. Xiong et al. (2016) present a co-attention encoder to focus on relevant parts of the representations of the question and document and use a dynamic pointing decoder to locate the answer. Cui et al. (2016) propose a two-way attention mechanism to encode the passage and question mutually and induce attended attention for final answer predictions. Self-attention mechanism is an attention mechanism aiming at aligning the sequence with itself, which has been successfully us"
C18-1314,D15-1110,0,0.0767786,"network to capture the interactions between participants on argument level. Experimental results on a publicly available dataset show that the proposed model significantly outperforms some state-of-the-art methods for persuasiveness evaluation. Further analysis reveals that attention weights computed in our model are able to extract interactive argument pairs from the original post and the reply. 1 Introduction Computational argumentation is a growing field in natural language processing. Existing research covers argument unit detection (Al-Khatib et al., 2016), argument structure prediction (Peldszus and Stede, 2015; Stab and Gurevych, 2014), argumentation scheme classification (Feng et al., 2014), etc. Recently, the automatic assessment of argumentation quality has started gaining attention. It can be analyzed at two levels, namely monological argumentation and dialogical argumentation. Monological argumentation refers to a composition of arguments on a certain issue (Wachsmuth et al., 2017). A typical example of quality evaluation for monological argumentation is automated essay scoring, which aims to process argumentative essays without human interference (Taghipour and Ng, 2016). It takes an essay as"
C18-1314,D14-1162,0,0.0817283,"as a ranking task and utilize a pairwise hinge loss for training. Given a triple (OP, R+ , R− ), where R+ and R− respectively denote the positive and the negative reply for OP . The loss function is defined in Equation 18. L = max(0, 1 − S(OP, R+ ) + S(OP, R− )) (18) where S(OP, R+ ) and S(OP, R− ) are the corresponding persuasiveness scores. The model is trained by stochastic gradient descent on 105 epochs, and evaluated on the development set at every epoch to select the best model. Dropout (Srivastava et al., 2014) has proved to be an effective method and is used in our work. We use Glove (Pennington et al., 2014) word embeddings, which are 50-dimension word vectors trained with a crawled large corpus with 840 billion tokens. Embeddings for words not present are randomly initialized with sampled numbers from a uniform distribution [0.25,0.25]. We set initial learning rate to 0.1, batch size to 20, filter sizes to 5, filter numbers to 100 and the hidden unit of BiGRU to 200. Early stopping was used with a patience of 15 epochs. We implemented our model using TensorFlow. The model converged in 23 hours on an NVIDIA Titan X machine. 3707 Original post Positive reply Negative reply Avew 10 10 10 Training S"
C18-1314,D14-1006,0,0.0506881,"eractions between participants on argument level. Experimental results on a publicly available dataset show that the proposed model significantly outperforms some state-of-the-art methods for persuasiveness evaluation. Further analysis reveals that attention weights computed in our model are able to extract interactive argument pairs from the original post and the reply. 1 Introduction Computational argumentation is a growing field in natural language processing. Existing research covers argument unit detection (Al-Khatib et al., 2016), argument structure prediction (Peldszus and Stede, 2015; Stab and Gurevych, 2014), argumentation scheme classification (Feng et al., 2014), etc. Recently, the automatic assessment of argumentation quality has started gaining attention. It can be analyzed at two levels, namely monological argumentation and dialogical argumentation. Monological argumentation refers to a composition of arguments on a certain issue (Wachsmuth et al., 2017). A typical example of quality evaluation for monological argumentation is automated essay scoring, which aims to process argumentative essays without human interference (Taghipour and Ng, 2016). It takes an essay as the input and outputs a n"
C18-1314,D16-1193,0,0.0383443,"tructure prediction (Peldszus and Stede, 2015; Stab and Gurevych, 2014), argumentation scheme classification (Feng et al., 2014), etc. Recently, the automatic assessment of argumentation quality has started gaining attention. It can be analyzed at two levels, namely monological argumentation and dialogical argumentation. Monological argumentation refers to a composition of arguments on a certain issue (Wachsmuth et al., 2017). A typical example of quality evaluation for monological argumentation is automated essay scoring, which aims to process argumentative essays without human interference (Taghipour and Ng, 2016). It takes an essay as the input and outputs a numeric score, considering features of content, grammar, discourse structure and lexical richness (Burstein et al., 2013). Most of the efforts are made on the exploration for better document representation. Dialogical argumentation refers to a series of interactive arguments related to a given topic, involving argument retraction, view exchange, and so on (Besnard et al., 2014). With the popularity of online debating forums like convinceme 1 , debatepedia 2 and change my view (CMV) 3 , researchers pay increasing attention to evaluating the quality"
C18-1314,E17-1017,0,0.0698165,"and the reply. 1 Introduction Computational argumentation is a growing field in natural language processing. Existing research covers argument unit detection (Al-Khatib et al., 2016), argument structure prediction (Peldszus and Stede, 2015; Stab and Gurevych, 2014), argumentation scheme classification (Feng et al., 2014), etc. Recently, the automatic assessment of argumentation quality has started gaining attention. It can be analyzed at two levels, namely monological argumentation and dialogical argumentation. Monological argumentation refers to a composition of arguments on a certain issue (Wachsmuth et al., 2017). A typical example of quality evaluation for monological argumentation is automated essay scoring, which aims to process argumentative essays without human interference (Taghipour and Ng, 2016). It takes an essay as the input and outputs a numeric score, considering features of content, grammar, discourse structure and lexical richness (Burstein et al., 2013). Most of the efforts are made on the exploration for better document representation. Dialogical argumentation refers to a series of interactive arguments related to a given topic, involving argument retraction, view exchange, and so on ("
C18-1314,Q17-1016,0,0.22711,"all of the attention representations in a linear function to get the integrated information. The detail is illustrated in Equation 12. n m U = f (U 1 , U 2 , U 3 , {riOP }i=1 , {rjR }j=1 ) (12) where f is a simple linear function, U 3 is a matrix that is tiled n times by u3 . 2.3 Aggregation Network After acquiring the local alignment representation by the co-attention network, we employ a filtration gate to hold the interactive information. Then, we fuse the interactive information via a bi-directional GRU and compute the persuasiveness score. Filtration Gate We utilize the filtration gate (Wang et al., 2017b) to hold the information that helps to understand the argument-level interactions between the original post and the reply. The formulas are in Equation 13 and 14. gt = sigmoid(Wg U + b) (13) U ∗ = gt U (14) We fuse the interactive information reserved by the filtration gate via a bi-directional GRU. The calculation is described in Equation 15. Ot = BiGRU (Ot−1 , Ut∗ ) (15) Then, we use an attention pooling operation over the whole hidden states of this BiGRU to summarize the interactive features into a dense vector O∗ . Scoring Tay et al. (2017) prove that adding some manual features such as"
C18-1314,P17-1018,0,0.170286,"all of the attention representations in a linear function to get the integrated information. The detail is illustrated in Equation 12. n m U = f (U 1 , U 2 , U 3 , {riOP }i=1 , {rjR }j=1 ) (12) where f is a simple linear function, U 3 is a matrix that is tiled n times by u3 . 2.3 Aggregation Network After acquiring the local alignment representation by the co-attention network, we employ a filtration gate to hold the interactive information. Then, we fuse the interactive information via a bi-directional GRU and compute the persuasiveness score. Filtration Gate We utilize the filtration gate (Wang et al., 2017b) to hold the information that helps to understand the argument-level interactions between the original post and the reply. The formulas are in Equation 13 and 14. gt = sigmoid(Wg U + b) (13) U ∗ = gt U (14) We fuse the interactive information reserved by the filtration gate via a bi-directional GRU. The calculation is described in Equation 15. Ot = BiGRU (Ot−1 , Ut∗ ) (15) Then, we use an attention pooling operation over the whole hidden states of this BiGRU to summarize the interactive features into a dense vector O∗ . Scoring Tay et al. (2017) prove that adding some manual features such as"
C18-1314,P16-2032,1,0.941255,"score, considering features of content, grammar, discourse structure and lexical richness (Burstein et al., 2013). Most of the efforts are made on the exploration for better document representation. Dialogical argumentation refers to a series of interactive arguments related to a given topic, involving argument retraction, view exchange, and so on (Besnard et al., 2014). With the popularity of online debating forums like convinceme 1 , debatepedia 2 and change my view (CMV) 3 , researchers pay increasing attention to evaluating the quality of debating or persuasive content (Tan et al., 2016; Wei and Liu, 2016; Wei et al., 2016; Habernal and Gurevych, 2016a). Although some similarity features of content between the reply and the original post are used to evaluate the quality of the given reply, they are computed on word level without considering the exchange of opinions on the basis of arguments. An example of dialogical argumentation is shown in Figure 1. There are three posts, one is original and the other two are replies. The repliers post to change the view of the original poster. And the positive reply is deemed to be more persuasive than the negative reply. We have three observations. First,"
C18-1314,W16-2820,1,0.734769,"features of content, grammar, discourse structure and lexical richness (Burstein et al., 2013). Most of the efforts are made on the exploration for better document representation. Dialogical argumentation refers to a series of interactive arguments related to a given topic, involving argument retraction, view exchange, and so on (Besnard et al., 2014). With the popularity of online debating forums like convinceme 1 , debatepedia 2 and change my view (CMV) 3 , researchers pay increasing attention to evaluating the quality of debating or persuasive content (Tan et al., 2016; Wei and Liu, 2016; Wei et al., 2016; Habernal and Gurevych, 2016a). Although some similarity features of content between the reply and the original post are used to evaluate the quality of the given reply, they are computed on word level without considering the exchange of opinions on the basis of arguments. An example of dialogical argumentation is shown in Figure 1. There are three posts, one is original and the other two are replies. The repliers post to change the view of the original poster. And the positive reply is deemed to be more persuasive than the negative reply. We have three observations. First, viewpoints of the"
chen-etal-2004-evaluating,eickeler-etal-2002-creation,0,\N,Missing
D08-1010,2007.tmi-papers.6,0,0.021171,"work (He et al., 2008), we reported improvements by integrating a MERS model into a formally syntax-based SMT model, the hierarchical phrase-based model (Chiang, 2005). In this paper, we incorporate the MERS model into a stateof-the-art linguistically syntax-based SMT model, the tree-to-string alignment template (TAT) model (Liu et al., 2006). The basic differences are: NPB DEG X 2 :NN NN overall standard of the match Figure 2: Training examples for rules in Figure 1 to be helpful for rule selection. Recent research showed that contextual information can help perform word or phrase selection. Carpuat and Wu (2007b) and Chan et al. (2007) showed improvents by integrating wordsense-disambiguation (WSD) system into a phrasebased (Koehn, 2004) and a hierarchical phrasebased (Chiang, 2005) SMT system, respectively. Similar to WSD, Carpuat and Wu (2007a) used contextual information to solve the ambiguity problem for phrases. They integrated a phrase-sensedisambiguation (PSD) model into a phrase-based SMT system and achieved improvements. In this paper, we propose a novel solution for rule selection for syntax-based SMT. We use the maximum entropy approach to combine rich contextual information around a rule"
D08-1010,D07-1007,0,0.118445,"work (He et al., 2008), we reported improvements by integrating a MERS model into a formally syntax-based SMT model, the hierarchical phrase-based model (Chiang, 2005). In this paper, we incorporate the MERS model into a stateof-the-art linguistically syntax-based SMT model, the tree-to-string alignment template (TAT) model (Liu et al., 2006). The basic differences are: NPB DEG X 2 :NN NN overall standard of the match Figure 2: Training examples for rules in Figure 1 to be helpful for rule selection. Recent research showed that contextual information can help perform word or phrase selection. Carpuat and Wu (2007b) and Chan et al. (2007) showed improvents by integrating wordsense-disambiguation (WSD) system into a phrasebased (Koehn, 2004) and a hierarchical phrasebased (Chiang, 2005) SMT system, respectively. Similar to WSD, Carpuat and Wu (2007a) used contextual information to solve the ambiguity problem for phrases. They integrated a phrase-sensedisambiguation (PSD) model into a phrase-based SMT system and achieved improvements. In this paper, we propose a novel solution for rule selection for syntax-based SMT. We use the maximum entropy approach to combine rich contextual information around a rule"
D08-1010,P07-1005,0,0.196541,"reported improvements by integrating a MERS model into a formally syntax-based SMT model, the hierarchical phrase-based model (Chiang, 2005). In this paper, we incorporate the MERS model into a stateof-the-art linguistically syntax-based SMT model, the tree-to-string alignment template (TAT) model (Liu et al., 2006). The basic differences are: NPB DEG X 2 :NN NN overall standard of the match Figure 2: Training examples for rules in Figure 1 to be helpful for rule selection. Recent research showed that contextual information can help perform word or phrase selection. Carpuat and Wu (2007b) and Chan et al. (2007) showed improvents by integrating wordsense-disambiguation (WSD) system into a phrasebased (Koehn, 2004) and a hierarchical phrasebased (Chiang, 2005) SMT system, respectively. Similar to WSD, Carpuat and Wu (2007a) used contextual information to solve the ambiguity problem for phrases. They integrated a phrase-sensedisambiguation (PSD) model into a phrase-based SMT system and achieved improvements. In this paper, we propose a novel solution for rule selection for syntax-based SMT. We use the maximum entropy approach to combine rich contextual information around a rule and the information of s"
D08-1010,P05-1033,0,0.0940463,"he decoder hardly distinguish the two rules. Intuitively, information of sub-trees covered by nonterminals as well as contextual information of rules are believed 89 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 89–97, c Honolulu, October 2008. 2008 Association for Computational Linguistics NP DNP X 1 :NP NP NPB DEG X 2 :NN DNP NN industrial products manufacturing levels X 1 :NP In our previous work (He et al., 2008), we reported improvements by integrating a MERS model into a formally syntax-based SMT model, the hierarchical phrase-based model (Chiang, 2005). In this paper, we incorporate the MERS model into a stateof-the-art linguistically syntax-based SMT model, the tree-to-string alignment template (TAT) model (Liu et al., 2006). The basic differences are: NPB DEG X 2 :NN NN overall standard of the match Figure 2: Training examples for rules in Figure 1 to be helpful for rule selection. Recent research showed that contextual information can help perform word or phrase selection. Carpuat and Wu (2007b) and Chan et al. (2007) showed improvents by integrating wordsense-disambiguation (WSD) system into a phrasebased (Koehn, 2004) and a hierarchica"
D08-1010,P05-1066,0,0.0340478,"to Liu et al. (2006). We perform minimum error rate training (Och, 2003) to tune the feature weights for the log-linear model to maximize the systems’s BLEU score on the development set. The weights are shown in Table 2. These weights are then used to run Lynx and Lynx+MERS on the test sets. Table 3 shows the results. Lynx obtains BLEU scores of 26.15 on NIST03 and 26.09 on NIST05. Using all features described in Section 3.2, Lynx+MERS finally obtains BLEU scores of 27.05 on NIST03 and 27.28 on NIST05. The absolute improvements is 0.90 and 1.19, respectively. Using the sign-test described by Collins et al. (2005), both improvements are statistically significant at p &lt; 0.01. Moreover, Lynx+MERS also achieves higher n-gram precisions than Lynx. Test Set NIST03 NIST05 System BLEU-4 Lynx +MERS Lynx +MERS 26.15 27.05 26.09 27.28 Individual n-gram precisions 1 2 3 4 71.62 35.64 18.64 9.82 72.00 36.72 19.51 10.37 70.39 35.12 18.53 10.11 71.16 36.19 19.62 10.95 Table 3: BLEU-4 scores (case-insensitive) on the test sets. 5.4 Analysis The baseline system only uses four features for rule selection: the translation probabilities P (e!|T!) and P (T!|e!); and the lexical weights Pw (e!|T!) and Pw (T!|e!). These fea"
D08-1010,P06-1121,0,0.0809525,"ion of sub-trees covered by variables in rules. Therefore, our model allows the decoder to perform context-dependent rule selection during decoding. We incorporate the MERS model into a state-of-the-art linguistically syntax-based SMT model, the treeto-string alignment template model. Experiments show that our approach achieves significant improvements over the baseline system. 1 NP X1 DEG NP NPB NN X2 X 1 X 2 levels NN DNP NP X1 DEG NPB NN NN X2 X 2 standard of X 1 Figure 1: Example of translation rules Introduction Syntax-based statistical machine translation (SMT) models (Liu et al., 2006; Galley et al., 2006; Huang et al., 2006) capture long distance reorderings by using rules with structural and linguistical information as translation knowledge. Typically, a translation rule consists of a source-side and a target-side. However, the source-side of a rule usually corresponds to multiple target-sides in multiple rules. Therefore, during decoding, the decoder should select a correct target-side for a source-side. We call this rule selection. Rule selection is of great importance to syntaxbased SMT systems. Comparing with word selection in word-based SMT and phrase selection in phrase-based SMT, rule"
D08-1010,C08-1041,1,0.381762,"or example, the information of X 1 and X 2 is not recorded when the rules in Figure 1 extracted from the training examples in Figure 2. This makes the decoder hardly distinguish the two rules. Intuitively, information of sub-trees covered by nonterminals as well as contextual information of rules are believed 89 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 89–97, c Honolulu, October 2008. 2008 Association for Computational Linguistics NP DNP X 1 :NP NP NPB DEG X 2 :NN DNP NN industrial products manufacturing levels X 1 :NP In our previous work (He et al., 2008), we reported improvements by integrating a MERS model into a formally syntax-based SMT model, the hierarchical phrase-based model (Chiang, 2005). In this paper, we incorporate the MERS model into a stateof-the-art linguistically syntax-based SMT model, the tree-to-string alignment template (TAT) model (Liu et al., 2006). The basic differences are: NPB DEG X 2 :NN NN overall standard of the match Figure 2: Training examples for rules in Figure 1 to be helpful for rule selection. Recent research showed that contextual information can help perform word or phrase selection. Carpuat and Wu (2007b)"
D08-1010,2006.amta-papers.8,0,0.0215417,"red by variables in rules. Therefore, our model allows the decoder to perform context-dependent rule selection during decoding. We incorporate the MERS model into a state-of-the-art linguistically syntax-based SMT model, the treeto-string alignment template model. Experiments show that our approach achieves significant improvements over the baseline system. 1 NP X1 DEG NP NPB NN X2 X 1 X 2 levels NN DNP NP X1 DEG NPB NN NN X2 X 2 standard of X 1 Figure 1: Example of translation rules Introduction Syntax-based statistical machine translation (SMT) models (Liu et al., 2006; Galley et al., 2006; Huang et al., 2006) capture long distance reorderings by using rules with structural and linguistical information as translation knowledge. Typically, a translation rule consists of a source-side and a target-side. However, the source-side of a rule usually corresponds to multiple target-sides in multiple rules. Therefore, during decoding, the decoder should select a correct target-side for a source-side. We call this rule selection. Rule selection is of great importance to syntaxbased SMT systems. Comparing with word selection in word-based SMT and phrase selection in phrase-based SMT, rule selection is more ge"
D08-1010,N03-1017,0,0.083958,"Missing"
D08-1010,koen-2004-pharaoh,0,0.0449247,"ase-based model (Chiang, 2005). In this paper, we incorporate the MERS model into a stateof-the-art linguistically syntax-based SMT model, the tree-to-string alignment template (TAT) model (Liu et al., 2006). The basic differences are: NPB DEG X 2 :NN NN overall standard of the match Figure 2: Training examples for rules in Figure 1 to be helpful for rule selection. Recent research showed that contextual information can help perform word or phrase selection. Carpuat and Wu (2007b) and Chan et al. (2007) showed improvents by integrating wordsense-disambiguation (WSD) system into a phrasebased (Koehn, 2004) and a hierarchical phrasebased (Chiang, 2005) SMT system, respectively. Similar to WSD, Carpuat and Wu (2007a) used contextual information to solve the ambiguity problem for phrases. They integrated a phrase-sensedisambiguation (PSD) model into a phrase-based SMT system and achieved improvements. In this paper, we propose a novel solution for rule selection for syntax-based SMT. We use the maximum entropy approach to combine rich contextual information around a rule and the information of sub-trees covered by nonterminals in a rule. For each ambiguous source-side of translation rules, a maxim"
D08-1010,P06-1077,1,0.945521,"rules and information of sub-trees covered by variables in rules. Therefore, our model allows the decoder to perform context-dependent rule selection during decoding. We incorporate the MERS model into a state-of-the-art linguistically syntax-based SMT model, the treeto-string alignment template model. Experiments show that our approach achieves significant improvements over the baseline system. 1 NP X1 DEG NP NPB NN X2 X 1 X 2 levels NN DNP NP X1 DEG NPB NN NN X2 X 2 standard of X 1 Figure 1: Example of translation rules Introduction Syntax-based statistical machine translation (SMT) models (Liu et al., 2006; Galley et al., 2006; Huang et al., 2006) capture long distance reorderings by using rules with structural and linguistical information as translation knowledge. Typically, a translation rule consists of a source-side and a target-side. However, the source-side of a rule usually corresponds to multiple target-sides in multiple rules. Therefore, during decoding, the decoder should select a correct target-side for a source-side. We call this rule selection. Rule selection is of great importance to syntaxbased SMT systems. Comparing with word selection in word-based SMT and phrase selection in p"
D08-1010,P00-1056,0,0.236409,"Missing"
D08-1010,P02-1038,0,0.175438,"racted from the training example in Figure 3: lexicalized (the left), partially lexicalized (the middle), unlexicalized (the right). Lexicalized TAT contains only terminals, which is similar to phrase-to-phrase translation in phrase-based model except that it is constrained by a syntactic tree on the source-side. Partially lexicalized TAT contains both terminals and non-terminals, which can be used for both lexical translation and phrase reordering. Unlexicalized TAT contains only nonterminals and can only be used for phrase reordering. Lynx builds translation model in a log-linear framework (Och and Ney, 2002): (2) P (eI1 |T (f1J )) = &quot; exp[ m λm hm (eI1 , T (f1J ))] &quot; &quot; I J e! exp[ m λm hm (e1 , T (f1 ))] The incomes of city and village resident continued to grow Figure 3: Word-aligned, source-parsed training example. NN • Lexical weights: Pw (e!|T!) and Pw (T!|e!); • TAT penalty: exp(1), which is analogous to phrase penalty in phrase-based model; • Language model Plm (eI1 ); • Word penalty I. In Lynx, rule selection mainly depends on translation probabilities and lexical weights. These four scores describe how well a source tree links to a target string, which are estimated on the training corpus"
D08-1010,P03-1021,0,0.275965,"Missing"
D08-1010,P02-1040,0,0.0755056,"res lm1 lm2 0.171 0.013 0.152 0.014 TP -0.055 0.027 WP 0.403 0.270 Prs 0.194 AP 0.207 Table 2: Feature weights obtained by minimum error rate training on the development set. The first 8 features are used by Lynx. TP=TAT penalty, WP=word penalty, AP=ambiguous TAT penalty. Note that in fact, the positive weight for WP and AP indicate a reward. the training corpus and the Xinhua portion of the Gigaword corpus, respectively. NIST MT 2002 test set is used as the development set. NIST MT 2003 and NIST MT 2005 test sets are used as the test sets. The translation quality is evaluated by BLEU metric (Papineni et al., 2002), as calculated by mtevalv11b.pl with case-insensitive matching of n-grams, where n = 4. 5.2 Training To train the translation model, we first run GIZA++ (Och and Ney, 2000) to obtain word alignment in both translation directions. Then the word alignment is refined by performing “grow-diag-final” method (Koehn et al., 2003). We use a Chinese parser developed by Deyi Xiong (Xiong et al., 2005) to parse the Chinese sentences of the training corpus. Our TAT extraction algorithm is similar to Liu et al. (2006), except that we make some tiny modifications to extract contextual features for MERS mod"
D08-1010,I05-1007,1,0.881304,"Missing"
D08-1102,C82-1023,0,0.439987,"ic boundaries shared by both languages, and the resulting monolingual fragments will conform to the grammar of the corresponding language. In this CS theory the relationship between both languages is symmetric –lexical items from one language can be replaced by the corresponding items in the second language and vice versa. Another prevalent linguistic theory argues the contrary: there is an asymmetric relation where the changes can occur only in one direction, which reflects the existence of a Matrix Language (ML), the dominant language, and an Embedded Language (EL), or subordinate language (Joshi, 1982). The Matrix Language Frame model, proposed and extended by Scotton-Myers, supports this asymmetric relation theory. This formalism prescribes that content morphemes can come from the 974 ML or the EL, whereas late system morphemes, the elements that indicate grammatical relations, can only be provided by the ML (Myers-Scotton, 1997). Until an empirical evaluation is carried out on large representative samples of discourse involving a large number of different speakers, and different language-pairs, the production of CS discourse will not be explained satisfactorily. The goal of this work is t"
D08-1102,P98-1002,0,0.457312,"ingual discourse, and corpus-driven studies about CS can also inform linguistic theories. In this paper we present exploratory work on learning to predict CS points using a machine learning approach. Such an approach can be used to reduce perplexity of language models for bilingual discourse. We believe that CS behavior can be learned by a classifier and the results presented in this paper support our belief. One of the difficult aspects of trying to predict CS points is how to evaluate the performance of the learner since switching is intrinsically motivated and there are no forced switches (Sankoff, 1998b). Therefore, standard classification measures for this task such as precision, recall, F-measure, or accuracy, are not the best approach for measuring the effectiveness of a CS predictor. To comple973 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 973–981, c Honolulu, October 2008. 2008 Association for Computational Linguistics ment the evaluation of our approach, we designed a task involving human judgements on the naturalness of automatically generated code-switched sentences. Both evaluations yielded encouraging results. The next section disc"
D08-1102,D08-1110,1,0.780716,"anguage processing tasks, including optical character recognition, text classification, named entity extraction, and many more. The premise of our paper is that machine learning algorithms can also be successful at learning how to code-switch as well as humans. At the very least we want to provide encouraging evidence that this is possible. To the best of our knowledge, there is no previous work related to the problem of automatically predicting CS points. Our machine learning framework then is inspired by existing theories of CS and existing work on part-of-speech tagging code-switched text (Solorio and Liu, 2008). In our approach, each word boundary is a potential point for switching – an instance of the learning task. It should be noted that we can only rely on the history of words preceding potential CS points in orFeature id 1 2 3 4 5 6 7 8 9 10 Description Word Language id Gold-standard POS tag BIO chunk English Tree Tagger POS English Tree Tagger prob English Tree Tagger lemma Spanish Tree Tagger POS Spanish Tree Tagger prob Spanish Tree Tagger lemma Table 1: Features explored in learning to predict CS points. der to extract meaningful features. Otherwise, if we look also into the future, we coul"
D08-1102,C98-1002,0,\N,Missing
D08-1110,carreras-padro-2002-flexible,0,0.0484014,"Missing"
D08-1110,C82-1023,0,0.54418,"n subject to judge sentences generated by a PCFG induced from training data and the language model. However, they only used one human judge. Regarding the automated POS tagging and parsing of code-mixed utterances there is little prior work. To the best of our knowledge, there is no parser, nor POS tagger, currently available for the syntactic analysis of this type of discourse. There are theoretical approaches that propose formalisms to represent the structure of code-switched utterances and describe a framework for parsing and generating mixed sentences, for example for Marathi and English (Joshi, 1982), or Hindi and English (Goyal et al., 2003). Sankoff proposed a production model of bilingual discourse that accounts for the equivalence constraint and the unpredictability of code-switching (Sankoff, 1998a; Sankoff, 1998b). His real-time production model draws on the alternation of fragments from two virtual monolingual sentences. It also accounts for other types of codeswitching such as repetition-translation and insertional code-switching. But no statistical assessment has been conducted on real corpora. Our goal is to develop a POS tagger for codeswitched utterances, which is the first st"
D08-1110,W96-0213,0,0.0249762,"y et al., 1992). Currently, there is no annotation of code-switched text of comparable size. But in contrast to the lack of linguistic resources available for Spanish-English code-mixed discourse, English and Spanish have sufficient resources, especially English. Thus, rather than starting from scratch, we will draw on existing taggers for both languages, which will reduce the amount of code-switched data needed. Some examples of POS taggers that perform reasonably well on monolingual text of each language can be found in (Brants, 2000; Brill, 1992; Carreras and Padro´ , 2002; Charniak, 1993; Ratnaparkhi, 1996; Schmid, 1994). However, these tools are designed to work on monolingual text, therefore if applied as they are to code-switched text, their accuracy will decrease by a large margin. In the following sections we will explore different methods for combining monolingual taggers. 4 Table 1: Excerpts taken from the Spanglish data set. Spanglish (a)Entonces le di´o el virus y no se lo atendi´o and the virus spread through his body. (b)Cuando yo lo vi he looked pretty bad. (c)I think she was taller than he was. Y un car´acter muy bonito tambi´en ella. Very easy going. English Translation (a)Then he"
D08-1110,P98-1002,0,0.586452,"rs have been analyzed on different language pairs, including English-French, English-Dutch, Finish-English, Arabic-French, and Spanish-English, to name a few. There is a general agreement that code-switched patterns are not generated randomly; according to these studies, they follow specific grammatical rules. Furthermore, some studies suggest that, if these rules are violated, the resulting discourse will sound unnatural (Toribio, 2001b; Toribio, 2001a). The following shows the rules governing code-switching discourse described in several studies (Poplack, 1980; Poplack, 1981; Sankoff, 1981; Sankoff, 1998a). • Switches can take place only between full word boundaries. This is also known as the free morpheme constraint. • Monolingual constructs within the sentence 1052 Although these rules are somewhat controversial, and most of the studies on this area have been conducted on small samples, we cannot ignore the fact that patterns bearing the above rules have emerged in different bilingual communities with different backgrounds. 3 Automated Processing of Code-Switched Discourse A previous work related to the processing of codeswitched text deals with language identification on English-Maltese co"
D08-1110,H92-1073,0,\N,Missing
D08-1110,C98-1002,0,\N,Missing
D08-1110,A00-1031,0,\N,Missing
D09-1106,P06-1009,0,0.113398,"Missing"
D09-1106,J93-2003,0,0.0572883,"Missing"
D09-1106,C04-1032,0,0.0330266,"GIZA++ usually produces high quality 1-best alignments for closelyrelated European language pairs, especially when trained on millions of sentences. 6 Related Work Recent studies has shown that SMT systems can benefit from making the annotation pipeline wider: using packed forests instead of 1-best trees (Mi et al., 2008), word lattices instead of 1-best segmentations (Dyer et al., 2008), and n-best alignments instead of 1-best alignments (Venugopal et al., 2008). We propose a compact representation of multiple word alignments that enables SMT systems to make a better use of noisy alignments. Matusov et al. (2004) propose “cost matrices” for producing symmetric alignments. Kumar et al. (2007) describe how to use “posterior probability matrices” to improve alignment accuracy via a bridge language. Although not using the term ”weighted matrices” directly, they both assign a probability to each word pair. We follow Och and Ney (2004) to develop a new phrase extraction algorithm for weighted alignment matrices. The methods for calculating relative frequencies (Och and Ney, 2004) and lexical weights (Koehn et al., 2003) are also adapted for the weighted matrix case. Many researchers (e.g., (Venugopal et al."
D09-1106,J07-2003,0,0.033434,"tiple language pairs show that using weighted matrices achieves consistent improvements over using n-best lists in significant less extraction time. 1 Introduction Statistical machine translation (SMT) relies heavily on annotated bilingual corpora. Word alignment, which indicates the correspondence between the words in a parallel text, is one of the most important annotations in SMT. Word-aligned corpora have been found to be an excellent source for translation-related knowledge, not only for phrase-based models (Och and Ney, 2004; Koehn et al., 2003), but also for syntax-based models (e.g., (Chiang, 2007; Galley et al., 2006; Shen et al., 2008; Liu et al., 2006)). Och and Ney (2003) indicate that the quality of machine translation output depends directly on the quality of initial word alignment. Modern alignment methods can be divided into two major categories: generative methods and discriminative methods. Generative methods (Brown et al., 1993; Vogel and Ney, 1996) treat word alignment as a hidden process and maximize the likelihood of bilingual training corpus using the expectation maximization (EM) algorithm. In contrast, discriminative methods (e.g., (Moore et al., 2006; Taskar et al., 2"
D09-1106,P08-1023,1,0.123632,"-insensitive BLEU scores of 1-best, 10-best, and m(10) on the Europarl data. Using weighted packed matrices continued to show advantage over using 1-best alignments on multiple language pairs. However, these improvements were very small and not significant. We attribute this to the fact that GIZA++ usually produces high quality 1-best alignments for closelyrelated European language pairs, especially when trained on millions of sentences. 6 Related Work Recent studies has shown that SMT systems can benefit from making the annotation pipeline wider: using packed forests instead of 1-best trees (Mi et al., 2008), word lattices instead of 1-best segmentations (Dyer et al., 2008), and n-best alignments instead of 1-best alignments (Venugopal et al., 2008). We propose a compact representation of multiple word alignments that enables SMT systems to make a better use of noisy alignments. Matusov et al. (2004) propose “cost matrices” for producing symmetric alignments. Kumar et al. (2007) describe how to use “posterior probability matrices” to improve alignment accuracy via a bridge language. Although not using the term ”weighted matrices” directly, they both assign a probability to each word pair. We foll"
D09-1106,P05-1066,0,0.0301148,"Missing"
D09-1106,P06-1065,0,0.119801,"Missing"
D09-1106,P08-1010,0,0.346784,"Missing"
D09-1106,W08-0303,0,0.0176702,"weights (Koehn et al., 2003) are also adapted for the weighted matrix case. Many researchers (e.g., (Venugopal et al., 2003; Deng et al., 2008)) observe that softening the alignment consistency constraint help improve translation quality. For example, Deng et al. (2008) define a feature named “within phrase pair consistency ratio” to measure the degree of consistency. As each link is associated with a probability in a weighted matrix, we use these probabilities to evaluate the validity of a phrase pair. We estimate the link probabilities by calculating relative frequencies over n-best lists. Niehues and Vogel (2008) propose a discriminative approach to modeling the alignment matrix directly. The difference is that they assign a boolean value instead of a probability to each word pair. 7 Conclusion and Future Work We have presented a new structure called weighted alignment matrix that encodes the alignment distribution for a sentence pair. Accordingly, we develop new methods for extracting phrase pairs and estimating their probabilities. Our experiments show that the proposed approach achieves better translation quality over using n-best lists in less extraction time. An interesting finding is that our ap"
D09-1106,P08-1115,0,0.0558642,"oparl data. Using weighted packed matrices continued to show advantage over using 1-best alignments on multiple language pairs. However, these improvements were very small and not significant. We attribute this to the fact that GIZA++ usually produces high quality 1-best alignments for closelyrelated European language pairs, especially when trained on millions of sentences. 6 Related Work Recent studies has shown that SMT systems can benefit from making the annotation pipeline wider: using packed forests instead of 1-best trees (Mi et al., 2008), word lattices instead of 1-best segmentations (Dyer et al., 2008), and n-best alignments instead of 1-best alignments (Venugopal et al., 2008). We propose a compact representation of multiple word alignments that enables SMT systems to make a better use of noisy alignments. Matusov et al. (2004) propose “cost matrices” for producing symmetric alignments. Kumar et al. (2007) describe how to use “posterior probability matrices” to improve alignment accuracy via a bridge language. Although not using the term ”weighted matrices” directly, they both assign a probability to each word pair. We follow Och and Ney (2004) to develop a new phrase extraction algorithm"
D09-1106,J07-3002,0,0.0312434,"Missing"
D09-1106,P06-1121,0,0.0474764,"pairs show that using weighted matrices achieves consistent improvements over using n-best lists in significant less extraction time. 1 Introduction Statistical machine translation (SMT) relies heavily on annotated bilingual corpora. Word alignment, which indicates the correspondence between the words in a parallel text, is one of the most important annotations in SMT. Word-aligned corpora have been found to be an excellent source for translation-related knowledge, not only for phrase-based models (Och and Ney, 2004; Koehn et al., 2003), but also for syntax-based models (e.g., (Chiang, 2007; Galley et al., 2006; Shen et al., 2008; Liu et al., 2006)). Och and Ney (2003) indicate that the quality of machine translation output depends directly on the quality of initial word alignment. Modern alignment methods can be divided into two major categories: generative methods and discriminative methods. Generative methods (Brown et al., 1993; Vogel and Ney, 1996) treat word alignment as a hidden process and maximize the likelihood of bilingual training corpus using the expectation maximization (EM) algorithm. In contrast, discriminative methods (e.g., (Moore et al., 2006; Taskar et al., 2005; Liu et al., 2005"
D09-1106,N03-1017,0,0.0408612,"Missing"
D09-1106,P07-2045,0,0.00353568,"in the training data. We Then, we adapt Eq. (4) to calculate lexical obtained n-best lists by selecting the top n alignweight: ments from the 550-best lists. The probability of  |˜ e| each alignment in the n-best list was re-estimated Y 1 e|f˜, m) = pw (˜ × by re-normalization (Venugopal et al., 2008). Fi{j|pm (j, i) &gt; 0} i=1 nally, these n-best alignments served as samples  X p(ei |fj ) × pm (j, i) + for constructing weighted alignment matrices. After extracting phrase pairs from n-best lists ∀j:pm (j,i)&gt;0 and weighted alignment matrices, we ran Moses ! |f˜| Y p(ei |f0 ) × p¯m (j, i) (16) (Koehn et al., 2007) to translate the development and test sets. We used the simple distance-based j=1 reordering model to remove the dependency of For example, for the target word “of” in Figure lexicalization on word alignments for Moses. 4, the sum of aligned and unaligned probabilities is 5.2 Effect of Pruning Threshold 1 × (p(of|de) × 0.6 + p(of|fazhan) × 0.4) + Our first experiment investigated the effect of 2 p(of|NULL) × 0.24 pruning threshold on translation quality (BLEU scores on the test set) and the phrase table size (filNote that we take link probabilities into account tered for the test set), as sho"
D09-1106,D07-1005,0,0.0209405,"language pairs, especially when trained on millions of sentences. 6 Related Work Recent studies has shown that SMT systems can benefit from making the annotation pipeline wider: using packed forests instead of 1-best trees (Mi et al., 2008), word lattices instead of 1-best segmentations (Dyer et al., 2008), and n-best alignments instead of 1-best alignments (Venugopal et al., 2008). We propose a compact representation of multiple word alignments that enables SMT systems to make a better use of noisy alignments. Matusov et al. (2004) propose “cost matrices” for producing symmetric alignments. Kumar et al. (2007) describe how to use “posterior probability matrices” to improve alignment accuracy via a bridge language. Although not using the term ”weighted matrices” directly, they both assign a probability to each word pair. We follow Och and Ney (2004) to develop a new phrase extraction algorithm for weighted alignment matrices. The methods for calculating relative frequencies (Och and Ney, 2004) and lexical weights (Koehn et al., 2003) are also adapted for the weighted matrix case. Many researchers (e.g., (Venugopal et al., 2003; Deng et al., 2008)) observe that softening the alignment consistency con"
D09-1106,P05-1057,1,0.854198,"Missing"
D09-1106,P06-1077,1,0.338522,"achieves consistent improvements over using n-best lists in significant less extraction time. 1 Introduction Statistical machine translation (SMT) relies heavily on annotated bilingual corpora. Word alignment, which indicates the correspondence between the words in a parallel text, is one of the most important annotations in SMT. Word-aligned corpora have been found to be an excellent source for translation-related knowledge, not only for phrase-based models (Och and Ney, 2004; Koehn et al., 2003), but also for syntax-based models (e.g., (Chiang, 2007; Galley et al., 2006; Shen et al., 2008; Liu et al., 2006)). Och and Ney (2003) indicate that the quality of machine translation output depends directly on the quality of initial word alignment. Modern alignment methods can be divided into two major categories: generative methods and discriminative methods. Generative methods (Brown et al., 1993; Vogel and Ney, 1996) treat word alignment as a hidden process and maximize the likelihood of bilingual training corpus using the expectation maximization (EM) algorithm. In contrast, discriminative methods (e.g., (Moore et al., 2006; Taskar et al., 2005; Liu et al., 2005; Blunsom and Cohn, 2006)) have the fr"
D09-1106,J03-1002,0,0.0181765,"Missing"
D09-1106,J04-4002,0,0.735347,"ted alignment matrices and estimating their probabilities. Our experiments on multiple language pairs show that using weighted matrices achieves consistent improvements over using n-best lists in significant less extraction time. 1 Introduction Statistical machine translation (SMT) relies heavily on annotated bilingual corpora. Word alignment, which indicates the correspondence between the words in a parallel text, is one of the most important annotations in SMT. Word-aligned corpora have been found to be an excellent source for translation-related knowledge, not only for phrase-based models (Och and Ney, 2004; Koehn et al., 2003), but also for syntax-based models (e.g., (Chiang, 2007; Galley et al., 2006; Shen et al., 2008; Liu et al., 2006)). Och and Ney (2003) indicate that the quality of machine translation output depends directly on the quality of initial word alignment. Modern alignment methods can be divided into two major categories: generative methods and discriminative methods. Generative methods (Brown et al., 1993; Vogel and Ney, 1996) treat word alignment as a hidden process and maximize the likelihood of bilingual training corpus using the expectation maximization (EM) algorithm. In c"
D09-1106,P02-1040,0,0.110018,"+ 8.9M words) as the training data. For language model, we used the SRI Language Modeling Toolkit (Stolcke, 2002) to train a 4-gram model on the Xinhua portion of GIGAWORD cor4.3 Calculating Lexical Weights pus. We used the NIST 2002 MT evaluation test Recall that we need to obtain two translation probset as our development set, and used the NIST ability tables w(e|f ) and w(f |e) before calculat2005 test set as our test set. We evaluated the transing lexical weights (see Section 2). Following lation quality using case-insensitive BLEU metric Koehn et al. (2003), we estimate the two distribu(Papineni et al., 2002). tions by relative frequencies from the training corTo obtain weighted alignment matrices, we folpus annotated with weighted alignment matrices. lowed Venugopal et al. (2008) to produce nIn other words, we still use Eq. (3) but the way of best lists via GIZA++. We first ran GIZA++ calculating fractional counts is different now. to produce 50-best lists in two translation direcGiven a source word fj , a target word ei , and tions. Then, we used the refinement technique a weighted alignment matrix, the fractional count “grow-diag-final-and” (Koehn et al., 2003) to all count(fj , ei ) is pm (j,"
D09-1106,P08-1066,0,0.0232111,"Missing"
D09-1106,H05-1010,0,0.123168,"Missing"
D09-1106,P03-1041,0,0.4864,"ov et al. (2004) propose “cost matrices” for producing symmetric alignments. Kumar et al. (2007) describe how to use “posterior probability matrices” to improve alignment accuracy via a bridge language. Although not using the term ”weighted matrices” directly, they both assign a probability to each word pair. We follow Och and Ney (2004) to develop a new phrase extraction algorithm for weighted alignment matrices. The methods for calculating relative frequencies (Och and Ney, 2004) and lexical weights (Koehn et al., 2003) are also adapted for the weighted matrix case. Many researchers (e.g., (Venugopal et al., 2003; Deng et al., 2008)) observe that softening the alignment consistency constraint help improve translation quality. For example, Deng et al. (2008) define a feature named “within phrase pair consistency ratio” to measure the degree of consistency. As each link is associated with a probability in a weighted matrix, we use these probabilities to evaluate the validity of a phrase pair. We estimate the link probabilities by calculating relative frequencies over n-best lists. Niehues and Vogel (2008) propose a discriminative approach to modeling the alignment matrix directly. The difference is that"
D09-1106,2008.amta-papers.18,0,0.505656,"ical weight can be calculated as pw (˜ e|f˜, a ˜) = |˜ e| Y X 1 w(ei |fj ) (4) |{j|(j, i) ∈ a ˜}| i=1 If there are multiple alignments a ˜ for a phrase ˜ pair (f , e˜), Koehn et al. (2003) choose the one with the highest lexical weight: n o pw (˜ e|f˜) = max pw (˜ e|f˜, a ˜) a ˜ (5) Simple and effective, relative frequencies and lexical weights have become the standard features in modern discriminative SMT systems. 3 Weighted Alignment Matrix We believe that offering more candidate alignments to extracting translation rules might help improve translation quality. Instead of using nbest lists (Venugopal et al., 2008), we propose a new structure called weighted alignment matrix. We use an example to illustrate our idea. Figure 2(a) and Figure 2(b) show two alignments of a Chinese-English sentence pair. We observe that some links (e.g., (1,4) corresponding to the word 1018 (b) fazhan fazhan (a) zhongguo de jingji of development the fazhan of development the economy 0 0 1.0 0 ’s 0 0.4 0.4 0 China 1.0 0 0 0 of 0 0.6 0 0.4 development 0 0 0 1.0 the 0 0 0 0 zhongguo de jingji economy ’s China zhongguo de jingji economy ’s China (c) Figure 2: (a) One alignment of a sentence pair; (b) another alignment of the sam"
D09-1106,C96-2141,0,0.611347,"Missing"
D09-1115,P08-1115,0,0.0751179,"eels like apples He prefer lations. Note that the phrase “is fond of” is attached to an edge. Now, it is unlikely to obtain a translation like “He is like of apples”. A lattice G = hV, Ei is a directed acyclic graph, formally a weighted finite state automation (FSA), where V is the set of nodes and E is the set of edges. The nodes in a lattice are usually labeled according to an appropriate numbering to reflect how to produce a translation. Each edge in a lattice is attached with a sequence of words as well as the associated probability. As lattice is a more general form of confusion network (Dyer et al., 2008), we expect that replacing confusion networks with lattices will further improve system combination. He feels like apples apples He prefer apples He feels like apples He feels like apples He is fond of apples He is fond of apples (a) unidirectional alignments (b) bidirectional alignments He ε feels prefer like of ε apples is fond (c) confusion network he ε feels prefer like apples is fond of (d) lattice 2.2 Figure 1: Comparison of a confusion network and a lattice. 2 Background 2.1 Confusion Network and Lattice We use an example shown in Figure 1 to illustrate our idea. Suppose that there are"
D09-1115,A94-1016,0,0.202214,"in which several words are connected to another several words. Instead, we propose a lattice-based system combination model that allows for such phrase alignments and uses lattices to encode all candidate translations. Experiments show that our approach achieves significant improvements over the state-ofthe-art baseline system on Chinese-to-English translation test sets. 1 Introduction System combination aims to find consensus translations among different machine translation systems. It has been proven that such consensus translations are usually better than the output of individual systems (Frederking and Nirenburg, 1994). In recent several years, the system combination methods based on confusion networks developed rapidly (Bangalore et al., 2001; Matusov et al., 2006; Sim et al., 2007; Rosti et al., 2007a; Rosti et al., 2007b; Rosti et al., 2008; He et al., 2008), which show state-of-the-art performance in benchmarks. A confusion network consists of a sequence of sets of candidate words. Each candidate word is associated with a score. The optimal consensus translation can be obtained by selecting one word from each set to maximizing the overall score. However, a confusion network is restricted in such a way t"
D09-1115,D08-1011,0,0.510095,"s significant improvements over the state-ofthe-art baseline system on Chinese-to-English translation test sets. 1 Introduction System combination aims to find consensus translations among different machine translation systems. It has been proven that such consensus translations are usually better than the output of individual systems (Frederking and Nirenburg, 1994). In recent several years, the system combination methods based on confusion networks developed rapidly (Bangalore et al., 2001; Matusov et al., 2006; Sim et al., 2007; Rosti et al., 2007a; Rosti et al., 2007b; Rosti et al., 2008; He et al., 2008), which show state-of-the-art performance in benchmarks. A confusion network consists of a sequence of sets of candidate words. Each candidate word is associated with a score. The optimal consensus translation can be obtained by selecting one word from each set to maximizing the overall score. However, a confusion network is restricted in such a way that only 1-to-1 mappings are allowed in hypothesis alignment. This is not the fact even for word alignments between the same languages. It is more common that several words are connected to another several words. For example, “be capable of” and “"
D09-1115,W07-0711,0,0.0521771,"s were collected as the input of the system combination. In our experiments, the weights were all tuned on the NIST MT02 Chinese-to-English test set, including 878 sentences, and the test data was the NIST MT05 Chinese-to-English test set, including 1082 sentences, except the experiments in Table 2. A 5gram language model was used which was trained on the XinHua portion of Gigaword corpus. The results were all reported in case sensitive BLEU score and the weights were tuned in Powell’s method to maximum BLEU score. The IHMM-based alignment module was implemented according to He et al. (2008), He (2007) and Vogel et al. (1996). In all experiments, the parameters for IHMM-based alignment module were set to: the smoothing factor for the surface similarity model, ρ = 3; the controlling factor for the distortion model, K = 2. 6.1 Comparison with Confusion-network-based model In order to compare the lattice-based system with the confusion-network-based system fairly, we used IHMM-based system combination model on behalf of the confusion-network-based model described in He et al. (2008). In both lattice-based and IHMMbased systems, the bilingual dictionaries were extracted on the FBIS data set whi"
D09-1115,W05-1506,0,0.0415363,"hich store more than one non-null word. The above two numbers are gotten by counting both backbone arcs and hypothesis arcs. α and β are the corresponding weights of the numbers, respectively. Nword (e) is the non-null word number and γ is its weight. Each arc has different confidences concerned with different systems, and the confidence of system s is denoted by ps (arc). ps (arc) is increased by 1/(k + 1) if the hypothesis ranking k in the system s contains the arc (Rosti et al., 2007a; He et al., 2008). Cube pruning algorithm with beam search is employed to search for the consensus output (Huang and Chiang, 2005). The nodes in the lattice are searched in a topological order and each node retains a list of N best candidate partial translations. 6 Experiments The candidate systems participating in the system combination are as listed in Table 1: System A is a BTG-based system using a MaxEnt-based reordering model; System B is a hierarchical phrase-based system; System C is the Moses decoder (Koehn et al., 2007); System D is a syntax-based system. 10best hypotheses from each candidate system on the dev and test sets were collected as the input of the system combination. In our experiments, the weights we"
D09-1115,N03-1017,0,0.027572,"he hypothesis with the minimum cost of edits against all hypotheses is selected. The backbone is significant for it influences not only the word order, but also the following alignments. The backbone is selected as follows: EB = argmin E ′ ∈E X T ER(E ′ , E) (8) E∈E (3) Get the alignments of the backbone and hypothesis pairs. First, each pair is aligned in both directions using the IHMM-based alignment method. In the IHMM alignment model, bilingual dictionaries in both directions are indispensable. Then, we apply a grow-diag-final algorithm which is widely used in bilingual phrase extraction (Koehn et al., 2003) to monolingual alignments. The bidirectional alignments are combined to one resorting to the grow-diag-final algorithm, allowing n-to-n mappings. (4)Normalize the alignment pairs. The word order of the backbone determines the word order of consensus outputs, so the word order of hypotheses must be consistent with that of the backbone. All words of a hypotheses are reordered according to the alignment to the backbone. For a word aligned to null, an actual null word may be inserted to the proper position. The alignment units are extracted first and then the hypothesis words in each unit are shi"
D09-1115,P07-2045,0,0.00971279,"1) if the hypothesis ranking k in the system s contains the arc (Rosti et al., 2007a; He et al., 2008). Cube pruning algorithm with beam search is employed to search for the consensus output (Huang and Chiang, 2005). The nodes in the lattice are searched in a topological order and each node retains a list of N best candidate partial translations. 6 Experiments The candidate systems participating in the system combination are as listed in Table 1: System A is a BTG-based system using a MaxEnt-based reordering model; System B is a hierarchical phrase-based system; System C is the Moses decoder (Koehn et al., 2007); System D is a syntax-based system. 10best hypotheses from each candidate system on the dev and test sets were collected as the input of the system combination. In our experiments, the weights were all tuned on the NIST MT02 Chinese-to-English test set, including 878 sentences, and the test data was the NIST MT05 Chinese-to-English test set, including 1082 sentences, except the experiments in Table 2. A 5gram language model was used which was trained on the XinHua portion of Gigaword corpus. The results were all reported in case sensitive BLEU score and the weights were tuned in Powell’s meth"
D09-1115,E06-1005,0,0.230719,"nts and uses lattices to encode all candidate translations. Experiments show that our approach achieves significant improvements over the state-ofthe-art baseline system on Chinese-to-English translation test sets. 1 Introduction System combination aims to find consensus translations among different machine translation systems. It has been proven that such consensus translations are usually better than the output of individual systems (Frederking and Nirenburg, 1994). In recent several years, the system combination methods based on confusion networks developed rapidly (Bangalore et al., 2001; Matusov et al., 2006; Sim et al., 2007; Rosti et al., 2007a; Rosti et al., 2007b; Rosti et al., 2008; He et al., 2008), which show state-of-the-art performance in benchmarks. A confusion network consists of a sequence of sets of candidate words. Each candidate word is associated with a score. The optimal consensus translation can be obtained by selecting one word from each set to maximizing the overall score. However, a confusion network is restricted in such a way that only 1-to-1 mappings are allowed in hypothesis alignment. This is not the fact even for word alignments between the same languages. It is more co"
D09-1115,P07-1040,0,0.376587,"idate translations. Experiments show that our approach achieves significant improvements over the state-ofthe-art baseline system on Chinese-to-English translation test sets. 1 Introduction System combination aims to find consensus translations among different machine translation systems. It has been proven that such consensus translations are usually better than the output of individual systems (Frederking and Nirenburg, 1994). In recent several years, the system combination methods based on confusion networks developed rapidly (Bangalore et al., 2001; Matusov et al., 2006; Sim et al., 2007; Rosti et al., 2007a; Rosti et al., 2007b; Rosti et al., 2008; He et al., 2008), which show state-of-the-art performance in benchmarks. A confusion network consists of a sequence of sets of candidate words. Each candidate word is associated with a score. The optimal consensus translation can be obtained by selecting one word from each set to maximizing the overall score. However, a confusion network is restricted in such a way that only 1-to-1 mappings are allowed in hypothesis alignment. This is not the fact even for word alignments between the same languages. It is more common that several words are connected"
D09-1115,N07-1029,0,0.250842,"ere collected as the input of the system combination. In our experiments, the weights were all tuned on the NIST MT02 Chinese-to-English test set, including 878 sentences, and the test data was the NIST MT05 Chinese-to-English test set, including 1082 sentences, except the experiments in Table 2. A 5gram language model was used which was trained on the XinHua portion of Gigaword corpus. The results were all reported in case sensitive BLEU score and the weights were tuned in Powell’s method to maximum BLEU score. The IHMM-based alignment module was implemented according to He et al. (2008), He (2007) and Vogel et al. (1996). In all experiments, the parameters for IHMM-based alignment module were set to: the smoothing factor for the surface similarity model, ρ = 3; the controlling factor for the distortion model, K = 2. 6.1 Comparison with Confusion-network-based model In order to compare the lattice-based system with the confusion-network-based system fairly, we used IHMM-based system combination model on behalf of the confusion-network-based model described in He et al. (2008). In both lattice-based and IHMMbased systems, the bilingual dictionaries were extracted on the FBIS data set whi"
D09-1115,W08-0329,0,0.156101,"our approach achieves significant improvements over the state-ofthe-art baseline system on Chinese-to-English translation test sets. 1 Introduction System combination aims to find consensus translations among different machine translation systems. It has been proven that such consensus translations are usually better than the output of individual systems (Frederking and Nirenburg, 1994). In recent several years, the system combination methods based on confusion networks developed rapidly (Bangalore et al., 2001; Matusov et al., 2006; Sim et al., 2007; Rosti et al., 2007a; Rosti et al., 2007b; Rosti et al., 2008; He et al., 2008), which show state-of-the-art performance in benchmarks. A confusion network consists of a sequence of sets of candidate words. Each candidate word is associated with a score. The optimal consensus translation can be obtained by selecting one word from each set to maximizing the overall score. However, a confusion network is restricted in such a way that only 1-to-1 mappings are allowed in hypothesis alignment. This is not the fact even for word alignments between the same languages. It is more common that several words are connected to another several words. For example, “be"
D09-1115,C96-2141,0,0.491588,"ed as the input of the system combination. In our experiments, the weights were all tuned on the NIST MT02 Chinese-to-English test set, including 878 sentences, and the test data was the NIST MT05 Chinese-to-English test set, including 1082 sentences, except the experiments in Table 2. A 5gram language model was used which was trained on the XinHua portion of Gigaword corpus. The results were all reported in case sensitive BLEU score and the weights were tuned in Powell’s method to maximum BLEU score. The IHMM-based alignment module was implemented according to He et al. (2008), He (2007) and Vogel et al. (1996). In all experiments, the parameters for IHMM-based alignment module were set to: the smoothing factor for the surface similarity model, ρ = 3; the controlling factor for the distortion model, K = 2. 6.1 Comparison with Confusion-network-based model In order to compare the lattice-based system with the confusion-network-based system fairly, we used IHMM-based system combination model on behalf of the confusion-network-based model described in He et al. (2008). In both lattice-based and IHMMbased systems, the bilingual dictionaries were extracted on the FBIS data set which included 289K sentenc"
D11-1081,D08-1023,0,0.040359,"(the ration between them is 1.5 to 1). Lexicalize operator is used more frequently mainly dues to that the reference derivations are initialized with reusable (thus 887 Minimum error rate training (Och, 2003) is perhaps the most popular discriminative training for SMT. However, it fails to scale to large number of features. Researchers have propose many learning algorithms to train many features: perceptron (Shen et al., 2004; Liang et al., 2006), minimum risk (Smith and Eisner, 2006; Li et al., 2009), MIRA (Watanabe et al., 2007; Chiang et al., 2009), gradient descent (Blunsom et al., 2008; Blunsom and Osborne, 2008). The complexity of n-best lists or packed forests generation hamper these algorithms to scale to a large amount of data. For efficiency, we only use neighboring derivations for training. Such motivation is same as contrastive estimation (Smith and Eisner, 2005; Poon et al., 2009). The difference lies in that the previous work actually care about their latent variables (pos tags, segmentation, dependency trees, etc), while we are only interested in their marginal distribution. Furthermore, we focus on how to fast generate translation forest for training. The local operators lexicalize/generali"
D11-1081,P08-1024,0,0.575199,"derivation tree with maximum score by the help of word alignment, and then traverse the tree to generate the subset forest in linear time. Besides the efficiency improvement, such a forest allows us to train the model without resortDiscriminative model (Och and Ney, 2002) can easily incorporate non-independent and overlapping features, and has been dominating the research field of statistical machine translation (SMT) in the last decade. Recent work have shown that SMT benefits a lot from exploiting large amount of features (Liang et al., 2006; Tillmann and Zhang, 2006; Watanabe et al., 2007; Blunsom et al., 2008; Chiang et al., 2009). However, the training of the large number of features was always restricted in fairly small data sets. Some systems limit the number of training examples, while others use short sentences to maintain efficiency. 1 Exactly, there are no reference derivations, since derivation Overfitting problem often comes when training is a latent variable in SMT. We call them reference derivation many features on a small data (Watanabe et al., just for convenience. 880 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 880–888, c Edinburgh, S"
D11-1081,P09-1088,0,0.271938,"ince rule table is fixed, the second step is a process of decoding. Therefore, sometimes we may fail to create a new reference derivation (like r2 may not exist in the rule table). In such case, we keep the origin derivation unchanged. The changes made by the two operators are local. Considering the change of rules, the lexicalize operator deletes two rules and adds one new rule, while the generalize operator deletes one rule and adds two new rules. Such local changes provide us with a way to incrementally calculate the scores of new derivations. We use this method motivated by Gibbs Sampler (Blunsom et al., 2009) which has been used for efficiently learning rules. The different lies in that we use the operator for decoding where the rule table is fixing. 4.2 Initialize a Reference Derivation The generation starts from an initial reference derivation with max score. This requires bi-parsing (Dyer, 2010) over the source sentence f and the reference translation e. In practice, we may face three problems. First is efficiency problem. Exhaustive search over the space under SCFG requires O(|f |3 |e|3 ). To parse quickly, we only visit the tight consistent (Zhang et al., 2008) bi-spans with the help of word"
D11-1081,P07-1005,0,0.21909,"h is called degenerate solution (DeNero et al., 2006). That is because the translation rules with rare source/target sides always receive a very high translation probability. We add a prior score log(#rule) for each rule, where #rule is the number of occurrence of a rule, to reward frequent reusable rules and derivations with more rules. Finally, we may fail to create reference derivations due to the limitation in rule extraction. We create minimum trees for (f , e, a) using shift-reduce (Zhang et al., 2008). Some minimum rules in the trees may be illegal according to the definition of Chiang (2007). We also add these rules to the rule table, so as to make sure every sentence is reachable given the rule table. A source sentence is reachable given a rule table if reference derivations exists. We refer these rules as added rules. However, this may introduce rules with more than two variables and increase the complexity of bi-parsing. To tackle this problem, we initialize the chart with minimum parallel tree from the Zhang et al. (2008) algorithm, ensuring that the bi-parsing has at least one path to create a reference derivation. Then we only need to consider the traditional rules during b"
D11-1081,N09-1025,0,0.449562,"unt of overlapping features, it is hard to scale up to large data due to decoding complexity. We propose a new algorithm to generate translation forest of training data in linear time with the help of word alignment. Our algorithm also alleviates the oracle selection problem by ensuring that a forest always contains derivations that exactly yield the reference translation. With millions of features trained on 519K sentences in 0.03 second per sentence, our system achieves significant improvement by 0.84 B LEU over the baseline system on the NIST Chinese-English test sets. 1 Introduction 2007; Chiang et al., 2009). Obviously, using much more data can alleviate such problem. Furthermore, large data also enables us to globally train millions of sparse lexical features which offer accurate clues for SMT. Despite these advantages, to the best of our knowledge, no previous discriminative training paradigms scale up to use a large amount of training data. The main obstacle comes from the complexity of packed forests or n-best lists generation which requires to search through all possible translations of each training example, which is computationally prohibitive in practice for SMT. To make normalization eff"
D11-1081,J07-2003,0,0.839921,"Overall, both the generation of forests and the training algorithm are scalable, enabling us to train millions of features on large-scale data. To show the effect of our framework, we globally train millions of word level context features motivated by word sense disambiguation (Chan et al., 2007) together with the features used in traditional SMT system (Section 6). Training on 519K sentence pairs in 0.03 seconds per sentence, we achieve significantly improvement over the traditional pipeline by 0.84 B LEU. 2 Synchronous Context Free Grammar We work on synchronous context free grammar (SCFG) (Chiang, 2007) based translation. The elementary structures in an SCFG are rewrite rules of the form: X ⇒ ⟨γ, α⟩ where γ and α are strings of terminals and nonterminals. We call γ and α as the source side and the target side of rule respectively. Here a rule means a phrase translation (Koehn et al., 2003) or a translation pair that contains nonterminals. We call a sequence of translation steps as a derivation. In context of SCFG, a derivation is a se881 quence of SCFG rules {ri }. Translation forest (Mi et al., 2008; Li and Eisner, 2009) is a compact representation of all the derivations for a given sentenc"
D11-1081,D09-1037,0,0.140873,"005; Poon et al., 2009). The difference lies in that the previous work actually care about their latent variables (pos tags, segmentation, dependency trees, etc), while we are only interested in their marginal distribution. Furthermore, we focus on how to fast generate translation forest for training. The local operators lexicalize/generalize are use for greedy decoding. The idea is related to “pegging” algorithm (Brown et al., 1993) and greedy decoding (Germann et al., 2001). Such types of local operators are also used in Gibbs sampler for synchronous grammar induction (Blunsom et al., 2009; Cohn and Blunsom, 2009). 8 Conclusion and Future Work We have presented a fast generation algorithm for translation forest which contains both reference derivations and neighboring non-reference derivations for large-scale SMT discriminative training. We have achieved significantly improvement of 0.84 B LEU by incorporate 13.9M feature trained on 519K data in 0.03 second per sentence. In this paper, we define the forest based on competing derivations which only differ in one rule. There may be better classes of forest that can produce a better performance. It’s interesting to modify the definition of forest, and use"
D11-1081,W02-1001,0,0.0671022,"s. We first create an initial reference derivation for every training examples using bi-parsing (lines 4-5), and then online learn the parameters using SGD (lines 6-12). We use the G ENERATE function to calculate the gradient. In practice, instead of storing all the derivations in a list, we traverse the tree twice. The first time is calculating the partition function, and the second time calculates the gradient normalized by partition function. During training, we also change the derivations (line 10). When training is finished after M epochs, the algorithm returns an averaged weight vector (Collins, 2002) to avoid overfitting (line 13). We use a development set to select total epoch m, which is set as M = 5 in our experiments. 6 Experiments Our method is able to train a large number of features on large data. We use a set of word context features motivated by word sense disambiguation (Chan et al., 2007) to test scalability. A word level context feature is a triple (f, e, f+1 ), which counts the number of time that f is aligned to e and f+1 occurs to the right of f . Triple (f, e, f−1 ) is similar except that f−1 locates to the left of f . We retain word alignment information in the extracted"
D11-1081,W06-3105,0,0.0168007,"). To parse quickly, we only visit the tight consistent (Zhang et al., 2008) bi-spans with the help of word alignment a. Only visiting tight consistent spans greatly speeds up bi-parsing. Besides efficiency, adoption of this constraint receives support from the fact that heuristic SCFG rule extraction only extracts tight consistent initial phrases (Chiang, 2007). Second is degenerate problem. If we only use the features as traditional SCFG systems, the biparsing may end with a derivation consists of some giant rules or rules with rare source/target sides, which is called degenerate solution (DeNero et al., 2006). That is because the translation rules with rare source/target sides always receive a very high translation probability. We add a prior score log(#rule) for each rule, where #rule is the number of occurrence of a rule, to reward frequent reusable rules and derivations with more rules. Finally, we may fail to create reference derivations due to the limitation in rule extraction. We create minimum trees for (f , e, a) using shift-reduce (Zhang et al., 2008). Some minimum rules in the trees may be illegal according to the definition of Chiang (2007). We also add these rules to the rule table, so"
D11-1081,N10-1033,0,0.161422,"ge of rules, the lexicalize operator deletes two rules and adds one new rule, while the generalize operator deletes one rule and adds two new rules. Such local changes provide us with a way to incrementally calculate the scores of new derivations. We use this method motivated by Gibbs Sampler (Blunsom et al., 2009) which has been used for efficiently learning rules. The different lies in that we use the operator for decoding where the rule table is fixing. 4.2 Initialize a Reference Derivation The generation starts from an initial reference derivation with max score. This requires bi-parsing (Dyer, 2010) over the source sentence f and the reference translation e. In practice, we may face three problems. First is efficiency problem. Exhaustive search over the space under SCFG requires O(|f |3 |e|3 ). To parse quickly, we only visit the tight consistent (Zhang et al., 2008) bi-spans with the help of word alignment a. Only visiting tight consistent spans greatly speeds up bi-parsing. Besides efficiency, adoption of this constraint receives support from the fact that heuristic SCFG rule extraction only extracts tight consistent initial phrases (Chiang, 2007). Second is degenerate problem. If we o"
D11-1081,P01-1030,0,0.0518157,"ata. For efficiency, we only use neighboring derivations for training. Such motivation is same as contrastive estimation (Smith and Eisner, 2005; Poon et al., 2009). The difference lies in that the previous work actually care about their latent variables (pos tags, segmentation, dependency trees, etc), while we are only interested in their marginal distribution. Furthermore, we focus on how to fast generate translation forest for training. The local operators lexicalize/generalize are use for greedy decoding. The idea is related to “pegging” algorithm (Brown et al., 1993) and greedy decoding (Germann et al., 2001). Such types of local operators are also used in Gibbs sampler for synchronous grammar induction (Blunsom et al., 2009; Cohn and Blunsom, 2009). 8 Conclusion and Future Work We have presented a fast generation algorithm for translation forest which contains both reference derivations and neighboring non-reference derivations for large-scale SMT discriminative training. We have achieved significantly improvement of 0.84 B LEU by incorporate 13.9M feature trained on 519K data in 0.03 second per sentence. In this paper, we define the forest based on competing derivations which only differ in one"
D11-1081,N03-1017,0,0.124393,"al., 2007) together with the features used in traditional SMT system (Section 6). Training on 519K sentence pairs in 0.03 seconds per sentence, we achieve significantly improvement over the traditional pipeline by 0.84 B LEU. 2 Synchronous Context Free Grammar We work on synchronous context free grammar (SCFG) (Chiang, 2007) based translation. The elementary structures in an SCFG are rewrite rules of the form: X ⇒ ⟨γ, α⟩ where γ and α are strings of terminals and nonterminals. We call γ and α as the source side and the target side of rule respectively. Here a rule means a phrase translation (Koehn et al., 2003) or a translation pair that contains nonterminals. We call a sequence of translation steps as a derivation. In context of SCFG, a derivation is a se881 quence of SCFG rules {ri }. Translation forest (Mi et al., 2008; Li and Eisner, 2009) is a compact representation of all the derivations for a given sentence under an SCFG (see Figure 1). A tree t in the forest corresponds to a derivation. In our paper, tree means the same as derivation. More formally, a forest is a pair ⟨V, E⟩, where V is the set of nodes, E is the set of hyperedge. For a given source sentence f = f1n , Each node v ∈ V is in t"
D11-1081,W04-3250,0,0.397179,"Missing"
D11-1081,D09-1005,0,0.0590317,"nous Context Free Grammar We work on synchronous context free grammar (SCFG) (Chiang, 2007) based translation. The elementary structures in an SCFG are rewrite rules of the form: X ⇒ ⟨γ, α⟩ where γ and α are strings of terminals and nonterminals. We call γ and α as the source side and the target side of rule respectively. Here a rule means a phrase translation (Koehn et al., 2003) or a translation pair that contains nonterminals. We call a sequence of translation steps as a derivation. In context of SCFG, a derivation is a se881 quence of SCFG rules {ri }. Translation forest (Mi et al., 2008; Li and Eisner, 2009) is a compact representation of all the derivations for a given sentence under an SCFG (see Figure 1). A tree t in the forest corresponds to a derivation. In our paper, tree means the same as derivation. More formally, a forest is a pair ⟨V, E⟩, where V is the set of nodes, E is the set of hyperedge. For a given source sentence f = f1n , Each node v ∈ V is in the form Xi,j , which denotes the recognition of nonterminal X spanning the substring from the i through j (that is fi+1 ...fj ). Each hyperedge e ∈ E connects a set of antecedent to a single consequent node and corresponds to an SCFG rul"
D11-1081,P09-1067,0,0.187943,"Missing"
D11-1081,P06-1096,0,0.26191,"Missing"
D11-1081,P08-1023,1,0.817451,"B LEU. 2 Synchronous Context Free Grammar We work on synchronous context free grammar (SCFG) (Chiang, 2007) based translation. The elementary structures in an SCFG are rewrite rules of the form: X ⇒ ⟨γ, α⟩ where γ and α are strings of terminals and nonterminals. We call γ and α as the source side and the target side of rule respectively. Here a rule means a phrase translation (Koehn et al., 2003) or a translation pair that contains nonterminals. We call a sequence of translation steps as a derivation. In context of SCFG, a derivation is a se881 quence of SCFG rules {ri }. Translation forest (Mi et al., 2008; Li and Eisner, 2009) is a compact representation of all the derivations for a given sentence under an SCFG (see Figure 1). A tree t in the forest corresponds to a derivation. In our paper, tree means the same as derivation. More formally, a forest is a pair ⟨V, E⟩, where V is the set of nodes, E is the set of hyperedge. For a given source sentence f = f1n , Each node v ∈ V is in the form Xi,j , which denotes the recognition of nonterminal X spanning the substring from the i through j (that is fi+1 ...fj ). Each hyperedge e ∈ E connects a set of antecedent to a single consequent node and corr"
D11-1081,P02-1038,0,0.342674,"erence” derivations that fail to produce the reference translation.1 However, the complexity of generating this translation forest is up to O(n6 ), because we still need biparsing to create the reference derivations. Consequently, we propose a method to fast generate a subset of the forest. The key idea (Section 4) is to initialize a reference derivation tree with maximum score by the help of word alignment, and then traverse the tree to generate the subset forest in linear time. Besides the efficiency improvement, such a forest allows us to train the model without resortDiscriminative model (Och and Ney, 2002) can easily incorporate non-independent and overlapping features, and has been dominating the research field of statistical machine translation (SMT) in the last decade. Recent work have shown that SMT benefits a lot from exploiting large amount of features (Liang et al., 2006; Tillmann and Zhang, 2006; Watanabe et al., 2007; Blunsom et al., 2008; Chiang et al., 2009). However, the training of the large number of features was always restricted in fairly small data sets. Some systems limit the number of training examples, while others use short sentences to maintain efficiency. 1 Exactly, there"
D11-1081,J03-1002,0,0.00856682,"Missing"
D11-1081,P03-1021,0,0.412179,"otivated by word sense disambiguation (Chan et al., 2007) to test scalability. A word level context feature is a triple (f, e, f+1 ), which counts the number of time that f is aligned to e and f+1 occurs to the right of f . Triple (f, e, f−1 ) is similar except that f−1 locates to the left of f . We retain word alignment information in the extracted rules to exploit such features. To demonstrate the importance of scaling up the size of training data and the effect of our method, we compare three types of training configurations which differ in the size of features and data. MERT. We use MERT (Och, 2003) to training 8 features on a small data. The 8 features is the same as Chiang (2007) including 4 rule scores (direct and reverse translation scores; direct and reverse lexical translation scores); 1 target side language model score; 3 penalties for word counts, extracted rules and glue rule. Actually, traditional pipeline often uses such configuration. Perceptron. We also learn thousands of context word features together with the 8 traditional features on a small data using perceptron. Following (Chiang et al., 2009), we only use 100 most frequent words for word context feature. This setting u"
D11-1081,P02-1040,0,0.081006,"Missing"
D11-1081,N09-1024,0,0.184056,"blem. Furthermore, large data also enables us to globally train millions of sparse lexical features which offer accurate clues for SMT. Despite these advantages, to the best of our knowledge, no previous discriminative training paradigms scale up to use a large amount of training data. The main obstacle comes from the complexity of packed forests or n-best lists generation which requires to search through all possible translations of each training example, which is computationally prohibitive in practice for SMT. To make normalization efficient, contrastive estimation (Smith and Eisner, 2005; Poon et al., 2009) introduce neighborhood for unsupervised log-linear model, and has presented positive results in various tasks. Motivated by these work, we use a translation forest (Section 3) which contains both “reference” derivations that potentially yield the reference translation and also neighboring “non-reference” derivations that fail to produce the reference translation.1 However, the complexity of generating this translation forest is up to O(n6 ), because we still need biparsing to create the reference derivations. Consequently, we propose a method to fast generate a subset of the forest. The key i"
D11-1081,N04-1023,0,0.0899572,"Missing"
D11-1081,P05-1044,0,0.416688,"a can alleviate such problem. Furthermore, large data also enables us to globally train millions of sparse lexical features which offer accurate clues for SMT. Despite these advantages, to the best of our knowledge, no previous discriminative training paradigms scale up to use a large amount of training data. The main obstacle comes from the complexity of packed forests or n-best lists generation which requires to search through all possible translations of each training example, which is computationally prohibitive in practice for SMT. To make normalization efficient, contrastive estimation (Smith and Eisner, 2005; Poon et al., 2009) introduce neighborhood for unsupervised log-linear model, and has presented positive results in various tasks. Motivated by these work, we use a translation forest (Section 3) which contains both “reference” derivations that potentially yield the reference translation and also neighboring “non-reference” derivations that fail to produce the reference translation.1 However, the complexity of generating this translation forest is up to O(n6 ), because we still need biparsing to create the reference derivations. Consequently, we propose a method to fast generate a subset of t"
D11-1081,P06-2101,0,0.1775,"on average, where 6.1 of them are reference derivations. Furthermore, we use lexicalize operator more often than generalize operator (the ration between them is 1.5 to 1). Lexicalize operator is used more frequently mainly dues to that the reference derivations are initialized with reusable (thus 887 Minimum error rate training (Och, 2003) is perhaps the most popular discriminative training for SMT. However, it fails to scale to large number of features. Researchers have propose many learning algorithms to train many features: perceptron (Shen et al., 2004; Liang et al., 2006), minimum risk (Smith and Eisner, 2006; Li et al., 2009), MIRA (Watanabe et al., 2007; Chiang et al., 2009), gradient descent (Blunsom et al., 2008; Blunsom and Osborne, 2008). The complexity of n-best lists or packed forests generation hamper these algorithms to scale to a large amount of data. For efficiency, we only use neighboring derivations for training. Such motivation is same as contrastive estimation (Smith and Eisner, 2005; Poon et al., 2009). The difference lies in that the previous work actually care about their latent variables (pos tags, segmentation, dependency trees, etc), while we are only interested in their marg"
D11-1081,P06-1091,0,0.0476138,"ey idea (Section 4) is to initialize a reference derivation tree with maximum score by the help of word alignment, and then traverse the tree to generate the subset forest in linear time. Besides the efficiency improvement, such a forest allows us to train the model without resortDiscriminative model (Och and Ney, 2002) can easily incorporate non-independent and overlapping features, and has been dominating the research field of statistical machine translation (SMT) in the last decade. Recent work have shown that SMT benefits a lot from exploiting large amount of features (Liang et al., 2006; Tillmann and Zhang, 2006; Watanabe et al., 2007; Blunsom et al., 2008; Chiang et al., 2009). However, the training of the large number of features was always restricted in fairly small data sets. Some systems limit the number of training examples, while others use short sentences to maintain efficiency. 1 Exactly, there are no reference derivations, since derivation Overfitting problem often comes when training is a latent variable in SMT. We call them reference derivation many features on a small data (Watanabe et al., just for convenience. 880 Proceedings of the 2011 Conference on Empirical Methods in Natural Langu"
D11-1081,D07-1080,0,0.779874,"initialize a reference derivation tree with maximum score by the help of word alignment, and then traverse the tree to generate the subset forest in linear time. Besides the efficiency improvement, such a forest allows us to train the model without resortDiscriminative model (Och and Ney, 2002) can easily incorporate non-independent and overlapping features, and has been dominating the research field of statistical machine translation (SMT) in the last decade. Recent work have shown that SMT benefits a lot from exploiting large amount of features (Liang et al., 2006; Tillmann and Zhang, 2006; Watanabe et al., 2007; Blunsom et al., 2008; Chiang et al., 2009). However, the training of the large number of features was always restricted in fairly small data sets. Some systems limit the number of training examples, while others use short sentences to maintain efficiency. 1 Exactly, there are no reference derivations, since derivation Overfitting problem often comes when training is a latent variable in SMT. We call them reference derivation many features on a small data (Watanabe et al., just for convenience. 880 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 8"
D11-1081,C08-1136,0,0.111034,"d motivated by Gibbs Sampler (Blunsom et al., 2009) which has been used for efficiently learning rules. The different lies in that we use the operator for decoding where the rule table is fixing. 4.2 Initialize a Reference Derivation The generation starts from an initial reference derivation with max score. This requires bi-parsing (Dyer, 2010) over the source sentence f and the reference translation e. In practice, we may face three problems. First is efficiency problem. Exhaustive search over the space under SCFG requires O(|f |3 |e|3 ). To parse quickly, we only visit the tight consistent (Zhang et al., 2008) bi-spans with the help of word alignment a. Only visiting tight consistent spans greatly speeds up bi-parsing. Besides efficiency, adoption of this constraint receives support from the fact that heuristic SCFG rule extraction only extracts tight consistent initial phrases (Chiang, 2007). Second is degenerate problem. If we only use the features as traditional SCFG systems, the biparsing may end with a derivation consists of some giant rules or rules with rare source/target sides, which is called degenerate solution (DeNero et al., 2006). That is because the translation rules with rare source/"
D11-1081,J93-2003,0,\N,Missing
D12-1046,W09-1207,0,0.0171353,"ing, we use the standard parseval evaluation metrics: bracketing precision, recall and F-score. 1 Note that the joint task refers to automatic segmentation and tagging/parsing. It can be achieved using a pipeline system or our joint decoding method. 507 4.3 Parameter Estimation We train three submodels using the gold features, that is, POS tagger is trained using the perfect segmentation, and parser is trained using perfect segmentation and POS tags. Some studies reported that better performance may be achieved by training subsequent models using representative output of the preceding models (Che et al., 2009). Hence for comparison we trained another parser using automatically generated POS tags obtained from 10-fold cross validation, but did not find significant difference between these two parsers when testing on the perfectly segmented development dataset. Therefore we use the parser trained with perfect POS tags for the joint task. Three hyper-parameters, α, β, and γ, are tuned on development data using a heuristic search. Parameters that achieved the best joint parsing result are selected. In the search, we fixed γ = 1 and varied α, β. First, we set β = 1, and enumerate α = 14 , 21 , 1, 2, . ."
D12-1046,W02-1001,0,0.0261493,"which is not the real scenario. In a fully automatic system, a pipeline approach is often adopted, where raw sentences are In this paper, we propose a unified model for joint Chinese word segmentation, POS tagging, and parsing. Three sub-models are independently trained using the state-of-the-art methods. We do not use the joint inference algorithm for training because of the high complexity caused by the large amount of parameters. We use linear chain Conditional Random Fields (CRFs) (Lafferty et al., 2001) to train the word segmentation model and POS tagging model, and averaged perceptron (Collins, 2002) to learn the parsing model. During decoding, parameters of each sub-model are scaled to represent its importance in the joint model. Our decoding algorithm is an extension of CYK parsing. Initially, weights of all possible words together with their POS tags are calculated. When searching the parse tree, the word and POS tagging features are dynamically generated and the transition information of POS tagging is considered in the span merge operation. Experiments are conducted on Chinese Tree Bank (CTB) 5 dataset, which is widely used for Chinese word segmentation, POS tagging and parsing. We c"
D12-1046,P08-1109,0,0.0330092,"Missing"
D12-1046,P08-1043,0,0.0484021,"Missing"
D12-1046,C10-1045,0,0.0196988,"Missing"
D12-1046,I11-1136,0,0.11213,"le 11 shows the increase and decrease of error patterns of the joint model over the pipeline POS tagger. An error pattern “X → Y” means that the word whose true tag is ‘X’ is assigned a tag ‘Y’. All the patterns are ranked in descending order of the reduction/increase of the error number. We can see that the joint model has a clear advantage in the disambiguation of {VV, NN} and {DEG, DEC}, which results in the overall improved performance. In contrast, the joint method performs worse on ambiguous POS pairs such as {N N, N R}. This observation is similar to those reported by (Li et al., 2011; Hatori et al., 2011). 5 Conclusion In this paper, we proposed a new algorithm for joint Chinese word segmentation, POS tagging, and parsing. Our algorithm is an extension of the CYK error pattern NN→ VV VV→ NN DEG→ DEC NN→ JJ DEC→ DEG JJ→ NN # 47 42 23 29 11 12 ↓ 19 13 10 8 4 4 error pattern NN→ NR NR→ NN JJ→ P NN→ DT P→ VV AD→ NN # 15 7 1 2 3 1 ↑ 12 5 4 4 2 2 Table 11: POS tagging error patterns. # means the error number of the corresponding pattern made by the pipeline tagging model. ↓ and ↑ mean the error number reduced or increased by the joint model. parsing method. The sub-models are independently trained f"
D12-1046,P08-1102,0,0.020394,"Missing"
D12-1046,C08-1049,0,0.0185417,"Missing"
D12-1046,P09-1059,0,0.122509,"Missing"
D12-1046,I08-4010,0,0.00994157,"vectors. For simplicity, we denote θseg = θCRF ⊕M E , fseg = fCRF ⊕M E , where θCRF ⊕M E means the concatenation of θCRF and θM E . Scores for single character words are defined similarly. These word scores will be used in the joint segmentation and parsing task Section 3.4. 3.2 POS Tagging Model Though syntax parsing model can directly predict the POS tag itself, we choose not to use this, but use an independent POS tagger for two reasons. First, 503 there is a large amount of data with labeled POS tags but no syntax annotations, such as the People’s Daily corpus and SIGHAN bakeoff corpora (Jin and Chen, 2008). Such data can only be used to train POS taggers, but not for training the parsing model. Often using a larger training set will result in a better POS tagger. Second, the state-of-the-art POS tagging systems are often trained by sequence labeling models, not parsing models. (2.1) wi−2 ti , wi−1 ti , wi ti , wi+1 ti , wi+2 ti (2.2) wi−2 wi−1 ti , wi−1 wi ti , wi wi+1 ti , wi+1 wi+2 ti wi−1 wi+1 ti (2.3) c1 (wi )ti , c2 (wi )ti , c3 (wi )ti , c−2 (wi )ti c−1 (wi )ti (2.4) c1 (wi )c2 (wi )ti , c−2 (wi )c−1 (wi )ti (2.5) l(wi )ti (2.5) ti−1 ti Table 2: Feature templates for POS tagging. wi is th"
D12-1046,P09-1058,0,0.0788275,"Missing"
D12-1046,P11-1089,0,0.0181351,"Missing"
D12-1046,D11-1109,0,0.0284723,"joint models. Table 11 shows the increase and decrease of error patterns of the joint model over the pipeline POS tagger. An error pattern “X → Y” means that the word whose true tag is ‘X’ is assigned a tag ‘Y’. All the patterns are ranked in descending order of the reduction/increase of the error number. We can see that the joint model has a clear advantage in the disambiguation of {VV, NN} and {DEG, DEC}, which results in the overall improved performance. In contrast, the joint method performs worse on ambiguous POS pairs such as {N N, N R}. This observation is similar to those reported by (Li et al., 2011; Hatori et al., 2011). 5 Conclusion In this paper, we proposed a new algorithm for joint Chinese word segmentation, POS tagging, and parsing. Our algorithm is an extension of the CYK error pattern NN→ VV VV→ NN DEG→ DEC NN→ JJ DEC→ DEG JJ→ NN # 47 42 23 29 11 12 ↓ 19 13 10 8 4 4 error pattern NN→ NR NR→ NN JJ→ P NN→ DT P→ VV AD→ NN # 15 7 1 2 3 1 ↑ 12 5 4 4 2 2 Table 11: POS tagging error patterns. # means the error number of the corresponding pattern made by the pipeline tagging model. ↓ and ↑ mean the error number reduced or increased by the joint model. parsing method. The sub-models are i"
D12-1046,N07-1051,0,0.0846223,"Missing"
D12-1046,D10-1019,1,0.846134,"wi−1 ti , wi−1 wi ti , wi wi+1 ti , wi+1 wi+2 ti wi−1 wi+1 ti (2.3) c1 (wi )ti , c2 (wi )ti , c3 (wi )ti , c−2 (wi )ti c−1 (wi )ti (2.4) c1 (wi )c2 (wi )ti , c−2 (wi )c−1 (wi )ti (2.5) l(wi )ti (2.5) ti−1 ti Table 2: Feature templates for POS tagging. wi is the ith word in the sentence, ti is its POS tag. For a word w, cj (w) is its j th character, c−j (w) is the last j th character, and l(w) is its length. The POS tagging problem is to assign a POS tag t ∈ T to each word in a sentence. We also use linear chain CRFs for POS tagging. Feature templates shown in Table 2 are the same as those in (Qian et al., 2010), which have been shown effective on CTB corpus. Three feature sets are considered: (i) word level features, including surrounding word unigrams, bigrams, and word length; (ii) character level features, such as the first and last characters in the words; (iii) transition features. 3.3 Parsing Model We choose discriminative models for parsing since it is easy to handle unknown words by simply adding character level features. Online structured learning algorithms were demonstrated to be effective for training, such as stochastic optimization (Finkel et al., 2008). In this study, we use averaged"
D12-1046,roark-etal-2006-sparseval,1,0.734695,"Missing"
D12-1046,P11-1139,0,0.177066,"Missing"
D12-1046,P08-1101,0,0.2155,"Missing"
D12-1046,D10-1082,0,0.213891,"Missing"
D12-1046,J11-1005,0,0.0325652,"firms our choice of using an independent POS tagger for the sub-model, rather than relying on a parser for POS tagging. However, since there are no reported results for this setup, we demonstrate the competence of our POS tagger using the joint word segmentation and POS tagging task. Table 8 shows the performance of a few systems along with ours, all using the pipeline approach where automatic segmentation is followed by POS tagging. We can see that our POS tagger is comparable to the others. System (Jiang et al., 2008b) (Jiang et al., 2008a) (Kruengkrai et al., 2009) (Zhang and Clark, 2010) (Zhang and Clark, 2011) (Sun, 2011) Ours (pipeline) P 93.28 93.10 R 94.07 93.96 F 93.37 93.41 93.67 93.67 93.67 94.02 93.53 Table 8: Results for the joint word segmentation and POS tagging task. For parsing, Table 9 presents the parsing result on gold standard segmented sentence. Notice that the result of (Harper and Huang, 2009; Zhang and Clark, 2011) are not directly comparable to ours, as they used a different data split. The best published system result on CTB5 is Petrov and Klein’s, which used PCFG with latent Variables. Our system performs better mainly because it benefits from a large amount of features. Syst"
D12-1046,P06-2123,0,0.0120639,"experimental results show that the joint model significantly outperforms the pipeline method based on the state-of-the-art sub-models. ual models independently during training and perform joint decoding using them. In this section, we first describe the three sub-models and then the joint decoding algorithm. 2 Methods for Chinese word segmentation can be broadly categorized into character based and word based models. Previous studies showed that character-based models are more effective to detect out-of-vocabulary words while word-based models are more accurate to predict in-vocabulary words (Zhang et al., 2006). Here, we use order-0 semiMarkov model (Sarawagi and Cohen, 2004) to take advantages of both approaches. More specifically, given a sentence x = c1 , c2 , . . . , cl (where ci is the ith Chinese character, l is the sentence length), the character-based model assigns each character with a word boundary tag. Here we use the BCDIES tag set, which achieved the best official performance (Zhao and Kit, 2008): B, C, D, E denote the first, second, third, and last character of a multi-character word respectively, I denotes the other characters, and S denotes the single character word. We use the same"
D12-1046,I08-4017,0,0.0115711,"based models. Previous studies showed that character-based models are more effective to detect out-of-vocabulary words while word-based models are more accurate to predict in-vocabulary words (Zhang et al., 2006). Here, we use order-0 semiMarkov model (Sarawagi and Cohen, 2004) to take advantages of both approaches. More specifically, given a sentence x = c1 , c2 , . . . , cl (where ci is the ith Chinese character, l is the sentence length), the character-based model assigns each character with a word boundary tag. Here we use the BCDIES tag set, which achieved the best official performance (Zhao and Kit, 2008): B, C, D, E denote the first, second, third, and last character of a multi-character word respectively, I denotes the other characters, and S denotes the single character word. We use the same characterbased feature templates as in the best official system, shown in Table 1 (1.1-1.3), including character unigram and bigram features, and transition features. Linear chain CRFs are used for training. Feature templates in the word-based model are shown in Table 1 (1.4-1.6), including word features, sub-word features, and character bigrams within words. The word feature is activated if a predicted"
D12-1109,P05-1022,0,0.0171017,"the vote) = f (r4 ) · 1 · lm(of the vote) NP r3 VP r5 r4 of the vote NN2 of NN1 was released at night Figure 4: The translation forest composed of applicable CFG rules for the partial derivation of step 3 in Figure 3. 3.4 Future Cost Partial derivations covering different tree nodes may be grouped in the same bin for beam pruning5 . In order to perform more accurate pruning, we take into consideration future cost, the cost of the uncovered part. The merit of a derivation is the covered cost (the cost of the covered part) plus the future cost. We borrow ideas from the Inside-Outside algorithm (Charniak and Johnson, 2005; Huang, 2008; Mi et al., 2008) to compute the merit. In our algorithm, the merit of a derivation is just the Viterbi inside cost β of the root node calculated with the derivations continuing from the current derivation. Given a partial derivation, we calculate its future cost by searching through the translation forest defined by all applicable CFG rules. Figure 4 shows the translation forest for the derivation of step 3. We calculate the future cost for each node as follows: given a node v, we define its cost function f (v) as   v is completed 1 f (v) = lm(v) v is a terminal string  ∏"
D12-1109,P05-1033,0,0.199485,"ompete actions until the next action is grow. The predict and grow actions decide which rules can be used to expand hypotheses next, so we update the applicable rule set during these two actions. Given a source sentence with n words, we maintain n beams, and let each beam hold b hypotheses 1197 at most. Besides, we prune viable prefixes of each node up to u, so each hypothesis can expand to u new hypotheses at most, so the time complexity of beam search is O(nub). 4 Related Work Watanabe et al. (2006) present a novel Earleystyle top-down decoding algorithm for hierarchical phrase-based model (Chiang, 2005). Their framework extracts Greibach Normal Form rules only, which always has at least one terminal on the left of each rule, and discards other rules. Dyer and Resnik (2010) describe a translation model that combines the merits of syntax-based models and phrase-based models. Their decoder works in two passes: for first pass, the decoder collects a context-free forest and performs tree-based source reordering without a LM. For the second pass, the decoder adds a LM and performs bottomup CKY decoding. Feng et al. (2010) proposed a shift-reduce algorithm to add BTG constraints to phrase-based mod"
D12-1109,J07-2003,0,0.371901,"ure consistent syntactic transformations between the source and target languages, e.g., from subject-verb-object to subject-object-verb word orderings. Decoding algorithms for grammar-based translation seek to find the best string in the intersection between a weighted context free grammar (the translation mode, given a source string/tree) and a weighted finite state acceptor (an n-gram language model). This intersection is problematic, as it results in an intractably large grammar, and makes exact search impossible. Most researchers have resorted to approximate search, typically beam search (Chiang, 2007). The decoder parses the source sentence, recording the target translations for each span.1 As the partial translation hypothesis grows, its component ngrams are scored and the hypothesis score is updated. This decoding method though is inefficient as it requires recording the language model context (n − 1 words) on the left and right edges of each chart cell. These contexts allow for boundary ngrams to be evaluated when the cell is used in another grammar production. In contrast, if the target string is generated in left-to-right order, then only one language model context is required, and th"
D12-1109,P05-1066,0,0.0607385,"5.2 Performance Comparison Our bottom-up left-to-right decoder employs the same features as the traditional decoder: rule probability, lexical probability, language model probability, rule count and word count. In order to compare them fairly, we used the same beam size which is 20 and employed cube pruning technique (Huang and Chiang, 2005). We show the results in Table 3. From the results, we can see that the bottom-up decoder outperforms top-down decoder and traditional decoder by 1.1 and 0.8 BLEU points respectively and the improvements are statistically significant using the sign-test of Collins et al. (2005) (p < 0.01). The improvement may result from dynamically searching for a whole derivation which leads to more accurate estimation of a partial derivation. The additional time consumption of the bottom-up decoder against the top-down decoder comes from dynamic future cost computation. Next we compare decoding speed versus translation quality using various beam sizes. The results are shown in Figure 5. We can see that our bottomup decoder can produce better BLEU score at the same decoding speed. At small beams (decoding time around 0.5 second), the improvement of translation quality is much bigg"
D12-1109,N10-1128,0,0.0532922,"e set during these two actions. Given a source sentence with n words, we maintain n beams, and let each beam hold b hypotheses 1197 at most. Besides, we prune viable prefixes of each node up to u, so each hypothesis can expand to u new hypotheses at most, so the time complexity of beam search is O(nub). 4 Related Work Watanabe et al. (2006) present a novel Earleystyle top-down decoding algorithm for hierarchical phrase-based model (Chiang, 2005). Their framework extracts Greibach Normal Form rules only, which always has at least one terminal on the left of each rule, and discards other rules. Dyer and Resnik (2010) describe a translation model that combines the merits of syntax-based models and phrase-based models. Their decoder works in two passes: for first pass, the decoder collects a context-free forest and performs tree-based source reordering without a LM. For the second pass, the decoder adds a LM and performs bottomup CKY decoding. Feng et al. (2010) proposed a shift-reduce algorithm to add BTG constraints to phrase-based models. This algorithm constructs a BTG tree in a reduce-eager manner while the algorithm in this paper searches for a best derivation which must be derived from the source tre"
D12-1109,P81-1022,0,0.741421,"nt Conference on Empirical Methods in Natural Language Processing and Computational Natural c Language Learning, pages 1191–1200, Jeju Island, Korea, 12–14 July 2012. 2012 Association for Computational Linguistics nodes, it is necessary to consider the cost of uncovered nodes, i.e., the future cost. We show that a good future cost estimate is essential for accurate and efficient search, leading to high quality translation output. Other researchers have also considered the leftto-right decoding algorithm for tree-to-string models. Huang and Mi (2010) developed an Earleystyle parsing algorithm (Earley, 1970). In their approach, hypotheses covering the same number of tree nodes were binned together. Their method uses a top-down depth-first search, with a mechanism for early elimation of some rules which lead to deadends in the search. Huang and Mi (2010)’s method was shown to outperform the traditional post-ordertraversal decoding algorithm, considering fewer hypotheses and thus decoding much faster at the same level of performance. However their algorithm used a very rough estimate of future cost, resulting in more search errors than our approach. Our experiments show that compared with the Earle"
D12-1109,C10-2033,1,0.88656,"ovel Earleystyle top-down decoding algorithm for hierarchical phrase-based model (Chiang, 2005). Their framework extracts Greibach Normal Form rules only, which always has at least one terminal on the left of each rule, and discards other rules. Dyer and Resnik (2010) describe a translation model that combines the merits of syntax-based models and phrase-based models. Their decoder works in two passes: for first pass, the decoder collects a context-free forest and performs tree-based source reordering without a LM. For the second pass, the decoder adds a LM and performs bottomup CKY decoding. Feng et al. (2010) proposed a shift-reduce algorithm to add BTG constraints to phrase-based models. This algorithm constructs a BTG tree in a reduce-eager manner while the algorithm in this paper searches for a best derivation which must be derived from the source tree. Galley and Manning (2008) use the shift-reduce algorithm to conduct hierarchical phrase reordering so as to capture long-distance reordering. This algorithm shows good performance on phrase-based models, but can not be applied to syntax-based models directly. 5 Experiments In the experiments, we use two baseline systems: our in-house tree-to-str"
D12-1109,D08-1089,0,0.0379223,"scribe a translation model that combines the merits of syntax-based models and phrase-based models. Their decoder works in two passes: for first pass, the decoder collects a context-free forest and performs tree-based source reordering without a LM. For the second pass, the decoder adds a LM and performs bottomup CKY decoding. Feng et al. (2010) proposed a shift-reduce algorithm to add BTG constraints to phrase-based models. This algorithm constructs a BTG tree in a reduce-eager manner while the algorithm in this paper searches for a best derivation which must be derived from the source tree. Galley and Manning (2008) use the shift-reduce algorithm to conduct hierarchical phrase reordering so as to capture long-distance reordering. This algorithm shows good performance on phrase-based models, but can not be applied to syntax-based models directly. 5 Experiments In the experiments, we use two baseline systems: our in-house tree-to-string decoder implemented according to Liu et al. (2006) (denoted as traditional) and the Earley-style top-down decoder implemented according to Huang and Mi (2010) (denoted as topdown), respectively. We compare our bottom-up left-to-right decoder (denoted as bottom-up) with the"
D12-1109,N04-1035,0,0.0554355,"t decoding (Huang and Mi, 2010) and the traditional post-order-traversal decoding (Liu et al., 2006) algorithms, our algorithm achieves a significant improvement on search capacity and better translation performance at the same level of speed. 2 Background A typical tree-to-string system (Liu et al., 2006; Huang et al., 2006) searches through a 1-best source parse tree for the best derivation. It transduces the source tree into a target-language string using a Synchronous Tree Substitution Grammar (STSG). The grammar rules are extracted from bilingual word alignments using the GHKM algorithm (Galley et al., 2004). We will briefly review the traditional decoding algorithm (Liu et al., 2006) and the Earley-style topdown decoding algorithm (Huang and Mi, 2010) for the tree-to-string model. 2.1 Traditional Decoding The traditional decoding algorithm processes source tree nodes one by one according to a post-order traversal. For each node, it applies matched STSG rules by substituting each non-terminal with its cor1192 traditional top-down bottom-up in theory O(nc˙|V |4(g−1) ) O(c(cr)d |V |g−1 ) O((cr)d |V |g−1 ) beam search O(ncb2 ) O(ncb) O(nub) Table 1: Time complexity of different algorithms. tradition"
D12-1109,W05-1506,0,0.043939,"Missing"
D12-1109,D10-1027,0,0.664834,"rding to a postorder traversal. 1191 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural c Language Learning, pages 1191–1200, Jeju Island, Korea, 12–14 July 2012. 2012 Association for Computational Linguistics nodes, it is necessary to consider the cost of uncovered nodes, i.e., the future cost. We show that a good future cost estimate is essential for accurate and efficient search, leading to high quality translation output. Other researchers have also considered the leftto-right decoding algorithm for tree-to-string models. Huang and Mi (2010) developed an Earleystyle parsing algorithm (Earley, 1970). In their approach, hypotheses covering the same number of tree nodes were binned together. Their method uses a top-down depth-first search, with a mechanism for early elimation of some rules which lead to deadends in the search. Huang and Mi (2010)’s method was shown to outperform the traditional post-ordertraversal decoding algorithm, considering fewer hypotheses and thus decoding much faster at the same level of performance. However their algorithm used a very rough estimate of future cost, resulting in more search errors than our a"
D12-1109,2006.amta-papers.8,0,0.277257,"algorithm, considering fewer hypotheses and thus decoding much faster at the same level of performance. However their algorithm used a very rough estimate of future cost, resulting in more search errors than our approach. Our experiments show that compared with the Earley-style left-to-right decoding (Huang and Mi, 2010) and the traditional post-order-traversal decoding (Liu et al., 2006) algorithms, our algorithm achieves a significant improvement on search capacity and better translation performance at the same level of speed. 2 Background A typical tree-to-string system (Liu et al., 2006; Huang et al., 2006) searches through a 1-best source parse tree for the best derivation. It transduces the source tree into a target-language string using a Synchronous Tree Substitution Grammar (STSG). The grammar rules are extracted from bilingual word alignments using the GHKM algorithm (Galley et al., 2004). We will briefly review the traditional decoding algorithm (Liu et al., 2006) and the Earley-style topdown decoding algorithm (Huang and Mi, 2010) for the tree-to-string model. 2.1 Traditional Decoding The traditional decoding algorithm processes source tree nodes one by one according to a post-order trav"
D12-1109,P08-1067,0,0.0222632,"(of the vote) NP r3 VP r5 r4 of the vote NN2 of NN1 was released at night Figure 4: The translation forest composed of applicable CFG rules for the partial derivation of step 3 in Figure 3. 3.4 Future Cost Partial derivations covering different tree nodes may be grouped in the same bin for beam pruning5 . In order to perform more accurate pruning, we take into consideration future cost, the cost of the uncovered part. The merit of a derivation is the covered cost (the cost of the covered part) plus the future cost. We borrow ideas from the Inside-Outside algorithm (Charniak and Johnson, 2005; Huang, 2008; Mi et al., 2008) to compute the merit. In our algorithm, the merit of a derivation is just the Viterbi inside cost β of the root node calculated with the derivations continuing from the current derivation. Given a partial derivation, we calculate its future cost by searching through the translation forest defined by all applicable CFG rules. Figure 4 shows the translation forest for the derivation of step 3. We calculate the future cost for each node as follows: given a node v, we define its cost function f (v) as   v is completed 1 f (v) = lm(v) v is a terminal string  ∏  maxr∈Rv f (r)"
D12-1109,J99-4005,0,0.212094,"e: 4 We bundle the successive terminals in one rule into a symbol 1195 grow [ IP] [NN2 ] −→ [ IP] [NN2  of the vote] From the above definition, we can find that there may be an ambiguity about whether to use a complete action or a grow action. Similarly, predict actions must select a viable prefix form the set for a node. For example in step 5, although we select to perform complete with r4 in the example, r7 is applicable, too. In our implementation, if both r4 and r7 are applicable, we apply them both to generate two seperate hypotheses. To limit the exponential explosion of hypotheses (Knight, 1999), we use beam search over bins of similar partial hypotheses (Koehn, 2004). IP r6 r4 , then r7 f (N P ) = f (r4 ) · f (N N2 ) · lm(of the vote) = f (r4 ) · 1 · lm(of the vote) NP r3 VP r5 r4 of the vote NN2 of NN1 was released at night Figure 4: The translation forest composed of applicable CFG rules for the partial derivation of step 3 in Figure 3. 3.4 Future Cost Partial derivations covering different tree nodes may be grouped in the same bin for beam pruning5 . In order to perform more accurate pruning, we take into consideration future cost, the cost of the uncovered part. The merit of a d"
D12-1109,koen-2004-pharaoh,0,0.674331,"[ IP] [NN2 ] −→ [ IP] [NN2  of the vote] From the above definition, we can find that there may be an ambiguity about whether to use a complete action or a grow action. Similarly, predict actions must select a viable prefix form the set for a node. For example in step 5, although we select to perform complete with r4 in the example, r7 is applicable, too. In our implementation, if both r4 and r7 are applicable, we apply them both to generate two seperate hypotheses. To limit the exponential explosion of hypotheses (Knight, 1999), we use beam search over bins of similar partial hypotheses (Koehn, 2004). IP r6 r4 , then r7 f (N P ) = f (r4 ) · f (N N2 ) · lm(of the vote) = f (r4 ) · 1 · lm(of the vote) NP r3 VP r5 r4 of the vote NN2 of NN1 was released at night Figure 4: The translation forest composed of applicable CFG rules for the partial derivation of step 3 in Figure 3. 3.4 Future Cost Partial derivations covering different tree nodes may be grouped in the same bin for beam pruning5 . In order to perform more accurate pruning, we take into consideration future cost, the cost of the uncovered part. The merit of a derivation is the covered cost (the cost of the covered part) plus the futu"
D12-1109,P06-1077,1,0.969992,"od uses a top-down depth-first search, with a mechanism for early elimation of some rules which lead to deadends in the search. Huang and Mi (2010)’s method was shown to outperform the traditional post-ordertraversal decoding algorithm, considering fewer hypotheses and thus decoding much faster at the same level of performance. However their algorithm used a very rough estimate of future cost, resulting in more search errors than our approach. Our experiments show that compared with the Earley-style left-to-right decoding (Huang and Mi, 2010) and the traditional post-order-traversal decoding (Liu et al., 2006) algorithms, our algorithm achieves a significant improvement on search capacity and better translation performance at the same level of speed. 2 Background A typical tree-to-string system (Liu et al., 2006; Huang et al., 2006) searches through a 1-best source parse tree for the best derivation. It transduces the source tree into a target-language string using a Synchronous Tree Substitution Grammar (STSG). The grammar rules are extracted from bilingual word alignments using the GHKM algorithm (Galley et al., 2004). We will briefly review the traditional decoding algorithm (Liu et al., 2006) a"
D12-1109,P08-1023,1,0.858066,"NP r3 VP r5 r4 of the vote NN2 of NN1 was released at night Figure 4: The translation forest composed of applicable CFG rules for the partial derivation of step 3 in Figure 3. 3.4 Future Cost Partial derivations covering different tree nodes may be grouped in the same bin for beam pruning5 . In order to perform more accurate pruning, we take into consideration future cost, the cost of the uncovered part. The merit of a derivation is the covered cost (the cost of the covered part) plus the future cost. We borrow ideas from the Inside-Outside algorithm (Charniak and Johnson, 2005; Huang, 2008; Mi et al., 2008) to compute the merit. In our algorithm, the merit of a derivation is just the Viterbi inside cost β of the root node calculated with the derivations continuing from the current derivation. Given a partial derivation, we calculate its future cost by searching through the translation forest defined by all applicable CFG rules. Figure 4 shows the translation forest for the derivation of step 3. We calculate the future cost for each node as follows: given a node v, we define its cost function f (v) as   v is completed 1 f (v) = lm(v) v is a terminal string  ∏  maxr∈Rv f (r) π∈rhs(r) f (π) ot"
D12-1109,J03-1002,0,0.00491025,"topdown), respectively. We compare our bottom-up left-to-right decoder (denoted as bottom-up) with the baseline in terms of performance, translation quality and decoding speed with different beam sizes, and search capacity. Lastly, we show the influence of future cost. All systems are implemented in C++. System Traditional Top-down Bottom-up 5.1 Data Setup Time (s) 0.84 0.41 0.81 Table 3: Performance comparison. 30.8 30.6 30.4 BLEU Score We used the FBIS corpus consisting of about 250K Chinese-English sentence pairs as the training set. We aligned the sentence pairs using the GIZA++ toolkit (Och and Ney, 2003) and extracted tree-tostring rules according to the GHKM algorithm (Galley et al., 2004). We used the SRILM toolkit (Stolcke, 2002) to train a 4-gram language model on the Xinhua portion of the GIGAWORD corpus. We used the 2002 NIST MT Chinese-English test set (571 sentences) as the development set and the 2005 NIST MT Chinese-English test set (1082 sentences) as the test set. We evaluated translation quality using BLEU-metric (Papineni et al., 2002) with case-insensitive n-gram matching up to n = 4. We used the standard minimum error rate training (Och, 2003) to tune feature weights to maximi"
D12-1109,P03-1021,0,0.0478665,"Missing"
D12-1109,P02-1040,0,0.0894745,"Missing"
D12-1109,P06-1098,0,0.566743,"as “closure” actions. That is to say, once there are some complete actions after a scan action, we finish all the compete actions until the next action is grow. The predict and grow actions decide which rules can be used to expand hypotheses next, so we update the applicable rule set during these two actions. Given a source sentence with n words, we maintain n beams, and let each beam hold b hypotheses 1197 at most. Besides, we prune viable prefixes of each node up to u, so each hypothesis can expand to u new hypotheses at most, so the time complexity of beam search is O(nub). 4 Related Work Watanabe et al. (2006) present a novel Earleystyle top-down decoding algorithm for hierarchical phrase-based model (Chiang, 2005). Their framework extracts Greibach Normal Form rules only, which always has at least one terminal on the left of each rule, and discards other rules. Dyer and Resnik (2010) describe a translation model that combines the merits of syntax-based models and phrase-based models. Their decoder works in two passes: for first pass, the decoder collects a context-free forest and performs tree-based source reordering without a LM. For the second pass, the decoder adds a LM and performs bottomup CK"
D13-1047,D10-1047,0,0.0959622,"h has seen great development over the last fifty years (Nenkova and McKeown, 2011). Compared to the abstractive counterpart, extractive summarization has received considerable attention due to its clear problem formulation – to extract a set of salient and non-redundant sentences from the given document set. Both unsupervised and supervised approaches have been explored for sentence selection. The supervised approaches include the Bayesian classifier (Kupiec et al., 1995), maximum entropy (Osborne, 2002), skip-chain condi491 tional random fields (CRF) (Galley, 2006), discriminative reranking (Aker et al., 2010), among others. The extractive summary sentence selection problem can also be formulated in an optimization framework. Previous approaches include the integer linear programming (ILP) and submodular functions, which are used to solve the optimization problem. In particular, Gillick et al. (2009) proposed a concept-based ILP approach for summarization. Li et al. (2013) improved it by using supervised stragety to estimate concept weight in ILP framework. In (Lin and Bilmes, 2010), the authors model the sentence selection problem as maximizing a submodular function under a budget constraint. A gr"
D13-1047,P13-1020,0,0.137564,"constituents and make space for more salient information that is otherwise dropped due to the summary length constraint. Two general strategies have been used for compressive summarization. One is a pipeline approach, where sentence-based extractive summarization is followed or proceeded by sentence compression (Knight and Marcu, 2000; Lin, 2003; Zajic et al., 2007; Wang et al., 2013). Another line of work uses joint compression and summarization. They have been shown to achieve promising performance (Daum´e, 2006; Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011; Chali and Hasan, 2012; Almeida and Martins, 2013; Qian and Liu, 2013). One popular approach for such joint compression and summarization is via integer linear programming (ILP). However, since words are the units in the optimization framework, solving this ILP problem can be expensive. In this study, we use the pipeline compression and summarization method because of its computational efficiency. Prior work using such pipeline methods simply uses generic sentence-based compression for each sentence in the documents, no matter whether compression is done before or after summary sentence extraction. We propose to use sum490 Proceedings of the"
D13-1047,P11-1049,0,0.810124,"ummaries since they can remove insignificant sentence constituents and make space for more salient information that is otherwise dropped due to the summary length constraint. Two general strategies have been used for compressive summarization. One is a pipeline approach, where sentence-based extractive summarization is followed or proceeded by sentence compression (Knight and Marcu, 2000; Lin, 2003; Zajic et al., 2007; Wang et al., 2013). Another line of work uses joint compression and summarization. They have been shown to achieve promising performance (Daum´e, 2006; Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011; Chali and Hasan, 2012; Almeida and Martins, 2013; Qian and Liu, 2013). One popular approach for such joint compression and summarization is via integer linear programming (ILP). However, since words are the units in the optimization framework, solving this ILP problem can be expensive. In this study, we use the pipeline compression and summarization method because of its computational efficiency. Prior work using such pipeline methods simply uses generic sentence-based compression for each sentence in the documents, no matter whether compression is done before or after summary sentence extra"
D13-1047,C12-1029,0,0.553418,"insignificant sentence constituents and make space for more salient information that is otherwise dropped due to the summary length constraint. Two general strategies have been used for compressive summarization. One is a pipeline approach, where sentence-based extractive summarization is followed or proceeded by sentence compression (Knight and Marcu, 2000; Lin, 2003; Zajic et al., 2007; Wang et al., 2013). Another line of work uses joint compression and summarization. They have been shown to achieve promising performance (Daum´e, 2006; Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011; Chali and Hasan, 2012; Almeida and Martins, 2013; Qian and Liu, 2013). One popular approach for such joint compression and summarization is via integer linear programming (ILP). However, since words are the units in the optimization framework, solving this ILP problem can be expensive. In this study, we use the pipeline compression and summarization method because of its computational efficiency. Prior work using such pipeline methods simply uses generic sentence-based compression for each sentence in the documents, no matter whether compression is done before or after summary sentence extraction. We propose to us"
D13-1047,N10-1131,0,0.0191521,"(2013) propose a graph-cut based method that improves the speed of joint compression and summarization. The pipeline approach, where sentence-based extractive summarization is followed or proceeded by sentence compression, is also popular. Knight and Marcu (2000) utilize the noisy channel and decision tree method to perform sentence compression; Lin (2003) shows that pure syntactic-based compression may not improve the system performance; Zajic et al. (2007) compare two sentence compression approaches for multi-document summarization, including a ‘parse-and-trim’ and a noisy-channel approach; Galanis and Androutsopoulos (2010) use the maximum entropy model to generate the candidate compressions by removing the branches from the source sentences; Liu and Liu (2013) couple the sentence compression and extraction approaches for summarizing the spoken documents; Wang et al. (2013) design a series of learning-based compression models built on parse trees, and integrate them in query-focused multi-document summarization. Prior studies often rely heavily on the generic sentence compression approaches (McDonald, 2006; Nomoto, 2007; Clarke and Lapata, 2008; Thadani and McKeown, 2013) for compressing the sentences in the doc"
D13-1047,W06-1643,0,0.0344792,"lts. 2 Related Work Summarization research has seen great development over the last fifty years (Nenkova and McKeown, 2011). Compared to the abstractive counterpart, extractive summarization has received considerable attention due to its clear problem formulation – to extract a set of salient and non-redundant sentences from the given document set. Both unsupervised and supervised approaches have been explored for sentence selection. The supervised approaches include the Bayesian classifier (Kupiec et al., 1995), maximum entropy (Osborne, 2002), skip-chain condi491 tional random fields (CRF) (Galley, 2006), discriminative reranking (Aker et al., 2010), among others. The extractive summary sentence selection problem can also be formulated in an optimization framework. Previous approaches include the integer linear programming (ILP) and submodular functions, which are used to solve the optimization problem. In particular, Gillick et al. (2009) proposed a concept-based ILP approach for summarization. Li et al. (2013) improved it by using supervised stragety to estimate concept weight in ILP framework. In (Lin and Bilmes, 2010), the authors model the sentence selection problem as maximizing a submo"
D13-1047,P13-1099,1,0.185695,"for sentence selection. The supervised approaches include the Bayesian classifier (Kupiec et al., 1995), maximum entropy (Osborne, 2002), skip-chain condi491 tional random fields (CRF) (Galley, 2006), discriminative reranking (Aker et al., 2010), among others. The extractive summary sentence selection problem can also be formulated in an optimization framework. Previous approaches include the integer linear programming (ILP) and submodular functions, which are used to solve the optimization problem. In particular, Gillick et al. (2009) proposed a concept-based ILP approach for summarization. Li et al. (2013) improved it by using supervised stragety to estimate concept weight in ILP framework. In (Lin and Bilmes, 2010), the authors model the sentence selection problem as maximizing a submodular function under a budget constraint. A greedy algorithm is proposed to efficiently approximate the solution to this NP-hard problem. Compressive summarization receives increasing attention in recent years, since it offers a viable step towards abstractive summarization. The compressed summaries can be generated through a joint model of the sentence selection and compression processes, or through a pipeline a"
D13-1047,N10-1134,0,0.166154,"aximum entropy (Osborne, 2002), skip-chain condi491 tional random fields (CRF) (Galley, 2006), discriminative reranking (Aker et al., 2010), among others. The extractive summary sentence selection problem can also be formulated in an optimization framework. Previous approaches include the integer linear programming (ILP) and submodular functions, which are used to solve the optimization problem. In particular, Gillick et al. (2009) proposed a concept-based ILP approach for summarization. Li et al. (2013) improved it by using supervised stragety to estimate concept weight in ILP framework. In (Lin and Bilmes, 2010), the authors model the sentence selection problem as maximizing a submodular function under a budget constraint. A greedy algorithm is proposed to efficiently approximate the solution to this NP-hard problem. Compressive summarization receives increasing attention in recent years, since it offers a viable step towards abstractive summarization. The compressed summaries can be generated through a joint model of the sentence selection and compression processes, or through a pipeline approach that integrates a generic sentence compression model with a summary sentence pre-selection or post-selec"
D13-1047,W03-1101,0,0.0326955,"rest in recent years on generating compressed document summaries as a viable step towards abstractive summarization. These compressive summaries often contain more information than sentence-based extractive summaries since they can remove insignificant sentence constituents and make space for more salient information that is otherwise dropped due to the summary length constraint. Two general strategies have been used for compressive summarization. One is a pipeline approach, where sentence-based extractive summarization is followed or proceeded by sentence compression (Knight and Marcu, 2000; Lin, 2003; Zajic et al., 2007; Wang et al., 2013). Another line of work uses joint compression and summarization. They have been shown to achieve promising performance (Daum´e, 2006; Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011; Chali and Hasan, 2012; Almeida and Martins, 2013; Qian and Liu, 2013). One popular approach for such joint compression and summarization is via integer linear programming (ILP). However, since words are the units in the optimization framework, solving this ILP problem can be expensive. In this study, we use the pipeline compression and summarization method because of"
D13-1047,W04-1013,0,0.631382,"oint modeling studies. Using our created compression data, we train a supervised compression model using a variety of word-, sentence-, and document-level features. During summarization, we generate multiple compression candidates for each sentence, and use the ILP framework to select compressed summary sentences. In addition, we also propose to apply a preselection step to select some important sentences, which can both speed up the summarization system and improve performance. We evaluate our proposed summarization approach on the TAC 2008 and 2011 data sets using the standard ROUGE metric (Lin, 2004). Our results show that by incorporating a guided sentence compression model, our summarization system can yield significant performance gain as compared to the state-of-the-art reported results. 2 Related Work Summarization research has seen great development over the last fifty years (Nenkova and McKeown, 2011). Compared to the abstractive counterpart, extractive summarization has received considerable attention due to its clear problem formulation – to extract a set of salient and non-redundant sentences from the given document set. Both unsupervised and supervised approaches have been expl"
D13-1047,P09-2066,1,0.644677,"Missing"
D13-1047,W09-1801,0,0.668396,"ntence-based extractive summaries since they can remove insignificant sentence constituents and make space for more salient information that is otherwise dropped due to the summary length constraint. Two general strategies have been used for compressive summarization. One is a pipeline approach, where sentence-based extractive summarization is followed or proceeded by sentence compression (Knight and Marcu, 2000; Lin, 2003; Zajic et al., 2007; Wang et al., 2013). Another line of work uses joint compression and summarization. They have been shown to achieve promising performance (Daum´e, 2006; Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011; Chali and Hasan, 2012; Almeida and Martins, 2013; Qian and Liu, 2013). One popular approach for such joint compression and summarization is via integer linear programming (ILP). However, since words are the units in the optimization framework, solving this ILP problem can be expensive. In this study, we use the pipeline compression and summarization method because of its computational efficiency. Prior work using such pipeline methods simply uses generic sentence-based compression for each sentence in the documents, no matter whether compression is done before"
D13-1047,E06-1038,0,0.0976203,"ulti-document summarization, including a ‘parse-and-trim’ and a noisy-channel approach; Galanis and Androutsopoulos (2010) use the maximum entropy model to generate the candidate compressions by removing the branches from the source sentences; Liu and Liu (2013) couple the sentence compression and extraction approaches for summarizing the spoken documents; Wang et al. (2013) design a series of learning-based compression models built on parse trees, and integrate them in query-focused multi-document summarization. Prior studies often rely heavily on the generic sentence compression approaches (McDonald, 2006; Nomoto, 2007; Clarke and Lapata, 2008; Thadani and McKeown, 2013) for compressing the sentences in the documents, yet a generic compression system may not be the best fit for the summarization purpose. In this paper, we adopt the pipeline-based compressive summarization framework, but propose a novel guided compression method that is catered to the summarization task. We expect this approach to take advantage of the efficient pipeline processing while producing satisfying results as the joint models. We train a supervised guided compression model to produce n-best compressions for each sente"
D13-1047,C12-1128,0,0.124461,"The above ILP method can offer an exact solution to the defined objective function. However, ILP is computationally expensive when the formulation involves large quantities of variables, i.e, when we have many sentences and a large number of candidate compressions for each sentence. We therefore propose to apply a sentence pre-selection step before the compression. This kind of selection step has been used in previous ILP-based summarization systems (Berg-Kirkpatrick et al., 2011; Gillick et al., 2009). In this work, we propose to use a simple supervised support vector regression (SVR) model (Ng et al., 2012) to predict a salience score for each sentence and select the top ranked sentences for further processing (compression and summarization). To train the SVR model, the target value for each sentence is the ROUGE-2 score between the sentence and the four human abstracts (this same value is used for sentence selection in corpus annotation (Section 3)). We employ three commonly used features: (1) sentence position in the document; (2) sentence length as indicated by a binary feature: it takes the value of 0 if the number of words in the sentence is greater than 50 or less than 10, otherwise the fe"
D13-1047,W02-0401,0,0.0970346,"rformance gain as compared to the state-of-the-art reported results. 2 Related Work Summarization research has seen great development over the last fifty years (Nenkova and McKeown, 2011). Compared to the abstractive counterpart, extractive summarization has received considerable attention due to its clear problem formulation – to extract a set of salient and non-redundant sentences from the given document set. Both unsupervised and supervised approaches have been explored for sentence selection. The supervised approaches include the Bayesian classifier (Kupiec et al., 1995), maximum entropy (Osborne, 2002), skip-chain condi491 tional random fields (CRF) (Galley, 2006), discriminative reranking (Aker et al., 2010), among others. The extractive summary sentence selection problem can also be formulated in an optimization framework. Previous approaches include the integer linear programming (ILP) and submodular functions, which are used to solve the optimization problem. In particular, Gillick et al. (2009) proposed a concept-based ILP approach for summarization. Li et al. (2013) improved it by using supervised stragety to estimate concept weight in ILP framework. In (Lin and Bilmes, 2010), the aut"
D13-1047,N07-1051,0,0.0111604,"rds and the current word. • POS n-grams: same as the word n-grams, but use the part-of-speech tags instead. • Named entity tags: binary features representing whether the current word is a person, location, or temporal expression. We use the Stanford CoreNLP tools3 for named entity tagging. • Stopwords: whether the current word is a stopword or not. • Conjunction features: (1) conjunction of the current word with its relative position in the sentence; (2) conjunction of the NER tag with its relative position. • Syntactic features: We obtain the syntactic parsing tree using the Berkeley Parser (Petrov and Klein, 2007), then obtain the following features: (1) the last sentence constituent tag in the path from the root to the word; (2) depth: length of the path starting from the root node to the word; (3) normalized depth: depth divided by the longest path in the parsing tree; (4) whether the word is under an SBAR node; (5) depth and normalized depth of the SBAR node if the word is under an SBAR node; • Dependency features: We employ the Penn2Malt toolkit 4 to convert the parse result from the Berkeley parser to the dependency parsing tree, and use these dependency 3 4 http://nlp.stanford.edu/software/corenl"
D13-1047,D13-1156,1,0.64139,"for more salient information that is otherwise dropped due to the summary length constraint. Two general strategies have been used for compressive summarization. One is a pipeline approach, where sentence-based extractive summarization is followed or proceeded by sentence compression (Knight and Marcu, 2000; Lin, 2003; Zajic et al., 2007; Wang et al., 2013). Another line of work uses joint compression and summarization. They have been shown to achieve promising performance (Daum´e, 2006; Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011; Chali and Hasan, 2012; Almeida and Martins, 2013; Qian and Liu, 2013). One popular approach for such joint compression and summarization is via integer linear programming (ILP). However, since words are the units in the optimization framework, solving this ILP problem can be expensive. In this study, we use the pipeline compression and summarization method because of its computational efficiency. Prior work using such pipeline methods simply uses generic sentence-based compression for each sentence in the documents, no matter whether compression is done before or after summary sentence extraction. We propose to use sum490 Proceedings of the 2013 Conference on E"
D13-1047,J02-4001,0,0.0756133,"Missing"
D13-1047,W13-3508,0,0.478243,"m’ and a noisy-channel approach; Galanis and Androutsopoulos (2010) use the maximum entropy model to generate the candidate compressions by removing the branches from the source sentences; Liu and Liu (2013) couple the sentence compression and extraction approaches for summarizing the spoken documents; Wang et al. (2013) design a series of learning-based compression models built on parse trees, and integrate them in query-focused multi-document summarization. Prior studies often rely heavily on the generic sentence compression approaches (McDonald, 2006; Nomoto, 2007; Clarke and Lapata, 2008; Thadani and McKeown, 2013) for compressing the sentences in the documents, yet a generic compression system may not be the best fit for the summarization purpose. In this paper, we adopt the pipeline-based compressive summarization framework, but propose a novel guided compression method that is catered to the summarization task. We expect this approach to take advantage of the efficient pipeline processing while producing satisfying results as the joint models. We train a supervised guided compression model to produce n-best compressions for each sentence, and use an ILP formulation to select the best set of summary s"
D13-1047,P05-1036,0,0.0137398,"ssion model using our created compression data, with a variety of features.then we use this model to generate n-best compressions for each sentence; we feed the multiple compressed sentences to the ILP framework to select the best summary sentences. In addition, we propose a sentence pre-selection step that can both speed up the summarization system and improve the performance. 4.1 Guided Sentence Compression Sentence compression has been explored in previous studies using both supervised and unsupervised approaches, including the noisy-channel and decision tree model (Knight and Marcu, 2000; Turner and Charniak, 2005), discriminative learning (McDonald, 2006), integer linear programming (Clarke and Lapata, 2008; Thadani and McKeown, 2013), conditional random fields (CRF) (Nomoto, 2007; Liu and Liu, 2013), etc. In this paper, we employ the CRF-based compression approach due to its proved performance and its flexibility to integrate different levels of discriminative features. Under this framework, sentence compression is formulated as a sequence labeling problem, where each word is labeled as either “0” (retained) or “1” (removed). We develop different levels of features to capture word-specific characteris"
D13-1047,P07-1070,0,0.0111622,"tators were explicitly informed about the important summary words during the compression annotation. We then train a supervised compression model to capture the guided compression process using a set of word-, sentence-, and document-level features. We conduct experiments on the TAC 2008 and 2011 summarization data sets and show that by incorporating the guided sentence compression model, our summarization system can yield significant performance gain as compared to the state-of-the-art. In future, we would like to further explore the reinforcement relationship between keywords and summaries (Wan et al., 2007), improve the readability of the sentences generated from the guided compression system, and report results using multiple evaluation metrics (Nenkova et al., 2007; Louis and Nenkova, 2012) as well as performing human evaluations. Acknowledgments Part of this work was done during the first author’s internship in Bosch Research and Technology Center. The work is also partially supported by NSF award IIS-0845484 and DARPA Contract No. FA8750-13-2-0041. Any opinions, findings, and conclusions or recommendations expressed are those of the author and do not necessarily reflect the views of the fund"
D13-1047,P13-1136,0,0.610583,"ing compressed document summaries as a viable step towards abstractive summarization. These compressive summaries often contain more information than sentence-based extractive summaries since they can remove insignificant sentence constituents and make space for more salient information that is otherwise dropped due to the summary length constraint. Two general strategies have been used for compressive summarization. One is a pipeline approach, where sentence-based extractive summarization is followed or proceeded by sentence compression (Knight and Marcu, 2000; Lin, 2003; Zajic et al., 2007; Wang et al., 2013). Another line of work uses joint compression and summarization. They have been shown to achieve promising performance (Daum´e, 2006; Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011; Chali and Hasan, 2012; Almeida and Martins, 2013; Qian and Liu, 2013). One popular approach for such joint compression and summarization is via integer linear programming (ILP). However, since words are the units in the optimization framework, solving this ILP problem can be expensive. In this study, we use the pipeline compression and summarization method because of its computational efficiency. Prior work"
D13-1047,D12-1022,0,0.508507,"ses, or through a pipeline approach that integrates a generic sentence compression model with a summary sentence pre-selection or post-selection step. Many studies explore the joint sentence compression and selection setting. Martins and Smith (2009) jointly perform sentence extraction and compression by solving an ILP problem; Berg-Kirkpatrick et al. (2011) propose an approach to score the candidate summaries according to a combined linear model of extractive sentence selection and compression. They train the model using a margin-based objective whose loss captures the final summary quality. Woodsend and Lapata (2012) present a method where the summary’s informativeness, succinctness, and grammaticality are learned separately from data but optimized jointly using an ILP setup; Yoshikawa et al. (2012) incorporate semantic role information in the ILP model; Chali and Hasan (2012) investigate three strategies in compressive summarization: compression before extraction, after extraction, or joint compression and extraction in one global optimization framework. These joint models offer a promise for high quality summaries, but they often have high computational cost. Qian and Liu (2013) propose a graph-cut base"
D13-1047,P12-2068,0,0.0263344,"e compression and selection setting. Martins and Smith (2009) jointly perform sentence extraction and compression by solving an ILP problem; Berg-Kirkpatrick et al. (2011) propose an approach to score the candidate summaries according to a combined linear model of extractive sentence selection and compression. They train the model using a margin-based objective whose loss captures the final summary quality. Woodsend and Lapata (2012) present a method where the summary’s informativeness, succinctness, and grammaticality are learned separately from data but optimized jointly using an ILP setup; Yoshikawa et al. (2012) incorporate semantic role information in the ILP model; Chali and Hasan (2012) investigate three strategies in compressive summarization: compression before extraction, after extraction, or joint compression and extraction in one global optimization framework. These joint models offer a promise for high quality summaries, but they often have high computational cost. Qian and Liu (2013) propose a graph-cut based method that improves the speed of joint compression and summarization. The pipeline approach, where sentence-based extractive summarization is followed or proceeded by sentence compres"
D13-1047,W01-0100,0,\N,Missing
D13-1054,P06-1067,0,0.014521,"ns for variable-sized phrases, which enable predicting orders to exploit syntactic and semantic information from a neural language modeling’s perspective. Experiments on the NIST 2008 dataset show that our system significantly improves over the MaxEnt classifier by 1.07 BLEU points. However, as phrase-based decoding casts translation as a string concatenation problem and permits arbitrary permutations, it proves to be NP-complete (Knight, 1999). Therefore, phrase reordering modeling has attracted intensive attention in the past decade (e.g., Och et al., 2004; Tillman, 2004; Zens et al., 2004; Al-Onaizan and Papineni, 2006; Xiong et al., 2006; Koehn et al., 2007; Galley and Manning, 2008; Feng et al., 2010; Green et al., 2010; Bisazza and Federico, 2012; Cherry, 2013). Introduction Phrase-based models (Koehn et al., 2003; Och and Ney, 2004) have been widely used in practical machine translation (MT) systems due to their effectiveness, simplicity, and applicability. First, as sequences of consecutive words, phrases are capable of memorizing local word selection and reordering, making them an effective mechanism for translating idioms or translations with word insertions or omissions. Moreover, n-gram language mo"
D13-1054,P12-1050,0,0.12561,"ing’s perspective. Experiments on the NIST 2008 dataset show that our system significantly improves over the MaxEnt classifier by 1.07 BLEU points. However, as phrase-based decoding casts translation as a string concatenation problem and permits arbitrary permutations, it proves to be NP-complete (Knight, 1999). Therefore, phrase reordering modeling has attracted intensive attention in the past decade (e.g., Och et al., 2004; Tillman, 2004; Zens et al., 2004; Al-Onaizan and Papineni, 2006; Xiong et al., 2006; Koehn et al., 2007; Galley and Manning, 2008; Feng et al., 2010; Green et al., 2010; Bisazza and Federico, 2012; Cherry, 2013). Introduction Phrase-based models (Koehn et al., 2003; Och and Ney, 2004) have been widely used in practical machine translation (MT) systems due to their effectiveness, simplicity, and applicability. First, as sequences of consecutive words, phrases are capable of memorizing local word selection and reordering, making them an effective mechanism for translating idioms or translations with word insertions or omissions. Moreover, n-gram language models can be seamlessly integrated into phrase-based decoders since partial translations grow left to right in decoding. Finally, phra"
D13-1054,N13-1003,0,0.0456632,"ts on the NIST 2008 dataset show that our system significantly improves over the MaxEnt classifier by 1.07 BLEU points. However, as phrase-based decoding casts translation as a string concatenation problem and permits arbitrary permutations, it proves to be NP-complete (Knight, 1999). Therefore, phrase reordering modeling has attracted intensive attention in the past decade (e.g., Och et al., 2004; Tillman, 2004; Zens et al., 2004; Al-Onaizan and Papineni, 2006; Xiong et al., 2006; Koehn et al., 2007; Galley and Manning, 2008; Feng et al., 2010; Green et al., 2010; Bisazza and Federico, 2012; Cherry, 2013). Introduction Phrase-based models (Koehn et al., 2003; Och and Ney, 2004) have been widely used in practical machine translation (MT) systems due to their effectiveness, simplicity, and applicability. First, as sequences of consecutive words, phrases are capable of memorizing local word selection and reordering, making them an effective mechanism for translating idioms or translations with word insertions or omissions. Moreover, n-gram language models can be seamlessly integrated into phrase-based decoders since partial translations grow left to right in decoding. Finally, phrase-based system"
D13-1054,C10-2033,1,0.891478,"nformation from a neural language modeling’s perspective. Experiments on the NIST 2008 dataset show that our system significantly improves over the MaxEnt classifier by 1.07 BLEU points. However, as phrase-based decoding casts translation as a string concatenation problem and permits arbitrary permutations, it proves to be NP-complete (Knight, 1999). Therefore, phrase reordering modeling has attracted intensive attention in the past decade (e.g., Och et al., 2004; Tillman, 2004; Zens et al., 2004; Al-Onaizan and Papineni, 2006; Xiong et al., 2006; Koehn et al., 2007; Galley and Manning, 2008; Feng et al., 2010; Green et al., 2010; Bisazza and Federico, 2012; Cherry, 2013). Introduction Phrase-based models (Koehn et al., 2003; Och and Ney, 2004) have been widely used in practical machine translation (MT) systems due to their effectiveness, simplicity, and applicability. First, as sequences of consecutive words, phrases are capable of memorizing local word selection and reordering, making them an effective mechanism for translating idioms or translations with word insertions or omissions. Moreover, n-gram language models can be seamlessly integrated into phrase-based decoders since partial translatio"
D13-1054,D08-1089,0,0.423945,"t syntactic and semantic information from a neural language modeling’s perspective. Experiments on the NIST 2008 dataset show that our system significantly improves over the MaxEnt classifier by 1.07 BLEU points. However, as phrase-based decoding casts translation as a string concatenation problem and permits arbitrary permutations, it proves to be NP-complete (Knight, 1999). Therefore, phrase reordering modeling has attracted intensive attention in the past decade (e.g., Och et al., 2004; Tillman, 2004; Zens et al., 2004; Al-Onaizan and Papineni, 2006; Xiong et al., 2006; Koehn et al., 2007; Galley and Manning, 2008; Feng et al., 2010; Green et al., 2010; Bisazza and Federico, 2012; Cherry, 2013). Introduction Phrase-based models (Koehn et al., 2003; Och and Ney, 2004) have been widely used in practical machine translation (MT) systems due to their effectiveness, simplicity, and applicability. First, as sequences of consecutive words, phrases are capable of memorizing local word selection and reordering, making them an effective mechanism for translating idioms or translations with word insertions or omissions. Moreover, n-gram language models can be seamlessly integrated into phrase-based decoders since"
D13-1054,N10-1129,0,0.0324751,"eural language modeling’s perspective. Experiments on the NIST 2008 dataset show that our system significantly improves over the MaxEnt classifier by 1.07 BLEU points. However, as phrase-based decoding casts translation as a string concatenation problem and permits arbitrary permutations, it proves to be NP-complete (Knight, 1999). Therefore, phrase reordering modeling has attracted intensive attention in the past decade (e.g., Och et al., 2004; Tillman, 2004; Zens et al., 2004; Al-Onaizan and Papineni, 2006; Xiong et al., 2006; Koehn et al., 2007; Galley and Manning, 2008; Feng et al., 2010; Green et al., 2010; Bisazza and Federico, 2012; Cherry, 2013). Introduction Phrase-based models (Koehn et al., 2003; Och and Ney, 2004) have been widely used in practical machine translation (MT) systems due to their effectiveness, simplicity, and applicability. First, as sequences of consecutive words, phrases are capable of memorizing local word selection and reordering, making them an effective mechanism for translating idioms or translations with word insertions or omissions. Moreover, n-gram language models can be seamlessly integrated into phrase-based decoders since partial translations grow left to righ"
D13-1054,P13-1088,0,0.022199,"et al. (2006) only use boundary words (i.e., the first and the last words in a string) to predict the ordering. What if we look inside? Is it possible to avoid manual feature engineering and learn semantic representations from the data? Fortunately, the rapid development of intersecting deep learning with natural language processing (Bengio et al., 2003; Collobert and Weston, 2008; Collobert et al., 2011; Glorot et al., 2011; Bordes et al., 2011; Socher et al., 2011a; Socher et al., 2011b; Socher et al., 2011c; Socher et al., 2012; Bordes et al., 2012; Huang et al., 2012; Socher et al., 2013; Hermann and Blunsom, 2013) brings hope for alleviating this problem. In these efforts, natural language words are represented as real-valued vectors, which can be naturally fed to neural networks as input. More importantly, it is possible to learn vector space representations for multi-word phrases using recursive autoencoders (Socher et al., 2011c), which opens the door to leveraging semantic representations of phrases in reordering models from a neural language modeling point of view. In this work, we propose an ITG reordering classifier based on recursive autoencoders. The neural network consists of four autoencoder"
D13-1054,P12-1092,0,0.00995628,"y for long sentences. As a result, Xiong et al. (2006) only use boundary words (i.e., the first and the last words in a string) to predict the ordering. What if we look inside? Is it possible to avoid manual feature engineering and learn semantic representations from the data? Fortunately, the rapid development of intersecting deep learning with natural language processing (Bengio et al., 2003; Collobert and Weston, 2008; Collobert et al., 2011; Glorot et al., 2011; Bordes et al., 2011; Socher et al., 2011a; Socher et al., 2011b; Socher et al., 2011c; Socher et al., 2012; Bordes et al., 2012; Huang et al., 2012; Socher et al., 2013; Hermann and Blunsom, 2013) brings hope for alleviating this problem. In these efforts, natural language words are represented as real-valued vectors, which can be naturally fed to neural networks as input. More importantly, it is possible to learn vector space representations for multi-word phrases using recursive autoencoders (Socher et al., 2011c), which opens the door to leveraging semantic representations of phrases in reordering models from a neural language modeling point of view. In this work, we propose an ITG reordering classifier based on recursive autoencoders"
D13-1054,P08-1067,0,0.0105112,"As recursive autoencoders are capable of producing vector space representations for arbitrary multi-word strings in decoding, our neural ITG system achieves an absolute improvement of 1.07 BLEU points over the baseline on the NIST 2008 Chinese-English dataset. There are a number of interesting directions we would like to pursue in the near future. First, replacing the MaxEnt classifier with a neural one redefines the conditions for risk-free hypothesis recombination. We find that the number of hypotheses that can be recombined reduces in our system. Therefore, we plan to use forest reranking (Huang, 2008) to alleviate this problem. Second, it is interesting to follow Socher et al. (2013) to combine linguistically-motivated labels with recursive neural networks. Another problem with our system is that the decoding speed is much slower than the baseline system because of the computational overhead introduced by RAEs. It is necessary to investigate more efficient decoding algorithms. Finally, it is possible to apply our method to other phrase-based and even syntax-based systems. Acknowledgments This research is supported by the 863 Program under the grant No. 2012AA011102, by the Boeing Tsinghua"
D13-1054,J99-4005,0,0.0200293,"e recursive autoencoders to make full use of the entire merging blocks alternatively. The recursive autoencoders are capable of generating vector space representations for variable-sized phrases, which enable predicting orders to exploit syntactic and semantic information from a neural language modeling’s perspective. Experiments on the NIST 2008 dataset show that our system significantly improves over the MaxEnt classifier by 1.07 BLEU points. However, as phrase-based decoding casts translation as a string concatenation problem and permits arbitrary permutations, it proves to be NP-complete (Knight, 1999). Therefore, phrase reordering modeling has attracted intensive attention in the past decade (e.g., Och et al., 2004; Tillman, 2004; Zens et al., 2004; Al-Onaizan and Papineni, 2006; Xiong et al., 2006; Koehn et al., 2007; Galley and Manning, 2008; Feng et al., 2010; Green et al., 2010; Bisazza and Federico, 2012; Cherry, 2013). Introduction Phrase-based models (Koehn et al., 2003; Och and Ney, 2004) have been widely used in practical machine translation (MT) systems due to their effectiveness, simplicity, and applicability. First, as sequences of consecutive words, phrases are capable of memo"
D13-1054,N03-1017,0,0.0747853,"significantly improves over the MaxEnt classifier by 1.07 BLEU points. However, as phrase-based decoding casts translation as a string concatenation problem and permits arbitrary permutations, it proves to be NP-complete (Knight, 1999). Therefore, phrase reordering modeling has attracted intensive attention in the past decade (e.g., Och et al., 2004; Tillman, 2004; Zens et al., 2004; Al-Onaizan and Papineni, 2006; Xiong et al., 2006; Koehn et al., 2007; Galley and Manning, 2008; Feng et al., 2010; Green et al., 2010; Bisazza and Federico, 2012; Cherry, 2013). Introduction Phrase-based models (Koehn et al., 2003; Och and Ney, 2004) have been widely used in practical machine translation (MT) systems due to their effectiveness, simplicity, and applicability. First, as sequences of consecutive words, phrases are capable of memorizing local word selection and reordering, making them an effective mechanism for translating idioms or translations with word insertions or omissions. Moreover, n-gram language models can be seamlessly integrated into phrase-based decoders since partial translations grow left to right in decoding. Finally, phrase-based systems can be applicable to most domains and languages, esp"
D13-1054,P07-2045,0,0.0116822,"ing orders to exploit syntactic and semantic information from a neural language modeling’s perspective. Experiments on the NIST 2008 dataset show that our system significantly improves over the MaxEnt classifier by 1.07 BLEU points. However, as phrase-based decoding casts translation as a string concatenation problem and permits arbitrary permutations, it proves to be NP-complete (Knight, 1999). Therefore, phrase reordering modeling has attracted intensive attention in the past decade (e.g., Och et al., 2004; Tillman, 2004; Zens et al., 2004; Al-Onaizan and Papineni, 2006; Xiong et al., 2006; Koehn et al., 2007; Galley and Manning, 2008; Feng et al., 2010; Green et al., 2010; Bisazza and Federico, 2012; Cherry, 2013). Introduction Phrase-based models (Koehn et al., 2003; Och and Ney, 2004) have been widely used in practical machine translation (MT) systems due to their effectiveness, simplicity, and applicability. First, as sequences of consecutive words, phrases are capable of memorizing local word selection and reordering, making them an effective mechanism for translating idioms or translations with word insertions or omissions. Moreover, n-gram language models can be seamlessly integrated into p"
D13-1054,J04-4002,0,0.104451,"es over the MaxEnt classifier by 1.07 BLEU points. However, as phrase-based decoding casts translation as a string concatenation problem and permits arbitrary permutations, it proves to be NP-complete (Knight, 1999). Therefore, phrase reordering modeling has attracted intensive attention in the past decade (e.g., Och et al., 2004; Tillman, 2004; Zens et al., 2004; Al-Onaizan and Papineni, 2006; Xiong et al., 2006; Koehn et al., 2007; Galley and Manning, 2008; Feng et al., 2010; Green et al., 2010; Bisazza and Federico, 2012; Cherry, 2013). Introduction Phrase-based models (Koehn et al., 2003; Och and Ney, 2004) have been widely used in practical machine translation (MT) systems due to their effectiveness, simplicity, and applicability. First, as sequences of consecutive words, phrases are capable of memorizing local word selection and reordering, making them an effective mechanism for translating idioms or translations with word insertions or omissions. Moreover, n-gram language models can be seamlessly integrated into phrase-based decoders since partial translations grow left to right in decoding. Finally, phrase-based systems can be applicable to most domains and languages, espeAmong them, reorder"
D13-1054,N04-1021,0,0.0119397,"are capable of generating vector space representations for variable-sized phrases, which enable predicting orders to exploit syntactic and semantic information from a neural language modeling’s perspective. Experiments on the NIST 2008 dataset show that our system significantly improves over the MaxEnt classifier by 1.07 BLEU points. However, as phrase-based decoding casts translation as a string concatenation problem and permits arbitrary permutations, it proves to be NP-complete (Knight, 1999). Therefore, phrase reordering modeling has attracted intensive attention in the past decade (e.g., Och et al., 2004; Tillman, 2004; Zens et al., 2004; Al-Onaizan and Papineni, 2006; Xiong et al., 2006; Koehn et al., 2007; Galley and Manning, 2008; Feng et al., 2010; Green et al., 2010; Bisazza and Federico, 2012; Cherry, 2013). Introduction Phrase-based models (Koehn et al., 2003; Och and Ney, 2004) have been widely used in practical machine translation (MT) systems due to their effectiveness, simplicity, and applicability. First, as sequences of consecutive words, phrases are capable of memorizing local word selection and reordering, making them an effective mechanism for translating idioms or translation"
D13-1054,W05-0908,0,0.0293904,"aseline by replacing the MaxEnt reordering model with a neural model. Both the systems have the same pruning settings: the threshold pruning parameter is set to 0.5 and the histogram pruning parameter to 40. For minimum-error-rate training, both systems generate 200-best lists. 4.2 MT Evaluation Table 1 shows the case-insensitive BLEU-4 scores of the baseline system and our system on the development and test sets. Our system outperforms the baseline system by 1.21 BLEU points on the development set and 1.07 on the test set. Both the differences are statistically significant at p = 0.01 level (Riezler and Maxwell, 2005). Table 2 shows the number of sentences that our system has a higher (>), equal (=) or lower (&lt;) BLEU score on the NIST 2008 dataset. We find that our system is superior to the baseline system for long 3 The choice of α is very important for achieving high BLEU scores. We tried a number of intervals and found that the classification accuracy is most stable in the interval [0.100,0.125]. 574 neural maxent 5 10 15 20 Length 25 30 35 Figure 4: Comparison of reordering classification accuracies between the MaxEnt and neural classifiers over varying phrase lengths. “Length” denotes the sum of the l"
D13-1054,D11-1014,0,0.255232,"Missing"
D13-1054,D12-1110,0,0.0166609,"ither manually or automatically, especially for long sentences. As a result, Xiong et al. (2006) only use boundary words (i.e., the first and the last words in a string) to predict the ordering. What if we look inside? Is it possible to avoid manual feature engineering and learn semantic representations from the data? Fortunately, the rapid development of intersecting deep learning with natural language processing (Bengio et al., 2003; Collobert and Weston, 2008; Collobert et al., 2011; Glorot et al., 2011; Bordes et al., 2011; Socher et al., 2011a; Socher et al., 2011b; Socher et al., 2011c; Socher et al., 2012; Bordes et al., 2012; Huang et al., 2012; Socher et al., 2013; Hermann and Blunsom, 2013) brings hope for alleviating this problem. In these efforts, natural language words are represented as real-valued vectors, which can be naturally fed to neural networks as input. More importantly, it is possible to learn vector space representations for multi-word phrases using recursive autoencoders (Socher et al., 2011c), which opens the door to leveraging semantic representations of phrases in reordering models from a neural language modeling point of view. In this work, we propose an ITG reordering c"
D13-1054,P13-1045,0,0.0698655,". As a result, Xiong et al. (2006) only use boundary words (i.e., the first and the last words in a string) to predict the ordering. What if we look inside? Is it possible to avoid manual feature engineering and learn semantic representations from the data? Fortunately, the rapid development of intersecting deep learning with natural language processing (Bengio et al., 2003; Collobert and Weston, 2008; Collobert et al., 2011; Glorot et al., 2011; Bordes et al., 2011; Socher et al., 2011a; Socher et al., 2011b; Socher et al., 2011c; Socher et al., 2012; Bordes et al., 2012; Huang et al., 2012; Socher et al., 2013; Hermann and Blunsom, 2013) brings hope for alleviating this problem. In these efforts, natural language words are represented as real-valued vectors, which can be naturally fed to neural networks as input. More importantly, it is possible to learn vector space representations for multi-word phrases using recursive autoencoders (Socher et al., 2011c), which opens the door to leveraging semantic representations of phrases in reordering models from a neural language modeling point of view. In this work, we propose an ITG reordering classifier based on recursive autoencoders. The neural network"
D13-1054,N04-4026,0,0.093585,"erating vector space representations for variable-sized phrases, which enable predicting orders to exploit syntactic and semantic information from a neural language modeling’s perspective. Experiments on the NIST 2008 dataset show that our system significantly improves over the MaxEnt classifier by 1.07 BLEU points. However, as phrase-based decoding casts translation as a string concatenation problem and permits arbitrary permutations, it proves to be NP-complete (Knight, 1999). Therefore, phrase reordering modeling has attracted intensive attention in the past decade (e.g., Och et al., 2004; Tillman, 2004; Zens et al., 2004; Al-Onaizan and Papineni, 2006; Xiong et al., 2006; Koehn et al., 2007; Galley and Manning, 2008; Feng et al., 2010; Green et al., 2010; Bisazza and Federico, 2012; Cherry, 2013). Introduction Phrase-based models (Koehn et al., 2003; Och and Ney, 2004) have been widely used in practical machine translation (MT) systems due to their effectiveness, simplicity, and applicability. First, as sequences of consecutive words, phrases are capable of memorizing local word selection and reordering, making them an effective mechanism for translating idioms or translations with word ins"
D13-1054,J97-3002,0,0.42934,"T) systems due to their effectiveness, simplicity, and applicability. First, as sequences of consecutive words, phrases are capable of memorizing local word selection and reordering, making them an effective mechanism for translating idioms or translations with word insertions or omissions. Moreover, n-gram language models can be seamlessly integrated into phrase-based decoders since partial translations grow left to right in decoding. Finally, phrase-based systems can be applicable to most domains and languages, espeAmong them, reordering models based on inversion transduction grammar (ITG) (Wu, 1997) are one of the important ongoing research directions. As a formalism for bilingual modeling of sentence pairs, ITG is particularly well suited to predicting ordering shifts between languages. As a result, a number of authors have incorporated ITG into leftto-right decoding to constrain the reordering space and reported significant improvements (e.g., Zens et al., 2004; Feng et al., 2010). Along another line, Xiong et al. (2006) propose a maximum entropy (MaxEnt) reordering model based on ITG. They use the CKY algorithm to recursively merge two blocks (i.e., a pair of source and target strings"
D13-1054,P06-1066,0,0.651966,"which enable predicting orders to exploit syntactic and semantic information from a neural language modeling’s perspective. Experiments on the NIST 2008 dataset show that our system significantly improves over the MaxEnt classifier by 1.07 BLEU points. However, as phrase-based decoding casts translation as a string concatenation problem and permits arbitrary permutations, it proves to be NP-complete (Knight, 1999). Therefore, phrase reordering modeling has attracted intensive attention in the past decade (e.g., Och et al., 2004; Tillman, 2004; Zens et al., 2004; Al-Onaizan and Papineni, 2006; Xiong et al., 2006; Koehn et al., 2007; Galley and Manning, 2008; Feng et al., 2010; Green et al., 2010; Bisazza and Federico, 2012; Cherry, 2013). Introduction Phrase-based models (Koehn et al., 2003; Och and Ney, 2004) have been widely used in practical machine translation (MT) systems due to their effectiveness, simplicity, and applicability. First, as sequences of consecutive words, phrases are capable of memorizing local word selection and reordering, making them an effective mechanism for translating idioms or translations with word insertions or omissions. Moreover, n-gram language models can be seamless"
D13-1054,I08-1066,0,0.274945,"lingual phrases, the MaxEnt ITG reordering model is a two-category classifier (i.e., straight or inverted) for two arbitrary bilingual phrases of which the source phrases are adjacent. This potentially alleviates the data sparseness 567 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 567–577, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics problem since there are usually a large number of reordering training examples available (Xiong et al., 2006). As a result, the MaxEnt ITG model and its extensions (Xiong et al., 2008; Xiong et al., 2010) have achieved competing performance as compared with state-of-the-art phrase-based systems. Despite these successful efforts, the ITG reordering classifiers still face a major challenge: how to extract features from training examples (i.e., a pair of bilingual strings). It is hard to decide which words are representative for predicting reordering, either manually or automatically, especially for long sentences. As a result, Xiong et al. (2006) only use boundary words (i.e., the first and the last words in a string) to predict the ordering. What if we look inside? Is it po"
D13-1054,J10-3009,0,0.0150292,"MaxEnt ITG reordering model is a two-category classifier (i.e., straight or inverted) for two arbitrary bilingual phrases of which the source phrases are adjacent. This potentially alleviates the data sparseness 567 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 567–577, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics problem since there are usually a large number of reordering training examples available (Xiong et al., 2006). As a result, the MaxEnt ITG model and its extensions (Xiong et al., 2008; Xiong et al., 2010) have achieved competing performance as compared with state-of-the-art phrase-based systems. Despite these successful efforts, the ITG reordering classifiers still face a major challenge: how to extract features from training examples (i.e., a pair of bilingual strings). It is hard to decide which words are representative for predicting reordering, either manually or automatically, especially for long sentences. As a result, Xiong et al. (2006) only use boundary words (i.e., the first and the last words in a string) to predict the ordering. What if we look inside? Is it possible to avoid manua"
D13-1054,C04-1030,0,0.0511187,"space representations for variable-sized phrases, which enable predicting orders to exploit syntactic and semantic information from a neural language modeling’s perspective. Experiments on the NIST 2008 dataset show that our system significantly improves over the MaxEnt classifier by 1.07 BLEU points. However, as phrase-based decoding casts translation as a string concatenation problem and permits arbitrary permutations, it proves to be NP-complete (Knight, 1999). Therefore, phrase reordering modeling has attracted intensive attention in the past decade (e.g., Och et al., 2004; Tillman, 2004; Zens et al., 2004; Al-Onaizan and Papineni, 2006; Xiong et al., 2006; Koehn et al., 2007; Galley and Manning, 2008; Feng et al., 2010; Green et al., 2010; Bisazza and Federico, 2012; Cherry, 2013). Introduction Phrase-based models (Koehn et al., 2003; Och and Ney, 2004) have been widely used in practical machine translation (MT) systems due to their effectiveness, simplicity, and applicability. First, as sequences of consecutive words, phrases are capable of memorizing local word selection and reordering, making them an effective mechanism for translating idioms or translations with word insertions or omission"
D13-1054,J09-4009,0,\N,Missing
D13-1156,R09-1002,0,0.0227286,"be arbitrarily large. In ∑ our experiments, we set λmax = j wj , ∑ which empirically guarantees the summary length i zi ≤ L when λ = λmax . The choice of the step size for updating p is similar to the projected subgradient method in dual decomposition (Koo et al., 2010). 1497 Though our algorithm can handle general word ngram concepts, we restrict the concepts to word bigrams, which have been widely used recently in the sentence-based ILP extractive summarization systems. For a concept cj , we define the following features, some of which have been used in previous work (Brandow et al., 1995; Aker and Gaizauskas, 2009; Edmundson, 1969; Radev, 2001; Li et al., 2013b). All of these features are non-negative. • Term frequency: the frequency of cj in the given topic. • Stop word ratio: ratio of stop words in cj . The value can be {0, 0.5, 1}. • Similarity with topic title: the number of common words in these two strings, divided by the length of the longer string. • Document ratio: percentage of documents containing cj . • Sentence ratio: percentage of sentences containing cj . We also define some hard constraints for subtree deletion to improve the readability of the generated compressed sentences. • C0 Arc a"
D13-1156,P13-1020,0,0.618515,". (2011) found that LP relaxation gave poor results, finding unacceptably suboptimal solutions. For speedup, they proposed a two stage method where they performed some sentence selection in the first step to reduce the number of candidates. Despite their empirical success, such 1492 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1492–1502, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics a pruning approach has its inherent problem in that it may eliminate correct sentences in the first step. Recently, Almeida and Martins (2013) proposed a fast joint decoding algorithm based on dual decomposition. For fast convergence, they added quadratic penalty terms to alleviate the learning rate problem. In this paper, we propose an efficient decoding algorithm for fast ILP based compressive summarization using graph cuts. Our assumption is that all concepts are word n-grams and non-negatively scored. The rationale for the non-negativity assumption is straightforward: the score of a concept reflects its informativeness, hence should be non-negative. Given a set of documents, each word is associated with a binary variable, indica"
D13-1156,P11-1049,0,0.382988,"ILP suffers from exponential complexity, word-based compression summarization is an order of magnitude slower than sentence-based extraction. Introduction Automatic multi-document summarization helps readers get the most important information from large amounts of texts. Summarization techniques can be roughly divided into two categories: extractive and abstractive. Extractive summarization casts the summarization task as a sentence selection problem: identifying important summary sentences from One common way to solve an ILP problem is to use its LP relaxation and round the results. However Berg-Kirkpatrick et al. (2011) found that LP relaxation gave poor results, finding unacceptably suboptimal solutions. For speedup, they proposed a two stage method where they performed some sentence selection in the first step to reduce the number of candidates. Despite their empirical success, such 1492 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1492–1502, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics a pruning approach has its inherent problem in that it may eliminate correct sentences in the first step. Recently, Almeida"
D13-1156,C12-1029,0,0.338157,"quality of sentence-based extractive summarization is limited by the informativeness of the original sentences and the summary length constraint. To remove the unimportant part from a long sentence, sentence compression is proposed to generate more informative summaries (Liu and Liu, 2009; Li et al., 2013a). Recent studies show that joint sentence compression and extraction, namely compressive summarization, outperforms pipeline systems that run extractive summarization on the compressed sentences or compress selected summary sentences (Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011; Chali and Hasan, 2012). In Berg-Kirkpatrick et al. (2011), compressive summarization integrates the concept model for extractive summarization (Gillick and Favre, 2009) and subtree deletion model for sentence compression. The score of a compressive summary consists of two parts, scores of selected concepts, and scores of the broken arcs in the dependency parse trees. The selected words must satisfy the length constraint and grammar constraints that include subtree constraint and some manually defined hard constraints. Formally, let x = x1 . . . xI denote the word sequence of documents, where s1 = x1 , . . . xl1 cor"
D13-1156,W09-1802,0,0.326879,"xt Analysis Conference (TAC) datasets using the same train/test splits as previous work (Berg-Kirkpatrick et al., 2011). We compare our approach with the state-of-the-art ILP based approach in terms of summary quality (ROUGE scores and sentence quality) and speed. Experimental results show that our proposed method achieves competitive performance with ILP, while about 100 times faster. 1493 2 Compressive Summarization 2.1 Extractive Summarization As our method is an approximation of ILP based method, we first briefly review the ILP based extractive summarization and compressive summarization. Gillick and Favre (2009) introduced the conceptbased ILP for summarization. A concept is a basic semantic unit. They used word bigrams as such language concepts. Their system achieved the highest ROUGE score on the TAC 2009 evaluation. This approach selects sentences so that the total score of language concepts appearing in the summary is maximized. The association between the language concepts and sentences serves as the constraints, in addition to the summary length constraint. Formally, given a set of sentences S = {sn }N n=1 , extractive summarization can be represented by a binary vector y, where yn indicates wh"
D13-1156,P13-1044,0,0.0221594,"en we fix z, q, and update p using projected Algorithm 1 Compressive Summarization via Graph Cuts subgradient. That is pnew j ( ) ∂R = P ∆ pj + α ∂pj (8) where α > 0 is the step size in line search, and function P∆ (q) denotes the projection of q onto the feasible set ∆ P∆ (q) = min ∥p − q∥2 p∈∆ which can be solved efficiently by sorting (Duchi et al., 2008). 3.4 Initialize p Using Convex Relaxation Since R is non-concave, searching its maximum using subgradient method suffers from local optimality. Though one can use techniques such as branchand-bound for exact inference (Qian and Liu, 2013; Gormley and Eisner, 2013), here for fast decoding, we use convex relaxation to choose a good seed p(0). Recall that pjk denotes the percentage of the k th occurrence contributing to cj . The larger pjk is, the more likely the k th occurrence is selected. To estimate such likelihood, we replace the binary constraint in extractive summarization (Problem (1)) by 0 ≤ y, v ≤ 1, since solving a relaxed LP is much faster than ILP. Suppose y∗ is the optimal solution for such a relaxed LP problem, we initialize p by pjk yn∗ = ∑ jk∗ k ynjk 4 Features and Hard Constraints We choose discriminative models to learn the scores of co"
D13-1156,D10-1125,0,0.033181,"eak end if end while where fconcept (cj ) is the feature vector of cj , and θconcept is the corresponding weight vector of feature fconcept (cj ). Similarly, score wahm is defined as T wahm = θarc farc (ahm ) 1 |oj | where |oj |is the frequency of cj . 3.5 Summary For clarity, we summarize our decoding algorithm in Algorithm 1. Initial λmax can be arbitrarily large. In ∑ our experiments, we set λmax = j wj , ∑ which empirically guarantees the summary length i zi ≤ L when λ = λmax . The choice of the step size for updating p is similar to the projected subgradient method in dual decomposition (Koo et al., 2010). 1497 Though our algorithm can handle general word ngram concepts, we restrict the concepts to word bigrams, which have been widely used recently in the sentence-based ILP extractive summarization systems. For a concept cj , we define the following features, some of which have been used in previous work (Brandow et al., 1995; Aker and Gaizauskas, 2009; Edmundson, 1969; Radev, 2001; Li et al., 2013b). All of these features are non-negative. • Term frequency: the frequency of cj in the given topic. • Stop word ratio: ratio of stop words in cj . The value can be {0, 0.5, 1}. • Similarity with to"
D13-1156,D13-1047,1,0.728844,"0 concepts per topic on the TAC2009 dataset. Therefore the actual scale of ILP is rather small after pruning (e.g., 176 variables and 372 constraints per topic). Empirical studies showed that such small scale ILP can be solved within a few seconds (Gillick and Favre, 2009). 2.2 Compressive Summarization The quality of sentence-based extractive summarization is limited by the informativeness of the original sentences and the summary length constraint. To remove the unimportant part from a long sentence, sentence compression is proposed to generate more informative summaries (Liu and Liu, 2009; Li et al., 2013a). Recent studies show that joint sentence compression and extraction, namely compressive summarization, outperforms pipeline systems that run extractive summarization on the compressed sentences or compress selected summary sentences (Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011; Chali and Hasan, 2012). In Berg-Kirkpatrick et al. (2011), compressive summarization integrates the concept model for extractive summarization (Gillick and Favre, 2009) and subtree deletion model for sentence compression. The score of a compressive summary consists of two parts, scores of selected concepts"
D13-1156,P13-1099,1,0.686119,"0 concepts per topic on the TAC2009 dataset. Therefore the actual scale of ILP is rather small after pruning (e.g., 176 variables and 372 constraints per topic). Empirical studies showed that such small scale ILP can be solved within a few seconds (Gillick and Favre, 2009). 2.2 Compressive Summarization The quality of sentence-based extractive summarization is limited by the informativeness of the original sentences and the summary length constraint. To remove the unimportant part from a long sentence, sentence compression is proposed to generate more informative summaries (Liu and Liu, 2009; Li et al., 2013a). Recent studies show that joint sentence compression and extraction, namely compressive summarization, outperforms pipeline systems that run extractive summarization on the compressed sentences or compress selected summary sentences (Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011; Chali and Hasan, 2012). In Berg-Kirkpatrick et al. (2011), compressive summarization integrates the concept model for extractive summarization (Gillick and Favre, 2009) and subtree deletion model for sentence compression. The score of a compressive summary consists of two parts, scores of selected concepts"
D13-1156,P11-1052,0,0.136542,"Missing"
D13-1156,P09-2066,1,0.405448,"ntences and about 80 concepts per topic on the TAC2009 dataset. Therefore the actual scale of ILP is rather small after pruning (e.g., 176 variables and 372 constraints per topic). Empirical studies showed that such small scale ILP can be solved within a few seconds (Gillick and Favre, 2009). 2.2 Compressive Summarization The quality of sentence-based extractive summarization is limited by the informativeness of the original sentences and the summary length constraint. To remove the unimportant part from a long sentence, sentence compression is proposed to generate more informative summaries (Liu and Liu, 2009; Li et al., 2013a). Recent studies show that joint sentence compression and extraction, namely compressive summarization, outperforms pipeline systems that run extractive summarization on the compressed sentences or compress selected summary sentences (Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011; Chali and Hasan, 2012). In Berg-Kirkpatrick et al. (2011), compressive summarization integrates the concept model for extractive summarization (Gillick and Favre, 2009) and subtree deletion model for sentence compression. The score of a compressive summary consists of two parts, scores of"
D13-1156,W09-1801,0,0.033302,"lick and Favre, 2009). 2.2 Compressive Summarization The quality of sentence-based extractive summarization is limited by the informativeness of the original sentences and the summary length constraint. To remove the unimportant part from a long sentence, sentence compression is proposed to generate more informative summaries (Liu and Liu, 2009; Li et al., 2013a). Recent studies show that joint sentence compression and extraction, namely compressive summarization, outperforms pipeline systems that run extractive summarization on the compressed sentences or compress selected summary sentences (Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011; Chali and Hasan, 2012). In Berg-Kirkpatrick et al. (2011), compressive summarization integrates the concept model for extractive summarization (Gillick and Favre, 2009) and subtree deletion model for sentence compression. The score of a compressive summary consists of two parts, scores of selected concepts, and scores of the broken arcs in the dependency parse trees. The selected words must satisfy the length constraint and grammar constraints that include subtree constraint and some manually defined hard constraints. Formally, let x = x1 . . . xI denote the wo"
D13-1156,Q13-1004,1,0.912275,"predefined threshold. 3.3 Eliminate v Using Supermodular Relaxation Now we consider the inner maximization Problem (5). It is still not a SBQP, since the objective function is not a linear function of zi zj . We propose to approximate the objective function using SBQP. Our solution consists of two steps. First we relax the first constraint of Problem (5) by bounding the objective function with a family of supermodular pseudo boolean functions (Boros and Hammer, 2002). Second we reformulate these pseudo boolean functions equivalently as quadratic functions. Similar to the bounding strategy in (Qian and Liu, 2013), we relax the logical disjunction by linearization. Using the fact that for any binary vector a, we have ∪ ai = max ∑ p∈∆ problem max z,p ∆ = {p| j=1 s.t. pj ∈∆ k ∑ (1 − zh )zm ∑ zi ) (6) ∀j Let Q(p, z) denote the objective function of Problem (6). Given p, we can see that Q is a supermodular pseudo boolean function because coefficients of all non-linear terms are non-negative. Using the fact that for any binary vector a = [a1 , . . . ar ]T , ai ∈ {0, 1}, 1 ≤ i ≤ r, ) ( r r ∏ ∑ ai = max ai − r + 1 b b∈{0,1} i=1 i=1 (Freedman and Drineas, 2005), we get the following equivalent optimization pro"
D13-1156,D12-1022,0,0.0852831,"s for various systems on the TAC2008 data set. We show both the summarization performance and the speed2 of the system. The presented systems include our graphcut based method, the ILP based compression and summarization, and the sentence-based extractive summarization. ILP 2-step refers to the 2-step fast decoding strategy proposed by (Berg-Kirkpatrick et al., 2011). We also list the performance of some state-of-theart systems, including the two ICSI systems (Gillick et al., 2008), the compressive summarization system of Berg-Kirkpatrick et al. (2011) (GBK’11), the multi-aspect ILP system of Woodsend and Lapata (2012)(WL’12) and the dual decomposition based system (Almeida and Martins, 2013) (AM’13). Note that for these referred systems since the linguistic quality results are not comparable due to different judgment methods. For our graph-cut based method, to study the tradeoff between the readability of the summary and the ROUGE scores, we present two versions for this method: one uses all the constraints (C0-C3), the other does not use C0. We can see that our proposed method balanced speed and quality. Compared with ILP, we achieved competitive ROUGE scores, but with about 100x speedup. Our method is al"
D14-1076,D10-1047,0,0.018162,". 2 Related Work Summarization research has seen great development over the last fifty years (Nenkova and McKeown, 2011). Compared to the abstractive counterpart, extractive summarization has received considerable attention due to its clear problem formulation: to extract a set of salient and nonredundant sentences from the given document set. Both unsupervised and supervised approaches have been explored for sentence selection. Supervised approaches include the Bayesian classifier (Kupiec et al., 1995), maximum entropy (Osborne, 2002), skip-chain CRF (Galley, 2006), discriminative reranking (Aker et al., 2010), among others. The extractive summary sentence selection problem can also be formulated in an optimization framework. Previous methods include using integer linear programming (ILP) and submodular functions to solve the optimization problem (Gillick et al., 2009; Li et al., 2013b; Lin and Bilmes, 2010). Compressive summarization receives increasing attention in recent years, since it offers a viable step towards abstractive summarization. The compressed summaries can be generated through a joint model of the sentence selection and compression processes, or through a pipeline approach that int"
D14-1076,P13-1020,0,0.0722131,"s since they can remove insignificant sentence constituents and make space for more salient information that is otherwise dropped due to the summary length constraint. Two general strategies have been used for compressive summarization. One is a pipeline approach, where sentencebased extractive summarization is followed or proceeded by sentence compression (Lin, 2003; Zajic et al., 2007; Vanderwende et al., 2007; Wang et al., 2013). Another line of work uses joint compression and summarization. Such methods have been shown to achieve promising performance (Daum´e, 2006; Chali and Hasan, 2012; Almeida and Martins, 2013; Qian and Liu, 2013), but they are typically computationally expensive. In this study, we propose an innovative sentence compression model based on expanded constituent parse trees. Our model uses integer linear programming (ILP) to search the entire space of compression, and is discriminatively trained. It is built based on the discriminative sentence compression model from (McDonald, 2006) and (Clarke and Lapata, 2008), but our method uses an expanded constituent parse tree rather than only the leaf nodes in previous work. Therefore we can extract rich features for every node in the constit"
D14-1076,P11-1049,0,0.380047,"and Bilmes, 2010). Compressive summarization receives increasing attention in recent years, since it offers a viable step towards abstractive summarization. The compressed summaries can be generated through a joint model of the sentence selection and compression processes, or through a pipeline approach that integrates a sentence compression model with a summary sentence pre-selection or post-selection step. Many studies have explored the joint sentence compression and selection setting. Martins and Smith (2009) jointly performed sentence extraction and compression by solving an ILP problem. Berg-Kirkpatrick et al. (2011) proposed an approach to score the candidate summaries according to a combined linear model of extractive sentence selection and compression. They trained the model using a margin-based objective whose loss captures the final summary qualIn addition to the work on sentence compression as a stand-alone task, prior studies have also investigated compression for the summarization task. Knight and Marcu (2000) utilized the noisy channel and decision tree method to perform sentence compression in the summarization task. Lin (2003) showed that pure syntactic-based compression may not significantly i"
D14-1076,briscoe-carroll-2002-robust,0,0.0757113,"preserve the grammar relationship; Wang et al. (2013) developed a novel beam search decoder using the treebased compression model on the constituent parse tree, which could find the most probable compression efficiently. Table 4 shows the compression results of various systems, along with the compression ratio (C Rate) of the system output. We adopt the compression metrics as used in (Martins and Smith, 2009) that measures the macro F-measure for the retained unigrams (Uni-F1), and the one used in (Clarke and Lapata, 2008) that calculates the F1 score of the grammatical relations labeled by (Briscoe and Carroll, 2002) (Rel-F1). We can see that our proposed compression method performs well, similar to the state-of-the-art systems. To evaluate the power of using the expanded parse tree in our model, we conducted another experiment where we only consider the bottom level of the constituent parse tree. In some sense, this could be considered as the system in (Clarke and Lapata, 2008). Furthermore, we use two different setups: one uses the lexical features (about the words) and the other does not. Table 5 shows the results using the data in (Clarke and Lapata, 2008). For a comparison, we also include the result"
D14-1076,C12-1029,0,0.118385,"sed extractive summaries since they can remove insignificant sentence constituents and make space for more salient information that is otherwise dropped due to the summary length constraint. Two general strategies have been used for compressive summarization. One is a pipeline approach, where sentencebased extractive summarization is followed or proceeded by sentence compression (Lin, 2003; Zajic et al., 2007; Vanderwende et al., 2007; Wang et al., 2013). Another line of work uses joint compression and summarization. Such methods have been shown to achieve promising performance (Daum´e, 2006; Chali and Hasan, 2012; Almeida and Martins, 2013; Qian and Liu, 2013), but they are typically computationally expensive. In this study, we propose an innovative sentence compression model based on expanded constituent parse trees. Our model uses integer linear programming (ILP) to search the entire space of compression, and is discriminatively trained. It is built based on the discriminative sentence compression model from (McDonald, 2006) and (Clarke and Lapata, 2008), but our method uses an expanded constituent parse tree rather than only the leaf nodes in previous work. Therefore we can extract rich features fo"
D14-1076,D13-1047,1,0.165504,"constituent parse trees. Our model uses integer linear programming (ILP) to search the entire space of compression, and is discriminatively trained. It is built based on the discriminative sentence compression model from (McDonald, 2006) and (Clarke and Lapata, 2008), but our method uses an expanded constituent parse tree rather than only the leaf nodes in previous work. Therefore we can extract rich features for every node in the constituent parser tree. This is an advantage of treebased compression technique (Knight and Marcu, 2000; Galley and McKeown, 2007; Wang et al., 2013). Similar to (Li et al., 2013a), we use a pipeline summarization framework where multiple compression candidates are generated for each pre-selected important sentence, and then an ILPIn this paper, we focus on the problem of using sentence compression techniques to improve multi-document summarization. We propose an innovative sentence compression method by considering every node in the constituent parse tree and deciding its status – remove or retain. Integer liner programming with discriminative training is used to solve the problem. Under this model, we incorporate various constraints to improve the linguistic quality"
D14-1076,P13-1099,1,0.254022,"constituent parse trees. Our model uses integer linear programming (ILP) to search the entire space of compression, and is discriminatively trained. It is built based on the discriminative sentence compression model from (McDonald, 2006) and (Clarke and Lapata, 2008), but our method uses an expanded constituent parse tree rather than only the leaf nodes in previous work. Therefore we can extract rich features for every node in the constituent parser tree. This is an advantage of treebased compression technique (Knight and Marcu, 2000; Galley and McKeown, 2007; Wang et al., 2013). Similar to (Li et al., 2013a), we use a pipeline summarization framework where multiple compression candidates are generated for each pre-selected important sentence, and then an ILPIn this paper, we focus on the problem of using sentence compression techniques to improve multi-document summarization. We propose an innovative sentence compression method by considering every node in the constituent parse tree and deciding its status – remove or retain. Integer liner programming with discriminative training is used to solve the problem. Under this model, we incorporate various constraints to improve the linguistic quality"
D14-1076,N10-1134,0,0.234957,"ndant sentences from the given document set. Both unsupervised and supervised approaches have been explored for sentence selection. Supervised approaches include the Bayesian classifier (Kupiec et al., 1995), maximum entropy (Osborne, 2002), skip-chain CRF (Galley, 2006), discriminative reranking (Aker et al., 2010), among others. The extractive summary sentence selection problem can also be formulated in an optimization framework. Previous methods include using integer linear programming (ILP) and submodular functions to solve the optimization problem (Gillick et al., 2009; Li et al., 2013b; Lin and Bilmes, 2010). Compressive summarization receives increasing attention in recent years, since it offers a viable step towards abstractive summarization. The compressed summaries can be generated through a joint model of the sentence selection and compression processes, or through a pipeline approach that integrates a sentence compression model with a summary sentence pre-selection or post-selection step. Many studies have explored the joint sentence compression and selection setting. Martins and Smith (2009) jointly performed sentence extraction and compression by solving an ILP problem. Berg-Kirkpatrick e"
D14-1076,W03-1101,0,0.0207147,"li,yangl@hlt.utdallas.edu} {feiliu@cs.cmu.edu} {lin.zhao,fuliang.weng@us.bosch.com} Abstract a viable step towards abstractive summarization. These compressive summaries often contain more information than sentence-based extractive summaries since they can remove insignificant sentence constituents and make space for more salient information that is otherwise dropped due to the summary length constraint. Two general strategies have been used for compressive summarization. One is a pipeline approach, where sentencebased extractive summarization is followed or proceeded by sentence compression (Lin, 2003; Zajic et al., 2007; Vanderwende et al., 2007; Wang et al., 2013). Another line of work uses joint compression and summarization. Such methods have been shown to achieve promising performance (Daum´e, 2006; Chali and Hasan, 2012; Almeida and Martins, 2013; Qian and Liu, 2013), but they are typically computationally expensive. In this study, we propose an innovative sentence compression model based on expanded constituent parse trees. Our model uses integer linear programming (ILP) to search the entire space of compression, and is discriminatively trained. It is built based on the discriminati"
D14-1076,W04-1013,0,0.220689,"ered more difficult, involving sophisticated techniques for meaning representation, content planning, surface realization, etc. There has been a surge of interest in recent years on generating compressed document summaries as 691 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 691–701, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics based summarization model is used to select the final compressed sentences. We evaluate our proposed method on the TAC 2008 and 2011 data sets using the standard ROUGE metric (Lin, 2004) and human evaluation of the linguistic quality. Our results show that using our proposed sentence compression model in the summarization system can yield significant performance gain in linguistic quality, without losing much performance on the ROUGE metric. ity. Woodsend and Lapata (2012) presented another method where the summary’s informativeness, succinctness, and grammaticality are learned separately from data but optimized jointly using an ILP setup. Yoshikawa et al. (2012) incorporated semantic role information in the ILP model. Our work is closely related with the pipeline approach, w"
D14-1076,W02-1001,0,0.0347668,"icked during the spring are tasty. partmod(truffles,picked) Oil price futures. nn(futures,oil) She looks very beautiful. acomp(looks,beautiful) He felt sad after learning that tragedy. pcomp(after,learning) I am certain that he did it. ccomp(certain,did) Last night I swam in the pool. tmod(swam,night) Table 1: Some dependency relations used for extra constraints. All the examples are from (Marneffe and Manning, 2002) • For type III relations, if the parent node in these relations is retained, the child node should be kept as well. 3.4 weights using the structured perceptron learning strategy (Collins, 2002). The reference label for every node in the expanded constituent parse tree is obtained automatically from the bottom to the top of the tree. Since every leaf node (word) is human annotated (removed or retain), we annotate the internal nodes as removed if all of its children are removed. Otherwise, the node is annotated as retained. During perceptron training, a fixed learning rate is used and parameters are averaged to prevent overfitting. In our experiment, we observe stable convergence using the held-out development corpus, with best performance usually obtained around 10-20 epochs. Feature"
D14-1076,W03-0501,0,0.147922,"Missing"
D14-1076,D13-1155,0,0.0186923,"summarization task. McDonald (2006) firstly introduced a discriminative sentence compression model to directly optimize the quality of the compressed sentences produced. Clarke and Lapata (2008) improved the above discriminative model by using ILP in decoding, making it convenient to add constraints to preserve grammatical structure. Nomoto (2007) treated the compression task as a sequence labeling problem and used CRF for it. Thadani and McKeown (2013) presented an approach for discriminative sentence compression that jointly produces sequential and syntactic representations for output text. Filippova and Altun (2013) presented a method to automatically build a sentence compression corpus with hundreds of thousands of instances on which deletion-based compression algorithms can be trained. 2 Related Work Summarization research has seen great development over the last fifty years (Nenkova and McKeown, 2011). Compared to the abstractive counterpart, extractive summarization has received considerable attention due to its clear problem formulation: to extract a set of salient and nonredundant sentences from the given document set. Both unsupervised and supervised approaches have been explored for sentence sele"
D14-1076,W09-1801,0,0.800543,"P) and submodular functions to solve the optimization problem (Gillick et al., 2009; Li et al., 2013b; Lin and Bilmes, 2010). Compressive summarization receives increasing attention in recent years, since it offers a viable step towards abstractive summarization. The compressed summaries can be generated through a joint model of the sentence selection and compression processes, or through a pipeline approach that integrates a sentence compression model with a summary sentence pre-selection or post-selection step. Many studies have explored the joint sentence compression and selection setting. Martins and Smith (2009) jointly performed sentence extraction and compression by solving an ILP problem. Berg-Kirkpatrick et al. (2011) proposed an approach to score the candidate summaries according to a combined linear model of extractive sentence selection and compression. They trained the model using a margin-based objective whose loss captures the final summary qualIn addition to the work on sentence compression as a stand-alone task, prior studies have also investigated compression for the summarization task. Knight and Marcu (2000) utilized the noisy channel and decision tree method to perform sentence compre"
D14-1076,E06-1038,0,0.489741,"al., 2007; Wang et al., 2013). Another line of work uses joint compression and summarization. Such methods have been shown to achieve promising performance (Daum´e, 2006; Chali and Hasan, 2012; Almeida and Martins, 2013; Qian and Liu, 2013), but they are typically computationally expensive. In this study, we propose an innovative sentence compression model based on expanded constituent parse trees. Our model uses integer linear programming (ILP) to search the entire space of compression, and is discriminatively trained. It is built based on the discriminative sentence compression model from (McDonald, 2006) and (Clarke and Lapata, 2008), but our method uses an expanded constituent parse tree rather than only the leaf nodes in previous work. Therefore we can extract rich features for every node in the constituent parser tree. This is an advantage of treebased compression technique (Knight and Marcu, 2000; Galley and McKeown, 2007; Wang et al., 2013). Similar to (Li et al., 2013a), we use a pipeline summarization framework where multiple compression candidates are generated for each pre-selected important sentence, and then an ILPIn this paper, we focus on the problem of using sentence compression"
D14-1076,N10-1131,0,0.0361554,"e whose loss captures the final summary qualIn addition to the work on sentence compression as a stand-alone task, prior studies have also investigated compression for the summarization task. Knight and Marcu (2000) utilized the noisy channel and decision tree method to perform sentence compression in the summarization task. Lin (2003) showed that pure syntactic-based compression may not significantly improve the summarization performance. Zajic et al. (2007) compared two sentence compression approaches for multidocument summarization, including a ‘parse-andtrim’ and a noisy-channel approach. Galanis and Androutsopoulos (2010) used the maximum entropy model to generate the candidate compressions by removing branches from the source sentences. Woodsend and Lapata (2010) presented a joint content selection and compression model for single-document summarization. They operated over a phrase-based representation of the source document which they obtained by merging information from PCFG parse trees and dependency graphs. Liu and Liu (2013) adopted the CRFbased sentence compression approach for summa692 rizing spoken documents. Unlike the word-based operation, some of these models e.g (Knight and Marcu, 2000; Siddhartha"
D14-1076,N07-1023,0,0.0221158,"an innovative sentence compression model based on expanded constituent parse trees. Our model uses integer linear programming (ILP) to search the entire space of compression, and is discriminatively trained. It is built based on the discriminative sentence compression model from (McDonald, 2006) and (Clarke and Lapata, 2008), but our method uses an expanded constituent parse tree rather than only the leaf nodes in previous work. Therefore we can extract rich features for every node in the constituent parser tree. This is an advantage of treebased compression technique (Knight and Marcu, 2000; Galley and McKeown, 2007; Wang et al., 2013). Similar to (Li et al., 2013a), we use a pipeline summarization framework where multiple compression candidates are generated for each pre-selected important sentence, and then an ILPIn this paper, we focus on the problem of using sentence compression techniques to improve multi-document summarization. We propose an innovative sentence compression method by considering every node in the constituent parse tree and deciding its status – remove or retain. Integer liner programming with discriminative training is used to solve the problem. Under this model, we incorporate vari"
D14-1076,W06-1643,0,0.0329105,"sed compression algorithms can be trained. 2 Related Work Summarization research has seen great development over the last fifty years (Nenkova and McKeown, 2011). Compared to the abstractive counterpart, extractive summarization has received considerable attention due to its clear problem formulation: to extract a set of salient and nonredundant sentences from the given document set. Both unsupervised and supervised approaches have been explored for sentence selection. Supervised approaches include the Bayesian classifier (Kupiec et al., 1995), maximum entropy (Osborne, 2002), skip-chain CRF (Galley, 2006), discriminative reranking (Aker et al., 2010), among others. The extractive summary sentence selection problem can also be formulated in an optimization framework. Previous methods include using integer linear programming (ILP) and submodular functions to solve the optimization problem (Gillick et al., 2009; Li et al., 2013b; Lin and Bilmes, 2010). Compressive summarization receives increasing attention in recent years, since it offers a viable step towards abstractive summarization. The compressed summaries can be generated through a joint model of the sentence selection and compression proc"
D14-1076,C12-1128,0,0.243243,"ethod to select the best summary sentences from the multiple compressed sentences. The sentence pre-selection model is a simple supervised support vector regression (SVR) model that predicts a salience score for each sentence and selects the top ranked sentences for further processing (compression and summarization). The target value for each sentence during training is the ROUGE-2 score between the sentence and the human written abstracts. We use three common features: (1) sentence position in the document; (2) sentence length; and (3) interpolated n-gram document frequency as introduced in (Ng et al., 2012). The final sentence selection process follows the Table 2: Features used in our system besides those used in (Clarke and Lapata, 2008). 3.5 Learning To learn the feature weights during training, we perform ILP decoding on every sentence in the training set, to find the best hypothesis for each node in the expanded constituent parse tree. If the hypothesis is incorrect, we update the feature 696 CRF4 to implement the CRF sentence compression model. SVMlight5 is used for the summary sentence pre-selection model. Gurobi ILP solver6 does all ILP decoding. ILP method introduced in (Gillick et al.,"
D14-1076,W02-0401,0,0.0272814,"f instances on which deletion-based compression algorithms can be trained. 2 Related Work Summarization research has seen great development over the last fifty years (Nenkova and McKeown, 2011). Compared to the abstractive counterpart, extractive summarization has received considerable attention due to its clear problem formulation: to extract a set of salient and nonredundant sentences from the given document set. Both unsupervised and supervised approaches have been explored for sentence selection. Supervised approaches include the Bayesian classifier (Kupiec et al., 1995), maximum entropy (Osborne, 2002), skip-chain CRF (Galley, 2006), discriminative reranking (Aker et al., 2010), among others. The extractive summary sentence selection problem can also be formulated in an optimization framework. Previous methods include using integer linear programming (ILP) and submodular functions to solve the optimization problem (Gillick et al., 2009; Li et al., 2013b; Lin and Bilmes, 2010). Compressive summarization receives increasing attention in recent years, since it offers a viable step towards abstractive summarization. The compressed summaries can be generated through a joint model of the sentence"
D14-1076,P06-1055,0,0.0140636,"nnotated by (Li et al., 2013a) using TAC2010 data. In the compression module, for each word we also used its document level feature.2 Compression Data We also evaluate our compression model using the data set from (Clarke and Lapata, 2008). It includes 82 newswire articles with manually produced compression for each sentence. We use the same partitions as (Martins and Smith, 2009), i.e., 1,188 sentences for training and 441 for testing. Data Processing We use Stanford CoreNLP toolkit3 to tokenize the sentences, extract name entity tags, and generate the dependency parse tree. Berkeley Parser (Petrov et al., 2006) is adopted to obtain the constituent parse tree for every sentence and POS tag for every token. We use Pocket 4 http://sourceforge.net/projects/pocket-crf-1/ http://svmlight.joachims.org/ 6 http://www.gurobi.com 7 We chose to evaluate the linguistic quality for this system because of two reasons: one is that we have an implementation of that method; the other more important one is that it has the highest reported ROUGE results among the compared methods. 1 5 http://www.nist.gov/tac/data/index.html 2 Document level features for a word include information such as the word’s document frequency i"
D14-1076,D13-1156,1,0.838736,"ignificant sentence constituents and make space for more salient information that is otherwise dropped due to the summary length constraint. Two general strategies have been used for compressive summarization. One is a pipeline approach, where sentencebased extractive summarization is followed or proceeded by sentence compression (Lin, 2003; Zajic et al., 2007; Vanderwende et al., 2007; Wang et al., 2013). Another line of work uses joint compression and summarization. Such methods have been shown to achieve promising performance (Daum´e, 2006; Chali and Hasan, 2012; Almeida and Martins, 2013; Qian and Liu, 2013), but they are typically computationally expensive. In this study, we propose an innovative sentence compression model based on expanded constituent parse trees. Our model uses integer linear programming (ILP) to search the entire space of compression, and is discriminatively trained. It is built based on the discriminative sentence compression model from (McDonald, 2006) and (Clarke and Lapata, 2008), but our method uses an expanded constituent parse tree rather than only the leaf nodes in previous work. Therefore we can extract rich features for every node in the constituent parser tree. Thi"
D14-1076,C04-1129,0,0.0108443,"los (2010) used the maximum entropy model to generate the candidate compressions by removing branches from the source sentences. Woodsend and Lapata (2010) presented a joint content selection and compression model for single-document summarization. They operated over a phrase-based representation of the source document which they obtained by merging information from PCFG parse trees and dependency graphs. Liu and Liu (2013) adopted the CRFbased sentence compression approach for summa692 rizing spoken documents. Unlike the word-based operation, some of these models e.g (Knight and Marcu, 2000; Siddharthan et al., 2004; Turner and Charniak, 2005; Galanis and Androutsopoulos, 2010; Wang et al., 2013), are tree-based approaches that operate on the parse trees and thus the compression decision can be made for a constituent, instead of a single word. ( 1 δi = 0 ∀i ∈ [1..n] ( 1 αi = 0 3 Sentence Compression Method if xi starts the compression otherwise ∀i ∈ [1..n] Sentence compression is a task of producing a summary for a single sentence. The compressed sentence should be shorter, contain important content from the original sentence, and be grammatical. In some sense, sentence compression can be described as a"
D14-1076,W13-3508,0,0.0719819,"where sentence-based extractive summarization is followed or proceeded by sentence compression. There have been many studies on sentence compression, independent of the summarization task. McDonald (2006) firstly introduced a discriminative sentence compression model to directly optimize the quality of the compressed sentences produced. Clarke and Lapata (2008) improved the above discriminative model by using ILP in decoding, making it convenient to add constraints to preserve grammatical structure. Nomoto (2007) treated the compression task as a sequence labeling problem and used CRF for it. Thadani and McKeown (2013) presented an approach for discriminative sentence compression that jointly produces sequential and syntactic representations for output text. Filippova and Altun (2013) presented a method to automatically build a sentence compression corpus with hundreds of thousands of instances on which deletion-based compression algorithms can be trained. 2 Related Work Summarization research has seen great development over the last fifty years (Nenkova and McKeown, 2011). Compared to the abstractive counterpart, extractive summarization has received considerable attention due to its clear problem formulat"
D14-1076,P05-1036,0,0.0120166,"m entropy model to generate the candidate compressions by removing branches from the source sentences. Woodsend and Lapata (2010) presented a joint content selection and compression model for single-document summarization. They operated over a phrase-based representation of the source document which they obtained by merging information from PCFG parse trees and dependency graphs. Liu and Liu (2013) adopted the CRFbased sentence compression approach for summa692 rizing spoken documents. Unlike the word-based operation, some of these models e.g (Knight and Marcu, 2000; Siddharthan et al., 2004; Turner and Charniak, 2005; Galanis and Androutsopoulos, 2010; Wang et al., 2013), are tree-based approaches that operate on the parse trees and thus the compression decision can be made for a constituent, instead of a single word. ( 1 δi = 0 ∀i ∈ [1..n] ( 1 αi = 0 3 Sentence Compression Method if xi starts the compression otherwise ∀i ∈ [1..n] Sentence compression is a task of producing a summary for a single sentence. The compressed sentence should be shorter, contain important content from the original sentence, and be grammatical. In some sense, sentence compression can be described as a ‘scaled down version of the"
D14-1076,P13-1136,0,0.654749,"fuliang.weng@us.bosch.com} Abstract a viable step towards abstractive summarization. These compressive summaries often contain more information than sentence-based extractive summaries since they can remove insignificant sentence constituents and make space for more salient information that is otherwise dropped due to the summary length constraint. Two general strategies have been used for compressive summarization. One is a pipeline approach, where sentencebased extractive summarization is followed or proceeded by sentence compression (Lin, 2003; Zajic et al., 2007; Vanderwende et al., 2007; Wang et al., 2013). Another line of work uses joint compression and summarization. Such methods have been shown to achieve promising performance (Daum´e, 2006; Chali and Hasan, 2012; Almeida and Martins, 2013; Qian and Liu, 2013), but they are typically computationally expensive. In this study, we propose an innovative sentence compression model based on expanded constituent parse trees. Our model uses integer linear programming (ILP) to search the entire space of compression, and is discriminatively trained. It is built based on the discriminative sentence compression model from (McDonald, 2006) and (Clarke an"
D14-1076,P10-1058,0,0.0165962,"ompression for the summarization task. Knight and Marcu (2000) utilized the noisy channel and decision tree method to perform sentence compression in the summarization task. Lin (2003) showed that pure syntactic-based compression may not significantly improve the summarization performance. Zajic et al. (2007) compared two sentence compression approaches for multidocument summarization, including a ‘parse-andtrim’ and a noisy-channel approach. Galanis and Androutsopoulos (2010) used the maximum entropy model to generate the candidate compressions by removing branches from the source sentences. Woodsend and Lapata (2010) presented a joint content selection and compression model for single-document summarization. They operated over a phrase-based representation of the source document which they obtained by merging information from PCFG parse trees and dependency graphs. Liu and Liu (2013) adopted the CRFbased sentence compression approach for summa692 rizing spoken documents. Unlike the word-based operation, some of these models e.g (Knight and Marcu, 2000; Siddharthan et al., 2004; Turner and Charniak, 2005; Galanis and Androutsopoulos, 2010; Wang et al., 2013), are tree-based approaches that operate on the p"
D14-1076,D12-1022,0,0.489972,"al Methods in Natural Language Processing (EMNLP), pages 691–701, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics based summarization model is used to select the final compressed sentences. We evaluate our proposed method on the TAC 2008 and 2011 data sets using the standard ROUGE metric (Lin, 2004) and human evaluation of the linguistic quality. Our results show that using our proposed sentence compression model in the summarization system can yield significant performance gain in linguistic quality, without losing much performance on the ROUGE metric. ity. Woodsend and Lapata (2012) presented another method where the summary’s informativeness, succinctness, and grammaticality are learned separately from data but optimized jointly using an ILP setup. Yoshikawa et al. (2012) incorporated semantic role information in the ILP model. Our work is closely related with the pipeline approach, where sentence-based extractive summarization is followed or proceeded by sentence compression. There have been many studies on sentence compression, independent of the summarization task. McDonald (2006) firstly introduced a discriminative sentence compression model to directly optimize the"
D14-1076,P12-2068,0,0.0436108,"final compressed sentences. We evaluate our proposed method on the TAC 2008 and 2011 data sets using the standard ROUGE metric (Lin, 2004) and human evaluation of the linguistic quality. Our results show that using our proposed sentence compression model in the summarization system can yield significant performance gain in linguistic quality, without losing much performance on the ROUGE metric. ity. Woodsend and Lapata (2012) presented another method where the summary’s informativeness, succinctness, and grammaticality are learned separately from data but optimized jointly using an ILP setup. Yoshikawa et al. (2012) incorporated semantic role information in the ILP model. Our work is closely related with the pipeline approach, where sentence-based extractive summarization is followed or proceeded by sentence compression. There have been many studies on sentence compression, independent of the summarization task. McDonald (2006) firstly introduced a discriminative sentence compression model to directly optimize the quality of the compressed sentences produced. Clarke and Lapata (2008) improved the above discriminative model by using ILP in decoding, making it convenient to add constraints to preserve gram"
D14-1076,W01-0100,0,\N,Missing
D15-1144,J07-2003,0,0.822202,"hand, aligners seek to find the alignment with the highest alignment model score, without regard to structural constraints. Consequently, sensible translation rules may not be extracted because they violate consistency constraints required by translation rule extraction (Och and Ney, 2004). Wang et al. (2010) find that the standard alignment tools are not optimal for training syntax-based models. As a result, they have to resort to realigning. On the other hand, the consistency constraint used in most translation rule extraction algorithms tolerate wrong links within consistent phrase pairs. Chiang (2007) uses the union of two unidirectional alignments, which usually has a low precision, for extracting hierarchical phrases. Therefore, it is important to include both alignment model score and the consistency constraint in the optimization objective of word alignment. In this work, we propose to use coverage, which measures how well extracted phrases can 1228 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1228–1237, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. Figure 1: (a) An alignment resulting in a set"
D15-1144,D09-1037,0,0.12616,"n often constitute two consecutive steps in the training pipeline. Wordaligned bilingual corpora serve as a fundamental resource for translation rule extraction, not only for phrase-based models (Koehn et al., 2003; Och and Ney, 2004), but also for syntax-based models (Chiang, 2005; Galley et al., 2006). Dividing alignment and extraction into two separate steps significantly improves the efficiency and scalability of parameter estimation as compared with directly learning translation models from bilingual ∗ Corresponding author: Yang Liu. corpora (Marcu and Wong, 2002; DeNero and Klein, 2008; Cohn and Blunsom, 2009). However, separating word alignment from translation rule extraction suffers from a major problem: maximizing the accuracy of word alignment does not necessarily lead to the improvement of translation quality. A number of studies show that alignment error rate (AER) only has a loose correlation with BLEU (Callison-Burch et al., 2004; Goutte et al., 2004; Ittycheriah and Roukos, 2005). Ayan and Dorr (2006) find that precision-oriented alignments result in better translation performance than recall-oriented alignments. Fraser and Marcu (2007) show that using AER and balanced F-measure can only"
D15-1144,P08-2007,0,0.132395,"anslation rule extraction often constitute two consecutive steps in the training pipeline. Wordaligned bilingual corpora serve as a fundamental resource for translation rule extraction, not only for phrase-based models (Koehn et al., 2003; Och and Ney, 2004), but also for syntax-based models (Chiang, 2005; Galley et al., 2006). Dividing alignment and extraction into two separate steps significantly improves the efficiency and scalability of parameter estimation as compared with directly learning translation models from bilingual ∗ Corresponding author: Yang Liu. corpora (Marcu and Wong, 2002; DeNero and Klein, 2008; Cohn and Blunsom, 2009). However, separating word alignment from translation rule extraction suffers from a major problem: maximizing the accuracy of word alignment does not necessarily lead to the improvement of translation quality. A number of studies show that alignment error rate (AER) only has a loose correlation with BLEU (Callison-Burch et al., 2004; Goutte et al., 2004; Ittycheriah and Roukos, 2005). Ayan and Dorr (2006) find that precision-oriented alignments result in better translation performance than recall-oriented alignments. Fraser and Marcu (2007) show that using AER and bal"
D15-1144,P10-1147,0,0.0768572,"p between alignment and translation. A phrase pair consistent with alignment tolerates wrong inside links. For example, even if “oumeng” is aligned with “Russia”, (f13 , e31 ) is still consistent. This is one possible reason that maximizing alignment accuracy does not necessarily lead to improved translation performance. 3 Modeling Consistency in Word Alignment Our intuition is that including the consistency constraint in word alignment can hopefully reduce the discrepancy between alignment and translation. While this idea has been suggested by a number of authors (e.g., (Deng and Zhou, 2009; DeNero and Klein, 2010)), our goal is to optimize arbitrary alignment models with respect to end-toend translation in the search phase without labeled data (see Related Work for detailed comparison). A natural way is to include consistency in the optimization objective as a regularization term. However, as consistency is only defined at the phrase level (see Definition 4), we need a sentence-level measure to reflect how well an alignment conforms to the consistency constraint. A straightforward measure is the number of bilingual phrases consistent with the alignment (phrase count for short), which is easy and effici"
D15-1144,P09-2058,0,0.26144,"s a loose relationship between alignment and translation. A phrase pair consistent with alignment tolerates wrong inside links. For example, even if “oumeng” is aligned with “Russia”, (f13 , e31 ) is still consistent. This is one possible reason that maximizing alignment accuracy does not necessarily lead to improved translation performance. 3 Modeling Consistency in Word Alignment Our intuition is that including the consistency constraint in word alignment can hopefully reduce the discrepancy between alignment and translation. While this idea has been suggested by a number of authors (e.g., (Deng and Zhou, 2009; DeNero and Klein, 2010)), our goal is to optimize arbitrary alignment models with respect to end-toend translation in the search phase without labeled data (see Related Work for detailed comparison). A natural way is to include consistency in the optimization objective as a regularization term. However, as consistency is only defined at the phrase level (see Definition 4), we need a sentence-level measure to reflect how well an alignment conforms to the consistency constraint. A straightforward measure is the number of bilingual phrases consistent with the alignment (phrase count for short),"
D15-1144,J07-3002,0,0.0292539,"rpora (Marcu and Wong, 2002; DeNero and Klein, 2008; Cohn and Blunsom, 2009). However, separating word alignment from translation rule extraction suffers from a major problem: maximizing the accuracy of word alignment does not necessarily lead to the improvement of translation quality. A number of studies show that alignment error rate (AER) only has a loose correlation with BLEU (Callison-Burch et al., 2004; Goutte et al., 2004; Ittycheriah and Roukos, 2005). Ayan and Dorr (2006) find that precision-oriented alignments result in better translation performance than recall-oriented alignments. Fraser and Marcu (2007) show that using AER and balanced F-measure can only partially explain the effect of alignment quality on BLEU for several language pairs. We believe that the correlation problem arises from the discrepancy between word alignment and translation rule extraction. On one hand, aligners seek to find the alignment with the highest alignment model score, without regard to structural constraints. Consequently, sensible translation rules may not be extracted because they violate consistency constraints required by translation rule extraction (Och and Ney, 2004). Wang et al. (2010) find that the stand"
D15-1144,P06-1121,0,0.304,"discriminative alignment approaches across various languages and translation models. 1 Introduction Word alignment, which aims to identify the correspondence between words in two languages, plays an important role in statistical machine translation (Brown et al., 1993). Word alignment and translation rule extraction often constitute two consecutive steps in the training pipeline. Wordaligned bilingual corpora serve as a fundamental resource for translation rule extraction, not only for phrase-based models (Koehn et al., 2003; Och and Ney, 2004), but also for syntax-based models (Chiang, 2005; Galley et al., 2006). Dividing alignment and extraction into two separate steps significantly improves the efficiency and scalability of parameter estimation as compared with directly learning translation models from bilingual ∗ Corresponding author: Yang Liu. corpora (Marcu and Wong, 2002; DeNero and Klein, 2008; Cohn and Blunsom, 2009). However, separating word alignment from translation rule extraction suffers from a major problem: maximizing the accuracy of word alignment does not necessarily lead to the improvement of translation quality. A number of studies show that alignment error rate (AER) only has a lo"
D15-1144,P04-1064,0,0.0363259,"eps significantly improves the efficiency and scalability of parameter estimation as compared with directly learning translation models from bilingual ∗ Corresponding author: Yang Liu. corpora (Marcu and Wong, 2002; DeNero and Klein, 2008; Cohn and Blunsom, 2009). However, separating word alignment from translation rule extraction suffers from a major problem: maximizing the accuracy of word alignment does not necessarily lead to the improvement of translation quality. A number of studies show that alignment error rate (AER) only has a loose correlation with BLEU (Callison-Burch et al., 2004; Goutte et al., 2004; Ittycheriah and Roukos, 2005). Ayan and Dorr (2006) find that precision-oriented alignments result in better translation performance than recall-oriented alignments. Fraser and Marcu (2007) show that using AER and balanced F-measure can only partially explain the effect of alignment quality on BLEU for several language pairs. We believe that the correlation problem arises from the discrepancy between word alignment and translation rule extraction. On one hand, aligners seek to find the alignment with the highest alignment model score, without regard to structural constraints. Consequently, s"
D15-1144,P06-1002,0,0.0282657,"ility of parameter estimation as compared with directly learning translation models from bilingual ∗ Corresponding author: Yang Liu. corpora (Marcu and Wong, 2002; DeNero and Klein, 2008; Cohn and Blunsom, 2009). However, separating word alignment from translation rule extraction suffers from a major problem: maximizing the accuracy of word alignment does not necessarily lead to the improvement of translation quality. A number of studies show that alignment error rate (AER) only has a loose correlation with BLEU (Callison-Burch et al., 2004; Goutte et al., 2004; Ittycheriah and Roukos, 2005). Ayan and Dorr (2006) find that precision-oriented alignments result in better translation performance than recall-oriented alignments. Fraser and Marcu (2007) show that using AER and balanced F-measure can only partially explain the effect of alignment quality on BLEU for several language pairs. We believe that the correlation problem arises from the discrepancy between word alignment and translation rule extraction. On one hand, aligners seek to find the alignment with the highest alignment model score, without regard to structural constraints. Consequently, sensible translation rules may not be extracted becaus"
D15-1144,P09-1104,0,0.0448938,"Missing"
D15-1144,J93-2003,0,0.0539631,"Missing"
D15-1144,H05-1012,0,0.0401707,"roves the efficiency and scalability of parameter estimation as compared with directly learning translation models from bilingual ∗ Corresponding author: Yang Liu. corpora (Marcu and Wong, 2002; DeNero and Klein, 2008; Cohn and Blunsom, 2009). However, separating word alignment from translation rule extraction suffers from a major problem: maximizing the accuracy of word alignment does not necessarily lead to the improvement of translation quality. A number of studies show that alignment error rate (AER) only has a loose correlation with BLEU (Callison-Burch et al., 2004; Goutte et al., 2004; Ittycheriah and Roukos, 2005). Ayan and Dorr (2006) find that precision-oriented alignments result in better translation performance than recall-oriented alignments. Fraser and Marcu (2007) show that using AER and balanced F-measure can only partially explain the effect of alignment quality on BLEU for several language pairs. We believe that the correlation problem arises from the discrepancy between word alignment and translation rule extraction. On one hand, aligners seek to find the alignment with the highest alignment model score, without regard to structural constraints. Consequently, sensible translation rules may n"
D15-1144,P04-1023,0,0.0949041,"traction into two separate steps significantly improves the efficiency and scalability of parameter estimation as compared with directly learning translation models from bilingual ∗ Corresponding author: Yang Liu. corpora (Marcu and Wong, 2002; DeNero and Klein, 2008; Cohn and Blunsom, 2009). However, separating word alignment from translation rule extraction suffers from a major problem: maximizing the accuracy of word alignment does not necessarily lead to the improvement of translation quality. A number of studies show that alignment error rate (AER) only has a loose correlation with BLEU (Callison-Burch et al., 2004; Goutte et al., 2004; Ittycheriah and Roukos, 2005). Ayan and Dorr (2006) find that precision-oriented alignments result in better translation performance than recall-oriented alignments. Fraser and Marcu (2007) show that using AER and balanced F-measure can only partially explain the effect of alignment quality on BLEU for several language pairs. We believe that the correlation problem arises from the discrepancy between word alignment and translation rule extraction. On one hand, aligners seek to find the alignment with the highest alignment model score, without regard to structural constra"
D15-1144,P07-1036,0,0.0227503,"Wong, 2002; DeNero and Klein, 2008; Cohn and Blunsom, 2009). However, separating word alignment from translation rule extraction suffers from a major problem: maximizing the accuracy of word alignment does not necessarily lead to the improvement of translation quality. A number of studies show that alignment error rate (AER) only has a loose correlation with BLEU (Callison-Burch et al., 2004; Goutte et al., 2004; Ittycheriah and Roukos, 2005). Ayan and Dorr (2006) find that precision-oriented alignments result in better translation performance than recall-oriented alignments. Fraser and Marcu (2007) show that using AER and balanced F-measure can only partially explain the effect of alignment quality on BLEU for several language pairs. We believe that the correlation problem arises from the discrepancy between word alignment and translation rule extraction. On one hand, aligners seek to find the alignment with the highest alignment model score, without regard to structural constraints. Consequently, sensible translation rules may not be extracted because they violate consistency constraints required by translation rule extraction (Och and Ney, 2004). Wang et al. (2010) find that the stand"
D15-1144,D07-1091,0,0.294215,"n English sentence “EU and Russia hold summit in Moscow”. We use black circles to denote links. The link (1, 1) indicates that the first Chinese word “oumeng” and the first English word “EU” are translations of each other. 1229 1. No words in the source phrase are aligned with words outside the target phrase and vice versa: ∀(j, i) ∈ a : j1 ≤ j ≤ j2 ↔ i1 ≤ i ≤ i2 , 2. At least one word in the source phrase is aligned with at least one word in the target phrase: ∃(j, i) ∈ a : j1 ≤ j ≤ j2 ∧ i1 ≤ i ≤ i2 . Alignment consistency forms the basis of translation rule extraction in modern SMT systems (Koehn and Hoang, 2007; Chiang, 2007; Galley et al., 2006; Liu et al., 2006). In Figure 1, (f13 , e31 ) is consistent with the alignment because all words in “oumeng he eluosi” are aligned with all words in “EU and Russia”. In contrast, in Figure 1(b), “huiwu shounao” and “hold summit” are not consistent with the alignment because “hold” is also aligned to a word “juxing” outside. However, alignment consistency only defines a loose relationship between alignment and translation. A phrase pair consistent with alignment tolerates wrong inside links. For example, even if “oumeng” is aligned with “Russia”, (f13 , e31 )"
D15-1144,N03-1017,0,0.0633295,"Missing"
D15-1144,P06-1096,0,0.1416,"e phrase level (see Definition 4), we need a sentence-level measure to reflect how well an alignment conforms to the consistency constraint. A straightforward measure is the number of bilingual phrases consistent with the alignment (phrase count for short), which is easy and efficient to calculate during search (Deng and Zhou, 2009). Unfortunately, optimizing with respect to phrase count is prone to yield alignments with very few links in a biased way, which result in a large number of bilingual phrases extracted from a small fraction of the training data. Another alternative is reachability (Liang et al., 2006a; Yu et al., 2013) that indicates whether there exists a full derivation to recover the training data. However, calculating reachability faces a major problem: a large portion of training data cannot be fully recovered due to noisy alignments and the distortion limit (Yu et al., 2013). In this work, we propose coverage, which reflects how well extracted phrases can recover the training data, to measure the sentence-level consistency. In the following, we will introduce a number of definitions to facilitate the exposition. Definition 5 A source word fj is said to be covered by a bilingual phra"
D15-1144,N06-1014,0,0.0399502,"eter estimation as compared with directly learning translation models from bilingual ∗ Corresponding author: Yang Liu. corpora (Marcu and Wong, 2002; DeNero and Klein, 2008; Cohn and Blunsom, 2009). However, separating word alignment from translation rule extraction suffers from a major problem: maximizing the accuracy of word alignment does not necessarily lead to the improvement of translation quality. A number of studies show that alignment error rate (AER) only has a loose correlation with BLEU (Callison-Burch et al., 2004; Goutte et al., 2004; Ittycheriah and Roukos, 2005). Ayan and Dorr (2006) find that precision-oriented alignments result in better translation performance than recall-oriented alignments. Fraser and Marcu (2007) show that using AER and balanced F-measure can only partially explain the effect of alignment quality on BLEU for several language pairs. We believe that the correlation problem arises from the discrepancy between word alignment and translation rule extraction. On one hand, aligners seek to find the alignment with the highest alignment model score, without regard to structural constraints. Consequently, sensible translation rules may not be extracted becaus"
D15-1144,P06-1077,1,0.862866,"We use black circles to denote links. The link (1, 1) indicates that the first Chinese word “oumeng” and the first English word “EU” are translations of each other. 1229 1. No words in the source phrase are aligned with words outside the target phrase and vice versa: ∀(j, i) ∈ a : j1 ≤ j ≤ j2 ↔ i1 ≤ i ≤ i2 , 2. At least one word in the source phrase is aligned with at least one word in the target phrase: ∃(j, i) ∈ a : j1 ≤ j ≤ j2 ∧ i1 ≤ i ≤ i2 . Alignment consistency forms the basis of translation rule extraction in modern SMT systems (Koehn and Hoang, 2007; Chiang, 2007; Galley et al., 2006; Liu et al., 2006). In Figure 1, (f13 , e31 ) is consistent with the alignment because all words in “oumeng he eluosi” are aligned with all words in “EU and Russia”. In contrast, in Figure 1(b), “huiwu shounao” and “hold summit” are not consistent with the alignment because “hold” is also aligned to a word “juxing” outside. However, alignment consistency only defines a loose relationship between alignment and translation. A phrase pair consistent with alignment tolerates wrong inside links. For example, even if “oumeng” is aligned with “Russia”, (f13 , e31 ) is still consistent. This is one possible reason that"
D15-1144,J10-3002,1,0.893926,"4 ( Similarly, we also distinguish between Cs+t and Cs+l depending on the tightness of extracted phrases. P|f (s) | Algorithm 2 Updating the set of extracted bilingual phrases after adding a link. a∈A(f ,e) (5) where A(f , e) is a set of all possible alignments for the sentence pair. Algorithm 1 shows the consistency-aware search algorithm for word alignment. The input of the algorithm includes a source sentence f , a target sentence e, a set of model parameters θ, phrase length limit w, pruning parameters β and b, and the number of most likely alignments to be retaind n (line 1). Inspired by Liu et al. (2010), 2 Note that training algorithms are unchanged. We only introduce a new search algorithm that takes coverage into consideration. We leave consistency-aware training algorithms for arbitrary alignment models for future work. 1231 the algorithm starts with an empty alignment a together with an empty phrase set B. We use open to store active alignments during search and N to store top-n alignments after search (lines 2-4). The procedure A DD(open, ha, Bi, β, b) adds ha, Bi to open and discards any alignment that has a score worse than β multiplied by the best score in the list or the score of th"
D15-1144,W02-1018,0,0.271474,"Word alignment and translation rule extraction often constitute two consecutive steps in the training pipeline. Wordaligned bilingual corpora serve as a fundamental resource for translation rule extraction, not only for phrase-based models (Koehn et al., 2003; Och and Ney, 2004), but also for syntax-based models (Chiang, 2005; Galley et al., 2006). Dividing alignment and extraction into two separate steps significantly improves the efficiency and scalability of parameter estimation as compared with directly learning translation models from bilingual ∗ Corresponding author: Yang Liu. corpora (Marcu and Wong, 2002; DeNero and Klein, 2008; Cohn and Blunsom, 2009). However, separating word alignment from translation rule extraction suffers from a major problem: maximizing the accuracy of word alignment does not necessarily lead to the improvement of translation quality. A number of studies show that alignment error rate (AER) only has a loose correlation with BLEU (Callison-Burch et al., 2004; Goutte et al., 2004; Ittycheriah and Roukos, 2005). Ayan and Dorr (2006) find that precision-oriented alignments result in better translation performance than recall-oriented alignments. Fraser and Marcu (2007) sho"
D15-1144,J03-1002,0,0.0563828,"h” denotes “hard”, “s” denotes “soft”, “l” denotes “loose”, and “t” denotes “tight”. The BLEU scores were calculated on the development set. For quick validation, we used a small fraction of the training data to train the phrase-based model. 5 5.1 Experiments Setup 5.1.1 Languages and Datasets We evaluated our approach in terms of alignment and translation quality on five language pairs: Chinese-English (ZH-EN), Czech-English (CSEN), German-English (DE-EN), Spanish-English (ES-EN), and French-English (FR-EN). The evaluation metrics for alignment and translation are alignment error rate (AER) (Och and Ney, 2003) and case-insensitive BLEU (Papineni et al., 2002), respectively. For Chinese-English, the training data consists of 1.2M pairs of sentences with 30.9M Chinese words and 35.5M English words. We used the SRILM toolkit (Stolcke, 2002) to train a 4gram language model on the Xinhua portion of the English GIGAWORD corpus, which contains 398.6M words. For alignment evaluation, we used the Tsinghua Chinese-English word alignment evaluation data set (Liu and Sun, 2015). 3 For translation evaluation, we used the NIST 2006 dataset as the development set and the NIST 2002, 2003, 2004, 2005 and 2008 datas"
D15-1144,J04-4002,0,0.798185,"ware search algorithm significantly outperforms both generative and discriminative alignment approaches across various languages and translation models. 1 Introduction Word alignment, which aims to identify the correspondence between words in two languages, plays an important role in statistical machine translation (Brown et al., 1993). Word alignment and translation rule extraction often constitute two consecutive steps in the training pipeline. Wordaligned bilingual corpora serve as a fundamental resource for translation rule extraction, not only for phrase-based models (Koehn et al., 2003; Och and Ney, 2004), but also for syntax-based models (Chiang, 2005; Galley et al., 2006). Dividing alignment and extraction into two separate steps significantly improves the efficiency and scalability of parameter estimation as compared with directly learning translation models from bilingual ∗ Corresponding author: Yang Liu. corpora (Marcu and Wong, 2002; DeNero and Klein, 2008; Cohn and Blunsom, 2009). However, separating word alignment from translation rule extraction suffers from a major problem: maximizing the accuracy of word alignment does not necessarily lead to the improvement of translation quality."
D15-1144,P02-1040,0,0.0963257,"otes “loose”, and “t” denotes “tight”. The BLEU scores were calculated on the development set. For quick validation, we used a small fraction of the training data to train the phrase-based model. 5 5.1 Experiments Setup 5.1.1 Languages and Datasets We evaluated our approach in terms of alignment and translation quality on five language pairs: Chinese-English (ZH-EN), Czech-English (CSEN), German-English (DE-EN), Spanish-English (ES-EN), and French-English (FR-EN). The evaluation metrics for alignment and translation are alignment error rate (AER) (Och and Ney, 2003) and case-insensitive BLEU (Papineni et al., 2002), respectively. For Chinese-English, the training data consists of 1.2M pairs of sentences with 30.9M Chinese words and 35.5M English words. We used the SRILM toolkit (Stolcke, 2002) to train a 4gram language model on the Xinhua portion of the English GIGAWORD corpus, which contains 398.6M words. For alignment evaluation, we used the Tsinghua Chinese-English word alignment evaluation data set (Liu and Sun, 2015). 3 For translation evaluation, we used the NIST 2006 dataset as the development set and the NIST 2002, 2003, 2004, 2005 and 2008 datasets as the test sets. For other languages, the tra"
D15-1144,P08-1066,0,0.0179969,"In this work, we have presented a general framework for optimizing word alignment with respect to machine translation. We introduce coverage to measure how well extracted bilingual phrases can recover the training data. We develop a consistency-aware search algorithm that calculates coverage on the fly during search efficiently. Experiments show the our approach is effective in both alignment and translation tasks across various alignment models, translation models, and language pairs. In the future, we plan to apply our approach to syntax-based models (Galley et al., 2006; Liu et al., 2006; Shen et al., 2008) and include the constituency constraint in the optimization objective. It is also interesting to develop consistency-aware training algorithms for word alignment. Acknowledgements Yang Liu and Maosong Sun are supported by the 863 Program (2015AA011808) and the National Natural Science Foundation of China (No. 61331013 and No. 61432013). Huanbo Luan is supported by the National Natural Science Foundation of China (No. 61303075). This research is also supported by the Singapore National Research Foundation under its International Research Centre@Singapore Funding Initiative and administered by"
D15-1144,J10-2004,0,0.0155466,"ed alignments. Fraser and Marcu (2007) show that using AER and balanced F-measure can only partially explain the effect of alignment quality on BLEU for several language pairs. We believe that the correlation problem arises from the discrepancy between word alignment and translation rule extraction. On one hand, aligners seek to find the alignment with the highest alignment model score, without regard to structural constraints. Consequently, sensible translation rules may not be extracted because they violate consistency constraints required by translation rule extraction (Och and Ney, 2004). Wang et al. (2010) find that the standard alignment tools are not optimal for training syntax-based models. As a result, they have to resort to realigning. On the other hand, the consistency constraint used in most translation rule extraction algorithms tolerate wrong links within consistent phrase pairs. Chiang (2007) uses the union of two unidirectional alignments, which usually has a low precision, for extracting hierarchical phrases. Therefore, it is important to include both alignment model score and the consistency constraint in the optimization objective of word alignment. In this work, we propose to use"
D15-1144,J97-3002,0,0.460318,"ndicate that using forced decoding to select reachable sentences with an unlimited distortion limit runs in O(2n n3 ) time. In contrast, calculating coverage is much easier and more efficient by ignoring the dependency between phrases but still retains the spirit of measuring recovery. 6.2 Structural Constraints for Alignment Modeling structural constraints in alignment has received intensive attention in the community, either directly modeling phrase-to-phrase alignment (Marcu and Wong, 2002; DeNero and Klein, 2008; Cohn and Blunsom, 2009) or intersecting synchronous grammars with alignment (Wu, 1997; Zhang and Gildea, 2005; Haghighi et al., 2009). Our work is in spirit most close to (Deng and Zhou, 2009) and (DeNero and Klein, 2010). Deng and Bowen (2009) cast combining IBM Model 4 alignments in two directions as an optimization problem driven by an effectiveness function. They evaluate the impact of adding or removing a link with respect to phrase extraction using the effectiveness function of phrase count. The major difference is that we generalize their idea to arbitrary alignment models in the search phase rather than bidirectional alignment combination in the post-processing phase."
D15-1144,D13-1112,0,0.0671753,"efinition 4), we need a sentence-level measure to reflect how well an alignment conforms to the consistency constraint. A straightforward measure is the number of bilingual phrases consistent with the alignment (phrase count for short), which is easy and efficient to calculate during search (Deng and Zhou, 2009). Unfortunately, optimizing with respect to phrase count is prone to yield alignments with very few links in a biased way, which result in a large number of bilingual phrases extracted from a small fraction of the training data. Another alternative is reachability (Liang et al., 2006a; Yu et al., 2013) that indicates whether there exists a full derivation to recover the training data. However, calculating reachability faces a major problem: a large portion of training data cannot be fully recovered due to noisy alignments and the distortion limit (Yu et al., 2013). In this work, we propose coverage, which reflects how well extracted phrases can recover the training data, to measure the sentence-level consistency. In the following, we will introduce a number of definitions to facilitate the exposition. Definition 5 A source word fj is said to be covered by a bilingual phrase B = (fjj12 , eii"
D15-1144,P05-1059,0,0.0630576,"at using forced decoding to select reachable sentences with an unlimited distortion limit runs in O(2n n3 ) time. In contrast, calculating coverage is much easier and more efficient by ignoring the dependency between phrases but still retains the spirit of measuring recovery. 6.2 Structural Constraints for Alignment Modeling structural constraints in alignment has received intensive attention in the community, either directly modeling phrase-to-phrase alignment (Marcu and Wong, 2002; DeNero and Klein, 2008; Cohn and Blunsom, 2009) or intersecting synchronous grammars with alignment (Wu, 1997; Zhang and Gildea, 2005; Haghighi et al., 2009). Our work is in spirit most close to (Deng and Zhou, 2009) and (DeNero and Klein, 2010). Deng and Bowen (2009) cast combining IBM Model 4 alignments in two directions as an optimization problem driven by an effectiveness function. They evaluate the impact of adding or removing a link with respect to phrase extraction using the effectiveness function of phrase count. The major difference is that we generalize their idea to arbitrary alignment models in the search phase rather than bidirectional alignment combination in the post-processing phase. In addition, we find tha"
D15-1146,D14-1082,0,0.10557,"units in machine translation have long since shifted from words to phrases (sequence of words), of which syntactic and semantic information cannot be adequately captured and represented by word embeddings. Therefore, learning compact vector representations for phrases or even longer expressions is more crucial for successful “deep” SMT. To address this issue, many efforts have been initiated on learning representations for bilingual phrases in the context of SMT, inspired by the success of work on monolingual phrase embeddings (Socher et al., 2010; Socher et al., 2011a; Socher et al., 2013b; Chen and Manning, 2014; Kalchbrenner et al., 2014; Kim, 2014). The learning process of bilingual phrase embeddings in these efforts is normally interacted and mingled with single or multiple essential components of SMT, e.g., with reordering models (Li et al., 2013), translation models (Cui et al., 2014; Zhang et al., 2014; Gao et al., 2014), or both language and translation models (Liu et al., 2014). In spite of their success, these approaches center around capturing relations between entire source and target phrases. They do not take into account internal phrase structures and bilingual correspondences of sub-phr"
D15-1146,P14-1013,0,0.0880714,"ressions is more crucial for successful “deep” SMT. To address this issue, many efforts have been initiated on learning representations for bilingual phrases in the context of SMT, inspired by the success of work on monolingual phrase embeddings (Socher et al., 2010; Socher et al., 2011a; Socher et al., 2013b; Chen and Manning, 2014; Kalchbrenner et al., 2014; Kim, 2014). The learning process of bilingual phrase embeddings in these efforts is normally interacted and mingled with single or multiple essential components of SMT, e.g., with reordering models (Li et al., 2013), translation models (Cui et al., 2014; Zhang et al., 2014; Gao et al., 2014), or both language and translation models (Liu et al., 2014). In spite of their success, these approaches center around capturing relations between entire source and target phrases. They do not take into account internal phrase structures and bilingual correspondences of sub-phrases within source and target phrases. The neglect of these important clues may be due to the big challenge imposed by the integration of them into the learning process of bilingual phrase representations. However, we believe such internal structures and correspondences can help us"
D15-1146,P14-1129,0,0.0900992,"Missing"
D15-1146,P14-1066,0,0.0451391,"l “deep” SMT. To address this issue, many efforts have been initiated on learning representations for bilingual phrases in the context of SMT, inspired by the success of work on monolingual phrase embeddings (Socher et al., 2010; Socher et al., 2011a; Socher et al., 2013b; Chen and Manning, 2014; Kalchbrenner et al., 2014; Kim, 2014). The learning process of bilingual phrase embeddings in these efforts is normally interacted and mingled with single or multiple essential components of SMT, e.g., with reordering models (Li et al., 2013), translation models (Cui et al., 2014; Zhang et al., 2014; Gao et al., 2014), or both language and translation models (Liu et al., 2014). In spite of their success, these approaches center around capturing relations between entire source and target phrases. They do not take into account internal phrase structures and bilingual correspondences of sub-phrases within source and target phrases. The neglect of these important clues may be due to the big challenge imposed by the integration of them into the learning process of bilingual phrase representations. However, we believe such internal structures and correspondences can help us learn better phrase representations si"
D15-1146,D14-1176,0,0.224828,"on for x, we employ a greedy algorithm (Socher et al., 2011c) to minimize the sum of reconstruction error at each node in the binary tree T (x): X 1 Erec (x; θ) = k [c1 ; c2 ]n −[c01 ; c02 ]n k2 (3) 2 n∈T (x) where θ denotes model parameters and n represents a node in T (x). 2.2 BRAE BRAE jointly learns two RAEs for source and target phrase embeddings as shown in Figure 1(a). The core idea behind BRAE is that a source phrase and its target correct translation should share the same semantic representations, while non-equivalent pairs should have different semantic representations. Zhang et al. (2014) use this intuition to constrain semantic pharse embedding learning. As shown in Figure 2, in addition to the abovementioned reconstruction error, BRAE introduces a max-semantic-margin error to minimize the semantic distance between translation equivalents and maximize the semantic distance between non(5) where Esem (f |e, θ) is defined as the semantic distance between the learned vector representations of f and e, denoted by pf and pe , respectively. Since phrase embeddings for the source and target language are learned separately in different vec(3) tor spaces, a transformation matrix Wf ∈ R"
D15-1146,P14-1006,0,0.0826745,"Missing"
D15-1146,D13-1176,0,0.0714049,"tures, Bengio et al. (2003) convert words to dense, real-valued vectors by learning probability distributions of n-grams. Mikolov et al. (2013) generate word vectors by predicting their limited context words. Instead of exploiting outside context information, recursive auto-encoder is usually adopted to learn the composition of internal words (Socher et al., 2010; Socher et al., 2011b; Socher et al., 2013b; Socher et al., 2013a). Recently, convolution architecture has drawn more and more attention due to its ability to explicitly capture short and long-range relations (Collobert et al., 2011; Kalchbrenner and Blunsom, 2013; Kalchbrenner et al., 2014; Kim, 2014). (2) Bilingual Word/Phrase Embeddings. In the field of machine translation and cross-lingual information processing, bilingual embedding learning has become an increasingly important study. The bilingual embedding research origins in the word embedding learning, upon which Zou et al. (2013) utilize word alignments to constrain translational equivalence. Koˇcisk´y et al. (2014) propose a probability model to capture more semantic information by marginalizing over word alignments. More specifically to SMT, its main components have been exploited to learn b"
D15-1146,P14-1062,0,0.234774,"ation have long since shifted from words to phrases (sequence of words), of which syntactic and semantic information cannot be adequately captured and represented by word embeddings. Therefore, learning compact vector representations for phrases or even longer expressions is more crucial for successful “deep” SMT. To address this issue, many efforts have been initiated on learning representations for bilingual phrases in the context of SMT, inspired by the success of work on monolingual phrase embeddings (Socher et al., 2010; Socher et al., 2011a; Socher et al., 2013b; Chen and Manning, 2014; Kalchbrenner et al., 2014; Kim, 2014). The learning process of bilingual phrase embeddings in these efforts is normally interacted and mingled with single or multiple essential components of SMT, e.g., with reordering models (Li et al., 2013), translation models (Cui et al., 2014; Zhang et al., 2014; Gao et al., 2014), or both language and translation models (Liu et al., 2014). In spite of their success, these approaches center around capturing relations between entire source and target phrases. They do not take into account internal phrase structures and bilingual correspondences of sub-phrases within source and targ"
D15-1146,D14-1181,0,0.0346093,"ed from words to phrases (sequence of words), of which syntactic and semantic information cannot be adequately captured and represented by word embeddings. Therefore, learning compact vector representations for phrases or even longer expressions is more crucial for successful “deep” SMT. To address this issue, many efforts have been initiated on learning representations for bilingual phrases in the context of SMT, inspired by the success of work on monolingual phrase embeddings (Socher et al., 2010; Socher et al., 2011a; Socher et al., 2013b; Chen and Manning, 2014; Kalchbrenner et al., 2014; Kim, 2014). The learning process of bilingual phrase embeddings in these efforts is normally interacted and mingled with single or multiple essential components of SMT, e.g., with reordering models (Li et al., 2013), translation models (Cui et al., 2014; Zhang et al., 2014; Gao et al., 2014), or both language and translation models (Liu et al., 2014). In spite of their success, these approaches center around capturing relations between entire source and target phrases. They do not take into account internal phrase structures and bilingual correspondences of sub-phrases within source and target phrases."
D15-1146,D13-1054,1,0.947567,"tations for phrases or even longer expressions is more crucial for successful “deep” SMT. To address this issue, many efforts have been initiated on learning representations for bilingual phrases in the context of SMT, inspired by the success of work on monolingual phrase embeddings (Socher et al., 2010; Socher et al., 2011a; Socher et al., 2013b; Chen and Manning, 2014; Kalchbrenner et al., 2014; Kim, 2014). The learning process of bilingual phrase embeddings in these efforts is normally interacted and mingled with single or multiple essential components of SMT, e.g., with reordering models (Li et al., 2013), translation models (Cui et al., 2014; Zhang et al., 2014; Gao et al., 2014), or both language and translation models (Liu et al., 2014). In spite of their success, these approaches center around capturing relations between entire source and target phrases. They do not take into account internal phrase structures and bilingual correspondences of sub-phrases within source and target phrases. The neglect of these important clues may be due to the big challenge imposed by the integration of them into the learning process of bilingual phrase representations. However, we believe such internal stru"
D15-1146,P13-1078,0,0.088091,"ic relations within bilingual phrases. In order to examine the effectiveness of BCorrRAE, we incorporate both semantic and structural similarity features built on bilingual phrase representations and tree structures learned by BCorrRAE into a state-of-the-art SMT system. Experiments on NIST Chinese-English test sets show that our model achieves a substantial improvement of up to 1.55 BLEU points over the baseline. 1 Introduction Recently a variety of “deep architecture” approaches, including autoencoders, have been successfully used in statistical machine translation (SMT) (Yang et al., 2013; Liu et al., 2013; Zou et al., 2013; Devlin et al., 2014; Tamura et al., 2014; Sundermeyer et al., 2014; Wang et al., 2014; Koˇcisk´y et al., 2014). Typically, these approaches represent words as dense, low-dimensional and ∗ Corresponding author. real-valued vectors, i.e., word embeddings. However, translation units in machine translation have long since shifted from words to phrases (sequence of words), of which syntactic and semantic information cannot be adequately captured and represented by word embeddings. Therefore, learning compact vector representations for phrases or even longer expressions is more c"
D15-1146,P14-1140,0,0.190425,"on for x, we employ a greedy algorithm (Socher et al., 2011c) to minimize the sum of reconstruction error at each node in the binary tree T (x): X 1 Erec (x; θ) = k [c1 ; c2 ]n −[c01 ; c02 ]n k2 (3) 2 n∈T (x) where θ denotes model parameters and n represents a node in T (x). 2.2 BRAE BRAE jointly learns two RAEs for source and target phrase embeddings as shown in Figure 1(a). The core idea behind BRAE is that a source phrase and its target correct translation should share the same semantic representations, while non-equivalent pairs should have different semantic representations. Zhang et al. (2014) use this intuition to constrain semantic pharse embedding learning. As shown in Figure 2, in addition to the abovementioned reconstruction error, BRAE introduces a max-semantic-margin error to minimize the semantic distance between translation equivalents and maximize the semantic distance between non(5) where Esem (f |e, θ) is defined as the semantic distance between the learned vector representations of f and e, denoted by pf and pe , respectively. Since phrase embeddings for the source and target language are learned separately in different vec(3) tor spaces, a transformation matrix Wf ∈ R"
D15-1146,P02-1038,0,0.273486,"es a maximal entropy classifier based reordering model that predicts orientations of neighboring blocks. During training, we extract bilingual phrases containing up to 7 words on the source side from the training corpus. With the collected reordering examples, we adopt the maximal entropy toolkit3 developed by Zhang to train the reordering model with the following parameters: iteration number iter=200 and gaussian prior g=1.0. Following Xiong et al. (2006), we use only boundary words of blocks to trigger the reordering model. The whole translation model is organized in a log-linear framework (Och and Ney, 2002). The adopted sub-models mainly include: (1) rule translation probabilities in two directions, (2) lexical weights in two directions, (3) targets-side word number, (4) phrase number, (5) language model score, and (6) the score of maximal entropy based reordering model. We perform minimum error rate training (Och, 2003) to tune various feature weights. During decoding, we set ttablelimit=20 for translation candidates kept for each source phrase, stack-size=100 for hypotheses in each span, and swap-span=15 for the length of the maximal reordering span. 5.2 Setup Our bilingual data is the combina"
D15-1146,J03-1002,0,0.00554671,"consistency encoded in bilingual phrase structure learning, which is the basis of our model. Then, we describe the objective function which is composed of three types of errors. Finally, we provide details on the training of our model. 3.1 Structural Alignment Consistency We adapt word alignment to structural alignment and introduce some related concepts. Given a bilingual phrase (f, e) with its binary tree structures (Tf , Te ), if the source node nf¯ ∈ Tf covers a source-side sub-phrase f¯, and there exists a target-side sub-phrase e¯ such that (f¯, e¯) are consistent with word alignments (Och and Ney, 2003), we say nf¯ satisfies the structural alignment consistency, and it is referred to as a structuralalignment-consistent (SAC) node. Further, if e¯ is covered by a target node ne¯ ∈ Te , we say ne¯ is the aligned node of nf¯. In this way, several different target nodes may be all aligned to the same source node because of null alignments. For this, we choose the target node with the smallest span as the aligned one for the considered source node. This is because a smaller span reflects a stronger semantic relevance in most situations. Likewise, we have similar definitions for target nodes. Note"
D15-1146,P03-1021,0,0.0252438,"he reordering model with the following parameters: iteration number iter=200 and gaussian prior g=1.0. Following Xiong et al. (2006), we use only boundary words of blocks to trigger the reordering model. The whole translation model is organized in a log-linear framework (Och and Ney, 2002). The adopted sub-models mainly include: (1) rule translation probabilities in two directions, (2) lexical weights in two directions, (3) targets-side word number, (4) phrase number, (5) language model score, and (6) the score of maximal entropy based reordering model. We perform minimum error rate training (Och, 2003) to tune various feature weights. During decoding, we set ttablelimit=20 for translation candidates kept for each source phrase, stack-size=100 for hypotheses in each span, and swap-span=15 for the length of the maximal reordering span. 5.2 Setup Our bilingual data is the combination of the FBIS corpus and Hansards part of LDC2004T07 corpus, which contains 1.0M parallel sentences (25.2M Chinese words and 29M English words). Following Zhang et al. (2014), we collected 1.44M bilingual phrases using forced decoding (Wuebker et al., 2010) to train BCorrRAE from the training data. We used a 5-gram"
D15-1146,P02-1040,0,0.0961193,"es in each span, and swap-span=15 for the length of the maximal reordering span. 5.2 Setup Our bilingual data is the combination of the FBIS corpus and Hansards part of LDC2004T07 corpus, which contains 1.0M parallel sentences (25.2M Chinese words and 29M English words). Following Zhang et al. (2014), we collected 1.44M bilingual phrases using forced decoding (Wuebker et al., 2010) to train BCorrRAE from the training data. We used a 5-gram language model trained on the Xinhua portion of Gigaword corpus using SRILM Toolkits4 . Translation quality is evaluated by case-insensitive BLEU-4 metric (Papineni et al., 2002). We performed paired bootstrap sampling (Koehn, 2004) to test the significance in BLEU score differences. In our experiments, we used NIST MT05 and MT06/MT08 data set as the 2 A block is a bilingual phrase without maximum length limitation. 3 http://homepages.inf.ed.ac.uk/lzhang10/maxent toolkit.html 4 http://www.speech.sri.com/projects/srilm/download.html Parameter α β γ λL λrec λcon λlcrec BRAE 0.119 4.95 ×10−5 2.64 ×10−7 9.31 ×10−5 BCorrRAE 0.121 0.6331 0.2459 3.13 ×10−5 2.05 ×10−5 7.32 ×10−6 5.25 ×10−6 Table 1: Hyper-parameters for BCorrRAE and BRAE model. Method BCorrRAESM BCorrRAEST d 2"
D15-1146,W04-3250,0,0.191213,"reordering span. 5.2 Setup Our bilingual data is the combination of the FBIS corpus and Hansards part of LDC2004T07 corpus, which contains 1.0M parallel sentences (25.2M Chinese words and 29M English words). Following Zhang et al. (2014), we collected 1.44M bilingual phrases using forced decoding (Wuebker et al., 2010) to train BCorrRAE from the training data. We used a 5-gram language model trained on the Xinhua portion of Gigaword corpus using SRILM Toolkits4 . Translation quality is evaluated by case-insensitive BLEU-4 metric (Papineni et al., 2002). We performed paired bootstrap sampling (Koehn, 2004) to test the significance in BLEU score differences. In our experiments, we used NIST MT05 and MT06/MT08 data set as the 2 A block is a bilingual phrase without maximum length limitation. 3 http://homepages.inf.ed.ac.uk/lzhang10/maxent toolkit.html 4 http://www.speech.sri.com/projects/srilm/download.html Parameter α β γ λL λrec λcon λlcrec BRAE 0.119 4.95 ×10−5 2.64 ×10−7 9.31 ×10−5 BCorrRAE 0.121 0.6331 0.2459 3.13 ×10−5 2.05 ×10−5 7.32 ×10−6 5.25 ×10−6 Table 1: Hyper-parameters for BCorrRAE and BRAE model. Method BCorrRAESM BCorrRAEST d 25 50 75 100 25 50 75 100 MT06 30.81 30.58↓ 30.50 30.34"
D15-1146,D11-1014,0,0.360999,"Missing"
D15-1146,P14-2037,0,0.0449862,"Missing"
D15-1146,P13-1045,0,0.253422,". However, translation units in machine translation have long since shifted from words to phrases (sequence of words), of which syntactic and semantic information cannot be adequately captured and represented by word embeddings. Therefore, learning compact vector representations for phrases or even longer expressions is more crucial for successful “deep” SMT. To address this issue, many efforts have been initiated on learning representations for bilingual phrases in the context of SMT, inspired by the success of work on monolingual phrase embeddings (Socher et al., 2010; Socher et al., 2011a; Socher et al., 2013b; Chen and Manning, 2014; Kalchbrenner et al., 2014; Kim, 2014). The learning process of bilingual phrase embeddings in these efforts is normally interacted and mingled with single or multiple essential components of SMT, e.g., with reordering models (Li et al., 2013), translation models (Cui et al., 2014; Zhang et al., 2014; Gao et al., 2014), or both language and translation models (Liu et al., 2014). In spite of their success, these approaches center around capturing relations between entire source and target phrases. They do not take into account internal phrase structures and bilingual c"
D15-1146,D13-1170,0,0.0484163,". However, translation units in machine translation have long since shifted from words to phrases (sequence of words), of which syntactic and semantic information cannot be adequately captured and represented by word embeddings. Therefore, learning compact vector representations for phrases or even longer expressions is more crucial for successful “deep” SMT. To address this issue, many efforts have been initiated on learning representations for bilingual phrases in the context of SMT, inspired by the success of work on monolingual phrase embeddings (Socher et al., 2010; Socher et al., 2011a; Socher et al., 2013b; Chen and Manning, 2014; Kalchbrenner et al., 2014; Kim, 2014). The learning process of bilingual phrase embeddings in these efforts is normally interacted and mingled with single or multiple essential components of SMT, e.g., with reordering models (Li et al., 2013), translation models (Cui et al., 2014; Zhang et al., 2014; Gao et al., 2014), or both language and translation models (Liu et al., 2014). In spite of their success, these approaches center around capturing relations between entire source and target phrases. They do not take into account internal phrase structures and bilingual c"
D15-1146,D14-1003,0,0.014634,"f BCorrRAE, we incorporate both semantic and structural similarity features built on bilingual phrase representations and tree structures learned by BCorrRAE into a state-of-the-art SMT system. Experiments on NIST Chinese-English test sets show that our model achieves a substantial improvement of up to 1.55 BLEU points over the baseline. 1 Introduction Recently a variety of “deep architecture” approaches, including autoencoders, have been successfully used in statistical machine translation (SMT) (Yang et al., 2013; Liu et al., 2013; Zou et al., 2013; Devlin et al., 2014; Tamura et al., 2014; Sundermeyer et al., 2014; Wang et al., 2014; Koˇcisk´y et al., 2014). Typically, these approaches represent words as dense, low-dimensional and ∗ Corresponding author. real-valued vectors, i.e., word embeddings. However, translation units in machine translation have long since shifted from words to phrases (sequence of words), of which syntactic and semantic information cannot be adequately captured and represented by word embeddings. Therefore, learning compact vector representations for phrases or even longer expressions is more crucial for successful “deep” SMT. To address this issue, many efforts have been initia"
D15-1146,P14-1138,0,0.0135589,"e the effectiveness of BCorrRAE, we incorporate both semantic and structural similarity features built on bilingual phrase representations and tree structures learned by BCorrRAE into a state-of-the-art SMT system. Experiments on NIST Chinese-English test sets show that our model achieves a substantial improvement of up to 1.55 BLEU points over the baseline. 1 Introduction Recently a variety of “deep architecture” approaches, including autoencoders, have been successfully used in statistical machine translation (SMT) (Yang et al., 2013; Liu et al., 2013; Zou et al., 2013; Devlin et al., 2014; Tamura et al., 2014; Sundermeyer et al., 2014; Wang et al., 2014; Koˇcisk´y et al., 2014). Typically, these approaches represent words as dense, low-dimensional and ∗ Corresponding author. real-valued vectors, i.e., word embeddings. However, translation units in machine translation have long since shifted from words to phrases (sequence of words), of which syntactic and semantic information cannot be adequately captured and represented by word embeddings. Therefore, learning compact vector representations for phrases or even longer expressions is more crucial for successful “deep” SMT. To address this issue, man"
D15-1146,D14-1175,0,0.0183492,"learning has become an increasingly important study. The bilingual embedding research origins in the word embedding learning, upon which Zou et al. (2013) utilize word alignments to constrain translational equivalence. Koˇcisk´y et al. (2014) propose a probability model to capture more semantic information by marginalizing over word alignments. More specifically to SMT, its main components have been exploited to learn better bilingual phrase embeddings in different aspects: language models (Wang et al., 2014; Garmash and Monz, 2014), reordering models (Li et al., 2013) and translation models (Tran et al., 2014; Zhang et al., 2014). Instead of exploiting a single model, Liu et al. (2014) combine the recursive and recurrent neural network to incorporate the language and translation model. Different from the methods mentioned above, our model considers both the cross-language consistency of phrase structures and internal correspondence relations inside bilingual phrases. The most related works include Zhang et al. (2014) and Socher et al. (2011a). Compared with these works, our model exploits different levels of correspondence relations inside bilingual phrases instead of only the top level of entire"
D15-1146,D14-1023,0,0.0803451,"both semantic and structural similarity features built on bilingual phrase representations and tree structures learned by BCorrRAE into a state-of-the-art SMT system. Experiments on NIST Chinese-English test sets show that our model achieves a substantial improvement of up to 1.55 BLEU points over the baseline. 1 Introduction Recently a variety of “deep architecture” approaches, including autoencoders, have been successfully used in statistical machine translation (SMT) (Yang et al., 2013; Liu et al., 2013; Zou et al., 2013; Devlin et al., 2014; Tamura et al., 2014; Sundermeyer et al., 2014; Wang et al., 2014; Koˇcisk´y et al., 2014). Typically, these approaches represent words as dense, low-dimensional and ∗ Corresponding author. real-valued vectors, i.e., word embeddings. However, translation units in machine translation have long since shifted from words to phrases (sequence of words), of which syntactic and semantic information cannot be adequately captured and represented by word embeddings. Therefore, learning compact vector representations for phrases or even longer expressions is more crucial for successful “deep” SMT. To address this issue, many efforts have been initiated on learning rep"
D15-1146,D14-1015,0,0.0152033,"pecially the BCorrRAEST model, tends to choose shorter translations that are consistent with word alignments. 6 Related Work A variety of efforts have been devoted to learning vector representations for words/phrases with deep neural networks. According to the difference of learning contexts, previous work mainly include the following two strands. (1) Monolingual Word/Phrase Embeddings. The straightforward approach to represent word/phrases is to learn their hidden representations with traditional feature vectors, which requires manual and task-dependent feature engineering (Cui et al., 2014; Wu et al., 2014; 1255 Source Phrase 䌓㥎 (advocate) 惮䜃 坝揔 (serious challenge) 䋺㟙 䀛 嗪䛢 (data released) BRAE to advocate the in preaching the the promotion of as well as severe challenges a serious challenge to a serious challenge from by the figures published by the the statistics released by data published by the BCorrRAESM out to advocate been encouraging an advocate of rigorous challenges as well as severe challenges of severe challenges to the estimates announced at the figures published the statistics released by BCorrRAEST encouraging claimed advocate rigorous challenge enormous challenge severe challenge"
D15-1146,J97-3002,0,0.606435,"de n, and Cf and Ce count the number of nodes in the source and target tree structure respectively. Note that if we only compute the similarity for root nodes in the bilingual tree of (f, e), the structural similarity equals to the semantic similarity in Eq. (19). 5 Experiments We conducted experiments on NIST ChineseEnglish translation task to validate the effectiveness of BCorrRAE. 5.1 System Overview Our baseline decoder is a state-of-the-art phrasebased translation system equipped with a maximum entropy based reordering model (MEBTG). It adopts three bracketing transduction grammar rules (Wu, 1997; Xiong et al., 2006): merging 1253 rules A → [A1 , A2 ]|hA1 , A2 i which are used to merge two neighboring blocks2 A1 and A2 in a straight|inverted order, and lexical rule A → f /e used to translate a source phrase f into a target phrase e. The MEBTG system features a maximal entropy classifier based reordering model that predicts orientations of neighboring blocks. During training, we extract bilingual phrases containing up to 7 words on the source side from the training corpus. With the collected reordering examples, we adopt the maximal entropy toolkit3 developed by Zhang to train the reor"
D15-1146,P10-1049,0,0.0513633,"ntropy based reordering model. We perform minimum error rate training (Och, 2003) to tune various feature weights. During decoding, we set ttablelimit=20 for translation candidates kept for each source phrase, stack-size=100 for hypotheses in each span, and swap-span=15 for the length of the maximal reordering span. 5.2 Setup Our bilingual data is the combination of the FBIS corpus and Hansards part of LDC2004T07 corpus, which contains 1.0M parallel sentences (25.2M Chinese words and 29M English words). Following Zhang et al. (2014), we collected 1.44M bilingual phrases using forced decoding (Wuebker et al., 2010) to train BCorrRAE from the training data. We used a 5-gram language model trained on the Xinhua portion of Gigaword corpus using SRILM Toolkits4 . Translation quality is evaluated by case-insensitive BLEU-4 metric (Papineni et al., 2002). We performed paired bootstrap sampling (Koehn, 2004) to test the significance in BLEU score differences. In our experiments, we used NIST MT05 and MT06/MT08 data set as the 2 A block is a bilingual phrase without maximum length limitation. 3 http://homepages.inf.ed.ac.uk/lzhang10/maxent toolkit.html 4 http://www.speech.sri.com/projects/srilm/download.html Pa"
D15-1146,P06-1066,1,0.784584,"Cf and Ce count the number of nodes in the source and target tree structure respectively. Note that if we only compute the similarity for root nodes in the bilingual tree of (f, e), the structural similarity equals to the semantic similarity in Eq. (19). 5 Experiments We conducted experiments on NIST ChineseEnglish translation task to validate the effectiveness of BCorrRAE. 5.1 System Overview Our baseline decoder is a state-of-the-art phrasebased translation system equipped with a maximum entropy based reordering model (MEBTG). It adopts three bracketing transduction grammar rules (Wu, 1997; Xiong et al., 2006): merging 1253 rules A → [A1 , A2 ]|hA1 , A2 i which are used to merge two neighboring blocks2 A1 and A2 in a straight|inverted order, and lexical rule A → f /e used to translate a source phrase f into a target phrase e. The MEBTG system features a maximal entropy classifier based reordering model that predicts orientations of neighboring blocks. During training, we extract bilingual phrases containing up to 7 words on the source side from the training corpus. With the collected reordering examples, we adopt the maximal entropy toolkit3 developed by Zhang to train the reordering model with the"
D15-1146,P13-1017,0,0.0236294,"nt levels of semantic relations within bilingual phrases. In order to examine the effectiveness of BCorrRAE, we incorporate both semantic and structural similarity features built on bilingual phrase representations and tree structures learned by BCorrRAE into a state-of-the-art SMT system. Experiments on NIST Chinese-English test sets show that our model achieves a substantial improvement of up to 1.55 BLEU points over the baseline. 1 Introduction Recently a variety of “deep architecture” approaches, including autoencoders, have been successfully used in statistical machine translation (SMT) (Yang et al., 2013; Liu et al., 2013; Zou et al., 2013; Devlin et al., 2014; Tamura et al., 2014; Sundermeyer et al., 2014; Wang et al., 2014; Koˇcisk´y et al., 2014). Typically, these approaches represent words as dense, low-dimensional and ∗ Corresponding author. real-valued vectors, i.e., word embeddings. However, translation units in machine translation have long since shifted from words to phrases (sequence of words), of which syntactic and semantic information cannot be adequately captured and represented by word embeddings. Therefore, learning compact vector representations for phrases or even longer exp"
D15-1146,P14-1011,0,0.106705,"rucial for successful “deep” SMT. To address this issue, many efforts have been initiated on learning representations for bilingual phrases in the context of SMT, inspired by the success of work on monolingual phrase embeddings (Socher et al., 2010; Socher et al., 2011a; Socher et al., 2013b; Chen and Manning, 2014; Kalchbrenner et al., 2014; Kim, 2014). The learning process of bilingual phrase embeddings in these efforts is normally interacted and mingled with single or multiple essential components of SMT, e.g., with reordering models (Li et al., 2013), translation models (Cui et al., 2014; Zhang et al., 2014; Gao et al., 2014), or both language and translation models (Liu et al., 2014). In spite of their success, these approaches center around capturing relations between entire source and target phrases. They do not take into account internal phrase structures and bilingual correspondences of sub-phrases within source and target phrases. The neglect of these important clues may be due to the big challenge imposed by the integration of them into the learning process of bilingual phrase representations. However, we believe such internal structures and correspondences can help us learn better phrase"
D15-1146,D13-1141,0,0.299247,"n bilingual phrases. In order to examine the effectiveness of BCorrRAE, we incorporate both semantic and structural similarity features built on bilingual phrase representations and tree structures learned by BCorrRAE into a state-of-the-art SMT system. Experiments on NIST Chinese-English test sets show that our model achieves a substantial improvement of up to 1.55 BLEU points over the baseline. 1 Introduction Recently a variety of “deep architecture” approaches, including autoencoders, have been successfully used in statistical machine translation (SMT) (Yang et al., 2013; Liu et al., 2013; Zou et al., 2013; Devlin et al., 2014; Tamura et al., 2014; Sundermeyer et al., 2014; Wang et al., 2014; Koˇcisk´y et al., 2014). Typically, these approaches represent words as dense, low-dimensional and ∗ Corresponding author. real-valued vectors, i.e., word embeddings. However, translation units in machine translation have long since shifted from words to phrases (sequence of words), of which syntactic and semantic information cannot be adequately captured and represented by word embeddings. Therefore, learning compact vector representations for phrases or even longer expressions is more crucial for success"
D15-1210,J93-2003,0,0.172973,"gnificant improvements over two state-ofthe-art alignment methods. 1 Instead of using heuristic symmetrization, Liang et al. (2006) introduce a principled approach that encourages the agreement between asymmetric alignments in two directions. The basic idea is to favor links on which both unidirectional models agree. They associate two models via the agreement constraint and show that agreement-based joint training improves alignment accuracy significantly. Introduction Word alignment is a natural language processing task that aims to specify the correspondence between words in two languages (Brown et al., 1993). It plays an important role in statistical machine translation (SMT) as word-aligned bilingual corpora serve as the input of translation rule extraction (Koehn et al., 2003; Chiang, 2007; Galley et al., 2006; Liu et al., 2006). Although state-of-the-art generative alignment models (Brown et al., 1993; Vogel et al., 1996) have been widely used in practical SMT systems, they fail to model the symmetry of word alignment. While word alignments in real-world bilingual data usually exhibit complicated mappings (i.e., mixed with one-to-one, one-to-many, manyto-one, and many-to-many links), these mod"
D15-1210,N10-1015,0,0.0575352,"Missing"
D15-1210,P10-1065,0,0.0238281,"n word alignment and other structures such as phrase segmentations and parse trees. 6.2 Joint Modeling of Multiple NLP Tasks It is well accepted that different NLP tasks can help each other by providing additional information for resolving ambiguities. As a result, joint modeling of multiple NLP tasks has received intensive attention in recent years, including phrase segmentation and alignment (Zhang et al., 2003), alignment and parsing (Burkett et al., 2010), tokenization and translation (Xiao et al., 2010), parsing and translation (Liu and Liu, 2010), alignment and named entity recognition (Chen et al., 2010; Wang et al., 2013). Among them, Zhang et al. (2003)’s integrated search algorithm for phrase segmentation and alignment is most close to our work. They use Point-wise Mutual Information to identify possible phrase pairs. The major difference is we train models jointly instead of integrated decoding. 7 Conclusion We have presented generalized agreement for bidirectional word alignment. The loss functions can be defined both between asymmetric alignments and between alignments and other latent structures such as phrase segmentations. We develop a Viterbi EM algorithm to train the joint model."
D15-1210,J07-2003,0,0.502152,"between asymmetric alignments in two directions. The basic idea is to favor links on which both unidirectional models agree. They associate two models via the agreement constraint and show that agreement-based joint training improves alignment accuracy significantly. Introduction Word alignment is a natural language processing task that aims to specify the correspondence between words in two languages (Brown et al., 1993). It plays an important role in statistical machine translation (SMT) as word-aligned bilingual corpora serve as the input of translation rule extraction (Koehn et al., 2003; Chiang, 2007; Galley et al., 2006; Liu et al., 2006). Although state-of-the-art generative alignment models (Brown et al., 1993; Vogel et al., 1996) have been widely used in practical SMT systems, they fail to model the symmetry of word alignment. While word alignments in real-world bilingual data usually exhibit complicated mappings (i.e., mixed with one-to-one, one-to-many, manyto-one, and many-to-many links), these models assume that each target word is aligned to exactly ∗ Corresponding author: Yang Liu. However, enforcing agreement in joint training faces a major problem: the two models are restricte"
D15-1210,P06-1121,0,0.0607179,"tric alignments in two directions. The basic idea is to favor links on which both unidirectional models agree. They associate two models via the agreement constraint and show that agreement-based joint training improves alignment accuracy significantly. Introduction Word alignment is a natural language processing task that aims to specify the correspondence between words in two languages (Brown et al., 1993). It plays an important role in statistical machine translation (SMT) as word-aligned bilingual corpora serve as the input of translation rule extraction (Koehn et al., 2003; Chiang, 2007; Galley et al., 2006; Liu et al., 2006). Although state-of-the-art generative alignment models (Brown et al., 1993; Vogel et al., 1996) have been widely used in practical SMT systems, they fail to model the symmetry of word alignment. While word alignments in real-world bilingual data usually exhibit complicated mappings (i.e., mixed with one-to-one, one-to-many, manyto-one, and many-to-many links), these models assume that each target word is aligned to exactly ∗ Corresponding author: Yang Liu. However, enforcing agreement in joint training faces a major problem: the two models are restricted to one-to-one align"
D15-1210,D07-1091,0,0.657698,"telligent Technology and Systems Tsinghua National Laboratory for Information Science and Technology Department of Computer Science and Technology, Tsinghua University, Beijing 100084, China ‡ Samsung R&D Institute of China, Beijing 100028, China {liuchunyang2012,liuyang.china,luanhuanbo}@gmail.com, sms@tsinghua.edu.cn h0517.yu@samsung.com Abstract one source word. To alleviate this problem, heuristic methods (e.g., grow-diag-final) have been proposed to combine two asymmetric alignments (source-to-target and target-to-source) to generate symmetric bidirectional alignments (Och and Ney, 2003; Koehn and Hoang, 2007). While agreement-based joint training has proven to deliver state-of-the-art alignment accuracy, the produced word alignments are usually restricted to one-toone mappings because of the hard constraint on agreement. We propose a general framework to allow for arbitrary loss functions that measure the disagreement between asymmetric alignments. The loss functions can not only be defined between asymmetric alignments but also between alignments and other latent structures such as phrase segmentations. We use a Viterbi EM algorithm to train the joint model since the inference is intractable. Exp"
D15-1210,N03-1017,0,0.105236,"Missing"
D15-1210,N06-1014,0,0.273714,"t. We propose a general framework to allow for arbitrary loss functions that measure the disagreement between asymmetric alignments. The loss functions can not only be defined between asymmetric alignments but also between alignments and other latent structures such as phrase segmentations. We use a Viterbi EM algorithm to train the joint model since the inference is intractable. Experiments on ChineseEnglish translation show that joint training with generalized agreement achieves significant improvements over two state-ofthe-art alignment methods. 1 Instead of using heuristic symmetrization, Liang et al. (2006) introduce a principled approach that encourages the agreement between asymmetric alignments in two directions. The basic idea is to favor links on which both unidirectional models agree. They associate two models via the agreement constraint and show that agreement-based joint training improves alignment accuracy significantly. Introduction Word alignment is a natural language processing task that aims to specify the correspondence between words in two languages (Brown et al., 1993). It plays an important role in statistical machine translation (SMT) as word-aligned bilingual corpora serve as"
D15-1210,C10-1080,1,0.851806,"that our framework allows for including the agreement between word alignment and other structures such as phrase segmentations and parse trees. 6.2 Joint Modeling of Multiple NLP Tasks It is well accepted that different NLP tasks can help each other by providing additional information for resolving ambiguities. As a result, joint modeling of multiple NLP tasks has received intensive attention in recent years, including phrase segmentation and alignment (Zhang et al., 2003), alignment and parsing (Burkett et al., 2010), tokenization and translation (Xiao et al., 2010), parsing and translation (Liu and Liu, 2010), alignment and named entity recognition (Chen et al., 2010; Wang et al., 2013). Among them, Zhang et al. (2003)’s integrated search algorithm for phrase segmentation and alignment is most close to our work. They use Point-wise Mutual Information to identify possible phrase pairs. The major difference is we train models jointly instead of integrated decoding. 7 Conclusion We have presented generalized agreement for bidirectional word alignment. The loss functions can be defined both between asymmetric alignments and between alignments and other latent structures such as phrase segmentations. W"
D15-1210,P06-1077,1,0.755036,"o directions. The basic idea is to favor links on which both unidirectional models agree. They associate two models via the agreement constraint and show that agreement-based joint training improves alignment accuracy significantly. Introduction Word alignment is a natural language processing task that aims to specify the correspondence between words in two languages (Brown et al., 1993). It plays an important role in statistical machine translation (SMT) as word-aligned bilingual corpora serve as the input of translation rule extraction (Koehn et al., 2003; Chiang, 2007; Galley et al., 2006; Liu et al., 2006). Although state-of-the-art generative alignment models (Brown et al., 1993; Vogel et al., 1996) have been widely used in practical SMT systems, they fail to model the symmetry of word alignment. While word alignments in real-world bilingual data usually exhibit complicated mappings (i.e., mixed with one-to-one, one-to-many, manyto-one, and many-to-many links), these models assume that each target word is aligned to exactly ∗ Corresponding author: Yang Liu. However, enforcing agreement in joint training faces a major problem: the two models are restricted to one-to-one alignments (Liang et al."
D15-1210,J03-1002,0,0.269429,"ey Laboratory of Intelligent Technology and Systems Tsinghua National Laboratory for Information Science and Technology Department of Computer Science and Technology, Tsinghua University, Beijing 100084, China ‡ Samsung R&D Institute of China, Beijing 100028, China {liuchunyang2012,liuyang.china,luanhuanbo}@gmail.com, sms@tsinghua.edu.cn h0517.yu@samsung.com Abstract one source word. To alleviate this problem, heuristic methods (e.g., grow-diag-final) have been proposed to combine two asymmetric alignments (source-to-target and target-to-source) to generate symmetric bidirectional alignments (Och and Ney, 2003; Koehn and Hoang, 2007). While agreement-based joint training has proven to deliver state-of-the-art alignment accuracy, the produced word alignments are usually restricted to one-toone mappings because of the hard constraint on agreement. We propose a general framework to allow for arbitrary loss functions that measure the disagreement between asymmetric alignments. The loss functions can not only be defined between asymmetric alignments but also between alignments and other latent structures such as phrase segmentations. We use a Viterbi EM algorithm to train the joint model since the infer"
D15-1210,P02-1040,0,0.0951639,".2M sentence pairs with 32M Chinese words and 35.4M English words. We used the SRILM toolkit (Stolcke, 2002) to train a 4-gram language model on the Xinhua portion of the English GIGAWORD corpus, which contains 398.6M words. For alignment evaluation, we used the Tsinghua Chinese-English word alignment evaluation data set.1 The evaluation metric is alignment error rate (AER) (Och and Ney, 2003). For translation evaluation, we used the NIST 2006 dataset as the development set and the NIST 2002, 2003, 2004, 2005, and 2008 datasets as the test sets. The evaluation metric is case-insensitive BLEU (Papineni et al., 2002). We used both phrase-based (Koehn et al., 2003) and hierarchical phrase-based (Chiang, 2007) translation systems to evaluate whether our approach improves translation performance. For the phrase-based model, we used the open-source toolkit Moses (Koehn and Hoang, 2007). For the hierarchical phrase-based model, we used an inhouse re-implementation on par with state-of-theart open-source decoders. We compared our approach with two state-ofthe-art generative alignment models: 1. G IZA ++ (Och and Ney, 2003): unsupervised training of IBM models (Brown et al., 1993) and the HMM model (Vogel et al."
D15-1210,C96-2141,0,0.919358,"associate two models via the agreement constraint and show that agreement-based joint training improves alignment accuracy significantly. Introduction Word alignment is a natural language processing task that aims to specify the correspondence between words in two languages (Brown et al., 1993). It plays an important role in statistical machine translation (SMT) as word-aligned bilingual corpora serve as the input of translation rule extraction (Koehn et al., 2003; Chiang, 2007; Galley et al., 2006; Liu et al., 2006). Although state-of-the-art generative alignment models (Brown et al., 1993; Vogel et al., 1996) have been widely used in practical SMT systems, they fail to model the symmetry of word alignment. While word alignments in real-world bilingual data usually exhibit complicated mappings (i.e., mixed with one-to-one, one-to-many, manyto-one, and many-to-many links), these models assume that each target word is aligned to exactly ∗ Corresponding author: Yang Liu. However, enforcing agreement in joint training faces a major problem: the two models are restricted to one-to-one alignments (Liang et al., 2006). This significantly limits the translation accuracy, especially for distantly-related la"
D15-1210,P13-1106,0,0.0353913,"Missing"
D15-1210,C10-1135,1,0.818061,"case of our framework. Another difference is that our framework allows for including the agreement between word alignment and other structures such as phrase segmentations and parse trees. 6.2 Joint Modeling of Multiple NLP Tasks It is well accepted that different NLP tasks can help each other by providing additional information for resolving ambiguities. As a result, joint modeling of multiple NLP tasks has received intensive attention in recent years, including phrase segmentation and alignment (Zhang et al., 2003), alignment and parsing (Burkett et al., 2010), tokenization and translation (Xiao et al., 2010), parsing and translation (Liu and Liu, 2010), alignment and named entity recognition (Chen et al., 2010; Wang et al., 2013). Among them, Zhang et al. (2003)’s integrated search algorithm for phrase segmentation and alignment is most close to our work. They use Point-wise Mutual Information to identify possible phrase pairs. The major difference is we train models jointly instead of integrated decoding. 7 Conclusion We have presented generalized agreement for bidirectional word alignment. The loss functions can be defined both between asymmetric alignments and between alignments and other late"
D16-1130,W05-0613,0,0.03059,"ces to form a coherent text. Automatically recognizing discourse relations can help many downstream tasks such as question answering and automatic summarization. Despite great progress in classifying explicit discourse relations where the discourse connectives (e.g., “because”, “but”) explicitly exist in the text, implicit discourse relation recognition remains a challenge due to the absence of discourse connectives. Previous research mainly focus on exploring various kinds of efficient features and machine learning models to classify the implicit discourse relations (Soricut and Marcu, 2003; Baldridge and Lascarides, 2005; Subba and Di Eugenio, 2009; Hernault et al., 2010; Pitler et al., 2009; Joty et al., 2012). To some extent, these methods simulate the single-pass reading process that a person quickly skim the text through one-pass reading and directly collect important clues for understanding the text. Although single-pass reading plays a crucial role when we just want the general meaning and do not necessarily need to understand every single point of the text, it is not enough for tackling tasks that need a deep analysis of the text. In contrast with single-pass reading, repeated reading involves the proc"
D16-1130,D15-1262,0,0.470245,"ic parse tree to induce the representation of the arguments and the entity spans. • Zhang2015: Zhang et al. (2015) proposed to use shallow convolutional neural networks to model two arguments respectively. We replicated their model since they used a different setting in preprocessing PDTB. • R&X2014, R&X2015: Rutherford and Xue (2014) selected lexical features, production rules, and Brown cluster pairs, and fed them into a maximum entropy classifier. Rutherford and Xue (2015) further proposed to gather extra weakly labeled data based on the discourse connectives for the classifier. • B&D2015: Braud and Denis (2015) combined several hand-crafted lexical features and word embeddings to train a max-entropy classifier. It is noted that P&C2012 and J&E2015 merged the “EntRel” relation into the “Expansion” relation1 . For a comprehensive comparison, we also experiment our model by adding a Expa.+EntRel vs Other classification. Our NNMA model with two attention levels exhibits obvious advantages over the six baseline methods on the whole. It is worth noting that NNMA is even better than the R&X2015 approach which employs extra data. As for the performance on each discourse relation, with respect to the F1 meas"
D16-1130,Q15-1024,0,0.622009,"2), we use the following formulae for Arg-1. where Wp ∈ Rn×6d and bp ∈ Rn are the transformation weights. 2.3 Model Training To train our model, the training objective is defined as the cross-entropy loss between the outputs of the softmax layer and the ground-truth class labels. We use stochastic gradient descent (SGD) with momentum to train the neural networks. To avoid over-fitting, dropout operation is applied on the top feature vector before the softmax layer. Also, we use different learning rates λ and λe to train the neural network parameters Θ and the word embeddings Θe referring to (Ji and Eisenstein, 2015). λe is set to a small value for preventing overfitting on this task. In the experimental part, we will introduce the setting of the hyper-parameters. 3 3.1 Experiments Preparation We evaluate our model on the Penn Discourse Treebank (PDTB) (Prasad et al., 2008). In our work, we experiment on the four top-level classes in this corpus as in previous work (Rutherford and Xue, 2015). We extract all the implicit relations of PDTB, and follow the setup of (Rutherford and Xue, 2015). We split the data into a training set (Sections 220), development set (Sections 0-1), and test set (Section 21-22). T"
D16-1130,N16-1037,0,0.416089,"than the R&X2015 approach which employs extra data. As for the performance on each discourse relation, with respect to the F1 measure, we can see that our NNMA model can achieve the best results on the “Expansion”, “Expansion+EntRel” and “Temporal” relations and competitive results on the “Contingency” relation . The performance of recognizing the “Comparison” relation is only worse than R&X2014 and R&X2015. As (Rutherford and Xue, 2014) stated, the “Comparison” relation is closely related to the constituent parse feature of the text, like production rules. How to represent and 1229 • Ji2016: Ji et al. (2016) proposed a neural language model over sequences of words and used the discourse relations as latent variables to connect the adjacent sequences. 1 EntRel is the entity-based coherence relation which is independent of implicit and explicit relations in PDTB. However some research merges it into the implicit Expansion relation. exploit these information in our model will be our next research focus. 3.3 Analysis of Attention Levels The multiple attention levels in our model greatly boost the performance of classifying implicit discourse relations. In this subsection, we perform both qualitative"
D16-1130,D12-1083,0,0.0207705,"s such as question answering and automatic summarization. Despite great progress in classifying explicit discourse relations where the discourse connectives (e.g., “because”, “but”) explicitly exist in the text, implicit discourse relation recognition remains a challenge due to the absence of discourse connectives. Previous research mainly focus on exploring various kinds of efficient features and machine learning models to classify the implicit discourse relations (Soricut and Marcu, 2003; Baldridge and Lascarides, 2005; Subba and Di Eugenio, 2009; Hernault et al., 2010; Pitler et al., 2009; Joty et al., 2012). To some extent, these methods simulate the single-pass reading process that a person quickly skim the text through one-pass reading and directly collect important clues for understanding the text. Although single-pass reading plays a crucial role when we just want the general meaning and do not necessarily need to understand every single point of the text, it is not enough for tackling tasks that need a deep analysis of the text. In contrast with single-pass reading, repeated reading involves the process where learners repeatedly read the text in detail with specific learning aims, and has t"
D16-1130,D14-1181,0,0.00743079,"th Expansion relation Figure 2: Visualization Examples: Illustrating Attentions Learned by NNMA. (The blue grid means the the attention on this word is lower than the value of a uniform distribution and the red red grid means the attention is higher than that.) Arg-1 Arg-2 0.6 0.5 0.4 0.3 0.2 0.1 0 l Figure 3: KL-divergences between attention levels latent variables connecting two token sequences and trained a discourse informed language model. 4.2 Neural Networks and Attention Mechanism Recently, neural network-based methods have gained prominence in the field of natural language processing (Kim, 2014). Such methods are primarily based on learning a distributed representation for each word, which is also called a word embedding (Collobert et al., 2011). Attention mechanism was first introduced into neural models to solve the alignment problem 1231 between different modalities. Graves (2013) designed a neural network to generate handwriting based on a text. It assigned a window on the input text at each step and generate characters based on the content within the window. Bahdanau et al. (2014) introduced this idea into machine translation, where their model computed a probabilistic distribut"
D16-1130,D09-1036,0,0.666984,"ided for annotation. In our study, we use the four top-level tags, including Temporal, Contingency, Comparison and Expansion. These four core relations allow us to be theory-neutral, since they are almost included in all discourse theories, sometimes with different names (Wang et al., 2012). Implicit discourse relation recognition is often treated as a classification problem. The first work to tackle this task on PDTB is (Pitler et al., 2009). They selected several surface features to train four binary classifiers, each for one of the top-level PDTB relation classes. Extending from this work, Lin et al. (2009) further identified four different feature types representing the context, the constituent parse trees, the dependency parse trees and the raw text respectively. Rutherford and Xue (2014) used brown cluster to replace the word pair features for solving the sparsity problem. Ji and Eisenstein (2015) adopted two recursive neural networks to exploit the representation of arguments and entity spans. Very recently, Liu et al. (2016) proposed a twodimensional convolutional neural network (CNN) to model the argument pairs and employed a multitask learning framework to boost the performance by learnin"
D16-1130,N13-1090,0,0.0262584,"attention level, we adopt the attention mechanism to determine which words should be focused on. An external short-term memory is designed to remember what has seen in the prior levels and guide the attention tuning process in current level. Specifically, in the first attention level, we concatenate R01 , R02 and R01 −R02 and apply a non-linear transformation over the concatenation to catch the general understanding of the argument pair. The use of R01 −R02 takes a cue from the difference between two vector representations which has been found explainable and meaningful in many applications (Mikolov et al., 2013). Then, we get the memory vector M1 ∈ Rdm of the first attention level as M1 = tanh(Wm,1 [R01 , R02 , R01 −R02 ]) (9) where Wm,1 ∈ Rdm ×6d is the weight matrix. With M1 recording the general meaning of the argument pair, our model re-calculates the importance of each word. We assign each word a weight measuring to what degree our model should pay attention to it. The weights are so-called “attention” in our paper. This process is designed to simulate the process that we re-read the arguments and pay more attention to some specific words with an overall understanding derived from the first-pass"
D16-1130,W12-1614,0,0.16192,"he State-of-the-art Approaches. that the “Comparison” relation needs more passes of reading compared to the other three relations. The reason may be that the identification of the “Comparison” depends more on some deep analysis such as semantic parsing, according to (Zhou et al., 2010). Next, we compare our models with six state-ofthe-art baseline approaches, as shown in Table 4. The six baselines are introduced as follows. • Liu2016: Liu et al. (2016) proposed to better classify the discourse relations by learning from other discourse-related tasks with a multitask neural network. • P&C2012: Park and Cardie (2012) designed a feature-based method and promoted the performance through optimizing the feature set. • J&E2015: Ji and Eisenstein (2015) used two recursive neural networks on the syntactic parse tree to induce the representation of the arguments and the entity spans. • Zhang2015: Zhang et al. (2015) proposed to use shallow convolutional neural networks to model two arguments respectively. We replicated their model since they used a different setting in preprocessing PDTB. • R&X2014, R&X2015: Rutherford and Xue (2014) selected lexical features, production rules, and Brown cluster pairs, and fed th"
D16-1130,D14-1162,0,0.0873507,"Missing"
D16-1130,P09-1077,0,0.565611,"many downstream tasks such as question answering and automatic summarization. Despite great progress in classifying explicit discourse relations where the discourse connectives (e.g., “because”, “but”) explicitly exist in the text, implicit discourse relation recognition remains a challenge due to the absence of discourse connectives. Previous research mainly focus on exploring various kinds of efficient features and machine learning models to classify the implicit discourse relations (Soricut and Marcu, 2003; Baldridge and Lascarides, 2005; Subba and Di Eugenio, 2009; Hernault et al., 2010; Pitler et al., 2009; Joty et al., 2012). To some extent, these methods simulate the single-pass reading process that a person quickly skim the text through one-pass reading and directly collect important clues for understanding the text. Although single-pass reading plays a crucial role when we just want the general meaning and do not necessarily need to understand every single point of the text, it is not enough for tackling tasks that need a deep analysis of the text. In contrast with single-pass reading, repeated reading involves the process where learners repeatedly read the text in detail with specific lear"
D16-1130,prasad-etal-2008-penn,0,0.897266,"h class labels. We use stochastic gradient descent (SGD) with momentum to train the neural networks. To avoid over-fitting, dropout operation is applied on the top feature vector before the softmax layer. Also, we use different learning rates λ and λe to train the neural network parameters Θ and the word embeddings Θe referring to (Ji and Eisenstein, 2015). λe is set to a small value for preventing overfitting on this task. In the experimental part, we will introduce the setting of the hyper-parameters. 3 3.1 Experiments Preparation We evaluate our model on the Penn Discourse Treebank (PDTB) (Prasad et al., 2008). In our work, we experiment on the four top-level classes in this corpus as in previous work (Rutherford and Xue, 2015). We extract all the implicit relations of PDTB, and follow the setup of (Rutherford and Xue, 2015). We split the data into a training set (Sections 220), development set (Sections 0-1), and test set (Section 21-22). Table 1 summarizes the statistics of the four PDTB discourse relations, i.e., Comparison, Contingency, Expansion and Temporal. Relation Comparison Contingency Expansion Temporal Total Train 1855 3235 6673 582 12345 Dev 189 281 638 48 1156 Test 145 273 538 55 1011"
D16-1130,E14-1068,0,0.574712,"ning from other discourse-related tasks with a multitask neural network. • P&C2012: Park and Cardie (2012) designed a feature-based method and promoted the performance through optimizing the feature set. • J&E2015: Ji and Eisenstein (2015) used two recursive neural networks on the syntactic parse tree to induce the representation of the arguments and the entity spans. • Zhang2015: Zhang et al. (2015) proposed to use shallow convolutional neural networks to model two arguments respectively. We replicated their model since they used a different setting in preprocessing PDTB. • R&X2014, R&X2015: Rutherford and Xue (2014) selected lexical features, production rules, and Brown cluster pairs, and fed them into a maximum entropy classifier. Rutherford and Xue (2015) further proposed to gather extra weakly labeled data based on the discourse connectives for the classifier. • B&D2015: Braud and Denis (2015) combined several hand-crafted lexical features and word embeddings to train a max-entropy classifier. It is noted that P&C2012 and J&E2015 merged the “EntRel” relation into the “Expansion” relation1 . For a comprehensive comparison, we also experiment our model by adding a Expa.+EntRel vs Other classification. O"
D16-1130,N15-1081,0,0.513125,"fitting, dropout operation is applied on the top feature vector before the softmax layer. Also, we use different learning rates λ and λe to train the neural network parameters Θ and the word embeddings Θe referring to (Ji and Eisenstein, 2015). λe is set to a small value for preventing overfitting on this task. In the experimental part, we will introduce the setting of the hyper-parameters. 3 3.1 Experiments Preparation We evaluate our model on the Penn Discourse Treebank (PDTB) (Prasad et al., 2008). In our work, we experiment on the four top-level classes in this corpus as in previous work (Rutherford and Xue, 2015). We extract all the implicit relations of PDTB, and follow the setup of (Rutherford and Xue, 2015). We split the data into a training set (Sections 220), development set (Sections 0-1), and test set (Section 21-22). Table 1 summarizes the statistics of the four PDTB discourse relations, i.e., Comparison, Contingency, Expansion and Temporal. Relation Comparison Contingency Expansion Temporal Total Train 1855 3235 6673 582 12345 Dev 189 281 638 48 1156 Test 145 273 538 55 1011 Table 1: Statistics of Implicit Discourse Relations in PDTB. We first convert the tokens in PDTB to lowercase. The word"
D16-1130,N03-1030,0,0.0845232,") support a set of sentences to form a coherent text. Automatically recognizing discourse relations can help many downstream tasks such as question answering and automatic summarization. Despite great progress in classifying explicit discourse relations where the discourse connectives (e.g., “because”, “but”) explicitly exist in the text, implicit discourse relation recognition remains a challenge due to the absence of discourse connectives. Previous research mainly focus on exploring various kinds of efficient features and machine learning models to classify the implicit discourse relations (Soricut and Marcu, 2003; Baldridge and Lascarides, 2005; Subba and Di Eugenio, 2009; Hernault et al., 2010; Pitler et al., 2009; Joty et al., 2012). To some extent, these methods simulate the single-pass reading process that a person quickly skim the text through one-pass reading and directly collect important clues for understanding the text. Although single-pass reading plays a crucial role when we just want the general meaning and do not necessarily need to understand every single point of the text, it is not enough for tackling tasks that need a deep analysis of the text. In contrast with single-pass reading, re"
D16-1130,N09-1064,0,0.0940206,"Missing"
D16-1130,P15-2116,0,0.0604773,"Missing"
D16-1130,C12-1168,1,0.854064,"159 Wall Street Journal articles. Each document is annotated with the predicate-argument structure, where the predicate is the discourse connective (e.g. while) and the arguments are two text spans around the connective. The discourse connective can be either explicit or implicit. In PDTB, a hierarchy of relation tags is provided for annotation. In our study, we use the four top-level tags, including Temporal, Contingency, Comparison and Expansion. These four core relations allow us to be theory-neutral, since they are almost included in all discourse theories, sometimes with different names (Wang et al., 2012). Implicit discourse relation recognition is often treated as a classification problem. The first work to tackle this task on PDTB is (Pitler et al., 2009). They selected several surface features to train four binary classifiers, each for one of the top-level PDTB relation classes. Extending from this work, Lin et al. (2009) further identified four different feature types representing the context, the constituent parse trees, the dependency parse trees and the raw text respectively. Rutherford and Xue (2014) used brown cluster to replace the word pair features for solving the sparsity problem."
D16-1130,D15-1266,0,0.443472,"pare our models with six state-ofthe-art baseline approaches, as shown in Table 4. The six baselines are introduced as follows. • Liu2016: Liu et al. (2016) proposed to better classify the discourse relations by learning from other discourse-related tasks with a multitask neural network. • P&C2012: Park and Cardie (2012) designed a feature-based method and promoted the performance through optimizing the feature set. • J&E2015: Ji and Eisenstein (2015) used two recursive neural networks on the syntactic parse tree to induce the representation of the arguments and the entity spans. • Zhang2015: Zhang et al. (2015) proposed to use shallow convolutional neural networks to model two arguments respectively. We replicated their model since they used a different setting in preprocessing PDTB. • R&X2014, R&X2015: Rutherford and Xue (2014) selected lexical features, production rules, and Brown cluster pairs, and fed them into a maximum entropy classifier. Rutherford and Xue (2015) further proposed to gather extra weakly labeled data based on the discourse connectives for the classifier. • B&D2015: Braud and Denis (2015) combined several hand-crafted lexical features and word embeddings to train a max-entropy c"
D16-1130,C10-2172,0,0.533579,".29 57.17 44.95 57.57 Comp. 31.32 35.93 32.03 39.70 41.00 36.36 37.91 36.70 39.86 Cont. 49.82 52.78 47.08 54.40 53.80 55.76 55.88 54.48 53.69 Binary Expa. Expa.+EntRel 79.22 80.02 68.96 80.22 70.20 80.44 69.40 61.76 69.97 70.43 80.73 69.71 80.86 Temp. 26.57 27.63 20.29 28.70 33.30 27.30 37.17 38.84 37.61 Table 4: Comparison with the State-of-the-art Approaches. that the “Comparison” relation needs more passes of reading compared to the other three relations. The reason may be that the identification of the “Comparison” depends more on some deep analysis such as semantic parsing, according to (Zhou et al., 2010). Next, we compare our models with six state-ofthe-art baseline approaches, as shown in Table 4. The six baselines are introduced as follows. • Liu2016: Liu et al. (2016) proposed to better classify the discourse relations by learning from other discourse-related tasks with a multitask neural network. • P&C2012: Park and Cardie (2012) designed a feature-based method and promoted the performance through optimizing the feature set. • J&E2015: Ji and Eisenstein (2015) used two recursive neural networks on the syntactic parse tree to induce the representation of the arguments and the entity spans."
D17-1133,W05-0613,0,0.0309287,"2008). In this work, we focus on RST-style discourse parsing, where a tree representation is derived for an entire document. In PDTB, discourse relations are annotated mostly between adjacent sentences and no global tree structure is provided. Early approaches to discourse parsing (Marcu, 2000; LeThanh et al., 2004) have primarily focused on overt discourse markers (or cue words) and used a series of rules to derive the discourse tree structure. Soricut and Marcu (2003) employed a standard bottom-up chart parsing algorithm with syntactic and lexical features to conduct sentencelevel parsing. Baldridge and Lascarides (2005) and Sagae (2009) used probabilistic head-driven parsing techniques. Subba and Di Eugenio (2009) were the first to incorporate rich compositional semantics into sentence- and document-level discourse parsing. HILDA (Hernault et al., 2010) has been one of the most influential document-level discourse parsers paving the way for many machine learning-based models. HILDA parses a document pre-segmented into EDUs with two support vector machine classifiers working iteratively in a pipeline. At each iteration, a binary SVM predicts which adjacent units should be merged and then a multi-class SVM pre"
D17-1133,W04-2504,0,0.122516,"nce a big Kidder insider-trading scandal two years ago.]e3 Figure 1: Example text (bottom) composed of two sentences (three EDUs) and its RST discourse tree representation (top). Introduction The computational treatment of discourse phenomena has recently attracted much attention, due to their increasing importance for potential applications. Knowing how text units can be composed into a coherent document and how they relate to each other e.g., whether they express contrast, cause, or elaboration, can usefully aid downstream tasks such summarization (Yoshida et al., 2014), question answering (Chai and Jin, 2004), and sentiment analysis (Somasundaran, 2010). Rhetorical Structure Theory (RST, Mann and Thompson 1988), one of the most influential frameworks in discourse processing, represents texts by trees whose leaves correspond to Elementary Discourse Units (EDUs) and whose nodes specify how these and larger units (e.g., multisentence segments) are linked to each other by rhetorical relations. Discourse units are further characterized in terms of their importance in text: nuclei denote central segments, whereas satellites denote peripheral ones. Figure 1 shows an example of a discourse tree representi"
D17-1133,P12-1007,0,0.117309,"lgorithm to generate a global optimal tree. The high order complexity renders such parsers inefficient in practice, especially when processing large documents. As a result, more efficient linear-time discourse parsers have been proposed (Feng and Hirst, 2014; Ji and Eisenstein, 2014) which make local decisions and model the structure of the discourse and its relations separately. In this case, features are extracted from a local context (i.e., a small window of discourse constituents) without considering document-level information, which has been previously found useful in discourse analysis (Feng and Hirst, 2012). In this paper, we propose a simple and efficient linear-time discourse parser with a novel way of learning contextual representations for discourse constituents. To guarantee linear-time complexity, we use a two-stage approach: we first parse each sentence in a document into a tree whose leaves correspond to EDUs, and then parse the document into a tree whose leaves correspond to already preprocessed sentences. The feature learning process for both stages is based on neural network models. At the sentence level, Long-Short Term Memory Networks (LSTMs; Hochreiter and Schmidhuber 1997) learn r"
D17-1133,P14-1048,0,0.486016,"here n denotes the number of sen1289 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1289–1298 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics tences in the document). These systems model the relations between all possible adjacent discourse segments and use a CKY-style algorithm to generate a global optimal tree. The high order complexity renders such parsers inefficient in practice, especially when processing large documents. As a result, more efficient linear-time discourse parsers have been proposed (Feng and Hirst, 2014; Ji and Eisenstein, 2014) which make local decisions and model the structure of the discourse and its relations separately. In this case, features are extracted from a local context (i.e., a small window of discourse constituents) without considering document-level information, which has been previously found useful in discourse analysis (Feng and Hirst, 2012). In this paper, we propose a simple and efficient linear-time discourse parser with a novel way of learning contextual representations for discourse constituents. To guarantee linear-time complexity, we use a two-stage approach: we firs"
D17-1133,P14-1002,0,0.547825,"ber of sen1289 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1289–1298 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics tences in the document). These systems model the relations between all possible adjacent discourse segments and use a CKY-style algorithm to generate a global optimal tree. The high order complexity renders such parsers inefficient in practice, especially when processing large documents. As a result, more efficient linear-time discourse parsers have been proposed (Feng and Hirst, 2014; Ji and Eisenstein, 2014) which make local decisions and model the structure of the discourse and its relations separately. In this case, features are extracted from a local context (i.e., a small window of discourse constituents) without considering document-level information, which has been previously found useful in discourse analysis (Feng and Hirst, 2012). In this paper, we propose a simple and efficient linear-time discourse parser with a novel way of learning contextual representations for discourse constituents. To guarantee linear-time complexity, we use a two-stage approach: we first parse each sentence in a"
D17-1133,P13-1048,0,0.631934,"bba and Di Eugenio (2009) were the first to incorporate rich compositional semantics into sentence- and document-level discourse parsing. HILDA (Hernault et al., 2010) has been one of the most influential document-level discourse parsers paving the way for many machine learning-based models. HILDA parses a document pre-segmented into EDUs with two support vector machine classifiers working iteratively in a pipeline. At each iteration, a binary SVM predicts which adjacent units should be merged and then a multi-class SVM predicts their discourse relation. Subsequent work (Feng and Hirst, 2014; Joty et al., 2013) has shown that two-stage systems are not only efficient but can also achieve competitive performance. CKY-based parsers which guarantee globally optimal results have also been developed (Joty et al., 2013; Li et al., 2014). Ji and Eisenstein (2014) were the first to apply neural network models to RST discourse parsing; their shift-reduce parser uses a feedforward neural network to learn the representations of the 1290 transition stack and queue. Li et al. (2014) proposed a CKY-based parser which uses recursive neural networks to learn representations for EDUs and their composition during the"
D17-1133,Q16-1023,0,0.0304662,"t,R Figure 3: Intra-sentential relation CRF with pairwise modeling. alleviates the need for elaborate feature engineering and selection. Our approach is based on LSTMs (Hochreiter and Schmidhuber, 1997) which have recently emerged as a popular architecture for modeling sequences and have been successfully applied to a variety of tasks ranging from machine translation (Sutskever et al., 2014), to speech recognition (Graves et al., 2013), and image description generation (Vinyals et al., 2015b). LSTMs have also been incorporated into syntactic parsing in a variety of ways (Vinyals et al. 2015a; Kiperwasser and Goldberg 2016; Dyer et al. 2015, inter alia). Of particular relevance to this work is LSTM-minus, a method for learning embeddings of text spans, which has achieved competitive performance in both dependency and constituency parsing (Wang and Chang, 2016; Cross and Huang, 2016). We describe below how we extend this method which is based on subtraction between LSTM hidden vectors to discourse parsing. We represent each sentence as a sequence of wsos , w 1 , · · · , w i , · · · , w n , w eos ] word embeddings [w and insert a special embedding wE to indicate the boundaries of EDUs. We run a bidirectional LSTM"
D17-1133,D16-1001,0,0.0388215,"quences and have been successfully applied to a variety of tasks ranging from machine translation (Sutskever et al., 2014), to speech recognition (Graves et al., 2013), and image description generation (Vinyals et al., 2015b). LSTMs have also been incorporated into syntactic parsing in a variety of ways (Vinyals et al. 2015a; Kiperwasser and Goldberg 2016; Dyer et al. 2015, inter alia). Of particular relevance to this work is LSTM-minus, a method for learning embeddings of text spans, which has achieved competitive performance in both dependency and constituency parsing (Wang and Chang, 2016; Cross and Huang, 2016). We describe below how we extend this method which is based on subtraction between LSTM hidden vectors to discourse parsing. We represent each sentence as a sequence of wsos , w 1 , · · · , w i , · · · , w n , w eos ] word embeddings [w and insert a special embedding wE to indicate the boundaries of EDUs. We run a bidirectional LSTM over the sentence and obtain the output vector sequence [hh0 , · · · , h i , · · · , ht ], where h i = [~h i ,h~i ] is the output vector for the ith word, and ~h i and h~i are the output vectors from the forward and backward directions, respectively. We represent"
D17-1133,P15-1033,0,0.0158928,"relation CRF with pairwise modeling. alleviates the need for elaborate feature engineering and selection. Our approach is based on LSTMs (Hochreiter and Schmidhuber, 1997) which have recently emerged as a popular architecture for modeling sequences and have been successfully applied to a variety of tasks ranging from machine translation (Sutskever et al., 2014), to speech recognition (Graves et al., 2013), and image description generation (Vinyals et al., 2015b). LSTMs have also been incorporated into syntactic parsing in a variety of ways (Vinyals et al. 2015a; Kiperwasser and Goldberg 2016; Dyer et al. 2015, inter alia). Of particular relevance to this work is LSTM-minus, a method for learning embeddings of text spans, which has achieved competitive performance in both dependency and constituency parsing (Wang and Chang, 2016; Cross and Huang, 2016). We describe below how we extend this method which is based on subtraction between LSTM hidden vectors to discourse parsing. We represent each sentence as a sequence of wsos , w 1 , · · · , w i , · · · , w n , w eos ] word embeddings [w and insert a special embedding wE to indicate the boundaries of EDUs. We run a bidirectional LSTM over the sentence"
D17-1133,D14-1220,0,0.0842061,"tences with three EDUs (e1 , e2 , and e3 ). EDUs e1 and e2 are connected with a mononuclear relation (i.e., Consequence), where e1 is the nucleus and e2 the satellite (indicated by the left pointing arrow in the figure). Span e1:2 is related to e3 via List, a multi-nuclear relation, expressing the fact that both spans are equally important and therefore both nucleus. Given such tree-based representations of discourse structure, it is not surprising that RST-style document analysis is often viewed as a parsing task. State-of-the-art performance on RST parsing is achieved by cubic-time parsers (Li, Li, and Hovy, 2014; Li, Li, and Chang, 2016), with O(n3 ) time complexity (where n denotes the number of sen1289 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1289–1298 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics tences in the document). These systems model the relations between all possible adjacent discourse segments and use a CKY-style algorithm to generate a global optimal tree. The high order complexity renders such parsers inefficient in practice, especially when processing large documents. As a result, more ef"
D17-1133,1993.eamt-1.1,0,0.573193,"Their CYK-based parser adopts a recursive deep model for composing EDUs hierarchically together with several additional features to boost performance. CID ER performs slightly worse on span and nuclearity compared to Li et al. (2016), but is better at identifying relations. Their system uses an attention-based hierarchical neural network for modeling text spans and a tensor-based transformation for combining two spans. A CKY-like algorithm is used to generate the discourse tree structure. In comparison, CID ER is conceptually simpler, and more efficient. We used paired bootstrap re-sampling (Efron and Tibshirani, 1993) to assess whether differences in performance are statistically significant. CID ER is significantly better than Feng and Hirst’s 2014 system on the relation metric (p < 0.05); it is also significantly better (p < 0.05) than Heilman and Sagae (2015) on all three metrics and better than Ji and Eisenstein (2014) on the span metric. Compared to Li et al. (2014), CID ER is significantly better on the span and relation metrics (p < 0.05). Unfortunately, we cannot perform significance tests against Li et al. (2016) as we do not have access to the output of their system. We also evaluated the speed o"
D17-1133,D16-1035,0,0.148105,"e1 , e2 , and e3 ). EDUs e1 and e2 are connected with a mononuclear relation (i.e., Consequence), where e1 is the nucleus and e2 the satellite (indicated by the left pointing arrow in the figure). Span e1:2 is related to e3 via List, a multi-nuclear relation, expressing the fact that both spans are equally important and therefore both nucleus. Given such tree-based representations of discourse structure, it is not surprising that RST-style document analysis is often viewed as a parsing task. State-of-the-art performance on RST parsing is achieved by cubic-time parsers (Li, Li, and Hovy, 2014; Li, Li, and Chang, 2016), with O(n3 ) time complexity (where n denotes the number of sen1289 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1289–1298 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics tences in the document). These systems model the relations between all possible adjacent discourse segments and use a CKY-style algorithm to generate a global optimal tree. The high order complexity renders such parsers inefficient in practice, especially when processing large documents. As a result, more efficient linear-time disco"
D17-1133,P14-5010,0,0.00392229,"e-trained word embeddings in the training set. Matrix W can be subsequently used to to estimate fine-tuned embeddings for words in the test set. Tokenization, POS tagging and sentence splitting were performed using the Stanford 1 For calculating the binary potential scores, 41 relations will lead to a large number of parameters; to avoid this, we only use 18 relations without nuclearity. 2 https://radimrehurek.com/gensim/ S 79.5 82.7 83.6 83.6 N 68.1 69.3 70.1 71.1 R 56.6 55.6 55.4 57.3 Table 1: CID ER performance using different constituent representations (RST-DT test set). CoreNLP toolkit (Manning et al., 2014). All neural network parameters were initialized randomly with Xavier’s initialization (Glorot and Bengio, 2010). The hyper-parameters are tuned by crossvalidation on the training set. Additional Features Most existing state-of-theart systems rely heavily on handcrafted features (Hernault et al., 2010; Feng and Hirst, 2014; Joty et al., 2013) some of which have been also proved helpful in neural network models (Li et al., 2014, 2016). In our experiments, we use the following basic features which have been widely adopted in various discourse parsing models: (1) the first three words and the las"
D17-1133,P16-1218,0,0.0358024,"ecture for modeling sequences and have been successfully applied to a variety of tasks ranging from machine translation (Sutskever et al., 2014), to speech recognition (Graves et al., 2013), and image description generation (Vinyals et al., 2015b). LSTMs have also been incorporated into syntactic parsing in a variety of ways (Vinyals et al. 2015a; Kiperwasser and Goldberg 2016; Dyer et al. 2015, inter alia). Of particular relevance to this work is LSTM-minus, a method for learning embeddings of text spans, which has achieved competitive performance in both dependency and constituency parsing (Wang and Chang, 2016; Cross and Huang, 2016). We describe below how we extend this method which is based on subtraction between LSTM hidden vectors to discourse parsing. We represent each sentence as a sequence of wsos , w 1 , · · · , w i , · · · , w n , w eos ] word embeddings [w and insert a special embedding wE to indicate the boundaries of EDUs. We run a bidirectional LSTM over the sentence and obtain the output vector sequence [hh0 , · · · , h i , · · · , ht ], where h i = [~h i ,h~i ] is the output vector for the ith word, and ~h i and h~i are the output vectors from the forward and backward directions, res"
D17-1133,D14-1196,0,0.0208935,"ent General Electric Co. had been frayed since a big Kidder insider-trading scandal two years ago.]e3 Figure 1: Example text (bottom) composed of two sentences (three EDUs) and its RST discourse tree representation (top). Introduction The computational treatment of discourse phenomena has recently attracted much attention, due to their increasing importance for potential applications. Knowing how text units can be composed into a coherent document and how they relate to each other e.g., whether they express contrast, cause, or elaboration, can usefully aid downstream tasks such summarization (Yoshida et al., 2014), question answering (Chai and Jin, 2004), and sentiment analysis (Somasundaran, 2010). Rhetorical Structure Theory (RST, Mann and Thompson 1988), one of the most influential frameworks in discourse processing, represents texts by trees whose leaves correspond to Elementary Discourse Units (EDUs) and whose nodes specify how these and larger units (e.g., multisentence segments) are linked to each other by rhetorical relations. Discourse units are further characterized in terms of their importance in text: nuclei denote central segments, whereas satellites denote peripheral ones. Figure 1 shows"
D17-1133,J00-3005,0,0.776825,"in Section 5. Section 7 concludes the paper. 2 Related Work Recent advances in discourse modeling have greatly benefited from the availability of resources annotated with discourse-level information such as the RST Discourse Treebank (RST-DT; Carlson et al. 2003) and the Penn Discourse Treebank (PDTB, Prasad et al. 2008). In this work, we focus on RST-style discourse parsing, where a tree representation is derived for an entire document. In PDTB, discourse relations are annotated mostly between adjacent sentences and no global tree structure is provided. Early approaches to discourse parsing (Marcu, 2000; LeThanh et al., 2004) have primarily focused on overt discourse markers (or cue words) and used a series of rules to derive the discourse tree structure. Soricut and Marcu (2003) employed a standard bottom-up chart parsing algorithm with syntactic and lexical features to conduct sentencelevel parsing. Baldridge and Lascarides (2005) and Sagae (2009) used probabilistic head-driven parsing techniques. Subba and Di Eugenio (2009) were the first to incorporate rich compositional semantics into sentence- and document-level discourse parsing. HILDA (Hernault et al., 2010) has been one of the most"
D17-1133,prasad-etal-2008-penn,0,0.242149,"eing more efficient. The rest of this paper is organized as follows. We overview related work in the following section. We describe the general flow of our parser in Section 3 and provide details on our parsing algorithm and feature learning method in Section 4. Experimental results are reported in Section 5. Section 7 concludes the paper. 2 Related Work Recent advances in discourse modeling have greatly benefited from the availability of resources annotated with discourse-level information such as the RST Discourse Treebank (RST-DT; Carlson et al. 2003) and the Penn Discourse Treebank (PDTB, Prasad et al. 2008). In this work, we focus on RST-style discourse parsing, where a tree representation is derived for an entire document. In PDTB, discourse relations are annotated mostly between adjacent sentences and no global tree structure is provided. Early approaches to discourse parsing (Marcu, 2000; LeThanh et al., 2004) have primarily focused on overt discourse markers (or cue words) and used a series of rules to derive the discourse tree structure. Soricut and Marcu (2003) employed a standard bottom-up chart parsing algorithm with syntactic and lexical features to conduct sentencelevel parsing. Baldri"
D17-1133,W09-3813,0,0.258191,"T-style discourse parsing, where a tree representation is derived for an entire document. In PDTB, discourse relations are annotated mostly between adjacent sentences and no global tree structure is provided. Early approaches to discourse parsing (Marcu, 2000; LeThanh et al., 2004) have primarily focused on overt discourse markers (or cue words) and used a series of rules to derive the discourse tree structure. Soricut and Marcu (2003) employed a standard bottom-up chart parsing algorithm with syntactic and lexical features to conduct sentencelevel parsing. Baldridge and Lascarides (2005) and Sagae (2009) used probabilistic head-driven parsing techniques. Subba and Di Eugenio (2009) were the first to incorporate rich compositional semantics into sentence- and document-level discourse parsing. HILDA (Hernault et al., 2010) has been one of the most influential document-level discourse parsers paving the way for many machine learning-based models. HILDA parses a document pre-segmented into EDUs with two support vector machine classifiers working iteratively in a pipeline. At each iteration, a binary SVM predicts which adjacent units should be merged and then a multi-class SVM predicts their disco"
D17-1133,D12-1110,0,0.403935,"for entire sentences. Treating a sentence as a sequence of EDUs and a document as a sequence of sentences allows to incorporate important contextual information on both levels capturing long-distance dependencies. Recurrent neural networks excel at modeling sequences, but cannot capture hierarchical structure which is important when analyzing multisentential discourse. We therefore adopt a more structure-aware representation at the document level which we argue is complementary to the flat representations obtained from the LSTM. We represent documents as trees using recursive neural networks (Socher et al., 2012). Experimental evaluation on the RST Treebank shows that our parser yields comparable performance to previous lineartime systems, without requiring extensive manual feature engineering and improves upon related neural models (Li et al., 2014, 2016) on discourse relation classification, while being more efficient. The rest of this paper is organized as follows. We overview related work in the following section. We describe the general flow of our parser in Section 3 and provide details on our parsing algorithm and feature learning method in Section 4. Experimental results are reported in Sectio"
D17-1133,N03-1030,0,0.129936,"ith discourse-level information such as the RST Discourse Treebank (RST-DT; Carlson et al. 2003) and the Penn Discourse Treebank (PDTB, Prasad et al. 2008). In this work, we focus on RST-style discourse parsing, where a tree representation is derived for an entire document. In PDTB, discourse relations are annotated mostly between adjacent sentences and no global tree structure is provided. Early approaches to discourse parsing (Marcu, 2000; LeThanh et al., 2004) have primarily focused on overt discourse markers (or cue words) and used a series of rules to derive the discourse tree structure. Soricut and Marcu (2003) employed a standard bottom-up chart parsing algorithm with syntactic and lexical features to conduct sentencelevel parsing. Baldridge and Lascarides (2005) and Sagae (2009) used probabilistic head-driven parsing techniques. Subba and Di Eugenio (2009) were the first to incorporate rich compositional semantics into sentence- and document-level discourse parsing. HILDA (Hernault et al., 2010) has been one of the most influential document-level discourse parsers paving the way for many machine learning-based models. HILDA parses a document pre-segmented into EDUs with two support vector machine"
D17-1133,N09-1064,0,0.0569087,"Missing"
D17-1207,D16-1250,0,0.196759,"eted as translations. Moreover, this interpretation naturally handles multiple alternative translations. For example, the Chinese word “mao” can be translated to “cat” or “kitten”, as shown in Figure 1. 2.2 The Form of the Transformation The approximate isomorphism across embedding spaces inspires researchers to use a simple form of transformation. For example, Mikolov et al. (2013a) chose to use a linear transformation, i.e. the transformation G parametrized by a matrix. Later, proposals for using an orthogonal transformation are supported empirically (Xing et al., 2015; Zhang et al., 2016c; Artetxe et al., 2016) and theoretically (Smith et al., 2017). Indeed, an orthogonal transformation has desirable properties in this setting. If G is an orthogonal matrix that transforms the source embeddings into the target space, then its transpose (also its inverse) G> 1935 performs transformation in the reverse direction. In that case, any word embedding a can be recovered by transforming back and forth because G> Ga = a. Moreover, computing the cosine similarity between a source embedding a and a target embedding b will be independent of the semantic space in which the similarity is measured, > because > > > b"
D17-1207,W16-1208,0,0.0182111,"ing. The use of Wasserstein GAN addresses this problem and allows our simple architecture to be trained successfully. 5.2 Language Distance Quantifying language difference is an open question with on-going efforts that put forward better measures based on manually compiled data (Albu, 2006; Hammarstr¨om and O’Connor, 2013). Researchers in computational linguistics also try to contribute corpus-based approaches to this question. Parallel data is typically exploited, and ideas range from information-theoretic (Juola, 1998), statistical (Mayer and Cysouw, 2012), to graphbased (Eger et al., 2016; Asgari and Mofrad, 2016). To our knowledge, the earth mover’s distance is proposed for language distance for the first time, with the distinctive feature of relying on nonparallel data only. 5.3 The Earth Mover’s Distance First introduced into computer vision (Rubner et al., 1998), the earth mover’s distance also finds application in natural language processing (Kusner et al., 2015; Huang et al., 2016), including bilingual lexicon induction (Zhang et al., 2016b,a). Zhang et al. (2016b) build upon bilingual word embeddings and apply the EMD program as a postprocessing step to automatically produce multiple alternative"
D17-1207,W16-1614,0,0.0828985,"the distributions considered in that work are the hidden states of neural embedding models during the course of training. They are assumed to be Gaussian, so that the matching of distributions reduces to matching their means and variances, but this assumption is hard to justify and interpret. In contrast, our proposal does not make any assumption on the distributions, and directly matches the transformed source embedding distribution with the target distribution by minimizing their earth mover’s distance. Another attempt to learn cross-lingual embedding transformation without supervision is (Barone, 2016). Architectures of generative adversarial nets and adversarial autoencoders (Makhzani et al., 2015) are experimented, but the reported results are not positive. We tried the publicly available code on our data and obtained negative results as well. This outcome is likely caused by the training difficulty pointed out by (Arjovsky and Bottou, 2017), as traditional GAN training minimizes Jensen-Shannon divergence between distributions, which can provide pathological gradient to the generator and hamper its learning. The use of Wasserstein GAN addresses this problem and allows our simple architect"
D17-1207,C16-1171,0,0.0888885,"), the sentence level (Zou et al., 2013; Chandar A P et al., 2014; Hermann and Blunsom, 2014; Koˇcisk´y et al., 2014; Gouws et al., 2015; Luong et al., 2015; Coulmance et al., 2015; Oshikiri et al., 2016), or the word level (i.e. in the form of seed lexicon) (Gouws and Søgaard, 2015; Wick et al., 2016; Duong et al., 2016; Shi et al., 2015; Mikolov et al., 2013a; Faruqui and Dyer, 2014; Lu et al., 2015; Dinu et al., 2015; Lazaridou et al., 2015; Ammar et al., 2016; Zhang et al., 2016a, 2017; Smith et al., 2017). There is a recent work that aims to remove the need for cross-lingual supervision (Cao et al., 2016). Similar to ours, the underlying idea is to match cross-lingually at the level of distribution rather than word. However, the distributions considered in that work are the hidden states of neural embedding models during the course of training. They are assumed to be Gaussian, so that the matching of distributions reduces to matching their means and variances, but this assumption is hard to justify and interpret. In contrast, our proposal does not make any assumption on the distributions, and directly matches the transformed source embedding distribution with the target distribution by minimiz"
D17-1207,D15-1131,0,0.0101445,"a statistical model that attempts to decipher the source language. Following the popularity of monolingual word embeddings, cross-lingual word representation learning has also attracted significant attention in recent years. Building bilingual lexica from the learned cross-lingual embeddings is often considered an evaluative tool. Most methods rely on supervision encoded in parallel data, at the document 1940 level (Vuli´c and Moens, 2015), the sentence level (Zou et al., 2013; Chandar A P et al., 2014; Hermann and Blunsom, 2014; Koˇcisk´y et al., 2014; Gouws et al., 2015; Luong et al., 2015; Coulmance et al., 2015; Oshikiri et al., 2016), or the word level (i.e. in the form of seed lexicon) (Gouws and Søgaard, 2015; Wick et al., 2016; Duong et al., 2016; Shi et al., 2015; Mikolov et al., 2013a; Faruqui and Dyer, 2014; Lu et al., 2015; Dinu et al., 2015; Lazaridou et al., 2015; Ammar et al., 2016; Zhang et al., 2016a, 2017; Smith et al., 2017). There is a recent work that aims to remove the need for cross-lingual supervision (Cao et al., 2016). Similar to ours, the underlying idea is to match cross-lingually at the level of distribution rather than word. However, the distributions considered in that wor"
D17-1207,D12-1025,0,0.0141087,"Work Bilingual Lexicon Induction Bilingual lexicon induction is a long-standing research task in cross-lingual natural language processing. Traditional methods build statistical models for monolingual word co-occurrence, and combine cross-lingual supervision to solve the task. As word alignment for parallel sentences can produce fairly good bilingual lexica (Och and Ney, 2003), these methods focus on non-parallel data with a seed lexicon as cross-lingual supervision (Rapp, 1999; Gaussier et al., 2004). An exception that does not rely on cross-lingual supervision is the decipherment approach (Dou and Knight, 2012, 2013; Dou et al., 2015). It views the source language as a cipher for the target language, and solves a statistical model that attempts to decipher the source language. Following the popularity of monolingual word embeddings, cross-lingual word representation learning has also attracted significant attention in recent years. Building bilingual lexica from the learned cross-lingual embeddings is often considered an evaluative tool. Most methods rely on supervision encoded in parallel data, at the document 1940 level (Vuli´c and Moens, 2015), the sentence level (Zou et al., 2013; Chandar A P e"
D17-1207,D13-1173,0,0.148339,"Missing"
D17-1207,P15-1081,0,0.0146857,"ction Bilingual lexicon induction is a long-standing research task in cross-lingual natural language processing. Traditional methods build statistical models for monolingual word co-occurrence, and combine cross-lingual supervision to solve the task. As word alignment for parallel sentences can produce fairly good bilingual lexica (Och and Ney, 2003), these methods focus on non-parallel data with a seed lexicon as cross-lingual supervision (Rapp, 1999; Gaussier et al., 2004). An exception that does not rely on cross-lingual supervision is the decipherment approach (Dou and Knight, 2012, 2013; Dou et al., 2015). It views the source language as a cipher for the target language, and solves a statistical model that attempts to decipher the source language. Following the popularity of monolingual word embeddings, cross-lingual word representation learning has also attracted significant attention in recent years. Building bilingual lexica from the learned cross-lingual embeddings is often considered an evaluative tool. Most methods rely on supervision encoded in parallel data, at the document 1940 level (Vuli´c and Moens, 2015), the sentence level (Zou et al., 2013; Chandar A P et al., 2014; Hermann and"
D17-1207,C16-1331,0,0.0736013,"ive language pairs considered in this paper, as well as their typology dissimilarity and geographical distance. The typology dissimilarity is computed from features in the WALS database (Dryer and Haspelmath, 2013). It is defined as one minus relative Hamming similarity, which is in turn defined as the number of agreeing features divided by the number of total features available for the language pair (Albu, 2006; Cysouw, 2013b). As a rough approximation, the geographical distance is measured by the distance between the capital cities of the countries where the considered languages are spoken (Eger et al., 2016). The typology dissimilarity reflects genealogical influence on the divergence between languages, while the geographical distance indicates the effect of language contact. Both play important roles in shaping the languages we perceive today, and they also correlate with each other (Cysouw, 5.1 Related Work Bilingual Lexicon Induction Bilingual lexicon induction is a long-standing research task in cross-lingual natural language processing. Traditional methods build statistical models for monolingual word co-occurrence, and combine cross-lingual supervision to solve the task. As word alignment f"
D17-1207,E14-1049,0,0.740932,"ic universals manifest themselves at various levels of linguistic units. At the word level, there is evidence that different languages represent concepts with similar structure (Youn et al., 2016). Interestingly, as computational models of word semantics, monolingual word embeddings also exhibit isomorphism across languages (Mikolov et al., 2013a). This finding opens up the possibility to use a simple transformation, e.g. a linear map, to connect separately trained word embeddings cross-lingually. Learning such a transformation typically calls for cross-lingual supervision from parallel data (Faruqui and Dyer, 2014; Lu et al., 2015; Dinu et al., 2015; Lazaridou et al., 2015; Smith et al., 2017). In this paper, we ask the question: Can we uncover the transformation without any cross-lingual supervision? At first sight, this task appears formidable, as it would imply that a bilingual semantic space can be constructed by using monolingual corpora only. On the other hand, the existence of structural isomorphism across monolingual embedding spaces points to the feasibility of this task: The transformation exists right there only to be discovered by the right tool. We propose such a tool to answer the above q"
D17-1207,P04-1067,0,0.0364016,"tant roles in shaping the languages we perceive today, and they also correlate with each other (Cysouw, 5.1 Related Work Bilingual Lexicon Induction Bilingual lexicon induction is a long-standing research task in cross-lingual natural language processing. Traditional methods build statistical models for monolingual word co-occurrence, and combine cross-lingual supervision to solve the task. As word alignment for parallel sentences can produce fairly good bilingual lexica (Och and Ney, 2003), these methods focus on non-parallel data with a seed lexicon as cross-lingual supervision (Rapp, 1999; Gaussier et al., 2004). An exception that does not rely on cross-lingual supervision is the decipherment approach (Dou and Knight, 2012, 2013; Dou et al., 2015). It views the source language as a cipher for the target language, and solves a statistical model that attempts to decipher the source language. Following the popularity of monolingual word embeddings, cross-lingual word representation learning has also attracted significant attention in recent years. Building bilingual lexica from the learned cross-lingual embeddings is often considered an evaluative tool. Most methods rely on supervision encoded in parall"
D17-1207,W98-1217,0,0.117098,"butions, which can provide pathological gradient to the generator and hamper its learning. The use of Wasserstein GAN addresses this problem and allows our simple architecture to be trained successfully. 5.2 Language Distance Quantifying language difference is an open question with on-going efforts that put forward better measures based on manually compiled data (Albu, 2006; Hammarstr¨om and O’Connor, 2013). Researchers in computational linguistics also try to contribute corpus-based approaches to this question. Parallel data is typically exploited, and ideas range from information-theoretic (Juola, 1998), statistical (Mayer and Cysouw, 2012), to graphbased (Eger et al., 2016; Asgari and Mofrad, 2016). To our knowledge, the earth mover’s distance is proposed for language distance for the first time, with the distinctive feature of relying on nonparallel data only. 5.3 The Earth Mover’s Distance First introduced into computer vision (Rubner et al., 1998), the earth mover’s distance also finds application in natural language processing (Kusner et al., 2015; Huang et al., 2016), including bilingual lexicon induction (Zhang et al., 2016b,a). Zhang et al. (2016b) build upon bilingual word embedding"
D17-1207,P14-2037,0,0.0366263,"Missing"
D17-1207,P15-1027,0,0.332622,"istic units. At the word level, there is evidence that different languages represent concepts with similar structure (Youn et al., 2016). Interestingly, as computational models of word semantics, monolingual word embeddings also exhibit isomorphism across languages (Mikolov et al., 2013a). This finding opens up the possibility to use a simple transformation, e.g. a linear map, to connect separately trained word embeddings cross-lingually. Learning such a transformation typically calls for cross-lingual supervision from parallel data (Faruqui and Dyer, 2014; Lu et al., 2015; Dinu et al., 2015; Lazaridou et al., 2015; Smith et al., 2017). In this paper, we ask the question: Can we uncover the transformation without any cross-lingual supervision? At first sight, this task appears formidable, as it would imply that a bilingual semantic space can be constructed by using monolingual corpora only. On the other hand, the existence of structural isomorphism across monolingual embedding spaces points to the feasibility of this task: The transformation exists right there only to be discovered by the right tool. We propose such a tool to answer the above question in the affirmative. The key insight is to view embed"
D17-1207,N15-1028,0,0.189155,"hemselves at various levels of linguistic units. At the word level, there is evidence that different languages represent concepts with similar structure (Youn et al., 2016). Interestingly, as computational models of word semantics, monolingual word embeddings also exhibit isomorphism across languages (Mikolov et al., 2013a). This finding opens up the possibility to use a simple transformation, e.g. a linear map, to connect separately trained word embeddings cross-lingually. Learning such a transformation typically calls for cross-lingual supervision from parallel data (Faruqui and Dyer, 2014; Lu et al., 2015; Dinu et al., 2015; Lazaridou et al., 2015; Smith et al., 2017). In this paper, we ask the question: Can we uncover the transformation without any cross-lingual supervision? At first sight, this task appears formidable, as it would imply that a bilingual semantic space can be constructed by using monolingual corpora only. On the other hand, the existence of structural isomorphism across monolingual embedding spaces points to the feasibility of this task: The transformation exists right there only to be discovered by the right tool. We propose such a tool to answer the above question in the af"
D17-1207,W15-1521,0,0.0692121,"anguage, and solves a statistical model that attempts to decipher the source language. Following the popularity of monolingual word embeddings, cross-lingual word representation learning has also attracted significant attention in recent years. Building bilingual lexica from the learned cross-lingual embeddings is often considered an evaluative tool. Most methods rely on supervision encoded in parallel data, at the document 1940 level (Vuli´c and Moens, 2015), the sentence level (Zou et al., 2013; Chandar A P et al., 2014; Hermann and Blunsom, 2014; Koˇcisk´y et al., 2014; Gouws et al., 2015; Luong et al., 2015; Coulmance et al., 2015; Oshikiri et al., 2016), or the word level (i.e. in the form of seed lexicon) (Gouws and Søgaard, 2015; Wick et al., 2016; Duong et al., 2016; Shi et al., 2015; Mikolov et al., 2013a; Faruqui and Dyer, 2014; Lu et al., 2015; Dinu et al., 2015; Lazaridou et al., 2015; Ammar et al., 2016; Zhang et al., 2016a, 2017; Smith et al., 2017). There is a recent work that aims to remove the need for cross-lingual supervision (Cao et al., 2016). Similar to ours, the underlying idea is to match cross-lingually at the level of distribution rather than word. However, the distribution"
D17-1207,W12-0209,0,0.021362,"pathological gradient to the generator and hamper its learning. The use of Wasserstein GAN addresses this problem and allows our simple architecture to be trained successfully. 5.2 Language Distance Quantifying language difference is an open question with on-going efforts that put forward better measures based on manually compiled data (Albu, 2006; Hammarstr¨om and O’Connor, 2013). Researchers in computational linguistics also try to contribute corpus-based approaches to this question. Parallel data is typically exploited, and ideas range from information-theoretic (Juola, 1998), statistical (Mayer and Cysouw, 2012), to graphbased (Eger et al., 2016; Asgari and Mofrad, 2016). To our knowledge, the earth mover’s distance is proposed for language distance for the first time, with the distinctive feature of relying on nonparallel data only. 5.3 The Earth Mover’s Distance First introduced into computer vision (Rubner et al., 1998), the earth mover’s distance also finds application in natural language processing (Kusner et al., 2015; Huang et al., 2016), including bilingual lexicon induction (Zhang et al., 2016b,a). Zhang et al. (2016b) build upon bilingual word embeddings and apply the EMD program as a postp"
D17-1207,N15-1157,0,0.00994847,"gual word embeddings, cross-lingual word representation learning has also attracted significant attention in recent years. Building bilingual lexica from the learned cross-lingual embeddings is often considered an evaluative tool. Most methods rely on supervision encoded in parallel data, at the document 1940 level (Vuli´c and Moens, 2015), the sentence level (Zou et al., 2013; Chandar A P et al., 2014; Hermann and Blunsom, 2014; Koˇcisk´y et al., 2014; Gouws et al., 2015; Luong et al., 2015; Coulmance et al., 2015; Oshikiri et al., 2016), or the word level (i.e. in the form of seed lexicon) (Gouws and Søgaard, 2015; Wick et al., 2016; Duong et al., 2016; Shi et al., 2015; Mikolov et al., 2013a; Faruqui and Dyer, 2014; Lu et al., 2015; Dinu et al., 2015; Lazaridou et al., 2015; Ammar et al., 2016; Zhang et al., 2016a, 2017; Smith et al., 2017). There is a recent work that aims to remove the need for cross-lingual supervision (Cao et al., 2016). Similar to ours, the underlying idea is to match cross-lingually at the level of distribution rather than word. However, the distributions considered in that work are the hidden states of neural embedding models during the course of training. They are assumed to b"
D17-1207,P15-2118,0,0.0193638,"Missing"
D17-1207,J03-1002,0,0.0343742,"the divergence between languages, while the geographical distance indicates the effect of language contact. Both play important roles in shaping the languages we perceive today, and they also correlate with each other (Cysouw, 5.1 Related Work Bilingual Lexicon Induction Bilingual lexicon induction is a long-standing research task in cross-lingual natural language processing. Traditional methods build statistical models for monolingual word co-occurrence, and combine cross-lingual supervision to solve the task. As word alignment for parallel sentences can produce fairly good bilingual lexica (Och and Ney, 2003), these methods focus on non-parallel data with a seed lexicon as cross-lingual supervision (Rapp, 1999; Gaussier et al., 2004). An exception that does not rely on cross-lingual supervision is the decipherment approach (Dou and Knight, 2012, 2013; Dou et al., 2015). It views the source language as a cipher for the target language, and solves a statistical model that attempts to decipher the source language. Following the popularity of monolingual word embeddings, cross-lingual word representation learning has also attracted significant attention in recent years. Building bilingual lexica from"
D17-1207,P16-2080,0,0.0168343,"attempts to decipher the source language. Following the popularity of monolingual word embeddings, cross-lingual word representation learning has also attracted significant attention in recent years. Building bilingual lexica from the learned cross-lingual embeddings is often considered an evaluative tool. Most methods rely on supervision encoded in parallel data, at the document 1940 level (Vuli´c and Moens, 2015), the sentence level (Zou et al., 2013; Chandar A P et al., 2014; Hermann and Blunsom, 2014; Koˇcisk´y et al., 2014; Gouws et al., 2015; Luong et al., 2015; Coulmance et al., 2015; Oshikiri et al., 2016), or the word level (i.e. in the form of seed lexicon) (Gouws and Søgaard, 2015; Wick et al., 2016; Duong et al., 2016; Shi et al., 2015; Mikolov et al., 2013a; Faruqui and Dyer, 2014; Lu et al., 2015; Dinu et al., 2015; Lazaridou et al., 2015; Ammar et al., 2016; Zhang et al., 2016a, 2017; Smith et al., 2017). There is a recent work that aims to remove the need for cross-lingual supervision (Cao et al., 2016). Similar to ours, the underlying idea is to match cross-lingually at the level of distribution rather than word. However, the distributions considered in that work are the hidden states"
D17-1207,N15-1104,0,0.271726,"transport scheme can be readily interpreted as translations. Moreover, this interpretation naturally handles multiple alternative translations. For example, the Chinese word “mao” can be translated to “cat” or “kitten”, as shown in Figure 1. 2.2 The Form of the Transformation The approximate isomorphism across embedding spaces inspires researchers to use a simple form of transformation. For example, Mikolov et al. (2013a) chose to use a linear transformation, i.e. the transformation G parametrized by a matrix. Later, proposals for using an orthogonal transformation are supported empirically (Xing et al., 2015; Zhang et al., 2016c; Artetxe et al., 2016) and theoretically (Smith et al., 2017). Indeed, an orthogonal transformation has desirable properties in this setting. If G is an orthogonal matrix that transforms the source embeddings into the target space, then its transpose (also its inverse) G> 1935 performs transformation in the reverse direction. In that case, any word embedding a can be recovered by transforming back and forth because G> Ga = a. Moreover, computing the cosine similarity between a source embedding a and a target embedding b will be independent of the semantic space in which t"
D17-1207,P99-1067,0,0.248459,"h play important roles in shaping the languages we perceive today, and they also correlate with each other (Cysouw, 5.1 Related Work Bilingual Lexicon Induction Bilingual lexicon induction is a long-standing research task in cross-lingual natural language processing. Traditional methods build statistical models for monolingual word co-occurrence, and combine cross-lingual supervision to solve the task. As word alignment for parallel sentences can produce fairly good bilingual lexica (Och and Ney, 2003), these methods focus on non-parallel data with a seed lexicon as cross-lingual supervision (Rapp, 1999; Gaussier et al., 2004). An exception that does not rely on cross-lingual supervision is the decipherment approach (Dou and Knight, 2012, 2013; Dou et al., 2015). It views the source language as a cipher for the target language, and solves a statistical model that attempts to decipher the source language. Following the popularity of monolingual word embeddings, cross-lingual word representation learning has also attracted significant attention in recent years. Building bilingual lexica from the learned cross-lingual embeddings is often considered an evaluative tool. Most methods rely on super"
D17-1207,P15-2093,1,0.643394,"has also attracted significant attention in recent years. Building bilingual lexica from the learned cross-lingual embeddings is often considered an evaluative tool. Most methods rely on supervision encoded in parallel data, at the document 1940 level (Vuli´c and Moens, 2015), the sentence level (Zou et al., 2013; Chandar A P et al., 2014; Hermann and Blunsom, 2014; Koˇcisk´y et al., 2014; Gouws et al., 2015; Luong et al., 2015; Coulmance et al., 2015; Oshikiri et al., 2016), or the word level (i.e. in the form of seed lexicon) (Gouws and Søgaard, 2015; Wick et al., 2016; Duong et al., 2016; Shi et al., 2015; Mikolov et al., 2013a; Faruqui and Dyer, 2014; Lu et al., 2015; Dinu et al., 2015; Lazaridou et al., 2015; Ammar et al., 2016; Zhang et al., 2016a, 2017; Smith et al., 2017). There is a recent work that aims to remove the need for cross-lingual supervision (Cao et al., 2016). Similar to ours, the underlying idea is to match cross-lingually at the level of distribution rather than word. However, the distributions considered in that work are the hidden states of neural embedding models during the course of training. They are assumed to be Gaussian, so that the matching of distributions reduces"
D17-1207,C16-1300,1,0.890233,"Missing"
D17-1207,N16-1156,0,0.40728,"1. In order to connect the separate embedding spaces, we can try to transform the source embeddings so that they align well with target ones. Naturally, we need a measure for the quality of the alignment to guide our search for the transformation. As we aim to eliminate the need for crosslingual supervision from word translation pairs, the measure cannot be defined at the word level as in previous work (Mikolov et al., 2013a). Rather, it should quantify the difference between the entire distributions of embeddings. With this in mind, we find the earth mover’s distance to be a suitable choice (Zhang et al., 2016b). Its workings are illustrated in the right part of Figure 1. We can think of target embeddings as piles of earth, and transformed source embeddings as holes to be filled. Then the earth mover’s distance computes the minimal cost of moving the earth to fill the holes. Clearly, if the two sets of embeddings align well, the earth mover’s distance will be small. Therefore, we can try to find the transformation that minimizes the earth mover’s distance. Another desirable feature of the earth mover’s distance is that the computed transport scheme can be readily interpreted as translations. Moreov"
D17-1207,D13-1141,0,0.0146437,"approach (Dou and Knight, 2012, 2013; Dou et al., 2015). It views the source language as a cipher for the target language, and solves a statistical model that attempts to decipher the source language. Following the popularity of monolingual word embeddings, cross-lingual word representation learning has also attracted significant attention in recent years. Building bilingual lexica from the learned cross-lingual embeddings is often considered an evaluative tool. Most methods rely on supervision encoded in parallel data, at the document 1940 level (Vuli´c and Moens, 2015), the sentence level (Zou et al., 2013; Chandar A P et al., 2014; Hermann and Blunsom, 2014; Koˇcisk´y et al., 2014; Gouws et al., 2015; Luong et al., 2015; Coulmance et al., 2015; Oshikiri et al., 2016), or the word level (i.e. in the form of seed lexicon) (Gouws and Søgaard, 2015; Wick et al., 2016; Duong et al., 2016; Shi et al., 2015; Mikolov et al., 2013a; Faruqui and Dyer, 2014; Lu et al., 2015; Dinu et al., 2015; Lazaridou et al., 2015; Ammar et al., 2016; Zhang et al., 2016a, 2017; Smith et al., 2017). There is a recent work that aims to remove the need for cross-lingual supervision (Cao et al., 2016). Similar to ours, the"
D17-1207,L16-1521,0,0.0065638,"V X s=1 t=1 (k) Tst cos GwsS , wtT (13)  + const S = min − G∈O(d) B T V X V X s=1 t=1  (k) Tst cos GwsS , wtT . Data Preparation B.1 Non-Parallel Corpora for Training Embeddings http://linguatools.org/tools/corpora/wikipediacomparable-corpora 3 https://github.com/BYVoid/OpenCC 4 http://thulac.thunlp.org 5 http://www.nltk.org 6 http://www.cis.uni-muenchen.de/˜schmid/tools/ TreeTagger and Moens, 2013). For the Japanese corpus, we use MeCab7 for word segmentation and POS tagging. For Turkish, we utilize the preprocessing tools (tokenization and POS tagging) provided in LORELEI Language Packs (Strassel and Tracey, 2016), and its English side is preprocessed by NLTK. The statistics of the preprocessed corpora is given in Table 3. B.2 The data for training monolingual word embeddings comes from Wikipedia comparable corpora.2 Following (Vuli´c and Moens, 2013), we retain only nouns with at least 1,000 occurrences except for Turkish-English, whose frequency cutoff threshold is 100, as the amount of data is relatively small in this low-resource setting. For the Chinese side, we first use OpenCC3 to normalize characters to be simplified, and then perform Chinese word segmentation and POS tagging with THULAC.4 The"
D17-1207,Q13-1001,0,0.0351308,"Missing"
D17-1207,P16-1024,0,0.0615432,"Missing"
D17-1207,N13-1011,0,0.0493086,"Missing"
D17-1231,W13-4031,0,0.162829,"Missing"
D17-1231,C16-1025,0,0.0771497,"Missing"
D17-1231,W13-3214,0,0.0674142,"on Dialog act (DA) represents a function of a speaker’s utterance in either human-to-human or human-to-computer conversations. Correct identification of DAs is important for understanding human conversations, as well as for developing intelligent human-to-computer dialog systems (either written or spoken dialogs). For example, recognizing DAs can help identify questions and answers in meetings, customer service, online forum, etc. Many machine learning techniques have been investigated and shown reasonable performance for DA classification, for example, (Ang et al., 2005; Ji and Bilmes, 2005; Kalchbrenner and Blunsom, 2013; Ribeiro et al., 2015), just to name a few. Intuitively we would expect that leveraging dialog context can help classify the current utterance. For example, if the previous sentence is a question, then there is a high probability that the current sentence is a response to that question. Such context information has been explored in some previous methods, for example, hidden Markov models (HMM), conditional random fields (CRF), dynamic Bayesian networks (DBN). Given the recent success of the deep learning framework in various language processing tasks, in this work we also employ neural networ"
D17-1231,D10-1084,0,0.0526463,"Missing"
D17-1231,D14-1181,0,0.00850791,"he ‘answer’ DA follows the ‘question’ one, which is quite intuitive. Our goal is thus to model such sequential information for DA classification. Again in this work we only use the transcriptions of the utterances along with the speaker information (i.e., if the current utterance is from the same or different speaker as the previous one), without any speech related features. 3.2 CNN for utterance classification All of our methods are built based on the basic CNN sentence representation, which has been widely used recently in sentence as well as document classification (Collobert et al., 2011; Kim, 2014), therefore we first briefly describe this baseline. Figure 1 shows the context independent CNN-based classification method. Let w[1...n] represent the word embedding sequence for a sentence with n words, where wi ∈ Rd is the ddimensional embedding vector for the ith word. A temporal convolution operation is applied to the sentence: ˜ [1...n] ∗ f c[1...n] = w which is then used as the input in a multi-layer perceptron (MLP) or feedforward neural network for sentence classification. We only use one layer MLP in this work. This baseline CNN model learns textual information in each sentence for D"
D17-1231,2007.sigdial-1.30,0,0.0498218,"Missing"
D18-1041,D15-1041,0,0.0260727,"ared contexts into annotations. However, by contrast, we introduce domain classifier and adversarial domain classifier simultaneously to distinguish different kinds of contexts for NMT more explicitly. Here we describe only the modeling procedure of the domain classifier, while it is also applicable to the adversarial domain classifier. Specifically, Er (x) is defined as follows: Er (x) = N X α i hi , (3) i=1 exp(ei ) where αi = PN , i′ exp(ei′ ) ei = (va )⊤ tanh(Wa hi ), and va and Wa are the relevant attention parameters. Then, we feed Er (x) into a fully connected layer with ReLU function (Ballesteros et al., 2015), and then pass its output through a softmax layer to implement domain classification P where H(p(·))=− K k=1 pk (·) log pk (·) is an entropy of distribution p(·) with K domain labels, s1 and θ s2 denote the parameters of softmax θadc adc layer and the generation layer of Es (x) in this classifier, respectively. By this means, Er (x) and Es (x) are expected to encode the domain-specific and domain-shared semantic representations of x, respectively. It should be noted that our utilization of domain classifiers is similar to adversarial training used in (Pryzant et al., 2017) which injects s p(·"
D18-1041,W17-3205,0,0.383123,"gs of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 447–457 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics target words will be assigned greater weights than others in the objective function of our model. Our work demonstrates the benefits of separate modeling of the domain-specific and domainshared contexts, which echoes with the successful applications of the multi-task learning based on shared-private architecture in many tasks, such as discourse relation recognition (Liu et al., 2017b), word segmentation (Chen et al., 2017b), text classification (Liu et al., 2017a), and image classification (Liu et al., 2016). Overall, the main contributions of our work are summarized as follows: ent roles in multi-domain NMT, nevertheless, they are not being distinguished by the current models. Take the sentence shown in Figure 1 for example. The Chinese words “‘Œ¬”(congress), “Æ Y”(bills), “ ”(inclusion), and “Æ§”(agenda) are frequently used in Laws domain and imply the Laws style of the sentence, while other words in this sentence are common in all domains and they mainly indicate the semantic meaning of the sentence. Thus,"
D18-1041,2016.amta-researchers.10,0,0.0830563,"riminative mixing that jointly models NMT with domain classification, adversarial discriminative mixing, and target token mixing which appends a domain token to the target sequence. Sajjad et al. (2017) explored data concatenation, model stacking, data selection and multi-model ensemble to train multi-domain NMT. By exploiting domain as a tag or a feature, Tars and Fishel (2018) treated text domains as distinct languages in order to use multi-lingual approaches when implementing multi-domain NMT. Inspired by topicbased SMT, some researchers resorted to incorporating topical contexts into NMT. Chen et al. (2016) used the topic information of input sentence as an additional input to decoder. Zhang et al. (2016) enhanced the word representation by adding its topic embedding. However, these methods require to have explicit document boundaries between training data, which unfortunately do not exist in most datasets. Overall, our work is related to the second type of approach with (Pryzant et al., 2017) and (Chen et al., 2017a) most related to ours. Unlike (Pryzant et al., 2017) applying adversarial training to only capture domain-shared translation knowledge, we further exploit domain-specific translatio"
D18-1041,P17-1110,0,0.0998462,"Missing"
D18-1041,2015.iwslt-evaluation.11,0,0.162787,"Missing"
D18-1041,P17-2061,0,0.292514,"tence representation vectors of Laws domain does not completely coincide with 3.3 Experimental Analysis Furthermore, we conducted several visualization experiments to empirically analyze the individual effectiveness of the added model components. 3.3.1 Visualizations of Gating Vectors We first visualized the gating vectors gir and gis to quantify their effects on extracting domainspecific and domain-shared contexts from initial source-side annotations. Since both gir and gis are high dimension vectors, which are difficult to be visualized directly, we followed Li et al. (2016) and Zhou et al. (2017) to visualize their individual contributions to the final output, which can be 453 Domain Laws Spoken Thesis News Table 3: Top10 Target Words Article, Chapter, Principles, regulations, Provisions, Political, Servants, specify, China, Municipal meanly, Rusty, 1910s, scours, mountaintops, paralyze, Puff, perpetrators, hitter, weightlifting aggregation, Activities, Computation, Alzheimer, nn, Contemporarily, EVALUATION, ethoxycarbonyl, sCRC, Announced months, agency, outweighed, unconstitutionally, Congolese, session, Asia, news, hurts, francs Examples of Domain-Specific Target Words. those of th"
D18-1041,D13-1176,0,0.110763,"oblem, recently, researchers have carried out many constructive and in-depth studies (Kobus et al., 2016; Zhang et al., 2016; Pryzant et al., 2017; Farajian et al., 2017). However, most of these studies mainly focus on the utilization of domain contexts as a whole in NMT, while ignoring the discrimination of domain contexts at finer-grained level. In each sentence, some words are closely associated with its domain, while others are domain-independent. Intuitively, these two kinds of words play differIntroduction In recent years, neural machine translation (NMT) has achieved great advancement (Nal and Phil, 2013; Sutskever et al., 2014; Bahdanau et al., 2015). However, two difficulties are encountered in the practical applications of NMT. On the one hand, training a NMT model for a spe∗ Corresponding author 447 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 447–457 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics target words will be assigned greater weights than others in the objective function of our model. Our work demonstrates the benefits of separate modeling of the domain-specific and domainshared c"
D18-1041,2012.amta-papers.4,0,0.0646248,"Missing"
D18-1041,P02-1040,0,0.101761,"nsure encoding accuracy of domain-shared contexts, we follow Chen et al. (2017b) to adopt an alternative two-phase strategy in training, where we alternatively optimize s1 and {θ-θ s1 } respectively J (D; θ) with θadc adc fixed at a time. 3 Domain Train Dev Test Laws Spoken Thesis News Medical News Parliamentary 219K 219K 299K 300K 1.09M 180K 2.04M 600 600 800 800 800 800 800 456 455 625 650 2000 2000 2000 Table 1: Sentence numbers of data sets in our experiments. Encoding (Sennrich et al., 2016) to convert all words into subwords. The translation quality was evaluated by case-sensitive BLEU (Papineni et al., 2002). Contrast Models. Since our model is essentially a standard attentional NMT model enhanced with word-level domain contexts, we refer to it as +WDC. We compared it with the following models, namely: Experiment • OpenNMT5 . A famous open-source NMT system used widely in the NMT community trained on mix-domain training set. To investigate the effectiveness of our model, we conducted multi-domain translation experiments on Chinese-English and English-French datasets. • DL4NMT-single (Bahdanau et al., 2015). A reimplemented attentional NMT trained on a single domain dataset. 3.1 Setup Datasets. Fo"
D18-1041,P15-1166,0,0.0245519,"tation on Chinese sentences using Stanford Segmenter3 , and tokenized English and French sentences using MOSES script4 . Then, we employed Byte Pair • DL4NMT-mix (Bahdanau et al., 2015). A reimplemented attentional NMT trained on mix-domain training set. • DL4NMT-finetune (Luong and Manning, 2015). A reimplemented attentional NMT which is first trained using out-of-domain training corpus and then fine-tuned using indomain dataset. • +Domain Control (+DC) (Kobus et al., 2016). It directly introduces embeddings of source domain tag to enrich annotations of encoder. • +Multitask Learning (+ML1) (Dong et al., 2015). It adopts a multi-task learning framework that shares encoder representation and separates the decoder modeling of different domains. • +Multitask Learning (+ML2) (Pryzant et al., 2017). This model jointly trains 1 https://www.ldc.upenn.edu/. http://opus.nlpl.eu/ 3 https://nlp.stanford.edu/ 4 http://www.statmt.org/moses/ 2 5 451 http://opennmt.net/. Model NMT with domain classification via multitask learning. Laws Spoken Contrast Models (1×hd) OpenNMT 45.82 9.15 DL4NMT-single 43.66 5.49 46.82 8.95 DL4NMT-mix DL4NMT-finetune 54.19 8.77 +DC 49.83 9.18 +ML1 46.82 6.66 +ML2 48.95 9.45 +ADM 48.30"
D18-1041,W17-4712,0,0.714484,"eral to different domains. Since the textual styles, sentence structures and terminologies in different domains are often remarkably distinctive, whether such domainspecific translation knowledge is effectively preserved could have a direct effect on the performance of the NMT model. Therefore, how to simultaneously exploit the exclusive and shared translation knowledge of mixed-domain parallel sentences for multi-domain NMT remains a challenging task. To tackle this problem, recently, researchers have carried out many constructive and in-depth studies (Kobus et al., 2016; Zhang et al., 2016; Pryzant et al., 2017; Farajian et al., 2017). However, most of these studies mainly focus on the utilization of domain contexts as a whole in NMT, while ignoring the discrimination of domain contexts at finer-grained level. In each sentence, some words are closely associated with its domain, while others are domain-independent. Intuitively, these two kinds of words play differIntroduction In recent years, neural machine translation (NMT) has achieved great advancement (Nal and Phil, 2013; Sutskever et al., 2014; Bahdanau et al., 2015). However, two difficulties are encountered in the practical applications of NMT"
D18-1041,W17-4713,0,0.183833,"ins. Since the textual styles, sentence structures and terminologies in different domains are often remarkably distinctive, whether such domainspecific translation knowledge is effectively preserved could have a direct effect on the performance of the NMT model. Therefore, how to simultaneously exploit the exclusive and shared translation knowledge of mixed-domain parallel sentences for multi-domain NMT remains a challenging task. To tackle this problem, recently, researchers have carried out many constructive and in-depth studies (Kobus et al., 2016; Zhang et al., 2016; Pryzant et al., 2017; Farajian et al., 2017). However, most of these studies mainly focus on the utilization of domain contexts as a whole in NMT, while ignoring the discrimination of domain contexts at finer-grained level. In each sentence, some words are closely associated with its domain, while others are domain-independent. Intuitively, these two kinds of words play differIntroduction In recent years, neural machine translation (NMT) has achieved great advancement (Nal and Phil, 2013; Sutskever et al., 2014; Bahdanau et al., 2015). However, two difficulties are encountered in the practical applications of NMT. On the one hand, train"
D18-1041,E17-2045,0,0.0883275,"source sequence. Similarly, Johnson et al. (2016) added an artificial token to the input sequence to indicate the required target language. Contrastingly, Farajian et al. (2017) utilized the similarity between each test sentence and the training instances to dynamically set the hyper-parameters of the learning algorithm and update the generic model on the fly. Pryzant et al. (2017) proposed three novel models: discriminative mixing that jointly models NMT with domain classification, adversarial discriminative mixing, and target token mixing which appends a domain token to the target sequence. Sajjad et al. (2017) explored data concatenation, model stacking, data selection and multi-model ensemble to train multi-domain NMT. By exploiting domain as a tag or a feature, Tars and Fishel (2018) treated text domains as distinct languages in order to use multi-lingual approaches when implementing multi-domain NMT. Inspired by topicbased SMT, some researchers resorted to incorporating topical contexts into NMT. Chen et al. (2016) used the topic information of input sentence as an additional input to decoder. Zhang et al. (2016) enhanced the word representation by adding its topic embedding. However, these meth"
D18-1041,P16-1162,0,0.0958223,"θ s1 , θ s2 }, and λ is tively, θ={θnmt , θdc dc adc adc the hyper-parameter for adversarial learning. Particularly, to ensure encoding accuracy of domain-shared contexts, we follow Chen et al. (2017b) to adopt an alternative two-phase strategy in training, where we alternatively optimize s1 and {θ-θ s1 } respectively J (D; θ) with θadc adc fixed at a time. 3 Domain Train Dev Test Laws Spoken Thesis News Medical News Parliamentary 219K 219K 299K 300K 1.09M 180K 2.04M 600 600 800 800 800 800 800 456 455 625 650 2000 2000 2000 Table 1: Sentence numbers of data sets in our experiments. Encoding (Sennrich et al., 2016) to convert all words into subwords. The translation quality was evaluated by case-sensitive BLEU (Papineni et al., 2002). Contrast Models. Since our model is essentially a standard attentional NMT model enhanced with word-level domain contexts, we refer to it as +WDC. We compared it with the following models, namely: Experiment • OpenNMT5 . A famous open-source NMT system used widely in the NMT community trained on mix-domain training set. To investigate the effectiveness of our model, we conducted multi-domain translation experiments on Chinese-English and English-French datasets. • DL4NMT-s"
D18-1041,P13-1082,0,0.10203,"Missing"
D18-1041,2015.mtsummit-papers.19,0,0.0776266,"Missing"
D18-1041,N16-1082,0,0.0306063,"re 4 (b), we observe that the sentence representation vectors of Laws domain does not completely coincide with 3.3 Experimental Analysis Furthermore, we conducted several visualization experiments to empirically analyze the individual effectiveness of the added model components. 3.3.1 Visualizations of Gating Vectors We first visualized the gating vectors gir and gis to quantify their effects on extracting domainspecific and domain-shared contexts from initial source-side annotations. Since both gir and gis are high dimension vectors, which are difficult to be visualized directly, we followed Li et al. (2016) and Zhou et al. (2017) to visualize their individual contributions to the final output, which can be 453 Domain Laws Spoken Thesis News Table 3: Top10 Target Words Article, Chapter, Principles, regulations, Provisions, Political, Servants, specify, China, Municipal meanly, Rusty, 1910s, scours, mountaintops, paralyze, Puff, perpetrators, hitter, weightlifting aggregation, Activities, Computation, Alzheimer, nn, Contemporarily, EVALUATION, ethoxycarbonyl, sCRC, Announced months, agency, outweighed, unconstitutionally, Congolese, session, Asia, news, hurts, francs Examples of Domain-Specific Ta"
D18-1041,tian-etal-2014-um,0,0.109532,"ttentional NMT model enhanced with word-level domain contexts, we refer to it as +WDC. We compared it with the following models, namely: Experiment • OpenNMT5 . A famous open-source NMT system used widely in the NMT community trained on mix-domain training set. To investigate the effectiveness of our model, we conducted multi-domain translation experiments on Chinese-English and English-French datasets. • DL4NMT-single (Bahdanau et al., 2015). A reimplemented attentional NMT trained on a single domain dataset. 3.1 Setup Datasets. For Chinese-English translation, our data comes from UM-Corpus (Tian et al., 2014) and LDC1 . To ensure data quality, we chose only the parallel sentences with domain label Laws, Spoken, and Thesis from UM-Corpus, and the LDC bilingual sentences related to News domain as our dataset. We used randomly selected sentences from UM-Corpus and LDC as development set, and combined the test set of UM-Corpus and randomly selected sentences from LDC to construct our test set. For English-French translation, we conducted experiments on the datasets of OPUS corpus2 , containing sentence pairs from Medical, News, and Parliamentary domains. We also divided these datasets into training, d"
D18-1041,P17-1001,0,0.376499,"tence representation vectors of Laws domain does not completely coincide with 3.3 Experimental Analysis Furthermore, we conducted several visualization experiments to empirically analyze the individual effectiveness of the added model components. 3.3.1 Visualizations of Gating Vectors We first visualized the gating vectors gir and gis to quantify their effects on extracting domainspecific and domain-shared contexts from initial source-side annotations. Since both gir and gis are high dimension vectors, which are difficult to be visualized directly, we followed Li et al. (2016) and Zhou et al. (2017) to visualize their individual contributions to the final output, which can be 453 Domain Laws Spoken Thesis News Table 3: Top10 Target Words Article, Chapter, Principles, regulations, Provisions, Political, Servants, specify, China, Municipal meanly, Rusty, 1910s, scours, mountaintops, paralyze, Puff, perpetrators, hitter, weightlifting aggregation, Activities, Computation, Alzheimer, nn, Contemporarily, EVALUATION, ethoxycarbonyl, sCRC, Announced months, agency, outweighed, unconstitutionally, Congolese, session, Asia, news, hurts, francs Examples of Domain-Specific Target Words. those of th"
D18-1041,P17-2089,0,0.0641518,"Missing"
D18-1041,D17-1155,0,0.0716763,"Missing"
D18-1041,C16-1170,0,0.0934532,"ring a NMT model general to different domains. Since the textual styles, sentence structures and terminologies in different domains are often remarkably distinctive, whether such domainspecific translation knowledge is effectively preserved could have a direct effect on the performance of the NMT model. Therefore, how to simultaneously exploit the exclusive and shared translation knowledge of mixed-domain parallel sentences for multi-domain NMT remains a challenging task. To tackle this problem, recently, researchers have carried out many constructive and in-depth studies (Kobus et al., 2016; Zhang et al., 2016; Pryzant et al., 2017; Farajian et al., 2017). However, most of these studies mainly focus on the utilization of domain contexts as a whole in NMT, while ignoring the discrimination of domain contexts at finer-grained level. In each sentence, some words are closely associated with its domain, while others are domain-independent. Intuitively, these two kinds of words play differIntroduction In recent years, neural machine translation (NMT) has achieved great advancement (Nal and Phil, 2013; Sutskever et al., 2014; Bahdanau et al., 2015). However, two difficulties are encountered in the practic"
D18-1041,P17-1101,0,0.0202202,"that the sentence representation vectors of Laws domain does not completely coincide with 3.3 Experimental Analysis Furthermore, we conducted several visualization experiments to empirically analyze the individual effectiveness of the added model components. 3.3.1 Visualizations of Gating Vectors We first visualized the gating vectors gir and gis to quantify their effects on extracting domainspecific and domain-shared contexts from initial source-side annotations. Since both gir and gis are high dimension vectors, which are difficult to be visualized directly, we followed Li et al. (2016) and Zhou et al. (2017) to visualize their individual contributions to the final output, which can be 453 Domain Laws Spoken Thesis News Table 3: Top10 Target Words Article, Chapter, Principles, regulations, Provisions, Political, Servants, specify, China, Municipal meanly, Rusty, 1910s, scours, mountaintops, paralyze, Puff, perpetrators, hitter, weightlifting aggregation, Activities, Computation, Alzheimer, nn, Contemporarily, EVALUATION, ethoxycarbonyl, sCRC, Announced months, agency, outweighed, unconstitutionally, Congolese, session, Asia, news, hurts, francs Examples of Domain-Specific Target Words. those of th"
D18-1041,D16-1163,0,0.0298909,"WDC(T) 81.51 33.76 +WDC 83.35 34.17 News 30.22 29.56 31.62 34.04 33.94 31.92 33.48 33.43 33.37 30.23 33.28 34.20 33.52 31.90 33.62 33.34 33.68 34.31 33.78 34.87 Table 4: Overall Evaluation on the English-French translation task. 2015; Sennrich et al., 2013). As for NMT, the dominant strategies for domain adaptation generally fall into two categories: The first category is to transfer out-of-domain knowledge to in-domain translation. The conventional method is fine-tuning, which first trains the model on out-of-domain dataset and then finetunes it on in-domain dataset (Luong and Manning, 2015; Zoph et al., 2016; Servan et al., 2016). Freitag and Al-Onaizan (2016) proceeded further by ensembling the fine-tuned model with the original one. Chu et al. (2017) fine-tuned the model using the mix of in-domain and out-of-domain training corpora. From the perspective of data selection, Chen et al. (2017a) scaled the top-level costs of NMT system according to each training sentence’s similarity to the development set. Wang et al. (2017a) explored the data selection strategy based on sentence embeddings for NMT domain adaptation. Moreover, Wang et al. (2017b) further proposed several sentence and domain weight"
D18-1049,N18-1118,0,0.216407,"ce and Technology # Sogou Inc., Beijing, China § Soochow University, Suzhou, China Abstract pability to minimize the path length between longdistance dependencies in neural networks contributes to its exceptional performance. However, the Transformer model still suffers from a major drawback: it performs translation only at the sentence level and ignores documentlevel context. Document-level context has proven to be beneficial for improving translation performance, not only for conventional SMT (Gong et al., 2011; Hardmeier et al., 2012), but also for NMT (Wang et al., 2017; Tu et al., 2018). Bawden et al. (2018) indicate that it is important to exploit document-level context to deal with contextdependent phenomena which are problematic for machine translation such as coreference, lexical cohesion, and lexical disambiguation. While document-level NMT has attracted increasing attention from the community in the past two years (Jean et al., 2017; Kuang et al., 2017; Tiedemann and Scherrer, 2017; Wang et al., 2017; Maruf and Haffari, 2018; Bawden et al., 2018; Tu et al., 2018; Voita et al., 2018), to the best of our knowledge, only one existing work has endeavored to model document-level context for the"
D18-1049,D11-1084,1,0.945014,"ology, Tsinghua University, Beijing, China ‡ Beijing National Research Center for Information Science and Technology # Sogou Inc., Beijing, China § Soochow University, Suzhou, China Abstract pability to minimize the path length between longdistance dependencies in neural networks contributes to its exceptional performance. However, the Transformer model still suffers from a major drawback: it performs translation only at the sentence level and ignores documentlevel context. Document-level context has proven to be beneficial for improving translation performance, not only for conventional SMT (Gong et al., 2011; Hardmeier et al., 2012), but also for NMT (Wang et al., 2017; Tu et al., 2018). Bawden et al. (2018) indicate that it is important to exploit document-level context to deal with contextdependent phenomena which are problematic for machine translation such as coreference, lexical cohesion, and lexical disambiguation. While document-level NMT has attracted increasing attention from the community in the past two years (Jean et al., 2017; Kuang et al., 2017; Tiedemann and Scherrer, 2017; Wang et al., 2017; Maruf and Haffari, 2018; Bawden et al., 2018; Tu et al., 2018; Voita et al., 2018), to the"
D18-1049,D12-1108,0,0.535179,"versity, Beijing, China ‡ Beijing National Research Center for Information Science and Technology # Sogou Inc., Beijing, China § Soochow University, Suzhou, China Abstract pability to minimize the path length between longdistance dependencies in neural networks contributes to its exceptional performance. However, the Transformer model still suffers from a major drawback: it performs translation only at the sentence level and ignores documentlevel context. Document-level context has proven to be beneficial for improving translation performance, not only for conventional SMT (Gong et al., 2011; Hardmeier et al., 2012), but also for NMT (Wang et al., 2017; Tu et al., 2018). Bawden et al. (2018) indicate that it is important to exploit document-level context to deal with contextdependent phenomena which are problematic for machine translation such as coreference, lexical cohesion, and lexical disambiguation. While document-level NMT has attracted increasing attention from the community in the past two years (Jean et al., 2017; Kuang et al., 2017; Tiedemann and Scherrer, 2017; Wang et al., 2017; Maruf and Haffari, 2018; Bawden et al., 2018; Tu et al., 2018; Voita et al., 2018), to the best of our knowledge, o"
D18-1049,P15-1001,0,0.0438623,"h et al., 2016) with 32K merges to segment words into sub-word units for all languages. For the original Transformer model and our extended model, the hidden size is set to 512 and the Unfortunately, large-scale document-level parallel corpora are usually unavailable, even for resource-rich languages such as English and Chinese. Under small-data training conditions, document-level NMT is prone to underperform sentence-level NMT because of poor estimates of low-frequency events. To address this problem, we adopt the idea of freezing some parameters while tuning the remaining part of the model (Jean et al., 2015; Zoph et al., 2016). We propose a two-step training strategy that uses an additional sentence-level parallel corpus Ds , which can be larger than Dd . We divide model parameters into two subsets: θ = θs ∪ θd , where θs is a set of original sentencelevel model parameters (highlighted in blue in Figure 1(b)) and θd is a set of newly-introduced document-level model parameters (highlighted in red in Figure 1(b)). In the first step, sentence-level parameters θs are estimated on the combined sentence-level parallel corpus Ds ∪ Dd : 2 X θˆs = argmax log P (y|x; θs ). (24) hx,yi∈Ds ∪Dd Note that the"
D18-1049,D13-1163,1,0.886079,"tlevel context to the encoder and decoder (see Section 2.3). It is clear that integrating document-level context into the encoder (Eq. 12) brings significant improvements (i.e., 45.97 vs. 47.51). Similarly, it is also beneficial to integrate document-level context into the decoder (Eq. 16). Combining both leads to further improvements. This observation suggests that documentlevel context does help to improve Transformer. 3.9 4 Developing document-level models for machine translation has been an important research direction, both for conventional SMT (Gong et al., 2011; Hardmeier et al., 2012; Xiong et al., 2013a,b; Garcia et al., 2014) and NMT (Jean et al., 2017; Kuang et al., 2017; Tiedemann and Scherrer, 2017; Wang et al., 2017; Maruf and Haffari, 2018; Bawden et al., 2018; Tu et al., 2018; Voita et al., 2018). Most existing work on document-level NMT has focused on integrating document-level context into the RNNsearch model (Bahdanau et al., Effect of Context Gating As shown in Table 9, we also validated the effectiveness of context gating (see Section 2.3.3). We find that replacing residual connections with context gating leads to an overall improvement of 0.38 BLEU point. 3.10 Related Work Anal"
D18-1049,P18-1249,0,0.0320133,"nt-level context often includes several sentences, it is important to capture long-range dependencies and identify relevant information. We use multi-head self-attention (Vaswani et al., 2017) to compute the representation of documentlevel context because it is capable of reducing the maximum path length between long-range dependencies to O(1) (Vaswani et al., 2017) and determining the relative importance of different locations in the context (Bahdanau et al., 2015). Because of this property, multi-head self-attention has proven to be effective in other NLP tasks such as constituency parsing (Kitaev and Klein, 2018). where C(1) ∈ RD×M is the annotation of X&lt;k af(1) ter the first layer, A·,m ∈ RD×1 is the column vector for the m-th contextual word, and FNN(·) is a position-wise fully connected feed-forward network (Vaswani et al., 2017). This process iterates Nc times as follows:   A(n) = MultiHead C(n−1) , C(n−1) , C(n−1) , (8) h i (n) (n) C(n) = FNN(A·,1 ); . . . ; FNN(A·,M ) , (9) 535 (k) where A(n) and C(n) (n = 1, . . . , Nc ) are the hidden state and annotation at the n-th layer, respectively. Note that C(0) = Xc . 2.3 where y0 ∈ RD×1 is the vector representation of a begin-of-sentence token and Y"
D18-1049,P17-4012,0,0.171804,"alculated on the development set. filter size is set to 2,048. The multi-head attention has 8 individual attention heads. We set N = Ns = Nt = 6. In training, we use Adam (Kingma and Ba, 2015) for optimization. Each mini-batch contains approximately 24K words. We use the learning rate decay policy described by Vaswani et al. (2017). In decoding, the beam size is set to 4. We use the length penalty (Wu et al., 2016) and set the hyper-parameter α to 0.6. We use four Tesla P40 GPUs for training and one Tesla P40 GPU for decoding. We implement our approach on top of the open-source toolkit THUMT (Zhang et al., 2017). 4 3.2 2. (Kuang et al., 2017): using a cache which stores previous translated words and topical words to incorporate document-level context into the RNNsearch model. They use a document-level parallel corpus containing 2.8M sentence pairs. Table 3 gives the BLEU scores reported in their paper. 3. (Vaswani et al., 2017): the state-of-the-art NMT model that does not exploit documentlevel context. We use the open-source toolkit THUMT (Zhang et al., 2017) to train and evaluate the model. The training dataset is our sentence-level parallel corpus containing 2M sentence pairs. Effect of Context Le"
D18-1049,P18-1118,0,0.40399,"al for improving translation performance, not only for conventional SMT (Gong et al., 2011; Hardmeier et al., 2012), but also for NMT (Wang et al., 2017; Tu et al., 2018). Bawden et al. (2018) indicate that it is important to exploit document-level context to deal with contextdependent phenomena which are problematic for machine translation such as coreference, lexical cohesion, and lexical disambiguation. While document-level NMT has attracted increasing attention from the community in the past two years (Jean et al., 2017; Kuang et al., 2017; Tiedemann and Scherrer, 2017; Wang et al., 2017; Maruf and Haffari, 2018; Bawden et al., 2018; Tu et al., 2018; Voita et al., 2018), to the best of our knowledge, only one existing work has endeavored to model document-level context for the Transformer model (Voita et al., 2018). Previous approaches to document-level NMT have concentrated on the RNNsearch model (Bahdanau et al., 2015). It is challenging to adapt these approaches to Transformer because they are designed specifically for RNNsearch. In this work, we propose to extend the Transformer model to take advantage of documentlevel context. The basic idea is to use multihead self-attention (Vaswani et al., 20"
D18-1049,D16-1163,0,0.0330673,"h 32K merges to segment words into sub-word units for all languages. For the original Transformer model and our extended model, the hidden size is set to 512 and the Unfortunately, large-scale document-level parallel corpora are usually unavailable, even for resource-rich languages such as English and Chinese. Under small-data training conditions, document-level NMT is prone to underperform sentence-level NMT because of poor estimates of low-frequency events. To address this problem, we adopt the idea of freezing some parameters while tuning the remaining part of the model (Jean et al., 2015; Zoph et al., 2016). We propose a two-step training strategy that uses an additional sentence-level parallel corpus Ds , which can be larger than Dd . We divide model parameters into two subsets: θ = θs ∪ θd , where θs is a set of original sentencelevel model parameters (highlighted in blue in Figure 1(b)) and θd is a set of newly-introduced document-level model parameters (highlighted in red in Figure 1(b)). In the first step, sentence-level parameters θs are estimated on the combined sentence-level parallel corpus Ds ∪ Dd : 2 X θˆs = argmax log P (y|x; θs ). (24) hx,yi∈Ds ∪Dd Note that the newly introduced mod"
D18-1049,2012.eamt-1.60,0,0.196011,"2M Chinese-English sentence pairs with 54.8M Chinese words and 60.8M English words. 3 The document-level parallel corpus is a subset of the full training set, including 41K documents with 940K sentence pairs. On average, each document in the training set contains 22.9 sentences. We use the NIST 2006 dataset as the development set and the NIST 2002, 2003, 2004, 2005, 2008 datasets as test sets. The development and test sets contain 588 documents with 5,833 sentences. On average, each document contains 9.9 sentences. In French-English translation task, we use the IWSLT bilingual training data (Mauro et al., 2012) which contains 1,824 documents with 220K sentence pairs as training set. For development and testing, we use the IWSLT 2010 development and test sets, which contains 8 documents with 887 sentence pairs and 11 documents with 1,664 sentence pairs respectively. The evaluation metric for both tasks is case-insensitive BLEU score as calculated by the multi-bleu.perl script. In preprocessing, we use byte pair encoding (Sennrich et al., 2016) with 32K merges to segment words into sub-word units for all languages. For the original Transformer model and our extended model, the hidden size is set to 51"
D18-1049,P16-1162,0,0.260437,"88 documents with 5,833 sentences. On average, each document contains 9.9 sentences. In French-English translation task, we use the IWSLT bilingual training data (Mauro et al., 2012) which contains 1,824 documents with 220K sentence pairs as training set. For development and testing, we use the IWSLT 2010 development and test sets, which contains 8 documents with 887 sentence pairs and 11 documents with 1,664 sentence pairs respectively. The evaluation metric for both tasks is case-insensitive BLEU score as calculated by the multi-bleu.perl script. In preprocessing, we use byte pair encoding (Sennrich et al., 2016) with 32K merges to segment words into sub-word units for all languages. For the original Transformer model and our extended model, the hidden size is set to 512 and the Unfortunately, large-scale document-level parallel corpora are usually unavailable, even for resource-rich languages such as English and Chinese. Under small-data training conditions, document-level NMT is prone to underperform sentence-level NMT because of poor estimates of low-frequency events. To address this problem, we adopt the idea of freezing some parameters while tuning the remaining part of the model (Jean et al., 20"
D18-1049,P16-1159,1,0.842461,"ion. To address this problem, we replace the residual connections after the context attention sub-layer with a position-wise context gating sub-layer: this step. P (y|x; θs ) is identical to the original Transformer model, which is a special case of our model. In the second step, document-level parameters θd are estimated on the document-level parallel corpus Dd only: X log P (Y|X; θˆs , θd ). (25) θˆd = argmax Gating(H) = λH + (1 − λ)SubLayer(H). (21) θd The gating weight is given by λ = σ(Wi H + Ws SubLayer(H)), Our approach is also similar to pre-training which has been widely used in NMT (Shen et al., 2016; Tu et al., 2018). The major difference is that our approach keeps θˆs fixed when estimating θd to prevent the model from overfitting on the relatively smaller document-level parallel corpora. (22) where σ(·) is a sigmoid function, Wi and Ws are model parameters. 2.4 Training Given a document-level parallel corpus Dd , the standard training objective is to maximize the loglikelihood of the training data: ( ) X θˆ = argmax log P (Y|X; θ) . (23) θ 3 Setup We evaluate our approach on Chinese-English and French-English translation tasks. In ChineseEnglish translation task, the training set contai"
D18-1049,W17-4811,0,0.179065,"Document-level context has proven to be beneficial for improving translation performance, not only for conventional SMT (Gong et al., 2011; Hardmeier et al., 2012), but also for NMT (Wang et al., 2017; Tu et al., 2018). Bawden et al. (2018) indicate that it is important to exploit document-level context to deal with contextdependent phenomena which are problematic for machine translation such as coreference, lexical cohesion, and lexical disambiguation. While document-level NMT has attracted increasing attention from the community in the past two years (Jean et al., 2017; Kuang et al., 2017; Tiedemann and Scherrer, 2017; Wang et al., 2017; Maruf and Haffari, 2018; Bawden et al., 2018; Tu et al., 2018; Voita et al., 2018), to the best of our knowledge, only one existing work has endeavored to model document-level context for the Transformer model (Voita et al., 2018). Previous approaches to document-level NMT have concentrated on the RNNsearch model (Bahdanau et al., 2015). It is challenging to adapt these approaches to Transformer because they are designed specifically for RNNsearch. In this work, we propose to extend the Transformer model to take advantage of documentlevel context. The basic idea is to use"
D18-1049,P18-1117,0,0.365642,"onal SMT (Gong et al., 2011; Hardmeier et al., 2012), but also for NMT (Wang et al., 2017; Tu et al., 2018). Bawden et al. (2018) indicate that it is important to exploit document-level context to deal with contextdependent phenomena which are problematic for machine translation such as coreference, lexical cohesion, and lexical disambiguation. While document-level NMT has attracted increasing attention from the community in the past two years (Jean et al., 2017; Kuang et al., 2017; Tiedemann and Scherrer, 2017; Wang et al., 2017; Maruf and Haffari, 2018; Bawden et al., 2018; Tu et al., 2018; Voita et al., 2018), to the best of our knowledge, only one existing work has endeavored to model document-level context for the Transformer model (Voita et al., 2018). Previous approaches to document-level NMT have concentrated on the RNNsearch model (Bahdanau et al., 2015). It is challenging to adapt these approaches to Transformer because they are designed specifically for RNNsearch. In this work, we propose to extend the Transformer model to take advantage of documentlevel context. The basic idea is to use multihead self-attention (Vaswani et al., 2017) to compute the representation of document-level context"
D18-1049,D17-1301,0,0.15683,"Research Center for Information Science and Technology # Sogou Inc., Beijing, China § Soochow University, Suzhou, China Abstract pability to minimize the path length between longdistance dependencies in neural networks contributes to its exceptional performance. However, the Transformer model still suffers from a major drawback: it performs translation only at the sentence level and ignores documentlevel context. Document-level context has proven to be beneficial for improving translation performance, not only for conventional SMT (Gong et al., 2011; Hardmeier et al., 2012), but also for NMT (Wang et al., 2017; Tu et al., 2018). Bawden et al. (2018) indicate that it is important to exploit document-level context to deal with contextdependent phenomena which are problematic for machine translation such as coreference, lexical cohesion, and lexical disambiguation. While document-level NMT has attracted increasing attention from the community in the past two years (Jean et al., 2017; Kuang et al., 2017; Tiedemann and Scherrer, 2017; Wang et al., 2017; Maruf and Haffari, 2018; Bawden et al., 2018; Tu et al., 2018; Voita et al., 2018), to the best of our knowledge, only one existing work has endeavored"
D18-1184,D15-1075,0,0.484435,"in the first sentence to candidate spans in the second sentence, simultaneously discovering the tree structure of each sentence. Our model is fully differentiable and trained only on the matching objective. We evaluate this model on two tasks, entailment detection and answer sentence selection, and find that modeling latent tree structures results in superior performance. Analysis of the learned sentence structures shows they can reflect some syntactic phenomena. 1 Introduction There are many tasks in natural language processing that require matching two sentences: natural language inference (Bowman et al., 2015; Nangia et al., 2017) and paraphrase detection (Wang et al., 2017b) are classification tasks over sentence pairs, and question answering often requires an alignment between a question and a passage of text that may contain the answer (Tan et al., 2016a; Rajpurkar et al., 2016; Joshi et al., 2017). Most neural models for these tasks perform comparisons between the two sentences either at ∗ Work done during an internship at Allen Institute for Artificial Intelligence. the word level (Parikh et al., 2016), or at the sentence level (Bowman et al., 2015). Word-level comparisons ignore the inherent"
D18-1184,P16-1139,0,0.0994531,"these tasks perform comparisons between the two sentences either at ∗ Work done during an internship at Allen Institute for Artificial Intelligence. the word level (Parikh et al., 2016), or at the sentence level (Bowman et al., 2015). Word-level comparisons ignore the inherent structure of the sentences being compared, at best relying on a recurrent neural network such as an LSTM (Hochreiter and Schmidhuber, 1997) to incorporate some amount of context from neighboring words into each word’s representation. Sentence-level comparisons can incorporate the structure of each sentence individually (Bowman et al., 2016; Tai et al., 2015), but cannot easily compare substructures between the sentences, as these are all squashed into a single vector. Some models do incorporate sentence structure by comparing subtrees between the two sentences (Zhao et al., 2016; Chen et al., 2017), but require pipelined approaches where a parser is run in a non-differentiable preprocessing step, losing the benefits of end-to-end training. In this paper we propose a method, which we call structured alignment networks, to perform comparisons between substructures in two sentences, in a more interpretable way, and without relying"
D18-1184,P17-1152,0,0.213268,"the inherent structure of the sentences being compared, at best relying on a recurrent neural network such as an LSTM (Hochreiter and Schmidhuber, 1997) to incorporate some amount of context from neighboring words into each word’s representation. Sentence-level comparisons can incorporate the structure of each sentence individually (Bowman et al., 2016; Tai et al., 2015), but cannot easily compare substructures between the sentences, as these are all squashed into a single vector. Some models do incorporate sentence structure by comparing subtrees between the two sentences (Zhao et al., 2016; Chen et al., 2017), but require pipelined approaches where a parser is run in a non-differentiable preprocessing step, losing the benefits of end-to-end training. In this paper we propose a method, which we call structured alignment networks, to perform comparisons between substructures in two sentences, in a more interpretable way, and without relying on an external, non-differentiable parser. We use a structured attention mechanism (Kim et al., 2017; Liu and Lapata, 2018) to compute a structured alignment between the two sentences, jointly learning a latent tree structure for each sentence and aligning spans"
D18-1184,D16-1053,1,0.88839,"Missing"
D18-1184,D16-1001,0,0.015428,"ght child on a non-terminal node and the second term is the score for spanij being the left child. In Figure 2b, we illustrate the outside process with the target span spanij being the right 1556 child of a non-terminal node. This process is calculated recursively from root to bottom. The normalized marginal probability ρij for each span spanij , where 1 ≤ i < n, i < j ≤ n can be calculated by: ρij = αij βij /α0n (12) To compute the representations of all possible spans, we use Long Short-Term Memory Neural Networks (LSTMs; Hochreiter and Schmidhuber 1997) with max-pooling and minus features (Cross and Huang, 2016; Liu and Lapata, 2017). We represent each sentence as a sequence of word embeddings [wsos , w1 , · · · , wt , · · · , wn , weos ] and run a bidirectional LSTM to obtain the output vectors. ht = [~ht , h~t ] is the output vector for the tth word, and ~ht and h~t are the output vectors from the forward and backward directions, respectively. We represent a constituent from position i to j with a span vector spij : spmaxpool = max(hi , · · · , hj ) ij spminus = [~hj − ~hi−1 , h~i − h~j+1 ] ij spij = [spmaxpool , spminus ] ij ij (13) spans across the two sentences, and the attended vectors can be"
D18-1184,P08-1109,0,0.220956,"Missing"
D18-1184,Q15-1035,0,0.0238931,"all structured alignment networks, to perform comparisons between substructures in two sentences, in a more interpretable way, and without relying on an external, non-differentiable parser. We use a structured attention mechanism (Kim et al., 2017; Liu and Lapata, 2018) to compute a structured alignment between the two sentences, jointly learning a latent tree structure for each sentence and aligning spans between the two sentences. Our method constructs a CKY chart for each sentence using the inside-outside algorithm (Manning et al., 1999), which is fully differentiable (Li and Eisner, 2009; Gormley et al., 2015). This chart has a node for each possible span in the sentence, and a score for the likelihood of that span being a constituent in a parse of the sentence, marginalized over all possible parses. We take these two charts and find alignments between them, representing each span in each sentence with structured attention over spans in the other sentence. These 1554 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1554–1564 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics span representations, weighted b"
D18-1184,N16-1108,0,0.0460947,"Missing"
D18-1184,D16-1073,0,0.0145631,"recent development. Kim et al. (2017) were the first to use inference to compute structured attentions over latent sentence variables, inducing tree structures trained on the end-to-end objective. Liu and Lapata (2018) showed how to do this more efficiently, although their work is still limited to structured attention over a single sentence. Our model is the first to include latent structured alignments between two sentences. Grammar Induction Unsupervised grammar induction is a well-studied problem (Cohen and Smith, 2009). The most recent work in this direction was the Neural E-DMV model of Jiang et al. (2016). While our goal is not to induce a grammar, we do produce a probabilistic grammar as a byproduct of our model. Our results suggest that training on more complex objectives may be a good way to pursue grammar induction in the future; forcing the model to construct consistent, comparable subtrees between the two sentences is a strong signal for grammar induction. Very recently, a few models attempt to infer latent dependency tree structures with neural models in sentence modeling tasks (Yogatama et al., 2017; Choi et al., 2018). 6 Conclusions In this paper we have considered the problem of comp"
D18-1184,P17-1147,0,0.0264384,"d find that modeling latent tree structures results in superior performance. Analysis of the learned sentence structures shows they can reflect some syntactic phenomena. 1 Introduction There are many tasks in natural language processing that require matching two sentences: natural language inference (Bowman et al., 2015; Nangia et al., 2017) and paraphrase detection (Wang et al., 2017b) are classification tasks over sentence pairs, and question answering often requires an alignment between a question and a passage of text that may contain the answer (Tan et al., 2016a; Rajpurkar et al., 2016; Joshi et al., 2017). Most neural models for these tasks perform comparisons between the two sentences either at ∗ Work done during an internship at Allen Institute for Artificial Intelligence. the word level (Parikh et al., 2016), or at the sentence level (Bowman et al., 2015). Word-level comparisons ignore the inherent structure of the sentences being compared, at best relying on a recurrent neural network such as an LSTM (Hochreiter and Schmidhuber, 1997) to incorporate some amount of context from neighboring words into each word’s representation. Sentence-level comparisons can incorporate the structure of eac"
D18-1184,N09-1009,0,0.019384,"6), the use of inference algorithms as intermediate layers in end-to-end neural networks is a recent development. Kim et al. (2017) were the first to use inference to compute structured attentions over latent sentence variables, inducing tree structures trained on the end-to-end objective. Liu and Lapata (2018) showed how to do this more efficiently, although their work is still limited to structured attention over a single sentence. Our model is the first to include latent structured alignments between two sentences. Grammar Induction Unsupervised grammar induction is a well-studied problem (Cohen and Smith, 2009). The most recent work in this direction was the Neural E-DMV model of Jiang et al. (2016). While our goal is not to induce a grammar, we do produce a probabilistic grammar as a byproduct of our model. Our results suggest that training on more complex objectives may be a good way to pursue grammar induction in the future; forcing the model to construct consistent, comparable subtrees between the two sentences is a strong signal for grammar induction. Very recently, a few models attempt to infer latent dependency tree structures with neural models in sentence modeling tasks (Yogatama et al., 20"
D18-1184,P04-1061,0,0.182318,"del considers all possible span comparisons, weighted by the spans’ marginal likelihood. likely each span is to appear as a constituent in a parse of the sentence. We use the non-terminal nodes of a binary constituency parse to represent spans. Because of this choice of representation, we can use the nodes in a CKY parsing chart to efficiently marginalize span likelihood over all possible parses for each sentence, and compare nodes in each sentence’s chart. 3.1 Learning Latent Constituency Trees A constituency parser can be partially formalized as a graphical model with the following cliques (Klein and Manning, 2004): the latent variables cikj ∈ 0, 1 for all i < j, indicating whether the span from the i-th token to the j-th token (spanij ) is a constituency node built from the merging of sub-node spanik and span(k+1)j . Given a sentence x = [xi , · · · , xn ], the probability of a tree z is, Q cikj ∈z p(cikj = 1) Q p(z|x) = P (8) z 0 ∈Z cikj ∈z 0 p(cikj = 1) where Z represents all possible constituency trees for x. The parameters for the graph-based CRF constituency parser are δikj reflecting the scores of spanij forming a binary constituency node with k as the splitting point. It is possible to calculate"
D18-1184,W17-5301,0,0.0849449,"to candidate spans in the second sentence, simultaneously discovering the tree structure of each sentence. Our model is fully differentiable and trained only on the matching objective. We evaluate this model on two tasks, entailment detection and answer sentence selection, and find that modeling latent tree structures results in superior performance. Analysis of the learned sentence structures shows they can reflect some syntactic phenomena. 1 Introduction There are many tasks in natural language processing that require matching two sentences: natural language inference (Bowman et al., 2015; Nangia et al., 2017) and paraphrase detection (Wang et al., 2017b) are classification tasks over sentence pairs, and question answering often requires an alignment between a question and a passage of text that may contain the answer (Tan et al., 2016a; Rajpurkar et al., 2016; Joshi et al., 2017). Most neural models for these tasks perform comparisons between the two sentences either at ∗ Work done during an internship at Allen Institute for Artificial Intelligence. the word level (Parikh et al., 2016), or at the sentence level (Bowman et al., 2015). Word-level comparisons ignore the inherent structure of the sent"
D18-1184,D16-1244,0,0.0890286,"Missing"
D18-1184,D14-1162,0,0.0837504,"t (Separated Parameters) QA-LSTM (Tan et al., 2016b) Attentive Pooling Network (Santos et al., 2016) Pairwise Word Interaction (He and Lin, 2016) Lexical Decomposition and Composition (Wang et al., 2016) Noise-Contrastive Estimation (Rao et al., 2016) BiMPM (Wang et al., 2017b) MAP 0.764 0.772 0.780 0.780 0.786 0.730 0.753 0.777 0.771 0.801 0.802 MRR 0.842 0.851 0.846 0.860 0.860 0.824 0.851 0.836 0.845 0.877 0.875 Table 1: Results of our models (top) and previously proposed systems (bottom) on the TREC-QA test set. For both tasks, we initialize our model with 300D 840B GloVe word embeddings (Pennington et al., 2014). The hidden size for the BiLSTM is 150. The feed-forward networks F1 and F2 are two-layer perceptrons with ReLU as the hidden activation function and the size of the hidden and output layers is set to 300. All hyperparameters are selected based on the model’s performance on the development set. 4.1 Answer Sentence Selection We first study the effectiveness of our model for answer sentence selection tasks. Given a question, answer sentence selection aims to rank a list of candidate answer sentences based on their relatedness to the question. We experiment on the TRECQA dataset (Wang et al., 20"
D18-1184,N16-1030,0,0.0192327,"l., 2017; Zhao et al., 2016). However, all of these models are pipelined; they obtain the sentence structure in a non-differentiable preprocessing step, losing the benefits of end-to-end training. Ours is the first model to allow comparison between latent tree structures, trained end-to-end on the comparison objective. Structured attention While it has long been known that inference in graphical models is differentiable (Li and Eisner, 2009; Domke, 2011), and using inference in, e.g., a CRF (Lafferty et al., 2001) as the last layer in a neural network is common practice (Liu and Lapata, 2017; Lample et al., 2016), the use of inference algorithms as intermediate layers in end-to-end neural networks is a recent development. Kim et al. (2017) were the first to use inference to compute structured attentions over latent sentence variables, inducing tree structures trained on the end-to-end objective. Liu and Lapata (2018) showed how to do this more efficiently, although their work is still limited to structured attention over a single sentence. Our model is the first to include latent structured alignments between two sentences. Grammar Induction Unsupervised grammar induction is a well-studied problem (Co"
D18-1184,D16-1264,0,0.0321899,"r sentence selection, and find that modeling latent tree structures results in superior performance. Analysis of the learned sentence structures shows they can reflect some syntactic phenomena. 1 Introduction There are many tasks in natural language processing that require matching two sentences: natural language inference (Bowman et al., 2015; Nangia et al., 2017) and paraphrase detection (Wang et al., 2017b) are classification tasks over sentence pairs, and question answering often requires an alignment between a question and a passage of text that may contain the answer (Tan et al., 2016a; Rajpurkar et al., 2016; Joshi et al., 2017). Most neural models for these tasks perform comparisons between the two sentences either at ∗ Work done during an internship at Allen Institute for Artificial Intelligence. the word level (Parikh et al., 2016), or at the sentence level (Bowman et al., 2015). Word-level comparisons ignore the inherent structure of the sentences being compared, at best relying on a recurrent neural network such as an LSTM (Hochreiter and Schmidhuber, 1997) to incorporate some amount of context from neighboring words into each word’s representation. Sentence-level comparisons can incorporate"
D18-1184,D09-1005,0,0.0804877,"a method, which we call structured alignment networks, to perform comparisons between substructures in two sentences, in a more interpretable way, and without relying on an external, non-differentiable parser. We use a structured attention mechanism (Kim et al., 2017; Liu and Lapata, 2018) to compute a structured alignment between the two sentences, jointly learning a latent tree structure for each sentence and aligning spans between the two sentences. Our method constructs a CKY chart for each sentence using the inside-outside algorithm (Manning et al., 1999), which is fully differentiable (Li and Eisner, 2009; Gormley et al., 2015). This chart has a node for each possible span in the sentence, and a score for the likelihood of that span being a constituent in a parse of the sentence, marginalized over all possible parses. We take these two charts and find alignments between them, representing each span in each sentence with structured attention over spans in the other sentence. These 1554 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1554–1564 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics span repr"
D18-1184,D17-1133,1,0.925393,"inal node and the second term is the score for spanij being the left child. In Figure 2b, we illustrate the outside process with the target span spanij being the right 1556 child of a non-terminal node. This process is calculated recursively from root to bottom. The normalized marginal probability ρij for each span spanij , where 1 ≤ i < n, i < j ≤ n can be calculated by: ρij = αij βij /α0n (12) To compute the representations of all possible spans, we use Long Short-Term Memory Neural Networks (LSTMs; Hochreiter and Schmidhuber 1997) with max-pooling and minus features (Cross and Huang, 2016; Liu and Lapata, 2017). We represent each sentence as a sequence of word embeddings [wsos , w1 , · · · , wt , · · · , wn , weos ] and run a bidirectional LSTM to obtain the output vectors. ht = [~ht , h~t ] is the output vector for the tth word, and ~ht and h~t are the output vectors from the forward and backward directions, respectively. We represent a constituent from position i to j with a span vector spij : spmaxpool = max(hi , · · · , hj ) ij spminus = [~hj − ~hi−1 , h~i − h~j+1 ] ij spij = [spmaxpool , spminus ] ij ij (13) spans across the two sentences, and the attended vectors can be calculated as: eij,kl ="
D18-1184,Q18-1005,1,0.934904,"uashed into a single vector. Some models do incorporate sentence structure by comparing subtrees between the two sentences (Zhao et al., 2016; Chen et al., 2017), but require pipelined approaches where a parser is run in a non-differentiable preprocessing step, losing the benefits of end-to-end training. In this paper we propose a method, which we call structured alignment networks, to perform comparisons between substructures in two sentences, in a more interpretable way, and without relying on an external, non-differentiable parser. We use a structured attention mechanism (Kim et al., 2017; Liu and Lapata, 2018) to compute a structured alignment between the two sentences, jointly learning a latent tree structure for each sentence and aligning spans between the two sentences. Our method constructs a CKY chart for each sentence using the inside-outside algorithm (Manning et al., 1999), which is fully differentiable (Li and Eisner, 2009; Gormley et al., 2015). This chart has a node for each possible span in the sentence, and a score for the likelihood of that span being a constituent in a parse of the sentence, marginalized over all possible parses. We take these two charts and find alignments between t"
D18-1184,W09-3714,0,0.0422386,"sequence. There are some efforts to strengthen the decomposable attention model with a recurrent neural network (Liu and Lapata, 2018) or intrasentence attention (Parikh et al., 2016). However, these models amount to simply changing the input vectors a and b, and still only perform a wordlevel alignment between the two sentences. 3 Structured Alignment Networks Language is inherently tree structured, and the meaning of sentences comes largely from composing the meanings of subtrees (Chomsky, 2002). It is natural, then, to compare the meaning of two sentences by comparing their substructures (MacCartney and Manning, 2009). For example, when determining the relationship between two sentences in Figure 1, the ideal units of comparison are spans determined by subtrees: “is in Seattle” compared to “based in Washington state”. The challenge with comparing spans drawn from subtrees is that the tree structure of the sentence is latent and must be inferred, either during pre-processing or in the model itself. In this section we present a model that operates on the latent tree structure of each sentence, comparing all possible spans in one sentence with all possible spans in the second sentence, weighted by how 1555 A:"
D18-1184,P14-5010,0,0.0148714,"re our model and variants thereof against several baselines. The first baseline is the Word-level Decomposable Attention model strengthened with a bidirectional LSTM for obtaining a contextualized representation for each word. The second baseline is a Simple Span Alignment model; we use an MLP layer over the LSTM outputs to calculate the unnormalized scores and replace the inside-outside algorithm with a simple softmax function to obtain the probability distribution over all candidate spans. We also introduce a pipelined baseline where we extract constituents from trees parsed by the CoreNLP (Manning et al., 2014) constituency parser, and use the Simple Span Alignment model to only align these constituents. As shown in Table 1, we use two variants of the Structured Alignment model, since the structure of the question and the answer sentence may be different; the first model shares parameters across the question and the answer for computing the structures, while the second one uses separate parameters. We view the sentence selection task as a binary classification problem and the final ranking is based on the predicted probability of the sentence containing the correct answer (positive label). We apply"
D18-1184,P15-1150,0,0.324285,"Missing"
D18-1184,P16-1044,0,0.162701,"detection and answer sentence selection, and find that modeling latent tree structures results in superior performance. Analysis of the learned sentence structures shows they can reflect some syntactic phenomena. 1 Introduction There are many tasks in natural language processing that require matching two sentences: natural language inference (Bowman et al., 2015; Nangia et al., 2017) and paraphrase detection (Wang et al., 2017b) are classification tasks over sentence pairs, and question answering often requires an alignment between a question and a passage of text that may contain the answer (Tan et al., 2016a; Rajpurkar et al., 2016; Joshi et al., 2017). Most neural models for these tasks perform comparisons between the two sentences either at ∗ Work done during an internship at Allen Institute for Artificial Intelligence. the word level (Parikh et al., 2016), or at the sentence level (Bowman et al., 2015). Word-level comparisons ignore the inherent structure of the sentences being compared, at best relying on a recurrent neural network such as an LSTM (Hochreiter and Schmidhuber, 1997) to incorporate some amount of context from neighboring words into each word’s representation. Sentence-level co"
D18-1184,D07-1003,0,0.0413725,"n et al., 2014). The hidden size for the BiLSTM is 150. The feed-forward networks F1 and F2 are two-layer perceptrons with ReLU as the hidden activation function and the size of the hidden and output layers is set to 300. All hyperparameters are selected based on the model’s performance on the development set. 4.1 Answer Sentence Selection We first study the effectiveness of our model for answer sentence selection tasks. Given a question, answer sentence selection aims to rank a list of candidate answer sentences based on their relatedness to the question. We experiment on the TRECQA dataset (Wang et al., 2007), in which all questions with only positive or negative answers are removed. This leaves us with 1,162 training questions, 65 development questions and 68 test questions. Experimental results are listed in Table 1. We measure performance by the mean average precision (MAP) and mean reciprocal rank (MRR) using the standard TREC evaluation script. In the first block of Table 1, we compare our model and variants thereof against several baselines. The first baseline is the Word-level Decomposable Attention model strengthened with a bidirectional LSTM for obtaining a contextualized representation f"
D18-1184,P17-1018,0,0.242835,"ultaneously discovering the tree structure of each sentence. Our model is fully differentiable and trained only on the matching objective. We evaluate this model on two tasks, entailment detection and answer sentence selection, and find that modeling latent tree structures results in superior performance. Analysis of the learned sentence structures shows they can reflect some syntactic phenomena. 1 Introduction There are many tasks in natural language processing that require matching two sentences: natural language inference (Bowman et al., 2015; Nangia et al., 2017) and paraphrase detection (Wang et al., 2017b) are classification tasks over sentence pairs, and question answering often requires an alignment between a question and a passage of text that may contain the answer (Tan et al., 2016a; Rajpurkar et al., 2016; Joshi et al., 2017). Most neural models for these tasks perform comparisons between the two sentences either at ∗ Work done during an internship at Allen Institute for Artificial Intelligence. the word level (Parikh et al., 2016), or at the sentence level (Bowman et al., 2015). Word-level comparisons ignore the inherent structure of the sentences being compared, at best relying on a r"
D18-1184,C16-1127,0,0.0591136,"Missing"
D18-1184,C16-1212,0,0.0745934,"comparisons ignore the inherent structure of the sentences being compared, at best relying on a recurrent neural network such as an LSTM (Hochreiter and Schmidhuber, 1997) to incorporate some amount of context from neighboring words into each word’s representation. Sentence-level comparisons can incorporate the structure of each sentence individually (Bowman et al., 2016; Tai et al., 2015), but cannot easily compare substructures between the sentences, as these are all squashed into a single vector. Some models do incorporate sentence structure by comparing subtrees between the two sentences (Zhao et al., 2016; Chen et al., 2017), but require pipelined approaches where a parser is run in a non-differentiable preprocessing step, losing the benefits of end-to-end training. In this paper we propose a method, which we call structured alignment networks, to perform comparisons between substructures in two sentences, in a more interpretable way, and without relying on an external, non-differentiable parser. We use a structured attention mechanism (Kim et al., 2017; Liu and Lapata, 2018) to compute a structured alignment between the two sentences, jointly learning a latent tree structure for each sentence"
D19-1073,P17-2090,0,0.0506904,"systems in the industry (Wu et al., 2016; Hassan et al., 2018). Despite the remarkable success, NMT suffers from the data scarcity problem. For most language pairs, large-scale, high-quality, and widecoverage bilingual corpora do not exist. Even for the top handful of resource-rich languages, the major sources of available parallel corpora are often restricted to government documents or news articles. Therefore, improving NMT under small-data training conditions has attracted extensive attention in recent years (Sennrich et al., 2016a; Cheng et al., 2016; Zoph et al., 2016; Chen et al., 2017; Fadaee et al., 2017; Ren et al., 2018; Lample et al., 2018). Among them, back-translation (Sennrich et al., 2016a) is an important direction. Its basic idea is to use an NMT model trained on limited authentic bilingual corpora to generate synthetic bilingual corpora using abundant monolingual data. The authentic and synthetic bilingual corpora are then combined to re-train NMT models. Due to its simplicity and effectiveness, backtranslation has been widely used in low-resource language translation. However, as the synthetic corpora generated by the NMT model are inevitably noisy, translation errors can be propag"
D19-1073,P11-1022,0,0.0271415,"neural machine translation and use uncertainty-based confidence measures to improve back-translation. The key idea is to use Monte Carlo Dropout to sample translation probabilities to calculate model uncertainty, without the need for manually labeled data. As our approach is transparent to model architectures, we plan to further verify the effectiveness of our approach on other downstream applications of NMT such as post-editing and interactive MT in the future. Confidence Estimation Estimating the confidence or quality of the output of MT systems (Ueffing and Ney, 2007; Specia et al., 2009; Bach et al., 2011; Salehi et al., 2014; Rikters and Fishel, 2017; Kepler et al., 2019) is important for enabling downstream applications such as post-editing and interactive MT to better cope with translation mistakes. While existing methods rely on external models to estimate confidence, our approach leverages model uncertainty to derive confidence measures. The major benefit is that our approach does not need labeled data. 5.3 Conclusions Acknowledgments We thank all anonymous reviewers for their valuable comments. This work is supported by the National Key R&D Program of China (No. 2017YFB0202204), National"
D19-1073,D18-1040,0,0.465139,"oisy, translation errors can be propagated to subsequent steps and prone to hinder the ∗ Yang Liu is the corresponding author: liuyang2011@ tsinghua.edu.cn. 1 The source code is available at https://github. com/THUNLP-MT/UCE4BT 791 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 791–802, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics The second step is to use the trained model θˆy→x to translate the monolingual corpus Dm : performance (Fadaee and Monz, 2018; Poncelas et al., 2018). In this work, we propose a method to quantify the confidence of NMT model predictions to enable back-translation to better cope with translation errors. The central idea is to use model uncertainty (Buntine and Weigend, 1991; Gal and Ghahramani, 2016; Dong et al., 2018; Xiao and Wang, 2019) to measure whether the model parameters can best describe the data distribution. Based on the expectation and variance of wordand sentence-level translation probabilities calculated by Monte Carlo Dropout (Gal and Ghahramani, 2016), we introduce various confidence measures. Differe"
D19-1073,C04-1046,0,0.177503,"ence of NMT model predictions to enable back-translation to better cope with translation errors. The central idea is to use model uncertainty (Buntine and Weigend, 1991; Gal and Ghahramani, 2016; Dong et al., 2018; Xiao and Wang, 2019) to measure whether the model parameters can best describe the data distribution. Based on the expectation and variance of wordand sentence-level translation probabilities calculated by Monte Carlo Dropout (Gal and Ghahramani, 2016), we introduce various confidence measures. Different from most previous quality estimation studies that require feature extraction (Blatz et al., 2004; Specia et al., 2009; Salehi et al., 2014) or post-edited data (Kim et al., 2017; Wang et al., 2018; Ive et al., 2018) to train external confidence estimators, all our approach needs is the NMT model itself. Hence, it is easy to apply our approach to arbitrary NMT models trained for arbitrary language pairs. Experiments on Chinese-English and English-German translation tasks show that our approach significantly improves the performance of back-translation. 2 n o ˆ (n) = argmax P (x|y(n) , θˆy→x ) , x (n) (n) x ˆi (5) θx→y This three-step process can iterate until convergence (Hoang et al., 20"
D19-1073,P17-1176,1,0.842201,"in large-scale MT systems in the industry (Wu et al., 2016; Hassan et al., 2018). Despite the remarkable success, NMT suffers from the data scarcity problem. For most language pairs, large-scale, high-quality, and widecoverage bilingual corpora do not exist. Even for the top handful of resource-rich languages, the major sources of available parallel corpora are often restricted to government documents or news articles. Therefore, improving NMT under small-data training conditions has attracted extensive attention in recent years (Sennrich et al., 2016a; Cheng et al., 2016; Zoph et al., 2016; Chen et al., 2017; Fadaee et al., 2017; Ren et al., 2018; Lample et al., 2018). Among them, back-translation (Sennrich et al., 2016a) is an important direction. Its basic idea is to use an NMT model trained on limited authentic bilingual corpora to generate synthetic bilingual corpora using abundant monolingual data. The authentic and synthetic bilingual corpora are then combined to re-train NMT models. Due to its simplicity and effectiveness, backtranslation has been widely used in low-resource language translation. However, as the synthetic corpora generated by the NMT model are inevitably noisy, translation"
D19-1073,W18-2703,0,0.0307731,"atz et al., 2004; Specia et al., 2009; Salehi et al., 2014) or post-edited data (Kim et al., 2017; Wang et al., 2018; Ive et al., 2018) to train external confidence estimators, all our approach needs is the NMT model itself. Hence, it is easy to apply our approach to arbitrary NMT models trained for arbitrary language pairs. Experiments on Chinese-English and English-German translation tasks show that our approach significantly improves the performance of back-translation. 2 n o ˆ (n) = argmax P (x|y(n) , θˆy→x ) , x (n) (n) x ˆi (5) θx→y This three-step process can iterate until convergence (Hoang et al., 2018; Cotterell and Kreutzer, 2018). A problem with back-translation is that model predictions are inevitably erroneous. Translation errors can be propagated to subsequent steps and impair the performance of back-translation, espe˜ b is much larger than Db (Pinnis cially when D et al., 2017; Fadaee and Monz, 2018; Poncelas et al., 2018). Therefore, it is crucial to develop principled solutions to enable back-translation to better deal with the error propagation problem. 3 Approach This work aims to find solutions to the two following problems: 1. How to quantify the confidence of model predictions"
D19-1073,P16-1185,1,0.846769,"et al., 2003) and been widely deployed in large-scale MT systems in the industry (Wu et al., 2016; Hassan et al., 2018). Despite the remarkable success, NMT suffers from the data scarcity problem. For most language pairs, large-scale, high-quality, and widecoverage bilingual corpora do not exist. Even for the top handful of resource-rich languages, the major sources of available parallel corpora are often restricted to government documents or news articles. Therefore, improving NMT under small-data training conditions has attracted extensive attention in recent years (Sennrich et al., 2016a; Cheng et al., 2016; Zoph et al., 2016; Chen et al., 2017; Fadaee et al., 2017; Ren et al., 2018; Lample et al., 2018). Among them, back-translation (Sennrich et al., 2016a) is an important direction. Its basic idea is to use an NMT model trained on limited authentic bilingual corpora to generate synthetic bilingual corpora using abundant monolingual data. The authentic and synthetic bilingual corpora are then combined to re-train NMT models. Due to its simplicity and effectiveness, backtranslation has been widely used in low-resource language translation. However, as the synthetic corpora generated by the NMT m"
D19-1073,W18-2707,0,0.069184,"IST06 show that targeting difficult words improves over randomly selecting monolingual data (46.23 → 46.60 BLEU), confirming the finding of Fadaee and Monz (2018). Using uncertainty-based confidence can further imBack-translation Back-translation is a simple and effective approach to leveraging monolingual data for NMT (Sennrich et al., 2016a). There has been a growing body of literature that analyzes and extends back-translation recently. Currey et al. (2017) show that low-resource NMT can benefit from the synthetic data generated by simply copying target monolingual data to the source side. Imamura et al. (2018) and Edunov et al. (2018) demonstrate that it is more effective to generate source sentences via sampling rather than beam search. Cotterell and Kreutzer (2018) and Hoang et al. 798 target Man gewährt dem Sterbenden je nach Wunsch eine Mundpflege mit Brandy oder Pepsi . reference A person who is dying will accept being helped to drink brandy or Pepsi , whatever is their tipple . prediction The dying person is given oral care with brandy or Pepsi as desired . PTP EXP VAR CEV Figure 5: Example of confidence measures. terested in calculating uncertainty after the model has made the prediction rat"
D19-1073,W17-4715,0,0.0200581,"eriments, we used the same amount of monolingual data that was derived from a larger monolingual corpus using different data selection methods. Results on NIST06 show that targeting difficult words improves over randomly selecting monolingual data (46.23 → 46.60 BLEU), confirming the finding of Fadaee and Monz (2018). Using uncertainty-based confidence can further imBack-translation Back-translation is a simple and effective approach to leveraging monolingual data for NMT (Sennrich et al., 2016a). There has been a growing body of literature that analyzes and extends back-translation recently. Currey et al. (2017) show that low-resource NMT can benefit from the synthetic data generated by simply copying target monolingual data to the source side. Imamura et al. (2018) and Edunov et al. (2018) demonstrate that it is more effective to generate source sentences via sampling rather than beam search. Cotterell and Kreutzer (2018) and Hoang et al. 798 target Man gewährt dem Sterbenden je nach Wunsch eine Mundpflege mit Brandy oder Pepsi . reference A person who is dying will accept being helped to drink brandy or Pepsi , whatever is their tipple . prediction The dying person is given oral care with brandy or"
D19-1073,C18-1266,0,0.027334,"use model uncertainty (Buntine and Weigend, 1991; Gal and Ghahramani, 2016; Dong et al., 2018; Xiao and Wang, 2019) to measure whether the model parameters can best describe the data distribution. Based on the expectation and variance of wordand sentence-level translation probabilities calculated by Monte Carlo Dropout (Gal and Ghahramani, 2016), we introduce various confidence measures. Different from most previous quality estimation studies that require feature extraction (Blatz et al., 2004; Specia et al., 2009; Salehi et al., 2014) or post-edited data (Kim et al., 2017; Wang et al., 2018; Ive et al., 2018) to train external confidence estimators, all our approach needs is the NMT model itself. Hence, it is easy to apply our approach to arbitrary NMT models trained for arbitrary language pairs. Experiments on Chinese-English and English-German translation tasks show that our approach significantly improves the performance of back-translation. 2 n o ˆ (n) = argmax P (x|y(n) , θˆy→x ) , x (n) (n) x ˆi (5) θx→y This three-step process can iterate until convergence (Hoang et al., 2018; Cotterell and Kreutzer, 2018). A problem with back-translation is that model predictions are inevitably erroneous."
D19-1073,P18-1069,0,0.406084,"Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 791–802, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics The second step is to use the trained model θˆy→x to translate the monolingual corpus Dm : performance (Fadaee and Monz, 2018; Poncelas et al., 2018). In this work, we propose a method to quantify the confidence of NMT model predictions to enable back-translation to better cope with translation errors. The central idea is to use model uncertainty (Buntine and Weigend, 1991; Gal and Ghahramani, 2016; Dong et al., 2018; Xiao and Wang, 2019) to measure whether the model parameters can best describe the data distribution. Based on the expectation and variance of wordand sentence-level translation probabilities calculated by Monte Carlo Dropout (Gal and Ghahramani, 2016), we introduce various confidence measures. Different from most previous quality estimation studies that require feature extraction (Blatz et al., 2004; Specia et al., 2009; Salehi et al., 2014) or post-edited data (Kim et al., 2017; Wang et al., 2018; Ive et al., 2018) to train external confidence estimators, all our approach needs is the NMT"
D19-1073,D18-1045,0,0.545914,"007). We used byte pair encoding (Sennrich et al., 2016b) to perform subword segmentation with 32k merge operations for Chinese-English and 35k merge operations for English-German. Sentence pairs are batched together by approximate length and each batch has roughly 25,000 source and target tokens. We distinguish between three kinds of translations of the monolingual corpus: 1. N ONE: there is no translation and only the authentic bilingual corpus is used; 2. S EARCH: the translations are generated by beam search (Sennrich et al., 2016a); 3. S AMPLE: the translations are generated by sampling (Edunov et al., 2018). As neural quality estimation (Kim et al., 2017; Wang et al., 2018) can also give word- and sentence-level confidences for the output of NMT models when labeled data is available, we distinguish between two kinds of confidence estimation methods: 1. N EURAL Q E: the confidences are given by an external neural quality estimator; 4.2 2. U NCERTAINTY: the proposed uncertaintybased confidence estimation method. Comparison of Confidence Measures Table 1 shows the comparison of confidence measures on the Chinese-English development set. We find that using either the translation probabilities output"
D19-1073,2001.mtsummit-papers.68,0,0.0301344,"ord-level Confidence As the source side instead of the target side of the synthetic bilingual corpus is noisy, wordlevel confidence cannot be integrated into backtranslation in a similar way to sentence-level confidence. This is because the word-level confidence associated with each source word does not get involved in backpropagation during training. Alternatively, we build a real-valued word-level confidence vector: n oI (n) ˆ &lt;i , x c = C y(n) , x ˆi , θˆy→x . (14) 4 4.1 Setup We evaluated our approach on Chinese-English and English-German translation tasks. The evaluation metric is BLEU (Papineni et al., 2001) as calculated by the multi-bleu.perl script. We use the paired bootstrap resampling (Koehn, 2004) for significance testing. For the Chinese-English task, the training set contains 1.25M sentence pairs from LDC5 with 27.8M Chinese words and 34.5M English words. To build the monolingual corpus for backtranslation, we extracted the English side of the training set of the WMT 2017 Chinese-English news translation task. After removing sentences longer than 256 words, we randomly selected 10M English sentences as the monolingual corpus. NIST06 is used as the development set and NIST02, 03, 04, 05,"
D19-1073,P19-3020,0,0.0433318,"Missing"
D19-1073,W17-4763,0,0.124017,"ation errors. The central idea is to use model uncertainty (Buntine and Weigend, 1991; Gal and Ghahramani, 2016; Dong et al., 2018; Xiao and Wang, 2019) to measure whether the model parameters can best describe the data distribution. Based on the expectation and variance of wordand sentence-level translation probabilities calculated by Monte Carlo Dropout (Gal and Ghahramani, 2016), we introduce various confidence measures. Different from most previous quality estimation studies that require feature extraction (Blatz et al., 2004; Specia et al., 2009; Salehi et al., 2014) or post-edited data (Kim et al., 2017; Wang et al., 2018; Ive et al., 2018) to train external confidence estimators, all our approach needs is the NMT model itself. Hence, it is easy to apply our approach to arbitrary NMT models trained for arbitrary language pairs. Experiments on Chinese-English and English-German translation tasks show that our approach significantly improves the performance of back-translation. 2 n o ˆ (n) = argmax P (x|y(n) , θˆy→x ) , x (n) (n) x ˆi (5) θx→y This three-step process can iterate until convergence (Hoang et al., 2018; Cotterell and Kreutzer, 2018). A problem with back-translation is that model"
D19-1073,W04-3250,0,0.452125,"y, wordlevel confidence cannot be integrated into backtranslation in a similar way to sentence-level confidence. This is because the word-level confidence associated with each source word does not get involved in backpropagation during training. Alternatively, we build a real-valued word-level confidence vector: n oI (n) ˆ &lt;i , x c = C y(n) , x ˆi , θˆy→x . (14) 4 4.1 Setup We evaluated our approach on Chinese-English and English-German translation tasks. The evaluation metric is BLEU (Papineni et al., 2001) as calculated by the multi-bleu.perl script. We use the paired bootstrap resampling (Koehn, 2004) for significance testing. For the Chinese-English task, the training set contains 1.25M sentence pairs from LDC5 with 27.8M Chinese words and 34.5M English words. To build the monolingual corpus for backtranslation, we extracted the English side of the training set of the WMT 2017 Chinese-English news translation task. After removing sentences longer than 256 words, we randomly selected 10M English sentences as the monolingual corpus. NIST06 is used as the development set and NIST02, 03, 04, 05, and 08 datasets as test sets. For the English-German task, we used the dataset of the WMT 2014 Eng"
D19-1073,P18-1006,0,0.0164722,"ry (Wu et al., 2016; Hassan et al., 2018). Despite the remarkable success, NMT suffers from the data scarcity problem. For most language pairs, large-scale, high-quality, and widecoverage bilingual corpora do not exist. Even for the top handful of resource-rich languages, the major sources of available parallel corpora are often restricted to government documents or news articles. Therefore, improving NMT under small-data training conditions has attracted extensive attention in recent years (Sennrich et al., 2016a; Cheng et al., 2016; Zoph et al., 2016; Chen et al., 2017; Fadaee et al., 2017; Ren et al., 2018; Lample et al., 2018). Among them, back-translation (Sennrich et al., 2016a) is an important direction. Its basic idea is to use an NMT model trained on limited authentic bilingual corpora to generate synthetic bilingual corpora using abundant monolingual data. The authentic and synthetic bilingual corpora are then combined to re-train NMT models. Due to its simplicity and effectiveness, backtranslation has been widely used in low-resource language translation. However, as the synthetic corpora generated by the NMT model are inevitably noisy, translation errors can be propagated to subsequent"
D19-1073,P07-2045,0,0.0102164,"moothing was set as ls = 0.1 (Szegedy et al., 2016; Pereyra et al., 2017). During training and the Monte Carlo Dropout process, the hyper-parameter of dropout was set to 0.1 and 0.3 for Transformer base and big models, respectively. K was set to 20. Through experiments, we find our method works best when the α and β are set to 2. All experiments were conducted on 8 NVIDIA GTX 1080Ti GPUs. the development set and newstest 2012, 2014, and 2015 as test sets. Chinese sentences were segmented by an opensource toolkit THULAC6 . German and English sentences were tokenized by the tokenizer in Moses (Koehn et al., 2007). We used byte pair encoding (Sennrich et al., 2016b) to perform subword segmentation with 32k merge operations for Chinese-English and 35k merge operations for English-German. Sentence pairs are batched together by approximate length and each batch has roughly 25,000 source and target tokens. We distinguish between three kinds of translations of the monolingual corpus: 1. N ONE: there is no translation and only the authentic bilingual corpus is used; 2. S EARCH: the translations are generated by beam search (Sennrich et al., 2016a); 3. S AMPLE: the translations are generated by sampling (Edun"
D19-1073,N03-1017,0,0.0872106,"Missing"
D19-1073,D18-1549,0,0.052654,"Missing"
D19-1073,P16-1009,0,0.393142,"translation (SMT) (Koehn et al., 2003) and been widely deployed in large-scale MT systems in the industry (Wu et al., 2016; Hassan et al., 2018). Despite the remarkable success, NMT suffers from the data scarcity problem. For most language pairs, large-scale, high-quality, and widecoverage bilingual corpora do not exist. Even for the top handful of resource-rich languages, the major sources of available parallel corpora are often restricted to government documents or news articles. Therefore, improving NMT under small-data training conditions has attracted extensive attention in recent years (Sennrich et al., 2016a; Cheng et al., 2016; Zoph et al., 2016; Chen et al., 2017; Fadaee et al., 2017; Ren et al., 2018; Lample et al., 2018). Among them, back-translation (Sennrich et al., 2016a) is an important direction. Its basic idea is to use an NMT model trained on limited authentic bilingual corpora to generate synthetic bilingual corpora using abundant monolingual data. The authentic and synthetic bilingual corpora are then combined to re-train NMT models. Due to its simplicity and effectiveness, backtranslation has been widely used in low-resource language translation. However, as the synthetic corpora g"
D19-1073,P16-1162,0,0.555111,"translation (SMT) (Koehn et al., 2003) and been widely deployed in large-scale MT systems in the industry (Wu et al., 2016; Hassan et al., 2018). Despite the remarkable success, NMT suffers from the data scarcity problem. For most language pairs, large-scale, high-quality, and widecoverage bilingual corpora do not exist. Even for the top handful of resource-rich languages, the major sources of available parallel corpora are often restricted to government documents or news articles. Therefore, improving NMT under small-data training conditions has attracted extensive attention in recent years (Sennrich et al., 2016a; Cheng et al., 2016; Zoph et al., 2016; Chen et al., 2017; Fadaee et al., 2017; Ren et al., 2018; Lample et al., 2018). Among them, back-translation (Sennrich et al., 2016a) is an important direction. Its basic idea is to use an NMT model trained on limited authentic bilingual corpora to generate synthetic bilingual corpora using abundant monolingual data. The authentic and synthetic bilingual corpora are then combined to re-train NMT models. Due to its simplicity and effectiveness, backtranslation has been widely used in low-resource language translation. However, as the synthetic corpora g"
D19-1073,2009.mtsummit-papers.16,0,0.202609,"edictions to enable back-translation to better cope with translation errors. The central idea is to use model uncertainty (Buntine and Weigend, 1991; Gal and Ghahramani, 2016; Dong et al., 2018; Xiao and Wang, 2019) to measure whether the model parameters can best describe the data distribution. Based on the expectation and variance of wordand sentence-level translation probabilities calculated by Monte Carlo Dropout (Gal and Ghahramani, 2016), we introduce various confidence measures. Different from most previous quality estimation studies that require feature extraction (Blatz et al., 2004; Specia et al., 2009; Salehi et al., 2014) or post-edited data (Kim et al., 2017; Wang et al., 2018; Ive et al., 2018) to train external confidence estimators, all our approach needs is the NMT model itself. Hence, it is easy to apply our approach to arbitrary NMT models trained for arbitrary language pairs. Experiments on Chinese-English and English-German translation tasks show that our approach significantly improves the performance of back-translation. 2 n o ˆ (n) = argmax P (x|y(n) , θˆy→x ) , x (n) (n) x ˆi (5) θx→y This three-step process can iterate until convergence (Hoang et al., 2018; Cotterell and Kre"
D19-1073,J07-1003,0,0.0432043,"method for qualifying model uncertainty for neural machine translation and use uncertainty-based confidence measures to improve back-translation. The key idea is to use Monte Carlo Dropout to sample translation probabilities to calculate model uncertainty, without the need for manually labeled data. As our approach is transparent to model architectures, we plan to further verify the effectiveness of our approach on other downstream applications of NMT such as post-editing and interactive MT in the future. Confidence Estimation Estimating the confidence or quality of the output of MT systems (Ueffing and Ney, 2007; Specia et al., 2009; Bach et al., 2011; Salehi et al., 2014; Rikters and Fishel, 2017; Kepler et al., 2019) is important for enabling downstream applications such as post-editing and interactive MT to better cope with translation mistakes. While existing methods rely on external models to estimate confidence, our approach leverages model uncertainty to derive confidence measures. The major benefit is that our approach does not need labeled data. 5.3 Conclusions Acknowledgments We thank all anonymous reviewers for their valuable comments. This work is supported by the National Key R&D Program"
D19-1073,W18-6465,0,0.172598,"central idea is to use model uncertainty (Buntine and Weigend, 1991; Gal and Ghahramani, 2016; Dong et al., 2018; Xiao and Wang, 2019) to measure whether the model parameters can best describe the data distribution. Based on the expectation and variance of wordand sentence-level translation probabilities calculated by Monte Carlo Dropout (Gal and Ghahramani, 2016), we introduce various confidence measures. Different from most previous quality estimation studies that require feature extraction (Blatz et al., 2004; Specia et al., 2009; Salehi et al., 2014) or post-edited data (Kim et al., 2017; Wang et al., 2018; Ive et al., 2018) to train external confidence estimators, all our approach needs is the NMT model itself. Hence, it is easy to apply our approach to arbitrary NMT models trained for arbitrary language pairs. Experiments on Chinese-English and English-German translation tasks show that our approach significantly improves the performance of back-translation. 2 n o ˆ (n) = argmax P (x|y(n) , θˆy→x ) , x (n) (n) x ˆi (5) θx→y This three-step process can iterate until convergence (Hoang et al., 2018; Cotterell and Kreutzer, 2018). A problem with back-translation is that model predictions are ine"
D19-1073,P17-4012,0,0.0769606,"e software officially recommended by the QE shared task of WMT. Following the guide of OpenKiwi, we used a GermanEnglish parallel corpus containing 2.09M sentence pairs to train the predictor and a post-edited corpus containing 25k sentence triples to train the estimator. All the data used to train QE models are provided by WMT. As there are no post-edited corpora for the Chinese-English task, N EURAL Q E can only be used in the English-German task in our experiments. For N EURAL Q E, both word- and sentence- level quality scores were considered. We implemented our method on the top of THUMT (Zhang et al., 2017). The NMT model we use is Transformer (Vaswani et al., 2017). We used the base model for the Chinese-English task and the big model for the English-German task. We used the Adam optimizer (Kingma and Ba, 2015) with β1 = 0.9, β2 = 0.98 and  = 10−9 to optimize model parameters. We used the same warmup strategy for learning rate as Vaswani et al. (2017) with warmup steps = 4, 000. During training, the hyper-parameter of label smoothing was set as ls = 0.1 (Szegedy et al., 2016; Pereyra et al., 2017). During training and the Monte Carlo Dropout process, the hyper-parameter of dropout was set to"
D19-1073,D16-1163,0,0.0432688,"een widely deployed in large-scale MT systems in the industry (Wu et al., 2016; Hassan et al., 2018). Despite the remarkable success, NMT suffers from the data scarcity problem. For most language pairs, large-scale, high-quality, and widecoverage bilingual corpora do not exist. Even for the top handful of resource-rich languages, the major sources of available parallel corpora are often restricted to government documents or news articles. Therefore, improving NMT under small-data training conditions has attracted extensive attention in recent years (Sennrich et al., 2016a; Cheng et al., 2016; Zoph et al., 2016; Chen et al., 2017; Fadaee et al., 2017; Ren et al., 2018; Lample et al., 2018). Among them, back-translation (Sennrich et al., 2016a) is an important direction. Its basic idea is to use an NMT model trained on limited authentic bilingual corpora to generate synthetic bilingual corpora using abundant monolingual data. The authentic and synthetic bilingual corpora are then combined to re-train NMT models. Due to its simplicity and effectiveness, backtranslation has been widely used in low-resource language translation. However, as the synthetic corpora generated by the NMT model are inevitably"
D19-1078,P17-2061,0,0.306,"s research and applications. To deal with this issue, many researchers have conducted studies on the domain adaptation for NMT, which can be classified into two general categories. One is to transfer the rich-resource domain (out-of-domain) translation knowledge to benefit the low-resource (in-domain) NMT model. The other is to use the mixed-domain training corpus to construct a unified NMT model for all domains. Here, we mainly focus on the first type of research, of which typical methods include finetuning (Luong and Manning, 2015; Zoph et al., 2016; Servan et al., 2016), mixed fine-tuning (Chu et al., 2017), cost weighting (Chen et al., 2017), data selection (Wang et al., 2017a,b; Zhang et al., 2019a) and so on. The underlying assumption of these approaches is that in-domain and out-ofdomain NMT models share the same parameter space or prior distributions, and the useful out-ofdomain translation knowledge can be completely transferred to in-domain NMT model in a onepass manner. However, it is difficult to achieve this goal due to domain differences. Particularly, when the domain difference is significant, such conventional brute-force transfer may be unsuccessful, facing the similar issue as the"
D19-1078,C18-1111,0,0.0238287,"anwhile, applying data weighting into NMT domain adaptation has attracted much attention. Wang et al. (2017a) and Wang et al. (2017b) proposed several sentence and domain weighting methods with a dynamic weight learning strategy. Zhang et al. (2019a) ranked unlabeled domain training samples based on their similarity to in-domain data, and then adopts a probabilistic curriculum learning strategy during training. Chen et al. (2017) applied the sentence-level cost weighting to refine the training of NMT model. Recently, Vilar (2018) introduced a weight to each hidden unit of out-of-domain model. Chu and Wang (2018) gave a comprehensive survey of the dominant domain adaptation techniques for NMT. Gu et al. (2019) not only maintained a private encoder and a private decoder for each domain, but also introduced a common encoder and a common decoder shared by all domains. Significantly different from the above methods, along with the studies of dual learning for NMT (He et al., 2016; Wang et al., 2018; Zhang et al., 2019b), we iteratively perform bidirectional translation knowledge transfer between in-domain and out-of-domain training corpora. To the best of our knowledge, our work is the first attempt to ex"
D19-1078,Q19-1002,1,0.797814,"28.78 — 31.86 27.49 29.67 — 31.33 27.96 29.64 — Multi-domain NMT Methods 31.13 28.02 29.57 21.61 31.57 27.60 29.58 21.75 31.87 27.82 29.84 21.86 IDDA Framework 32.11 28.10 30.11 22.01 32.93 28.88 30.91† 22.17∗ Out-of-domain2 Medical EMEA 51.11 50.60 — — — 52.25 52.60 52.84 52.07 53.39† Table 4: Experimental results of the English-German translation task. * indicates statistically significantly better than (ρ &lt;0.05) the result of WDCD. Model IDDA-mix IDDA IDDA Transfer Order —— {θoutnews , θoutmedical } {θoutmedical , θoutnews } AVE . 30.17 30.51 30.91 (Bahdanau et al., 2015; Su et al., 2018; Song et al., 2019), so as to verify the generality of our framework. Table 5: Experimental results of IDDA using different configurations. Acknowledgments The authors were supported by National Natural Science Foundation of China (No. 61672440), Beijing Advanced Innovation Center for Language Resources, NSF Award (No. 1704337), the Fundamental Research Funds for the Central Universities (Grant No. ZK1024), and Scientific Research Project of National Language Committee of China (Grant No. YB135-49). We also thank the reviewers for their insightful comments tic distance between each out-of-domain and indomain tra"
D19-1078,W17-4713,0,0.0342627,",θ v ∗ if EvalModel(Dout out ) &gt; EvalModel(Dout , θout ) (k) ∗ ←θ θout out end if (k) (k) ∗ ) θin ← TransferModel(θout , Din , θin v , θ (k) ) &gt; EvalModel(D v , θ ∗ ) if EvalModel(Din in in in ∗ ← θ (k) θin in end if end for lation under our framework, we iteratively update teacher models for better domain adaptation. In contrast, all language-specific teacher NMT models in (Tan et al., 2019) remain fixed. of multi-domain NMT, which focus on building a unified NMT model trained on the mixed-domain training corpus for translation tasks in all domains (Kobus et al., 2016; Tars and Fishel, 2018; Farajian et al., 2017; Pryzant et al., 2017; Sajjad et al., 2017; Zeng et al., 2018; Bapna and Firat, 2019). Although our framework is also able to refine outof-domain NMT model, it is still significantly different from multi-domain NMT, since only the performance of in-domain NMT model is considered. 3 Iterative Dual Domain Adaptation Framework In this section, we first detailedly describe our proposed framework for conventional one-to-one NMT domain adaptation, and then extend this framework to the scenario of multiple out-ofdomain corpora (many-to-one). Finally, note that similar to our work, Tan et al. (2019)"
D19-1078,N19-1312,0,0.0550436,"(2017a) and Wang et al. (2017b) proposed several sentence and domain weighting methods with a dynamic weight learning strategy. Zhang et al. (2019a) ranked unlabeled domain training samples based on their similarity to in-domain data, and then adopts a probabilistic curriculum learning strategy during training. Chen et al. (2017) applied the sentence-level cost weighting to refine the training of NMT model. Recently, Vilar (2018) introduced a weight to each hidden unit of out-of-domain model. Chu and Wang (2018) gave a comprehensive survey of the dominant domain adaptation techniques for NMT. Gu et al. (2019) not only maintained a private encoder and a private decoder for each domain, but also introduced a common encoder and a common decoder shared by all domains. Significantly different from the above methods, along with the studies of dual learning for NMT (He et al., 2016; Wang et al., 2018; Zhang et al., 2019b), we iteratively perform bidirectional translation knowledge transfer between in-domain and out-of-domain training corpora. To the best of our knowledge, our work is the first attempt to explore such a dual learning based framework for NMT domain adaptation. Furthermore, we extend our fr"
D19-1078,N18-2080,0,0.0238204,"2016) combined the fine-tuned model with the baseline via ensemble method. Meanwhile, applying data weighting into NMT domain adaptation has attracted much attention. Wang et al. (2017a) and Wang et al. (2017b) proposed several sentence and domain weighting methods with a dynamic weight learning strategy. Zhang et al. (2019a) ranked unlabeled domain training samples based on their similarity to in-domain data, and then adopts a probabilistic curriculum learning strategy during training. Chen et al. (2017) applied the sentence-level cost weighting to refine the training of NMT model. Recently, Vilar (2018) introduced a weight to each hidden unit of out-of-domain model. Chu and Wang (2018) gave a comprehensive survey of the dominant domain adaptation techniques for NMT. Gu et al. (2019) not only maintained a private encoder and a private decoder for each domain, but also introduced a common encoder and a common decoder shared by all domains. Significantly different from the above methods, along with the studies of dual learning for NMT (He et al., 2016; Wang et al., 2018; Zhang et al., 2019b), we iteratively perform bidirectional translation knowledge transfer between in-domain and out-of-domain"
D19-1078,P17-2089,0,0.0374011,"Missing"
D19-1078,D16-1139,0,0.441352,"pirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 845–855, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics 2 spectively, and then iteratively perform bidirectional translation knowledge transfer (from indomain to out-of-domain and then vice versa). In this way, both in-domain and out-of-domain NMT models are expected to constantly reinforce each other, which is likely to achieve better NMT domain adaptation. Particularly, we employ a knowledge distillation (Hinton et al., 2015; Kim and Rush, 2016) based approach to transfer translation knowledge. During this process, the targetdomain NMT model is first initialized with the source-domain NMT model, and then trained to fit its own training data and match the output of its previous best model simultaneously. By doing so, the previously transferred translation knowledge can be effectively retained for better NMT domain adaptation. Finally, we further extend the proposed framework to the scenario of multiple out-of-domain training corpora, where the abovementioned bidirectional knowledge transfer is performed sequentially between the in-dom"
D19-1078,D17-1155,0,0.0787754,"Missing"
D19-1078,2015.iwslt-evaluation.11,0,0.278219,"Missing"
D19-1078,P02-1040,0,0.105168,"as in-domain test sets, WMT news-test2014 (News topic) and 1K sampled sentence pairs of OPUS EMEA corpus were used as two out-ofdomain test sets. We first employed Stanford Segmenter3 to conduct word segmentation on Chinese sentences and MOSES script4 to tokenize English and German sentences. Then, we limited the length of sentences to 50 words in the training stage. Besides, we employed Byte Pair Encoding (Sennrich et al., 2016) to split words into subwords and set the vocabulary size for both ChineseEnglish and English-German as 32,000. We evaluated the translation quality with BLEU scores (Papineni et al., 2002) as calculated by multi-bleu.perl script . Settings. We chose Transformer (Vaswani et al., 2017) as our NMT model, which exhibits excellent performance due to its flexibility in parallel computation and long-range dependency modeling. We followed Vaswani et al. (2017) to set the configurations. The dimensionality of all input and output layers is 512, and that of FFN layer is 2048. We employed 8 parallel attention heads in both encoder and decoder. Parameter optimization was performed using stochastic gradient descent, where Adam (Kingma and Ba, 2015) was used to automatically adjust the learn"
D19-1078,W17-4712,0,0.476982,"t out ) &gt; EvalModel(Dout , θout ) (k) ∗ ←θ θout out end if (k) (k) ∗ ) θin ← TransferModel(θout , Din , θin v , θ (k) ) &gt; EvalModel(D v , θ ∗ ) if EvalModel(Din in in in ∗ ← θ (k) θin in end if end for lation under our framework, we iteratively update teacher models for better domain adaptation. In contrast, all language-specific teacher NMT models in (Tan et al., 2019) remain fixed. of multi-domain NMT, which focus on building a unified NMT model trained on the mixed-domain training corpus for translation tasks in all domains (Kobus et al., 2016; Tars and Fishel, 2018; Farajian et al., 2017; Pryzant et al., 2017; Sajjad et al., 2017; Zeng et al., 2018; Bapna and Firat, 2019). Although our framework is also able to refine outof-domain NMT model, it is still significantly different from multi-domain NMT, since only the performance of in-domain NMT model is considered. 3 Iterative Dual Domain Adaptation Framework In this section, we first detailedly describe our proposed framework for conventional one-to-one NMT domain adaptation, and then extend this framework to the scenario of multiple out-ofdomain corpora (many-to-one). Finally, note that similar to our work, Tan et al. (2019) introduced knowledge d"
D19-1078,E17-2045,0,0.0359692,"ut , θout ) (k) ∗ ←θ θout out end if (k) (k) ∗ ) θin ← TransferModel(θout , Din , θin v , θ (k) ) &gt; EvalModel(D v , θ ∗ ) if EvalModel(Din in in in ∗ ← θ (k) θin in end if end for lation under our framework, we iteratively update teacher models for better domain adaptation. In contrast, all language-specific teacher NMT models in (Tan et al., 2019) remain fixed. of multi-domain NMT, which focus on building a unified NMT model trained on the mixed-domain training corpus for translation tasks in all domains (Kobus et al., 2016; Tars and Fishel, 2018; Farajian et al., 2017; Pryzant et al., 2017; Sajjad et al., 2017; Zeng et al., 2018; Bapna and Firat, 2019). Although our framework is also able to refine outof-domain NMT model, it is still significantly different from multi-domain NMT, since only the performance of in-domain NMT model is considered. 3 Iterative Dual Domain Adaptation Framework In this section, we first detailedly describe our proposed framework for conventional one-to-one NMT domain adaptation, and then extend this framework to the scenario of multiple out-ofdomain corpora (many-to-one). Finally, note that similar to our work, Tan et al. (2019) introduced knowledge distillation into mult"
D19-1078,P16-1162,0,0.0699703,"S EMEA corpus2 . As for development sets, we chose IWSLT tst2012, WMT tst2012 and 1K sampled sentence pairs of OPUS EMEA corpus, respectively. In addition, IWSLT tst2013, tst2014 were used as in-domain test sets, WMT news-test2014 (News topic) and 1K sampled sentence pairs of OPUS EMEA corpus were used as two out-ofdomain test sets. We first employed Stanford Segmenter3 to conduct word segmentation on Chinese sentences and MOSES script4 to tokenize English and German sentences. Then, we limited the length of sentences to 50 words in the training stage. Besides, we employed Byte Pair Encoding (Sennrich et al., 2016) to split words into subwords and set the vocabulary size for both ChineseEnglish and English-German as 32,000. We evaluated the translation quality with BLEU scores (Papineni et al., 2002) as calculated by multi-bleu.perl script . Settings. We chose Transformer (Vaswani et al., 2017) as our NMT model, which exhibits excellent performance due to its flexibility in parallel computation and long-range dependency modeling. We followed Vaswani et al. (2017) to set the configurations. The dimensionality of all input and output layers is 512, and that of FFN layer is 2048. We employed 8 parallel att"
D19-1078,D18-1041,1,0.823069,"combined the fine-tuned model with the baseline via ensemble method. Meanwhile, applying data weighting into NMT domain adaptation has attracted much attention. Wang et al. (2017a) and Wang et al. (2017b) proposed several sentence and domain weighting methods with a dynamic weight learning strategy. Zhang et al. (2019a) ranked unlabeled domain training samples based on their similarity to in-domain data, and then adopts a probabilistic curriculum learning strategy during training. Chen et al. (2017) applied the sentence-level cost weighting to refine the training of NMT model. Recently, Vilar (2018) introduced a weight to each hidden unit of out-of-domain model. Chu and Wang (2018) gave a comprehensive survey of the dominant domain adaptation techniques for NMT. Gu et al. (2019) not only maintained a private encoder and a private decoder for each domain, but also introduced a common encoder and a common decoder shared by all domains. Significantly different from the above methods, along with the studies of dual learning for NMT (He et al., 2016; Wang et al., 2018; Zhang et al., 2019b), we iteratively perform bidirectional translation knowledge transfer between in-domain and out-of-domain"
D19-1078,N19-1189,0,0.103432,"d Wang et al. (2017b) proposed several sentence and domain weighting methods with a dynamic weight learning strategy. Zhang et al. (2019a) ranked unlabeled domain training samples based on their similarity to in-domain data, and then adopts a probabilistic curriculum learning strategy during training. Chen et al. (2017) applied the sentence-level cost weighting to refine the training of NMT model. Recently, Vilar (2018) introduced a weight to each hidden unit of out-of-domain model. Chu and Wang (2018) gave a comprehensive survey of the dominant domain adaptation techniques for NMT. Gu et al. (2019) not only maintained a private encoder and a private decoder for each domain, but also introduced a common encoder and a common decoder shared by all domains. Significantly different from the above methods, along with the studies of dual learning for NMT (He et al., 2016; Wang et al., 2018; Zhang et al., 2019b), we iteratively perform bidirectional translation knowledge transfer between in-domain and out-of-domain training corpora. To the best of our knowledge, our work is the first attempt to explore such a dual learning based framework for NMT domain adaptation. Furthermore, we extend our fr"
D19-1078,D16-1163,0,0.15549,"fore, NMT for low-resource domains becomes a challenge in its research and applications. To deal with this issue, many researchers have conducted studies on the domain adaptation for NMT, which can be classified into two general categories. One is to transfer the rich-resource domain (out-of-domain) translation knowledge to benefit the low-resource (in-domain) NMT model. The other is to use the mixed-domain training corpus to construct a unified NMT model for all domains. Here, we mainly focus on the first type of research, of which typical methods include finetuning (Luong and Manning, 2015; Zoph et al., 2016; Servan et al., 2016), mixed fine-tuning (Chu et al., 2017), cost weighting (Chen et al., 2017), data selection (Wang et al., 2017a,b; Zhang et al., 2019a) and so on. The underlying assumption of these approaches is that in-domain and out-ofdomain NMT models share the same parameter space or prior distributions, and the useful out-ofdomain translation knowledge can be completely transferred to in-domain NMT model in a onepass manner. However, it is difficult to achieve this goal due to domain differences. Particularly, when the domain difference is significant, such conventional brute-force t"
D19-1387,D18-1443,0,0.730429,"cal Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 3730–3740, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics coder (Vaswani et al., 2017). We design a new training schedule which separates the optimizers of the encoder and the decoder in order to accommodate the fact that the former is pretrained while the latter must be trained from scratch. Finally, motivated by previous work showing that the combination of extractive and abstractive objectives can help generate better summaries (Gehrmann et al., 2018), we present a two-stage approach where the encoder is fine-tuned twice, first with an extractive objective and subsequently on the abstractive summarization task. We evaluate the proposed approach on three single-document news summarization datasets representative of different writing conventions (e.g., important information is concentrated at the beginning of the document or distributed more evenly throughout) and summary styles (e.g., verbose vs. more telegraphic; extractive vs. abstractive). Across datasets, we experimentally show that the proposed models achieve state-of-the-art results u"
D19-1387,P16-1154,0,0.0327232,"riting conventions (e.g., important information is concentrated at the beginning of the document or distributed more evenly throughout) and summary styles (e.g., verbose vs. more telegraphic; extractive vs. abstractive). Across datasets, we experimentally show that the proposed models achieve state-of-the-art results under both extractive and abstractive settings. Our contributions in this work are threefold: a) we highlight the importance of document encoding for the summarization task; a variety of recently proposed techniques aim to enhance summarization performance via copying mechanisms (Gu et al., 2016; See et al., 2017; Nallapati et al., 2017), reinforcement learning (Narayan et al., 2018b; Paulus et al., 2018; Dong et al., 2018), and multiple communicating encoders (Celikyilmaz et al., 2018). We achieve better results with a minimum-requirement model without using any of these mechanisms; b) we showcase ways to effectively employ pretrained language models in summarization under both extractive and abstractive settings; we would expect any improvements in model pretraining to translate in better summarization in the future; and c) the proposed models can be used as a stepping stone to fur"
D19-1387,N18-1150,0,0.442654,"aphic; extractive vs. abstractive). Across datasets, we experimentally show that the proposed models achieve state-of-the-art results under both extractive and abstractive settings. Our contributions in this work are threefold: a) we highlight the importance of document encoding for the summarization task; a variety of recently proposed techniques aim to enhance summarization performance via copying mechanisms (Gu et al., 2016; See et al., 2017; Nallapati et al., 2017), reinforcement learning (Narayan et al., 2018b; Paulus et al., 2018; Dong et al., 2018), and multiple communicating encoders (Celikyilmaz et al., 2018). We achieve better results with a minimum-requirement model without using any of these mechanisms; b) we showcase ways to effectively employ pretrained language models in summarization under both extractive and abstractive settings; we would expect any improvements in model pretraining to translate in better summarization in the future; and c) the proposed models can be used as a stepping stone to further improve summarization performance as well as baselines against which new proposals are tested. 2 2.1 Background Pretrained Language Models Pretrained language models (Peters et al., 2018; Ra"
D19-1387,J10-3005,1,0.914197,"erence summaries, but in XSum, this gap is much smaller. We also observe that on CNN/DailyMail, B ERT E XTA BS produces less novel n-ngrams than B ERTA BS, which is not surprising. B ERT E XTA BS is more biased towards selecting sentences from the source document since it is initially trained as an extractive model. The supplementary material includes examples of system output and additional ablation studies. 5.3 Human Evaluation In addition to automatic evaluation, we also evaluated system output by eliciting human judgments. We report experiments following a questionanswering (QA) paradigm (Clarke and Lapata, 2010; Narayan et al., 2018b) which quantifies the degree to which summarization models retain key information from the document. Under this paradigm, a set of questions is created based on the gold summary under the assumption that it highlights the most important document content. Participants are then asked to answer these questions by reading system summaries alone without access to the article. The more questions a system can answer, the better it is at summarizing the document as a whole. Moreover, we also assessed the overall quality of the summaries produced by abstractive systems which due"
D19-1387,N19-1423,0,0.483496,"ning schedule which adopts different optimizers for the encoder and the decoder as a means of alleviating the mismatch between the two (the former is pretrained while the latter is not). We also demonstrate that a two-staged fine-tuning approach can further boost the quality of the generated summaries. Experiments on three datasets show that our model achieves stateof-the-art results across the board in both extractive and abstractive settings.1 1 In most cases, pretrained language models have been employed as encoders for sentence- and paragraph-level natural language understanding problems (Devlin et al., 2019) involving various classification tasks (e.g., predicting whether any two sentences are in an entailment relationship; or determining the completion of a sentence among four alternative sentences). In this paper, we examine the influence of language model pretraining on text summarization. Different from previous tasks, summarization requires wide-coverage natural language understanding going beyond the meaning of individual words and sentences. The aim is to condense a document into a shorter version while preserving most of its meaning. Furthermore, under abstractive modeling formulations, t"
D19-1387,D18-1409,0,0.156843,"Missing"
D19-1387,P16-1188,0,0.527234,"material. CNN/DailyMail contains news articles and associated highlights, i.e., a few bullet points giving a brief overview of the article. We used the standard splits of Hermann et al. (2015) for training, validation, and testing (90,266/1,220/1,093 CNN documents and 196,961/12,148/10,397 DailyMail documents). We did not anonymize entities. We first split sentences with the Stanford CoreNLP toolkit (Manning et al., 2014) and pre-processed the dataset following See et al. (2017). Input documents were truncated to 512 tokens. NYT contains 110,540 articles with abstractive summaries. Following Durrett et al. (2016), we split these into 100,834/9,706 training/test examples, based on the date of publication (the test set contains all articles published from January 1, 2007 onward). We used 4,000 examples from the training as validation set. We also followed their filtering procedure, documents with summaries less than 50 words were removed from the dataset. The filtered test set (NYT50) includes 3,452 examples. Sentences were split with the Stanford CoreNLP toolkit (Manning et al., 2014) and preprocessed following Durrett et al. (2016). Input documents were truncated to 800 tokens. XSum contains 226,711 n"
D19-1387,N19-1409,0,0.0270286,"yers: ˜ l = LN(hl−1 + MHAtt(hl−1 )) h ˜ l + FFN(h ˜ l )) hl = LN(h (1) (2) where h0 = x are the input vectors; LN is the layer normalization operation (Ba et al., 2016); MHAtt is the multi-head attention operation (Vaswani et al., 2017); superscript l indicates the depth of the stacked layer. On the top layer, B ERT will generate an output vector ti for each token with rich contextual information. Pretrained language models are usually used to enhance performance in language understanding tasks. Very recently, there have been attempts to apply pretrained models to various generation problems (Edunov et al., 2019; Rothe et al., 2019). When fine-tuning for a specific task, unlike ELMo whose parameters are usually fixed, parameters in B ERT are jointly fine-tuned with additional taskspecific parameters. 2.2 Extractive Summarization Extractive summarization systems create a summary by identifying (and subsequently concatenating) the most important sentences in a document. Neural models consider extractive sum3731 Or iginal BERT I n pu t Docu m en t Tok en Em beddi n gs Segm en t Em beddi n gs Posi t i on Em beddi n gs BERT for Summar ization [ CLS] sent one [ SEP] 2nd sent [ SEP] sent again [ SEP] [ CLS]"
D19-1387,P17-2074,0,0.0214302,"s paradigm, a set of questions is created based on the gold summary under the assumption that it highlights the most important document content. Participants are then asked to answer these questions by reading system summaries alone without access to the article. The more questions a system can answer, the better it is at summarizing the document as a whole. Moreover, we also assessed the overall quality of the summaries produced by abstractive systems which due to their ability to rewrite content may produce disfluent or ungrammatical output. Specifically, we followed the Best-Worst Scaling (Kiritchenko and Mohammad, 2017) method where participants were presented with the output of two systems (and the original document) and 3737 Proportion of novel n-grams B ERT S UM E XT A BS B ERT S UM A BS Reference Abstractive 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0.0 L EAD PTG EN B OTTOM U P TC ONV S2S G OLD B ERT S UM 1-grams 2-grams NYT QA Rank 36.2† — 30.5† -0.27† — — — — — 0.33† 41.8 -0.07 XSum QA Rank 9.20† — 23.7† -0.36† — — 52.1 -0.20† — 0.38† 57.5 0.19 Table 7: QA-based and ranking-based evaluation. Models with † are significantly different from B ERTS UM (using a paired student t-test; p &lt; 0.05). Table cells are filled"
D19-1387,P17-4012,0,0.0508356,"Input documents were truncated to 512 tokens. Aside from various statistics on the three datasets, Table 1 also reports the proportion of novel bi-grams in gold summaries as a measure of their abstractiveness. We would expect models with extractive biases to perform better on datasets with (mostly) extractive summaries, and abstractive models to perform more rewrite operations on datasets with abstractive summaries. CNN/DailyMail and NYT are somewhat extractive, while XSum is highly abstractive. 4.2 Implementation Details For both extractive and abstractive settings, we used PyTorch, OpenNMT (Klein et al., 2017) and the ‘bert-base-uncased’2 version of B ERT to implement B ERT S UM. Both source and target texts 3734 2 https://git.io/fhbJQ were tokenized with B ERT’s subwords tokenizer. Extractive Summarization All extractive models were trained for 50,000 steps on 3 GPUs (GTX 1080 Ti) with gradient accumulation every two steps. Model checkpoints were saved and evaluated on the validation set every 1,000 steps. We selected the top-3 checkpoints based on the evaluation loss on the validation set, and report the averaged results on the test set. We used a greedy algorithm similar to Nallapati et al. (201"
D19-1387,D18-1205,0,0.0576713,".16 54.70 83.31 Table 1: Comparison of summarization datasets: size of training, validation, and test sets and average document and summary length (in terms of words and sentences). The proportion of novel bi-grams that do not appear in source documents but do appear in the gold summaries quantifies corpus bias towards extractive methods. In addition, we propose a two-stage fine-tuning approach, where we first fine-tune the encoder on the extractive summarization task (Section 3.2) and then fine-tune it on the abstractive summarization task (Section 3.3). Previous work (Gehrmann et al., 2018; Li et al., 2018) suggests that using extractive objectives can boost the performance of abstractive summarization. Also notice that this two-stage approach is conceptually very simple, the model can take advantage of information shared between these two tasks, without fundamentally changing its architecture. We name the default abstractive model B ERT S UM A BS and the two-stage fine-tuned model B ERT S UM E XTA BS. 4 Experimental Setup In this section, we describe the summarization datasets used in our experiments and discuss various implementation details. 4.1 Summarization Datasets We evaluated our model o"
D19-1387,W04-1013,0,0.498496,"verlap; RL is the longest common subsequence). Results for comparison systems are taken from the authors’ respective papers or obtained on our data by running publicly released software. we focus on building a minimum-requirements model and these mechanisms may introduce additional hyper-parameters to tune. Thanks to the subwords tokenizer, we also rarely observe issues with out-of-vocabulary words in the output; moreover, trigram-blocking produces diverse summaries managing to reduce repetitions. 5 5.1 Results Automatic Evaluation We evaluated summarization quality automatically using ROUGE (Lin, 2004). We report unigram and bigram overlap (ROUGE-1 and ROUGE-2) as a means of assessing informativeness and the longest common subsequence (ROUGE-L) as a means of assessing fluency. Table 2 summarizes our results on the CNN/DailyMail dataset. The first block in the table includes the results of an extractive O RACLE system as an upper bound. We also present the L EAD -3 baseline (which simply selects the first three sentences in a document). The second block in the table includes various extractive models trained on the CNN/DailyMail dataset (see Section 2.2 for an overview). For 3735 Model O RAC"
D19-1387,N19-1173,1,0.890271,"llapati et al., 2017) is one of the earliest neural approaches adopting an encoder based on Recurrent Neural Networks. R EFRESH (Narayan et al., 2018b) is a reinforcement learning-based system trained by globally optimizing the ROUGE metric. More recent work achieves higher performance with more sophisticated model structures. L A TENT (Zhang et al., 2018) frames extractive summarization as a latent variable inference problem; instead of maximizing the likelihood of “gold” standard labels, their latent model directly maximizes the likelihood of human summaries given selected sentences. S UMO (Liu et al., 2019) capitalizes on the notion of structured attention to induce a multi-root dependency tree representation of the document while predicting the output summary. N EU S UM (Zhou et al., 2018) scores and selects sentences jointly and represents the state of the art in extractive summarization. 2.3 Abstractive Summarization Neural approaches to abstractive summarization conceptualize the task as a sequence-to-sequence problem, where an encoder maps a sequence of tokens in the source document x = [x1 , ..., xn ] to a sequence of continuous representations z = [z1 , ..., zn ], and a decoder then gener"
D19-1387,P14-5010,0,0.00510181,"ore cut and paste operations while others are genuinely abstractive). Table 1 presents statistics on these datasets (test set); example (gold-standard) summaries are provided in the supplementary material. CNN/DailyMail contains news articles and associated highlights, i.e., a few bullet points giving a brief overview of the article. We used the standard splits of Hermann et al. (2015) for training, validation, and testing (90,266/1,220/1,093 CNN documents and 196,961/12,148/10,397 DailyMail documents). We did not anonymize entities. We first split sentences with the Stanford CoreNLP toolkit (Manning et al., 2014) and pre-processed the dataset following See et al. (2017). Input documents were truncated to 512 tokens. NYT contains 110,540 articles with abstractive summaries. Following Durrett et al. (2016), we split these into 100,834/9,706 training/test examples, based on the date of publication (the test set contains all articles published from January 1, 2007 onward). We used 4,000 examples from the training as validation set. We also followed their filtering procedure, documents with summaries less than 50 words were removed from the dataset. The filtered test set (NYT50) includes 3,452 examples. Se"
D19-1387,K16-1028,0,0.145126,"Missing"
D19-1387,D18-1206,1,0.068098,"the document or distributed more evenly throughout) and summary styles (e.g., verbose vs. more telegraphic; extractive vs. abstractive). Across datasets, we experimentally show that the proposed models achieve state-of-the-art results under both extractive and abstractive settings. Our contributions in this work are threefold: a) we highlight the importance of document encoding for the summarization task; a variety of recently proposed techniques aim to enhance summarization performance via copying mechanisms (Gu et al., 2016; See et al., 2017; Nallapati et al., 2017), reinforcement learning (Narayan et al., 2018b; Paulus et al., 2018; Dong et al., 2018), and multiple communicating encoders (Celikyilmaz et al., 2018). We achieve better results with a minimum-requirement model without using any of these mechanisms; b) we showcase ways to effectively employ pretrained language models in summarization under both extractive and abstractive settings; we would expect any improvements in model pretraining to translate in better summarization in the future; and c) the proposed models can be used as a stepping stone to further improve summarization performance as well as baselines against which new proposals a"
D19-1387,N18-1158,1,0.0556355,"the document or distributed more evenly throughout) and summary styles (e.g., verbose vs. more telegraphic; extractive vs. abstractive). Across datasets, we experimentally show that the proposed models achieve state-of-the-art results under both extractive and abstractive settings. Our contributions in this work are threefold: a) we highlight the importance of document encoding for the summarization task; a variety of recently proposed techniques aim to enhance summarization performance via copying mechanisms (Gu et al., 2016; See et al., 2017; Nallapati et al., 2017), reinforcement learning (Narayan et al., 2018b; Paulus et al., 2018; Dong et al., 2018), and multiple communicating encoders (Celikyilmaz et al., 2018). We achieve better results with a minimum-requirement model without using any of these mechanisms; b) we showcase ways to effectively employ pretrained language models in summarization under both extractive and abstractive settings; we would expect any improvements in model pretraining to translate in better summarization in the future; and c) the proposed models can be used as a stepping stone to further improve summarization performance as well as baselines against which new proposals a"
D19-1387,N18-1202,0,0.444811,"task requires language generation capabilities in order to create summaries containing novel words and phrases not featured in the source text, while extractive summarization is often defined as a binary classification task with labels indicating whether a text span (typically a sentence) should be included in the summary. Introduction Language model pretraining has advanced the state of the art in many NLP tasks ranging from sentiment analysis, to question answering, natural language inference, named entity recognition, and textual similarity. State-of-the-art pretrained models include ELMo (Peters et al., 2018), GPT (Radford et al., 2018), and more recently Bidirectional Encoder Representations from Transformers (B ERT; Devlin et al. 2019). B ERT combines both word and sentence representations in a single very large Transformer (Vaswani et al., 2017); it is 1 Our code is available at https://github.com/ nlpyang/PreSumm. We explore the potential of B ERT for text summarization under a general framework encompassing both extractive and abstractive modeling paradigms. We propose a novel documentlevel encoder based on B ERT which is able to encode a document and obtain representations for its sentences."
D19-1387,P17-1099,0,0.2058,"s (e.g., important information is concentrated at the beginning of the document or distributed more evenly throughout) and summary styles (e.g., verbose vs. more telegraphic; extractive vs. abstractive). Across datasets, we experimentally show that the proposed models achieve state-of-the-art results under both extractive and abstractive settings. Our contributions in this work are threefold: a) we highlight the importance of document encoding for the summarization task; a variety of recently proposed techniques aim to enhance summarization performance via copying mechanisms (Gu et al., 2016; See et al., 2017; Nallapati et al., 2017), reinforcement learning (Narayan et al., 2018b; Paulus et al., 2018; Dong et al., 2018), and multiple communicating encoders (Celikyilmaz et al., 2018). We achieve better results with a minimum-requirement model without using any of these mechanisms; b) we showcase ways to effectively employ pretrained language models in summarization under both extractive and abstractive settings; we would expect any improvements in model pretraining to translate in better summarization in the future; and c) the proposed models can be used as a stepping stone to further improve summa"
D19-1387,D18-1088,1,0.939802,"ustrated in red and green color) to distinguish multiple sentences. marization as a sentence classification problem: a neural encoder creates sentence representations and a classifier predicts which sentences should be selected as summaries. S UMMA RU NN ER (Nallapati et al., 2017) is one of the earliest neural approaches adopting an encoder based on Recurrent Neural Networks. R EFRESH (Narayan et al., 2018b) is a reinforcement learning-based system trained by globally optimizing the ROUGE metric. More recent work achieves higher performance with more sophisticated model structures. L A TENT (Zhang et al., 2018) frames extractive summarization as a latent variable inference problem; instead of maximizing the likelihood of “gold” standard labels, their latent model directly maximizes the likelihood of human summaries given selected sentences. S UMO (Liu et al., 2019) capitalizes on the notion of structured attention to induce a multi-root dependency tree representation of the document while predicting the output summary. N EU S UM (Zhou et al., 2018) scores and selects sentences jointly and represents the state of the art in extractive summarization. 2.3 Abstractive Summarization Neural approaches to"
D19-1387,P19-1499,0,0.206479,"without using any of these mechanisms; b) we showcase ways to effectively employ pretrained language models in summarization under both extractive and abstractive settings; we would expect any improvements in model pretraining to translate in better summarization in the future; and c) the proposed models can be used as a stepping stone to further improve summarization performance as well as baselines against which new proposals are tested. 2 2.1 Background Pretrained Language Models Pretrained language models (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2019; Dong et al., 2019; Zhang et al., 2019) have recently emerged as a key technology for achieving impressive gains in a wide variety of natural language tasks. These models extend the idea of word embeddings by learning contextual representations from large-scale corpora using a language modeling objective. Bidirectional Encoder Representations from Transformers (B ERT; Devlin et al. 2019) is a new language representation model which is trained with a masked language modeling and a “next sentence prediction” task on a corpus of 3,300M words. The general architecture of B ERT is shown in the left part of Figure 1. Input text is first"
D19-1387,P18-1061,0,0.437953,"system trained by globally optimizing the ROUGE metric. More recent work achieves higher performance with more sophisticated model structures. L A TENT (Zhang et al., 2018) frames extractive summarization as a latent variable inference problem; instead of maximizing the likelihood of “gold” standard labels, their latent model directly maximizes the likelihood of human summaries given selected sentences. S UMO (Liu et al., 2019) capitalizes on the notion of structured attention to induce a multi-root dependency tree representation of the document while predicting the output summary. N EU S UM (Zhou et al., 2018) scores and selects sentences jointly and represents the state of the art in extractive summarization. 2.3 Abstractive Summarization Neural approaches to abstractive summarization conceptualize the task as a sequence-to-sequence problem, where an encoder maps a sequence of tokens in the source document x = [x1 , ..., xn ] to a sequence of continuous representations z = [z1 , ..., zn ], and a decoder then generates the target summary y = [y1 , ..., ym ] token-by-token, in an auto-regressive manner, hence modeling the conditional probability: p(y1 , ..., ym |x1 , ..., xn ). Rush et al. (2015) an"
D19-1387,D15-1044,0,0.403321,"M (Zhou et al., 2018) scores and selects sentences jointly and represents the state of the art in extractive summarization. 2.3 Abstractive Summarization Neural approaches to abstractive summarization conceptualize the task as a sequence-to-sequence problem, where an encoder maps a sequence of tokens in the source document x = [x1 , ..., xn ] to a sequence of continuous representations z = [z1 , ..., zn ], and a decoder then generates the target summary y = [y1 , ..., ym ] token-by-token, in an auto-regressive manner, hence modeling the conditional probability: p(y1 , ..., ym |x1 , ..., xn ). Rush et al. (2015) and Nallapati et al. (2016) were among the first to apply the neural encoderdecoder architecture to text summarization. See et al. (2017) enhance this model with a pointergenerator network (PT GEN) which allows it to copy words from the source text, and a coverage mechanism (C OV) which keeps track of words that have been summarized. Celikyilmaz et al. (2018) propose an abstractive system where multiple agents (encoders) represent the document together with a hierarchical attention mechanism (over the agents) for decoding. Their Deep Communicating Agents (DCA) model is trained end-to-end with"
D19-1634,W17-4773,0,0.218412,"Missing"
D19-1634,W18-6452,0,0.0281174,"Missing"
D19-1634,P15-2026,0,0.024013,"it the erroneous translation to obtain a correct translation (pe). Our work aims to explicitly model how to copy words from mt to pe (highlighted in bold), which is a common phenomenon in APE. Introduction Automatic post-editing (APE) is an important natural language processing (NLP) task that aims to automatically correct errors made by machine translation systems (Knight and Chander, 1994). It can be considered as an efficient way to modify translations to a specific domain or to incorporate additional information into translations rather than translating from scratch (McKeown et al., 2012; Chatterjee et al., 2015, 2018). Approaches to APE can be roughly divided into two broad categories: statistical and neural approaches. While early efforts focused on statistical approaches relying on manual feature engineering (Simard et al., 2007; B´echara et al., 2011), neural network based approaches capable of learning representations from data have ∗ Corresponding author: Yang Liu The source code is available at https://github. com/THUNLP-MT/L2Copy4APE 1 I ate a cake yesterday Ich esse einen Hamburger Ich hatte gestern einen Kuchen gegessen shown remarkable superiority over their statistical counterparts (Varis"
D19-1634,P16-1154,0,0.362486,"e translation systems in a post-processing step, is an important task in natural language processing. While recent work has achieved considerable performance gains by using neural networks, how to model the copying mechanism for APE remains a challenge. In this work, we propose a new method for modeling copying for APE. To better identify translation errors, our method learns the representations of source sentences and system outputs in an interactive way. These representations are used to explicitly indicate which words in the system outputs should be copied, which is useful to help CopyNet (Gu et al., 2016) better generate post-edited translations. Experiments on the datasets of the WMT 2016-2017 APE shared tasks show that our approach outperforms all best published results. 1 1 Table 1: Example of automatic post-editing (APE). Given a source sentence (src) and a machine translation (mt), the goal of APE is to post-edit the erroneous translation to obtain a correct translation (pe). Our work aims to explicitly model how to copy words from mt to pe (highlighted in bold), which is a common phenomenon in APE. Introduction Automatic post-editing (APE) is an important natural language processing (NLP"
D19-1634,W16-2378,0,0.0514658,"MT Additional Dataset training set dev2016 test2016 test2017 training set dev2018 artificial-small artificial-big eSCAPE-PBSMT eSCAPE-NMT α 0.1 0.5 0.9 0.9 0.9 # Sent. 23,000 1,000 2,000 2,000 13,442 1,000 526,368 4,391,180 7,258,533 7,258,533 λ 1.0 1.0 0.1 0.5 1.0 TER↓ 18.83 18.45 18.89 18.47 18.38 BLEU↑ 72.59 72.83 72.27 72.83 72.99 Table 3: Effect of α and λ. The TER and BLEU scores are calculated on the WMT 2016 APE official development set. Table 2: Statistics of the English-German datasets in the WMT APE task. Note that the NMT official data only contains training and development sets. (Junczys-Dowmunt and Grundkiewicz, 2016; Negri et al., 2014). We used the WMT official dataset for the PBSMT task and the NMT task separately. The artificial training data (Junczys-Dowmunt and Grundkiewicz, 2016) was also used for both tasks. More precisely, we used the concatenation of the official training data and the artificial-small data to learn a truecasing model (Koehn et al., 2007) and obtain sub-word units using byte-pair encoding (BPE) (Sennrich et al., 2015) with 32k merges. Then, we applied truecasing and BPE to all datasets. We oversampled the official training data 20 times and concatenated them with both artificial-"
D19-1634,I17-1013,0,0.0352878,"Missing"
D19-1634,W18-6467,0,0.415563,"x, y ˜, y&lt;j ; θ), (1) j=1 where yj is the j-th target word in pe, y&lt;j = y1 . . . yj−1 is a partial translation, θ is a set of model parameters, and P (yj |x, y ˜, y&lt;j ; θ) is a word-level translation probability. 6123 Output Prob. Encoder Encoder Output Prob. Original Prob. FFN Copy Prob. Self-Att CopyNet FFN Original Prob. Softmax × × Copy Prob. Self-Att Softmax Linear CopyNet Linear Sigmoid FFN Decoder Encoder × Src-Dec-Att FFN Mt-Dec-Att Self-Att Self-Att Predictor × × Linear FFN FFN Enc-Dec-Att Self-Att Self-Att (a) Decoder × (b) Figure 2: (a) The architecture of multi-source Transformer (Junczys-Dowmunt and Grundkiewicz, 2018) equipped with CopyNet (Gu et al., 2016) and (b) the architecture of our approach. While the existing work learns the representations of src and mt separately, our approach allows for learning the representations of src and mt in an interactive way by concatenating them as a single input. In addition, our approach introduces a Predictor module to explicitly indicate which words in mt should be copied. The word-level translation probability in Eq. (1) is computed as Hsrc = Encodersrc (x, θ), mt H hpe j (2) mt = Encoder (˜ y, θ), src (3) mt = Decoder(y&lt;j , H , H , θ), P (yj |x, y ˜, y&lt;j ; θ) ∝ e"
D19-1634,P07-2045,0,0.00591135,"and BLEU scores are calculated on the WMT 2016 APE official development set. Table 2: Statistics of the English-German datasets in the WMT APE task. Note that the NMT official data only contains training and development sets. (Junczys-Dowmunt and Grundkiewicz, 2016; Negri et al., 2014). We used the WMT official dataset for the PBSMT task and the NMT task separately. The artificial training data (Junczys-Dowmunt and Grundkiewicz, 2016) was also used for both tasks. More precisely, we used the concatenation of the official training data and the artificial-small data to learn a truecasing model (Koehn et al., 2007) and obtain sub-word units using byte-pair encoding (BPE) (Sennrich et al., 2015) with 32k merges. Then, we applied truecasing and BPE to all datasets. We oversampled the official training data 20 times and concatenated them with both artificial-small and artificial-big datasets (JunczysDowmunt and Grundkiewicz, 2018). Finally, we obtained a dataset containing nearly 5M triplets for both tasks. To test our approach on a larger PBSMT dataset, we used the eSCAPE synthetic dataset (Negri et al., 2014), which contains 7.2M sentences. By including the eSCAPE dataset, the training set is enlarged to"
D19-1634,W16-2361,0,0.0526664,"Missing"
D19-1634,2012.eamt-1.34,0,0.062667,"Missing"
D19-1634,C16-1172,0,0.0550767,"Missing"
D19-1634,W18-6468,0,0.0235546,"Missing"
D19-1634,P17-1099,0,0.0647436,"other, which might lead to the inability to find which src word is untranslated and which mt word is incorrect. For example, the mt sentence in Figure 1 is fluent and meaningful. Without src, the APE system is unable to identify translation errors. In addition, the multisource Transformer does not explicitly model the copying between mt and pe in neither the Encoder nor the Decoder. 2.2 CopyNet CopyNet (Gu et al., 2016) is a widely used method for modeling copying in sequence-to-sequence learning. It has been successfully applied to single-turn dialogue (Gu et al., 2016), text summarization (See et al., 2017), and grammar error correction (Zhao et al., 2019). It is possible to extend the multi-source Transformer with CopyNet to explicitly model the copying mechanism, as shown in Figure 2(a). CopyNet defines the word-level translation probability in Eq. (1) as a linear interpolation of copying and generating probabilities: P (yj |x, y ˜, y&lt;j ; θ) = γj × P copy (yj ) + (1 − γj ) × P gen (yj ), (6) where P copy (yj ) is the copying probability for yj , P gen (yj ) is the generating probability for yj , and γj is a gating weight. They are defined as follows: P copy (yj ) ∝ exp(g(Hmt , hpe j )), P gen"
D19-1634,W18-6470,0,0.193069,"t-editing. Second, it is possible to predict which words in mt should be copied because it is easy to automatically construct labeled data by comparing mt and pe. Such predictions can be combined with CopyNet to better model copying for APE. Experiments show that our approach outperforms the best published results on the datasets of the WMT 20162017 APE shared tasks. 2 2.1 Background Multi-source Sequence-to-Sequence Learning Multi-source sequence-to-sequence learning has been widely used in APE in recent years (JunczysDowmunt and Grundkiewicz, 2018; Pal et al., 2018; Tebbifakhr et al., 2018; Shin and Lee, 2018). The architecture of multi-source Transformer is shown in Figure 2(a). It can be equipped with CopyNet (see Section 2.2) to serve as a baseline in our experiments. It is worth noting that src and mt are encoded separately. Let x = x1 . . . xI be a source sentence (i.e., src) with I words, y ˜ = y˜1 . . . y˜K be a translation output by a machine translation system (i.e., mt) with K words, and y = y1 . . . yJ be the postedited translation (i.e., pe) with J words. The APE model is given by P (y|x, y ˜; θ) = J Y P (yj |x, y ˜, y&lt;j ; θ), (1) j=1 where yj is the j-th target word in pe, y&lt;j = y1 . ."
D19-1634,N07-1064,0,0.0614393,"(APE) is an important natural language processing (NLP) task that aims to automatically correct errors made by machine translation systems (Knight and Chander, 1994). It can be considered as an efficient way to modify translations to a specific domain or to incorporate additional information into translations rather than translating from scratch (McKeown et al., 2012; Chatterjee et al., 2015, 2018). Approaches to APE can be roughly divided into two broad categories: statistical and neural approaches. While early efforts focused on statistical approaches relying on manual feature engineering (Simard et al., 2007; B´echara et al., 2011), neural network based approaches capable of learning representations from data have ∗ Corresponding author: Yang Liu The source code is available at https://github. com/THUNLP-MT/L2Copy4APE 1 I ate a cake yesterday Ich esse einen Hamburger Ich hatte gestern einen Kuchen gegessen shown remarkable superiority over their statistical counterparts (Varis and Bojar, 2017; Chatterjee et al., 2017; Junczys-Dowmunt and Grundkiewicz, 2017; Unanue et al., 2018). Most of them cast APE as a multi-source sequence-to-sequence learning problem (Zoph and Knight, 2016): given a source s"
D19-1634,W18-2702,0,0.036302,"Missing"
D19-1634,W17-4777,0,0.0338594,"Missing"
D19-1634,D18-1341,0,0.0186565,"to 2,048. The number of individual attention heads was set to 8 for multi-head attention. We set N = Ne = Nd = 6, Np = 3 and we tied all three src, mt, pe embeddings for saving memory. The embeddings and softmax weights were also tied. In training, we used Adam (Kingma and Ba, 2014) for optimization. Each mini-batch contains approximately 25K tokens. We used the learning rate decay policy described 6127 1. O RIGINAL: the original mt without any postediting. 2. C OPY N ET (Gu et al., 2016; Zhao et al., 2019): the multi-source Transformer equipped with CopyNet (see Figure 2(a)). 3. N PI -A PE (Vu and Haffari, 2018): a neural programmer-interpreter approach. 4. M S U EDIN (Junczys-Dowmunt and Grundkiewicz, 2018): a multi-source Transformerbased APE system that shares the encoders of src and mt. It is the champion of the WMT 2018 APE shared task. 5. U SAAR D FKI (Pal et al., 2018): a multisource Transformer-based APE system with a joint encoder that attends over a combination of two encoded sequences. It is a participant of the WMT 2018 APE shared task. 6 7 https://github.com/THUNLP-MT/THUMT http://www.cs.umd.edu/snover/tercom/ TEST16 TEST17 TEST16+17 TER↓ BLEU↑ TER↓ BLEU↑ TER↓ BLEU↑ (1) O RIGINAL 24.76 6"
D19-1634,D18-1475,0,0.0272315,"., “Kuchen” in Figure 1). This can be done by generating explicit labels using bilingual word alignment. We leave this for future work. 6125 0.8 0.6 where α and λ are hyper-parameters. The first part is the original log-likelihood loss function of APE: 5 0.2 Copying Scores Lape (θ) = − log P (y|x, y ˜; θ). (19) pe The second part is related to the CopyNet: src K 1 X Lcopy (θ) = (lk − ck )2 , K mt (20) k=1 Figure 4: Copying scores as scaling masks. The copying scores are used to modify the attention layers of the Encoder, Decoder, and CopyNet. Encoder, the Decoder, and the CopyNet. Inspired by Yang et al. (2018)’s strategy to integrate localness to self-attention, we propose to incorporate copying scores into our model by modifying attention weights involved in the aforementioned three modules. The original scaled dot-product attention (Vaswani et al., 2017) is defined as qK&gt; energy = √ , d Att(q, K) = softmax(energy), where lk is the ground-truth label (see Section 3.2) and ck is a quantity that measures how likely the k-th word in mt to be copied by CopyNet: ck = J X γj × P copy (˜ yk ). j=1 Note that γ × P copy (y) is the term related to copying the target word y in Eq. (6). The third part is a cr"
D19-1634,2020.amta-research.11,1,0.905528,"Missing"
D19-1634,N19-1014,0,0.334755,"which src word is untranslated and which mt word is incorrect. For example, the mt sentence in Figure 1 is fluent and meaningful. Without src, the APE system is unable to identify translation errors. In addition, the multisource Transformer does not explicitly model the copying between mt and pe in neither the Encoder nor the Decoder. 2.2 CopyNet CopyNet (Gu et al., 2016) is a widely used method for modeling copying in sequence-to-sequence learning. It has been successfully applied to single-turn dialogue (Gu et al., 2016), text summarization (See et al., 2017), and grammar error correction (Zhao et al., 2019). It is possible to extend the multi-source Transformer with CopyNet to explicitly model the copying mechanism, as shown in Figure 2(a). CopyNet defines the word-level translation probability in Eq. (1) as a linear interpolation of copying and generating probabilities: P (yj |x, y ˜, y&lt;j ; θ) = γj × P copy (yj ) + (1 − γj ) × P gen (yj ), (6) where P copy (yj ) is the copying probability for yj , P gen (yj ) is the generating probability for yj , and γj is a gating weight. They are defined as follows: P copy (yj ) ∝ exp(g(Hmt , hpe j )), P gen (yj ) ∝ γj = exp(hpe j Wg ), mt u(H , hpe j ), (7)"
D19-1634,N16-1004,0,0.124599,"Missing"
I11-1109,P06-2005,0,0.863411,"(a) Aye.oops,dat was spose to be a txt (b) Rndm fct bout wife: n the past 10 yrs I can cnt on one hand the num Xs she’s 4gotn to unlock my car door (c) OMG I LOVE YOU GUYS. You pwn :) !!! (d) i need to qo to bedd qotta wakee up at 7am for school.... (e) heard it again! xD 3 TIMES.a sng i nvr hear! 3 Machine translation (MT) techniques trained at the word- or phrase-level are also common. Translation of SMS from one language to another led Bangalore et al. (2002) to use consensus translations to bootstrap a translation system for instant messages and chat rooms where abbreviations are common. Aw et al. (2006) view SMS lingo as if it were another language with its own words and grammar to produce grammatically correct English sentences using MT. Q. and H. (2009) trained an MT system using three on-line SMS dictionaries for normalizing chat-like messages on Twitter. Kobus et al. (2008) incorporate a second phase in the translation model that maps characters in the texting abbreviation to phonemes, which are viewed as the output of an automatic speech recognition (ASR) system. They use a nondeterministic phonemic transducer to decode the phonemes into English words. The technical paper of Raghunathan"
I11-1109,C02-1134,0,0.209671,"re modeling, we use Twitter’s meta-data to collect only messages sent via SMS. Some examples of highly abbreviated messages from our corpus are shown below. (a) Aye.oops,dat was spose to be a txt (b) Rndm fct bout wife: n the past 10 yrs I can cnt on one hand the num Xs she’s 4gotn to unlock my car door (c) OMG I LOVE YOU GUYS. You pwn :) !!! (d) i need to qo to bedd qotta wakee up at 7am for school.... (e) heard it again! xD 3 TIMES.a sng i nvr hear! 3 Machine translation (MT) techniques trained at the word- or phrase-level are also common. Translation of SMS from one language to another led Bangalore et al. (2002) to use consensus translations to bootstrap a translation system for instant messages and chat rooms where abbreviations are common. Aw et al. (2006) view SMS lingo as if it were another language with its own words and grammar to produce grammatically correct English sentences using MT. Q. and H. (2009) trained an MT system using three on-line SMS dictionaries for normalizing chat-like messages on Twitter. Kobus et al. (2008) incorporate a second phase in the translation model that maps characters in the texting abbreviation to phonemes, which are viewed as the output of an automatic speech re"
I11-1109,P11-2013,1,0.401454,"l text is important for various lan974 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 974–982, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP vised noisy channel approach using probabilistic models for common abbreviation types and choosing the English word with the highest probability after combining the models. Deletion-based abbreviations were addressed in our past work using statistical models (maximum entropy and conditional random fields) combined with an in-domain language model (LM) (Pennell and Liu, 2010; Pennell and Liu, 2011). Liu et al. (2011) extend the statistical model to be independent of abbreviation type with good results. tortion limit and maximum phrase length allowed by the MT system during decoding. Contractor et al. (2010) also uses an SMT model; however, in an attempt to get around the problem of collecting and annotating a large parallel corpus, they automatically create a noisy list of word-abbreviation pairs for training using some heuristics. As far as we know, we are the first to use an MT system at the character-level for this task. Whitelaw et al. (2009) used a noisy channel model based on orthographic edit dista"
I11-1109,P10-1079,0,0.430524,"roblem of collecting and annotating a large parallel corpus, they automatically create a noisy list of word-abbreviation pairs for training using some heuristics. As far as we know, we are the first to use an MT system at the character-level for this task. Whitelaw et al. (2009) used a noisy channel model based on orthographic edit distance using the web to generate a large set of automatically generated (noisy) pairs to be used for training and for spelling suggestions. Although they use the web for collection, they do not focus on informal text but rather on unintentional spelling mistakes. Beaufort et al. (2010) combine a noisy channel model with a rule-based finite-state transducer and got reasonable results on French SMS, but have not tried their method on English text. Han and Baldwin (2011) first determine whether a given out-of-vocabulary (OOV) word needs to be expanded or is some other type of properly-formed OOV. For those predicted to be ill-formed, a confusion set of possible candidate words is generated based on a combination of lexical and phonetic edit distance and the top word is chosen in context using LM and dependency parse information. Due to the lack of a large parallel corpus suita"
I11-1109,W10-0513,0,0.0613594,"ore specific versus more general data when training the MT system, we used the following training sets: We wanted to determine how the order of the character-level LM used by Moses affected performance. A smaller order model may not capture larger phrases or long distance dependencies, but the smaller model may generalize better due to the sparseness of many high-order phrases. We explore different character LMs used in decoding to generate word hypotheses, including character-based 3-, 5-, and 7-gram LMs. These are trained on a subset of 16,543,813 messages from the Edinburgh Twitter corpus (Petrovic et al., 2010) containing no OOV words compared to an English dictionary. Each model is used in the above training/testing configurations. 1. General Setup: Training and testing are both Formation Method Deletions Substitutions Repetitions Swaps Insertions Example ppl (people) 2nite (tonight) yeeeeesssss (yes) tounge (tongue) borded (bored) Table 1: Examples of major abbreviation types. 978 All (14611) Abbreviations (2842) Standard Form (10629) Deletions (1406) Substitutions (620) Repetitions (510) Swaps (106) Insertions (55) Combination (140) Order 3 Top-1 Top-20 59.30 81.32 11.75 53.24 76.89 94.39 8.68 52"
I11-1109,C10-2022,0,0.276113,"2011 AFNLP vised noisy channel approach using probabilistic models for common abbreviation types and choosing the English word with the highest probability after combining the models. Deletion-based abbreviations were addressed in our past work using statistical models (maximum entropy and conditional random fields) combined with an in-domain language model (LM) (Pennell and Liu, 2010; Pennell and Liu, 2011). Liu et al. (2011) extend the statistical model to be independent of abbreviation type with good results. tortion limit and maximum phrase length allowed by the MT system during decoding. Contractor et al. (2010) also uses an SMT model; however, in an attempt to get around the problem of collecting and annotating a large parallel corpus, they automatically create a noisy list of word-abbreviation pairs for training using some heuristics. As far as we know, we are the first to use an MT system at the character-level for this task. Whitelaw et al. (2009) used a noisy channel model based on orthographic edit distance using the web to generate a large set of automatically generated (noisy) pairs to be used for training and for spelling suggestions. Although they use the web for collection, they do not foc"
I11-1109,W09-2010,0,0.610685,"ny formal domains. Sproat et al. (2001) provides a good resource for text normalization and its associated problems. Spell-checking algorithms are mostly ineffective on this type of data because they do not account for the phenomena in informal text. Some prior work instead focused on single typographic errors using edit distance (Kukich, 1992), perhaps combined with pronunciation modeling, such as (Toutanova and Moore, 2002). One line of research views normalization as a noisy channel problem. Choudhury et al. (2007) describe a supervised noisy channel model using HMMs for SMS normalization. Cook and Stevenson (2009) extend this work to create an unsuperText messaging (SMS) is a rapidly growing form of alternative communication for cell phones. This popularity has caused safety concerns leading many US states to pass laws prohibiting texting while driving. The technology is also difficult for users with visual impairments or physical handicaps to use. We believe a text-to-speech (TTS) system for cell phones can decrease these problems to promote safe travel and ease of use for all. Text normalization is the usual first step for TTS. Text message lingo is also similar to the chatspeak that is prolific on f"
I11-1109,P02-1019,0,0.0173502,"anguage model, in the context of neighboring words. 1 2 Introduction Related Work Text normalization is an important first step for any text-to-speech (TTS) system and has been widely studied in many formal domains. Sproat et al. (2001) provides a good resource for text normalization and its associated problems. Spell-checking algorithms are mostly ineffective on this type of data because they do not account for the phenomena in informal text. Some prior work instead focused on single typographic errors using edit distance (Kukich, 1992), perhaps combined with pronunciation modeling, such as (Toutanova and Moore, 2002). One line of research views normalization as a noisy channel problem. Choudhury et al. (2007) describe a supervised noisy channel model using HMMs for SMS normalization. Cook and Stevenson (2009) extend this work to create an unsuperText messaging (SMS) is a rapidly growing form of alternative communication for cell phones. This popularity has caused safety concerns leading many US states to pass laws prohibiting texting while driving. The technology is also difficult for users with visual impairments or physical handicaps to use. We believe a text-to-speech (TTS) system for cell phones can d"
I11-1109,P11-1038,0,0.167762,"re the first to use an MT system at the character-level for this task. Whitelaw et al. (2009) used a noisy channel model based on orthographic edit distance using the web to generate a large set of automatically generated (noisy) pairs to be used for training and for spelling suggestions. Although they use the web for collection, they do not focus on informal text but rather on unintentional spelling mistakes. Beaufort et al. (2010) combine a noisy channel model with a rule-based finite-state transducer and got reasonable results on French SMS, but have not tried their method on English text. Han and Baldwin (2011) first determine whether a given out-of-vocabulary (OOV) word needs to be expanded or is some other type of properly-formed OOV. For those predicted to be ill-formed, a confusion set of possible candidate words is generated based on a combination of lexical and phonetic edit distance and the top word is chosen in context using LM and dependency parse information. Due to the lack of a large parallel corpus suitable for our study, we are building a corpus using status updates from twitter.com. Twitter allows users to update status messages by sending an SMS message to a number designated for the"
I11-1109,D09-1093,0,0.00791998,"guage model (LM) (Pennell and Liu, 2010; Pennell and Liu, 2011). Liu et al. (2011) extend the statistical model to be independent of abbreviation type with good results. tortion limit and maximum phrase length allowed by the MT system during decoding. Contractor et al. (2010) also uses an SMT model; however, in an attempt to get around the problem of collecting and annotating a large parallel corpus, they automatically create a noisy list of word-abbreviation pairs for training using some heuristics. As far as we know, we are the first to use an MT system at the character-level for this task. Whitelaw et al. (2009) used a noisy channel model based on orthographic edit distance using the web to generate a large set of automatically generated (noisy) pairs to be used for training and for spelling suggestions. Although they use the web for collection, they do not focus on informal text but rather on unintentional spelling mistakes. Beaufort et al. (2010) combine a noisy channel model with a rule-based finite-state transducer and got reasonable results on French SMS, but have not tried their method on English text. Han and Baldwin (2011) first determine whether a given out-of-vocabulary (OOV) word needs to"
I11-1109,C08-1056,0,0.821952,"IMES.a sng i nvr hear! 3 Machine translation (MT) techniques trained at the word- or phrase-level are also common. Translation of SMS from one language to another led Bangalore et al. (2002) to use consensus translations to bootstrap a translation system for instant messages and chat rooms where abbreviations are common. Aw et al. (2006) view SMS lingo as if it were another language with its own words and grammar to produce grammatically correct English sentences using MT. Q. and H. (2009) trained an MT system using three on-line SMS dictionaries for normalizing chat-like messages on Twitter. Kobus et al. (2008) incorporate a second phase in the translation model that maps characters in the texting abbreviation to phonemes, which are viewed as the output of an automatic speech recognition (ASR) system. They use a nondeterministic phonemic transducer to decode the phonemes into English words. The technical paper of Raghunathan and Krawczyk (2009) details an explicit study varying language model orders, disData 3.1 Choosing Messages for Annotation An annotator’s time is wasted if he is presented with many messages containing no abbreviations, or with sentences all containing the same, very common abbre"
I11-1109,P07-2045,0,\N,Missing
I11-1125,C04-1053,0,0.0442906,"Missing"
I11-1125,P11-1135,0,0.0371814,"Missing"
I11-1125,P04-1074,0,0.0266503,"ed lexically instead of morphologically. There are useful contextual cues that can help determine the tense for the whole Chinese sentence or individual verbs, such as temporal adverbs or phrases, aspect auxiliary words and prepositions. For instance, in example (1a), the aspect particle “ (a particle word in Chinese, there is no literal translation to English)” and temporal phrase “AUc(several days ago)” together indicate the past tense of the sentence, and thus the correct translation of the sentence is “I arrived at Shanghai several days ago”. Most of the previous work on tense prediction (Li et al., 2004; Cao et al., 2004; Ye and Zhang, 2005; Lin, 2006) has been conducted using relatively small data sets (e.g., hundreds of 1116 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 1116–1124, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP Chinese sentences) and typically news article domain. They often require hand crafted rules in the systems. In this paper, we adopt a statistical classification framework for Chinese tense prediction. This study is different from previous work in that (a) we propose to utilize the parallel ChineseEnglish corpor"
I11-1125,E91-1047,0,0.0805091,"Missing"
I11-1125,C04-1101,0,0.031452,"ead of morphologically. There are useful contextual cues that can help determine the tense for the whole Chinese sentence or individual verbs, such as temporal adverbs or phrases, aspect auxiliary words and prepositions. For instance, in example (1a), the aspect particle “ (a particle word in Chinese, there is no literal translation to English)” and temporal phrase “AUc(several days ago)” together indicate the past tense of the sentence, and thus the correct translation of the sentence is “I arrived at Shanghai several days ago”. Most of the previous work on tense prediction (Li et al., 2004; Cao et al., 2004; Ye and Zhang, 2005; Lin, 2006) has been conducted using relatively small data sets (e.g., hundreds of 1116 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 1116–1124, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP Chinese sentences) and typically news article domain. They often require hand crafted rules in the systems. In this paper, we adopt a statistical classification framework for Chinese tense prediction. This study is different from previous work in that (a) we propose to utilize the parallel ChineseEnglish corpora to automatically"
I11-1125,N06-1020,0,0.029863,"Missing"
I11-1125,W09-2201,0,0.0253171,"Missing"
I11-1125,J03-1002,0,0.00514727,", for each instance we compare the labels generated based on three sources: original automatically derived label, prediction from the current classifier, and prediction from the initial self-trained classifier. We filter out cases using two different constraints: (a) Constraint I: neither the prediction from the current classifier nor the self-trained one is the same as the automatically derived label. (b) Constraint II: none of those three labels agree with others, that is, they are all different. 4 4.1 data (mainly word segmentation for Chinese and tokenization for English), we used GIZA++ (Och and Ney, 2003) to obtain a word-level alignment. After applying heuristic rules to eliminate verbs that have no aligned English verbs or are aligned to multiple verbs with conflicting tenses (see Section 3.3 for reference tense creation), we finally created a training set consisting of 279,379 verb instances and 38,087 sentences. We chose the maximum entropy (ME) model as the classifier for tense prediction, because ME can effectively utilize many features and performs competitively with other approaches in many classification tasks. We used the ME implementation from (Zhang, 2006) with a Gaussian prior of"
I11-1125,W09-2307,0,0.02469,"Missing"
I11-1125,P06-1102,0,0.0199689,"Missing"
I11-1125,P05-1022,0,0.0139753,"Missing"
I11-1125,P02-1006,0,0.0141151,"Missing"
I11-1125,W09-2209,0,0.0538955,"Missing"
I11-1125,N10-1113,0,0.225704,"Missing"
I11-1125,W99-0613,0,0.137963,"Missing"
I11-1125,feldman-etal-2006-cross,0,0.0520664,"Missing"
I11-1125,D07-1117,0,0.0416627,"Missing"
I11-1125,xue-etal-2008-annotating,0,0.233337,"Missing"
I11-1125,I05-1077,0,0.155041,"ally. There are useful contextual cues that can help determine the tense for the whole Chinese sentence or individual verbs, such as temporal adverbs or phrases, aspect auxiliary words and prepositions. For instance, in example (1a), the aspect particle “ (a particle word in Chinese, there is no literal translation to English)” and temporal phrase “AUc(several days ago)” together indicate the past tense of the sentence, and thus the correct translation of the sentence is “I arrived at Shanghai several days ago”. Most of the previous work on tense prediction (Li et al., 2004; Cao et al., 2004; Ye and Zhang, 2005; Lin, 2006) has been conducted using relatively small data sets (e.g., hundreds of 1116 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 1116–1124, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP Chinese sentences) and typically news article domain. They often require hand crafted rules in the systems. In this paper, we adopt a statistical classification framework for Chinese tense prediction. This study is different from previous work in that (a) we propose to utilize the parallel ChineseEnglish corpora to automatically generate reference"
I11-1125,W01-1305,0,0.0887181,"Missing"
I11-1145,P05-1033,0,0.668587,"i Zhaopeng Tu, Yang Liu, Qun Liu and Shouxun Lin Key Lab. of Intelligent Info. Processing Institute of Computing Technology Chinese Academy of Sciences P.O. Box 2704, Beijing 100190, China {tuzhaopeng,yliu,liuqun,sxlin}@ict.ac.cn Figure 1: (a) One alignment of a sentence pair; (b) another alignment of the same sentence pair. Here coreless dots denote wrong links. 1 Introduction Word alignment plays an important role in statistical machine translation (SMT). Most SMT systems, not only phrase-based models (Och and Ney, 2004; Koehn et al., 2003; Xiong et al., 2006), but also syntax-based models (Chiang, 2005; Liu et al., 2006; Galley et al., 2006; Huang et al., 2006; Shen et al., 2008), usually extract rules from word aligned corpora. However, these systems suffer from a major drawback: they only extract rules from 1-best alignments, which adversely affects the rule sets quality due to alignment mistakes. Typically, syntax-based models are more sensitive to word alignments because they care about inside (i.e., subtracted phrases). Figure 1(a) shows an alignment of a sentence pair. Since there is a wrong link (de, of), we could not extract many useful hierarchical rules such as (zhongguo X1 jingji"
I11-1145,P05-1066,0,0.271267,"Missing"
I11-1145,N03-1017,0,0.106149,"han economy ’s China fazhan economy ’s China Abstract zhongguo de jingji Zhaopeng Tu, Yang Liu, Qun Liu and Shouxun Lin Key Lab. of Intelligent Info. Processing Institute of Computing Technology Chinese Academy of Sciences P.O. Box 2704, Beijing 100190, China {tuzhaopeng,yliu,liuqun,sxlin}@ict.ac.cn Figure 1: (a) One alignment of a sentence pair; (b) another alignment of the same sentence pair. Here coreless dots denote wrong links. 1 Introduction Word alignment plays an important role in statistical machine translation (SMT). Most SMT systems, not only phrase-based models (Och and Ney, 2004; Koehn et al., 2003; Xiong et al., 2006), but also syntax-based models (Chiang, 2005; Liu et al., 2006; Galley et al., 2006; Huang et al., 2006; Shen et al., 2008), usually extract rules from word aligned corpora. However, these systems suffer from a major drawback: they only extract rules from 1-best alignments, which adversely affects the rule sets quality due to alignment mistakes. Typically, syntax-based models are more sensitive to word alignments because they care about inside (i.e., subtracted phrases). Figure 1(a) shows an alignment of a sentence pair. Since there is a wrong link (de, of), we could not e"
I11-1145,P06-1077,1,0.914482,"Yang Liu, Qun Liu and Shouxun Lin Key Lab. of Intelligent Info. Processing Institute of Computing Technology Chinese Academy of Sciences P.O. Box 2704, Beijing 100190, China {tuzhaopeng,yliu,liuqun,sxlin}@ict.ac.cn Figure 1: (a) One alignment of a sentence pair; (b) another alignment of the same sentence pair. Here coreless dots denote wrong links. 1 Introduction Word alignment plays an important role in statistical machine translation (SMT). Most SMT systems, not only phrase-based models (Och and Ney, 2004; Koehn et al., 2003; Xiong et al., 2006), but also syntax-based models (Chiang, 2005; Liu et al., 2006; Galley et al., 2006; Huang et al., 2006; Shen et al., 2008), usually extract rules from word aligned corpora. However, these systems suffer from a major drawback: they only extract rules from 1-best alignments, which adversely affects the rule sets quality due to alignment mistakes. Typically, syntax-based models are more sensitive to word alignments because they care about inside (i.e., subtracted phrases). Figure 1(a) shows an alignment of a sentence pair. Since there is a wrong link (de, of), we could not extract many useful hierarchical rules such as (zhongguo X1 jingji, China X1 economy"
I11-1145,D10-1053,0,0.130861,"Missing"
I11-1145,D09-1106,1,0.815845,"Missing"
I11-1145,P08-1010,0,0.0172715,"the two approaches. Criteria Total Rules Best Rule More Rules 9.2M 21.4M NIST03 Rules BLEU 590K 28.92 1.54M 29.07 NIST04 Rules BLEU 836K 31.77 1.97M 31.66 NIST05 Rules BLEU 677K 28.87 1.72M 29.02 Table 6: Comparison of rule tables learned from weighted matrices using different criterions. “Best Rule” denotes the rule table using the criteria described in Section 3.2, “More Rules” denotes the rule table using the criteria that retains all candidate target phrases that reach pruning threshold. and Huang (2008) and Tu et al. (2010) use forests instead of 1-best trees; Venugopal et al. (2003) and Deng et al. (2008) soft the alignment consistency constraint to extract more rules; Dyer et al. (2008) use word lattices instead of 1-best segmentations to generate more alignments for a sentence pair; Venugopal et al. (2008) use n-best alignments directly for rule extraction. To generate larger rule sets, de Gispert et al. (2010) extract hierarchical rules from alignment posterior probabilities. They concern how to extract larger rule sets using simple yet powerful hierarchical grammar, while we focus on whether weighted alignment matrix could overcome the alignment errors for different translation models (e.g"
I11-1145,D08-1022,0,0.184832,"Missing"
I11-1145,P08-1115,0,0.0604813,"s BLEU 590K 28.92 1.54M 29.07 NIST04 Rules BLEU 836K 31.77 1.97M 31.66 NIST05 Rules BLEU 677K 28.87 1.72M 29.02 Table 6: Comparison of rule tables learned from weighted matrices using different criterions. “Best Rule” denotes the rule table using the criteria described in Section 3.2, “More Rules” denotes the rule table using the criteria that retains all candidate target phrases that reach pruning threshold. and Huang (2008) and Tu et al. (2010) use forests instead of 1-best trees; Venugopal et al. (2003) and Deng et al. (2008) soft the alignment consistency constraint to extract more rules; Dyer et al. (2008) use word lattices instead of 1-best segmentations to generate more alignments for a sentence pair; Venugopal et al. (2008) use n-best alignments directly for rule extraction. To generate larger rule sets, de Gispert et al. (2010) extract hierarchical rules from alignment posterior probabilities. They concern how to extract larger rule sets using simple yet powerful hierarchical grammar, while we focus on whether weighted alignment matrix could overcome the alignment errors for different translation models (e.g. phrase-based, hierarchical phrase-based and tree-based models). They use phrase po"
I11-1145,P02-1038,0,0.0206047,"p(′ s|N U LL) × 0.36 ×   p(economy|jingji) × 1.0 ′ ′ Here the probability that economy translates a source NULL token is 0.0. 4 Experiments 4.1 Data Preparation Our experiments are on Chinese-English translation based on replications of hierarchical phrasebased system (Chiang, 2007) and tree-to-string system (Liu et al., 2006). We train a 4-gram language model on the Xinhua portion of GIGAWORD corpus using the SRI Language Modeling Toolkit (Stolcke, 2002) with modified KneserNey smoothing (Kneser and Ney, 1995). We optimize feature weights using the minimum error rate training algorithm (Och and Ney, 2002) on the NIST 2002 test set. We evaluate the translation quality using case-insensitive BLEU metric (Papineni et al., 2002) on the NIST 2003/2004/2005 test sets. To obtain weighted alignment matrices, we follow Venugopal et al. (2008) to produce n-best lists via GIZA++. We produce 20-best lists in two translation directions, then used “grow-diag-finaland” (Koehn et al., 2003) to all 20 × 20 bidirectional alignment pairs. We follow Liu et al. (2009) to use ps2t × pt2s as the probabilities of an alignment pair. Analogously, we abandon duplicate alignments that are produced from different alignmen"
I11-1145,P06-1121,0,0.373917,"and Shouxun Lin Key Lab. of Intelligent Info. Processing Institute of Computing Technology Chinese Academy of Sciences P.O. Box 2704, Beijing 100190, China {tuzhaopeng,yliu,liuqun,sxlin}@ict.ac.cn Figure 1: (a) One alignment of a sentence pair; (b) another alignment of the same sentence pair. Here coreless dots denote wrong links. 1 Introduction Word alignment plays an important role in statistical machine translation (SMT). Most SMT systems, not only phrase-based models (Och and Ney, 2004; Koehn et al., 2003; Xiong et al., 2006), but also syntax-based models (Chiang, 2005; Liu et al., 2006; Galley et al., 2006; Huang et al., 2006; Shen et al., 2008), usually extract rules from word aligned corpora. However, these systems suffer from a major drawback: they only extract rules from 1-best alignments, which adversely affects the rule sets quality due to alignment mistakes. Typically, syntax-based models are more sensitive to word alignments because they care about inside (i.e., subtracted phrases). Figure 1(a) shows an alignment of a sentence pair. Since there is a wrong link (de, of), we could not extract many useful hierarchical rules such as (zhongguo X1 jingji, China X1 economy).To alleviate this p"
I11-1145,P09-1104,0,0.0325122,"Missing"
I11-1145,2006.amta-papers.8,0,0.437981,"Lab. of Intelligent Info. Processing Institute of Computing Technology Chinese Academy of Sciences P.O. Box 2704, Beijing 100190, China {tuzhaopeng,yliu,liuqun,sxlin}@ict.ac.cn Figure 1: (a) One alignment of a sentence pair; (b) another alignment of the same sentence pair. Here coreless dots denote wrong links. 1 Introduction Word alignment plays an important role in statistical machine translation (SMT). Most SMT systems, not only phrase-based models (Och and Ney, 2004; Koehn et al., 2003; Xiong et al., 2006), but also syntax-based models (Chiang, 2005; Liu et al., 2006; Galley et al., 2006; Huang et al., 2006; Shen et al., 2008), usually extract rules from word aligned corpora. However, these systems suffer from a major drawback: they only extract rules from 1-best alignments, which adversely affects the rule sets quality due to alignment mistakes. Typically, syntax-based models are more sensitive to word alignments because they care about inside (i.e., subtracted phrases). Figure 1(a) shows an alignment of a sentence pair. Since there is a wrong link (de, of), we could not extract many useful hierarchical rules such as (zhongguo X1 jingji, China X1 economy).To alleviate this problem, a natural so"
I11-1145,J04-4002,0,0.0845271,"o-string model. fazhan economy ’s China fazhan economy ’s China Abstract zhongguo de jingji Zhaopeng Tu, Yang Liu, Qun Liu and Shouxun Lin Key Lab. of Intelligent Info. Processing Institute of Computing Technology Chinese Academy of Sciences P.O. Box 2704, Beijing 100190, China {tuzhaopeng,yliu,liuqun,sxlin}@ict.ac.cn Figure 1: (a) One alignment of a sentence pair; (b) another alignment of the same sentence pair. Here coreless dots denote wrong links. 1 Introduction Word alignment plays an important role in statistical machine translation (SMT). Most SMT systems, not only phrase-based models (Och and Ney, 2004; Koehn et al., 2003; Xiong et al., 2006), but also syntax-based models (Chiang, 2005; Liu et al., 2006; Galley et al., 2006; Huang et al., 2006; Shen et al., 2008), usually extract rules from word aligned corpora. However, these systems suffer from a major drawback: they only extract rules from 1-best alignments, which adversely affects the rule sets quality due to alignment mistakes. Typically, syntax-based models are more sensitive to word alignments because they care about inside (i.e., subtracted phrases). Figure 1(a) shows an alignment of a sentence pair. Since there is a wrong link (de,"
I11-1145,P02-1040,0,0.0830624,"oken is 0.0. 4 Experiments 4.1 Data Preparation Our experiments are on Chinese-English translation based on replications of hierarchical phrasebased system (Chiang, 2007) and tree-to-string system (Liu et al., 2006). We train a 4-gram language model on the Xinhua portion of GIGAWORD corpus using the SRI Language Modeling Toolkit (Stolcke, 2002) with modified KneserNey smoothing (Kneser and Ney, 1995). We optimize feature weights using the minimum error rate training algorithm (Och and Ney, 2002) on the NIST 2002 test set. We evaluate the translation quality using case-insensitive BLEU metric (Papineni et al., 2002) on the NIST 2003/2004/2005 test sets. To obtain weighted alignment matrices, we follow Venugopal et al. (2008) to produce n-best lists via GIZA++. We produce 20-best lists in two translation directions, then used “grow-diag-finaland” (Koehn et al., 2003) to all 20 × 20 bidirectional alignment pairs. We follow Liu et al. (2009) to use ps2t × pt2s as the probabilities of an alignment pair. Analogously, we abandon duplicate alignments that are produced from different alignment pairs. After these steps, there are 110 candidate alignments on average for each sentence pair. We obtained n-best lists"
I11-1145,P08-1066,0,0.158557,"Info. Processing Institute of Computing Technology Chinese Academy of Sciences P.O. Box 2704, Beijing 100190, China {tuzhaopeng,yliu,liuqun,sxlin}@ict.ac.cn Figure 1: (a) One alignment of a sentence pair; (b) another alignment of the same sentence pair. Here coreless dots denote wrong links. 1 Introduction Word alignment plays an important role in statistical machine translation (SMT). Most SMT systems, not only phrase-based models (Och and Ney, 2004; Koehn et al., 2003; Xiong et al., 2006), but also syntax-based models (Chiang, 2005; Liu et al., 2006; Galley et al., 2006; Huang et al., 2006; Shen et al., 2008), usually extract rules from word aligned corpora. However, these systems suffer from a major drawback: they only extract rules from 1-best alignments, which adversely affects the rule sets quality due to alignment mistakes. Typically, syntax-based models are more sensitive to word alignments because they care about inside (i.e., subtracted phrases). Figure 1(a) shows an alignment of a sentence pair. Since there is a wrong link (de, of), we could not extract many useful hierarchical rules such as (zhongguo X1 jingji, China X1 economy).To alleviate this problem, a natural solution is to extract"
I11-1145,J10-4005,0,0.0532568,"s the complement. Note that the probabilities of “Shared” rules are different for the two approaches. Criteria Total Rules Best Rule More Rules 9.2M 21.4M NIST03 Rules BLEU 590K 28.92 1.54M 29.07 NIST04 Rules BLEU 836K 31.77 1.97M 31.66 NIST05 Rules BLEU 677K 28.87 1.72M 29.02 Table 6: Comparison of rule tables learned from weighted matrices using different criterions. “Best Rule” denotes the rule table using the criteria described in Section 3.2, “More Rules” denotes the rule table using the criteria that retains all candidate target phrases that reach pruning threshold. and Huang (2008) and Tu et al. (2010) use forests instead of 1-best trees; Venugopal et al. (2003) and Deng et al. (2008) soft the alignment consistency constraint to extract more rules; Dyer et al. (2008) use word lattices instead of 1-best segmentations to generate more alignments for a sentence pair; Venugopal et al. (2008) use n-best alignments directly for rule extraction. To generate larger rule sets, de Gispert et al. (2010) extract hierarchical rules from alignment posterior probabilities. They concern how to extract larger rule sets using simple yet powerful hierarchical grammar, while we focus on whether weighted alignm"
I11-1145,P03-1041,0,0.0272029,"ed” rules are different for the two approaches. Criteria Total Rules Best Rule More Rules 9.2M 21.4M NIST03 Rules BLEU 590K 28.92 1.54M 29.07 NIST04 Rules BLEU 836K 31.77 1.97M 31.66 NIST05 Rules BLEU 677K 28.87 1.72M 29.02 Table 6: Comparison of rule tables learned from weighted matrices using different criterions. “Best Rule” denotes the rule table using the criteria described in Section 3.2, “More Rules” denotes the rule table using the criteria that retains all candidate target phrases that reach pruning threshold. and Huang (2008) and Tu et al. (2010) use forests instead of 1-best trees; Venugopal et al. (2003) and Deng et al. (2008) soft the alignment consistency constraint to extract more rules; Dyer et al. (2008) use word lattices instead of 1-best segmentations to generate more alignments for a sentence pair; Venugopal et al. (2008) use n-best alignments directly for rule extraction. To generate larger rule sets, de Gispert et al. (2010) extract hierarchical rules from alignment posterior probabilities. They concern how to extract larger rule sets using simple yet powerful hierarchical grammar, while we focus on whether weighted alignment matrix could overcome the alignment errors for different"
I11-1145,2008.amta-papers.18,0,0.410017,"ord aligned corpora. However, these systems suffer from a major drawback: they only extract rules from 1-best alignments, which adversely affects the rule sets quality due to alignment mistakes. Typically, syntax-based models are more sensitive to word alignments because they care about inside (i.e., subtracted phrases). Figure 1(a) shows an alignment of a sentence pair. Since there is a wrong link (de, of), we could not extract many useful hierarchical rules such as (zhongguo X1 jingji, China X1 economy).To alleviate this problem, a natural solution is to extract rules from nbest alignments (Venugopal et al., 2008). However, using n-best alignments still face two major challenges. First, n-best alignments have to be processed individually although they share many links, see (zhongguo, China) and (jingji, economy) in Figure 1. Second, regardless of probabilities of links in each alignment, numerous wrong rule would be extracted from n-best alignments. For example, a wrong rule (X1 de jingji, of X1 ’s economy) would be extracted from the alignment in Figure 1(a). Since Liu et al. (2009) show that weighted alignment matrix provides an elegant solution to these two drawbacks, we apply it to the hierarchical"
I11-1145,P06-1066,1,0.872693,"fazhan economy ’s China Abstract zhongguo de jingji Zhaopeng Tu, Yang Liu, Qun Liu and Shouxun Lin Key Lab. of Intelligent Info. Processing Institute of Computing Technology Chinese Academy of Sciences P.O. Box 2704, Beijing 100190, China {tuzhaopeng,yliu,liuqun,sxlin}@ict.ac.cn Figure 1: (a) One alignment of a sentence pair; (b) another alignment of the same sentence pair. Here coreless dots denote wrong links. 1 Introduction Word alignment plays an important role in statistical machine translation (SMT). Most SMT systems, not only phrase-based models (Och and Ney, 2004; Koehn et al., 2003; Xiong et al., 2006), but also syntax-based models (Chiang, 2005; Liu et al., 2006; Galley et al., 2006; Huang et al., 2006; Shen et al., 2008), usually extract rules from word aligned corpora. However, these systems suffer from a major drawback: they only extract rules from 1-best alignments, which adversely affects the rule sets quality due to alignment mistakes. Typically, syntax-based models are more sensitive to word alignments because they care about inside (i.e., subtracted phrases). Figure 1(a) shows an alignment of a sentence pair. Since there is a wrong link (de, of), we could not extract many useful hi"
I11-1145,P05-1059,0,0.0789146,"Missing"
I11-1145,J07-2003,0,\N,Missing
I11-1164,P08-1081,0,0.0597041,"orums often contain useful information, such as answers to questions. When people use search engines to search the forum for answers, the returned threads can be of very low quality in terms of providing informative answers. Some threads are either long conversations without any final conclusive answers or contain answers that do not work. This is especially the case in technical forums. There has been some work on extracting information from online forums. For example, Cong et al. (Cong et al., 2008) used labeled sequential patterns to detect question sentences in online forums. Ding et al. (Ding et al., 2008) used Conditional Random Fields to extract context of questions for answer detection. Huang et al. (Huang et al., 2007) used SVM to automatically extract and rank title-reply pairs from online discussion forums for chatbot knowledge. These previous studies provide a relatively good foundation for answer finding and extraction. In online environments like mailing list or forums, question 2 Data Collection and Annotation We created our own data collection and annotation.1 We crawled 20,000 threads from an active online forum (Oracle database support forum – general section). As an initial study,"
I13-1154,D09-1156,0,0.0266905,"e attributes in each block. Intuitively, more frequent a template is found in a weak semi-block, more likely a string extracted by that template is an attribute. Based on this idea, templates with higher frequencies will have higher priority than those with the lower frequencies when extracting attributes. After we run through all the pages of the site, we get a collection of templates and attributes. Then we rank them to obtain site-level knowledge. 2.1.3 Ranker To rank obtained templates and attributes to get site-level knowledge, we use the graph walk based technique (Wang and Cohen, 2007)(Wang and Cohen, 2009). In the graph (Figure 2), attributes in initial attribute list are used as seeds. And these seeds are used to match the attributes in weak semi-block of a document (or a page) to learn templates. Then these templates are used to extract new attributes from the weak semi-block of a document (or a page). Intuitively, we consider that seeds appearing frequently are with high quality, templates derived by these seeds are tend to have good quality, and documents containing these seeds and templates are also deemed as high quality. Inversely, high quality documents also produce high quality attribu"
I13-1154,P09-1070,0,0.0749009,"Missing"
J10-3002,P06-1002,0,0.0788809,"Missing"
J10-3002,N06-1013,0,0.0974408,"Missing"
J10-3002,H05-1024,0,0.0373228,"Missing"
J10-3002,H05-1009,0,0.356028,"Missing"
J10-3002,P06-1009,0,0.0816815,"Missing"
J10-3002,J93-2003,0,0.141447,"Missing"
J10-3002,P03-1012,0,0.104375,"Missing"
J10-3002,P06-2014,0,0.0374391,"Missing"
J10-3002,P05-1033,0,0.227161,"Missing"
J10-3002,J07-2003,0,0.219245,"Missing"
J10-3002,E09-1020,0,0.0767397,"Missing"
J10-3002,P06-1097,0,0.0306278,"Missing"
J10-3002,D07-1006,0,0.0780369,"Missing"
J10-3002,J07-3002,0,0.288262,"Missing"
J10-3002,P06-1121,0,0.146113,"Missing"
J10-3002,D08-1011,0,0.0449614,"Missing"
J10-3002,D07-1091,0,0.0407599,"Missing"
J10-3002,N03-1017,0,0.0433454,"Missing"
J10-3002,N06-1015,0,0.0264963,"Missing"
J10-3002,N06-1014,0,0.0937511,"Missing"
J10-3002,P05-1057,1,0.623793,"Missing"
J10-3002,P06-1077,1,0.887335,"Missing"
J10-3002,W06-1606,0,0.0510981,"Missing"
J10-3002,W05-0809,0,0.0666533,"Missing"
J10-3002,J00-2004,0,0.116772,"Missing"
J10-3002,W03-0301,0,0.0158309,"Missing"
J10-3002,H05-1011,0,0.0700406,"Missing"
J10-3002,P06-1065,0,0.333202,"Missing"
J10-3002,W08-0303,0,0.0323884,"Missing"
J10-3002,P03-1021,0,0.0209775,"Missing"
J10-3002,P02-1038,0,0.204882,"Missing"
J10-3002,J03-1002,0,0.0439931,"Missing"
J10-3002,J04-4002,0,0.376267,"Missing"
J10-3002,P05-1034,0,0.0691884,"Missing"
J10-3002,H05-1010,0,0.110155,"Missing"
J10-3002,C96-2141,0,0.848696,"Missing"
J10-3002,P07-1040,0,\N,Missing
J10-3002,P09-1104,0,\N,Missing
K17-3005,P13-1104,0,0.0329559,"a. We scale PMI 1 Vietnamese requires word segmentation because white spaces occur both inter- and intra-words. When segmenting Vietnamese, white space-separated tokens are used as inputs, rather than characters as in Chinese and Japanese. In addition, we don’t consider Korean here since the Korean input texts have already been segmented in the corpus provided by the task. Word Segmentation We develop our own word segmentation models particularly for languages which do not have ex53 2.3 The transition-based dependency parsing algorithm with a list-based arc-eager transition system proposed by Choi and McCallum (2013) is used in our parser. We base our parser mainly on the Stack-LSTM model proposed by Dyer et al. (2015), where three Stack-LSTMs are utilized to incrementally obtain the representations of the buffer β, the stack σ and the transition action sequence A. In addition, a dependency-based Recursive Neural Network (RecNN) is used to compute the partially constructed tree representation. However, compared with the arc-standard algorithm (Nivre, 2004) used by Dyer et al. (2015), the list-based arc-eager transition system has an extra component in each configuration, i.e., the deque δ. So we use an ad"
K17-3005,P16-2006,0,0.0510368,"sively combining head-modifier pairs. Whereas in Tree-LSTM, a head is combined with all of its modifiers simultaneously in each LSTM unit. However, our implementation of Tree-LSTM is different from the conventional one. Unlike traditional bottom-up Tree-LSTMs in which each head and all of its modifiers are combined simultaneously, the modifiers are found incrementally during our parsing procedure. Therefore, we propose Incremental Tree-LSTM, which obtains sub-tree representations incrementally. To be more specific, each time a dependency arc is generated, 2016; Kiperwasser and Goldberg, 2016; Cross and Huang, 2016), thus called Bi-LSTM Subtraction. The forward and backward subtractions are calculated independently, i.e., bf = hf (l)−hf (f ) and bb = bb (f ) − bb (l), where hf (f ) and hf (l) are the hidden vectors of the first and the last words in the forward LSTM, hb (f ) and hb (l) are the hidden vectors of the first and the last words in the backward LSTM. Then bf and bb are concatenated as the buffer representation. As illustrated in Figure 5, the forward and backward subtractions for the buffer are bf = hf (here) − hf (nice) and bb = hb (nice) − hb (here) respectively. 55 Target Source we collect"
K17-3005,P15-1033,0,0.103906,". When segmenting Vietnamese, white space-separated tokens are used as inputs, rather than characters as in Chinese and Japanese. In addition, we don’t consider Korean here since the Korean input texts have already been segmented in the corpus provided by the task. Word Segmentation We develop our own word segmentation models particularly for languages which do not have ex53 2.3 The transition-based dependency parsing algorithm with a list-based arc-eager transition system proposed by Choi and McCallum (2013) is used in our parser. We base our parser mainly on the Stack-LSTM model proposed by Dyer et al. (2015), where three Stack-LSTMs are utilized to incrementally obtain the representations of the buffer β, the stack σ and the transition action sequence A. In addition, a dependency-based Recursive Neural Network (RecNN) is used to compute the partially constructed tree representation. However, compared with the arc-standard algorithm (Nivre, 2004) used by Dyer et al. (2015), the list-based arc-eager transition system has an extra component in each configuration, i.e., the deque δ. So we use an additional Stack-LSTM to learn the representation of δ. More importantly, we introduce two LSTM-based tech"
K17-3005,N13-1073,0,0.0392245,"nt data. Target model Unified model Cross-lingual word embeddings train BiTexts Source fine-tuning Target 4.1.2 Figure 7: The cross-lingual transfer approach. Target Source bxr tr & ug & kk kmr fa sme fi ftb & fi We use the provided 100-dimensional multilingual word embeddings5 in our tokenization, POS tagging and parsing models, and use the Wikipedia and CommonCrawl data for training Brown clusters. The number of clusters is set to 256. For cross-lingual transfer parsing of lowresource languages, we use parallel data from OPUS to derive cross-lingual word embeddings.6 The fast align toolkit (Dyer et al., 2013) is used for word alignment.7 We use the Dynet toolkit for the implementation of all our neural models.8 hsb cs Table 2: Cross-lingual delexicalized transfer settings for surprise languages. After that, target language-specific parsers are obtained through fine-tuning on their own treebanks. Figure 7 illustrates the flow of our transfer approach. For the surprise languages in the final test phase, we use the transfer settings in Table 2. We use multi-source delexicalized transfer for surprise language parsing, considering that bilingual parallel data which is required for obtaining crosslingua"
K17-3005,P15-1119,1,0.848142,"raining treebank of each domain, to obtain target domain-specific parsers. In practice, for each language considered here, we treat the largest treebank as our source-domain data, and the rest as target-domain data. Only target-domain models are fine-tuned from the unified parser, while the source-domain parser is trained separately using the source treebank alone. For the new parallel test sets in test phase, we simply use the model trained on source-domain data, without any assumption on the target domain. 3.2 qa en resource, and employ the cross-lingual model transfer approach described in Guo et al. (2015, 2016) to benefit from existing resource-rich languages. The low-resource languages here include Ukrainian (uk), Irish (ga), Uyghur (ug) and Kazakh (kk). We determine their source language (treebank) according to the language families they belong to and their linguistic typological similarity. Specifically, the transfer setting is shown in Table 1. The transfer approach is similar to crossdomain transfer as described above, with one important difference. Here, we use cross-lingual word embeddings and Brown clusters derived by the robust projection approach (Guo et al., 2015) when training the"
K17-3005,L16-1262,0,0.0932897,"Missing"
K17-3005,L16-1680,0,0.0624147,"Missing"
K17-3005,N12-1052,0,0.0885217,"Missing"
K17-3005,P15-1150,0,0.128276,"Missing"
K17-3005,N16-1064,0,0.0315829,"Missing"
K17-3005,P15-1032,0,0.0259162,"or evaluation. Eliyahu Kiperwasser and Yoav Goldberg. 2016. Simple and accurate dependency parsing using bidirectional lstm feature representations. TACL 4:313– 327. 6 Percy Liang. 2005. Semi-supervised learning for natural language. Master thesis, Massachusetts Institute of Technology. Credits There are a few references we would like to give proper credit, especially to data providers: the core Universal Dependencies paper from LREC 2016 (Nivre et al., 2016), the UD version 2.0 datasets (Nivre et al., 2017b,a), the baseline UDPipe models (Straka et al., 2016b), the baseline SyntaxNet models (Weiss et al., 2015) and the evaluation platform TIRA (Potthast et al., 2014). Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient estimation of word representations in vector space. International Conference on Learning Representations (ICLR) Workshop . Joakim Nivre. 2004. Incrementality in deterministic dependency parsing. In Proc. of the Workshop on Incremental Parsing: Bringing Engineering and Cognition Together. pages 50–57. Acknowledgments ˇ Joakim Nivre, Zeljko Agi´c, Lars Ahrenberg, Lene Antonsen, Maria Jesus Aranzabe, Masayuki Asahara, Luma Ateyah, Mohammed Attia, Aitziber Atutxa, Ele"
K17-3005,Q16-1023,0,\N,Missing
K17-3015,W05-1506,0,0.13171,"Missing"
K17-3015,P10-1001,0,0.0224985,"Missing"
K17-3015,N16-1030,0,0.159287,"Missing"
K17-3015,P05-1012,0,0.289454,"Missing"
K17-3015,Q13-1002,0,0.0447955,"Missing"
K17-3015,K17-3001,0,0.0848465,"Missing"
K17-3015,Q14-1004,0,\N,Missing
K18-1018,D17-1047,0,0.753896,"left context and the right context, which were determined by each other. Huang et al. (2018) introduced an attentionover-attention network modeled the aspects and sentences in a joint way, which jointly learned the representations for aspects and sentences and automatically focused on the important parts in sentences. In addition, other current researches focus on capturing more accurate information by adopting multiple attentions. Tang et al. (2016b) designed a deep memory network which consisted of multiple computational layers, each of which was an attention model over an external memory. Chen et al. (2017) proposed a recurrent attention based network which introduced multiple attention mechanisms. 182 rh r1 r2 r3 rn rt 1t x1 x2 x3  2t 3t  mt xn Figure 1: The architecture of the proposed model. Compared with the above models, we introduce position embeddings when modelling the sentence to generate position-aware representations; on this basis, we propose a hierarchical attention based fusion mechanism to fuse the clues of aspects and the contexts. 3 (Mikolov et al., 2013), where is the dimension of the word embeddings and is the size of word vocabulary. As described in Section 1, we also int"
K18-1018,P11-1016,0,0.42469,"Missing"
K18-1018,S14-2076,0,0.326236,"e-grained task in sentiment analysis, which aims to identify the sentiment polarity (i.e., negative, neutral, or positive) of a specific opinion target expressed in a comment/review by a reviewer. For example, given a sentence “The price is reasonable although the service is poor”, the sentiment polarity for aspects “price” and “service” are positive and negative respectively. Traditional methods for aspect-level sentiment analysis mainly focus on designing a set of features (such as bag-of-words, sentiment lexicons, and linguistic features) to train a classifier for sentiment classification (Kiritchenko et al., 2014; 181 Proceedings of the 22nd Conference on Computational Natural Language Learning (CoNLL 2018), pages 181–189 c Brussels, Belgium, October 31 - November 1, 2018. 2018 Association for Computational Linguistics encoding the sentence. We argue that the position of a candidate aspect is important for the sentence modelling. For instance, consider the sentence “I bought a mobile phone, its camera is wonderful but the battery life is a bit short”. For the candidate aspect “battery life”, “wonderful” and “short” are both likely to be considered as its adjunct word. In this case, if we encode the po"
K18-1018,D14-1162,0,0.0839461,"al attention based fusion layer and the output layer. 3.1 3.2 Bi-GRU Based Sentence Encoder In this paper, we apply a Bi-GRU (Cho et al., 2014) to learn a more abstract representation of the sentence. In the following, we describe our encoding layer in detail. In the encoding phase, we first transform each token in the sentence into a real-valued vector using the concatenation of the following vectors: Input Embedding The embedding layer has two parts: the word embeddings and the position embeddings. Let ∈ ℝ × be a word embedding lookup table generated by an unsupervised method such as GloVe (Pennington et al., 2014) or CBOW  The pre-trained word embeddings of .  The position embeddings of : the relevant position between the i-th word and the target is defined as the relative offset with respect 183 to the target and calculated by the follow equation: − − − 0 &lt; ≥ > + + ≥ ≥ representation of the aspect. The source2* attention is inspired by the related research of selfattention network (Shen et al., 2017). First, we introduce a score function by taking the word embeddings of each word in target as inputs. (1) = where k is the index of the first word of target, m is the length of the target, n is the leng"
K18-1018,C16-1311,0,0.562577,"the sentence modelling. For instance, consider the sentence “I bought a mobile phone, its camera is wonderful but the battery life is a bit short”. For the candidate aspect “battery life”, “wonderful” and “short” are both likely to be considered as its adjunct word. In this case, if we encode the position information into the representation of each word effectively, we would have more confidence in concluding that the “short” is the adjunct word of “battery life” and predict the sentiment as negative. Then, the next problem is how to introduce the position information. In some previous works (Tang et al., 2016b; Chen et al,. 2017), they weighted the representation of each word according to the position, and the words close to the aspect could be paid more attention. However, this operation is not always reasonable and sometimes the adjunct word may be far away from the target word. Thus, we introduce position embeddings when modelling the sentence and further generate the positionaware representations. In other words, the position information is considered as a kind of features and embedded into position embeddings. The model will learn to exploit both of the semantic information and the position c"
K18-1018,D16-1021,0,0.60368,"the sentence modelling. For instance, consider the sentence “I bought a mobile phone, its camera is wonderful but the battery life is a bit short”. For the candidate aspect “battery life”, “wonderful” and “short” are both likely to be considered as its adjunct word. In this case, if we encode the position information into the representation of each word effectively, we would have more confidence in concluding that the “short” is the adjunct word of “battery life” and predict the sentiment as negative. Then, the next problem is how to introduce the position information. In some previous works (Tang et al., 2016b; Chen et al,. 2017), they weighted the representation of each word according to the position, and the words close to the aspect could be paid more attention. However, this operation is not always reasonable and sometimes the adjunct word may be far away from the target word. Thus, we introduce position embeddings when modelling the sentence and further generate the positionaware representations. In other words, the position information is considered as a kind of features and embedded into position embeddings. The model will learn to exploit both of the semantic information and the position c"
K18-1018,S14-2036,0,0.0473269,"domain. The experimental results demonstrate that the proposed approach is effective for aspect-level sentiment classification, and it outperforms state-of-the-art approaches with remarkable gains. We make our source code public at https://github.com/DUTLiuYang/Aspect-Sentiment-Analysis. 2 Related Work Many approaches have been proposed to address the problem of aspect-level sentiment analysis. Traditional approaches to this task normally exploited a diverse set of strategies to convert classification clues (i.e., sentiment lexicons, bagof-word) into feature vectors (Kiritchenko et al., 2014; Wagner et al., 2014; Vo and Zhang, 2015). Although these methods have achieved comparable performance, their models highly depend on the effectiveness of the handcraft features which are labor intensive and lack generalization. Therefore, many neural network based models have been proposed in recent years. And most current state-of-the-art works in aspect-based sentiment analysis pay more attention to fusing the information of the targets and contextual words. Wang et al., (2016) proposed an attention based LSTM which introduced the aspect clues by concatenating the aspect embeddings and the word representations"
K18-1018,D16-1058,0,0.105725,"t of strategies to convert classification clues (i.e., sentiment lexicons, bagof-word) into feature vectors (Kiritchenko et al., 2014; Wagner et al., 2014; Vo and Zhang, 2015). Although these methods have achieved comparable performance, their models highly depend on the effectiveness of the handcraft features which are labor intensive and lack generalization. Therefore, many neural network based models have been proposed in recent years. And most current state-of-the-art works in aspect-based sentiment analysis pay more attention to fusing the information of the targets and contextual words. Wang et al., (2016) proposed an attention based LSTM which introduced the aspect clues by concatenating the aspect embeddings and the word representations. Tang et al. (2016a) developed two target-dependent LSTM to model the left and right contexts with target, where the target information was automatically taken into account. Tay et al. (2017) proposed an attention based LSTM which learned to attend based on associative relationships between sentence words and aspect by adopting circular convolution and circular correlation. Ma et al. (2017) proposed an interactive attention network which interactively learned"
L16-1159,P02-1040,0,0.111085,"Missing"
L16-1159,A00-2002,0,0.236834,"Missing"
L16-1159,D08-1035,0,\N,Missing
L18-1700,P03-1036,0,0.100019,"Missing"
L18-1700,W15-0812,0,0.0244584,"ﯔ ﺷﻴﺎﯞﭘﯩﯔDeng Xiaoping) is often misspelled as  ﺩﯦﯔ ﺷﻴﺎﯞﭘﯩﯔeven including its Wikipedia title; the Wikipedia title of “( ﺟﯘﯕﮕﻮChina”) is also misspelled as ( ﺋﯜﺭﯛﻣﭽﻰ ;ﺟﻮﯕﮕﻮUrumqi) is often misspelled as ﺋﯜﺭﯛﻣﭽﻰ. Up to date there are no effective Uyghur spelling correction techniques available yet. 3..9 Name Definition and Annotation Challenges In the past two decades, many efforts have been made at defining the name tagging task, including Message Understanding Conference (MUC) (Grishman and Sundheim, 1996), Automatic Content Extraction (ACE) 6 , and Entity, Relation and Event (ERE) (Song et al., 2015). However, there are many open issues which may cause confusions for both human annotators and systems. In particular, for low-resource languages like Uyghur, it’s also challenging to train native speakers to follow a long annotation guideline 4424 6 http://www.itl.nist.gov/iad/mig/tests/ace/ Table 5: Long Nested Organizations Nested Organization [ORG [ ﻣﯘﺳﯘﻟﻤﺎﻧﭽﻪ ﻳﯩﻤﻪﻛﻠﯩﻚ ﺳﺎﻧﺎﺋﯩﺘﻰ ﮔﯘﺭﯗﮬﻰ ﭼﻪﻛﻠﯩﻚ ﺷﯩﺮﻛﯩﺘﻰPER [ ]ﺋﺎﺭﻣﺎﻥGPE ]]ﺷﯩﻨﺠﺎﯓ [ORG  ﺗﯧﺨﻨﯩﻜﺎ ﺗﻪﺭﻩﻗﻘﯩﻴﺎﺕ ﭼﻪﻛﻠﯩﻚ ﺷﯩﺮﻛﯩﺘﻰ- [ ﺑﯩﺌﻮ ﭘﻪﻥPER]ﺋﺎﺑﯩﺪﻩ [GPE ] ]ﺷﯩﻨﺠﺎﯓ [ORG [ORG [ ﺋﯧﻜﻮﻟﻮﮔﯩﻴﻪﺳﻰ ﯞﻩ ﺟﯘﻏﺮﺍﭘﯩﻴﻪ ﺗﻪﺗﻘﯩﻘﺎﺕ ﻣﻪﺭﻛﯩﺰﯨﻨﯩﯔGPE [ ]]ﺷﯩﻨﺠﺎﯓOR"
L18-1700,C96-1079,0,0.684694,"n Uyghur texts in both formal and informal genres are misspelled. For example, the common person name ( ﺩﯨﯔ ﺷﻴﺎﯞﭘﯩﯔDeng Xiaoping) is often misspelled as  ﺩﯦﯔ ﺷﻴﺎﯞﭘﯩﯔeven including its Wikipedia title; the Wikipedia title of “( ﺟﯘﯕﮕﻮChina”) is also misspelled as ( ﺋﯜﺭﯛﻣﭽﻰ ;ﺟﻮﯕﮕﻮUrumqi) is often misspelled as ﺋﯜﺭﯛﻣﭽﻰ. Up to date there are no effective Uyghur spelling correction techniques available yet. 3..9 Name Definition and Annotation Challenges In the past two decades, many efforts have been made at defining the name tagging task, including Message Understanding Conference (MUC) (Grishman and Sundheim, 1996), Automatic Content Extraction (ACE) 6 , and Entity, Relation and Event (ERE) (Song et al., 2015). However, there are many open issues which may cause confusions for both human annotators and systems. In particular, for low-resource languages like Uyghur, it’s also challenging to train native speakers to follow a long annotation guideline 4424 6 http://www.itl.nist.gov/iad/mig/tests/ace/ Table 5: Long Nested Organizations Nested Organization [ORG [ ﻣﯘﺳﯘﻟﻤﺎﻧﭽﻪ ﻳﯩﻤﻪﻛﻠﯩﻚ ﺳﺎﻧﺎﺋﯩﺘﻰ ﮔﯘﺭﯗﮬﻰ ﭼﻪﻛﻠﯩﻚ ﺷﯩﺮﻛﯩﺘﻰPER [ ]ﺋﺎﺭﻣﺎﻥGPE ]]ﺷﯩﻨﺠﺎﯓ [ORG  ﺗﯧﺨﻨﯩﻜﺎ ﺗﻪﺭﻩﻗﻘﯩﻴﺎﺕ ﭼﻪﻛﻠﯩﻚ ﺷﯩﺮﻛﯩﺘﻰ- [ ﺑﯩﺌﻮ ﭘﻪﻥPER]ﺋﺎﺑﯩﺪ"
L18-1700,K16-1022,0,0.0248593,"rtment, the power supply and so on, have left for the affected Atchan township. ”, it’s difficult to decide whether “the health department” and “the fire department” are names or nominals. 4. Conclusions and Future Work We conducted a thorough study on both quantitative and qualitative analysis on a wide variety of errors from a stateof-the-art Uyghur name tagger. We also discussed possible solutions for the remaining challenges. Recently there is a trend in the community to push the rapid development of language universal techniques for name tagging (Zhang et al., 2016; Littell et al., 2016; Tsai et al., 2016; Pan et al., 2017). These methods have achieved some success at setting up baseline name taggers with reasonable performance. However, based on the Uyghur case study in this paper we can clearly see that most of the remaining challenges are specific to the target language, and thus we will need to embrace language-specific resources and knowledge in order to break the performance ceiling. We hope that the detailed analysis we did in this paper can shed a light on future efforts to focus on Uyghur resource development instead of simply borrowing language-independent features and machine learni"
L18-1700,N16-1030,0,0.065419,"alysis because of two reasons: (1) it achieves top performance at NIST LoreHLT2016 Evaluation 3 so it represents state-of-the-art; (2) unlike most previous work, this system has already exploited extensive language-specific features. We briefly describe the system as follows. 2..1 Learning Model This system considers name tagging as a sequence labeling problem, to tag each token in a sentence as the Beginning (B), Inside (I) or Outside (O) of a name mention with a certain type (Person (PER), Organization (ORG), Geo-political Entity (GPE), and Location (LOC)). Following a framework similar to (Lample et al., 2016). The architecture consists of Bi-directional Long Short-Term Memory and Conditional Random Fields (CRFs) network. After processing through the Bi-LSTM networks, each token in a sentence sequence obtains a feature embedding that captures left and 4421 3 https://www.nist.gov/itl/iad/mig/lorehlt16-evaluations right context information, which is then fed into the CRF networks. • If a name includes a URL link, remove the URL. • Label places that don’t have governing organizations as Location(LOC), including all continents, ﺋﻮﺗﺘﯘﺭﺍ ﺷﻪﺭﻕ (Latin: ottura sheriq, English: Middle East), etc. • Label p"
L18-1700,C16-1095,0,0.0238989,"police, the fire department, the power supply and so on, have left for the affected Atchan township. ”, it’s difficult to decide whether “the health department” and “the fire department” are names or nominals. 4. Conclusions and Future Work We conducted a thorough study on both quantitative and qualitative analysis on a wide variety of errors from a stateof-the-art Uyghur name tagger. We also discussed possible solutions for the remaining challenges. Recently there is a trend in the community to push the rapid development of language universal techniques for name tagging (Zhang et al., 2016; Littell et al., 2016; Tsai et al., 2016; Pan et al., 2017). These methods have achieved some success at setting up baseline name taggers with reasonable performance. However, based on the Uyghur case study in this paper we can clearly see that most of the remaining challenges are specific to the target language, and thus we will need to embrace language-specific resources and knowledge in order to break the performance ceiling. We hope that the detailed analysis we did in this paper can shed a light on future efforts to focus on Uyghur resource development instead of simply borrowing language-independent features"
L18-1700,N16-1029,1,0.843536,"alth department, the police, the fire department, the power supply and so on, have left for the affected Atchan township. ”, it’s difficult to decide whether “the health department” and “the fire department” are names or nominals. 4. Conclusions and Future Work We conducted a thorough study on both quantitative and qualitative analysis on a wide variety of errors from a stateof-the-art Uyghur name tagger. We also discussed possible solutions for the remaining challenges. Recently there is a trend in the community to push the rapid development of language universal techniques for name tagging (Zhang et al., 2016; Littell et al., 2016; Tsai et al., 2016; Pan et al., 2017). These methods have achieved some success at setting up baseline name taggers with reasonable performance. However, based on the Uyghur case study in this paper we can clearly see that most of the remaining challenges are specific to the target language, and thus we will need to embrace language-specific resources and knowledge in order to break the performance ceiling. We hope that the detailed analysis we did in this paper can shed a light on future efforts to focus on Uyghur resource development instead of simply borrowing languag"
L18-1700,N15-1119,1,0.794018,"n in China”) should be tagged as one single organization mention. • Boundary extension: if a name doesn’t include any suffix and its right contextual word is a name designator, then extend the name boundary to include the designator. • Cross-genre propagation: when the types of the same name mention are conflicting between formal genres and informal genres, propagate the types from formal genres to informal genres. • Poster names: Extract all poster names from the original thread structures, and identify all mentions in the posts, posters, and Twitter user names. Apply English entity linking (Pan et al., 2015) to each string after ‘@’ or ‘#’, and if it’s linkable and its type can be inferred based on KB properties, then assign the type; otherwise tag it as PER. 2..2 Pre-processing The system starts with segmenting a document into tokens based on 50 punctuations pulled from Uyghur grammar books ((translated by Anne Lee), 2003; Zakir, 2007; Engesæth et al., 2009). Since Uyghur is a morphologically rich language, a set of name related suffixes is also extracted from grammar books, Wikitionary 4 and WALS 5 , for stemming and feature encoding. 2..3 Features Typical implicit linguistic features including"
L18-1700,P17-1178,1,0.844656,"upply and so on, have left for the affected Atchan township. ”, it’s difficult to decide whether “the health department” and “the fire department” are names or nominals. 4. Conclusions and Future Work We conducted a thorough study on both quantitative and qualitative analysis on a wide variety of errors from a stateof-the-art Uyghur name tagger. We also discussed possible solutions for the remaining challenges. Recently there is a trend in the community to push the rapid development of language universal techniques for name tagging (Zhang et al., 2016; Littell et al., 2016; Tsai et al., 2016; Pan et al., 2017). These methods have achieved some success at setting up baseline name taggers with reasonable performance. However, based on the Uyghur case study in this paper we can clearly see that most of the remaining challenges are specific to the target language, and thus we will need to embrace language-specific resources and knowledge in order to break the performance ceiling. We hope that the detailed analysis we did in this paper can shed a light on future efforts to focus on Uyghur resource development instead of simply borrowing language-independent features and machine learning methods which we"
N03-3007,P92-1008,0,0.0675452,"Missing"
N03-3007,J99-4003,0,0.0672527,"Missing"
N06-2021,J00-3003,0,0.0331651,"mplemented using the SRILM toolkit (Stolcke, 2002). 3.2 Maximum Entropy (Maxent) Classifier Sentence 1 Sentence 2 Sentence 3 ………… Sentence 1 Sentence 2 ………… where O is the observation sequence, in which Oi corresponds to one speaker turn. If we assume what a speaker says is only dependent on his or her role, then: Y P (O|R) = P (Oi |Ri ). (2) A Maxent model estimates the conditional probability: P (Ri |O) = Figure 1: A graphical representation of the HMM approach for speaker role labeling. This is a simple first order HMM. The HMM has been widely used in many tagging problems. Stolcke et al. (Stolcke et al., 2000) used it for dialog act classification, where each utterance (or dialog act) is used as the observation. In speaker role detection, the observation is composed of a much longer word sequence, i.e., the entire speech from one speaker. Figure 1 shows the graphical representation of the HMM for speaker role identification, in which the states are the speaker roles, and the observation associated with a state consists of the utterances from a speaker. The most likely role sequence ˆ is: R ˆ = argmax P (R|O) = argmax P (O|R)P (R), (1) R R R 82 X 1 exp( λk gk (Ri , O)), Zλ (O) (3) k where Zλ (O) is"
N07-2026,P05-1051,0,0.0619458,"Missing"
N07-2026,N06-2021,1,\N,Missing
N09-1006,W07-1001,0,0.127858,"both omit and substitute articles and clitics, while the dominant errors for Italian-speakers are omissions. 3 Our Approach We use language models (LMs) in our initial investigation, and later explore more complex ML algorithms to improve the results. Our ultimate goal is to discover a highly accurate ML method that can be used to assist clinicians in the task of LI identification in children. 3.1 Language Models for Predicting Language Impairment LMs are statistical models used to estimate the probability of a given sequence of words. They have been explored previously for clinical purposes. Roark et al. (2007) proposed cross entropy of LMs trained on Part-of-Speech (POS) sequences as a measure of syntactic complexity with the aim of determining mild cognitive impairment in adults. Solorio and Liu (2008) evaluated LMs on a small data set in a preliminary trial on LI prediction. The intuition behind using LMs is that they can identify atypical grammatical patterns and help discriminate the population with potential LI from the Typically Developing (TD) one. We use LMs trained on POS tags rather than on words. Using POS tags can address the data sparsity issue in LMs, and place less emphasis on the vo"
N09-1006,W08-0626,1,0.845656,"xplore more complex ML algorithms to improve the results. Our ultimate goal is to discover a highly accurate ML method that can be used to assist clinicians in the task of LI identification in children. 3.1 Language Models for Predicting Language Impairment LMs are statistical models used to estimate the probability of a given sequence of words. They have been explored previously for clinical purposes. Roark et al. (2007) proposed cross entropy of LMs trained on Part-of-Speech (POS) sequences as a measure of syntactic complexity with the aim of determining mild cognitive impairment in adults. Solorio and Liu (2008) evaluated LMs on a small data set in a preliminary trial on LI prediction. The intuition behind using LMs is that they can identify atypical grammatical patterns and help discriminate the population with potential LI from the Typically Developing (TD) one. We use LMs trained on POS tags rather than on words. Using POS tags can address the data sparsity issue in LMs, and place less emphasis on the vocabulary and more emphasis on the syntactic patterns. We trained two separate LMs using POS tags from the transcripts of TD and LI children, respectively. The language status of a child is predicte"
N09-1070,A00-1031,0,0.0167113,"o key phrases are generated. 2 Note that by unsupervised methods, we mean that no data annotated with keywords is needed. These methods do require the use of some data to generate information such as IDF, or possibly a development set to optimize some parameters or heuristic rules. 623 In addition to using a stopword list to remove words from consideration, we also leverage POS information to filter unlikely keywords. Our hypothesis is that verb, noun and adjective words are more likely to be keywords, so we restrict our selection to words with these POS tags only. We used the TnT POS tagger (Brants, 2000) trained from the Switchboard data to tag the meeting transcripts. (C) Integrating word clustering One weakness of the baseline TFIDF is that it counts the frequency for a particular word, without considering any words that are similar to it in terms of semantic meaning. In addition, when the document is short, the TF may not be a reliable indicator of the importance of the word. Our idea is therefore to account for the frequency of other similar words when calculating the TF of a word in the document. For this, we group all the words into clusters in an unsupervised fashion. If the total term"
N09-1070,P03-1071,0,0.0084388,", 2005)) to optimize our keyword extraction methods, and the remaining 21 meetings for final testing in Section 5. One example of the annotated keywords for a topic segment is: • Annotator I: analysis, constraints, template matcher; • Annotator II: syntactic analysis, parser, pattern matcher, finite-state transducers; • Annotator III: lexicon, set processing, chunk parser. Note that these meetings are research discussions, and that the annotators may not be very familiar with 1 We selected these 27 meetings because they have been used in previous work for topic segmentation and summarization (Galley et al., 2003; Murray et al., 2005). 622 the topics discussed and often had trouble deciding the important sentences or keywords. In addition, limiting the number of keywords that an annotator can select for a topic also created some difficulty. Sometimes there are more possible keywords and the annotators felt it is hard to decide which five are the most topic indicative. Among the three annotators, we notice that in general the quality of annotator I is the poorest. This is based on the authors’ judgment, and is also confirmed later by an independent human evaluation (in Section 6). For a better understa"
N09-1070,W03-1028,0,0.88654,"Missing"
N09-1070,W05-0905,0,0.0131623,"Missing"
N09-1070,N04-1019,0,0.0805389,"Missing"
N09-1070,van-der-plas-etal-2004-automatic,0,0.0142481,"nforcement approach to do keyword extraction and summarization simultaneously, on the assumption that important sentences usually contain keywords and keywords are usually seen in important sentences. We also find that this assumption also holds using statistics obtained from the meeting corpus used in this study. Graph-based methods have not been used in a genre like the meeting domain; therefore, it remains to be seen whether these approaches can be applied to meetings. Not many studies have been performed on speech transcripts for keyword extraction. The most relevant work to our study is (Plas et al., 2004), where the task is keyword extraction in the multiparty meeting corpus. They showed that leveraging semantic resources can yield significant performance improvement compared to the approach based on the relative frequency ratio (similar to IDF). There is also some work using keywords for other speech processing tasks, e.g., (Munteanu et al., 2007; Bulyko et al., 2007; Wu et al., 2007; Desilets et al., 2002; Rogina, 2002). (Wu et al., 2007) showed that keyword extraction combined with semantic verification can be used to improve speech retrieval performance on broadcast news data. In (Rogina,"
N09-1070,W04-2319,0,0.0181216,"Missing"
N09-1070,P07-1070,0,0.636413,", since keywords often appear early in the document (e.g., in the first paragraph). However, for the less well structured meeting domain (lack of title and paragraph), these kinds of features may not be indicative. A supervised approach to keyword extraction was used in (Liu et al., 2008). Even though the data set in that study is not very big, it seems that a supervised learning approach can achieve reasonable performance for this task. Another line of research for keyword extraction has adopted graph-based methods similar to Google’s PageRank algorithm (Brin and Page, 1998). In particular, (Wan et al., 2007) attempted to use a reinforcement approach to do keyword extraction and summarization simultaneously, on the assumption that important sentences usually contain keywords and keywords are usually seen in important sentences. We also find that this assumption also holds using statistics obtained from the meeting corpus used in this study. Graph-based methods have not been used in a genre like the meeting domain; therefore, it remains to be seen whether these approaches can be applied to meetings. Not many studies have been performed on speech transcripts for keyword extraction. The most relevant"
N10-1006,W04-1013,0,0.0134695,"Missing"
N10-1006,E06-1005,0,0.0338789,"efined on the document’s semantic graph, and showed better performance comparing to other graph-based approaches. Rich speech recognition results, such as N-best hypotheses and confusion networks, were first used in multi-pass ASR systems to improve speech recognition performance (Stolcke et al., 1997; Mangu et al., 2000). They have been widely used in many subsequent spoken language processing tasks, such as machine translation, spoken document understanding and retrieval. Confusion network decoding was applied to combine the outputs of multiple machine translation systems (Sim et al., 2007; Matusov et al., 2006). In the task of spoken document retrieval, (Chia et al., 2008) proposed to compute the expected word counts from document and query lattices, and estimate the statistical models from these counts, and reported better retrieval accuracy than using only 1-best transcripts. (Hakkani-Tur et al., 2006) investigated using confusion networks for name entity detection and extraction and user intent classifi47 cation. They also obtained better performance than using ASR 1-best output. There is very limited previous work using more than 1-best ASR output for speech summarization. Several studies used a"
N10-1006,W05-0905,0,0.100315,"rvised approaches. Since we use unsupervised methods in this study, we will not describe previous work using supervised approaches because of the space limit. Unsupervised meth46 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 46–54, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics ods are simple and robust to different corpora, and do not need any human labeled data for training. MMR was introduced in (Carbonell and Goldstein, 1998) for text summarization, and was used widely in meeting summarization (Murray et al., 2005a; Xie and Liu, 2008). Latent semantic analysis (LSA) approaches have also been used (Murray et al., 2005a), which can better measure document similarity at the semantic level rather than relying on literal word matching. In (Gillick et al., 2009), the authors introduced a concept-based global optimization framework using integer linear programming (ILP), where concepts were used as the minimum units, and the important sentences were extracted to cover as many concepts as possible. They showed better performance than MMR. In a follow-up study, (Xie et al., 2009) incorporated sentence informati"
N10-1006,A00-2025,0,0.556955,"s from document and query lattices, and estimate the statistical models from these counts, and reported better retrieval accuracy than using only 1-best transcripts. (Hakkani-Tur et al., 2006) investigated using confusion networks for name entity detection and extraction and user intent classifi47 cation. They also obtained better performance than using ASR 1-best output. There is very limited previous work using more than 1-best ASR output for speech summarization. Several studies used acoustic confidence scores in the 1-best ASR hypothesis in the summarization systems (Valenza et al., 1999; Zechner and Waibel, 2000; Hori and Furui, 2003). (Liu et al., 2010) evaluated using n-best hypotheses for meeting summarization, and showed improved performance with the gain coming mainly from the first few candidates. In (Lin and Chen, 2009), confusion networks and position specific posterior lattices were considered in a generative summarization framework for Chinese broadcast news summarization, and they showed promising results by using more ASR hypotheses. We investigate using confusion networks for meeting summarization in this study. This work differs from (Lin and Chen, 2009) in terms of the language and gen"
N10-1042,E09-1004,0,0.105955,"ntal analysis has been performed in prior work, from binary classes to more fine grained categories. Pang et al. (2002) defined this task as a binary classification task and applied it to movie reviews. More sentiment classes, such as document objectivity and subjectivity as well as different rating scales on the subjectivity, have also been taken into consideration (Pang and Lee, 2005; Boiy et al., 2007). In terms of granularity, this task has been investigated from building word level sentiment lexicon (Turney, 2002; Moilanen and Pulman, 2008) to detecting phrase-level (Wilson et al., 2005; Agarwal et al., 2009) and sentence-level (Riloff and Wiebe, 2003; Hu and Liu, 2004) sentiment orientation. However, most previous work has mainly focused on reviews (Pang et al., 2002; Hu and Liu, 2004), news resources (Wilson et al., 2005), and multi-domain adaptation (Blitzer et al., 2007; Mansour et al., 2008). Sentiment analysis on blogs (Chesley et al., 2005; Kim et al., 2009) is still at its early stage. In this paper we investigate binary polarity classification (positive vs. negative). We evaluate the genre effect between blogs and review data and show the difference of (i) Lexical features (LF) We use the"
N10-1042,P07-1056,0,0.402216,"y as well as different rating scales on the subjectivity, have also been taken into consideration (Pang and Lee, 2005; Boiy et al., 2007). In terms of granularity, this task has been investigated from building word level sentiment lexicon (Turney, 2002; Moilanen and Pulman, 2008) to detecting phrase-level (Wilson et al., 2005; Agarwal et al., 2009) and sentence-level (Riloff and Wiebe, 2003; Hu and Liu, 2004) sentiment orientation. However, most previous work has mainly focused on reviews (Pang et al., 2002; Hu and Liu, 2004), news resources (Wilson et al., 2005), and multi-domain adaptation (Blitzer et al., 2007; Mansour et al., 2008). Sentiment analysis on blogs (Chesley et al., 2005; Kim et al., 2009) is still at its early stage. In this paper we investigate binary polarity classification (positive vs. negative). We evaluate the genre effect between blogs and review data and show the difference of (i) Lexical features (LF) We use the bag of words for the lexical features as they have been shown very useful in previous work. (ii) Polarized lexical features (PL) We tagged each sentiment word in our data set with its polarity tag based on the sentiment lexicon (“POS” for positive, and “NEG” for negati"
N10-1042,P09-1029,0,0.0123416,"n (Pang and Lee, 2005; Boiy et al., 2007). In terms of granularity, this task has been investigated from building word level sentiment lexicon (Turney, 2002; Moilanen and Pulman, 2008) to detecting phrase-level (Wilson et al., 2005; Agarwal et al., 2009) and sentence-level (Riloff and Wiebe, 2003; Hu and Liu, 2004) sentiment orientation. However, most previous work has mainly focused on reviews (Pang et al., 2002; Hu and Liu, 2004), news resources (Wilson et al., 2005), and multi-domain adaptation (Blitzer et al., 2007; Mansour et al., 2008). Sentiment analysis on blogs (Chesley et al., 2005; Kim et al., 2009) is still at its early stage. In this paper we investigate binary polarity classification (positive vs. negative). We evaluate the genre effect between blogs and review data and show the difference of (i) Lexical features (LF) We use the bag of words for the lexical features as they have been shown very useful in previous work. (ii) Polarized lexical features (PL) We tagged each sentiment word in our data set with its polarity tag based on the sentiment lexicon (“POS” for positive, and “NEG” for negative), along with its partof-speech tag. For example, in the sentence “It is good, and I like i"
N10-1042,P08-2028,0,0.0151516,"fying opinionated text towards a given subject. Different levels of sentimental analysis has been performed in prior work, from binary classes to more fine grained categories. Pang et al. (2002) defined this task as a binary classification task and applied it to movie reviews. More sentiment classes, such as document objectivity and subjectivity as well as different rating scales on the subjectivity, have also been taken into consideration (Pang and Lee, 2005; Boiy et al., 2007). In terms of granularity, this task has been investigated from building word level sentiment lexicon (Turney, 2002; Moilanen and Pulman, 2008) to detecting phrase-level (Wilson et al., 2005; Agarwal et al., 2009) and sentence-level (Riloff and Wiebe, 2003; Hu and Liu, 2004) sentiment orientation. However, most previous work has mainly focused on reviews (Pang et al., 2002; Hu and Liu, 2004), news resources (Wilson et al., 2005), and multi-domain adaptation (Blitzer et al., 2007; Mansour et al., 2008). Sentiment analysis on blogs (Chesley et al., 2005; Kim et al., 2009) is still at its early stage. In this paper we investigate binary polarity classification (positive vs. negative). We evaluate the genre effect between blogs and revie"
N10-1042,P05-1015,0,0.0908898,"2005). The features we explored are listed below. Introduction Sentimental analysis is a task of text categorization that focuses on recognizing and classifying opinionated text towards a given subject. Different levels of sentimental analysis has been performed in prior work, from binary classes to more fine grained categories. Pang et al. (2002) defined this task as a binary classification task and applied it to movie reviews. More sentiment classes, such as document objectivity and subjectivity as well as different rating scales on the subjectivity, have also been taken into consideration (Pang and Lee, 2005; Boiy et al., 2007). In terms of granularity, this task has been investigated from building word level sentiment lexicon (Turney, 2002; Moilanen and Pulman, 2008) to detecting phrase-level (Wilson et al., 2005; Agarwal et al., 2009) and sentence-level (Riloff and Wiebe, 2003; Hu and Liu, 2004) sentiment orientation. However, most previous work has mainly focused on reviews (Pang et al., 2002; Hu and Liu, 2004), news resources (Wilson et al., 2005), and multi-domain adaptation (Blitzer et al., 2007; Mansour et al., 2008). Sentiment analysis on blogs (Chesley et al., 2005; Kim et al., 2009) is"
N10-1042,W02-1011,0,0.0245537,"ion For the binary polarity classification task, we use a supervised learning framework to determine whether a document is positive or negative. We used a subjective lexicon, containing 2304 positive words and 4145 negative words respectively, based on (Wilson et al., 2005). The features we explored are listed below. Introduction Sentimental analysis is a task of text categorization that focuses on recognizing and classifying opinionated text towards a given subject. Different levels of sentimental analysis has been performed in prior work, from binary classes to more fine grained categories. Pang et al. (2002) defined this task as a binary classification task and applied it to movie reviews. More sentiment classes, such as document objectivity and subjectivity as well as different rating scales on the subjectivity, have also been taken into consideration (Pang and Lee, 2005; Boiy et al., 2007). In terms of granularity, this task has been investigated from building word level sentiment lexicon (Turney, 2002; Moilanen and Pulman, 2008) to detecting phrase-level (Wilson et al., 2005; Agarwal et al., 2009) and sentence-level (Riloff and Wiebe, 2003; Hu and Liu, 2004) sentiment orientation. However, mos"
N10-1042,W03-1014,0,0.132068,"work, from binary classes to more fine grained categories. Pang et al. (2002) defined this task as a binary classification task and applied it to movie reviews. More sentiment classes, such as document objectivity and subjectivity as well as different rating scales on the subjectivity, have also been taken into consideration (Pang and Lee, 2005; Boiy et al., 2007). In terms of granularity, this task has been investigated from building word level sentiment lexicon (Turney, 2002; Moilanen and Pulman, 2008) to detecting phrase-level (Wilson et al., 2005; Agarwal et al., 2009) and sentence-level (Riloff and Wiebe, 2003; Hu and Liu, 2004) sentiment orientation. However, most previous work has mainly focused on reviews (Pang et al., 2002; Hu and Liu, 2004), news resources (Wilson et al., 2005), and multi-domain adaptation (Blitzer et al., 2007; Mansour et al., 2008). Sentiment analysis on blogs (Chesley et al., 2005; Kim et al., 2009) is still at its early stage. In this paper we investigate binary polarity classification (positive vs. negative). We evaluate the genre effect between blogs and review data and show the difference of (i) Lexical features (LF) We use the bag of words for the lexical features as t"
N10-1042,P02-1053,0,0.0194646,"ing and classifying opinionated text towards a given subject. Different levels of sentimental analysis has been performed in prior work, from binary classes to more fine grained categories. Pang et al. (2002) defined this task as a binary classification task and applied it to movie reviews. More sentiment classes, such as document objectivity and subjectivity as well as different rating scales on the subjectivity, have also been taken into consideration (Pang and Lee, 2005; Boiy et al., 2007). In terms of granularity, this task has been investigated from building word level sentiment lexicon (Turney, 2002; Moilanen and Pulman, 2008) to detecting phrase-level (Wilson et al., 2005; Agarwal et al., 2009) and sentence-level (Riloff and Wiebe, 2003; Hu and Liu, 2004) sentiment orientation. However, most previous work has mainly focused on reviews (Pang et al., 2002; Hu and Liu, 2004), news resources (Wilson et al., 2005), and multi-domain adaptation (Blitzer et al., 2007; Mansour et al., 2008). Sentiment analysis on blogs (Chesley et al., 2005; Kim et al., 2009) is still at its early stage. In this paper we investigate binary polarity classification (positive vs. negative). We evaluate the genre ef"
N10-1042,H05-1044,0,0.0421981,"based topic analysis to extract relevant sentences to the given topics for polarity classification. Second, we adopted an adaptive method where we train classifiers from review data and incorporate their hypothesis as features. Both methods yielded performance gain for polarity classification on blog data. 1 2 Features for Polarity Classification For the binary polarity classification task, we use a supervised learning framework to determine whether a document is positive or negative. We used a subjective lexicon, containing 2304 positive words and 4145 negative words respectively, based on (Wilson et al., 2005). The features we explored are listed below. Introduction Sentimental analysis is a task of text categorization that focuses on recognizing and classifying opinionated text towards a given subject. Different levels of sentimental analysis has been performed in prior work, from binary classes to more fine grained categories. Pang et al. (2002) defined this task as a binary classification task and applied it to movie reviews. More sentiment classes, such as document objectivity and subjectivity as well as different rating scales on the subjectivity, have also been taken into consideration (Pang"
N13-1102,N01-1016,0,0.873981,"Missing"
N13-1102,W02-1001,0,0.0152543,"region), O (other). For example, the previous sentence is represented as: I/O want/O a/O flight/O to/BE Boston/EE uh/O I/O mean/O to/O Denver/O We use the F-score as the evaluation metrics (Zwarts and Johnson, 2011; Johnson and Charniak, 2004), which is defined as the harmonic mean of the precision and recall of the edited words: P = R = F = #correctly predicted edited words #predicted edited words #correctly predicted edited words #gold standard edited words 2×P ×R P +R There are many methods to train the sequence model, such as CRFs (Lafferty et al., 2001), averaged structured perceptrons (Collins, 2002), structured SVM (Altun et al., 2003), online passive aggressive learning (Crammer et al., 2006). Previous work has shown that minimizing F-loss is more effective than minimizing log-loss (Zwarts and Johnson, 2011), because edited words are much fewer than normal words. In this paper, we use Max-margin Markov Networks (Taskar et al., 2004) because our preliminary 821 results showed that they outperform other classifiers, and using weighted hamming loss is simple in this approach (whereas for perceptron or CRFs, the modification of the objective function is not straightforward). The learning ta"
N13-1102,N09-2028,0,0.666279,"Missing"
N13-1102,P04-1005,0,0.843616,"te that our proposed method outperforms other state-of-the-art systems for edit disfluency detection. 2 Balancing Precision and Recall Using Weighted M3 Ns We use a sequence labeling model for edit detection. Each word is assigned one of the five labels: BE (beginning of the multi-word edited region), IE (in the edited region), EE (end of the edited region), SE (single word edited region), O (other). For example, the previous sentence is represented as: I/O want/O a/O flight/O to/BE Boston/EE uh/O I/O mean/O to/O Denver/O We use the F-score as the evaluation metrics (Zwarts and Johnson, 2011; Johnson and Charniak, 2004), which is defined as the harmonic mean of the precision and recall of the edited words: P = R = F = #correctly predicted edited words #predicted edited words #correctly predicted edited words #gold standard edited words 2×P ×R P +R There are many methods to train the sequence model, such as CRFs (Lafferty et al., 2001), averaged structured perceptrons (Collins, 2002), structured SVM (Altun et al., 2003), online passive aggressive learning (Crammer et al., 2006). Previous work has shown that minimizing F-loss is more effective than minimizing log-loss (Zwarts and Johnson, 2011), because edited"
N13-1102,H05-1030,0,0.609319,"Missing"
N13-1102,P06-1071,0,0.780716,"Missing"
N13-1102,P11-1071,0,0.693378,"board corpus, we demonstrate that our proposed method outperforms other state-of-the-art systems for edit disfluency detection. 2 Balancing Precision and Recall Using Weighted M3 Ns We use a sequence labeling model for edit detection. Each word is assigned one of the five labels: BE (beginning of the multi-word edited region), IE (in the edited region), EE (end of the edited region), SE (single word edited region), O (other). For example, the previous sentence is represented as: I/O want/O a/O flight/O to/BE Boston/EE uh/O I/O mean/O to/O Denver/O We use the F-score as the evaluation metrics (Zwarts and Johnson, 2011; Johnson and Charniak, 2004), which is defined as the harmonic mean of the precision and recall of the edited words: P = R = F = #correctly predicted edited words #predicted edited words #correctly predicted edited words #gold standard edited words 2×P ×R P +R There are many methods to train the sequence model, such as CRFs (Lafferty et al., 2001), averaged structured perceptrons (Collins, 2002), structured SVM (Altun et al., 2003), online passive aggressive learning (Crammer et al., 2006). Previous work has shown that minimizing F-loss is more effective than minimizing log-loss (Zwarts and J"
N15-1079,baccianella-etal-2010-sentiwordnet,0,0.00305815,"entity from each sentence, and if the recognized entity is also identified as a named entity by Stanford CoreNLP8 , we use this entity’s DBpedia abstract content to extend the bigrams. For example, in the bigram ‘Kashmir area’, the word ‘Kashmir’ is recognized as an entity by both (Stanford CoreNLP and DBpedia Spotlight service), then we use the description for ‘Kashmir’ from DBpedia9 to extend this bigram, and calculate the cosine similarity between this description and the topic query and top-k most frequent unigrams in the documents. 4.7 Sentiment Feature from SentiWordNet10 SentiWordNet (Baccianella et al., 2010) is an extension on WordNet and it further assigns to each synset of WordNet three sentiment scores: positivity, negativity, objectivity. The sentiment score of a bigram is the average score of the two words in the bigram. To sum up, the features we use include the internal features, and external ones derived from various resources: news article corpus with summaries, Wikipeida, DBpedia, WordNet and SentiWordNet. Some external features represent the inherent importance of bigrams. For example, features extracted from the news article corpus and wikipedia are used to represent how often bigrams"
N15-1079,W02-1001,0,0.020389,"ed to the bigram candidates. After the above filtering, we further drop bigrams if both words are stop words, as previous work in (Gillick et al., 2009). 3.2 Weight Training We propose to train the feature weights in a joint learning fashion. In the ILP summarization framework, we use the following new objective function: max P i (θ · f(bi ))ci (7) We replace the wi in Formula 1 with a vector inner product of bigram features and their corresponding weights. Constraints remain the same as those in Formula 2 to 6. To train the model (feature weights), we leverage structured perceptron strategy (Collins, 2002) to update the feature weights whenever the hypothesis offered by the ILP decoding process is incorrect. Binary class labels are used for bigrams in the learning process, that is, we only consider whether a bigram is in the system generated summary or human summaries, not their term or document frequency. During perceptron training, a fixed learning rate is used and parameters are averaged to prevent overfitting. 4 Features for Bigrams We use a rich set of features to represent each bigram candidate, including internal features based on the test documents, and features extracted from external"
N15-1079,P14-1119,0,0.0156288,"ation performance. Woodsend and Lapata (2012) and Li et al. (2014) both leveraged constituent parser trees to help sentence compression, which is also modeled in the optimization framework. But these kinds of work involve using complex linguistic information, often based on syntactic analysis. Since the language concepts (or bigrams) can be considered as key phrases of the documents, the other line related to our work is how to extract and measure the importance of key phrases from documents. In particular, our work is related to key phrase extraction by using external resources. A survey by (Hasan and Ng, 2014) showed that using external resources to extract and measure key phrases is very effective. In (Medelyan et al., 2009), Wikipedia-based key phrases are determined based on a candidate’s document frequency multiplied by the ratio of the number of Wikipedia articles containing the candidate as a link to the number of articles containing the candidate. Query logs were also used as another external resource by (Yih et al., 2006) to exploit the observation that a candidate is potentially important if it was used as a search query. Similarly terminological databases have been exploited to encode the"
N15-1079,E14-1075,0,0.285923,"uery logs were also used as another external resource by (Yih et al., 2006) to exploit the observation that a candidate is potentially important if it was used as a search query. Similarly terminological databases have been exploited to encode the salience of candidate key phrases in scientific papers (Lopez and Romary, 2010). In summarization, external information has also been used to measure word salience. Some TAC systems like (Kumar et al., 2010; Jia et al., 2010) used Wiki as an important external resource to measure the words’ importance, which helped improve the summarization results. Hong and Nenkova (2014) introduced a supervised model for predicting word importance that incorporated a rich set of features. Tweets information is leveraged by (Wei and Gao, 2014) to help generate news highlights. In this paper our focus is on choosing useful bigrams and estimating accurate weights to use in the concept-based ILP methods. We explore many external resources to extract features for bigram candidates, and more importantly, propose to estimate the feature weights in a joint process via structured perceptron learning that optimizes summary sentence selection. 3 Summarization System In this study we use"
N15-1079,O97-1002,0,0.045674,"lar to the above method, here we still focus on measuring the similarity between a bigram and the topic query, but based on WordNet. We use WordNet to identify the synonyms of nouns, verbs, 2 3 https://code.google.com/p/word2vec/ http://wordnet.princeton.edu/ and adjectives from each bigram and the query of the topic. Then every bigram and sentence can be represented as a bag of synonyms of the original words. Finally based on these synonyms we leverage the following four similarity measurements: Lin Similarity (Lin, 1998), Wu-Palmer Similarity (Wu and Palmer, 1994), Jiang-Conrath Similarity (Jiang and Conrath, 1997), and Resnik Similarity (Resnik, 1995). These four similarity measurements are all implemented in the NLTK toolkit4 . We expect that these features would improve the estimation accuracy because they can overcome the ambiguity and the diversity of the vocabulary. 4.5 Importance based on Wikipedia Wikipedia is a very popular resource used in many different tasks. In order to obtain more precise external information from Wikipedia for our task, we collect the articles from Wikipedia by two steps. If the query is already the title of a wiki page, we will not further gather other wiki pages for thi"
N15-1079,P98-1112,0,0.0433599,"ula 1) is simply set as their document frequency. Although this setting has been demonstrated to be quite effective, its gap with the oracle experiment (using bigrams that appear in the human summaries) is still very large, suggesting potential gains by using better 780 bigrams/concepts in the ILP optimization method. Details are described in (Gillick et al., 2009). In this paper, rather than considering all the bigrams, we propose to utilize syntactic information to help select important bigrams. Intuitively bigrams containing content words carry more topic related information. As proven in (Klavans and Kan, 1998), nouns, verbs, and adjectives were indeed beneficial in document analysis. Therefore we focus on choosing bigrams containing these words. First, we use a bottom-up strategy to go through the constituent parse tree and identify the ‘NP’ nodes in the lowest level of the tree. Then all the bigrams in these base NPs are kept as candidates. Second, we find the verbs and adjectives from the sentence based on the POS tags, and construct bigrams by concatenating the previous or the next word of that verb or adjective. If these bigrams are not included in those already found from the base NPs, they ar"
N15-1079,D13-1047,1,0.829429,"08 and 2009 summarization task (Gillick et al., 2008; Gillick et al., 2009). After that, the global optimization strategy attracted increasing attention in the summarization task. Lin and Bilmes (2010) treated the summarization task as a maximization problem of submodular functions. Davis et al. (2012) proposed an optimal combinatorial covering algorithm combined with LSA to measure the term weight for extractive summarization. Takamura and Okumura (2009) also defined the summarization problem as a maximum coverage problem and used a branch-and-bound method to search for the optimal solution. Li et al. (2013b) used the same ILP framework as (Gillick et al., 2009), but incorporated a supervised model to estimate the bigram frequency in the final summary. Similar optimization methods are also widely used in the abstractive summarization task. Martins and Smith (2009) leveraged ILP technique to jointly select and compress sentences for multi-document summarization. A novel summary guided sentence compression was proposed by (Li et al., 2013a) and it successfully improved the summarization performance. Woodsend and Lapata (2012) and Li et al. (2014) both leveraged constituent parser trees to help sen"
N15-1079,P13-1099,1,0.723081,"08 and 2009 summarization task (Gillick et al., 2008; Gillick et al., 2009). After that, the global optimization strategy attracted increasing attention in the summarization task. Lin and Bilmes (2010) treated the summarization task as a maximization problem of submodular functions. Davis et al. (2012) proposed an optimal combinatorial covering algorithm combined with LSA to measure the term weight for extractive summarization. Takamura and Okumura (2009) also defined the summarization problem as a maximum coverage problem and used a branch-and-bound method to search for the optimal solution. Li et al. (2013b) used the same ILP framework as (Gillick et al., 2009), but incorporated a supervised model to estimate the bigram frequency in the final summary. Similar optimization methods are also widely used in the abstractive summarization task. Martins and Smith (2009) leveraged ILP technique to jointly select and compress sentences for multi-document summarization. A novel summary guided sentence compression was proposed by (Li et al., 2013a) and it successfully improved the summarization performance. Woodsend and Lapata (2012) and Li et al. (2014) both leveraged constituent parser trees to help sen"
N15-1079,D14-1076,1,0.640471,"and-bound method to search for the optimal solution. Li et al. (2013b) used the same ILP framework as (Gillick et al., 2009), but incorporated a supervised model to estimate the bigram frequency in the final summary. Similar optimization methods are also widely used in the abstractive summarization task. Martins and Smith (2009) leveraged ILP technique to jointly select and compress sentences for multi-document summarization. A novel summary guided sentence compression was proposed by (Li et al., 2013a) and it successfully improved the summarization performance. Woodsend and Lapata (2012) and Li et al. (2014) both leveraged constituent parser trees to help sentence compression, which is also modeled in the optimization framework. But these kinds of work involve using complex linguistic information, often based on syntactic analysis. Since the language concepts (or bigrams) can be considered as key phrases of the documents, the other line related to our work is how to extract and measure the importance of key phrases from documents. In particular, our work is related to key phrase extraction by using external resources. A survey by (Hasan and Ng, 2014) showed that using external resources to extrac"
N15-1079,N10-1134,0,0.0647145,"mpetitiveness of our proposed methods. 2 Related Work Optimization methods have been widely used in extractive summarization lately. McDonald (2007) first introduced the sentence level ILP for summarization. Later Gillick et al. (2009) revised it to concept-based ILP, which is similar to the Bud779 geted Maximal Coverage problem in (Khuller et al., 1999). The concept-based ILP system performed very well in the TAC 2008 and 2009 summarization task (Gillick et al., 2008; Gillick et al., 2009). After that, the global optimization strategy attracted increasing attention in the summarization task. Lin and Bilmes (2010) treated the summarization task as a maximization problem of submodular functions. Davis et al. (2012) proposed an optimal combinatorial covering algorithm combined with LSA to measure the term weight for extractive summarization. Takamura and Okumura (2009) also defined the summarization problem as a maximum coverage problem and used a branch-and-bound method to search for the optimal solution. Li et al. (2013b) used the same ILP framework as (Gillick et al., 2009), but incorporated a supervised model to estimate the bigram frequency in the final summary. Similar optimization methods are also"
N15-1079,W04-1013,0,0.0553827,"Missing"
N15-1079,S10-1055,0,0.0277648,"ure key phrases is very effective. In (Medelyan et al., 2009), Wikipedia-based key phrases are determined based on a candidate’s document frequency multiplied by the ratio of the number of Wikipedia articles containing the candidate as a link to the number of articles containing the candidate. Query logs were also used as another external resource by (Yih et al., 2006) to exploit the observation that a candidate is potentially important if it was used as a search query. Similarly terminological databases have been exploited to encode the salience of candidate key phrases in scientific papers (Lopez and Romary, 2010). In summarization, external information has also been used to measure word salience. Some TAC systems like (Kumar et al., 2010; Jia et al., 2010) used Wiki as an important external resource to measure the words’ importance, which helped improve the summarization results. Hong and Nenkova (2014) introduced a supervised model for predicting word importance that incorporated a rich set of features. Tweets information is leveraged by (Wei and Gao, 2014) to help generate news highlights. In this paper our focus is on choosing useful bigrams and estimating accurate weights to use in the concept-bas"
N15-1079,W09-1801,0,0.0746698,"oblem of submodular functions. Davis et al. (2012) proposed an optimal combinatorial covering algorithm combined with LSA to measure the term weight for extractive summarization. Takamura and Okumura (2009) also defined the summarization problem as a maximum coverage problem and used a branch-and-bound method to search for the optimal solution. Li et al. (2013b) used the same ILP framework as (Gillick et al., 2009), but incorporated a supervised model to estimate the bigram frequency in the final summary. Similar optimization methods are also widely used in the abstractive summarization task. Martins and Smith (2009) leveraged ILP technique to jointly select and compress sentences for multi-document summarization. A novel summary guided sentence compression was proposed by (Li et al., 2013a) and it successfully improved the summarization performance. Woodsend and Lapata (2012) and Li et al. (2014) both leveraged constituent parser trees to help sentence compression, which is also modeled in the optimization framework. But these kinds of work involve using complex linguistic information, often based on syntactic analysis. Since the language concepts (or bigrams) can be considered as key phrases of the docu"
N15-1079,D09-1137,0,0.0146729,"sentence compression, which is also modeled in the optimization framework. But these kinds of work involve using complex linguistic information, often based on syntactic analysis. Since the language concepts (or bigrams) can be considered as key phrases of the documents, the other line related to our work is how to extract and measure the importance of key phrases from documents. In particular, our work is related to key phrase extraction by using external resources. A survey by (Hasan and Ng, 2014) showed that using external resources to extract and measure key phrases is very effective. In (Medelyan et al., 2009), Wikipedia-based key phrases are determined based on a candidate’s document frequency multiplied by the ratio of the number of Wikipedia articles containing the candidate as a link to the number of articles containing the candidate. Query logs were also used as another external resource by (Yih et al., 2006) to exploit the observation that a candidate is potentially important if it was used as a search query. Similarly terminological databases have been exploited to encode the salience of candidate key phrases in scientific papers (Lopez and Romary, 2010). In summarization, external informati"
N15-1079,C12-1128,0,0.0236835,".2.1 Summarization Results Table 1 shows the ROUGE-2 results of our proposed joint system, the ICSI system (which uses document frequency threshold to select bigram concepts and uses df as weights), the best performing system in the NIST TAC evaluation, and the state of the art performance we could find. The result of our proposed method is statistically significantly better than that of ICSI ILP (p &lt; 0.05 based on paired ttest). It is also statistically significantly (p &lt; 0.05) better than that of TAC Rank1 except 2011, and previous best in 2008 and 2010. The 2011 previous best results from (Ng et al., 2012) involve some rule-based sentence compression, which improves the ROUGE value. If we apply the same or similar rule-based sentence compression on our results, and the ROUGE-2 of our proposed method improves to 14.38. ICSI ILP TAC Rank1 Previous Best Proposed Method 2008 10.23 10.38 10.76† 11.84 2009 11.60 12.16 12.46† 12.77 2010 10.03 9.57 10.8‡ 11.78 2011 12.71 13.44 13.93∗ 13.97 34 30 28 26 24 22 20 2008 11 http://www.gurobi.com 783 2009 2010 2011 Figure 1: Percentage of correct bigrams in the selected bigrams from ICSI and our proposed system. Table 1: ROUGE-2 summarization results.† is fro"
N15-1079,W12-2601,0,0.0166704,"Missing"
N15-1079,P06-1055,0,0.0273014,"Missing"
N15-1079,E09-1089,0,0.0220486,"ncept-based ILP, which is similar to the Bud779 geted Maximal Coverage problem in (Khuller et al., 1999). The concept-based ILP system performed very well in the TAC 2008 and 2009 summarization task (Gillick et al., 2008; Gillick et al., 2009). After that, the global optimization strategy attracted increasing attention in the summarization task. Lin and Bilmes (2010) treated the summarization task as a maximization problem of submodular functions. Davis et al. (2012) proposed an optimal combinatorial covering algorithm combined with LSA to measure the term weight for extractive summarization. Takamura and Okumura (2009) also defined the summarization problem as a maximum coverage problem and used a branch-and-bound method to search for the optimal solution. Li et al. (2013b) used the same ILP framework as (Gillick et al., 2009), but incorporated a supervised model to estimate the bigram frequency in the final summary. Similar optimization methods are also widely used in the abstractive summarization task. Martins and Smith (2009) leveraged ILP technique to jointly select and compress sentences for multi-document summarization. A novel summary guided sentence compression was proposed by (Li et al., 2013a) and"
N15-1079,C14-1083,0,0.0184099,"a search query. Similarly terminological databases have been exploited to encode the salience of candidate key phrases in scientific papers (Lopez and Romary, 2010). In summarization, external information has also been used to measure word salience. Some TAC systems like (Kumar et al., 2010; Jia et al., 2010) used Wiki as an important external resource to measure the words’ importance, which helped improve the summarization results. Hong and Nenkova (2014) introduced a supervised model for predicting word importance that incorporated a rich set of features. Tweets information is leveraged by (Wei and Gao, 2014) to help generate news highlights. In this paper our focus is on choosing useful bigrams and estimating accurate weights to use in the concept-based ILP methods. We explore many external resources to extract features for bigram candidates, and more importantly, propose to estimate the feature weights in a joint process via structured perceptron learning that optimizes summary sentence selection. 3 Summarization System In this study we use the ILP-based summarization framework (Formulas 1-6) that tries to maximize the weights of the selected concepts (bigrams) under the summary length constrain"
N15-1079,D12-1022,0,0.195343,"rage problem and used a branch-and-bound method to search for the optimal solution. Li et al. (2013b) used the same ILP framework as (Gillick et al., 2009), but incorporated a supervised model to estimate the bigram frequency in the final summary. Similar optimization methods are also widely used in the abstractive summarization task. Martins and Smith (2009) leveraged ILP technique to jointly select and compress sentences for multi-document summarization. A novel summary guided sentence compression was proposed by (Li et al., 2013a) and it successfully improved the summarization performance. Woodsend and Lapata (2012) and Li et al. (2014) both leveraged constituent parser trees to help sentence compression, which is also modeled in the optimization framework. But these kinds of work involve using complex linguistic information, often based on syntactic analysis. Since the language concepts (or bigrams) can be considered as key phrases of the documents, the other line related to our work is how to extract and measure the importance of key phrases from documents. In particular, our work is related to key phrase extraction by using external resources. A survey by (Hasan and Ng, 2014) showed that using externa"
N15-1079,C98-1108,0,\N,Missing
N15-1079,P94-1019,0,\N,Missing
N15-1145,P11-1049,0,0.105003,"Missing"
N15-1145,N10-3003,0,0.132105,"on assumes that users already have some information about a given topic from an old data set, and thus for a new data set the system aims to generate a summary that contains as much novel information as possible. This task was first introduced at DUC 2007 and then continued until TAC 2011. It is very useful to chronological events in real applications. Most basic update summarization methods are variants of multi-document summarization methods, with some consideration of the difference between the earlier and later document sets (Boudin et al., 2008; Fisher and Roark, 2008; Long et al., 2010; Bysani, 2010). One important line is to use graphbased co-ranking. They rank the sentences in the earlier and later document sets simultaneously by considering the sentence relationship. For example, Li et al. (2008) was inspired by the intuition that “a sentence receives a positive influence from the sentences that correlate to it in the same collection, whereas receives a negative influence from the sentences that correlates to it in the different (or previously read) collection’, and proposed a graph based sentence ranking algorithm for update summarization. Wan (2012) integrated two co-ranking processe"
N15-1145,W02-1001,0,0.113173,"by (Ng et al., 2012): α wu ∈S dfnew (wu )+(1−α) |S| wb ∈S dfnew (wb ) , where wu and wb are unigrams and bigrams respectively in sentence S. Feature 18 and 19 are variants of Features 11, where instead of document frequency (df in the formula above), bigram and unigram’s novelty and uniqueness values are used. Among these features, the feature values of feature 4, 5 and 6 are discrete. In this study, we discretized all the other continuous values into ten categories according to the value range in the training data. To train the model (feature weights), we use the average perceptron strategy (Collins, 2002) to update the feature weights whenever the hypothesis by the ILP decoding process is incorrect. Binary class labels are used for bigrams in the learning process, that is, we only consider whether a bigram is in the system generated summary or human summaries, not their term or document frequency. We use a fixed learning rate (0.1) in training. 2.3 Sentence Reranking on ILP Results In the ILP method, sentence selection is done by considering the concepts that a sentence contains. It is difficult to add indicative features in this framework to explicitly represent the sentence’s salience, and m"
N15-1145,E12-1022,0,0.0637541,"ame collection, whereas receives a negative influence from the sentences that correlates to it in the different (or previously read) collection’, and proposed a graph based sentence ranking algorithm for update summarization. Wan (2012) integrated two co-ranking processes by adding some strict constraints, which led to more accurate computation of sentences’ scores for update summarization. A similar method was also applied earlier by (Wan et al., 2011) for multilingual news summarization. In addition, generative models, such as topic models, have also been adopted for this task. For example, Delort and Alfonseca (2012) proposed a novel nonparametric Bayesian approach, a variant of Latent Dirichlet Allocation (LDA), aiming to distinguish between common information and novel information. Li et al. (2012) borrowed the idea of evolutionary clustering and proposed a three-level HDP (Hierarchical Dirichlet Process) model to represent the diversity and commonality between aspects discovered from two different document data sets. One of the most competitive summarization methods is based on Integer Linear Programming (ILP). It has been widely adopted in the generic summarization task (Martins and Smith, 2009; BergK"
N15-1145,C08-1062,0,0.015948,"as possible. This task was first introduced at DUC 2007 and then continued until TAC 2011. It is very useful to chronological events in real applications. Most basic update summarization methods are variants of multi-document summarization methods, with some consideration of the difference between the earlier and later document sets (Boudin et al., 2008; Fisher and Roark, 2008; Long et al., 2010; Bysani, 2010). One important line is to use graphbased co-ranking. They rank the sentences in the earlier and later document sets simultaneously by considering the sentence relationship. For example, Li et al. (2008) was inspired by the intuition that “a sentence receives a positive influence from the sentences that correlate to it in the same collection, whereas receives a negative influence from the sentences that correlates to it in the different (or previously read) collection’, and proposed a graph based sentence ranking algorithm for update summarization. Wan (2012) integrated two co-ranking processes by adding some strict constraints, which led to more accurate computation of sentences’ scores for update summarization. A similar method was also applied earlier by (Wan et al., 2011) for multilingual"
N15-1145,C12-1098,0,0.0634913,"m for update summarization. Wan (2012) integrated two co-ranking processes by adding some strict constraints, which led to more accurate computation of sentences’ scores for update summarization. A similar method was also applied earlier by (Wan et al., 2011) for multilingual news summarization. In addition, generative models, such as topic models, have also been adopted for this task. For example, Delort and Alfonseca (2012) proposed a novel nonparametric Bayesian approach, a variant of Latent Dirichlet Allocation (LDA), aiming to distinguish between common information and novel information. Li et al. (2012) borrowed the idea of evolutionary clustering and proposed a three-level HDP (Hierarchical Dirichlet Process) model to represent the diversity and commonality between aspects discovered from two different document data sets. One of the most competitive summarization methods is based on Integer Linear Programming (ILP). It has been widely adopted in the generic summarization task (Martins and Smith, 2009; BergKirkpatrick et al., 2011; Woodsend and Lapata, 2012; Li et al., 2013a; Li et al., 2013b; Li et al., 2014). In this paper, we use the ILP summarization 1317 Human Language Technologies: The"
N15-1145,D13-1047,1,0.859056,"variant of Latent Dirichlet Allocation (LDA), aiming to distinguish between common information and novel information. Li et al. (2012) borrowed the idea of evolutionary clustering and proposed a three-level HDP (Hierarchical Dirichlet Process) model to represent the diversity and commonality between aspects discovered from two different document data sets. One of the most competitive summarization methods is based on Integer Linear Programming (ILP). It has been widely adopted in the generic summarization task (Martins and Smith, 2009; BergKirkpatrick et al., 2011; Woodsend and Lapata, 2012; Li et al., 2013a; Li et al., 2013b; Li et al., 2014). In this paper, we use the ILP summarization 1317 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1317–1322, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics framework for the update summarization task, and make improvement from two aspects, with the goal to more discriminatively represent both the salience and novelty of words and sentences. First, we use supervised models and a rich set of features to learn the weights for the bigram concepts used in the ILP"
N15-1145,P13-1099,1,0.863919,"variant of Latent Dirichlet Allocation (LDA), aiming to distinguish between common information and novel information. Li et al. (2012) borrowed the idea of evolutionary clustering and proposed a three-level HDP (Hierarchical Dirichlet Process) model to represent the diversity and commonality between aspects discovered from two different document data sets. One of the most competitive summarization methods is based on Integer Linear Programming (ILP). It has been widely adopted in the generic summarization task (Martins and Smith, 2009; BergKirkpatrick et al., 2011; Woodsend and Lapata, 2012; Li et al., 2013a; Li et al., 2013b; Li et al., 2014). In this paper, we use the ILP summarization 1317 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1317–1322, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics framework for the update summarization task, and make improvement from two aspects, with the goal to more discriminatively represent both the salience and novelty of words and sentences. First, we use supervised models and a rich set of features to learn the weights for the bigram concepts used in the ILP"
N15-1145,D14-1076,1,0.872661,"ion (LDA), aiming to distinguish between common information and novel information. Li et al. (2012) borrowed the idea of evolutionary clustering and proposed a three-level HDP (Hierarchical Dirichlet Process) model to represent the diversity and commonality between aspects discovered from two different document data sets. One of the most competitive summarization methods is based on Integer Linear Programming (ILP). It has been widely adopted in the generic summarization task (Martins and Smith, 2009; BergKirkpatrick et al., 2011; Woodsend and Lapata, 2012; Li et al., 2013a; Li et al., 2013b; Li et al., 2014). In this paper, we use the ILP summarization 1317 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1317–1322, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics framework for the update summarization task, and make improvement from two aspects, with the goal to more discriminatively represent both the salience and novelty of words and sentences. First, we use supervised models and a rich set of features to learn the weights for the bigram concepts used in the ILP model. Second, we design a sentence r"
N15-1145,W04-1013,0,0.0277371,"utput from the first ILP step) is not too big, redundancy is not a big problem. 3 Experiments and Results 3.1 Data and Experiment Setup We evaluate our methods using several recent TAC data sets, from 2008 to 2011. Every topic has two sets of 10 documents (Set A and B). The update task aims to create a 100-word summary from Set B given a topic query and Set A. When evaluating on one year’s data, we use the data from the other three years as the training set. This applies to both the supervised ILP method and the sentence reranking regression model. All the summaries are evaluated using ROUGE (Lin, 2004). An academic free solver2 does all the ILP decoding and libsvm3 is used for SVR implementation. 3.2 Results Table 2 and Table 3 show the R2 and R-SU4 values on different TAC data sets for the following systems. • ILP baseline. This is the unsupervised ILPbased summarization system (Gillick et al., 2009), in which only bigrams with document frequency greater than 2 are used in the ILP summarization process, and weight wi is the document frequency of that bigram. • TAC best. This is the best result in the TAC update summarization evaluation.4 Note that 2 http://www.gurobi.com http://www.csie.nt"
N15-1145,W09-1801,0,0.0682842,"e, Delort and Alfonseca (2012) proposed a novel nonparametric Bayesian approach, a variant of Latent Dirichlet Allocation (LDA), aiming to distinguish between common information and novel information. Li et al. (2012) borrowed the idea of evolutionary clustering and proposed a three-level HDP (Hierarchical Dirichlet Process) model to represent the diversity and commonality between aspects discovered from two different document data sets. One of the most competitive summarization methods is based on Integer Linear Programming (ILP). It has been widely adopted in the generic summarization task (Martins and Smith, 2009; BergKirkpatrick et al., 2011; Woodsend and Lapata, 2012; Li et al., 2013a; Li et al., 2013b; Li et al., 2014). In this paper, we use the ILP summarization 1317 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1317–1322, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics framework for the update summarization task, and make improvement from two aspects, with the goal to more discriminatively represent both the salience and novelty of words and sentences. First, we use supervised models and a rich set"
N15-1145,C12-1128,0,0.125028,"Missing"
N15-1145,C12-2126,0,0.163482,"Roark, 2008; Long et al., 2010; Bysani, 2010). One important line is to use graphbased co-ranking. They rank the sentences in the earlier and later document sets simultaneously by considering the sentence relationship. For example, Li et al. (2008) was inspired by the intuition that “a sentence receives a positive influence from the sentences that correlate to it in the same collection, whereas receives a negative influence from the sentences that correlates to it in the different (or previously read) collection’, and proposed a graph based sentence ranking algorithm for update summarization. Wan (2012) integrated two co-ranking processes by adding some strict constraints, which led to more accurate computation of sentences’ scores for update summarization. A similar method was also applied earlier by (Wan et al., 2011) for multilingual news summarization. In addition, generative models, such as topic models, have also been adopted for this task. For example, Delort and Alfonseca (2012) proposed a novel nonparametric Bayesian approach, a variant of Latent Dirichlet Allocation (LDA), aiming to distinguish between common information and novel information. Li et al. (2012) borrowed the idea of"
N15-1145,D12-1022,0,0.095154,"metric Bayesian approach, a variant of Latent Dirichlet Allocation (LDA), aiming to distinguish between common information and novel information. Li et al. (2012) borrowed the idea of evolutionary clustering and proposed a three-level HDP (Hierarchical Dirichlet Process) model to represent the diversity and commonality between aspects discovered from two different document data sets. One of the most competitive summarization methods is based on Integer Linear Programming (ILP). It has been widely adopted in the generic summarization task (Martins and Smith, 2009; BergKirkpatrick et al., 2011; Woodsend and Lapata, 2012; Li et al., 2013a; Li et al., 2013b; Li et al., 2014). In this paper, we use the ILP summarization 1317 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1317–1322, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics framework for the update summarization task, and make improvement from two aspects, with the goal to more discriminatively represent both the salience and novelty of words and sentences. First, we use supervised models and a rich set of features to learn the weights for the bigram concepts"
N19-1173,D07-1101,0,0.0263053,"i = j ¯ ij = Lij + exp(ˆ L Lij otherwise −1 ¯ eij = (1 − δ1,j )Aij [L ]jj ¯ −1 ]ji − (1 − δi,1 )Aij [L ¯ −1 ]i1 ri = exp(ˆ ri )[L return ri , eij access to labels for the roots (aka summary sentences), while tree edges are latent and learned without an explicit training signal. And as previous work (Liu and Lapata, 2017) has shown, a single application of TMT leads to shallow tree structures. Secondly, the calculation of r˜i and e˜ij would be based on first-order features alone, however, higher-order information pertaining to siblings and grandchildren has proved useful in discourse parsing (Carreras, 2007). We address these issues with an inference algorithm which iteratively infers latent trees. In contrast to multi-layer neural network architectures like the Transformer or Recursive Neural Networks (Tai et al., 2015) where word representations are updated at every layer based on the output of previous layers, we refine only the tree structure during each iteration, word representations are not passed across multiple layers. Empirically, at early iterations, the model learns shallow and 1748 simple trees, and information propagates mostly between neighboring nodes; as the structure gets more r"
N19-1173,P14-1048,0,0.10089,"Missing"
N19-1173,D14-1168,0,0.0867573,"e-like document representations obtained by a parser trained on discourse annotated corpora (Carlson et al., 2003; Prasad et al., 2008). For instance, Marcu (1999) argues that a good summary can be generated by traversing the RST discourse tree structure top-down, following nucleus nodes (discourse units in RST are characterized regarding their text importance; nuclei denote central units, whereas satellites denote peripheral ones). Other work (Hirao et al., 2013; Yoshida et al., 2014) extends this idea by transforming RST trees into dependency trees and generating summaries by tree trimming. Gerani et al. (2014) summarize product reviews; their system aggregates RST trees rep1745 Proceedings of NAACL-HLT 2019, pages 1745–1755 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics 1. One wily coyote traveled a bit too far from home, and its resulting adventure through Harlem had alarmed residents doing a double take and scampering to get out of its way Wednesday morning. Police say frightened New Yorkers reported the coyote sighting around 9:30 a.m., and an emergency service unit was dispatched to find the animal. The little troublemaker was caught and tranquil"
N19-1173,N18-1150,0,0.0583725,"Missing"
N19-1173,D13-1158,0,0.0555327,"Missing"
N19-1173,P16-1046,1,0.938134,"ually sentences) in a document. Recent 1 Our code is publicly available at https://github. com/nlpyang/SUMO. approaches to (single-document) extractive summarization frame the task as a sequence labeling problem taking advantage of the success of neural network architectures (Bahdanau et al., 2015). The idea is to predict a label for each sentence specifying whether it should be included in the summary. Existing systems mostly rely on recurrent neural networks (Hochreiter and Schmidhuber, 1997) to model the document and obtain a vector representation for each sentence (Nallapati et al., 2017; Cheng and Lapata, 2016). Intersentential relations are captured in a sequential manner, without taking the structure of the document into account, although the latter has been shown to correlate with what readers perceive as important in a text (Marcu, 1999). Another problem in neural-based extractive models is the lack of interpretability. While capable of identifying summary sentences, these models are not able to rationalize their predictions (e.g., a sentence is in the summary because it describes important content upon which other related sentences elaborate). The summarization literature offers examples of mod"
N19-1173,D14-1179,0,0.0207797,"Missing"
N19-1173,J10-3005,1,0.827546,"doc-att) outperform L EAD -3 across metrics. S UMO (3-layer) is competitive or better than stateof-the-art approaches. Examples of system output are shown in Table 4. Finally, we should point out that S UMO is superior to Marcu (1999) even though the latter employs linguistically informed document representations. 3.4 Human Evaluation In addition to automatic evaluation, we also assessed system performance by eliciting human judgments. Our first evaluation quantified the degree to which summarization models retain key information from the document following a question-answering (QA) paradigm (Clarke and Lapata, 2010; Narayan et al., 2018). We created a set of questions based on the gold summary under the assumption that it highlights the most important document content. We then examined whether participants were able to answer these questions by reading system summaries alone without access to the article. The more questions a system can answer, the better it is at summarizing the document as a whole. We randomly selected 20 documents from the CNN/DailyMail and NYT datasets, respectively and wrote multiple question-answer pairs for each gold summary. We created 71 questions in total varying from two to s"
N19-1173,D18-1409,0,0.0886611,"Missing"
N19-1173,P16-1188,0,0.493253,"luated S UMO on two benchmark datasets, namely the CNN/DailyMail news highlights dataset (Hermann et al., 2015) and the New York Times Annotated Corpus (NYT; Sandhaus 2008). The CNN/DailyMail dataset contains news articles and associated highlights, i.e., a few bullet points giving a brief overview of the article. We used the standard splits of Hermann et al. (2015) for training, validation, and testing (90,266/1,220/1,093 CNN documents and 196,961/12,148/10,397 DailyMail documents). We did not anonymize entities. The NYT dataset contains 110,540 articles with abstractive summaries. Following Durrett et al. (2016), we split these into 100,834 training and 9,706 test examples, based on date of publication (test is all articles published on January 1, 2007 or later). We also followed their filtering procedure, documents with summaries that are shorter than 50 words were removed from the raw dataset. The 1749 Model R-1 L EAD-3 29.2 Narayan et al. (2018) 30.4 25.6 Marcu (1999) Durrett et al. (2016) — See et al. (2017) — Celikyilmaz et al. (2018) — Transformer (no doc-att) 29.2 Transformer (1-layer doc-att) 29.5 Transformer (3-layer doc-att) 29.6 S UMO (1-layer) 29.5 S UMO (3-layer) 29.7 CNN R-2 R-L 11.2 26"
N19-1173,P17-2074,0,0.0457368,"Missing"
N19-1173,D07-1015,0,0.723746,"Missing"
N19-1173,D18-1149,0,0.0136327,"tability in the summarization process by helping explain how document content contributes to the model’s decisions. We design a new iterative structure refinement algorithm, which learns to induce document-level structures through repeatedly refining the trees predicted by previous iterations and allows the model to infer complex trees which go beyond simple parent-child relations (Liu and Lapata, 2018; Kim et al., 2017). The idea of structure refinement is conceptually related to recently proposed models for solving iterative inference problems (Marino et al., 2018; Putzky and Welling, 2017; Lee et al., 2018). It is also related to structured prediction energy networks (Belanger et al., 2017) which approach structured prediction as iterative miminization of an energy function. However, we are not aware of any previous work considering structure refinement for tree induction problems. Our contributions in this work are three-fold: a novel conceptualization of extractive summarization as a tree induction problem; a model which capitalizes on the notion of structured attention to learn document representations based on iterative structure refinement; and large-scale evaluation studies (both automatic"
N19-1173,W04-1013,0,0.0496642,"was set to 30K. We used 300D word embeddings which were initialized randomly from N (0, 0.01). The sentence-level Transformer has 6 layers and the hidden size of FFN was set to 512. The number of heads in MHAtt was set to 4. Adam was used for training (β1 = 0.9, β2 = 0.999). We adopted the learning rate schedule from Vaswani et al. (2017) with warming-up on the first 8,000 steps. S UMO and related Transformer models produced 3-sentence summaries for each document at test time (for both CNN/DailyMail and NYT datasets). 3.3 Automatic Evaluation We evaluated summarization quality using ROUGE F1 (Lin, 2004). We report unigram and bigram overlap (ROUGE-1 and ROUGE-2) as a means of assessing informativeness and the longest common subsequence (ROUGE-L) as a means of assessing fluency. Table 1 summarizes our results. We evaluated two variants of S UMO, with one and three structured-attention layers. We compared against a baseline which simply selects the first three sentences in each document (L EAD-3) and several incarnations of the basic Transformer model introduced in Section 2.1. These include a Transformer without document-level self-attention and two variants with document-level self attention"
N19-1173,Q18-1005,1,0.934844,"tractive summary is generated. Despite the intuitive appeal of discourse structure for the summarization task, the reliance on a parser which is both expensive to obtain (since it must be trained on labeled data) and error prone, presents a major obstacle to its widespread use. Recognizing the merits of structure-aware representations for various NLP tasks, recent efforts have focused on learning latent structures (e.g., parse trees) while optimizing a neural network model for a down-stream task. Various methods impose structural constraints on the basic attention mechanism (Kim et al., 2017; Liu and Lapata, 2018), formulate structure learning as a reinforcement learning problem (Yogatama et al., 2017; Williams et al., 2018), or sparsify the set of possible structures (Niculae et al., 2018). Although latent structures are mostly induced for individual sentences, Liu and Lapata (2018) induce dependency-like structures for entire documents. Drawing inspiration from this work and existing discourse-informed summarization models (Marcu, 1999; Hirao et al., 2013), we frame extractive summarization as a tree induction problem. Our model represents documents as multiroot dependency trees where each root node"
N19-1173,W01-0100,0,0.421629,"summarizer1 performs competitively against state-of-the-art methods. 1 Introduction Single-document summarization is the task of automatically generating a shorter version of a document while retaining its most important information. The task has received much attention in the natural language processing community due to its potential for various information access applications. Examples include tools which digest textual content (e.g., news, social media, reviews), answer questions, or provide recommendations. Of the many summarization paradigms that have been identified over the years (see Mani 2001 and Nenkova and McKeown 2011 for comprehensive overviews), two have consistently attracted attention. In abstractive summarization, various text rewriting operations generate summaries using words or phrases that were not in the original text, while extractive approaches form summaries by copying and concatenating the most important spans (usually sentences) in a document. Recent 1 Our code is publicly available at https://github. com/nlpyang/SUMO. approaches to (single-document) extractive summarization frame the task as a sequence labeling problem taking advantage of the success of neural n"
N19-1173,D17-1159,1,0.817496,"i and cli represent parent and child vectors, respectively, while vector zil is updated with contextual information at hop l. At the final iteration (lines 9 and 10), the top sentence embeddings v K−1 are used to calculate the final root probabilities rK . We define the model’s loss function as the summation of the losses of all iterations: L= K X [y log(rk ) + (1 − y) log(1 − rk )] (12) k=1 S UMO uses the root probabilities of the top layer as the scores for summary sentences. The k-Hop-Propagation function resembles the computation used in Graph Convolution Networks (Kipf and Welling, 2017; Marcheggiani and Titov, 2017). GCNs have been been recently applied to latent trees (Corro and Titov, 2019), however not in combination with iterative refinement. 3 Experiments In this section we present our experimental setup, describe the summarization datasets we used, discuss implementation details, our evaluation protocol, and analyze our results. Algorithm 2: Structured Summarization Model Input: Document d Output: Root probabilities r K after K iterations 1 Calculate sentence vectors s using sentence-level Transformer TS 0 2 v ←s 3 for k ← 1 to K − 1 do 4 Calculate unnormalized root scores: r˜ik = Wrk vik−1 5 Calcu"
N19-1173,J93-2004,0,0.0707446,"Missing"
N19-1173,N18-1158,1,0.785996,"of Hermann et al. (2015) for training, validation, and testing (90,266/1,220/1,093 CNN documents and 196,961/12,148/10,397 DailyMail documents). We did not anonymize entities. The NYT dataset contains 110,540 articles with abstractive summaries. Following Durrett et al. (2016), we split these into 100,834 training and 9,706 test examples, based on date of publication (test is all articles published on January 1, 2007 or later). We also followed their filtering procedure, documents with summaries that are shorter than 50 words were removed from the raw dataset. The 1749 Model R-1 L EAD-3 29.2 Narayan et al. (2018) 30.4 25.6 Marcu (1999) Durrett et al. (2016) — See et al. (2017) — Celikyilmaz et al. (2018) — Transformer (no doc-att) 29.2 Transformer (1-layer doc-att) 29.5 Transformer (3-layer doc-att) 29.6 S UMO (1-layer) 29.5 S UMO (3-layer) 29.7 CNN R-2 R-L 11.2 26.0 11.7 26.9 6.10 19.5 — — — — — — 11.1 25.6 11.4 26.0 11.8 26.3 11.6 26.2 12.0 26.5 R-1 40.7 41.0 31.9 — — — 40.5 41.5 41.7 41.6 42.0 DM R-2 R-L 18.3 37.2 18.8 37.7 12.4 23.5 — — — — — — 18.1 36.8 18.7 38.0 18.8 38.0 18.8 37.6 19.1 38.0 CNN+DM R-1 R-2 R-L 39.6 17.7 36.2 40.0 18.2 36.6 26.5 9.80 20.4 — — — 39.5 17.3 36.4 41.7 19.5 37.9 39.7"
N19-1173,D18-1108,0,0.0129512,"must be trained on labeled data) and error prone, presents a major obstacle to its widespread use. Recognizing the merits of structure-aware representations for various NLP tasks, recent efforts have focused on learning latent structures (e.g., parse trees) while optimizing a neural network model for a down-stream task. Various methods impose structural constraints on the basic attention mechanism (Kim et al., 2017; Liu and Lapata, 2018), formulate structure learning as a reinforcement learning problem (Yogatama et al., 2017; Williams et al., 2018), or sparsify the set of possible structures (Niculae et al., 2018). Although latent structures are mostly induced for individual sentences, Liu and Lapata (2018) induce dependency-like structures for entire documents. Drawing inspiration from this work and existing discourse-informed summarization models (Marcu, 1999; Hirao et al., 2013), we frame extractive summarization as a tree induction problem. Our model represents documents as multiroot dependency trees where each root node is a summary sentence, and the subtrees attached to it are sentences whose content is related to and covered by the summary sentence. An example of a document and its corresponding"
N19-1173,prasad-etal-2008-penn,0,0.0815454,"le of identifying summary sentences, these models are not able to rationalize their predictions (e.g., a sentence is in the summary because it describes important content upon which other related sentences elaborate). The summarization literature offers examples of models which exploit the structure of the underlying document, inspired by existing theories of discourse such as Rhetorical Structure Theory (RST; Mann and Thompson 1988). Most approaches produce summaries based on tree-like document representations obtained by a parser trained on discourse annotated corpora (Carlson et al., 2003; Prasad et al., 2008). For instance, Marcu (1999) argues that a good summary can be generated by traversing the RST discourse tree structure top-down, following nucleus nodes (discourse units in RST are characterized regarding their text importance; nuclei denote central units, whereas satellites denote peripheral ones). Other work (Hirao et al., 2013; Yoshida et al., 2014) extends this idea by transforming RST trees into dependency trees and generating summaries by tree trimming. Gerani et al. (2014) summarize product reviews; their system aggregates RST trees rep1745 Proceedings of NAACL-HLT 2019, pages 1745–175"
N19-1173,P17-1099,0,0.725316,"66/1,220/1,093 CNN documents and 196,961/12,148/10,397 DailyMail documents). We did not anonymize entities. The NYT dataset contains 110,540 articles with abstractive summaries. Following Durrett et al. (2016), we split these into 100,834 training and 9,706 test examples, based on date of publication (test is all articles published on January 1, 2007 or later). We also followed their filtering procedure, documents with summaries that are shorter than 50 words were removed from the raw dataset. The 1749 Model R-1 L EAD-3 29.2 Narayan et al. (2018) 30.4 25.6 Marcu (1999) Durrett et al. (2016) — See et al. (2017) — Celikyilmaz et al. (2018) — Transformer (no doc-att) 29.2 Transformer (1-layer doc-att) 29.5 Transformer (3-layer doc-att) 29.6 S UMO (1-layer) 29.5 S UMO (3-layer) 29.7 CNN R-2 R-L 11.2 26.0 11.7 26.9 6.10 19.5 — — — — — — 11.1 25.6 11.4 26.0 11.8 26.3 11.6 26.2 12.0 26.5 R-1 40.7 41.0 31.9 — — — 40.5 41.5 41.7 41.6 42.0 DM R-2 R-L 18.3 37.2 18.8 37.7 12.4 23.5 — — — — — — 18.1 36.8 18.7 38.0 18.8 38.0 18.8 37.6 19.1 38.0 CNN+DM R-1 R-2 R-L 39.6 17.7 36.2 40.0 18.2 36.6 26.5 9.80 20.4 — — — 39.5 17.3 36.4 41.7 19.5 37.9 39.7 17.0 35.9 40.6 18.1 36.7 40.6 18.1 36.9 40.5 18.0 36.8 41.0 18.4"
N19-1173,P15-1150,0,0.168463,"Missing"
N19-1173,Q18-1019,0,0.0619806,"Missing"
N19-1173,D14-1196,0,0.0219041,"ries of discourse such as Rhetorical Structure Theory (RST; Mann and Thompson 1988). Most approaches produce summaries based on tree-like document representations obtained by a parser trained on discourse annotated corpora (Carlson et al., 2003; Prasad et al., 2008). For instance, Marcu (1999) argues that a good summary can be generated by traversing the RST discourse tree structure top-down, following nucleus nodes (discourse units in RST are characterized regarding their text importance; nuclei denote central units, whereas satellites denote peripheral ones). Other work (Hirao et al., 2013; Yoshida et al., 2014) extends this idea by transforming RST trees into dependency trees and generating summaries by tree trimming. Gerani et al. (2014) summarize product reviews; their system aggregates RST trees rep1745 Proceedings of NAACL-HLT 2019, pages 1745–1755 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics 1. One wily coyote traveled a bit too far from home, and its resulting adventure through Harlem had alarmed residents doing a double take and scampering to get out of its way Wednesday morning. Police say frightened New Yorkers reported the coyote sighting"
N19-1173,D18-1088,1,0.863752,"f-the-art methods while being able to rationalize model predictions. 2 Model Description Let d denote a document containing several sentences [sent1 , sent2 , · · · , sentm ], where senti is the i-th sentence in the document. Extractive summarization can be defined as the task of assigning a label yi ∈ {0, 1} to each senti , indicating whether the sentence should be included in the summary. It is assumed that summary sentences represent the most important content of the document. 1746 2.1 Baseline Model Most extractive models frame summarization as a classification problem. Recent approaches (Zhang et al., 2018; Dong et al., 2018; Nallapati et al., 2017; Cheng and Lapata, 2016) incorporate a neural network-based encoder to build representations for sentences and apply a binary classifier over these representations to predict whether the sentences should be included in the summary. Given predicted scores r and gold labels y, the loss function can be defined as: L=− m X For our extractive summarization task, the baseline system is composed of a sentence-level Transformer (TS ) and a document-level Transformer (TD ), which have the same structure. For each sentence si = [wi1 , wi2 , · · · , win ] in th"
N19-1173,D17-1225,0,0.0143628,"attention instantiated with one and three layers. Several stateof-the-art models are also included in Table 1, both extractive and abstractive. R EFRESH (Narayan et al., 2018) is an extractive summarization system trained by globally optimizing the ROUGE metric with reinforcement learning. The system of Marcu (1999) is another extractive summarizer based on RST parsing. It uses discourse structures and RST’s notion of nuclearity to score document sentences in terms of their importance and selects the most important ones as the summary. Our re-implementation of Marcu (1999) used the parser of Zhao and Huang (2017) to obtain RST trees. Durrett et al. (2016) develop a summarization system which integrates a compression model that enforces grammaticality and coherence. See et al. (2017) present an abstractive summarization system based on 1750 an encoder-decoder architecture. Celikyilmaz et al.’s (2018) system is state-of-the-art in abstractive summarization using multiple agents to represent the document as well a hierarchical attention mechanism over the agents for decoding. As far as S UMO is concerned, we observe that it outperforms a simple Transformer model without any document attention as well as"
P05-1056,W04-3209,1,0.68097,"rion more closely related to classification performance. Second, the N-gram LM underlying the HMM transition model makes it difficult to use features that are highly correlated (such as words and POS labels) without greatly increasing the number of model parameters, which in turn would make robust estimation difficult. More details about using textual information in the HMM system are provided in Section 3. 1.2 Sentence Segmentation Using Maxent A maximum entropy (Maxent) posterior classification method has been evaluated in an attempt to overcome some of the shortcomings of the HMM approach (Liu et al., 2004; Huang and Zweig, 2002). For a boundary position i, the Maxent model takes the exponential form: ( ij P E i T ;F i) = 1  (Ti ; Fi ) Z e P k Fi Oi In the HMM, the forward-backward algorithm is used to determine the event with the highest posterior probability for each interword boundary: Ei Wi+1 Ei+1 i?1 Ei?1 ) W 1 E1 : : : W E Ei k gk (Ei ;Ti ;Fi ) (2) where Z (Ti ; Fi ) is a normalization term and Ti represents textual information. The indicator functions gk (Ei ; Ti ; Fi ) correspond to features defined over events, words, and prosody. The parameters in 2 In the prosody model implementat"
P05-1056,W03-0430,0,0.0544118,"tection task. The rest of the paper is organized as follows. Section 2 describes the CRF model and discusses how it differs from the HMM and Maxent models. Section 3 describes the data and features used in the models to be compared. Section 4 summarizes the experimental results for the sentence boundary detection task. Conclusions and future work appear in Section 5. 2 CRF Model Description E1 A CRF is a random field that is globally conditioned on an observation sequence O . CRFs have been successfully used for a variety of text processing tasks (Lafferty et al., 2001; Sha and Pereira, 2003; McCallum and Li, 2003), but they have not been widely applied to a speech-related task with both acoustic and textual knowledge sources. The top graph in Figure 2 is a general CRF model. The states of the model correspond to event labels E . The observations O are composed of the textual features, as well as the ^ prosodic features. The most likely event sequence E for the given input sequence (observations) O is ^ = arg max E P e E k k Gk (E;O) Z  (O ) (3) where the functions G are potential functions over the events and the observations, and Z is the normalization term:  (O ) Z = X P G e k k E;O) k( (4) E Ev"
P05-1056,H89-1002,0,0.36263,"Missing"
P05-1056,N03-1028,0,0.0216611,"he sentence boundary detection task. The rest of the paper is organized as follows. Section 2 describes the CRF model and discusses how it differs from the HMM and Maxent models. Section 3 describes the data and features used in the models to be compared. Section 4 summarizes the experimental results for the sentence boundary detection task. Conclusions and future work appear in Section 5. 2 CRF Model Description E1 A CRF is a random field that is globally conditioned on an observation sequence O . CRFs have been successfully used for a variety of text processing tasks (Lafferty et al., 2001; Sha and Pereira, 2003; McCallum and Li, 2003), but they have not been widely applied to a speech-related task with both acoustic and textual knowledge sources. The top graph in Figure 2 is a general CRF model. The states of the model correspond to event labels E . The observations O are composed of the textual features, as well as the ^ prosodic features. The most likely event sequence E for the given input sequence (observations) O is ^ = arg max E P e E k k Gk (E;O) Z  (O ) (3) where the functions G are potential functions over the events and the observations, and Z is the normalization term:  (O ) Z = X P"
P05-1057,J00-2004,0,0.367369,"Missing"
P05-1057,P02-1038,0,0.441599,"of IBM translation models and HMM model. Although Model 6 yields better results than naive IBM models, it fails to include dependencies other than IBM models and HMM model. Cherry and Lin (2003) developed a 459 Proceedings of the 43rd Annual Meeting of the ACL, pages 459–466, c Ann Arbor, June 2005. 2005 Association for Computational Linguistics statistical model to find word alignments, which allow easy integration of context-specific features. Log-linear models, which are very suitable to incorporate additional dependencies, have been successfully applied to statistical machine translation (Och and Ney, 2002). In this paper, we present a framework for word alignment based on log-linear models, allowing statistical models to be easily extended by incorporating additional syntactic dependencies. We use IBM Model 3 alignment probabilities, POS correspondence, and bilingual dictionary coverage as features. Our experiments show that log-linear models significantly outperform IBM translation models. We begin by describing log-linear models for word alignment. The design of feature functions is discussed then. Next, we present the training method and the search algorithm for log-linear models. We will fo"
P05-1057,P03-1021,0,0.0507453,"Missing"
P05-1057,J03-1002,0,0.488153,"iu, liuqun, sxlin}@ict.ac.cn Abstract data, try to describe the relationship between a bilingual sentence pair (Brown et al., 1993; Vogel and Ney, 1996). Heuristic approaches obtain word alignments by using various similarity functions between the types of the two languages (Smadja et al., 1996; Ker and Chang, 1997; Melamed, 2000). The central distinction between statistical and heuristic approaches is that statistical approaches are based on well-founded probabilistic models while heuristic ones are not. Studies reveal that statistical alignment models outperform the simple Dice coefficient (Och and Ney, 2003). We present a framework for word alignment based on log-linear models. All knowledge sources are treated as feature functions, which depend on the source langauge sentence, the target language sentence and possible additional variables. Log-linear models allow statistical alignment models to be easily extended by incorporating syntactic information. In this paper, we use IBM Model 3 alignment probabilities, POS correspondence, and bilingual dictionary coverage as features. Our experiments show that log-linear models significantly outperform IBM translation models. 1 Introduction Word alignmen"
P05-1057,J96-1002,0,0.0242527,"Missing"
P05-1057,J96-1001,0,0.125699,"Missing"
P05-1057,J95-4004,0,0.0974074,"ptimized on the development corpus. Therefore, we have a new search algorithm: Input: e, f , eT, fT, D and t Output: a 1. Start with a = φ. 2. Do for each l = (i, j) and l ∈ / a: Compute gain(a, l) 3 We still call the new heuristic function gain to reduce notational overhead, although the gain in Eq. 13 is not equivalent to the one in Eq. 12. 463 The Chinese sentences in both the development and test corpus are segmented and POS tagged by ICTCLAS (Zhang et al., 2003). The English sentences are tokenized by a simple tokenizer of ours and POS tagged by a rule-based tagger written by Eric Brill (Brill, 1995). We manually aligned 935 sentences, in which we selected 500 sentences as test corpus. The remaining 435 sentences are used as development corpus to train POS tags transition probabilities and to optimize the model parameters and gain threshold. Provided with human-annotated word-level alignment, we use precision, recall and AER (Och and Model 3 E → C Model 3 C → E Intersection Union Refined Method Model 3 E → C + Model 3 C → E + POS E → C + POS C → E + Dict 1K 0.4497 0.4688 0.4588 0.4596 0.4154 0.4490 0.3970 0.3828 0.3795 0.3650 Size of Training Corpus 5K 9K 39K 0.4081 0.4009 0.3791 0.4261 0"
P05-1057,E03-1026,0,0.0457116,"aches and heuristic approaches. Statistical approaches, which depend on a set of unknown parameters that are learned from training Finding word alignments between parallel texts, however, is still far from a trivial work due to the diversity of natural languages. For example, the alignment of words within idiomatic expressions, free translations, and missing content or function words is problematic. When two languages widely differ in word order, finding word alignments is especially hard. Therefore, it is necessary to incorporate all useful linguistic information to alleviate these problems. Tiedemann (2003) introduced a word alignment approach based on combination of association clues. Clues combination is done by disjunction of single clues, which are defined as probabilities of associations. The crucial assumption of clue combination that clues are independent of each other, however, is not always true. Och and Ney (2003) proposed Model 6, a log-linear combination of IBM translation models and HMM model. Although Model 6 yields better results than naive IBM models, it fails to include dependencies other than IBM models and HMM model. Cherry and Lin (2003) developed a 459 Proceedings of the 43r"
P05-1057,J93-2003,0,0.0881567,"Missing"
P05-1057,P03-1012,0,0.464916,"ic information to alleviate these problems. Tiedemann (2003) introduced a word alignment approach based on combination of association clues. Clues combination is done by disjunction of single clues, which are defined as probabilities of associations. The crucial assumption of clue combination that clues are independent of each other, however, is not always true. Och and Ney (2003) proposed Model 6, a log-linear combination of IBM translation models and HMM model. Although Model 6 yields better results than naive IBM models, it fails to include dependencies other than IBM models and HMM model. Cherry and Lin (2003) developed a 459 Proceedings of the 43rd Annual Meeting of the ACL, pages 459–466, c Ann Arbor, June 2005. 2005 Association for Computational Linguistics statistical model to find word alignments, which allow easy integration of context-specific features. Log-linear models, which are very suitable to incorporate additional dependencies, have been successfully applied to statistical machine translation (Och and Ney, 2002). In this paper, we present a framework for word alignment based on log-linear models, allowing statistical models to be easily extended by incorporating additional syntactic d"
P05-1057,C96-2141,0,0.953108,"Missing"
P05-1057,P03-1011,0,0.0495679,"Missing"
P05-1057,J97-2004,0,0.032521,"Missing"
P05-1057,P01-1067,0,0.229123,"Missing"
P05-1057,W03-1730,1,0.260281,"l, e, f ) − hm (a, e, f )] Note that we restrict h(a, e, f ) ≥ 0 for all feature functions. Gain threshold t is a real-valued number, which can be optimized on the development corpus. Therefore, we have a new search algorithm: Input: e, f , eT, fT, D and t Output: a 1. Start with a = φ. 2. Do for each l = (i, j) and l ∈ / a: Compute gain(a, l) 3 We still call the new heuristic function gain to reduce notational overhead, although the gain in Eq. 13 is not equivalent to the one in Eq. 12. 463 The Chinese sentences in both the development and test corpus are segmented and POS tagged by ICTCLAS (Zhang et al., 2003). The English sentences are tokenized by a simple tokenizer of ours and POS tagged by a rule-based tagger written by Eric Brill (Brill, 1995). We manually aligned 935 sentences, in which we selected 500 sentences as test corpus. The remaining 435 sentences are used as development corpus to train POS tags transition probabilities and to optimize the model parameters and gain threshold. Provided with human-annotated word-level alignment, we use precision, recall and AER (Och and Model 3 E → C Model 3 C → E Intersection Union Refined Method Model 3 E → C + Model 3 C → E + POS E → C + POS C → E +"
P05-1057,W02-1012,0,\N,Missing
P06-1021,N01-1016,0,0.406874,"rally lack appropriate rules for analyzing these constructions. One possible response to this mismatch between grammatical resources and the brute facts of disfluent speech is to make one look more like the other, for the purpose of parsing. In this separate-processing approach, reparanda are located through a variety of acoustic, lexical or string-based techniques, then excised before submission to a parser (Stolcke and Shriberg, 1996; Heeman and Allen, 1999; Spilker et al., 2000; Johnson and Charniak, 2004). The resulting parse tree then has the reparandum re-attached in a standardized way (Charniak and Johnson, 2001). An alternative strategy, adopted in this paper, is to use the same grammar to model fluent speech, disfluent speech, and their interleaving. Such an integrated approach can use syntactic properties of the reparandum itself. For instance, in example (1) the reparandum is an unfinished noun phrase, the repair a finished noun phrase. This sort of phrasal correspondence, while not absolute, is strong in conversational speech, and cannot be exploited on the separate-processing approach. Section 3 applies metarules (Weischedel and Sondheimer, 1983; McKelvie, 1998a; Core and Schubert, 1999) in reco"
P06-1021,A00-2018,0,0.0169867,"ation × √ both × √ EDIT F none Parseval F Break index POST (Ratnaparkhi, 1996) which was itself trained on Switchboard. Finally, as described in section 2 these tags were augmented with a special prosodic break symbol if the decision tree rated the probability a ToBI ‘p’ symbol higher than the threshold value of 0.75. Annotation speech repairs. The first two use the CYK algorithm to find the most likely parse tree on a grammar read-off from example trees annotated as in Figures 2 and 4. The third experiment measures the benefit from syntactic indicators alone in Charniak’s lexicalized parser (Charniak, 2000). The tables in subsections 4.1, 4.2, and 4.3 summarize the accuracy of output parse trees on two measures. One is the standard Parseval F-measure, which tracks the precision and recall for all labeled constituents as compared to a gold-standard parse. The other measure, EDIT-finding F, restricts consideration to just constituents that are reparanda. It measures the per-word performance identifying a word as dominated by EDITED or not. As in previous studies, reference transcripts were used in all √ cases. A check ( ) indicates an experiment where prosodic breaks where automatically inferred b"
P06-1021,P99-1053,0,0.591037,"way (Charniak and Johnson, 2001). An alternative strategy, adopted in this paper, is to use the same grammar to model fluent speech, disfluent speech, and their interleaving. Such an integrated approach can use syntactic properties of the reparandum itself. For instance, in example (1) the reparandum is an unfinished noun phrase, the repair a finished noun phrase. This sort of phrasal correspondence, while not absolute, is strong in conversational speech, and cannot be exploited on the separate-processing approach. Section 3 applies metarules (Weischedel and Sondheimer, 1983; McKelvie, 1998a; Core and Schubert, 1999) in recognizing these correspondences using standard context-free grammars. At the same time as it defies parsing, conversational speech offers the possibility of leveraging prosodic cues to speech repairs. SecA grammatical method of combining two kinds of speech repair cues is presented. One cue, prosodic disjuncture, is detected by a decision tree-based ensemble classifier that uses acoustic cues to identify where normal prosody seems to be interrupted (Lickley, 1996). The other cue, syntactic parallelism, codifies the expectation that repairs continue a syntactic category that was left unfi"
P06-1021,N04-1011,0,0.0312862,"Missing"
P06-1021,H05-1030,1,0.8458,"ecision and recall trade-off on its detection can be adjusted using a threshold on the posterior probability of predicting “p”, as shown in Figure 3. In essence, the large number of acoustic and prosodic features related to disfluency are encoded via the ToBI label ‘p’, and provided as additional observations to the PCFG. This is unlike previous work on incorporating prosodic information (Gre0.6 0.5 Probability of Miss 0.4 0.3 0.2 0.1 0 0 0.1 0.2 0.3 Probability of False Alarm 0.4 0.5 0.6 Figure 3: DET curve for detecting disfluent breaks from acoustics. gory et al., 2004; Lease et al., 2005; Kahn et al., 2005) as described further in Section 6. 3 Syntactic parallelism The other striking property of speech repairs is their parallel character: subsequent repair regions ‘line up’ with preceding reparandum regions. This property can be harnessed to better estimate the length of the reparandum by considering parallelism from the perspective of syntax. For instance, in Figure 4(a) the unfinished reparandum noun phrase is repaired by another noun phrase – the syntactic categories are parallel. 3.1 Levelt’s WFR and Conjunction The idea that the reparandum is syntactically parallel to the repair can be trac"
P06-1021,J93-2004,0,0.0291288,"ion of Levelt’s WFR can be applied to Treebanks by systematically recoding the annotations to indicate which phrases are unfinished and to distinguish matching from nonmatching repairs. (2) If, as schema (2) suggests, conjunction does favor like-categories, and, as Levelt suggests, wellformed repairs are conjoinable with finished versions of their reparanda, then the syntactic categories of repairs ought to match the syntactic categories of (finished versions of) reparanda. 3.2 (3) 3.3.1 Unfinished phrases Some Treebanks already mark unfinished phrases. For instance, the Penn Treebank policy (Marcus et al., 1993; Marcus et al., 1994) is to annotate the lowest node that is unfinished with an -UNF tag as in Figure 4(a). It is straightforward to propagate this mark upwards in the tree from wherever it is annotated to the nearest enclosing EDITED node, just as -BRK is propagated upwards from disjuncture marks on individual words. This percolation simulates the action of McKelvie’s [abort = true]. The resulting PCFG is one in which distributions on phrase structure rules with ‘missing’ daughters are segregated from distributions on ‘complete’ rules. A WFR for grammars Levelt’s WFR imposes two requirements"
P06-1021,C00-2169,0,0.0526183,"Missing"
P06-1021,H94-1020,0,0.0752404,"an be applied to Treebanks by systematically recoding the annotations to indicate which phrases are unfinished and to distinguish matching from nonmatching repairs. (2) If, as schema (2) suggests, conjunction does favor like-categories, and, as Levelt suggests, wellformed repairs are conjoinable with finished versions of their reparanda, then the syntactic categories of repairs ought to match the syntactic categories of (finished versions of) reparanda. 3.2 (3) 3.3.1 Unfinished phrases Some Treebanks already mark unfinished phrases. For instance, the Penn Treebank policy (Marcus et al., 1993; Marcus et al., 1994) is to annotate the lowest node that is unfinished with an -UNF tag as in Figure 4(a). It is straightforward to propagate this mark upwards in the tree from wherever it is annotated to the nearest enclosing EDITED node, just as -BRK is propagated upwards from disjuncture marks on individual words. This percolation simulates the action of McKelvie’s [abort = true]. The resulting PCFG is one in which distributions on phrase structure rules with ‘missing’ daughters are segregated from distributions on ‘complete’ rules. A WFR for grammars Levelt’s WFR imposes two requirements on a grammar • distin"
P06-1021,J83-3003,0,0.415711,"has the reparandum re-attached in a standardized way (Charniak and Johnson, 2001). An alternative strategy, adopted in this paper, is to use the same grammar to model fluent speech, disfluent speech, and their interleaving. Such an integrated approach can use syntactic properties of the reparandum itself. For instance, in example (1) the reparandum is an unfinished noun phrase, the repair a finished noun phrase. This sort of phrasal correspondence, while not absolute, is strong in conversational speech, and cannot be exploited on the separate-processing approach. Section 3 applies metarules (Weischedel and Sondheimer, 1983; McKelvie, 1998a; Core and Schubert, 1999) in recognizing these correspondences using standard context-free grammars. At the same time as it defies parsing, conversational speech offers the possibility of leveraging prosodic cues to speech repairs. SecA grammatical method of combining two kinds of speech repair cues is presented. One cue, prosodic disjuncture, is detected by a decision tree-based ensemble classifier that uses acoustic cues to identify where normal prosody seems to be interrupted (Lickley, 1996). The other cue, syntactic parallelism, codifies the expectation that repairs conti"
P06-1021,W05-1519,0,0.0261845,"Missing"
P06-1021,H91-1073,0,0.273826,"onstituents labeled EDITED. Such NP NP NP Prosodic disjuncture Everyday experience as well as acoustic analysis suggests that the syntactic interruption in speech repairs is typically accompanied by a change in prosody (Nakatani and Hirschberg, 1994; Shriberg, 1994). For instance, the spectrogram corresponding to example (2), shown in Figure 1, (2) DT NNP the jehovah NNP POS witness EDITED CC NP CC−BRK or NNPS or~+ mormons ’s Figure 2: Propagating BRK, the evidence of disfluent juncture, from acoustics to syntax. disjuncture symbols are identified in the ToBI labeling scheme as break indices (Price et al., 1991; Silverman et al., 1992). The availability of a corpus annotated with ToBI labels makes it possible to design a break index classifier via supervised training. The corpus is a subset of the Switchboard corpus, consisting of sixty-four telephone conversations manually annotated by an experienced linguist according to a simplified ToBI labeling scheme (Ostendorf et al., 2001). In ToBI, degree of disjuncture is indicated by integer values from 0 to 4, where a value of 0 corresponds to clitic and 4 to a major phrase break. In addition, a suffix p denotes perceptually disfluent events reflecting,"
P06-1021,W96-0213,0,\N,Missing
P06-1021,P83-1019,0,\N,Missing
P06-1021,J99-4003,0,\N,Missing
P06-1021,P04-1005,0,\N,Missing
P06-1077,P02-1038,0,0.220937,") (6) D = X D = X D (1) P r(D|T (f1J ), f1J ) K Y P r(S˜k |T˜k ) (7) k=1 3 The notational convention will be as follows. We use the symbol P r(·) to denote general probability distribution with no specific assumptions. In contrast, for model-based probability distributions, we use generic symbol p(·). 2 We use T (·) to denote a parse tree. To reduce notational overhead, we use T (z) to represent the parse tree in z. Similarly, S(z) denotes the string in z. 610 中国 的 经济 发展 ⇒ X3 X4 of China parsing ⇒ economic X4 of China NP DNP ⇒ economic development of China NP NP DEG NN NN NR 的 经济 发展 Following Och and Ney (2002), we base our model on log-linear framework. Hence, all knowledge sources are described as feature functions that include the given source string f1J , the target string eI1 , and hidden variables. The hidden variable T (f1J ) is omitted because we usually make use of only single best output of a parser. As we assume that all detachment have the same probability, the hidden variable D is also omitted. As a result, the model we actually adopt for experiments is limited because the parse, detachment, and TAT application sub-models are simplified. 中国 detachment production NP DNP NP NP NP NP NR NN"
P06-1077,J00-1004,0,0.0168889,"to be observed on training data. However, a key limitation of phrase-based models is that they fail to model reordering at the phrase level robustly. Typically, phrase reordering is modeled in terms of offset positions at the word level (Koehn, 2004; Och and Ney, 2004), making little or no direct use of syntactic information. Recent research on statistical machine translation has lead to the development of syntax-based models. Wu (1997) proposes Inversion Transduction Grammars, treating translation as a process of parallel parsing of the source and target language via a synchronized grammar. Alshawi et al. (2000) represent each production in parallel dependency tree as a finite transducer. Melamed (2004) formalizes machine translation problem as synchronous parsing based on multitext grammars. Graehl and Knight (2004) describe training and decoding algorithms for both generalized tree-to-tree and tree-to-string transducers. Chiang (2005) presents a hierarchical phrasebased model that uses hierarchical phrase pairs, which are formally productions of a synchronous context-free grammar. Ding and Palmer (2005) propose a syntax-based translation model based on a probabilistic synchronous dependency insert"
P06-1077,J04-4002,0,0.859996,"g both terminals and non-terminals and performing reordering at both low and high levels. The model is linguistically syntaxbased because TATs are extracted automatically from word-aligned, source side parsed parallel texts. To translate a source sentence, we first employ a parser to produce a source parse tree and then apply TATs to transform the tree into a target string. Our experiments show that the TAT-based model significantly outperforms Pharaoh, a state-of-the-art decoder for phrase-based models. 1 Introduction Phrase-based translation models (Marcu and Wong, 2002; Koehn et al., 2003; Och and Ney, 2004), which go beyond the original IBM translation models (Brown et al., 1993) 1 by modeling translations of phrases rather than individual words, have been suggested to be the state-of-theart in statistical machine translation by empirical evaluations. In phrase-based models, phrases are usually strings of adjacent words instead of syntactic constituents, excelling at capturing local reordering and performing translations that are localized to 1 The mathematical notation we use in this paper is taken from that paper: a source string f1J = f1 , . . . , fj , . . . , fJ is to be translated into a ta"
P06-1077,J93-2003,0,0.0130061,"Missing"
P06-1077,P03-1021,0,0.0395834,"Missing"
P06-1077,P02-1040,0,0.105269,"Missing"
P06-1077,P05-1034,0,0.674858,"ematical notation we use in this paper is taken from that paper: a source string f1J = f1 , . . . , fj , . . . , fJ is to be translated into a target string eI1 = e1 , . . . , ei , . . . , eI . Here, I is the length of the target string, and J is the length of the source string. 609 Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 609–616, c Sydney, July 2006. 2006 Association for Computational Linguistics operations that transform a target parse tree into a source string. Paying more attention to source language analysis, Quirk et al. (2005) employ a source language dependency parser, a target language word segmentation component, and an unsupervised word alignment component to learn treelet translations from parallel corpus. In this paper, we propose a statistical translation model based on tree-to-string alignment template which describes the alignment between a source parse tree and a target string. A TAT is capable of generating both terminals and non-terminals and performing reordering at both low and high levels. The model is linguistically syntax-based because TATs are extracted automatically from word-aligned, source side"
P06-1077,P05-1033,0,0.68391,"earch on statistical machine translation has lead to the development of syntax-based models. Wu (1997) proposes Inversion Transduction Grammars, treating translation as a process of parallel parsing of the source and target language via a synchronized grammar. Alshawi et al. (2000) represent each production in parallel dependency tree as a finite transducer. Melamed (2004) formalizes machine translation problem as synchronous parsing based on multitext grammars. Graehl and Knight (2004) describe training and decoding algorithms for both generalized tree-to-tree and tree-to-string transducers. Chiang (2005) presents a hierarchical phrasebased model that uses hierarchical phrase pairs, which are formally productions of a synchronous context-free grammar. Ding and Palmer (2005) propose a syntax-based translation model based on a probabilistic synchronous dependency insert grammar, a version of synchronous grammars defined on dependency trees. All these approaches, though different in formalism, make use of synchronous grammars or tree-based transduction rules to model both source and target languages. Another class of approaches make use of syntactic information in the target language alone, treat"
P06-1077,P05-1067,0,0.44874,"on as a process of parallel parsing of the source and target language via a synchronized grammar. Alshawi et al. (2000) represent each production in parallel dependency tree as a finite transducer. Melamed (2004) formalizes machine translation problem as synchronous parsing based on multitext grammars. Graehl and Knight (2004) describe training and decoding algorithms for both generalized tree-to-tree and tree-to-string transducers. Chiang (2005) presents a hierarchical phrasebased model that uses hierarchical phrase pairs, which are formally productions of a synchronous context-free grammar. Ding and Palmer (2005) propose a syntax-based translation model based on a probabilistic synchronous dependency insert grammar, a version of synchronous grammars defined on dependency trees. All these approaches, though different in formalism, make use of synchronous grammars or tree-based transduction rules to model both source and target languages. Another class of approaches make use of syntactic information in the target language alone, treating the translation problem as a parsing problem. Yamada and Knight (2001) use a parser in the target language to train probabilities on a set of We present a novel transla"
P06-1077,N04-1035,0,0.79996,"h low and high levels. The model is linguistically syntax-based because TATs are extracted automatically from word-aligned, source side parsed parallel texts. To translate a source sentence, we first employ a parser to produce a source parse tree and then apply TATs to transform the tree into a target string. One advantage of our model is that TATs can be automatically acquired to capture linguistically motivated reordering at both low (word) and high (phrase, clause) levels. In addition, the training of TAT-based model is less computationally expensive than tree-to-tree models. Similarly to (Galley et al., 2004), the tree-to-string alignment templates discussed in this paper are actually transformation rules. The major difference is that we model the syntax of the source language instead of the target side. As a result, the task of our decoder is to find the best target string while Galley’s is to seek the most likely target tree. 2 Figure 1 shows three TATs automatically learned from training data. Note that when demonstrating a TAT graphically, we represent non-terminals in the target strings by blanks. NP NR NN 布什 总统 President Bush NP NR CC 美国 和 NP LC NR DNP 间 NP NP DEG between United States and F"
P06-1077,2005.eamt-1.36,0,0.0313335,"Missing"
P06-1077,N04-1014,0,0.0569363,"set positions at the word level (Koehn, 2004; Och and Ney, 2004), making little or no direct use of syntactic information. Recent research on statistical machine translation has lead to the development of syntax-based models. Wu (1997) proposes Inversion Transduction Grammars, treating translation as a process of parallel parsing of the source and target language via a synchronized grammar. Alshawi et al. (2000) represent each production in parallel dependency tree as a finite transducer. Melamed (2004) formalizes machine translation problem as synchronous parsing based on multitext grammars. Graehl and Knight (2004) describe training and decoding algorithms for both generalized tree-to-tree and tree-to-string transducers. Chiang (2005) presents a hierarchical phrasebased model that uses hierarchical phrase pairs, which are formally productions of a synchronous context-free grammar. Ding and Palmer (2005) propose a syntax-based translation model based on a probabilistic synchronous dependency insert grammar, a version of synchronous grammars defined on dependency trees. All these approaches, though different in formalism, make use of synchronous grammars or tree-based transduction rules to model both sour"
P06-1077,J97-3002,0,0.322061,"of Sciences No.6 Kexueyuan South Road, Haidian District P. O. Box 2704, Beijing, 100080, China {yliu,liuqun,sxlin}@ict.ac.cn Abstract substrings that are common enough to be observed on training data. However, a key limitation of phrase-based models is that they fail to model reordering at the phrase level robustly. Typically, phrase reordering is modeled in terms of offset positions at the word level (Koehn, 2004; Och and Ney, 2004), making little or no direct use of syntactic information. Recent research on statistical machine translation has lead to the development of syntax-based models. Wu (1997) proposes Inversion Transduction Grammars, treating translation as a process of parallel parsing of the source and target language via a synchronized grammar. Alshawi et al. (2000) represent each production in parallel dependency tree as a finite transducer. Melamed (2004) formalizes machine translation problem as synchronous parsing based on multitext grammars. Graehl and Knight (2004) describe training and decoding algorithms for both generalized tree-to-tree and tree-to-string transducers. Chiang (2005) presents a hierarchical phrasebased model that uses hierarchical phrase pairs, which are"
P06-1077,N03-1017,0,0.154587,"Missing"
P06-1077,I05-1007,1,0.458254,"Missing"
P06-1077,koen-2004-pharaoh,0,0.0144452,"ion economic development of China P r(eI1 , z1K |f1J ) Figure 2: Graphic illustration for translation process P I J K exp[ M m=1 λm hm (e1 , f1 , z1 )] =P PM J 0K 0I e0 I ,z 0 K exp[ m=1 λm hm (e 1 , f1 , z 1 )] 1 ˜ T˜), the tree-toTo further decompose P r(S| string alignment template, denoted by the variable z, is introduced as a hidden variable. = X X ˜ z|T˜) P r(S, (8) ˜ T˜) P r(z|T˜)P r(S|z, (9) ˜ T˜) = P r(S| eˆI1 1 = argmax X M eI1 ,z1K  λm hm (eI1 , f1J , z1K ) m=1 For our experiments we use the following seven feature functions 4 that are analogous to default feature set of Pharaoh (Koehn, 2004). To simplify the notation, we omit the dependence on the hidden variables of the model. z z Therefore, the TAT-based translation model can be decomposed into four sub-models: h1 (eI1 , f1J ) = log 1. parse model: P r(T (f1J )|f1J ) 2. detachment model: P r(D|T (f1J ), f1J ) h2 (eI1 , f1J ) = log 3. TAT selection model: P r(z|T˜) h3 (eI1 , f1J ) = log ˜ T˜) 4. TAT application model: P r(S|z, Figure 2 shows how TATs work to perform translation. First, the input source sentence is parsed. Next, the parse tree is detached into five subtrees with a preorder transversal. For each subtree, a TAT is"
P06-1077,P01-1067,0,0.778013,"es hierarchical phrase pairs, which are formally productions of a synchronous context-free grammar. Ding and Palmer (2005) propose a syntax-based translation model based on a probabilistic synchronous dependency insert grammar, a version of synchronous grammars defined on dependency trees. All these approaches, though different in formalism, make use of synchronous grammars or tree-based transduction rules to model both source and target languages. Another class of approaches make use of syntactic information in the target language alone, treating the translation problem as a parsing problem. Yamada and Knight (2001) use a parser in the target language to train probabilities on a set of We present a novel translation model based on tree-to-string alignment template (TAT) which describes the alignment between a source parse tree and a target string. A TAT is capable of generating both terminals and non-terminals and performing reordering at both low and high levels. The model is linguistically syntaxbased because TATs are extracted automatically from word-aligned, source side parsed parallel texts. To translate a source sentence, we first employ a parser to produce a source parse tree and then apply TATs t"
P06-1077,zhang-etal-2004-interpreting,0,0.0519163,"Missing"
P06-1077,P05-1057,1,0.629394,"urce side, bilingual phrases can be utilized for the TAT-based model to get further improvement. It should be emphasized that the restrictions we impose on TAT extraction limit the expressive power of TAT. Preliminary experiments reveal that removing these restrictions does improve translation quality, but leads to large memory requirements. We feel that both parsing and word alignment qualities have important effects on the TATbased model. We will retrain the Chinese parser on Penn Chinese Treebank version 5.0 and try to improve word alignment quality using log-linear models as suggested in (Liu et al., 2005). 5.4 Using bilingual phrases It is interesting to use bilingual phrases to strengthen the TAT-based model. As we mentioned before, some useful non-syntactic phrase pairs can never be obtained in form of TAT because we restrict that there must be a corresponding parse tree for the source phrase. Moreover, it takes more time to obtain TATs than bilingual phrases on the same training data because parsing is usually very time-consuming. Given an input subtree T (Fjj12 ), if Fjj12 is a string of terminals, we find all bilingual phrases that the source phrase is equal to Fjj12 . Then we build a 0 0"
P06-1077,W02-1018,0,0.0149416,"rget string. A TAT is capable of generating both terminals and non-terminals and performing reordering at both low and high levels. The model is linguistically syntaxbased because TATs are extracted automatically from word-aligned, source side parsed parallel texts. To translate a source sentence, we first employ a parser to produce a source parse tree and then apply TATs to transform the tree into a target string. Our experiments show that the TAT-based model significantly outperforms Pharaoh, a state-of-the-art decoder for phrase-based models. 1 Introduction Phrase-based translation models (Marcu and Wong, 2002; Koehn et al., 2003; Och and Ney, 2004), which go beyond the original IBM translation models (Brown et al., 1993) 1 by modeling translations of phrases rather than individual words, have been suggested to be the state-of-theart in statistical machine translation by empirical evaluations. In phrase-based models, phrases are usually strings of adjacent words instead of syntactic constituents, excelling at capturing local reordering and performing translations that are localized to 1 The mathematical notation we use in this paper is taken from that paper: a source string f1J = f1 , . . . , fj ,"
P06-1077,P04-1083,0,0.00484775,"to model reordering at the phrase level robustly. Typically, phrase reordering is modeled in terms of offset positions at the word level (Koehn, 2004; Och and Ney, 2004), making little or no direct use of syntactic information. Recent research on statistical machine translation has lead to the development of syntax-based models. Wu (1997) proposes Inversion Transduction Grammars, treating translation as a process of parallel parsing of the source and target language via a synchronized grammar. Alshawi et al. (2000) represent each production in parallel dependency tree as a finite transducer. Melamed (2004) formalizes machine translation problem as synchronous parsing based on multitext grammars. Graehl and Knight (2004) describe training and decoding algorithms for both generalized tree-to-tree and tree-to-string transducers. Chiang (2005) presents a hierarchical phrasebased model that uses hierarchical phrase pairs, which are formally productions of a synchronous context-free grammar. Ding and Palmer (2005) propose a syntax-based translation model based on a probabilistic synchronous dependency insert grammar, a version of synchronous grammars defined on dependency trees. All these approaches,"
P06-1077,P00-1056,0,0.750306,"Missing"
P06-1077,J08-3004,0,\N,Missing
P07-1085,W06-1644,0,0.0304114,"odel. Proposed by Blei et al. (2003), Latent Dirichlet Allocation (LDA) loosens the constraint of the document-specific fixed weights by using a prior distribution and has quickly become one of the most popular probabilistic text modeling tech673 niques. LDA can overcome the drawbacks in the PLSA model, and has been shown to outperform PLSA in corpus perplexity and text classification experiments (Blei et al., 2003). Tam and Schultz (2005) successfully applied the LDA model to unsupervised LM adaptation by interpolating the background LM with the dynamic unigram LM estimated by the LDA model. Hsu and Glass (2006) investigated using hidden Markov model with LDA to allow for both topic and style adaptation. Mrva and Woodland (2006) achieved WER reduction on broadcast conversation recognition using an LDA based adaptation approach that effectively combined the LMs trained from corpora with different styles: broadcast news and broadcast conversation data. In this paper, we investigate unsupervised LM adaptation using clustering and LDA based topic analysis. Unlike the clustering based interpolation method as in (Iyer and Ostendorf, 1996), we explore different distance measure methods for topic analysis. D"
P07-1085,P05-1051,0,0.0143991,"tal data. The data set we used is the LDC Mandarin TDT4 corpus, consisting of 337 broadcast news shows with transcriptions. These files were split into small pieces, which we call documents here, according to the topic segmentation information marked in the LDC’s transcription. In total, there are 26,646 such documents in our data set. We randomly chose 2661 files as the test data (which is balanced for different news sources). The rest was used for topic analysis and also generic LM training. Punctuation marks were used to determine sentences in the transcriptions. We used the NYU NE tagger (Ji and Grishman, 2005) to recognize four kinds of NEs: Person, Location, Organization, and Geo-political. Table 1 shows the statistics of the data set in our experiments. We trained trigram LMs using the SRILM toolkit (Stolcke, 2002). A fixed weight (i.e., λ in Equation (2) and (3)) was used for the entire test set when interpolating the generic LM with the adapted topic LM. Perplexity was used to measure the performance of different adapted LMs in our experiments. 4.2 Latent Topic Analysis Results Topic # of Files Top 10 Descriptive Items (Translated from Chinese) U.S., Israel, Washington, Palestine, 1 Clustering"
P07-1085,H05-1034,0,0.15121,"rieval. Statistical N-gram LMs have been widely used; however, they capture only local contextual information. In addition, even with the increasing amount of LM training data, there is often a mismatch problem because of differences in domain, topics, or styles. Adaptation of LM, therefore, is very important in order to better deal with a variety of topics and styles. Many studies have been conducted for LM adaptation. One method is supervised LM adaptation, where topic information is typically available and a topic specific LM is interpolated with the generic LM (Kneser and Steinbiss, 1993; Suzuki and Gao, 2005). In contrast, various unsupervised approaches perform latent topic analysis for LM adaptation. To identify implicit topics from the unlabeled corpus, one simple technique is to group the documents into topic clusters by assigning only one topic label to a document (Iyer and Ostendorf, 1996). Recently several other methods in the line of latent semantic analysis have been proposed and used in LM adaptation, such as latent semantic analysis (LSA) (Bellegarda, 2000), probabilistic latent semantic analysis (PLSA) (Gildea and Hofmann, 1999), and LDA (Blei et al., 2003). Most of these existing appr"
P07-1089,P00-1056,0,0.239329,"Missing"
P07-1089,P02-1038,0,0.153368,"of rules used for each cell. There are two ways to limit the rule table size: by a fixed limit a of how many rules are retrieved for each cell, and by a probability threshold α that specify that the rule probability has to be above some value. Also, instead of keeping the full list of derivations for a cell, we store a top-scoring subset of the derivations. This can also be done by a fixed limit b or a threshold β. The subcell division array D, in which divisions containing forest cells have priority over those composed of only tree cells, is pruned by keeping only a-best divisions. Following Och and Ney (2002), we base our model on log-linear framework and adopt the seven feature functions described in (Liu et al., 2006). It is very important to balance the preference between conventional tree-to-string rules and the newlyintroduced forest-to-string and auxiliary rules. As the probabilities of auxiliary rules are not learnt from training data, we add a feature that sums up the 5 There are no default rules for forests because only tree-tostring rules are essential to tree-to-string translation models. 709 node count of auxiliary rules of a derivation to penalize the use of forest-to-string and auxil"
P07-1089,J04-4002,0,0.294023,"Missing"
P07-1089,P03-1021,0,0.253825,"Missing"
P07-1089,P02-1040,0,0.10786,"Missing"
P07-1089,P05-1033,0,0.439763,"Missing"
P07-1089,N04-1035,0,0.482432,"Missing"
P07-1089,P06-1121,0,0.480961,"Missing"
P07-1089,N03-1017,0,0.0750438,"hows the statistics of rules used in our experiments. We find that even though forest-to-string rules are introduced the total number (i.e. 73, 592) of lexicalized tree-to-string and forest-to-string rules is still far less than that (i.e. 251, 173) of bilingual phrases. This difference results from the restriction we impose in training that both the first and last symbols in the target string must be aligned to some source symbols. For the forest-to-string rules, partial lexicalized ones are in the majority. We compared our system Lynx against a freely available phrase-based decoder Pharaoh (Koehn et al., 2003). For Pharaoh, we set a = 20, α = 0, b = 100, β = 10−5 , and distortion limit dl = 4. For Lynx, we set a = 20, α = 0, b = 100, and β = 0. Two postprocessing procedures ran to improve the outputs of both systems: OOVs removal and recapitalization. Table 5 shows results on test set using Pharaoh and Lynx with different rule sets. Note that Lynx is capable of using only bilingual phrases plus de710 Forest-to-String Rule Set None L P U L+P+U BLEU4 0.2225 ± 0.0085 0.2297 ± 0.0081 0.2279 ± 0.0083 0.2270 ± 0.0087 0.2312 ± 0.0082 Table 6: Effect of lexicalized, partial lexicalized, and unlexicalized f"
P07-1089,P06-1077,1,0.81906,"Marcu et al. (2006) create a xRS rule headed by a pseudo, nonsyntactic nonterminal symbol that subsumes the phrase and corresponding multi-headed syntactic structure; and one sibling xRS rule that explains how the non-syntactic nonterminal symbol can be combined with other genuine nonterminals so as to obtain genuine parse trees. The name of the pseudo nonterminal is designed to reflect how the corresponding rule can be fully realized. However, they neglect alignment consistency when creating sibling rules. In addition, it is hard for the naming mechanism to deal with more complex phenomena. Liu et al. (2006) treat bilingual phrases as lexicalized TATs (Tree-to-string Alignment Template). A bilingual phrase can be used in decoding if the source phrase is subsumed by the input parse tree. Although this solution does help, only syntactic bilingual phrases are available to the TAT-based model. Moreover, it is problematic to combine the translation probabilities of bilingual phrases and TATs, which are estimated independently. In this paper, we propose forest-to-string rules which describe the correspondence between multiple parse trees and a string. They can not only capture non-syntactic phrase pair"
P07-1089,W06-1606,0,0.390951,"ax into statistical translation. On the other hand, the performance of linguistically syntax-based models can be hindered by making use of only syntactic phrase pairs. Studies reveal that linguistically syntax-based models are sensitive to syntactic analysis (Quirk and Corston-Oliver, 2006), which is still not reliable enough to handle real-world texts due to limited size and domain of training data. Various solutions are proposed to tackle the problem. Galley et al. (2004) handle non-constituent phrasal translation by traversing the tree upwards until reaches a node that subsumes the phrase. Marcu et al. (2006) argue that this choice is inap704 Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 704–711, c Prague, Czech Republic, June 2007. 2007 Association for Computational Linguistics propriate because large applicability contexts are required. For a non-syntactic phrase pair, Marcu et al. (2006) create a xRS rule headed by a pseudo, nonsyntactic nonterminal symbol that subsumes the phrase and corresponding multi-headed syntactic structure; and one sibling xRS rule that explains how the non-syntactic nonterminal symbol can be combined with other genuine no"
P07-1089,W06-1608,0,0.0370969,"Missing"
P07-1089,P05-1034,0,0.369713,"Missing"
P07-1089,2005.eamt-1.36,0,0.0380564,"Missing"
P07-1089,I05-1007,1,0.276878,"Missing"
P07-1089,zhang-etal-2004-interpreting,0,0.0432704,"Missing"
P08-2051,hovy-etal-2006-automated,0,0.0193971,"ssesses a summarization system in itself (for example, informativeness, redundancy, and coherence). Extrinsic evaluation (Mani et al., 1998) tests the effectiveness of a summarization system on other tasks. In this study, we concentrate on the automatic intrinsic summarization evaluation. It has been extensively studied in text summarization. Different approaches have been proposed to measure matches using words or more meaningful semantic units, for example, ROUGE (Lin, 2004), factoid analysis (Teufel and Halteren, 2004), pyramid method (Nenkova and Passonneau, 2004), and Basic Element (BE) (Hovy et al., 2006). With the increasing recent research of summarization moving into speech, especially meeting recordings, issues related to spoken language are yet to be explored for their impact on the evaluation metrics. Inspired by automatic speech recognition (ASR) evaluation, (Hori et al., 2003) proposed the summarization accuracy metric (SumACCY) based on a word network created by merging manual summaries. However (Zhu and Penn, 2005) found a statistically significant difference between the ASR-inspired metrics and those taken from text summarization (e.g., RU, ROUGE) on a subset of the Switchboard data"
P08-2051,W04-1013,0,0.0787546,"summarization. Our experiments show that generally the correlation is rather low, but a significantly better correlation can be obtained by accounting for several unique meeting characteristics, such as disfluencies and speaker information, especially when evaluating system-generated summaries. 1 2 Introduction Meeting summarization has drawn an increasing attention recently; therefore a study on the automatic evaluation metrics for this task is timely. Automatic evaluation helps to advance system development and avoids the labor-intensive and potentially inconsistent human evaluation. ROUGE (Lin, 2004) has been widely used for summarization evaluation. In the news article domain, ROUGE scores have been shown to be generally highly correlated with human evaluation in content match (Lin, 2004). However, there are many differences between written texts (e.g., news wire) and spoken documents, especially in the meeting domain, for example, the presence of disfluencies and multiple speakers, and the lack of structure in spontaneous utterances. The question of whether ROUGE is a good metric for meeting summarization is unclear. (Murray et al., 2005) have reported that ROUGE-1 (unigram match) score"
P08-2051,W05-0905,0,0.284546,"sive and potentially inconsistent human evaluation. ROUGE (Lin, 2004) has been widely used for summarization evaluation. In the news article domain, ROUGE scores have been shown to be generally highly correlated with human evaluation in content match (Lin, 2004). However, there are many differences between written texts (e.g., news wire) and spoken documents, especially in the meeting domain, for example, the presence of disfluencies and multiple speakers, and the lack of structure in spontaneous utterances. The question of whether ROUGE is a good metric for meeting summarization is unclear. (Murray et al., 2005) have reported that ROUGE-1 (unigram match) scores have low correlation with human evaluation in meetings. In this paper we investigate the correlation between ROUGE and human evaluation of extractive meeting summaries and focus on two issues specific to the meeting domain: disfluencies and multiple speakers. Both Related work Automatic summarization evaluation can be broadly classified into two categories (Jones and Galliers, 1996): intrinsic and extrinsic evaluation. Intrinsic evaluation, such as relative utility based metric proposed in (Radev et al., 2004), assesses a summarization system"
P08-2051,N04-1019,0,0.0371693,"tility based metric proposed in (Radev et al., 2004), assesses a summarization system in itself (for example, informativeness, redundancy, and coherence). Extrinsic evaluation (Mani et al., 1998) tests the effectiveness of a summarization system on other tasks. In this study, we concentrate on the automatic intrinsic summarization evaluation. It has been extensively studied in text summarization. Different approaches have been proposed to measure matches using words or more meaningful semantic units, for example, ROUGE (Lin, 2004), factoid analysis (Teufel and Halteren, 2004), pyramid method (Nenkova and Passonneau, 2004), and Basic Element (BE) (Hovy et al., 2006). With the increasing recent research of summarization moving into speech, especially meeting recordings, issues related to spoken language are yet to be explored for their impact on the evaluation metrics. Inspired by automatic speech recognition (ASR) evaluation, (Hori et al., 2003) proposed the summarization accuracy metric (SumACCY) based on a word network created by merging manual summaries. However (Zhu and Penn, 2005) found a statistically significant difference between the ASR-inspired metrics and those taken from text summarization (e.g., RU"
P08-2051,W04-2319,0,0.0187245,"Missing"
P08-2051,W04-3254,0,0.0461119,"on. Intrinsic evaluation, such as relative utility based metric proposed in (Radev et al., 2004), assesses a summarization system in itself (for example, informativeness, redundancy, and coherence). Extrinsic evaluation (Mani et al., 1998) tests the effectiveness of a summarization system on other tasks. In this study, we concentrate on the automatic intrinsic summarization evaluation. It has been extensively studied in text summarization. Different approaches have been proposed to measure matches using words or more meaningful semantic units, for example, ROUGE (Lin, 2004), factoid analysis (Teufel and Halteren, 2004), pyramid method (Nenkova and Passonneau, 2004), and Basic Element (BE) (Hovy et al., 2006). With the increasing recent research of summarization moving into speech, especially meeting recordings, issues related to spoken language are yet to be explored for their impact on the evaluation metrics. Inspired by automatic speech recognition (ASR) evaluation, (Hori et al., 2003) proposed the summarization accuracy metric (SumACCY) based on a word network created by merging manual summaries. However (Zhu and Penn, 2005) found a statistically significant difference between the ASR-inspired metrics an"
P08-2051,N06-2050,0,0.022889,") showed low correlation of ROUGE and human evaluation in meeting summarization evaluation; however, they 201 Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 201–204, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics simply used ROUGE as is and did not take into account the meeting characteristics during evaluation. In this paper, we ask the question of whether ROUGE correlates with human evaluation of extractive meeting summaries and whether we can modify ROUGE to account for the meeting style for a better correlation with human evaluation. 3 Zhu and Penn, 2006) and human readability (Jones et al., 2003). However, it is not clear whether disfluencies impact automatic evaluation of extractive meeting summarization. Since we use extractive summarization, summary sentences may contain difluencies. We hand annotated the transcripts for the 6 meetings and marked the disfluencies such that we can remove them to obtain cleaned up sentences for those selected summary sentences. To study the impact of disfluencies, we run ROUGE using two different inputs: summaries based on the original transcription, and the summaries with disfluencies removed. Experimental"
P08-2051,W06-1643,0,\N,Missing
P09-1061,N06-1029,0,0.0554602,"Missing"
P09-1061,W03-0407,0,0.0801032,"Missing"
P09-1063,N03-1017,0,0.0413288,"Missing"
P09-1063,P07-2045,0,0.00662405,"Missing"
P09-1063,P06-1077,1,0.717464,"ding target forest in parallel. Comparable to the state-of-the-art phrase-based system Moses, using packed forests in tree-to-tree translation results in a significant absolute improvement of 3.6 BLEU points over using 1-best trees. 1 Introduction Approaches to syntax-based statistical machine translation make use of parallel data with syntactic annotations, either in the form of phrase structure trees or dependency trees. They can be roughly divided into three categories: string-to-tree models (e.g., (Galley et al., 2006; Marcu et al., 2006; Shen et al., 2008)), tree-to-string models (e.g., (Liu et al., 2006; Huang et al., 2006)), and tree-totree models (e.g., (Eisner, 2003; Ding and Palmer, 2005; Cowan et al., 2006; Zhang et al., 2008)). By modeling the syntax of both source and target languages, tree-to-tree approaches have the potential benefit of providing rules linguistically better motivated. However, while string-to-tree and tree-to-string models demonstrate promising results in empirical evaluations, tree-to-tree models have still been underachieving. 558 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 558–566, c Suntec, Singapore, 2-7 August 2009."
P09-1063,P05-1022,0,0.0179469,"h translation. The training corpus contains 840K Chinese words and 950K English words. A trigram language model was trained on the English sentences of the training corpus. We used the 2002 NIST MT Evaluation test set as our development set, and used the 2005 NIST MT Evaluation test set as our test set. We evaluated the translation quality using the BLEU metric, as calculated by mteval-v11b.pl with its default setting except that we used case-insensitive matching of n-grams. To obtain packed forests, we used the Chinese parser (Xiong et al., 2005) modified by Haitao Mi and the English parser (Charniak and Johnson, 2005) modified by Liang Huang to produce entire parse forests. Then, we ran the Python scripts (Huang, 2008) provided by Liang Huang to output packed forests. To prune the packed forests, Huang (2008) uses inside and outside probabilities to compute the distance of the best derivation that traverses a hyperedge away from the globally best derivation. A hyperedge will be pruned away if the difference is greater than a threshold p. Nodes with all incoming hyperedges pruned are also pruned. The greater the threshold p is, p(ts ) × α(root(ts )) × v∈leaves(ts ) β(v) c(r) = β(¯ vs ) Q p(tt ) × α(root(tt"
P09-1063,J07-2003,0,0.588754,"ncies in two directions, lexical weights in two directions, number of rules used, language model score, number of target words produced, and the probability of matched source tree (Mi et al., 2008). Given a source parse forest and an STSG grammar G, we first apply the conversion algorithm proposed by Mi et al. (2008) to produce a translation forest. The translation forest has a similar hypergraph structure. While the nodes are the same as those of the parse forest, each hyperedge is associated with an STSG rule. Then, the decoder runs on the translation forest. We use the cube pruning method (Chiang, 2007) to approximately intersect the translation forest with the language model. Traversing the translation forest in a bottom-up order, the decoder tries to build target parses at each node. After the first pass, we use lazy Algorithm 3 (Huang and Chiang, 2005) to generate k-best translations for minimum error rate training. Inferring Composed Rules After minimal rules are learned, composed rules can be obtained by composing two or more minimal rules. For example, the composition of the second rule and the third rule in Table 1 produces a new rule: NP-B(NR(shalong)) → NP(NNP(Sharon)) While minimal"
P09-1063,P07-1089,1,0.944778,"odels that rely on 1-best parses are prone to learn noisy translation rules in training phase and produce degenerate translations in decoding phase (Quirk and CorstonOliver, 2006). This situation aggravates for treeto-tree models that use syntax on both sides. Second, tree-to-tree rules provide poorer rule coverage. As a tree-to-tree rule requires that there must be trees on both sides, tree-to-tree models lose a larger amount of linguistically unmotivated mappings. Studies reveal that the absence of such non-syntactic mappings will impair translation quality dramatically (Marcu et al., 2006; Liu et al., 2007; DeNeefe et al., 2007; Zhang et al., 2008). Compactly encoding exponentially many parses, packed forests prove to be an excellent fit for alleviating the above two problems (Mi et al., 2008; Mi and Huang, 2008). In this paper, we propose a forest-based tree-to-tree model. To learn STSG rules from aligned forest pairs, we introduce a series of notions for identifying minimal tree-to-tree rules. Our decoder first converts the source forest to a translation forest and then finds the best derivation that has the source yield of one source tree in the forest. Comparable to Moses, our forest-based"
P09-1063,W06-1606,0,0.546348,"tion, syntax-based models that rely on 1-best parses are prone to learn noisy translation rules in training phase and produce degenerate translations in decoding phase (Quirk and CorstonOliver, 2006). This situation aggravates for treeto-tree models that use syntax on both sides. Second, tree-to-tree rules provide poorer rule coverage. As a tree-to-tree rule requires that there must be trees on both sides, tree-to-tree models lose a larger amount of linguistically unmotivated mappings. Studies reveal that the absence of such non-syntactic mappings will impair translation quality dramatically (Marcu et al., 2006; Liu et al., 2007; DeNeefe et al., 2007; Zhang et al., 2008). Compactly encoding exponentially many parses, packed forests prove to be an excellent fit for alleviating the above two problems (Mi et al., 2008; Mi and Huang, 2008). In this paper, we propose a forest-based tree-to-tree model. To learn STSG rules from aligned forest pairs, we introduce a series of notions for identifying minimal tree-to-tree rules. Our decoder first converts the source forest to a translation forest and then finds the best derivation that has the source yield of one source tree in the forest. Comparable to Moses,"
P09-1063,D08-1022,0,0.604845,"or treeto-tree models that use syntax on both sides. Second, tree-to-tree rules provide poorer rule coverage. As a tree-to-tree rule requires that there must be trees on both sides, tree-to-tree models lose a larger amount of linguistically unmotivated mappings. Studies reveal that the absence of such non-syntactic mappings will impair translation quality dramatically (Marcu et al., 2006; Liu et al., 2007; DeNeefe et al., 2007; Zhang et al., 2008). Compactly encoding exponentially many parses, packed forests prove to be an excellent fit for alleviating the above two problems (Mi et al., 2008; Mi and Huang, 2008). In this paper, we propose a forest-based tree-to-tree model. To learn STSG rules from aligned forest pairs, we introduce a series of notions for identifying minimal tree-to-tree rules. Our decoder first converts the source forest to a translation forest and then finds the best derivation that has the source yield of one source tree in the forest. Comparable to Moses, our forest-based tree-to-tree model achieves an absolute improvement of 3.6 BLEU points over conventional tree-based model. Current tree-to-tree models suffer from parsing errors as they usually use only 1best parses for rule ex"
P09-1063,W06-1628,0,0.393124,"Missing"
P09-1063,P08-1023,1,0.832306,"tion aggravates for treeto-tree models that use syntax on both sides. Second, tree-to-tree rules provide poorer rule coverage. As a tree-to-tree rule requires that there must be trees on both sides, tree-to-tree models lose a larger amount of linguistically unmotivated mappings. Studies reveal that the absence of such non-syntactic mappings will impair translation quality dramatically (Marcu et al., 2006; Liu et al., 2007; DeNeefe et al., 2007; Zhang et al., 2008). Compactly encoding exponentially many parses, packed forests prove to be an excellent fit for alleviating the above two problems (Mi et al., 2008; Mi and Huang, 2008). In this paper, we propose a forest-based tree-to-tree model. To learn STSG rules from aligned forest pairs, we introduce a series of notions for identifying minimal tree-to-tree rules. Our decoder first converts the source forest to a translation forest and then finds the best derivation that has the source yield of one source tree in the forest. Comparable to Moses, our forest-based tree-to-tree model achieves an absolute improvement of 3.6 BLEU points over conventional tree-based model. Current tree-to-tree models suffer from parsing errors as they usually use only 1be"
P09-1063,D07-1079,0,0.0594257,"1-best parses are prone to learn noisy translation rules in training phase and produce degenerate translations in decoding phase (Quirk and CorstonOliver, 2006). This situation aggravates for treeto-tree models that use syntax on both sides. Second, tree-to-tree rules provide poorer rule coverage. As a tree-to-tree rule requires that there must be trees on both sides, tree-to-tree models lose a larger amount of linguistically unmotivated mappings. Studies reveal that the absence of such non-syntactic mappings will impair translation quality dramatically (Marcu et al., 2006; Liu et al., 2007; DeNeefe et al., 2007; Zhang et al., 2008). Compactly encoding exponentially many parses, packed forests prove to be an excellent fit for alleviating the above two problems (Mi et al., 2008; Mi and Huang, 2008). In this paper, we propose a forest-based tree-to-tree model. To learn STSG rules from aligned forest pairs, we introduce a series of notions for identifying minimal tree-to-tree rules. Our decoder first converts the source forest to a translation forest and then finds the best derivation that has the source yield of one source tree in the forest. Comparable to Moses, our forest-based tree-to-tree model ach"
P09-1063,P05-1067,0,0.266114,"m Moses, using packed forests in tree-to-tree translation results in a significant absolute improvement of 3.6 BLEU points over using 1-best trees. 1 Introduction Approaches to syntax-based statistical machine translation make use of parallel data with syntactic annotations, either in the form of phrase structure trees or dependency trees. They can be roughly divided into three categories: string-to-tree models (e.g., (Galley et al., 2006; Marcu et al., 2006; Shen et al., 2008)), tree-to-string models (e.g., (Liu et al., 2006; Huang et al., 2006)), and tree-totree models (e.g., (Eisner, 2003; Ding and Palmer, 2005; Cowan et al., 2006; Zhang et al., 2008)). By modeling the syntax of both source and target languages, tree-to-tree approaches have the potential benefit of providing rules linguistically better motivated. However, while string-to-tree and tree-to-string models demonstrate promising results in empirical evaluations, tree-to-tree models have still been underachieving. 558 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 558–566, c Suntec, Singapore, 2-7 August 2009. 2009 ACL and AFNLP IP 1 VP 3 NP 2 PP 4 NP-B6 NR9 VP-B5 NP-B7 CC10P 11 NR12 NP-B8 VV13 AS"
P09-1063,P03-2041,0,0.736034,"se-based system Moses, using packed forests in tree-to-tree translation results in a significant absolute improvement of 3.6 BLEU points over using 1-best trees. 1 Introduction Approaches to syntax-based statistical machine translation make use of parallel data with syntactic annotations, either in the form of phrase structure trees or dependency trees. They can be roughly divided into three categories: string-to-tree models (e.g., (Galley et al., 2006; Marcu et al., 2006; Shen et al., 2008)), tree-to-string models (e.g., (Liu et al., 2006; Huang et al., 2006)), and tree-totree models (e.g., (Eisner, 2003; Ding and Palmer, 2005; Cowan et al., 2006; Zhang et al., 2008)). By modeling the syntax of both source and target languages, tree-to-tree approaches have the potential benefit of providing rules linguistically better motivated. However, while string-to-tree and tree-to-string models demonstrate promising results in empirical evaluations, tree-to-tree models have still been underachieving. 558 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 558–566, c Suntec, Singapore, 2-7 August 2009. 2009 ACL and AFNLP IP 1 VP 3 NP 2 PP 4 NP-B6 NR9 VP-B5 NP-B7 CC10P"
P09-1063,P02-1038,0,0.0541119,"Missing"
P09-1063,J03-1002,0,0.0077588,"Missing"
P09-1063,W06-1608,0,0.11842,"Missing"
P09-1063,N04-1035,0,0.578009,"rs. 3 1 For example, the span of the source node “VP-B5 ” is {4, 5, 6} as it covers three source words: “juxing”, “le”, and “huitan”. For convenience, we use {4-6} to denotes a contiguous span {4, 5, 6}. Table 2: Node attributes of the example forest pair. 3 Rule Extraction Definition 2 Given a node v, its corresponding span γ(v) is the index set of aligned words on another side. Given an aligned forest pair as shown in Figure 1, how to extract all valid tree-to-tree rules that explain its synchronous generation process? By constructing a theory that gives formal semantics to word alignments, Galley et al. (2004) give principled answers to these questions for extracting tree-to-string rules. Their GHKM procedure draws connections among word alignments, derivations, and rules. They first identify the tree nodes that subsume tree-string pairs consistent with word alignments and then extract rules from these nodes. By this means, GHKM proves to be able to extract all valid tree-to-string rules from training instances. Although originally developed for the tree-to-string case, it is possible to extend GHKM to extract all valid tree-to-tree rules from aligned packed forests. In this section, we introduce o"
P09-1063,P06-1121,0,0.565149,"es in the source forest into elementary trees using the source projection of STSG while building target forest in parallel. Comparable to the state-of-the-art phrase-based system Moses, using packed forests in tree-to-tree translation results in a significant absolute improvement of 3.6 BLEU points over using 1-best trees. 1 Introduction Approaches to syntax-based statistical machine translation make use of parallel data with syntactic annotations, either in the form of phrase structure trees or dependency trees. They can be roughly divided into three categories: string-to-tree models (e.g., (Galley et al., 2006; Marcu et al., 2006; Shen et al., 2008)), tree-to-string models (e.g., (Liu et al., 2006; Huang et al., 2006)), and tree-totree models (e.g., (Eisner, 2003; Ding and Palmer, 2005; Cowan et al., 2006; Zhang et al., 2008)). By modeling the syntax of both source and target languages, tree-to-tree approaches have the potential benefit of providing rules linguistically better motivated. However, while string-to-tree and tree-to-string models demonstrate promising results in empirical evaluations, tree-to-tree models have still been underachieving. 558 Proceedings of the 47th Annual Meeting of the"
P09-1063,W05-1506,0,0.530286,"ree-to-string models demonstrate promising results in empirical evaluations, tree-to-tree models have still been underachieving. 558 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 558–566, c Suntec, Singapore, 2-7 August 2009. 2009 ACL and AFNLP IP 1 VP 3 NP 2 PP 4 NP-B6 NR9 VP-B5 NP-B7 CC10P 11 NR12 NP-B8 VV13 AS 14 NN15 bushi yu shalong juxing le huitan Bush held a talk with Sharon NNP16 VBD17 NP22 Formally, a packed parse forest is a compact representation of all the derivations (i.e., parse trees) for a given sentence under a context-free grammar. Huang and Chiang (2005) define a forest as a tuple hV, E, v¯, Ri, where V is a finite set of nodes, E is a finite set of hyperedges, v¯ ∈ V is a distinguished node that denotes the goal item in parsing, and R is the set of weights. For a given sentence w1:l = w1 . . . wl , each node v ∈ V is in the form of Xi,j , which denotes the recognition of non-terminal X spanning the substring from positions i through j (that is, wi+1 . . . wj ). Each hyperedge e ∈ E is a triple e = hT (e), h(e), f (e)i, where h(e) ∈ V is its head, T (e) ∈ V ∗ is a vector of tail nodes, and f (e) is a weight function from R|T (e) |to R. Our fo"
P09-1063,P07-1019,0,0.16388,"extraction time grew faster than decoding time with the increase of p. One possible reason is that the number of frontier tree pairs (see Figure 3) rose dramatically when more parses were included in packed forests. 5.5 Effect of Maximal Node Count Figure 5 shows the effect of maximal node count on BLEU scores. With the increase of maximal node count, the BLEU score increased dramatically. This implies that allowing tree-to-tree rules to capture larger contexts will strengthen the expressive power of tree-to-tree model. 5.6 In machine translation, the concept of packed forest is first used by Huang and Chiang (2007) to characterize the search space of decoding with language models. The first direct use of packed forest is proposed by Mi et al. (2008). They replace 1-best trees with packed forests both in training and decoding and show superior translation quality over the state-of-the-art hierarchical phrasebased system. We follow the same direction and apply packed forests to tree-to-tree translation. Zhang et al. (2008) present a tree-to-tree model that uses STSG. To capture non-syntactic phrases, they apply tree-sequence rules (Liu et al., 2007) to tree-to-tree models. Their extraction algorithm first"
P09-1063,2006.amta-papers.8,0,0.646284,"P-B PP P 11 IN 20 7 PP (a) 7 NP-B 12 NR 4 IN P 4 (b) (c) 21 NNP 24 NP 26 20 PP 4 NP-B NP 7 24 26 (d) Figure 2: (a) A frontier tree; (b) a minimal frontier tree; (c) a frontier tree pair; (d) a minimal frontier tree pair. All trees are taken from the example forest pair in Figure 1. Shaded nodes are frontier nodes. Each node is assigned an identity for reference. Definition 5 A node v is said to be a frontier node if and only if: 3.2 Identifying Minimum Rules Given the frontier nodes, the next step is to identify aligned tree pairs, from which tree-to-tree rules derive. Following Galley et al. (2006), we distinguish between minimal and composed rules. As a composed rule can be decomposed as a sequence of minimal rules, we are particularly interested in how to extract minimal rules. Also, we introduce a number of notions to help identify minimal rules. 1. v is consistent; 2. There exists at least one consistent node v ′ on another side satisfying: • closure(γ(v ′ )) ⊆ σ(v); • closure(γ(v)) ⊆ σ(v ′ ). v ′ is said to be a counterpart of v. We use τ (v) to denote the set of counterparts of v. Definition 6 A frontier tree is a subtree in a forest satisfying: A frontier node often has multiple"
P09-1063,P08-1066,0,0.11728,"rees using the source projection of STSG while building target forest in parallel. Comparable to the state-of-the-art phrase-based system Moses, using packed forests in tree-to-tree translation results in a significant absolute improvement of 3.6 BLEU points over using 1-best trees. 1 Introduction Approaches to syntax-based statistical machine translation make use of parallel data with syntactic annotations, either in the form of phrase structure trees or dependency trees. They can be roughly divided into three categories: string-to-tree models (e.g., (Galley et al., 2006; Marcu et al., 2006; Shen et al., 2008)), tree-to-string models (e.g., (Liu et al., 2006; Huang et al., 2006)), and tree-totree models (e.g., (Eisner, 2003; Ding and Palmer, 2005; Cowan et al., 2006; Zhang et al., 2008)). By modeling the syntax of both source and target languages, tree-to-tree approaches have the potential benefit of providing rules linguistically better motivated. However, while string-to-tree and tree-to-string models demonstrate promising results in empirical evaluations, tree-to-tree models have still been underachieving. 558 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pag"
P09-1063,I05-1007,1,0.700477,"s 5.1 Data Preparation We evaluated our model on Chinese-to-English translation. The training corpus contains 840K Chinese words and 950K English words. A trigram language model was trained on the English sentences of the training corpus. We used the 2002 NIST MT Evaluation test set as our development set, and used the 2005 NIST MT Evaluation test set as our test set. We evaluated the translation quality using the BLEU metric, as calculated by mteval-v11b.pl with its default setting except that we used case-insensitive matching of n-grams. To obtain packed forests, we used the Chinese parser (Xiong et al., 2005) modified by Haitao Mi and the English parser (Charniak and Johnson, 2005) modified by Liang Huang to produce entire parse forests. Then, we ran the Python scripts (Huang, 2008) provided by Liang Huang to output packed forests. To prune the packed forests, Huang (2008) uses inside and outside probabilities to compute the distance of the best derivation that traverses a hyperedge away from the globally best derivation. A hyperedge will be pruned away if the difference is greater than a threshold p. Nodes with all incoming hyperedges pruned are also pruned. The greater the threshold p is, p(ts )"
P09-1063,zhang-etal-2004-interpreting,0,0.0787519,"Missing"
P09-1063,P08-1064,0,0.77893,"ne to learn noisy translation rules in training phase and produce degenerate translations in decoding phase (Quirk and CorstonOliver, 2006). This situation aggravates for treeto-tree models that use syntax on both sides. Second, tree-to-tree rules provide poorer rule coverage. As a tree-to-tree rule requires that there must be trees on both sides, tree-to-tree models lose a larger amount of linguistically unmotivated mappings. Studies reveal that the absence of such non-syntactic mappings will impair translation quality dramatically (Marcu et al., 2006; Liu et al., 2007; DeNeefe et al., 2007; Zhang et al., 2008). Compactly encoding exponentially many parses, packed forests prove to be an excellent fit for alleviating the above two problems (Mi et al., 2008; Mi and Huang, 2008). In this paper, we propose a forest-based tree-to-tree model. To learn STSG rules from aligned forest pairs, we introduce a series of notions for identifying minimal tree-to-tree rules. Our decoder first converts the source forest to a translation forest and then finds the best derivation that has the source yield of one source tree in the forest. Comparable to Moses, our forest-based tree-to-tree model achieves an absolute imp"
P09-1063,P08-1067,0,\N,Missing
P09-1065,P06-1077,1,0.898463,"e word is associated with a score. The optimal consensus translation can be obtained by selecting one word from each set of candidates to maximizing the overall score. While it is easy and efficient to manipulate strings, current methods usually have no access to most information available in decoding phase, which might be useful for obtaining further improvements. In this paper, we propose a framework for combining multiple translation models directly in deWe evaluated our joint decoder that integrated a hierarchical phrase-based model (Chiang, 2005; Chiang, 2007) and a tree-to-string model (Liu et al., 2006) on the NIST 2005 Chinese-English testset. Experimental results show that joint decod1 It might be controversial to use the term “model”, which usually has a very precise definition in the field. Some researchers prefer to saying “phrase-based approaches” or “phrase-based systems”. On the other hand, other authors (e.g., (Och and Ney, 2004; Koehn et al., 2003; Chiang, 2007)) do use the expression “phrase-based models”. In this paper, we use the term “model” to emphasize that we integrate different approaches directly in decoding phase rather than postprocessing system outputs. 576 Proceedings"
P09-1065,P07-1089,1,0.914806,"Missing"
P09-1065,D08-1076,0,0.100386,"Missing"
P09-1065,D08-1022,1,0.459626,"case. Blunsom et al. (2008) first distinguish between max-derivation decoding and max-translation decoding explicitly. They show that max-translation decoding outperforms max-derivation decoding for the latent variable model. While they train the parameters using a maximum a posteriori estimator, we extend the MERT algorithm (Och, 2003) to take the evaluation metric into account. Comparison with System Combination Hypergraphs have been successfully used in parsing (Klein and Manning., 2001; Huang and Chiang, 2005; Huang, 2008) and machine translation (Huang and Chiang, 2007; Mi et al., 2008; Mi and Huang, 2008). Both Mi et al. (2008) and Blunsom et al. (2008) use a translation hypergraph to represent search space. The difference is that their hypergraphs are specifically designed for the forest-based tree-to-string model and the hierarchical phrase-based model, respectively, while ours is more general and can be applied to arbitrary models. We re-implemented a state-of-the-art system combination method (Rosti et al., 2007). As shown in Table 3, taking the translations of the two individual decoders as input, the system combination method achieved a BLEU score of 31.50, slightly lower than that of jo"
P09-1065,D08-1023,0,0.00900115,"j − i = l do for all m ∈ M do A DD(G, i, j, m) end for P RUNE(G, i, j) end for end for end procedure IP(x1 :VV, x2 :NN) → x1 x2 X → hfabiao, givei X → hyanjiang, a talki Figure 4: A derivation composed of both SCFG and tree-to-string rules. pairs and tree-to-string rules. Hierarchical phrase pairs are used for translating smaller units and tree-to-string rules for bigger ones. It is appealing to combine them in such a way because the hierarchical phrase-based model provides excellent rule coverage while the tree-to-string model offers linguistically motivated non-local reordering. Similarly, Blunsom and Osborne (2008) use both hierarchical phrase pairs and tree-to-string rules in decoding, where source parse trees serve as conditioning context rather than hard constraints. Depending on the target side output, we distinguish between string-targeted and tree-targeted models. String-targeted models include phrasebased, hierarchical phrase-based, and tree-tostring models. Tree-targeted models include string-to-tree and tree-to-tree models. All models can be combined at the translation level. Models that share with same target output structure can be further combined at the derivation level. The joint decoder u"
P09-1065,P08-1024,0,0.0833649,"ces of decisions that translate a source sentence into a target sentence step by step. For example, Figure 1 shows a sequence of SCFG rules (Chiang, 2005; Chiang, 2007) that translates a Chinese sentence “fabiao yanjiang” into an English sentence “give a talk”. Such sequence of decisions is called a derivation. In phrase-based models, a decision can be translating a source phrase into a target phrase or reordering the target phrases. In syntax-based models, decisions usually correspond to transduction rules. Often, there are many derivations that are distinct yet produce the same translation. Blunsom et al. (2008) present a latent variable model that describes the relationship between translation and derivation clearly. Given a source sentence f , the probability of a target sentence e being its translation is the sum over all possible derivations: ˆ = argmax e e P r(d, e|f ) = m λm hm (d, e, f ) Z(f ) ( X d∈∆(e,f ) ˆ ≈ argmax e e,d exp X ) λm hm (d, e, f ) m X m  λm hm (d, e, f ) (6) We refer to Eq. (5) as max-translation decoding and Eq. (6) as max-derivation decoding, which are first termed by Blunsom et al. (2008). By now, most current SMT systems, adopting either max-derivation decoding or max-t"
P09-1065,P05-1033,0,0.867394,"of a sequence of sets of candidate words. Each candidate word is associated with a score. The optimal consensus translation can be obtained by selecting one word from each set of candidates to maximizing the overall score. While it is easy and efficient to manipulate strings, current methods usually have no access to most information available in decoding phase, which might be useful for obtaining further improvements. In this paper, we propose a framework for combining multiple translation models directly in deWe evaluated our joint decoder that integrated a hierarchical phrase-based model (Chiang, 2005; Chiang, 2007) and a tree-to-string model (Liu et al., 2006) on the NIST 2005 Chinese-English testset. Experimental results show that joint decod1 It might be controversial to use the term “model”, which usually has a very precise definition in the field. Some researchers prefer to saying “phrase-based approaches” or “phrase-based systems”. On the other hand, other authors (e.g., (Och and Ney, 2004; Koehn et al., 2003; Chiang, 2007)) do use the expression “phrase-based models”. In this paper, we use the term “model” to emphasize that we integrate different approaches directly in decoding phas"
P09-1065,J07-2003,0,0.876253,"of sets of candidate words. Each candidate word is associated with a score. The optimal consensus translation can be obtained by selecting one word from each set of candidates to maximizing the overall score. While it is easy and efficient to manipulate strings, current methods usually have no access to most information available in decoding phase, which might be useful for obtaining further improvements. In this paper, we propose a framework for combining multiple translation models directly in deWe evaluated our joint decoder that integrated a hierarchical phrase-based model (Chiang, 2005; Chiang, 2007) and a tree-to-string model (Liu et al., 2006) on the NIST 2005 Chinese-English testset. Experimental results show that joint decod1 It might be controversial to use the term “model”, which usually has a very precise definition in the field. Some researchers prefer to saying “phrase-based approaches” or “phrase-based systems”. On the other hand, other authors (e.g., (Och and Ney, 2004; Koehn et al., 2003; Chiang, 2007)) do use the expression “phrase-based models”. In this paper, we use the term “model” to emphasize that we integrate different approaches directly in decoding phase rather than p"
P09-1065,P08-1023,1,0.682905,"decoding in this case. Blunsom et al. (2008) first distinguish between max-derivation decoding and max-translation decoding explicitly. They show that max-translation decoding outperforms max-derivation decoding for the latent variable model. While they train the parameters using a maximum a posteriori estimator, we extend the MERT algorithm (Och, 2003) to take the evaluation metric into account. Comparison with System Combination Hypergraphs have been successfully used in parsing (Klein and Manning., 2001; Huang and Chiang, 2005; Huang, 2008) and machine translation (Huang and Chiang, 2007; Mi et al., 2008; Mi and Huang, 2008). Both Mi et al. (2008) and Blunsom et al. (2008) use a translation hypergraph to represent search space. The difference is that their hypergraphs are specifically designed for the forest-based tree-to-string model and the hierarchical phrase-based model, respectively, while ours is more general and can be applied to arbitrary models. We re-implemented a state-of-the-art system combination method (Rosti et al., 2007). As shown in Table 3, taking the translations of the two individual decoders as input, the system combination method achieved a BLEU score of 31.50, slightly"
P09-1065,P02-1038,0,0.474353,"and corresponding translation e conditioned on a source sentence f : P (4) where Z(f ) is not needed in decoding because it is independent of e. Most SMT systems approximate the summation over all possible derivations by using 1-best derivation for efficiency. They search for the 1best derivation and take its target yield as the best translation: d∈∆(e,f ) exp p(d) where d is a decision in the derivation d. Although originally proposed for supporting large sets of non-independent and overlapping features, the latent variable model is actually a more general form of conventional linear model (Och and Ney, 2002). Accordingly, decoding for the latent variable model can be formalized as 2 Background P r(d, e|f ) Y d∈d ing with multiple models achieves an absolute improvement of 1.5 BLEU points over individual decoding with single models (Section 5). X (3) A feature value is usually decomposed as the product of decision probabilities: 2 h(d, e, f ) = P r(e|f ) = λm hm (d, e, f ) m e d∈∆(e,f ) Figure 1: A derivation composed of SCFG rules that translates a Chinese sentence “fabiao yanjiang” into an English sentence “give a talk”. X 3 Joint Decoding There are two major challenges for combining multiple mo"
P09-1065,J03-1002,0,0.00639286,"Missing"
P09-1065,J04-4002,0,0.208011,"ning further improvements. In this paper, we propose a framework for combining multiple translation models directly in deWe evaluated our joint decoder that integrated a hierarchical phrase-based model (Chiang, 2005; Chiang, 2007) and a tree-to-string model (Liu et al., 2006) on the NIST 2005 Chinese-English testset. Experimental results show that joint decod1 It might be controversial to use the term “model”, which usually has a very precise definition in the field. Some researchers prefer to saying “phrase-based approaches” or “phrase-based systems”. On the other hand, other authors (e.g., (Och and Ney, 2004; Koehn et al., 2003; Chiang, 2007)) do use the expression “phrase-based models”. In this paper, we use the term “model” to emphasize that we integrate different approaches directly in decoding phase rather than postprocessing system outputs. 576 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 576–584, c Suntec, Singapore, 2-7 August 2009. 2009 ACL and AFNLP where hm is a feature function, λm is the associated feature weight, and Z(f ) is a constant for normalization: S → hX1 , X1 i X → hfabiao X1 , give a X1 i X → hyanjiang, talki Z(f ) = X X exp Stati"
P09-1065,P03-1021,0,0.815341,"rtial translations (Section 3.2). Although such translation-level combination will not produce new translations, it does change the way of selecting promising candidates. • Two models could even share derivations with each other if they produce the same structures on the target side (Section 3.3), which we refer to as derivation-level combination. This method enlarges the search space by allowing for mixing different types of translation rules within one derivation. • As multiple derivations are used for finding optimal translations, we extend the minimum error rate training (MERT) algorithm (Och, 2003) to tune feature weights with respect to BLEU score for max-translation decoding (Section 4). 1 Introduction System combination aims to find consensus translations among different machine translation systems. It proves that such consensus translations are usually better than the output of individual systems (Frederking and Nirenburg, 1994). Recent several years have witnessed the rapid development of system combination methods based on confusion networks (e.g., (Rosti et al., 2007; He et al., 2008)), which show state-of-theart performance in MT benchmarks. A confusion network consists of a seq"
P09-1065,A94-1016,0,0.256094,"derivation-level combination. This method enlarges the search space by allowing for mixing different types of translation rules within one derivation. • As multiple derivations are used for finding optimal translations, we extend the minimum error rate training (MERT) algorithm (Och, 2003) to tune feature weights with respect to BLEU score for max-translation decoding (Section 4). 1 Introduction System combination aims to find consensus translations among different machine translation systems. It proves that such consensus translations are usually better than the output of individual systems (Frederking and Nirenburg, 1994). Recent several years have witnessed the rapid development of system combination methods based on confusion networks (e.g., (Rosti et al., 2007; He et al., 2008)), which show state-of-theart performance in MT benchmarks. A confusion network consists of a sequence of sets of candidate words. Each candidate word is associated with a score. The optimal consensus translation can be obtained by selecting one word from each set of candidates to maximizing the overall score. While it is easy and efficient to manipulate strings, current methods usually have no access to most information available in"
P09-1065,P06-1121,0,0.228974,"ion hypergraph produced by another model; (c) the packed translation hypergraph based on (a) and (b). Solid and dashed lines denote the translation rules of the two models, respectively. Shaded nodes occur in both (a) and (b), indicating that the two models produce the same translations. and thus need to collect different information during decoding. For example, taking a source parse as input, a tree-to-string decoder (e.g., (Liu et al., 2006)) pattern-matches the source parse with treeto-string rules and produces a string on the target side. On the contrary, a string-to-tree decoder (e.g., (Galley et al., 2006; Shen et al., 2008)) is a parser that applies string-to-tree rules to obtain a target parse for the source string. As a result, the hypothesis structures of the two models are fundamentally different. tion 3.3). 3.1 Translation Hypergraph Despite the diversity of translation models, they all have to produce partial translations for substrings of input sentences. Therefore, we represent the search space of a translation model as a structure called translation hypergraph. Figure 2(a) demonstrates a translation hypergraph for one model, for example, a hierarchical phrase-based model. A node in a"
P09-1065,D08-1011,0,0.0936928,"are used for finding optimal translations, we extend the minimum error rate training (MERT) algorithm (Och, 2003) to tune feature weights with respect to BLEU score for max-translation decoding (Section 4). 1 Introduction System combination aims to find consensus translations among different machine translation systems. It proves that such consensus translations are usually better than the output of individual systems (Frederking and Nirenburg, 1994). Recent several years have witnessed the rapid development of system combination methods based on confusion networks (e.g., (Rosti et al., 2007; He et al., 2008)), which show state-of-theart performance in MT benchmarks. A confusion network consists of a sequence of sets of candidate words. Each candidate word is associated with a score. The optimal consensus translation can be obtained by selecting one word from each set of candidates to maximizing the overall score. While it is easy and efficient to manipulate strings, current methods usually have no access to most information available in decoding phase, which might be useful for obtaining further improvements. In this paper, we propose a framework for combining multiple translation models directly"
P09-1065,W99-0623,0,0.227171,"Missing"
P09-1065,W05-1506,0,0.6237,"g translation rule derivation model first parses f to obtain a source tree T (f ) and then transforms T (f ) to the target sentence e. Conversely, a string-to-tree model first parses f into a target tree T (e) and then takes the surface string e as the translation. Despite different inside, their derivations must begin with f and end with e. This situation remains the same for derivations between a source substring fij and its partial translation t during joint decoding: Table 1: Correspondence between translation hypergraph and decoding. More formally, a hypergraph (Klein and Manning., 2001; Huang and Chiang, 2005) is a tuple hV, E, Ri, where V is a set of nodes, E is a set of hyperedges, and R is a set of weights. For a given source sentence f = f1n = f1 . . . fn , each node v ∈ V is in the form of ht, [i, j]i, which denotes the recognition of t as one translation of the source substring spanning from i through j (that is, fi+1 . . . fj ). Each hyperedge e ∈ E is a tuple e = htails(e), head(e), w(e)i, where head(e) ∈ V is the consequent node in the deductive step, tails(e) ∈ V ∗ is the list of antecedent nodes, and w(e) is a weight function from R|tails(e) |to R. As a general representation, a translat"
P09-1065,P07-1019,0,0.0420047,"o surpass max-derivation decoding in this case. Blunsom et al. (2008) first distinguish between max-derivation decoding and max-translation decoding explicitly. They show that max-translation decoding outperforms max-derivation decoding for the latent variable model. While they train the parameters using a maximum a posteriori estimator, we extend the MERT algorithm (Och, 2003) to take the evaluation metric into account. Comparison with System Combination Hypergraphs have been successfully used in parsing (Klein and Manning., 2001; Huang and Chiang, 2005; Huang, 2008) and machine translation (Huang and Chiang, 2007; Mi et al., 2008; Mi and Huang, 2008). Both Mi et al. (2008) and Blunsom et al. (2008) use a translation hypergraph to represent search space. The difference is that their hypergraphs are specifically designed for the forest-based tree-to-string model and the hierarchical phrase-based model, respectively, while ours is more general and can be applied to arbitrary models. We re-implemented a state-of-the-art system combination method (Rosti et al., 2007). As shown in Table 3, taking the translations of the two individual decoders as input, the system combination method achieved a BLEU score of"
P09-1065,P08-1067,0,0.0234869,"ax-translation decoding still failed to surpass max-derivation decoding in this case. Blunsom et al. (2008) first distinguish between max-derivation decoding and max-translation decoding explicitly. They show that max-translation decoding outperforms max-derivation decoding for the latent variable model. While they train the parameters using a maximum a posteriori estimator, we extend the MERT algorithm (Och, 2003) to take the evaluation metric into account. Comparison with System Combination Hypergraphs have been successfully used in parsing (Klein and Manning., 2001; Huang and Chiang, 2005; Huang, 2008) and machine translation (Huang and Chiang, 2007; Mi et al., 2008; Mi and Huang, 2008). Both Mi et al. (2008) and Blunsom et al. (2008) use a translation hypergraph to represent search space. The difference is that their hypergraphs are specifically designed for the forest-based tree-to-string model and the hierarchical phrase-based model, respectively, while ours is more general and can be applied to arbitrary models. We re-implemented a state-of-the-art system combination method (Rosti et al., 2007). As shown in Table 3, taking the translations of the two individual decoders as input, the sy"
P09-1065,W01-1812,0,0.192643,"Missing"
P09-1065,P02-1040,0,0.106668,"ections while two lines have at most one intersection. Fortunately, as the curve is monotonically increasing, 5 Experiments 5.1 Data Preparation Our experiments were on Chinese-to-English translation. We used the FBIS corpus (6.9M + 8.9M words) as the training corpus. For language model, we used the SRI Language Modeling Toolkit (Stolcke, 2002) to train a 4-gram model on the Xinhua portion of GIGAWORD corpus. We used the NIST 2002 MT Evaluation test set as our development set, and used the NIST 2005 test set as test set. We evaluated the translation quality using case-insensitive BLEU metric (Papineni et al., 2002). Our joint decoder included two models. The 581 Model Combination hierarchical tree-to-string N/A N/A translation derivation both Max-derivation Time BLEU 40.53 30.11 6.13 27.23 N/A N/A 48.45 31.63 Max-translation Time BLEU 44.87 29.82 6.69 27.11 55.89 30.79 54.91 31.49 Table 2: Comparison of individual decoding and joint decoding on average decoding time (seconds/sentence) and BLEU score (case-insensitive). 5.2 percentage first model was the hierarchical phrase-based model (Chiang, 2005; Chiang, 2007). We obtained word alignments of training data by first running GIZA++ (Och and Ney, 2003) a"
P09-1065,P07-1040,0,0.331325,"ultiple derivations are used for finding optimal translations, we extend the minimum error rate training (MERT) algorithm (Och, 2003) to tune feature weights with respect to BLEU score for max-translation decoding (Section 4). 1 Introduction System combination aims to find consensus translations among different machine translation systems. It proves that such consensus translations are usually better than the output of individual systems (Frederking and Nirenburg, 1994). Recent several years have witnessed the rapid development of system combination methods based on confusion networks (e.g., (Rosti et al., 2007; He et al., 2008)), which show state-of-theart performance in MT benchmarks. A confusion network consists of a sequence of sets of candidate words. Each candidate word is associated with a score. The optimal consensus translation can be obtained by selecting one word from each set of candidates to maximizing the overall score. While it is easy and efficient to manipulate strings, current methods usually have no access to most information available in decoding phase, which might be useful for obtaining further improvements. In this paper, we propose a framework for combining multiple translati"
P09-1065,P08-1066,0,0.0477497,"ed by another model; (c) the packed translation hypergraph based on (a) and (b). Solid and dashed lines denote the translation rules of the two models, respectively. Shaded nodes occur in both (a) and (b), indicating that the two models produce the same translations. and thus need to collect different information during decoding. For example, taking a source parse as input, a tree-to-string decoder (e.g., (Liu et al., 2006)) pattern-matches the source parse with treeto-string rules and produces a string on the target side. On the contrary, a string-to-tree decoder (e.g., (Galley et al., 2006; Shen et al., 2008)) is a parser that applies string-to-tree rules to obtain a target parse for the source string. As a result, the hypothesis structures of the two models are fundamentally different. tion 3.3). 3.1 Translation Hypergraph Despite the diversity of translation models, they all have to produce partial translations for substrings of input sentences. Therefore, we represent the search space of a translation model as a structure called translation hypergraph. Figure 2(a) demonstrates a translation hypergraph for one model, for example, a hierarchical phrase-based model. A node in a hypergraph denotes"
P09-1065,P05-1044,0,0.021838,"Missing"
P09-1065,P06-1098,0,0.0135744,"ensus translations among different models. Now the question is how to decode with multiple models jointly in just one decoder. We believe that both left-to-right and bottom-up strategies can be used for joint decoding. Although phrase-based decoders usually produce translations from left to right, they can adopt bottom-up decoding in principle. Xiong et al. (2006) develop a bottom-up decoder for BTG (Wu, 1997) that uses only phrase pairs. They treat reordering of phrases as a binary classification problem. On the other hand, it is possible for syntax-based models to decode from left to right. Watanabe et al. (2006) propose leftto-right target generation for hierarchical phrasebased translation. Although left-to-right decoding might enable a more efficient use of language models and hopefully produce better translations, we adopt bottom-up decoding in this paper just for convenience. Figure 3 demonstrates the search algorithm of our joint decoder. The input is a source language sentence f1n , and a set of translation models M Translation-Level Combination The conventional interpretation of Eq. (1) is that the probability of a translation is the sum over all possible derivations coming from the same model"
P09-1065,J97-3002,0,0.0490287,"model. The shaded nodes are shared by both models. Therefore, the two models are combined at the translation level. Intuitively, shared nodes should be favored in decoding because they offer consensus translations among different models. Now the question is how to decode with multiple models jointly in just one decoder. We believe that both left-to-right and bottom-up strategies can be used for joint decoding. Although phrase-based decoders usually produce translations from left to right, they can adopt bottom-up decoding in principle. Xiong et al. (2006) develop a bottom-up decoder for BTG (Wu, 1997) that uses only phrase pairs. They treat reordering of phrases as a binary classification problem. On the other hand, it is possible for syntax-based models to decode from left to right. Watanabe et al. (2006) propose leftto-right target generation for hierarchical phrasebased translation. Although left-to-right decoding might enable a more efficient use of language models and hopefully produce better translations, we adopt bottom-up decoding in this paper just for convenience. Figure 3 demonstrates the search algorithm of our joint decoder. The input is a source language sentence f1n , and a"
P09-1065,P06-1066,1,0.502138,"rst model and the dashed lines denote those of the second model. The shaded nodes are shared by both models. Therefore, the two models are combined at the translation level. Intuitively, shared nodes should be favored in decoding because they offer consensus translations among different models. Now the question is how to decode with multiple models jointly in just one decoder. We believe that both left-to-right and bottom-up strategies can be used for joint decoding. Although phrase-based decoders usually produce translations from left to right, they can adopt bottom-up decoding in principle. Xiong et al. (2006) develop a bottom-up decoder for BTG (Wu, 1997) that uses only phrase pairs. They treat reordering of phrases as a binary classification problem. On the other hand, it is possible for syntax-based models to decode from left to right. Watanabe et al. (2006) propose leftto-right target generation for hierarchical phrasebased translation. Although left-to-right decoding might enable a more efficient use of language models and hopefully produce better translations, we adopt bottom-up decoding in this paper just for convenience. Figure 3 demonstrates the search algorithm of our joint decoder. The i"
P09-1065,N03-1017,0,\N,Missing
P09-2035,N03-1017,0,0.0252889,"Missing"
P09-2035,P06-1077,1,0.876446,"Missing"
P09-2035,P09-1063,1,0.78314,"Missing"
P09-2035,D08-1022,1,0.882756,"(Liu et al., 2006; Huang et al., 2006), tree-to-tree models (Quirk et al.,2005;Zhang et al., 2008). Especially, when incorporated with forest, the correspondent forest-based tree-to-string models (Mi et al., 2008; Zhang et al., 2009), tree-to-tree models (Liu et al., 2009) have achieved a promising improvements over correspondent treebased systems. However, when we translate long sentences, we argue that two major issues will be raised. On one hand, parsing accuracy will be lower as the length of sentence grows. It will inevitably hurt the translation quality (Quirk and Corston-Oliver, 2006; Mi and Huang, 2008). On the other hand, decoding on long sentences will be time consuming, especially for forest approaches. So splitting long sentences into subsentences becomes a natural way in MT literature. A simple way is to split long sentences by punctuations. However, without concerning about the original whole tree structures, this approach will result in ill-formed sub-trees which don’t respect to original structures. In this paper, we present a new approach, which pays more attention to parse trees on the long sentences. We firstly parse the long sentences into trees, and then divide them accordingly"
P09-2035,P03-1021,0,0.0109418,"on forest-based tree-tostring translation model (Mi et al. 2008). Our training corpus consists of 2.56 million sentence pairs. Forest-based rule extractor (Mi and Huang 2008) is used with a pruning threshold p=3. And we use SRI Language Modeling Toolkit (Stolcke, 2002) to train two 5-gram language models with Kneser-Ney smoothing on the English side of the training corpus and the Xinhua portion of Gigaword corpora respectively. We use 2006 NIST MT Evaluation test set as development set, and 2002, 2005 and 2008 NIST MT Evaluation test sets as test sets. We also use minimum error-rate training (Och, 2003) to tune our feature weights. We evaluate our results with case-sensitive BLEU-4 metric (Papineni et al., 2002). The pruning threshold p for parse forest in decoding time is 12. 5.2 Results The final BLEU results are shown in Table 2, our approach has achieved a BLEU score that is 1.1 higher than direct decoding and 0.3 higher than always splitting on commas. The decoding time results are presented in Table 3. The search space of our experiment is extremely large due to the large pruning threshold (p=12), thus resulting in a long decoding time. However, our approach has reduced the decoding ti"
P09-2035,P02-1040,0,0.0782487,"Missing"
P09-2035,P05-1034,0,0.145683,"Missing"
P09-2035,W05-1506,0,0.0668665,"Missing"
P09-2035,2006.amta-papers.8,0,\N,Missing
P09-2035,I05-1007,1,\N,Missing
P09-2035,P09-1020,0,\N,Missing
P09-2035,P08-1023,1,\N,Missing
P09-2035,P08-1064,0,\N,Missing
P09-2035,W06-1608,0,\N,Missing
P09-2035,W06-1606,0,\N,Missing
P09-2035,P07-1019,0,\N,Missing
P09-2066,N01-1016,0,0.0102414,"sentence in the example in Sec 1. Human: we have to refine the tasks in order to avoid rephrasing Markov (S1): we have to refine the tasks more and more which we haven’t done in order to avoid this rephrasing Markov (S2): we have to refine the tasks which we haven’t done order to avoid this rephrasing FP + IP: we have to refine the tasks more and more which we haven’t done to avoid this rephrasing 2.2.4 Compression Using Lexicalized Markov Grammars The last sentence compression method we use is the lexicalized Markov grammar-based approach (Galley and McKeown, 2007) with edit word detection (Charniak and Johnson, 2001). Two outputs were generated using this method with different compression rates (defined as the number of words preserved in the compression divided by the total number of words in the original sentence).5 We name them “Markov (S1)” and “Markov (S2)” respectively. 3 Info. 4.35 3.64 2.89 3.70 Since our goal is to answer the question if we can use sentence compression to generate abstractive summaries, we compare the compressed summaries, as well as the original extractive summaries, against the reference abstractive summaries. The ROUGE-1 results along with the word compression ratio for each c"
P09-2066,N07-1023,0,0.121649,"mpression on an extractive summary to improve its readability and make it more like an abstractive summary. Compressing sentences could be a first step toward our ultimate goal of creating an abstract for spoken documents. Sentence compression has been widely studied in language processing. (Knight and Marcu, 2002; Cohn and Lapata, 2009) learned rewriting rules that indicate which words should be dropped in a given context. (Knight and Marcu, 2002; Turner and Charniak, 2005) applied the noisy-channel framework to predict the possibilities of translating a sentence to a shorter word sequence. (Galley and McKeown, 2007) extended the noisy-channel approach and proposed a head-driven Markovization formulation of synchronous contextfree grammar (SCFG) deletion rules. Unlike these approaches that need a training corpus, (Clarke and Lapata, 2008) encoded the language model and a variety of linguistic constraints as linear inequalities, and employed the integer programming approach to find a subset of words that maximize an objective function. Our focus in this paper is not on new compression algorithms, but rather on using compression to bridge the gap of extractive and abstractive summarization. We use different"
P09-2066,W06-1643,0,0.0494182,"ummarization has focused on extractive summarization, that is, it extracts important sentences (or dialogue acts) from speech transcripts, either manual transcripts or automatic speech recognition (ASR) output. Various approaches to extractive summarization have been evaluated recently. Popular unsupervised approaches are maximum marginal relevance (MMR), latent semantic analysis (LSA) (Murray et al., 2005a), and integer programming (Gillick et al., 2009). Supervised methods include hidden Markov model (HMM), maximum entropy, conditional random fields (CRF), and support vector machines (SVM) (Galley, 2006; Buist et al., 2005; Xie et al., 2008; Maskey and Hirschberg, 2006). (Hori et al., 2003) used a word based speech summarization approach that utilized dynamic programming to obtain a set of words to maximize a summarization score. Most of these summarization approaches aim for selecting the most informative sentences, while less attempt has been made to generate abstractive summaries, or compress the extracted sentences and merge them into a concise summary. Simply concatenating 261 Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 261–264, c Suntec, Singapore, 4 August 2009."
P09-2066,W04-1013,0,0.0305099,"est results in the future. 2.2.3 Compression Using Integer Programming We employ the integer programming (IP) approach in the same way as (Clarke and Lapata, 2008). Given an utterance S = w1 , w2 , ..., wn , the IP approach forms a compression of this utterance only by dropping words and preserving the word sequence that maximizes an objective function, defined as the sum of the signifi1 The extractive units are DAs. We use DAs and sentences interchangeably in this paper when there is no ambiguity. 2 http://www.mturk.com/mturk/welcome 262 For a comparison, we also include the ROUGE-1 Fscores (Lin, 2004) of each system output against the human compressed sentences. cance scores of the consisting words and n-gram probabilities from a language model: n P yi · Sig(wi ) max λ · i=1 n−2 P P n−1 + (1 − λ) · n P i=0 j=i+1 k=j+1 Approach Human Markov (S1) Markov (S2) FP + IP xijk · P (wk |wi , wj ) where yi and xijk are two binary variables: yi = 1 represents that word wi is in the compressed sentence; xijk = 1 represents that the sequence wi , wj , wk is in the compressed sentence. A trade-off parameter λ is used to balance the contribution from the significance scores for individual words and the l"
P09-2066,N06-2023,0,0.0442652,", that is, it extracts important sentences (or dialogue acts) from speech transcripts, either manual transcripts or automatic speech recognition (ASR) output. Various approaches to extractive summarization have been evaluated recently. Popular unsupervised approaches are maximum marginal relevance (MMR), latent semantic analysis (LSA) (Murray et al., 2005a), and integer programming (Gillick et al., 2009). Supervised methods include hidden Markov model (HMM), maximum entropy, conditional random fields (CRF), and support vector machines (SVM) (Galley, 2006; Buist et al., 2005; Xie et al., 2008; Maskey and Hirschberg, 2006). (Hori et al., 2003) used a word based speech summarization approach that utilized dynamic programming to obtain a set of words to maximize a summarization score. Most of these summarization approaches aim for selecting the most informative sentences, while less attempt has been made to generate abstractive summaries, or compress the extracted sentences and merge them into a concise summary. Simply concatenating 261 Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 261–264, c Suntec, Singapore, 4 August 2009. 2009 ACL and AFNLP 2.2.2 Filler Phrase Detection We define filler ph"
P09-2066,W05-0905,0,0.0461979,"framework, where we also introduce a filler phrase (FP) detection Introduction Meeting summaries provide an efficient way for people to browse through the lengthy recordings. Most current research on meeting summarization has focused on extractive summarization, that is, it extracts important sentences (or dialogue acts) from speech transcripts, either manual transcripts or automatic speech recognition (ASR) output. Various approaches to extractive summarization have been evaluated recently. Popular unsupervised approaches are maximum marginal relevance (MMR), latent semantic analysis (LSA) (Murray et al., 2005a), and integer programming (Gillick et al., 2009). Supervised methods include hidden Markov model (HMM), maximum entropy, conditional random fields (CRF), and support vector machines (SVM) (Galley, 2006; Buist et al., 2005; Xie et al., 2008; Maskey and Hirschberg, 2006). (Hori et al., 2003) used a word based speech summarization approach that utilized dynamic programming to obtain a set of words to maximize a summarization score. Most of these summarization approaches aim for selecting the most informative sentences, while less attempt has been made to generate abstractive summaries, or compr"
P09-2066,W04-2319,0,0.0161906,"on, we also use human compression. All of these compressed sentences are compared to abstractive summaries. Our experiments using the ICSI meeting corpus show that compressing extractive summaries can improve human readability and the ROUGE scores against the reference abstractive summaries. 2 Sentence Compression of Extractive Summaries 2.1 Corpus We used the ICSI meeting corpus (Janin et al., 2003), which contains naturally occurring meetings, each about an hour long. All the meetings have been transcribed and annotated with dialogue acts (DAs), topics, abstractive and extractive summaries (Shriberg et al., 2004; Murray et al., 2005b). In this study, we use the extractive and abstractive summaries of 6 meetings from this corpus. These 6 meetings were chosen because they have been used previously in other related studies, such as summarization and keyword extraction (Murray et al., 2005a). On average, an extractive summary contains 76 sentences1 (1252 words), and an abstractive summary contains 5 sentences (111 words). 2.2 where N is the total number of sentences and Ni is the number of sentences containing this phrase. Phrases with low ISF scores mean that they appear in many occasions and are not do"
P09-2066,P05-1036,0,0.0281651,"roup decided to hire the wizard and continue with the refinement... In this paper, our goal is to answer the question if we can perform sentence compression on an extractive summary to improve its readability and make it more like an abstractive summary. Compressing sentences could be a first step toward our ultimate goal of creating an abstract for spoken documents. Sentence compression has been widely studied in language processing. (Knight and Marcu, 2002; Cohn and Lapata, 2009) learned rewriting rules that indicate which words should be dropped in a given context. (Knight and Marcu, 2002; Turner and Charniak, 2005) applied the noisy-channel framework to predict the possibilities of translating a sentence to a shorter word sequence. (Galley and McKeown, 2007) extended the noisy-channel approach and proposed a head-driven Markovization formulation of synchronous contextfree grammar (SCFG) deletion rules. Unlike these approaches that need a training corpus, (Clarke and Lapata, 2008) encoded the language model and a variety of linguistic constraints as linear inequalities, and employed the integer programming approach to find a subset of words that maximize an objective function. Our focus in this paper is"
P10-2003,P05-1022,0,0.10367,"Missing"
P10-2003,P08-1067,0,0.0732652,"Missing"
P10-2003,N03-1017,0,0.0461596,"ations of a sentence pair, to learn lexicalized reordering models efficiently. Experimental results on the NIST Chinese-English test sets show that our approach significantly outperforms the baseline method. 1 Figure 1: Occurrence of a swap with different numbers of adjacent bilingual phrases: only one phrase in (a) and three phrases in (b). Black squares denote word alignments and gray rectangles denote bilingual phrases. [s,t] indicates the target-side span of bilingual phrase bp and [u,v] represents the source-side span of bilingual phrase bp. Introduction Phrase-based translation systems (Koehn et al., 2003; Och and Ney, 2004) prove to be the stateof-the-art as they have delivered translation performance in recent machine translation evaluations. While excelling at memorizing local translation and reordering, phrase-based systems have difficulties in modeling permutations among phrases. As a result, it is important to develop effective reordering models to capture such non-local reordering. The early phrase-based paradigm (Koehn et al., 2003) applies a simple distance-based distortion penalty to model the phrase movements. More recently, many researchers have presented lexicalized reordering mod"
P10-2003,P07-2045,0,0.162702,"ion evaluations. While excelling at memorizing local translation and reordering, phrase-based systems have difficulties in modeling permutations among phrases. As a result, it is important to develop effective reordering models to capture such non-local reordering. The early phrase-based paradigm (Koehn et al., 2003) applies a simple distance-based distortion penalty to model the phrase movements. More recently, many researchers have presented lexicalized reordering models that take advantage of lexical information to predict reordering (Tillmann, 2004; Xiong et al., 2006; Zens and Ney, 2006; Koehn et al., 2007; Galley and Manning, 2008). These models are learned from a word-aligned corpus to predict three orientations of a phrase pair with respect to the previous bilingual phrase: monotone (M ), swap (S), and discontinuous (D). Take the bilingual phrase bp in Figure 1(a) for example. The wordbased reordering model (Koehn et al., 2007) analyzes the word alignments at positions (s − 1, u − 1) and (s − 1, v + 1). The orientation of bp is set to D because the position (s − 1, v + 1) contains no word alignment. The phrase-based reordering model (Tillmann, 2004) determines the presence of the adjacent bi"
P10-2003,W04-3250,0,0.0231813,"e used to generate a word-aligned corpus, where we extract bilingual phrases with maximum length 7. We use SRILM Toolkits (Stolcke, 2002) to train a 4-gram language model on the Xinhua portion of Gigaword corpus. In exception to the reordering probabilities, we use the same features in the comparative experiments. During decoding, we set ttable-limit = 20, stack = 100, and perform minimum-error-rate training (Och, 2003) to tune various feature weights. The translation quality is evaluated by case-insensitive BLEU-4 metric (Papineni et al., 2002). Finally, we conduct paired bootstrap sampling (Koehn, 2004) to test the significance in BLEU scores differences. 3.2 Experimental Results Table 2 shows the results of experiments with the small training corpus. For the msd-fe model, the BLEU scores by our method are 30.51 32.78 and 29.50, achieving absolute improvements of 0.89, 0.66 and 0.62 on the three test sets, respectively. For the msd-bidirectional-fe model, our method obtains BLEU scores of 30.49 32.73 and 29.24, with absolute improvements of 1.11, 0.73 and 0.60 over the baseline method. 1 The phrase-based lexical reordering model (Tillmann, 2004) is also closely related to our model. However,"
P10-2003,J03-1002,0,0.00561231,"inuous. If a msd-bidirectional-fe model is used, then the number of features doubles: one for each direction. 3.1 Experiment Setup Two different sizes of training corpora are used in our experiments: one is a small-scale corpus that comes from FBIS corpus consisting of 239K bilingual sentence pairs, the other is a large-scale corpus that includes 1.55M bilingual sentence pairs from LDC. The 2002 NIST MT evaluation test data is used as the development set and the 2003, 2004, 2005 NIST MT test data are the test sets. We choose the MOSES1 (Koehn et al., 2007) as the experimental decoder. GIZA++ (Och and Ney, 2003) and the heuristics “grow-diag-final-and” are used to generate a word-aligned corpus, where we extract bilingual phrases with maximum length 7. We use SRILM Toolkits (Stolcke, 2002) to train a 4-gram language model on the Xinhua portion of Gigaword corpus. In exception to the reordering probabilities, we use the same features in the comparative experiments. During decoding, we set ttable-limit = 20, stack = 100, and perform minimum-error-rate training (Och, 2003) to tune various feature weights. The translation quality is evaluated by case-insensitive BLEU-4 metric (Papineni et al., 2002). Fin"
P10-2003,J04-4002,0,0.0483088,"pair, to learn lexicalized reordering models efficiently. Experimental results on the NIST Chinese-English test sets show that our approach significantly outperforms the baseline method. 1 Figure 1: Occurrence of a swap with different numbers of adjacent bilingual phrases: only one phrase in (a) and three phrases in (b). Black squares denote word alignments and gray rectangles denote bilingual phrases. [s,t] indicates the target-side span of bilingual phrase bp and [u,v] represents the source-side span of bilingual phrase bp. Introduction Phrase-based translation systems (Koehn et al., 2003; Och and Ney, 2004) prove to be the stateof-the-art as they have delivered translation performance in recent machine translation evaluations. While excelling at memorizing local translation and reordering, phrase-based systems have difficulties in modeling permutations among phrases. As a result, it is important to develop effective reordering models to capture such non-local reordering. The early phrase-based paradigm (Koehn et al., 2003) applies a simple distance-based distortion penalty to model the phrase movements. More recently, many researchers have presented lexicalized reordering models that take advant"
P10-2003,P03-1021,0,0.210537,"cally, the model collects bilingual phrases and distinguishes their orientations with respect to the previous bilingual phrase into three categories:   M o= S   D ai − ai−1 = 1 ai − ai−1 = −1 |ai − ai−1 |6= 1 (1) Using the relative-frequency approach, the reordering probability regarding bp is Count(o, bp) 0 o0 Count(o , bp) p(o|bp) = P (2) 2.2 Reordering Graph For a parallel sentence pair, its reordering graph indicates all possible translation derivations consisting of the extracted bilingual phrases. To construct a reordering graph, we first extract bilingual phrases using the way of (Och, 2003). Then, the adjacent bilingual phrases are linked according to the targetside order. Some bilingual phrases, which have no adjacent bilingual phrases because of maximum length limitation, are linked to the nearest bilingual phrases in the target-side order. Shown in Figure 2(b), the reordering graph for the parallel sentence pair (Figure 2(a)) can be represented as an undirected graph, where each rectangle corresponds to a phrase pair, each link is the orientation relationship between adjacent bilingual phrases, and two distinguished rectangles bs and be indicate the beginning and ending of th"
P10-2003,P02-1040,0,0.082129,"GIZA++ (Och and Ney, 2003) and the heuristics “grow-diag-final-and” are used to generate a word-aligned corpus, where we extract bilingual phrases with maximum length 7. We use SRILM Toolkits (Stolcke, 2002) to train a 4-gram language model on the Xinhua portion of Gigaword corpus. In exception to the reordering probabilities, we use the same features in the comparative experiments. During decoding, we set ttable-limit = 20, stack = 100, and perform minimum-error-rate training (Och, 2003) to tune various feature weights. The translation quality is evaluated by case-insensitive BLEU-4 metric (Papineni et al., 2002). Finally, we conduct paired bootstrap sampling (Koehn, 2004) to test the significance in BLEU scores differences. 3.2 Experimental Results Table 2 shows the results of experiments with the small training corpus. For the msd-fe model, the BLEU scores by our method are 30.51 32.78 and 29.50, achieving absolute improvements of 0.89, 0.66 and 0.62 on the three test sets, respectively. For the msd-bidirectional-fe model, our method obtains BLEU scores of 30.49 32.73 and 29.24, with absolute improvements of 1.11, 0.73 and 0.60 over the baseline method. 1 The phrase-based lexical reordering model (T"
P10-2003,N04-4026,0,0.371205,"vered translation performance in recent machine translation evaluations. While excelling at memorizing local translation and reordering, phrase-based systems have difficulties in modeling permutations among phrases. As a result, it is important to develop effective reordering models to capture such non-local reordering. The early phrase-based paradigm (Koehn et al., 2003) applies a simple distance-based distortion penalty to model the phrase movements. More recently, many researchers have presented lexicalized reordering models that take advantage of lexical information to predict reordering (Tillmann, 2004; Xiong et al., 2006; Zens and Ney, 2006; Koehn et al., 2007; Galley and Manning, 2008). These models are learned from a word-aligned corpus to predict three orientations of a phrase pair with respect to the previous bilingual phrase: monotone (M ), swap (S), and discontinuous (D). Take the bilingual phrase bp in Figure 1(a) for example. The wordbased reordering model (Koehn et al., 2007) analyzes the word alignments at positions (s − 1, u − 1) and (s − 1, v + 1). The orientation of bp is set to D because the position (s − 1, v + 1) contains no word alignment. The phrase-based reordering model"
P10-2003,P06-1066,1,0.837358,"n performance in recent machine translation evaluations. While excelling at memorizing local translation and reordering, phrase-based systems have difficulties in modeling permutations among phrases. As a result, it is important to develop effective reordering models to capture such non-local reordering. The early phrase-based paradigm (Koehn et al., 2003) applies a simple distance-based distortion penalty to model the phrase movements. More recently, many researchers have presented lexicalized reordering models that take advantage of lexical information to predict reordering (Tillmann, 2004; Xiong et al., 2006; Zens and Ney, 2006; Koehn et al., 2007; Galley and Manning, 2008). These models are learned from a word-aligned corpus to predict three orientations of a phrase pair with respect to the previous bilingual phrase: monotone (M ), swap (S), and discontinuous (D). Take the bilingual phrase bp in Figure 1(a) for example. The wordbased reordering model (Koehn et al., 2007) analyzes the word alignments at positions (s − 1, u − 1) and (s − 1, v + 1). The orientation of bp is set to D because the position (s − 1, v + 1) contains no word alignment. The phrase-based reordering model (Tillmann, 2004) de"
P10-2003,W06-3108,0,0.02101,"ent machine translation evaluations. While excelling at memorizing local translation and reordering, phrase-based systems have difficulties in modeling permutations among phrases. As a result, it is important to develop effective reordering models to capture such non-local reordering. The early phrase-based paradigm (Koehn et al., 2003) applies a simple distance-based distortion penalty to model the phrase movements. More recently, many researchers have presented lexicalized reordering models that take advantage of lexical information to predict reordering (Tillmann, 2004; Xiong et al., 2006; Zens and Ney, 2006; Koehn et al., 2007; Galley and Manning, 2008). These models are learned from a word-aligned corpus to predict three orientations of a phrase pair with respect to the previous bilingual phrase: monotone (M ), swap (S), and discontinuous (D). Take the bilingual phrase bp in Figure 1(a) for example. The wordbased reordering model (Koehn et al., 2007) analyzes the word alignments at positions (s − 1, u − 1) and (s − 1, v + 1). The orientation of bp is set to D because the position (s − 1, v + 1) contains no word alignment. The phrase-based reordering model (Tillmann, 2004) determines the presenc"
P10-2003,D08-1089,0,\N,Missing
P10-5002,2006.amta-papers.8,1,\N,Missing
P10-5002,N04-1035,0,\N,Missing
P10-5002,D08-1022,1,\N,Missing
P10-5002,J93-2003,0,\N,Missing
P10-5002,D09-1037,0,\N,Missing
P10-5002,W05-1506,1,\N,Missing
P10-5002,P09-1020,0,\N,Missing
P10-5002,P07-1089,1,\N,Missing
P10-5002,P08-1023,1,\N,Missing
P10-5002,P06-1077,1,\N,Missing
P10-5002,P08-1064,0,\N,Missing
P10-5002,J04-4002,0,\N,Missing
P10-5002,P09-1063,1,\N,Missing
P10-5002,W06-1606,0,\N,Missing
P10-5002,P05-1033,0,\N,Missing
P10-5002,N03-1017,0,\N,Missing
P10-5002,P07-1019,1,\N,Missing
P10-5002,P06-1121,0,\N,Missing
P10-5002,P08-1066,0,\N,Missing
P10-5002,P89-1018,0,\N,Missing
P10-5002,C10-1080,1,\N,Missing
P10-5002,J07-2003,0,\N,Missing
P10-5002,W01-1812,0,\N,Missing
P10-5002,D07-1078,0,\N,Missing
P10-5002,P09-1065,1,\N,Missing
P10-5002,2008.amta-papers.18,0,\N,Missing
P11-1034,C10-2004,0,0.0458907,"Missing"
P11-1034,W04-1013,0,0.00451293,"es and abstractive summaries as well as their standard deviation. Because in conversations, utterance length varies a lot, we use words as units when calculating the compression ratio. extractive summaries abstractive summaries avg ratio stdev 0.26 0.13 0.13 0.06 Table 2: Compression ratio and standard deviation of extractive and abstractive summaries. We measured the inter-annotator agreement among the three annotators for the 18 conversations (each has two speakers, thus 36 “documents” in total). Results are shown in Table 3. For the extractive or abstractive summaries, we use ROUGE scores (Lin, 2004), a metric used to evaluate automatic summarization performance, to measure the pairwise agreement of summaries from different annotators. ROUGE F-scores are shown in the table for different matches, unigram (R-1), bigram (R-2), and longest subsequence (R-L). For the overall opinion category, since it is a multiclass label (not binary decision), we use Krippendorff’s α coefficient to measure human agreement, and the difference func2 = (c − k)2 (where c, k are tion for interval data: δck the interval values, on a scale of 1 to 5 corresponding to the five categories for the overall opinion). We"
P11-1034,W08-0112,1,0.828372,"es R-2 0.13 R-L 0.25 overall opinion α = 0.79 Table 3: Inter-annotator agreement for extractive and abstractive summaries, and overall opinion. bination of scores from several dimensions. The second one is a graph-based method, which incorporates the dialogue structure in ranking. We choose to investigate these two methods since they have been widely used in text and speech summarization, and perform competitively. In addition, they do not require a large labeled data set for modeling training, as needed in some classification or feature based summarization approaches. 4.1 summary annotation (Liu and Liu, 2008). The agreement on abstractive summaries is much lower than extractive summaries, which is as expected. Even for the same opinion or sentence, annotators use different words in the abstractive summaries. The agreement for the overall opinion annotation is similar to other opinion/emotion studies (Wilson, 2008b), but slightly lower than the level recommended by Krippendorff for reliable data (α = 0.8) (Hayes and Krippendorff, 2007), which shows it is even difficult for humans to determine what opinion a person holds (support or against something). Often human annotators have different interpret"
P11-1034,P06-2079,0,0.0169951,"previous work, unsupervised methods like Maximal Marginal Relevance (MMR), Latent Semantic Analysis (LSA), and supervised methods that cast the extraction problem as a binary classification task have been adopted. Prior research has also explored using speech specific information, including prosodic features, dialog structure, and speech recognition confidence. In order to provide a summary over opinions, we need to find out which utterances in the conversation contain opinion. Most previous work in senti332 ment analysis has focused on reviews (Pang and Lee, 2004; Popescu and Etzioni, 2005; Ng et al., 2006) and news resources (Wiebe and Riloff, 2005). Many kinds of features are explored, such as lexical features (unigram, bigram and trigram), part-of-speech tags, dependency relations. Most of prior work used classification methods such as naive Bayes or SVMs to perform the polarity classification or opinion detection. Only a handful studies have used conversational speech for opinion recognition (Murray and Carenini, 2009; Raaijmakers et al., 2008), in which some domain-specific features are utilized such as structural features and prosodic features. Our work is also related to question answerin"
P11-1034,C10-2105,0,0.0256018,"(Stoyanov et al., 2005) applies a subjectivity filter based on traditional QA systems to generate opinionated answers. (Balahur et al., 2010) answers some specific opinion questions like “Why do people criticize Richard Branson?” by retrieving candidate sentences using traditional QA methods and selecting the ones with the same polarity as the question. Our work is different in that we are not going to answer specific opinion questions, instead, we provide a summary on the speaker’s opinion towards a given topic. There exists some work on opinion summarization. For example, (Hu and Liu, 2004; Nishikawa et al., 2010) have explored opinion summarization in review domain, and (Paul et al., 2010) summarizes contrastive viewpoints in opinionated text. However, opinion summarization in spontaneous conversation is seldom studied. 3 Corpus Creation Though there are many annotated data sets for the research of speech summarization and sentiment analysis, there is no corpus available for opinion summarization on spontaneous speech. Thus for this study, we create a new pilot data set using a subset of the Switchboard corpus (Godfrey and Holliman, 1997).1 These are conversational telephone speech between two strange"
P11-1034,P04-1035,0,0.146476,"lt than well written text for summarization. In previous work, unsupervised methods like Maximal Marginal Relevance (MMR), Latent Semantic Analysis (LSA), and supervised methods that cast the extraction problem as a binary classification task have been adopted. Prior research has also explored using speech specific information, including prosodic features, dialog structure, and speech recognition confidence. In order to provide a summary over opinions, we need to find out which utterances in the conversation contain opinion. Most previous work in senti332 ment analysis has focused on reviews (Pang and Lee, 2004; Popescu and Etzioni, 2005; Ng et al., 2006) and news resources (Wiebe and Riloff, 2005). Many kinds of features are explored, such as lexical features (unigram, bigram and trigram), part-of-speech tags, dependency relations. Most of prior work used classification methods such as naive Bayes or SVMs to perform the polarity classification or opinion detection. Only a handful studies have used conversational speech for opinion recognition (Murray and Carenini, 2009; Raaijmakers et al., 2008), in which some domain-specific features are utilized such as structural features and prosodic features."
P11-1034,D10-1007,0,0.0183064,"ms to generate opinionated answers. (Balahur et al., 2010) answers some specific opinion questions like “Why do people criticize Richard Branson?” by retrieving candidate sentences using traditional QA methods and selecting the ones with the same polarity as the question. Our work is different in that we are not going to answer specific opinion questions, instead, we provide a summary on the speaker’s opinion towards a given topic. There exists some work on opinion summarization. For example, (Hu and Liu, 2004; Nishikawa et al., 2010) have explored opinion summarization in review domain, and (Paul et al., 2010) summarizes contrastive viewpoints in opinionated text. However, opinion summarization in spontaneous conversation is seldom studied. 3 Corpus Creation Though there are many annotated data sets for the research of speech summarization and sentiment analysis, there is no corpus available for opinion summarization on spontaneous speech. Thus for this study, we create a new pilot data set using a subset of the Switchboard corpus (Godfrey and Holliman, 1997).1 These are conversational telephone speech between two strangers that were assigned a topic to talk about for around 5 minutes. They were to"
P11-1034,H05-1043,0,0.024751,"text for summarization. In previous work, unsupervised methods like Maximal Marginal Relevance (MMR), Latent Semantic Analysis (LSA), and supervised methods that cast the extraction problem as a binary classification task have been adopted. Prior research has also explored using speech specific information, including prosodic features, dialog structure, and speech recognition confidence. In order to provide a summary over opinions, we need to find out which utterances in the conversation contain opinion. Most previous work in senti332 ment analysis has focused on reviews (Pang and Lee, 2004; Popescu and Etzioni, 2005; Ng et al., 2006) and news resources (Wiebe and Riloff, 2005). Many kinds of features are explored, such as lexical features (unigram, bigram and trigram), part-of-speech tags, dependency relations. Most of prior work used classification methods such as naive Bayes or SVMs to perform the polarity classification or opinion detection. Only a handful studies have used conversational speech for opinion recognition (Murray and Carenini, 2009; Raaijmakers et al., 2008), in which some domain-specific features are utilized such as structural features and prosodic features. Our work is also related to"
P11-1034,D08-1049,0,0.0785308,"been explored much. This can be useful for many domains, especially for processing the This task is built upon opinion recognition and topic or query based summarization. However, this problem is challenging in that: (a) Summarization in spontaneous speech is more difficult than well structured text (Mckeown et al., 2005), because speech is always less organized and has recognition errors when using speech recognition output; (b) Sentiment analysis in dialogues is also much harder because of the genre difference compared to other domains like product reviews or news resources, as reported in (Raaijmakers et al., 2008); (c) In conversational speech, information density is low and there are often off topic discussions, therefore presenting a need to identify utterances that are relevant to the topic. In this paper we perform an exploratory study on opinion summarization in conversations. We compare two unsupervised methods that have been 331 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 331–339, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics widely used in extractive summarization: sentenceranking and graph-based methods."
P11-1034,H05-1116,0,0.0224654,"ny kinds of features are explored, such as lexical features (unigram, bigram and trigram), part-of-speech tags, dependency relations. Most of prior work used classification methods such as naive Bayes or SVMs to perform the polarity classification or opinion detection. Only a handful studies have used conversational speech for opinion recognition (Murray and Carenini, 2009; Raaijmakers et al., 2008), in which some domain-specific features are utilized such as structural features and prosodic features. Our work is also related to question answering (QA), especially opinion question answering. (Stoyanov et al., 2005) applies a subjectivity filter based on traditional QA systems to generate opinionated answers. (Balahur et al., 2010) answers some specific opinion questions like “Why do people criticize Richard Branson?” by retrieving candidate sentences using traditional QA methods and selecting the ones with the same polarity as the question. Our work is different in that we are not going to answer specific opinion questions, instead, we provide a summary on the speaker’s opinion towards a given topic. There exists some work on opinion summarization. For example, (Hu and Liu, 2004; Nishikawa et al., 2010)"
P11-1034,W03-2102,0,0.0362134,"it into a positive one (denoted by P M I + (w, topic)) by adding the absolute value of the minimum value. The final relevance score of each sentence is normalized to [0, 1] using linear normalization: RELorig (s, topic) = X P M I + (w, topic) w∈s REL(s, topic) = score(s) = RELorig (s, topic) − M in M ax − M in • sentiment(s) indicates the probability that utterance s contains opinion. To obtain this, we trained a maximum entropy classifier with a bag-of-words model using a combination of data sets from several domains, including movie data (Pang and Lee, 2004), news articles from MPQA corpus (Wilson and Wiebe, 2003), and meeting transcripts from AMI corpus (Wilson, 2008a). Each sentence (or DA) in these corpora is annotated as “subjective” or “objective”. We use each utterance’s probability of being “subjective” predicted by the classifier as its sentiment score. • length(s) is the length of the utterance. This score can effectively penalize the short sentences which typically do not contain much important content, especially the backchannels that appear frequently in dialogues. We also perform linear normalization such that the final value lies in [0, 1]. 4.2 is modeled as an adjacency matrix, where eac"
P11-1034,wilson-2008-annotating,0,0.169028,"methods since they have been widely used in text and speech summarization, and perform competitively. In addition, they do not require a large labeled data set for modeling training, as needed in some classification or feature based summarization approaches. 4.1 summary annotation (Liu and Liu, 2008). The agreement on abstractive summaries is much lower than extractive summaries, which is as expected. Even for the same opinion or sentence, annotators use different words in the abstractive summaries. The agreement for the overall opinion annotation is similar to other opinion/emotion studies (Wilson, 2008b), but slightly lower than the level recommended by Krippendorff for reliable data (α = 0.8) (Hayes and Krippendorff, 2007), which shows it is even difficult for humans to determine what opinion a person holds (support or against something). Often human annotators have different interpretations about the same sentence, and a speaker’s opinion/attitude is sometimes ambiguous. Therefore this also demonstrates that it is more appropriate to provide a summary rather than a simple opinion category to answer questions about a person’s opinion towards something. 4 Opinion Summarization Methods Autom"
P11-1034,J02-4003,0,0.0409622,"mmarization system in Section 4 and present experimental results and analysis in Section 5. Section 6 concludes the paper. 2 Related Work Research in document summarization has been well established over the past decades. Many tasks have been defined such as single-document summarization, multi-document summarization, and querybased summarization. Previous studies have used various domains, including news articles, scientific articles, web documents, reviews. Recently there is an increasing research interest in speech summarization, such as conversational telephone speech (Zhu and Penn, 2006; Zechner, 2002), broadcast news (Maskey and Hirschberg, 2005; Lin et al., 2009), lectures (Zhang et al., 2007; Furui et al., 2004), meetings (Murray et al., 2005; Xie and Liu, 2010), voice mails (Koumpis and Renals, 2005). In general speech domains seem to be more difficult than well written text for summarization. In previous work, unsupervised methods like Maximal Marginal Relevance (MMR), Latent Semantic Analysis (LSA), and supervised methods that cast the extraction problem as a binary classification task have been adopted. Prior research has also explored using speech specific information, including pro"
P11-1034,H05-2017,0,\N,Missing
P11-1074,N09-1035,0,0.0295759,"have shown that acoustic pitch-accent is strongly correlated with lexical items, such as canonical stress pattern and syllable identity that can be easily acquired from the output of conventional ASR and pronunciation dictionary. We treat pitch-accent detection as a binary classification task, that is, a classifier is used to determine whether the base unit is prominent or not. Since pitch-accent is usually carried by syllables, we use syllables as our units, and the syllable definition of each word is based on CMU pronunciation dictionary which has lexical stress and syllable boundary marks (Bartlett et al., 2009). We separately develop acoustic-prosodic and lexical-prosodic models and use the correlation between the two models for each syllable to rescore the n-best hypotheses of baseline ASR systems. 3.1 Acoustic-prosodic Features Similar to most previous work, the prosodic features we use include pitch, energy, and duration. We also add delta features of pitch and energy. Duration information for syllables is derived from the speech waveform and phone-level forced alignment of the transcriptions. In order to reduce the effect by both inter-speaker and intra-speaker variation, both pitch and energy v"
P11-1074,A00-2029,0,0.0460573,"oring approach in Section 4. Section 5 describes our corpus and baseline ASR setup. Section 6 presents our experiments and results. The last section gives a brief summary along with future directions. 2 Previous Work Prosody is of interest to speech researchers because it plays an important role in comprehension of spoken language by human listeners. The use of prosody in speech understanding applications has been quite extensive. A variety of applications have been explored, such as sentence and topic segmentation (Shriberg et al., 2000; Rosenberg and Hirschberg, 2006), word error detection (Litman et al., 2000), dialog act detection (Sridhar et al., 2009), speaker recognition (Shriberg et al., 2005), and emotion recognition (Benus et al., 2007), just to name a few. Incorporating prosodic knowledge is expected to improve the performance of speech recognition. However, how to effectively integrate prosody within the traditional ASR framework is a difficult problem, since prosodic features are not well defined and they come from a longer region, which is different from spectral features used in current ASR systems. Various research has been conducted trying to incorporate prosodic information in ASR. O"
P11-1074,N06-2032,0,\N,Missing
P11-1128,C90-3001,0,0.408538,"od candidate. As a formal tree rewriting system, TAG (Joshi et al., 1975; Joshi, 1985) provides a larger domain of locality than CFG to state linguistic dependencies that are far apart since the formalism treats trees as basic building blocks. As a mildly context-sensitive grammar, TAG is conjectured to be powerful enough to model natural languages. Synchronous TAG generalizes TAG by allowing the construction of a pair of trees using the TAG operations of substitution and adjoining on tree pairs. The idea of using synchronous TAG in machine translation has been pursued by several researchers (Abeille et al., 1990; Prigent, 1994; Dras, 1999), but only recently in its probabilistic form (Nesson et al., 2006; DeNeefe and Knight, 2009). Shieber (2007) argues that probabilistic synchronous TAG possesses appealing properties such as expressivity and trainability for building a machine translation system. However, one major challenge for applying synchronous TAG to machine translation is computational complexity. While TAG requires O(n6 ) time for monolingual parsing, synchronous TAG requires O(n12 ) for bilingual parsing. One solution is to use tree insertion grammars (TIG) introduced by Schabes and Waters"
P11-1128,2000.iwpt-1.9,0,0.081984,"h α; Ps (α|η) is the probability of substituting α at η; Pa (β|η) is the probability of adjoining β at η; finally, Pa (NONE|η) is the probability of nothing adjoining at η. For tree-to-string translation, these parameters can be treated as feature functions of a discriminative framework (Och, 2003) combined with other conventional features such as relative frequency, lexical weight, rule count, language model, and word count (Liu et al., 2006). 3 Rule Extraction Inducing a synchronous TAG from training data often begins with converting Treebank-style parse trees to TAG derivations (Xia, 1999; Chen and Vijay-Shanker, 2000; Chiang, 2003). DeNeefe and 1281 Knight (2009) propose an algorithm to extract synchronous TIG rules from an aligned and parsed bilingual corpus. They first classify tree nodes into heads, arguments, and adjuncts using heuristics (Collins, 2003), then transform a Treebank-style tree into a TIG derivation, and finally extract minimallysized rules from the derivation tree and the string on the other side, constrained by the alignments. Probabilistic models can be estimated by collecting counts over the derivation trees. However, one challenge is that there are many TAG derivations that can yiel"
P11-1128,J07-2003,0,0.834293,"ing translation casts decoding as a tree parsing problem rather than parsing, the decoder still runs fast when adjoining is included. Less than 2 times slower, the adjoining tree-tostring system improves translation quality by +0.7 BLEU over the baseline system only allowing for tree substitution on NIST ChineseEnglish test sets. 1 Introduction Syntax-based translation models, which exploit hierarchical structures of natural languages to guide machine translation, have become increasingly popular in recent years. So far, most of them have been based on synchronous context-free grammars (CFG) (Chiang, 2007), tree substitution grammars (TSG) (Eisner, 2003; Galley et al., 2006; Liu et al., 2006; Huang et al., 2006; Zhang et al., 2008), and inversion transduction grammars (ITG) (Wu, 1997; Xiong et al., 2006). Although these formalisms present simple and precise mechanisms for describing the basic recursive structure of sentences, they are not powerful enough to model some important features of natural language syntax. For example, Chiang (2006) points out that the translation of languages that can stack an unbounded number of clauses in an “inside-out” way (Wu, 1997) 1278 provably goes beyond the e"
P11-1128,P10-1146,0,0.0616372,"Missing"
P11-1128,J03-4003,0,0.0141421,"a discriminative framework (Och, 2003) combined with other conventional features such as relative frequency, lexical weight, rule count, language model, and word count (Liu et al., 2006). 3 Rule Extraction Inducing a synchronous TAG from training data often begins with converting Treebank-style parse trees to TAG derivations (Xia, 1999; Chen and Vijay-Shanker, 2000; Chiang, 2003). DeNeefe and 1281 Knight (2009) propose an algorithm to extract synchronous TIG rules from an aligned and parsed bilingual corpus. They first classify tree nodes into heads, arguments, and adjuncts using heuristics (Collins, 2003), then transform a Treebank-style tree into a TIG derivation, and finally extract minimallysized rules from the derivation tree and the string on the other side, constrained by the alignments. Probabilistic models can be estimated by collecting counts over the derivation trees. However, one challenge is that there are many TAG derivations that can yield the same derived tree, even with respect to a single grammar. It is difficult to choose appropriate single derivations that enable the resulting grammar to translate unseen data well. DeNeefe and Knight (2009) indicate that the way to reconstru"
P11-1128,D09-1076,0,0.500129,"ocality than CFG to state linguistic dependencies that are far apart since the formalism treats trees as basic building blocks. As a mildly context-sensitive grammar, TAG is conjectured to be powerful enough to model natural languages. Synchronous TAG generalizes TAG by allowing the construction of a pair of trees using the TAG operations of substitution and adjoining on tree pairs. The idea of using synchronous TAG in machine translation has been pursued by several researchers (Abeille et al., 1990; Prigent, 1994; Dras, 1999), but only recently in its probabilistic form (Nesson et al., 2006; DeNeefe and Knight, 2009). Shieber (2007) argues that probabilistic synchronous TAG possesses appealing properties such as expressivity and trainability for building a machine translation system. However, one major challenge for applying synchronous TAG to machine translation is computational complexity. While TAG requires O(n6 ) time for monolingual parsing, synchronous TAG requires O(n12 ) for bilingual parsing. One solution is to use tree insertion grammars (TIG) introduced by Schabes and Waters (1995). As a restricted form of TAG, TIG still allows for adjoining of unbounded trees but only requires O(n3 ) time for"
P11-1128,P99-1011,0,0.0396303,"ing system, TAG (Joshi et al., 1975; Joshi, 1985) provides a larger domain of locality than CFG to state linguistic dependencies that are far apart since the formalism treats trees as basic building blocks. As a mildly context-sensitive grammar, TAG is conjectured to be powerful enough to model natural languages. Synchronous TAG generalizes TAG by allowing the construction of a pair of trees using the TAG operations of substitution and adjoining on tree pairs. The idea of using synchronous TAG in machine translation has been pursued by several researchers (Abeille et al., 1990; Prigent, 1994; Dras, 1999), but only recently in its probabilistic form (Nesson et al., 2006; DeNeefe and Knight, 2009). Shieber (2007) argues that probabilistic synchronous TAG possesses appealing properties such as expressivity and trainability for building a machine translation system. However, one major challenge for applying synchronous TAG to machine translation is computational complexity. While TAG requires O(n6 ) time for monolingual parsing, synchronous TAG requires O(n12 ) for bilingual parsing. One solution is to use tree insertion grammars (TIG) introduced by Schabes and Waters (1995). As a restricted form"
P11-1128,P03-2041,0,0.501101,"problem rather than parsing, the decoder still runs fast when adjoining is included. Less than 2 times slower, the adjoining tree-tostring system improves translation quality by +0.7 BLEU over the baseline system only allowing for tree substitution on NIST ChineseEnglish test sets. 1 Introduction Syntax-based translation models, which exploit hierarchical structures of natural languages to guide machine translation, have become increasingly popular in recent years. So far, most of them have been based on synchronous context-free grammars (CFG) (Chiang, 2007), tree substitution grammars (TSG) (Eisner, 2003; Galley et al., 2006; Liu et al., 2006; Huang et al., 2006; Zhang et al., 2008), and inversion transduction grammars (ITG) (Wu, 1997; Xiong et al., 2006). Although these formalisms present simple and precise mechanisms for describing the basic recursive structure of sentences, they are not powerful enough to model some important features of natural language syntax. For example, Chiang (2006) points out that the translation of languages that can stack an unbounded number of clauses in an “inside-out” way (Wu, 1997) 1278 provably goes beyond the expressive power of synchronous CFG and TSG. Ther"
P11-1128,N04-1035,0,0.830186,"adjoined at NN-X in α2 to generate α3 . the use of synchronous TIG for machine translation and report promising results. DeNeefe and Knight (2009) prove that adjoining can improve translation quality significantly over a state-of-the-art stringto-tree system (Galley et al., 2006) that uses synchronous TSG with tractable computational complexity. In this paper, we introduce synchronous TAG into tree-to-string translation (Liu et al., 2006; Huang et al., 2006), which is the simplest and fastest among syntax-based approaches (Section 2). We propose a new rule extraction algorithm based on GHKM (Galley et al., 2004) that directly induces a synchronous TAG from an aligned and parsed bilingual corpus without converting Treebank-style trees to TAG derivations explicitly (Section 3). As tree-tostring translation takes a source parse tree as input, the decoding can be cast as a tree parsing problem (Eisner, 2003): reconstructing TAG derivations from a derived tree using tree-to-string rules that allow for both substitution and adjoining. We describe how to convert TAG derivations to translation forest (Section 4). We evaluated the new tree-to-string system on NIST Chinese-English tests and obtained consistent"
P11-1128,P06-1121,0,0.528016,"than parsing, the decoder still runs fast when adjoining is included. Less than 2 times slower, the adjoining tree-tostring system improves translation quality by +0.7 BLEU over the baseline system only allowing for tree substitution on NIST ChineseEnglish test sets. 1 Introduction Syntax-based translation models, which exploit hierarchical structures of natural languages to guide machine translation, have become increasingly popular in recent years. So far, most of them have been based on synchronous context-free grammars (CFG) (Chiang, 2007), tree substitution grammars (TSG) (Eisner, 2003; Galley et al., 2006; Liu et al., 2006; Huang et al., 2006; Zhang et al., 2008), and inversion transduction grammars (ITG) (Wu, 1997; Xiong et al., 2006). Although these formalisms present simple and precise mechanisms for describing the basic recursive structure of sentences, they are not powerful enough to model some important features of natural language syntax. For example, Chiang (2006) points out that the translation of languages that can stack an unbounded number of clauses in an “inside-out” way (Wu, 1997) 1278 provably goes beyond the expressive power of synchronous CFG and TSG. Therefore, it is necessar"
P11-1128,P07-1019,0,0.349006,"ring is no longer than 7. To learn the probability models Pi (α), Ps (α|η), Pa (β|η), and Pa (NONE|η), we collect and normalize counts over these extracted rules following DeNeefe and Knight (2009). 4 Decoding Given a synchronous TAG and a derived source tree π, a tree-to-string decoder finds the English yield of the best derivation of which the Chinese yield matches π:   eˆ = e arg max P (D) (4) D s.t. f (D)=π This is called tree parsing (Eisner, 2003) as the decoder finds ways of decomposing π into elementary trees. Tree-to-string decoding with STSG is usually treated as forest rescoring (Huang and Chiang, 2007) that involves two steps. The decoder first converts the input tree into a translation forest using a translation rule set by pattern matching. Huang et al. (2006) show that this step is a depth-first search with memorization in O(n) time. Then, the decoder searches for the best derivation in the translation forest intersected with n-gram language models and outputs the target string. 2 Decoding with STAG, however, poses one major challenge to forest rescoring. As translation forest only supports substitution, it is difficult to construct a translation forest for STAG derivations because of 2"
P11-1128,D10-1027,0,0.0471897,"slation forest using a translation rule set by pattern matching. Huang et al. (2006) show that this step is a depth-first search with memorization in O(n) time. Then, the decoder searches for the best derivation in the translation forest intersected with n-gram language models and outputs the target string. 2 Decoding with STAG, however, poses one major challenge to forest rescoring. As translation forest only supports substitution, it is difficult to construct a translation forest for STAG derivations because of 2 Mi et al. (2008) give a detailed description of the two-step decoding process. Huang and Mi (2010) systematically analyze the decoding complexity of tree-to-string translation. α1 α2 IP0,8 NP2,3 β1 NR2,3 nê VP3,8 ↓ ` aob¯ amˇ a NR2,3 ↓ β2 NP0,2 ↓ NP0,1 NR0,1 ↓ r1 r2 r3 r4 r5 r6 NP2,3 ∗ NN1,2 ↓ NP0,2 NP2,3 ∗ elementary tree α1 α2 β1 β2 β3 α3 NP1,2 α3 β3 NP0,3 NP0,3 NN2,3 oÚ NP1,2 ∗ zˇ ongtˇ ong translation rule ( IP ( NP0:1 ( x1 :NR↓ ) ) ( x2 :VP↓ ) ) → x1 x2 ( NR a` ob¯amˇa ) → Obama ( NP ( NP0:1 ( x1 :NN↓ ) ) ( x2 :NP∗ ) ) → x1 x2 ( NP ( x1 :NP↓ ) ( x2 :NP∗ ) ) → x1 x2 ( NP ( NP ( x1 :NR↓ ) ) ( x2 :NP∗ ) ) → x1 x2 ( NN zˇongtˇong ) → President Figure 3: Matched trees and corresponding rul"
P11-1128,2006.amta-papers.8,0,0.27879,"chine translation, have become increasingly popular in recent years. So far, most of them have been based on synchronous context-free grammars (CFG) (Chiang, 2007), tree substitution grammars (TSG) (Eisner, 2003; Galley et al., 2006; Liu et al., 2006; Huang et al., 2006; Zhang et al., 2008), and inversion transduction grammars (ITG) (Wu, 1997; Xiong et al., 2006). Although these formalisms present simple and precise mechanisms for describing the basic recursive structure of sentences, they are not powerful enough to model some important features of natural language syntax. For example, Chiang (2006) points out that the translation of languages that can stack an unbounded number of clauses in an “inside-out” way (Wu, 1997) 1278 provably goes beyond the expressive power of synchronous CFG and TSG. Therefore, it is necessary to find ways to take advantage of more powerful synchronous grammars to improve machine translation. Synchronous tree adjoining grammars (TAG) (Shieber and Schabes, 1990) are a good candidate. As a formal tree rewriting system, TAG (Joshi et al., 1975; Joshi, 1985) provides a larger domain of locality than CFG to state linguistic dependencies that are far apart since th"
P11-1128,P07-2045,0,0.00420358,"e root node. For example, in Figure 2, the distance between NP0,1 and NP0,3 is 2 and the distance between VP6,8 and VP3,8 is 1. As most foot nodes are usually very close to the root nodes, we restrict that a foot node must be the direct descendant of the root node in our experiments. Table 3 shows the BLEU scores on the NIST Chinese-English test sets. Our baseline system is the tree-to-string system using STSG (Liu et al., 2006; Huang et al., 2006). The STAG system outperforms the STSG system significantly on the MT04 and MT05 test sets at pl.01 level. Table 3 also gives the results of Moses (Koehn et al., 2007) and an in-house hierarchical phrase-based system (Chiang, 2007). Our STAG system achieves comparable performance with the hierarchical system. The absolute improvement of +0.7 BLEU over STSG is close to the finding of DeNeefe and Knight (2009) on string-to-tree translation. We feel that one major obstacle for achieving further improvement is that composed rules generated on the fly during decoding (e.g., r1 + r3 + r5 in Figure 4) usually have too many non-terminals, making cube pruning in the inmatching conversion intersection other total STSG 0.086 0.000 0.946 0.012 1.044 STAG 0.109 0.562 1."
P11-1128,P06-1077,1,0.400643,"coder still runs fast when adjoining is included. Less than 2 times slower, the adjoining tree-tostring system improves translation quality by +0.7 BLEU over the baseline system only allowing for tree substitution on NIST ChineseEnglish test sets. 1 Introduction Syntax-based translation models, which exploit hierarchical structures of natural languages to guide machine translation, have become increasingly popular in recent years. So far, most of them have been based on synchronous context-free grammars (CFG) (Chiang, 2007), tree substitution grammars (TSG) (Eisner, 2003; Galley et al., 2006; Liu et al., 2006; Huang et al., 2006; Zhang et al., 2008), and inversion transduction grammars (ITG) (Wu, 1997; Xiong et al., 2006). Although these formalisms present simple and precise mechanisms for describing the basic recursive structure of sentences, they are not powerful enough to model some important features of natural language syntax. For example, Chiang (2006) points out that the translation of languages that can stack an unbounded number of clauses in an “inside-out” way (Wu, 1997) 1278 provably goes beyond the expressive power of synchronous CFG and TSG. Therefore, it is necessary to find ways to"
P11-1128,P09-1063,1,0.901939,"Missing"
P11-1128,D08-1022,0,0.0473835,"Missing"
P11-1128,P08-1023,1,0.946539,"that involves two steps. The decoder first converts the input tree into a translation forest using a translation rule set by pattern matching. Huang et al. (2006) show that this step is a depth-first search with memorization in O(n) time. Then, the decoder searches for the best derivation in the translation forest intersected with n-gram language models and outputs the target string. 2 Decoding with STAG, however, poses one major challenge to forest rescoring. As translation forest only supports substitution, it is difficult to construct a translation forest for STAG derivations because of 2 Mi et al. (2008) give a detailed description of the two-step decoding process. Huang and Mi (2010) systematically analyze the decoding complexity of tree-to-string translation. α1 α2 IP0,8 NP2,3 β1 NR2,3 nê VP3,8 ↓ ` aob¯ amˇ a NR2,3 ↓ β2 NP0,2 ↓ NP0,1 NR0,1 ↓ r1 r2 r3 r4 r5 r6 NP2,3 ∗ NN1,2 ↓ NP0,2 NP2,3 ∗ elementary tree α1 α2 β1 β2 β3 α3 NP1,2 α3 β3 NP0,3 NP0,3 NN2,3 oÚ NP1,2 ∗ zˇ ongtˇ ong translation rule ( IP ( NP0:1 ( x1 :NR↓ ) ) ( x2 :VP↓ ) ) → x1 x2 ( NR a` ob¯amˇa ) → Obama ( NP ( NP0:1 ( x1 :NN↓ ) ) ( x2 :NP∗ ) ) → x1 x2 ( NP ( x1 :NP↓ ) ( x2 :NP∗ ) ) → x1 x2 ( NP ( NP ( x1 :NR↓ ) ) ( x2 :NP∗ ) ) →"
P11-1128,2006.amta-papers.15,0,0.0196565,"a larger domain of locality than CFG to state linguistic dependencies that are far apart since the formalism treats trees as basic building blocks. As a mildly context-sensitive grammar, TAG is conjectured to be powerful enough to model natural languages. Synchronous TAG generalizes TAG by allowing the construction of a pair of trees using the TAG operations of substitution and adjoining on tree pairs. The idea of using synchronous TAG in machine translation has been pursued by several researchers (Abeille et al., 1990; Prigent, 1994; Dras, 1999), but only recently in its probabilistic form (Nesson et al., 2006; DeNeefe and Knight, 2009). Shieber (2007) argues that probabilistic synchronous TAG possesses appealing properties such as expressivity and trainability for building a machine translation system. However, one major challenge for applying synchronous TAG to machine translation is computational complexity. While TAG requires O(n6 ) time for monolingual parsing, synchronous TAG requires O(n12 ) for bilingual parsing. One solution is to use tree insertion grammars (TIG) introduced by Schabes and Waters (1995). As a restricted form of TAG, TIG still allows for adjoining of unbounded trees but onl"
P11-1128,J03-1002,0,0.00360945,"n training corpus. Figure 5: Average occurrences of foot node labels VP, NP, and IP over various distances. 5 Evaluation We evaluated our adjoining tree-to-string translation system on Chinese-English translation. The bilingual corpus consists of 1.5M sentences with 42.1M Chinese words and 48.3M English words. The Chinese sentences in the bilingual corpus were parsed by an in-house parser. To maintain a reasonable grammar size, we follow Liu et al. (2006) to restrict that the height of a rule tree is no greater than 3 and the surface string’s length is no greater than 7. After running GIZA++ (Och and Ney, 2003) to obtain word alignment, our rule extraction algorithm extracted 23.0M initial rules without adjoining sites, 6.6M initial rules with adjoining sites, and 5.3M auxiliary rules. We used the SRILM toolkit (Stolcke, 2002) to train a 4-gram language model on the Xinhua portion of the GIGAWORD corpus, which contains 238M English words. We used the 2002 NIST MT Chinese-English test set as the development set and the 2003-2005 NIST test sets as the test sets. We evaluated translation quality using the BLEU metric, as calculated by mteval-v11b.pl with case-insensitive matching of n-grams. Table 2 sh"
P11-1128,P03-1021,0,0.0220395,"n source and target non-terminals. The parameters of a probabilistic synchronous TAG are X Pi (α) = 1 (1) Ps (α|η) = 1 (2) Pa (β|η) + Pa (NONE|η) = 1 (3) α X α X β where α ranges over initial tree pairs, β over auxiliary tree pairs, and η over node pairs. Pi (α) is the probability of beginning a derivation with α; Ps (α|η) is the probability of substituting α at η; Pa (β|η) is the probability of adjoining β at η; finally, Pa (NONE|η) is the probability of nothing adjoining at η. For tree-to-string translation, these parameters can be treated as feature functions of a discriminative framework (Och, 2003) combined with other conventional features such as relative frequency, lexical weight, rule count, language model, and word count (Liu et al., 2006). 3 Rule Extraction Inducing a synchronous TAG from training data often begins with converting Treebank-style parse trees to TAG derivations (Xia, 1999; Chen and Vijay-Shanker, 2000; Chiang, 2003). DeNeefe and 1281 Knight (2009) propose an algorithm to extract synchronous TIG rules from an aligned and parsed bilingual corpus. They first classify tree nodes into heads, arguments, and adjuncts using heuristics (Collins, 2003), then transform a Treeba"
P11-1128,J95-4002,0,0.132014,"beille et al., 1990; Prigent, 1994; Dras, 1999), but only recently in its probabilistic form (Nesson et al., 2006; DeNeefe and Knight, 2009). Shieber (2007) argues that probabilistic synchronous TAG possesses appealing properties such as expressivity and trainability for building a machine translation system. However, one major challenge for applying synchronous TAG to machine translation is computational complexity. While TAG requires O(n6 ) time for monolingual parsing, synchronous TAG requires O(n12 ) for bilingual parsing. One solution is to use tree insertion grammars (TIG) introduced by Schabes and Waters (1995). As a restricted form of TAG, TIG still allows for adjoining of unbounded trees but only requires O(n3 ) time for monolingual parsing. Nesson et al. (2006) firstly demonstrate Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1278–1287, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics NP α1 NP NP α2 X NN , oÚ President NR {I zˇ ongtˇ ong X , X , β1 NP∗ US NP↓ X∗ X↓ mˇeigu´ o NP NP∗ X X∗ NP β2 NP X , NN President oÚ zˇ ongtˇ ong α3 NP X X NP X , NR NN US President {I oÚ ongtˇ ong mˇeigu´ o zˇ Figure 1: Initial and"
P11-1128,C90-3045,0,0.442401,"ese formalisms present simple and precise mechanisms for describing the basic recursive structure of sentences, they are not powerful enough to model some important features of natural language syntax. For example, Chiang (2006) points out that the translation of languages that can stack an unbounded number of clauses in an “inside-out” way (Wu, 1997) 1278 provably goes beyond the expressive power of synchronous CFG and TSG. Therefore, it is necessary to find ways to take advantage of more powerful synchronous grammars to improve machine translation. Synchronous tree adjoining grammars (TAG) (Shieber and Schabes, 1990) are a good candidate. As a formal tree rewriting system, TAG (Joshi et al., 1975; Joshi, 1985) provides a larger domain of locality than CFG to state linguistic dependencies that are far apart since the formalism treats trees as basic building blocks. As a mildly context-sensitive grammar, TAG is conjectured to be powerful enough to model natural languages. Synchronous TAG generalizes TAG by allowing the construction of a pair of trees using the TAG operations of substitution and adjoining on tree pairs. The idea of using synchronous TAG in machine translation has been pursued by several rese"
P11-1128,W07-0412,0,0.157041,"inguistic dependencies that are far apart since the formalism treats trees as basic building blocks. As a mildly context-sensitive grammar, TAG is conjectured to be powerful enough to model natural languages. Synchronous TAG generalizes TAG by allowing the construction of a pair of trees using the TAG operations of substitution and adjoining on tree pairs. The idea of using synchronous TAG in machine translation has been pursued by several researchers (Abeille et al., 1990; Prigent, 1994; Dras, 1999), but only recently in its probabilistic form (Nesson et al., 2006; DeNeefe and Knight, 2009). Shieber (2007) argues that probabilistic synchronous TAG possesses appealing properties such as expressivity and trainability for building a machine translation system. However, one major challenge for applying synchronous TAG to machine translation is computational complexity. While TAG requires O(n6 ) time for monolingual parsing, synchronous TAG requires O(n12 ) for bilingual parsing. One solution is to use tree insertion grammars (TIG) introduced by Schabes and Waters (1995). As a restricted form of TAG, TIG still allows for adjoining of unbounded trees but only requires O(n3 ) time for monolingual pars"
P11-1128,J97-3002,0,0.124134,"g system improves translation quality by +0.7 BLEU over the baseline system only allowing for tree substitution on NIST ChineseEnglish test sets. 1 Introduction Syntax-based translation models, which exploit hierarchical structures of natural languages to guide machine translation, have become increasingly popular in recent years. So far, most of them have been based on synchronous context-free grammars (CFG) (Chiang, 2007), tree substitution grammars (TSG) (Eisner, 2003; Galley et al., 2006; Liu et al., 2006; Huang et al., 2006; Zhang et al., 2008), and inversion transduction grammars (ITG) (Wu, 1997; Xiong et al., 2006). Although these formalisms present simple and precise mechanisms for describing the basic recursive structure of sentences, they are not powerful enough to model some important features of natural language syntax. For example, Chiang (2006) points out that the translation of languages that can stack an unbounded number of clauses in an “inside-out” way (Wu, 1997) 1278 provably goes beyond the expressive power of synchronous CFG and TSG. Therefore, it is necessary to find ways to take advantage of more powerful synchronous grammars to improve machine translation. Synchrono"
P11-1128,P06-1066,1,0.784987,"mproves translation quality by +0.7 BLEU over the baseline system only allowing for tree substitution on NIST ChineseEnglish test sets. 1 Introduction Syntax-based translation models, which exploit hierarchical structures of natural languages to guide machine translation, have become increasingly popular in recent years. So far, most of them have been based on synchronous context-free grammars (CFG) (Chiang, 2007), tree substitution grammars (TSG) (Eisner, 2003; Galley et al., 2006; Liu et al., 2006; Huang et al., 2006; Zhang et al., 2008), and inversion transduction grammars (ITG) (Wu, 1997; Xiong et al., 2006). Although these formalisms present simple and precise mechanisms for describing the basic recursive structure of sentences, they are not powerful enough to model some important features of natural language syntax. For example, Chiang (2006) points out that the translation of languages that can stack an unbounded number of clauses in an “inside-out” way (Wu, 1997) 1278 provably goes beyond the expressive power of synchronous CFG and TSG. Therefore, it is necessary to find ways to take advantage of more powerful synchronous grammars to improve machine translation. Synchronous tree adjoining gra"
P11-1128,P00-1058,0,\N,Missing
P11-1128,P08-1064,0,\N,Missing
P11-1128,W90-0102,0,\N,Missing
P11-2013,W10-0513,0,0.051345,"tylistic variation (6) letter repetition tgthr, weeknd, shudnt 4got, sumbody, kulture t0gether, h3r3, 5top, doinq thimg, macam betta, hubbie, cutie pleeeaas, togtherrr (7) any combination of (1) to (6) luvvvin, 2moro, m0rnin Table 2: Nonstandard tokens that can be processed by the unified letter transformation approach. 2.2 Web based Data Collection w/o Supervision We propose to automatically collect training data (annotate nonstandard words with the corresponding English forms) using a web-based approach, therefore avoiding the expensive human annotation. We use the Edinburgh Twitter corpus (Petrovic et al., 2010) for data collection, which contains 97 million Twitter messages. The English tweets were extracted using the TextCat language identification toolkit (Cavnar and Trenkle, 1994), and tokenized into a sequence of clean tokens consisting of letters, digits, and apostrophe. For the out-of-vocabulary (OOV) tokens consisting of letters and apostrophe, we form n Google queries for each of them in the form of either “w1 w2 w3 ” OOV or OOV “w1 w2 w3 ”, where w1 to w3 are consecutive context words extracted from the tweets that contain this OOV. n is set to 6 in this study. The first 32 returned snippet"
P11-2013,P02-1019,0,0.0674139,"001), Cook and Stevenson (2009) employed the noisy channel model to find the most probable word sequence given the observed noisy message. Their approaches first classified the nonstandard tokens into various categories (e.g., abbreviation, stylistic variation, prefix-clipping), then calculated the posterior probability of the nonstandard tokens based on each category. Choudhury et al. (2007) developed a hidden Markov model using hand annotated training data. Yang et al. (2009), Pennell and Liu (2010) focused on modeling word abbreviations formed by dropping characters from the original word. Toutanova and Moore (2002) addressed the phonetic substitution problem by extending the initial letter-to-phone model. Aw et al. (2006), Kobus et al. (2008) viewed the text message normalization as a statistical machine translation process from the texting language to standard English. Beaufort et al. (2010) experimented with the weighted finitestate machines for normalizing French SMS messages. Most of the above approaches rely heavily on the hand annotated data and involve categorizing the nonstandard tokens in the first place, which gives rise to three problems: (1) the labeled data is very expensive and time consum"
P11-2013,N09-2069,0,0.0566637,"ortland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics (for text messages or other domains), Sproat et al. (2001), Cook and Stevenson (2009) employed the noisy channel model to find the most probable word sequence given the observed noisy message. Their approaches first classified the nonstandard tokens into various categories (e.g., abbreviation, stylistic variation, prefix-clipping), then calculated the posterior probability of the nonstandard tokens based on each category. Choudhury et al. (2007) developed a hidden Markov model using hand annotated training data. Yang et al. (2009), Pennell and Liu (2010) focused on modeling word abbreviations formed by dropping characters from the original word. Toutanova and Moore (2002) addressed the phonetic substitution problem by extending the initial letter-to-phone model. Aw et al. (2006), Kobus et al. (2008) viewed the text message normalization as a statistical machine translation process from the texting language to standard English. Beaufort et al. (2010) experimented with the weighted finitestate machines for normalizing French SMS messages. Most of the above approaches rely heavily on the hand annotated data and involve ca"
P11-2013,P06-2005,0,0.717287,"rved noisy message. Their approaches first classified the nonstandard tokens into various categories (e.g., abbreviation, stylistic variation, prefix-clipping), then calculated the posterior probability of the nonstandard tokens based on each category. Choudhury et al. (2007) developed a hidden Markov model using hand annotated training data. Yang et al. (2009), Pennell and Liu (2010) focused on modeling word abbreviations formed by dropping characters from the original word. Toutanova and Moore (2002) addressed the phonetic substitution problem by extending the initial letter-to-phone model. Aw et al. (2006), Kobus et al. (2008) viewed the text message normalization as a statistical machine translation process from the texting language to standard English. Beaufort et al. (2010) experimented with the weighted finitestate machines for normalizing French SMS messages. Most of the above approaches rely heavily on the hand annotated data and involve categorizing the nonstandard tokens in the first place, which gives rise to three problems: (1) the labeled data is very expensive and time consuming to obtain; (2) it is hard to establish a standard taxonomy for categorizing the tokens found in text mess"
P11-2013,P10-1079,0,0.460264,"lculated the posterior probability of the nonstandard tokens based on each category. Choudhury et al. (2007) developed a hidden Markov model using hand annotated training data. Yang et al. (2009), Pennell and Liu (2010) focused on modeling word abbreviations formed by dropping characters from the original word. Toutanova and Moore (2002) addressed the phonetic substitution problem by extending the initial letter-to-phone model. Aw et al. (2006), Kobus et al. (2008) viewed the text message normalization as a statistical machine translation process from the texting language to standard English. Beaufort et al. (2010) experimented with the weighted finitestate machines for normalizing French SMS messages. Most of the above approaches rely heavily on the hand annotated data and involve categorizing the nonstandard tokens in the first place, which gives rise to three problems: (1) the labeled data is very expensive and time consuming to obtain; (2) it is hard to establish a standard taxonomy for categorizing the tokens found in text messages; (3) the lack of optimized way to integrate various category-specific models often compromises the system performance, as confirmed by (Cook and Stevenson, 2009). In thi"
P11-2013,N07-1047,0,0.0213641,"feature vector for each letter in the dictionary word, using its mapped character as the reference label. This labeled data set is used to train a CRF model with LBFGS (Lafferty et al., 2001; Kudo, 2005). We use the following features: • Character-level features Character n-grams: c−1 , c0 , c1 , (c−2 c−1 ), (c−1 c0 ), (c0 c1 ), (c1 c2 ), (c−3 c−2 c−1 ), (c−2 c−1 c0 ), (c−1 c0 c1 ), (c0 c1 c2 ), (c1 c2 c3 ). The relative position of character in the word. • Phonetic-level features Phoneme n-grams: p−1 , p0 , p1 , (p−1 p0 ), (p0 p1 ). We use the many-to-many letterphoneme alignment algorithm (Jiampojamarn et al., 2007) to map each letter to multiple phonemes (1-to-2 alignment). We use three binary features to indicate whether the current, previous, or next character is a vowel. • Syllable-level features Relative position of the current syllable in the 74 word; two binary features indicating whether the character is at the beginning or the end of the current syllable. The English hyphenation dictionary (Hindson, 2006) is used to mark all the syllable information. The trained CRF model can be applied to any English word to generate its variants with probabilities. 3 Experiments We evaluate the system performa"
P11-2013,C08-1056,0,\N,Missing
P11-2013,W09-2010,0,\N,Missing
P11-5003,D10-1047,0,0.0627991,"Missing"
P11-5003,P06-2020,0,0.0457541,"term t which occurs with probability p in a text consisting of N words is given by t  Estimate the probability of t in three ways    Input + background corpus combines Input only Background only 29 Testing which hypothesis is more likely: log-likelihood ratio test λ= -2 log λ Likelihood of the data given H1 Likelihood of the data given H2 has a known statistical distribution: chi-square At a given significance level, we can decide if a word is descriptive of the input or not. This feature is used in the best performing systems for multi-document summarization of news [Lin and Hovy, 2000; Conroy et al., 2006] 30 15 Motivation & Definition Frequency, TF*IDF, Topic Words Topic Models [LSA, EM, Bayesian] Topic Models Graph Based Methods Supervised Techniques Features, Discriminative Training Sampling, Data, Co-training Global Optimization Methods Iterative, Greedy, Dynamic Programming ILP, Sub-Modular Selection Speech Summarization Segmentation, ASR Acoustic Information, Disfluency Evaluation Manual (Pyramid), Automatic (Rouge, F-Measure) Fully Automatic 31 The background corpus takes more central stage  Learn topics from the background corpus     topic ~ themes often discusses in the backgroun"
P11-5003,W03-1203,0,0.0765119,"Missing"
P11-5003,W06-1643,0,0.0464152,"Missing"
P11-5003,W09-1802,0,0.0487993,"Missing"
P11-5003,W00-0405,0,0.230027,"Missing"
P11-5003,W06-0701,0,0.0604512,"Missing"
P11-5003,N09-1041,0,0.0828029,"Missing"
P11-5003,C00-1072,0,0.0532023,"elihood of observing term t which occurs with probability p in a text consisting of N words is given by t  Estimate the probability of t in three ways    Input + background corpus combines Input only Background only 29 Testing which hypothesis is more likely: log-likelihood ratio test λ= -2 log λ Likelihood of the data given H1 Likelihood of the data given H2 has a known statistical distribution: chi-square At a given significance level, we can decide if a word is descriptive of the input or not. This feature is used in the best performing systems for multi-document summarization of news [Lin and Hovy, 2000; Conroy et al., 2006] 30 15 Motivation & Definition Frequency, TF*IDF, Topic Words Topic Models [LSA, EM, Bayesian] Topic Models Graph Based Methods Supervised Techniques Features, Discriminative Training Sampling, Data, Co-training Global Optimization Methods Iterative, Greedy, Dynamic Programming ILP, Sub-Modular Selection Speech Summarization Segmentation, ASR Acoustic Information, Disfluency Evaluation Manual (Pyramid), Automatic (Rouge, F-Measure) Fully Automatic 31 The background corpus takes more central stage  Learn topics from the background corpus     topic ~ themes often discu"
P11-5003,N10-1134,0,0.0420107,"Missing"
P11-5003,P09-2066,1,0.894416,"Missing"
P11-5003,D09-1032,1,0.898421,"Missing"
P11-5003,N06-2023,1,0.869977,"Missing"
P11-5003,W05-0905,0,0.0631504,"Missing"
P11-5003,N04-1019,1,0.385841,"Missing"
P11-5003,W02-0401,0,0.10109,"Missing"
P11-5003,P08-1054,0,0.0732559,"Missing"
P11-5003,C04-1129,1,0.86581,"Missing"
P11-5003,J02-4004,0,0.106324,"Missing"
P11-5003,C08-1124,0,0.0978198,"Missing"
P11-5003,N10-1006,1,0.89478,"Missing"
P11-5003,J02-4003,0,0.227966,"bout ASR errors?  Deliver summary using original speech    Can avoid showing recognition errors in the delivered text summary But still need to correctly identify summary sentences/segments Use recognition confidence measure and multiple candidates to help better summarize 127 Address problems due to ASR errors  Re-define summarization task: select sentences that are most informative, at the same time have high recognition accuracy   Important words tend to have high recognition accuracy Use ASR confidence measure or n-gram language model scores in summarization  Unsupervised methods [Zechner, 2002; Kikuchi et al., 2003; Maskey, 2008]  Use as a feature in supervised methods 128 64 Address problems due to ASR errors  Use multiple recognition candidates    n-best lists [Liu et al., 2010] Lattices [Lin et al., 2010] Confusion network [Xie and Liu, 2010]     Use in MMR framework Summarization segment/unit contains all the word candidates (or pruned ones based on probabilities) Term weights (TF, IDF) use candidate’s posteriors Improved performance over using 1-best recognition output 129 Motivation & Definition Topic Models Frequency, TF*IDF, Topic Words Topic Models [LSA, EM, Bayes"
P11-5003,A00-2025,0,0.0976678,"Missing"
P11-5003,N06-2050,0,0.0596245,"Missing"
P11-5003,P09-1062,0,0.0430067,"Missing"
P12-1058,N09-2013,0,0.0284299,"Missing"
P12-1058,W04-2328,0,0.0147313,"evant answers to questions. An approach using email structure to detect and summarize question answer pairs was introduced in (Shrestha and Mckeown, 2004). These studies focused primarily on finding questions and answers in an online environment. In this paper, in order to provide a better foundation for question answer detection in online forums, we investigate tagging sentences with a much richer set of categories, as well as identifying their dependency relationships. The sentence types we use are similar to dialog acts (DA), but defined specifically for question answering forums. Work of (Clark and Popescu-Belis, 2004) defined a reusable multi-level tagset that can be mapped from conversational speech corpora such as the ICSI meeting data. However, it is hard to reuse any available corpus or DA tagset because our task is different, and also online forum has a different style from speech data. Automatic DA tagging has been studied a lot previously. For example, in (Stolcke et al., 2000), Hidden Markov Models (HMMs) were used for DA tagging; in (Ji and Bilmes, 2005), different types of graphical models were explored. Our study is different in several aspects: we are using forum domains, unlike most work of DA"
P12-1058,P08-1081,0,0.163458,"nd the sentence dependency structure of a QA thread. Towards this goal, in this paper, we define two tasks: labeling the types for sentences, and finding the dependency relations between sentences. For the first task of sentence type labeling, we define a rich set of categories representing the purpose 555 There is a lot of useful knowledge in the user generated content such as forums. This knowledge source could substantially help automatic question answering systems. There has been some previous work focusing on the extraction of question and corresponding answer pairs in online forums. In (Ding et al., 2008), a two-pass approach was used to find relevant solutions for a given question, and a skipchain CRF was adopted to model long range dependency between sentences. A graph propagation method was used in (Cong et al., 2008) to rank relevant answers to questions. An approach using email structure to detect and summarize question answer pairs was introduced in (Shrestha and Mckeown, 2004). These studies focused primarily on finding questions and answers in an online environment. In this paper, in order to provide a better foundation for question answer detection in online forums, we investigate tag"
P12-1058,C04-1128,0,0.0267268,"as forums. This knowledge source could substantially help automatic question answering systems. There has been some previous work focusing on the extraction of question and corresponding answer pairs in online forums. In (Ding et al., 2008), a two-pass approach was used to find relevant solutions for a given question, and a skipchain CRF was adopted to model long range dependency between sentences. A graph propagation method was used in (Cong et al., 2008) to rank relevant answers to questions. An approach using email structure to detect and summarize question answer pairs was introduced in (Shrestha and Mckeown, 2004). These studies focused primarily on finding questions and answers in an online environment. In this paper, in order to provide a better foundation for question answer detection in online forums, we investigate tagging sentences with a much richer set of categories, as well as identifying their dependency relationships. The sentence types we use are similar to dialog acts (DA), but defined specifically for question answering forums. Work of (Clark and Popescu-Belis, 2004) defined a reusable multi-level tagset that can be mapped from conversational speech corpora such as the ICSI meeting data."
P12-1058,J00-3003,0,0.251283,"Missing"
P12-1058,W00-1308,0,0.0659922,"small in the data set, it plays a vital role to determine the quality of answers. The main reason for the small number is that, unlike problem descriptions, much fewer sentences are needed to give feedbacks. 5 Experiment In the experiment, we randomly split annotated threads into three disjoint sets, and run a three-fold cross validation. Within each fold, first sentence types are labeled using linear-chain CRFs, then the resulting sentence type tagging is used in the second pass to determine dependency relations. For part-of-speech (POS) tagging of the sentences, we used Stanford POS Tagger (Toutanova and Manning, 2000). All the graphical inference and estimations are done using MALLET package (McCallum, 2002). In this paper, we evaluate the results using standard precision and recall. In the sentence type tagging task, we calculate precision, recall, and F1 score for each individual tag. For the dependency tagging task, a pair identified by the system is correct only if the exact pair appears in the reference annotation. Precision and recall scores are calculated accordingly. 5.1 Sentence Type Tagging Results The results of sentence type tagging using linearchain CRFs are shown in Table 5. For a comparison,"
P12-2033,P05-1022,0,0.04833,"from the Google Web 1T 5-gram. • transition features: a combination of the current output label and the previous one, together with some observation features such as the unigram and bigrams of word or POS tag. 3.2 Discriminative Reranking Although CRFs is able to model the dependency of adjacent labels, it does not measure the quality of the whole sentence. In this work, we propose to use discriminative training to rerank the candidates generated in the first step. Reranking has been used in many tasks to find better global solutions, such as machine translation (Wang et al., 2007), parsing (Charniak and Johnson, 2005), and disfluency detection (Zwarts and Johnson, 2011). We use a maximum Entropy reranker to learn distributions over a set of candidates such that the probability of the best compression is maximized. The conditional probability of output y given observation x in the maximum entropy model is defined as: p(y|x) = 1 Z(x) exp hP k i=1 λi f (x, y) then “so I 10”, “so I should 100” are included in the features (1 means the word is deleted). • The log likelihood of the candidate sentence based on the language model. • The absolute difference of the compression ratio of the candidate sentence with th"
P12-2033,C08-1018,0,0.023876,"early defined. We use sentences and utterances interchangeably when there is no ambiguity. In this study we investigate sentence compression of spoken utterances in order to remove redundant or unnecessary words while trying to preserve the information in the original sentence. Sentence compression has been studied from formal text domain to speech domain. In text domain, (Knight and Marcu, 2000) applies noisy-channel model and decision tree approaches on this problem. (Galley and Mckeown, 2007) proposes to use a synchronous context-free grammars (SCFG) based method to compress the sentence. (Cohn and Lapata, 2008) expands the operation set by including insertion, substitution and reordering, and incorporates grammar rules. In speech domain, (Clarke and Lapata, 2008) investigates sentence compression in broadcast news using an integer linear programming approach. There is only a few existing work in spontaneous speech domains. (Liu and Liu, 2010) modeled it as a sequence labeling problem using conditional random fields model. (Liu and Liu, 2009) compared the effect of different compression methods on a meeting summarization task, but did not evaluate sentence compression itself. We propose to use a two-"
P12-2033,N07-1023,0,0.0231817,"tences1 from human annotation (removed words shown in bold): [original sentence] 1 For speech domains, “sentences” are not clearly defined. We use sentences and utterances interchangeably when there is no ambiguity. In this study we investigate sentence compression of spoken utterances in order to remove redundant or unnecessary words while trying to preserve the information in the original sentence. Sentence compression has been studied from formal text domain to speech domain. In text domain, (Knight and Marcu, 2000) applies noisy-channel model and decision tree approaches on this problem. (Galley and Mckeown, 2007) proposes to use a synchronous context-free grammars (SCFG) based method to compress the sentence. (Cohn and Lapata, 2008) expands the operation set by including insertion, substitution and reordering, and incorporates grammar rules. In speech domain, (Clarke and Lapata, 2008) investigates sentence compression in broadcast news using an integer linear programming approach. There is only a few existing work in spontaneous speech domains. (Liu and Liu, 2010) modeled it as a sequence labeling problem using conditional random fields model. (Liu and Liu, 2009) compared the effect of different compr"
P12-2033,P09-2066,1,0.842646,"ee approaches on this problem. (Galley and Mckeown, 2007) proposes to use a synchronous context-free grammars (SCFG) based method to compress the sentence. (Cohn and Lapata, 2008) expands the operation set by including insertion, substitution and reordering, and incorporates grammar rules. In speech domain, (Clarke and Lapata, 2008) investigates sentence compression in broadcast news using an integer linear programming approach. There is only a few existing work in spontaneous speech domains. (Liu and Liu, 2010) modeled it as a sequence labeling problem using conditional random fields model. (Liu and Liu, 2009) compared the effect of different compression methods on a meeting summarization task, but did not evaluate sentence compression itself. We propose to use a two-step approach in this paper for sentence compression of spontaneous speech utterances. The contributions of our work are: • Our proposed two-step approach allows us to incorporate features from local and global levels. In the first step, we adopt a similar sequence labeling method as used in (Liu and Liu, 2010), but expanded the feature set, which 166 Proceedings of the 50th Annual Meeting of the Association for Computational Linguisti"
P12-2033,W11-1611,0,0.0446747,"Missing"
P12-2033,P11-1071,0,0.0291077,"a combination of the current output label and the previous one, together with some observation features such as the unigram and bigrams of word or POS tag. 3.2 Discriminative Reranking Although CRFs is able to model the dependency of adjacent labels, it does not measure the quality of the whole sentence. In this work, we propose to use discriminative training to rerank the candidates generated in the first step. Reranking has been used in many tasks to find better global solutions, such as machine translation (Wang et al., 2007), parsing (Charniak and Johnson, 2005), and disfluency detection (Zwarts and Johnson, 2011). We use a maximum Entropy reranker to learn distributions over a set of candidates such that the probability of the best compression is maximized. The conditional probability of output y given observation x in the maximum entropy model is defined as: p(y|x) = 1 Z(x) exp hP k i=1 λi f (x, y) then “so I 10”, “so I should 100” are included in the features (1 means the word is deleted). • The log likelihood of the candidate sentence based on the language model. • The absolute difference of the compression ratio of the candidate sentence with that of the first ranked candidate. This is because we"
P13-1001,P06-1121,0,0.102437,"Missing"
P13-1001,D11-1044,0,0.0209726,"s to parse partial translations at the same time. One challenge is that MST parsing itself is not incremental, making it expensive to identify loops during hypothesis expansion. On the contrary, shiftreduce parsing is naturally incremental and can be seamlessly integrated into left-to-right phrasebased decoding. More importantly, in our work dependency trees are memorized for phrases rather than being generated word by word on the fly in decoding. This treatment might not only reduce decoding complexity but also potentially revolve local parsing ambiguity. Our decoding algorithm is similar to Gimpel and Smith (2011)’s lattice parsing algorithm as we divide decoding into two steps: hypergraph generation and hypergraph rescoring. The major difference is that our hypergraph is not a phrasal lattice because each phrase pair is associated with a dependency structure on the target side. In other words, our second pass is to find the Viterbi derivation with addition features rather than parsing the phrasal lattice. In addition, their algorithm produces phrasal dependency parse trees while the leaves of our dependency trees are words, making dependency language models can be directly used. Shift-reduce parsing h"
P13-1001,P07-1019,0,0.12886,"@tsinghua.edu.cn Abstract 2006; Liu et al., 2006; Huang et al., 2006; Shen et al., 2008; Mi and Huang, 2008; Zhang et al., 2008). As syntactic information can be exploited to provide linguistically-motivated reordering rules, predicting non-local permutation is computationally tractable in syntax-based approaches. Unfortunately, as syntax-based decoders often generate target-language words in a bottom-up way using the CKY algorithm, integrating n-gram language models becomes more expensive because they have to maintain target boundary words at both ends of a partial translation (Chiang, 2007; Huang and Chiang, 2007). Moreover, syntax-based approaches often suffer from the rule coverage problem since syntactic constraints rule out a large portion of nonsyntactic phrase pairs, which might help decoders generalize well to unseen data (Marcu et al., 2006). Furthermore, the introduction of nonterminals makes the grammar size significantly bigger than phrase tables and leads to higher memory requirement (Chiang, 2007). We introduce a shift-reduce parsing algorithm for phrase-based string-todependency translation. As the algorithm generates dependency trees for partial translations left-to-right in decoding, it"
P13-1001,D10-1027,0,0.0600961,"rmits arbitrary permutation, it proves to be NP-complete (Knight, 1999). Syntax-based approaches, on the other hand, model the hierarchical structure of natural languages (Wu, 1997; Yamada and Knight, 2001; Chiang, 2005; Quirk et al., 2005; Galley et al., As a result, incremental decoding with hierarchical structures has attracted increasing attention in recent years. While some authors try to integrate syntax into phrase-based decoding (Galley and Manning, 2008; Galley and Manning, 2009; Feng et al., 2010), others develop incremental algorithms for syntax-based models (Watanabe et al., 2006; Huang and Mi, 2010; Dyer and Resnik, 2010; Feng et al., 2012). Despite these successful efforts, challenges still remain for both directions. While parsing algorithms can be used to parse partial translations in phrase-based decoding, the search space is significantly enlarged since there are exponentially many parse trees for exponentially many translations. On the other hand, although target words can be generated left-to-right by altering the way of tree transversal in syntaxbased models, it is still difficult to reach full rule coverage as compared with phrase table. 1 Proceedings of the 51st Annual Meeting"
P13-1001,P10-1110,0,0.247526,"Missing"
P13-1001,2006.amta-papers.8,0,0.0258954,"ecoding but limited to adding structural constraints. Galley and Manning (2008) propose a shift-reduce algorithm to integrate a hierarchical reordering model into phrase-based systems. Feng et al. (2010) use shift-reduce parsing to impose ITG (Wu, 1997) constraints on phrase permutation. Our work differs from theirs by going further to incorporate linguistic syntax into phrase-based decoding. Along another line, a number of authors have developed incremental algorithms for syntaxbased models (Watanabe et al., 2006; Huang and Mi, 2010; Dyer and Resnik, 2010; Feng et al., 2012). Watanabe et al. (2006) introduce an Earlystyle top-down parser based on binary-branching Greibach Normal Form. Huang et al. (2010), Dyer 32.50 32.00 31.50 31.00 this work Moses 30.50 0 2 4 6 8 10 12 distortion limit Figure 5: Performance of Moses and our system with various distortion limits. dency language models without loss in rule coverage, it achieves significantly better results than Moses on all test sets. The gains in TER are much larger than BLEU because dependency language models do not model n-grams directly. Compared with the bottom-up string-to-dependency system, our system outperforms consistently but"
P13-1001,D09-1127,0,0.0863584,"ng exponentially many derivations compactly, the negative effect of propagating mistakes made in the first pass to the second pass can be minimized. To improve rule coverage, we follow Shen et al. (2008) to use ill-formed structures in decoding. If an ill-formed structure has a single root, it can treated as a (pseudo) fixed structure; otherwise it is transformed to one (pseudo) left floating structure and one (pseudo) right floating structure. We use a feature to count how many ill-formed structures are used in decoding. only need to focus on “h+h” states. In addition, we follow Huang et al. (2009) to use the heuristic of “shortest stack” to always prefer Rl to S. 4 Decoding Our decoder is based on a linear model (Och, 2003) with the following features: 1. relative frequencies in two directions; 2. lexical weights in two directions; 3. phrase penalty; 4. distance-based reordering model; 5. lexicaized reordering model; 6. n-gram language model model; 7. word penalty; 8. ill-formed structure penalty; 9. dependency language model; 5 Experiments 10. maximum entropy parsing model. We evaluated our phrase-based string-todependency translation system on ChineseEnglish translation. The training"
P13-1001,P08-1067,0,0.0302332,"re 1. β multiplied with the best score in the stack, or 2. the score of b-th best state in the stack. As the stack of a state keeps changing during the decoding process, the context information needed to calculate dependency language model and maximum entropy model probabilities (e.g., root word, leftmost child, etc.) changes dynamically as well. As a result, the chance of risk-free hypothesis recombination (Koehn et al., 2003) significantly decreases because complicated contextual information is much less likely to be identical. Therefore, we use hypergraph reranking (Huang and Chiang, 2007; Huang, 2008), which proves to be effective for integrating non-local features into dynamic programming, to alleviate this problem. The decoding process is divided into two passes. In the first pass, only standard features (i.e., features 1-7 in the list in the beginning of this section) are used to produce a hypergraph. 3 In the second pass, we use the hypergraph reranking algorithm (Huang, 2008) to find promising translations using additional dependency features (i.e., features 8-10 in the list). As hypergraph is capable of storing exponentially many derivations compactly, the negative effect of propagat"
P13-1001,P03-1054,0,0.00411698,"h, 2003) with the following features: 1. relative frequencies in two directions; 2. lexical weights in two directions; 3. phrase penalty; 4. distance-based reordering model; 5. lexicaized reordering model; 6. n-gram language model model; 7. word penalty; 8. ill-formed structure penalty; 9. dependency language model; 5 Experiments 10. maximum entropy parsing model. We evaluated our phrase-based string-todependency translation system on ChineseEnglish translation. The training data consists of 2.9M pairs of sentences with 76.0M Chinese words and 82.2M English words. We used the Stanford parser (Klein and Manning, 2003) to get dependency trees for English sentences. We used the SRILM toolkit (Stolcke, 2002) to train a In practice, we extend deterministic shiftreduce parsing with beam search (Zhang and Clark, 2008; Huang et al., 2009). As shown in Algorithm 1, the algorithm maintains a list of stacks V and each stack groups states with the same number of accumulated actions (line 2). The stack list V initializes with an empty state v0 (line 3). Then, the states in the stack are iteratively extended until there are no incomplete states (lines 4-12). The search space is constrained by discarding any state that"
P13-1001,P05-1033,0,0.0968776,"and reordering. In addition, it is straightforward to integrate n-gram language models into phrase-based decoders in which translation always grows left-to-right. As a result, phrase-based decoders only need to maintain the boundary words on one end to calculate language model probabilities. However, as phrase-based decoding usually casts translation as a string concatenation problem and permits arbitrary permutation, it proves to be NP-complete (Knight, 1999). Syntax-based approaches, on the other hand, model the hierarchical structure of natural languages (Wu, 1997; Yamada and Knight, 2001; Chiang, 2005; Quirk et al., 2005; Galley et al., As a result, incremental decoding with hierarchical structures has attracted increasing attention in recent years. While some authors try to integrate syntax into phrase-based decoding (Galley and Manning, 2008; Galley and Manning, 2009; Feng et al., 2010), others develop incremental algorithms for syntax-based models (Watanabe et al., 2006; Huang and Mi, 2010; Dyer and Resnik, 2010; Feng et al., 2012). Despite these successful efforts, challenges still remain for both directions. While parsing algorithms can be used to parse partial translations in phrase-"
P13-1001,J99-4005,0,0.115621,"et al., 2003; Och and Ney, 2004). As phrases are capable of memorizing local context, phrase-based approaches excel at handling local word selection and reordering. In addition, it is straightforward to integrate n-gram language models into phrase-based decoders in which translation always grows left-to-right. As a result, phrase-based decoders only need to maintain the boundary words on one end to calculate language model probabilities. However, as phrase-based decoding usually casts translation as a string concatenation problem and permits arbitrary permutation, it proves to be NP-complete (Knight, 1999). Syntax-based approaches, on the other hand, model the hierarchical structure of natural languages (Wu, 1997; Yamada and Knight, 2001; Chiang, 2005; Quirk et al., 2005; Galley et al., As a result, incremental decoding with hierarchical structures has attracted increasing attention in recent years. While some authors try to integrate syntax into phrase-based decoding (Galley and Manning, 2008; Galley and Manning, 2009; Feng et al., 2010), others develop incremental algorithms for syntax-based models (Watanabe et al., 2006; Huang and Mi, 2010; Dyer and Resnik, 2010; Feng et al., 2012). Despite"
P13-1001,J07-2003,0,0.500659,"na liuyang2011@tsinghua.edu.cn Abstract 2006; Liu et al., 2006; Huang et al., 2006; Shen et al., 2008; Mi and Huang, 2008; Zhang et al., 2008). As syntactic information can be exploited to provide linguistically-motivated reordering rules, predicting non-local permutation is computationally tractable in syntax-based approaches. Unfortunately, as syntax-based decoders often generate target-language words in a bottom-up way using the CKY algorithm, integrating n-gram language models becomes more expensive because they have to maintain target boundary words at both ends of a partial translation (Chiang, 2007; Huang and Chiang, 2007). Moreover, syntax-based approaches often suffer from the rule coverage problem since syntactic constraints rule out a large portion of nonsyntactic phrase pairs, which might help decoders generalize well to unseen data (Marcu et al., 2006). Furthermore, the introduction of nonterminals makes the grammar size significantly bigger than phrase tables and leads to higher memory requirement (Chiang, 2007). We introduce a shift-reduce parsing algorithm for phrase-based string-todependency translation. As the algorithm generates dependency trees for partial translations left"
P13-1001,N03-1017,0,0.106459,"Missing"
P13-1001,N10-1128,0,0.0905483,"utation, it proves to be NP-complete (Knight, 1999). Syntax-based approaches, on the other hand, model the hierarchical structure of natural languages (Wu, 1997; Yamada and Knight, 2001; Chiang, 2005; Quirk et al., 2005; Galley et al., As a result, incremental decoding with hierarchical structures has attracted increasing attention in recent years. While some authors try to integrate syntax into phrase-based decoding (Galley and Manning, 2008; Galley and Manning, 2009; Feng et al., 2010), others develop incremental algorithms for syntax-based models (Watanabe et al., 2006; Huang and Mi, 2010; Dyer and Resnik, 2010; Feng et al., 2012). Despite these successful efforts, challenges still remain for both directions. While parsing algorithms can be used to parse partial translations in phrase-based decoding, the search space is significantly enlarged since there are exponentially many parse trees for exponentially many translations. On the other hand, although target words can be generated left-to-right by altering the way of tree transversal in syntaxbased models, it is still difficult to reach full rule coverage as compared with phrase table. 1 Proceedings of the 51st Annual Meeting of the Association for"
P13-1001,P07-2045,0,0.0411688,"-dependency translation. The basic unit of translation in our model is string-to-dependency phrase pair, which consists of a phrase on the source side and a dependency structure on the target side. The algorithm generates well-formed dependency structures for partial translations left-to-right using string-todependency phrase pairs. Therefore, our approach is capable of combining the advantages of both phrase-based and syntax-based approaches: We evaluate our method on the NIST ChineseEnglish translation datasets. Experiments show that our approach significantly outperforms both phrase-based (Koehn et al., 2007) and string-todependency approaches (Shen et al., 2008) in terms of BLEU and TER. 1. compact rule table: our rule table is a subset of the original string-to-dependency grammar (Shen et al., 2008; Shen et al., 2010) by excluding rules with non-terminals. 2 Shift-Reduce Parsing for Phrase-based String-to-Dependency Translation 2. full rule coverage: all phrase pairs, both syntactic and non-syntactic, can be used in our algorithm. This is the same with Moses (Koehn et al., 2007). Figure 1 shows a training example consisting of a (romanized) Chinese sentence, an English dependency tree, and the w"
P13-1001,C10-2033,1,0.934544,"ties. However, as phrase-based decoding usually casts translation as a string concatenation problem and permits arbitrary permutation, it proves to be NP-complete (Knight, 1999). Syntax-based approaches, on the other hand, model the hierarchical structure of natural languages (Wu, 1997; Yamada and Knight, 2001; Chiang, 2005; Quirk et al., 2005; Galley et al., As a result, incremental decoding with hierarchical structures has attracted increasing attention in recent years. While some authors try to integrate syntax into phrase-based decoding (Galley and Manning, 2008; Galley and Manning, 2009; Feng et al., 2010), others develop incremental algorithms for syntax-based models (Watanabe et al., 2006; Huang and Mi, 2010; Dyer and Resnik, 2010; Feng et al., 2012). Despite these successful efforts, challenges still remain for both directions. While parsing algorithms can be used to parse partial translations in phrase-based decoding, the search space is significantly enlarged since there are exponentially many parse trees for exponentially many translations. On the other hand, although target words can be generated left-to-right by altering the way of tree transversal in syntaxbased models, it is still dif"
P13-1001,P06-1077,1,0.870757,"Missing"
P13-1001,P01-1067,0,0.135338,"ing local word selection and reordering. In addition, it is straightforward to integrate n-gram language models into phrase-based decoders in which translation always grows left-to-right. As a result, phrase-based decoders only need to maintain the boundary words on one end to calculate language model probabilities. However, as phrase-based decoding usually casts translation as a string concatenation problem and permits arbitrary permutation, it proves to be NP-complete (Knight, 1999). Syntax-based approaches, on the other hand, model the hierarchical structure of natural languages (Wu, 1997; Yamada and Knight, 2001; Chiang, 2005; Quirk et al., 2005; Galley et al., As a result, incremental decoding with hierarchical structures has attracted increasing attention in recent years. While some authors try to integrate syntax into phrase-based decoding (Galley and Manning, 2008; Galley and Manning, 2009; Feng et al., 2010), others develop incremental algorithms for syntax-based models (Watanabe et al., 2006; Huang and Mi, 2010; Dyer and Resnik, 2010; Feng et al., 2012). Despite these successful efforts, challenges still remain for both directions. While parsing algorithms can be used to parse partial translati"
P13-1001,W06-1606,0,0.0287588,"al permutation is computationally tractable in syntax-based approaches. Unfortunately, as syntax-based decoders often generate target-language words in a bottom-up way using the CKY algorithm, integrating n-gram language models becomes more expensive because they have to maintain target boundary words at both ends of a partial translation (Chiang, 2007; Huang and Chiang, 2007). Moreover, syntax-based approaches often suffer from the rule coverage problem since syntactic constraints rule out a large portion of nonsyntactic phrase pairs, which might help decoders generalize well to unseen data (Marcu et al., 2006). Furthermore, the introduction of nonterminals makes the grammar size significantly bigger than phrase tables and leads to higher memory requirement (Chiang, 2007). We introduce a shift-reduce parsing algorithm for phrase-based string-todependency translation. As the algorithm generates dependency trees for partial translations left-to-right in decoding, it allows for efficient integration of both n-gram and dependency language models. To resolve conflicts in shift-reduce parsing, we propose a maximum entropy model trained on the derivation graph of training data. As our approach combines the"
P13-1001,D08-1059,0,0.287297,"y translation. Figure 2 shows an example. We describe a state (i.e., parser configuration) as a tuple hS, Ci where S is a stack that stores items and C is a coverage vector that indicates which source words have been translated. Each item s ∈ S is a well-formed dependency structure. The algorithm starts with an empty state. At each step, it chooses one of the three actions (Huang et al., 2009) to extend a state: The decoding process terminates when all source words are covered and there is a complete dependency tree in the stack. Note that unlike monolingual shift-reduce parsers (Nivre, 2004; Zhang and Clark, 2008; Huang et al., 2009), our algorithm does not maintain a queue for remaining words of the input because the future dependency structure to be shifted is unknown in advance in the translation scenario. Instead, we use a coverage vector on the source side to determine when to terminate the algorithm. For an input sentence of J words, the number of actions is 2K − 1, where K is the number of rules used in decoding. 1 There are always K shifts and 1. shift (S): move a target dependency structure onto the stack; 1 Empirically, we find that the average number of stacks for J words is about 1.5 × J o"
P13-1001,H05-1066,0,0.0466218,"ines: 1. The Moses phrase-based decoder (Koehn et al., 2007). 2. A re-implementation of bottom-up string-todependency decoder (Shen et al., 2008). All the three systems share with the same targetside parsed, word-aligned training data. The histogram pruning parameter b is set to 100 and 4 7 http://homepages.inf.ed.ac.uk/lzhang10/maxent.html forms Moses in all cases, suggesting that adding dependency helps improve phrase reordering. 34.50 34.00 33.50 6 Related Work BLEU 33.00 The work of Galley and Manning (2009) is closest in spirit to ours. They introduce maximum spanning tree (MST) parsing (McDonald et al., 2005) into phrase-based translation. The system is phrase-based except that an MST parser runs to parse partial translations at the same time. One challenge is that MST parsing itself is not incremental, making it expensive to identify loops during hypothesis expansion. On the contrary, shiftreduce parsing is naturally incremental and can be seamlessly integrated into left-to-right phrasebased decoding. More importantly, in our work dependency trees are memorized for phrases rather than being generated word by word on the fly in decoding. This treatment might not only reduce decoding complexity but"
P13-1001,P08-1064,0,0.0364739,"Missing"
P13-1001,P08-1023,0,0.0416333,"Missing"
P13-1001,W04-0308,0,0.0230457,"g-todependency translation. Figure 2 shows an example. We describe a state (i.e., parser configuration) as a tuple hS, Ci where S is a stack that stores items and C is a coverage vector that indicates which source words have been translated. Each item s ∈ S is a well-formed dependency structure. The algorithm starts with an empty state. At each step, it chooses one of the three actions (Huang et al., 2009) to extend a state: The decoding process terminates when all source words are covered and there is a complete dependency tree in the stack. Note that unlike monolingual shift-reduce parsers (Nivre, 2004; Zhang and Clark, 2008; Huang et al., 2009), our algorithm does not maintain a queue for remaining words of the input because the future dependency structure to be shifted is unknown in advance in the translation scenario. Instead, we use a coverage vector on the source side to determine when to terminate the algorithm. For an input sentence of J words, the number of actions is 2K − 1, where K is the number of rules used in decoding. 1 There are always K shifts and 1. shift (S): move a target dependency structure onto the stack; 1 Empirically, we find that the average number of stacks for J w"
P13-1001,J04-4002,0,0.0408721,"o resolve conflicts in shift-reduce parsing, we propose a maximum entropy model trained on the derivation graph of training data. As our approach combines the merits of phrase-based and string-todependency models, it achieves significant improvements over the two baselines on the NIST Chinese-English datasets. 1 Introduction Modern statistical machine translation approaches can be roughly divided into two broad categories: phrase-based and syntax-based. Phrase-based approaches treat phrase, which is usually a sequence of consecutive words, as the basic unit of translation (Koehn et al., 2003; Och and Ney, 2004). As phrases are capable of memorizing local context, phrase-based approaches excel at handling local word selection and reordering. In addition, it is straightforward to integrate n-gram language models into phrase-based decoders in which translation always grows left-to-right. As a result, phrase-based decoders only need to maintain the boundary words on one end to calculate language model probabilities. However, as phrase-based decoding usually casts translation as a string concatenation problem and permits arbitrary permutation, it proves to be NP-complete (Knight, 1999). Syntax-based appr"
P13-1001,P03-1021,0,0.0613618,"pass can be minimized. To improve rule coverage, we follow Shen et al. (2008) to use ill-formed structures in decoding. If an ill-formed structure has a single root, it can treated as a (pseudo) fixed structure; otherwise it is transformed to one (pseudo) left floating structure and one (pseudo) right floating structure. We use a feature to count how many ill-formed structures are used in decoding. only need to focus on “h+h” states. In addition, we follow Huang et al. (2009) to use the heuristic of “shortest stack” to always prefer Rl to S. 4 Decoding Our decoder is based on a linear model (Och, 2003) with the following features: 1. relative frequencies in two directions; 2. lexical weights in two directions; 3. phrase penalty; 4. distance-based reordering model; 5. lexicaized reordering model; 6. n-gram language model model; 7. word penalty; 8. ill-formed structure penalty; 9. dependency language model; 5 Experiments 10. maximum entropy parsing model. We evaluated our phrase-based string-todependency translation system on ChineseEnglish translation. The training data consists of 2.9M pairs of sentences with 76.0M Chinese words and 82.2M English words. We used the Stanford parser (Klein an"
P13-1001,P02-1040,0,0.0875822,"Missing"
P13-1001,P05-1034,0,0.082473,"Missing"
P13-1001,W05-0908,0,0.0608627,"Missing"
P13-1001,P08-1066,0,0.274118,"r1 r2 r3 r4 r5 yu jiang President will source phrase fangwen yu siyue zongtong jiang yu siyue lai lundun zongtong jiang siyue visit lai London target phrase visit in April The President will London in April President will lundun fangwen in April dependency {} {1 → 2} {2 → 1} {2 → 3} {} category fixed fixed floating left floating right ill-formed Figure 1: A training example consisting of a (romanized) Chinese sentence, an English dependency tree, and the word alignment between them. Each translation rule is composed of a source phrase, a target phrase with a set of dependency arcs. Following Shen et al. (2008), we distinguish between fixed, floating, and ill-formed structures. 5. resolving local parsing ambiguity: as dependency trees for phrases are memorized in rules, our approach avoids resolving local parsing ambiguity and explores in a smaller search space than parsing word-by-word on the fly in decoding (Galley and Manning, 2009). In this paper, we propose a shift-reduce parsing algorithm for phrase-based string-to-dependency translation. The basic unit of translation in our model is string-to-dependency phrase pair, which consists of a phrase on the source side and a dependency structure on t"
P13-1001,J10-4005,0,0.0325939,"rates well-formed dependency structures for partial translations left-to-right using string-todependency phrase pairs. Therefore, our approach is capable of combining the advantages of both phrase-based and syntax-based approaches: We evaluate our method on the NIST ChineseEnglish translation datasets. Experiments show that our approach significantly outperforms both phrase-based (Koehn et al., 2007) and string-todependency approaches (Shen et al., 2008) in terms of BLEU and TER. 1. compact rule table: our rule table is a subset of the original string-to-dependency grammar (Shen et al., 2008; Shen et al., 2010) by excluding rules with non-terminals. 2 Shift-Reduce Parsing for Phrase-based String-to-Dependency Translation 2. full rule coverage: all phrase pairs, both syntactic and non-syntactic, can be used in our algorithm. This is the same with Moses (Koehn et al., 2007). Figure 1 shows a training example consisting of a (romanized) Chinese sentence, an English dependency tree, and the word alignment between them. Following Shen et al. (2008), string-todependency rules without non-terminals can be extracted from the training example. As shown in Figure 1, each rule is composed of a source phrase an"
P13-1001,2006.amta-papers.25,0,0.0607103,"Missing"
P13-1001,P06-1098,0,0.119746,"tenation problem and permits arbitrary permutation, it proves to be NP-complete (Knight, 1999). Syntax-based approaches, on the other hand, model the hierarchical structure of natural languages (Wu, 1997; Yamada and Knight, 2001; Chiang, 2005; Quirk et al., 2005; Galley et al., As a result, incremental decoding with hierarchical structures has attracted increasing attention in recent years. While some authors try to integrate syntax into phrase-based decoding (Galley and Manning, 2008; Galley and Manning, 2009; Feng et al., 2010), others develop incremental algorithms for syntax-based models (Watanabe et al., 2006; Huang and Mi, 2010; Dyer and Resnik, 2010; Feng et al., 2012). Despite these successful efforts, challenges still remain for both directions. While parsing algorithms can be used to parse partial translations in phrase-based decoding, the search space is significantly enlarged since there are exponentially many parse trees for exponentially many translations. On the other hand, although target words can be generated left-to-right by altering the way of tree transversal in syntaxbased models, it is still difficult to reach full rule coverage as compared with phrase table. 1 Proceedings of the"
P13-1001,J97-3002,0,0.0876557,"l at handling local word selection and reordering. In addition, it is straightforward to integrate n-gram language models into phrase-based decoders in which translation always grows left-to-right. As a result, phrase-based decoders only need to maintain the boundary words on one end to calculate language model probabilities. However, as phrase-based decoding usually casts translation as a string concatenation problem and permits arbitrary permutation, it proves to be NP-complete (Knight, 1999). Syntax-based approaches, on the other hand, model the hierarchical structure of natural languages (Wu, 1997; Yamada and Knight, 2001; Chiang, 2005; Quirk et al., 2005; Galley et al., As a result, incremental decoding with hierarchical structures has attracted increasing attention in recent years. While some authors try to integrate syntax into phrase-based decoding (Galley and Manning, 2008; Galley and Manning, 2009; Feng et al., 2010), others develop incremental algorithms for syntax-based models (Watanabe et al., 2006; Huang and Mi, 2010; Dyer and Resnik, 2010; Feng et al., 2012). Despite these successful efforts, challenges still remain for both directions. While parsing algorithms can be used t"
P13-1001,D08-1089,0,\N,Missing
P13-1001,P09-1087,0,\N,Missing
P13-1001,D12-1109,1,\N,Missing
P13-1084,D08-1043,0,0.123824,"ions are translated. (2) Multiple words that have similar meanings in one language may be translated into an unique word or a few words in a foreign language. For example in Table 1, English words such as “company” and “firm” are translated into “公司 (gōngsī)”, “rheum” and “catarrh” are translated into “感 冒(gǎnmào)” in Chinese. Thus, word mismatch problem can be somewhat alleviated by using other languages. question may not be easily distinguished from an irrelevant one. Researchers have proposed the use of wordbased translation models (Berger et al., 2000; Jeon et al., 2005; Xue et al., 2008; Lee et al., 2008; Bernhard and Gurevych, 2009) to solve the word mismatch problem. As a principle approach to capture semantic word relations, wordbased translation models are built by using the IBM model 1 (Brown et al., 1993) and have been shown to outperform traditional models (e.g., VSM, BM25, LM) for question retrieval. Besides, Riezler et al. (2007) and Zhou et al. (2011) proposed the phrase-based translation models for question and answer retrieval. The basic idea is to capture the contextual information in modeling the translation of phrases as a whole, thus the word ambiguity problem is somewhat alle"
P13-1084,P07-1059,0,0.0826258,"ord mismatch problem can be somewhat alleviated by using other languages. question may not be easily distinguished from an irrelevant one. Researchers have proposed the use of wordbased translation models (Berger et al., 2000; Jeon et al., 2005; Xue et al., 2008; Lee et al., 2008; Bernhard and Gurevych, 2009) to solve the word mismatch problem. As a principle approach to capture semantic word relations, wordbased translation models are built by using the IBM model 1 (Brown et al., 1993) and have been shown to outperform traditional models (e.g., VSM, BM25, LM) for question retrieval. Besides, Riezler et al. (2007) and Zhou et al. (2011) proposed the phrase-based translation models for question and answer retrieval. The basic idea is to capture the contextual information in modeling the translation of phrases as a whole, thus the word ambiguity problem is somewhat alleviated. However, all these existing studies in the literature are basically monolingual approaches which are restricted to the use of original language of questions. While useful, the effectiveness of these models is highly dependent on the availability of quality parallel monolingual corpora (e.g., question-answer pairs) in the absence of"
P13-1084,P09-1082,0,0.100382,"d. (2) Multiple words that have similar meanings in one language may be translated into an unique word or a few words in a foreign language. For example in Table 1, English words such as “company” and “firm” are translated into “公司 (gōngsī)”, “rheum” and “catarrh” are translated into “感 冒(gǎnmào)” in Chinese. Thus, word mismatch problem can be somewhat alleviated by using other languages. question may not be easily distinguished from an irrelevant one. Researchers have proposed the use of wordbased translation models (Berger et al., 2000; Jeon et al., 2005; Xue et al., 2008; Lee et al., 2008; Bernhard and Gurevych, 2009) to solve the word mismatch problem. As a principle approach to capture semantic word relations, wordbased translation models are built by using the IBM model 1 (Brown et al., 1993) and have been shown to outperform traditional models (e.g., VSM, BM25, LM) for question retrieval. Besides, Riezler et al. (2007) and Zhou et al. (2011) proposed the phrase-based translation models for question and answer retrieval. The basic idea is to capture the contextual information in modeling the translation of phrases as a whole, thus the word ambiguity problem is somewhat alleviated. However, all these exi"
P13-1084,D12-1116,0,0.140189,"vious work. Row 3 is the word-based translation model (Jeon et al., 2005), and row 4 is the wordbased translation language model, which linearly combines the word-based translation model and language model into a unified framework (Xue et al., 2008). Row 5 is the phrase-based translation model, which translates a sequence of words as whole (Zhou et al., 2011). Row 6 is the entitybased translation model, which extends the wordbased translation model and explores strategies to learn the translation probabilities between words and the concepts using the CQA archives and a popular entity catalog (Singh, 2012). Row 7 is the bilingual translation model, which translates the English questions from Yahoo! Answers into Chinese questions using Google Translate and expands the English words with the translated Chinese words (Zhou et al., 2012). For these previous work, we use the same parameter settings in the original papers. Row 8 and row 9 are our proposed method, which leverages statistical machine translation to improve question retrieval via matrix factorization. In row 8, we only consider two languages (English and Chinese) and translate English questions into Chinese using Google Translate in ord"
P13-1084,P10-1125,0,0.026787,"Missing"
P13-1084,P11-1066,1,0.94206,"e somewhat alleviated by using other languages. question may not be easily distinguished from an irrelevant one. Researchers have proposed the use of wordbased translation models (Berger et al., 2000; Jeon et al., 2005; Xue et al., 2008; Lee et al., 2008; Bernhard and Gurevych, 2009) to solve the word mismatch problem. As a principle approach to capture semantic word relations, wordbased translation models are built by using the IBM model 1 (Brown et al., 1993) and have been shown to outperform traditional models (e.g., VSM, BM25, LM) for question retrieval. Besides, Riezler et al. (2007) and Zhou et al. (2011) proposed the phrase-based translation models for question and answer retrieval. The basic idea is to capture the contextual information in modeling the translation of phrases as a whole, thus the word ambiguity problem is somewhat alleviated. However, all these existing studies in the literature are basically monolingual approaches which are restricted to the use of original language of questions. While useful, the effectiveness of these models is highly dependent on the availability of quality parallel monolingual corpora (e.g., question-answer pairs) in the absence of which they are trouble"
P13-1084,C12-1193,1,0.242258,"ismatch problems by taking advantage of potentially rich semantic information drawn from other languages. Through other languages, various ways of adding semantic information to a question could be available, thereby leading to potentially more improvements than using the original language only. Taking a step toward using other languages, we propose the use of translated representation by alternatively enriching the original questions with the words from other languages. The idea of improving question retrieval with statistical machine translation is based on the following two observaAlthough Zhou et al. (2012) exploited bilingual translation for question retrieval and obtained the better performance than traditional monolingual translation models. However, there are two problems with this enrichment: (1) enriching the original questions with the translated words from other languages increases the dimensionality and makes the question representation even more sparse; (2) statistical machine translation may introduce noise, which can harm the performance of question retrieval. To solve these two problems, we propose to leverage statistical machine translation to improve question retrieval via matrix"
P13-1084,J93-2003,0,\N,Missing
P13-1099,C12-1056,0,0.0466913,"ion task. Maximum marginal relevance (MMR) (Carbonell and Goldstein, 1998) uses a greedy algorithm to find summary sentences. (McDonald, 2007) improved the MMR algorithm to dynamic programming. They used a modified objective function in order to consider whether the selected sentence is globally optimal. Sentencelevel ILP was also first introduced in (McDonald, 2007), but (Gillick and Favre, 2009) revised it to concept-based ILP. (Woodsend and Lapata, 2012) utilized ILP to jointly optimize different aspects including content selection, surface realization, and rewrite rules in summarization. (Galanis et al., 2012) uses ILP to jointly maximize the importance of the sentences and their diversity in the summary. (Berg-Kirkpatrick et al., 2011) applied a similar idea to conduct the sentence compression and extraction for multiple document summarization. (Jin et al., 2010) made a comparative study on sentence/concept selection and pairwise and list ranking algorithms, and concluded ILP performed better than MMR and the diversity penalty strategy in sentence/concept selection. Other global optimization methods include submodularity (Lin and Bilmes, 2010) and graph-based approaches (Erkan and Radev, 2004; Les"
P13-1099,W09-1802,0,0.829851,"gram-based ILP for Extractive Summarization Chen Li, Xian Qian, and Yang Liu The University of Texas at Dallas Computer Science Department chenli,qx,yangl@hlt.utdallas.edu Abstract tion. Their system achieved the best result in the TAC 09 summarization task based on the ROUGE evaluation metric. In this approach the goal is to maximize the sum of the weights of the language concepts that appear in the summary. They used bigrams as such language concepts. The association between the language concepts and sentences serves as the constraints. This ILP method is formally represented as below (see (Gillick and Favre, 2009) for more details): In this paper, we propose a bigram based supervised method for extractive document summarization in the integer linear programming (ILP) framework. For each bigram, a regression model is used to estimate its frequency in the reference summary. The regression model uses a variety of indicative features and is trained discriminatively to minimize the distance between the estimated and the ground truth bigram frequency in the reference summary. During testing, the sentence selection problem is formulated as an ILP problem to maximize the bigram gains. We demonstrate that our s"
P13-1099,R09-1002,0,0.0428398,"bigram b in reference summaries. Finally, we replace Nb,ref in Formula (15) with Eq (14) and get the objective function below: P ′ X i,b =b exp{w f (bi )} e (16) max Nb,ref log Pi ′ j exp{w f (bj )} b This shares the same form as the contrastive estimation proposed by (Smith and Eisner, 2005). We use gradient decent method for parameter estimation, initial w is set with zero. 2.3 Features Each bigram is represented using a set of features in the above regression model. We use two types of features: word level and sentence level features. Some of these features have been used in previous work (Aker and Gaizauskas, 2009; Brandow et al., 1995; Edmundson, 1969; Radev, 2001): – 10. Paragraph starter: Binary feature indicating whether this sentence is the beginning of a paragraph. 3 Experiments 3.1 Data We evaluate our method using several recent TAC data sets, from 2008 to 2011. The TAC summarization task is to generate at most 100 words summaries from 10 documents for a given topic query (with a title and more detailed description). For model training, we also included two years’ DUC data (2006 and 2007). When evaluating on one TAC data set, we use the other years of the TAC data plus the two DUC data sets as"
P13-1099,D10-1047,0,0.369937,"LSA) to produce term weights and selected summary sentences by computing an approximate solution to the Budgeted Maximal Coverage problem. 6 Conclusion and Future Work In contrast to these unsupervised approaches, there are also various efforts on supervised learning for summarization where a model is trained to predict whether a sentence is in the summary or not. Different features and classifiers have been explored for this task, such as Bayesian method (Kupiec et al., 1995), maximum entropy (Osborne, 2002), CRF (Galley, 2006), and recently reinforcement learning (Ryang and Abekawa, 2012). (Aker et al., 2010) used discriminative reranking on multiple candidates generated by A* search. Recently, research has also been performed to address some issues in the supervised setup, such as the class In this paper, we leverage the ILP method as a core component in our summarization system. Different from the previous ILP summarization approach, we propose a supervised learning method (a discriminatively trained regression model) to determine the importance of the bigrams fed to the ILP module. In addition, we revise the ILP to maximize the bigram gain (which is expected to be highly correlated with ROUGE-2"
P13-1099,P11-1049,0,0.132676,"nces. (McDonald, 2007) improved the MMR algorithm to dynamic programming. They used a modified objective function in order to consider whether the selected sentence is globally optimal. Sentencelevel ILP was also first introduced in (McDonald, 2007), but (Gillick and Favre, 2009) revised it to concept-based ILP. (Woodsend and Lapata, 2012) utilized ILP to jointly optimize different aspects including content selection, surface realization, and rewrite rules in summarization. (Galanis et al., 2012) uses ILP to jointly maximize the importance of the sentences and their diversity in the summary. (Berg-Kirkpatrick et al., 2011) applied a similar idea to conduct the sentence compression and extraction for multiple document summarization. (Jin et al., 2010) made a comparative study on sentence/concept selection and pairwise and list ranking algorithms, and concluded ILP performed better than MMR and the diversity penalty strategy in sentence/concept selection. Other global optimization methods include submodularity (Lin and Bilmes, 2010) and graph-based approaches (Erkan and Radev, 2004; Leskovec et al., 2005; Mihalcea and Tarau, 2004). Various unsupervised probabilistic topic models have also been investigated for su"
P13-1099,P11-1050,0,0.0951625,"Missing"
P13-1099,C10-2060,0,0.0132945,"the selected sentence is globally optimal. Sentencelevel ILP was also first introduced in (McDonald, 2007), but (Gillick and Favre, 2009) revised it to concept-based ILP. (Woodsend and Lapata, 2012) utilized ILP to jointly optimize different aspects including content selection, surface realization, and rewrite rules in summarization. (Galanis et al., 2012) uses ILP to jointly maximize the importance of the sentences and their diversity in the summary. (Berg-Kirkpatrick et al., 2011) applied a similar idea to conduct the sentence compression and extraction for multiple document summarization. (Jin et al., 2010) made a comparative study on sentence/concept selection and pairwise and list ranking algorithms, and concluded ILP performed better than MMR and the diversity penalty strategy in sentence/concept selection. Other global optimization methods include submodularity (Lin and Bilmes, 2010) and graph-based approaches (Erkan and Radev, 2004; Leskovec et al., 2005; Mihalcea and Tarau, 2004). Various unsupervised probabilistic topic models have also been investigated for summarization and shown promising. For example, (Celikyilmaz and Hakkani-T¨ur, 2011) used it to model the hidden abstract concepts a"
P13-1099,N10-1134,0,0.198027,"face realization, and rewrite rules in summarization. (Galanis et al., 2012) uses ILP to jointly maximize the importance of the sentences and their diversity in the summary. (Berg-Kirkpatrick et al., 2011) applied a similar idea to conduct the sentence compression and extraction for multiple document summarization. (Jin et al., 2010) made a comparative study on sentence/concept selection and pairwise and list ranking algorithms, and concluded ILP performed better than MMR and the diversity penalty strategy in sentence/concept selection. Other global optimization methods include submodularity (Lin and Bilmes, 2010) and graph-based approaches (Erkan and Radev, 2004; Leskovec et al., 2005; Mihalcea and Tarau, 2004). Various unsupervised probabilistic topic models have also been investigated for summarization and shown promising. For example, (Celikyilmaz and Hakkani-T¨ur, 2011) used it to model the hidden abstract concepts across documents as well as the correlation between these concepts to generate topically coherent and non-redundant summaries. (Darling and Song, 2011) applied it to separate the semantically important words from the lowcontent function words. data imbalance problem (Xie and Liu, 2010)."
P13-1099,W04-1013,0,0.0839556,"hich are selected from a subset of the sentences, and their document frequency as the weight in the objective function. In this paper, we propose to find a candidate summary such that the language concepts (e.g., bigrams) in this candidate summary and the reference summary can have the same frequency. We expect this restriction is more consistent with the 1004 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1004–1013, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics ROUGE evaluation metric used for summarization (Lin, 2004). In addition, in the previous conceptbased ILP method, the constraints are with respect to the appearance of language concepts, hence it cannot distinguish the importance of different language concepts in the reference summary. Our method can decide not only which language concepts to use in ILP, but also the frequency of these language concepts in the candidate summary. To estimate the bigram frequency in the summary, we propose to use a supervised regression model that is discriminatively trained using a variety of features. Our experiments on several TAC summarization data sets demonstrate"
P13-1099,W04-3252,0,0.0126852,"maximize the importance of the sentences and their diversity in the summary. (Berg-Kirkpatrick et al., 2011) applied a similar idea to conduct the sentence compression and extraction for multiple document summarization. (Jin et al., 2010) made a comparative study on sentence/concept selection and pairwise and list ranking algorithms, and concluded ILP performed better than MMR and the diversity penalty strategy in sentence/concept selection. Other global optimization methods include submodularity (Lin and Bilmes, 2010) and graph-based approaches (Erkan and Radev, 2004; Leskovec et al., 2005; Mihalcea and Tarau, 2004). Various unsupervised probabilistic topic models have also been investigated for summarization and shown promising. For example, (Celikyilmaz and Hakkani-T¨ur, 2011) used it to model the hidden abstract concepts across documents as well as the correlation between these concepts to generate topically coherent and non-redundant summaries. (Darling and Song, 2011) applied it to separate the semantically important words from the lowcontent function words. data imbalance problem (Xie and Liu, 2010). In this paper, we propose to incorporate the supervised method into the concept-based ILP framework"
P13-1099,P11-2113,0,0.0162996,"etter than MMR and the diversity penalty strategy in sentence/concept selection. Other global optimization methods include submodularity (Lin and Bilmes, 2010) and graph-based approaches (Erkan and Radev, 2004; Leskovec et al., 2005; Mihalcea and Tarau, 2004). Various unsupervised probabilistic topic models have also been investigated for summarization and shown promising. For example, (Celikyilmaz and Hakkani-T¨ur, 2011) used it to model the hidden abstract concepts across documents as well as the correlation between these concepts to generate topically coherent and non-redundant summaries. (Darling and Song, 2011) applied it to separate the semantically important words from the lowcontent function words. data imbalance problem (Xie and Liu, 2010). In this paper, we propose to incorporate the supervised method into the concept-based ILP framework. Unlike previous work using sentencebased supervised learning, we use a regression model to estimate the bigrams and their weights, and use these to guide sentence selection. Compared to the direct sentence-based classification or regression methods mentioned above, our method has an advantage. When abstractive summaries are given, one needs to use that informa"
P13-1099,W02-0401,0,0.20145,"er study closely related to ours is (Davis et al., 2012), which leveraged Latent Semantic Analysis (LSA) to produce term weights and selected summary sentences by computing an approximate solution to the Budgeted Maximal Coverage problem. 6 Conclusion and Future Work In contrast to these unsupervised approaches, there are also various efforts on supervised learning for summarization where a model is trained to predict whether a sentence is in the summary or not. Different features and classifiers have been explored for this task, such as Bayesian method (Kupiec et al., 1995), maximum entropy (Osborne, 2002), CRF (Galley, 2006), and recently reinforcement learning (Ryang and Abekawa, 2012). (Aker et al., 2010) used discriminative reranking on multiple candidates generated by A* search. Recently, research has also been performed to address some issues in the supervised setup, such as the class In this paper, we leverage the ILP method as a core component in our summarization system. Different from the previous ILP summarization approach, we propose a supervised learning method (a discriminatively trained regression model) to determine the importance of the bigrams fed to the ILP module. In additio"
P13-1099,D12-1024,0,0.0319853,"Latent Semantic Analysis (LSA) to produce term weights and selected summary sentences by computing an approximate solution to the Budgeted Maximal Coverage problem. 6 Conclusion and Future Work In contrast to these unsupervised approaches, there are also various efforts on supervised learning for summarization where a model is trained to predict whether a sentence is in the summary or not. Different features and classifiers have been explored for this task, such as Bayesian method (Kupiec et al., 1995), maximum entropy (Osborne, 2002), CRF (Galley, 2006), and recently reinforcement learning (Ryang and Abekawa, 2012). (Aker et al., 2010) used discriminative reranking on multiple candidates generated by A* search. Recently, research has also been performed to address some issues in the supervised setup, such as the class In this paper, we leverage the ILP method as a core component in our summarization system. Different from the previous ILP summarization approach, we propose a supervised learning method (a discriminatively trained regression model) to determine the importance of the bigrams fed to the ILP module. In addition, we revise the ILP to maximize the bigram gain (which is expected to be highly co"
P13-1099,P05-1044,0,0.0368922,"number of the selected sentences. – 7. Sentence similarity: Sentence similarity with topic’s query, which is the concatenation of topic title and description. – 8. Sentence position: Sentence position in the document. – 9. Sentence length: The number of words in the sentence. b eb,ref is the true normalized frequency of where N bigram b in reference summaries. Finally, we replace Nb,ref in Formula (15) with Eq (14) and get the objective function below: P ′ X i,b =b exp{w f (bi )} e (16) max Nb,ref log Pi ′ j exp{w f (bj )} b This shares the same form as the contrastive estimation proposed by (Smith and Eisner, 2005). We use gradient decent method for parameter estimation, initial w is set with zero. 2.3 Features Each bigram is represented using a set of features in the above regression model. We use two types of features: word level and sentence level features. Some of these features have been used in previous work (Aker and Gaizauskas, 2009; Brandow et al., 1995; Edmundson, 1969; Radev, 2001): – 10. Paragraph starter: Binary feature indicating whether this sentence is the beginning of a paragraph. 3 Experiments 3.1 Data We evaluate our method using several recent TAC data sets, from 2008 to 2011. The TA"
P13-1099,D12-1022,0,0.427964,"vised methods have been widely used. In particular, recently several optimization approaches have demonstrated 1010 competitive performance for extractive summarization task. Maximum marginal relevance (MMR) (Carbonell and Goldstein, 1998) uses a greedy algorithm to find summary sentences. (McDonald, 2007) improved the MMR algorithm to dynamic programming. They used a modified objective function in order to consider whether the selected sentence is globally optimal. Sentencelevel ILP was also first introduced in (McDonald, 2007), but (Gillick and Favre, 2009) revised it to concept-based ILP. (Woodsend and Lapata, 2012) utilized ILP to jointly optimize different aspects including content selection, surface realization, and rewrite rules in summarization. (Galanis et al., 2012) uses ILP to jointly maximize the importance of the sentences and their diversity in the summary. (Berg-Kirkpatrick et al., 2011) applied a similar idea to conduct the sentence compression and extraction for multiple document summarization. (Jin et al., 2010) made a comparative study on sentence/concept selection and pairwise and list ranking algorithms, and concluded ILP performed better than MMR and the diversity penalty strategy in s"
P13-1099,W06-1643,0,\N,Missing
P14-2054,P11-1049,0,0.146497,"near Programming (ILP) for inference which requires exponential running time in the worst case. 1 Introduction Sentence compression aims to shorten a sentence by removing uninformative words to reduce reading time. It has been widely used in compressive summarization (Liu and Liu, 2009; Li et al., 2013; Martins and Smith, 2009; Chali and Hasan, 2012; Qian and Liu, 2013). To make the compressed sentence readable, some techniques consider the n-gram language models of the compressed sentence (Clarke and Lapata, 2008; McDonald, 2006). Recent studies used a subtree deletion model for compression (Berg-Kirkpatrick et al., 2011; Morita et al., 2013; Qian and Liu, 2013), which deletes a word only if its modifier in the parse tree is deleted. Despite its empirical success, such a model fails to generate compressions that are not subject to the subtree constraint (see Figure 1). In fact, we parsed the Edinburgh sentence compression corpus using the MSTparser1 , 1 In this paper, we propose a new exact decoding algorithm for the joint model using dynamic programming. Our method extends Eisner’s cubic time parsing algorithm by adding signatures to each span, which indicate the number of deleted words and the rightmost kep"
P14-2054,W13-3508,0,0.0817817,"stems. The first is linear chain CRFs, where the compression task is casted as a binary sequence labeling problem. It usually achieves high unigram F1 score but low grammatical relation F1 score since it only considers the local interdependence between adjacent words. The second is the subtree deletion model (Berg-Kirkpatrick et al., 2011) which is solved by integer linear programming (ILP)2 . The third one is the bigram model proposed by McDonald (McDonald, 2006) which adopts dynamic programming for efficient inference. The last one jointly infers tree structures alongside bigrams using ILP (Thadani and McKeown, 2013). For fair comparison, systems were restricted to produce compressions that matched their average gold compression rate if possible. • Case 4’ Merge an incomplete span and a complete span. The incomplete span is covered by a non-virtual arc. For left-headed spans, the rule is Cji (k, q) = Iri (k ′ , p) + Cjr (k ′′ , q), k ′ + k ′′ = k, and the score of the new span is the sum of the two spanbgr s plus wpr ; for right-headed spans, the rule is Cji (k, i) = Iri (k ′ , i) + Cjr (k ′′ , r), and the score of the new span is the sum of the two spans. The modified algorithm requires O(n6 ) running ti"
P14-2054,briscoe-carroll-2002-robust,0,0.0551896,"t these parse trees by adding virtual arcs and get the full parse trees of their corresponding original sentences. In this way, the annoation is transformed into a set of sentences with their augmented parse trees. The learning task is similar to training a parser. We run a CRF based POS tagger to generate POS related features. We adopt the compression evaluation metric as used in (Martins and Smith, 2009) that measures the macro F-measure for the retained unigrams (Fugr ), and the one used in (Clarke and Lapata, 2008) that calculates the F1 score of the grammatical relations labeled by RASP (Briscoe and Carroll, 2002). We compare our method with other 4 state-ofthe-art systems. The first is linear chain CRFs, where the compression task is casted as a binary sequence labeling problem. It usually achieves high unigram F1 score but low grammatical relation F1 score since it only considers the local interdependence between adjacent words. The second is the subtree deletion model (Berg-Kirkpatrick et al., 2011) which is solved by integer linear programming (ILP)2 . The third one is the bigram model proposed by McDonald (McDonald, 2006) which adopts dynamic programming for efficient inference. The last one joint"
P14-2054,C12-1029,0,0.0184899,"3) proposed the joint compression model, which simultaneously considers the ngram model and dependency parse tree of the compressed sentence. However, the time complexity greatly increases since the parse tree dynamically depends on the compression. They used Integer Linear Programming (ILP) for inference which requires exponential running time in the worst case. 1 Introduction Sentence compression aims to shorten a sentence by removing uninformative words to reduce reading time. It has been widely used in compressive summarization (Liu and Liu, 2009; Li et al., 2013; Martins and Smith, 2009; Chali and Hasan, 2012; Qian and Liu, 2013). To make the compressed sentence readable, some techniques consider the n-gram language models of the compressed sentence (Clarke and Lapata, 2008; McDonald, 2006). Recent studies used a subtree deletion model for compression (Berg-Kirkpatrick et al., 2011; Morita et al., 2013; Qian and Liu, 2013), which deletes a word only if its modifier in the parse tree is deleted. Despite its empirical success, such a model fails to generate compressions that are not subject to the subtree constraint (see Figure 1). In fact, we parsed the Edinburgh sentence compression corpus using t"
P14-2054,C96-1058,0,0.385156,"x1 ROOT Warren says the economy continues the steady improvement Figure 3: Connect deleted words using virtual arcs. Figure 2: Graph illustration for the objective function. In this example, words x2 , xi , xi+1 , xj are kept, others are deleted. The value of the obtok + w tok + jective function is w2tok + witok + wi+1 j dep dep dep dep bgr bgr 3 Proposed Method 3.1 Eisner’s Cubic Time Parsing Algorithm bgr w0i +wi2 +wii+1 +wij +w2i +wii+1 +wi+1j . Throughout the paper, we assume that all the parse trees are projective. Our method is a generalization of Eisner’s dynamic programming algorithm (Eisner, 1996), where two types of structures are used in each iteration, incomplete spans and complete spans. A span is a subtree over a number of consecutive words, with the leftmost or the rightmost word as its root. An incomplete span denoted as Iji is a subtree inside a single arc xi → xj , with root xi . A complete span is denoted as Cji , where xi is the root of the subtree, and xj is the furthest descendant of xi . Eisner’s algorithm searches the optimal tree in a bottom up order. In each step, it merges two adjacent spans into a larger one. There are two rules for merging spans: one merges two comp"
P14-2054,P12-2069,0,0.0591656,"Missing"
P14-2054,D13-1047,1,0.843852,"nd Lapalme, 2012; Thadani and McKeown, 2013) proposed the joint compression model, which simultaneously considers the ngram model and dependency parse tree of the compressed sentence. However, the time complexity greatly increases since the parse tree dynamically depends on the compression. They used Integer Linear Programming (ILP) for inference which requires exponential running time in the worst case. 1 Introduction Sentence compression aims to shorten a sentence by removing uninformative words to reduce reading time. It has been widely used in compressive summarization (Liu and Liu, 2009; Li et al., 2013; Martins and Smith, 2009; Chali and Hasan, 2012; Qian and Liu, 2013). To make the compressed sentence readable, some techniques consider the n-gram language models of the compressed sentence (Clarke and Lapata, 2008; McDonald, 2006). Recent studies used a subtree deletion model for compression (Berg-Kirkpatrick et al., 2011; Morita et al., 2013; Qian and Liu, 2013), which deletes a word only if its modifier in the parse tree is deleted. Despite its empirical success, such a model fails to generate compressions that are not subject to the subtree constraint (see Figure 1). In fact, we parsed t"
P14-2054,P09-2066,1,0.83324,"smatches. (Genest and Lapalme, 2012; Thadani and McKeown, 2013) proposed the joint compression model, which simultaneously considers the ngram model and dependency parse tree of the compressed sentence. However, the time complexity greatly increases since the parse tree dynamically depends on the compression. They used Integer Linear Programming (ILP) for inference which requires exponential running time in the worst case. 1 Introduction Sentence compression aims to shorten a sentence by removing uninformative words to reduce reading time. It has been widely used in compressive summarization (Liu and Liu, 2009; Li et al., 2013; Martins and Smith, 2009; Chali and Hasan, 2012; Qian and Liu, 2013). To make the compressed sentence readable, some techniques consider the n-gram language models of the compressed sentence (Clarke and Lapata, 2008; McDonald, 2006). Recent studies used a subtree deletion model for compression (Berg-Kirkpatrick et al., 2011; Morita et al., 2013; Qian and Liu, 2013), which deletes a word only if its modifier in the parse tree is deleted. Despite its empirical success, such a model fails to generate compressions that are not subject to the subtree constraint (see Figure 1). In"
P14-2054,W09-1801,0,0.474631,"Thadani and McKeown, 2013) proposed the joint compression model, which simultaneously considers the ngram model and dependency parse tree of the compressed sentence. However, the time complexity greatly increases since the parse tree dynamically depends on the compression. They used Integer Linear Programming (ILP) for inference which requires exponential running time in the worst case. 1 Introduction Sentence compression aims to shorten a sentence by removing uninformative words to reduce reading time. It has been widely used in compressive summarization (Liu and Liu, 2009; Li et al., 2013; Martins and Smith, 2009; Chali and Hasan, 2012; Qian and Liu, 2013). To make the compressed sentence readable, some techniques consider the n-gram language models of the compressed sentence (Clarke and Lapata, 2008; McDonald, 2006). Recent studies used a subtree deletion model for compression (Berg-Kirkpatrick et al., 2011; Morita et al., 2013; Qian and Liu, 2013), which deletes a word only if its modifier in the parse tree is deleted. Despite its empirical success, such a model fails to generate compressions that are not subject to the subtree constraint (see Figure 1). In fact, we parsed the Edinburgh sentence com"
P14-2054,P05-1012,0,0.204253,"Missing"
P14-2054,E06-1038,0,0.208053,"nce the parse tree dynamically depends on the compression. They used Integer Linear Programming (ILP) for inference which requires exponential running time in the worst case. 1 Introduction Sentence compression aims to shorten a sentence by removing uninformative words to reduce reading time. It has been widely used in compressive summarization (Liu and Liu, 2009; Li et al., 2013; Martins and Smith, 2009; Chali and Hasan, 2012; Qian and Liu, 2013). To make the compressed sentence readable, some techniques consider the n-gram language models of the compressed sentence (Clarke and Lapata, 2008; McDonald, 2006). Recent studies used a subtree deletion model for compression (Berg-Kirkpatrick et al., 2011; Morita et al., 2013; Qian and Liu, 2013), which deletes a word only if its modifier in the parse tree is deleted. Despite its empirical success, such a model fails to generate compressions that are not subject to the subtree constraint (see Figure 1). In fact, we parsed the Edinburgh sentence compression corpus using the MSTparser1 , 1 In this paper, we propose a new exact decoding algorithm for the joint model using dynamic programming. Our method extends Eisner’s cubic time parsing algorithm by add"
P14-2054,P13-1101,0,0.0181446,"rence which requires exponential running time in the worst case. 1 Introduction Sentence compression aims to shorten a sentence by removing uninformative words to reduce reading time. It has been widely used in compressive summarization (Liu and Liu, 2009; Li et al., 2013; Martins and Smith, 2009; Chali and Hasan, 2012; Qian and Liu, 2013). To make the compressed sentence readable, some techniques consider the n-gram language models of the compressed sentence (Clarke and Lapata, 2008; McDonald, 2006). Recent studies used a subtree deletion model for compression (Berg-Kirkpatrick et al., 2011; Morita et al., 2013; Qian and Liu, 2013), which deletes a word only if its modifier in the parse tree is deleted. Despite its empirical success, such a model fails to generate compressions that are not subject to the subtree constraint (see Figure 1). In fact, we parsed the Edinburgh sentence compression corpus using the MSTparser1 , 1 In this paper, we propose a new exact decoding algorithm for the joint model using dynamic programming. Our method extends Eisner’s cubic time parsing algorithm by adding signatures to each span, which indicate the number of deleted words and the rightmost kept word within the spa"
P14-2054,Q13-1002,0,0.0307974,"The second one is an approximation of the first algorithm. It adopts Lagrangian relaxation to eliminate the compression ratio constraint, yielding lower time complexity T O(n4 ). In practice it achieves nearly the same accuracy as the exact one, but is much faster.3 The main assumption of our method is that the dependency parse tree is projective, which is not true for some other languages. In that case, our method is invalid, but (Thadani and McKeown, 2013) still works. In the future, we will study the non-projective cases based on the recent parsing techniques for 1-endpoint-crossing trees (Pitler et al., 2013). Table 1: Feature templates. wi denotes the word form of token xi and ti denotes the POS tag of xi . al., 2005), which were shown to be very effective for dependency parsing. 4.3 Results We show the comparison results in Table 2. As expected, the joint models (ours and TM13) consistently outperform the subtree deletion model, since the joint models do not suffer from the subtree restriction. They also outperform McDonald’s, demonstrating the effectiveness of considering the grammar structure for compression. It is not surprising that CRFs achieve high unigram F scores but low syntactic F scor"
P14-2054,D13-1156,1,0.848649,"ompression model, which simultaneously considers the ngram model and dependency parse tree of the compressed sentence. However, the time complexity greatly increases since the parse tree dynamically depends on the compression. They used Integer Linear Programming (ILP) for inference which requires exponential running time in the worst case. 1 Introduction Sentence compression aims to shorten a sentence by removing uninformative words to reduce reading time. It has been widely used in compressive summarization (Liu and Liu, 2009; Li et al., 2013; Martins and Smith, 2009; Chali and Hasan, 2012; Qian and Liu, 2013). To make the compressed sentence readable, some techniques consider the n-gram language models of the compressed sentence (Clarke and Lapata, 2008; McDonald, 2006). Recent studies used a subtree deletion model for compression (Berg-Kirkpatrick et al., 2011; Morita et al., 2013; Qian and Liu, 2013), which deletes a word only if its modifier in the parse tree is deleted. Despite its empirical success, such a model fails to generate compressions that are not subject to the subtree constraint (see Figure 1). In fact, we parsed the Edinburgh sentence compression corpus using the MSTparser1 , 1 In"
P14-2054,D10-1001,0,0.0352919,"The relaxed version of Problem (1) is ∑ ∑ dep min max witok zi + wij zi zj yij (2) λ z,y i + ∑ i&lt;j i,j bgr wij zi zj ∏ (1 − zk ) i&lt;k&lt;j ∑ +λ( zi − L) s.t. i z is binary y is a projective parse tree over the subgraph: {xi |zi = 1} Fixing λ, the optimal z, y can be found using a simpler version of the algorithm above. We drop the signature of the virtual arc number from each span, and thus obtain an O(n4 ) time algorithm. Space complexity is O(n3 ). Fixing z, y, the dual variable is updated by λ = λ + α(L − ∑ zi ) i where α &gt; 0 is the learning rate. In this paper, our choice of α is the same as (Rush et al., 2010). 4 Experiments 4.2 Features 4.1 Data and Settings Three types of features are used to learn our model: unigram features, bigram features and dependency features, as shown in Table 1. We also use the in-between features proposed by (McDonald et We evaluate our method on the data set from (Clarke and Lapata, 2008). It includes 82 newswire articles with manually produced compression for each sentence. We use the same partitions as (Martins and Smith, 2009), i.e., 1,188 sentences for training and 441 for testing. 2 We use Gurobi as the ILP solver in the paper. http://www.gurobi.com/ 330 Features"
P14-3012,C12-1097,1,0.915643,"-level. The model labels every character in a standard word as ‘Y’ or ‘N’ to represent whether it appears or not in a possible abbreviation token. The features used for the classification task represent the character’s position, pronunciation and context information. Using the sequence labeling model, a standard word can generate many possible non-standard words. A reverse look-up table is used to store the corresponding possible standard words for the non-standard words for reverse lookup during testing. Liu et al. (2011) extended the above model to handle other types of non-standard words. (Li and Liu, 2012a) used character-blocks (same ones as that in the character-block MT method above) as the units in this sequence labeling framework. There is one list of word candidates from this method. 2 Previous Normalization Methods Used in Reranking In this work we adopt several normalization methods developed in previous studies. The following briefly describes these previous approaches. Next section will introduce our proposed methods using unsupervised learning and discriminative reranking for system combination. 2.1 Character-block level MT Pennell and Liu (2011) proposed to use a character-level MT"
P14-3012,P06-2005,0,0.0548887,"addition, the noisy channel model has also been utilized on the sentence level. Choudhury et al. (2007) used a hidden Markov model to simulate SMS message generation, considering the non-standard tokens in the input sentence as emission states in HMM and labeling results as possible candidates. Cook and Stevenson (2009) extended work by adding several more subsystems in this error model according to the most common non-standard token’s formation process. Machine translation (MT) is another commonly chosen method for text normalization. It is also used on both the token and the sentence level. Aw et al. (2006) treated SMS as another language, and used MT methods to translate this ‘foreign language’ to regular English. Contractor et al. (2010) used an MT model as well but the focus of their work is to utilize an unsupervised method to clean noisy text. Pennell and Liu (2011) firstly introduced an MT method at the token level which translates an unnormalized token to a possible corVarious models have been developed for normalizing informal text. In this paper, we propose two methods to improve normalization performance. First is an unsupervised approach that automatically identifies pairs of a non-st"
P14-3012,P05-1022,0,0.0101642,"quence labeling models have better precision, the two-step MT model has a broader coverage of candidates, and the spell checker has a high confidence for simple non-standard words. Therefore combining these systems is expected to yield better overall results. We propose to use a supervised maximum entropy reranking model to combine our proposed unsupervised method with those described in Section 2 (4 systems that have 5 candidate lists). The features we used in the normalization reranking model are shown in Table 1. This maxent reranking method has shown success in many previous work such as (Charniak and Johnson, 2005; Ji et al., 2006). Features: 1.Boolean value to indicate whether a candidate is on the list of each system. There are 6 lists and thus 6 such features. 2.A concatenation of the 6 boolean features above. 3.The position of this candidate in each candidate list. If this candidate is not on a list, the value of this feature is -1 for that list. 4.The unigram language model probability of the candidate. 5.Boolean value to indicate whether the first character of the candidate and non-standard word is the same. 6.Boolean value to indicate whether the last character of the candidate and non-standard"
P14-3012,P11-2013,1,0.960083,"ional spell checking model, which is usually based on edit distance (Damerau, 1964; Levenshtein, 1966). However, this model can not well handle the nonstandard words in social media text due to the large variation in generating them. Another line of work in normalization adopts a noisy channel model. For a non-standard token A, this method finds the most possible standard word Sˆ based on the Bayes rule: Sˆ = argmaxP (S|A) = argmaxP (A|S) ∗ P (S). Different methods have been used to compute P (A|S). Pennell and Liu (2010) used a CRF sequence modeling approach for deletion-based abbreviations. Liu et al. (2011) further extended this work by considering more types of non-standard words without explicit pre-categorization for nonstandard tokens. In addition, the noisy channel model has also been utilized on the sentence level. Choudhury et al. (2007) used a hidden Markov model to simulate SMS message generation, considering the non-standard tokens in the input sentence as emission states in HMM and labeling results as possible candidates. Cook and Stevenson (2009) extended work by adding several more subsystems in this error model according to the most common non-standard token’s formation process. Ma"
P14-3012,W02-1001,0,0.00634159,"al features in the reranking model. We also tried some other lexical features such as the length difference of the non-standard word and the candidate, whether non-standard word contains numbers, etc. But they did not obtain performance gain. Another advantage of the reranker is that we can use information about multiple systems, such as the first three features. 3.2.2 weights), we perform sentence level Viterbi decoding on the training set to find the best hypothesis for each non-standard word. If the hypothesis is incorrect, we update the feature weight using structured perceptron strategy (Collins, 2002). We will explore these different feature and training configurations for reranking in the following experiments. 4 Experiments 4.1 Experimental Setup The following data sets are used in our experiments. We use Data 1 and Data 2 as test data, and Data 3 as training data for all the supervised models. • Data 1: 558 pairs of non-standard tokens and standard words collected from 549 tweets in 2010 by (Han and Baldwin, 2011). Sentence Level Reranking and Decoding In the above reranking method, we only use information about the individual words. When contextual words are available (in sentences or"
P14-3012,P12-1109,0,0.656185,"ts from different systems. This allows us to incorporate information that is hard to model in individual systems as well as consider multiple systems to generate a final rank for a test case. Both word- and sentence-level optimization schemes are explored in this study. We evaluate our approach on data sets used in prior studies, and demonstrate that our proposed methods perform better than the state-of-the-art systems. 1 Introduction There has been a lot of research efforts recently on analysis of social media text (e.g., from Twitter and Facebook) (Ritter et al., 2011; Owoputi et al., 2013; Liu et al., 2012b). One challenge in processing social media text is how to deal with the frequently occurring non-standard words, such as bday (meaning birthday), snd (meaning sound) and gl (meaning girl) . Normalizing informal text (changing non-standard words to standard ones) will ease subsequent language processing modules. Text normalization has been an important topic for the text-to-speech field. See (Sproat et al., 2001) for a good report of this problem. Recently, much research on normalization has been done 86 Proceedings of the ACL 2014 Student Research Workshop, pages 86–93, c Baltimore, Maryland"
P14-3012,C10-2022,0,0.0189352,"del to simulate SMS message generation, considering the non-standard tokens in the input sentence as emission states in HMM and labeling results as possible candidates. Cook and Stevenson (2009) extended work by adding several more subsystems in this error model according to the most common non-standard token’s formation process. Machine translation (MT) is another commonly chosen method for text normalization. It is also used on both the token and the sentence level. Aw et al. (2006) treated SMS as another language, and used MT methods to translate this ‘foreign language’ to regular English. Contractor et al. (2010) used an MT model as well but the focus of their work is to utilize an unsupervised method to clean noisy text. Pennell and Liu (2011) firstly introduced an MT method at the token level which translates an unnormalized token to a possible corVarious models have been developed for normalizing informal text. In this paper, we propose two methods to improve normalization performance. First is an unsupervised approach that automatically identifies pairs of a non-standard token and proper word from a large unlabeled corpus. We use semantic similarity based on continuous word vector representation,"
P14-3012,P12-1055,0,0.177774,"ts from different systems. This allows us to incorporate information that is hard to model in individual systems as well as consider multiple systems to generate a final rank for a test case. Both word- and sentence-level optimization schemes are explored in this study. We evaluate our approach on data sets used in prior studies, and demonstrate that our proposed methods perform better than the state-of-the-art systems. 1 Introduction There has been a lot of research efforts recently on analysis of social media text (e.g., from Twitter and Facebook) (Ritter et al., 2011; Owoputi et al., 2013; Liu et al., 2012b). One challenge in processing social media text is how to deal with the frequently occurring non-standard words, such as bday (meaning birthday), snd (meaning sound) and gl (meaning girl) . Normalizing informal text (changing non-standard words to standard ones) will ease subsequent language processing modules. Text normalization has been an important topic for the text-to-speech field. See (Sproat et al., 2001) for a good report of this problem. Recently, much research on normalization has been done 86 Proceedings of the ACL 2014 Student Research Workshop, pages 86–93, c Baltimore, Maryland"
P14-3012,W09-2010,0,0.0332492,"Different methods have been used to compute P (A|S). Pennell and Liu (2010) used a CRF sequence modeling approach for deletion-based abbreviations. Liu et al. (2011) further extended this work by considering more types of non-standard words without explicit pre-categorization for nonstandard tokens. In addition, the noisy channel model has also been utilized on the sentence level. Choudhury et al. (2007) used a hidden Markov model to simulate SMS message generation, considering the non-standard tokens in the input sentence as emission states in HMM and labeling results as possible candidates. Cook and Stevenson (2009) extended work by adding several more subsystems in this error model according to the most common non-standard token’s formation process. Machine translation (MT) is another commonly chosen method for text normalization. It is also used on both the token and the sentence level. Aw et al. (2006) treated SMS as another language, and used MT methods to translate this ‘foreign language’ to regular English. Contractor et al. (2010) used an MT model as well but the focus of their work is to utilize an unsupervised method to clean noisy text. Pennell and Liu (2011) firstly introduced an MT method at"
P14-3012,J03-1002,0,0.0163151,"Missing"
P14-3012,P11-1038,0,0.206393,"decoding on the training set to find the best hypothesis for each non-standard word. If the hypothesis is incorrect, we update the feature weight using structured perceptron strategy (Collins, 2002). We will explore these different feature and training configurations for reranking in the following experiments. 4 Experiments 4.1 Experimental Setup The following data sets are used in our experiments. We use Data 1 and Data 2 as test data, and Data 3 as training data for all the supervised models. • Data 1: 558 pairs of non-standard tokens and standard words collected from 549 tweets in 2010 by (Han and Baldwin, 2011). Sentence Level Reranking and Decoding In the above reranking method, we only use information about the individual words. When contextual words are available (in sentences or Tweets), we can use that information. If a sentence containing OOV words is given during testing, we can perform standard sentence level Viterbi decoding to combine information from the normalization candidates and language model scores. Furthermore, if sentences are available during training (not just isolated word pairs as used in all the previous supervised individual systems and the Maxent reranking above), we can al"
P14-3012,N13-1039,0,0.0125959,"Missing"
P14-3012,D12-1039,0,0.0303775,"translated to standard words in the second step. This method has been shown to yield high coverage (high accuracy in its n-best hypotheses). There are two candidate lists generated by this two-step MT method. The first one is based on the pronunciation list produced in the first step (some phonetic sequences directly correspond to standard words). The second list is generated from the second translation step. rect word. Recently, a new line of work surges relying on the analysis of huge amount of twitter data, often in an unsupervised fashion. By using context information from a large corpus, Han et al. (2012) generated possible variant and normalization pairs, and constructed a dictionary of lexical variants of known words, which are further ranked by string similarity. This dictionary can facilitate lexical normalization via simple string substitution. Hassan and Menezes (2013) proposed an approach based on the random walk algorithm on a contextual similarity bipartite graph, constructed from n-gram sequences on a large unlabeled text corpus. Yang and Eisenstein (2013) presented a unified unsupervised statistical model for text normalization. 2.3 Character-Block level Sequence Labeling Pennell an"
P14-3012,P13-1155,0,0.142065,"in the first step (some phonetic sequences directly correspond to standard words). The second list is generated from the second translation step. rect word. Recently, a new line of work surges relying on the analysis of huge amount of twitter data, often in an unsupervised fashion. By using context information from a large corpus, Han et al. (2012) generated possible variant and normalization pairs, and constructed a dictionary of lexical variants of known words, which are further ranked by string similarity. This dictionary can facilitate lexical normalization via simple string substitution. Hassan and Menezes (2013) proposed an approach based on the random walk algorithm on a contextual similarity bipartite graph, constructed from n-gram sequences on a large unlabeled text corpus. Yang and Eisenstein (2013) presented a unified unsupervised statistical model for text normalization. 2.3 Character-Block level Sequence Labeling Pennell and Liu (2010) used sequence labeling model (CRF) for normalizing deletion-based abbreviation at the character-level. The model labels every character in a standard word as ‘Y’ or ‘N’ to represent whether it appears or not in a possible abbreviation token. The features used fo"
P14-3012,I11-1109,1,0.92087,"results as possible candidates. Cook and Stevenson (2009) extended work by adding several more subsystems in this error model according to the most common non-standard token’s formation process. Machine translation (MT) is another commonly chosen method for text normalization. It is also used on both the token and the sentence level. Aw et al. (2006) treated SMS as another language, and used MT methods to translate this ‘foreign language’ to regular English. Contractor et al. (2010) used an MT model as well but the focus of their work is to utilize an unsupervised method to clean noisy text. Pennell and Liu (2011) firstly introduced an MT method at the token level which translates an unnormalized token to a possible corVarious models have been developed for normalizing informal text. In this paper, we propose two methods to improve normalization performance. First is an unsupervised approach that automatically identifies pairs of a non-standard token and proper word from a large unlabeled corpus. We use semantic similarity based on continuous word vector representation, together with other surface similarity measurement. Second we propose a reranking strategy to combine the results from different syste"
P14-3012,W06-3607,0,0.0218599,"better precision, the two-step MT model has a broader coverage of candidates, and the spell checker has a high confidence for simple non-standard words. Therefore combining these systems is expected to yield better overall results. We propose to use a supervised maximum entropy reranking model to combine our proposed unsupervised method with those described in Section 2 (4 systems that have 5 candidate lists). The features we used in the normalization reranking model are shown in Table 1. This maxent reranking method has shown success in many previous work such as (Charniak and Johnson, 2005; Ji et al., 2006). Features: 1.Boolean value to indicate whether a candidate is on the list of each system. There are 6 lists and thus 6 such features. 2.A concatenation of the 6 boolean features above. 3.The position of this candidate in each candidate list. If this candidate is not on a list, the value of this feature is -1 for that list. 4.The unigram language model probability of the candidate. 5.Boolean value to indicate whether the first character of the candidate and non-standard word is the same. 6.Boolean value to indicate whether the last character of the candidate and non-standard word is the same."
P14-3012,W10-0513,0,0.0391831,"the difference is that this is an unsupervised method whereas the sequence labeling uses supervised learning to generate possible candidates. Unsupervised Corpus-based Similarity for Normalization Previous work has shown that unlabeled text can be used to induce unsupervised word clusters that can improve performance of many supervised NLP tasks (Koo et al., 2008; Turian et al., 2010; T¨ackstr¨om et al., 2012). We investigate using a large unlabeled Twitter corpus to automatically identify pairs of non-standard words and their corresponding standard words. We use the Edinburgh Twitter corpus (Petrovic et al., 2010), and a dictionary obtained from http://ciba.iciba.com/ to identify all the invocabulary and out-of-vocabulary (OOV) words in the corpus. The task is then to automatically find the corresponding OOV words (if any) for each dictionary word, and the likelihood of each pair. The key question is how to compute this likelihood or similarity. We propose to use an unsupervised method based on the large corpus to induce dense realvalued low-dimension word embedding and then use the inner product as a measure of semantic similarity. We use the continuous bag-of-words model that is similar to the feedfo"
P14-3012,P07-2045,0,0.00256631,"Missing"
P14-3012,D11-1141,0,0.0159035,"e a reranking strategy to combine the results from different systems. This allows us to incorporate information that is hard to model in individual systems as well as consider multiple systems to generate a final rank for a test case. Both word- and sentence-level optimization schemes are explored in this study. We evaluate our approach on data sets used in prior studies, and demonstrate that our proposed methods perform better than the state-of-the-art systems. 1 Introduction There has been a lot of research efforts recently on analysis of social media text (e.g., from Twitter and Facebook) (Ritter et al., 2011; Owoputi et al., 2013; Liu et al., 2012b). One challenge in processing social media text is how to deal with the frequently occurring non-standard words, such as bday (meaning birthday), snd (meaning sound) and gl (meaning girl) . Normalizing informal text (changing non-standard words to standard ones) will ease subsequent language processing modules. Text normalization has been an important topic for the text-to-speech field. See (Sproat et al., 2001) for a good report of this problem. Recently, much research on normalization has been done 86 Proceedings of the ACL 2014 Student Research Work"
P14-3012,P08-1068,0,0.0166074,"ible standard words for each non-standard word, which is used during testing. This framework is similar to the sequence labeling method described in Section 2.3 in the sense of creating the mapping table between the OOV and dictionary words. However, the difference is that this is an unsupervised method whereas the sequence labeling uses supervised learning to generate possible candidates. Unsupervised Corpus-based Similarity for Normalization Previous work has shown that unlabeled text can be used to induce unsupervised word clusters that can improve performance of many supervised NLP tasks (Koo et al., 2008; Turian et al., 2010; T¨ackstr¨om et al., 2012). We investigate using a large unlabeled Twitter corpus to automatically identify pairs of non-standard words and their corresponding standard words. We use the Edinburgh Twitter corpus (Petrovic et al., 2010), and a dictionary obtained from http://ciba.iciba.com/ to identify all the invocabulary and out-of-vocabulary (OOV) words in the corpus. The task is then to automatically find the corresponding OOV words (if any) for each dictionary word, and the likelihood of each pair. The key question is how to compute this likelihood or similarity. We p"
P14-3012,N12-1052,0,0.0205637,"Missing"
P14-3012,P10-1040,0,0.0105736,"s for each non-standard word, which is used during testing. This framework is similar to the sequence labeling method described in Section 2.3 in the sense of creating the mapping table between the OOV and dictionary words. However, the difference is that this is an unsupervised method whereas the sequence labeling uses supervised learning to generate possible candidates. Unsupervised Corpus-based Similarity for Normalization Previous work has shown that unlabeled text can be used to induce unsupervised word clusters that can improve performance of many supervised NLP tasks (Koo et al., 2008; Turian et al., 2010; T¨ackstr¨om et al., 2012). We investigate using a large unlabeled Twitter corpus to automatically identify pairs of non-standard words and their corresponding standard words. We use the Edinburgh Twitter corpus (Petrovic et al., 2010), and a dictionary obtained from http://ciba.iciba.com/ to identify all the invocabulary and out-of-vocabulary (OOV) words in the corpus. The task is then to automatically find the corresponding OOV words (if any) for each dictionary word, and the likelihood of each pair. The key question is how to compute this likelihood or similarity. We propose to use an unsu"
P14-3012,D13-1007,0,0.592257,"elying on the analysis of huge amount of twitter data, often in an unsupervised fashion. By using context information from a large corpus, Han et al. (2012) generated possible variant and normalization pairs, and constructed a dictionary of lexical variants of known words, which are further ranked by string similarity. This dictionary can facilitate lexical normalization via simple string substitution. Hassan and Menezes (2013) proposed an approach based on the random walk algorithm on a contextual similarity bipartite graph, constructed from n-gram sequences on a large unlabeled text corpus. Yang and Eisenstein (2013) presented a unified unsupervised statistical model for text normalization. 2.3 Character-Block level Sequence Labeling Pennell and Liu (2010) used sequence labeling model (CRF) for normalizing deletion-based abbreviation at the character-level. The model labels every character in a standard word as ‘Y’ or ‘N’ to represent whether it appears or not in a possible abbreviation token. The features used for the classification task represent the character’s position, pronunciation and context information. Using the sequence labeling model, a standard word can generate many possible non-standard wor"
P15-1023,W11-1014,0,0.0265826,"Missing"
P15-1023,D07-1007,0,0.24854,"ish translation example to illustrate the effect of local contexts and global topics as well as their correlations on lexical selection. Each black line indicates a set of translation candidates for a Chinese content word (within a dotted box). Green lines point to translations that are favored by local contexts while blue lines show bidirectional associations between global topics and their consistent target-side translations. Previous studies that explore richer information for lexical selection can be divided into two categories: 1) incorporating sentence-level contexts (Chan et al., 2007; Carpuat and Wu, 2007; Hasan et al., 2008; Mauser et al., 2009; He et al., 2008; Shen et al., 2009) or 2) integrating document-level topics (Xiao et al., 2011; Ture et al., 2012; Xiao et al., 2012; Eidelman et al., 2012; Hewavitharana et al., 2013; Xiong et al., 2013; Hasler et al., 2014a; Hasler et al., 2014b) into SMT. The methods in these two strands have shown their effectiveness on lexical selection. However, correlations between sentence- and document-level contexts have never been explored before. It is clear that local contexts and global topics are often highly correlated. Consider a ChineseEnglish transl"
P15-1023,P07-1005,0,0.0775738,"Missing"
P15-1023,P11-2031,0,0.106229,"Missing"
P15-1023,P12-2023,0,0.214218,"r a Chinese content word (within a dotted box). Green lines point to translations that are favored by local contexts while blue lines show bidirectional associations between global topics and their consistent target-side translations. Previous studies that explore richer information for lexical selection can be divided into two categories: 1) incorporating sentence-level contexts (Chan et al., 2007; Carpuat and Wu, 2007; Hasan et al., 2008; Mauser et al., 2009; He et al., 2008; Shen et al., 2009) or 2) integrating document-level topics (Xiao et al., 2011; Ture et al., 2012; Xiao et al., 2012; Eidelman et al., 2012; Hewavitharana et al., 2013; Xiong et al., 2013; Hasler et al., 2014a; Hasler et al., 2014b) into SMT. The methods in these two strands have shown their effectiveness on lexical selection. However, correlations between sentence- and document-level contexts have never been explored before. It is clear that local contexts and global topics are often highly correlated. Consider a ChineseEnglish translation example presented in Figure 1. On the one hand, if local contexts suggest that the source word “á|/l`ıchˇang” should be translated in229 Proceedings of the 53rd Annual Meeting of the Associati"
P15-1023,P06-1121,0,0.055962,"a translation system for lexical selection. Experiment results on NIST ChineseEnglish test sets demonstrate that 1) our model significantly outperforms previous lexical selection methods and 2) modeling correlations between local words and global topics can further improve translation quality. 1 Introduction Lexical selection is a very important task in statistical machine translation (SMT). Given a sentence in the source language, lexical selection statistically predicts translations for source words, based on various translation knowledge. Most conventional SMT systems (Koehn et al., 2003; Galley et al., 2006; Chiang, 2007) exploit very limited context information contained in bilingual rules for lexical selection. ∗ Corresponding author. duì gāi wèntí zhōngguó bǎochí zhōnglì lìchǎng {problem, issue ...}wèntí {stance, attitude ...}lìchǎng [Economy topic, Politics topic ...] Figure 1: A Chinese-English translation example to illustrate the effect of local contexts and global topics as well as their correlations on lexical selection. Each black line indicates a set of translation candidates for a Chinese content word (within a dotted box). Green lines point to translations that are favored by local"
P15-1023,D12-1010,1,0.890599,"Missing"
P15-1023,D08-1039,0,0.104532,"e to illustrate the effect of local contexts and global topics as well as their correlations on lexical selection. Each black line indicates a set of translation candidates for a Chinese content word (within a dotted box). Green lines point to translations that are favored by local contexts while blue lines show bidirectional associations between global topics and their consistent target-side translations. Previous studies that explore richer information for lexical selection can be divided into two categories: 1) incorporating sentence-level contexts (Chan et al., 2007; Carpuat and Wu, 2007; Hasan et al., 2008; Mauser et al., 2009; He et al., 2008; Shen et al., 2009) or 2) integrating document-level topics (Xiao et al., 2011; Ture et al., 2012; Xiao et al., 2012; Eidelman et al., 2012; Hewavitharana et al., 2013; Xiong et al., 2013; Hasler et al., 2014a; Hasler et al., 2014b) into SMT. The methods in these two strands have shown their effectiveness on lexical selection. However, correlations between sentence- and document-level contexts have never been explored before. It is clear that local contexts and global topics are often highly correlated. Consider a ChineseEnglish translation example presen"
P15-1023,E14-1035,0,0.15347,"anslations that are favored by local contexts while blue lines show bidirectional associations between global topics and their consistent target-side translations. Previous studies that explore richer information for lexical selection can be divided into two categories: 1) incorporating sentence-level contexts (Chan et al., 2007; Carpuat and Wu, 2007; Hasan et al., 2008; Mauser et al., 2009; He et al., 2008; Shen et al., 2009) or 2) integrating document-level topics (Xiao et al., 2011; Ture et al., 2012; Xiao et al., 2012; Eidelman et al., 2012; Hewavitharana et al., 2013; Xiong et al., 2013; Hasler et al., 2014a; Hasler et al., 2014b) into SMT. The methods in these two strands have shown their effectiveness on lexical selection. However, correlations between sentence- and document-level contexts have never been explored before. It is clear that local contexts and global topics are often highly correlated. Consider a ChineseEnglish translation example presented in Figure 1. On the one hand, if local contexts suggest that the source word “á|/l`ıchˇang” should be translated in229 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conf"
P15-1023,W14-3358,0,0.182188,"anslations that are favored by local contexts while blue lines show bidirectional associations between global topics and their consistent target-side translations. Previous studies that explore richer information for lexical selection can be divided into two categories: 1) incorporating sentence-level contexts (Chan et al., 2007; Carpuat and Wu, 2007; Hasan et al., 2008; Mauser et al., 2009; He et al., 2008; Shen et al., 2009) or 2) integrating document-level topics (Xiao et al., 2011; Ture et al., 2012; Xiao et al., 2012; Eidelman et al., 2012; Hewavitharana et al., 2013; Xiong et al., 2013; Hasler et al., 2014a; Hasler et al., 2014b) into SMT. The methods in these two strands have shown their effectiveness on lexical selection. However, correlations between sentence- and document-level contexts have never been explored before. It is clear that local contexts and global topics are often highly correlated. Consider a ChineseEnglish translation example presented in Figure 1. On the one hand, if local contexts suggest that the source word “á|/l`ıchˇang” should be translated in229 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conf"
P15-1023,C08-1041,0,0.0994588,"xts and global topics as well as their correlations on lexical selection. Each black line indicates a set of translation candidates for a Chinese content word (within a dotted box). Green lines point to translations that are favored by local contexts while blue lines show bidirectional associations between global topics and their consistent target-side translations. Previous studies that explore richer information for lexical selection can be divided into two categories: 1) incorporating sentence-level contexts (Chan et al., 2007; Carpuat and Wu, 2007; Hasan et al., 2008; Mauser et al., 2009; He et al., 2008; Shen et al., 2009) or 2) integrating document-level topics (Xiao et al., 2011; Ture et al., 2012; Xiao et al., 2012; Eidelman et al., 2012; Hewavitharana et al., 2013; Xiong et al., 2013; Hasler et al., 2014a; Hasler et al., 2014b) into SMT. The methods in these two strands have shown their effectiveness on lexical selection. However, correlations between sentence- and document-level contexts have never been explored before. It is clear that local contexts and global topics are often highly correlated. Consider a ChineseEnglish translation example presented in Figure 1. On the one hand, if l"
P15-1023,N03-1017,0,0.0655323,"l is integrated into a translation system for lexical selection. Experiment results on NIST ChineseEnglish test sets demonstrate that 1) our model significantly outperforms previous lexical selection methods and 2) modeling correlations between local words and global topics can further improve translation quality. 1 Introduction Lexical selection is a very important task in statistical machine translation (SMT). Given a sentence in the source language, lexical selection statistically predicts translations for source words, based on various translation knowledge. Most conventional SMT systems (Koehn et al., 2003; Galley et al., 2006; Chiang, 2007) exploit very limited context information contained in bilingual rules for lexical selection. ∗ Corresponding author. duì gāi wèntí zhōngguó bǎochí zhōnglì lìchǎng {problem, issue ...}wèntí {stance, attitude ...}lìchǎng [Economy topic, Politics topic ...] Figure 1: A Chinese-English translation example to illustrate the effect of local contexts and global topics as well as their correlations on lexical selection. Each black line indicates a set of translation candidates for a Chinese content word (within a dotted box). Green lines point to translations that"
P15-1023,W04-3250,0,0.175547,"baseline system is a state-of-the-art SMT system, which adapts bracketing transduction grammars (Wu, 1997) to phrasal translation and equips itself with a maximum entropy based reordering model (MEBTG) (Xiong et al., 2006). We used the toolkit4 developed by Zhang (2004) to train the reordering model with the following parameters: iteration number iter=200 and Gaussian prior g=1.0. During decoding, we set the ttable-limit as 20, the stack-size as 100. The translation quality is evaluated by case-insensitive BLEU-4 (Papineni et al., 2002) metric. Finally, we conducted paired bootstrap sampling (Koehn, 2004) to test the significance in BLEU score differences. 2 http://people.sutd.edu.sg/∼yue zhang/doc/index.html http://nlp.stanford.edu/software 4 http://homepages.inf.ed.ac.uk/lzhang10/maxenttoolkit.html 3 (6) 233 Model CATM (± 6w) CATM (± 8w) CATM (± 10w) CATM (± 12w) CATM (± 14w) MT05 33.35 33.43 33.42 33.49 33.30 Table 2: Experiment results on the development set using different window sizes ws . To train CATM, we set the topic number Nz as 25.5 For hyperparameters α and β, we empirically set α=50/Nz and β=0.1, as implemented in (Griffiths and Steyvers, 2004). Following Han et al. (2012), we se"
P15-1023,D08-1010,1,0.79717,"inspired by (Han and Sun, 2012), where an entity-topic model is presented for entity linking. We successfully adapt this work to lexical selection in SMT. The related work mainly includes the following two strands. (1) Lexical Selection in SMT. In order to explore rich context information for lexical selection, some researchers propose trigger-based lexicon models to capture long-distance dependencies (Hasan et al., 2008; Mauser et al., 2009), and many more researchers build classifiers to select desirable translations during decoding (Chan et al., 2007; Carpuat and Wu, 2007; He et al., 2008; Liu et al., 2008). Along this line, Shen et al. (2009) introduce four new linguistic and contextual features for translation selection in SMT. Recently, we have witnessed an increasing efforts in exploiting document-level context information to improve lexical selection. Xiao et al. (2011) impose a hard constraint to guarantee 235 Target-side Topical Items UNHCR republic refugee refugee Kosovo federal military missile military United States system war country development economy international economic trade Taiwan China relation cross-strait cross-strait relation issue Topic Source-side Contextual Words J¬(ref"
P15-1023,D09-1022,0,0.124816,"effect of local contexts and global topics as well as their correlations on lexical selection. Each black line indicates a set of translation candidates for a Chinese content word (within a dotted box). Green lines point to translations that are favored by local contexts while blue lines show bidirectional associations between global topics and their consistent target-side translations. Previous studies that explore richer information for lexical selection can be divided into two categories: 1) incorporating sentence-level contexts (Chan et al., 2007; Carpuat and Wu, 2007; Hasan et al., 2008; Mauser et al., 2009; He et al., 2008; Shen et al., 2009) or 2) integrating document-level topics (Xiao et al., 2011; Ture et al., 2012; Xiao et al., 2012; Eidelman et al., 2012; Hewavitharana et al., 2013; Xiong et al., 2013; Hasler et al., 2014a; Hasler et al., 2014b) into SMT. The methods in these two strands have shown their effectiveness on lexical selection. However, correlations between sentence- and document-level contexts have never been explored before. It is clear that local contexts and global topics are often highly correlated. Consider a ChineseEnglish translation example presented in Figure 1. On t"
P15-1023,P02-1038,0,0.0449124,"topical items. Formally, for the position i in the document corresponding to the content word f , we collect the sampled count that translation e˜ generates f , denoted by Csam (˜ e, f ). This count can be normalized to form a new translation probability in the following way: Csam (˜ e, f ) + k p(˜ e|f ) = Csam + k · Ne˜,f where Csam is the total number of samples during inference and Ne˜,f is the number of candidate translations of f . Here we apply add-k smoothing to refine this translation probability, where k is a tunable global smoothing constant. Under the framework of log-linear model (Och and Ney, 2002), we use this translation probability as a new feature to improve lexical selection in SMT. 4 Experiments In order to examine the effectiveness of our model, we carried out several groups of experiments on Chinese-to-English translation. 4.1 Setup Our bilingual training corpus is from the FBIS corpus and the Hansards part of LDC2004T07 corpus (1M parallel sentences, 54.6K documents, with 25.2M Chinese words and 29M English words). We first used ZPar toolkit2 and Stanford toolkit3 to preprocess (i.e., word segmenting, PoS tagging) the Chinese and English parts of training corpus, and then word-"
P15-1023,J03-1002,0,0.00668871,"robability as a new feature to improve lexical selection in SMT. 4 Experiments In order to examine the effectiveness of our model, we carried out several groups of experiments on Chinese-to-English translation. 4.1 Setup Our bilingual training corpus is from the FBIS corpus and the Hansards part of LDC2004T07 corpus (1M parallel sentences, 54.6K documents, with 25.2M Chinese words and 29M English words). We first used ZPar toolkit2 and Stanford toolkit3 to preprocess (i.e., word segmenting, PoS tagging) the Chinese and English parts of training corpus, and then word-aligned them using GIZA++ (Och and Ney, 2003) with the option “grow-diag-final-and”. We chose the NIST evaluation set of MT05 as the development set, and the sets of MT06/MT08 as test sets. On average, these three sets contain 17.2, 13.9 and 14.1 content words per sentence, respectively. We trained a 5-gram language model on the Xinhua portion of Gigaword corpus using the SRILM Toolkit (Stolcke, 2002). Our baseline system is a state-of-the-art SMT system, which adapts bracketing transduction grammars (Wu, 1997) to phrasal translation and equips itself with a maximum entropy based reordering model (MEBTG) (Xiong et al., 2006). We used the"
P15-1023,P03-1021,0,0.0641451,"Missing"
P15-1023,J04-4002,0,0.305645,"Missing"
P15-1023,D09-1008,0,0.126118,"pics as well as their correlations on lexical selection. Each black line indicates a set of translation candidates for a Chinese content word (within a dotted box). Green lines point to translations that are favored by local contexts while blue lines show bidirectional associations between global topics and their consistent target-side translations. Previous studies that explore richer information for lexical selection can be divided into two categories: 1) incorporating sentence-level contexts (Chan et al., 2007; Carpuat and Wu, 2007; Hasan et al., 2008; Mauser et al., 2009; He et al., 2008; Shen et al., 2009) or 2) integrating document-level topics (Xiao et al., 2011; Ture et al., 2012; Xiao et al., 2012; Eidelman et al., 2012; Hewavitharana et al., 2013; Xiong et al., 2013; Hasler et al., 2014a; Hasler et al., 2014b) into SMT. The methods in these two strands have shown their effectiveness on lexical selection. However, correlations between sentence- and document-level contexts have never been explored before. It is clear that local contexts and global topics are often highly correlated. Consider a ChineseEnglish translation example presented in Figure 1. On the one hand, if local contexts sugges"
P15-1023,P12-1048,1,0.783622,"or missile and war, respectively; “ü” and “W” together means cross-starit. the document-level translation consistency. Ture et al. (2012) soften this consistency constraint by integrating three counting features into decoder. Also relevant is the work of Xiong et al.(2013), who use three different models to capture lexical cohesion for document-level SMT. (2) SMT with Topic Models. In this strand, Zhao and Xing (2006, 2007) first present a bilingual topical admixture formalism for word alignment in SMT. Tam et al. (2007) and Ruiz et al. (2012) apply topic model into language model adaptation. Su et al. (2012) conduct translation model adaptation with monolingual topic information. Gong et al. (2010) and Xiao et al. (2012) introduce topic-based similarity models to improve SMT system. Axelrod et al. (2012) build topic-specific translation models from the TED corpus and select topic-relevant data from the UN corpus to improve coverage. Eidelman et al. (2012) incorporate topic-specific lexical weights into translation model. Hewavitharana et al. (2013) propose an incremental topic based translation model adaptation approach that satisfies the causality constraint imposed by spoken conversations. Hasl"
P15-1023,N12-1046,0,0.0474752,"Missing"
P15-1023,J97-3002,0,0.0509132,"ord segmenting, PoS tagging) the Chinese and English parts of training corpus, and then word-aligned them using GIZA++ (Och and Ney, 2003) with the option “grow-diag-final-and”. We chose the NIST evaluation set of MT05 as the development set, and the sets of MT06/MT08 as test sets. On average, these three sets contain 17.2, 13.9 and 14.1 content words per sentence, respectively. We trained a 5-gram language model on the Xinhua portion of Gigaword corpus using the SRILM Toolkit (Stolcke, 2002). Our baseline system is a state-of-the-art SMT system, which adapts bracketing transduction grammars (Wu, 1997) to phrasal translation and equips itself with a maximum entropy based reordering model (MEBTG) (Xiong et al., 2006). We used the toolkit4 developed by Zhang (2004) to train the reordering model with the following parameters: iteration number iter=200 and Gaussian prior g=1.0. During decoding, we set the ttable-limit as 20, the stack-size as 100. The translation quality is evaluated by case-insensitive BLEU-4 (Papineni et al., 2002) metric. Finally, we conducted paired bootstrap sampling (Koehn, 2004) to test the significance in BLEU score differences. 2 http://people.sutd.edu.sg/∼yue zhang/do"
P15-1023,2011.mtsummit-papers.13,0,0.169815,"h black line indicates a set of translation candidates for a Chinese content word (within a dotted box). Green lines point to translations that are favored by local contexts while blue lines show bidirectional associations between global topics and their consistent target-side translations. Previous studies that explore richer information for lexical selection can be divided into two categories: 1) incorporating sentence-level contexts (Chan et al., 2007; Carpuat and Wu, 2007; Hasan et al., 2008; Mauser et al., 2009; He et al., 2008; Shen et al., 2009) or 2) integrating document-level topics (Xiao et al., 2011; Ture et al., 2012; Xiao et al., 2012; Eidelman et al., 2012; Hewavitharana et al., 2013; Xiong et al., 2013; Hasler et al., 2014a; Hasler et al., 2014b) into SMT. The methods in these two strands have shown their effectiveness on lexical selection. However, correlations between sentence- and document-level contexts have never been explored before. It is clear that local contexts and global topics are often highly correlated. Consider a ChineseEnglish translation example presented in Figure 1. On the one hand, if local contexts suggest that the source word “á|/l`ıchˇang” should be translated"
P15-1023,P12-1079,1,0.93568,"ation candidates for a Chinese content word (within a dotted box). Green lines point to translations that are favored by local contexts while blue lines show bidirectional associations between global topics and their consistent target-side translations. Previous studies that explore richer information for lexical selection can be divided into two categories: 1) incorporating sentence-level contexts (Chan et al., 2007; Carpuat and Wu, 2007; Hasan et al., 2008; Mauser et al., 2009; He et al., 2008; Shen et al., 2009) or 2) integrating document-level topics (Xiao et al., 2011; Ture et al., 2012; Xiao et al., 2012; Eidelman et al., 2012; Hewavitharana et al., 2013; Xiong et al., 2013; Hasler et al., 2014a; Hasler et al., 2014b) into SMT. The methods in these two strands have shown their effectiveness on lexical selection. However, correlations between sentence- and document-level contexts have never been explored before. It is clear that local contexts and global topics are often highly correlated. Consider a ChineseEnglish translation example presented in Figure 1. On the one hand, if local contexts suggest that the source word “á|/l`ıchˇang” should be translated in229 Proceedings of the 53rd Annual M"
P15-1023,P06-1066,1,0.778544,"using GIZA++ (Och and Ney, 2003) with the option “grow-diag-final-and”. We chose the NIST evaluation set of MT05 as the development set, and the sets of MT06/MT08 as test sets. On average, these three sets contain 17.2, 13.9 and 14.1 content words per sentence, respectively. We trained a 5-gram language model on the Xinhua portion of Gigaword corpus using the SRILM Toolkit (Stolcke, 2002). Our baseline system is a state-of-the-art SMT system, which adapts bracketing transduction grammars (Wu, 1997) to phrasal translation and equips itself with a maximum entropy based reordering model (MEBTG) (Xiong et al., 2006). We used the toolkit4 developed by Zhang (2004) to train the reordering model with the following parameters: iteration number iter=200 and Gaussian prior g=1.0. During decoding, we set the ttable-limit as 20, the stack-size as 100. The translation quality is evaluated by case-insensitive BLEU-4 (Papineni et al., 2002) metric. Finally, we conducted paired bootstrap sampling (Koehn, 2004) to test the significance in BLEU score differences. 2 http://people.sutd.edu.sg/∼yue zhang/doc/index.html http://nlp.stanford.edu/software 4 http://homepages.inf.ed.ac.uk/lzhang10/maxenttoolkit.html 3 (6) 233"
P15-1023,P14-1137,1,0.852105,"Missing"
P15-1023,P06-2124,0,0.0377211,"topical items and contextual words learned by CATM with Nz =25 and Ws =12. Chinese words that do not have direct English translations are denoted with ”*”. Here “q” and “|” are Chinese quantifiers for missile and war, respectively; “ü” and “W” together means cross-starit. the document-level translation consistency. Ture et al. (2012) soften this consistency constraint by integrating three counting features into decoder. Also relevant is the work of Xiong et al.(2013), who use three different models to capture lexical cohesion for document-level SMT. (2) SMT with Topic Models. In this strand, Zhao and Xing (2006, 2007) first present a bilingual topical admixture formalism for word alignment in SMT. Tam et al. (2007) and Ruiz et al. (2012) apply topic model into language model adaptation. Su et al. (2012) conduct translation model adaptation with monolingual topic information. Gong et al. (2010) and Xiao et al. (2012) introduce topic-based similarity models to improve SMT system. Axelrod et al. (2012) build topic-specific translation models from the TED corpus and select topic-relevant data from the UN corpus to improve coverage. Eidelman et al. (2012) incorporate topic-specific lexical weights into t"
P15-1023,P02-1040,0,\N,Missing
P15-1023,W11-2133,0,\N,Missing
P15-1023,P13-2122,0,\N,Missing
P15-1023,J07-2003,0,\N,Missing
P15-1090,P14-3012,1,0.913154,"haracter level probability of this token based on a character level language model Normalization Features 11. whether each individual candidate list has any candidates for this token 12. how many candidates each individual candidate list has 13. whether each individual list’s top 10 candidates contain this token itself 14. the max number of lists that have the same top one candidate 15. the similarity value between each individual normalization system’s first candidate w and this token t, calculated by to combine and rerank these candidate lists, using a rich set of features. Please refer to (Li and Liu, 2014) for more details. By analyzing each individual system, we find that for ill-OOV words most normalization systems can generate many candidates, which may contain a correct candidate; for correct-OOV words, many normalization systems have few candidates or may not provide any candidates. For example, only two of the six lists have candidates for the token Newsfeed and Metropcs. Therefore, we believe the patterns of these normalization results contain useful information to classify OOVs. Note that this kind of feature is only applicable for those tokens that are judged as OOV by the given dictio"
P15-1090,P06-2005,0,0.0309784,"al. (2011) created a specific set of POS tags for twitter data. With this tag set and word cluster information extracted from a huge Twitter corpus, their proposed system obtained significant improvement on POS tagging accuracy in Twitter data. Related Work There has been a surge of interest in lexical normalization with the advent of social media data. Lots of approaches have been developed for this task, from using edit distance (Damerau, 1964; Levenshtein, 1966), to the noisy channel model (Cook and Stevenson, 2009; Pennell and Liu, 2010; Liu et al., 2012a) and machine translation method (Aw et al., 2006; Pennell and Liu, 2011; Li and Liu, 2012b; Li and Liu, 2012a). Normalization performance on some benchmark data has been improved a lot. Currently, unsupervised models are widely used to extract latent relationship between non-standard words and correct words from a huge corpus. Hassan and Menezes (2013) applied the random walk algorithm on a contextual similarity bipartite graph, constructed from n-gram sequences on a large unlabeled text corpus to build relation between non-standard tokens and correct words. Yang and Eisenstein (2013) presented a unified unsupervised statistical At the same"
P15-1090,P12-1109,0,0.499758,"n NER compared with the baseline systems. Gimpel et al. (2011) created a specific set of POS tags for twitter data. With this tag set and word cluster information extracted from a huge Twitter corpus, their proposed system obtained significant improvement on POS tagging accuracy in Twitter data. Related Work There has been a surge of interest in lexical normalization with the advent of social media data. Lots of approaches have been developed for this task, from using edit distance (Damerau, 1964; Levenshtein, 1966), to the noisy channel model (Cook and Stevenson, 2009; Pennell and Liu, 2010; Liu et al., 2012a) and machine translation method (Aw et al., 2006; Pennell and Liu, 2011; Li and Liu, 2012b; Li and Liu, 2012a). Normalization performance on some benchmark data has been improved a lot. Currently, unsupervised models are widely used to extract latent relationship between non-standard words and correct words from a huge corpus. Hassan and Menezes (2013) applied the random walk algorithm on a contextual similarity bipartite graph, constructed from n-gram sequences on a large unlabeled text corpus to build relation between non-standard tokens and correct words. Yang and Eisenstein (2013) presen"
P15-1090,P14-2111,0,0.0217856,"common in short texts for various reasons (e.g., length limitation, need to convey much information, writing style). They post problems to many NLP techniques in this domain. 929 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 929–938, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics model, in which the relationship between the standard and non-standard words is characterized by a log-linear model, permitting the use of arbitrary features. Chrupała (2014) proposed a text normalization model based on learning edit operations from labeled data while incorporating features induced from unlabeled data via recurrent network derived character-level neural text embeddings. sification process. Our experiment results demonstrate that our proposed system gives a significant performance improvement on NSW detection compared with the dictionary baseline system. On the other hand, the impact of normalization or NSW detection on NER has not been well studied in social media domain. In this paper, we propose two methods to incorporate the NSW detection infor"
P15-1090,P12-1055,0,0.135892,"n NER compared with the baseline systems. Gimpel et al. (2011) created a specific set of POS tags for twitter data. With this tag set and word cluster information extracted from a huge Twitter corpus, their proposed system obtained significant improvement on POS tagging accuracy in Twitter data. Related Work There has been a surge of interest in lexical normalization with the advent of social media data. Lots of approaches have been developed for this task, from using edit distance (Damerau, 1964; Levenshtein, 1966), to the noisy channel model (Cook and Stevenson, 2009; Pennell and Liu, 2010; Liu et al., 2012a) and machine translation method (Aw et al., 2006; Pennell and Liu, 2011; Li and Liu, 2012b; Li and Liu, 2012a). Normalization performance on some benchmark data has been improved a lot. Currently, unsupervised models are widely used to extract latent relationship between non-standard words and correct words from a huge corpus. Hassan and Menezes (2013) applied the random walk algorithm on a contextual similarity bipartite graph, constructed from n-gram sequences on a large unlabeled text corpus to build relation between non-standard tokens and correct words. Yang and Eisenstein (2013) presen"
P15-1090,W09-2010,0,0.0681296,"Missing"
P15-1090,P11-2093,0,0.015552,"n-gram sequences on a large unlabeled text corpus to build relation between non-standard tokens and correct words. Yang and Eisenstein (2013) presented a unified unsupervised statistical At the same time, increasing research work has been done to integrate lexical normalization into the NLP tasks in social media data. Kaji and Kitsuregawa (2014) combined lexical normalization, word segmentation and POS tagging on Japanese microblog. They used rich character-level and word-level features from the state-of-the-art models of joint word segmentation and POS tagging in Japanese (Kudo et al., 2004; Neubig et al., 2011). Their model can also be trained on a partially annotated corpus. Li and Liu (2015) conducted a similar research on joint POS tagging and text normalization for English. Wang and Kan 930 words will be considered as correct-OOV. Therefore all the tokens will have these three labels: IV, ill-OOV, and correct-OOV. Throughout this paper, we use GNU spell dictionary (v0.60.6.1) to determine whether a token is OOV.1 Twitter mentions (e.g., @twitter), hashtags and urls are excluded from consideration for OOV. Dictionary lookup of Internet slang2 is performed to filter those ill-OOV words whose corre"
P15-1090,P11-2008,0,0.134859,"Missing"
P15-1090,N13-1039,0,0.0626686,"Missing"
P15-1090,P11-1038,0,0.659006,"We also create a new data set with newly added normalization annotation beyond the existing named entity labels. This is the first data set with such annotation and we release it for research purpose. Our experiment results demonstrate the effectiveness of our NSW detection method and the benefit of NSW detection for NER. Our proposed methods perform better than the state-of-the-art NER system. 1 However, most of previous work on normalization assumed that they already knew which tokens are NSW that need normalization. Then different methods are applied only to these tokens. To our knowledge, Han and Baldwin (2011) is the only previous work which made a pilot research on NSW detection. One straight forward method to do this is to use a dictionary to classify a token into in-vocabulary (IV) words and out-of-vocabulary (OOV) words, and just treat all the OOV words as NSW. The shortcoming of this method is obvious. For example, tokens like ‘iPhone’, ‘PES’(a game name) and ‘Xbox’ will be considered as NSW, however, these words do not need normalization. Han and Baldwin (2011) called these OOV words correct-OOV, and named those OOV words that do need normalization as ill-OOV. We will follow their naming conv"
P15-1090,I11-1109,1,0.852606,"ed a specific set of POS tags for twitter data. With this tag set and word cluster information extracted from a huge Twitter corpus, their proposed system obtained significant improvement on POS tagging accuracy in Twitter data. Related Work There has been a surge of interest in lexical normalization with the advent of social media data. Lots of approaches have been developed for this task, from using edit distance (Damerau, 1964; Levenshtein, 1966), to the noisy channel model (Cook and Stevenson, 2009; Pennell and Liu, 2010; Liu et al., 2012a) and machine translation method (Aw et al., 2006; Pennell and Liu, 2011; Li and Liu, 2012b; Li and Liu, 2012a). Normalization performance on some benchmark data has been improved a lot. Currently, unsupervised models are widely used to extract latent relationship between non-standard words and correct words from a huge corpus. Hassan and Menezes (2013) applied the random walk algorithm on a contextual similarity bipartite graph, constructed from n-gram sequences on a large unlabeled text corpus to build relation between non-standard tokens and correct words. Yang and Eisenstein (2013) presented a unified unsupervised statistical At the same time, increasing resea"
P15-1090,P13-1155,0,0.0154267,"lexical normalization with the advent of social media data. Lots of approaches have been developed for this task, from using edit distance (Damerau, 1964; Levenshtein, 1966), to the noisy channel model (Cook and Stevenson, 2009; Pennell and Liu, 2010; Liu et al., 2012a) and machine translation method (Aw et al., 2006; Pennell and Liu, 2011; Li and Liu, 2012b; Li and Liu, 2012a). Normalization performance on some benchmark data has been improved a lot. Currently, unsupervised models are widely used to extract latent relationship between non-standard words and correct words from a huge corpus. Hassan and Menezes (2013) applied the random walk algorithm on a contextual similarity bipartite graph, constructed from n-gram sequences on a large unlabeled text corpus to build relation between non-standard tokens and correct words. Yang and Eisenstein (2013) presented a unified unsupervised statistical At the same time, increasing research work has been done to integrate lexical normalization into the NLP tasks in social media data. Kaji and Kitsuregawa (2014) combined lexical normalization, word segmentation and POS tagging on Japanese microblog. They used rich character-level and word-level features from the sta"
P15-1090,W10-0513,0,0.0343477,"i+1 (i = −2, −1, 0, 1) 5. Token’s predicted NSW label: Unigram: Li (i = 0) Bigram: Li Li+1 (i = −2, −1, 0, 1) Trigram: Li−1 Li Li+1 (i = −2, −1, 0, 1) 6. Compound features using lexical and NSW labels: Wi Di , Wi Li , Wi Di Li (i = 0) 7. Compound features using POS and NSW labels: Pi Di , Pi Li , Pi Di Li (i = 0) 8. Compound features using word, POS, and NSW labels: Wi Pi Di Li (i = 0) Data and Experiment 4.1 Data Set and Experiment Setup The NSW detection model is trained using the data released by (Li and Liu, 2014). It has 2,577 Twitter messages (selected from the Edinburgh Twitter corpus (Petrovic et al., 2010)), in which there are 2,333 unique pairs of NSW and their standard words. This data is used for training the different normalization models. We labeled this data set using the given dictionary for NSW detection. 4,121 tokens are labeled as ill-OOV, 1,455 as correctOOV, and the rest 33,740 tokens are IV words. We have two test sets for evaluating the NSW detection system. One is from (Han and Baldwin, 2011), which includes 549 tweets. Each tweet contains at least one ill-OOV and the corresponding correct word. We call it Test set 1 in the following. The other is from (Li and Liu, 2015), who fur"
P15-1090,D14-1011,0,0.0112627,"improved a lot. Currently, unsupervised models are widely used to extract latent relationship between non-standard words and correct words from a huge corpus. Hassan and Menezes (2013) applied the random walk algorithm on a contextual similarity bipartite graph, constructed from n-gram sequences on a large unlabeled text corpus to build relation between non-standard tokens and correct words. Yang and Eisenstein (2013) presented a unified unsupervised statistical At the same time, increasing research work has been done to integrate lexical normalization into the NLP tasks in social media data. Kaji and Kitsuregawa (2014) combined lexical normalization, word segmentation and POS tagging on Japanese microblog. They used rich character-level and word-level features from the state-of-the-art models of joint word segmentation and POS tagging in Japanese (Kudo et al., 2004; Neubig et al., 2011). Their model can also be trained on a partially annotated corpus. Li and Liu (2015) conducted a similar research on joint POS tagging and text normalization for English. Wang and Kan 930 words will be considered as correct-OOV. Therefore all the tokens will have these three labels: IV, ill-OOV, and correct-OOV. Throughout th"
P15-1090,C14-1092,0,0.0192864,"of named entity normalization (NEN) for tweets. They proposed a novel graphical model to simultaneously conduct NER and NEN on multiple tweets. Although this work involved text normalization, it only focused on the NER task, and there was no reported result for normalization. On Turkish tweets, Kucuk and Steinberger (2014) adapted NER rules and resources to better fit Twitter language by relaxing its capitalization constraint, expanding its lexical resources based on diacritics, and using a normalization scheme on tweets. These showed positive effect on the overall NER performance. Rangarajan Sridhar et al. (2014) decoupled the SMS translation task into normalization followed by translation. They exploited bi-text resources, and presented a normalization approach using distributed representation of words learned through neural networks. In this study, we propose new methods to effectively integrate information of OOV words and their normalization for the NER task. In particular, by adopting joint decoding for both NSW detection and NER, we are able to outperform stateof-the-art results for both tasks. This is the first study that systematically evaluates the effect of OOV words and normalization on NER"
P15-1090,W14-1309,0,0.0194336,"word segmentation in Chinese Microblog. But with their method, ill-OOV words are merely recognized and not normalized. Therefore, they did not investigate how to exploit the information that may be derived from normalization to increase word segmentation accuracy. Liu et al. (2012b) studied the problem of named entity normalization (NEN) for tweets. They proposed a novel graphical model to simultaneously conduct NER and NEN on multiple tweets. Although this work involved text normalization, it only focused on the NER task, and there was no reported result for normalization. On Turkish tweets, Kucuk and Steinberger (2014) adapted NER rules and resources to better fit Twitter language by relaxing its capitalization constraint, expanding its lexical resources based on diacritics, and using a normalization scheme on tweets. These showed positive effect on the overall NER performance. Rangarajan Sridhar et al. (2014) decoupled the SMS translation task into normalization followed by translation. They exploited bi-text resources, and presented a normalization approach using distributed representation of words learned through neural networks. In this study, we propose new methods to effectively integrate information"
P15-1090,D11-1141,0,0.780509,"|IV) Ill-OOV O IV p(correct-OOV|IV) correct-OOV I p(O|B) p(I|Messi) IV IV Ill-OOV Ill-OOV is well-known p(Ill-OOV|Messi) p(O|Messi) is Messi Messi well-known (A) B_IV (B) p(B|B)+ β * p(IVIIV) p(B|Messi)+ α* p(IV|Messi) B_correct-OOV p(B|B)+ β * p(ill-OOVIIV) B_IV B_correct-OOV B_IV B_correct-OOV p(B|is)+ α* p(correct-OOV|is) B_ill-OOV B_ill-OOV p(I|B)+ β * p(IVIIV) I_IV I_IV . . . . . . Messi is B_ill-OOV p(B|well-known)+ α* p(ill-OOV|well-known) I_IV p(I|B)+ β * p(IVIill-OOV) . . . well-known (C) Figure 1: Trellis Viterbi decoding for different systems. The data with the NER labels are from (Ritter et al., 2011) who annotated 2,396 tweets (34K tokens) with named entities, but there is no information on the tweets’ ill-OOV words. In order to evaluate the impact of ill-OOV on NER, we ask six annotators to annotate the ill-OOV words and the corresponding standard words in this data. There are only 1,012 sentences with ill-OOV words. We use all the sentences (2,396) for the NER experiments. This data set,6 to our knowledge, is the first one having both ill-OOV and NER annotation in social media domain. For joint decoding, the parameters α and β are empirically set as 0.95 and 0.5. 4.2 in Table 1. First n"
P15-1090,D14-1037,0,0.0278947,"Missing"
P15-1090,P13-1072,0,0.0609524,"Missing"
P15-1090,D13-1007,0,0.017368,"l and Liu, 2010; Liu et al., 2012a) and machine translation method (Aw et al., 2006; Pennell and Liu, 2011; Li and Liu, 2012b; Li and Liu, 2012a). Normalization performance on some benchmark data has been improved a lot. Currently, unsupervised models are widely used to extract latent relationship between non-standard words and correct words from a huge corpus. Hassan and Menezes (2013) applied the random walk algorithm on a contextual similarity bipartite graph, constructed from n-gram sequences on a large unlabeled text corpus to build relation between non-standard tokens and correct words. Yang and Eisenstein (2013) presented a unified unsupervised statistical At the same time, increasing research work has been done to integrate lexical normalization into the NLP tasks in social media data. Kaji and Kitsuregawa (2014) combined lexical normalization, word segmentation and POS tagging on Japanese microblog. They used rich character-level and word-level features from the state-of-the-art models of joint word segmentation and POS tagging in Japanese (Kudo et al., 2004; Neubig et al., 2011). Their model can also be trained on a partially annotated corpus. Li and Liu (2015) conducted a similar research on join"
P15-1090,W04-3230,0,\N,Missing
P15-1090,C12-1097,1,\N,Missing
P15-1114,E12-1009,0,0.0411197,"Missing"
P15-1114,P02-1034,0,0.122962,"re not adjacent siblings. 5 Efficient Candidate Generation 5.1 Polynomial Kernel As mentioned above, we generate the r + 1order candidates by combining the candidates of order r. An efficient feature generation algorithm should be carefully designed to avoid duplicates, otherwise #f + and #f − may be over counted. The candidate generation algorithm is kernel dependent. For polynomial kernel, we just combine any two r-order candidates and remove the combined feature if its order is not r + 1. This method requires square running time for each example. 5.2 Dependency Tree Kernel 5.2.1 Definition Collins and Duffy (2002) proposed tree kernels for constituent parsing which includes the all-subtree features. Similarly, we define dependency tree kernel for dependency parsing. For compatibility with the previously studied subtree features for dependency parsing, we propose a new dependency tree kernel that is different from Culotta and Sorensen’s (Culotta and Sorensen, 2004). Given a dependency parse tree T composed of L words, L − 1 arcs, each arc has several basic features, such as the concatenation of the head word and the modifier word, the concatenation of the word left to the head and the lower case of the"
P15-1114,P04-1054,0,0.0977302,"kernel dependent. For polynomial kernel, we just combine any two r-order candidates and remove the combined feature if its order is not r + 1. This method requires square running time for each example. 5.2 Dependency Tree Kernel 5.2.1 Definition Collins and Duffy (2002) proposed tree kernels for constituent parsing which includes the all-subtree features. Similarly, we define dependency tree kernel for dependency parsing. For compatibility with the previously studied subtree features for dependency parsing, we propose a new dependency tree kernel that is different from Culotta and Sorensen’s (Culotta and Sorensen, 2004). Given a dependency parse tree T composed of L words, L − 1 arcs, each arc has several basic features, such as the concatenation of the head word and the modifier word, the concatenation of the word left to the head and the lower case of the word right to the modifier, the distance of the arc, the direction of the arc, the 1184 concatenation of the POS tags of the head and the modifier, etc. A feature tree of T is a tree that has the same structure as T , while each arc is replaced by any of its basic features. For a parse tree that has L − 1 arcs, and each arc has d basic features, the numbe"
P15-1114,Q13-1012,0,0.0355938,"Missing"
P15-1114,P98-1106,0,0.172945,"the impact of our system on non-English treebanks. We evaluate our system on six other languages from the CoNLL 2009 sharedtask. We used the best setting in the previous experiment: reranking model is trained using the features selected in the dependency tree kernel space. For POS tag features we used the predicted tags. As the third order parser can not handle non-projective parse trees, we used the graph transformation techniques to produce nonprojective structures (Nivre and Nilsson, 2005). First, the training data for the parser is projectivized by applying a number of lifting operations (Kahane et al., 1998) and encoding information about these lifts in arc labels. We used the path encoding scheme where the label of each arc is concatenated with two binary tags, one indicates if the arc is lifted, the other indicates if the arc is along the lifting path from the syntactic to the linear head. Then we train a projective Language Chinese Japanese German Spanish Czech Catalan Ours 76.77 92.68 87.40 87.82 80.51 86.98 Official Best 79.17 92.57 87.48 87.64 80.38 87.86 Table 4: Experimental Results on CoNLL 2009 non-English datasets. parser on the transformed data without arc label information and a clas"
P15-1114,P10-1001,0,0.207849,"ental Results on English Dataset 6.1.1 Settings First we used the English Penn Tree Bank (PTB) with standard train/develop/test for evaluation. Sections 2-21 (around 40K sentences) were used as training data, section 22 was used as the development set and section 23 was used as the final test set. We extracted dependencies using Joakim Nivre’s Penn2Malt tool with Yamada and Matsumoto’s rules (Yamada and Matsumoto, 2003). Unlabeled attachment score (UAS) ignoring punctuation is used to evaluate parsing quality. We apply our technique to rerank the parse trees generated by a third order parser (Koo and Collins, 2010) trained using 10 best MIRA algorithm with 10 iterations. We generate the top 10 best candidate parse trees using 10 fold cross validation for each sentence in the training data. The gold parse tree is added if it is not in the candidate list. Then we learn a reranking model using these candidate trees. During testing, the score for a parse tree T is a linear combination of the two models: score(T ) = βscoreO3 (T ) + scorererank (T ) where the meta-parameter β = 5 is tuned by grid search using the development dataset. scoreO3 (T ) and scorererank (T ) are the outputs of the third order parser"
P15-1114,P05-1024,0,0.176266,"requires |T1 |2 ∗ |T2 |2 ∗ min{|T1 |, |T2 |} running time in the worst case, as we need to enumerate p, q, l and n1 , n2 . One way to incorporate the dependency tree kernel for parsing is to rerank the K best candidate parse trees generated by a simple linear model. Suppose there are n training samples, the size of the kernel matrix is (K ∗ n)2 , which is unacceptable for large datasets. 5.2.2 Candidate Generation For constituent parsing, Kudo et al. showed such an all-subtrees representation is extremely redundant and a comparable accuracy can be achieved using just a small set of subtrees (Kudo et al., 2005). Suzuki et al. even showed that the over-fitting problem often arises when convolution kernels are used in NLP tasks (Suzuki et al., 2004). Now we attempt to select representative sub 1185 feature trees in the kernel space using Algorithm 1. The r-order features in dependency tree kernel space are the sub feature trees with r arcs. The candidate feature generation in Line 17 has two steps: first we generate the subtrees with r arcs, then we generate the sub feature trees for each subtree. The simplest way for subtree generation is to enumerate the combinations of r + 2 words in the sentence,"
P15-1114,P13-2109,0,0.0610056,"h−1 pm , ph−1 wm , ph pm−1 , wh pm−1 ph+1 pm , ph+1 wm , ph pm+1 , wh pm+1 ph−1 ph pm , ph ph+1 pm , ph pm−1 pm , ph pm pm+1 Concatenate features above with length and direction ph pb pm System Third Order Parser Quadratic Kernel(QK) Biquadratic Kernel(BK) 8-th Degree Polynomial Kernel(8K) Dependency Tree Kernel (DTK) LM with Template Features LM with Features in QK LM with Features in BK LM with Features in 8K LM with Features in DTK (Zhang and McDonald, 2014) (Zhang et al., 2013) (Ma and Zhao, 2012) (Bohnet and Kuhn, 2012) (Rush and Petrov, 2012) (Qian and Liu, 2013) (Hayashi et al., 2013) (Martins et al., 2013) (Zhang and McDonald, 2012) (Koo and Collins, 2010) (Zhang and Nivre, 2011) Table 1: Basic features in polynomial and dependency tree kernel spaces, wh : the word of head node, wm denotes the word of modifier node, ph : the POS of head node, pm denotes the POS of modifier node, ph+1 : POS to the right of head node, ph−1 : POS to the left of modifier node, pm+1 : POS to the right of head node, pm−1 : POS to the left of modifier node, pb : POS of a word in between head and modifier nodes. direction of the arcs, and the POS tags of the words lying between the head and modifier, as shown in Table"
P15-1114,P05-1013,0,0.0476449,"s grows drastically, and the program runs out of memory (OOM). 6.2 Experimental Results on CoNLL 2009 Dataset Now we looked at the impact of our system on non-English treebanks. We evaluate our system on six other languages from the CoNLL 2009 sharedtask. We used the best setting in the previous experiment: reranking model is trained using the features selected in the dependency tree kernel space. For POS tag features we used the predicted tags. As the third order parser can not handle non-projective parse trees, we used the graph transformation techniques to produce nonprojective structures (Nivre and Nilsson, 2005). First, the training data for the parser is projectivized by applying a number of lifting operations (Kahane et al., 1998) and encoding information about these lifts in arc labels. We used the path encoding scheme where the label of each arc is concatenated with two binary tags, one indicates if the arc is lifted, the other indicates if the arc is along the lifting path from the syntactic to the linear head. Then we train a projective Language Chinese Japanese German Spanish Czech Catalan Ours 76.77 92.68 87.40 87.82 80.51 86.98 Official Best 79.17 92.57 87.48 87.64 80.38 87.86 Table 4: Exper"
P15-1114,Q13-1004,1,0.900103,"Missing"
P15-1114,N12-1054,0,0.290766,"Missing"
P15-1114,P04-1016,0,0.19426,"more than 20 million features. Tan et al. (2012) proposed adaptive feature scaling (AFS) scheme for ultra-high dimensional feature selection. The dimensionality of the features in their experiments is up to 30 millions. Previous studies on feature selection in kernel space typically used mining based approaches to prune feature candidates. The key idea for efficient pruning is to estimate the upper bound of statistics of features without explicit calculation. The simplest example is frequent mining where for any n-gram feature, its frequency is bounded by any of its substrings. Suzuki et al. (Suzuki et al., 2004) proposed to select features in convolution kernel space based on their chi-squared values. They derived a concise form to estimate the upper bound of chisquare values, and used PrefixScan algorithm to enumerates all the significant sub-sequences of features efficiently. Okanohara and Tsujii (Okanohara and Tsujii, 2009) further combined the pruning technique with L1 regularization. They showed the connection between L1 regularization and frequent mining: the L1 regularizer provides a minimum support threshold to prune the gradients of parameters. They selected the combination features in a coa"
P15-1114,P07-2017,0,0.0336929,"a, July 26-31, 2015. 2015 Association for Computational Linguistics 2 Related Works There are two solutions for learning in ultra high dimensional feature space: kernel method and feature selection. Fast kernel methods have been intensively studied in the past few years. Recently, randomized methods have attracted more attention due to its theoretical and empirical success, such as the Nystr¨om method (Williams and Seeger, 2001) and random projection (Lu et al., 2014). In NLP domain, previous studies mainly focused on polynomial kernels, such as the splitSVM and approximate polynomial kernel (Wu et al., 2007). In feature selection domain, there has been plenty of work focusing on fast computation, while feature selection in extremely high dimensional feature space is relatively less studied. Zhang et al. (2006) proposed a progressive feature selection framework that splits the feature space into tractable disjoint sub-spaces such that a feature selection algorithm can be performed on each one of them, and then merges the selected features from different sub-spaces. The search space they studied contained more than 20 million features. Tan et al. (2012) proposed adaptive feature scaling (AFS) schem"
P15-1114,W03-3023,0,0.216994,"hich enumerates all subtrees from a given tree efficiently. This method starts with a set of trees consisting of single nodes, and then expands each subtree attaching a new node. 6 Experiments 6.1 Experimental Results on English Dataset 6.1.1 Settings First we used the English Penn Tree Bank (PTB) with standard train/develop/test for evaluation. Sections 2-21 (around 40K sentences) were used as training data, section 22 was used as the development set and section 23 was used as the final test set. We extracted dependencies using Joakim Nivre’s Penn2Malt tool with Yamada and Matsumoto’s rules (Yamada and Matsumoto, 2003). Unlabeled attachment score (UAS) ignoring punctuation is used to evaluate parsing quality. We apply our technique to rerank the parse trees generated by a third order parser (Koo and Collins, 2010) trained using 10 best MIRA algorithm with 10 iterations. We generate the top 10 best candidate parse trees using 10 fold cross validation for each sentence in the training data. The gold parse tree is added if it is not in the candidate list. Then we learn a reranking model using these candidate trees. During testing, the score for a parse tree T is a linear combination of the two models: score(T"
P15-1114,D12-1030,0,0.202572,"Missing"
P15-1114,P14-2107,0,0.0124618,"stablish the kernel spaces include the combinations of contextual words or POS tags of head and modifier, the length and 1186 wh wm , p h pm , w h pm , p h wm ph−1 pm , ph−1 wm , ph pm−1 , wh pm−1 ph+1 pm , ph+1 wm , ph pm+1 , wh pm+1 ph−1 ph pm , ph ph+1 pm , ph pm−1 pm , ph pm pm+1 Concatenate features above with length and direction ph pb pm System Third Order Parser Quadratic Kernel(QK) Biquadratic Kernel(BK) 8-th Degree Polynomial Kernel(8K) Dependency Tree Kernel (DTK) LM with Template Features LM with Features in QK LM with Features in BK LM with Features in 8K LM with Features in DTK (Zhang and McDonald, 2014) (Zhang et al., 2013) (Ma and Zhao, 2012) (Bohnet and Kuhn, 2012) (Rush and Petrov, 2012) (Qian and Liu, 2013) (Hayashi et al., 2013) (Martins et al., 2013) (Zhang and McDonald, 2012) (Koo and Collins, 2010) (Zhang and Nivre, 2011) Table 1: Basic features in polynomial and dependency tree kernel spaces, wh : the word of head node, wm denotes the word of modifier node, ph : the POS of head node, pm denotes the POS of modifier node, ph+1 : POS to the right of head node, ph−1 : POS to the left of modifier node, pm+1 : POS to the right of head node, pm−1 : POS to the left of modifier node, pb : PO"
P15-1114,P11-2033,0,0.0823585,"Missing"
P15-1114,P06-1071,0,0.0371893,"kernel methods have been intensively studied in the past few years. Recently, randomized methods have attracted more attention due to its theoretical and empirical success, such as the Nystr¨om method (Williams and Seeger, 2001) and random projection (Lu et al., 2014). In NLP domain, previous studies mainly focused on polynomial kernels, such as the splitSVM and approximate polynomial kernel (Wu et al., 2007). In feature selection domain, there has been plenty of work focusing on fast computation, while feature selection in extremely high dimensional feature space is relatively less studied. Zhang et al. (2006) proposed a progressive feature selection framework that splits the feature space into tractable disjoint sub-spaces such that a feature selection algorithm can be performed on each one of them, and then merges the selected features from different sub-spaces. The search space they studied contained more than 20 million features. Tan et al. (2012) proposed adaptive feature scaling (AFS) scheme for ultra-high dimensional feature selection. The dimensionality of the features in their experiments is up to 30 millions. Previous studies on feature selection in kernel space typically used mining base"
P15-1114,D13-1093,0,0.0281219,"Missing"
P15-1114,C98-1102,0,\N,Missing
P15-1114,N09-2025,0,\N,Missing
P15-1132,P13-1088,0,0.0722726,"e of the text is not yet fully employed in these models. Two ideas are motivated by the example shown in Figure 2: First, the composition function for the noun phrase ‘the movie/NP’ should be different from that for the adjective phrase ‘very interesting/ADJP’ since the two phrases are quite syntactically different. More specifically to sentiment analysis, a noun phrase is much less likely to express sentiment than an adjective phrase. There are two notable works mentioned here: (Socher et al., 2013a) presented to combine the parsing and composition processes, but the purpose is for parsing; (Hermann and Blunsom, 2013) designed composition functions according to the combinatory rules and categories in CCG grammar, however, only marginal improvement against Naive Bayes was reported. Our proposed model, tag guided RNN (TG-RNN), is designed to use the syntactic tag of the parent phrase to guide the composition process from the child nodes. As an example, we design a function for composing noun phrase (NP) and another one for adjective phrase (ADJP). This simple strategy obtains remarkable improvements against strong baselines. ing/JJ’ apparently contributes more to sentiment expression. To address this issue,"
P15-1132,P14-1062,0,0.00735143,"RNN (ours) TE-RNN (ours) TE-RNTN (ours) CNN DCNN Para-Vec for composition function which could model the meaning of longer phrases and capture negation rules. • AdaMC. Adaptive Multi-Compositionality for RNN and RNTN (Dong et al., 2014) trains more than one composition functions and adaptively learns the weight for each function. We compare our models with several methods which are evaluated on the Sentiment Treebank corpus. The baseline results are reported in (Dong et al., 2014) and (Kim, 2014). We make comparison to the following baselines: • DCNN/CNN. Dynamic Convolutional Neural Network (Kalchbrenner et al., 2014) and a simple Convolutional Neural Network (Kim, 2014), though these models are of different genres to RNN, we include them here for fair comparison since they are among top performing approaches on this task. • SVM. A SVM model with bag-of-words representation (Pang and Lee, 2008). • MNB/bi-MNB. Multinomial Naive Bayes and its bigram variant, adopted from (Wang and Manning, 2012). • Para-Vec. A word2vec variant (Le and Mikolov, 2014) that encodes paragraph information into word embedding learning. A simple but very competitive model. • RNN. The first Recursive Neural Network model proposed by"
P15-1132,D14-1181,0,0.0442612,"01 and a constant learning rate of 0.005. Method SVM MNB bi-MNB RNN MV-RNN RNTN AdaMC-RNN AdaMC-RNTN DRNN TG-RNN (ours) TE-RNN (ours) TE-RNTN (ours) CNN DCNN Para-Vec for composition function which could model the meaning of longer phrases and capture negation rules. • AdaMC. Adaptive Multi-Compositionality for RNN and RNTN (Dong et al., 2014) trains more than one composition functions and adaptively learns the weight for each function. We compare our models with several methods which are evaluated on the Sentiment Treebank corpus. The baseline results are reported in (Dong et al., 2014) and (Kim, 2014). We make comparison to the following baselines: • DCNN/CNN. Dynamic Convolutional Neural Network (Kalchbrenner et al., 2014) and a simple Convolutional Neural Network (Kim, 2014), though these models are of different genres to RNN, we include them here for fair comparison since they are among top performing approaches on this task. • SVM. A SVM model with bag-of-words representation (Pang and Lee, 2008). • MNB/bi-MNB. Multinomial Naive Bayes and its bigram variant, adopted from (Wang and Manning, 2012). • Para-Vec. A word2vec variant (Le and Mikolov, 2014) that encodes paragraph information i"
P15-1132,D13-1054,1,0.596359,"ie Huang, Yang Liu*, Xuan Zhu*, Xiaoyan Zhu State Key Lab. of Intelligent Technology and Systems, National Lab. for Information Science and Technology, Dept. of Computer Science and Technology, Tsinghua University, Beijing 100084, PR China *Samsung R&D Institute Beijing, China qianqiaodecember29@126.com , smxtianbo@gmail.com aihuang@tsinghua.edu.cn , yang.liu@samsung.com xuan.zhu@samsung.com , zxy-dcs@tsinghua.edu.cn Abstract semantic relationship classification (Socher et al., 2012), syntactic parsing (Socher et al., 2013a), sentiment analysis (Socher et al., 2013b), and machine translation (Li et al., 2013). The key component of RNN and its variants is the composition function: how to compose the vector representation for a longer text from the vector of its child words or phrases. For instance, as shown in Figure 2, the vector of ‘is very interesting’ can be composed from the vector of the left node ‘is’ and that of the right node ‘very interesting’. It’s worth to mention again, the composition process is conducted with the syntactic structure of the text, making RNN more interpretable than other deep learning models. Recursive neural network is one of the most successful deep learning models f"
P15-1132,P08-1028,0,0.0144891,"are considered. In order to distinguish antonyms with similar contexts, neural word vectors (Bengio et al., 2003) are proposed and can be learnt in an unsupervised manner. Word2vec (Mikolov et al., 2013a) introduces a simpler network structure making computation more efficiently and makes billions of samples feasible for training. Semantic composition deals with representing a longer text from its shorter components, which is extensively studied recently. In many previous works, a phrase vector is usually obtained by average (Landauer and Dumais, 1997), addition, element-wise multiplication (Mitchell and Lapata, 2008) or tensor product (Smolensky, 1990) of word vectors. In addition to using vector representations, matrices can also be used to represent phrases and the composition process can be done through matrix multiplication (Rudolph and Giesbrecht, 2010; Yessenalina and Cardie, 2011). Recursive neural models utilize the recursive structure (usually a parse tree) of a phrase or sentence for semantic composition. In Recursive Neural Network (Socher et al., 2011), the tree with the least reconstruction error is built and the vectors for interior nodes is composed by a global matrix. Matrix-Vector Recursi"
P15-1132,J07-2002,0,0.0165882,"Missing"
P15-1132,P10-1093,0,0.037432,"e making computation more efficiently and makes billions of samples feasible for training. Semantic composition deals with representing a longer text from its shorter components, which is extensively studied recently. In many previous works, a phrase vector is usually obtained by average (Landauer and Dumais, 1997), addition, element-wise multiplication (Mitchell and Lapata, 2008) or tensor product (Smolensky, 1990) of word vectors. In addition to using vector representations, matrices can also be used to represent phrases and the composition process can be done through matrix multiplication (Rudolph and Giesbrecht, 2010; Yessenalina and Cardie, 2011). Recursive neural models utilize the recursive structure (usually a parse tree) of a phrase or sentence for semantic composition. In Recursive Neural Network (Socher et al., 2011), the tree with the least reconstruction error is built and the vectors for interior nodes is composed by a global matrix. Matrix-Vector Recursive Neural Network (MV-RNN) (Socher et al., 2012) assigns matrices for every words so that it could capture the relationship between two children. In Recursive Neural Tensor Networks (RNTN) (Socher et al., 2013b), the composition process is perfo"
P15-1132,D11-1014,0,0.70505,"node ‘is very interesting’ is composed from the phrase node ‘very interesting’ and the word node ‘is’ . Introduction Among a variety of deep learning models for natural language processing, Recursive Neural Network (RNN) may be one of the most popular models. Thanks to the compositional nature of natural text, recursive neural network utilizes the recursive structure of the input such as a phrase or sentence, and has shown to be very effective for many natural language processing tasks including There are various attempts to design the composition function in RNN (or related models). In RNN (Socher et al., 2011), a global matrix is used to linearly combine the elements of vectors. In RNTN (Socher et al., 2013b), a global tensor is used to compute the tensor products of dimensions to favor the association between different el1365 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 1365–1374, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics ements of the vectors. Sometimes it is challenging to find a single function to model the composition process. As a"
P15-1132,D12-1110,0,0.459358,"er et al., 2013b), a global tensor is used to compute the tensor products of dimensions to favor the association between different el1365 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 1365–1374, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics ements of the vectors. Sometimes it is challenging to find a single function to model the composition process. As an alternative, multiple composition functions can be used. For instance, in MV-RNN (Socher et al., 2012), different matrices is designed for different words though the model is suffered from too much parameters. In AdaMC RNN/RNTN (Dong et al., 2014), a fixed number of composition functions is linearly combined and the weight for each function is adaptively learned. In spite of the success of RNN and its variants, the syntactic knowledge of the text is not yet fully employed in these models. Two ideas are motivated by the example shown in Figure 2: First, the composition function for the noun phrase ‘the movie/NP’ should be different from that for the adjective phrase ‘very interesting/ADJP’ sinc"
P15-1132,P13-1045,0,0.103785,"and Tag-specific Composition Functions in Recursive Neural Network Qiao Qian, Bo Tian, Minlie Huang, Yang Liu*, Xuan Zhu*, Xiaoyan Zhu State Key Lab. of Intelligent Technology and Systems, National Lab. for Information Science and Technology, Dept. of Computer Science and Technology, Tsinghua University, Beijing 100084, PR China *Samsung R&D Institute Beijing, China qianqiaodecember29@126.com , smxtianbo@gmail.com aihuang@tsinghua.edu.cn , yang.liu@samsung.com xuan.zhu@samsung.com , zxy-dcs@tsinghua.edu.cn Abstract semantic relationship classification (Socher et al., 2012), syntactic parsing (Socher et al., 2013a), sentiment analysis (Socher et al., 2013b), and machine translation (Li et al., 2013). The key component of RNN and its variants is the composition function: how to compose the vector representation for a longer text from the vector of its child words or phrases. For instance, as shown in Figure 2, the vector of ‘is very interesting’ can be composed from the vector of the left node ‘is’ and that of the right node ‘very interesting’. It’s worth to mention again, the composition process is conducted with the syntactic structure of the text, making RNN more interpretable than other deep learni"
P15-1132,D13-1170,0,0.432546,"and Tag-specific Composition Functions in Recursive Neural Network Qiao Qian, Bo Tian, Minlie Huang, Yang Liu*, Xuan Zhu*, Xiaoyan Zhu State Key Lab. of Intelligent Technology and Systems, National Lab. for Information Science and Technology, Dept. of Computer Science and Technology, Tsinghua University, Beijing 100084, PR China *Samsung R&D Institute Beijing, China qianqiaodecember29@126.com , smxtianbo@gmail.com aihuang@tsinghua.edu.cn , yang.liu@samsung.com xuan.zhu@samsung.com , zxy-dcs@tsinghua.edu.cn Abstract semantic relationship classification (Socher et al., 2012), syntactic parsing (Socher et al., 2013a), sentiment analysis (Socher et al., 2013b), and machine translation (Li et al., 2013). The key component of RNN and its variants is the composition function: how to compose the vector representation for a longer text from the vector of its child words or phrases. For instance, as shown in Figure 2, the vector of ‘is very interesting’ can be composed from the vector of the left node ‘is’ and that of the right node ‘very interesting’. It’s worth to mention again, the composition process is conducted with the syntactic structure of the text, making RNN more interpretable than other deep learni"
P15-1132,P12-2018,0,0.00831717,"valuated on the Sentiment Treebank corpus. The baseline results are reported in (Dong et al., 2014) and (Kim, 2014). We make comparison to the following baselines: • DCNN/CNN. Dynamic Convolutional Neural Network (Kalchbrenner et al., 2014) and a simple Convolutional Neural Network (Kim, 2014), though these models are of different genres to RNN, we include them here for fair comparison since they are among top performing approaches on this task. • SVM. A SVM model with bag-of-words representation (Pang and Lee, 2008). • MNB/bi-MNB. Multinomial Naive Bayes and its bigram variant, adopted from (Wang and Manning, 2012). • Para-Vec. A word2vec variant (Le and Mikolov, 2014) that encodes paragraph information into word embedding learning. A simple but very competitive model. • RNN. The first Recursive Neural Network model proposed by (Socher et al., 2011). • RNTN. Recursive Neural Tenser Network (Socher et al., 2013b) employs a tensor Pos./Neg. 79.4 81.8 83.1 82.4 82.9 85.4 87.1 88.5 87.7 86.3 86.8 87.7 88.1 86.8 87.8 Table 1: Classification accuray. Fine-grained stands for 5-class prediction and Pos./Neg. means binary prediction which ignores all neutral instances. All the accuracy is at the sentence level ("
P15-1132,D11-1016,0,0.00905229,"ciently and makes billions of samples feasible for training. Semantic composition deals with representing a longer text from its shorter components, which is extensively studied recently. In many previous works, a phrase vector is usually obtained by average (Landauer and Dumais, 1997), addition, element-wise multiplication (Mitchell and Lapata, 2008) or tensor product (Smolensky, 1990) of word vectors. In addition to using vector representations, matrices can also be used to represent phrases and the composition process can be done through matrix multiplication (Rudolph and Giesbrecht, 2010; Yessenalina and Cardie, 2011). Recursive neural models utilize the recursive structure (usually a parse tree) of a phrase or sentence for semantic composition. In Recursive Neural Network (Socher et al., 2011), the tree with the least reconstruction error is built and the vectors for interior nodes is composed by a global matrix. Matrix-Vector Recursive Neural Network (MV-RNN) (Socher et al., 2012) assigns matrices for every words so that it could capture the relationship between two children. In Recursive Neural Tensor Networks (RNTN) (Socher et al., 2013b), the composition process is performed on a parse tree in which e"
P15-2009,D14-1076,1,0.849007,"r than its original corresponding news sentence; therefore applying extractive summarization methods directly to sentences in a news article is not enough to generate high quality highlights. Sentence compression aims to retain the most important information of an original sentence in a shorter form while being grammatical at the same time. Previous research has shown the effectiveness of sentence compression for automatic document summarization (Knight and Marcu, 2000; Lin, 2003; Galanis and Androutsopoulos, 2010; Chali and Hasan, 2012; Wang et al., 2013; Li et al., 2013; Qian and Liu, 2013; Li et al., 2014). The compressed summaries can be generated through 50 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 50–56, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics original news sentences and relevant tweets in an unsupervised way. Both of them focused on using tweets to help sentence extraction while we leverage tweet information to guide sentence compression for compressive summary generation. We extend an unsupervised dependency"
P15-2009,C12-1029,0,0.0185575,"y reduce reader’s information load. A highlight sentence is usually much shorter than its original corresponding news sentence; therefore applying extractive summarization methods directly to sentences in a news article is not enough to generate high quality highlights. Sentence compression aims to retain the most important information of an original sentence in a shorter form while being grammatical at the same time. Previous research has shown the effectiveness of sentence compression for automatic document summarization (Knight and Marcu, 2000; Lin, 2003; Galanis and Androutsopoulos, 2010; Chali and Hasan, 2012; Wang et al., 2013; Li et al., 2013; Qian and Liu, 2013; Li et al., 2014). The compressed summaries can be generated through 50 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 50–56, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics original news sentences and relevant tweets in an unsupervised way. Both of them focused on using tweets to help sentence extraction while we leverage tweet information to guide sentence compressio"
P15-2009,P06-1048,0,0.0250579,"subsequent compression component. Framework 2.2 We adopt a pipeline approach for compressive news highlights generation. The framework integrates a sentence extraction component and a post-sentence compression component. Each is described below. 2.1 tfw,x tfw,y (idfw )2 qP 2 2 ∈x (tfwi ,x idfwi ) × w ∈y (tfwi ,y idfwi ) P sim(x, y) = qP Dependency Tree Based Sentence Compression We use an unsupervised dependency tree based compression framework (Filippova and Strube, 2008) as our baseline. This method achieved a higher F-score (Riezler et al., 2003) than other systems on the Edinburgh corpus (Clarke and Lapata, 2006). We will introduce the baseline in this part and describe our extended model that leverages tweet information in the next subsection. The sentence compression task can be defined as follows: given a sentence s, consisting of words w1 , w2 , ..., wm , identify a subset of the words of s, such that it is grammatical and preserves essential information of s. In the baseline framework, a dependency graph for an original sentence is first generated and then the compression is done by deleting edges of the dependency graph. The goal is to find a subtree with the highest score: X f (X) = xe × winf o"
P15-2009,W03-1101,0,0.0224033,"the gist of the document, and can dramatically reduce reader’s information load. A highlight sentence is usually much shorter than its original corresponding news sentence; therefore applying extractive summarization methods directly to sentences in a news article is not enough to generate high quality highlights. Sentence compression aims to retain the most important information of an original sentence in a shorter form while being grammatical at the same time. Previous research has shown the effectiveness of sentence compression for automatic document summarization (Knight and Marcu, 2000; Lin, 2003; Galanis and Androutsopoulos, 2010; Chali and Hasan, 2012; Wang et al., 2013; Li et al., 2013; Qian and Liu, 2013; Li et al., 2014). The compressed summaries can be generated through 50 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 50–56, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics original news sentences and relevant tweets in an unsupervised way. Both of them focused on using tweets to help sentence extraction while"
P15-2009,W04-1013,0,0.00405067,"performs the corresponding original baseline, LexRank and HGRW. • The improvement obtained by LexRank+SC+both compared to LexRank is more promising than that obtained by HGRW+SC+both compared to HGRW. This may be because HGRW has used tweet information already, and leaves limited room for improvement for the sentence compression model when using the same source of information. Table 2: Overall Performance. Bold: the best value in each group in terms of different metrics. Following (Wei and Gao, 2014), we output 4 sentences for each news article as the highlights and report the ROUGE-1 scores (Lin, 2004) using human-generated highlights as the reference. The sentence compression rates are set to 0.8 for short sentences containing fewer than 9 words, and 0.5 for long sentences with more than 9 words, following (Filippova and Strube, 2008). We empirically use 0.8 for α, β and  such that tweets have more impact for both sentence selection and compression. We leveraged The New York Times Annotated Corpus (LDC Catalog No: LDC2008T19) as the background news corpus. It has both the original news articles and human generated summaries. The Stanford Parser4 is used to obtain dependency trees. The bac"
P15-2009,D13-1156,1,0.858019,"usually much shorter than its original corresponding news sentence; therefore applying extractive summarization methods directly to sentences in a news article is not enough to generate high quality highlights. Sentence compression aims to retain the most important information of an original sentence in a shorter form while being grammatical at the same time. Previous research has shown the effectiveness of sentence compression for automatic document summarization (Knight and Marcu, 2000; Lin, 2003; Galanis and Androutsopoulos, 2010; Chali and Hasan, 2012; Wang et al., 2013; Li et al., 2013; Qian and Liu, 2013; Li et al., 2014). The compressed summaries can be generated through 50 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 50–56, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics original news sentences and relevant tweets in an unsupervised way. Both of them focused on using tweets to help sentence extraction while we leverage tweet information to guide sentence compression for compressive summary generation. We extend an unsup"
P15-2009,D13-1155,0,0.114691,"elevant tweets. For a tweet winf o (e) = 51 Psummary (n) Particle (n) (4) wsyn (e) = P (l|h) PrelevantT (n) and PbackgroundT (n) are the unigram probabilities of word n in two language models trained on the relevant tweet dataset and a background tweet dataset respectively. The new syntactic importance score is: (5) where Psummary (n) and Particle (n) are the unigram probabilities of word n in the two language models trained on human generated summaries and the original articles respectively. P (l|h) is the conditional probability of label l given head h. Note that here we use the formula in (Filippova and Altun, 2013) for winf o (e), which was shown to be more effective for sentence compression than the original formula in (Filippova and Strube, 2008). The optimization problem can be solved under the tree structure and length constraints by integer linear programming1 . Given that L is the maximum number of words permitted for the compression, the length constraint is simply represented as: X xe ≤ L (6) T wsyn (e) = 3 Setup We evaluate our pipeline news highlights generation framework on a public corpus based on CNN/USAToday news (Wei and Gao, 2014). This corpus was constructed via an event-oriented strate"
P15-2009,N03-1026,0,0.0556107,"ut the top news sentences as the highlights, and the input to the subsequent compression component. Framework 2.2 We adopt a pipeline approach for compressive news highlights generation. The framework integrates a sentence extraction component and a post-sentence compression component. Each is described below. 2.1 tfw,x tfw,y (idfw )2 qP 2 2 ∈x (tfwi ,x idfwi ) × w ∈y (tfwi ,y idfwi ) P sim(x, y) = qP Dependency Tree Based Sentence Compression We use an unsupervised dependency tree based compression framework (Filippova and Strube, 2008) as our baseline. This method achieved a higher F-score (Riezler et al., 2003) than other systems on the Edinburgh corpus (Clarke and Lapata, 2006). We will introduce the baseline in this part and describe our extended model that leverages tweet information in the next subsection. The sentence compression task can be defined as follows: given a sentence s, consisting of words w1 , w2 , ..., wm , identify a subset of the words of s, such that it is grammatical and preserves essential information of s. In the baseline framework, a dependency graph for an original sentence is first generated and then the compression is done by deleting edges of the dependency graph. The go"
P15-2009,W08-1105,0,0.386495,"DF value. Although both types of nodes can be ranked in this framework, we only output the top news sentences as the highlights, and the input to the subsequent compression component. Framework 2.2 We adopt a pipeline approach for compressive news highlights generation. The framework integrates a sentence extraction component and a post-sentence compression component. Each is described below. 2.1 tfw,x tfw,y (idfw )2 qP 2 2 ∈x (tfwi ,x idfwi ) × w ∈y (tfwi ,y idfwi ) P sim(x, y) = qP Dependency Tree Based Sentence Compression We use an unsupervised dependency tree based compression framework (Filippova and Strube, 2008) as our baseline. This method achieved a higher F-score (Riezler et al., 2003) than other systems on the Edinburgh corpus (Clarke and Lapata, 2006). We will introduce the baseline in this part and describe our extended model that leverages tweet information in the next subsection. The sentence compression task can be defined as follows: given a sentence s, consisting of words w1 , w2 , ..., wm , identify a subset of the words of s, such that it is grammatical and preserves essential information of s. In the baseline framework, a dependency graph for an original sentence is first generated and"
P15-2009,N10-1131,0,0.0613254,"f the document, and can dramatically reduce reader’s information load. A highlight sentence is usually much shorter than its original corresponding news sentence; therefore applying extractive summarization methods directly to sentences in a news article is not enough to generate high quality highlights. Sentence compression aims to retain the most important information of an original sentence in a shorter form while being grammatical at the same time. Previous research has shown the effectiveness of sentence compression for automatic document summarization (Knight and Marcu, 2000; Lin, 2003; Galanis and Androutsopoulos, 2010; Chali and Hasan, 2012; Wang et al., 2013; Li et al., 2013; Qian and Liu, 2013; Li et al., 2014). The compressed summaries can be generated through 50 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 50–56, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics original news sentences and relevant tweets in an unsupervised way. Both of them focused on using tweets to help sentence extraction while we leverage tweet information to gu"
P15-2009,P13-1136,0,0.0247074,"mation load. A highlight sentence is usually much shorter than its original corresponding news sentence; therefore applying extractive summarization methods directly to sentences in a news article is not enough to generate high quality highlights. Sentence compression aims to retain the most important information of an original sentence in a shorter form while being grammatical at the same time. Previous research has shown the effectiveness of sentence compression for automatic document summarization (Knight and Marcu, 2000; Lin, 2003; Galanis and Androutsopoulos, 2010; Chali and Hasan, 2012; Wang et al., 2013; Li et al., 2013; Qian and Liu, 2013; Li et al., 2014). The compressed summaries can be generated through 50 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 50–56, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics original news sentences and relevant tweets in an unsupervised way. Both of them focused on using tweets to help sentence extraction while we leverage tweet information to guide sentence compression for compressive s"
P15-2009,C14-1083,1,0.8058,"er together with their comments. The availability of cross-media information provides new opportunities for traditional tasks of Natural Language Processing (Zhao et al., 2011; Subaˇsi´c and Berendt, 2011; Gao et al., 2012; Kothari et al., 2013; ˇ Stajner et al., 2013). In this paper, we propose to use relevant tweets of a news article to guide the sentence compression process in a pipeline framework for generating compressive news highlights. This is a pioneer study for using such parallel data to guide sentence compression for document summarization. Our work shares some similar ideas with (Wei and Gao, 2014; Wei and Gao, 2015). They also attempted to use tweets to help news highlights generation. Wei and Gao (2014) derived external features based on the relevant tweet collection to assist the ranking of the original sentences for extractive summarization in a fashion of supervised machine learning. Wei and Gao (2015) proposed a graph-based approach to simultaneously rank the We explore using relevant tweets of a given news article to help sentence compression for generating compressive news highlights. We extend an unsupervised dependency-tree based sentence compression approach by incorporating"
P15-2009,D13-1047,1,0.889893,"Missing"
P15-2047,P03-1054,0,0.0500541,"output. Parameters are learned using the back-propagation method (Rumelhart et al., 1988). 4 Experiments We compare DepNN against multiple baselines on SemEval-2010 dataset (Hendrickx et al., 2010). The training set includes 8000 sentences, and the test set includes 2717 sentences. There are 9 287 Model relation types, and each type has two directions. Instances which don’t fall in any of these classes are labeled as Other. The official evaluation metric is the macro-averaged F1-score (excluding Other) and the direction is considered. We use dependency trees generated by the Stanford Parser (Klein and Manning, 2003) with the collapsed option. 4.1 SVM MV-RNN CNN FCM DT-RNN DepNN Contributions of different components baseline (Path words) +Depedency relations +Attached subtrees +Lexical features 50-d 73.8 80.3 81.2 82.7 F1 200-d 75.5 81.8 82.8 83.6 We start with a baseline model using a CNN with only the words on the shortest path. We then add dependency relations and attached subtrees. The results indicate that both parts are effective for relation classification. The rich linguistic information embedded in the dependency relations and subtrees can on one hand, help distinguish different functions of the"
P15-2047,N07-2032,0,0.0416795,"Microsoft Research, Beijing, China 4 Computer Science Department, Rensselaer Polytechnic Institute, Troy, NY, USA {cs-ly, lisujian, wanghf}@pku.edu.cn {furu, mingzhou}@microsoft.com jih@rpi.edu Abstract in different ways. Culotta and Sorensen (2004) designed a dependency tree kernel and attached more information including Part-of-Speech tag, chunking tag of each node in the tree. Interestingly, Bunescu and Mooney (2005) provided an important insight that the shortest path between two entities in a dependency graph concentrates most of the information for identifying the relation between them. Nguyen et al. (2007) developed these ideas by analyzing multiple subtrees with the guidance of pre-extracted keywords. Previous work showed that the most useful dependency information in relation classification includes the shortest dependency path and dependency subtrees. These two kinds of information serve different functions and their collaboration can boost the performance of relation classification (see Section 2 for detailed examples). However, how to uniformly and efficiently combine these two components is still an open problem. In this paper, we propose a novel structure named Augmented Dependency Path"
P15-2047,S10-1057,0,0.346496,"ve for relation classification. The rich linguistic information embedded in the dependency relations and subtrees can on one hand, help distinguish different functions of the same word, and on the other hand infer an unseen word’s role in the sentence. Finally, the lexical features are added and DepNN achieves state-of-the-art results. Comparison with Baselines In this subsection, we compare DepNN with several baseline relation classification approaches. Here, DepNN and the baselines are all based on the 200-d embeddings trained on Gigaword due to the larger corpus and higher dimensions. SVM (Rink and Harabagiu, 2010): This is the top performed system in SemEval-2010. It utilizes many external corpora to extract features from the sentence to build an SVM classifier. 1 82.2 81.82 82.7 83.0 73.1 83.0 83.6 MV-RNN (Socher et al., 2012): This model finds the path between the two entities in the constituent parse tree and then learns the distributed representation of its highest node with a matrix for each word to make the compositions specific. CNN: Zeng et al. (2014) build a convolutional model over the tokens of a sentence to learn the sentence level feature vector. It uses a special position vector that indi"
P15-2047,D12-1110,0,0.0907578,"word’s role in the sentence. Finally, the lexical features are added and DepNN achieves state-of-the-art results. Comparison with Baselines In this subsection, we compare DepNN with several baseline relation classification approaches. Here, DepNN and the baselines are all based on the 200-d embeddings trained on Gigaword due to the larger corpus and higher dimensions. SVM (Rink and Harabagiu, 2010): This is the top performed system in SemEval-2010. It utilizes many external corpora to extract features from the sentence to build an SVM classifier. 1 82.2 81.82 82.7 83.0 73.1 83.0 83.6 MV-RNN (Socher et al., 2012): This model finds the path between the two entities in the constituent parse tree and then learns the distributed representation of its highest node with a matrix for each word to make the compositions specific. CNN: Zeng et al. (2014) build a convolutional model over the tokens of a sentence to learn the sentence level feature vector. It uses a special position vector that indicates the relative distances of current input word to two marked entities. FCM (Yu et al., 2014): FCM decomposes the sentence into substructures and extracts features for each of them, forming substructure embeddings."
P15-2047,Q14-1017,0,0.023951,"n learns the distributed representation of its highest node with a matrix for each word to make the compositions specific. CNN: Zeng et al. (2014) build a convolutional model over the tokens of a sentence to learn the sentence level feature vector. It uses a special position vector that indicates the relative distances of current input word to two marked entities. FCM (Yu et al., 2014): FCM decomposes the sentence into substructures and extracts features for each of them, forming substructure embeddings. These embeddings are combined by sumpooling and input into a sof tmax classifier. DT-RNN (Socher et al., 2014) : This is an RNN for modeling dependency trees. It combines node’s word embedding with its children through a linear combination but not a subtree embedding. We adapt the augmented dependency path into a dependency subtree and apply DT-RNN. As shown in Table 2, DepNN achieves the best result (83.6) using NER features. WordNet features can also improve the performance of DepNN, but not as obvious as NER. Yu et al. (2014) had similar observations, since the larger number of WordNet tags may cause overfitting. SVM achieves a comparable result, though the quality of feature engineering highly rel"
P15-2047,H05-1091,0,0.250704,"n Li1,2 Heng Ji4 Ming Zhou3 Houfeng Wang1,2 1 Key Laboratory of Computational Linguistics, Peking University, MOE, China 2 Collaborative Innovation Center for Language Ability, Xuzhou, Jiangsu, China 3 Microsoft Research, Beijing, China 4 Computer Science Department, Rensselaer Polytechnic Institute, Troy, NY, USA {cs-ly, lisujian, wanghf}@pku.edu.cn {furu, mingzhou}@microsoft.com jih@rpi.edu Abstract in different ways. Culotta and Sorensen (2004) designed a dependency tree kernel and attached more information including Part-of-Speech tag, chunking tag of each node in the tree. Interestingly, Bunescu and Mooney (2005) provided an important insight that the shortest path between two entities in a dependency graph concentrates most of the information for identifying the relation between them. Nguyen et al. (2007) developed these ideas by analyzing multiple subtrees with the guidance of pre-extracted keywords. Previous work showed that the most useful dependency information in relation classification includes the shortest dependency path and dependency subtrees. These two kinds of information serve different functions and their collaboration can boost the performance of relation classification (see Section 2"
P15-2047,I08-2119,0,0.124479,"Missing"
P15-2047,C14-1220,0,0.431582,"re, DepNN and the baselines are all based on the 200-d embeddings trained on Gigaword due to the larger corpus and higher dimensions. SVM (Rink and Harabagiu, 2010): This is the top performed system in SemEval-2010. It utilizes many external corpora to extract features from the sentence to build an SVM classifier. 1 82.2 81.82 82.7 83.0 73.1 83.0 83.6 MV-RNN (Socher et al., 2012): This model finds the path between the two entities in the constituent parse tree and then learns the distributed representation of its highest node with a matrix for each word to make the compositions specific. CNN: Zeng et al. (2014) build a convolutional model over the tokens of a sentence to learn the sentence level feature vector. It uses a special position vector that indicates the relative distances of current input word to two marked entities. FCM (Yu et al., 2014): FCM decomposes the sentence into substructures and extracts features for each of them, forming substructure embeddings. These embeddings are combined by sumpooling and input into a sof tmax classifier. DT-RNN (Socher et al., 2014) : This is an RNN for modeling dependency trees. It combines node’s word embedding with its children through a linear combinat"
P15-2047,P05-1053,0,\N,Missing
P15-2047,W08-1301,0,\N,Missing
P15-2047,P04-1054,0,\N,Missing
P15-2047,D14-1070,0,\N,Missing
P15-2047,P06-1104,0,\N,Missing
P15-2093,P14-2037,0,0.339882,"Missing"
P15-2093,2005.mtsummit-papers.11,0,0.131041,"Missing"
P15-2093,D13-1167,0,0.0174992,"Missing"
P15-2093,E14-1049,0,0.229795,"zy, liuyang2011, sms}@tsinghua.edu.cn Abstract transfer models across languages. This is especially important for those low-resource languages, where it allows one to develop accurate word representations of one language by exploiting the abundant textual resources in another language, e.g., English, which has a high resource density. The joint-space models are not only technically plausible, but also useful for cross-lingual model transfer. Further, studies have shown that using cross-lingual correlation can improve the quality of word representations trained solely with monolingual corpora (Faruqui and Dyer, 2014). Defining a cross-lingual learning objective is crucial at the core of the joint-space model. Hermann and Blunsom (2014) and Chandar A P et al. (2014) tried to calculate parallel sentence (or document) representations and to minimize the differences between the semantically equivalent pairs. These methods are useful in capturing semantic information carried by high-level units (such as phrases and beyond) and usually do not rely on word alignments. However, they suffer from reduced accuracy for representing rare tokens, whose semantic information may not be well generalized. In these cases, f"
P15-2093,D14-1162,0,0.103768,"for cross-lingual word embeddings are very desirable, as language-invariant semantic features can be generalized to make it easy to 567 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 567–572, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics 2 Ltotal = li l1 l1 ?? ? ≈ ?? ⋅ ??? ? bilingual relations and constraints li li P · j,k Xjk Xjk P P li li , j Xjk · k Xjk where X li is the matrix of word-context co-occurrence counts. As Pennington et al. (2014), we add separate terms blwi j , blcik for each word and context to absorb the effect of any possible word-specific biases. We also add an additional matrix bias bli for the ease of sharing embeddings among matrices. The loss function is written as the sum of the weighted square error, i Llmono = X  2 li li f (Xjk ) wjli · clki + blwi j + blcik + bli − Mjk , j,k (2) where we choose the same weighting function as the GloVe model to place less confidence on those word-context pairs with rare occurrences, ( f (x) = (x/xmax )α 1 if x &lt; xmax . otherwise (3) li Notice that we only have to optimize"
P15-2093,D11-1014,0,0.0441177,"e cross-lingual constraints can be derived from parallel corpora, with or without word alignments. Empirical results on a task of cross-lingual document classification show that our method is effective to encode cross-lingual knowledge as constraints for cross-lingual word embeddings. 1 Introduction Word embeddings allow one to represent words in a continuous vector space, which characterizes the lexico-semanic relations among words. In many NLP tasks, they prove to be high-quality features, successful applications of which include language modelling (Bengio et al., 2003), sentiment analysis (Socher et al., 2011) and word sense discrimination (Huang et al., 2012). Like words having synonyms in the same language, there are also word pairs across languages which share resembling semantic properties. Mikolov et al. (2013a) observed a strong similarity of the geometric arrangements of corresponding concepts between the vector spaces of different languages, and suggested that a crosslingual mapping between the two vector spaces is technically plausible. In the meantime, the jointspace models for cross-lingual word embeddings are very desirable, as language-invariant semantic features can be generalized to"
P15-2093,P14-1006,0,0.033965,"e low-resource languages, where it allows one to develop accurate word representations of one language by exploiting the abundant textual resources in another language, e.g., English, which has a high resource density. The joint-space models are not only technically plausible, but also useful for cross-lingual model transfer. Further, studies have shown that using cross-lingual correlation can improve the quality of word representations trained solely with monolingual corpora (Faruqui and Dyer, 2014). Defining a cross-lingual learning objective is crucial at the core of the joint-space model. Hermann and Blunsom (2014) and Chandar A P et al. (2014) tried to calculate parallel sentence (or document) representations and to minimize the differences between the semantically equivalent pairs. These methods are useful in capturing semantic information carried by high-level units (such as phrases and beyond) and usually do not rely on word alignments. However, they suffer from reduced accuracy for representing rare tokens, whose semantic information may not be well generalized. In these cases, finer-grained information at lexical level, such as aligned word pairs, dictionaries, and word translation probabilities,"
P15-2093,P12-1092,0,0.0672644,"llel corpora, with or without word alignments. Empirical results on a task of cross-lingual document classification show that our method is effective to encode cross-lingual knowledge as constraints for cross-lingual word embeddings. 1 Introduction Word embeddings allow one to represent words in a continuous vector space, which characterizes the lexico-semanic relations among words. In many NLP tasks, they prove to be high-quality features, successful applications of which include language modelling (Bengio et al., 2003), sentiment analysis (Socher et al., 2011) and word sense discrimination (Huang et al., 2012). Like words having synonyms in the same language, there are also word pairs across languages which share resembling semantic properties. Mikolov et al. (2013a) observed a strong similarity of the geometric arrangements of corresponding concepts between the vector spaces of different languages, and suggested that a crosslingual mapping between the two vector spaces is technically plausible. In the meantime, the jointspace models for cross-lingual word embeddings are very desirable, as language-invariant semantic features can be generalized to make it easy to 567 Proceedings of the 53rd Annual"
P15-2093,D13-1141,0,0.17614,"ly on word alignments. However, they suffer from reduced accuracy for representing rare tokens, whose semantic information may not be well generalized. In these cases, finer-grained information at lexical level, such as aligned word pairs, dictionaries, and word translation probabilities, is considered to be helpful. Koˇcisk`y et al. (2014) integrated word aligning process and word embedding in machine translation models. This method makes full use of parallel corpora and produces high-quality word alignments. However, it is unable to exploit the richer monolingual corpora. On the other hand, Zou et al. (2013) and Faruqui and Dyer (2014) learnt word embeddings of different languages in separate spaces with monolingual corpora and projected the embeddings into a joint space, but they can only capture linear transformation. In this paper, we address the above challenges with a framework of matrix co-factorization. We A joint-space model for cross-lingual distributed representations generalizes language-invariant semantic features. In this paper, we present a matrix cofactorization framework for learning cross-lingual word embeddings. We explicitly define monolingual training objectives in the form of"
P15-2093,C12-1089,0,0.655529,"tance, distance(wjl1 , wkl2 ) = ||wjl1 − wkl2 ||2 . Notice that similar to the monolingual objective, we may optimize for only those sim(j, k) 6= 0, which is efficient as the matrix of translation probabilities or dictionary is sparse. We call this method CLSim. 5 Experiments To evaluate the quality of the relatedness between words in different languages, we induce the task of cross-lingual document classification for the English-German language pair, where a classifier is trained in one language and later used to classify documents in another. We exactly replicated the experiment settings of Klementiev et al. (2012). Cross-lingual Similarities 5.1 An alternative way to set cross-lingual constraints is to minimize the distances between similar word pairs. Here the semantic similarities can be measured by equivalence in translation, sim(j, k), which is produced by a machine translation system. In this paper, we use the translation probabilities produced by a machine translation system. Minimizing the distances of related words in the two languages weighted by their similarities gives us the cross-lingual objective Data and Training For optimizing the monolingual objectives, We used exactly the same subset"
P15-2093,C14-1048,0,\N,Missing
P15-2093,D10-1025,0,\N,Missing
P15-2093,P14-2095,0,\N,Missing
P15-2093,D13-1168,0,\N,Missing
P15-2109,P07-1106,0,0.019582,"ocess. Introduction Unlike English text in which sentences are sequences of words delimited by white spaces, in Chinese text, sentences are usually represented and stored as strings of Chinese characters without similar natural delimiters. To find the basic language units, i.e. words, segmentation is a necessary initial step for Chinese language processing. Currently most of state-of-the-art methods for Chinese word segmentation (CWS) are based on supervised learning, which depend on large scale annotated corpus. These supervised methods obtain high accuracies on newswire (Xue and Shen, 2003; Zhang and Clark, 2007; Jiang et al., 2009; Zhao et al., 2010; Sun and Xu, 2011). However, Figure 1: Typical Chinese Pinyin input method (Sogou-Pinyin). The typical way to type in Chinese words is in a sequential manner (Wang et al., 2001). iRearch (2009) showed that Pinyin input methods have the biggest share of Chinese speakers. We take one of them for example. Suppose users want to type in Chinese word “ 今 天 (today)”. Firstly, they mentally generate and physically type in corresponding Pinyin “jintian”. Then, a Chinese Pinyin input method displays a list of Chinese homophones, as shown in Figure 1. Finally, user"
P15-2109,P09-1059,0,0.025286,"ike English text in which sentences are sequences of words delimited by white spaces, in Chinese text, sentences are usually represented and stored as strings of Chinese characters without similar natural delimiters. To find the basic language units, i.e. words, segmentation is a necessary initial step for Chinese language processing. Currently most of state-of-the-art methods for Chinese word segmentation (CWS) are based on supervised learning, which depend on large scale annotated corpus. These supervised methods obtain high accuracies on newswire (Xue and Shen, 2003; Zhang and Clark, 2007; Jiang et al., 2009; Zhao et al., 2010; Sun and Xu, 2011). However, Figure 1: Typical Chinese Pinyin input method (Sogou-Pinyin). The typical way to type in Chinese words is in a sequential manner (Wang et al., 2001). iRearch (2009) showed that Pinyin input methods have the biggest share of Chinese speakers. We take one of them for example. Suppose users want to type in Chinese word “ 今 天 (today)”. Firstly, they mentally generate and physically type in corresponding Pinyin “jintian”. Then, a Chinese Pinyin input method displays a list of Chinese homophones, as shown in Figure 1. Finally, users visually search th"
P15-2109,J09-4006,0,0.0153896,"he performance can drop severely when the test data shift from newswire to blogs, computer forums, and Internet literature (Liu and Zhang, 2012;).Supervised approaches often have a high requirement on the quality and quantity of annotated corpus, which is always not easy to build. As a result, many previous methods utilize the information of free data which contain limited but useful segmentation information over the Internet, including large-scale unlabeled data, domain-specific lexicons and semi-annotated web pages such as Wikipedia. There has been work on making use of both unlabeled data (Li and Sun, 2009; Sun and Xu, 2011; Wang et al., 2011; Qiu et al., 2014) and Wikipedia (Jiang et al., 2013; Liu et al., 2014;) to improve segmentation. But none of them notice the segmentation information produced by users while typing Chinese. Chinese is unique due to its logographic writing system. Chinese users cannot directly type in Chinese words using a QWERTY keyboard. Input methods have been proposed to assist users to type in Chinese words (Chen, 1997). Substantial information has been produced, but not recorded and stored during text typing process. Introduction Unlike English text in which sentence"
P15-2109,C12-2073,1,0.846605,"be used for improving the performance of supervised word segmentation model in out-of-domain. Experiments show that a classification model combined with a voting mechanism can reliably identify the high-quality NTAs texts that are more readily available labeled corpus. Furthermore, the NTAs might be particularly useful to deal with out-of-vocabulary (OOV) words such as proper names and neo-logisms. 1 manually annotated training data mostly come from the news domain, and the performance can drop severely when the test data shift from newswire to blogs, computer forums, and Internet literature (Liu and Zhang, 2012;).Supervised approaches often have a high requirement on the quality and quantity of annotated corpus, which is always not easy to build. As a result, many previous methods utilize the information of free data which contain limited but useful segmentation information over the Internet, including large-scale unlabeled data, domain-specific lexicons and semi-annotated web pages such as Wikipedia. There has been work on making use of both unlabeled data (Li and Sun, 2009; Sun and Xu, 2011; Wang et al., 2011; Qiu et al., 2014) and Wikipedia (Jiang et al., 2013; Liu et al., 2014;) to improve segme"
P15-2109,D14-1093,0,0.0259639,"net literature (Liu and Zhang, 2012;).Supervised approaches often have a high requirement on the quality and quantity of annotated corpus, which is always not easy to build. As a result, many previous methods utilize the information of free data which contain limited but useful segmentation information over the Internet, including large-scale unlabeled data, domain-specific lexicons and semi-annotated web pages such as Wikipedia. There has been work on making use of both unlabeled data (Li and Sun, 2009; Sun and Xu, 2011; Wang et al., 2011; Qiu et al., 2014) and Wikipedia (Jiang et al., 2013; Liu et al., 2014;) to improve segmentation. But none of them notice the segmentation information produced by users while typing Chinese. Chinese is unique due to its logographic writing system. Chinese users cannot directly type in Chinese words using a QWERTY keyboard. Input methods have been proposed to assist users to type in Chinese words (Chen, 1997). Substantial information has been produced, but not recorded and stored during text typing process. Introduction Unlike English text in which sentences are sequences of words delimited by white spaces, in Chinese text, sentences are usually represented and s"
P15-2109,D11-1090,0,0.0459913,"drop severely when the test data shift from newswire to blogs, computer forums, and Internet literature (Liu and Zhang, 2012;).Supervised approaches often have a high requirement on the quality and quantity of annotated corpus, which is always not easy to build. As a result, many previous methods utilize the information of free data which contain limited but useful segmentation information over the Internet, including large-scale unlabeled data, domain-specific lexicons and semi-annotated web pages such as Wikipedia. There has been work on making use of both unlabeled data (Li and Sun, 2009; Sun and Xu, 2011; Wang et al., 2011; Qiu et al., 2014) and Wikipedia (Jiang et al., 2013; Liu et al., 2014;) to improve segmentation. But none of them notice the segmentation information produced by users while typing Chinese. Chinese is unique due to its logographic writing system. Chinese users cannot directly type in Chinese words using a QWERTY keyboard. Input methods have been proposed to assist users to type in Chinese words (Chen, 1997). Substantial information has been produced, but not recorded and stored during text typing process. Introduction Unlike English text in which sentences are sequences of"
P15-2109,W03-1728,0,0.0216217,"uring text typing process. Introduction Unlike English text in which sentences are sequences of words delimited by white spaces, in Chinese text, sentences are usually represented and stored as strings of Chinese characters without similar natural delimiters. To find the basic language units, i.e. words, segmentation is a necessary initial step for Chinese language processing. Currently most of state-of-the-art methods for Chinese word segmentation (CWS) are based on supervised learning, which depend on large scale annotated corpus. These supervised methods obtain high accuracies on newswire (Xue and Shen, 2003; Zhang and Clark, 2007; Jiang et al., 2009; Zhao et al., 2010; Sun and Xu, 2011). However, Figure 1: Typical Chinese Pinyin input method (Sogou-Pinyin). The typical way to type in Chinese words is in a sequential manner (Wang et al., 2001). iRearch (2009) showed that Pinyin input methods have the biggest share of Chinese speakers. We take one of them for example. Suppose users want to type in Chinese word “ 今 天 (today)”. Firstly, they mentally generate and physically type in corresponding Pinyin “jintian”. Then, a Chinese Pinyin input method displays a list of Chinese homophones, as shown in"
P15-2109,I11-1035,0,\N,Missing
P15-2109,P13-1075,0,\N,Missing
P15-2109,C14-1109,0,\N,Missing
P15-2109,P11-2085,0,\N,Missing
P16-1008,J93-2003,0,0.0568553,"Missing"
P16-1008,P15-1001,0,0.0941512,"Missing"
P16-1008,N03-1017,0,0.0513269,"Missing"
P16-1008,P07-2045,0,0.0228456,"Missing"
P16-1008,N06-1014,0,0.143259,"Missing"
P16-1008,D15-1166,0,0.112,"Missing"
P16-1008,J07-2003,0,0.0902386,"Missing"
P16-1008,J03-1002,0,0.011396,"Missing"
P16-1008,W14-4012,0,0.186136,"Missing"
P16-1008,D14-1179,0,0.0785675,"Missing"
P16-1008,P05-1066,0,0.0753936,"Missing"
P16-1008,P03-1021,0,0.0087897,"Missing"
P16-1008,P02-1040,0,0.100194,"Missing"
P16-1008,P16-1159,1,0.746732,"Missing"
P16-1008,koen-2004-pharaoh,0,\N,Missing
P16-1008,D08-1060,0,\N,Missing
P16-1008,P05-1033,0,\N,Missing
P16-1008,P09-1065,1,\N,Missing
P16-1008,N13-1073,0,\N,Missing
P16-1008,D11-1125,0,\N,Missing
P16-1008,D13-1052,0,\N,Missing
P16-1097,J93-2003,0,0.117603,"Missing"
P16-1097,2010.iwslt-papers.3,0,0.0182997,"alignment and machine translation evaluations. 2 Background Given a monolingual corpus of source language phrases E = {e(s) }Ss=1 and a monolingual corpus of target language phrases F = {f (t) }Tt=1 , we assume there exists a parallel corpus D = {he(s) , f (t) i|e(s) ↔ f (t) }, where e(s) ↔ f (t) denotes that e(s) and f (t) are translations of each other. As a long sentence in E is usually unlikely to have an translation in F and vise versa, most previous efforts build on the assumption that phrases are more likely to have translational equivalents on the other side (Munteanu and Marcu, 2006; Cettolo et al., 2010; Zhang and Zong, 2013; Dong et al., 2015). Such a set of phrases can be constructed by collecting either constituents of parsed sentences or strings with hyperlinks on webpages (e.g., Wikipedia). Therefore, we assume the two monolingual corpora are readily available and focus on how to extract D from E and F . To address this problem, Dong et al. (2015) introduce a corpus-level latent-variable translation model in a non-parallel scenario: X P (F |E; θ) = P (F, m|E; θ) , (1) | {z } m phrase alignment where m is phrase alignment and θ is a set of model parameters. Each target phrase f (t) is re"
P16-1097,C14-1192,1,0.824319,"− −|f (t) , e(s) ; ← P (← a θ)× − −). δ(→ a ,← a (25) We experimented with collecting counts from both the unidirectional and agreement terms but obtained much worse results than counting only from the agreement term. (26) Counts of target-to-source length and translation models can be calculated in a similar way. The new length probabilities can be obtained by c(J|I; E, F ) . 0 J 0 c(J |I; E, F ) i=0 (s) δ(e, ei ) The new translation probabilities can be obtained by ˆ hs,ti∈m p(J|I) = P I X +σ(f, e, d). s∈{0,1,...,S} This can be cast as a translation retrieval problem (Zhang and Zong, 2013; Dong et al., 2014). Please refer to (Dong et al., 2015) for more details. The target-to-source Viterbi phrase alignment can be calculated similarly. (s) (t) δ(f, fj ) (28) For example, Figure 3 shows two examples of Chinese-to-English and English-to-Chinese word alignments. The shared links are highlighted in 1028 red. Our intuition is that a source phrase and a target phrase are more likely to be translations of each other if the two translation models also agree on word alignment within aligned phrases. 3.3.2 Training Objective and Algorithm The training objective for inner agreement is given by → − ← − Jinne"
P16-1097,D14-1061,0,0.0242113,"Missing"
P16-1097,W04-3208,0,0.103748,"Missing"
P16-1097,P04-1067,0,0.0994235,"Missing"
P16-1097,P08-1088,0,0.0752765,"Missing"
P16-1097,N03-1017,0,0.215573,"Missing"
P16-1097,P07-2045,0,0.0178646,"an extracted parallel corpus can be measured by F1 = 2|D ∩ G|/(|D |+ |G|). 4.1.2 o ← − P (hi, ji|f (t) , e(s) ; θ ) , (31) Alignment Evaluation Data Preparation Although it is appealing to apply our approach to dealing with real-world non-parallel corpora, it is time-consuming and labor-intensive to manually construct a ground truth parallel corpus. Therefore, we follow Dong et al. (2015) to build synthetic E, F , and G to facilitate the evaluation. We first extract a set of parallel phrases from a sentence-level parallel corpus using the stateof-the-art phrase-based translation system Moses (Koehn et al., 2007) and discard low-probability parallel phrases. Then, E and F can be constructed by corrupting the parallel phrase set by 1029 noise C E 0 0 0 10K 0 20K 10K 0 20K 0 10K 10K 20K 20K 0.40 0.35 inner outer no agreement agreement ratio 0.30 0.25 0.20 0.15 1 2 3 4 5 6 iteration 7 8 9 Outer Inner 58.5 41.0 28.3 54.7 50.4 34.9 22.4 61.2 54.4 48.3 43.1 31.4 34.4 23.1 86.5 83.6 80.1 84.9 83.8 80.0 73.6 86.1 83.8 81.2 84.3 83.6 79.7 74.3 10 Figure 4: Comparison of agreement ratios on the development set. seed 50 100 500 1,000 C→E 4.1 5.1 7.5 22.4 E→C 4.8 5.5 8.4 23.1 Outer 60.8 65.6 70.4 73.6 Inner 66.2"
P16-1097,P93-1003,0,0.628466,"Missing"
P16-1097,N06-1014,0,0.486435,"ter running for 70 iterations. In addition, their approach seems prone to be affected by noisy data in non-parallel corpora as the accuracy drops significantly with the increase of noise. Since asymmetric word alignment and phrase alignment models are usually complementary, it is natural to combine them to make more accurate predictions. In this work, we propose to in1024 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1024–1033, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics troduce agreement-based learning (Liang et al., 2006; Liang et al., 2008) into extracting parallel lexicons and phrases from non-parallel corpora. Based on the latent-variable model proposed by Dong et al. (2015), we propose two kinds of loss functions to take into account the agreement between both phrase alignment and word alignment in two directions. As the inference is intractable, we resort to a Viterbi EM algorithm to train the two models efficiently. Experiments on the Chinese-English dataset show that agreementbased learning is more robust to noisy data and leads to substantial improvements in phrase alignment and machine translation ev"
P16-1097,W02-1018,0,0.132384,"Missing"
P16-1097,W97-0311,0,0.350534,"Missing"
P16-1097,P06-1011,0,0.0359141,"al improvements in phrase alignment and machine translation evaluations. 2 Background Given a monolingual corpus of source language phrases E = {e(s) }Ss=1 and a monolingual corpus of target language phrases F = {f (t) }Tt=1 , we assume there exists a parallel corpus D = {he(s) , f (t) i|e(s) ↔ f (t) }, where e(s) ↔ f (t) denotes that e(s) and f (t) are translations of each other. As a long sentence in E is usually unlikely to have an translation in F and vise versa, most previous efforts build on the assumption that phrases are more likely to have translational equivalents on the other side (Munteanu and Marcu, 2006; Cettolo et al., 2010; Zhang and Zong, 2013; Dong et al., 2015). Such a set of phrases can be constructed by collecting either constituents of parsed sentences or strings with hyperlinks on webpages (e.g., Wikipedia). Therefore, we assume the two monolingual corpora are readily available and focus on how to extract D from E and F . To address this problem, Dong et al. (2015) introduce a corpus-level latent-variable translation model in a non-parallel scenario: X P (F |E; θ) = P (F, m|E; θ) , (1) | {z } m phrase alignment where m is phrase alignment and θ is a set of model parameters. Each tar"
P16-1097,P12-1017,0,0.0336177,"Missing"
P16-1097,J03-1002,0,0.0293945,"Missing"
P16-1097,P02-1040,0,0.0960556,"allel corpora consists of 2.65M Chinese phrases and 3.67M English phrases extracted from LDC news articles. We use a small out-domain parallel corpus extracted from financial news of FTChina which contains 10K phrase pairs. The task is to extract a parallel corpus from in-domain non-parallel corpora starting from a small out-domain parallel corpus. We use the state-of-the-art translation system Moses (Koehn et al., 2007) and evaluate the performance on Chinese-English NIST datasets. The development set is NIST 2006 and the test set is NIST 2005. The evaluation metric is caseinsensitive BLEU4 (Papineni et al., 2002). We use the SRILM toolkit (Stolcke, 2002) to train a 4-gram English language model on a monolingual corpus with 399M English words. Table 4 shows the results. At iteration 0, only the out-domain corpus is used and the BLEU score is 5.61. All methods iteratively extract parallel phrases from non-parallel corpora and enlarge the extracted parallel corpus. We find that agreementbased learning achieves much higher BLEU scores while obtains a smaller parallel corpus as compared with independent learning. One possible reason is that the agreement-based learning rules out most unlikely phrase pairs"
P16-1097,P11-1002,0,0.0291102,"Missing"
P16-1097,H94-1027,0,0.297624,"Missing"
P16-1097,C96-2141,0,0.528128,"irs by encouraging consensus between two models. 5 Conclusion We have presented agreement-based training for learning parallel lexicons and phrases from nonparallel corpora. By modeling the agreement on both phrase alignment and word alignment, our approach achieves significant improvements in both alignment and translation evaluations. In the future, we plan to apply our approach to real-world non-parallel corpora to further verify its effectiveness. It is also interesting to extend the phrase translation model to more sophisticated models such as IBM models 2-5 (Brown et al., 1993) and HMM (Vogel and Ney, 1996). Acknowledgments We sincerely thank the reviewers for their valuable suggestions. We also thank Meng Zhang, Yankai Lin, Shiqi Shen, Meiping Dong and Congyu Fu for their insightful discussions. Yang Liu is sup1031 Iteration 0 1 2 3 4 5 6 7 8 9 10 E→ C 145k 195k 209k 214k 217k 219k 222k 224k 227k 229k Corpus Size C→ E Outer 10k 162k 59k 215k 69k 231k 88k 238k 106k 241k 123k 243k 137k 247k 140k 249k 153k 251k 159k 254k 163k Inner E→ C 73k 101k 132k 159k 181k 197k 207k 220k 233k 239k 8.65 8.82 8.42 8.46 8.87 8.52 8.81 8.71 8.92 8.33 BLEU C→ E Outer 5.61 8.90 13.53 9.47 15.26 9.29 16.88 9.27 17.15"
P16-1097,D13-1168,0,0.0421124,"Missing"
P16-1097,P15-2118,0,0.0311429,"Missing"
P16-1097,1994.amta-1.26,0,0.604428,"Missing"
P16-1097,P13-1140,0,0.303666,"rpora, including parallel sentence and lexicon extraction via bootstrapping (Fung and Cheung, 2004), inducing parallel lexicons via canonical correlation analysis (Haghighi ∗ Corresponding author: Yang Liu. et al., 2008), training IBM models on monolingual corpora as decipherment (Ravi and Knight, 2011; Nuhn et al., 2012; Dou et al., 2014), and deriving parallel lexicons from bilingual word embeddings (Vuli´c and Moens, 2013; Mikolov et al., 2013; Vuli´c and Moens, 2015). Recently, a number of authors have turned to a more challenging task: learning parallel phrases from non-parallel corpora (Zhang and Zong, 2013; Dong et al., 2015). Zhang and Zong (2013) present a method for retrieving parallel phrases from non-parallel corpora using a seed parallel lexicon. Dong et al. (2015) continue this line of research to further introduce an iterative approach to joint learning of parallel lexicons and phrases. They introduce a corpus-level latentvariable translation model in a non-parallel scenario and develop a training algorithm that alternates between (1) using a parallel lexicon to extract parallel phrases from non-parallel corpora and (2) using the extracted parallel phrases to enlarge the parallel lexico"
P16-1159,J93-2003,0,0.0998959,"Missing"
P16-1159,P05-1033,0,0.394114,"d-to-end neural machine translation (NMT) (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015) has attracted increasing attention from the community. Providing a new paradigm for machine translation, NMT aims at training a single, large neural network that directly transforms a sourcelanguage sentence to a target-language sentence without explicitly modeling latent structures (e.g., word alignment, phrase segmentation, phrase reordering, and SCFG derivation) that are vital in conventional statistical machine translation (SMT) (Brown et al., 1993; Koehn et al., 2003; Chiang, 2005). Current NMT models are based on the encoderdecoder framework (Cho et al., 2014; Sutskever et al., 2014), with an encoder to read and encode a source-language sentence into a vector, from which a decoder generates a target-language sentence. While early efforts encode the input into a ∗ Corresponding author: Yang Liu. fixed-length vector, Bahdanau et al. (2015) advocate the attention mechanism to dynamically generate a context vector for a target word being generated. Although NMT models have achieved results on par with or better than conventional SMT, they still suffer from a major drawback"
P16-1159,P14-1066,0,0.055646,"ages over MLE: 1. Direct optimization with respect to evaluation metrics: MRT introduces evaluation metrics as loss functions and aims to minimize expected loss on the training data. 2. Applicable to arbitrary loss functions: our approach allows arbitrary sentence-level loss functions, which are not necessarily differentiable. 3. Transparent to architectures: MRT does not assume the specific architectures of NMT and can be applied to any end-to-end NMT systems. While MRT has been widely used in conventional SMT (Och, 2003; Smith and Eisner, 2006; He and Deng, 2012) and deep learning based MT (Gao et al., 2014), to the best of our knowledge, this work is the first effort to introduce MRT 1683 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1683–1692, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics into end-to-end NMT. Experiments on a variety of language pairs (Chinese-English, English-French, and English-German) show that MRT leads to significant improvements over MLE on a state-ofthe-art NMT system (Bahdanau et al., 2015). 2 Background Given a source sentence x = x1 , . . . , xm , . . . , xM and a target sentence y"
P16-1159,P12-1031,0,0.0465582,"the training data. MRT has the following advantages over MLE: 1. Direct optimization with respect to evaluation metrics: MRT introduces evaluation metrics as loss functions and aims to minimize expected loss on the training data. 2. Applicable to arbitrary loss functions: our approach allows arbitrary sentence-level loss functions, which are not necessarily differentiable. 3. Transparent to architectures: MRT does not assume the specific architectures of NMT and can be applied to any end-to-end NMT systems. While MRT has been widely used in conventional SMT (Och, 2003; Smith and Eisner, 2006; He and Deng, 2012) and deep learning based MT (Gao et al., 2014), to the best of our knowledge, this work is the first effort to introduce MRT 1683 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1683–1692, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics into end-to-end NMT. Experiments on a variety of language pairs (Chinese-English, English-French, and English-German) show that MRT leads to significant improvements over MLE on a state-ofthe-art NMT system (Bahdanau et al., 2015). 2 Background Given a source sentence x = x1 , ."
P16-1159,P15-1001,0,0.46654,"missing. By optimizing model parameters directly with respect to sentence-level BLEU, RNN SEARCH -MRT seems to be able to generate translations more consistently at the sentence level. 4.7 Results on English-French Translation Table 7 shows the results on English-French translation. We list existing end-to-end NMT systems that are comparable to our system. All these systems use the same subset of the WMT 2014 training corpus and adopt MLE as the training criterion. They differ in network architectures and vocabulary sizes. Our RNN SEARCH -MLE system achieves a BLEU score comparable to that of Jean et al. (2015). RNN SEARCH -MRT achieves the highest BLEU score in this setting even with a vocabulary size smaller than Luong et al. (2015b) and Sutskever et al. (2014). Note that our approach does not assume specific architectures and can in principle be applied to any NMT systems. 4.8 Results on English-German Translation Table 8 shows the results on English-German translation. Our approach still significantly out1689 Source Reference M OSES RNN SEARCH -MLE RNN SEARCH -MRT meiguo daibiao tuan baokuo laizi shidanfu daxue de yi wei zhongguo zhuanjia , liang ming canyuan waijiao zhengce zhuli yiji yi wei fu"
P16-1159,D15-1166,0,0.616781,"se-English, the training data consists of 2.56M pairs of sentences with 67.5M Chinese words and 74.8M English words, respectively. We ˜ ∂ R(θ) used the NIST 2006 dataset as the validation set ∂θi (hyper-parameter optimization and model selec"" S (s) X tion) and the NIST 2002, 2003, 2004, 2005, and ∂P (y|x ; θ)/∂θi =α Ey|x(s) ;θ,α × 2008 datasets as test sets. (s) P (y|x ; θ) s=1  For English-French, to compare with the results ∆(y, y(s) ) − reported by previous work on end-to-end NMT # (Sutskever et al., 2014; Bahdanau et al., 2015;  0 (s) Ey0 |x(s) ;θ,α [∆(y , y )] . (14) Jean et al., 2015; Luong et al., 2015b), we used the same subset of the WMT 2014 training corpus that contains 12M sentence pairs with 304M Since |S(x(s) ) | |Y(x(s) )|, the expectations English words and 348M French words. The conin Eq. (14) can be efficiently calculated by excatenation of news-test 2012 and news-test 2013 plicitly enumerating all candidates in S(x(s) ). In serves as the validation set and news-test 2014 as our experiments, we find that approximating the the test set. full space with 100 samples (i.e., k = 100) works For English-German, to compare with the very well and further increasing sample size does resul"
P16-1159,P15-1002,0,0.727498,"se-English, the training data consists of 2.56M pairs of sentences with 67.5M Chinese words and 74.8M English words, respectively. We ˜ ∂ R(θ) used the NIST 2006 dataset as the validation set ∂θi (hyper-parameter optimization and model selec"" S (s) X tion) and the NIST 2002, 2003, 2004, 2005, and ∂P (y|x ; θ)/∂θi =α Ey|x(s) ;θ,α × 2008 datasets as test sets. (s) P (y|x ; θ) s=1  For English-French, to compare with the results ∆(y, y(s) ) − reported by previous work on end-to-end NMT # (Sutskever et al., 2014; Bahdanau et al., 2015;  0 (s) Ey0 |x(s) ;θ,α [∆(y , y )] . (14) Jean et al., 2015; Luong et al., 2015b), we used the same subset of the WMT 2014 training corpus that contains 12M sentence pairs with 304M Since |S(x(s) ) | |Y(x(s) )|, the expectations English words and 348M French words. The conin Eq. (14) can be efficiently calculated by excatenation of news-test 2012 and news-test 2013 plicitly enumerating all candidates in S(x(s) ). In serves as the validation set and news-test 2014 as our experiments, we find that approximating the the test set. full space with 100 samples (i.e., k = 100) works For English-German, to compare with the very well and further increasing sample size does resul"
P16-1159,P03-1021,0,0.667288,"the expected loss (i.e., risk) on the training data. MRT has the following advantages over MLE: 1. Direct optimization with respect to evaluation metrics: MRT introduces evaluation metrics as loss functions and aims to minimize expected loss on the training data. 2. Applicable to arbitrary loss functions: our approach allows arbitrary sentence-level loss functions, which are not necessarily differentiable. 3. Transparent to architectures: MRT does not assume the specific architectures of NMT and can be applied to any end-to-end NMT systems. While MRT has been widely used in conventional SMT (Och, 2003; Smith and Eisner, 2006; He and Deng, 2012) and deep learning based MT (Gao et al., 2014), to the best of our knowledge, this work is the first effort to introduce MRT 1683 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1683–1692, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics into end-to-end NMT. Experiments on a variety of language pairs (Chinese-English, English-French, and English-German) show that MRT leads to significant improvements over MLE on a state-ofthe-art NMT system (Bahdanau et al., 2015). 2 B"
P16-1159,P02-1040,0,0.11537,". . . , yN , end-to-end NMT directly models the translation probability: P (y|x; θ) = N Y P (yn |x, y<n ; θ), (1) n=1 First, while the models are trained only on the training data distribution, they are used to generate target words on previous model predictions, which can be erroneous, at test time. This is referred to as exposure bias (Ranzato et al., 2015). Second, MLE usually uses the cross-entropy loss focusing on word-level errors to maximize the probability of the next correct word, which might hardly correlate well with corpus-level and sentence-level evaluation metrics such as BLEU (Papineni et al., 2002) and TER (Snover et al., 2006). As a result, it is important to introduce new training algorithms for end-to-end NMT to include model predictions during training and optimize model parameters directly with respect to evaluation metrics. where θ is a set of model parameters and y<n = y1 , . . . , yn−1 is a partial translation. 3 Minimum Risk Training for Neural Predicting the n-th target word can be modeled Machine Translation by using a recurrent neural network: n o P (yn |x, y<n ; θ) ∝ exp q(yn−1 , zn , cn , θ) , (2) Minimum risk training (MRT), which aims to minimize the expected loss on the"
P16-1159,P06-2101,0,0.836096,"ed loss (i.e., risk) on the training data. MRT has the following advantages over MLE: 1. Direct optimization with respect to evaluation metrics: MRT introduces evaluation metrics as loss functions and aims to minimize expected loss on the training data. 2. Applicable to arbitrary loss functions: our approach allows arbitrary sentence-level loss functions, which are not necessarily differentiable. 3. Transparent to architectures: MRT does not assume the specific architectures of NMT and can be applied to any end-to-end NMT systems. While MRT has been widely used in conventional SMT (Och, 2003; Smith and Eisner, 2006; He and Deng, 2012) and deep learning based MT (Gao et al., 2014), to the best of our knowledge, this work is the first effort to introduce MRT 1683 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1683–1692, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics into end-to-end NMT. Experiments on a variety of language pairs (Chinese-English, English-French, and English-German) show that MRT leads to significant improvements over MLE on a state-ofthe-art NMT system (Bahdanau et al., 2015). 2 Background Given a source"
P16-1159,2006.amta-papers.25,0,0.0949686,"ectly models the translation probability: P (y|x; θ) = N Y P (yn |x, y<n ; θ), (1) n=1 First, while the models are trained only on the training data distribution, they are used to generate target words on previous model predictions, which can be erroneous, at test time. This is referred to as exposure bias (Ranzato et al., 2015). Second, MLE usually uses the cross-entropy loss focusing on word-level errors to maximize the probability of the next correct word, which might hardly correlate well with corpus-level and sentence-level evaluation metrics such as BLEU (Papineni et al., 2002) and TER (Snover et al., 2006). As a result, it is important to introduce new training algorithms for end-to-end NMT to include model predictions during training and optimize model parameters directly with respect to evaluation metrics. where θ is a set of model parameters and y<n = y1 , . . . , yn−1 is a partial translation. 3 Minimum Risk Training for Neural Predicting the n-th target word can be modeled Machine Translation by using a recurrent neural network: n o P (yn |x, y<n ; θ) ∝ exp q(yn−1 , zn , cn , θ) , (2) Minimum risk training (MRT), which aims to minimize the expected loss on the training data, has been widel"
P16-1159,D13-1176,0,0.0837584,"on. Unlike conventional maximum likelihood estimation, minimum risk training is capable of optimizing model parameters directly with respect to arbitrary evaluation metrics, which are not necessarily differentiable. Experiments show that our approach achieves significant improvements over maximum likelihood estimation on a state-of-the-art neural machine translation system across various languages pairs. Transparent to architectures, our approach can be applied to more neural networks and potentially benefit more NLP tasks. 1 Introduction Recently, end-to-end neural machine translation (NMT) (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015) has attracted increasing attention from the community. Providing a new paradigm for machine translation, NMT aims at training a single, large neural network that directly transforms a sourcelanguage sentence to a target-language sentence without explicitly modeling latent structures (e.g., word alignment, phrase segmentation, phrase reordering, and SCFG derivation) that are vital in conventional statistical machine translation (SMT) (Brown et al., 1993; Koehn et al., 2003; Chiang, 2005). Current NMT models are based on the encoderdecoder framewo"
P16-1159,D07-1091,0,0.0128786,"s with 91M English words and To build the subset, an alternative to sampling is computing top-k translations. We prefer sampling to comput87M German words. The concatenation of newsing top-k translations for efficiency: sampling is more effitest 2012 and news-test 2013 is used as the validacient and easy-to-implement than calculating k-best lists, estion set and news-test 2014 as the test set. pecially given the extremely parallel architectures of GPUs. 1686 40 35 35 30 30 BLEU score BLEU score 40 25 20 15 10 0 5 10 15 20 25 30 Training time (hours) k=100 k=50 k=25 5 α=1×10 35 0 40 1. M OSES (Koehn and Hoang, 2007): a phrasebased SMT system using minimum error rate training (Och, 2003). 2. RNN SEARCH (Bahdanau et al., 2015): an attention-based NMT system using maximum likelihood estimation. M OSES uses the parallel corpus to train a phrase-based translation model and the target part to train a 4-gram language model using the SRILM toolkit (Stolcke, 2002). 2 The log-linear model Moses uses is trained by the minimum error rate training (MERT) algorithm (Och, 2003) that directly optimizes model parameters with respect to evaluation metrics. RNN SEARCH uses the parallel corpus to train an attention-based ne"
P16-1159,N03-1017,0,0.112266,"duction Recently, end-to-end neural machine translation (NMT) (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015) has attracted increasing attention from the community. Providing a new paradigm for machine translation, NMT aims at training a single, large neural network that directly transforms a sourcelanguage sentence to a target-language sentence without explicitly modeling latent structures (e.g., word alignment, phrase segmentation, phrase reordering, and SCFG derivation) that are vital in conventional statistical machine translation (SMT) (Brown et al., 1993; Koehn et al., 2003; Chiang, 2005). Current NMT models are based on the encoderdecoder framework (Cho et al., 2014; Sutskever et al., 2014), with an encoder to read and encode a source-language sentence into a vector, from which a decoder generates a target-language sentence. While early efforts encode the input into a ∗ Corresponding author: Yang Liu. fixed-length vector, Bahdanau et al. (2015) advocate the attention mechanism to dynamically generate a context vector for a target word being generated. Although NMT models have achieved results on par with or better than conventional SMT, they still suffer from a"
P16-1159,W04-1013,0,0.0217352,"late reinforcement reward while MRT generates multiple samples to calculate the expected risk. Figure 2 indicates that multiple samples potentially increases MRT’s capability of discriminating between diverse candidates and thus benefit translation quality. Our experiments confirm their finding that taking evaluation metrics into account when optimizing model parameters does help to improve sentence-level text generation. More recently, our approach has been successfully applied to summarization (Ayana et al., 2016). They optimize neural networks for headline generation with respect to ROUGE (Lin, 2004) and also achieve significant improvements, confirming the effectiveness and applicability of our approach. 5 6 Related Work Our work originated from the minimum risk training algorithms in conventional statistical machine translation (Och, 2003; Smith and Eisner, 2006; He and Deng, 2012). Och (2003) describes a smoothed error count to allow calculating gradients, which directly inspires us to use a parameter α to adjust the smoothness of the objective function. As neural networks are non-linear, our approach has to minimize the expected loss on the sentence level rather than the loss of 1-bes"
P16-1159,D14-1179,0,\N,Missing
P16-1185,J93-2003,0,0.0781716,"Missing"
P16-1185,P05-1033,0,0.174897,"t show that our approach achieves significant improvements over state-of-the-art SMT and NMT systems. 1 Introduction End-to-end neural machine translation (NMT), which leverages a single, large neural network to directly transform a source-language sentence into a target-language sentence, has attracted increasing attention in recent several years (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015). Free of latent structure design and feature engineering that are critical in conventional statistical machine translation (SMT) (Brown et al., 1993; Koehn et al., 2003; Chiang, 2005), NMT has proven to excel in model∗ Yang Liu is the corresponding author. ing long-distance dependencies by enhancing recurrent neural networks (RNNs) with the gating (Hochreiter and Schmidhuber, 1993; Cho et al., 2014; Sutskever et al., 2014) and attention mechanisms (Bahdanau et al., 2015). However, most existing NMT approaches suffer from a major drawback: they heavily rely on parallel corpora for training translation models. This is because NMT directly models the probability of a target-language sentence given a source-language sentence and does not have a separate language model like SMT"
P16-1185,W14-4012,0,0.0289659,"Missing"
P16-1185,D14-1061,0,0.0330026,"Missing"
P16-1185,P15-1001,0,0.069953,"Missing"
P16-1185,D13-1176,0,0.0583835,"arget and target-to-source translation models serve as the encoder and decoder, respectively. Our approach can not only exploit the monolingual corpora of the target language, but also of the source language. Experiments on the ChineseEnglish dataset show that our approach achieves significant improvements over state-of-the-art SMT and NMT systems. 1 Introduction End-to-end neural machine translation (NMT), which leverages a single, large neural network to directly transform a source-language sentence into a target-language sentence, has attracted increasing attention in recent several years (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015). Free of latent structure design and feature engineering that are critical in conventional statistical machine translation (SMT) (Brown et al., 1993; Koehn et al., 2003; Chiang, 2005), NMT has proven to excel in model∗ Yang Liu is the corresponding author. ing long-distance dependencies by enhancing recurrent neural networks (RNNs) with the gating (Hochreiter and Schmidhuber, 1993; Cho et al., 2014; Sutskever et al., 2014) and attention mechanisms (Bahdanau et al., 2015). However, most existing NMT approaches suffer from a major drawback: they h"
P16-1185,P13-1140,0,0.0205566,"Missing"
P16-1185,E12-1014,0,0.015191,"pora for machine translation and (2) autoencoders in unsupervised and semi-supervised learning. 4.1 Exploiting Monolingual Corpora for Machine Translation Exploiting monolingual corpora for conventional SMT has attracted intensive attention in recent years. Several authors have introduced transductive learning to make full use of monolingual corpora (Ueffing et al., 2007; Bertoldi and Federico, 2009). They use an existing translation model to translate unseen source text, which can be paired with its translations to form a pseudo parallel corpus. This process iterates until convergence. While Klementiev et al. (2012) propose an approach to estimating phrase translation probabilities from monolingual corpora, Zhang and Zong (2013) directly extract parallel phrases from monolingual corpora using retrieval techniques. Another important line of research is to treat translation on monolingual corpora as a decipherment problem (Ravi and Knight, 2011; Dou et al., 2014). 1972 Closely related to Gulccehre et al. (2015) and Sennrich et al. (2015), our approach focuses on learning birectional NMT models via autoencoders on monolingual corpora. The major advantages of our approach are the transparency to network arch"
P16-1185,N03-1017,0,0.145099,"hineseEnglish dataset show that our approach achieves significant improvements over state-of-the-art SMT and NMT systems. 1 Introduction End-to-end neural machine translation (NMT), which leverages a single, large neural network to directly transform a source-language sentence into a target-language sentence, has attracted increasing attention in recent several years (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015). Free of latent structure design and feature engineering that are critical in conventional statistical machine translation (SMT) (Brown et al., 1993; Koehn et al., 2003; Chiang, 2005), NMT has proven to excel in model∗ Yang Liu is the corresponding author. ing long-distance dependencies by enhancing recurrent neural networks (RNNs) with the gating (Hochreiter and Schmidhuber, 1993; Cho et al., 2014; Sutskever et al., 2014) and attention mechanisms (Bahdanau et al., 2015). However, most existing NMT approaches suffer from a major drawback: they heavily rely on parallel corpora for training translation models. This is because NMT directly models the probability of a target-language sentence given a source-language sentence and does not have a separate language"
P16-1185,P07-2045,0,0.0608607,"ts serve as test sets. Each Chinese sentence has four reference translations. For English-to-Chinese translation, we use the NIST datasets in a reverse direction: treating the first English sentence in the four reference translations as a source sentence and the original input Chinese sentence as the single reference translation. The evaluation metric is case-insensitive BLEU (Papineni et al., 2002) as calculated by the multi-bleu.perl script. We compared our approach with two state-ofthe-art SMT and NMT systems: 2. RNN SEARCH (Bahdanau et al., 2015): an attention-based NMT system. 1. M OSES (Koehn et al., 2007): a phrase-based SMT system; For M OSES, we use the default setting to train the phrase-based translation on the parallel corpus and optimize the parameters of log-linear models using the minimum error rate training algorithm (Och, 2003). We use the SRILM toolkit (Stolcke, 2002) to train 4-gram language models. For RNNS EARCH, we use the parallel corpus to train the attention-based neural translation models. We set the vocabulary size of word embeddings to 30K for both Chinese and English. We follow Luong et al. (2015) to address rare words. On top of RNNS EARCH, our approach is capable of tra"
P16-1185,P15-1002,0,0.100715,"N SEARCH (Bahdanau et al., 2015): an attention-based NMT system. 1. M OSES (Koehn et al., 2007): a phrase-based SMT system; For M OSES, we use the default setting to train the phrase-based translation on the parallel corpus and optimize the parameters of log-linear models using the minimum error rate training algorithm (Och, 2003). We use the SRILM toolkit (Stolcke, 2002) to train 4-gram language models. For RNNS EARCH, we use the parallel corpus to train the attention-based neural translation models. We set the vocabulary size of word embeddings to 30K for both Chinese and English. We follow Luong et al. (2015) to address rare words. On top of RNNS EARCH, our approach is capable of training bidirectional attention-based neural translation models on the concatenation of parallel and monolingual corpora. The sample size k is set to 10. We set the hyper-parameter λ1 = 0.1 and 1969 λ2 = 0 when we add the target monolingual corpus, and λ1 = 0 and λ2 = 0.1 for source monolingual corpus incorporation. The threshold of gradient clipping is set to 0.05. The parameters of our model are initialized by the model trained on parallel corpus. 3.2 Effect of Sample Size k As the inference of our approach is intracta"
P16-1185,P03-1021,0,0.0280237,"sentence and the original input Chinese sentence as the single reference translation. The evaluation metric is case-insensitive BLEU (Papineni et al., 2002) as calculated by the multi-bleu.perl script. We compared our approach with two state-ofthe-art SMT and NMT systems: 2. RNN SEARCH (Bahdanau et al., 2015): an attention-based NMT system. 1. M OSES (Koehn et al., 2007): a phrase-based SMT system; For M OSES, we use the default setting to train the phrase-based translation on the parallel corpus and optimize the parameters of log-linear models using the minimum error rate training algorithm (Och, 2003). We use the SRILM toolkit (Stolcke, 2002) to train 4-gram language models. For RNNS EARCH, we use the parallel corpus to train the attention-based neural translation models. We set the vocabulary size of word embeddings to 30K for both Chinese and English. We follow Luong et al. (2015) to address rare words. On top of RNNS EARCH, our approach is capable of training bidirectional attention-based neural translation models on the concatenation of parallel and monolingual corpora. The sample size k is set to 10. We set the hyper-parameter λ1 = 0.1 and 1969 λ2 = 0 when we add the target monolingua"
P16-1185,P02-1040,0,0.119118,"nese validation set. For Chinese-to-English translation, we use the NIST 2006 Chinese-English dataset as the validation set for hyper-parameter optimization and model selection. The NIST 2002, 2003, 2004, and 2005 datasets serve as test sets. Each Chinese sentence has four reference translations. For English-to-Chinese translation, we use the NIST datasets in a reverse direction: treating the first English sentence in the four reference translations as a source sentence and the original input Chinese sentence as the single reference translation. The evaluation metric is case-insensitive BLEU (Papineni et al., 2002) as calculated by the multi-bleu.perl script. We compared our approach with two state-ofthe-art SMT and NMT systems: 2. RNN SEARCH (Bahdanau et al., 2015): an attention-based NMT system. 1. M OSES (Koehn et al., 2007): a phrase-based SMT system; For M OSES, we use the default setting to train the phrase-based translation on the parallel corpus and optimize the parameters of log-linear models using the minimum error rate training algorithm (Och, 2003). We use the SRILM toolkit (Stolcke, 2002) to train 4-gram language models. For RNNS EARCH, we use the parallel corpus to train the attention-base"
P16-1185,P11-1002,0,0.0466152,"Missing"
P16-1185,D11-1014,0,0.0792271,"icted to government documents and news reports. Therefore, the availability of large-scale, high-quality, and wide-coverage parallel corpora becomes a major obstacle for NMT. 2.2 the observed source sentence via a latent target sentence: → − ← − P (x0 |x; θ , θ ) X ← − = P (x0 , y|x; θ ) y Autoencoders on Monolingual Corpora It is appealing to explore the more readily available, abundant monolingual corpora to improve NMT. Let us first consider an unsupervised setting: how to train NMT models on a monolingual corpus T = {y(t) }Tt=1 ? Our idea is to leverage autoencoders (Vincent et al., 2010; Socher et al., 2011): (1) encoding an observed target sentence into a latent source sentence using a target-to-source translation model and (2) decoding the source sentence to reconstruct the observed target sentence using a source-to-target model. For example, as shown in Figure 1(b), given an observed English sentence “Bush held a talk with Sharon”, a target-to-source translation model (i.e., encoder) transforms it into a Chinese translation “bushi yu shalong juxing le huitan” that is unobserved on the training data (highlighted in grey). Then, a source-to-target translation model (i.e., decoder) reconstructs t"
P16-1185,P07-1004,0,0.451948,"y of the observed source sentence x0 from the latent target sentence. As a result, monolingual corpora can be combined with parallel corpora to train bidirectional NMT models in a semi-supervised setting. and attention model are fixed when training on these pseudo parallel sentence pairs. In the second approach, they first train a nerual translation model on the parallel corpus and then use the learned model to translate a monolingual corpus. The monolingual corpus and its translations constitute an additional pseudo parallel corpus. Similar ideas have also been suggested in conventional SMT (Ueffing et al., 2007; Bertoldi and Federico, 2009). Sennrich et al. (2015) report that their approach significantly improves translation quality across a variety of language pairs. In this paper, we propose semi-supervised learning for neural machine translation. Given labeled (i.e., parallel corpora) and unlabeled (i.e., monolingual corpora) data, our approach jointly trains source-to-target and target-to-source translation models. The key idea is to append a reconstruction term to the training objective, which aims to reconstruct the observed monolingual corpora using an autoencoder. In the autoencoder, the sou"
P16-1185,W09-0432,0,\N,Missing
P16-2032,N15-1172,0,0.0433705,"s have been paying increasing attentions to analyzing persuasive content, including identification of arguing expressions in online debates (Trabelsi and Zaıane, 2014), recognition of stance in ideological online debates (Somasundaran and Wiebe, 2010; Hasan and Ng, 2014; Ranade et al., 2013b), and debate summarization (Ranade et al., 2013a). However, how to automatically determine if a text is persuasive is still an unsolved problem. Text quality and popularity evaluation has been studied in different domains in the past few years (Louis and Nenkova, 2013; Tan et al., 2014; Park et al., 2016; Guerini et al., 2015). However, 1 2 http://idebate.org/ http://convinceme.net quality evaluation of argumentative text in the online forum has some unique characterisitcs. First, persuasive text contains argument that is not common in other genres. Second, beside the text itself, the interplay between a comment and what it responds to is crucial. Third, the community reaction to the comment also needs to be taken into consideration. In this paper, we propose several sets of features to capture the above mentioned characteristics for persuasive comment identification in the online forum. We constructed a dataset fr"
P16-2032,D14-1083,0,0.0113119,"lated features. Our experiments show that the surface textual features do not perform well compared to the argumentation based features, and the social interaction based features are effective especially when more users participate in the discussion. 1 Introduction With the popularity of online forums such as idebate1 and convinceme2 , researchers have been paying increasing attentions to analyzing persuasive content, including identification of arguing expressions in online debates (Trabelsi and Zaıane, 2014), recognition of stance in ideological online debates (Somasundaran and Wiebe, 2010; Hasan and Ng, 2014; Ranade et al., 2013b), and debate summarization (Ranade et al., 2013a). However, how to automatically determine if a text is persuasive is still an unsolved problem. Text quality and popularity evaluation has been studied in different domains in the past few years (Louis and Nenkova, 2013; Tan et al., 2014; Park et al., 2016; Guerini et al., 2015). However, 1 2 http://idebate.org/ http://convinceme.net quality evaluation of argumentative text in the online forum has some unique characterisitcs. First, persuasive text contains argument that is not common in other genres. Second, beside the te"
P16-2032,D15-1239,0,0.026047,"(2014) looked into the effect of wording while predicting the popularity of social media content. Park et al. (2016) developed an interactive system to assist human moderators to select high quality news. Guerini et al. (2015) modeled a notion of euphony and explored the impact of sounds on different forms of persuasiveness. Their research focused on the phonetic aspect instead of language usage. Reddit based research: Reddit has been used recently for research on social news analysis and recommendation (e.g., (Buntain and Golbeck, 2014)). Researchers also analyzed the language use on Reddit. Jaech et al. (2015) studied how language use affects community reaction to comments in Reddit. Tan et al. (2016) analyzed the interaction dynamics and persuasion strategies in CMV. 6 Conclusion In this paper, we studied the impact of different sets of features on the identification of persuasive comments in the online forum. Our experiment results show that argumentation based features work the best in the early stage of the discussion, while the effectiveness of social interaction based features increases when the number of comments in the thread grows. There are three major future directions for this research."
P16-2032,W13-4008,0,0.0432121,"experiments show that the surface textual features do not perform well compared to the argumentation based features, and the social interaction based features are effective especially when more users participate in the discussion. 1 Introduction With the popularity of online forums such as idebate1 and convinceme2 , researchers have been paying increasing attentions to analyzing persuasive content, including identification of arguing expressions in online debates (Trabelsi and Zaıane, 2014), recognition of stance in ideological online debates (Somasundaran and Wiebe, 2010; Hasan and Ng, 2014; Ranade et al., 2013b), and debate summarization (Ranade et al., 2013a). However, how to automatically determine if a text is persuasive is still an unsolved problem. Text quality and popularity evaluation has been studied in different domains in the past few years (Louis and Nenkova, 2013; Tan et al., 2014; Park et al., 2016; Guerini et al., 2015). However, 1 2 http://idebate.org/ http://convinceme.net quality evaluation of argumentative text in the online forum has some unique characterisitcs. First, persuasive text contains argument that is not common in other genres. Second, beside the text itself, the interp"
P16-2032,W10-0214,0,0.00858958,"ents and social interaction related features. Our experiments show that the surface textual features do not perform well compared to the argumentation based features, and the social interaction based features are effective especially when more users participate in the discussion. 1 Introduction With the popularity of online forums such as idebate1 and convinceme2 , researchers have been paying increasing attentions to analyzing persuasive content, including identification of arguing expressions in online debates (Trabelsi and Zaıane, 2014), recognition of stance in ideological online debates (Somasundaran and Wiebe, 2010; Hasan and Ng, 2014; Ranade et al., 2013b), and debate summarization (Ranade et al., 2013a). However, how to automatically determine if a text is persuasive is still an unsolved problem. Text quality and popularity evaluation has been studied in different domains in the past few years (Louis and Nenkova, 2013; Tan et al., 2014; Park et al., 2016; Guerini et al., 2015). However, 1 2 http://idebate.org/ http://convinceme.net quality evaluation of argumentative text in the online forum has some unique characterisitcs. First, persuasive text contains argument that is not common in other genres. S"
P16-2032,D14-1006,0,0.0517899,"Manning et al., 2014) was used to preprocess the text (i.e., comment splitting, sentence tokenization, POS tagging and NER recognition.). • Argumentation Related Features: We believe a comment’s argumentation quality is a good indicator of its persuasiveness. In order to capture the argumentation related information, we propose two sub-groups of features based on the comment itself and the interplay between the comment and other comments in the discussion. a) Local features: we trained a binary classifier to classify sentences as argumentative and non-argumentative using features proposed in (Stab and Gurevych, 2014). We then use the number and percentage of argumentative sen9 197 It is a comment that replies to the original post directly. Approach random author entry-order LTRtext LTRsocial LTRarg LTRtext+social LTRtext+arg LTRsocial+arg LTRT +S+A LTRall NDCG@1 0.258 0.382 0.460 0.372 0.475† 0.475† 0.494† 0.485† 0.502† ‡ 0.508† ‡ 0.521† ‡ NDCG@5 0.440 0.567 0.600 0.558 0.650† 0.652† 0.666† 0.654† 0.674† ‡ 0.676† ‡ 0.685† ‡ NDCG@10 0.564 0.664 0.689 0.658 0.718† 0.725† 0.733† 0.729† 0.740† 0.743† ‡ 0.752† ‡ Table 3: Performance of first-10 comments ranking (T+S+A: the combination of the three sets of feat"
P16-2032,P14-1017,0,0.208326,"idebate1 and convinceme2 , researchers have been paying increasing attentions to analyzing persuasive content, including identification of arguing expressions in online debates (Trabelsi and Zaıane, 2014), recognition of stance in ideological online debates (Somasundaran and Wiebe, 2010; Hasan and Ng, 2014; Ranade et al., 2013b), and debate summarization (Ranade et al., 2013a). However, how to automatically determine if a text is persuasive is still an unsolved problem. Text quality and popularity evaluation has been studied in different domains in the past few years (Louis and Nenkova, 2013; Tan et al., 2014; Park et al., 2016; Guerini et al., 2015). However, 1 2 http://idebate.org/ http://convinceme.net quality evaluation of argumentative text in the online forum has some unique characterisitcs. First, persuasive text contains argument that is not common in other genres. Second, beside the text itself, the interplay between a comment and what it responds to is crucial. Third, the community reaction to the comment also needs to be taken into consideration. In this paper, we propose several sets of features to capture the above mentioned characteristics for persuasive comment identification in the"
P16-2032,W14-1305,0,0.038754,"k comments for their persuasive scores, including textual information in the comments and social interaction related features. Our experiments show that the surface textual features do not perform well compared to the argumentation based features, and the social interaction based features are effective especially when more users participate in the discussion. 1 Introduction With the popularity of online forums such as idebate1 and convinceme2 , researchers have been paying increasing attentions to analyzing persuasive content, including identification of arguing expressions in online debates (Trabelsi and Zaıane, 2014), recognition of stance in ideological online debates (Somasundaran and Wiebe, 2010; Hasan and Ng, 2014; Ranade et al., 2013b), and debate summarization (Ranade et al., 2013a). However, how to automatically determine if a text is persuasive is still an unsolved problem. Text quality and popularity evaluation has been studied in different domains in the past few years (Louis and Nenkova, 2013; Tan et al., 2014; Park et al., 2016; Guerini et al., 2015). However, 1 2 http://idebate.org/ http://convinceme.net quality evaluation of argumentative text in the online forum has some unique characterisi"
P16-2032,Q13-1028,0,0.0366981,"of online forums such as idebate1 and convinceme2 , researchers have been paying increasing attentions to analyzing persuasive content, including identification of arguing expressions in online debates (Trabelsi and Zaıane, 2014), recognition of stance in ideological online debates (Somasundaran and Wiebe, 2010; Hasan and Ng, 2014; Ranade et al., 2013b), and debate summarization (Ranade et al., 2013a). However, how to automatically determine if a text is persuasive is still an unsolved problem. Text quality and popularity evaluation has been studied in different domains in the past few years (Louis and Nenkova, 2013; Tan et al., 2014; Park et al., 2016; Guerini et al., 2015). However, 1 2 http://idebate.org/ http://convinceme.net quality evaluation of argumentative text in the online forum has some unique characterisitcs. First, persuasive text contains argument that is not common in other genres. Second, beside the text itself, the interplay between a comment and what it responds to is crucial. Third, the community reaction to the comment also needs to be taken into consideration. In this paper, we propose several sets of features to capture the above mentioned characteristics for persuasive comment ide"
P16-2032,P14-5010,0,0.00207461,".2 Features We propose several key features that we hypothesize are predictive of persuasive comments. The full feature list is given in Table 2. • Surface Text Features8 : In order to capture the basic textual information, we use the comment length and content diversity represented as the number of words, POS tags, URLs, and punctuation marks. We also explored unigram features and named entity based features, but they did not improve system performance and are thus not included. • Social Interaction Features: We hypothesize that if a comment attracts more social attention 8 Stanford CoreNLP (Manning et al., 2014) was used to preprocess the text (i.e., comment splitting, sentence tokenization, POS tagging and NER recognition.). • Argumentation Related Features: We believe a comment’s argumentation quality is a good indicator of its persuasiveness. In order to capture the argumentation related information, we propose two sub-groups of features based on the comment itself and the interplay between the comment and other comments in the discussion. a) Local features: we trained a binary classifier to classify sentences as argumentative and non-argumentative using features proposed in (Stab and Gurevych, 20"
P16-3019,W09-2209,0,0.0255686,"ssification tasks. We show that our model achieves a comparable and even better performance than the traditional MT-based method. 1 Introduction With the rapid growth of global Internet, huge amounts of information are created in different languages. It is important to develop cross-lingual NLP systems in order to leverage information from other languages, especially languages with rich annotations. Traditionally, cross-lingual systems rely highly on machine translation (MT) systems (Wan et al., 2011; Wan, 2011; Rigutini et al., 2005; Ling et al., 2008; Amini et al., 2009; Guo and Xiao, 2012; Chen and Ji, 2009; Duh et al., 2011). They translate data in one language into the other, and then apply monolingual models. One problem of such cross-lingual systems is that there is hardly any decent MT system for resourcepoor languages. Another problem is the lack of high quality parallel corpora for resource-poor languages, which is required by MT systems. Other work tried to address these problems by developping language independent representation learning and structural correspondence learning (SCL) (Prettenhofer and Stein, 2010; Xiao and Guo, 2013). They showed some promising results on document level c"
P16-3019,P04-1035,0,0.030122,"idation and test sets. Question classification (QC) aims to determine the category of a given question sentence. For English, we use the TREC1 dataset. For Chinese, we use a QA corpus from HIT-IRLab2 . We kept the six overlapped question types for both English and Chinese corpora. The final corpus includes 4,313 English questions and 4,031 Chinese questions (859 for testing, 859 for validation and 2,313 for training3 ). Sentiment classification on movie review (SC-M) aims to classify a piece of given movie review into positive or negative. For English, we use IMDB polarity movie reviews from (Pang and Lee, 2004) (5,331 positive and 5,331 negative). For Chinese, we use the short Chinese movie reviews from Douban4 . Like IMDB, users from Douban leave their comments along with a score for the movie. We collected 250 one star reviews (lowest score), and 250 five star reviews (highest score). We randomly split the 500 reviews into 200 for validation and 300 for testing. Sentiment classification on product review (SC-P) aims to classify a piece of given product review into positive or negative. We use corpora from (Wan, 2011). Their Chinese dataset contains mostly short reviews. However, their English Amaz"
P16-3019,P10-1114,0,0.0149825,"2011; Rigutini et al., 2005; Ling et al., 2008; Amini et al., 2009; Guo and Xiao, 2012; Chen and Ji, 2009; Duh et al., 2011). They translate data in one language into the other, and then apply monolingual models. One problem of such cross-lingual systems is that there is hardly any decent MT system for resourcepoor languages. Another problem is the lack of high quality parallel corpora for resource-poor languages, which is required by MT systems. Other work tried to address these problems by developping language independent representation learning and structural correspondence learning (SCL) (Prettenhofer and Stein, 2010; Xiao and Guo, 2013). They showed some promising results on document level classification tasks. However, their methods require carefully designed language specific features and find the “pivot features” across languages, which can be very expensive and inefficient. To solve these problems, we develop an efficient and feasible cross-lingual sentence model that is based on convolutional neural network (CNN). Sentence modeling using CNN has shown its great potential in recent years (Kalchbrenner et al., 2014; Kim, 2014; Ma et al., 2015). One of the advantages is that CNN requires much less expe"
P16-3019,P11-2075,0,0.0126748,"We show that our model achieves a comparable and even better performance than the traditional MT-based method. 1 Introduction With the rapid growth of global Internet, huge amounts of information are created in different languages. It is important to develop cross-lingual NLP systems in order to leverage information from other languages, especially languages with rich annotations. Traditionally, cross-lingual systems rely highly on machine translation (MT) systems (Wan et al., 2011; Wan, 2011; Rigutini et al., 2005; Ling et al., 2008; Amini et al., 2009; Guo and Xiao, 2012; Chen and Ji, 2009; Duh et al., 2011). They translate data in one language into the other, and then apply monolingual models. One problem of such cross-lingual systems is that there is hardly any decent MT system for resourcepoor languages. Another problem is the lack of high quality parallel corpora for resource-poor languages, which is required by MT systems. Other work tried to address these problems by developping language independent representation learning and structural correspondence learning (SCL) (Prettenhofer and Stein, 2010; Xiao and Guo, 2013). They showed some promising results on document level classification tasks"
P16-3019,P14-1062,0,0.0548184,"ge independent representation learning and structural correspondence learning (SCL) (Prettenhofer and Stein, 2010; Xiao and Guo, 2013). They showed some promising results on document level classification tasks. However, their methods require carefully designed language specific features and find the “pivot features” across languages, which can be very expensive and inefficient. To solve these problems, we develop an efficient and feasible cross-lingual sentence model that is based on convolutional neural network (CNN). Sentence modeling using CNN has shown its great potential in recent years (Kalchbrenner et al., 2014; Kim, 2014; Ma et al., 2015). One of the advantages is that CNN requires much less expertise knowledge than traditional feature based models. The only input of the model, word embeddings, can be learned automatically from large unlabeled text data. There are roughly two main differences between different languages, lexicon and grammar. Lexicon can be seen as a set of symbols with each symbol representing certain meanings. A bilingual dictionary easily enables us to map from one symbol set to another. As for grammar, it decides the organization of lexical symbols, i.e., word order. Different l"
P16-3019,D14-1181,0,0.0208467,"Missing"
P16-3019,J11-3005,0,0.046285,"poor languages. We evaluate our model using English and Chinese data on several sentence classification tasks. We show that our model achieves a comparable and even better performance than the traditional MT-based method. 1 Introduction With the rapid growth of global Internet, huge amounts of information are created in different languages. It is important to develop cross-lingual NLP systems in order to leverage information from other languages, especially languages with rich annotations. Traditionally, cross-lingual systems rely highly on machine translation (MT) systems (Wan et al., 2011; Wan, 2011; Rigutini et al., 2005; Ling et al., 2008; Amini et al., 2009; Guo and Xiao, 2012; Chen and Ji, 2009; Duh et al., 2011). They translate data in one language into the other, and then apply monolingual models. One problem of such cross-lingual systems is that there is hardly any decent MT system for resourcepoor languages. Another problem is the lack of high quality parallel corpora for resource-poor languages, which is required by MT systems. Other work tried to address these problems by developping language independent representation learning and structural correspondence learning (SCL) (Pret"
P16-3019,D13-1153,0,0.0183419,"ing et al., 2008; Amini et al., 2009; Guo and Xiao, 2012; Chen and Ji, 2009; Duh et al., 2011). They translate data in one language into the other, and then apply monolingual models. One problem of such cross-lingual systems is that there is hardly any decent MT system for resourcepoor languages. Another problem is the lack of high quality parallel corpora for resource-poor languages, which is required by MT systems. Other work tried to address these problems by developping language independent representation learning and structural correspondence learning (SCL) (Prettenhofer and Stein, 2010; Xiao and Guo, 2013). They showed some promising results on document level classification tasks. However, their methods require carefully designed language specific features and find the “pivot features” across languages, which can be very expensive and inefficient. To solve these problems, we develop an efficient and feasible cross-lingual sentence model that is based on convolutional neural network (CNN). Sentence modeling using CNN has shown its great potential in recent years (Kalchbrenner et al., 2014; Kim, 2014; Ma et al., 2015). One of the advantages is that CNN requires much less expertise knowledge than"
P16-3019,P15-2029,0,0.177819,"and structural correspondence learning (SCL) (Prettenhofer and Stein, 2010; Xiao and Guo, 2013). They showed some promising results on document level classification tasks. However, their methods require carefully designed language specific features and find the “pivot features” across languages, which can be very expensive and inefficient. To solve these problems, we develop an efficient and feasible cross-lingual sentence model that is based on convolutional neural network (CNN). Sentence modeling using CNN has shown its great potential in recent years (Kalchbrenner et al., 2014; Kim, 2014; Ma et al., 2015). One of the advantages is that CNN requires much less expertise knowledge than traditional feature based models. The only input of the model, word embeddings, can be learned automatically from large unlabeled text data. There are roughly two main differences between different languages, lexicon and grammar. Lexicon can be seen as a set of symbols with each symbol representing certain meanings. A bilingual dictionary easily enables us to map from one symbol set to another. As for grammar, it decides the organization of lexical symbols, i.e., word order. Different languages organize their words"
P16-3019,P14-5010,0,0.00354431,"7 +Lex+Phrase 82.19 79.22 83.60 85.01 Table 1: Results of different systems. +Lex: lexical features are used; +Phrase: phrase-based bilingual word embeddings and grammar are used. gradient descent (SGD) learning method. We apply random dropout (Hinton et al., 2012) on the last fully connected layer for regularization. We use ADADELTA (Zeiler, 2012) algorithm to automatically control the learning rate and progress. The batch size for SGD and feature maps are tuned on the validation set for each task and fixed across different configurations. We preprocess all our corpora with Stanford CoreNLP (Manning et al., 2014), including word segmentation, sentence segmentation and dependency parsing. 4.2 Results Table 1 shows the results of different systems. When using the MT based methods, the basic CNN achieves better results than DCNN. One possible reason is that the translation system produces errors, which may affect the performance of dependency parsing. For our method using bilingual word embeddings, basic CNN encodes only lexicon mapping information, and is not good at capturing grammar patterns. Therefore, it is natural this system has the lowest result. DCNN performs better than CNN, because it is able"
P17-1106,P05-1033,0,0.0251887,"errors. 1 Introduction End-to-end neural machine translation (NMT), which leverages neural networks to directly map between natural languages, has gained increasing popularity recently (Sutskever et al., 2014; Bahdanau et al., 2015). NMT proves to outperform conventional statistical machine translation (SMT) significantly across a variety of language pairs (Junczys-Dowmunt et al., 2016) and becomes the new de facto method in practical MT systems (Wu et al., 2016). However, there still remains a severe challenge: it is hard to interpret the internal workings of NMT. In SMT (Koehn et al., 2003; Chiang, 2005), the translation process can be denoted as a derivation that comprises a sequence of translation rules (e.g., phrase pairs and synchronous CFG rules). Defined on language structures with varying granularities, these translation rules are interpretable from a linguistic perspective. In contrast, NMT takes an end-to-end approach: all internal information is represented as real-valued vectors or ∗ Corresponding author. matrices. It is challenging to associate hidden states in neural networks with interpretable language structures. As a result, the lack of interpretability makes it very difficult"
P17-1106,E14-1049,0,0.00569481,"ord. (6) where g(·) is a non-linear function, yj−1 denotes the vector representation of the (j − 1)-th target word. Finally, the word-level translation probability is given by P (yj |x, y&lt;j ; θ) = ρ(yj−1 , sj , cj ), (7) where ρ(·) is a non-linear function. Although NMT proves to deliver state-of-theart translation performance with the capability to handle long-distance dependencies due to GRU and attention, it is hard to interpret the internal → − ← − information such as h i , h i , hi , cj , and sj in the encoder-decoder framework. Though projecting word embedding space into two dimensions (Faruqui and Dyer, 2014) and the attention matrix (Bahdanau et al., 2015) shed partial light on how NMT works, how to interpret the entire network still remains a challenge. Therefore, it is important to develop new methods for understanding the translation process and analyzing translation errors for NMT. 3 3.1 Approach Problem Statement Recent efforts on interpreting and visualizing neural models has focused on calculating the contribution of a unit at the input layer to the final decision at the output layer (Simonyan et al., 2014; Mahendran and Vedaldi, 2015; Nguyen et al., 2015; 1151 in New York 在 纽约 &lt;/s> in &lt;/s"
P17-1106,N03-1017,0,0.0722159,"analyze translation errors. 1 Introduction End-to-end neural machine translation (NMT), which leverages neural networks to directly map between natural languages, has gained increasing popularity recently (Sutskever et al., 2014; Bahdanau et al., 2015). NMT proves to outperform conventional statistical machine translation (SMT) significantly across a variety of language pairs (Junczys-Dowmunt et al., 2016) and becomes the new de facto method in practical MT systems (Wu et al., 2016). However, there still remains a severe challenge: it is hard to interpret the internal workings of NMT. In SMT (Koehn et al., 2003; Chiang, 2005), the translation process can be denoted as a derivation that comprises a sequence of translation rules (e.g., phrase pairs and synchronous CFG rules). Defined on language structures with varying granularities, these translation rules are interpretable from a linguistic perspective. In contrast, NMT takes an end-to-end approach: all internal information is represented as real-valued vectors or ∗ Corresponding author. matrices. It is challenging to associate hidden states in neural networks with interpretable language structures. As a result, the lack of interpretability makes it"
P17-1106,N16-1082,0,0.563077,"ty makes it very difficult to understand translation process and debug NMT systems. Therefore, it is important to develop new methods for visualizing and understanding NMT. Existing work on visualizing and interpreting neural models has been extensively investigated in computer vision (Krizhevsky et al., 2012; Mahendran and Vedaldi, 2015; Szegedy et al., 2014; Simonyan et al., 2014; Nguyen et al., 2015; Girshick et al., 2014; Bach et al., 2015). Although visualizing and interpreting neural models for natural language processing has started to attract attention recently (Karpathy et al., 2016; Li et al., 2016), to the best of our knowledge, there is no existing work on visualizing NMT models. Note that the attention mechanism (Bahdanau et al., 2015) is restricted to demonstrate the connection between words in source and target languages and unable to offer more insights in interpreting how target words are generated (see Section 4.5). In this work, we propose to use layer-wise relevance propagation (LRP) (Bach et al., 2015) to visualize and interpret neural machine translation. Originally designed to compute the contributions of single pixels to predictions for image classifiers, LRP back-propagate"
P17-1106,P16-1159,1,0.694161,"Missing"
P17-1106,Q17-1007,1,0.326907,"at “history” repeats four times in the translation. Figure 8 visualizes the target hidden states of the 6-th target word “history”. According to the relevance of the target word embedding Ry6 , the first source word “meiguoren” (american), the second source word “lishi” (history) and the 5-th target word “the” are most relevant to the generation of “history”. Therefore, word repetition not only results from wrong attention but also is significantly influenced by target side context. This finding confirms the importance of controlling source and target contexts to improve fluency and adequacy (Tu et al., 2017). 4.3.2 Word Repetition Given a source sentence “meiguoren lishi shang you jiang chengxi de chuantong , you fancuo rencuo de chuantong” (in history , the people of america have the tradition of honesty and would not hesitate to admit their mistakes), the NMT model produces a wrong translation “in the history of the history of the history of the americans , there is a tradition of faith in the history of mistakes”. The 4.3.3 Unrelated Words Given a source sentence “ci ci huiyi de yi ge zhongyao yiti shi kuadaxiyang guanxi” (one the the top agendas of the meeting is to discuss the cross-atlantic"
P17-1106,P16-1008,1,0.894221,"Missing"
P17-1139,J93-2003,0,0.0876488,"Missing"
P17-1139,P16-1097,1,0.921497,"ge in machine translation is usually represented in discrete symbolic forms such as dictionaries and rules (Nirenburg, 1989) that explicitly encode translation regularities. It is difficult to transform prior knowledge represented in discrete forms to continuous representations required by neural networks. Therefore, a number of authors have endeavored to integrate prior knowledge into NMT in recent years, either by modifying model architectures (Tu et al., 2016; Cohn et al., 2016; Tang et al., 2016; Feng et al., 2016) or by modifying training objectives (Cohn et al., 2016; Feng et al., 2016; Cheng et al., 2016). For example, to address the over-translation and under-translation problems widely observed in NMT, Tu et al. (2016) directly extend standard NMT to model the coverage constraint that each source phrase should be translated into exactly one target phrase (Koehn et al., 2003). Alternatively, Cohn et al. (2016) and Feng et al. (2016) propose to control the fertilities of source words by appending additional additive terms to training objectives. Although these approaches have demonstrated clear benefits of incorporating prior knowledge into NMT, how to combine multiple overlapping, arbitrary p"
P17-1139,P05-1033,0,0.0556812,"onstraint features to facilitate inference. As maximizing F (θ, q) involves minimizing the KL divergence, Ganchev et al. (2010) present a minorization-maximization algorithm akin to EM at sentence level:   E : q (t+1) = argmin KL q(y) P (y|x(n) ; θ (t) ) q h i M : θ (t+1) = argmax Eq(t+1) log P (y|x(n) ; θ) θ However, directly applying posterior regularization to neural machine translation faces a major difficulty: it is hard to specify the hyper-parameter b to effectively bound the expectation of features, which are usually real-valued in translation (Och and Ney, 2002; Koehn et al., 2003; Chiang, 2005). For example, the coverage penalty constraint (Wu et al., 2016) proves to be an essential feature for controlling the length of a translation in NMT. As the value of coverage penalty varies significantly over different sentences, it is difficult to set an appropriate bound for all sentences on the training data. In addition, the minorization-maximization algorithm involves an additional step to find q (t+1) as compared with standard NMT, which increases training time significantly. 3 3.1 = λ1 L(θ) − N   X (n) (n) λ2 KL Q(y|x ; γ) P (y|x ; θ) , (7)   min KL q(y) P (y|x(n) ; θ), (5) Posteri"
P17-1139,C16-1290,0,0.117642,"idden state in neural networks from a linguistic perspective. On the other hand, prior knowledge in machine translation is usually represented in discrete symbolic forms such as dictionaries and rules (Nirenburg, 1989) that explicitly encode translation regularities. It is difficult to transform prior knowledge represented in discrete forms to continuous representations required by neural networks. Therefore, a number of authors have endeavored to integrate prior knowledge into NMT in recent years, either by modifying model architectures (Tu et al., 2016; Cohn et al., 2016; Tang et al., 2016; Feng et al., 2016) or by modifying training objectives (Cohn et al., 2016; Feng et al., 2016; Cheng et al., 2016). For example, to address the over-translation and under-translation problems widely observed in NMT, Tu et al. (2016) directly extend standard NMT to model the coverage constraint that each source phrase should be translated into exactly one target phrase (Koehn et al., 2003). Alternatively, Cohn et al. (2016) and Feng et al. (2016) propose to control the fertilities of source words by appending additional additive terms to training objectives. Although these approaches have demonstrated clear benef"
P17-1139,D07-1091,0,0.0111553,"ds RNN SEARCH by incorporating prior knowledge. For each source sentence, we sample 80 candidate translations to ap˜ distributions. The hyperproximate the P˜ and Q parameter α is set to 0.2. The batch size is 1. The hyper-parameters λ1 and λ2 are set to 8×10−5 and 2.5 × 10−4 . Note that they not only balance the preference between likelihood and posterior regularization, but also control the values of gradients to fall in a reasonable range for optimization. We construct bilingual dictionary and phrase table in an automatic way. First, we run the statistical machine translation system M OSES (Koehn and Hoang, 2007) to obtain probabilistic bilingual dictionary and phrase table. For the bilingual dictionary, we retain entries with probabilities higher than 0.1 in both source-to-target and 1519 Feature BD PT CP LR BD+PT BD+PT+CP BD+PT+CP+LR Rerank w/o w/ w/o w/ w/o w/ w/o w/ w/o w/ w/o w/ w/o w/ MT02 36.06 36.61 34.98 35.07 34.68 34.68 34.60 34.57 35.76 36.30 35.71 36.11 36.06 36.10 MT03 32.99 33.47 32.01 32.11 31.99 31.99 31.89 31.89 33.27 33.83 33.15 33.64 33.01 33.64 MT04 35.62 36.04 34.71 34.73 34.67 34.67 34.79 34.95 35.64 36.02 35.81 36.36 35.86 36.48 MT05 32.59 32.96 31.77 31.84 31.37 31.37 31.72 31"
P17-1139,N03-1017,0,0.18717,"s required by neural networks. Therefore, a number of authors have endeavored to integrate prior knowledge into NMT in recent years, either by modifying model architectures (Tu et al., 2016; Cohn et al., 2016; Tang et al., 2016; Feng et al., 2016) or by modifying training objectives (Cohn et al., 2016; Feng et al., 2016; Cheng et al., 2016). For example, to address the over-translation and under-translation problems widely observed in NMT, Tu et al. (2016) directly extend standard NMT to model the coverage constraint that each source phrase should be translated into exactly one target phrase (Koehn et al., 2003). Alternatively, Cohn et al. (2016) and Feng et al. (2016) propose to control the fertilities of source words by appending additional additive terms to training objectives. Although these approaches have demonstrated clear benefits of incorporating prior knowledge into NMT, how to combine multiple overlapping, arbitrary prior knowledge sources still remains a major challenge. It is difficult to achieve this end by directly modifying model architectures because neural networks usually impose strong independence assumptions between hidden states. As a result, extending a neural model requires th"
P17-1139,D16-1162,0,0.108937,"Bahdanau et al., 2015). We will introduce a number of features used in our experiments as follows. number of authors have proposed to model the fertility (Brown et al., 1993) and converge constraint (Koehn et al., 2003) to improve the adequacy of translation (Tu et al., 2016; Cohn et al., 2016; Feng et al., 2016; Wu et al., 2016; Mi et al., 2016). We follow Wu et al. (2016) to define a coverage penalty (CP) feature to penalize source words with lower sum of attention weights: 2 3.2.1 Bilingual Dictionary It is natural to leverage a bilingual dictionary D to improve neural machine translation. Arthur et al. (2016) propose to incorporate discrete translation lexicons into NMT by using the attention vector to select lexical probabilities on which to be focused. In our work, for each entry hx, yi ∈ D in the dictionary, a bilingual dictionary (BD) feature is defined at the sentence level:  1 if x ∈ x ∧ y ∈ y . (9) φBDhx,yi (x, y) = 0 otherwise where ai,j is the attention probability of the j-th target word on the i-th source word. Note that the value of coverage penalty feature varies significantly over sentences of different lengths. Note that number of bilingual dictionary features depends on the vocabu"
P17-1139,D16-1096,0,0.048484,"ed on latent structures such as phrase pairs and synchronous CFG rules, which are not accessible to the decoding process of NMT. Fortunately, we can still leverage internal information in neural models that is linguistically meaningful such as the attention matrix a (Bahdanau et al., 2015). We will introduce a number of features used in our experiments as follows. number of authors have proposed to model the fertility (Brown et al., 1993) and converge constraint (Koehn et al., 2003) to improve the adequacy of translation (Tu et al., 2016; Cohn et al., 2016; Feng et al., 2016; Wu et al., 2016; Mi et al., 2016). We follow Wu et al. (2016) to define a coverage penalty (CP) feature to penalize source words with lower sum of attention weights: 2 3.2.1 Bilingual Dictionary It is natural to leverage a bilingual dictionary D to improve neural machine translation. Arthur et al. (2016) propose to incorporate discrete translation lexicons into NMT by using the attention vector to select lexical probabilities on which to be focused. In our work, for each entry hx, yi ∈ D in the dictionary, a bilingual dictionary (BD) feature is defined at the sentence level:  1 if x ∈ x ∧ y ∈ y . (9) φBDhx,yi (x, y) = 0 othe"
P17-1139,P02-1038,0,0.0611375,"training objective is to maximize the logization for incorporating indirect supervision via likelihood of the training set: constraints on posterior distributions of structured n o latent-variable models. The basic idea is to penalθˆMLE = argmax L(θ) , (3) ize the log-likelihood of a neural translation model θ 1515 with the KL divergence between a desired distribution that incorporates prior knowledge and the model posteriors. The posterior regularized likelihood is defined as F (θ, q) chine translation. The major difference is that we represent the desired distribution as a log-linear model (Och and Ney, 2002) rather than a constrained posterior set as described in (Ganchev et al., 2010): J (θ, γ) = λ1 L(θ) − λ2 N X n=1 q∈Q where λ1 and λ2 are hyper-parameters to balance the preference between likelihood and posterior regularization, Q is a set of constrained posteriors: Q = {q(y) : Eq [φ(x, y)] ≤ b}, (6) where φ(x, y) is constraint feature and b is the bound of constraint feature expectations. Ganchev et al. (2010) use constraint features to encode structural bias and define the set of valid distributions with respect to the expectations of constraint features to facilitate inference. As maximizin"
P17-1139,P16-1159,1,0.781961,"table P:  1 if x ˜ ∈ x ∧ y˜ ∈ y φPTh˜x,˜yi (x, y) = . (10) 0 otherwise The number of phrase table features also depends on the vocabulary of the neural translation model. 3.2.3 Coverage Penalty To overcome the over-translation and undertranslation problems widely observed in NMT, a φCP (x, y) = |x| X i=1 3.2.4 log  min |y| X j=1  ai,j , 1.0 , (11) Length Ratio Controlling the length of translations is very important in NMT as neural models tend to generate short translations for long sentences, which deteriorates the translation performance of NMT for long sentences as compared with SMT (Shen et al., 2016). Therefore, we define the length ratio (LR) feature to encourage the length of a translation to fall in a reasonable range:  (β|x|)/|y |if β|x |< |y| φLR (x, y) = , (12) |y|/(β|x|) otherwise where β is a hyper-parameter for penalizing too long or too short translations. For example, to convey the same meaning, an English sentence is usually about 1.2 times longer than a Chinese sentence. As a result, we can set β = 1.2. If the length of a Chinese sentence |x |is 10 and the length of an English sentence |y |is 12, then, φLR (x, y) = 1. If the translation is too long (e.g., |y |= 100), then th"
P17-1139,P16-1008,1,0.865444,"Yang Liu. et al., 2014), it is hard to interpret each hidden state in neural networks from a linguistic perspective. On the other hand, prior knowledge in machine translation is usually represented in discrete symbolic forms such as dictionaries and rules (Nirenburg, 1989) that explicitly encode translation regularities. It is difficult to transform prior knowledge represented in discrete forms to continuous representations required by neural networks. Therefore, a number of authors have endeavored to integrate prior knowledge into NMT in recent years, either by modifying model architectures (Tu et al., 2016; Cohn et al., 2016; Tang et al., 2016; Feng et al., 2016) or by modifying training objectives (Cohn et al., 2016; Feng et al., 2016; Cheng et al., 2016). For example, to address the over-translation and under-translation problems widely observed in NMT, Tu et al. (2016) directly extend standard NMT to model the coverage constraint that each source phrase should be translated into exactly one target phrase (Koehn et al., 2003). Alternatively, Cohn et al. (2016) and Feng et al. (2016) propose to control the fertilities of source words by appending additional additive terms to training objective"
P17-1139,P16-5005,0,0.0348747,"arameters θˆ and γ sion rule for translating an unseen source sentence x is given by n o ˆ . ˆ = argmax P (y|x; θ) y (19) Y(x) The search process can be factorized at the word level: n o ˆ , ˆ <j ; θ) yˆj = argmax P (y|x, y (20) y∈Vy where Vy is the target language vocabulary. Although this decision rule shares the same efficiency and simplicity with standard NMT (Bahdanau et al., 2015), it does not involve prior knowledge in decoding. Previous studies reveal that incorporating prior knowledge in decoding also significantly boosts translation performance (Arthur et al., 2016; He et al., 2016; Wang et al., 2016). As directly incorporating prior knowledge into the decoding process of NMT depends on both model structure and the locality of features, we resort to a coarse-to-fine approach instead to keep the architecture transparency of our approach. Given a source sentence x in the test set, we first ˆ to genuse the neural translation model P (y|x; θ) erate a k-best list of candidate translation C(x). Then, the algorithm decides on the most probable candidate translation using the following decision rule: n o ˆ +γ ˆ · φ(x, y) . (21) ˆ = argmax log P (y|x; θ) y y∈C(x) 4 Experiments 4.1 Setup We evaluate"
P17-1176,2008.iwslt-papers.1,0,0.397606,"el corpora to deliver state-of-the-art translation performance (Wu et al., 2016; Johnson et al., 2016). Zoph et al. (2016) report that NMT obtains much lower BLEU scores than SMT if only small-scale parallel corpora are available. Therefore, the heavy dependence on the quantity of training data poses a severe challenge for NMT to translate zeroresource language pairs. Simple and easy-to-implement, pivot-based methods have been widely used in SMT for 1926 translating zero-resource language pairs (de Gispert and Mari˜no, 2006; Cohn and Lapata, 2007; Utiyama and Isahara, 2007; Wu and Wang, 2007; Bertoldi et al., 2008; Wu and Wang, 2009; Zahabi et al., 2013; Kholy et al., 2013). As pivotbased methods are agnostic to model structures, they have been adapted to NMT recently (Cheng et al., 2016a; Johnson et al., 2016). Figure 1(a) illustrates the basic idea of pivotbased approaches to zero-resource NMT (Cheng et al., 2016a). Let X, Y, and Z denote source, target, and pivot languages. We use dashed lines to denote language pairs with parallel corpora available and solid lines with arrows to denote translation directions. Intuitively, the source-to-target translation can be indirectly modeled by bridging two NM"
P17-1176,P16-1185,1,0.0732017,"ces for the zeroresource language pair. Another direction is to develop a universal NMT model in multilingual scenarios (Johnson et al., 2016; Ha et al., 2016). They use parallel corpora of multiple languages to train one single model, which is then able to translate a language pair without parallel corpora available. Although these approaches prove to be effective, the combination of multiple languages in modeling and training leads to increased complexity compared with standard NMT. Another direction is to achieve source-to-target NMT without parallel data via a pivot, which is either text (Cheng et al., 2016a) or image (Nakayama and Nishida, 2016). Cheng et al. (2016a) propose a pivot-based method for zeroresource NMT: it first translates the source language to a pivot language, which is then translated to the target language. Nakayama and Nishida (2016) show that using multimedia information as pivot also benefits zero-resource translation. However, pivot-based approaches usually need to divide the decoding process into two steps, which is not only more computationally expensive, but also potentially suffers from the error propagation problem (Zhu et al., 2013). In this paper, we propose a new m"
P17-1176,P07-1092,0,0.450225,"pproach, NMT heavily relies on the availability of large-scale parallel corpora to deliver state-of-the-art translation performance (Wu et al., 2016; Johnson et al., 2016). Zoph et al. (2016) report that NMT obtains much lower BLEU scores than SMT if only small-scale parallel corpora are available. Therefore, the heavy dependence on the quantity of training data poses a severe challenge for NMT to translate zeroresource language pairs. Simple and easy-to-implement, pivot-based methods have been widely used in SMT for 1926 translating zero-resource language pairs (de Gispert and Mari˜no, 2006; Cohn and Lapata, 2007; Utiyama and Isahara, 2007; Wu and Wang, 2007; Bertoldi et al., 2008; Wu and Wang, 2009; Zahabi et al., 2013; Kholy et al., 2013). As pivotbased methods are agnostic to model structures, they have been adapted to NMT recently (Cheng et al., 2016a; Johnson et al., 2016). Figure 1(a) illustrates the basic idea of pivotbased approaches to zero-resource NMT (Cheng et al., 2016a). Let X, Y, and Z denote source, target, and pivot languages. We use dashed lines to denote language pairs with parallel corpora available and solid lines with arrows to denote translation directions. Intuitively, the sour"
P17-1176,N16-1101,0,0.469266,"ating low-resource languages. Due to the large parameter space, neural models usually learn poorly from low-count events, resulting in a poor choice for low-resource language pairs. Zoph et ∗ Corresponding author: Yang Liu. al. (2016) indicate that NMT obtains much worse translation quality than a statistical machine translation (SMT) system on low-resource languages. As a result, a number of authors have endeavored to explore methods for translating language pairs without parallel corpora available. These methods can be roughly divided into two broad categories: multilingual and pivot-based. Firat et al. (2016b) present a multi-way, multilingual model with shared attention to achieve zeroresource translation. They fine-tune the attention part using pseudo bilingual sentences for the zeroresource language pair. Another direction is to develop a universal NMT model in multilingual scenarios (Johnson et al., 2016; Ha et al., 2016). They use parallel corpora of multiple languages to train one single model, which is then able to translate a language pair without parallel corpora available. Although these approaches prove to be effective, the combination of multiple languages in modeling and training lea"
P17-1176,D16-1026,0,0.709856,"ating low-resource languages. Due to the large parameter space, neural models usually learn poorly from low-count events, resulting in a poor choice for low-resource language pairs. Zoph et ∗ Corresponding author: Yang Liu. al. (2016) indicate that NMT obtains much worse translation quality than a statistical machine translation (SMT) system on low-resource languages. As a result, a number of authors have endeavored to explore methods for translating language pairs without parallel corpora available. These methods can be roughly divided into two broad categories: multilingual and pivot-based. Firat et al. (2016b) present a multi-way, multilingual model with shared attention to achieve zeroresource translation. They fine-tune the attention part using pseudo bilingual sentences for the zeroresource language pair. Another direction is to develop a universal NMT model in multilingual scenarios (Johnson et al., 2016; Ha et al., 2016). They use parallel corpora of multiple languages to train one single model, which is then able to translate a language pair without parallel corpora available. Although these approaches prove to be effective, the combination of multiple languages in modeling and training lea"
P17-1176,P15-1002,0,0.0350535,"(“teacher”) on a source-pivot parallel corpus. Experimental results show that the proposed method significantly improves over a baseline pivot-based model by +3.0 BLEU points across various language pairs. 1 Introduction Neural machine translation (NMT) (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015), which directly models the translation process in an end-to-end way, has attracted intensive attention from the community. Although NMT has achieved state-of-the-art translation performance on resource-rich language pairs such as English-French and German-English (Luong et al., 2015; Jean et al., 2015; Wu et al., 2016; Johnson et al., 2016), it still suffers from the unavailability of large-scale parallel corpora for translating low-resource languages. Due to the large parameter space, neural models usually learn poorly from low-count events, resulting in a poor choice for low-resource language pairs. Zoph et ∗ Corresponding author: Yang Liu. al. (2016) indicate that NMT obtains much worse translation quality than a statistical machine translation (SMT) system on low-resource languages. As a result, a number of authors have endeavored to explore methods for translating l"
P17-1176,P03-1021,0,0.0331732,"totarget model P (y|x; θx→y ) at different iterations to the trained pivot-to-target model P (y|z; θˆz→y ) by caculating JSENT (Equation (5)) and JWORD 3 We can also adopt sampling and k-best list for approximation. Random sampling brings a large variance (Sutskever et al., 2014; Ranzato et al., 2015; He et al., 2016) for sentence-level teaching. For k-best list, we renormalize the probabilities P (y|z; θˆz→y )α P (y|z; θˆz→y ) ∼ P , α ˆ y∈Yk P (y|z; θz→y ) where Yk is the k-best list from beam search of the teacher model and α is a hyperparameter controling the sharpness of the distribution (Och, 2003). We set k = 5 and α = 5×10−3 . The results on test set for Eureparl Corpus are 32.24 BLEU over Spanish-French translation and 24.91 BLEU over German-French translation, which are slightly better than the sent-beam method. However, considering the traing time and the memory consumption, we think mode approximation is already a good way to approximate the target sentence space for sentence-level teaching. 1929 Approx. JSENT JWORD greedy beam greedy beam sampling 0 313.0 323.5 274.0 288.7 268.6 Iterations 2w 4w 73.1 61.5 73.1 60.7 51.5 43.1 52.7 43.3 53.8 46.6 6w 56.8 55.4 39.4 39.2 42.8 8w 55.1"
P17-1176,P02-1040,0,0.12396,"n-French (De-Fr) translation tasks. For the WMT corpus, we evaluate approach on the Spanish-French (Es-Fr) translation task. English is used as a pivot language in all experiments. zero-resource scenario. To avoid the trilingual corpus constituted by the source-pivot and pivottarget corpora, we split the overlapping pivot sentences of the original source-pivot and pivot-target corpora into two equal parts and merge them separately with the non-overlapping parts for each language pair. The development and test sets are from WMT 2006 shared task.1 The evaluation metric is case-insensitive BLEU (Papineni et al., 2002) as calculated by the multi-bleu.perl script. To deal with out-of-vocabulary words, we adopt byte pair encoding (BPE) (Sennrich et al., 2016) to split words into sub-words. The size of sub-words is set to 30K for each language. For the WMT corpus, we evaluate our approach on a Spanish-French (Es-Fr) translation task with a zero-resource setting. We combine the following corpora to form the Es-En and En-Fr parallel corpora: Common Crawl, News Commentary, Europarl v7 and UN. All the sentences are tokenized by the tokenize.perl script. Newstest2011 serves as the development set and Newstest2012 a"
P17-1176,P16-1162,0,0.0504544,"as a pivot language in all experiments. zero-resource scenario. To avoid the trilingual corpus constituted by the source-pivot and pivottarget corpora, we split the overlapping pivot sentences of the original source-pivot and pivot-target corpora into two equal parts and merge them separately with the non-overlapping parts for each language pair. The development and test sets are from WMT 2006 shared task.1 The evaluation metric is case-insensitive BLEU (Papineni et al., 2002) as calculated by the multi-bleu.perl script. To deal with out-of-vocabulary words, we adopt byte pair encoding (BPE) (Sennrich et al., 2016) to split words into sub-words. The size of sub-words is set to 30K for each language. For the WMT corpus, we evaluate our approach on a Spanish-French (Es-Fr) translation task with a zero-resource setting. We combine the following corpora to form the Es-En and En-Fr parallel corpora: Common Crawl, News Commentary, Europarl v7 and UN. All the sentences are tokenized by the tokenize.perl script. Newstest2011 serves as the development set and Newstest2012 and Newstest2013 serve as test sets. We use case-sensitive BLEU to evaluate translation results. BPE is also used to reduce the vocabulary siz"
P17-1176,P16-1159,1,0.856011,"otarget model parameters that minimizes the training objective: n o θˆx→y = argmin JSENT (θx→y ) . (8) JWORD (θx→y ) h i X Ey|z;θˆz→y S(x, y, z, θˆz→y , θx→y ) , (11) =− hx,zi∈Dx,z θx→y With learned source-to-target model parameters θˆx→y , we use the standard decision rule as shown ˆ for a in Equation (1) to find the translation y source sentence x. However, a major difficulty faced by our approach is the intractability in calculating the gradients because of the exponential search space of target sentences. To address this problem, it is possible to construct a sub-space by either sampling (Shen et al., 2016), generating a k-best list (Cheng et al., 2016b) or mode approximation (Kim and Rush, 2016). Then, standard stochastic gradient descent algorithms can be used to optimize model parameters. 3.3 Word-Level Teaching Instead of minimizing the KL divergence between the teacher and student models at the sentence level, we further define a training objective at the word level based on Assumption 2: hx,zi∈Dx,z where S(x, y, z, θˆz→y , θx→y ) = |y| X X j=1 y∈Vy P (y|z, y&lt;j ; θˆz→y ) × log P (y|x, y&lt;j ; θx→y ). (12) Therefore, our goal is to find a set of source-totarget model parameters that minimizes"
P17-1176,P15-1001,0,0.0350013,"urce-pivot parallel corpus. Experimental results show that the proposed method significantly improves over a baseline pivot-based model by +3.0 BLEU points across various language pairs. 1 Introduction Neural machine translation (NMT) (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015), which directly models the translation process in an end-to-end way, has attracted intensive attention from the community. Although NMT has achieved state-of-the-art translation performance on resource-rich language pairs such as English-French and German-English (Luong et al., 2015; Jean et al., 2015; Wu et al., 2016; Johnson et al., 2016), it still suffers from the unavailability of large-scale parallel corpora for translating low-resource languages. Due to the large parameter space, neural models usually learn poorly from low-count events, resulting in a poor choice for low-resource language pairs. Zoph et ∗ Corresponding author: Yang Liu. al. (2016) indicate that NMT obtains much worse translation quality than a statistical machine translation (SMT) system on low-resource languages. As a result, a number of authors have endeavored to explore methods for translating language pairs witho"
P17-1176,N07-1061,0,0.322025,"lies on the availability of large-scale parallel corpora to deliver state-of-the-art translation performance (Wu et al., 2016; Johnson et al., 2016). Zoph et al. (2016) report that NMT obtains much lower BLEU scores than SMT if only small-scale parallel corpora are available. Therefore, the heavy dependence on the quantity of training data poses a severe challenge for NMT to translate zeroresource language pairs. Simple and easy-to-implement, pivot-based methods have been widely used in SMT for 1926 translating zero-resource language pairs (de Gispert and Mari˜no, 2006; Cohn and Lapata, 2007; Utiyama and Isahara, 2007; Wu and Wang, 2007; Bertoldi et al., 2008; Wu and Wang, 2009; Zahabi et al., 2013; Kholy et al., 2013). As pivotbased methods are agnostic to model structures, they have been adapted to NMT recently (Cheng et al., 2016a; Johnson et al., 2016). Figure 1(a) illustrates the basic idea of pivotbased approaches to zero-resource NMT (Cheng et al., 2016a). Let X, Y, and Z denote source, target, and pivot languages. We use dashed lines to denote language pairs with parallel corpora available and solid lines with arrows to denote translation directions. Intuitively, the source-to-target translation ca"
P17-1176,D13-1176,0,0.0704469,"ns. In this paper, we propose a method for zero-resource NMT by assuming that parallel sentences have close probabilities of generating a sentence in a third language. Based on the assumption, our method is able to train a source-to-target NMT model (“student”) without parallel corpora available guided by an existing pivot-to-target NMT model (“teacher”) on a source-pivot parallel corpus. Experimental results show that the proposed method significantly improves over a baseline pivot-based model by +3.0 BLEU points across various language pairs. 1 Introduction Neural machine translation (NMT) (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015), which directly models the translation process in an end-to-end way, has attracted intensive attention from the community. Although NMT has achieved state-of-the-art translation performance on resource-rich language pairs such as English-French and German-English (Luong et al., 2015; Jean et al., 2015; Wu et al., 2016; Johnson et al., 2016), it still suffers from the unavailability of large-scale parallel corpora for translating low-resource languages. Due to the large parameter space, neural models usually learn poorly from low-count events, re"
P17-1176,P13-2073,0,0.036745,"Missing"
P17-1176,D16-1139,0,0.129234,"θx→y ) . (8) JWORD (θx→y ) h i X Ey|z;θˆz→y S(x, y, z, θˆz→y , θx→y ) , (11) =− hx,zi∈Dx,z θx→y With learned source-to-target model parameters θˆx→y , we use the standard decision rule as shown ˆ for a in Equation (1) to find the translation y source sentence x. However, a major difficulty faced by our approach is the intractability in calculating the gradients because of the exponential search space of target sentences. To address this problem, it is possible to construct a sub-space by either sampling (Shen et al., 2016), generating a k-best list (Cheng et al., 2016b) or mode approximation (Kim and Rush, 2016). Then, standard stochastic gradient descent algorithms can be used to optimize model parameters. 3.3 Word-Level Teaching Instead of minimizing the KL divergence between the teacher and student models at the sentence level, we further define a training objective at the word level based on Assumption 2: hx,zi∈Dx,z where S(x, y, z, θˆz→y , θx→y ) = |y| X X j=1 y∈Vy P (y|z, y&lt;j ; θˆz→y ) × log P (y|x, y&lt;j ; θx→y ). (12) Therefore, our goal is to find a set of source-totarget model parameters that minimizes the training objective: n o ˆ θx→y = argmin JWORD (θx→y ) . (13) θx→y We use similar approa"
P17-1176,2005.mtsummit-papers.11,0,0.238348,"Missing"
P17-1176,P07-1108,0,0.299129,"large-scale parallel corpora to deliver state-of-the-art translation performance (Wu et al., 2016; Johnson et al., 2016). Zoph et al. (2016) report that NMT obtains much lower BLEU scores than SMT if only small-scale parallel corpora are available. Therefore, the heavy dependence on the quantity of training data poses a severe challenge for NMT to translate zeroresource language pairs. Simple and easy-to-implement, pivot-based methods have been widely used in SMT for 1926 translating zero-resource language pairs (de Gispert and Mari˜no, 2006; Cohn and Lapata, 2007; Utiyama and Isahara, 2007; Wu and Wang, 2007; Bertoldi et al., 2008; Wu and Wang, 2009; Zahabi et al., 2013; Kholy et al., 2013). As pivotbased methods are agnostic to model structures, they have been adapted to NMT recently (Cheng et al., 2016a; Johnson et al., 2016). Figure 1(a) illustrates the basic idea of pivotbased approaches to zero-resource NMT (Cheng et al., 2016a). Let X, Y, and Z denote source, target, and pivot languages. We use dashed lines to denote language pairs with parallel corpora available and solid lines with arrows to denote translation directions. Intuitively, the source-to-target translation can be indirectly mod"
P17-1176,P09-1018,0,0.119079,"tate-of-the-art translation performance (Wu et al., 2016; Johnson et al., 2016). Zoph et al. (2016) report that NMT obtains much lower BLEU scores than SMT if only small-scale parallel corpora are available. Therefore, the heavy dependence on the quantity of training data poses a severe challenge for NMT to translate zeroresource language pairs. Simple and easy-to-implement, pivot-based methods have been widely used in SMT for 1926 translating zero-resource language pairs (de Gispert and Mari˜no, 2006; Cohn and Lapata, 2007; Utiyama and Isahara, 2007; Wu and Wang, 2007; Bertoldi et al., 2008; Wu and Wang, 2009; Zahabi et al., 2013; Kholy et al., 2013). As pivotbased methods are agnostic to model structures, they have been adapted to NMT recently (Cheng et al., 2016a; Johnson et al., 2016). Figure 1(a) illustrates the basic idea of pivotbased approaches to zero-resource NMT (Cheng et al., 2016a). Let X, Y, and Z denote source, target, and pivot languages. We use dashed lines to denote language pairs with parallel corpora available and solid lines with arrows to denote translation directions. Intuitively, the source-to-target translation can be indirectly modeled by bridging two NMT models via a pivo"
P17-1176,P13-2057,0,0.264125,"nslation performance (Wu et al., 2016; Johnson et al., 2016). Zoph et al. (2016) report that NMT obtains much lower BLEU scores than SMT if only small-scale parallel corpora are available. Therefore, the heavy dependence on the quantity of training data poses a severe challenge for NMT to translate zeroresource language pairs. Simple and easy-to-implement, pivot-based methods have been widely used in SMT for 1926 translating zero-resource language pairs (de Gispert and Mari˜no, 2006; Cohn and Lapata, 2007; Utiyama and Isahara, 2007; Wu and Wang, 2007; Bertoldi et al., 2008; Wu and Wang, 2009; Zahabi et al., 2013; Kholy et al., 2013). As pivotbased methods are agnostic to model structures, they have been adapted to NMT recently (Cheng et al., 2016a; Johnson et al., 2016). Figure 1(a) illustrates the basic idea of pivotbased approaches to zero-resource NMT (Cheng et al., 2016a). Let X, Y, and Z denote source, target, and pivot languages. We use dashed lines to denote language pairs with parallel corpora available and solid lines with arrows to denote translation directions. Intuitively, the source-to-target translation can be indirectly modeled by bridging two NMT models via a pivot: P (y|x; θx→z , θz→"
P17-1176,D13-1050,0,0.0800056,"ia a pivot, which is either text (Cheng et al., 2016a) or image (Nakayama and Nishida, 2016). Cheng et al. (2016a) propose a pivot-based method for zeroresource NMT: it first translates the source language to a pivot language, which is then translated to the target language. Nakayama and Nishida (2016) show that using multimedia information as pivot also benefits zero-resource translation. However, pivot-based approaches usually need to divide the decoding process into two steps, which is not only more computationally expensive, but also potentially suffers from the error propagation problem (Zhu et al., 2013). In this paper, we propose a new method for zero-resource neural machine translation. Our 1925 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1925–1935 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1176 X P (y|z; ✓ z!y ) P (z|x; ✓ x!z ) P (y|x; ✓ x!y ) Y X Z Z Y P (y|z; ✓ z!y ) (a) (b) Figure 1: (a) The pivot-based approach and (b) the teacher-student approach to zero-resource neural machine translation. X, Y, and Z denote source, target, and pivot languages, respect"
P17-1176,D16-1163,0,0.341448,"corpus Dx,y , which is a set of parallel source-target sentences, the model parameters can be learned by maximizing the log-likelihood of the parallel corpus: ( ) X θˆx→y = argmax log P (y|x; θx→y ) . θx→y hx,yi∈Dx,y Given learned model parameters θˆx→y , the decision rule for finding the translation with the highest probability for a source sentence x is given by ( ) ˆ = argmax P (y|x; θˆx→y ) . (1) y y As a data-driven approach, NMT heavily relies on the availability of large-scale parallel corpora to deliver state-of-the-art translation performance (Wu et al., 2016; Johnson et al., 2016). Zoph et al. (2016) report that NMT obtains much lower BLEU scores than SMT if only small-scale parallel corpora are available. Therefore, the heavy dependence on the quantity of training data poses a severe challenge for NMT to translate zeroresource language pairs. Simple and easy-to-implement, pivot-based methods have been widely used in SMT for 1926 translating zero-resource language pairs (de Gispert and Mari˜no, 2006; Cohn and Lapata, 2007; Utiyama and Isahara, 2007; Wu and Wang, 2007; Bertoldi et al., 2008; Wu and Wang, 2009; Zahabi et al., 2013; Kholy et al., 2013). As pivotbased methods are agnostic to"
P17-1179,D16-1250,0,0.277787,"(1 − D (Gx)) . (2) For simplicity, here we write the loss with a minibatch size of 1; in our experiments we use 128. The generator loss is given by LG = − log D (Gx) . (3) In line with previous work (Goodfellow et al., 2014), we find this loss easier to minimize than the original form log (1 − D (Gx)). Orthogonal Constraint The above model is very difficult to train. One possible reason is that the parameter search space Rd×d for the generator may still be too large. Previous works have attempted to constrain the transformation matrix to be orthogonal (Xing et al., 2015; Zhang et al., 2016b; Artetxe et al., 2016). An orthogonal transformation is also theoretically appealing for its self-consistency (Smith et al., 2017) and numerical stability. However, using constrained optimization for our purpose is cumbersome, so we opt for an orthogonal parametrization (Mhammedi et al., 2016) of the generator instead. Model 2: Bidirectional Transformation The orthogonal parametrization is still quite slow. We can relax the orthogonal constraint and only require the transformation to be self-consistent (Smith et al., 2017): If G transforms the source word embedding space into the target language space, its transpos"
P17-1179,W16-1614,0,0.330476,"ingual embeddings trained beforehand and held fixed. More importantly, its learning mechanism is substantially different from ours. It encourages word embeddings from different languages to lie in the shared semantic space by matching the mean and variance of the hidden states, assumed to follow a Gaussian distribution, which is hard to justify. Our approach does not make any assumptions and directly matches the mapped source embedding distribution with the target distribution by adversarial training. A recent work also attempts adversarial training for cross-lingual embedding transformation (Barone, 2016). The model architectures are similar to ours, but the reported results are not positive. We tried the publicly available code on our data, but the results were not positive, either. Therefore, we attribute the outcome to the difference in the loss and training techniques, but not the model architectures or data. 5.2 Adversarial Training Generative adversarial networks are originally proposed for generating realistic images as an implicit generative model, but the adversarial training technique for matching distributions is generalizable to much more tasks, including natural language processin"
P17-1179,C16-1171,0,0.142232,"4; Koˇcisk´y et al., 2014; Gouws et al., 2015; Luong et al., 2015; Coulmance et al., 2015; Oshikiri et al., 2016), or word level (i.e. seed lexicon) (Gouws and Søgaard, 2015; Wick et al., 2016; Duong et al., 2016; Shi et al., 2015; Mikolov et al., 2013a; Dinu et al., 2015; Lazaridou et al., 2015; Faruqui and Dyer, 2014; Lu et al., 2015; Ammar et al., 2016; Zhang et al., 2016a, 2017; Smith et al., 2017). In contrast, our work completely removes the need for cross-lingual signals to connect monolingual word embeddings, trained on non-parallel text corpora. As one of our baselines, the method by Cao et al. (2016) also does not require cross-lingual signals to train bilingual word embeddings. It modifies the objective for training embeddings, whereas our approach uses monolingual embeddings trained beforehand and held fixed. More importantly, its learning mechanism is substantially different from ours. It encourages word embeddings from different languages to lie in the shared semantic space by matching the mean and variance of the hidden states, assumed to follow a Gaussian distribution, which is hard to justify. Our approach does not make any assumptions and directly matches the mapped source embeddi"
P17-1179,D15-1131,0,0.00893287,"ns, 2013). Recent advances in cross-lingual word embeddings (Vuli´c and Korhonen, 2016; Upadhyay et al., 12 As a confirmation, we ran MonoGiza in this setting and obtained comparable performance as reported. 1966 2016) have rekindled interest in bilingual lexicon induction. Like their traditional counterparts, these embedding-based methods require crosslingual signals encoded in parallel data, aligned at document level (Vuli´c and Moens, 2015), sentence level (Zou et al., 2013; Chandar A P et al., 2014; Hermann and Blunsom, 2014; Koˇcisk´y et al., 2014; Gouws et al., 2015; Luong et al., 2015; Coulmance et al., 2015; Oshikiri et al., 2016), or word level (i.e. seed lexicon) (Gouws and Søgaard, 2015; Wick et al., 2016; Duong et al., 2016; Shi et al., 2015; Mikolov et al., 2013a; Dinu et al., 2015; Lazaridou et al., 2015; Faruqui and Dyer, 2014; Lu et al., 2015; Ammar et al., 2016; Zhang et al., 2016a, 2017; Smith et al., 2017). In contrast, our work completely removes the need for cross-lingual signals to connect monolingual word embeddings, trained on non-parallel text corpora. As one of our baselines, the method by Cao et al. (2016) also does not require cross-lingual signals to train bilingual word emb"
P17-1179,D12-1025,0,0.0126253,"e words. We generally report the harshest top-1 accuracy, unless when comparing with published figures in Section 4.4. Table 1: Statistics of the non-parallel corpora. Language codes: zh = Chinese, en = English, es = Spanish, it = Italian, ja = Japanese, tr = Turkish. • Translation matrix (TM) (Mikolov et al., 2013a): the pioneer of this type of methods mentioned in the introduction, using linear transformation. We use a publicly available implementation.3 Baselines Almost all approaches to bilingual lexicon induction from non-parallel data depend on seed lexica. An exception is decipherment (Dou and Knight, 2012; Dou et al., 2015), and we use it as our baseline. The decipherment approach is not based on distributional semantics, but rather views the source language as a cipher for the target language, and attempts to learn a statistical model to decipher the source language. We run the MonoGiza system as recommended by the toolkit.2 It can also utilize monolingual embeddings (Dou et al., 2015); in this case, we use the same embeddings as the input to our approach. Sharing the underlying spirit with our approach, related methods also build upon monolingual word embeddings and find transformation to li"
P17-1179,D13-1173,0,0.0874179,"Missing"
P17-1179,P15-1081,0,0.0134005,"report the harshest top-1 accuracy, unless when comparing with published figures in Section 4.4. Table 1: Statistics of the non-parallel corpora. Language codes: zh = Chinese, en = English, es = Spanish, it = Italian, ja = Japanese, tr = Turkish. • Translation matrix (TM) (Mikolov et al., 2013a): the pioneer of this type of methods mentioned in the introduction, using linear transformation. We use a publicly available implementation.3 Baselines Almost all approaches to bilingual lexicon induction from non-parallel data depend on seed lexica. An exception is decipherment (Dou and Knight, 2012; Dou et al., 2015), and we use it as our baseline. The decipherment approach is not based on distributional semantics, but rather views the source language as a cipher for the target language, and attempts to learn a statistical model to decipher the source language. We run the MonoGiza system as recommended by the toolkit.2 It can also utilize monolingual embeddings (Dou et al., 2015); in this case, we use the same embeddings as the input to our approach. Sharing the underlying spirit with our approach, related methods also build upon monolingual word embeddings and find transformation to link different langua"
P17-1179,E14-1049,0,0.413993,"ed interest in bilingual lexicon induction. Like their traditional counterparts, these embedding-based methods require crosslingual signals encoded in parallel data, aligned at document level (Vuli´c and Moens, 2015), sentence level (Zou et al., 2013; Chandar A P et al., 2014; Hermann and Blunsom, 2014; Koˇcisk´y et al., 2014; Gouws et al., 2015; Luong et al., 2015; Coulmance et al., 2015; Oshikiri et al., 2016), or word level (i.e. seed lexicon) (Gouws and Søgaard, 2015; Wick et al., 2016; Duong et al., 2016; Shi et al., 2015; Mikolov et al., 2013a; Dinu et al., 2015; Lazaridou et al., 2015; Faruqui and Dyer, 2014; Lu et al., 2015; Ammar et al., 2016; Zhang et al., 2016a, 2017; Smith et al., 2017). In contrast, our work completely removes the need for cross-lingual signals to connect monolingual word embeddings, trained on non-parallel text corpora. As one of our baselines, the method by Cao et al. (2016) also does not require cross-lingual signals to train bilingual word embeddings. It modifies the objective for training embeddings, whereas our approach uses monolingual embeddings trained beforehand and held fixed. More importantly, its learning mechanism is substantially different from ours. It encou"
P17-1179,W04-3208,0,0.0280075,"ts tens of translations for a source word. This lenient evaluation protocol should explain MonoGiza’s higher numbers in Table 6 than what we report in the other experiments. In this setting, our approach is able to considerably outperform both MonoGiza and the method by Cao et al. (2016). 5 5.1 Related Work Cross-Lingual Word Embeddings for Bilingual Lexicon Induction Inducing bilingual lexica from non-parallel data is a long-standing cross-lingual task. Except for the decipherment approach, traditional statistical methods all require cross-lingual signals (Rapp, 1999; Koehn and Knight, 2002; Fung and Cheung, 2004; Gaussier et al., 2004; Haghighi et al., 2008; Vuli´c et al., 2011; Vuli´c and Moens, 2013). Recent advances in cross-lingual word embeddings (Vuli´c and Korhonen, 2016; Upadhyay et al., 12 As a confirmation, we ran MonoGiza in this setting and obtained comparable performance as reported. 1966 2016) have rekindled interest in bilingual lexicon induction. Like their traditional counterparts, these embedding-based methods require crosslingual signals encoded in parallel data, aligned at document level (Vuli´c and Moens, 2015), sentence level (Zou et al., 2013; Chandar A P et al., 2014; Hermann"
P17-1179,P04-1067,0,0.0800296,"for a source word. This lenient evaluation protocol should explain MonoGiza’s higher numbers in Table 6 than what we report in the other experiments. In this setting, our approach is able to considerably outperform both MonoGiza and the method by Cao et al. (2016). 5 5.1 Related Work Cross-Lingual Word Embeddings for Bilingual Lexicon Induction Inducing bilingual lexica from non-parallel data is a long-standing cross-lingual task. Except for the decipherment approach, traditional statistical methods all require cross-lingual signals (Rapp, 1999; Koehn and Knight, 2002; Fung and Cheung, 2004; Gaussier et al., 2004; Haghighi et al., 2008; Vuli´c et al., 2011; Vuli´c and Moens, 2013). Recent advances in cross-lingual word embeddings (Vuli´c and Korhonen, 2016; Upadhyay et al., 12 As a confirmation, we ran MonoGiza in this setting and obtained comparable performance as reported. 1966 2016) have rekindled interest in bilingual lexicon induction. Like their traditional counterparts, these embedding-based methods require crosslingual signals encoded in parallel data, aligned at document level (Vuli´c and Moens, 2015), sentence level (Zou et al., 2013; Chandar A P et al., 2014; Hermann and Blunsom, 2014; Koˇc"
P17-1179,P11-2071,0,0.016233,"Missing"
P17-1179,N15-1157,0,0.0434591,"016; Upadhyay et al., 12 As a confirmation, we ran MonoGiza in this setting and obtained comparable performance as reported. 1966 2016) have rekindled interest in bilingual lexicon induction. Like their traditional counterparts, these embedding-based methods require crosslingual signals encoded in parallel data, aligned at document level (Vuli´c and Moens, 2015), sentence level (Zou et al., 2013; Chandar A P et al., 2014; Hermann and Blunsom, 2014; Koˇcisk´y et al., 2014; Gouws et al., 2015; Luong et al., 2015; Coulmance et al., 2015; Oshikiri et al., 2016), or word level (i.e. seed lexicon) (Gouws and Søgaard, 2015; Wick et al., 2016; Duong et al., 2016; Shi et al., 2015; Mikolov et al., 2013a; Dinu et al., 2015; Lazaridou et al., 2015; Faruqui and Dyer, 2014; Lu et al., 2015; Ammar et al., 2016; Zhang et al., 2016a, 2017; Smith et al., 2017). In contrast, our work completely removes the need for cross-lingual signals to connect monolingual word embeddings, trained on non-parallel text corpora. As one of our baselines, the method by Cao et al. (2016) also does not require cross-lingual signals to train bilingual word embeddings. It modifies the objective for training embeddings, whereas our approach use"
P17-1179,W13-2233,0,0.0233917,"v et al., 2013a). Although trained independently, the two sets of embeddings exhibit approximate isomorphism. Introduction ∗ horse et al. (2013a) observe that word embeddings trained separately on monolingual corpora exhibit isomorphic structure across languages, as illustrated in Figure 1. This interesting finding is in line with research on human cognition (Youn et al., 2016). It also means a linear transformation may be established to connect word embedding spaces, allowing word feature transfer. This has far-reaching implication on low-resource scenarios (Daum´e III and Jagarlamudi, 2011; Irvine and Callison-Burch, 2013), because word embeddings only require plain text to train, which is the most abundant form of linguistic resource. However, connecting separate word embedding spaces typically requires supervision from crosslingual signals. For example, Mikolov et al. (2013a) use five thousand seed word translation pairs to train the linear transformation. In a recent study, Vuli´c and Korhonen (2016) show that at least hundreds of seed word translation pairs are needed for the model to generalize. This is unfortunate for low-resource languages and domains, 1959 Proceedings of the 55th Annual Meeting of the A"
P17-1179,W02-0902,0,0.051305,"uage pairs; it often lists tens of translations for a source word. This lenient evaluation protocol should explain MonoGiza’s higher numbers in Table 6 than what we report in the other experiments. In this setting, our approach is able to considerably outperform both MonoGiza and the method by Cao et al. (2016). 5 5.1 Related Work Cross-Lingual Word Embeddings for Bilingual Lexicon Induction Inducing bilingual lexica from non-parallel data is a long-standing cross-lingual task. Except for the decipherment approach, traditional statistical methods all require cross-lingual signals (Rapp, 1999; Koehn and Knight, 2002; Fung and Cheung, 2004; Gaussier et al., 2004; Haghighi et al., 2008; Vuli´c et al., 2011; Vuli´c and Moens, 2013). Recent advances in cross-lingual word embeddings (Vuli´c and Korhonen, 2016; Upadhyay et al., 12 As a confirmation, we ran MonoGiza in this setting and obtained comparable performance as reported. 1966 2016) have rekindled interest in bilingual lexicon induction. Like their traditional counterparts, these embedding-based methods require crosslingual signals encoded in parallel data, aligned at document level (Vuli´c and Moens, 2015), sentence level (Zou et al., 2013; Chandar A P"
P17-1179,P16-2080,0,0.0145854,"es in cross-lingual word embeddings (Vuli´c and Korhonen, 2016; Upadhyay et al., 12 As a confirmation, we ran MonoGiza in this setting and obtained comparable performance as reported. 1966 2016) have rekindled interest in bilingual lexicon induction. Like their traditional counterparts, these embedding-based methods require crosslingual signals encoded in parallel data, aligned at document level (Vuli´c and Moens, 2015), sentence level (Zou et al., 2013; Chandar A P et al., 2014; Hermann and Blunsom, 2014; Koˇcisk´y et al., 2014; Gouws et al., 2015; Luong et al., 2015; Coulmance et al., 2015; Oshikiri et al., 2016), or word level (i.e. seed lexicon) (Gouws and Søgaard, 2015; Wick et al., 2016; Duong et al., 2016; Shi et al., 2015; Mikolov et al., 2013a; Dinu et al., 2015; Lazaridou et al., 2015; Faruqui and Dyer, 2014; Lu et al., 2015; Ammar et al., 2016; Zhang et al., 2016a, 2017; Smith et al., 2017). In contrast, our work completely removes the need for cross-lingual signals to connect monolingual word embeddings, trained on non-parallel text corpora. As one of our baselines, the method by Cao et al. (2016) also does not require cross-lingual signals to train bilingual word embeddings. It modifies the"
P17-1179,P14-2037,0,0.0423755,"Missing"
P17-1179,P15-1027,0,0.420011,"[log (1 − D (G (x)))] . Theoretical analysis reveals that adversarial training tries to minimize the Jensen-Shannon divergence JSD py ||pf (x) (Goodfellow et al., 2014). Importantly, the minimization happens at the distribution level, without requiring word 1960 2.2 translation pairs to supervise training. 2.1 Model 1: Unidirectional Transformation The first model directly implements the adversarial game, as shown in Figure 2(a). As hinted by the isomorphism shown in Figure 1, previous works typically choose the mapping function f to be a linear map (Mikolov et al., 2013a; Dinu et al., 2015; Lazaridou et al., 2015). We therefore parametrize the generator as a transformation matrix G ∈ Rd×d . We also tried non-linear maps parametrized by neural networks, without success. In fact, if the generator is given sufficient capacity, it can in principle learn a constant mapping function to a target word embedding, which makes the discriminator impossible to distinguish, much like the “mode collapse” problem widely observed in the image domain (Radford et al., 2015; Salimans et al., 2016). We therefore believe it is crucial to grant the generator with suitable capacity. As a generic binary classifier, a standard"
P17-1179,N15-1028,0,0.090077,"lexicon induction. Like their traditional counterparts, these embedding-based methods require crosslingual signals encoded in parallel data, aligned at document level (Vuli´c and Moens, 2015), sentence level (Zou et al., 2013; Chandar A P et al., 2014; Hermann and Blunsom, 2014; Koˇcisk´y et al., 2014; Gouws et al., 2015; Luong et al., 2015; Coulmance et al., 2015; Oshikiri et al., 2016), or word level (i.e. seed lexicon) (Gouws and Søgaard, 2015; Wick et al., 2016; Duong et al., 2016; Shi et al., 2015; Mikolov et al., 2013a; Dinu et al., 2015; Lazaridou et al., 2015; Faruqui and Dyer, 2014; Lu et al., 2015; Ammar et al., 2016; Zhang et al., 2016a, 2017; Smith et al., 2017). In contrast, our work completely removes the need for cross-lingual signals to connect monolingual word embeddings, trained on non-parallel text corpora. As one of our baselines, the method by Cao et al. (2016) also does not require cross-lingual signals to train bilingual word embeddings. It modifies the objective for training embeddings, whereas our approach uses monolingual embeddings trained beforehand and held fixed. More importantly, its learning mechanism is substantially different from ours. It encourages word embedd"
P17-1179,W15-1521,0,0.245132,"Missing"
P17-1179,P99-1067,0,0.436112,"e other language pairs; it often lists tens of translations for a source word. This lenient evaluation protocol should explain MonoGiza’s higher numbers in Table 6 than what we report in the other experiments. In this setting, our approach is able to considerably outperform both MonoGiza and the method by Cao et al. (2016). 5 5.1 Related Work Cross-Lingual Word Embeddings for Bilingual Lexicon Induction Inducing bilingual lexica from non-parallel data is a long-standing cross-lingual task. Except for the decipherment approach, traditional statistical methods all require cross-lingual signals (Rapp, 1999; Koehn and Knight, 2002; Fung and Cheung, 2004; Gaussier et al., 2004; Haghighi et al., 2008; Vuli´c et al., 2011; Vuli´c and Moens, 2013). Recent advances in cross-lingual word embeddings (Vuli´c and Korhonen, 2016; Upadhyay et al., 12 As a confirmation, we ran MonoGiza in this setting and obtained comparable performance as reported. 1966 2016) have rekindled interest in bilingual lexicon induction. Like their traditional counterparts, these embedding-based methods require crosslingual signals encoded in parallel data, aligned at document level (Vuli´c and Moens, 2015), sentence level (Zou e"
P17-1179,P15-2093,1,0.793518,"his setting and obtained comparable performance as reported. 1966 2016) have rekindled interest in bilingual lexicon induction. Like their traditional counterparts, these embedding-based methods require crosslingual signals encoded in parallel data, aligned at document level (Vuli´c and Moens, 2015), sentence level (Zou et al., 2013; Chandar A P et al., 2014; Hermann and Blunsom, 2014; Koˇcisk´y et al., 2014; Gouws et al., 2015; Luong et al., 2015; Coulmance et al., 2015; Oshikiri et al., 2016), or word level (i.e. seed lexicon) (Gouws and Søgaard, 2015; Wick et al., 2016; Duong et al., 2016; Shi et al., 2015; Mikolov et al., 2013a; Dinu et al., 2015; Lazaridou et al., 2015; Faruqui and Dyer, 2014; Lu et al., 2015; Ammar et al., 2016; Zhang et al., 2016a, 2017; Smith et al., 2017). In contrast, our work completely removes the need for cross-lingual signals to connect monolingual word embeddings, trained on non-parallel text corpora. As one of our baselines, the method by Cao et al. (2016) also does not require cross-lingual signals to train bilingual word embeddings. It modifies the objective for training embeddings, whereas our approach uses monolingual embeddings trained beforehand and held fixe"
P17-1179,N15-1104,0,0.348617,"lue function (1): LD = − log D (y) − log (1 − D (Gx)) . (2) For simplicity, here we write the loss with a minibatch size of 1; in our experiments we use 128. The generator loss is given by LG = − log D (Gx) . (3) In line with previous work (Goodfellow et al., 2014), we find this loss easier to minimize than the original form log (1 − D (Gx)). Orthogonal Constraint The above model is very difficult to train. One possible reason is that the parameter search space Rd×d for the generator may still be too large. Previous works have attempted to constrain the transformation matrix to be orthogonal (Xing et al., 2015; Zhang et al., 2016b; Artetxe et al., 2016). An orthogonal transformation is also theoretically appealing for its self-consistency (Smith et al., 2017) and numerical stability. However, using constrained optimization for our purpose is cumbersome, so we opt for an orthogonal parametrization (Mhammedi et al., 2016) of the generator instead. Model 2: Bidirectional Transformation The orthogonal parametrization is still quite slow. We can relax the orthogonal constraint and only require the transformation to be self-consistent (Smith et al., 2017): If G transforms the source word embedding space"
P17-1179,L16-1521,0,0.0110972,"Missing"
P17-1179,P16-1157,0,0.0374091,"Missing"
P17-1179,C16-1300,1,0.877671,"Missing"
P17-1179,P16-1024,0,0.258895,"Missing"
P17-1179,N13-1011,0,0.0392668,"Missing"
P17-1179,P15-2118,0,0.0717992,"Missing"
P17-1179,N16-1156,0,0.306912,"D = − log D (y) − log (1 − D (Gx)) . (2) For simplicity, here we write the loss with a minibatch size of 1; in our experiments we use 128. The generator loss is given by LG = − log D (Gx) . (3) In line with previous work (Goodfellow et al., 2014), we find this loss easier to minimize than the original form log (1 − D (Gx)). Orthogonal Constraint The above model is very difficult to train. One possible reason is that the parameter search space Rd×d for the generator may still be too large. Previous works have attempted to constrain the transformation matrix to be orthogonal (Xing et al., 2015; Zhang et al., 2016b; Artetxe et al., 2016). An orthogonal transformation is also theoretically appealing for its self-consistency (Smith et al., 2017) and numerical stability. However, using constrained optimization for our purpose is cumbersome, so we opt for an orthogonal parametrization (Mhammedi et al., 2016) of the generator instead. Model 2: Bidirectional Transformation The orthogonal parametrization is still quite slow. We can relax the orthogonal constraint and only require the transformation to be self-consistent (Smith et al., 2017): If G transforms the source word embedding space into the target lang"
P17-1179,D13-1141,0,0.0617117,"1999; Koehn and Knight, 2002; Fung and Cheung, 2004; Gaussier et al., 2004; Haghighi et al., 2008; Vuli´c et al., 2011; Vuli´c and Moens, 2013). Recent advances in cross-lingual word embeddings (Vuli´c and Korhonen, 2016; Upadhyay et al., 12 As a confirmation, we ran MonoGiza in this setting and obtained comparable performance as reported. 1966 2016) have rekindled interest in bilingual lexicon induction. Like their traditional counterparts, these embedding-based methods require crosslingual signals encoded in parallel data, aligned at document level (Vuli´c and Moens, 2015), sentence level (Zou et al., 2013; Chandar A P et al., 2014; Hermann and Blunsom, 2014; Koˇcisk´y et al., 2014; Gouws et al., 2015; Luong et al., 2015; Coulmance et al., 2015; Oshikiri et al., 2016), or word level (i.e. seed lexicon) (Gouws and Søgaard, 2015; Wick et al., 2016; Duong et al., 2016; Shi et al., 2015; Mikolov et al., 2013a; Dinu et al., 2015; Lazaridou et al., 2015; Faruqui and Dyer, 2014; Lu et al., 2015; Ammar et al., 2016; Zhang et al., 2016a, 2017; Smith et al., 2017). In contrast, our work completely removes the need for cross-lingual signals to connect monolingual word embeddings, trained on non-parallel t"
P17-1179,P11-2084,0,0.0121872,"Missing"
P18-1163,D18-1549,0,0.0245783,"k that augments the training data with monolingual corpora (Sennrich et al., 2016a; Cheng et al., 2016; He et al., 2016a; Zhang and Zong, 2016). They all leverage complex models such as inverse NMT models to generate translation equivalents for monolingual corpora. Then they augment the parallel corpora with these pseudo corpora to improve NMT models. Some authors have recently endeavored to achieve zero-shot NMT through transferring knowledge from bilingual corpora of other language pairs (Chen et al., 2017; Zheng et al., 2017; Cheng et al., 2017) or monolingual corpora (Lample et al., 2018; Artetxe et al., 2018). Our work significantly differs from these work. We do not resort to any complicated models to generate perturbed data and do not depend on extra monolingual or bilingual corpora. The way we exploit is more convenient and easy to implement. We focus more on improving the robustness of NMT models. 6 Conclusion We have proposed adversarial stability training to improve the robustness of NMT models. The basic idea is to train both the encoder and decoder robust to input perturbations by enabling them to behave similarly for the original input and its perturbed counterpart. We propose two approac"
P18-1163,P17-1176,1,0.843537,"ugmentation has the capability to improve the robustness of NMT models. In NMT, there is a number of work that augments the training data with monolingual corpora (Sennrich et al., 2016a; Cheng et al., 2016; He et al., 2016a; Zhang and Zong, 2016). They all leverage complex models such as inverse NMT models to generate translation equivalents for monolingual corpora. Then they augment the parallel corpora with these pseudo corpora to improve NMT models. Some authors have recently endeavored to achieve zero-shot NMT through transferring knowledge from bilingual corpora of other language pairs (Chen et al., 2017; Zheng et al., 2017; Cheng et al., 2017) or monolingual corpora (Lample et al., 2018; Artetxe et al., 2018). Our work significantly differs from these work. We do not resort to any complicated models to generate perturbed data and do not depend on extra monolingual or bilingual corpora. The way we exploit is more convenient and easy to implement. We focus more on improving the robustness of NMT models. 6 Conclusion We have proposed adversarial stability training to improve the robustness of NMT models. The basic idea is to train both the encoder and decoder robust to input perturbations by en"
P18-1163,D14-1181,0,0.00580016,"f model parameters using Adam SGD (Kingma and Ba, 2015). Its learning rate is initially set to 0.05 and varies according to the formula in Vaswani et al. (2017). Our adversarial stability training initializes the model based on the parameters trained by maximum likelihood estimation (MLE). We denote adversarial stability training based on lexical-level perturbations and feature-level perturbations respectively as ASTlexical and ASTfeature . We only sample one perturbed neighbour x0 ∈ N (x) for training efficiency. For the discriminator used in Linv , we adopt the CNN discriminator proposed by Kim (2014) to address the variable-length problem of the sequence generated by the encoder. In the CNN discriminator, the filter windows are set to 3, 4, 5 and rectified linear units are applied after convolution operations. We tune the hyperparameters on the validation set through a grid search. We find that both the optimal values of α and β are set to 1.0. The standard variance in Gaussian noise used in the formula (6) is set to 0.01. The number of words that are replaced in the sentence x during lexical-level perturbations is taken as max(0.2|x|, 1) in which |x |is the length of x. The default beam"
P18-1163,P16-1185,1,0.558429,"robustness of networks (Goodfellow et al., 2015; Miyato et al., 2016; Zheng et al., 2016). Belinkov and Bisk (2018) introduce adversarial examples to training data for character-based NMT models. In contrast to theirs, adversarial stability training aims to stabilize both the encoder and decoder in NMT models. We adopt adversarial learning to learn the perturbation-invariant encoder. Data Augmentation Data augmentation has the capability to improve the robustness of NMT models. In NMT, there is a number of work that augments the training data with monolingual corpora (Sennrich et al., 2016a; Cheng et al., 2016; He et al., 2016a; Zhang and Zong, 2016). They all leverage complex models such as inverse NMT models to generate translation equivalents for monolingual corpora. Then they augment the parallel corpora with these pseudo corpora to improve NMT models. Some authors have recently endeavored to achieve zero-shot NMT through transferring knowledge from bilingual corpora of other language pairs (Chen et al., 2017; Zheng et al., 2017; Cheng et al., 2017) or monolingual corpora (Lample et al., 2018; Artetxe et al., 2018). Our work significantly differs from these work. We do not resort to any complic"
P18-1163,D17-1230,0,0.0323464,"bout 100K iterations, which indicates that discriminator outputs probability 0.5 for both positive and negative samples and it cannot distinguish them. Thus the behaviors of the encoder for x and its perturbed neighbour x0 perform nearly consistently. 5 Related Work Our work is inspired by two lines of research: (1) adversarial learning and (2) data augmentation. Adversarial Learning Generative Adversarial Network (GAN) (Goodfellow et al., 2014) and its related derivative have been widely applied in computer vision (Radford et al., 2015; Salimans et al., 2016) and natural language processing (Li et al., 2017; Yang et al., 2018). Previous work has constructed adversarial examples to attack trained networks and make networks resist them, which has proved to improve the robustness of networks (Goodfellow et al., 2015; Miyato et al., 2016; Zheng et al., 2016). Belinkov and Bisk (2018) introduce adversarial examples to training data for character-based NMT models. In contrast to theirs, adversarial stability training aims to stabilize both the encoder and decoder in NMT models. We adopt adversarial learning to learn the perturbation-invariant encoder. Data Augmentation Data augmentation has the capabi"
P18-1163,D15-1166,0,0.12942,"Missing"
P18-1163,P02-1040,0,0.102639,"of x and y, we also construct a mini-batch consisting of the perturbed neighbour x0 and y. We propagate the information to calculate these three loss functions according to arrows. Then, gradients are collected to update three sets of model parameters. Except for the gradients of Linv with respect to θenc are multiplying by −1, other gradients are normally backpropagated. Note that we update θinv and θenc simultaneously for training efficiency. 4 4.1 Experiments Setup We evaluated our adversarial stability training on translation tasks of several language pairs, and reported the 4-gram BLEU (Papineni et al., 2002) score as calculated by the multi-bleu.perl script. Chinese-English We used the LDC corpus consisting of 1.25M sentence pairs with 27.9M Chinese words and 34.5M English words respectively. We selected the best model using the NIST 2006 set as the validation set (hyper-parameter optimization and model selection). The NIST 2002, 2003, 2004, 2005, and 2008 datasets are used as test sets. English-German We used the WMT 14 corpus containing 4.5M sentence pairs with 118M English words and 111M German words. The validation set is newstest2013, and the test set is newstest2014. English-French We used"
P18-1163,P16-1009,0,0.199849,"on set is newstest2013, and the test set is newstest2014. English-French We used the IWSLT corpus which contains 0.22M sentence pairs with 4.03M English words and 4.12M French words. The IWLST corpus is very dissimilar from the NIST and WMT corpora. As they are collected from TED talks and inclined to spoken language, we want to verify our approaches on the nonnormative text. The IWSLT 14 test set is taken as the validation set and 15 test set is used as the test set. For English-German and English-French, we tokenize both English, German and French words using tokenize.perl script. We follow Sennrich et al. (2016b) to split words into subword units. The numbers of merge operations in byte pair encoding (BPE) are set to 30K, 40K and 30K respectively for Chinese-English, English-German, and English-French. We report the case-sensitive tokenized BLEU score for English-German and English-French and the caseinsensitive tokenized BLEU score for ChineseEnglish. Our baseline system is an in-house NMT system. Following Bahdanau et al. (2015), we implement an RNN-based NMT in which both the encoder and decoder are two-layer RNNs with residual connections between layers (He et al., 2016b). The gating mechanism o"
P18-1163,P16-1162,0,0.426482,"on set is newstest2013, and the test set is newstest2014. English-French We used the IWSLT corpus which contains 0.22M sentence pairs with 4.03M English words and 4.12M French words. The IWLST corpus is very dissimilar from the NIST and WMT corpora. As they are collected from TED talks and inclined to spoken language, we want to verify our approaches on the nonnormative text. The IWSLT 14 test set is taken as the validation set and 15 test set is used as the test set. For English-German and English-French, we tokenize both English, German and French words using tokenize.perl script. We follow Sennrich et al. (2016b) to split words into subword units. The numbers of merge operations in byte pair encoding (BPE) are set to 30K, 40K and 30K respectively for Chinese-English, English-German, and English-French. We report the case-sensitive tokenized BLEU score for English-German and English-French and the caseinsensitive tokenized BLEU score for ChineseEnglish. Our baseline system is an in-house NMT system. Following Bahdanau et al. (2015), we implement an RNN-based NMT in which both the encoder and decoder are two-layer RNNs with residual connections between layers (He et al., 2016b). The gating mechanism o"
P18-1163,P16-1159,1,0.736098,"on the validation set through a grid search. We find that both the optimal values of α and β are set to 1.0. The standard variance in Gaussian noise used in the formula (6) is set to 0.01. The number of words that are replaced in the sentence x during lexical-level perturbations is taken as max(0.2|x|, 1) in which |x |is the length of x. The default beam size for decoding is 10. 4.2 4.2.1 Translation Results NIST Chinese-English Translation Table 2 shows the results on Chinese-English translation. Our strong baseline system significantly outperforms previously reported results on 1760 System Shen et al. (2016) Wang et al. (2017) Zhang et al. (2018) this work Training MRT MLE MLE MLE ASTlexical ASTfeature MT06 37.34 37.29 38.38 41.38 43.57 44.44 MT02 40.36 – – 43.52 44.82 46.10 MT03 40.93 39.35 40.02 41.50 42.95 44.07 MT04 41.37 41.15 42.32 43.64 45.05 45.61 MT05 38.81 38.07 38.84 41.58 43.45 44.06 MT08 29.23 – – 31.60 34.85 34.94 Table 2: Case-insensitive BLEU scores on Chinese-English translation. System Shen et al. (2016) Luong et al. (2015) Kalchbrenner et al. (2017) Wang et al. (2017) Wu et al. (2016) Gehring et al. (2017) Vaswani et al. (2017) Architecture Gated RNN with 1 layer LSTM with 4 la"
P18-1163,P17-1013,0,0.0910566,"set through a grid search. We find that both the optimal values of α and β are set to 1.0. The standard variance in Gaussian noise used in the formula (6) is set to 0.01. The number of words that are replaced in the sentence x during lexical-level perturbations is taken as max(0.2|x|, 1) in which |x |is the length of x. The default beam size for decoding is 10. 4.2 4.2.1 Translation Results NIST Chinese-English Translation Table 2 shows the results on Chinese-English translation. Our strong baseline system significantly outperforms previously reported results on 1760 System Shen et al. (2016) Wang et al. (2017) Zhang et al. (2018) this work Training MRT MLE MLE MLE ASTlexical ASTfeature MT06 37.34 37.29 38.38 41.38 43.57 44.44 MT02 40.36 – – 43.52 44.82 46.10 MT03 40.93 39.35 40.02 41.50 42.95 44.07 MT04 41.37 41.15 42.32 43.64 45.05 45.61 MT05 38.81 38.07 38.84 41.58 43.45 44.06 MT08 29.23 – – 31.60 34.85 34.94 Table 2: Case-insensitive BLEU scores on Chinese-English translation. System Shen et al. (2016) Luong et al. (2015) Kalchbrenner et al. (2017) Wang et al. (2017) Wu et al. (2016) Gehring et al. (2017) Vaswani et al. (2017) Architecture Gated RNN with 1 layer LSTM with 4 layers ByteNet with 3"
P18-1163,N18-1122,0,0.0232541,"ons, which indicates that discriminator outputs probability 0.5 for both positive and negative samples and it cannot distinguish them. Thus the behaviors of the encoder for x and its perturbed neighbour x0 perform nearly consistently. 5 Related Work Our work is inspired by two lines of research: (1) adversarial learning and (2) data augmentation. Adversarial Learning Generative Adversarial Network (GAN) (Goodfellow et al., 2014) and its related derivative have been widely applied in computer vision (Radford et al., 2015; Salimans et al., 2016) and natural language processing (Li et al., 2017; Yang et al., 2018). Previous work has constructed adversarial examples to attack trained networks and make networks resist them, which has proved to improve the robustness of networks (Goodfellow et al., 2015; Miyato et al., 2016; Zheng et al., 2016). Belinkov and Bisk (2018) introduce adversarial examples to training data for character-based NMT models. In contrast to theirs, adversarial stability training aims to stabilize both the encoder and decoder in NMT models. We adopt adversarial learning to learn the perturbation-invariant encoder. Data Augmentation Data augmentation has the capability to improve the"
P18-1163,D16-1160,0,0.0309436,"al., 2015; Miyato et al., 2016; Zheng et al., 2016). Belinkov and Bisk (2018) introduce adversarial examples to training data for character-based NMT models. In contrast to theirs, adversarial stability training aims to stabilize both the encoder and decoder in NMT models. We adopt adversarial learning to learn the perturbation-invariant encoder. Data Augmentation Data augmentation has the capability to improve the robustness of NMT models. In NMT, there is a number of work that augments the training data with monolingual corpora (Sennrich et al., 2016a; Cheng et al., 2016; He et al., 2016a; Zhang and Zong, 2016). They all leverage complex models such as inverse NMT models to generate translation equivalents for monolingual corpora. Then they augment the parallel corpora with these pseudo corpora to improve NMT models. Some authors have recently endeavored to achieve zero-shot NMT through transferring knowledge from bilingual corpora of other language pairs (Chen et al., 2017; Zheng et al., 2017; Cheng et al., 2017) or monolingual corpora (Lample et al., 2018; Artetxe et al., 2018). Our work significantly differs from these work. We do not resort to any complicated models to generate perturbed data an"
P19-1352,P18-2049,0,0.0200019,"This is similar to the Chinese word “sangsheng” (paired with “killed”) and the English words “died” and “killed”. Figure 6(c) shows that the representations of the Chinese and English words which relate to “president” are very close. 4 Related Work Many previous works focus on improving the word representations of NMT by capturing the fine-grained (character) or coarse-grained (sub-word) monolingual characteristics, such as character-based NMT (Costa-Juss`a and Fonollosa, 2016; Ling et al., 2015; Cho et al., 2014; Chen et al., 2016), sub-word NMT (Sennrich et al., 2016b; Johnson et al., 2017; Ataman and Federico, 2018), and hybrid NMT (Luong and Manning, 2016). They effectively consider and utilize the morphological information to enhance the word representations. Our work aims to enhance word representations through the bilingual features that are cooperatively learned by the source and target words. Recently, Gu et al. (2018) propose to use the pre-trained target (English) embeddings as a universal representation to improve the representation learning of the source (low-resource) languages. 3620 In our work, both the source and target embeddings can make use of the common representation unit, i.e. the sou"
P19-1352,P18-1008,0,0.0298591,"n vectors. While in (b), the two word embeddings are made up of two parts, indicating the shared (lined nodes) and the private (unlined nodes) features. This enables the two words to make use of common representation units, leading to a closer relationship between them. With the introduction of ever more powerful architectures, neural machine translation (NMT) has become the most promising machine translation method (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015). For word representation, different architectures— including, but not limited to, recurrence-based (Chen et al., 2018), convolution-based (Gehring et al., 2017) and transformation-based (Vaswani et al., 2017) NMT models—have been taking advantage of the distributed word embeddings to capture the syntactic and semantic properties of words (Turian et al., 2010). Corresponding author Rd (a) Standard Introduction ∗ Long NMT usually utilizes three matrices to represent source embeddings, target input embeddings, and target output embeddings (also known as pre-softmax weight), respectively. These embeddings occupy most of the model parameters, which constrains the improvements of NMT because the recent methods beco"
P19-1352,P16-1186,0,0.0318903,"ion. These words are often treated as noises and they are generally ignored 3615 Pxlm ∈ R2×2 Slm ∈ R2×3 Long (Lange) Ju@@( (Ju@@) Sur ∈ R2×1 Sundial (Fiehlt) Exwf ∈ R2×5 ⊕ → ˜ ⊕ De@@( (De@@) Laden (Bericht) Long Pxwf ∈ R2×3 Swf ∈ R2×2 Ex ∈ R6×5 → ˜ ⊕ Italy (Italien) Exlm ∈ R2×5 Italy → Ju@@ De@@ Pxur ∈ R2×4 ˜ ⊕ Exur ∈ R2×5 ⊕ Laden Sundial → Figure 3: The example of assembling the source word embedding matrix. The words in parentheses denote the paired words sharing features with them. by the NMT systems (Feng et al., 2017). Motivated by the frequency clustering methods proposed by Chen et al. (2016) where they cluster the words with similar frequency for training a hierarchical language model, in this work, we propose to use a small vector to model the possible features that might be shared between the source and target words which are unrelated but having similar word frequencies. In addition, it can be regarded as a way to improve the robustness of learning the embeddings of low-frequency words because of the noisy dimensions (Wang et al., 2018). 2.2 3 Implementation Before looking up embedding at each training step, the source and target embedding matrix are assembled by the sub-embed"
P19-1352,P16-2058,0,0.0190662,"ion. These words are often treated as noises and they are generally ignored 3615 Pxlm ∈ R2×2 Slm ∈ R2×3 Long (Lange) Ju@@( (Ju@@) Sur ∈ R2×1 Sundial (Fiehlt) Exwf ∈ R2×5 ⊕ → ˜ ⊕ De@@( (De@@) Laden (Bericht) Long Pxwf ∈ R2×3 Swf ∈ R2×2 Ex ∈ R6×5 → ˜ ⊕ Italy (Italien) Exlm ∈ R2×5 Italy → Ju@@ De@@ Pxur ∈ R2×4 ˜ ⊕ Exur ∈ R2×5 ⊕ Laden Sundial → Figure 3: The example of assembling the source word embedding matrix. The words in parentheses denote the paired words sharing features with them. by the NMT systems (Feng et al., 2017). Motivated by the frequency clustering methods proposed by Chen et al. (2016) where they cluster the words with similar frequency for training a hierarchical language model, in this work, we propose to use a small vector to model the possible features that might be shared between the source and target words which are unrelated but having similar word frequencies. In addition, it can be regarded as a way to improve the robustness of learning the embeddings of low-frequency words because of the noisy dimensions (Wang et al., 2018). 2.2 3 Implementation Before looking up embedding at each training step, the source and target embedding matrix are assembled by the sub-embed"
P19-1352,P17-1106,1,0.801916,"ical meaning. Based on these observations, we find that the alignment quality is not a key factor affecting the model performance. In contrast, pairing as many as similar words possible helps the model to better learn the bilingual vector space, which improves the translation performance. The following qualitative analyses support these observations either. 3.5 Analysis of the Translation Results Table 6 shows two translation examples of the NIST Chinese-English translation task. To better understand the translations produced by these two models, we use layer-wise relevance propagation (LRP) (Ding et al., 2017) to produce the attention maps of the selected translations, as shown in Figure 4 and 5. In the first example, the Chinese word “sangsheng” is a low-frequency word and its ground truth is “killed”. It is observed the inadequate representation of “sangsheng” leads to a decline in the translation quality of the vanilla, direct bridging, and decoder WT methods. In our proposed 3619 president chief 0 zongtong also zhuxi sangsheng killed −0.1 died zhuxi president zongtong chief also 0.3 yebing 0.35 juzhang president chairman zongtong weiyuanzhang chief premier 0.3 0.2 −0.2 bing −0.3 ye −0.4 −0.2 0"
P19-1352,N13-1073,0,0.201607,"as parallel words that are the translation of each other. According to the word frequency, each source word x is paired with a target aligned word yˆ that has the highest alignment probability among the candidates, and is computed as follows: yˆ = arg max logA(y|x) (1) y∈a(x) where a(·) denotes the set of aligned candidates. It is worth noting the target words that have been paired with the source words cannot be used as candidates. A(·|·) denotes the alignment probability. These can be obtained by either the intrinsic attention mechanism (Bahdanau et al., 2015) or unsupervised word aligner (Dyer et al., 2013). 2.1.2 Words with Same Word Form As shown in Figure 2(b), the sub-word “Ju@@” simultaneously exists in English and German sentences. This kind of word tends to share a medium number of features of the word embeddings. Most of the time, the source and target words with the same word form also share similar lexical meaning. This category of words generally includes Arabic numbers, punctuations, named entities, cognates and loanwords. However, there are some bilingual homographs where the words in the source and target languages look the same but have completely different meanings. For example,"
P19-1352,D17-1146,0,0.228186,"Missing"
P19-1352,N18-1032,0,0.0214543,"pturing the fine-grained (character) or coarse-grained (sub-word) monolingual characteristics, such as character-based NMT (Costa-Juss`a and Fonollosa, 2016; Ling et al., 2015; Cho et al., 2014; Chen et al., 2016), sub-word NMT (Sennrich et al., 2016b; Johnson et al., 2017; Ataman and Federico, 2018), and hybrid NMT (Luong and Manning, 2016). They effectively consider and utilize the morphological information to enhance the word representations. Our work aims to enhance word representations through the bilingual features that are cooperatively learned by the source and target words. Recently, Gu et al. (2018) propose to use the pre-trained target (English) embeddings as a universal representation to improve the representation learning of the source (low-resource) languages. 3620 In our work, both the source and target embeddings can make use of the common representation unit, i.e. the source and target embedding help each other to learn a better representation. The previously proposed methods have shown the effectiveness of integrating prior word alignments into the attention mechanism (Mi et al., 2016; Liu et al., 2016; Cheng et al., 2016; Feng et al., 2017), leading to more accurate and adequate"
P19-1352,P02-1040,0,0.104132,"Missing"
P19-1352,D13-1176,0,0.102581,"and (b) shared-private word embeddings. In (a), the English word “Long” and the German word “Lange”, which have similar lexical meanings, are represented by two private d-dimension vectors. While in (b), the two word embeddings are made up of two parts, indicating the shared (lined nodes) and the private (unlined nodes) features. This enables the two words to make use of common representation units, leading to a closer relationship between them. With the introduction of ever more powerful architectures, neural machine translation (NMT) has become the most promising machine translation method (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015). For word representation, different architectures— including, but not limited to, recurrence-based (Chen et al., 2018), convolution-based (Gehring et al., 2017) and transformation-based (Vaswani et al., 2017) NMT models—have been taking advantage of the distributed word embeddings to capture the syntactic and semantic properties of words (Turian et al., 2010). Corresponding author Rd (a) Standard Introduction ∗ Long NMT usually utilizes three matrices to represent source embeddings, target input embeddings, and target output embeddings (also kno"
P19-1352,W04-3250,0,0.118362,"Missing"
P19-1352,P18-1164,0,0.205747,"is method can also be adapted to sub-word NMT with a shared source-target sub-word vocabulary and it performs well in language pairs with many of the same characters, such as English-German and English-French (Vaswani et al., 2017). Unfortunately, this method is not applicable to languages that are written in different alphabets, such as Chinese-English (Hassan et al., 2018). Another challenge facing the source and target word embeddings of NMT is the lack of interactions. This degrades the attention performance, leading to some unaligned translations that hurt the translation quality. Hence, Kuang et al. (2018) propose to bridge the source and target embeddings, which brings better attention to the related source and target words. Their method is applicable to any language pairs, providing a tight interaction between the source and target word pairs. However, their method requires additional components and model parameters. In this work, we aim to enhance the word representations and the interactions between the source and target words, while using even fewer parameters. To this end, we present a languageindependent method, which is called sharedprivate bilingual word embeddings, to share a part of"
P19-1352,E17-2025,0,0.382093,"ce and target words with the same word form also share similar lexical meaning. This category of words generally includes Arabic numbers, punctuations, named entities, cognates and loanwords. However, there are some bilingual homographs where the words in the source and target languages look the same but have completely different meanings. For example, the German word “Gift” means “Poison” in English. That is the reason we propose to first pair the words with similar lexical meaning instead of those words with same word forms. This might be the potential limitation of the three-way WT method (Press and Wolf, 2017), where words with the same word form indiscriminately share the same word embedding. 2.1.3 Unrelated Words We regard source and target words that cannot be paired with each other as unrelated words. Figure 2(c) shows an example of a pair of unrelated words. This category is mainly composed of lowfrequency words, such as misspelled words, special characters, and foreign words. In standard NMT, the embeddings of low-frequency words are usually inadequately trained, resulting in a poor word representation. These words are often treated as noises and they are generally ignored 3615 Pxlm ∈ R2×2 Sl"
P19-1352,W17-4739,0,0.0393269,"Missing"
P19-1352,P16-1162,0,0.847068,"nt the word embeddings Experiments We carry out our experiments on the small-scale IWSLT’17 {Arabic (Ar), Japanese (Ja), Korean (Ko), Chinese (Zh)}-to-English (En) translation tasks, medium-scale NIST Chinese-English (ZhEn) translation task, and large-scale WMT’14 English-German (En-De) translation task. For the IWSLT {Ar, Ja, Ko, Zh}-to-En translation tasks, there are respectively 236K, 234K, 227K, and 235K sentence pairs in each training set.4 The validation set is IWSLT17.TED.tst2014 and the test set is IWSLT17.TED.tst2015. For each language, we learn a BPE model with 16K merge operations (Sennrich et al., 2016b). For the NIST Zh-En translation task, the training corpus consists of 1.25M sentence pairs with 27.9M Chinese words and 34.5M English words. We use the NIST MT06 dataset as the validation set and the test sets are the NIST MT02, MT03, MT04, MT05, MT08 datasets. To compare with the recent works, the vocabulary size is limited to 4 https://wit3.fbk.eu/mt.php?release= 2017-01-trnted 3616 Architecture SMT* RNNsearch* Transformer Zh⇒En Vanilla Source bridging Target bridging Direct bridging Vanilla Direct bridging Decoder WT Shared-private Params 74.8M 78.5M 76.6M 78.9M 90.2M 90.5M 74.9M 62.8M E"
P19-1352,P10-1040,0,0.341652,"Missing"
P19-1352,D18-1100,0,0.0187321,"the paired words sharing features with them. by the NMT systems (Feng et al., 2017). Motivated by the frequency clustering methods proposed by Chen et al. (2016) where they cluster the words with similar frequency for training a hierarchical language model, in this work, we propose to use a small vector to model the possible features that might be shared between the source and target words which are unrelated but having similar word frequencies. In addition, it can be regarded as a way to improve the robustness of learning the embeddings of low-frequency words because of the noisy dimensions (Wang et al., 2018). 2.2 3 Implementation Before looking up embedding at each training step, the source and target embedding matrix are assembled by the sub-embedding matrices. As shown in Figure 3, the source embedding Ex ∈ R|V |×d is computed as follows:: Ex = Exlm ⊕ Exwf ⊕ Exur (2) where ⊕ is the row concatenation operator. Ex(·) ∈ R|V(·) |×d represents the word embeddings of the source words belong to different categories, e.g. lm represents the words with similar lexical meaning. |V(·) |denotes the vocabulary size of the corresponding category. The process of feature sharing is also implemented by matrix co"
P19-1352,D17-1154,0,0.0174627,"w that our model with fewer parameters yields consistent improvements over the strong Transformer baselines. 2 Approach In monolingual vector space, similar words tend to have commonalities in the same dimensions of their word vectors (Mikolov et al., 2013). These commonalities include: (1) a similar degree (value) of the same dimension and (2) a similar positive or negative correlation of the same dimension. Many previous works have noticed this phenomenon and have proposed to use shared vectors to represent similar words in monolingual vector space toward model compression (Li et al., 2016; Zhang et al., 2017b; Li et al., 2018). Motivated by these works, in NMT, we assume that the source and target words that have similar characteristics should also have similar vectors. Hence, we propose to perform this sharing technique in bilingual vector space. More precisely, we share the features (dimensions) between the paired source and target embeddings (vectors). However, in contrast to the previous studies, we also model the private features of the word embedding to preserve the private characteristics of words for source and target languages. The private 3614 features allow the words to better learn th"
P19-1352,C16-1291,0,0.0216196,"tures that are cooperatively learned by the source and target words. Recently, Gu et al. (2018) propose to use the pre-trained target (English) embeddings as a universal representation to improve the representation learning of the source (low-resource) languages. 3620 In our work, both the source and target embeddings can make use of the common representation unit, i.e. the source and target embedding help each other to learn a better representation. The previously proposed methods have shown the effectiveness of integrating prior word alignments into the attention mechanism (Mi et al., 2016; Liu et al., 2016; Cheng et al., 2016; Feng et al., 2017), leading to more accurate and adequate translation results with the assistance of prior guidance. We provide an alternative that integrates the prior alignments through the sharing of features, which can also leads to a reduction of model parameters. Kuang et al. (2018) propose to shorten the path length between the related source and target embeddings to enhance the embedding layer. We believe that the shared features can be seem as the zero distance between the paired word embeddings. Our proposed method also uses several ideas from the three-way WT m"
P19-1352,P16-1100,0,0.041235,"ion. These words are often treated as noises and they are generally ignored 3615 Pxlm ∈ R2×2 Slm ∈ R2×3 Long (Lange) Ju@@( (Ju@@) Sur ∈ R2×1 Sundial (Fiehlt) Exwf ∈ R2×5 ⊕ → ˜ ⊕ De@@( (De@@) Laden (Bericht) Long Pxwf ∈ R2×3 Swf ∈ R2×2 Ex ∈ R6×5 → ˜ ⊕ Italy (Italien) Exlm ∈ R2×5 Italy → Ju@@ De@@ Pxur ∈ R2×4 ˜ ⊕ Exur ∈ R2×5 ⊕ Laden Sundial → Figure 3: The example of assembling the source word embedding matrix. The words in parentheses denote the paired words sharing features with them. by the NMT systems (Feng et al., 2017). Motivated by the frequency clustering methods proposed by Chen et al. (2016) where they cluster the words with similar frequency for training a hierarchical language model, in this work, we propose to use a small vector to model the possible features that might be shared between the source and target words which are unrelated but having similar word frequencies. In addition, it can be regarded as a way to improve the robustness of learning the embeddings of low-frequency words because of the noisy dimensions (Wang et al., 2018). 2.2 3 Implementation Before looking up embedding at each training step, the source and target embedding matrix are assembled by the sub-embed"
P19-1352,D16-1249,0,0.0233985,"the bilingual features that are cooperatively learned by the source and target words. Recently, Gu et al. (2018) propose to use the pre-trained target (English) embeddings as a universal representation to improve the representation learning of the source (low-resource) languages. 3620 In our work, both the source and target embeddings can make use of the common representation unit, i.e. the source and target embedding help each other to learn a better representation. The previously proposed methods have shown the effectiveness of integrating prior word alignments into the attention mechanism (Mi et al., 2016; Liu et al., 2016; Cheng et al., 2016; Feng et al., 2017), leading to more accurate and adequate translation results with the assistance of prior guidance. We provide an alternative that integrates the prior alignments through the sharing of features, which can also leads to a reduction of model parameters. Kuang et al. (2018) propose to shorten the path length between the related source and target embeddings to enhance the embedding layer. We believe that the shared features can be seem as the zero distance between the paired word embeddings. Our proposed method also uses several ideas from"
P19-1352,D14-1179,0,\N,Missing
P19-1352,Q17-1024,0,\N,Missing
P19-1500,J05-3002,0,0.342095,"te the importance or salience of a passage recursively based on the entire graph. More recently, Yasunaga et al. (2017) propose a neural version of this framework, where salience is estimated using features extracted from sentence embeddings and graph convolutional networks (Kipf and Welling, 2017) applied over the relation graph representing cross-document links. Abstractive approaches have met with limited success. A few systems generate summaries based on sentence fusion, a technique which identifies fragments conveying common information across documents and combines these into sentences (Barzilay and McKeown, 2005; Filippova and Strube, 2008; Bing et al., 2015). Although neural abstractive models have achieved promising results on single-document summarization (See et al., 2017; Paulus et al., 2018; Gehrmann et al., 2018; Celikyilmaz et al., 2018), the extension of sequence-to-sequence architectures to multi-document summarization is less straightforward. Apart from the lack of sufficient training data, neural models also face the computational challenge of processing multiple source documents. Previous solutions include model transfer (Zhang et al., 2018; Lebanoff and Liu, 2018), where a sequence-to-s"
P19-1500,P15-1153,0,0.06386,"ased on the entire graph. More recently, Yasunaga et al. (2017) propose a neural version of this framework, where salience is estimated using features extracted from sentence embeddings and graph convolutional networks (Kipf and Welling, 2017) applied over the relation graph representing cross-document links. Abstractive approaches have met with limited success. A few systems generate summaries based on sentence fusion, a technique which identifies fragments conveying common information across documents and combines these into sentences (Barzilay and McKeown, 2005; Filippova and Strube, 2008; Bing et al., 2015). Although neural abstractive models have achieved promising results on single-document summarization (See et al., 2017; Paulus et al., 2018; Gehrmann et al., 2018; Celikyilmaz et al., 2018), the extension of sequence-to-sequence architectures to multi-document summarization is less straightforward. Apart from the lack of sufficient training data, neural models also face the computational challenge of processing multiple source documents. Previous solutions include model transfer (Zhang et al., 2018; Lebanoff and Liu, 2018), where a sequence-to-sequence model is pretrained on single-document s"
P19-1500,N18-1150,0,0.265358,"years, thanks to the popularity of neural network models and their ability to learn continuous representations without recourse to preprocessing tools or linguistic annotations. The availability of large-scale datasets (Sandhaus, 2008; Hermann et al., 2015; Grusky et al., 2018) containing hundreds of thousands of documentsummary pairs has driven the development of neural architectures for summarizing single documents. Several approaches have shown promising results with sequence-to-sequence models that encode a source document and then decode it into an abstractive summary (See et al., 2017; Celikyilmaz et al., 2018; Paulus et al., 2018; Gehrmann et al., 2018). Multi-document summarization — the task of producing summaries from clusters of themati1 Our code and data is available at https://github. com/nlpyang/hiersumm. cally related documents — has received significantly less attention, partly due to the paucity of suitable data for the application of learning methods. High-quality multi-document summarization datasets (i.e., document clusters paired with multiple reference summaries written by humans) have been produced for the Document Understanding and Text Analysis Conferences (DUC and TAC), but are"
P19-1500,N13-1136,0,0.315194,"Missing"
P19-1500,J10-3005,1,0.796189,"ncreasing training time. Table 3 summarizes ablation studies aiming to assess the contribution of individual components. Our experiments confirmed that encoding paragraph position in addition to token position within each paragraph is beneficial (see row w/o PP), as well as multi-head pooling (w/o MP is a model where the number of heads is set to 1), and the global transformer layer (w/o GT is a model with only 5 local transformer layers in the encoder). four questions per gold summary. Examples of questions and their answers are given in Table 5. We adopted the same scoring mechanism used in Clarke and Lapata (2010), i.e., correct answers are marked with 1, partially correct ones with 0.5, and 0 otherwise. A system’s score is the average of all question scores. Human Evaluation In addition to automatic evaluation, we also assessed system performance by eliciting human judgments on 20 randomly selected test instances. Our first evaluation study quantified the degree to which summarization models retain key information from the documents following a question-answering (QA) paradigm (Clarke and Lapata, 2010; Narayan et al., 2018). We created a set of questions based on the gold summary under the assumption"
P19-1500,D08-1019,0,0.21173,"e of a passage recursively based on the entire graph. More recently, Yasunaga et al. (2017) propose a neural version of this framework, where salience is estimated using features extracted from sentence embeddings and graph convolutional networks (Kipf and Welling, 2017) applied over the relation graph representing cross-document links. Abstractive approaches have met with limited success. A few systems generate summaries based on sentence fusion, a technique which identifies fragments conveying common information across documents and combines these into sentences (Barzilay and McKeown, 2005; Filippova and Strube, 2008; Bing et al., 2015). Although neural abstractive models have achieved promising results on single-document summarization (See et al., 2017; Paulus et al., 2018; Gehrmann et al., 2018; Celikyilmaz et al., 2018), the extension of sequence-to-sequence architectures to multi-document summarization is less straightforward. Apart from the lack of sufficient training data, neural models also face the computational challenge of processing multiple source documents. Previous solutions include model transfer (Zhang et al., 2018; Lebanoff and Liu, 2018), where a sequence-to-sequence model is pretrained"
P19-1500,D18-1443,0,0.277678,"ork models and their ability to learn continuous representations without recourse to preprocessing tools or linguistic annotations. The availability of large-scale datasets (Sandhaus, 2008; Hermann et al., 2015; Grusky et al., 2018) containing hundreds of thousands of documentsummary pairs has driven the development of neural architectures for summarizing single documents. Several approaches have shown promising results with sequence-to-sequence models that encode a source document and then decode it into an abstractive summary (See et al., 2017; Celikyilmaz et al., 2018; Paulus et al., 2018; Gehrmann et al., 2018). Multi-document summarization — the task of producing summaries from clusters of themati1 Our code and data is available at https://github. com/nlpyang/hiersumm. cally related documents — has received significantly less attention, partly due to the paucity of suitable data for the application of learning methods. High-quality multi-document summarization datasets (i.e., document clusters paired with multiple reference summaries written by humans) have been produced for the Document Understanding and Text Analysis Conferences (DUC and TAC), but are relatively small (in the range of a few hundr"
P19-1500,N18-1065,0,0.11036,"Missing"
P19-1500,P17-2074,0,0.0415555,"from two to 2 This was not the case with the other Transformer models. Our second evaluation study assessed the overall quality of the summaries by asking participants to rank them taking into account the following criteria: Informativeness (does the summary convey important facts about the topic in question?), Fluency (is the summary fluent and grammatical?), and Succinctness (does the summary avoid repetition?). We used Best-Worst Scaling (Louviere et al., 2015), a less labor-intensive alternative to paired comparisons that has been shown to produce more reliable results than rating scales (Kiritchenko and Mohammad, 2017). Participants were presented with the gold summary and summaries generated from 3 out of 4 systems and were asked to decide which summary was the best and which one was the worst in relation to the gold standard, taking into account the criteria mentioned above. The rating of each system was computed as the percentage of times it was chosen as best minus the times it was selected as worst. Ratings range from −1 (worst) to 1 (best). Both evaluations were conducted on the Amazon Mechanical Turk platform with 5 responses per hit. Participants evaluated summaries produced by the Lead baseline, th"
P19-1500,D18-2012,0,0.0322323,"aphs, and each paragraph has 70.1 tokens. The average length of the target summary is 139.4 tokens. We split the dataset with 1, 579, 360 instances for training, 38, 144 for validation and 38, 205 for test. ROUGE-L Recall L0 = 5 L0 = 10 L0 = 20 L0 = 40 Similarity 24.86 32.43 40.87 49.49 Ranking 39.38 46.74 53.84 60.42 Methods Table 1: ROUGE-L recall against target summary for L0 -best paragraphs obtained with tf-idf cosine similarity and our ranking model. For both ranking and summarization stages, we encode source paragraphs and target summaries using subword tokenization with SentencePiece (Kudo and Richardson, 2018). Our vocabulary consists of 32, 000 subwords and is shared for both source and target. Paragraph Ranking To train the regression model, we calculated the ROUGE-2 recall (Lin, 2004) of each paragraph against the target summary and used this as the ground-truth score. The hidden size of the two LSTMs was set to 256, and dropout (with dropout probability of 0.2) was used before all linear layers. Adagrad (Duchi et al., 2011) with learning rate 0.15 is used for optimization. We compare our ranking model against the method proposed in Liu et al. (2018) who use the tf-idf cosine similarity between"
P19-1500,D18-1387,0,0.0197233,"ese into sentences (Barzilay and McKeown, 2005; Filippova and Strube, 2008; Bing et al., 2015). Although neural abstractive models have achieved promising results on single-document summarization (See et al., 2017; Paulus et al., 2018; Gehrmann et al., 2018; Celikyilmaz et al., 2018), the extension of sequence-to-sequence architectures to multi-document summarization is less straightforward. Apart from the lack of sufficient training data, neural models also face the computational challenge of processing multiple source documents. Previous solutions include model transfer (Zhang et al., 2018; Lebanoff and Liu, 2018), where a sequence-to-sequence model is pretrained on single-document summarization data and finetuned on DUC (multi-document) benchmarks, or unsupervised models relying on reconstruction objectives (Ma et al., 2016; Chu and Liu, 2018). Liu et al. (2018) propose a methodology for constructing large-scale summarization datasets and a two-stage model which first extracts salient information from source documents and then uses a decoder-only architecture (that can attend to very long sequences) to generate the summary. We follow their setup in viewing multi-document summarization as a supervised"
P19-1500,W04-1013,0,0.279494,"for test. ROUGE-L Recall L0 = 5 L0 = 10 L0 = 20 L0 = 40 Similarity 24.86 32.43 40.87 49.49 Ranking 39.38 46.74 53.84 60.42 Methods Table 1: ROUGE-L recall against target summary for L0 -best paragraphs obtained with tf-idf cosine similarity and our ranking model. For both ranking and summarization stages, we encode source paragraphs and target summaries using subword tokenization with SentencePiece (Kudo and Richardson, 2018). Our vocabulary consists of 32, 000 subwords and is shared for both source and target. Paragraph Ranking To train the regression model, we calculated the ROUGE-2 recall (Lin, 2004) of each paragraph against the target summary and used this as the ground-truth score. The hidden size of the two LSTMs was set to 256, and dropout (with dropout probability of 0.2) was used before all linear layers. Adagrad (Duchi et al., 2011) with learning rate 0.15 is used for optimization. We compare our ranking model against the method proposed in Liu et al. (2018) who use the tf-idf cosine similarity between each paragraph and the article title to rank the input paragraphs. We take the first L0 paragraphs from the ordered paragraph set produced by our ranker and the similarity-based met"
P19-1500,Q18-1005,1,0.699319,"ion with weight Wc ∈ Rd∗d : ci = Wc [context1i ; · · · ; contextni head ] (21) where Wo1 ∈ Rdf f ∗d and Wo2 ∈ Rd∗df f are the weights, df f is the hidden size of the feed-forward later. This way, each token within paragraph Ri can collect information from other paragraphs in a hierarchical and efficient manner. 3.2.4 Graph-informed Attention The inter-paragraph attention mechanism can be viewed as learning a latent graph representation (self-attention weights) of the input paragraphs. Although previous work has shown that similar latent representations are beneficial for downstream NLP tasks (Liu and Lapata, 2018; Kim et al., 2017; Williams et al., 2018; Niculae et al., 2018; Fernandes et al., 2019), much work in multi-document summarization has taken advantage of explicit graph representations, each focusing on different facets of the summarization task 5074 (e.g., capturing redundant information or representing passages referring to the same event or entity). One advantage of the hierarchical transformer is that we can easily incorporate graphs external to the model, to generate better summaries. We experimented with two well-established graph representations which we discuss briefly below. However,"
P19-1500,C16-1143,0,0.0667403,"Missing"
P19-1500,N18-1158,1,0.84883,"answers are given in Table 5. We adopted the same scoring mechanism used in Clarke and Lapata (2010), i.e., correct answers are marked with 1, partially correct ones with 0.5, and 0 otherwise. A system’s score is the average of all question scores. Human Evaluation In addition to automatic evaluation, we also assessed system performance by eliciting human judgments on 20 randomly selected test instances. Our first evaluation study quantified the degree to which summarization models retain key information from the documents following a question-answering (QA) paradigm (Clarke and Lapata, 2010; Narayan et al., 2018). We created a set of questions based on the gold summary under the assumption that it contains the most important information from the input paragraphs. We then examined whether participants were able to answer these questions by reading system summaries alone without access to the gold summary. The more questions a system can answer, the better it is at summarization. We created 57 questions in total varying from two to 2 This was not the case with the other Transformer models. Our second evaluation study assessed the overall quality of the summaries by asking participants to rank them takin"
P19-1500,D18-1108,0,0.0285352,"tni head ] (21) where Wo1 ∈ Rdf f ∗d and Wo2 ∈ Rd∗df f are the weights, df f is the hidden size of the feed-forward later. This way, each token within paragraph Ri can collect information from other paragraphs in a hierarchical and efficient manner. 3.2.4 Graph-informed Attention The inter-paragraph attention mechanism can be viewed as learning a latent graph representation (self-attention weights) of the input paragraphs. Although previous work has shown that similar latent representations are beneficial for downstream NLP tasks (Liu and Lapata, 2018; Kim et al., 2017; Williams et al., 2018; Niculae et al., 2018; Fernandes et al., 2019), much work in multi-document summarization has taken advantage of explicit graph representations, each focusing on different facets of the summarization task 5074 (e.g., capturing redundant information or representing passages referring to the same event or entity). One advantage of the hierarchical transformer is that we can easily incorporate graphs external to the model, to generate better summaries. We experimented with two well-established graph representations which we discuss briefly below. However, there is nothing inherent in our model that restricts us to th"
P19-1500,W14-3703,0,0.149686,"quence, being agnostic of the hierarchical structures and the relations that might exist among documents. For example, different web pages might repeat the same content, include additional content, present contradictory information, or discuss the same fact in a different light (Radev, 2000). The realization that cross-document links are important in isolating salient information, eliminating redundancy, and creating overall coherent summaries, has led to the widespread adoption of graph-based models for multi-document summarization (Erkan and Radev, 2004; Christensen et al., 2013; Wan, 2008; Parveen and Strube, 2014). Graphs conveniently capture the relationships between textual units within a document collection and can be easily constructed under the assumption that text spans represent graph nodes and edges are semantic links between them. In this paper, we develop a neural summarization model which can effectively process multiple input documents and distill abstractive summaries. Our model augments the previously proposed Transformer architecture with the ability to encode multiple documents in a hierarchical manner. We represent cross-document relationships via an attention mechanism which allows to"
P19-1500,W00-1009,0,0.38217,"ges 5070–5081 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics 2 decode the summary. Although the model of Liu et al. (2018) takes an important first step towards abstractive multidocument summarization, it still considers the multiple input documents as a concatenated flat sequence, being agnostic of the hierarchical structures and the relations that might exist among documents. For example, different web pages might repeat the same content, include additional content, present contradictory information, or discuss the same fact in a different light (Radev, 2000). The realization that cross-document links are important in isolating salient information, eliminating redundancy, and creating overall coherent summaries, has led to the widespread adoption of graph-based models for multi-document summarization (Erkan and Radev, 2004; Christensen et al., 2013; Wan, 2008; Parveen and Strube, 2014). Graphs conveniently capture the relationships between textual units within a document collection and can be easily constructed under the assumption that text spans represent graph nodes and edges are semantic links between them. In this paper, we develop a neural s"
P19-1500,P17-1099,0,0.786089,"interest in recent years, thanks to the popularity of neural network models and their ability to learn continuous representations without recourse to preprocessing tools or linguistic annotations. The availability of large-scale datasets (Sandhaus, 2008; Hermann et al., 2015; Grusky et al., 2018) containing hundreds of thousands of documentsummary pairs has driven the development of neural architectures for summarizing single documents. Several approaches have shown promising results with sequence-to-sequence models that encode a source document and then decode it into an abstractive summary (See et al., 2017; Celikyilmaz et al., 2018; Paulus et al., 2018; Gehrmann et al., 2018). Multi-document summarization — the task of producing summaries from clusters of themati1 Our code and data is available at https://github. com/nlpyang/hiersumm. cally related documents — has received significantly less attention, partly due to the paucity of suitable data for the application of learning methods. High-quality multi-document summarization datasets (i.e., document clusters paired with multiple reference summaries written by humans) have been produced for the Document Understanding and Text Analysis Conferenc"
P19-1500,D08-1079,0,0.140383,"ted flat sequence, being agnostic of the hierarchical structures and the relations that might exist among documents. For example, different web pages might repeat the same content, include additional content, present contradictory information, or discuss the same fact in a different light (Radev, 2000). The realization that cross-document links are important in isolating salient information, eliminating redundancy, and creating overall coherent summaries, has led to the widespread adoption of graph-based models for multi-document summarization (Erkan and Radev, 2004; Christensen et al., 2013; Wan, 2008; Parveen and Strube, 2014). Graphs conveniently capture the relationships between textual units within a document collection and can be easily constructed under the assumption that text spans represent graph nodes and edges are semantic links between them. In this paper, we develop a neural summarization model which can effectively process multiple input documents and distill abstractive summaries. Our model augments the previously proposed Transformer architecture with the ability to encode multiple documents in a hierarchical manner. We represent cross-document relationships via an attentio"
P19-1500,Q18-1019,0,0.0566491,"Missing"
P19-1500,K17-1045,0,0.193695,"hods are extractive operating over graph-based representations of sentences or passages. Approaches vary depending on how edge weights are computed e.g., based on cosine similarity with tf-idf weights for words (Erkan and Radev, 2004) or on discourse relations (Christensen et al., 2013), and the specific algorithm adopted for ranking text units for inclusion in the final summary. Several variants of the PageRank algorithm have been adopted in the literature (Erkan and Radev, 2004) in order to compute the importance or salience of a passage recursively based on the entire graph. More recently, Yasunaga et al. (2017) propose a neural version of this framework, where salience is estimated using features extracted from sentence embeddings and graph convolutional networks (Kipf and Welling, 2017) applied over the relation graph representing cross-document links. Abstractive approaches have met with limited success. A few systems generate summaries based on sentence fusion, a technique which identifies fragments conveying common information across documents and combines these into sentences (Barzilay and McKeown, 2005; Filippova and Strube, 2008; Bing et al., 2015). Although neural abstractive models have ach"
P19-1500,W18-6545,0,0.027652,"ents and combines these into sentences (Barzilay and McKeown, 2005; Filippova and Strube, 2008; Bing et al., 2015). Although neural abstractive models have achieved promising results on single-document summarization (See et al., 2017; Paulus et al., 2018; Gehrmann et al., 2018; Celikyilmaz et al., 2018), the extension of sequence-to-sequence architectures to multi-document summarization is less straightforward. Apart from the lack of sufficient training data, neural models also face the computational challenge of processing multiple source documents. Previous solutions include model transfer (Zhang et al., 2018; Lebanoff and Liu, 2018), where a sequence-to-sequence model is pretrained on single-document summarization data and finetuned on DUC (multi-document) benchmarks, or unsupervised models relying on reconstruction objectives (Ma et al., 2016; Chu and Liu, 2018). Liu et al. (2018) propose a methodology for constructing large-scale summarization datasets and a two-stage model which first extracts salient information from source documents and then uses a decoder-only architecture (that can attend to very long sequences) to generate the summary. We follow their setup in viewing multi-document summa"
P19-1504,N04-1015,0,0.455054,"ation and released WikiSum, a large-scale summarization dataset which enables the training of neural models. Like most previous work on neural text generation (Gardent et al., 2017; See et al., 2017; Wiseman et al., 2017; Puduppully et al., 2019; Celikyilmaz et al., 2018; Liu et al., 2018; PerezBeltrachini and Lapata, 2018; Marcheggiani and Perez-Beltrachini, 2018), Liu et al. (2018) represent the target summaries as a single long sequence, despite the fact that documents are organized into topically coherent text segments, exhibiting a specific structure in terms of the content they discuss (Barzilay and Lee, 2004). This is especially the case when generating text within a specific domain where certain topics might be discussed in a specific order (Wray, 2002). For instance, the summary in Table 1 is about a species of damselfly; the second sentence describes the region where the species is found and the fourth the type of habitat the species lives in. We would expect other Animal Wikipedia summaries to exhibit similar content organization. In this work we propose a neural model which is guided by the topic structure of target summaries, i.e., the way content is organized into sentences and the type of"
P19-1504,N18-1150,0,0.0481031,"and human evaluation demonstrate that our summaries have better content coverage. 1 Introduction Abstractive multi-document summarization aims at generating a coherent summary from a cluster of thematically related documents. Recently, Liu et al. (2018) proposed generating the lead section of a Wikipedia article as a variant of multidocument summarization and released WikiSum, a large-scale summarization dataset which enables the training of neural models. Like most previous work on neural text generation (Gardent et al., 2017; See et al., 2017; Wiseman et al., 2017; Puduppully et al., 2019; Celikyilmaz et al., 2018; Liu et al., 2018; PerezBeltrachini and Lapata, 2018; Marcheggiani and Perez-Beltrachini, 2018), Liu et al. (2018) represent the target summaries as a single long sequence, despite the fact that documents are organized into topically coherent text segments, exhibiting a specific structure in terms of the content they discuss (Barzilay and Lee, 2004). This is especially the case when generating text within a specific domain where certain topics might be discussed in a specific order (Wray, 2002). For instance, the summary in Table 1 is about a species of damselfly; the second sentence describe"
P19-1504,J10-3005,1,0.836706,"luation with two human-based studies carried out on Amazon Mechanical Turk (AMT) over 45 randomly selected examples from the test set (15 from each domain). We compared the TSS2S, CV-S2S and CV-S2D+T models. The first study focused on assessing the extent to which generated summaries retain salient information from the input set of paragraphs. We folModel Company QA Rank TF-S2S 5 1.87 CV-S2S 5 2.27 CV-S2D+T 7 1.87 Film QA Rank 6 2.27 6.67 1.76 7 1.98 Animal QA Rank 9 1.87 8.33 2.04 9.33 2.09 Table 5: QA-based evaluation and system ranking. lowed a question-answering (QA) scheme as proposed in Clarke and Lapata (2010). Under this scheme, a set of questions are created based on the gold summary; participants are then asked to answer these questions by reading system summaries alone without access to the input. The more questions a system can answer, the better it is at summarizing the input paragraphs as a whole (see Appendix A for example questions). Correct answers are given a score of 1, partially correct answers score 0.5, and zero otherwise. The final score is the average of all question scores. We created between two and four factoid questions for each summary; a total of 40 questions for each domain."
P19-1504,W17-3518,1,0.815765,"isting sequential decoders on three data sets representing different domains. Automatic and human evaluation demonstrate that our summaries have better content coverage. 1 Introduction Abstractive multi-document summarization aims at generating a coherent summary from a cluster of thematically related documents. Recently, Liu et al. (2018) proposed generating the lead section of a Wikipedia article as a variant of multidocument summarization and released WikiSum, a large-scale summarization dataset which enables the training of neural models. Like most previous work on neural text generation (Gardent et al., 2017; See et al., 2017; Wiseman et al., 2017; Puduppully et al., 2019; Celikyilmaz et al., 2018; Liu et al., 2018; PerezBeltrachini and Lapata, 2018; Marcheggiani and Perez-Beltrachini, 2018), Liu et al. (2018) represent the target summaries as a single long sequence, despite the fact that documents are organized into topically coherent text segments, exhibiting a specific structure in terms of the content they discuss (Barzilay and Lee, 2004). This is especially the case when generating text within a specific domain where certain topics might be discussed in a specific order (Wray, 2002). For ins"
P19-1504,P15-1107,0,0.0199086,"ly unexplored within neural text generation, it has been been recognized as useful for summarization. Barzilay and Lee (2004) build a model of the content structure of source documents and target summaries and use it to extract salient facts from the source. Sauper and Barzilay (2009) cluster texts by target topic and use a global optimisation algorithm to select the best combination of facts from each cluster. Although these models have shown good results in terms of content selection, they cannot generate target summaries. Our model is also related to the hierarchical decoding approaches of Li et al. (2015) and Tan et al. (2017). However, the former approach is auto-encoding the same inputs (our model carries out content selection for the summarization task), while the latter generates independent sentences. They also both rely on recurrent neural models, while we use convolutional neural networks. To our knowledge this is the first hierarchical decoder proposed for a non-recurrent architecture. To evaluate our model, we introduce W IKI C ATS UM, a dataset1 derived from Liu et al. (2018) 1 Our dataset and code are available at https:// 5107 Proceedings of the 57th Annual Meeting of the Associati"
P19-1504,W04-1013,0,0.142034,"Missing"
P19-1504,D15-1166,0,0.0510967,"st with the previous target embedding gti : dlti = Wdl (olti + st ) + gti altij = P exp(dlti • zj ) • z 0) j l j 0 exp(dti clti = |X | X altij (zj + ej ) (7) (8) (9) j=1 (3) (4) j=1 s where αjt is the attention weight for the document-level decoder attending to input token xj at time step t. 3.2 l−1 {olt1 , · · · , oltn } = conv({o0 t1 , · · · , o0 tn ) (5) where ht is the LSTM hidden state of step t and cst is the context vector computed by attending to the input. The initial hidden state h0 is initialized with the averaged sum of the encoder output states. We use a soft attention mechanism (Luong et al., 2015) to compute the context vector cst : s αtj =P Hierarchical Convolutional Decoder Sentence-level Decoder Each sentence st = (yt1 , . . . , yt|st |) in target summary S is generated by a sentence-level decoder. The convolutional architecture proposed in Gehring et al. (2017) combines word embeddings with positional embeddings. That is, the word representation wti of each target word yti is combined with vector ei indicating where this word is in the sentence, wti = emb(yti ) + ei . We extend this The prediction of word yti is conditioned on the output vectors of the top convolutional layer, as L"
P19-1504,P14-5010,0,0.00587121,"Missing"
P19-1504,W18-6501,1,0.809122,"ge. 1 Introduction Abstractive multi-document summarization aims at generating a coherent summary from a cluster of thematically related documents. Recently, Liu et al. (2018) proposed generating the lead section of a Wikipedia article as a variant of multidocument summarization and released WikiSum, a large-scale summarization dataset which enables the training of neural models. Like most previous work on neural text generation (Gardent et al., 2017; See et al., 2017; Wiseman et al., 2017; Puduppully et al., 2019; Celikyilmaz et al., 2018; Liu et al., 2018; PerezBeltrachini and Lapata, 2018; Marcheggiani and Perez-Beltrachini, 2018), Liu et al. (2018) represent the target summaries as a single long sequence, despite the fact that documents are organized into topically coherent text segments, exhibiting a specific structure in terms of the content they discuss (Barzilay and Lee, 2004). This is especially the case when generating text within a specific domain where certain topics might be discussed in a specific order (Wray, 2002). For instance, the summary in Table 1 is about a species of damselfly; the second sentence describes the region where the species is found and the fourth the type of habitat the species lives in."
P19-1504,D18-1206,1,0.873442,"ls github.com/lauhaide/WikiCatSum. Generation with Content Guidance Our model takes as input a set of ranked paragraphs P = {p1 · · · p|P |} which we concatenate to form a flat input sequence X = (x1 · · · x|X |) where xi is the i-th token. The output of the model is a multi-sentence summary S = (s1 , · · · , s|S |) where st denotes the t-th sentence. We adopt an encoder-decoder architecture which makes use of convolutional neural networks (CNNs; Gehring et al. 2017). CNNs permit parallel training (Gehring et al., 2017) and have shown good performance in abstractive summarization tasks (e.g., Narayan et al. 2018). Figure 1 illustrates the architecture of our model. We use the convolutional encoder of Gehring et al. (2017) to obtain a sequence of states (z1 , · · · , z|X |) given an input sequence of tokens (x1 , · · · , x|X |). A hierarchical convolutional decoder generates the target sentences (based on the encoder outputs). Specifically, a document-level decoder first generates sentence vectors (LSTM Document Decoder in Figure 1), representing the content specification for each sentence that the model plans to decode. A sentence-level decoder (CNN Sentence Decoder in Figure 1) is then applied to gen"
P19-1504,P17-1108,0,0.113283,"Missing"
P19-1504,P09-1024,0,\N,Missing
P19-1504,Q16-1029,0,\N,Missing
P19-1504,P17-1099,0,\N,Missing
P19-1504,D17-1239,0,\N,Missing
P19-1623,P18-1094,0,0.217946,"egy is to impose constraints on the decoding process (Wu et al., 2016). Our work differs from prior studies in that contrastive learning is model agnostic. All previous coverage-based methods heavily rely on attention weights between source and target words to derive coverage for source words. Such attention weights are not readily available for all NMT models. In contrast, our method can be used to fine-tune arbitrary NMT models to reduce word omission errors in only hundreds of steps. 4.2 cabulary to replace a word in a ground-truth example (Vaswani et al., 2013; Mnih and Kavukcuoglu, 2013; Bose et al., 2018). Contrastive learning has also been investigated in neural language modelling (Huang et al., 2018), unsupervised word alignment (Liu and Sun, 2015), order embeddings (Vendrov et al., 2016; Bose et al., 2018), knowledge graph embeddings (Yang et al., 2015; Lin et al., 2015; Bose et al., 2018) and caption generation (Mao et al., 2016; Vedantam et al., 2017). The closest work to ours is (Wiseman and Rush, 2016), which leverages contrastive learning during beam search with the golden reference sentences as positive examples and the current output sentences as contrastive examples. While they focu"
P19-1623,D18-1150,0,0.039177,"r studies in that contrastive learning is model agnostic. All previous coverage-based methods heavily rely on attention weights between source and target words to derive coverage for source words. Such attention weights are not readily available for all NMT models. In contrast, our method can be used to fine-tune arbitrary NMT models to reduce word omission errors in only hundreds of steps. 4.2 cabulary to replace a word in a ground-truth example (Vaswani et al., 2013; Mnih and Kavukcuoglu, 2013; Bose et al., 2018). Contrastive learning has also been investigated in neural language modelling (Huang et al., 2018), unsupervised word alignment (Liu and Sun, 2015), order embeddings (Vendrov et al., 2016; Bose et al., 2018), knowledge graph embeddings (Yang et al., 2015; Lin et al., 2015; Bose et al., 2018) and caption generation (Mao et al., 2016; Vedantam et al., 2017). The closest work to ours is (Wiseman and Rush, 2016), which leverages contrastive learning during beam search with the golden reference sentences as positive examples and the current output sentences as contrastive examples. While they focus on improving the capability of Seq2Seq model to capture global dependencies, we focus on reducing"
P19-1623,N03-1017,0,0.103674,"ther quantify to what extent our approach reduces word omission errors, we asked human evaluators to manually count word omission errors on the test sets of all the translation tasks. Table 3 shows the error counts. We find that CLone achieves significant error reduction as compared with MLE, MLE + CP, and WordDropout for all the three language pairs. 4 Related Work Our work is related to two lines of research: modeling coverage for NMT and contrastive learning in NLP. 4.1 Modeling Coverage for NMT The notion of coverage dates back to conventional phrase-based statistical machine translation (Koehn et al., 2003). A coverage vector, which is used to indicate whether a source phrase is translated or not during the decoding process, ensures that each source phrase is translated exactly once. As there are no latent variables defined on language structures in neural networks, it is hard to directly introduce coverage into NMT. As a result, there are two strategies. The first strategy is to modify the model architectures to incorporate coverage (Tu et al., 2016; Mi et al., 2016), which requires considerable expertise. The second strategy is to impose constraints on the decoding process (Wu et al., 2016). O"
P19-1623,P14-5010,0,0.00241709,"ds randomly from the ground-truth translations in D; ˜ is constructed via omitting the • CLlow/high : D word with the lowest/highest frequency from each ground-truth translation in D; Figure 1: Visualization of margin differences between CLone and MLE on 500 sampled sentence pairs. We use red to highlight sentence pairs on which CLone achieves a larger margin than MLE. Blue points denote MLE achieves a higher margin. ˜ is constructed via omitting • CLV/IN : D one verb or preposition randomly from the ground-truth translation in D. The part-ofspeech information is given by the Stanford Parser (Manning et al., 2014). 3.2 Comparison of Margins To find out whether CL increases the margin compared with MLE, we calculate the following margin difference for a ground-truth sentence pair ˜ i: hx, yi and an erroneous sentence pair hx, y ∆M = log P (y|x; θˆCL )−log P (˜ y|x; θˆCL ) − log P (y|x; θˆMLE )+log P (˜ y|x; θˆMLE ) (6) Figure 1 shows the margin difference between CLone and MLE on 500 sampled sentence pairs from the training set for the Chinese-to-English task. “Sentence length” denotes the sum of the lengths of the source and target sentences (i.e., |x |+ |y|). Red points denote sentence pairs on which"
P19-1623,D16-1096,0,0.159341,"eline methods. 1 Introduction While neural machine translation (NMT) has achieved remarkable success (Sutskever et al., 2014; Bahdanau et al., 2015; Vaswani et al., 2017), there still remains a severe challenge: NMT systems are prone to omit essential words on the source side, which severely deteriorate the adequacy of machine translation. Due to the lack of interpretability of neural networks, it is hard to explain how these omission errors occur and design methods to eliminate them. Existing methods for reducing word omission errors in NMT have focused on modeling coverage (Tu et al., 2016; Mi et al., 2016; Wu et al., 2016; Wang et al., 2016; Tu et al., 2017). The central idea is to model the fertility (i.e., the number of corresponding target words) of a source word based on attention weights to avoid word omission. Although these methods prove to be effective in modeling coverage for NMT, they heavily rely on the attention weights provided by the ∗ Corresponding author: Yang Liu • Model agnostic. Our approach is applicable to all existing NMT models. Only the training objective and training data need to be changed. • Language independent. Our approach is independent of languages and can be ap"
P19-1623,P02-1040,0,0.106626,"Missing"
P19-1623,P16-1009,0,0.0378869,"newstest2017 datasets are used as the development set and test set, respectively. For the German-to-English translation task, we use the WMT 2017 dataset as the training set, which consists of 6M preprocessed sentence pairs. The newstest2014 and newstest2017 datasets are used as the development set and test set, respectively. For the Russian-to-English translation task, we use the WMT 2017 preprocessed dataset as the training set, which consists of 25M preprocessed sentence pairs. The newstest2015 and newstest2016 datasets are used as the development set and test set, respectively. Following Sennrich et al. (2016b), we split words into sub-word units. The numbers of merge operations in byte pair encoding (BPE) for both language pairs are set to 32K. After performing BPE, the training set of the Chinese-to-English task contains 550M Chinese sub-word units and 615M English sub-word units, the training set of the German-to-English task consists of 157M German sub-word units and 153M English subword units, and the training set of the Russian-toEnglish task consists of 653M Russian sub-word units and 629M English sub-word units. We used three baselines in our experiments: • MLE: Maximum likelihood estimati"
P19-1623,P16-1008,1,0.929386,"ce than three baseline methods. 1 Introduction While neural machine translation (NMT) has achieved remarkable success (Sutskever et al., 2014; Bahdanau et al., 2015; Vaswani et al., 2017), there still remains a severe challenge: NMT systems are prone to omit essential words on the source side, which severely deteriorate the adequacy of machine translation. Due to the lack of interpretability of neural networks, it is hard to explain how these omission errors occur and design methods to eliminate them. Existing methods for reducing word omission errors in NMT have focused on modeling coverage (Tu et al., 2016; Mi et al., 2016; Wu et al., 2016; Wang et al., 2016; Tu et al., 2017). The central idea is to model the fertility (i.e., the number of corresponding target words) of a source word based on attention weights to avoid word omission. Although these methods prove to be effective in modeling coverage for NMT, they heavily rely on the attention weights provided by the ∗ Corresponding author: Yang Liu • Model agnostic. Our approach is applicable to all existing NMT models. Only the training objective and training data need to be changed. • Language independent. Our approach is independent of langua"
P19-1623,D13-1140,0,0.0377863,"requires considerable expertise. The second strategy is to impose constraints on the decoding process (Wu et al., 2016). Our work differs from prior studies in that contrastive learning is model agnostic. All previous coverage-based methods heavily rely on attention weights between source and target words to derive coverage for source words. Such attention weights are not readily available for all NMT models. In contrast, our method can be used to fine-tune arbitrary NMT models to reduce word omission errors in only hundreds of steps. 4.2 cabulary to replace a word in a ground-truth example (Vaswani et al., 2013; Mnih and Kavukcuoglu, 2013; Bose et al., 2018). Contrastive learning has also been investigated in neural language modelling (Huang et al., 2018), unsupervised word alignment (Liu and Sun, 2015), order embeddings (Vendrov et al., 2016; Bose et al., 2018), knowledge graph embeddings (Yang et al., 2015; Lin et al., 2015; Bose et al., 2018) and caption generation (Mao et al., 2016; Vedantam et al., 2017). The closest work to ours is (Wiseman and Rush, 2016), which leverages contrastive learning during beam search with the golden reference sentences as positive examples and the current output se"
P19-1623,N16-1113,0,0.0142537,"e neural machine translation (NMT) has achieved remarkable success (Sutskever et al., 2014; Bahdanau et al., 2015; Vaswani et al., 2017), there still remains a severe challenge: NMT systems are prone to omit essential words on the source side, which severely deteriorate the adequacy of machine translation. Due to the lack of interpretability of neural networks, it is hard to explain how these omission errors occur and design methods to eliminate them. Existing methods for reducing word omission errors in NMT have focused on modeling coverage (Tu et al., 2016; Mi et al., 2016; Wu et al., 2016; Wang et al., 2016; Tu et al., 2017). The central idea is to model the fertility (i.e., the number of corresponding target words) of a source word based on attention weights to avoid word omission. Although these methods prove to be effective in modeling coverage for NMT, they heavily rely on the attention weights provided by the ∗ Corresponding author: Yang Liu • Model agnostic. Our approach is applicable to all existing NMT models. Only the training objective and training data need to be changed. • Language independent. Our approach is independent of languages and can be applied to arbitrary languages. • Fast"
P19-1623,D16-1137,0,0.115061,"Missing"
P19-1623,P16-1162,0,0.0618026,"newstest2017 datasets are used as the development set and test set, respectively. For the German-to-English translation task, we use the WMT 2017 dataset as the training set, which consists of 6M preprocessed sentence pairs. The newstest2014 and newstest2017 datasets are used as the development set and test set, respectively. For the Russian-to-English translation task, we use the WMT 2017 preprocessed dataset as the training set, which consists of 25M preprocessed sentence pairs. The newstest2015 and newstest2016 datasets are used as the development set and test set, respectively. Following Sennrich et al. (2016b), we split words into sub-word units. The numbers of merge operations in byte pair encoding (BPE) for both language pairs are set to 32K. After performing BPE, the training set of the Chinese-to-English task contains 550M Chinese sub-word units and 615M English sub-word units, the training set of the German-to-English task consists of 157M German sub-word units and 153M English subword units, and the training set of the Russian-toEnglish task consists of 653M Russian sub-word units and 629M English sub-word units. We used three baselines in our experiments: • MLE: Maximum likelihood estimati"
Q13-1004,P11-1048,0,0.0140689,"der models with richer sibling and grandchild features (Carreras, 2007; Koo and Collins, 2010). However, for models with general non-local features, DP is inefficient. There have been numerous studies on global inference algorithms for general higher order parsing. One popular approach is reranking (Collins, 2000; Smith and Eisner modeled dependency parsing using Markov Random Fields (MRFs) with global constraints and applied loopy belief propagation (LBP) for approximate learning and inference (Smith and Eisner, 2008). Similar work was done for Combinatorial Categorial Grammar (CCG) parsing (Auli and Lopez, 2011). They used posterior marginal beliefs for inference to satisfy the tree constraint: for each factor, only legal messages (satisfying global constraints) are considered in the partition function. A similar line of research investigated the use of integer linear programming (ILP) based parsing (Riedel and Clarke, 2006; Martins et al., 2009). This 37 Transactions of the Association for Computational Linguistics, 1 (2013) 37–48. Action Editor: Ryan McDonald. c Submitted 11/2012; Revised 2/2013; Published 3/2013. 2013 Association for Computational Linguistics. method is very expressive. It can han"
Q13-1004,E12-1009,0,0.165886,"Missing"
Q13-1004,D12-1133,0,0.0326496,"Missing"
Q13-1004,D07-1101,0,0.188303,"2) used B&B for MRFs, where they proposed two branching strategies and a novel data structure for efficient upper bound computation. Klenner and Ailloud (2009) proposed a variation of Balas algorithm (Balas, 1965) for coreference resolution, where candidate branching variables are sorted by their weights. Our bounding strategy is to find an upper bound for the score of each non-local factor c containing multiple edges. The bound is the sum of new scores of edges in the factor plus a constant ϕehm ,ehs (x) ϕc (x) ≤ ehm ,ehs ∈y There are two versions of second order models, used respectively by Carreras (2007) and Koo et al. (2010). The difference is that Carreras’ only considers the outermost grandchildren, while Koo and Collin’s allows all grandchild features. Both models permit O(n4 ) running time. Third-order models score edge triples such as three adjacent sibling modifiers, or grand-siblings that score a word, its modifier and its adjacent grandchildren, and the inference complexity is O(n4 ) (Koo and Collins, 2010). In this paper, for all the factors/features that can be handled by DP, we call them the local factors/features. 3 3.1 The Proposed Method For general high order models with non-l"
Q13-1004,P05-1022,0,0.053252,"Missing"
Q13-1004,P12-1023,0,0.033988,"Missing"
Q13-1004,W02-1001,0,0.357041,"Missing"
Q13-1004,C96-1058,0,0.15995,"riminative models, the score of a parse tree y is the weighted sum of the fired feature functions, which can be represented by the sum of the factors ∑ ∑ ϕ(x, y) = wT f (x, y) = wT f (x, c) = ϕc (x) c⊆y c⊆y where f (x, c) is the feature vector that depends on c. For example, we could define a feature for grandchild c = {egh , ehm }   1 if xg = would ∧ xh = be f (x, c) = ∧xm = happy ∧ c is selected   0 otherwise 2.2 Dynamic Programming for Local Models In first order models, all factors c in Eq(1) contain a single edge. The optimal parse tree can be derived by DP with running time O(n3 ) (Eisner, 1996). The algorithm has two types of structures: complete span, which consists of a headword and its descendants on one side, and incomplete span, which consists of a dependency and the region between the head and modifier. It starts at single word spans, and merges the spans in bottom up order. For second order models, the score function ϕ(x, y) adds the scores of siblings (adjacent edges with a common head) and grandchildren ϕ(x, y) = ∑ ϕehm (x) ehm ∈y + ∑ ϕehm ,egh (x) egh ,ehm ∈y + ∑ bound of the optimal parse tree score in the subspace: U BYi ≥ maxy∈Yi ϕ(x, y). If this bound is no more than a"
Q13-1004,P07-1050,0,0.0417476,"Missing"
Q13-1004,W05-1506,0,0.0903083,"Missing"
Q13-1004,P08-1067,0,0.118944,"Missing"
Q13-1004,E09-1051,0,0.0253446,"a common head) and grandchildren ϕ(x, y) = ∑ ϕehm (x) ehm ∈y + ∑ ϕehm ,egh (x) egh ,ehm ∈y + ∑ bound of the optimal parse tree score in the subspace: U BYi ≥ maxy∈Yi ϕ(x, y). If this bound is no more than any obtained parse tree score U BYi ≤ ϕ(x, y ′ ), then all parse trees in subspace Yi are no more optimal than y ′ , and Yi could be pruned safely. The efficiency of B&B depends on the branching strategy and upper bound computation. For example, Sun et al. (2012) used B&B for MRFs, where they proposed two branching strategies and a novel data structure for efficient upper bound computation. Klenner and Ailloud (2009) proposed a variation of Balas algorithm (Balas, 1965) for coreference resolution, where candidate branching variables are sorted by their weights. Our bounding strategy is to find an upper bound for the score of each non-local factor c containing multiple edges. The bound is the sum of new scores of edges in the factor plus a constant ϕehm ,ehs (x) ϕc (x) ≤ ehm ,ehs ∈y There are two versions of second order models, used respectively by Carreras (2007) and Koo et al. (2010). The difference is that Carreras’ only considers the outermost grandchildren, while Koo and Collin’s allows all grandchil"
Q13-1004,P10-1001,0,0.552501,"is a special symbol denoted by x0 which has exactly one modifier. In this paper, we focus on unlabeled projective dependency parsing but our algorithm can be adapted for labeled or non-projective dependency parsing (McDonald et al., 2005b). The inference problem is to search the optimal parse tree y ∗ y ∗ = arg maxy∈Y(x) ϕ(x, y) where Y(x) is the set of all candidate parse trees of sentence x. ϕ(x, y) is a given score function which is usually decomposed into small parts ∑ ϕ(x, y) = ϕc (x) (1) c⊆y where c is a subset of edges, and is called a factor. For example, in the all grandchild model (Koo and Collins, 2010), the score function can be represented as ∑ ∑ ϕ(x, y) = ϕehm (x) + ϕegh ,ehm (x) ehm ∈y egh ,ehm ∈y where the first term is the sum of scores of all edges xh → xm , and the second term is the sum of the scores of all edge chains xg → xh → xm . In discriminative models, the score of a parse tree y is the weighted sum of the fired feature functions, which can be represented by the sum of the factors ∑ ∑ ϕ(x, y) = wT f (x, y) = wT f (x, c) = ϕc (x) c⊆y c⊆y where f (x, c) is the feature vector that depends on c. For example, we could define a feature for grandchild c = {egh , ehm }   1 if xg ="
Q13-1004,P08-1068,0,0.0812309,"Missing"
Q13-1004,D10-1125,0,0.0914432,", where they proposed two branching strategies and a novel data structure for efficient upper bound computation. Klenner and Ailloud (2009) proposed a variation of Balas algorithm (Balas, 1965) for coreference resolution, where candidate branching variables are sorted by their weights. Our bounding strategy is to find an upper bound for the score of each non-local factor c containing multiple edges. The bound is the sum of new scores of edges in the factor plus a constant ϕehm ,ehs (x) ϕc (x) ≤ ehm ,ehs ∈y There are two versions of second order models, used respectively by Carreras (2007) and Koo et al. (2010). The difference is that Carreras’ only considers the outermost grandchildren, while Koo and Collin’s allows all grandchild features. Both models permit O(n4 ) running time. Third-order models score edge triples such as three adjacent sibling modifiers, or grand-siblings that score a word, its modifier and its adjacent grandchildren, and the inference complexity is O(n4 ) (Koo and Collins, 2010). In this paper, for all the factors/features that can be handled by DP, we call them the local factors/features. 3 3.1 The Proposed Method For general high order models with non-local features, we prop"
Q13-1004,P09-1039,0,0.587876,"dependency parsing using Markov Random Fields (MRFs) with global constraints and applied loopy belief propagation (LBP) for approximate learning and inference (Smith and Eisner, 2008). Similar work was done for Combinatorial Categorial Grammar (CCG) parsing (Auli and Lopez, 2011). They used posterior marginal beliefs for inference to satisfy the tree constraint: for each factor, only legal messages (satisfying global constraints) are considered in the partition function. A similar line of research investigated the use of integer linear programming (ILP) based parsing (Riedel and Clarke, 2006; Martins et al., 2009). This 37 Transactions of the Association for Computational Linguistics, 1 (2013) 37–48. Action Editor: Ryan McDonald. c Submitted 11/2012; Revised 2/2013; Published 3/2013. 2013 Association for Computational Linguistics. method is very expressive. It can handle arbitrary non-local features determined or bounded by linear inequalities of local features. For local models, LP is less efficient than DP. The reason is that, DP works on a small number of dimensions in each recursion, while for LP, the popular revised simplex method needs to solve a m dimensional linear system in each iteration (Noc"
Q13-1004,D11-1022,0,0.0547984,"case of Lagrangian relaxation. It relies on standard decoding algorithms as oracle solvers for sub-problems, together with a simple method for forcing agreement between the different oracles. This method does not need to consider the tree constraint explicitly, as it resorts to dynamic programming which guarantees its satisfaction. It works well if the sub-problems can be well defined, especially for joint learning tasks. However, for the task of dependency parsing, using various non-local features may result in many overlapped sub-problems, hence it may take a long time to reach a consensus (Martins et al., 2011). In this paper, we propose a novel Branch and Bound (B&B) algorithm for efficient parsing with various non-local features. B&B (Land and Doig, 1960) is generally used for combinatorial optimization problems such as ILP. The difference between our method and ILP is that the sub-problem in ILP is a relaxed LP, which requires a numerical solution, while ours bounds the non-local features by a linear combination of local features and uses DP for decoding as well as calculating the upper bound of the objective function. An exact solution is achieved if the bound is tight. Though in the worst case,"
Q13-1004,P05-1012,0,0.166766,"Missing"
Q13-1004,H05-1066,0,0.270028,"Missing"
Q13-1004,P08-1108,0,0.0605904,"Missing"
Q13-1004,W06-1616,0,0.0606569,"Smith and Eisner modeled dependency parsing using Markov Random Fields (MRFs) with global constraints and applied loopy belief propagation (LBP) for approximate learning and inference (Smith and Eisner, 2008). Similar work was done for Combinatorial Categorial Grammar (CCG) parsing (Auli and Lopez, 2011). They used posterior marginal beliefs for inference to satisfy the tree constraint: for each factor, only legal messages (satisfying global constraints) are considered in the partition function. A similar line of research investigated the use of integer linear programming (ILP) based parsing (Riedel and Clarke, 2006; Martins et al., 2009). This 37 Transactions of the Association for Computational Linguistics, 1 (2013) 37–48. Action Editor: Ryan McDonald. c Submitted 11/2012; Revised 2/2013; Published 3/2013. 2013 Association for Computational Linguistics. method is very expressive. It can handle arbitrary non-local features determined or bounded by linear inequalities of local features. For local models, LP is less efficient than DP. The reason is that, DP works on a small number of dimensions in each recursion, while for LP, the popular revised simplex method needs to solve a m dimensional linear system"
Q13-1004,D10-1001,0,0.207381,"ciation for Computational Linguistics. method is very expressive. It can handle arbitrary non-local features determined or bounded by linear inequalities of local features. For local models, LP is less efficient than DP. The reason is that, DP works on a small number of dimensions in each recursion, while for LP, the popular revised simplex method needs to solve a m dimensional linear system in each iteration (Nocedal and Wright, 2006), where m is the number of constraints, which is quadratic in sentence length for projective dependency parsing (Martins et al., 2009). Dual Decomposition (DD) (Rush et al., 2010; Koo et al., 2010) is a special case of Lagrangian relaxation. It relies on standard decoding algorithms as oracle solvers for sub-problems, together with a simple method for forcing agreement between the different oracles. This method does not need to consider the tree constraint explicitly, as it resorts to dynamic programming which guarantees its satisfaction. It works well if the sub-problems can be well defined, especially for joint learning tasks. However, for the task of dependency parsing, using various non-local features may result in many overlapped sub-problems, hence it may take a"
Q13-1004,D08-1016,0,0.0215931,"g for arc-factored models (Eisner, 1996; McDonald et al., 2005a) and biquadratic time for higher order models with richer sibling and grandchild features (Carreras, 2007; Koo and Collins, 2010). However, for models with general non-local features, DP is inefficient. There have been numerous studies on global inference algorithms for general higher order parsing. One popular approach is reranking (Collins, 2000; Smith and Eisner modeled dependency parsing using Markov Random Fields (MRFs) with global constraints and applied loopy belief propagation (LBP) for approximate learning and inference (Smith and Eisner, 2008). Similar work was done for Combinatorial Categorial Grammar (CCG) parsing (Auli and Lopez, 2011). They used posterior marginal beliefs for inference to satisfy the tree constraint: for each factor, only legal messages (satisfying global constraints) are considered in the partition function. A similar line of research investigated the use of integer linear programming (ILP) based parsing (Riedel and Clarke, 2006; Martins et al., 2009). This 37 Transactions of the Association for Computational Linguistics, 1 (2013) 37–48. Action Editor: Ryan McDonald. c Submitted 11/2012; Revised 2/2013; Publis"
Q13-1004,D09-1058,0,0.0383154,"Missing"
Q13-1004,W03-3023,0,0.101667,"Missing"
Q13-1004,D08-1059,0,0.0797135,"Missing"
Q13-1004,D12-1030,0,0.345256,"th sides of head. regulation occurs through inaction , rather than through ... Figure 3: An example of hand-craft feature: for the word sequence A . . . rather than A, where A is a preposition, the first A is the head of than, than is the head of rather and the second A. System PTB Our baseline 92.81 B&B +all grand-child 92.97 +all great grand-child 92.78 +all sibling 93.00 +all tri-sibling 92.79 +comb 92.86 +hand craft 92.89 +all grand-child + all sibling + com- 93.17 b + hand craft 3rd order re-impl. 93.03 TurboParser (reported) 92.62 TurboParser (our run) 92.82 Koo and Collins (2010) 93.04 Zhang and McDonald (2012) 93.06 Zhang and Nivre (2011) 92.90 System integration Bohnet and Kuhn (2012) 93.39 Systems using additional resources Suzuki et al. (2009) 93.79 Koo et al. (2008) 93.5 Chen et al. (2012) 92.76 CTB 86.89 87.02 86.77 87.05 86.81 86.91 N/A 87.25 87.07 N/A 86.05 N/A 86.87 86.00 87.5 N/A N/A N/A Table 2: Comparison between our system and thestate-of-art systems. 4.5 Main Result negative weighted factors. Step size α is initialized with maxc,ϕc =0 { |ϕ1c |}, as the vector p is bounded in a unit box. α is updated using the same strategy as Rush et al. (2010). Two stopping criteria are used. One is"
Q13-1004,P11-2033,0,0.0973999,"Missing"
Q14-1027,W02-1001,0,0.198147,"and shallow parsing. Recent studies have shown that the predictive power of CRFs can be strengthened by breaking the locality assumption. They either add long distance dependencies and patterns to linear chains for improved sequence labeling (Galley, 2006; Finkel et al., 2005; Kazama and Torisawa, 2007), or directly use the 4-connected neighborhood lattice (Ding et al., 2008). The resulting non-local models generally suffer from exponential time complexity of inference except some special cases (Sarawagi Approximate decoding algorithms have been proposed in the past decade, such as reranking (Collins, 2002b), loopy belief propagation (Sutton and Mccallum, 2006), tree reweighted belief propagation (Kolmogorov, 2006). In this paper, we focus on dual decomposition (DD), which has attracted much attention recently due to its simplicity and effectiveness (Rush and Collins, 2012). In short, it decomposes the decoding problem into several sub-problems. For each sub-problem, an efficient decoding algorithm is deployed as a slave solver. Finally a simple method forces agreement among different slaves. A popular choice is the subgradient algorithm. Martins et al. (2011b) showed that the success of the su"
Q14-1027,P02-1062,0,0.262235,"and shallow parsing. Recent studies have shown that the predictive power of CRFs can be strengthened by breaking the locality assumption. They either add long distance dependencies and patterns to linear chains for improved sequence labeling (Galley, 2006; Finkel et al., 2005; Kazama and Torisawa, 2007), or directly use the 4-connected neighborhood lattice (Ding et al., 2008). The resulting non-local models generally suffer from exponential time complexity of inference except some special cases (Sarawagi Approximate decoding algorithms have been proposed in the past decade, such as reranking (Collins, 2002b), loopy belief propagation (Sutton and Mccallum, 2006), tree reweighted belief propagation (Kolmogorov, 2006). In this paper, we focus on dual decomposition (DD), which has attracted much attention recently due to its simplicity and effectiveness (Rush and Collins, 2012). In short, it decomposes the decoding problem into several sub-problems. For each sub-problem, an efficient decoding algorithm is deployed as a slave solver. Finally a simple method forces agreement among different slaves. A popular choice is the subgradient algorithm. Martins et al. (2011b) showed that the success of the su"
Q14-1027,P08-1081,0,0.0177388,"afferty et al., 2001) (CRFs) are popular models for many NLP tasks. In particular, the linear chain CRFs explore local structure information for sequence labeling tasks, such as part-of-speech (POS) tagging, named entity recognition (NER), and shallow parsing. Recent studies have shown that the predictive power of CRFs can be strengthened by breaking the locality assumption. They either add long distance dependencies and patterns to linear chains for improved sequence labeling (Galley, 2006; Finkel et al., 2005; Kazama and Torisawa, 2007), or directly use the 4-connected neighborhood lattice (Ding et al., 2008). The resulting non-local models generally suffer from exponential time complexity of inference except some special cases (Sarawagi Approximate decoding algorithms have been proposed in the past decade, such as reranking (Collins, 2002b), loopy belief propagation (Sutton and Mccallum, 2006), tree reweighted belief propagation (Kolmogorov, 2006). In this paper, we focus on dual decomposition (DD), which has attracted much attention recently due to its simplicity and effectiveness (Rush and Collins, 2012). In short, it decomposes the decoding problem into several sub-problems. For each sub-probl"
Q14-1027,P05-1045,0,0.182324,"method outperforms spanning tree based dual decomposition. 1 Introduction Conditional Random Fields (Lafferty et al., 2001) (CRFs) are popular models for many NLP tasks. In particular, the linear chain CRFs explore local structure information for sequence labeling tasks, such as part-of-speech (POS) tagging, named entity recognition (NER), and shallow parsing. Recent studies have shown that the predictive power of CRFs can be strengthened by breaking the locality assumption. They either add long distance dependencies and patterns to linear chains for improved sequence labeling (Galley, 2006; Finkel et al., 2005; Kazama and Torisawa, 2007), or directly use the 4-connected neighborhood lattice (Ding et al., 2008). The resulting non-local models generally suffer from exponential time complexity of inference except some special cases (Sarawagi Approximate decoding algorithms have been proposed in the past decade, such as reranking (Collins, 2002b), loopy belief propagation (Sutton and Mccallum, 2006), tree reweighted belief propagation (Kolmogorov, 2006). In this paper, we focus on dual decomposition (DD), which has attracted much attention recently due to its simplicity and effectiveness (Rush and Coll"
Q14-1027,W06-1643,0,0.0351737,"show that our method outperforms spanning tree based dual decomposition. 1 Introduction Conditional Random Fields (Lafferty et al., 2001) (CRFs) are popular models for many NLP tasks. In particular, the linear chain CRFs explore local structure information for sequence labeling tasks, such as part-of-speech (POS) tagging, named entity recognition (NER), and shallow parsing. Recent studies have shown that the predictive power of CRFs can be strengthened by breaking the locality assumption. They either add long distance dependencies and patterns to linear chains for improved sequence labeling (Galley, 2006; Finkel et al., 2005; Kazama and Torisawa, 2007), or directly use the 4-connected neighborhood lattice (Ding et al., 2008). The resulting non-local models generally suffer from exponential time complexity of inference except some special cases (Sarawagi Approximate decoding algorithms have been proposed in the past decade, such as reranking (Collins, 2002b), loopy belief propagation (Sutton and Mccallum, 2006), tree reweighted belief propagation (Kolmogorov, 2006). In this paper, we focus on dual decomposition (DD), which has attracted much attention recently due to its simplicity and effecti"
Q14-1027,P11-2008,0,0.0419038,"Missing"
Q14-1027,D07-1033,0,0.126366,"panning tree based dual decomposition. 1 Introduction Conditional Random Fields (Lafferty et al., 2001) (CRFs) are popular models for many NLP tasks. In particular, the linear chain CRFs explore local structure information for sequence labeling tasks, such as part-of-speech (POS) tagging, named entity recognition (NER), and shallow parsing. Recent studies have shown that the predictive power of CRFs can be strengthened by breaking the locality assumption. They either add long distance dependencies and patterns to linear chains for improved sequence labeling (Galley, 2006; Finkel et al., 2005; Kazama and Torisawa, 2007), or directly use the 4-connected neighborhood lattice (Ding et al., 2008). The resulting non-local models generally suffer from exponential time complexity of inference except some special cases (Sarawagi Approximate decoding algorithms have been proposed in the past decade, such as reranking (Collins, 2002b), loopy belief propagation (Sutton and Mccallum, 2006), tree reweighted belief propagation (Kolmogorov, 2006). In this paper, we focus on dual decomposition (DD), which has attracted much attention recently due to its simplicity and effectiveness (Rush and Collins, 2012). In short, it dec"
Q14-1027,D10-1125,0,0.022348,"sition considers the equivalent problem max Y,Z1 ...ZM s.t. M ∑ ϕi (Zi ) i=1 Zi = Y ∀i Using Lagrangian relaxation to eliminate the constraint, we get min λ max Y,Z1 ...ZM M ∑ i=1 ϕi (Zi ) + ∑ i λTi (Y − Zi ) (4) which provides the upper bound of the original problem. λ is the Lagrange multiplier, which is typically optimized via sub-gradient algorithms. Martins et al. (2011b) showed that the success of sub-gradient algorithms is strongly tied to the ability of finding a good decomposition, i.e., one involving few slaves. Finding a concise decomposition is usually task dependent. For example, Koo et al. (2010) introduced dual decomposition for parsing with non-projective head automata. They used only two slaves: one is the arc-factored model, and the other is head automata which involves adjacent siblings and can be solved using dynamic programming in linear time. Dual decomposition is especially efficient for joint learning tasks because a concise decomposition can be derived naturally where each slave solves one subtask. For example, Rush et al. (2010) used two slaves for integrated phrase-structure parsing and trigram POS tagging task. However, for generalized higher order CRFs, a lightweight de"
Q14-1027,D11-1022,0,0.139625,"sed in the past decade, such as reranking (Collins, 2002b), loopy belief propagation (Sutton and Mccallum, 2006), tree reweighted belief propagation (Kolmogorov, 2006). In this paper, we focus on dual decomposition (DD), which has attracted much attention recently due to its simplicity and effectiveness (Rush and Collins, 2012). In short, it decomposes the decoding problem into several sub-problems. For each sub-problem, an efficient decoding algorithm is deployed as a slave solver. Finally a simple method forces agreement among different slaves. A popular choice is the subgradient algorithm. Martins et al. (2011b) showed that the success of the sub-gradient algorithm is strongly tied to the ability of finding a good decomposition, i.e., one involving few overlapping slaves. However, for generalized higher order graphical models, a lightweight decomposition is not at hand and many overlapping slaves may be involved. Martins et al. (2011b) showed that the sub-gradient algorithm exhibits extremely slow convergence in such cases, and they proposed the alternating directions method (DD-ADMM) to tackle these. In this paper, we propose a 2-slave dual decomposition approach for efficient decoding in higher o"
Q14-1027,N13-1039,0,0.0622654,"Missing"
Q14-1027,P12-1058,1,0.927707,"bjectives per instance (the lower the tighter), decoding time, and fraction of optimality certificates across iterations of the two DD algorithms on test data. Figure 2 shows the performances of the two algorithms relative to decoding time. Our method requires 0.0064 seconds for each iteration on average, about four times slower than the naive DD. However, our approach achieves a tighter upper bound and larger fraction of optimality certificates. 4.2 Sentence Dependency Tagging Our second experiment is sentence dependency tagging in Question Answering forums task studied in Qu and Liu’s work (Qu and Liu, 2012). The goal is to extract the dependency relationships between sentences for automatic question answering. For example, from the posts below, we would need to know that sentence S4 is a comment about sentence S1 and S2, not an answer to S3. 7300 0.9 7200 Dual Objective Percentage 1 0.8 0.7 %certificates of naive DD %certificates of 2−slave DD F score of naive DD F score of 2−slave DD 0.6 0.5 0 50 100 150 Decoding Time (sec.) naive DD 2−slave DD 7100 7000 6900 6800 0 200 50 100 150 Decoding Time (sec.) 200 Figure 2: Twitter NER: The Fmicro scores, dual objectives, and fraction of optimality cert"
Q14-1027,D11-1141,0,0.0192642,"in Table 1. In summary, each pattern in h(Z, u) requires at most 2|c |− 2 variables, so h(Z, u) has no more than ∑ ∑ c s∈c[·] (2|c |− 2) variables. Finally, the time complexity for each iteration in dual decomposition is ( |ϕc[s] |u0c[s] (2Zc[s] − 3)  3  ∑ ∑   O N |S|2 +  (2|c |− 2)   − j − 2) .  c s∈c[·] which is cubic in the total length of patterns. 344 4 Experimental Results 4.1 Named Entity Recognition in Tweets 4.1.1 Data Sets Our first experiment is named entity recognition in tweets. Recently, information extraction on Twitter or Facebook data is attracting much attention (Ritter et al., 2011). Different from traditional information extraction for news articles, messages posted on these social media websites are short and noisy, making the task more challenging. In this paper, we use generalized higher order CRFs for Twitter NER with discriminative training, and compare our 2-slave dual decomposition approach with spanning tree based dual decomposition approach and other decoding algorithms. So far as we know, there are two publicly available data sets for Twitter NER. One is the Ritter’s (Ritter et al., 2011), the other is from MSM2013 Concept Extraction Challenge (Basave et al.,"
Q14-1027,D10-1001,0,0.158261,"he ability of finding a good decomposition, i.e., one involving few slaves. Finding a concise decomposition is usually task dependent. For example, Koo et al. (2010) introduced dual decomposition for parsing with non-projective head automata. They used only two slaves: one is the arc-factored model, and the other is head automata which involves adjacent siblings and can be solved using dynamic programming in linear time. Dual decomposition is especially efficient for joint learning tasks because a concise decomposition can be derived naturally where each slave solves one subtask. For example, Rush et al. (2010) used two slaves for integrated phrase-structure parsing and trigram POS tagging task. However, for generalized higher order CRFs, a lightweight decomposition may be not at 341 hand. Martins et al. (2011a) showed that the sub-gradient algorithms exhibited extremely slow convergence when handling many slaves. For fast convergence, they employed alternating directions dual decomposition (AD3 ), which relaxes the agreement constraint via augmented Lagrangian Relaxation, where an additional quadratic penalty term was added into the Lagrangian (Eq (4)). Similarly, Jojic et al. (2010) added a strong"
Q17-1007,J93-2003,0,0.0952604,"ments in Section 5.2 show that our gating mechanism significantly outperforms linear interpolation when combining contexts. Comparison to Handling Null-Generated Words in SMT: In machine translation, there are certain syntactic elements of the target language that are missing in the source (i.e., null-generated words). In fact this was the preliminary motivation for our approach: current attention models lack a mechanism to control the generation of words that do not have a strong correspondence on the source side. The model structure of NMT is quite similar to the traditional word-based SMT (Brown et al., 1993). Therefore, techniques that have proven effective in SMT may also be applicable to NMT. Toutanova et al. (2002) extend the calculation of translation probabilities to include null-generated target words in word-based SMT. These words are generated based on both the special source token null and the neighbouring word in the target language by a mixture model. We have simplified and generalized their approach: we use context gates to dynamically control the contribution of source context. When producing null-generated words, the context gate can as93 sign lower weights to the source context, by"
Q17-1007,P05-1066,0,0.0240416,"Missing"
Q17-1007,P15-1001,0,0.0454042,"SMT and NMT6 models: • Moses (Koehn et al., 2007): an open source phrase-based translation system with default configuration and a 4-gram language model trained on the target portion of training data; • GroundHog (Bahdanau et al., 2015): an open source attention-based NMT model with default setting. We have two variants that differ in the activation function used in the decoder 5 The corpora include LDC2002E18, LDC2003E07, LDC2003E14, Hansards portion of LDC2004T07, LDC2004T08 and LDC2005T06. 6 There is some recent progress on aggregating multiple models or enlarging the vocabulary(e.g.,, in (Jean et al., 2015)), but here we focus on the generic models. # 1 2 3 4 5 6 7 8 9 System Moses GroundHog (vanilla) 2 + Context Gate (both) GroundHog (GRU ) 4 + Context Gate (source) 4 + Context Gate (target) 4 + Context Gate (both) GroundHog-Coverage (GRU ) 8 + Context Gate (both) #Parameters – 77.1M 80.7M 84.3M 87.9M 87.9M 87.9M 84.4M 88.0M MT05 31.37 26.07 30.86∗ 30.61 31.96∗ 32.38∗ 33.52∗ 32.73 34.13∗ MT06 30.85 27.34 30.85∗ 31.12 32.29∗ 32.11∗ 33.46∗ 32.47 34.83∗ MT08 23.01 20.38 24.71∗ 23.23 24.97∗ 23.78 24.85∗ 25.23 26.22∗ Ave. 28.41 24.60 28.81 28.32 29.74 29.42 30.61 30.14 31.73 Table 2: Evaluation of t"
Q17-1007,D13-1176,0,0.0685886,"ear , the export of new high level technology product was UNK - billion us dollars china ’s guangdong hi - tech exports hit 58 billion dollars china ’s export of high and new hi - tech exports of the export of the export of the export of the export of the export of the export of the export of the export of · · · Table 1: Source and target contexts are highly correlated to translation adequacy and fluency, respectively. 5src and 5tgt denote halving the contributions from the source and target contexts when generating the translation, respectively. Introduction Neural machine translation (NMT) (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015) has made significant progress in the past several years. Its goal is to construct and utilize a single large neural network to accomplish the entire translation task. One great advantage of NMT is that the translation system can be completely constructed by learning from data without human involvement (cf., feature engineering in statistical machine translation (SMT)). The encoderdecoder architecture is widely employed (Cho et al., 2014; Sutskever et al., 2014), in which the encoder summarizes the source sentence into a vector representation, an"
Q17-1007,P07-2045,0,0.0649955,"Missing"
Q17-1007,D15-1166,0,0.200096,"Missing"
Q17-1007,J03-1002,0,0.0544157,"Missing"
Q17-1007,P02-1040,0,0.118366,"Missing"
Q17-1007,W09-0441,0,0.0117211,"t words in the target sentence are more related to the translation adequacy, and thus should depend more on the source context. In contrast, function words in the target sentence are often more related to the translation fluency (e.g., “of” after “is fond”), and thus should depend more on the target context. In this work, we propose to use context gates to control the contributions of source and target contexts on the generation of target words (decoding) 1 Fluency measures whether the translation is fluent, while adequacy measures whether the translation is faithful to the original sentence (Snover et al., 2009). 88 Figure 1: Architecture of decoder RNN. in NMT. Context gates are non-linear gating units which can dynamically select the amount of context information in the decoding process. Specifically, at each decoding step, the context gate examines both the source and target contexts, and outputs a ratio between zero and one to determine the percentages of information to utilize from the two contexts. In this way, the system can balance the adequacy and fluency of the translation with regard to the generation of a word at each position. Experimental results show that introducing context gates lead"
Q17-1007,W02-1012,0,0.167148,"Missing"
Q17-1007,P16-1008,1,0.93311,"ted.4 The decoding state implicitly models the notion of “coverage” by recurrently reading the time-dependent source context si . Lowering its contribution weakens the “coverage” effect and encourages the decoder to regenerate phrases multiple times to achieve the desired translation length. 2. The translation is incomplete. As shown in Table 1, NMT can get stuck in an infinite loop repeatedly generating a phrase due to the overwhelming influence of the source context. As a result, generation terminates early because 4 The recently proposed coverage based technique can alleviate this problem (Tu et al., 2016). In this work, we consider another approach, which is complementary to the coverage mechanism. 90 Figure 3: Architecture of context gate. the translation reaches the maximum length allowed by the implementation, even though the decoding procedure is not finished. The quantitative (Figure 2) and qualitative (Table 1) results confirm our hypothesis, i.e., source and target contexts are highly correlated to translation adequacy and fluency. We believe that a mechanism that can dynamically select information from source context and target context would be useful for NMT models, and this is exactl"
Q17-1007,P16-1125,0,0.0156616,"context gate controls the effect of the source context based on its relative importance. Experiments in Section 5.2 show that combining the two methods can further improve translation performance. There is another difference as well: the coverage mechanism is only applicable to attention-based NMT models, while the context gate is applicable to all NMT models. Comparison to Exploiting Auxiliary Contexts in Language Modeling: A thread of work in language modeling (LM) attempts to exploit auxiliary sentence-level or document-level context in an RNN LM (Mikolov and Zweig, 2012; Ji et al., 2015; Wang and Cho, 2016). Independent of our work, Wang and Cho (2016) propose “early fusion” models of RNNs where additional information from an intersentence context is “fused” with the input to the RNN. Closely related to Wang and Cho (2016), our approach aims to dynamically control the contributions of required source and target contexts for machine translation, while theirs focuses on integrating auxiliary corpus-level contexts for language modelling to better approximate the corpus-level probability. In addition, we employ a gating mechanism to produce a dynamic weight at different decoding steps to combine sou"
Q18-1005,D15-1263,0,0.0584554,"ty of Edinburgh 10 Crichton Street, Edinburgh EH8 9AB yang.liu2@ed.ac.uk,mlap@inf.ed.ac.uk Abstract ture Theory (RST; Mann and Thompson, 1988), graphs (Lin et al., 2011; Wolf and Gibson, 2006), entity transitions (Barzilay and Lapata, 2008), or combinations thereof (Lin et al., 2011; Mesgar and Strube, 2015). The availability of discourse annotated corpora (Carlson et al., 2001; Prasad et al., 2008) has led to the development of off-the-shelf discourse parsers (e.g., Feng and Hirst, 2012; Liu and Lapata, 2017), and the common use of trees as representations of document structure. For example, Bhatia et al. (2015) improve document-level sentiment analysis by reweighing discourse units based on the depth of RST trees, whereas Ji and Smith (2017) show that a recursive neural network built on the output of an RST parser benefits text categorization in learning representations that focus on salient content. In this paper, we focus on learning structureaware document representations from data without recourse to a discourse parser or additional annotations. Drawing inspiration from recent efforts to empower neural networks with a structural bias (Cheng et al., 2016; Kim et al., 2017), we propose a model tha"
Q18-1005,D15-1075,0,0.0261047,"amely natural language inference. We then assess the document-level representations obtained by our model on a variety of classification tasks representing documents of different length, subject matter, and language. Our 68 Natural Language Inference The ability to reason about the semantic relationship between two sentences is an integral part of text understanding. We therefore evaluate our model on recognizing textual entailment, i.e., whether two premise-hypothesis pairs are entailing, contradictory, or neutral. For this task we used the Stanford Natural Language Inference (SNLI) dataset (Bowman et al., 2015), which contains premise-hypothesis pairs and target labels indicating their relation. After removing sentences with unknown labels, we obtained 549,367 pairs for training, 9,842 for development and 9,824 for testing. Sentence-level representations obtained by our model (with structured attention) were used to encode the premise and hypothesis by modifying the model of Parikh et al. (2016) as follows. Let [xp1 , · · · , xpn ] and [xh1 , · · · , xhm ] be the input vectors for the premise and hypothesis, respectively. Application of structured attention yields new vector reph ]. Then resentation"
Q18-1005,P16-1139,0,0.0515415,"r two-layer perceptron with a softmax layer to obtain the predicted distribution over the labels. The hidden size of the LSTM was set to 150. The dimensions of the semantic vector were 100 and the dimensions of structure vector were 50. We used pretrained 300-D Glove 840B (Pennington et al., 2014) vectors to initialize the word embeddings. All parameters (including word embeddings) were updated with Adagrad (Duchi et al., 2011), and the Models Classifier with handcrafted features (Bowman et al., 2015) 300D LSTM encoders (Bowman et al., 2015) 300D Stack-Augmented Parser-Interpreter Neural Net (Bowman et al., 2016) 100D LSTM with inter-attention (Rockt¨aschel et al., 2016) 200D Matching LSTMs (Wang and Jiang, 2016) 450D LSTMN with deep attention fusion (Cheng et al., 2016) Decomposable Attention over word embeddings (Parikh et al., 2016) Enhanced BiLSTM Inference Model (Chen et al., 2017) 175D No Attention 175D Simple intra-sentence attention 100D Structured intra-sentence attention with Inside-Outside 175D Structured intra-sentence attention with Matrix Inversion Acc 78.2 80.6 83.2 83.5 86.1 86.3 86.8 88.0 85.3 86.2 86.8 86.9 θ — 3.0M 3.7M 252K 1.9M 3.4M 582K 4.3M 600K 1.1M 1.2M 1.1M Table 1: Test accu"
Q18-1005,R13-1016,0,0.173339,"ention (sentence-level) 100D Structured Attention (document-level) 100D Structured Attention (both levels) Yelp IMDB CZ Movies Debates θ 59.8 40.9 78.5 74.0 — 57.7 34.1 — —— 59.7 — — — — 63.7 42.5 — — — 65.1 45.3 — — — — — — 75.7 — 68.2 49.4 80.8 74.0 273K 66.7 47.5 80.5 73.7 330K 67.7 48.2 81.4 75.3 860K 68.0 48.8 81.5 74.6 842K 67.8 48.6 81.1 75.2 842K 68.6 49.2 82.1 76.5 860K Table 4: Test accuracy on four datasets and number of parameters θ (excluding embeddings). Regarding feature-based classification methods, results on Yelp and IMDB are taken from Tang et al. (2015a), on CZ movies from Brychcın and Habernal (2013), and Debates from Yogatama and Smith (2014). Wherever available we also provide the size of the recurrent unit (LSTM or GRU). 4.2 Document Classification In this section, we evaluate our document-level model on a variety of classification tasks. We selected four datasets which we describe below. Table 3 summarizes some statistics for each dataset. Yelp reviews were obtained from the 2013 Yelp Dataset Challenge. This dataset contains restaurant reviews, each associated with human ratings on a scale from 1 (negative) to 5 (positive) which we used as gold labels for sentiment classification; we"
Q18-1005,W01-1605,0,0.333608,"Missing"
Q18-1005,P17-1152,0,0.0371546,"ton et al., 2014) vectors to initialize the word embeddings. All parameters (including word embeddings) were updated with Adagrad (Duchi et al., 2011), and the Models Classifier with handcrafted features (Bowman et al., 2015) 300D LSTM encoders (Bowman et al., 2015) 300D Stack-Augmented Parser-Interpreter Neural Net (Bowman et al., 2016) 100D LSTM with inter-attention (Rockt¨aschel et al., 2016) 200D Matching LSTMs (Wang and Jiang, 2016) 450D LSTMN with deep attention fusion (Cheng et al., 2016) Decomposable Attention over word embeddings (Parikh et al., 2016) Enhanced BiLSTM Inference Model (Chen et al., 2017) 175D No Attention 175D Simple intra-sentence attention 100D Structured intra-sentence attention with Inside-Outside 175D Structured intra-sentence attention with Matrix Inversion Acc 78.2 80.6 83.2 83.5 86.1 86.3 86.8 88.0 85.3 86.2 86.8 86.9 θ — 3.0M 3.7M 252K 1.9M 3.4M 582K 4.3M 600K 1.1M 1.2M 1.1M Table 1: Test accuracy on the SNLI dataset and number of parameters θ (excluding embeddings). Wherever available we also provide the size of the recurrent unit. Models No Attention Simple Attention Matrix Inversion Inside-Outside Speed Max Avg 0.0050 0.0033 0.0057 0.0042 0.0070 0.0045 0.1200 0.03"
Q18-1005,D16-1053,1,0.381844,"ions of document structure. For example, Bhatia et al. (2015) improve document-level sentiment analysis by reweighing discourse units based on the depth of RST trees, whereas Ji and Smith (2017) show that a recursive neural network built on the output of an RST parser benefits text categorization in learning representations that focus on salient content. In this paper, we focus on learning structureaware document representations from data without recourse to a discourse parser or additional annotations. Drawing inspiration from recent efforts to empower neural networks with a structural bias (Cheng et al., 2016; Kim et al., 2017), we propose a model that can encode a document while automatically inducing rich structural dependencies. Specifically, we embed a differentiable non-projective parsing algorithm into a neural model and use attention mechanisms to incorporate the structural biases. Experimental evaluations across different tasks and datasets show that the proposed model achieves state-of-the-art results on document modeling tasks while inducing intermediate structures which are both interpretable and meaningful. 1 Introduction Document modeling is a fundamental task in Natural Language Proc"
Q18-1005,P12-1007,0,0.0351431,"xt Representations Yang Liu and Mirella Lapata Institute for Language, Cognition and Computation School of Informatics, University of Edinburgh 10 Crichton Street, Edinburgh EH8 9AB yang.liu2@ed.ac.uk,mlap@inf.ed.ac.uk Abstract ture Theory (RST; Mann and Thompson, 1988), graphs (Lin et al., 2011; Wolf and Gibson, 2006), entity transitions (Barzilay and Lapata, 2008), or combinations thereof (Lin et al., 2011; Mesgar and Strube, 2015). The availability of discourse annotated corpora (Carlson et al., 2001; Prasad et al., 2008) has led to the development of off-the-shelf discourse parsers (e.g., Feng and Hirst, 2012; Liu and Lapata, 2017), and the common use of trees as representations of document structure. For example, Bhatia et al. (2015) improve document-level sentiment analysis by reweighing discourse units based on the depth of RST trees, whereas Ji and Smith (2017) show that a recursive neural network built on the output of an RST parser benefits text categorization in learning representations that focus on salient content. In this paper, we focus on learning structureaware document representations from data without recourse to a discourse parser or additional annotations. Drawing inspiration from"
Q18-1005,W16-3616,0,0.185888,"ical model over latent variables. They first calculate unnormalized pairwise attention scores for all tokens in a sentence and then use the inside-outside algorithm to normalize the scores with the marginal probabilities of a dependency tree. Without recourse to an external parser, their model learns meaningful task-specific dependency structures, achieving competitive results in several sentence-level tasks. However, for document modeling, this approach has two drawbacks. Firstly, it does not consider non-projective dependency structures, which are common in documentlevel discourse analysis (Hayashi et al., 2016; Lee et al., 2006). As illustrated in Figure 1, the tree structure of a document can be flexible and the dependency edges may cross. Secondly, the inside-outside algorithm involves a dynamic programming process which is difficult to parallelize, making it impractical for modeling long documents.1 In this paper, we propose a new model for representing documents while automatically learning richer structural dependencies. Using a variant of Kirchhoff’s Matrix-Tree Theorem (Tutte, 1984), our model implicitly considers non-projective depen1 In our experiments, adding the inside-outside pass incre"
Q18-1005,D13-1158,0,0.0469277,"ext vectors are concatenated with ei and transformed with weights Wr ∈ Rke ∗3ke to obtain the updated semantic vector ri ∈ Rke with rich structural information (see Figure 3). 3.3 Document Model We build document representations hierarchically: sentences are composed of words and documents are composed of sentences. Composition on the document level also makes use of structured attention in the form of a dependency graph. Dependencybased representations have been previously used for developing discourse parsers (Hayashi et al., 2016; Li et al., 2014) and in applications such as summarization (Hirao et al., 2013). As illustrated in Figure 4, given a document with n sentences [s1 , s2 , · · · , sn ], for each sentence si , the input is a sequence of word embeddings [ui1 , ui2 , · · · , uim ], where m is the number of tokens in si . By feeding the embeddings into a sentence-level bi-LSTM and applying the proposed structured attention mechanism, we obtain the updated semantic vector [ri1 , ri2 , · · · , rim ]. Then a pooling operation produces a fixed-length vector vi for each sentence. Analogously, we view the document as a sequence of sentence vectors [v1 , v2 , · · · , vn ] whose embeddings are fed to"
Q18-1005,P82-1020,0,0.852256,"Missing"
Q18-1005,P17-1092,0,0.456212,"n, 1988), graphs (Lin et al., 2011; Wolf and Gibson, 2006), entity transitions (Barzilay and Lapata, 2008), or combinations thereof (Lin et al., 2011; Mesgar and Strube, 2015). The availability of discourse annotated corpora (Carlson et al., 2001; Prasad et al., 2008) has led to the development of off-the-shelf discourse parsers (e.g., Feng and Hirst, 2012; Liu and Lapata, 2017), and the common use of trees as representations of document structure. For example, Bhatia et al. (2015) improve document-level sentiment analysis by reweighing discourse units based on the depth of RST trees, whereas Ji and Smith (2017) show that a recursive neural network built on the output of an RST parser benefits text categorization in learning representations that focus on salient content. In this paper, we focus on learning structureaware document representations from data without recourse to a discourse parser or additional annotations. Drawing inspiration from recent efforts to empower neural networks with a structural bias (Cheng et al., 2016; Kim et al., 2017), we propose a model that can encode a document while automatically inducing rich structural dependencies. Specifically, we embed a differentiable non-projec"
Q18-1005,D07-1015,0,0.721659,"Missing"
Q18-1005,P14-1003,0,0.026708,"and eroot is a special embedding for the root node. The context vectors are concatenated with ei and transformed with weights Wr ∈ Rke ∗3ke to obtain the updated semantic vector ri ∈ Rke with rich structural information (see Figure 3). 3.3 Document Model We build document representations hierarchically: sentences are composed of words and documents are composed of sentences. Composition on the document level also makes use of structured attention in the form of a dependency graph. Dependencybased representations have been previously used for developing discourse parsers (Hayashi et al., 2016; Li et al., 2014) and in applications such as summarization (Hirao et al., 2013). As illustrated in Figure 4, given a document with n sentences [s1 , s2 , · · · , sn ], for each sentence si , the input is a sequence of word embeddings [ui1 , ui2 , · · · , uim ], where m is the number of tokens in si . By feeding the embeddings into a sentence-level bi-LSTM and applying the proposed structured attention mechanism, we obtain the updated semantic vector [ri1 , ri2 , · · · , rim ]. Then a pooling operation produces a fixed-length vector vi for each sentence. Analogously, we view the document as a sequence of sente"
Q18-1005,P11-1100,0,0.0185251,"Missing"
Q18-1005,Q16-1037,0,0.0136631,"ucing structural information. We applied this approach to model documents hierarchically, incorporating both sentenceand document-level structure. Experiments on sentence and document modeling tasks show that the representations learned by our model achieve competitive performance against strong comparison systems. Analysis of the induced tree structures revealed that they are meaningful, albeit different from linguistics ones, without ever exposing the model to linguistic annotations or an external parser. Directions for future work are many and varied. Given appropriate training objectives (Linzen et al., 2016), it should be possible to induce linguistically meaningful dependency trees using the proposed attention mechanism. We also plan to explore how document-level trees can be usefully employed in summarization, e.g., as a means to represent or even extract important content. Acknowledgments The authors gratefully acknowledge the support of the European Research Council (award number 681760). We also thank the anonymous TACL reviewers and the action editor whose feedback helped improve the present paper, members of EdinburghNLP for helpful discussions and suggestions, and Barbora Skarabela for tr"
Q18-1005,D17-1133,1,0.796561,"g Liu and Mirella Lapata Institute for Language, Cognition and Computation School of Informatics, University of Edinburgh 10 Crichton Street, Edinburgh EH8 9AB yang.liu2@ed.ac.uk,mlap@inf.ed.ac.uk Abstract ture Theory (RST; Mann and Thompson, 1988), graphs (Lin et al., 2011; Wolf and Gibson, 2006), entity transitions (Barzilay and Lapata, 2008), or combinations thereof (Lin et al., 2011; Mesgar and Strube, 2015). The availability of discourse annotated corpora (Carlson et al., 2001; Prasad et al., 2008) has led to the development of off-the-shelf discourse parsers (e.g., Feng and Hirst, 2012; Liu and Lapata, 2017), and the common use of trees as representations of document structure. For example, Bhatia et al. (2015) improve document-level sentiment analysis by reweighing discourse units based on the depth of RST trees, whereas Ji and Smith (2017) show that a recursive neural network built on the output of an RST parser benefits text categorization in learning representations that focus on salient content. In this paper, we focus on learning structureaware document representations from data without recourse to a discourse parser or additional annotations. Drawing inspiration from recent efforts to empo"
Q18-1005,P14-5010,0,0.00398433,"Missing"
Q18-1005,W07-2216,0,0.0247203,"end-to-end fashion and induce discourse information that is helpful to specific tasks without an external parser. The inside-outside model of Kim et al. (2017) and our model both have a O(n3 ) worst case complexity. However, major operations in our approach can be parallelized efficiently on GPU computing hardware. Although our primary focus is on document modeling, there is nothing inherent in our model that prevents its application to individual sentences. Advantageously, it can induce non-projective structures which are required for representing languages with free or flexible word order (McDonald and Satta, 2007). Our contributions in this work are threefold: a model for learning document representations whilst taking structural information into account; an efficient training procedure which allows to compute document level representations of arbitrary length; and a large scale evaluation study showing that the proposed model performs competitively against strong baselines while inducing intermediate structures which are both interpretable and meaningful. 2 Background In this section, we describe how previous work uses the attention mechanism for representing individual sentences. The key idea is to c"
Q18-1005,S15-1036,0,0.0308688,"Missing"
Q18-1005,W13-3303,0,0.0285593,"e structural biases. Experimental evaluations across different tasks and datasets show that the proposed model achieves state-of-the-art results on document modeling tasks while inducing intermediate structures which are both interpretable and meaningful. 1 Introduction Document modeling is a fundamental task in Natural Language Processing useful to various downstream applications including topic labeling (Xie and Xing, 2013), summarization (Chen et al., 2016; Wolf and Gibson, 2006), sentiment analysis (Bhatia et al., 2015), question answering (Verberne et al., 2007), and machine translation (Meyer and Webber, 2013). Recent work provides strong evidence that better document representations can be obtained by incorporating structural knowledge (Bhatia et al., 2015; Ji and Smith, 2017; Yang et al., 2016). Inspired by existing theories of discourse, representations of document structure have assumed several guises in the literature, such as trees in the style of Rhetorical StrucLinguistically motivated representations of document structure rely on the availability of annotated corpora as well as a wider range of standard NLP tools (e.g., tokenizers, pos-taggers, syntactic parsers). Unfortunately, the relian"
Q18-1005,D16-1147,0,0.0189763,"aper we use bidirectional LSTMs as a way of representing elements in a sequence (i.e., words or sentences) together with their contexts, capturing the element and an “infinite” window around it. Specifically, we run a bidirectional LSTM over sentence T , and take the output vectors [h1 , h2 , · · · , hn ] as the representations of words in T , where ht ∈ Rk is the output vector for word ut based on its context. We then exploit the structure of T which we induce based on an attention mechanism detailed below to obtain more precise representations. Inspired by recent work (Daniluk et al., 2017; Miller et al., 2016), which shows that the conventional way of using LSTM output vectors for calculating both attention and encoding word semantics is overloaded and likely to cause performance deficiencies, we decompose the LSTM output vector in two parts: [et , dt ] = ht (7) Rkt , where et ∈ the semantic vector, encodes semantic information for specific tasks, and dt ∈ Rks , the structure vector, is used to calculate structured attention. We use a series of operations based on the MatrixTree Theorem (Tutte, 1984) to incorporate the struc66 Structured Attention Mechanism Dependency representations of natural lan"
Q18-1005,D16-1244,0,0.0309547,"Missing"
Q18-1005,D14-1162,0,0.121071,"(rjh ) m X exp(oij ) Pm r¯ip = [rip , ] k=1 exp(oik ) r¯ih = [rih , j=1 m X i=1 p r = n X i=1 g(¯ rip ), exp(oij ) Pm ] k=1 exp(okj ) h r = m X g(¯ rih ) (20) (21) (22) (23) i=1 where M LP () is a two-layer perceptron with a ReLU activation function. The new representations r p , r h are then concatenated and fed into another two-layer perceptron with a softmax layer to obtain the predicted distribution over the labels. The hidden size of the LSTM was set to 150. The dimensions of the semantic vector were 100 and the dimensions of structure vector were 50. We used pretrained 300-D Glove 840B (Pennington et al., 2014) vectors to initialize the word embeddings. All parameters (including word embeddings) were updated with Adagrad (Duchi et al., 2011), and the Models Classifier with handcrafted features (Bowman et al., 2015) 300D LSTM encoders (Bowman et al., 2015) 300D Stack-Augmented Parser-Interpreter Neural Net (Bowman et al., 2016) 100D LSTM with inter-attention (Rockt¨aschel et al., 2016) 200D Matching LSTMs (Wang and Jiang, 2016) 450D LSTMN with deep attention fusion (Cheng et al., 2016) Decomposable Attention over word embeddings (Parikh et al., 2016) Enhanced BiLSTM Inference Model (Chen et al., 2017"
Q18-1005,prasad-etal-2008-penn,0,0.0240246,"Missing"
Q18-1005,D15-1167,0,0.690205,"rmance. It is therefore not surprising that there have been attempts to induce document representations directly from data without recourse to a discourse parser or additional annotations. The main idea is 63 Transactions of the Association for Computational Linguistics, vol. 6, pp. 63–75, 2018. Action Editor: Bo Pang. Submission batch: 5/2017; Published 1/2018. c 2018 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. to obtain hierarchical representations by first building representations of sentences, and then aggregating those into a document representation (Tang et al., 2015a,b). Yang et al. (2016) further demonstrate how to implicitly inject structural knowledge onto the representation using an attention mechanism (Bahdanau et al., 2015) which acknowledges that sentences are differentially important in different contexts. Their model learns to pay more or less attention to individual sentences when constructing the representation of the document. Our work focus on learning deeper structureaware document representations, drawing inspiration from recent efforts to empower neural networks with a structural bias (Cheng et al., 2016). Kim et al. (2017) introduce stru"
Q18-1005,P15-1098,0,0.518433,"rmance. It is therefore not surprising that there have been attempts to induce document representations directly from data without recourse to a discourse parser or additional annotations. The main idea is 63 Transactions of the Association for Computational Linguistics, vol. 6, pp. 63–75, 2018. Action Editor: Bo Pang. Submission batch: 5/2017; Published 1/2018. c 2018 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. to obtain hierarchical representations by first building representations of sentences, and then aggregating those into a document representation (Tang et al., 2015a,b). Yang et al. (2016) further demonstrate how to implicitly inject structural knowledge onto the representation using an attention mechanism (Bahdanau et al., 2015) which acknowledges that sentences are differentially important in different contexts. Their model learns to pay more or less attention to individual sentences when constructing the representation of the document. Our work focus on learning deeper structureaware document representations, drawing inspiration from recent efforts to empower neural networks with a structural bias (Cheng et al., 2016). Kim et al. (2017) introduce stru"
Q18-1005,W06-1639,0,0.0129875,"associated with user ratings ranging from 1 to 10. Czech reviews were obtained from Brychcın and Habernal (2013). The dataset contains reviews from the Czech Movie Database2 each labeled as positive, neutral, or negative. We include Czech in our experiments since it has more flexible word order compared to English, with non-projective dependency structures being more frequent. Experiments on this dataset perform 10-fold cross-validation following previous work (Brychcın and Habernal, 2013). 2 http://www.csfd.cz/ 70 Congressional floor debates were obtained from a corpus originally created by Thomas et al. (2006) which contains transcripts of U.S. floor debates in the House of Representatives for the year 2005. Each debate consists of a series of speech segments, each labeled by the vote (“yea” or “nay”) cast for the proposed bill by the the speaker of each segment. We used the pre-processed corpus from Yogatama and Smith (2014).3 Following previous work (Yang et al., 2016), we only retained words appearing more than five times in building the vocabulary and replaced words with lesser frequencies with a special UNK token. Word embeddings were initialized by training word2vec (Mikolov et al., 2013) on"
Q18-1005,N16-1170,0,0.0432669,"Missing"
Q18-1005,N16-1174,0,0.685993,"iate structures which are both interpretable and meaningful. 1 Introduction Document modeling is a fundamental task in Natural Language Processing useful to various downstream applications including topic labeling (Xie and Xing, 2013), summarization (Chen et al., 2016; Wolf and Gibson, 2006), sentiment analysis (Bhatia et al., 2015), question answering (Verberne et al., 2007), and machine translation (Meyer and Webber, 2013). Recent work provides strong evidence that better document representations can be obtained by incorporating structural knowledge (Bhatia et al., 2015; Ji and Smith, 2017; Yang et al., 2016). Inspired by existing theories of discourse, representations of document structure have assumed several guises in the literature, such as trees in the style of Rhetorical StrucLinguistically motivated representations of document structure rely on the availability of annotated corpora as well as a wider range of standard NLP tools (e.g., tokenizers, pos-taggers, syntactic parsers). Unfortunately, the reliance on labeled data, which is both difficult and highly expensive to produce, presents a major obstacle to the widespread use of discourse structure for document modeling. Moreover, despite r"
Q18-1005,P14-1074,0,0.0313941,"on (document-level) 100D Structured Attention (both levels) Yelp IMDB CZ Movies Debates θ 59.8 40.9 78.5 74.0 — 57.7 34.1 — —— 59.7 — — — — 63.7 42.5 — — — 65.1 45.3 — — — — — — 75.7 — 68.2 49.4 80.8 74.0 273K 66.7 47.5 80.5 73.7 330K 67.7 48.2 81.4 75.3 860K 68.0 48.8 81.5 74.6 842K 67.8 48.6 81.1 75.2 842K 68.6 49.2 82.1 76.5 860K Table 4: Test accuracy on four datasets and number of parameters θ (excluding embeddings). Regarding feature-based classification methods, results on Yelp and IMDB are taken from Tang et al. (2015a), on CZ movies from Brychcın and Habernal (2013), and Debates from Yogatama and Smith (2014). Wherever available we also provide the size of the recurrent unit (LSTM or GRU). 4.2 Document Classification In this section, we evaluate our document-level model on a variety of classification tasks. We selected four datasets which we describe below. Table 3 summarizes some statistics for each dataset. Yelp reviews were obtained from the 2013 Yelp Dataset Challenge. This dataset contains restaurant reviews, each associated with human ratings on a scale from 1 (negative) to 5 (positive) which we used as gold labels for sentiment classification; we followed the preprocessing introduced in Tan"
Q18-1005,J08-1001,1,\N,Missing
Q18-1029,D12-1108,0,0.113404,"Missing"
Q18-1029,E14-1035,0,0.0675092,"Missing"
Q18-1029,P15-1001,0,0.0995313,"Missing"
Q18-1029,P14-1062,0,0.0258353,"Missing"
Q18-1029,P17-1137,0,0.0519107,"Missing"
Q18-1029,W17-3204,0,0.0627173,"Missing"
Q18-1029,P17-1064,1,0.884515,"Missing"
Q18-1029,D16-1147,0,0.0753802,"Missing"
Q18-1029,W04-3225,0,0.187657,"Missing"
Q18-1029,P02-1040,0,0.101239,"Missing"
Q18-1029,P16-1159,1,0.900068,"Missing"
Q18-1029,W10-2602,0,0.214099,"Missing"
Q18-1029,N16-1036,0,0.0609243,"Missing"
Q18-1029,P16-1008,1,0.898208,"Missing"
Q18-1029,Q17-1007,1,0.91196,"Missing"
Q18-1029,P16-1125,0,0.0781792,"Missing"
Q18-1029,D17-1301,1,0.617798,"Missing"
Q18-1029,D17-1149,1,0.87385,"Missing"
Q18-1029,2011.mtsummit-papers.13,0,0.403824,"Missing"
Q18-1029,P12-1079,0,0.0962214,"Missing"
Q18-1029,P17-2092,1,0.892768,"Missing"
roark-etal-2006-sparseval,A00-2018,1,\N,Missing
roark-etal-2006-sparseval,J93-2004,0,\N,Missing
roark-etal-2006-sparseval,P97-1003,0,\N,Missing
roark-etal-2006-sparseval,N01-1016,1,\N,Missing
roark-etal-2006-sparseval,N04-4032,1,\N,Missing
roark-etal-2006-sparseval,J01-2004,1,\N,Missing
S15-2014,S12-1051,0,0.0288259,"enge. Particularly, SemEval focuses on semantic similarity of short texts as a lot of researches about long texts have been done in past years and the demand of finding new methods to measure short texts similarity has become stronger in many new applications. In this paper, we proposed a SVM-based solution to compute the semantic similarity between two sentences which is the goal of SemEval 2015 Task 2. Knowledge-based and corpus-based features were involved in our solution. We used the combination of the word similarity to estimate sentence similarity. And the training data of SemEval 2012 (Agirre et al., 2012) was used to train our model. In our experiments, WordNet-based and LSA-based features performed better than other features. Out of the 73 submitted runs, our two runs ranked 38th and 42th, with mean Pearson correlation 0.7114 and 0.6964 respectively. The evaluation results showed that our solution has good generalization ability on the test dataset of SemEval 2015 which is very different from our training set in terms of the sources of the sentences. Some of the relatively new technologies such as Word2Vec (Mikolov et al., 2013) and Sentence2Vec (Le and Mikolov, 2014) are potential methods to"
S15-2014,P06-4018,0,0.00628906,"al 2015), pages 80–84, c Denver, Colorado, June 4-5, 2015. 2015 Association for Computational Linguistics 3 Feature engineering ܬሺܵଵ ǡ ܵଶ ሻ ൌ  ȁܵଵ ܵ תଶ ȁ ȁܵଵ ܵ ଶ ȁ Considering the training set used in our system, we were trying to generate features which have little relation with the sources where the sentences came from. Four kinds of features are included in our model. They are literal similarity, shallow syntactic similarity, WordNet-based similarity and latent semantic similarity. Where ܵଵ and ܵଶ are the collections of Part-OfSpeech tags of each sentence. We used the NLTK toolkit (Bird, 2006) to tag each sentence. Since Jaccard distance measure only cares about the appearance of the tags, and ignores the order of them, it can reduce the impact of the tense change. 3.1 3.3 Literal Similarity Intuitively, a pair of sentences that look similar to each other may be similar semantically. For example: S1: A boy is playing a guitar. S2: A man is playing a guitar. S3: Someone is drawing. Apparently, S1 and S2 look more similar and they are closer in semantics than S1 and S3. We chose the Edit Distance (also known as Levenshtein Distance) over characters to measure the similarity between t"
S15-2014,C04-1100,0,0.0340728,"English subtask). The system uses a support vector machine model with literal similarity, shallow syntactic similarity, WordNet-based similarity and latent semantic similarity to predict the semantic similarity score of two short texts. In our experiments, WordNet-based and LSA-based features performed better than other features. Out of the 73 submitted runs, our two runs ranked 38th and 42th, with mean Pearson correlation 0.7114 and 0.6964 respectively. 1 Introduction Semantic Text Similarity (STS) plays an important role in many Natural language processing tasks, such as Question Answering (Narayanan and Harabagiu, 2004), Machine Translation (Beale et al., 1995), Automatic Summarization (Wang et al., 2008) and Word Sense Disambiguation (Navigli and Velardi, 2005). Since STS is an essential challenge in NLP, that has attracted a significant amount of attention by the research community. SemEval has held tasks about STS for four years in a row, from which we can see the importance and difficulty of this challenge. Particularly, SemEval focuses on semantic similarity of short texts as a lot of researches about long texts have been done in past years and the demand of finding new methods to measure short texts si"
S15-2014,niraula-etal-2014-dare,0,0.0455481,"Missing"
S15-2014,S12-1060,0,0.0427783,"Missing"
W04-3209,J96-1002,0,0.159614,"2 Maxent Posterior Probability Model As observed, HMM training does not maximize the posterior probabilities of the correct labels. This mismatch between training and use of the model as a classifier would not arise if the model directly estimated the posterior boundary label probabilities P (ei jW; F ). A second problem with HMMs is that the underlying N-gram sequence model does not cope well with multiple representations (features) of the word sequence (words, POS, etc.) short of building a joint model of all variables. This type of situation is well-suited to a maximum entropy formulation (Berger et al., 1996), which allows conditioning features to apply simultaneously, and therefore gives greater freedom in choosing representations. Another desirable characteristic of maxent models is that they do not split the data recursively to condition their probability estimates, which makes them more robust than decision trees when training data is limited. 4.2.1 Model Formulation and Training We built a posterior probability model for sentence boundary classification in the maxent framework. Such a model takes the familiar exponential form4 P (ejW; F ) = 1 Z (W; F ) e P k k gk (e;W;F ) (3) where Z (W; F"
W04-3209,A00-1031,0,0.0267287,"condition allows use of the correct word transcripts. This condition allows us to study the segmentation task without the confounding effect of speech recognition errors, using perfect lexical information. 3 Features and Knowledge Sources Words and sentence boundaries are mutually constrained via syntactic structure. Therefore, the word identities themselves (from automatic recognition or human transcripts) constitute a primary knowledge source for the sentence segmentation task. We also make use of various automatic taggers that map the word sequence to other representations. The TnT tagger (Brants, 2000) is used to obtain part-ofspeech (POS) tags. A TBL chunker trained on Wall Street Journal corpus (Ngai and Florian, 2001) maps each word to an associated chunk tag, encoding chunk type and relative word position (beginning of an NP, inside a VP, etc.). The tagged versions of the word stream are provided to allow generalizations based on syntactic structure and to smooth out possibly undertrained word-based probability esti1 This is the same as simple per-event classification accuracy, except that the denominator counts only the “marked” events, thereby yielding error rates that are much higher"
W04-3209,J92-4003,0,0.145144,"k type and relative word position (beginning of an NP, inside a VP, etc.). The tagged versions of the word stream are provided to allow generalizations based on syntactic structure and to smooth out possibly undertrained word-based probability esti1 This is the same as simple per-event classification accuracy, except that the denominator counts only the “marked” events, thereby yielding error rates that are much higher than if one uses all potential boundary locations. mates. For the same reasons we also generate word class labels that are automatically induced from bigram word distributions (Brown et al., 1992). To model the prosodic structure of sentence boundaries, we extract several hundred features around each word boundary. These are based on the acoustic alignments produced by a speech recognizer (or forced alignments of the true words when given). The features capture duration, pitch, and energy patterns associated with the word boundaries. Informative features include the pause duration at the boundary, the difference in pitch before and after the boundary, and so on. A crucial aspect of many of these features is that they are highly correlated (e.g., by being derived from the same raw measu"
W04-3209,W02-1002,0,0.0243102,"Missing"
W04-3209,A94-1013,0,0.242823,"s in the maximum entropy (maxent) framework. Both models combine lexical, syntactic, and prosodic information. We develop a technique for integrating pretrained probability models into the maxent framework, and show that this approach can improve on an HMM-based state-of-the-art system for the sentence-boundary detection task. An even more substantial improvement is obtained by combining the posterior probabilities of the two systems. 1 Introduction Sentence boundary detection is a problem that has received limited attention in the text-based computational linguistics community (Schmid, 2000; Palmer and Hearst, 1994; Reynar and Ratnaparkhi, 1997), but which has recently acquired renewed importance through an effort by the DARPA EARS program (DARPA Information Processing Technology Office, 2003) to improve automatic speech transcription technology. Since standard speech recognizers output an unstructured stream of words, improving transcription means not only that word accuracy must be improved, but also that commonly used structural features such as sentence boundaries need to be recognized. The task is thus fundamentally based on both acoustic and textual (via automatic word recognition) information. Fr"
W04-3209,A97-1004,0,0.274153,"(maxent) framework. Both models combine lexical, syntactic, and prosodic information. We develop a technique for integrating pretrained probability models into the maxent framework, and show that this approach can improve on an HMM-based state-of-the-art system for the sentence-boundary detection task. An even more substantial improvement is obtained by combining the posterior probabilities of the two systems. 1 Introduction Sentence boundary detection is a problem that has received limited attention in the text-based computational linguistics community (Schmid, 2000; Palmer and Hearst, 1994; Reynar and Ratnaparkhi, 1997), but which has recently acquired renewed importance through an effort by the DARPA EARS program (DARPA Information Processing Technology Office, 2003) to improve automatic speech transcription technology. Since standard speech recognizers output an unstructured stream of words, improving transcription means not only that word accuracy must be improved, but also that commonly used structural features such as sentence boundaries need to be recognized. The task is thus fundamentally based on both acoustic and textual (via automatic word recognition) information. From a computational linguistics"
W04-3209,N01-1006,0,\N,Missing
W06-3402,H92-1022,0,0.0597804,"ative of small talk: (1) position in the conversation, (2) the use of present-tense verbs, and (3) a lack of common helper words such as “it”, “there”, and forms of “to be”. To model the effect of proximity to the beginning of the conversation, we attach to each utterance a feature that describes its approximate position in the conversation. We do not include a feature for proximity to the end of the conversation because our transcriptions include only the first ten minutes of each recorded conversation. In order to include features describing verb tense, we use Brill’s part-of-speech tagger (Brill, 1992) . Each part of speech (POS) is taken to be a feature, whose value is a count of the number of occurrences in the given utterance. To account for the words, we use a bag of words model with counts for each word. We normalize words from the human transcripts by converting everything to lower case and tokenizing contractions Features n word tokens standard POS tags as in Penn Treebank line number in conversation utterance type utterance length (number of words) number of laughs n word tokens in previous 5 utterances tags from POS tagger, previous 5 number of words, previous 5 number of laughs, p"
W06-3402,J97-1003,0,0.286036,"ne hundred percent in favor of, uh, computers in the classroom. 2: I think they’re a marvelous tool, educational tool. Table 1: A conversation fragment with annotations: (S)mall Talk, (M)etaconversation, and On-(T)opic. The two speakers are identified as “1” and “2”. Third, off-topic detection can be viewed as a segmentation of conversation into relevant and irrelevant parts. Thus our work has many similarities to topic segmentation systems, which incorporate cue words that indicate an abrupt change in topic (e.g. “so anyway...”), as well as long term variations in word occurrence statistics (Hearst, 1997; Reynar, 1999; Beeferman et al., 1999, e.g.). Our approach uses previous and subsequent sentences to approximate these ideas, but might benefit from a more explicitly segmentation-based strategy. 4 Data In our work we use human-transcribed conversations from the Fisher data (LDC, 2004). In each conversation, participants have been given a topic to discuss for ten minutes. Despite this, participants often talk about subjects that are not at all related to the assigned topic. Therefore, a convenient way to define irrelevance in conversations in this domain is segments which do not contribute to"
W06-3402,P99-1046,0,0.0227152,"cent in favor of, uh, computers in the classroom. 2: I think they’re a marvelous tool, educational tool. Table 1: A conversation fragment with annotations: (S)mall Talk, (M)etaconversation, and On-(T)opic. The two speakers are identified as “1” and “2”. Third, off-topic detection can be viewed as a segmentation of conversation into relevant and irrelevant parts. Thus our work has many similarities to topic segmentation systems, which incorporate cue words that indicate an abrupt change in topic (e.g. “so anyway...”), as well as long term variations in word occurrence statistics (Hearst, 1997; Reynar, 1999; Beeferman et al., 1999, e.g.). Our approach uses previous and subsequent sentences to approximate these ideas, but might benefit from a more explicitly segmentation-based strategy. 4 Data In our work we use human-transcribed conversations from the Fisher data (LDC, 2004). In each conversation, participants have been given a topic to discuss for ten minutes. Despite this, participants often talk about subjects that are not at all related to the assigned topic. Therefore, a convenient way to define irrelevance in conversations in this domain is segments which do not contribute to understanding"
W08-0112,J96-2004,0,0.252946,"n that of Data Set (III), which used a direct sentence extraction scheme on the whole transcript. This suggests that even using the abstracts as a guidance, people still have a high variation in extracting summary sentences. We also calculated the pairwise Kappa score between annotations in different data sets. The inter-group Kappa score is much lower than those of the intragroup agreement, most likely due to the different annotation specifications used in the two different data sets. 3.2 0 400 600 800 1000 1200 1400 Topic length 200 400 600 800 1000 1200 1400 Topic length Kappa coefficient (Carletta, 1996) is commonly used as a standard to reflect inter-annotator agreement. Table 1 shows the average Kappa results, calculated for each meeting using the data sets described in Section 2. Compared to Kappa score on text summarization, which is reported to be 0.38 by (Mani et al., 2002) on a set of TREC documents, the inter-annotator agreement on meeting corpus is lower. This is likely due to the difference between the meeting style and written text. Data Set Avg-Kappa 200 -0.2 0 Impacting Factors We further analyze inter-annotator agreement with respect to two factors: topic length and meeting part"
W08-0112,W06-1643,0,0.0159421,"ores, we also proposed a sentence distance score and divergence distance as a quantitative measure. This study is expected to help better define the speech summarization problem. 1 Introduction With the fast development of recording and storage techniques in recent years, speech summarization has received more attention. A variety of approaches have been investigated for speech summarization, for example, maximum entropy, conditional random fields, latent semantic analysis, support vector machines, maximum marginal relevance (Maskey and Hirschberg, 2003; Hori et al., 2003; Buist et al., 2005; Galley, 2006; Murray et al., 2005; Zhang et al., 2007; Xie and Liu, 2008). These studies used different domains, such as broadcast news, lectures, and meetings. In these approaches, different information sources have been examined from both text and speech related features (e.g., prosody, speaker activity, turn-taking, discourse). How to evaluate speech summaries has also been studied recently, but so far there is no consensus on evaluation yet. Often the goal in evaluation is to develop an automatic metric to have a high correlation with human evaluation scores. Different methods have been used in the ab"
W08-0112,N03-1020,0,0.418284,"es, and meetings. In these approaches, different information sources have been examined from both text and speech related features (e.g., prosody, speaker activity, turn-taking, discourse). How to evaluate speech summaries has also been studied recently, but so far there is no consensus on evaluation yet. Often the goal in evaluation is to develop an automatic metric to have a high correlation with human evaluation scores. Different methods have been used in the above summarization research to compare system generated summaries with human annotation, such as Fmeasure, ROUGE, Pyramid, sumACCY (Lin and Hovy, 2003; Nenkova and Passonneau, 2004; Hori et al., 2003). Typically multiple reference human summaries are used in evaluation in order to account for the inconsistency among human annotations. While there have been efforts on speech summarization approaches and evaluation, some fundamental problems are still unclear. For example, what are speech summaries? Do humans agree with each other on summary extraction? In this paper, we focus on the meeting domain, one of the most challenging speech genre, to analyze human summary annotation. Meetings often have several participants. Its speech is spontaneou"
W08-0112,W05-0905,0,0.0669308,"proposed a sentence distance score and divergence distance as a quantitative measure. This study is expected to help better define the speech summarization problem. 1 Introduction With the fast development of recording and storage techniques in recent years, speech summarization has received more attention. A variety of approaches have been investigated for speech summarization, for example, maximum entropy, conditional random fields, latent semantic analysis, support vector machines, maximum marginal relevance (Maskey and Hirschberg, 2003; Hori et al., 2003; Buist et al., 2005; Galley, 2006; Murray et al., 2005; Zhang et al., 2007; Xie and Liu, 2008). These studies used different domains, such as broadcast news, lectures, and meetings. In these approaches, different information sources have been examined from both text and speech related features (e.g., prosody, speaker activity, turn-taking, discourse). How to evaluate speech summaries has also been studied recently, but so far there is no consensus on evaluation yet. Often the goal in evaluation is to develop an automatic metric to have a high correlation with human evaluation scores. Different methods have been used in the above summarization res"
W08-0112,N04-1019,0,0.031141,"these approaches, different information sources have been examined from both text and speech related features (e.g., prosody, speaker activity, turn-taking, discourse). How to evaluate speech summaries has also been studied recently, but so far there is no consensus on evaluation yet. Often the goal in evaluation is to develop an automatic metric to have a high correlation with human evaluation scores. Different methods have been used in the above summarization research to compare system generated summaries with human annotation, such as Fmeasure, ROUGE, Pyramid, sumACCY (Lin and Hovy, 2003; Nenkova and Passonneau, 2004; Hori et al., 2003). Typically multiple reference human summaries are used in evaluation in order to account for the inconsistency among human annotations. While there have been efforts on speech summarization approaches and evaluation, some fundamental problems are still unclear. For example, what are speech summaries? Do humans agree with each other on summary extraction? In this paper, we focus on the meeting domain, one of the most challenging speech genre, to analyze human summary annotation. Meetings often have several participants. Its speech is spontaneous, contains disfluencies, and"
W08-0112,W04-2319,0,0.0507011,"Missing"
W08-0626,W07-1001,0,0.0592699,"PT (s) − P PI (s)) &gt; 0 T D otherwise 116 BioNLP 2008: Current Trends in Biomedical Natural Language Processing, pages 116–117, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics where P PT (s) is the perplexity of the model MT over the sample s, and P PI (s) is the perplexity of the model MI over the same sample s. In other words, if the perplexity of the LM trained on syntactic patterns of children with SLI is smaller than that of the LM trained on POS patterns of TD children, then we will predict that the sample belongs to a child with SLI. In a related work, (Roark et al., 2007) explored the use of cross entropy of LMs trained on POS tags as a measure of syntactic complexity. Their results were inconsistent across language tasks, which may be due to the meaning attached to cross entropy in this setting. Unlikely patterns are a deviation from what is expected; they are not necessarily complex or syntactically rich. Table 1: Perplexity and final output of the LMs for the discrimination of SLI and TD. Sample T D1 T D2 T D3 T D4 T D5 T D6 T D7 T D8 T D9 SLI1 SLI2 average TD average SLI P PT (s) 14.73 11.37 18.35 30.23 9.42 17.37 20.32 16.40 24.35 20.21 19.70 18.06 19.95"
W08-0626,D08-1110,1,0.857465,"Missing"
W10-0722,D09-1030,0,0.166887,"Missing"
W10-0722,N09-1041,0,0.0289411,"the same summary the same score just over half the time. 2.2 Evaluation without source documents One way to dramatically speed up evaluation is to use the experts’ reference summaries as a gold standard, leaving the source documents out entirely. This is the idea behind automatic evaluation with ROUGE (Lin, 2004), which measures ngram overlap with the references, and assisted evaluation with Pyramid (Nenkova and Passonneau, 2004), which measures overlap of facts or “Semantic Content Units” with the references. The same idea has also been employed in various manual evaluations, for example by Haghighi and Vanderwende (2009), to directly compare the summaries of two different systems. The potential bias introduced by such abbreviated evaluation has not been explored. 3 HIT design The overall structure of the HIT we designed for summary evaluation is as follows: The worker is asked to read the topic and description, and then two reference summaries (there is no mention of the source documents). The candidate summary appears next, followed by instructions to provide scores between 1 (very poor) and 10 (very good) in each category1 . Mouse-over on the category names provides 1 Besides Overall Quality and Linguistic"
W10-0722,W04-1013,0,0.0372048,"nfortunately, the lack of redundant judgments means we cannot estimate interannotator agreement. However, we note that out of all 4576 submitted summaries, there are 226 pairs that are identical, which allows us to estimate annotator consistency. Table 2 shows that an expert annotator will give the same summary the same score just over half the time. 2.2 Evaluation without source documents One way to dramatically speed up evaluation is to use the experts’ reference summaries as a gold standard, leaving the source documents out entirely. This is the idea behind automatic evaluation with ROUGE (Lin, 2004), which measures ngram overlap with the references, and assisted evaluation with Pyramid (Nenkova and Passonneau, 2004), which measures overlap of facts or “Semantic Content Units” with the references. The same idea has also been employed in various manual evaluations, for example by Haghighi and Vanderwende (2009), to directly compare the summaries of two different systems. The potential bias introduced by such abbreviated evaluation has not been explored. 3 HIT design The overall structure of the HIT we designed for summary evaluation is as follows: The worker is asked to read the topic and"
W10-0722,N04-1019,0,0.0153056,"ever, we note that out of all 4576 submitted summaries, there are 226 pairs that are identical, which allows us to estimate annotator consistency. Table 2 shows that an expert annotator will give the same summary the same score just over half the time. 2.2 Evaluation without source documents One way to dramatically speed up evaluation is to use the experts’ reference summaries as a gold standard, leaving the source documents out entirely. This is the idea behind automatic evaluation with ROUGE (Lin, 2004), which measures ngram overlap with the references, and assisted evaluation with Pyramid (Nenkova and Passonneau, 2004), which measures overlap of facts or “Semantic Content Units” with the references. The same idea has also been employed in various manual evaluations, for example by Haghighi and Vanderwende (2009), to directly compare the summaries of two different systems. The potential bias introduced by such abbreviated evaluation has not been explored. 3 HIT design The overall structure of the HIT we designed for summary evaluation is as follows: The worker is asked to read the topic and description, and then two reference summaries (there is no mention of the source documents). The candidate summary appe"
W10-0722,D08-1027,0,0.0649343,"Missing"
W11-0709,W06-1643,0,0.0190831,"aluation tracks of the DUC (Document Understanding Conference) and TAC (Text Analysis Conference). To some extent, Twitter topic summarization is related to spoken document summarization, since both tasks deal with the conversational text that is contributed by multiple participants and contains lots of ill-formed sentences, colloquial expressions, nonstandard word tokens or high word error rate, etc. To summarize the spoken text, (Zechner, 2002) aimed to address problems related to disfluencies, extraction units, crossspeaker coherence, etc. (Maskey and Hirschberg, 2005; Murray et al., 2006; Galley, 2006; Xie et al., 2008; Liu and Liu, 2010a) incorporated lexical, structural, speaker, and discourse cues to generate textual summaries for broadcast news and meeting conversations. For microblog summarization, (Sharifi et al., 2010a) proposed a phrase reinforcement (PR) algorithm to summarize the Twitter topic in one sentence. The algorithm builds a word graph using the topic phrase as the root node; each word node is weighted in proportion to its distance to the root and the corresponding phrase frequency. The summary sentence is selected as one of the highest weighted paths in the graph. (Shari"
W11-0709,W04-1013,0,0.0534228,"Missing"
W11-0709,P11-2013,1,0.558985,"Missing"
W11-0709,N06-1047,0,0.0248835,"iven by the annual evaluation tracks of the DUC (Document Understanding Conference) and TAC (Text Analysis Conference). To some extent, Twitter topic summarization is related to spoken document summarization, since both tasks deal with the conversational text that is contributed by multiple participants and contains lots of ill-formed sentences, colloquial expressions, nonstandard word tokens or high word error rate, etc. To summarize the spoken text, (Zechner, 2002) aimed to address problems related to disfluencies, extraction units, crossspeaker coherence, etc. (Maskey and Hirschberg, 2005; Murray et al., 2006; Galley, 2006; Xie et al., 2008; Liu and Liu, 2010a) incorporated lexical, structural, speaker, and discourse cues to generate textual summaries for broadcast news and meeting conversations. For microblog summarization, (Sharifi et al., 2010a) proposed a phrase reinforcement (PR) algorithm to summarize the Twitter topic in one sentence. The algorithm builds a word graph using the topic phrase as the root node; each word node is weighted in proportion to its distance to the root and the corresponding phrase frequency. The summary sentence is selected as one of the highest weighted paths in the"
W11-0709,N10-1132,0,0.0120899,"remaining sentence collection, until no sentence can be removed. On average, the reference summary for general and hashtag topics contains 44 and 40 words respectively. 4 Summarization System For each of the topic phrases, our goal is to generate a short textual summary that can best convey the main ideas of the topic contents. We explore and compare multiple text sources as summarization input, including the user-contributed tweets, web contents linked from the tweets, as well as combination of the two sources. The concept-based optimization approach (Gillick et al., 2009; Xie et al., 2009; Murray et al., 2010) was employed for selecting informative summary sentences and minimizing the redundancy. Note that our focus of this paper is not developing new summarization systems, but rather utilizing and integrating different text sources for generating more informative Twitter topic summaries. 4.1 Concept-based Optimization Framework Concept-based summarization approach first extracts a set of important concepts for each topic, then selects a collection of sentences that can cover as many important concepts as possible, while within the specified length limit. This idea is realized using the integer lin"
W11-0709,A97-1004,0,0.0224761,"y in the 2 For each Twitter topic, we collect a set of web pages linked by the topic tweets and use them as another source of summarization input. For each topic, we select up to n (n = 10) URLs that appear most frequently in the topic tweets and infrequently across different Twitter topics. This scheme is similar to the TF-IDF measure. This way we can select the salient URLs for each topic while avoiding the spam URLs. The contents of these URLs were collected and only distinct web pages were retained. We use an HTML parser3 to extract the textual contents, and perform sentence segmentation (Reynar and Ratnaparkhi, 1997) on the parsed web pages. All the pages corresponding to the same topic were sorted by the date they were first cited in the tweets. These web pages were taken as another input text source for the summarization system, denoted as “Web”. 4.2.4 We expect that taking advantage of both tweets and linked web contents would benefit the topic summarization system. Consolidating the distinct text sources may help boost the weight of key concepts and eliminate the spam information. As a preliminary study, we investigate concatenating either the original tweets or the normalized tweets with the linked w"
W11-0709,N10-1100,0,0.643955,"onversational text that is contributed by multiple participants and contains lots of ill-formed sentences, colloquial expressions, nonstandard word tokens or high word error rate, etc. To summarize the spoken text, (Zechner, 2002) aimed to address problems related to disfluencies, extraction units, crossspeaker coherence, etc. (Maskey and Hirschberg, 2005; Murray et al., 2006; Galley, 2006; Xie et al., 2008; Liu and Liu, 2010a) incorporated lexical, structural, speaker, and discourse cues to generate textual summaries for broadcast news and meeting conversations. For microblog summarization, (Sharifi et al., 2010a) proposed a phrase reinforcement (PR) algorithm to summarize the Twitter topic in one sentence. The algorithm builds a word graph using the topic phrase as the root node; each word node is weighted in proportion to its distance to the root and the corresponding phrase frequency. The summary sentence is selected as one of the highest weighted paths in the graph. (Sharifi et al., 2010b; Inouye, 2010) introduced a hybrid TF-IDF approach to extract one- or multiple-sentence summary for each topic. Sentences were ranked according to the average TF-IDF score of the consisting words; top weighted s"
W11-0709,J02-4003,0,0.0236621,"e is not much previous work on summarizing the Twitter topics. Most previous summarization literature focused on the written text domain, as driven by the annual evaluation tracks of the DUC (Document Understanding Conference) and TAC (Text Analysis Conference). To some extent, Twitter topic summarization is related to spoken document summarization, since both tasks deal with the conversational text that is contributed by multiple participants and contains lots of ill-formed sentences, colloquial expressions, nonstandard word tokens or high word error rate, etc. To summarize the spoken text, (Zechner, 2002) aimed to address problems related to disfluencies, extraction units, crossspeaker coherence, etc. (Maskey and Hirschberg, 2005; Murray et al., 2006; Galley, 2006; Xie et al., 2008; Liu and Liu, 2010a) incorporated lexical, structural, speaker, and discourse cues to generate textual summaries for broadcast news and meeting conversations. For microblog summarization, (Sharifi et al., 2010a) proposed a phrase reinforcement (PR) algorithm to summarize the Twitter topic in one sentence. The algorithm builds a word graph using the topic phrase as the root node; each word node is weighted in proport"
W11-1411,W10-2107,0,0.128306,"nt The University of Texas at Dallas Richardson, TX, USA yangl@hlt.utdallas.edu Khairun-nisa Hassanali Computer Science Department The University of Texas at Dallas Richardson, TX, USA nisa@hlt.utdallas.edu Abstract ungrammatical sentences. Since most parsers are trained on written data consisting mostly of grammatical sentences, the parsers face issues when parsing ungrammatical sentences. Automatic detection and correction of these ungrammatical sentences would improve the parser’s performance by detecting the ungrammatical sentences and performing a second parse on the corrected sentences (Caines and Buttery, 2010). From an education perspective, measuring language skills has been extensively explored. There are systems in place that automatically detect and correct errors for second language learners (Eeg-Olofsson and Knuttson, 2003; Leacock et al., 2010). Language sample analysis is an important technique used in measuring language development. At present, measures of grammatical complexity such as the Index of Productive Syntax (Scarborough, 1990) are used to measure language development in early childhood. Although these measures depict the overall competence in the usage of language, they do not pr"
W11-1411,A00-2018,0,0.278619,"Missing"
W11-1411,P08-1021,0,0.0729856,"Missing"
W11-1411,W07-0604,0,0.0453899,"Missing"
W11-1721,P09-1079,0,0.0197829,"sion and low recall, or low precision and high recall (Kim and Hovy, 2005). Recently, machine learning approaches have been adopted more often (Ng et al., 2006). There are limitations in both methods. In knowledge-based approaches, a predefined subjectivity lexicon may not adapt well to different domains. While in machine learning approach, human labeling efforts are required to create a large training set. To overcome the above drawbacks, unsupervised or semi-supervised methods have been explored in sentiment analysis. For polarity classification, some previous work used spectral techniques (Dasgupta and Ng, 2009) or co-training (Li et al., 2010) to mine the reviews in a semi-supervised manner. For subjectivity identification, Wiebe and Riloff (Wiebe and Riloff, 2005) applied a rule-based method to create a training set first and then used it to train a naive Bayes classifier. Melville et al. (Melville et al., 2009) used a pooling multinomial method to combine lexicon derived probability and statistical probability. Our work is similar to the study in (Wiebe and Riloff, 2005) in that we both use a rule-based method to create an initial training set and learn from Proceedings of the 2nd Workshop on Comp"
W11-1721,I05-2011,0,0.0237383,"mains: movie, news article, and meeting dialog. This can be explained by the inherent difference of the data, especially the task difficulty and classifier’s performance for a domain. We demonstrate that for some domains (e.g., movie data) the unsupervised learning methods can rival the supervised approach. 2 Related Work In the early age, knowledge-based methods were widely used for subjectivity detection. They used a lexicon or patterns and rules to predict whether a target is subjective or not. These methods tended to yield a high precision and low recall, or low precision and high recall (Kim and Hovy, 2005). Recently, machine learning approaches have been adopted more often (Ng et al., 2006). There are limitations in both methods. In knowledge-based approaches, a predefined subjectivity lexicon may not adapt well to different domains. While in machine learning approach, human labeling efforts are required to create a large training set. To overcome the above drawbacks, unsupervised or semi-supervised methods have been explored in sentiment analysis. For polarity classification, some previous work used spectral techniques (Dasgupta and Ng, 2009) or co-training (Li et al., 2010) to mine the review"
W11-1721,P10-1043,0,0.0136699,"nd high recall (Kim and Hovy, 2005). Recently, machine learning approaches have been adopted more often (Ng et al., 2006). There are limitations in both methods. In knowledge-based approaches, a predefined subjectivity lexicon may not adapt well to different domains. While in machine learning approach, human labeling efforts are required to create a large training set. To overcome the above drawbacks, unsupervised or semi-supervised methods have been explored in sentiment analysis. For polarity classification, some previous work used spectral techniques (Dasgupta and Ng, 2009) or co-training (Li et al., 2010) to mine the reviews in a semi-supervised manner. For subjectivity identification, Wiebe and Riloff (Wiebe and Riloff, 2005) applied a rule-based method to create a training set first and then used it to train a naive Bayes classifier. Melville et al. (Melville et al., 2009) used a pooling multinomial method to combine lexicon derived probability and statistical probability. Our work is similar to the study in (Wiebe and Riloff, 2005) in that we both use a rule-based method to create an initial training set and learn from Proceedings of the 2nd Workshop on Computational Approaches to Subjectiv"
W11-1721,D08-1081,0,0.0221955,"s the size and self-labeling accuracy of the initial training set. Our experiments and analysis show inherent differences across domains and performance gain from calibration in EM. 1 Introduction Subjectivity identification is to identify whether an expression contains opinion or sentiment. Automatic subjectivity identification can benefit many natural language processing (NLP) tasks. For example, information retrieval systems can provide affective or informative articles separately (Pang and Lee, 2008). Summarization systems may want to summarize factual and opinionated content differently (Murray and Carenini, 2008). In this paper, we perform subjectivity detection at sentence level, which is more appropriate for some subsequent processing such as opinion summarization. Previous work has shown that when enough labeled data is available, supervised classification methods can achieve high accuracy for subjectivity detection in some domains. However, it is often expensive to create such training data. On the other hand, a lot of unannotated data is readily available in various domains. Therefore an interesting and important problem is to develop semi-supervised or unsupervised learning methods that can lear"
W11-1721,P06-2079,0,0.0250882,"ference of the data, especially the task difficulty and classifier’s performance for a domain. We demonstrate that for some domains (e.g., movie data) the unsupervised learning methods can rival the supervised approach. 2 Related Work In the early age, knowledge-based methods were widely used for subjectivity detection. They used a lexicon or patterns and rules to predict whether a target is subjective or not. These methods tended to yield a high precision and low recall, or low precision and high recall (Kim and Hovy, 2005). Recently, machine learning approaches have been adopted more often (Ng et al., 2006). There are limitations in both methods. In knowledge-based approaches, a predefined subjectivity lexicon may not adapt well to different domains. While in machine learning approach, human labeling efforts are required to create a large training set. To overcome the above drawbacks, unsupervised or semi-supervised methods have been explored in sentiment analysis. For polarity classification, some previous work used spectral techniques (Dasgupta and Ng, 2009) or co-training (Li et al., 2010) to mine the reviews in a semi-supervised manner. For subjectivity identification, Wiebe and Riloff (Wieb"
W11-1721,P04-1035,0,0.0230345,"ing method they used, we use a calibrated EM iterative learning approach. Second, we compare the results on three different corpora in order to evaluate the domain/genre effect of the unsupervised method. Our crosscorpus study shows how the unsupervised learning approach performs in different domains and helps us understand what are the factors impacting the learning methods. 3 Data We use three data sets from different domains: movie, news resource, and meeting conversations. The first two are from written text domain and have been widely used in many previous studies for sentiment analysis (Pang and Lee, 2004; Raaijmakers and Kraaij, 2008). The third one is from speech transcripts. It has been used in a few recent studies (Raaijmakers et al., 2008; Murray and Carenini, 2009), but not as much as those text data. The following provides more details of the data. Table 1 summarizes statistics for the three data sets. We can see that sentences in meeting dialogs (AMI data) are generally shorter than the other domains, and that sentences in news domain (MPQA) are longer, and also have a larger variance. In addition, the inter-annotator agreement on AMI data is quite low, which shows it is even difficult"
W11-1721,C08-2019,0,0.0166234,"g dialogues. We also perform a thorough analysis to examine impacting factors on unsupervised learning, such as the size and self-labeling accuracy of the initial training set. Our experiments and analysis show inherent differences across domains and performance gain from calibration in EM. 1 Introduction Subjectivity identification is to identify whether an expression contains opinion or sentiment. Automatic subjectivity identification can benefit many natural language processing (NLP) tasks. For example, information retrieval systems can provide affective or informative articles separately (Pang and Lee, 2008). Summarization systems may want to summarize factual and opinionated content differently (Murray and Carenini, 2008). In this paper, we perform subjectivity detection at sentence level, which is more appropriate for some subsequent processing such as opinion summarization. Previous work has shown that when enough labeled data is available, supervised classification methods can achieve high accuracy for subjectivity detection in some domains. However, it is often expensive to create such training data. On the other hand, a lot of unannotated data is readily available in various domains. Theref"
W11-1721,D08-1049,0,0.0146898,"order to evaluate the domain/genre effect of the unsupervised method. Our crosscorpus study shows how the unsupervised learning approach performs in different domains and helps us understand what are the factors impacting the learning methods. 3 Data We use three data sets from different domains: movie, news resource, and meeting conversations. The first two are from written text domain and have been widely used in many previous studies for sentiment analysis (Pang and Lee, 2004; Raaijmakers and Kraaij, 2008). The third one is from speech transcripts. It has been used in a few recent studies (Raaijmakers et al., 2008; Murray and Carenini, 2009), but not as much as those text data. The following provides more details of the data. Table 1 summarizes statistics for the three data sets. We can see that sentences in meeting dialogs (AMI data) are generally shorter than the other domains, and that sentences in news domain (MPQA) are longer, and also have a larger variance. In addition, the inter-annotator agreement on AMI data is quite low, which shows it is even difficult for human to determine whether an utterance contains sentiment in meeting conversations. • The first corpus is movie data (Pang and Lee, 200"
W11-1721,W03-1014,0,0.0605892,"which shows it is even difficult for human to determine whether an utterance contains sentiment in meeting conversations. • The first corpus is movie data (Pang and Lee, 2004). It contains 5,000 subjective sentences collected from movie reviews and 5,000 objective sentences collected from movie plot summaries. The sentences in each collection are randomly ordered. • The second one is extracted from MPQA corpus (version 2.0) (Wilson and Wiebe, 2003), which is collected from news articles. This data has been annotated with subjective information at phrase level. We adopted the same rules as in (Riloff and Wiebe, 2003) to create the sentence level label: if a sentence has at least one private state of strength medium or higher, then the sentence is labeled SUBJECTIVE, otherwise it is labeled OBJECTIVE. We randomly extracted 5,000 subjective and 5,000 objective sentences from this corpus to make it comparable with the movie data. • The third data set is from AMI meeting corpus. It has been annotated using the scheme described in (Wilson, 2008). There are 3 main categories of annotations regarding sentiments: subjective utterances, subjective questions, and objective polar utterances. We consider the 162 min"
W11-1721,W03-0417,0,0.027443,"si ) is the count of word wt in a sentence si . We use additive smoothing with α = 0.1 for probability parameter estimation. |C |is the number of classes, which is 2 in our case, and |V |is the vocabulary size, obtained from the entire data set. In the first iteration, we assign P (cj |si ) using the pseudo training data generated based on lexicon information. If a sentence is labeled SUBJECTIVE, then P (sub|si ) is 1 and P (obj|si ) is 0; for the sentences with OBJECTIVE labels, P (sub|si ) is 0 and P (obj|si ) is 1. In our work, we use a variant of standard EM: calibrated EM, introduced by (Tsuruoka and Tsujii, 2003). The basic idea of this approach is to shift the probability values of unlabeled data to the extent such that the class distribution of unlabeled data is identical to the distribution in labeled data (balanced class in our case). In our approach, before model training (“M-step”) in each iteration, we adjust the posterior probability of each sentence in the following steps: • Transform the posterior probabilities through the inverse function of the sigmoid function. The outputs are real values. • Sort them and use the median of all the values as the border value. This is because our data is ba"
W11-1721,W03-2102,0,0.0160864,"other domains, and that sentences in news domain (MPQA) are longer, and also have a larger variance. In addition, the inter-annotator agreement on AMI data is quite low, which shows it is even difficult for human to determine whether an utterance contains sentiment in meeting conversations. • The first corpus is movie data (Pang and Lee, 2004). It contains 5,000 subjective sentences collected from movie reviews and 5,000 objective sentences collected from movie plot summaries. The sentences in each collection are randomly ordered. • The second one is extracted from MPQA corpus (version 2.0) (Wilson and Wiebe, 2003), which is collected from news articles. This data has been annotated with subjective information at phrase level. We adopted the same rules as in (Riloff and Wiebe, 2003) to create the sentence level label: if a sentence has at least one private state of strength medium or higher, then the sentence is labeled SUBJECTIVE, otherwise it is labeled OBJECTIVE. We randomly extracted 5,000 subjective and 5,000 objective sentences from this corpus to make it comparable with the movie data. • The third data set is from AMI meeting corpus. It has been annotated using the scheme described in (Wilson, 20"
W11-1721,H05-1044,0,0.00783923,"ated by human. 4 Unsupervised Subjectivity Detection In this section, we describe our unsupervised learning process that uses a knowledge-based method to create an initial training set, and then uses a calibrated EM approach to incorporate unannotated data into the learning process. We use a naive Bayes classifier as the base supervised classifier with a bag-ofwords model. 4.1 Create Initial Training Set A lexicon-based method is used to create an initial training set, since it can often achieve high precision rate (though low recall) for subjectivity detection. We use a subjectivity lexicon (Wilson et al., 2005) to calculate the subjectivity score for each sentence. This lexicon contains 8,221 entries that are categorized into strong and weak subjective clues. For each word w, we assign a subjectivity score sub(w): 1 to strong subjective clues, 0.5 to weak clues, and 0 for any other word. Then the subjectivity score of a sentence is the sum of the values of all the words in the sentence, normalized by the sentence length. We noticed that for sentences labeled as SUBJECTIVE in the three corpora, the subjective clues appear more frequently in movie data than the other two corpora. Thus we perform diffe"
W11-1721,wilson-2008-annotating,0,0.0206935,"ebe, 2003), which is collected from news articles. This data has been annotated with subjective information at phrase level. We adopted the same rules as in (Riloff and Wiebe, 2003) to create the sentence level label: if a sentence has at least one private state of strength medium or higher, then the sentence is labeled SUBJECTIVE, otherwise it is labeled OBJECTIVE. We randomly extracted 5,000 subjective and 5,000 objective sentences from this corpus to make it comparable with the movie data. • The third data set is from AMI meeting corpus. It has been annotated using the scheme described in (Wilson, 2008). There are 3 main categories of annotations regarding sentiments: subjective utterances, subjective questions, and objective polar utterances. We consider the 162 min sent length max mean variance vocabulary size Inter-annotator agreement Movie 3 100 20.37 75.26 15,847 N/A MPQA 1 246 22.38 147.18 13,414 0.77 AMI 3 67 8.78 34.26 3,337 0.56 Table 1: Statistics for the three data sets: movie, MPQA, and AMI data. The inter-annotator agreement on movie data is not available because it is not annotated by human. 4 Unsupervised Subjectivity Detection In this section, we describe our unsupervised lea"
W11-1911,P05-1022,0,0.0603318,"Missing"
W11-1911,P08-1067,0,0.0477932,"Missing"
W11-1911,P04-1018,0,0.370236,"andidates among all constituents from both given parse tree and packed forest. The packed forest is a compact representation of all parse trees for a given sentence. Readers can refer to (Mi et al., 2008) for detailed definitions. Once the mentions are identified, the left step is to group mentions referring to same object into similar entity. This problem can be viewed as binary classification problem of determining whether each mention pairs corefer. We use a Maximum Entropy classifier to predict the possibility that two mentions refer to the similar entity. And mainly following the work of Luo et al. (2004), we use a beam search algorithm based on Bell Tree to obtain the global optimal classification. As this is the first time we participate competition of coreference resolution, we mainly concentrate on developing fault tolerant capability of our system while omitting feature engineering and other helpful technologies. 2 Mention Detection The first step of the coreference resolution tries to recognize occurrences of mentions in documents. Note that we recognize mention boundaries only on development and test set while generating training 76 Proceedings of the 15th Conference on Computational Na"
W11-1911,P08-1023,1,0.901686,"Missing"
W11-1911,P10-1142,0,0.0187597,"we use the L-BFGS parameter estimation algorithm with gaussian prior smoothing (Chen and Rosenfeld, 1999). We set the gaussian prior to 2 and train the model in 100 iterations. • [A][B][C][D], [A][B][CD] 3.1 Creation of Entities This stage aims to create the mentions detected in the first stage into entities, according to the prediction of classifier. One simple method is to use a greedy algorithm, by comparing each mention to its previous mentions and refer to the one that has the highest probability. In principle, this algorithm is too greedy and sometimes results in unreasonable partition (Ng, 2010). To address this problem, we follow the literature (Luo et al., 2004) and propose to use beam search to find global optimal partition. Intuitively, creation of entities can be casted as partition problem. And the number of partitions equals the Bell Number (Bell, which has a ∑ 1934), kn “closed” formula B(n) = 1e ∞ . k=0 k! Clearly, this number is very huge when n is large, enumeration of all partitions is impossible, so we instead designing a beam search algorithm to find the best partition. Formally, the task is to optimize the following objective, yˆ = arg max ϕ∈P ∑ P rob(e) (1) e∈ϕ ∑ pos("
W11-1911,W11-1901,0,0.0939802,"Missing"
W11-1911,J01-4004,0,0.0947592,"ence, intuitively, we 77 3 Determining Coreference This stage is to determine which mentions belong to the same entity. We train a Maximum Entropy classifier (Le, 2004) to decide whether two mentions are coreferent. We use the method proposed by Soon, et al.’s to generate the training instances, where a positive instance is formed between current mention Mj and its closest preceding antecedent Mi , and a negative instance is created by paring Mj with each of the intervening mentions, Mi+1 , Mi+2 ,...,Mj−1 . We use the following features to train our classifier. Features in Soon et al.’s work (Soon et al., 2001) Lexical features IS PREFIX: whether the string of one mention is prefix of the other; IS SUFFIX: whether the string of one mention is suffix of the other; ACRONYM: whether one mention is the acronym of the other; Distance features SENT DIST: distance between the sentences containing the two mentions; MEN DIST: number of mentions between two mentions; Grammatical features IJ PRONOUN: whether both mentions are pronoun; I NESTED: whether mention i is nested in another mention; J NESTED: whether mention j is nested in another mention; Syntax features HEAD: whether the heads of two mentions have t"
W11-1911,D08-1022,0,\N,Missing
W13-1720,C12-1027,0,0.0600561,"function words, which belong to stop words, such as ‘the’, ‘at’, ‘which’, have been proven to be effective to distinguish native language for writers (Koppel et al., 2005). There are two settings: either punctuation marks are removed or kept. When punctuation marks are kept, they are viewed the same as word in constructing n-grams. For example, in the sentence “NLI is fun.”, “fun .” is viewed as a bigram. 4.2 Features In our system, word level n-grams are used to represent an essay. Previous studies have shown that word level n-grams are useful in determining the native language of a writer (Bykh and Meurers, 2012). One reasonable hypothesis is that nonnative English writers with the same native languages tend to choose more similar words to express the same or similar concepts. In addition, the combination of a sequence of words might also be affected by the different native language of writers. Therefore, word n-gram is useful to distinguish the native language of a writer. Even though some previous studies have looked into using word level n-grams as features, how to use word level ngrams has not been explored too much yet on TOEFL11 corpus. To our knowledge, the most recent study by Blanchard et al."
W13-1720,W07-0602,0,0.0242527,"Missing"
W13-1720,U09-1008,0,0.0372193,"Missing"
W13-1720,D11-1148,0,0.0400838,"Missing"
W13-1911,A00-2018,0,0.0986201,"Missing"
W13-1911,P11-1073,0,0.0304325,"cal pattern. Our approach to compute these new metrics does not require any special treatment on the transcripts or special purpose parsers beyond a POS tagger. On the contrary, we provide a set of measures that in addition to being easy to interpret by practitioners, are also easy to compute. 2 Background and Motivation To establish language proficiency, clinical researchers and practitioners rely on a variety of measures, such as number of different words, type-token ratio, distribution of part-of-speech tags, and mean length of sentences and words per minute (Lu, 2012; Yoon and Bhat, 2012; Chen and Zechner, 2011; Yang, 2011; Miller et al., 2006), to name a few. Most of these metrics can be categorized as low-level metrics since they only consider rates of different characteristics at the lexical level. These measures are helpful in the solution of several problems, for example, building automatic scoring models to evaluate non-native speech (Chen and Zechner, 2011). They can also be used as predictors of the rate of growth of English acquisition in specific populations, for instance, in typically developing (TD) and language impaired (LI) bilingual children (Rojas and Iglesias, 2012; Guti´errez-Clell"
W13-1911,N09-1006,1,0.850652,"Missing"
W13-1911,P05-1025,0,0.0244286,"les Iglesias Temple University Philadelphia, PA 19140, USA iglesias@temple.edu ˜ Lisa Bedore and Elizabeth Pena The University of Texas at Austin Austin, TX 78712, USA lbedore,lizp@mail.utexas.edu Abstract propose a set of linguistic measures for age prediction in children that combines three traditional measures from language assessment with a set of five data-driven measures from language samples of 7 children. A common theme in this emerging line of research is the study of the syntax in those language samples. For instance, to annotate data to be used in the study of language development (Sagae et al., 2005), or to build models to map utterances to their meaning, similar to what children do during the language acquisition stage (Kwiatkowski et al., 2012). In addition, language samples are also used for neurological assessment, as for example in (Roark et al., 2007; Roark et al., 2011) where they explored features such as Yngve and Frazier scores, together with features derived from automated parse trees to model syntactic complexity and surprisal. Similar features are used in the classification of language samples to discriminate between children developing typically and children suffering from a"
W13-1911,P12-2019,0,0.0981442,"t in children. 1 Introduction The analysis of spontaneous language samples is an important task across a variety of fields. For instance, in language assessment this task can help to extract information regarding language proficiency (e.g. is the child typically developing or language impaired). In second language acquisition, language samples can help determine if a child’s proficiency is similar to that of native speakers. In recent years, we have started seeing a growing interest in the exploration of NLP techniques for the analysis of language samples in the clinical setting. For example, Sahakian and Snyder (2012) 89 Proceedings of the 2013 Workshop on Biomedical Natural Language Processing (BioNLP 2013), pages 89–97, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics measures have demonstrated to be valuable in the assessment of language ability considering that practitioners often only need to focus on productivity, diversity of vocabulary, and sentence organization. Although useful, these metrics only provide superficial measures of the children’s language skills that fail to capture detailed lexicosyntactic information. For example, in addition to knowing that a chil"
W13-1911,E12-1024,0,0.0267808,"in Austin, TX 78712, USA lbedore,lizp@mail.utexas.edu Abstract propose a set of linguistic measures for age prediction in children that combines three traditional measures from language assessment with a set of five data-driven measures from language samples of 7 children. A common theme in this emerging line of research is the study of the syntax in those language samples. For instance, to annotate data to be used in the study of language development (Sagae et al., 2005), or to build models to map utterances to their meaning, similar to what children do during the language acquisition stage (Kwiatkowski et al., 2012). In addition, language samples are also used for neurological assessment, as for example in (Roark et al., 2007; Roark et al., 2011) where they explored features such as Yngve and Frazier scores, together with features derived from automated parse trees to model syntactic complexity and surprisal. Similar features are used in the classification of language samples to discriminate between children developing typically and children suffering from autism or language impairment (Prud’hommeaux et al., 2011). In a similar line of research, machine learning and features inspired by NLP have been exp"
W13-1911,W11-0604,0,0.027917,"h to compute these new metrics does not require any special treatment on the transcripts or special purpose parsers beyond a POS tagger. On the contrary, we provide a set of measures that in addition to being easy to interpret by practitioners, are also easy to compute. 2 Background and Motivation To establish language proficiency, clinical researchers and practitioners rely on a variety of measures, such as number of different words, type-token ratio, distribution of part-of-speech tags, and mean length of sentences and words per minute (Lu, 2012; Yoon and Bhat, 2012; Chen and Zechner, 2011; Yang, 2011; Miller et al., 2006), to name a few. Most of these metrics can be categorized as low-level metrics since they only consider rates of different characteristics at the lexical level. These measures are helpful in the solution of several problems, for example, building automatic scoring models to evaluate non-native speech (Chen and Zechner, 2011). They can also be used as predictors of the rate of growth of English acquisition in specific populations, for instance, in typically developing (TD) and language impaired (LI) bilingual children (Rojas and Iglesias, 2012; Guti´errez-Clellen et al., 2"
W13-1911,D12-1055,0,0.0206273,"one for that grammatical pattern. Our approach to compute these new metrics does not require any special treatment on the transcripts or special purpose parsers beyond a POS tagger. On the contrary, we provide a set of measures that in addition to being easy to interpret by practitioners, are also easy to compute. 2 Background and Motivation To establish language proficiency, clinical researchers and practitioners rely on a variety of measures, such as number of different words, type-token ratio, distribution of part-of-speech tags, and mean length of sentences and words per minute (Lu, 2012; Yoon and Bhat, 2012; Chen and Zechner, 2011; Yang, 2011; Miller et al., 2006), to name a few. Most of these metrics can be categorized as low-level metrics since they only consider rates of different characteristics at the lexical level. These measures are helpful in the solution of several problems, for example, building automatic scoring models to evaluate non-native speech (Chen and Zechner, 2011). They can also be used as predictors of the rate of growth of English acquisition in specific populations, for instance, in typically developing (TD) and language impaired (LI) bilingual children (Rojas and Iglesias"
W13-1911,W11-0610,0,0.0370794,"Missing"
W13-1911,W07-1001,0,\N,Missing
W13-1914,P12-2019,0,0.025245,"development. These include speech fluency, syntax, semantics, and coherence. For such analysis, spontaneous narratives have been widely used. Narrating a story or a personal experience requires the narrator to build a mental model of the story and use the knowledge of semantics and syntax to produce a coherent narrative. Children learn from a very early age to narrate stories. The different processes involved in generating a narrative have been shown to provide insights into the language status of children. There has been some prior work on child language sample analysis using NLP techniques. Sahakian and Snyder (2012) used a set of linguistic features computed on child speech samples to create language metrics that included age prediction. Gabani et al. (2011) combined commonly used measurements in communication disorders with LDA has been used in the field of narrative analysis. Wallace et al. (2012) adapted LDA to the task of multiple narrative disentanglement, in which the aim was to tease apart narratives by assigning passages from a text to the subnarratives that they belong to. They achieved strong empirical results. In this paper, we explore the use of LDA for child narrative analysis. We aim to ans"
W13-1914,N12-1001,0,0.0610602,"Missing"
W14-2403,basile-etal-2012-developing,0,0.0326692,"re the construction of the meaning representation directly follows the syntactic analysis (Steedman, 2001). However, the supervised induction of semantic CCGs—the inference of a CCG from a corpus of sentence-meaning pairs—has so far only been partially solved. While approaches are available that work on small corpora focused on specific domains (such as Geoquery and Freebase QA for question answering (Zelle and Mooney, 1996; Cai and Yates, 2013)), we are not aware of any approach that allows the extraction of a semantic CCG from a wide-coverage corpus such as the Groningen Meaning Bank (GMB) (Basile et al., 2012). This work attempts to fill this gap. Analogous to the work of Kwiatkowski et al. (2010), we view grammar induction as a series of splitting steps, each of which essentially reverses 12 Proceedings of the ACL 2014 Workshop on Semantic Parsing, pages 12–16, c Baltimore, Maryland USA, June 26 2014. 2014 Association for Computational Linguistics fore relate to the implementation of the SEMSPLIT function. ∃x Algorithm 1 A general splitting algorithm. C is the set of binary CCG combinators. The SEM SPLIT function returns possible splits of a meaning representation according to the reverse applicat"
W14-2403,P13-1042,0,0.0227007,"ammar (CCG) forms the basis of many current approaches to semantic parsing. It is attractive for semantic parsing due to its unified treatment of syntax and semantics, where the construction of the meaning representation directly follows the syntactic analysis (Steedman, 2001). However, the supervised induction of semantic CCGs—the inference of a CCG from a corpus of sentence-meaning pairs—has so far only been partially solved. While approaches are available that work on small corpora focused on specific domains (such as Geoquery and Freebase QA for question answering (Zelle and Mooney, 1996; Cai and Yates, 2013)), we are not aware of any approach that allows the extraction of a semantic CCG from a wide-coverage corpus such as the Groningen Meaning Bank (GMB) (Basile et al., 2012). This work attempts to fill this gap. Analogous to the work of Kwiatkowski et al. (2010), we view grammar induction as a series of splitting steps, each of which essentially reverses 12 Proceedings of the ACL 2014 Workshop on Semantic Parsing, pages 12–16, c Baltimore, Maryland USA, June 26 2014. 2014 Association for Computational Linguistics fore relate to the implementation of the SEMSPLIT function. ∃x Algorithm 1 A genera"
W14-2403,N04-1035,0,0.0606864,"rictive, as Figure 2 illustrates. Instead, we say that we decompose z into a hierarchy of components, with a split node at the root of each component. These components are labelled as f - and g-components in an alternating fashion. In this hierarchy, the members of an f component are not allowed to have alignments to words in l. A corresponding requirement holds for The first heuristic we introduce is borrowed from the field of statistical machine translation. There, alignments between words of two languages are used to identify corresponding phrase pairs, as in the well-known GHKM algorithm (Galley et al., 2004). In order to apply the same strategy to meaning representations, we represent them as their abstract syntax trees. Following Li et al. (2013), we can then align words in the sentence and nodes in the meaning representation to identify components that correspond to each other. This allows us to impose an extra constraint on the generation of splits: We require that nodes in f not be aligned to any words in the right sentencehalf r, and conversely, that nodes in g not be aligned to words in l. Alignment consistency helps the search to focus on more plausible splits by grouping elements of the m"
W14-2403,D10-1119,0,0.0329042,"lysis (Steedman, 2001). However, the supervised induction of semantic CCGs—the inference of a CCG from a corpus of sentence-meaning pairs—has so far only been partially solved. While approaches are available that work on small corpora focused on specific domains (such as Geoquery and Freebase QA for question answering (Zelle and Mooney, 1996; Cai and Yates, 2013)), we are not aware of any approach that allows the extraction of a semantic CCG from a wide-coverage corpus such as the Groningen Meaning Bank (GMB) (Basile et al., 2012). This work attempts to fill this gap. Analogous to the work of Kwiatkowski et al. (2010), we view grammar induction as a series of splitting steps, each of which essentially reverses 12 Proceedings of the ACL 2014 Workshop on Semantic Parsing, pages 12–16, c Baltimore, Maryland USA, June 26 2014. 2014 Association for Computational Linguistics fore relate to the implementation of the SEMSPLIT function. ∃x Algorithm 1 A general splitting algorithm. C is the set of binary CCG combinators. The SEM SPLIT function returns possible splits of a meaning representation according to the reverse application of a combinator. function SPLIT(x, z) if |x |= 1 then return {(x, z)} else G←∅ for 0"
W16-2820,W13-4008,0,0.0520225,"Missing"
W16-2820,W10-0214,0,0.0388071,"nt features for the prediction of highly voted comments in terms of delta score and karma score respectively. Although they considered some sorts of argumentation related features, such features are merely based on lexical similarity, without modeling persuasion behaviors. With the popularity of the online debating forum such as idebate1 , convinceme2 and createdebate3 , researchers have been paying increasing attention to analyze debating content, including identification of arguing expressions in online debate (Trabelsi and Zaıane, 2014), recognition of stance in ideological online debates (Somasundaran and Wiebe, 2010; Hasan and Ng, 2014; Ranade et al., 2013b), and debate summarization (Ranade et al., 2013a). However, there is still little research about quality evaluation of debating content. Tan et al. (2016) and Wei and Liu (2016) studied the persuasiveness of comments in sub-reddit change my view of Reddit.com. They evaluated In this paper, we focus on a particular action in the online debating forum, i.e., disputation. Within debate, disputation happens when a user disagrees with a specific comment. Figure 1 gives a disputation example from the online debating forum createdebate. It presents an origin"
W16-2820,W14-1305,0,0.0195494,"c is “Should the Gorilla have died?”) 1 Introduction the effectiveness of different features for the prediction of highly voted comments in terms of delta score and karma score respectively. Although they considered some sorts of argumentation related features, such features are merely based on lexical similarity, without modeling persuasion behaviors. With the popularity of the online debating forum such as idebate1 , convinceme2 and createdebate3 , researchers have been paying increasing attention to analyze debating content, including identification of arguing expressions in online debate (Trabelsi and Zaıane, 2014), recognition of stance in ideological online debates (Somasundaran and Wiebe, 2010; Hasan and Ng, 2014; Ranade et al., 2013b), and debate summarization (Ranade et al., 2013a). However, there is still little research about quality evaluation of debating content. Tan et al. (2016) and Wei and Liu (2016) studied the persuasiveness of comments in sub-reddit change my view of Reddit.com. They evaluated In this paper, we focus on a particular action in the online debating forum, i.e., disputation. Within debate, disputation happens when a user disagrees with a specific comment. Figure 1 gives a dis"
W16-2820,P16-2032,1,0.678326,"ity, without modeling persuasion behaviors. With the popularity of the online debating forum such as idebate1 , convinceme2 and createdebate3 , researchers have been paying increasing attention to analyze debating content, including identification of arguing expressions in online debate (Trabelsi and Zaıane, 2014), recognition of stance in ideological online debates (Somasundaran and Wiebe, 2010; Hasan and Ng, 2014; Ranade et al., 2013b), and debate summarization (Ranade et al., 2013a). However, there is still little research about quality evaluation of debating content. Tan et al. (2016) and Wei and Liu (2016) studied the persuasiveness of comments in sub-reddit change my view of Reddit.com. They evaluated In this paper, we focus on a particular action in the online debating forum, i.e., disputation. Within debate, disputation happens when a user disagrees with a specific comment. Figure 1 gives a disputation example from the online debating forum createdebate. It presents an original argument and an argument disputing it. Our study aims to evaluate the quality of a disputing comment given its original argument and the discussed topic. In order to have a deep understanding of disputation, we analyz"
W16-2820,D14-1083,0,\N,Missing
W19-2708,J93-2004,0,0.0645941,"either an argument or a connective), the annotation workflow may differ. Moreover, potential span overlaps with other relations are not problematic, since each relation is annotated independently, and signals for multiple relations are not visualized simultaneously. Annotators can thus annotate relations and signals concurrently. However, this tool is not suitable for the type of annotation addressed by our tool, since no hierarchy of discourse units can be represented in the way required for RST trees. Figure 2: PDTB connective and argument spans. the text of the Wall Street Journal corpus (Marcus et al., 1993). We examine the tools used to produce these, as well as other approaches, below. 2.1 Discourse Signals in PDTB PDTB employs a lexically grounded approach to discourse relations and their signals by annotating 1) Explicit and Implicit connectives and their associated argument spans, which are not constrained to be single clauses or sentences; 2) supplementary information that is considered relevant but not required for the interpretation; 3) textual expressions that establish coherence other than connectives called Alternative Lexicalizations (AltLex); 4) relation senses for Explicit and Impli"
W19-2708,W00-1434,0,0.383933,"Missing"
W19-2708,P12-1008,0,0.142277,"Building corpora annotated for discourse signals is important for empirical studies of how writers and speakers signal relations in naturally occurring text, and how readers or hearers are able to recognize them. However, for one of the most popular frameworks for analyzing discourse relations, Rhetorical Structure Theory (RST, Mann 2 Previous Work Numerous studies have examined discourse signals (e.g. Knott and Sanders 1998), but the largest corpora with signal annotations have been produced in the framework of the Penn Discourse Treebank (PDTB, Prasad et al. 2008, and similarly for Chinese, Zhou and Xue 2012, and other languages) and the RST Signalling Corpus (RSTSC, Taboada and Das 2013), both built on top of ∗ We would like to thank Richard Eckhart de Castilho, Debopam Das, Nathan Schneider and Maite Taboada, as well as three anonymous reviewers for valuable comments on earlier versions of this paper and the system it describes. 1 https://github.com/amir-zeldes/rstweb 56 Proceedings of Discourse Relation Parsing and Treebanking (DISRPT2019), pages 56–61 c Minneapolis, MN, June 6, 2019. 2019 Association for Computational Linguistics Figure 3: Signal annotation from RST-SC in the UAM tool. Figure"
W19-2708,prasad-etal-2008-penn,0,0.80269,"nd aspect can also signal discourse relations. Building corpora annotated for discourse signals is important for empirical studies of how writers and speakers signal relations in naturally occurring text, and how readers or hearers are able to recognize them. However, for one of the most popular frameworks for analyzing discourse relations, Rhetorical Structure Theory (RST, Mann 2 Previous Work Numerous studies have examined discourse signals (e.g. Knott and Sanders 1998), but the largest corpora with signal annotations have been produced in the framework of the Penn Discourse Treebank (PDTB, Prasad et al. 2008, and similarly for Chinese, Zhou and Xue 2012, and other languages) and the RST Signalling Corpus (RSTSC, Taboada and Das 2013), both built on top of ∗ We would like to thank Richard Eckhart de Castilho, Debopam Das, Nathan Schneider and Maite Taboada, as well as three anonymous reviewers for valuable comments on earlier versions of this paper and the system it describes. 1 https://github.com/amir-zeldes/rstweb 56 Proceedings of Discourse Relation Parsing and Treebanking (DISRPT2019), pages 56–61 c Minneapolis, MN, June 6, 2019. 2019 Association for Computational Linguistics Figure 3: Signal"
W19-2708,J14-4007,0,0.0233673,"entences; 2) supplementary information that is considered relevant but not required for the interpretation; 3) textual expressions that establish coherence other than connectives called Alternative Lexicalizations (AltLex); 4) relation senses for Explicit and Implicit connectives and AltLex relations; 5) attribution within discourse relations including categories such as source, type, scopal polarity, and determinacy (Prasad et al., 2008). Unlike RST, which identifies hierarchic structures in text, PDTB-style annotations do not form a hierarchy and need not cover the entire text. According to Prasad et al. (2014), annotation workflow in PDTB-style resources has varied in the development of comparable corpora in other languages (e.g. Zhou and Xue 2012) and genres (e.g. Prasad et al. 2011), which could potentially affect annotator effort and inter-annotator agreement (e.g. Sharma et al. 2013). For instance, when annotating the example in Figure 2 where an Ex2.2 Signals in RST-SC Since RST originally did not foresee annotating relation signals, RST-SC takes existing trees in the RST Discourse Treebank (RST-DT, Carlson et al. 2003) as a ground truth, and adds explicit annotations for how each relation can"
W19-2708,N16-3001,1,0.89421,"truct discourse trees and identify relations in a single system. Although we base our work on an existing RST interface, the expansions presented here bridge a substantial gap in RST annotation, which has to date been unable to link complete trees to concrete discourse signal positions in a single annotation tool and format, linking specific tokens and other signaling devices to positions in the tree. Our system, shown in Figure 1, features state of the art support for viewing and editing signals, and benefits from an underlying interface offering full RST editing capabilities, called rstWeb (Zeldes, 2016). No installation is needed for end users in a project since the tool is web-based, and annotators can easily collaborate. Docker images and a local version are available for easy deployment and we make all code available open source via GitHub.1 This paper presents a new system for openended discourse relation signal annotation in the framework of Rhetorical Structure Theory (RST), implemented on top of an online tool for RST annotation. We discuss existing projects annotating textual signals of discourse relations, which have so far not allowed simultaneously structuring and annotating words"
W19-2710,N16-3001,0,0.603763,"Missing"
W19-2710,W19-2708,1,0.815673,"Missing"
W19-2710,prasad-etal-2008-penn,0,0.286183,"certain contexts. Consider the following example from the Georgetown University Multilayer (GUM) corpus (Zeldes, 2017),1 in which the two textual units connected by the DM but form a C ONTRAST relation, meaning that the contents of the two textual units are comparable yet not identical. Recent research on discourse relations has found that they are cued not only by discourse markers (DMs) but also by other textual signals and that signaling information is indicative of genres. While several corpora exist with discourse relation signaling information such as the Penn Discourse Treebank (PDTB, Prasad et al. 2008) and the Rhetorical Structure Theory Signalling Corpus (RST-SC, Das and Taboada 2018), they both annotate the Wall Street Journal (WSJ) section of the Penn Treebank (PTB, Marcus et al. 1993), which is limited to the news domain. Thus, this paper adapts the signal identification and anchoring scheme (Liu and Zeldes, 2019) to three more genres, examines the distribution of signaling devices across relations and genres, and provides a taxonomy of indicative signals found in this dataset. 1 (1) Related cross-cultural studies have resulted in insufficient statistical power, but interesting trends ("
W19-2710,C10-2118,0,0.061687,"Missing"
W19-2710,J93-2004,0,\N,Missing
W19-2710,W01-1605,0,\N,Missing
W19-2717,Q17-1010,0,0.00739735,"tokens with predicted connective types (i.e. BC ONN, I-C ONN or not a connective). Rather than predicted labels, the system reports probabilities with which each label is suspected to apply to tokens, based on the top 5 optimal paths as ranked by the CRF layer of NCRF++’s output. RNNSegmenter To benefit from the predictive power of neural sequence models and word embeddings with good coverage for OOV items, we used NCRF++ (Yang and Zhang, 2018), a biLSTM/CNN-CRF sequence labeling framework. Features included Glove word embeddings for English (Pennington et al., 2014) and FastText embeddings (Bojanowski et al., 2017) for other languages, trainable character embeddings, as well as the features in Table 1, such as POS tags, dependency labels, binned distance to parent, genre, and BIEO dependency brackets, all encoded as dense embeddings. We optimized models for each dataset, including using CNN or LSTM encoding for character and word embeddings. Ensemble Connective Detector The connective ensemble is analogous to the segmenter ensemble, and relies on a Random Forest classifier fed the predicted labels and probabilities from base connective detectors, as well as the same features fed to the segmenter ensembl"
W19-2717,W99-0307,0,0.453396,"Missing"
W19-2717,E17-1028,0,0.355534,"Missing"
W19-2717,D17-1258,0,0.254294,"Missing"
W19-2717,D17-1136,0,0.322724,"Missing"
W19-2717,D13-1094,0,0.0221025,"ttle work has approached discourse connective detection as a separate task, as it is usually employed as an intermediate step for predicting discourse relations. Pitler and Nenkova (2009) used a Max Entropy classifier using a set of syntactic features extracted from the gold standard Penn Treebank (Marcus et al., 1993) parses of PDTB (Prasad et al., 2008) articles, such as the highest node which dominates exactly and only the words in the connective, the category of the immediate parent of that phrase, and the syntactic category of the sibling immediately to the left/right of the same phrase. Patterson and Kehler (2013) presented a logistic regression model trained on eight relation types extracted from PDTB, with features in three categories: Relation-level features such as the connective signaling the relation, attribution status of the relation, and its relevance to financial information; Argument-level features, capturing the size or complexity of each of its two arguments; and Discourse-level features, which incorporate the dependencies between the relation in question and its neighboring relations in the text. Polepalli Ramesh et al. (2012) used SVM and CRF for identifying discourse connectives in biom"
W19-2717,P15-1136,0,0.0762722,"Missing"
W19-2717,D14-1162,0,0.083605,"ary and may form spans, it classifies sequences of tokens with predicted connective types (i.e. BC ONN, I-C ONN or not a connective). Rather than predicted labels, the system reports probabilities with which each label is suspected to apply to tokens, based on the top 5 optimal paths as ranked by the CRF layer of NCRF++’s output. RNNSegmenter To benefit from the predictive power of neural sequence models and word embeddings with good coverage for OOV items, we used NCRF++ (Yang and Zhang, 2018), a biLSTM/CNN-CRF sequence labeling framework. Features included Glove word embeddings for English (Pennington et al., 2014) and FastText embeddings (Bojanowski et al., 2017) for other languages, trainable character embeddings, as well as the features in Table 1, such as POS tags, dependency labels, binned distance to parent, genre, and BIEO dependency brackets, all encoded as dense embeddings. We optimized models for each dataset, including using CNN or LSTM encoding for character and word embeddings. Ensemble Connective Detector The connective ensemble is analogous to the segmenter ensemble, and relies on a Random Forest classifier fed the predicted labels and probabilities from base connective detectors, as well"
W19-2717,P09-2004,0,0.0167488,"-LSTM-CRF sequence labeling approach on dependency parses, with words, POS tags, dependency relations and the same features for each word’s parent and grand-parent tokens, as well as the direction of attachment (left or right), achieving F-scores of .89 on segmenting RST-DT with parser-predicted syntax, and scores in the 80s, near or above previous SOA results, for a number of other corpora and languages. By contrast, comparatively little work has approached discourse connective detection as a separate task, as it is usually employed as an intermediate step for predicting discourse relations. Pitler and Nenkova (2009) used a Max Entropy classifier using a set of syntactic features extracted from the gold standard Penn Treebank (Marcus et al., 1993) parses of PDTB (Prasad et al., 2008) articles, such as the highest node which dominates exactly and only the words in the connective, the category of the immediate parent of that phrase, and the syntactic category of the sibling immediately to the left/right of the same phrase. Patterson and Kehler (2013) presented a logistic regression model trained on eight relation types extracted from PDTB, with features in three categories: Relation-level features such as t"
W19-2717,prasad-etal-2008-penn,0,0.353089,"well as the direction of attachment (left or right), achieving F-scores of .89 on segmenting RST-DT with parser-predicted syntax, and scores in the 80s, near or above previous SOA results, for a number of other corpora and languages. By contrast, comparatively little work has approached discourse connective detection as a separate task, as it is usually employed as an intermediate step for predicting discourse relations. Pitler and Nenkova (2009) used a Max Entropy classifier using a set of syntactic features extracted from the gold standard Penn Treebank (Marcus et al., 1993) parses of PDTB (Prasad et al., 2008) articles, such as the highest node which dominates exactly and only the words in the connective, the category of the immediate parent of that phrase, and the syntactic category of the sibling immediately to the left/right of the same phrase. Patterson and Kehler (2013) presented a logistic regression model trained on eight relation types extracted from PDTB, with features in three categories: Relation-level features such as the connective signaling the relation, attribution status of the relation, and its relevance to financial information; Argument-level features, capturing the size or compl"
W19-2717,D16-1035,0,0.218869,"Missing"
W19-2717,J14-4007,0,0.120884,"or less homogeneous corpora than RST-DT, and especially in the absence of gold syntax trees (which are realistically unavailable at test time for practical applications), hovers around the mid 80s, making it problematic for full discourse parsing in practice. This is more critical for languages and domains in which relatively small datasets are available, making the application of generic neural models less promising. The DISRPT 2019 Shared Task aims to identify spans associated with discourse relations in data from three formalisms: RST (Mann and Thompson, 1988), SDRT (Asher, 1993) and PDTB (Prasad et al., 2014). The targeted task varies actoss frameworks: Since RST and SDRT segment texts into spans covering the entire document, the corresponding task is to predict the starting point of new discourse units. In the PDTB framework, the basic locus identifying explicit discourse relations is the spans of discourse connectives which need to be identified among other words. In total, 15 corpora (10 from RST data, 3 from PDTB-style data, and 2 from SDRT) in 10 languages (Basque, Chinese, Dutch, English, French, German, Portuguese, Russian, Spanish, and Turkish) are used as the input data for the task. The"
W19-2717,N03-1030,0,0.13549,"ach contributor to work on a separate sub-project, all of which are combined in the complete system as an ensemble. The system was built by six graduate students and the instructor, with each student focusing on one module (notwithstanding occasional collaborations) in two phases: work on a high-accuracy ensemble sentence splitter for the automatic parsing scenario (see Section 3.2), followed by the development of a discourse unit segmenter or connective detection module (Sections 3.3 and 3.4). 2 Previous Work Following early work on rule-based segmenters (e.g. Marcu 2000, Thanh et al. 2004), Soricut and Marcu (2003) used a simple probabilistic model conditioning on lexicalized constituent trees, by using the highest node above each word that has a right-hand sibling, as well as its children. Like our approach, this and subsequent work below perform EDU segmentation as a token-wise binary classification task (boundary/no-boundary). In a more complex model, Sporleder and Lapata (2005) used a two-level stacked boosting classifier on syntactic chunks, POS tags, token and sentence lengths, and token positions within clauses, all of which are similar to or subsumed by some of our features below. They additiona"
W19-2717,H05-1033,0,0.0541431,"e Section 3.2), followed by the development of a discourse unit segmenter or connective detection module (Sections 3.3 and 3.4). 2 Previous Work Following early work on rule-based segmenters (e.g. Marcu 2000, Thanh et al. 2004), Soricut and Marcu (2003) used a simple probabilistic model conditioning on lexicalized constituent trees, by using the highest node above each word that has a right-hand sibling, as well as its children. Like our approach, this and subsequent work below perform EDU segmentation as a token-wise binary classification task (boundary/no-boundary). In a more complex model, Sporleder and Lapata (2005) used a two-level stacked boosting classifier on syntactic chunks, POS tags, token and sentence lengths, and token positions within clauses, all of which are similar to or subsumed by some of our features below. They additionally used the list of English connectives from Knott (1996) to identify connective tokens. Hernault et al. (2010) used an SVM model with features corresponding to token and POS trigrams at and preceding a potential segmentation point, as well as features encoding the lexical head of each token’s parent phrase in a phrase structure syn3 GumDrop Our system is organized aroun"
W19-2717,C04-1048,0,0.137076,"Missing"
W19-2717,P18-4013,0,0.0258071,"ective Detector This module is architecturally identical to the RNN EDU segmenter, but since connective labels are non-binary and may form spans, it classifies sequences of tokens with predicted connective types (i.e. BC ONN, I-C ONN or not a connective). Rather than predicted labels, the system reports probabilities with which each label is suspected to apply to tokens, based on the top 5 optimal paths as ranked by the CRF layer of NCRF++’s output. RNNSegmenter To benefit from the predictive power of neural sequence models and word embeddings with good coverage for OOV items, we used NCRF++ (Yang and Zhang, 2018), a biLSTM/CNN-CRF sequence labeling framework. Features included Glove word embeddings for English (Pennington et al., 2014) and FastText embeddings (Bojanowski et al., 2017) for other languages, trainable character embeddings, as well as the features in Table 1, such as POS tags, dependency labels, binned distance to parent, genre, and BIEO dependency brackets, all encoded as dense embeddings. We optimized models for each dataset, including using CNN or LSTM encoding for character and word embeddings. Ensemble Connective Detector The connective ensemble is analogous to the segmenter ensemble"
W19-2717,N16-3001,1,0.909173,"Missing"
W19-2717,J93-2004,0,\N,Missing
W19-2717,W01-1605,0,\N,Missing
W19-4416,W14-1702,0,0.0253764,"h, and there is a clear trend that state-of-the-art GEC systems are being shifted from traditional NLP methods to NMT based methods. In recent years, GEC performance has seen significant improvement in some public GEC test sets (Ge et al., 2018). In CoNLL-2013 (Ng et al., 2013) and CoNLL-2014 (Ng et al., 2014) GEC Shared Task, machine learning based GEC methods emerged with relatively good performance. Classification methods achieved the best result in CoNLL-2013 (Rozovskaya et al., 2013). After that, statistical machine translation (SMT) methods began to show better performance in CoNLL2014 (Felice et al., 2014). (Chollampatt et al., 2016) was the first study to obtain the state-ofthe-art result with neural networks. Then after (Junczys-Dowmunt and Grundkiewicz, 2016), machine translation methods became the mainstream in GEC solutions. In addition, an RNNbased context model achieved better results than previous traditional classification models (Wang et al., 2017). Using a CNN-based sequenceto-sequence architecture (Gehring et al., 2017), (Chollampatt and Ng, 2018) proposed the first end-to-end NMT model and reported the state-ofthe-art result. As Transformer (Vaswani et al., 2017) plays an increasin"
W19-4416,N18-2046,0,0.0242718,"z, 2016), machine translation methods became the mainstream in GEC solutions. In addition, an RNNbased context model achieved better results than previous traditional classification models (Wang et al., 2017). Using a CNN-based sequenceto-sequence architecture (Gehring et al., 2017), (Chollampatt and Ng, 2018) proposed the first end-to-end NMT model and reported the state-ofthe-art result. As Transformer (Vaswani et al., 2017) plays an increasingly important role in sequence modeling, Transformer-based end-to-end NMT models began to lead the current GEC research (Junczys-Dowmunt et al., 2018; Grundkiewicz and Junczys-Dowmunt, 2018; Ge et al., 2018; Zhao et al., 2019). It is worth mentioning that (Lichtarge et al., 2019) used Wikipedia edIn this paper, we describe two systems we developed for the three tracks we have participated in the BEA-2019 GEC Shared Task. We investigate competitive classification models with bi-directional recurrent neural networks (Bi-RNN) and neural machine translation (NMT) models. For different tracks, we use ensemble systems to selectively combine the NMT models, the classification models, and some rules, and demonstrate that an ensemble solution can effectively improve GEC performance over"
W19-4416,D16-1161,0,0.0249267,"nt years, GEC performance has seen significant improvement in some public GEC test sets (Ge et al., 2018). In CoNLL-2013 (Ng et al., 2013) and CoNLL-2014 (Ng et al., 2014) GEC Shared Task, machine learning based GEC methods emerged with relatively good performance. Classification methods achieved the best result in CoNLL-2013 (Rozovskaya et al., 2013). After that, statistical machine translation (SMT) methods began to show better performance in CoNLL2014 (Felice et al., 2014). (Chollampatt et al., 2016) was the first study to obtain the state-ofthe-art result with neural networks. Then after (Junczys-Dowmunt and Grundkiewicz, 2016), machine translation methods became the mainstream in GEC solutions. In addition, an RNNbased context model achieved better results than previous traditional classification models (Wang et al., 2017). Using a CNN-based sequenceto-sequence architecture (Gehring et al., 2017), (Chollampatt and Ng, 2018) proposed the first end-to-end NMT model and reported the state-ofthe-art result. As Transformer (Vaswani et al., 2017) plays an increasingly important role in sequence modeling, Transformer-based end-to-end NMT models began to lead the current GEC research (Junczys-Dowmunt et al., 2018; Grundkie"
W19-4416,N18-1055,0,0.0766815,"unczys-Dowmunt and Grundkiewicz, 2016), machine translation methods became the mainstream in GEC solutions. In addition, an RNNbased context model achieved better results than previous traditional classification models (Wang et al., 2017). Using a CNN-based sequenceto-sequence architecture (Gehring et al., 2017), (Chollampatt and Ng, 2018) proposed the first end-to-end NMT model and reported the state-ofthe-art result. As Transformer (Vaswani et al., 2017) plays an increasingly important role in sequence modeling, Transformer-based end-to-end NMT models began to lead the current GEC research (Junczys-Dowmunt et al., 2018; Grundkiewicz and Junczys-Dowmunt, 2018; Ge et al., 2018; Zhao et al., 2019). It is worth mentioning that (Lichtarge et al., 2019) used Wikipedia edIn this paper, we describe two systems we developed for the three tracks we have participated in the BEA-2019 GEC Shared Task. We investigate competitive classification models with bi-directional recurrent neural networks (Bi-RNN) and neural machine translation (NMT) models. For different tracks, we use ensemble systems to selectively combine the NMT models, the classification models, and some rules, and demonstrate that an ensemble solution can e"
W19-4416,Q17-1010,0,0.0144873,"tricted and Unrestricted tasks. The system uses several ensemble methods to combine the CNNbased and Transformer-based translation models, described in details below. 2.1.1 CNN-based translation ensemble systems We found that CNN-based systems obtained the best results for some error types, likely due to some characteristics derived from CNN. We trained four CNN-based ensemble systems, using the model architecture in (Chollampatt and Ng, 2018), but without reranking. Four best combinations to build the ensemble systems were selected. Unlike (Chollampatt and Ng, 2018), we did not use fastText (Bojanowski et al., 2017) to initialize word embeddings because we found no improvement on the development set by doing that. We tuned parameters for the system, such as batch size, word embedding dimension, etc. 2.1.2 • When combining outcomes from different systems, we treat the precision in a confidence table as the confidence. Each correction has its confidence obtained by looking up the precision of the corresponding type of the correction in the table. If two conflicting corrections are the same, we merge them and add α to the confidence of the correction; otherwise, the correction with a lower confidence will b"
W19-4416,P17-1074,0,0.0304732,"e three tracks are described in next section. In Section 3, we evaluate the systems on the development data and show the final results on the test data. Section 4 concludes the paper and summarizes the future work. 2 2.1.3 Ensemble methods We expect to combine these models trained above into a more powerful system through effective ensemble methods. Our ensemble work mainly focuses on rule-based solutions. We will introduce two main modules first. Confidence Table We can obtain the precision and F0.5 metric on each error type through sentence alignment and error type classification by Errant (Bryant et al., 2017). Errant provides performance statistics based on 55 error types and is also the tool used to evaluate this GEC shared task, thus we use the result of operation and error type span-level (Bryant et al., 2017) for a model or system as the confidence table. Conflict Solver We often encounter GEC error conflicts when combining multiple models or systems. For example, We love played soccer. One system corrects played to playing, while another system may correct played to to play. When two different corrections occur in the same place, we need to consider which one to choose. We solve this problem"
W19-4416,W19-4406,0,0.0677093,"ducational Applications, pages 159–167 c Florence, Italy, August 2, 2019. 2019 Association for Computational Linguistics domain and error adaptation. We also compared the results using 2 GPUs and 4 GPUs as the authors reported the difference in their Github repository1 . its history corpus, which is huge but noisy, and gained a result very close to the state-of-the-art result. Learning a GEC translation model from noisy data is a worthy future direction as the GEC parallel corpus is expensive to obtain. This paper describes our two systems for the three tracks in the BEA-2019 GEC Shared Task (Bryant et al., 2019). We use two popular NMT models and two improved versions of neural classification models to train the basic models. Ensemble strategies are then used to combine outcomes from different models. Our two systems for the three tracks are described in next section. In Section 3, we evaluate the systems on the development data and show the final results on the test data. Section 4 concludes the paper and summarizes the future work. 2 2.1.3 Ensemble methods We expect to combine these models trained above into a more powerful system through effective ensemble methods. Our ensemble work mainly focuses"
W19-4416,D18-1397,0,0.06848,"Missing"
W19-4416,P11-1019,0,0.019879,"based translation ensemble models We added the W&I corpus eight times to the training corpus for domain adaptation. Table 2 shows the performance of the single CNN-based translation models. All the parameters in Table 2 are tuned over the W&I+LOCNESS development set. Table 3 shows the results of the four CNN-based ensemble systems. We use ensembles in the same way as (Chollampatt and Ng, 2018). The above results prove that the ensemble method has yielded a very large improvement in this task. Data Sets Table 1 lists the data sets used in Restricted Track and Unrestricted Track, including FCE (Yannakoudakis et al., 2011), Lang-82 (Mizumoto et al., 2012), NUCLE (Ng et al., 2014), W&I+LOCNESS (Bryant et al., 2019) and Common Crawl. We use Common Crawl to pretrain the decoder parameters for the Transformer-based translation model. FCE, Lang-8, NUCLE and W&I are used to train all of the translation models. 2 Restricted and Unrestricted Track 3 https://lang-8.com 163 https://spacy.io Ensemble index 1 2 3 4 Combination 5012,5102,7011,7205 5001,5002,5003,5004 5005,5012,5102,7205 5005,5012,7011,7205 Precision 0.5076 0.5003 0.5156 0.5152 Recall 0.2195 0.1951 0.2150 0.2159 F0.5 0.4021 0.3811 0.4029 0.4034 Table 3: Resu"
W19-4416,N19-1014,0,0.0130711,"am in GEC solutions. In addition, an RNNbased context model achieved better results than previous traditional classification models (Wang et al., 2017). Using a CNN-based sequenceto-sequence architecture (Gehring et al., 2017), (Chollampatt and Ng, 2018) proposed the first end-to-end NMT model and reported the state-ofthe-art result. As Transformer (Vaswani et al., 2017) plays an increasingly important role in sequence modeling, Transformer-based end-to-end NMT models began to lead the current GEC research (Junczys-Dowmunt et al., 2018; Grundkiewicz and Junczys-Dowmunt, 2018; Ge et al., 2018; Zhao et al., 2019). It is worth mentioning that (Lichtarge et al., 2019) used Wikipedia edIn this paper, we describe two systems we developed for the three tracks we have participated in the BEA-2019 GEC Shared Task. We investigate competitive classification models with bi-directional recurrent neural networks (Bi-RNN) and neural machine translation (NMT) models. For different tracks, we use ensemble systems to selectively combine the NMT models, the classification models, and some rules, and demonstrate that an ensemble solution can effectively improve GEC performance over single systems. Our GEC systems ranke"
W19-4450,D15-1075,0,0.0373113,"netuning the model and the last layer is likely to be more tuned to the original BERT training tasks. These embeddings are then used as input to the BCA model (BERT-BCA), which is then trained on the essay scoring task. 3 Data set 1 2 Low 1,202 129 655 222 Essays 1783 1800 Avg. len 350 350 Score range 2-12 1-6 shown in Table 2. Since only the training samples are available for both sets, we report results for 5-fold cross-validation using the same splits as (Taghipour and Ng, 2016). Pretraining tasks use two data sets. The NLI task uses the Stanford natural language inference (SNLI) data set (Bowman et al., 2015). We cast our NLI task as a four-way classification task, because a subset of the data does not have gold labels. Unlabeled examples were used with an “X” label. While tuning on the main task, we found that including the fourth NLI label gave better performance on the essay scoring than not using it. The DM task is based on a collection of over 13K free books from www.smashwords.com – an online book distribution platform.4 Labeled discourse marker data was created by identifying sentence pairs that had a discourse marker at the start of the second sentence. We used 87 discourse markers, which"
W19-4450,P18-1189,0,0.0545451,"Missing"
W19-4450,N18-1202,0,0.0858161,"Missing"
W19-4450,N18-1024,0,0.170625,"ang Liu2 , and Mari Ostendorf1 1 Department of Electrical and Computer Engineering, University of Washington {farahn, ostendor}@uw.edu 2 LAIX Inc. {huy.nguyen, yang.liu}@liulishuo.com Abstract These problems (and the success of deep learning in other areas of language processing) have led to the development of neural methods for automatic essay scoring, moving away from feature engineering. A variety of studies (mostly LSTM-based) have reported AES performance comparable to or better than feature-based models (Taghipour and Ng, 2016; Cummins and Rei, 2018; Wang et al., 2018; Jin et al., 2018; Farag et al., 2018; Zhang and Litman, 2018). However, the current state-of-the-art models still use a combination of neural models and hand-crafted features (Liu et al., 2019). While vanilla RNNs, particularly LSTMs, are good at representing text sequences, essays are longer structured documents and less well suited to an RNN representation. Thus, our work looks at advancing AES by exploring other architectures that incorporate document structure for longer documents. Discourse structure and coherence are important aspects of essay writing and are often explicitly a part of grading rubrics. We explore methods t"
W19-4450,W19-4302,0,0.0466056,"Missing"
W19-4450,P09-1077,0,0.0150971,"mining features (Nguyen and Litman, 2018). In our implementation, we remove one set of basic features, e.g., word counts, spelling errors etc., since they are present in both models and keep the set from (Vajjala and Meurers, 2014). Given the extracted features, a gradient boosting algorithm is used to learn a regression model. Predicted scores are scaled and rounded to calculate Quadratic Weighted Kappa (QWK) against the true scores. These two feature sets are chosen because they incorporate discourse features in AES. In (Vajjala and Meurers, 2014), the authors used the addDiscourse toolkit (Pitler et al., 2009), which takes as input the syntactic tree of the sentence, and tags the discourse connectives, e.g., therefore, however, and their senses, e.g., CONTINGENCY.Cause, COMPARISON.Contrast. These automated annotations are then used to calculate connective based features, at 487 Model Arg (Klebanov16) Length (Klebanov16) Arg + Len (Klebanov16) Nguyen18 Feature baseline USE-HAN BERT-HAN HAN NLI-HAN DM-HAN NLI-DM-HAN BCA NLI-BCA DM-BCA NLI-DM-BCA BERT-BCA e.g., number of discourse connectives per sentence, number of each sense. In (Nguyen and Litman, 2018), an end-to-end pipeline system was built to p"
W19-4450,P17-1092,0,0.0200298,"u et al., 2019) are achieved with a model that learns the combination of hand-crafted features and the neural document representation. Thus, for tasks with limited labeled data, there is still a place for hand-crafted features. Like other neural models, our approach suffers from a lack of interpretability. While our analysis of sentence similarity with the DM-BCA model provides some useful insights into differences between high and low scoring TOEFL essays, the best scoring model did not have the same behavior. This remains an open problem. could also be explored, e.g., (Le and Mikolov, 2014; Ji and Smith, 2017; Card et al., 2018). Numerous previous studies have looked at using external data to improve performance of neural classifiers. One study that influenced our work is (Jernite et al., 2017), which showed that discoursebased tasks such as sentence order and conjunction prediction can improve neural sentence representations for several NLP tasks. This study used the Book Corpus data (Zhu et al., 2015) and the Gutenberg data (Stroube, 2003) for discoursebased tasks. Our task is similar, but we use a larger set of discourse markers. Representations from pretrained models including (Devlin et al.,"
W19-4450,P18-1100,0,0.172118,"Missing"
W19-4450,W16-2808,0,0.324532,"e systems and on validity studies (Shermis, 2014). The ability to evaluate student writing has always been important for language teaching and learning; now it also extends to science, since the focus is shifting towards assessments that can more accurately gauge construct knowledge as compared to multiple choice questions (Shermis, 2014). Most existing systems for automatic essay scoring leverage hand crafted features, ranging from wordcounts to argumentation structure and coherence, in linear regression and logistic regression models (Chodorow and Burstein, 2004; Shermis and Burstein, 2013; Klebanov et al., 2016; Nguyen and Litman, 2018). Improving feature-based models requires extensive redesigning of features (Taghipour and Ng, 2016). Due to high variability in types of student essays, feature-based systems are often individually designed for specific prompts (Burstein et al., 2013). This poses a challenge for building essay scoring systems. 484 Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications, pages 484–493 c Florence, Italy, August 2, 2019. 2019 Association for Computational Linguistics • Natural language inference (NLI): given a pair of senten"
W19-4450,D16-1193,0,0.235558,"ted Essay Scoring with Discourse-Aware Neural Models Farah Nadeem1 , Huy Nguyen2 , Yang Liu2 , and Mari Ostendorf1 1 Department of Electrical and Computer Engineering, University of Washington {farahn, ostendor}@uw.edu 2 LAIX Inc. {huy.nguyen, yang.liu}@liulishuo.com Abstract These problems (and the success of deep learning in other areas of language processing) have led to the development of neural methods for automatic essay scoring, moving away from feature engineering. A variety of studies (mostly LSTM-based) have reported AES performance comparable to or better than feature-based models (Taghipour and Ng, 2016; Cummins and Rei, 2018; Wang et al., 2018; Jin et al., 2018; Farag et al., 2018; Zhang and Litman, 2018). However, the current state-of-the-art models still use a combination of neural models and hand-crafted features (Liu et al., 2019). While vanilla RNNs, particularly LSTMs, are good at representing text sequences, essays are longer structured documents and less well suited to an RNN representation. Thus, our work looks at advancing AES by exploring other architectures that incorporate document structure for longer documents. Discourse structure and coherence are important aspects of essay"
W19-4450,W18-0505,1,0.924218,"d BCA models. The use of contextualized embeddings can also be thought of as pre-training with an auxiliary task of language modeling (or masked language modeling). In this work, we chose the bidirectional transformer architecture (BERT) embeddings (Devlin et al., 2018), which uses a transformer architecture trained on two tasks, masked language model and next sentence prediction. We hypothesized that the next sentence prediction would capture aspects of discourse coherence. • Hierarchical recurrent network with attention (HAN) (Yang et al., 2016) • Bidirectional context with attention (BCA) (Nadeem and Ostendorf, 2018) Both models are LSTM based. HAN captures the hierarchical structure within a document, by using two stacked layers of LSTMs. The first layer takes word embeddings as input and outputs contextualized word representations. Self attention is used to compute a sentence vector as a weighted average of the contextualized word vectors. The second LSTM takes sentence vectors as input and outputs a document vector based on averaging using self attention at the sentence level. BCA extends HAN to account for cross sentence dependencies. For each word, using the contextualized word vectors output from th"
W19-4450,D18-1090,0,0.290813,"odels Farah Nadeem1 , Huy Nguyen2 , Yang Liu2 , and Mari Ostendorf1 1 Department of Electrical and Computer Engineering, University of Washington {farahn, ostendor}@uw.edu 2 LAIX Inc. {huy.nguyen, yang.liu}@liulishuo.com Abstract These problems (and the success of deep learning in other areas of language processing) have led to the development of neural methods for automatic essay scoring, moving away from feature engineering. A variety of studies (mostly LSTM-based) have reported AES performance comparable to or better than feature-based models (Taghipour and Ng, 2016; Cummins and Rei, 2018; Wang et al., 2018; Jin et al., 2018; Farag et al., 2018; Zhang and Litman, 2018). However, the current state-of-the-art models still use a combination of neural models and hand-crafted features (Liu et al., 2019). While vanilla RNNs, particularly LSTMs, are good at representing text sequences, essays are longer structured documents and less well suited to an RNN representation. Thus, our work looks at advancing AES by exploring other architectures that incorporate document structure for longer documents. Discourse structure and coherence are important aspects of essay writing and are often explicitly a part of"
W19-4450,D14-1162,0,0.0876831,"m the first LSTM, a look-back and look-ahead context vector is computed based on the similarity with words in the previous and following sentence, respectively. The final word representation is then created as a concatenation of the LSTM output, the look-back and look-ahead context vectors, and then used to create a sentence vector using attention weights, which feeds into the second LSTM. The representation of cross-sentence dependencies makes this model discourse aware. 2.2 2.3 Training Methods All HAN models and a subset of BCA models are initialized with pretrained Glove word embeddings1 (Pennington et al., 2014). All models are trained with the essay training data. For models that are pretrained, the word-level LSTM and bidirectional context with attention (for BCA), are common for all tasks used in training. Given the word-level representations, the model computes attention weights over words for the target task (DM, NLI or essay scoring). The sentence representation is then computed by averaging the word representations using task-specific attention weights. For the pretraining tasks, the sentence representations the two sentences in the pair are concatenated, passed through a feedforward neural ne"
W19-4450,N16-1174,0,0.29595,"sentence pairs, so they impact the first-level LSTM of the HAN and BCA models. The use of contextualized embeddings can also be thought of as pre-training with an auxiliary task of language modeling (or masked language modeling). In this work, we chose the bidirectional transformer architecture (BERT) embeddings (Devlin et al., 2018), which uses a transformer architecture trained on two tasks, masked language model and next sentence prediction. We hypothesized that the next sentence prediction would capture aspects of discourse coherence. • Hierarchical recurrent network with attention (HAN) (Yang et al., 2016) • Bidirectional context with attention (BCA) (Nadeem and Ostendorf, 2018) Both models are LSTM based. HAN captures the hierarchical structure within a document, by using two stacked layers of LSTMs. The first layer takes word embeddings as input and outputs contextualized word representations. Self attention is used to compute a sentence vector as a weighted average of the contextualized word vectors. The second LSTM takes sentence vectors as input and outputs a document vector based on averaging using self attention at the sentence level. BCA extends HAN to account for cross sentence depende"
W19-4450,N10-1000,0,0.0899953,"Missing"
Y10-1009,P08-1023,1,0.829903,"Missing"
Y10-1009,D08-1022,1,\N,Missing
Y10-1009,P07-1089,1,\N,Missing
Y10-1009,P06-1077,1,\N,Missing
Y10-1009,P09-1063,1,\N,Missing
Y10-1009,D08-1010,1,\N,Missing
Y10-1009,C08-1041,1,\N,Missing
Y10-1009,C10-1080,1,\N,Missing
Y15-1006,S15-2045,0,0.0422242,"Missing"
Y15-1006,S12-1059,0,0.06988,"Missing"
Y15-1006,P06-4018,0,0.0136687,"Missing"
Y15-1006,W04-2502,0,0.0617941,"Missing"
Y15-1006,P02-1040,0,0.0918698,"Missing"
Y15-1006,S12-1060,0,0.0555482,"Missing"
Y15-1006,stefanescu-etal-2014-latent,0,0.0438937,"Missing"
Y15-1006,Q14-1018,0,0.0434042,"Missing"
Y15-1006,P94-1019,0,\N,Missing
Y15-1006,S14-2039,0,\N,Missing
Y15-1006,S13-1005,0,\N,Missing
