2021.naacl-main.156,Counterfactual Supporting Facts Extraction for Explainable Medical Record Based Diagnosis with Graph Network,2021,-1,-1,4,0,3726,haoran wu,Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"Providing a reliable explanation for clinical diagnosis based on the Electronic Medical Record (EMR) is fundamental to the application of Artificial Intelligence in the medical field. Current methods mostly treat the EMR as a text sequence and provide explanations based on a precise medical knowledge base, which is disease-specific and difficult to obtain for experts in reality. Therefore, we propose a counterfactual multi-granularity graph supporting facts extraction (CMGE) method to extract supporting facts from irregular EMR itself without external knowledge bases in this paper. Specifically, we first structure the sequence of EMR into a hierarchical graph network and then obtain the causal relationship between multi-granularity features and diagnosis results through counterfactual intervention on the graph. Features having the strongest causal connection with the results provide interpretive support for the diagnosis. Experimental results on real Chinese EMR of the lymphedema demonstrate that our method can diagnose four types of EMR correctly, and can provide accurate supporting facts for the results. More importantly, the results on different diseases demonstrate the robustness of our approach, which represents the potential application in the medical field."
2021.findings-emnlp.262,Locality Preserving Sentence Encoding,2021,-1,-1,4,0,7057,changrong min,Findings of the Association for Computational Linguistics: EMNLP 2021,0,"Although researches on word embeddings have made great progress in recent years, many tasks in natural language processing are on the sentence level. Thus, it is essential to learn sentence embeddings. Recently, Sentence BERT (SBERT) is proposed to learn embeddings on the sentence level, and it uses the inner product (or, cosine similarity) to compute semantic similarity between sentences. However, this measurement cannot well describe the semantic structures among sentences. The reason is that sentences may lie on a manifold in the ambient space rather than distribute in an Euclidean space. Thus, cosine similarity cannot approximate distances on the manifold. To tackle the severe problem, we propose a novel sentence embedding method called Sentence BERT with Locality Preserving (SBERT-LP), which discovers the sentence submanifold from a high-dimensional space and yields a compact sentence representation subspace by locally preserving geometric structures of sentences. We compare the SBERT-LP with several existing sentence embedding approaches from three perspectives: sentence similarity, sentence classification and sentence clustering. Experimental results and case studies demonstrate that our method encodes sentences better in the sense of semantic structures."
2021.ccl-1.23,"ç»åæ ç­¾è½¬ç§»å\
³ç³»çå¤ä»»å¡ç¬ç¹è¯å«æ¹æ³(Multi-task punchlines recognition method combined with label transfer relationship)",2021,-1,-1,3,0,11726,tongyue zhang,Proceedings of the 20th Chinese National Conference on Computational Linguistics,0,"{``}å¹½é»å¨äººç±»äº¤æµä¸­æ®æ¼çéè¦è§è²,å¹¶å¤§éå­å¨äºæ
æ¯åå§ä¸­ãç¬ç¹(punchline)æ¯æ
æ¯åå§å®ç°å¹½é»ææçå½¢å¼ä¹ä¸,å¨æ
æ¯åå§ç¬ç¹è¯å«ä»»å¡ä¸­,æ¯æ¡å¥å­çæ ç­¾ä»£è¡¨è¯¥å¥æ¯å¦ä¸ºç¬ç¹,ä½æ¯ä»¥å¾çç¬ç¹è¯å«å·¥ä½éå¸¸åªéè¿å»ºæ¨¡ä¸ä¸æè¯­ä¹å
³ç³»è¯å«ç¬ç¹,å¯¹æ ç­¾çå©ç¨å¹¶ä¸å

åãä¸ºäºå

åå©ç¨æ ç­¾åºåä¸­çä¿¡æ¯,æ¬ææåºäºä¸ç§æ°çè¯å«æ¹æ³,å³ç»åæ¡ä»¶éæºåºçåè¯çº§-å¥å­çº§å¤ä»»å¡å­¦ä¹ æ¨¡å,è¯¥æ¨¡åå¨ä¸¤æ¹é¢è¿è¡äºæ¹è¿,é¦å
å°æ ç­¾åºåä¸­ç¸é»ä¸¤ä¸ªæ ç­¾ä¹é´çè½¬ç§»å
³ç³»çä½å¹½é»çè®ºä¸­ä¸ä¸è´æ§çä¸ç§ä½ç°,å¹¶ä½¿ç¨æ¡ä»¶éæºåºå­¦ä¹ è¿ç§è½¬ç§»å
³ç³»,å
¶æ¬¡ç±äºå­¦ä¹ ç¸é»æ ç­¾ä¹é´çè½¬ç§»å
³ç³»ä»¥åä¸ä¸æè¯­ä¹å
³ç³»åè½å¤å­¦ä¹ å°éºå«åç¬ç¹ä¹é´çä¸ä¸è´æ§,ä¸¤è
ä¹é´å­å¨ç¸å
³æ§,ä¸ºäºä½¿æ¨¡åéè¿å©ç¨è¿ç§ç¸å
³æ§æé«ç¬ç¹è¯å«çææ,è¯¥æ¨¡åå¼å
¥äºå¤ä»»å¡å­¦ä¹ æ¹æ³,ä½¿ç¨å¤ä»»å¡å­¦ä¹ æ¹æ³åæ¶å­¦ä¹ æ¯æ¡å¥å­çå¥ä¹ãç»ææ¯æ¡å¥å­çææå­ç¬¦çè¯ä¹,åè¯çº§å«çæ ç­¾è½¬ç§»å
³ç³»ä»¥åå¥å­çº§å«çæ ç­¾è½¬ç§»å
³ç³»ãæ¬æå¨CCL2020{``}å°çæ¯{''}å¹½é»è®¡ç®{---}æ
æ¯åå§ç¬ç¹è¯å«è¯æµä»»å¡çè±ææ°æ®éä¸è¿è¡å®éª,ç»æè¡¨æ,æ¬ææåºçæ¹æ³æ¯ç®åæå¥½çæ¹æ³æé«äº3.2{\%},å¨æ
æ¯åå§å¹½é»ç¬ç¹è¯å«ä»»å¡ä¸åå¾äºæå¥½çææ,å¹¶éè¿æ¶èå®éªè¯æäºä¸è¿°ä¸¤æ¹é¢æ¹è¿çæææ§ã{''}"
2021.ccl-1.75,è½¯ä»¶æ è¯ç¬¦çèªç¶è¯­è¨è§èæ§ç ç©¶(Research on the Natural Language Normalness of Software Identifiers),2021,-1,-1,6,0,11837,dongzhen wen,Proceedings of the 20th Chinese National Conference on Computational Linguistics,0,"{``}è½¯ä»¶æºä»£ç ççè§£åæ¯è½¯ä»¶ååå¼åä¸ç»´æ¤çæ ¸å¿,èæºä»£ç ä¸­å åæ°ä»¥ä¸çæ è¯ç¬¦ççè§£åå¨è½¯ä»¶çè§£ä¸­èµ·å°éè¦ä½ç¨,ä¼ ç»è½¯ä»¶å·¥ç¨ä¸»è¦ç ç©¶éè¿å½åè§èéå¶æ è¯ç¬¦çå½åè¿ç¨ä»¥æé æ´æçè§£åäº¤æµçæ è¯ç¬¦ãæ¬æåå¨æ¢³çåæå¸¸è§ç¼ç¨è¯­è¨å½åè§èçåºç¡ä¸,æåºä¸ç§å
¨æ°çæ è¯ç¬¦å¯çè§£æ§è¯ä»·æ åãå
·ä½èè¨,æ¬æé¦å
æ»ç»æ¢³çäºå¸¸è§ä¸»æµç¼ç¨è¯­è¨ä¸­çå½åè§èå¹¶ç±»æ¯èªç¶è¯­è¨è¯­ç´ æ¦å¿µæ¬ææåºåºäºè½¯ä»¶è¯­ç´ çæ è¯ç¬¦ææè¿ç¨,å³æ è¯ç¬¦çææå¯è¢«è§ä¸ºè½¯ä»¶è¯­ç´ ççæãæååè¿æ¥è¿ç¨ãå¨æ­¤åºç¡ä¸,æ¬ææåºä¸ç§ç»åèªç¶è¯­æåºçè½¯ä»¶æ è¯ç¬¦è§èæ§è¯ä»·æ¹æ³,ç¨æ¥è¡¡éè½¯ä»¶æ è¯ç¬¦æ¯å¦æäºçè§£ãæå,æ¬æéè¿æºä»£ç çè§£æ°æ®éåä¹ä¹©ä¹´ä¹¨ä¹µä¹¢å¹³å°ä¸­å¼æºé¡¹ç®å¯¹è§èæ§ææ è¿è¡äºéªè¯æ§å®éª,ç»æè¡¨ææ¬ææåºçè§èæ§åæ°è½å¤å¾å¥½è¡¡éè½¯ä»¶é¡¹ç®çå¯çè§£æ§ã{''}"
2020.emnlp-main.275,Bridging the Gap between Prior and Posterior Knowledge Selection for Knowledge-Grounded Dialogue Generation,2020,-1,-1,6,1,6632,xiuyi chen,Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),0,"Knowledge selection plays an important role in knowledge-grounded dialogue, which is a challenging task to generate more informative responses by leveraging external knowledge. Recently, latent variable models have been proposed to deal with the diversity of knowledge selection by using both prior and posterior distributions over knowledge and achieve promising performance. However, these models suffer from a huge gap between prior and posterior knowledge selection. Firstly, the prior selection module may not learn to select knowledge properly because of lacking the necessary posterior information. Secondly, latent variable models suffer from the exposure bias that dialogue generation is based on the knowledge selected from the posterior distribution at training but from the prior distribution at inference. Here, we deal with these issues on two aspects: (1) We enhance the prior selection module with the necessary posterior information obtained from the specially designed Posterior Information Prediction Module (PIPM); (2) We propose a Knowledge Distillation Based Training Strategy (KDBTS) to train the decoder with the knowledge selected from the prior distribution, removing the exposure bias of knowledge selection. Experimental results on two knowledge-grounded dialogue datasets show that both PIPM and KDBTS achieve performance improvement over the state-of-the-art latent variable model and their combination shows further improvement."
2020.coling-main.392,Knowledge Aware Emotion Recognition in Textual Conversations via Multi-Task Incremental Transformer,2020,-1,-1,4,0,21483,duzhen zhang,Proceedings of the 28th International Conference on Computational Linguistics,0,"Emotion recognition in textual conversations (ERTC) plays an important role in a wide range of applications, such as opinion mining, recommender systems, and so on. ERTC, however, is a challenging task. For one thing, speakers often rely on the context and commonsense knowledge to express emotions; for another, most utterances contain neutral emotion in conversations, as a result, the confusion between a few non-neutral utterances and much more neutral ones restrains the emotion recognition performance. In this paper, we propose a novel Knowledge Aware Incremental Transformer with Multi-task Learning (KAITML) to address these challenges. Firstly, we devise a dual-level graph attention mechanism to leverage commonsense knowledge, which augments the semantic information of the utterance. Then we apply the Incremental Transformer to encode multi-turn contextual utterances. Moreover, we are the first to introduce multi-task learning to alleviate the aforementioned confusion and thus further improve the emotion recognition performance. Extensive experimental results show that our KAITML model outperforms the state-of-the-art models across five benchmark datasets."
2020.ccl-1.36,åºäº{BERT}çç«¯å°ç«¯ä¸­æç¯ç« äºä»¶æ½å(A {BERT}-based End-to-End Model for {C}hinese Document-level Event Extraction),2020,-1,-1,4,0,17794,hongkuan zhang,Proceedings of the 19th Chinese National Conference on Computational Linguistics,0,"ç¯ç« çº§äºä»¶æ½åç ç©¶ä»æ´ç¯ææ¡£ä¸­æ£æµäºä»¶,è¯å«åºäºä»¶å
å«çå
ç´ å¹¶èµäºæ¯ä¸ªå
ç´ ç¹å®çè§è²ãæ¬æéå¯¹éå®é¢åçä¸­æææ¡£æåºäºåºäºBERTçç«¯å°ç«¯æ¨¡å,å¨æ¨¡åçå
ç´ åè§è²è¯å«ä¸­ä¾æ¬¡å¼å
¥ååºå±è¾åºçäºä»¶ç±»åä»¥åå®ä½åµå
¥è¡¨ç¤º,å¢å¼ºææ¬çäºä»¶ãå
ç´ åè§è²å
³èè¡¨ç¤º,æé«ç¯ç« ä¸­åäºä»¶æå±å
ç´ çè¯å«ç²¾åº¦ãå¨æ­¤åºç¡ä¸å©ç¨æ é¢ä¿¡æ¯åäºä»¶äºå
ç»çåµå
¥å¼è¡¨ç¤º,å®ç°ä¸»ä»äºä»¶çåååå
ç´ èåãå®éªè¯ææ¬æçæ¹æ³ä¸ç°æå·¥ä½ç¸æ¯å
·æææ¾çæåã"
W19-5511,Transformer-Based Capsule Network For Stock Movement Prediction,2019,-1,-1,4,0,23821,jintao liu,Proceedings of the First Workshop on Financial Technology and Natural Language Processing,0,None
P19-1258,A Working Memory Model for Task-oriented Dialog Response Generation,2019,0,0,3,1,6632,xiuyi chen,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,1,"Recently, to incorporate external Knowledge Base (KB) information, one form of world knowledge, several end-to-end task-oriented dialog systems have been proposed. These models, however, tend to confound the dialog history with KB tuples and simply store them into one memory. Inspired by the psychological studies on working memory, we propose a working memory model (WMM2Seq) for dialog response generation. Our WMM2Seq adopts a working memory to interact with two separated long-term memories, which are the episodic memory for memorizing dialog history and the semantic memory for storing KB tuples. The working memory consists of a central executive to attend to the aforementioned memories, and a short-term storage system to store the {``}activated{''} contents from the long-term memories. Furthermore, we introduce a context-sensitive perceptual process for the token representations of dialog history, and then feed them into the episodic memory. Extensive experiments on two task-oriented dialog datasets demonstrate that our WMM2Seq significantly outperforms the state-of-the-art results in several evaluation metrics."
N19-1266,The World in My Mind: Visual Dialog with Adversarial Multi-modal Feature Encoding,2019,0,0,3,1,3905,yiqun yao,"Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",0,"Visual Dialog is a multi-modal task that requires a model to participate in a multi-turn human dialog grounded on an image, and generate correct, human-like responses. In this paper, we propose a novel Adversarial Multi-modal Feature Encoding (AMFE) framework for effective and robust auxiliary training of visual dialog systems. AMFE can force the language-encoding part of a model to generate hidden states in a distribution closely related to the distribution of real-world images, resulting in language features containing general knowledge from both modalities by nature, which can help generate both more correct and more general responses with reasonably low time cost. Experimental results show that AMFE can steadily bring performance gains to different models on different scales of data. Our method outperforms both the supervised learning baselines and other fine-tuning methods, achieving state-of-the-art results on most metrics of VisDial v0.5/v0.9 generative tasks."
P18-2024,Construction of a {C}hinese Corpus for the Analysis of the Emotionality of Metaphorical Expressions,2018,0,3,5,0,11732,dongyu zhang,Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"Metaphors are frequently used to convey emotions. However, there is little research on the construction of metaphor corpora annotated with emotion for the analysis of emotionality of metaphorical expressions. Furthermore, most studies focus on English, and few in other languages, particularly Sino-Tibetan languages such as Chinese, for emotion analysis from metaphorical texts, although there are likely to be many differences in emotional expressions of metaphorical usages across different languages. We therefore construct a significant new corpus on metaphor, with 5,605 manually annotated sentences in Chinese. We present an annotation scheme that contains annotations of linguistic metaphors, emotional categories (joy, anger, sadness, fear, love, disgust and surprise), and intensity. The annotation agreement analyses for multiple annotators are described. We also use the corpus to explore and analyze the emotionality of metaphors. To the best of our knowledge, this is the first relatively large metaphor corpus with an annotation of emotions in Chinese."
P18-1005,Unsupervised Neural Machine Translation with Weight Sharing,2018,24,8,4,1,20246,zhen yang,Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"Unsupervised neural machine translation (NMT) is a recently proposed approach for machine translation which aims to train the model without using any labeled data. The models proposed for unsupervised NMT often use only one shared encoder to map the pairs of sentences from different languages to a shared-latent space, which is weak in keeping the unique and internal characteristics of each language, such as the style, terminology, and sentence structure. To address this issue, we introduce an extension by utilizing two independent encoders but sharing some partial weights which are responsible for extracting high-level representations of the input sentences. Besides, two different generative adversarial networks (GANs), namely the local GAN and global GAN, are proposed to enhance the cross-language translation. With this new approach, we achieve significant improvements on English-German, English-French and Chinese-to-English translation tasks."
N18-1122,Improving Neural Machine Translation with Conditional Sequence Generative Adversarial Nets,2018,0,25,4,1,20246,zhen yang,"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",0,"This paper proposes an approach for applying GANs to NMT. We build a conditional sequence generative adversarial net which comprises of two adversarial sub models, a generator and a discriminator. The generator aims to generate sentences which are hard to be discriminated from human-translated sentences ( i.e., the golden target sentences); And the discriminator makes efforts to discriminate the machine-generated sentences from human-translated ones. The two sub models play a mini-max game and achieve the win-win situation when they reach a Nash Equilibrium. Additionally, the static sentence-level BLEU is utilized as the reinforced objective for the generator, which biases the generation towards high BLEU points. During training, both the dynamic discriminator and the static BLEU objective are employed to evaluate the generated sentences and feedback the evaluations to guide the learning of the generator. Experimental results show that the proposed model consistently outperforms the traditional RNNSearch and the newly emerged state-of-the-art Transformer on English-German and Chinese-English translation tasks."
D18-1118,Cascaded Mutual Modulation for Visual Reasoning,2018,22,1,4,1,3905,yiqun yao,Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,0,"Visual reasoning is a special visual question answering problem that is multi-step and compositional by nature, and also requires intensive text-vision interactions. We propose CMM: Cascaded Mutual Modulation as a novel end-to-end visual reasoning model. CMM includes a multi-step comprehension process for both question and image. In each step, we use a Feature-wise Linear Modulation (FiLM) technique to enable textual/visual pipeline to mutually control each other. Experiments show that CMM significantly outperforms most related models, and reach state-of-the-arts on two visual reasoning benchmarks: CLEVR and NLVR, collected from both synthetic and natural languages. Ablation studies confirm the effectiveness of CMM to comprehend natural language logics under the guidence of images. Our code is available at \url{https://github.com/FlamingHorizon/CMM-VR}."
D18-1272,{WECA}: A {W}ord{N}et-Encoded Collocation-Attention Network for Homographic Pun Recognition,2018,0,2,9,0,13501,yufeng diao,Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,0,"Homographic puns have a long history in human writing, widely used in written and spoken literature, which usually occur in a certain syntactic or stylistic structure. How to recognize homographic puns is an important research. However, homographic pun recognition does not solve very well in existing work. In this work, we first use WordNet to understand and expand word embedding for settling the polysemy of homographic puns, and then propose a WordNet-Encoded Collocation-Attention network model (WECA) which combined with the context weights for recognizing the puns. Our experiments on the SemEval2017 Task7 and Pun of the Day demonstrate that the proposed model is able to distinguish between homographic pun and non-homographic pun texts. We show the effectiveness of the model to present the capability of choosing qualitatively informative words. The results show that our model achieves the state-of-the-art performance on homographic puns recognition."
C18-1299,Semi-Supervised Disfluency Detection,2018,0,3,6,1,22593,feng wang,Proceedings of the 27th International Conference on Computational Linguistics,0,"While the disfluency detection has achieved notable success in the past years, it still severely suffers from the data scarcity. To tackle this problem, we propose a novel semi-supervised approach which can utilize large amounts of unlabelled data. In this work, a light-weight neural net is proposed to extract the hidden features based solely on self-attention without any Recurrent Neural Network (RNN) or Convolutional Neural Network (CNN). In addition, we use the unlabelled corpus to enhance the performance. Besides, the Generative Adversarial Network (GAN) training is applied to enforce the similar distribution between the labelled and unlabelled data. The experimental results show that our approach achieves significant improvements over strong baselines."
P17-1113,Joint Extraction of Entities and Relations Based on a Novel Tagging Scheme,2017,16,62,6,0,7370,suncong zheng,Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"Joint extraction of entities and relations is an important task in information extraction. To tackle this problem, we firstly propose a novel tagging scheme that can convert the joint extraction task to a tagging problem.. Then, based on our tagging scheme, we study different end-to-end models to extract entities and their relations directly, without identifying entities and relations separately. We conduct experiments on a public dataset produced by distant supervision method and the experimental results show that the tagging based methods are better than most of the existing pipelined and joint learning methods. What{'}s more, the end-to-end model proposed in this paper, achieves the best results on the public dataset."
I17-1017,Convolutional Neural Network with Word Embeddings for {C}hinese Word Segmentation,2017,36,1,2,0,30447,chunqi wang,Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers),0,"Character-based sequence labeling framework is flexible and efficient for Chinese word segmentation (CWS). Recently, many character-based neural models have been applied to CWS. While they obtain good performance, they have two obvious weaknesses. The first is that they heavily rely on manually designed bigram feature, i.e. they are not good at capturing $n$-gram features automatically. The second is that they make no use of full word information. For the first weakness, we propose a convolutional neural model, which is able to capture rich $n$-gram features without any feature engineering. For the second one, we propose an effective approach to integrate the proposed model with word embeddings. We evaluate the model on two benchmark datasets: PKU and MSR. Without any feature engineering, the model obtains competitive performance {---} 95.7{\%} on PKU and 97.3{\%} on MSR. Armed with word embeddings, the model achieves state-of-the-art performance on both datasets {---} 96.5{\%} on PKU and 98.0{\%} on MSR, without using any external labeled resource."
D17-1154,Towards Compact and Fast Neural Machine Translation Using a Combined Method,2017,19,6,5,0,33141,xiaowei zhang,Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing,0,"Neural Machine Translation (NMT) lays intensive burden on computation and memory cost. It is a challenge to deploy NMT models on the devices with limited computation and memory budgets. This paper presents a four stage pipeline to compress model and speed up the decoding for NMT. Our method first introduces a compact architecture based on convolutional encoder and weight shared embeddings. Then weight pruning is applied to obtain a sparse model. Next, we propose a fast sequence interpolation approach which enables the greedy decoding to achieve performance on par with the beam search. Hence, the time-consuming beam search can be replaced by simple greedy decoding. Finally, vocabulary selection is used to reduce the computation of softmax layer. Our final model achieves 10 times speedup, 17 times parameters reduction, less than 35MB storage size and comparable performance compared to the baseline model."
W16-4404,Combining Lexical and Semantic-based Features for Answer Sentence Selection,2016,-1,-1,5,0,33584,jing shi,Proceedings of the Open Knowledge Base and Question Answering Workshop ({OKBQA} 2016),0,"Question answering is always an attractive and challenging task in natural language processing area. There are some open domain question answering systems, such as IBM Waston, which take the unstructured text data as input, in some ways of humanlike thinking process and a mode of artificial intelligence. At the conference on Natural Language Processing and Chinese Computing (NLPCC) 2016, China Computer Federation hosted a shared task evaluation about Open Domain Question Answering. We achieve the 2nd place at the document-based subtask. In this paper, we present our solution, which consists of feature engineering in lexical and semantic aspects and model training methods. As the result of the evaluation shows, our solution provides a valuable and brief model which could be used in modelling question answering or sentence semantic relevance. We hope our solution would contribute to this vast and significant task with some heuristic thinking."
P16-2034,Attention-Based Bidirectional Long Short-Term Memory Networks for Relation Classification,2016,25,257,7,0,22992,peng zhou,Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"Relation classification is an important semantic processing task in the field of natural language processing (NLP). State-ofthe-art systems still rely on lexical resources such as WordNet or NLP systems like dependency parser and named entity recognizers (NER) to get high-level features. Another challenge is that important information can appear at any position in the sentence. To tackle these problems, we propose Attention-Based Bidirectional Long Short-Term Memory Networks(AttBLSTM) to capture the most important semantic information in a sentence. The experimental results on the SemEval-2010 relation classification task show that our method outperforms most of the existing methods, with only word vectors."
C16-1216,Hierarchical Memory Networks for Answer Selection on Unknown Words,2016,18,2,5,1,25689,jiaming xu,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers",0,"Recently, end-to-end memory networks have shown promising results on Question Answering task, which encode the past facts into an explicit memory and perform reasoning ability by making multiple computational steps on the memory. However, memory networks conduct the reasoning on sentence-level memory to output coarse semantic vectors and do not further take any attention mechanism to focus on words, which may lead to the model lose some detail information, especially when the answers are rare or unknown words. In this paper, we propose a novel Hierarchical Memory Networks, dubbed HMN. First, we encode the past facts into sentence-level memory and word-level memory respectively. Then, $k$-max pooling is exploited following reasoning module on the sentence-level memory to sample the $k$ most relevant sentences to a question and feed these sentences into attention mechanism on the word-level memory to focus the words in the selected sentences. Finally, the prediction is jointly learned over the outputs of the sentence-level reasoning module and the word-level attention mechanism. The experimental results demonstrate that our approach successfully conducts answer selection on unknown words and achieves a better performance than memory networks."
C16-1288,A Character-Aware Encoder for Neural Machine Translation,2016,4,7,4,1,20246,zhen yang,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers",0,"This article proposes a novel character-aware neural machine translation (NMT) model that views the input sequences as sequences of characters rather than words. On the use of row convolution (Amodei et al., 2015), the encoder of the proposed model composes word-level information from the input sequences of characters automatically. Since our model doesn{'}t rely on the boundaries between each word (as the whitespace boundaries in English), it is also applied to languages without explicit word segmentations (like Chinese). Experimental results on Chinese-English translation tasks show that the proposed character-aware NMT model can achieve comparable translation performance with the traditional word based NMT models. Despite the target side is still word based, the proposed model is able to generate much less unknown words."
C16-1329,Text Classification Improved by Integrating Bidirectional {LSTM} with Two-dimensional Max Pooling,2016,0,97,6,0,22992,peng zhou,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers",0,"Recurrent Neural Network (RNN) is one of the most popular architectures used in Natural Language Processsing (NLP) tasks because its recurrent structure is very suitable to process variable-length text. RNN can utilize distributed representations of words by first converting the tokens comprising each text into vectors, which form a matrix. And this matrix includes two dimensions: the time-step dimension and the feature vector dimension. Then most existing models usually utilize one-dimensional (1D) max pooling operation or attention-based operation only on the time-step dimension to obtain a fixed-length vector. However, the features on the feature vector dimension are not mutually independent, and simply applying 1D pooling operation over the time-step dimension independently may destroy the structure of the feature representation. On the other hand, applying two-dimensional (2D) pooling operation over the two dimensions may sample more meaningful features for sequence modeling tasks. To integrate the features on both dimensions of the matrix, this paper explores applying 2D max pooling operation to obtain a fixed-length representation of the text. This paper also utilizes 2D convolution to sample more meaningful information of the matrix. Experiments are conducted on six text classification tasks, including sentiment analysis, question classification, subjectivity classification and newsgroup classification. Compared with the state-of-the-art models, the proposed models achieve excellent performance on 4 out of 6 tasks. Specifically, one of the proposed models achieves highest accuracy on Stanford Sentiment Treebank binary classification and fine-grained classification tasks."
W15-4648,Dialogue Management based on Multi-domain Corpus,2015,28,4,2,0,36639,wendong ge,Proceedings of the 16th Annual Meeting of the Special Interest Group on Discourse and Dialogue,0,"Dialogue Management (DM) is a key issue in Spoken Dialogue System. Most of the existing data-driven DM schemes train the dialogue policy for some specific domain (or vertical domain), only using the dialogue corpus in this domain, which might suffer from the scarcity of dialogue corpus in some domains. In this paper, we divide Dialogue Act (DA), as semantic representation of utterance, into DA type and slot parameter, where the former one is domain-independent and the latter one is domain-specific. Firstly, based on multiple-domain dialogue corpus, the DA type prediction model is trained via Recurrent Neutral Networks (RNN). Moreover, DA type decision problem is modeled as a multi-order POMDP, and transformed to be a one-order MDP with continuous states, which is solved by Natural Actor Critic (NAC) algorithm and applicable for every domain. Furthermore, a slot parameter selection scheme is designed to generate a complete machine DA according to the features of specific domain, which yields the Multi-domain Corpus based Dialogue Management (MCDM) scheme. Finally, extensive experimental results illustrate the performance improvement of the MCDM scheme, compared with the existing schemes."
W15-1509,Short Text Clustering via Convolutional Neural Networks,2015,9,47,4,1,25689,jiaming xu,Proceedings of the 1st Workshop on Vector Space Modeling for Natural Language Processing,0,None
P15-2058,Semantic Clustering and Convolutional Neural Network for Short Text Categorization,2015,23,85,3,0,7039,peng wang,Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers),0,"Short texts usually encounter data sparsity and ambiguity problems in representations for their lack of context. In this paper, we propose a novel method to model short texts based on semantic clustering and convolutional neural network. Particularly, we first discover semantic cliques in embedding spaces by a fast clustering algorithm. Then, multi-scale semantic units are detected under the supervision of semantic cliques, which introduce useful external knowledge for short texts. These meaningful semantic units are combined and fed into convolutional layer, followed by max-pooling operation. Experimental results on two open benchmarks validate the effectiveness of the proposed method."
P15-2131,Dialogue Management based on Sentence Clustering,2015,16,4,2,0,36639,wendong ge,Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers),0,"Dialogue Management (DM) is a key issue in Spoken Dialogue System (SDS). Most of the existing studies on DM use Dialogue Act (DA) to represent semantic information of sentence, which might not represent the nuanced meaning sometimes. In this paper, we model DM based on sentence clusters which have more powerful semantic representation ability than DAs. Firstly, sentences are clustered not only based on the internal information such as words and sentence structures, but also based on the external information such as context in dialogue via Recurrent Neural Networks. Additionally, the DM problem is modeled as a Partially Observable Markov Decision Processes (POMDP) with sentence clusters. Finally, experimental results illustrate that the proposed DM scheme is superior to the existing one."
D15-1142,Semi-supervised {C}hinese Word Segmentation based on Bilingual Information,2015,26,2,2,1,3727,wei chen,Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,0,"This paper presents a bilingual semisupervised Chinese word segmentation (CWS) method that leverages the natural segmenting information of English sentences. The proposed method involves learning three levels of features, namely, character-level, phrase-level and sentence-level, provided by multiple submodels. We use a sub-model of conditional random fields (CRF) to learn monolingual grammars, a sub-model based on character-based alignment to obtain explicit segmenting knowledge, and another sub-model based on transliteration similarity to detect out-of-vocabulary (OOV) words. Moreover, we propose a sub-model leveraging neural network to ensure the proper treatment of the semantic gap and a phrase-based translation sub-model to score the translation probability of the Chinese segmentation and its corresponding English sentences. A cascaded log-linear model is employed to combine these features to segment bilingual unlabeled data, the results of which are used to justify the original supervised CWS model. The evaluation shows that our method results in superior results compared with those of the state-of-the-art monolingual and bilingual semi-supervised models that have been reported in the literature."
P14-1012,Learning New Semi-Supervised Deep Auto-encoder Features for Statistical Machine Translation,2014,32,13,3,1,39168,shixiang lu,Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"In this paper, instead of designing new features based on intuition, linguistic knowledge and domain, we learn some new and effective features using the deep autoencoder (DAE) paradigm for phrase-based translation model. Using the unsupervised pre-trained deep belief net (DBN) to initialize DAExe2x80x99s parameters and using the input original phrase features as a teacher for semi-supervised fine-tuning, we learn new semi-supervised DAE features, which are more effective and stable than the unsupervised DBN features. Moreover, to learn high dimensional feature representation, we introduce a natural horizontal composition of more DAEs for large hidden layers feature learning. On two ChineseEnglish tasks, our semi-supervised DAE features obtain statistically significant improvements of 1.34/2.45 (IWSLT) and 0.82/1.52 (NIST) BLEU points over the unsupervised DBN features and the baseline features, respectively."
I13-1129,Phrase-based Parallel Fragments Extraction from Comparable Corpora,2013,17,3,5,0,41706,xiaoyin fu,Proceedings of the Sixth International Joint Conference on Natural Language Processing,0,"We present a phrase-based method to extract parallel fragments from the comparable corpora. We do this by introducing a force decoder based on the hierarchical phrase-based (HPB) translation model to detect the alignments in comparable sentence pairs. This method enables us to extract useful training data for statistical machine translation (SMT) system. We evaluate our method by fragment detection and large-scale translation tasks, which show that our method can effectively extract parallel fragments and improve the performance of the state-of-the-art SMT system."
2013.iwslt-evaluation.25,The {CASIA} machine translation system for {IWSLT} 2013,2013,-1,-1,6,0,41956,xingyuan peng,Proceedings of the 10th International Workshop on Spoken Language Translation: Evaluation Campaign,0,"In this paper, we describe the CASIA statistical machine translation (SMT) system for the IWSLT2013 Evaluation Campaign. We participated in the Chinese-English and English-Chinese translation tasks. For both of these tasks, we used a hierarchical phrase-based (HPB) decoder and made it as our baseline translation system. A number of techniques were proposed to deal with these translation tasks, including parallel sentence extraction, pre-processing, translation model (TM) optimization, language model (LM) interpolation, turning, and post-processing. With these techniques, the translation results were significantly improved compared with that of the baseline system."
P12-1006,Automated Essay Scoring Based on Finite State Transducer: towards {ASR} Transcription of Oral {E}nglish Speech,2012,25,1,3,0,41956,xingyuan peng,Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"Conventional Automated Essay Scoring (AES) measures may cause severe problems when directly applied in scoring Automatic Speech Recognition (ASR) transcription as they are error sensitive and unsuitable for the characteristic of ASR transcription. Therefore, we introduce a framework of Finite State Transducer (FST) to avoid the shortcomings. Compared with the Latent Semantic Analysis with Support Vector Regression (LSA-SVR) method (stands for the conventional measures), our FST method shows better performance especially towards the ASR transcription. In addition, we apply the synonyms similarity to expand the FST model. The final scoring performance reaches an acceptable level of 0.80 which is only 0.07 lower than the correlation (0.87) between human raters."
D12-1047,Translation Model Based Cross-Lingual Language Model Adaptation: from Word Models to Phrase Models,2012,35,5,4,1,39168,shixiang lu,Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,0,"In this paper, we propose a novel translation model (TM) based cross-lingual data selection model for language model (LM) adaptation in statistical machine translation (SMT), from word models to phrase models. Given a source sentence in the translation task, this model directly estimates the probability that a sentence in the target LM training corpus is similar. Compared with the traditional approaches which utilize the first pass translation hypotheses, cross-lingual data selection model avoids the problem of noisy proliferation. Furthermore, phrase TM based cross-lingual data selection model is more effective than the traditional approaches based on bag-of-words models and word-based TM, because it captures contextual information in modeling the selection of phrase as a whole. Experiments conducted on large-scale data sets demonstrate that our approach significantly outperforms the state-of-the-art approaches on both LM perplexity and SMT performance."
2011.mtsummit-papers.45,Effective Use of Discontinuous Phrases for Hierarchical Phrase-based Translation,2011,-1,-1,2,1,6909,wei wei,Proceedings of Machine Translation Summit XIII: Papers,0,None
D07-1098,Probabilistic Parsing Action Models for Multi-Lingual Dependency Parsing,2007,15,20,3,0,8136,xiangyu duan,Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning ({EMNLP}-{C}o{NLL}),0,"Deterministic dependency parsers use parsing actions to construct dependencies. These parsers do not compute the probability of the whole dependency tree. They only determine parsing actions stepwisely by a trained classifier. To globally model parsing actions of all steps that are taken on the input sentence, we propose two kinds of probabilistic parsing action models that can compute the probability of the whole dependency tree. The tree with the maximal probability is outputted. The experiments are carried on 10 languages, and the results show that our probabilistic parsing action models outperform the original deterministic dependency parser."
W06-0108,Cluster-Based Language Model for Sentence Retrieval in {C}hinese Question Answering,2006,14,1,3,1,4600,youzheng wu,Proceedings of the Fifth {SIGHAN} Workshop on {C}hinese Language Processing,0,"Sentence retrieval plays a very important role in question answering system. In this paper, we present a novel cluster-based language model for sentence retrieval in Chinese question answering which is motivated in part by sentence clustering and language model. Sentence clustering is used to group sentences into clusters. Language model is used to properly represent sentences, which is combined with sentences model, cluster/topic model and collection model. For sentence clustering, we propose two approaches that are OneSentence-Multi-Topics and OneSentence-One-Topic respectively. From the experimental results on 807 Chinese testing questions, we can conclude that the proposed cluster-based language model outperforms over the standard language model for sentence retrieval in Chinese question answering."
O06-2004,Robust Target Speaker Tracking in Broadcast {TV} Streams,2006,18,0,5,0,50015,junmei bai,"International Journal of Computational Linguistics {\\&} {C}hinese Language Processing, Volume 11, Number 1, March 2006: Special Issue on Human Computer Speech Processing",0,"This paper addresses the problem of audio change detection and speaker tracking in broadcast TV streams. A two-pass audio change detection algorithm, which includes detection of the potential change boundaries and refinement, is proposed. Speaker tracking is performed based on the results of speaker change detection. In speaker tracking, Wiener filtering, endpoint detection of pitch, and segmental cepstral feature normalization are applied to obtain a more reliable result. The algorithm has low complexity. Our experiments show that the algorithm achieves very satisfactory results."
O06-2005,A Fast Framework for the Constrained Mean Trajectory Segment Model by Avoidance of Redundant Computation on Segment,2006,11,3,4,0,5768,yun tang,"International Journal of Computational Linguistics {\\&} {C}hinese Language Processing, Volume 11, Number 1, March 2006: Special Issue on Human Computer Speech Processing",0,"The segment model (SM) is a family of methods that use the segmental distribution rather than frame-based density (e.g. HMM) to represent the underlying characteristics of the observation sequence. It has been proved to be more precise than HMM. However, their high level of complexity prevents these models from being used in practical systems. In this paper, we propose a framework that can reduce the computational complexity of the Constrained Mean Trajectory Segment Model (CMTSM), one type of SM, by fixing the number of regions in a segment so as to share the intermediate computation results. Our work is twofold. First, we compare the complexity of SM with that of HMM and point out the source of the complexity in SM. Secondly, a fast CMTSM framework is proposed, and two examples are used to illustrate this framework. The fast CMTSM achieves a 95.0% string accurate rate in the speaker-independent test on our mandarin digit string data corpus, which is much higher than the performance obtained with HMM-based system. At the mean time, we successfully keep the computation complexity of SM at the same level as that of HMM."
I05-3006,Product Named Entity Recognition Based on Hierarchical Hidden {M}arkov Model,2005,12,12,4,0,26515,feifan liu,Proceedings of the Fourth {SIGHAN} Workshop on {C}hinese Language Processing,0,"A hierarchical hidden Markov model (HHMM) based approach of product named entity recognition (NER) from Chinese free text is presented in this paper. Characteristics and challenges in product NER is also investigated and analyzed deliberately compared with general NER. Within a unified statistical framework, the approach we proposed is able to make probabilistically reasonable decisions to a global optimization by leveraging diverse range of linguistic features and knowledge sources. Experimental results show that our approach performs quite well in two different domains."
H05-1054,{C}hinese Named Entity Recognition with Multiple Features,2005,0,0,3,1,4600,youzheng wu,Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing,0,None
2005.mtsummit-invited.8,Phrase-Based Statistical Machine Translation for {MANOS} System,2005,-1,-1,1,1,3729,bo xu,Proceedings of Machine Translation Summit X: Invited papers,0,"MANOS (Multilingual Application Network for Olympic Services) project. aims to provide intelligent multilingual information services in 2008 Olympic Games. By narrowing down the general language technology, this paper gives an overview of our new work on Phrase-Based Statistical Machine Translation (PBT) under the framework of the MANOS. Starting with the construction of large scale Chinese-English corpus (sentence aligned) and introduction four methods to extract phrases, The promising results from PBT systems lead us to confidences for constructing a high-quality translation system and harmoniously integrate it into MANOS platform."
2005.iwslt-1.14,The {CASIA} Phrase-Based Machine Translation System,2005,0,5,5,0,27672,wei pang,Proceedings of the Second International Workshop on Spoken Language Translation,0,None
W03-1509,{C}hinese Named Entity Recognition Combining Statistical Model wih Human Knowledge,2003,12,38,3,1,4600,youzheng wu,Proceedings of the {ACL} 2003 Workshop on Multilingual and Mixed-language Named Entity Recognition,0,"Named Entity Recognition is one of the key techniques in the fields of natural language processing, information retrieval, question answering and so on. Unfortunately, Chinese Named Entity Recognition (NER) is more difficult for the lack of capitalization information and the uncertainty in word segmentation. In this paper, we present a hybrid algorithm which can combine a class-based statistical model with various types of human knowledge very well. In order to avoid data sparseness problem, we employ a back-off model and [Abstract contained text which could not be captured.], a Chinese thesaurus, to smooth the parameters in the model. The F-measure of person names, location names, and organization names on the newswire test data for the 1999 IEER evaluation in Mandarin is 86.84%, 84.40% and 76.22% respectively."
W02-0709,Interactive {C}hinese-to-{E}nglish Speech Translation Based on Dialogue Management,2002,14,6,2,0,6630,chengqing zong,Proceedings of the {ACL}-02 Workshop on Speech-to-Speech Translation: Algorithms and Systems,0,"In this paper, we propose a novel paradigm for the Chinese-to-English speech-to-speech (S2S) translation, which is interactive under the guidance of dialogue management. In this approach, the input utterance is first pre-processed and then serially translated by the template-based translator and the inter-lingua based translator. The dialogue management mechanism (DMM) is employed to supervise the interactive analysis for disambiguation of the input. The interaction is led by the system, so the system always acts on its own initiative in the interactive procedure. In this approach, the complicated semantic analysis is not involved."
W02-0226,Bridging the Gap between Dialogue management and dialogue models,2002,51,20,2,0,37031,weiqun xu,Proceedings of the Third {SIG}dial Workshop on Discourse and Dialogue,0,Why do few working spoken dialogue systems make use of dialogue models in their dialogue management? We find out the causes and propose a generic dialogue model. It promises to bridge the gap between practical dialogue management and (pattern-based) dialogue model through integrating interaction patterns with the underling tasks and modeling interaction patterns via utterance groups using a high level construct different from dialogue act.
C02-2028,{C}hinese Syntactic Parsing Based on Extended {GLR} Parsing Algorithm with {PCFG}*,2002,6,0,2,0,10073,yan zhang,{COLING} 2002: The 17th International Conference on Computational Linguistics: Project Notes,0,This paper presents an extended GLR parsing algorithm with grammar PCFG* that is based on Tomita's GLR parsing algorithm and extends it further. We also define a new grammar---PCFG* that is based on PCFG and assigns not only probability but also frequency associated with each rule. So our syntactic parsing system is implemented based on rule-based approach and statistics approach. Furthermore our experiments are executed in two fields: Chinese base noun phrase identification and full syntactic parsing. And the results of these two fields are compared from three ways. The experiments prove that the extended GLR parsing algorithm with PCFG* is an efficient parsing method and a straightforward way to combine statistical property with rules. The experiment results of these two fields are presented in this paper.
