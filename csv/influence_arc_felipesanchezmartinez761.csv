2004.tmi-1.15,2001.mtsummit-papers.14,0,0.0444576,"Missing"
2004.tmi-1.15,A92-1018,0,0.581067,"languages are refined by using the statistical data supplied by the current HMM for the other language. Both models bootstrap by learning cooperatively in an unsupervised manner and require only monolingual texts; no aligned texts are needed. Preliminary results are promising and surpass those of traditional unsupervised approaches. 1 Introduction One of the main sources of errors in machine translation (MT) systems, specially for related languages, is the incorrect resolution of part-of-speech (PoS) ambiguities. Hidden Markov models (HMMs, Rabiner 1989) are the standard statistical approach (Cutting et al. 1992) to automatic PoS tagging. Typically, unsupervised training of this kind of taggers has been carried out from source-language (SL) untagged corpora (see below in this introduction) using the Baum-Welch algorithm (Rabiner 1989). But target-language (TL) information may also be taken into account in order to improve the performance of these PoS taggers, specially as to the resulting translation quality, an aspect not addressed by training algorithms which use information from the SL only. Statistics from the TL might be a source of useful information as well, an idea which will be explored in th"
2004.tmi-1.15,N01-1026,0,0.0370061,"w to train the two corresponding PoS taggers simultaneously in a single iterative process using cooperative learning. A brief overview of our proposal follows: consider a system translating between languages A and B. At every iteration, the existing HMM for A is refined by using B as TL and estimating the likelihood of the translations of each of the segments in A with the current HMM for B. Then, the roles are interchanged and the existing HMM for B is updated correspondingly. The resulting algorithm is based on a model of TL tags —instead of words— and still works in an unsupervised manner. Yarowsky & Ngai (2001) proposed a method which also uses information from TL in order to train PoS taggers. They consider information from aligned parallel corpora and from (at least) one manually tagged corpus for the TL. Our method, however, needs neither aligned parallel corpora nor manually tagged texts. Most current MT systems follow the indirect or transfer approach (Hutchins & Somers 1992, ch. 4): SL text is analysed and converted into an intermediate representation which becomes the basis for generating the corresponding TL text. Analysis modules usually include a PoS tagger for the SL. Our method for train"
2005.eamt-1.12,2001.mtsummit-papers.14,0,0.89117,"hat has to be done for each pattern EAMT 2005 Conference Proceedings An open-source shallow-transfer machine translation engine for the Romance languages of Spain (much like in languages such as perl or lex). Using a declarative notation such as XML is rather straightforward for the pattern part of rules but using it for the action (procedural) part means stretching it a bit; we have, however, found a reasonable way to translate the ad-hoc C-style action language used in the corresponding module of interNOSTRUM and Traductor Universia, which was defined in detail in Garrido-Alenda and Forcada (2001), into a simple XML notation having the same expressiveness. In this way, we follow as close as possible the declarative approach used in the XML files defining the linguistic data used for the tagger and for the lexical processing modules. 3.4. De-formatter and re-formatter The current de-formatter and re-formatter used in Traductor Universia and interNOSTRUM are different for each of the three formats supported (plain ISO-8859-1 text, HTML and RTF). Their behaviour is specified following a pattern–action scheme, with patterns specified as regular expressions and actions written in C code, us"
2005.eamt-1.12,carreras-etal-2004-freeling,0,0.106391,"Missing"
2005.eamt-1.12,2002.tmi-papers.7,1,0.882427,"Missing"
2005.mtsummit-osmtw.4,2001.mtsummit-papers.14,0,0.411613,"Missing"
2005.mtsummit-osmtw.4,2005.eamt-1.12,1,0.86182,"Missing"
2005.mtsummit-osmtw.4,2005.eamt-1.13,0,0.0322622,"Missing"
2005.mtsummit-osmtw.4,2002.tmi-papers.7,1,0.825583,"Missing"
2005.mtsummit-osmtw.4,sheremetyeva-nirenburg-2000-towards,0,0.0203702,"Missing"
2007.tmi-papers.22,2004.eamt-1.14,0,0.0254192,"Missing"
2007.tmi-papers.22,1999.tmi-1.3,0,0.0354785,"o languages. The parallel corpus is obtained by translating a controlled corpus from a “major” language (English or Spanish) to a “minor” language by means of an elicitation tool. This tool is also used to graphically annotate the word alignments between the two sentences. Finally, hierarchical syntactic rules, that can be seen as constituting a context-free transfer grammar, are inferred from the aligned parallel corpus. On the other hand, in the EBMT framework, some researchers deal with the problem of inferring the kinds of translation rules called translation templates (Kaji et al., 1992; Brown, 1999; Cicekli and G¨ uvenir, 2001). A translation template can be deﬁned as a bilingual pair of sentences in which corresponding units (words or phrases) are coupled and replaced by variables. Liu and Zong (2004) provide an interesting review of the diﬀerent research works dealing with translation templates. Brown (1999) uses a parallel corpus and some linguistic knowledge in the form of equivalence classes (both syntactic and semantic) to perform a generalization over the bilingual examples collected. The method works by replacing each word by its corresponding equivalence class and then using a"
2007.tmi-papers.22,A92-1018,0,0.0268649,"Missing"
2007.tmi-papers.22,P96-1043,0,0.0391791,"part-of-speech and inﬂection information provided by the TL word class; • the bilingual phrase the AT comes from cannot be reproduced by the MT system in which the transfer rules will be used. This happens when the translation equivalent (in the bilingual dictionary) diﬀers from that in the bilingual phrase extracted from the corpus. • if the word class corresponds to a lexicalized word, it is introduced as is; remember that word classes belonging to lexicalized words store complete lexical forms consisting of lemma, part-of-speech and inﬂection information. 10 A similar approach was used by Mikheev (1996) in his work on learning part-of-speech guessing rules to prioritize longer suﬃxes over shorter ones. Note that the information about SL lexicalized words is not taken into account when generating the code for a given AT. 187 Lang. es ca # sent. 100 834 100 834 # words 1 952 317 2 032 925 Trans. dir. es-ca ca-es Table 1: Number of sentences and words in the Spanish–Catalan parallel corpus used for training. Example of AT application. The following example illustrates how the AT shown in ﬁgure 3 would be applied to translate from Spanish to Catalan the input text vivieron en Francia.11 This tex"
2007.tmi-papers.22,J03-1002,0,0.0049987,"used in SMT, perform a generalization over bilingual phrase pairs using word classes instead of words. An AT z = (Sm , Tn , A) consists of a sequence Sm of m SL word classes, a sequence Tn of n TL word classes, and a set of pairs A = {(i, j) : i ∈ [1, n] ∧ j ∈ [1, m]} with the alignment information between TL and SL word classes. Learning a set of ATs from a parallel corpus consists of: 1. the computation of the word alignments, 2. the extraction of bilingual phrase pairs, and 3. the substitution of each word by its corresponding word class. Word alignments. A variety of methods, statistical (Och and Ney, 2003) or heuristic (Caseli et al., 2005), may be used to compute word alignments from a (sentence aligned) parallel corpus. For our experiments (section 6) we have used the open-source GIZA++ toolkit5 in the following way. First, standard GIZA++ training runs to estimate translation models to translate from language L1 to language L2 , and vice versa. Then, from the training corpus, Viterbi alignments6 A1 and A2 are obtained (one for each translation 5 http://www.fjoch.com/GIZA++.html The Viterbi alignment between source and target sentences is deﬁned as the alignment whose probability is maximal u"
2007.tmi-papers.22,J04-4002,0,0.0410588,"ces (Arnold, 2003) such as structural transfer rules. In this paper we focus on the automatic inference of structural transfer rules from parallel corpora, which are small compared to the size of corpora commonly used to build SMT or (some) EBMT systems. The approach we present is tested on the shallow transfer MT platform Apertium for which structural transfer rules are generated. Overview. In rule-based MT, transfer rules are needed to perform syntactic and lexical changes. The approach we present in this paper to infer shallow-transfer MT rules is based on the alignment templates approach (Och and Ney, 2004) already used in SMT (see section 3). An alignment template (AT) can be deﬁned as a generalization performed over aligned phrase1 pairs (or translation units) by using word classes. The method we present is entirely unsupervised and needs, in addition to the linguistic data used by the MT system in which the inferred rules are used, only a (comparatively) small parallel corpus and a ﬁle deﬁning a reduced set of lexical categories usually involved in lexical changes. S´anchez-Mart´ınez and Ney (2006) use ATs to infer shallow-transfer rules to be used in 1 For the purpose of this paper, with phr"
2007.tmi-papers.22,W99-0604,0,0.0567839,". 33):7 • ﬁrst the intersection A = A1 ∩ A2 of both alignments is computed, then • the alignment A is iteratively extended with alignments (i, j) ∈ A1 or (i, j) ∈ A2 if neither SL word wSj nor TL word wTi has an alignment in A, or the following two conditions hold: 1. One of the following (neighboring) alignments (i − 1, j), (i + 1, j), (i, j − 1), (i, j + 1) is already in A. 2. The new alignment A ∪ {(i, j)} does not contain any alignment with both horizontal ((i − 1, j), (i + 1, j)) and vertical ((i, j − 1), (i, j + 1)) neighbors. Bilingual phrase pairs. The extraction of bilingual phrases (Och et al., 1999) is performed by considering all possible pairs within a certain length and ensuring that (see ﬁgure 2): 1. all words are consecutive, and 2. words within the bilingual phrase are not aligned with words from outside. 7 For easier understanding, think about the alignment information as a binary matrix (see ﬁgure 2). 184 The set of bilingual phrases that are extracted from the word-aligned sentence pair (wS1 , . . . , wSJ ), (wT1 , . . . , wTI ) can be formally expressed as follows: BP (wS J1 , wT I1 , A) = {(wS j+m , wT i+n ): j i ∀(i0 , j 0 ) ∈ A : j ≤ j 0 ≤ j + m ⇔ i ≤ i0 ≤ i + n}. Generaliza"
2007.tmi-papers.22,C92-2101,0,\N,Missing
2009.eamt-1.20,J93-2003,0,0.0190121,"Missing"
2009.eamt-1.20,W09-0416,1,0.819247,"ot the first approach that has used the Marker Hypothesis in MT. While most work has centred on building EBMT systems which relate source and target phrase pairs comprised of words (e.g. (Juola, 1994; Way and Gough, 2003; Gough and Way, 2 2004; Groves and Way, 2005a)), systems have also been successfully constructed where phrases consist of word–morpheme mappings (e.g. (Stroppa et al., 2006) for English–Basque, and (Labaka et al., 2007) for Spanish–Basque). Marker-based chunking still plays a significant role in the M AT R E X (Stroppa and Way, 2006; Hassan et al., 2007; Tinsley et al., 2008; Du et al., 2009) system, where the sets of marker words needed for bilingual chunking are extracted automatically rather than by assembling these by hand as its predecessors did (e.g. (Way and Gough, 2003; Gough and Way, 2004)). To give the reader some idea of how marker words are used in practice in such systems, we revisit an example from (Groves and Way, 2005a), namely (2) (from (Koehn, 2005), Figure 2): While these results do not appear in the paper, they were included in the presentation accompanying that paper. Thanks to Yanjun Ma for this clarification. . Once 7 sets of closed-class words (determiners,"
2009.eamt-1.20,2005.mtsummit-papers.30,0,0.0384191,"Missing"
2009.eamt-1.20,2004.tmi-1.11,1,0.905113,"“Marker Hypothesis” (Green, 1979) (cf. section 3). Essentially, we use linguistic information in the form of closedclass word lists to filter the set of bilingual phrase pairs, by taking into account the alignment information and the type of the words involved in the alignments. The inspiration for the set of experiments carried out in this paper was that successful ExampleBased MT (EBMT) systems (Nagao, 1984; Carl and Way, 2003) have been built using the Marker Hypothesis to segment source–target aligned sentence pairs into linguistically motivated bilingual chunks (cf. (Way and Gough, 2003; Gough and Way, 2004)). These systems have proven to be particularly useful where good translation performance is required with much smaller translation tables than are traditionally used in PB-SMT. For example, Groves and Way (2005a) showed that for a range of systems built with different amounts of data, on average the translation table of a PB-SMT system was about five times the size of the equivalent EBMT system. In a related paper, on a training set of 203K English–French aligned sentence pairs, Groves and Way (2005b) showed that seeding a PB-SMT system built using Pharaoh (Koehn, 2004a) with 403,317 EBMT ali"
2009.eamt-1.20,W05-0833,1,0.961125,"nt information and the type of the words involved in the alignments. The inspiration for the set of experiments carried out in this paper was that successful ExampleBased MT (EBMT) systems (Nagao, 1984; Carl and Way, 2003) have been built using the Marker Hypothesis to segment source–target aligned sentence pairs into linguistically motivated bilingual chunks (cf. (Way and Gough, 2003; Gough and Way, 2004)). These systems have proven to be particularly useful where good translation performance is required with much smaller translation tables than are traditionally used in PB-SMT. For example, Groves and Way (2005a) showed that for a range of systems built with different amounts of data, on average the translation table of a PB-SMT system was about five times the size of the equivalent EBMT system. In a related paper, on a training set of 203K English–French aligned sentence pairs, Groves and Way (2005b) showed that seeding a PB-SMT system built using Pharaoh (Koehn, 2004a) with 403,317 EBMT alignments, a BLEU score of 36.43 was obtained, compared to a score of 37.53 with 1,732,715 phrase pairs built using Giza++ (Och and Ney, 2003). The remainder of this paper is organised as follows. Section 2 review"
2009.eamt-1.20,D07-1103,0,0.0498626,"Missing"
2009.eamt-1.20,koen-2004-pharaoh,0,0.0421452,"d Gough, 2003; Gough and Way, 2004)). These systems have proven to be particularly useful where good translation performance is required with much smaller translation tables than are traditionally used in PB-SMT. For example, Groves and Way (2005a) showed that for a range of systems built with different amounts of data, on average the translation table of a PB-SMT system was about five times the size of the equivalent EBMT system. In a related paper, on a training set of 203K English–French aligned sentence pairs, Groves and Way (2005b) showed that seeding a PB-SMT system built using Pharaoh (Koehn, 2004a) with 403,317 EBMT alignments, a BLEU score of 36.43 was obtained, compared to a score of 37.53 with 1,732,715 phrase pairs built using Giza++ (Och and Ney, 2003). The remainder of this paper is organised as follows. Section 2 reviews other research work that has also focused on the filtering of the bilingual phrase pairs. Then, in section 3 we describe our approach. Section 4 describes the experiments conducted on four language pairs and the results achieved. The paper ends with our concluding remarks together with avenues for further research. 2 Related Work Previous approaches to filter t"
2009.eamt-1.20,W04-3250,0,0.0556677,"d Gough, 2003; Gough and Way, 2004)). These systems have proven to be particularly useful where good translation performance is required with much smaller translation tables than are traditionally used in PB-SMT. For example, Groves and Way (2005a) showed that for a range of systems built with different amounts of data, on average the translation table of a PB-SMT system was about five times the size of the equivalent EBMT system. In a related paper, on a training set of 203K English–French aligned sentence pairs, Groves and Way (2005b) showed that seeding a PB-SMT system built using Pharaoh (Koehn, 2004a) with 403,317 EBMT alignments, a BLEU score of 36.43 was obtained, compared to a score of 37.53 with 1,732,715 phrase pairs built using Giza++ (Och and Ney, 2003). The remainder of this paper is organised as follows. Section 2 reviews other research work that has also focused on the filtering of the bilingual phrase pairs. Then, in section 3 we describe our approach. Section 4 describes the experiments conducted on four language pairs and the results achieved. The paper ends with our concluding remarks together with avenues for further research. 2 Related Work Previous approaches to filter t"
2009.eamt-1.20,2005.mtsummit-papers.11,0,0.0700668,"., 2006) for English–Basque, and (Labaka et al., 2007) for Spanish–Basque). Marker-based chunking still plays a significant role in the M AT R E X (Stroppa and Way, 2006; Hassan et al., 2007; Tinsley et al., 2008; Du et al., 2009) system, where the sets of marker words needed for bilingual chunking are extracted automatically rather than by assembling these by hand as its predecessors did (e.g. (Way and Gough, 2003; Gough and Way, 2004)). To give the reader some idea of how marker words are used in practice in such systems, we revisit an example from (Groves and Way, 2005a), namely (2) (from (Koehn, 2005), Figure 2): While these results do not appear in the paper, they were included in the presentation accompanying that paper. Thanks to Yanjun Ma for this clarification. . Once 7 sets of closed-class words (determiners, quantifiers, conjunctions, prepositions, whadverbs, possessive and personal pronouns, cf. (6) below) have been built for English and French, the marker words in (2) can be tagged, as in (3): &lt;DET&gt; that is almost &lt;DET&gt; a personal record &lt;PREP&gt; for &lt;PRON&gt; me &lt;DET&gt; this autumn! −→&lt;DET&gt; c’ est pratiquement &lt;DET&gt; un record personnel &lt;PREP&gt; pour &lt;PRON&gt; moi , &lt;DET&gt; cet automne! (3) The"
2009.eamt-1.20,N03-1017,0,0.0926979,"e show that our simple yet novel approach can filter the phrase table by up to a third yet still provide competitive results compared to the baseline. Furthermore, it provides a nice balance between the unfiltered approach and pruning using stop words, where the deterioration in translation quality is unacceptably high. 1 (1) ): , tj+m BP(sI1 , tJ1 , A) = {(si+n j i 0 0 0 ∀(i , j ) ∈ A : i ≤ i ≤ i + n ⇔ j ≤ j 0 ≤ j + m}, Introduction The state-of-the-art statistical approach to machine translation (MT) is the phrase-based model. Phrase-based statistical MT (PB-SMT) systems (Zens et al., 2002; Koehn et al., 2003) are based on the log linear model combination of several feature functions (Och and Ney, 2002), one c 2009 European Association for Machine Translation. where A = {(i, j) : i ∈ [1, I] ∧ j ∈ [1, J]} is a set of pairs with the alignment information between the words in the source sentence sI1 and the words in the target sentence tJ1 . According to equation (1), all words within a bilingual phrase pair are consecutive and not aligned with words from outside the bilingual phrase pair. It is worth noting that bilingual phrase pairs may contain words that are not aligned at all, even at the beginni"
2009.eamt-1.20,P07-2045,0,0.0145693,". We experimented with this second criterion because, as a result of the bilingual phrase pairs extraction algorithm, unaligned words may appear at the beginning or the end of a phrase, and we wanted to test whether this introduces any noise in the translation table. Note that, after extracting the set of bilingual phrase pairs from a word-aligned sentence pair, two or more bilingual phrase pairs may only differ in that some of them contain unaligned words at the beginning or the end of the phrases. Experiments All the experiments were performed using the Moses open-source decoder for PB-SMT (Koehn et al., 2007) and the SRILM language modelling toolkit (Stolcke, 2002). Training was carried out as follows: 1. Word alignments were obtained using Giza++ (Och and Ney, 2003) and symmetrized in the usual way (Koehn et al., 2003). 2. Bilingual phrase pairs were extracted from the word-aligned sentence pairs. 3. Extracted phrase pairs were filtered following the approach presented in this paper, and then scored. 4. Weights were optimised using minimum error rate training (MERT) in the usual manner (Och, 2003). With the two filtering criteria explained in Section 3, we tested different lists of words: closed"
2009.eamt-1.20,2007.mtsummit-papers.40,1,0.818689,"79), which states that the syntactic structure of a language is marked at the surface level by a closed set of marker (closed) words. As stated in the introduction, this paper is not the first approach that has used the Marker Hypothesis in MT. While most work has centred on building EBMT systems which relate source and target phrase pairs comprised of words (e.g. (Juola, 1994; Way and Gough, 2003; Gough and Way, 2 2004; Groves and Way, 2005a)), systems have also been successfully constructed where phrases consist of word–morpheme mappings (e.g. (Stroppa et al., 2006) for English–Basque, and (Labaka et al., 2007) for Spanish–Basque). Marker-based chunking still plays a significant role in the M AT R E X (Stroppa and Way, 2006; Hassan et al., 2007; Tinsley et al., 2008; Du et al., 2009) system, where the sets of marker words needed for bilingual chunking are extracted automatically rather than by assembling these by hand as its predecessors did (e.g. (Way and Gough, 2003; Gough and Way, 2004)). To give the reader some idea of how marker words are used in practice in such systems, we revisit an example from (Groves and Way, 2005a), namely (2) (from (Koehn, 2005), Figure 2): While these results do not ap"
2009.eamt-1.20,D07-1036,0,0.0415314,"Missing"
2009.eamt-1.20,2007.tmi-papers.14,1,0.88022,"Missing"
2009.eamt-1.20,P03-1021,0,0.0230491,"eriments All the experiments were performed using the Moses open-source decoder for PB-SMT (Koehn et al., 2007) and the SRILM language modelling toolkit (Stolcke, 2002). Training was carried out as follows: 1. Word alignments were obtained using Giza++ (Och and Ney, 2003) and symmetrized in the usual way (Koehn et al., 2003). 2. Bilingual phrase pairs were extracted from the word-aligned sentence pairs. 3. Extracted phrase pairs were filtered following the approach presented in this paper, and then scored. 4. Weights were optimised using minimum error rate training (MERT) in the usual manner (Och, 2003). With the two filtering criteria explained in Section 3, we tested different lists of words: closed words: A list of closed words in each language is provided to the filtering algorithm. These lists contain determiners, prepositions, pronouns, coordinate and subordinate conjunctions, relative and possessive pronouns, and punctuation marks. They consist of 193 Spanish words, 174 French words and 185 English words. Examples include those in (6): 3 147 http://www.statmt.org/wmt09/ In all cases the baseline system, i.e. when no filtering of the bilingual phrase pairs is done, performs better than"
2009.eamt-1.20,P02-1038,0,0.0580039,"provide competitive results compared to the baseline. Furthermore, it provides a nice balance between the unfiltered approach and pruning using stop words, where the deterioration in translation quality is unacceptably high. 1 (1) ): , tj+m BP(sI1 , tJ1 , A) = {(si+n j i 0 0 0 ∀(i , j ) ∈ A : i ≤ i ≤ i + n ⇔ j ≤ j 0 ≤ j + m}, Introduction The state-of-the-art statistical approach to machine translation (MT) is the phrase-based model. Phrase-based statistical MT (PB-SMT) systems (Zens et al., 2002; Koehn et al., 2003) are based on the log linear model combination of several feature functions (Och and Ney, 2002), one c 2009 European Association for Machine Translation. where A = {(i, j) : i ∈ [1, I] ∧ j ∈ [1, J]} is a set of pairs with the alignment information between the words in the source sentence sI1 and the words in the target sentence tJ1 . According to equation (1), all words within a bilingual phrase pair are consecutive and not aligned with words from outside the bilingual phrase pair. It is worth noting that bilingual phrase pairs may contain words that are not aligned at all, even at the beginning or the end of the phrase. In order to make the extraction of bilingual phrase pairs computat"
2009.eamt-1.20,J03-1002,0,0.0131914,"anslation tables than are traditionally used in PB-SMT. For example, Groves and Way (2005a) showed that for a range of systems built with different amounts of data, on average the translation table of a PB-SMT system was about five times the size of the equivalent EBMT system. In a related paper, on a training set of 203K English–French aligned sentence pairs, Groves and Way (2005b) showed that seeding a PB-SMT system built using Pharaoh (Koehn, 2004a) with 403,317 EBMT alignments, a BLEU score of 36.43 was obtained, compared to a score of 37.53 with 1,732,715 phrase pairs built using Giza++ (Och and Ney, 2003). The remainder of this paper is organised as follows. Section 2 reviews other research work that has also focused on the filtering of the bilingual phrase pairs. Then, in section 3 we describe our approach. Section 4 describes the experiments conducted on four language pairs and the results achieved. The paper ends with our concluding remarks together with avenues for further research. 2 Related Work Previous approaches to filter the phrase pairs used in PB-SMT can be divided into two classes: • those methods that filter the phrase table according to the text to be translated; • those more ge"
2009.eamt-1.20,P02-1040,0,0.0828818,"Missing"
2009.eamt-1.20,2006.amta-papers.25,0,0.0587861,"Missing"
2009.eamt-1.20,2006.iwslt-evaluation.4,1,0.867377,"ker (closed) words. As stated in the introduction, this paper is not the first approach that has used the Marker Hypothesis in MT. While most work has centred on building EBMT systems which relate source and target phrase pairs comprised of words (e.g. (Juola, 1994; Way and Gough, 2003; Gough and Way, 2 2004; Groves and Way, 2005a)), systems have also been successfully constructed where phrases consist of word–morpheme mappings (e.g. (Stroppa et al., 2006) for English–Basque, and (Labaka et al., 2007) for Spanish–Basque). Marker-based chunking still plays a significant role in the M AT R E X (Stroppa and Way, 2006; Hassan et al., 2007; Tinsley et al., 2008; Du et al., 2009) system, where the sets of marker words needed for bilingual chunking are extracted automatically rather than by assembling these by hand as its predecessors did (e.g. (Way and Gough, 2003; Gough and Way, 2004)). To give the reader some idea of how marker words are used in practice in such systems, we revisit an example from (Groves and Way, 2005a), namely (2) (from (Koehn, 2005), Figure 2): While these results do not appear in the paper, they were included in the presentation accompanying that paper. Thanks to Yanjun Ma for this cla"
2009.eamt-1.20,2006.amta-papers.26,1,0.849286,"ng is known as the Marker Hypothesis (Green, 1979), which states that the syntactic structure of a language is marked at the surface level by a closed set of marker (closed) words. As stated in the introduction, this paper is not the first approach that has used the Marker Hypothesis in MT. While most work has centred on building EBMT systems which relate source and target phrase pairs comprised of words (e.g. (Juola, 1994; Way and Gough, 2003; Gough and Way, 2 2004; Groves and Way, 2005a)), systems have also been successfully constructed where phrases consist of word–morpheme mappings (e.g. (Stroppa et al., 2006) for English–Basque, and (Labaka et al., 2007) for Spanish–Basque). Marker-based chunking still plays a significant role in the M AT R E X (Stroppa and Way, 2006; Hassan et al., 2007; Tinsley et al., 2008; Du et al., 2009) system, where the sets of marker words needed for bilingual chunking are extracted automatically rather than by assembling these by hand as its predecessors did (e.g. (Way and Gough, 2003; Gough and Way, 2004)). To give the reader some idea of how marker words are used in practice in such systems, we revisit an example from (Groves and Way, 2005a), namely (2) (from (Koehn, 2"
2009.eamt-1.20,W08-0326,1,0.84189,"Missing"
2009.eamt-1.20,2002.tmi-tutorials.2,0,0.403263,"cu.ie Felipe S´anchez-Mart´ınez Dept. Llenguatges i Sistemes Inform`atics Universitat d’Alacant E-03071 Alacant, Spain fsanchez@dlsi.ua.es Abstract of which is the phrase translation probability estimated after extracting bilingual phrase pairs from the parallel corpus.1 Bilingual phrase pairs are automatically extracted after computing the word alignments (Brown et al., 1993; Och and Ney, 2003). The set BP(sJ1 , tI1 , A) of bilingual phrase pairs extracted from the word-aligned sentence pair sI1 = (s1 , . . . , si , . . . , sI ) and tJ1 = (t1 , . . . , tj , . . . , tJ ) is defined as in (1) (Zens et al., 2002): State-of-the-art statistical machine translation systems make use of a large translation table obtained after scoring a set of bilingual phrase pairs automatically extracted from a parallel corpus. The number of bilingual phrase pairs extracted from a pair of aligned sentences grows exponentially as the length of the sentences increases; therefore, the number of entries in the phrase table used to carry out the translation may become unmanageable, especially when online, ‘on demand’ translation is required in real time. We describe the use of closed-class words to filter the set of bilingual"
2009.eamt-1.20,2007.iwslt-1.4,0,\N,Missing
2009.eamt-1.20,W10-1720,1,\N,Missing
2009.freeopmt-1.11,A00-1031,0,0.330604,"utive PoS tags. A sequence of three PoS tags is forbidden if it matches any extended forbid rule or does not match an extended enforce rule or if any of the two consecutive tag pairs in the sequence of three PoS tags matches a bigram forbid rule or fails to match a bigram enforce rule. 3.1.1 3 A second-order HMM-based PoS tagger In this section we describe our implementation of a trigram PoS tagger based on secondorder hidden Markov models (HMM). Trigram taggers based on second-order HMMs have already been implemented in various projects, the most well-know implementations are the TnT tagger (Brants, 2000) and its open-source alternative, HunPos (Hal´acsy et al., 2007).4 There is also an implementation of a trigram tagger in Acopost (A Collection Of Open Source PoS Taggers).5 3.1 Our implementation In a second-order HMM the transition probability matrix is three dimensional, as the probability 4 5 http://code.google.com/p/hunpos/ http://acopost.sourceforge.net Assumptions 1. The text sequence O1 . . . OT being disambiguated is always preceded by two unambiguous word O−1 = {I} and O0 = {I}. 2. The text to disambiguate is ended by two unambiguous words OT = {E1 } and OT +1 = {E2 }. 3. All word cl"
2009.freeopmt-1.11,W95-0201,0,0.0643099,"es even more acute in the case of a trigram tagger. To avoid null probabilities for those stateto-state transitions and output emissions that have not been seen in the training corpus, we employ a form of deleted interpolation (Jelinek, 1997) 71 for parameter smoothing in which weighted estimates are taken from second-order and first-order models and a uniform probability distribution. The smoothed trigram probabilities consist of a linear combination of trigram and bigram probabilities where the values of the smoothing coefficients are computed using the successive linear abstraction method (Brants and Samuelsson, 1995). 3.1.5 Viterbi algorithm The Viterbi algorithm for a second-order HMM, as described by Thede and Harper (1999) is used to disambiguate a sentence using the model we have trained above. It can be applied to text segments delimited by two unambiguous words. 4 Experiments We studied the PoS tagging performance of the second-order HMM-based PoS tagger on the Spanish language when it is trained both in a supervised way from hand-tagged corpora and in an unsupervised way from untagged corpora of different sizes. We compare the tagging accuracy achieved with that achieved by the first-order HMM-base"
2009.freeopmt-1.11,A92-1018,0,0.68957,"result of the PoS tag assigned to a word. For instance, in the translation into Spanish of the English text the green house, where green can be either a noun or an adjective, a reordering rule det+adj+noun → det+noun+adj is only applied if PoS tag adjective is assigned to word green. 2. Because the translation differs depending on how the PoS ambiguity is solved; e.g., the translation into Spanish of chair is silla when it is tagged as noun, and presidir when it is tagged as verb. The Apertium free/open-source RBMT platform implements a first-order hidden Markov model (HMM)-based PoS tagger (Cutting et al., 1992; Rabiner, 1989). In this paper we describe in detail the implementation of a second-order HMM-based PoS tagger for Apertium and we provide a comparative study of the tagging accuracy both when using the first-order HMM-based and the second-order HMM-based PoS taggers. J.A. P´ erez-Ortiz, F. S´ anchez-Mart´ınez, F.M. Tyers (eds.) Proceedings of the First International Workshop on Free/Open-Source Rule-Based Machine Translation, p. 67–74 Alacant, Spain, November 2009 The next section overviews the Apertium free/open-source RBMT platform with emphasis on the tagging approach it implements. Then"
2009.freeopmt-1.11,P07-2053,0,0.0502901,"Missing"
2009.freeopmt-1.11,J94-2001,0,0.059447,"nd a common step in many naturallanguage processing applications such as rulebased machine translation (RBMT). A PoS tagger attempts to assign the correct PoS tag or lexical category to all words of a given text, usually by relying on the assumption that a word can be assigned a single PoS tag by looking at the PoS tags of the neighbouring words. In RBMT PoS tags are usually assigned to words by looking them up in a lexicon, or by usFelipe S´anchez-Mart´ınez Dept. Llenguatges i Sistemes Inform`atics Universitat d’Alacant E-03071 Alacant, Spain fsanchez@dlsi.ua.es ing a morphological analyser (Merialdo, 1994). A large portion of the words found in a text are unambiguous since they can be assigned only a single PoS tag; however, there are ambiguous words that can be assigned more than one PoS tag. For instance, the word chair can be, as many other English words, either a noun or a verb. The choice of the correct PoS tag may be crucial when translating to another language. Mainly there are two reasons why a translation can be wrong due to PoS tagging errors: 1. Because some structural transformations are applied, or not, as a result of the PoS tag assigned to a word. For instance, in the translation"
2009.freeopmt-1.11,P99-1023,0,0.0481965,"and output emissions that have not been seen in the training corpus, we employ a form of deleted interpolation (Jelinek, 1997) 71 for parameter smoothing in which weighted estimates are taken from second-order and first-order models and a uniform probability distribution. The smoothed trigram probabilities consist of a linear combination of trigram and bigram probabilities where the values of the smoothing coefficients are computed using the successive linear abstraction method (Brants and Samuelsson, 1995). 3.1.5 Viterbi algorithm The Viterbi algorithm for a second-order HMM, as described by Thede and Harper (1999) is used to disambiguate a sentence using the model we have trained above. It can be applied to text segments delimited by two unambiguous words. 4 Experiments We studied the PoS tagging performance of the second-order HMM-based PoS tagger on the Spanish language when it is trained both in a supervised way from hand-tagged corpora and in an unsupervised way from untagged corpora of different sizes. We compare the tagging accuracy achieved with that achieved by the first-order HMM-based PoS tagger in Apertium. In addition, we report the tagging accuracy both when using forbid and enforce rules"
2009.freeopmt-1.11,H90-1056,0,0.121373,"n unsupervised way from untagged corpora via the Baum-Welch expectation-maximisation (EM) algorithm (Baum, 1972). In an untagged corpus each word has been assigned the set of all possible PoS tags that it can receive; untagged corpora can be easily obtained if a morphological analyser is available. Backward probabilities. The backward function is defined as the probability of the partial observation sequence from t + 1 to T , given transition si sj at times t − 1, t and the model λ : 2. Training the model in a supervised manner with hand-tagged corpora via the maximumlikelihood estimate (MLE; Gale and Church 1990). In a hand-tagged corpus all ambiguities have been manually solved; thus, this is a direct method to estimate the HMM parameters that consists of collecting frequency counts from the training corpus and using these counts to estimate the HMM parameters. βT +1 (i, j) = 1 3.1.3 Baum-Welch EM algorithm The Baum-Welch algorithm is an iterative algorithm that works by giving the highest probability to the state transitions and output emissions used the most. After each Baum-Welch iteration the new HMM parameters may be shown to give a higher probability to the observed sequence (Baum, 1972). A des"
2011.eamt-1.13,J96-1002,0,0.0362507,"words in s0 and it should be changed (see Figure 1). with one or more words in si that are matched with Note that wij may not be aligned with any word words in s0 , and, at the same time, it is aligned with 82 one or more unmatched words in si . In the experiments we have tried two ways of dealing with this, one that requires all SL words in si to be matched, and another one that only requires the majority of words aligned with wij to be matched. These strategies have been chosen because of their simplicity, although it could also be possible to use, for example, a maximum entropy classifier (Berger et al., 1996), in order to determine which words should be changed or kept unedited. In that case, fK would be one of the features used by the maximum entropy classifier. To illustrate these ideas, Figure 2 shows an example of a word-aligned pair of segments (si and ti ) and a segment s0 to be translated. As can be seen, the word he in ti is aligned with the word e´ l in si , which does not match with any word in s0 . Therefore, he should be marked to be changed. Conversely, the words his and brother are aligned with su and hermano, respectively, which are matched in s0 and, therefore should be kept unedit"
2011.eamt-1.13,J93-2003,0,0.0322612,"ems, we have chosen a fuzzy-match score function based on the Levenshtein distance (Levenshtein, 1966): score(s0 , si ) = 1 − a su hermano D(s0 , si ) max(|s0 |, |si |) where |x |stands for the length (in words) of string x and D(x, y) refers to the word-based Levenshtein distance (edit distance) between x and y. a su hermano Figure 2: Example of alignment and matching. 3.2 Corpora For the experiments in this paper we have used word alignments obtained by means of the free/open-source GIZA++1 tool (Och and Ney, 2003) which implements standard word-based statistical machine translation models (Brown et al., 1993) as well as a hidden-Markov-model-based alignment model (Vogel et al., 1996). GIZA++ produces alignments in which a source word can be aligned with many target words, whereas a target word is aligned with, at most, one source word. Following common practice in statistical machine translation (Koehn, 2010, Ch. 4) we have obtained The TMs we have used were extracted from the JRC-Acquis corpus version 3 (Steinberger et al., 2006),2 which contains the total body of European Union (EU) law. Before extracting the TMs used, this corpus was tokenized and lowercased, and then segment pairs in which eit"
2011.eamt-1.13,N03-1017,0,0.0214063,"ord would be marked neither as “keep” nor as “change”. Otherwise, if the criterion of majority is applied, the word would be marked to be changed. ti : si : [edit] he [?] missed J  J J  e´ l ech´o de menos s0 : ella ech´o de casa [keep] [keep] his brother a set of symmetric word alignments by running GIZA++ in both translation directions, and then symmetrizing both sets of alignments. In the experiments we have tried the following symmetrization methods: • the union of both sets of alignments, • the intersection of the two alignment sets, and • the use of the grow-diag-final-and heuristic (Koehn et al., 2003) as implemented in Moses (Koehn et al., 2007). 3 Experimental settings We have tested our approach in the translation of Spanish texts into English by using two TMs: TMtrans and TMtest . Evaluation was carried out by simulating the translation of the SL segments in TMtrans by using the TUs in TMtest . We firstly obtained the word alignments between the parallel segments of TMtest by training and running GIZA++ on the TM itself. Then, for each source segment in TMtrans , we obtained the TUs in TMtest having a fuzzy-match score above threshold Θ, and tagged the words in their target segments as"
2011.eamt-1.13,P07-2045,0,0.00745878,"“change”. Otherwise, if the criterion of majority is applied, the word would be marked to be changed. ti : si : [edit] he [?] missed J  J J  e´ l ech´o de menos s0 : ella ech´o de casa [keep] [keep] his brother a set of symmetric word alignments by running GIZA++ in both translation directions, and then symmetrizing both sets of alignments. In the experiments we have tried the following symmetrization methods: • the union of both sets of alignments, • the intersection of the two alignment sets, and • the use of the grow-diag-final-and heuristic (Koehn et al., 2003) as implemented in Moses (Koehn et al., 2007). 3 Experimental settings We have tested our approach in the translation of Spanish texts into English by using two TMs: TMtrans and TMtest . Evaluation was carried out by simulating the translation of the SL segments in TMtrans by using the TUs in TMtest . We firstly obtained the word alignments between the parallel segments of TMtest by training and running GIZA++ on the TM itself. Then, for each source segment in TMtrans , we obtained the TUs in TMtest having a fuzzy-match score above threshold Θ, and tagged the words in their target segments as “keep” or “change”. 3.1 Fuzzy-match score fun"
2011.eamt-1.13,J10-4005,0,0.0221968,". a su hermano Figure 2: Example of alignment and matching. 3.2 Corpora For the experiments in this paper we have used word alignments obtained by means of the free/open-source GIZA++1 tool (Och and Ney, 2003) which implements standard word-based statistical machine translation models (Brown et al., 1993) as well as a hidden-Markov-model-based alignment model (Vogel et al., 1996). GIZA++ produces alignments in which a source word can be aligned with many target words, whereas a target word is aligned with, at most, one source word. Following common practice in statistical machine translation (Koehn, 2010, Ch. 4) we have obtained The TMs we have used were extracted from the JRC-Acquis corpus version 3 (Steinberger et al., 2006),2 which contains the total body of European Union (EU) law. Before extracting the TMs used, this corpus was tokenized and lowercased, and then segment pairs in which either of the segments was empty or had more than 9 times words than its counterpart were removed. Finally, segments longer than 40 words (and their corresponding counterparts) were removed because of the inability of GIZA++ to align longer segments. 1 2 http://code.google.com/p/giza-pp/ 83 http://wt.jrc.it"
2011.eamt-1.13,kranias-samiotou-2004-automatic,0,0.652111,"anslation spotting consists of identifying, for a pair of parallel sentences, the words or phrases in a TL segment that correspond to the words in a SL segment. The work by Bourdaillet et al. (2009) follows a similar approach, although it does not focus on traditional TM-based CAT systems, but Mikel L. Forcada, Heidi Depraetere, Vincent Vandeghinste (eds.) Proceedings of the 15th Conference of the European Association for Machine Translation, p. 8188 Leuven, Belgium, May 2011 on the use of a bilingual concordancer to assist professional translators. More similar to our approach is the one by Kranias and Samiotou (2004) which is implemented on the ESTeam CAT system. Kranias and Samiotou (2004) align the source and target segments in each TU at different sub-segment levels by using a bilingual dictionary (Meyers et al., 1998), and then use these alignments to (i) identify the sub-segments in a translation proposal ti that need to be changed, and (ii) propose a machine translation for them. In this paper we propose a different way of using word alignments in a TM-based CAT system to alleviate the task of professional translators. The main difference between our approach and those previously described is that i"
2011.eamt-1.13,meyers-etal-1998-multilingual,0,0.560273,"approach, although it does not focus on traditional TM-based CAT systems, but Mikel L. Forcada, Heidi Depraetere, Vincent Vandeghinste (eds.) Proceedings of the 15th Conference of the European Association for Machine Translation, p. 8188 Leuven, Belgium, May 2011 on the use of a bilingual concordancer to assist professional translators. More similar to our approach is the one by Kranias and Samiotou (2004) which is implemented on the ESTeam CAT system. Kranias and Samiotou (2004) align the source and target segments in each TU at different sub-segment levels by using a bilingual dictionary (Meyers et al., 1998), and then use these alignments to (i) identify the sub-segments in a translation proposal ti that need to be changed, and (ii) propose a machine translation for them. In this paper we propose a different way of using word alignments in a TM-based CAT system to alleviate the task of professional translators. The main difference between our approach and those previously described is that in our approach word alignments are used only to recommend the words to be changed or kept unedited, without proposing a translation for them, so that the user can focus on choosing a translation where words ha"
2011.eamt-1.13,J03-1002,0,0.0566822,"e changed or kept unedited. Related work. In the literature one can find different approaches that use word or phrase alignments to improve existing TM-based CAT systems; although, to our knowledge, none of them use word alignments for the purpose we study in this paper. Simard (2003) focuses on the creation of TMbased CAT systems able to work at the sub-segment level by proposing as translation sub-segments extracted from longer segments in the matching TUs. To do this, he implements the translation spotting (V´eronis and Langlais, 2000) technique by using statistical word-alignment methods (Och and Ney, 2003); translation spotting consists of identifying, for a pair of parallel sentences, the words or phrases in a TL segment that correspond to the words in a SL segment. The work by Bourdaillet et al. (2009) follows a similar approach, although it does not focus on traditional TM-based CAT systems, but Mikel L. Forcada, Heidi Depraetere, Vincent Vandeghinste (eds.) Proceedings of the 15th Conference of the European Association for Machine Translation, p. 8188 Leuven, Belgium, May 2011 on the use of a bilingual concordancer to assist professional translators. More similar to our approach is the one"
2011.eamt-1.13,W03-0313,0,0.0294151,"do so, we pre-process the user’s TM to compute the word alignments between the source and target segments in each TU. Then, when a new segment s0 is to be translated, the TUs with a fuzzy-match score above the threshold Θ are obtained and the alignment between the words in si and ti are used to mark which words in ti should be changed or kept unedited. Related work. In the literature one can find different approaches that use word or phrase alignments to improve existing TM-based CAT systems; although, to our knowledge, none of them use word alignments for the purpose we study in this paper. Simard (2003) focuses on the creation of TMbased CAT systems able to work at the sub-segment level by proposing as translation sub-segments extracted from longer segments in the matching TUs. To do this, he implements the translation spotting (V´eronis and Langlais, 2000) technique by using statistical word-alignment methods (Och and Ney, 2003); translation spotting consists of identifying, for a pair of parallel sentences, the words or phrases in a TL segment that correspond to the words in a SL segment. The work by Bourdaillet et al. (2009) follows a similar approach, although it does not focus on tradit"
2011.eamt-1.13,steinberger-etal-2006-jrc,0,0.102721,"Missing"
2011.eamt-1.13,C96-2141,0,0.238788,"stance (Levenshtein, 1966): score(s0 , si ) = 1 − a su hermano D(s0 , si ) max(|s0 |, |si |) where |x |stands for the length (in words) of string x and D(x, y) refers to the word-based Levenshtein distance (edit distance) between x and y. a su hermano Figure 2: Example of alignment and matching. 3.2 Corpora For the experiments in this paper we have used word alignments obtained by means of the free/open-source GIZA++1 tool (Och and Ney, 2003) which implements standard word-based statistical machine translation models (Brown et al., 1993) as well as a hidden-Markov-model-based alignment model (Vogel et al., 1996). GIZA++ produces alignments in which a source word can be aligned with many target words, whereas a target word is aligned with, at most, one source word. Following common practice in statistical machine translation (Koehn, 2010, Ch. 4) we have obtained The TMs we have used were extracted from the JRC-Acquis corpus version 3 (Steinberger et al., 2006),2 which contains the total body of European Union (EU) law. Before extracting the TMs used, this corpus was tokenized and lowercased, and then segment pairs in which either of the segments was empty or had more than 9 times words than its counte"
2011.eamt-1.15,C02-1134,0,0.027936,"(Thurmair, 2009), always provide the best results. This c 2011 European Association for Machine Translation. is why some researchers have investigated the development of multi-engine MT (MEMT) systems (Eisele, 2005; Macherey and Och, 2007; Du et al., 2009; Du et al., 2010) aimed to provide translations of higher quality than those produced by the isolated MT systems in which they are based on. MEMT systems can be classified according to how they work. On one hand, we find systems that combine the translations provided by several MT systems into a consensus translation (Bangalore et al., 2001; Bangalore et al., 2002; Matusov et al., 2006; Heafield et al., 2009; Du et al., 2009; Du et al., 2010); the output of these MEMT systems may differ from those provided by the individual MT systems they are based on. On the other hand, we have systems that decide which translation, among all the translations computed by the MT systems they are based on, is the most appropriate one (Nomoto, 2004; Zwarts and Dras, 2008) and output this translation without changing it in any way. In-between, we find the MEMT systems that build a consensus translation from a reduced set of translations, i.e. systems that first chose the"
2011.eamt-1.15,J96-1002,0,0.0116388,"minate between the rule-based MT systems and the corpus-based ones. To find the set of relevant features we have used the chi-square method (Liu and Setiono, 1995) that evaluates features individually. We ranked all the features according to their chi-squared statistic (DeGroot and Schervish, 2002, Sec. 7.2) with respect to the classes and select the first N features in the ranking. To determine the best value of N we evaluated the translation performance achieved on a development corpus with all possible values of N . Training. For each MT system used we have trained a maximum entropy model (Berger et al., 1996) that will allow our system to compute for an input sentence the probability of that sentence being best translated by each system. In order to train these classifiers, and for each different evaluation measure we have tried, each parallel sentence in the training corpus is preprocessed as follows: 3. all the machine translated sentences are ranked according to the evaluation scores obtained, and the subset of MT system producing the best translation are determined; note that it may happen that several MT systems produce the same translation, or that several machine translated sentences are as"
2011.eamt-1.15,J07-2003,0,0.0284276,"The systems finally selected to translate the input sentence are the ones with the highest probabilities. In this papers we have tested this approach by selecting only a single MT system, the one with the highest probability. 3 We have tested our approach in the translation of English and French texts into Spanish. The systems we have used are: the shallow-transfer rule-based MT system A PERTIUM (Forcada et al., 2011),2 the rule-based MT system S YSTRAN (Surcin et al., 2007),3 the phrase-based statistical MT system M OSES (Koehn et al., 2007),4 the M OSES - CHART hierarchical phrase-based MT (Chiang, 2007) system, and the hybrid example-based–statistical MT system C UNEI (Phillips and Brown, 2009).5 The three corpus-based systems, namely M OSES, M OSES - CHART and C UNEI, were trained using the data set released as part of the WMT10 shared translation task.6 The corpora used to train and evaluate the five binary maximum entropy classifiers were 2 1. the SL sentence is translated into the TL through all the MT systems; 2. each translation is evaluated against the reference translation in the training parallel corpus; 99 Experimental settings and resources http://www.apertium.org We have used the"
2011.eamt-1.15,2009.mtsummit-posters.7,0,0.0121113,"ls in assimilation —to get the gist of a text written in a language the reader does not understand— and dissemination —to produce a draft translation to be post-edited for publication— tasks. However, none of the different approaches to MT, whether statistical (Koehn, 2010), example-based (Carl and Way, 2003), rulebased (Hutchins and Somers, 1992) or hybrid (Thurmair, 2009), always provide the best results. This c 2011 European Association for Machine Translation. is why some researchers have investigated the development of multi-engine MT (MEMT) systems (Eisele, 2005; Macherey and Och, 2007; Du et al., 2009; Du et al., 2010) aimed to provide translations of higher quality than those produced by the isolated MT systems in which they are based on. MEMT systems can be classified according to how they work. On one hand, we find systems that combine the translations provided by several MT systems into a consensus translation (Bangalore et al., 2001; Bangalore et al., 2002; Matusov et al., 2006; Heafield et al., 2009; Du et al., 2009; Du et al., 2010); the output of these MEMT systems may differ from those provided by the individual MT systems they are based on. On the other hand, we have systems that"
2011.eamt-1.15,W10-1742,0,0.0145026,"n —to get the gist of a text written in a language the reader does not understand— and dissemination —to produce a draft translation to be post-edited for publication— tasks. However, none of the different approaches to MT, whether statistical (Koehn, 2010), example-based (Carl and Way, 2003), rulebased (Hutchins and Somers, 1992) or hybrid (Thurmair, 2009), always provide the best results. This c 2011 European Association for Machine Translation. is why some researchers have investigated the development of multi-engine MT (MEMT) systems (Eisele, 2005; Macherey and Och, 2007; Du et al., 2009; Du et al., 2010) aimed to provide translations of higher quality than those produced by the isolated MT systems in which they are based on. MEMT systems can be classified according to how they work. On one hand, we find systems that combine the translations provided by several MT systems into a consensus translation (Bangalore et al., 2001; Bangalore et al., 2002; Matusov et al., 2006; Heafield et al., 2009; Du et al., 2009; Du et al., 2010); the output of these MEMT systems may differ from those provided by the individual MT systems they are based on. On the other hand, we have systems that decide which tran"
2011.eamt-1.15,W05-0828,0,0.0236489,"viable technology that helps individuals in assimilation —to get the gist of a text written in a language the reader does not understand— and dissemination —to produce a draft translation to be post-edited for publication— tasks. However, none of the different approaches to MT, whether statistical (Koehn, 2010), example-based (Carl and Way, 2003), rulebased (Hutchins and Somers, 1992) or hybrid (Thurmair, 2009), always provide the best results. This c 2011 European Association for Machine Translation. is why some researchers have investigated the development of multi-engine MT (MEMT) systems (Eisele, 2005; Macherey and Och, 2007; Du et al., 2009; Du et al., 2010) aimed to provide translations of higher quality than those produced by the isolated MT systems in which they are based on. MEMT systems can be classified according to how they work. On one hand, we find systems that combine the translations provided by several MT systems into a consensus translation (Bangalore et al., 2001; Bangalore et al., 2002; Matusov et al., 2006; Heafield et al., 2009; Du et al., 2009; Du et al., 2010); the output of these MEMT systems may differ from those provided by the individual MT systems they are based on"
2011.eamt-1.15,W09-0408,0,0.0156768,"lts. This c 2011 European Association for Machine Translation. is why some researchers have investigated the development of multi-engine MT (MEMT) systems (Eisele, 2005; Macherey and Och, 2007; Du et al., 2009; Du et al., 2010) aimed to provide translations of higher quality than those produced by the isolated MT systems in which they are based on. MEMT systems can be classified according to how they work. On one hand, we find systems that combine the translations provided by several MT systems into a consensus translation (Bangalore et al., 2001; Bangalore et al., 2002; Matusov et al., 2006; Heafield et al., 2009; Du et al., 2009; Du et al., 2010); the output of these MEMT systems may differ from those provided by the individual MT systems they are based on. On the other hand, we have systems that decide which translation, among all the translations computed by the MT systems they are based on, is the most appropriate one (Nomoto, 2004; Zwarts and Dras, 2008) and output this translation without changing it in any way. In-between, we find the MEMT systems that build a consensus translation from a reduced set of translations, i.e. systems that first chose the subset with the most promising translations,"
2011.eamt-1.15,N03-1017,0,0.00465715,"is parsed only once. • likelihood of the sentence as provided by an ngram language model trained on a SL corpus [slm]. The shift of a source word at position i is defined as abs(j − i), where j is the position of the first target word to which that source word is aligned. In the experiments we computed the mean and variance of both the shift and the fertility from a parallel corpus by computing word alignments in the usual way, i.e. by running G IZA ++ (Och and Ney, 2003) in both translation directions and then symmetrising both sets of alignments through the “grow-diag-final-and” heuristic (Koehn et al., 2003) implemented in M OSES (Koehn et al., 2007). We then use these pre-computed values when obtaining the features of an input sentence. The features obtained from the parse tree of the sentence try to describe the sentence in terms of the complexity of its structure. The features related to the shift and the fertility of the words to be translated are intended to describe the sentence in terms of the complexity of its words. The rest of features —sentence length, likelihood of the sentence to be translated and number of words not appearing in the parallel corpora used to train the corpus-based MT"
2011.eamt-1.15,P07-2045,0,0.0180415,"entence as provided by an ngram language model trained on a SL corpus [slm]. The shift of a source word at position i is defined as abs(j − i), where j is the position of the first target word to which that source word is aligned. In the experiments we computed the mean and variance of both the shift and the fertility from a parallel corpus by computing word alignments in the usual way, i.e. by running G IZA ++ (Och and Ney, 2003) in both translation directions and then symmetrising both sets of alignments through the “grow-diag-final-and” heuristic (Koehn et al., 2003) implemented in M OSES (Koehn et al., 2007). We then use these pre-computed values when obtaining the features of an input sentence. The features obtained from the parse tree of the sentence try to describe the sentence in terms of the complexity of its structure. The features related to the shift and the fertility of the words to be translated are intended to describe the sentence in terms of the complexity of its words. The rest of features —sentence length, likelihood of the sentence to be translated and number of words not appearing in the parallel corpora used to train the corpus-based MT systems— might be helpful to discriminate"
2011.eamt-1.15,W04-3250,0,0.251257,"Missing"
2011.eamt-1.15,J10-4005,0,0.0224512,"extracting a set of features from each SL sentence and then using maximum entropy classifiers trained over a set of parallel sentences. Preliminary experiments on two European language pairs show a small, non-statistical significant improvement.* 1 Introduction Machine translation (MT) has become a viable technology that helps individuals in assimilation —to get the gist of a text written in a language the reader does not understand— and dissemination —to produce a draft translation to be post-edited for publication— tasks. However, none of the different approaches to MT, whether statistical (Koehn, 2010), example-based (Carl and Way, 2003), rulebased (Hutchins and Somers, 1992) or hybrid (Thurmair, 2009), always provide the best results. This c 2011 European Association for Machine Translation. is why some researchers have investigated the development of multi-engine MT (MEMT) systems (Eisele, 2005; Macherey and Och, 2007; Du et al., 2009; Du et al., 2010) aimed to provide translations of higher quality than those produced by the isolated MT systems in which they are based on. MEMT systems can be classified according to how they work. On one hand, we find systems that combine the translations"
2011.eamt-1.15,W07-0734,0,0.0266949,"ion task. Variance and mean shifts and fertilities were calculated on the same corpora used to train the corpus-based MT systems. After translating the SL sentences in the training corpora through all the MT systems being considered, we used the A SIYA evaluation toolkit10 (Gim´enez and M`arquez, 2010) to evaluate, at the sentence level, the translation provided by each MT system against the TL reference in the training parallel corpora. For that we used the precisionoriented measure BLEU (Papineni et al., 2002), two edit distance-based measures, PER and TER (Snover et al., 2006); and METEOR (Lavie and Agarwal, 2007), a measure aimed at balancing precision and recall that considers stemming and, only for some languages, synonymy lookup using WordNet. In our experiments we only used stemming when computing the lexical similarity of two words. To train and test the five binary maximum entropy classifiers we used the W EKA machine learning toolkit (Witten and Frank, 2005) with default parameters; the class implementing the maximum entropy classifier is weka.classifiers.functions.Logistic. The class implementing the chi square method we used to select the set of relevant features on a development corpus is we"
2011.eamt-1.15,D07-1105,0,0.0676927,"ogy that helps individuals in assimilation —to get the gist of a text written in a language the reader does not understand— and dissemination —to produce a draft translation to be post-edited for publication— tasks. However, none of the different approaches to MT, whether statistical (Koehn, 2010), example-based (Carl and Way, 2003), rulebased (Hutchins and Somers, 1992) or hybrid (Thurmair, 2009), always provide the best results. This c 2011 European Association for Machine Translation. is why some researchers have investigated the development of multi-engine MT (MEMT) systems (Eisele, 2005; Macherey and Och, 2007; Du et al., 2009; Du et al., 2010) aimed to provide translations of higher quality than those produced by the isolated MT systems in which they are based on. MEMT systems can be classified according to how they work. On one hand, we find systems that combine the translations provided by several MT systems into a consensus translation (Bangalore et al., 2001; Bangalore et al., 2002; Matusov et al., 2006; Heafield et al., 2009; Du et al., 2009; Du et al., 2010); the output of these MEMT systems may differ from those provided by the individual MT systems they are based on. On the other hand, we"
2011.eamt-1.15,E06-1005,0,0.0242827,"provide the best results. This c 2011 European Association for Machine Translation. is why some researchers have investigated the development of multi-engine MT (MEMT) systems (Eisele, 2005; Macherey and Och, 2007; Du et al., 2009; Du et al., 2010) aimed to provide translations of higher quality than those produced by the isolated MT systems in which they are based on. MEMT systems can be classified according to how they work. On one hand, we find systems that combine the translations provided by several MT systems into a consensus translation (Bangalore et al., 2001; Bangalore et al., 2002; Matusov et al., 2006; Heafield et al., 2009; Du et al., 2009; Du et al., 2010); the output of these MEMT systems may differ from those provided by the individual MT systems they are based on. On the other hand, we have systems that decide which translation, among all the translations computed by the MT systems they are based on, is the most appropriate one (Nomoto, 2004; Zwarts and Dras, 2008) and output this translation without changing it in any way. In-between, we find the MEMT systems that build a consensus translation from a reduced set of translations, i.e. systems that first chose the subset with the most"
2011.eamt-1.15,P04-1063,0,0.0271358,"re based on. MEMT systems can be classified according to how they work. On one hand, we find systems that combine the translations provided by several MT systems into a consensus translation (Bangalore et al., 2001; Bangalore et al., 2002; Matusov et al., 2006; Heafield et al., 2009; Du et al., 2009; Du et al., 2010); the output of these MEMT systems may differ from those provided by the individual MT systems they are based on. On the other hand, we have systems that decide which translation, among all the translations computed by the MT systems they are based on, is the most appropriate one (Nomoto, 2004; Zwarts and Dras, 2008) and output this translation without changing it in any way. In-between, we find the MEMT systems that build a consensus translation from a reduced set of translations, i.e. systems that first chose the subset with the most promising translations, and then combine these translations to produce a single output (Macherey and Och, 2007). Even though MEMT systems that select the most promising translation and those that work on a reduced subset of translations do not use all the translations computed by all the MT system, both kinds of MEMT systems need to translate the inp"
2011.eamt-1.15,J03-1002,0,0.00628453,"words [len], • number of words not appearing in the corpora used to trained the corpus-based MT system used [unk], and because each sentence is parsed only once. • likelihood of the sentence as provided by an ngram language model trained on a SL corpus [slm]. The shift of a source word at position i is defined as abs(j − i), where j is the position of the first target word to which that source word is aligned. In the experiments we computed the mean and variance of both the shift and the fertility from a parallel corpus by computing word alignments in the usual way, i.e. by running G IZA ++ (Och and Ney, 2003) in both translation directions and then symmetrising both sets of alignments through the “grow-diag-final-and” heuristic (Koehn et al., 2003) implemented in M OSES (Koehn et al., 2007). We then use these pre-computed values when obtaining the features of an input sentence. The features obtained from the parse tree of the sentence try to describe the sentence in terms of the complexity of its structure. The features related to the shift and the fertility of the words to be translated are intended to describe the sentence in terms of the complexity of its words. The rest of features —sentence l"
2011.eamt-1.15,P02-1040,0,0.082704,"toolkit9 (Federico et al., 2008) by using the SL corpora distributed as part of the WMT10 shared translation task. Variance and mean shifts and fertilities were calculated on the same corpora used to train the corpus-based MT systems. After translating the SL sentences in the training corpora through all the MT systems being considered, we used the A SIYA evaluation toolkit10 (Gim´enez and M`arquez, 2010) to evaluate, at the sentence level, the translation provided by each MT system against the TL reference in the training parallel corpora. For that we used the precisionoriented measure BLEU (Papineni et al., 2002), two edit distance-based measures, PER and TER (Snover et al., 2006); and METEOR (Lavie and Agarwal, 2007), a measure aimed at balancing precision and recall that considers stemming and, only for some languages, synonymy lookup using WordNet. In our experiments we only used stemming when computing the lexical similarity of two words. To train and test the five binary maximum entropy classifiers we used the W EKA machine learning toolkit (Witten and Frank, 2005) with default parameters; the class implementing the maximum entropy classifier is weka.classifiers.functions.Logistic. The class impl"
2011.eamt-1.15,N07-1051,0,0.0170769,"e English sentence.7 After removing duplicated sentences and sentences longer than 200 words, we used the first 2,000 sentences for development, the second 2,000 sentences for testing, and the next 100,000 sentences for training. Note that some sentences in these corpora could not be parsed with the parser we have used (see below) and, therefore, they were removed before running the experiments. Table 1 provides detailed information about these corpora and the number of sentences finally used in the experiments. To parse the input SL sentences we used the Berkeley Parser (Petrov et al., 2006; Petrov and Klein, 2007) together with the parsing models available for English and French from the parser website.8 To compute the likelihood of the SL sentences we used a 5-gram language model trained by means of the IRSTLM language modelling toolkit9 (Federico et al., 2008) by using the SL corpora distributed as part of the WMT10 shared translation task. Variance and mean shifts and fertilities were calculated on the same corpora used to train the corpus-based MT systems. After translating the SL sentences in the training corpora through all the MT systems being considered, we used the A SIYA evaluation toolkit10"
2011.eamt-1.15,P06-1055,0,0.0136637,"s translation the same English sentence.7 After removing duplicated sentences and sentences longer than 200 words, we used the first 2,000 sentences for development, the second 2,000 sentences for testing, and the next 100,000 sentences for training. Note that some sentences in these corpora could not be parsed with the parser we have used (see below) and, therefore, they were removed before running the experiments. Table 1 provides detailed information about these corpora and the number of sentences finally used in the experiments. To parse the input SL sentences we used the Berkeley Parser (Petrov et al., 2006; Petrov and Klein, 2007) together with the parsing models available for English and French from the parser website.8 To compute the likelihood of the SL sentences we used a 5-gram language model trained by means of the IRSTLM language modelling toolkit9 (Federico et al., 2008) by using the SL corpora distributed as part of the WMT10 shared translation task. Variance and mean shifts and fertilities were calculated on the same corpora used to train the corpus-based MT systems. After translating the SL sentences in the training corpora through all the MT systems being considered, we used the A S"
2011.eamt-1.15,quirk-2004-training,0,0.318573,"tistical parser (see Section 3 to know about the parser we have used),1 while the others can be 1 It may be argued that parsing a sentence may be as time consuming as translating it; however, in MEMT a sentence is translated several times, and thus avoiding to perform such translations, even by using computationally expensive procedures such as parsing, helps saving computational resources 98 easily obtained from the SL sentence. Note that some of the (SL) features we have used have also been used in combination with other features for sentence-level confidence estimation (Blatz et al., 2003; Quirk, 2004; Specia et al., 2009), a related task aimed at assessing the correctness of a translation. A description of the features we have tried follows: • maximum depth of the parse tree [gmaxd], • mean depth of the parse tree [gmeand], • joint likelihood of the parse tree t and the words w in the sentence, i.e. p(t, w) [gjl], • likelihood of the parse tree given the words, i.e. p(t|w) [gcl], • sentence likelihood as provided by the model used to parse the sentence, i.e. summing out all possible parse trees [gsentl], • maximum number of child nodes per node found in the parse tree [gmaxc], • mean numb"
2011.eamt-1.15,2006.amta-papers.25,0,0.0145415,"part of the WMT10 shared translation task. Variance and mean shifts and fertilities were calculated on the same corpora used to train the corpus-based MT systems. After translating the SL sentences in the training corpora through all the MT systems being considered, we used the A SIYA evaluation toolkit10 (Gim´enez and M`arquez, 2010) to evaluate, at the sentence level, the translation provided by each MT system against the TL reference in the training parallel corpora. For that we used the precisionoriented measure BLEU (Papineni et al., 2002), two edit distance-based measures, PER and TER (Snover et al., 2006); and METEOR (Lavie and Agarwal, 2007), a measure aimed at balancing precision and recall that considers stemming and, only for some languages, synonymy lookup using WordNet. In our experiments we only used stemming when computing the lexical similarity of two words. To train and test the five binary maximum entropy classifiers we used the W EKA machine learning toolkit (Witten and Frank, 2005) with default parameters; the class implementing the maximum entropy classifier is weka.classifiers.functions.Logistic. The class implementing the chi square method we used to select the set of relevant"
2011.eamt-1.15,2009.mtsummit-papers.16,0,0.0127191,"er (see Section 3 to know about the parser we have used),1 while the others can be 1 It may be argued that parsing a sentence may be as time consuming as translating it; however, in MEMT a sentence is translated several times, and thus avoiding to perform such translations, even by using computationally expensive procedures such as parsing, helps saving computational resources 98 easily obtained from the SL sentence. Note that some of the (SL) features we have used have also been used in combination with other features for sentence-level confidence estimation (Blatz et al., 2003; Quirk, 2004; Specia et al., 2009), a related task aimed at assessing the correctness of a translation. A description of the features we have tried follows: • maximum depth of the parse tree [gmaxd], • mean depth of the parse tree [gmeand], • joint likelihood of the parse tree t and the words w in the sentence, i.e. p(t, w) [gjl], • likelihood of the parse tree given the words, i.e. p(t|w) [gcl], • sentence likelihood as provided by the model used to parse the sentence, i.e. summing out all possible parse trees [gsentl], • maximum number of child nodes per node found in the parse tree [gmaxc], • mean number of child nodes per"
2011.eamt-1.15,2007.mtsummit-papers.59,0,0.0152081,"robability of each MT system being the best system to translate that sentence is estimated by means of the different maximum entropy models. The systems finally selected to translate the input sentence are the ones with the highest probabilities. In this papers we have tested this approach by selecting only a single MT system, the one with the highest probability. 3 We have tested our approach in the translation of English and French texts into Spanish. The systems we have used are: the shallow-transfer rule-based MT system A PERTIUM (Forcada et al., 2011),2 the rule-based MT system S YSTRAN (Surcin et al., 2007),3 the phrase-based statistical MT system M OSES (Koehn et al., 2007),4 the M OSES - CHART hierarchical phrase-based MT (Chiang, 2007) system, and the hybrid example-based–statistical MT system C UNEI (Phillips and Brown, 2009).5 The three corpus-based systems, namely M OSES, M OSES - CHART and C UNEI, were trained using the data set released as part of the WMT10 shared translation task.6 The corpora used to train and evaluate the five binary maximum entropy classifiers were 2 1. the SL sentence is translated into the TL through all the MT systems; 2. each translation is evaluated against the"
2011.eamt-1.15,2009.mtsummit-posters.21,0,0.0451223,"ed over a set of parallel sentences. Preliminary experiments on two European language pairs show a small, non-statistical significant improvement.* 1 Introduction Machine translation (MT) has become a viable technology that helps individuals in assimilation —to get the gist of a text written in a language the reader does not understand— and dissemination —to produce a draft translation to be post-edited for publication— tasks. However, none of the different approaches to MT, whether statistical (Koehn, 2010), example-based (Carl and Way, 2003), rulebased (Hutchins and Somers, 1992) or hybrid (Thurmair, 2009), always provide the best results. This c 2011 European Association for Machine Translation. is why some researchers have investigated the development of multi-engine MT (MEMT) systems (Eisele, 2005; Macherey and Och, 2007; Du et al., 2009; Du et al., 2010) aimed to provide translations of higher quality than those produced by the isolated MT systems in which they are based on. MEMT systems can be classified according to how they work. On one hand, we find systems that combine the translations provided by several MT systems into a consensus translation (Bangalore et al., 2001; Bangalore et al."
2011.eamt-1.15,C08-1145,0,0.10168,"EMT systems can be classified according to how they work. On one hand, we find systems that combine the translations provided by several MT systems into a consensus translation (Bangalore et al., 2001; Bangalore et al., 2002; Matusov et al., 2006; Heafield et al., 2009; Du et al., 2009; Du et al., 2010); the output of these MEMT systems may differ from those provided by the individual MT systems they are based on. On the other hand, we have systems that decide which translation, among all the translations computed by the MT systems they are based on, is the most appropriate one (Nomoto, 2004; Zwarts and Dras, 2008) and output this translation without changing it in any way. In-between, we find the MEMT systems that build a consensus translation from a reduced set of translations, i.e. systems that first chose the subset with the most promising translations, and then combine these translations to produce a single output (Macherey and Och, 2007). Even though MEMT systems that select the most promising translation and those that work on a reduced subset of translations do not use all the translations computed by all the MT system, both kinds of MEMT systems need to translate the input source-language (SL)"
2011.eamt-1.15,C04-1046,0,\N,Missing
2011.mtsummit-papers.18,2011.eamt-1.13,1,0.857511,"Missing"
2011.mtsummit-papers.18,2010.jec-1.4,0,0.0703541,"towards ti . This is done by augmenting the PBSMT translation table with bilingual sub-segments coming from the fuzzy match (si , ti ). Simard and Isabelle (2009) propose a similar approach in which a new feature function is introduced in the log-linear model combination of a PBSMT system to promote the use of the bilingual sub-segments from the fuzzy match (si , ti ). Neither of these two approaches guarantees that the PBSMT system will produce a translated segment containing the translation of the sub-segments that are common to si and s . In contrast, Zhechev and van Genabith (2010) and Koehn and Senellart (2010), who also use a PBSMT system, guarantee that the sub-segments of ti that have been detected to be aligned with the sub-segments in si matched by s appear in the translated segment. Our approach differs from those described above in two ways. First, while they use the TM to improve the results of MT, or MT to translate sub-segments of the TUs, our MT-based approach uses MT to improve the experience of using a TM-based CAT system without actually translating any new material. Second, the approaches above focus on a speciﬁc MT system or family of MT systems (namely, SMT), whereas our MT-based a"
2011.mtsummit-papers.18,kranias-samiotou-2004-automatic,0,0.203955,"ther source of bilingual information such as dictionaries, glossaries, or terminology data bases. As regards commercial TM-based CAT tools, D´ej`aVu1 integrates example-based MT (EBMT) to suggest candidate translations in those cases in which 1 http://www.atril.com 173 an exact match is not found, but partial matches are available (Lagoudaki, 2008). The EBMT-inspired system is used to propose a translation by putting together sub-segments of the partial matchings available. Unfortunately, we have been unable to ﬁnd further details on how this method works. More similar to ours are the work by Kranias and Samiotou (2004), based on the ESTeam CAT system, and the one by Espl`a et al. (2011). Kranias and Samiotou (2004) align the words in each TU at different sub-sentential levels by using a bilingual dictionary (Meyers et al., 1998). Then, when a TU is proposed to the CAT user, the alignments previously computed and the MT system are used, respectively, to detect the target words that need to be changed, and to propose a translation for them. Espl`a et al. (2011) use statistical word-alignment (SWA) models computed by means of GIZA++ (Och and Ney, 2003) to align the SL and TL segments of each TU in the TM. Then"
2011.mtsummit-papers.18,2008.amta-srw.4,0,0.0255193,"x, and is therefore able to use one or more MT systems at once. In addition, as our MT-based approach does not need to have access to the inner workings of the MT systems, it is capable of using on-line MT systems (thus avoiding any local installation) or even any other source of bilingual information such as dictionaries, glossaries, or terminology data bases. As regards commercial TM-based CAT tools, D´ej`aVu1 integrates example-based MT (EBMT) to suggest candidate translations in those cases in which 1 http://www.atril.com 173 an exact match is not found, but partial matches are available (Lagoudaki, 2008). The EBMT-inspired system is used to propose a translation by putting together sub-segments of the partial matchings available. Unfortunately, we have been unable to ﬁnd further details on how this method works. More similar to ours are the work by Kranias and Samiotou (2004), based on the ESTeam CAT system, and the one by Espl`a et al. (2011). Kranias and Samiotou (2004) align the words in each TU at different sub-sentential levels by using a bilingual dictionary (Meyers et al., 1998). Then, when a TU is proposed to the CAT user, the alignments previously computed and the MT system are used,"
2011.mtsummit-papers.18,meyers-etal-1998-multilingual,0,0.0390193,"those cases in which 1 http://www.atril.com 173 an exact match is not found, but partial matches are available (Lagoudaki, 2008). The EBMT-inspired system is used to propose a translation by putting together sub-segments of the partial matchings available. Unfortunately, we have been unable to ﬁnd further details on how this method works. More similar to ours are the work by Kranias and Samiotou (2004), based on the ESTeam CAT system, and the one by Espl`a et al. (2011). Kranias and Samiotou (2004) align the words in each TU at different sub-sentential levels by using a bilingual dictionary (Meyers et al., 1998). Then, when a TU is proposed to the CAT user, the alignments previously computed and the MT system are used, respectively, to detect the target words that need to be changed, and to propose a translation for them. Espl`a et al. (2011) use statistical word-alignment (SWA) models computed by means of GIZA++ (Och and Ney, 2003) to align the SL and TL segments of each TU in the TM. Then, when a TU (si , ti ) is proposed to the user, the pre-computed word alignments are used to determine the target words to change or keep unedited by computing the likelihood of each word wij in ti being kept unedi"
2011.mtsummit-papers.18,J03-1002,0,0.00344947,"ethod works. More similar to ours are the work by Kranias and Samiotou (2004), based on the ESTeam CAT system, and the one by Espl`a et al. (2011). Kranias and Samiotou (2004) align the words in each TU at different sub-sentential levels by using a bilingual dictionary (Meyers et al., 1998). Then, when a TU is proposed to the CAT user, the alignments previously computed and the MT system are used, respectively, to detect the target words that need to be changed, and to propose a translation for them. Espl`a et al. (2011) use statistical word-alignment (SWA) models computed by means of GIZA++ (Och and Ney, 2003) to align the SL and TL segments of each TU in the TM. Then, when a TU (si , ti ) is proposed to the user, the pre-computed word alignments are used to determine the target words to change or keep unedited by computing the likelihood of each word wij in ti being kept unedited:  psK (wij , s , si , ti ) = vil ∈aligned(wij ) matched(vil ) |aligned(wij )| where aligned(wij ) is the set of source words in si that are aligned with the target word wij , and matched(vil ) equals 1 if the source word vil is part of the match between si and s , the segment to be translated, and 0 otherwise. This lik"
2011.mtsummit-papers.18,P02-1040,0,0.0802484,"ain as TMin . Evaluation was carried out by simulating a CAT job in which the source segments in TMtrans are translated using the TUs in TMtest . For each source segment in TMtrans , and using the same FMS threshold Θ used during training, we computed the set of the matching TUs in TMtest , and classiﬁed the words in their target segments as “keep” or “change”. We used the general-purpose free/open-source MT system apertium-en-es, version 0.7, built upon version 3.2 of the Apertium free/open-source MT platform2 (Forcada et al., 2011). This is a rule-based MT system that achieves a BLEU score (Papineni et al., 2002) of 0.20 on the test set provided for the WMT10 translation task.3 This BLEU score may be considered low compared to those achieved by other MT systems, ranging around 0.27, for the same language pair and on the same test set (Callison-Burch et al., 2010). 4.1 Corpora The TMs we have used were extracted from two different parallel corpora already aligned at the sentence level: the JRC-Acquis corpus version 3 (Steinberger et al., 2006),4 which contains the total body of European Union law, and the EMEA corpus version 0.3 (Tiedemann, 2009),5 which is a compilation of documents from the European"
2011.mtsummit-papers.18,2009.mtsummit-papers.14,0,0.151512,"etric classiﬁer whose parameters, the set of feature weights, can be obtained in advance from a separate training TM and then used to translate texts from a different domain without a signiﬁcant loss of accuracy, as demonstrated by our experiments. CAT users could therefore use this MT-based approach in their desktop workstations provided that they have the classiﬁer, on-line access to the MT system(s), and a set of suitable feature weights. Related work. In the literature one can ﬁnd several approaches that combine the beneﬁts of MT and TMs beyond the obvious β-combination scenario deﬁned by Simard and Isabelle (2009), in which MT is used to translate a new segment when no matching TU above a FMS threshold β is found in the TM. Bic¸ici and Dymetman (2008) integrate a phrase-based statistical MT (PBSMT) (Koehn, 2010) system using discontinuous bilingual sub-segments into a TM-based CAT tool. The PBSMT system is trained on the same TM and, when a new source segment s is to be translated, the segments si and ti in the best matching TU are used to bias the statistical translation of s towards ti . This is done by augmenting the PBSMT translation table with bilingual sub-segments coming from the fuzzy match ("
2011.mtsummit-papers.18,steinberger-etal-2006-jrc,0,0.0286856,"Missing"
2011.mtsummit-papers.18,W10-3806,0,0.62089,"Missing"
2011.mtsummit-papers.18,W10-1703,0,\N,Missing
2011.mtsummit-papers.64,H93-1039,0,0.401888,"Missing"
2011.mtsummit-papers.64,W11-2103,0,0.0844519,"Missing"
2011.mtsummit-papers.64,W07-0732,0,0.0371732,"tion algorithm. Other approaches go beyond simply adding a dictionary to the parallel corpus. For instance, Popovi´c and Ney (2006) propose combining that strategy with the use of hand-crafted rules to reorder the SL sentences to match the structure of the TL. Although RBMT transfer rules have also been reused in hybrid systems, they have been mostly used implicitly as part of a complete RBMT engine. 564 For instance, Dugast et al. (2008) show how a PBSMT system can be bootstrapped using only monolingual data and an RBMT engine; RBMT and PBSMT systems can also be combined in a serial fashion (Dugast et al., 2007). Another remarkable study (Eisele et al., 2008) presents a strategy based on the augmentation of the phrase table to include information provided by an RBMT system. In this approach, the sentences to be translated by the hybrid system are first translated with an RBMT system and a small phrase table is obtained from the resulting parallel corpus. Phrase pairs are extracted following the usual procedure (Koehn, 2010, sec. 5.2.3) which generates the set of all possible phrase pairs that are consistent with the word alignments. In order to obtain reliable word alignments, they are computed using"
2011.mtsummit-papers.64,W08-0327,0,0.0701323,"egy, multi-word expressions from the bilingual dictionary that appear in the SL sentences are translated as such because they may be split into smaller units by the phrase-extraction algorithm. Other approaches go beyond simply adding a dictionary to the parallel corpus. For instance, Popovi´c and Ney (2006) propose combining that strategy with the use of hand-crafted rules to reorder the SL sentences to match the structure of the TL. Although RBMT transfer rules have also been reused in hybrid systems, they have been mostly used implicitly as part of a complete RBMT engine. 564 For instance, Dugast et al. (2008) show how a PBSMT system can be bootstrapped using only monolingual data and an RBMT engine; RBMT and PBSMT systems can also be combined in a serial fashion (Dugast et al., 2007). Another remarkable study (Eisele et al., 2008) presents a strategy based on the augmentation of the phrase table to include information provided by an RBMT system. In this approach, the sentences to be translated by the hybrid system are first translated with an RBMT system and a small phrase table is obtained from the resulting parallel corpus. Phrase pairs are extracted following the usual procedure (Koehn, 2010, s"
2011.mtsummit-papers.64,W08-0328,0,0.365398,"d SMT system with resources taken from shallow-transfer 562 RBMT. Shallow-transfer RBMT systems do not perform a complete syntactic analysis of the input sentences, but they rather work with much simpler intermediate representations. Hybridisation between shallow-transfer RBMT and SMT has not yet been explored. Existing hybridisation strategies usually involve more complex RBMT systems and treat them as black boxes, whereas our approach improves SMT by explicitly using the RBMT linguistic resources. We provide an exhaustive evaluation of our hybridisation approach and of the most similar one (Eisele et al., 2008), on the Spanish–English and English–Spanish language pairs by using different training corpus sizes and evaluation corpora. The rest of the paper is organised as follows. Next section overviews the MT systems we combine in our approach. Section 3 outlines related hybrid approaches, whereas our approach is described in section 4. Sections 5 and 6 present the experiments conducted and the results achieved, respectively. The paper ends with some concluding remarks. 2 Translation approaches 2.1 Phrase-based statistical machine translation Phrase-based statistical machine translation systems (PBSM"
2011.mtsummit-papers.64,P07-2045,0,0.0113344,"Size (sentences) Europarl 1 650 152 Europarl 1 650 152 0 Europarl 2 000 Europarl 5 000 Europarl 10 000 Europarl 20 000 Europarl 40 000 Europarl 80 000 Europarl 1 272 260 Europarl 2 000 Europarl 2 000 WMT 2010 1 732 WMT 2010 2 215 Table 1: Data about the Spanish–English parallel corpora used in the experiments. testing. Both sets are distributed as part of the WMT 2010 shared translation task.1 Sentences containing more than 40 tokens were removed from all the bilingual corpora to avoid problems with the word alignment tool (Och and Ney, 2003).2 We used the free/open-source PBSMT system Moses (Koehn et al., 2007) with the SRILM language modeling toolkit (Stolcke, 2002), which was used to train a 5-gram language model using interpolated Kneser-Ney discounting (Goodman and Chen, 1998). Word alignments from the training parallel corpus were computed by means of GIZA++ (Och and Ney, 2003). The Apertium (Forcada et al., 2011) engine and the linguistic resources were downloaded from the Apertium Subversion repository. The linguistic data contains 326 228 entries in the bilingual dictionary; 106 first-level and 31 second-level structural transfer rules for Spanish–English; and 216 1 http://www.statmt.org/wmt"
2011.mtsummit-papers.64,2005.mtsummit-papers.11,0,0.0606384,"disation approach on the two translation directions of the Spanish–English language pair and with different corpus sizes to test the translation scenarios in which it best fits. Small corpus sizes allows us to test if our hybrid approach is useful in the translation between less-resourced language pairs where one of the languages is highly inflected (Spanish) while the other one (English) is not. We compare the performance of our approach to that by Eisele et al. (2008) because it is the most similar to ours. PBSMT systems for both directions were trained from the Europarl v5 parallel corpus (Koehn, 2005). Its whole target side, except for the Q4/2000 portion, was used to train a language model. Regarding the translation model, we learned it from corpora of different sizes; more precisely, we used fragments of the Europarl corpus consisting of 5 000, 10 000, 20 000, 40 000 and 80 000 parallel sentences. In addition we also used the whole Europarl corpus and an empty training set. The sentences in each training set were randomly chosen (avoiding the Q4/2000 portion) in such a way that larger corpora include the sentences in the smaller ones. We carried out both in-domain and out-of-domain evalu"
2011.mtsummit-papers.64,J10-4005,0,0.365563,"extensively evaluate a new hybridisation approach consisting of enriching the phrase table of a phrase-based statistical machine translation system with bilingual phrase pairs matching transfer rules and dictionary entries from a shallow-transfer rulebased machine translation system. The experiments conducted show an improvement in translation quality, specially when the parallel corpus available for training is small or when translating out-of-domain texts that are well covered by the shallow-transfer rulebased machine translation system. 1 Introduction Statistical machine translation (SMT) (Koehn, 2010) is currently the leading paradigm in machine translation (MT) research. SMT systems are very attractive because they may be built with little human effort when enough monolingual and bilingual corpora are available. However, bilingual corpora are not always easy to harvest, and they may not even exist for some language pairs. On the contrary, rule-based machine translation systems (RBMT) (Hutchins and Somers, 1992) may be built without any parallel corpus; however, they need an explicit representation of linguistic information, whose coding by human experts requires a considerable amount of t"
2011.mtsummit-papers.64,2006.amta-papers.11,0,0.0164488,"tionary by a wider margin when translating out-of-domain texts from English to Spanish than the other way round. As Spanish morphology is richer, transfer rules help to perform more agreement operations to determine the gender, number and person when translating into Spanish. As regards the three hybrid strategies we defined, no consistent differences exist between them, 568 probably because the Apertium-generated bilingual phrase pairs were too short for their subphrases to clearly improve the reordering model and because bigger improvements in aligment quality may be needed to improve BLEU (Lopez and Resnik, 2006). Finally, our hybridisation strategy provides better results than the approach by Eisele et al. (2008), specially when small corpora are used for training. Under such circumstance, no reliable alignment models can be learned for the Eisele’s set-up from the training corpus and then no reliable phrases pairs can be obtained from the input text and its rule-based translation. Our approach is not affected by this problem because it does not need any special alignment model. In addition, the approach by Eisele et al. (2008) involves concatenating two phrase tables independently learned, which cau"
2011.mtsummit-papers.64,J03-1002,0,0.0172514,"d the results achieved, respectively. The paper ends with some concluding remarks. 2 Translation approaches 2.1 Phrase-based statistical machine translation Phrase-based statistical machine translation systems (PBSMT) (Koehn, 2010, ch. 5) translate sentences by maximising the translation probability as defined by the log-linear combination of a number of feature functions, whose weights are chosen to optimise translation quality (Och, 2003). A core component of every PBSMT system is the phrase table, which contains bilingual phrase pairs extracted from a bilingual corpus after word alignment (Och and Ney, 2003). The set of translations from which the most probable one is chosen is built by segmenting the source-language (SL) sentence in all possible ways and then combining the translation of the different source segments according to the phrase table. Common feature functions are: source-to-target and target-to-source phrase translation probabilities, source-to-target and target-to-source lexical weightings (calculated by using a probabilistic bilingual dictionary), reordering costs, number of words in the output (word penalty), number of phrase pairs used (phrase penalty), and likelihood of the out"
2011.mtsummit-papers.64,P03-1021,0,0.0229305,"e in our approach. Section 3 outlines related hybrid approaches, whereas our approach is described in section 4. Sections 5 and 6 present the experiments conducted and the results achieved, respectively. The paper ends with some concluding remarks. 2 Translation approaches 2.1 Phrase-based statistical machine translation Phrase-based statistical machine translation systems (PBSMT) (Koehn, 2010, ch. 5) translate sentences by maximising the translation probability as defined by the log-linear combination of a number of feature functions, whose weights are chosen to optimise translation quality (Och, 2003). A core component of every PBSMT system is the phrase table, which contains bilingual phrase pairs extracted from a bilingual corpus after word alignment (Och and Ney, 2003). The set of translations from which the most probable one is chosen is built by segmenting the source-language (SL) sentence in all possible ways and then combining the translation of the different source segments according to the phrase table. Common feature functions are: source-to-target and target-to-source phrase translation probabilities, source-to-target and target-to-source lexical weightings (calculated by using"
2011.mtsummit-papers.64,P02-1040,0,0.0893836,"Missing"
2011.mtsummit-papers.64,W09-0423,0,0.126485,"&lt;pres> decide&lt;vblex>&lt;pp> to&lt;pr> come&lt;vblex>&lt;inf> Finally, the translation into TL is generated from the TL lexical forms: On the other hand my American friends have decided to come. 3 Related work Bilingual dictionaries are the most reused resource from RBMT. They have been added to SMT systems since its early days (Brown et al., 1993). One of the simplest strategies, which has already been put into practice with the Apertium bilingual dictionaries (Tyers, 2009), consists of adding the dictionary entries directly to the parallel corpus. In addition to the obvious increase in lexical coverage, Schwenk et al. (2009) state that the quality of the alignments obtained is also improved when the words in the bilingual dictionary appear in other sentences of the parallel corpus. However, it is not guaranteed that, following this strategy, multi-word expressions from the bilingual dictionary that appear in the SL sentences are translated as such because they may be split into smaller units by the phrase-extraction algorithm. Other approaches go beyond simply adding a dictionary to the parallel corpus. For instance, Popovi´c and Ney (2006) propose combining that strategy with the use of hand-crafted rules to reo"
2011.mtsummit-papers.64,2009.mtsummit-posters.21,0,0.0341445,"are very attractive because they may be built with little human effort when enough monolingual and bilingual corpora are available. However, bilingual corpora are not always easy to harvest, and they may not even exist for some language pairs. On the contrary, rule-based machine translation systems (RBMT) (Hutchins and Somers, 1992) may be built without any parallel corpus; however, they need an explicit representation of linguistic information, whose coding by human experts requires a considerable amount of time. When both parallel corpora and linguistic information exist, a hybrid approach (Thurmair, 2009) may be taken in order to make the most of such resources. In this paper we present a new hybrid approach which enriches a phrase-based SMT system with resources taken from shallow-transfer 562 RBMT. Shallow-transfer RBMT systems do not perform a complete syntactic analysis of the input sentences, but they rather work with much simpler intermediate representations. Hybridisation between shallow-transfer RBMT and SMT has not yet been explored. Existing hybridisation strategies usually involve more complex RBMT systems and treat them as black boxes, whereas our approach improves SMT by explicitl"
2011.mtsummit-papers.64,2009.eamt-1.29,0,0.194781,"removes chunks to generate a plain sequence of TL lexical forms: on the other hand&lt;adv> my&lt;det>&lt;pos>&lt;pl> american&lt;adj> friend&lt;n>&lt;pl> have&lt;vbhaver>&lt;pres> decide&lt;vblex>&lt;pp> to&lt;pr> come&lt;vblex>&lt;inf> Finally, the translation into TL is generated from the TL lexical forms: On the other hand my American friends have decided to come. 3 Related work Bilingual dictionaries are the most reused resource from RBMT. They have been added to SMT systems since its early days (Brown et al., 1993). One of the simplest strategies, which has already been put into practice with the Apertium bilingual dictionaries (Tyers, 2009), consists of adding the dictionary entries directly to the parallel corpus. In addition to the obvious increase in lexical coverage, Schwenk et al. (2009) state that the quality of the alignments obtained is also improved when the words in the bilingual dictionary appear in other sentences of the parallel corpus. However, it is not guaranteed that, following this strategy, multi-word expressions from the bilingual dictionary that appear in the SL sentences are translated as such because they may be split into smaller units by the phrase-extraction algorithm. Other approaches go beyond simply"
2011.mtsummit-papers.64,zhang-etal-2004-interpreting,0,0.01512,"pus-dict, phrase-dict, and pc-dict, respectively); and • the approach by Eisele et al. (2008), using the alignment model learned from the training corpus to get the word alignments between the source sentences and the RBMT-translated sentences (Eisele). 6 Results and discussion Table 2 reports the translation performance as measured by BLEU (Papineni et al., 2002) for the differ567 ent systems, training corpora and evaluation corpora previously described. Statistical significance of the difference between systems has been computed by performing 1 000 iterations of paired bootstrap resampling (Zhang et al., 2004) with a p-level of 0.05. Table 3 shows the proportion of Apertium phrases chosen by the decoder. The results show that our hybrid approaches outperform both pure RBMT and PBSMT systems in terms of BLEU. This improvement is statistically significant for all training corpus sizes when translating out-of-domain texts; for in-domain texts the statistical significance only holds when the training corpus is relatively small. Note that The out-ofdomain tuning and test sets come from a general (news) domain and Apertium data has been developed bearing in mind the translation of general texts (mainly n"
2012.eamt-1.54,2011.eamt-1.30,1,0.817279,"sible solutions would be to generate all possible combinations of translations, and score them on a language model of the target language. This approach is taken in the METIS - II system (Melero et al., 2007). This has the benefit of being easy to implement, and only requiring a bilingual dictionary and a monolingual target language corpus. It has the drawbacks of being both slow – many translations must be performed – and not very customisable – control over the final translation is left to the TL model. Another possible solution, and one that is already used in some Apertium language pairs (Brandt et al., 2011; Wiechetek et al., 2010) is to use constraint grammar (Karlsson et al., 1995) rules to choose between possible alternative translations. An advantage of this is that the constraint grammar formalism is well known, and powerful, allowing context searches of unlimited size. However, it is too slow to be able to be used for production systems, as the speed is in the order of a few hundred words per second as opposed to thousands of words per second for the slowest Apertium module. Another approach not requiring a parallel corpus is presented by Dagan and Itai (1994). They first parse the SL sent"
2012.eamt-1.54,2007.tmi-papers.6,0,0.0412968,"he most adequate sense. Thus, it is not necessary to choose between a series of finegrained senses if all these senses result in the same final translation. The dominant approach to MT for language pairs with sufficient training data is phrase-based statistical machine translation; in this approach, lexical selection is performed by a combination of coocurrence in the phrase table, and score from the targetlanguage model (Koehn, 2010). There have however been attempts to improve on this by looking at global lexical selection over the whole sentence, see e.g. (Venkatapathy and Bangalore, 2007; Carpuat and Wu, 2007). In order to test different approaches to lexical selection for RBMT, we use the Apertium (Forcada et al., 2011) platform. This free/open-source platform includes 30 language pairs (as of February 2012). Sánchez-Martínez et al. (2007) describe a method to perform lexical selection in Apertium based on training a source-language bag-of-words model using TL cooccurrence statistics. This approach was tested, but abandoned as it produced less adequate translations than using the translation marked as default by a linguist in the bilingual dictionary. Other possible solutions would be to generate"
2012.eamt-1.54,J94-4003,0,0.61255,"n some Apertium language pairs (Brandt et al., 2011; Wiechetek et al., 2010) is to use constraint grammar (Karlsson et al., 1995) rules to choose between possible alternative translations. An advantage of this is that the constraint grammar formalism is well known, and powerful, allowing context searches of unlimited size. However, it is too slow to be able to be used for production systems, as the speed is in the order of a few hundred words per second as opposed to thousands of words per second for the slowest Apertium module. Another approach not requiring a parallel corpus is presented by Dagan and Itai (1994). They first parse the SL sentence and extract syntactic relations, such as verb + object, they then translate these with a bilingual dictionary and use collocation statistics from a TL corpus to choose the most adequate translation. While this method does not rely on the existence of a parallel corpus, it does depend on some way of identifying SL syntactic relations – which may not be available in all RBMT systems. The rest of the paper is laid out as follows: Section 2 presents some design decisions that were made in the development of the module. Section 3 describes in detail the rule forma"
2012.eamt-1.54,J03-1002,0,0.00473992,"found in the Apertium monolingual dictionary. We use version 6.0 of the EuroParl corpus (Koehn, 2005), and take the first 1.4 million lines for training.1 We used the Apertium English to Spanish pair apertium-en-es2 as it is one of the few pairs that has dictionaries with more than one alternative translation per word.3 4.1 Learning lexical selection rules from a parallel corpus The procedure to learn rules from a parallel corpus is as follows: We first morphologically analyse and disambiguate for part-of-speech both the SL and TL sides of the corpus. These are then word-aligned with GIZA++ (Och and Ney, 2003). 1 The remaining lines were held out for future use. Available from http://wiki.apertium.org/wiki/ SVN; SVN revision: 35684 3 The lexical selection module is available as free/open-source software in the package apertium-lex-tools. This paper uses SVN revision: 35799 2 217 We then pass the SL side of the corpus through the lexical-transfer stage of the MT system we are learning the rules for; this gives three sets of sentences: the tagged SL sentences, the tagged TL sentences and the possible translations of the SL words into the TL yielded by the bilingual dictionary. We take these three set"
2012.eamt-1.54,P02-1040,0,0.0913672,"Missing"
2012.eamt-1.54,W04-3250,0,0.276079,"Missing"
2012.eamt-1.54,2005.mtsummit-papers.11,0,0.049944,"a parallel corpus, and test the module on a well-known task for the evaluation of MT. The experimental setup follows the training of the baseline system in the shared task on MT at WMT11 (Callison-Burch et al., 2011), with the following differences: In place of the default Moses perl-based tokeniser, tokenisation was done using the Apertium morphological analyser (CortésVaíllo and Ortiz-Rojas, 2011). The corpus was also not lowercased; instead the case of known words was changed to the dictionary case as found in the Apertium monolingual dictionary. We use version 6.0 of the EuroParl corpus (Koehn, 2005), and take the first 1.4 million lines for training.1 We used the Apertium English to Spanish pair apertium-en-es2 as it is one of the few pairs that has dictionaries with more than one alternative translation per word.3 4.1 Learning lexical selection rules from a parallel corpus The procedure to learn rules from a parallel corpus is as follows: We first morphologically analyse and disambiguate for part-of-speech both the SL and TL sides of the corpus. These are then word-aligned with GIZA++ (Och and Ney, 2003). 1 The remaining lines were held out for future use. Available from http://wiki.ape"
2012.eamt-1.54,J10-4005,0,0.0209504,"age (TL). The task is related to the task of word-sense disambiguation (Ide and Véronis, 1998). The difference is that its aim is to find the most adequate translation, not the most adequate sense. Thus, it is not necessary to choose between a series of finegrained senses if all these senses result in the same final translation. The dominant approach to MT for language pairs with sufficient training data is phrase-based statistical machine translation; in this approach, lexical selection is performed by a combination of coocurrence in the phrase table, and score from the targetlanguage model (Koehn, 2010). There have however been attempts to improve on this by looking at global lexical selection over the whole sentence, see e.g. (Venkatapathy and Bangalore, 2007; Carpuat and Wu, 2007). In order to test different approaches to lexical selection for RBMT, we use the Apertium (Forcada et al., 2011) platform. This free/open-source platform includes 30 language pairs (as of February 2012). Sánchez-Martínez et al. (2007) describe a method to perform lexical selection in Apertium based on training a source-language bag-of-words model using TL cooccurrence statistics. This approach was tested, but aba"
2012.eamt-1.54,W07-0413,0,0.0277975,"e most adequate translation, not the most adequate sense. Thus, it is not necessary to choose between a series of finegrained senses if all these senses result in the same final translation. The dominant approach to MT for language pairs with sufficient training data is phrase-based statistical machine translation; in this approach, lexical selection is performed by a combination of coocurrence in the phrase table, and score from the targetlanguage model (Koehn, 2010). There have however been attempts to improve on this by looking at global lexical selection over the whole sentence, see e.g. (Venkatapathy and Bangalore, 2007; Carpuat and Wu, 2007). In order to test different approaches to lexical selection for RBMT, we use the Apertium (Forcada et al., 2011) platform. This free/open-source platform includes 30 language pairs (as of February 2012). Sánchez-Martínez et al. (2007) describe a method to perform lexical selection in Apertium based on training a source-language bag-of-words model using TL cooccurrence statistics. This approach was tested, but abandoned as it produced less adequate translations than using the translation marked as default by a linguist in the bilingual dictionary. Other possible solution"
2012.eamt-1.54,H05-1097,0,0.430219,"Missing"
2012.eamt-1.54,2004.tmi-1.9,0,0.114922,"Missing"
2014.amta-researchers.4,2003.mtsummit-papers.4,0,0.0424401,"Our approach, while related to the research described above, exhibits three main novelties: (i) it removes the dependency on knowledge of the internal workings of the MT system used, (ii) it removes the need to modify an MT system’s behavior in some way and (iii) avoids having to pre-process a user’s TM. We repair the mismatched sub-segments in a translation unit using a simple, yet novel, method that, unlike those by Hewavitharana et al. (2005) and Dandapat et al. (2011), takes context around mismatched words into account. Patching uses overlapping sub-segments as powerful anchors much like Brown et al. (2003)’s maximal leftoverlap compositional (example-based) MT system where the use of overlapping sub-segments reduces ”boundary friction” problems and increases the likelihood of producing a correct translation. Since patching treats sources of external bilingual translation information as black boxes, we generate translations on the fly without training SMT models on the user’s TM. We are not aware of any research work that: (a) uses any source of bilingual information for translation or (b) uses the context around mismatched words for repair. In the following sections, we show how the fuzzy-match"
2014.amta-researchers.4,2011.eamt-1.28,1,0.77716,"threshold. Their work, unlike Koehn and Senellart (2010), takes matched parts in s and replaces them with their counterparts in t. The main drawback of the approaches from Ma et al. (2011), Koehn and Senellart (2010), Zhechev and Genabith (2010), and Bic¸ici and Dymetman (2008) is that they are all based on SMT and either have access to the internals of an SMT system trained on the Al-Onaizan & Simard (Eds.) Proceedings of AMTA 2014, vol. 1: MT Researchers Vancouver, BC © The Authors 43 user’s or related data or modify its behavior in some way. Other research work (Hewavitharana et al., 2005; Dandapat et al., 2011) focuses on the identification of the sub-segments in the TL segment t of the translation unit (s, t) needed to produce t0 and then produces a translation by applying a set of edit operations over t. In particular, Hewavitharana et al. (2005) first align the mismatches in s to their TL translations in t by means of a modified IBM model 1 and then apply the same edit operations —substitutions, deletions and insertions— that are needed to convert s into s0 to the TL segment t. Their resulting translation may contain agreement and reordering errors because their method assumes that edit operation"
2014.amta-researchers.4,2011.mtsummit-papers.18,1,0.935321,"Missing"
2014.amta-researchers.4,2005.eamt-1.18,0,0.497123,"match score being used as a threshold. Their work, unlike Koehn and Senellart (2010), takes matched parts in s and replaces them with their counterparts in t. The main drawback of the approaches from Ma et al. (2011), Koehn and Senellart (2010), Zhechev and Genabith (2010), and Bic¸ici and Dymetman (2008) is that they are all based on SMT and either have access to the internals of an SMT system trained on the Al-Onaizan & Simard (Eds.) Proceedings of AMTA 2014, vol. 1: MT Researchers Vancouver, BC © The Authors 43 user’s or related data or modify its behavior in some way. Other research work (Hewavitharana et al., 2005; Dandapat et al., 2011) focuses on the identification of the sub-segments in the TL segment t of the translation unit (s, t) needed to produce t0 and then produces a translation by applying a set of edit operations over t. In particular, Hewavitharana et al. (2005) first align the mismatches in s to their TL translations in t by means of a modified IBM model 1 and then apply the same edit operations —substitutions, deletions and insertions— that are needed to convert s into s0 to the TL segment t. Their resulting translation may contain agreement and reordering errors because their method ass"
2014.amta-researchers.4,2010.jec-1.4,0,0.0950407,"Missing"
2014.amta-researchers.4,P11-1124,0,0.11147,"Missing"
2014.amta-researchers.4,P00-1056,0,0.181992,"s.) Proceedings of AMTA 2014, vol. 1: MT Researchers Vancouver, BC © The Authors 46 any translation τ of any segment σ in s contain words for which there is no evidence to modify them. In the absence of information, they will not be changed. 2.4 Step 4: pair translations of τ and τ 0 to form patching operators After the initial matching occurs from the translations of s to form (σ, τ ) pairs, (σ 0 , τ 0 ) pairs are created by translating mismatched sub-segments from s0 . The alignment found between words in s and words in s0 during fuzzy matching are used by an algorithm, analogous to that by Och and Ney (2000), to extract phrase pairs that project mismatched σ sub-segments in s into the corresponding sub-segments σ 0 in s0 . The σ 0 sub-segments are sent to the MT system or other source of bilingual information to obtain their translations τ 0 . The final result is a set of patching operators that contain translations that match the previously mismatched words in s0 and s. In steps 1 through 3, we have already created the (σ, τ ) pairs; now we translate s0 subsegments to form (σ 0 , τ 0 ) pairs. In our example, the (σ 0 , τ 0 ) pairs translated by Apertium (Forcada et al., 2011) are: σ0 σ positions"
2014.amta-researchers.4,2009.mtsummit-papers.14,0,0.772869,"Missing"
2014.amta-researchers.4,W10-3806,0,0.12996,"Missing"
2014.eamt-1.4,A92-1018,0,0.0166991,"le outputs, A is the |Γ|×|Γ |matrix of state-to-state transition probabilities, B is the |Γ|×|Σ |matrix with the probability of each observable output σ ∈ Σ being emitted from each state γ ∈ Γ, and the vector π, with dimensionality |Γ|, defines the initial probability of each state. The system produces an output each time a state is reached after a transition. In our method, Γ is made up of all the paradigms in the dictionary and Σ corresponds to the set of suffixes produced by all these paradigms. Our HMMs are trained in a way very similar to HMMs used in unsupervised part-of-speech tagging (Cutting et al., 1992), that is, by using the Baum-Welch algorithm (Baum, 1972) with an untagged corpus. The training corpus is built from a text corpus as follows: (i) the monolingual dictionary is used in order to obtain the set F of all possible word forms; (ii) the word forms in the text corpus that belong to F are assigned all their corresponding suffix and paradigm pairs; (iii) the word forms not in F are assigned the set of suffix and paradigm pairs obtained from the set L of their compatible candidates, as described in Section 4. Once the HMM is trained, the probability qt (cn ) of assigning the word form l"
2014.eamt-1.4,C12-3013,0,0.0171133,"ound. For instance, an expert could decide that in order to correctly choose the inflection paradigm of most verbs in French the infinitive and the first person plural present indicative forms are needed; dictionary developers must then provide these two forms when inserting a new verb. Bartuskov´a and Sedl´acek (2002) also present a tool for semi-automatic assignment of words to declension patterns; their system is based on a decision tree with a question in every node. Their proposal, unlike ours, works only for nouns and is aimed at experts because of the technical nature of the questions. Desai et al. (2012) focus on verbs and present a system for paradigm assignment based on the information collected from a corpus for each compatible paradigm; if the automatic method fails, users are then required to manually enter the correct paradigm. As regards the automatic acquisition of morˇ phological resources for MT, the work by Snajder (2013) is of particular interest: he turns the choice of the most appropriate paradigm for a given word into a machine learning problem. Given the values of a set of features extracted from a monolingual corpus and from the orthographic properties of the lemmas, each com"
2014.eamt-1.4,E12-1066,0,0.0644761,"Missing"
2014.eamt-1.4,R11-1047,1,0.894239,"Missing"
2014.eamt-1.4,sanchez-cartagena-etal-2012-source,1,0.803826,"Missing"
2014.eamt-1.4,2012.freeopmt-1.4,0,0.0315719,"Missing"
2014.eamt-1.4,W13-2201,0,\N,Missing
2015.eamt-1.20,J96-1002,0,0.220719,"actually lexical selection in MT; they used a parser to identify syntactic relations such as subject–object or subject–verb. After generating all the possible translations for a given input sentence using an ambiguous bilingual dictionary, they extract the syntactic tuples from the TL and count the frequency in a previously-trained TL model of tuples. They use maximum-likelihood estimation to calculate the probability that a given 1 145 Such as a morphological or syntactic analyser. TL tuple is the translation of a given SL tuple, with an automatically determined confidence threshold. Later, Berger et al. (1996) illustrated the use of maximum-entropy classifiers on the specific problem of lexical selection in IBM-style word-based statistical MT. Other authors (Melero et al., 2007) have used TL models to rank the translations resulting from all possible combinations of lexical selections. Nowadays, in state-of-the-art phrasebased statistical MT (Koehn, 2010), lexical selection is taken care of by a combination of the translation model and the language model. The translation model provides probabilities of translation between words or word sequences (often referred to as phrases) in the source and targ"
2015.eamt-1.20,J94-4003,0,0.491982,"mputational cost. 1 Mikel L. Forcada Dept. Lleng. i Sist. Inform., Universitat d’Alacant, E-03071 Alacant 1.1 Introduction Corpus-based machine translation (MT) has been the primary research direction in the field of MT in recent years. However, rule-based MT (RBMT) systems are still being developed, and there are many successful commercial and non-commercial systems. One reason for the continued development of RBMT systems is that in order to be successful, c 2015 The authors. This article is licensed under a Creative Commons 3.0 licence, no derivative works, attribution, CCBY-ND. Prior work Dagan and Itai (1994) used the term word sense disambiguation to refer to what is actually lexical selection in MT; they used a parser to identify syntactic relations such as subject–object or subject–verb. After generating all the possible translations for a given input sentence using an ambiguous bilingual dictionary, they extract the syntactic tuples from the TL and count the frequency in a previously-trained TL model of tuples. They use maximum-likelihood estimation to calculate the probability that a given 1 145 Such as a morphological or syntactic analyser. TL tuple is the translation of a given SL tuple, wi"
2015.eamt-1.20,W04-3250,0,0.0634887,"module returns more than one translation, Apertium will select the default one if marked or the first one of not.12 The table in Figure 2 gives an overview of the inputs.In the description it is assumed that the reference translation has been annotated by hand. However, hand annotation is a time-consuming process, and was not possible. A description of how the reference was built is given in Section 3.4. diff(Tr (si ), Tt (si )) = 3.3.2 3.3.3 Confidence intervals Confidence intervals for both metrics will be calculated through bootstrap resampling (Efron and Tibshirani, 1994) as described by Koehn (2004). In all cases, bootstrap resampling will be carried out for 1,000 iterations. Where the p = 0.05 confidence intervals overlap, we will also perform paired bootstrap resampling (Koehn, 2004). 3.4 For creating the test corpora, providing a SL corpus for training, and a TL corpus for scoring, we used four parallel corpora: • Ofis ar Brezhoneg (OAB): This parallel corpus of Breton and French has been collected specifically for lexical-selection experiments from translations produced by Ofis ar Brezhoneg ‘The Office of the Breton language’. The corpus has recently been made available online throug"
2015.eamt-1.20,2005.mtsummit-papers.11,0,0.020643,"is ar Brezhoneg ‘The Office of the Breton language’. The corpus has recently been made available online through OPUS.13 • South-East European Times (SETimes): Described in Tyers and Alperen (2010), this corpus is a multilingual corpus of the Balkan languages (and English) in the news domain. The Macedonian and English part will be used. • Open Data Euskadi (OpenData): This is a Basque–Spanish parallel corpus made from the translation memories of the Herri Arduralaritzaren Euskal Erakundea ‘Basque Institute of Public Administration’.14 • European Parliament Proceedings (EuroParl): Described by Koehn (2005), this is a multilingual corpus of the European Union official languages. We are using the English–Spanish data from version 7.15 Machine translation performance This is an extrinsic evaluation, which ideally would test how much the system improves as regards an approximate measurement of final translation quality in a real system. For this task, we use the widely-used BLEU metric (Papineni et al., 2002). This is not ideal for evaluating the task of a lexical selection module as the performance of the module will depend greatly on (a) the coverage of the bilingual dictionaries of the RBMT syst"
2015.eamt-1.20,J10-4005,0,0.0126561,"use maximum-likelihood estimation to calculate the probability that a given 1 145 Such as a morphological or syntactic analyser. TL tuple is the translation of a given SL tuple, with an automatically determined confidence threshold. Later, Berger et al. (1996) illustrated the use of maximum-entropy classifiers on the specific problem of lexical selection in IBM-style word-based statistical MT. Other authors (Melero et al., 2007) have used TL models to rank the translations resulting from all possible combinations of lexical selections. Nowadays, in state-of-the-art phrasebased statistical MT (Koehn, 2010), lexical selection is taken care of by a combination of the translation model and the language model. The translation model provides probabilities of translation between words or word sequences (often referred to as phrases) in the source and target language. The TL model provides probabilities of word sequences in the TL. Mareˇcek et al. (2010) trained a maximum-entropy lexical selector for their dependency-grammar-based transfer system TectoMT using a bilingual corpus. More recently, Tyers et al. (2012) presented a method of lexical selection for RBMT based on rules which select or remove t"
2015.eamt-1.20,P07-2045,0,0.00877625,"cluded to show the upper bound for the performance of the lexical-selection module. • Target language model (TLM). One method of lexical selection is to use the existing MT system to generate all the possible translations for an input sentence, and then score these translations on-line on a model of the TL. The highest scoring sentence is then output. This is the method used by Melero et al. (2007). 4 Results As we are working with binary features, we use the implementation of generalised iterative scaling 16 The exact configuration of GIZA ++ used is equivalent to running the M OSES toolkit (Koehn et al., 2007) in default configuration up to step three of training. 17 The development corpus was used for checking the value for frequency pruning of features. 150 Pair br-fr mk-en eu-es en-es Lines 57,305 190,493 765,115 1,467,708 Extract. 4,668 19,747 87,907 312,162 train 2,668 17,747 85,907 310,162 dev 1,000 1,000 1,000 1,000 test 1,000 1,000 1,000 1,000 No. amb 603 13,134 1,806 2,082 Av. amb 3.06 3.06 3.11 2.28 Table 1: Statistics about the source corpora. The column no. amb gives the number of unique tokens with more than one possible translation. The column av. amb gives the average number of trans"
2015.eamt-1.20,W10-1730,0,0.0445016,"Missing"
2015.eamt-1.20,P14-2123,0,0.0132542,"arrain    0 otherwise (1) This feature considers a context of zero words to the left of the problem word and one word (+ handi) to the right of it. As a result of training, each of the nF features hsk (t, c) in the classifier is assigned a weight λsk . Combining these weights of active features as in equation (2) yields the probability of a translation t for word s in context c. n ps (t|c) = (2) k=1 In this equation, Z s (c) is a normalising constant. Thus, the most probable translation t? can be found using t? = arg max ps (t|c) = arg max t∈Ts (s) 2 The work by Ravi and Knight (2011) and Nuhn and Ney (2014), who decipher word-ciphered text using monolingual corpora only may be seen as a generalised version of the problem of lexical selection without parallel corpora. F X 1 exp λsk hsk (t, c) Z s (c) nF X λsk hsk (t, c), t∈Ts (s) k=1 (3) 3 146 We follow the notation of Berger et al. (1996) S→ preposti=|G| → ({gi }i=1 , S) → lexsel → (g ? , S) → → τ (g ? , S) lexsel lexsel Figure 1: A schema of the lexical selection process: source sentence S has |G |lexical selection paths gi : lexsel selects one of them g ? , which is used to generate translation τ (g ? , S). where Ts (s) is the set of possible"
2015.eamt-1.20,J03-1002,0,0.00850824,"ough for all pairs there is a parallel corpus available for evaluation (see Section 3.3).5 Breton–French (Tyers, 2010): Bilingual dictionaries were not built with polysemy in mind from the outset, but some entries were added later to start work on lexical selection.6 Macedonian–English: The Macedonian–English pair in Apertium was created specifically for the purposes of running lexical-selection experiments. The lexical resources for the pair were tuned to the SETimes parallel corpus (Tyers and Alperen, 2010). The most probable entry from automatic word alignment of this corpus using GIZA ++ (Och and Ney, 2003) was checked to ensure that it was an adequate translation, and if so marked as the default.7 As a result of attempting to include all possible translations, the average number of translations per word is much higher than in other pairs.8 Basque–Spanish (Ginest´ı-Rosell et al., 2009): alternative translations were included in the bilingual dictionary.9 English–Spanish: The English–Spanish pair was developed from a combination of the English– Catalan and Spanish–Catalan pairs, and contains a number of entries in the bilingual dictionary with more than one translation.10 3.3 Performance measures"
2015.eamt-1.20,P02-1040,0,0.100052,"Missing"
2015.eamt-1.20,P11-1002,0,0.0207978,"andi (t, c) = handi follows arrain    0 otherwise (1) This feature considers a context of zero words to the left of the problem word and one word (+ handi) to the right of it. As a result of training, each of the nF features hsk (t, c) in the classifier is assigned a weight λsk . Combining these weights of active features as in equation (2) yields the probability of a translation t for word s in context c. n ps (t|c) = (2) k=1 In this equation, Z s (c) is a normalising constant. Thus, the most probable translation t? can be found using t? = arg max ps (t|c) = arg max t∈Ts (s) 2 The work by Ravi and Knight (2011) and Nuhn and Ney (2014), who decipher word-ciphered text using monolingual corpora only may be seen as a generalised version of the problem of lexical selection without parallel corpora. F X 1 exp λsk hsk (t, c) Z s (c) nF X λsk hsk (t, c), t∈Ts (s) k=1 (3) 3 146 We follow the notation of Berger et al. (1996) S→ preposti=|G| → ({gi }i=1 , S) → lexsel → (g ? , S) → → τ (g ? , S) lexsel lexsel Figure 1: A schema of the lexical selection process: source sentence S has |G |lexical selection paths gi : lexsel selects one of them g ? , which is used to generate translation τ (g ? , S). where Ts (s)"
2015.eamt-1.20,2010.eamt-1.13,1,0.827828,"uild RBMT systems. Translation is implemented as a pipeline consisting of the following modules: morphological analysis, morphological disambiguation, lexical transfer, lexical selection, structural transfer and morphological generation. 3.2 Language pairs Evaluation will be performed using four Apertium (Forcada et al., 2011) language pairs. These pairs have been selected as they include languages with different morphological complexity, and different amounts of resources available — although for all pairs there is a parallel corpus available for evaluation (see Section 3.3).5 Breton–French (Tyers, 2010): Bilingual dictionaries were not built with polysemy in mind from the outset, but some entries were added later to start work on lexical selection.6 Macedonian–English: The Macedonian–English pair in Apertium was created specifically for the purposes of running lexical-selection experiments. The lexical resources for the pair were tuned to the SETimes parallel corpus (Tyers and Alperen, 2010). The most probable entry from automatic word alignment of this corpus using GIZA ++ (Och and Ney, 2003) was checked to ensure that it was an adequate translation, and if so marked as the default.7 As a r"
2015.eamt-1.20,2012.eamt-1.54,1,0.760078,"Missing"
2015.eamt-1.20,H05-1097,0,0.0896984,"Missing"
2015.eamt-1.4,W14-3339,0,0.136457,"Missing"
2015.eamt-1.4,W13-2242,0,0.274712,"Missing"
2015.eamt-1.4,W11-2131,0,0.07342,"Missing"
2015.eamt-1.4,C04-1046,0,0.124736,"ield of machine translation (MT) have led to the adoption of this technology by many companies and institutions all around the world in order to bypass the linguistic barriers and reach out to broader audiences. Unfortunately, we are still far from the point of having MT systems able to produce translations with the level of quality required for dissemination in formal scenarios, where human supervision and MT post-editing are unavoidable. It therefore becomes critical to minimise the cost of this human post-editing. This has motivated a growing interest in the field of MT quality estimation (Blatz et al., 2004; Specia et al., 2010; Specia and Soricut, 2013), which is the field that focuses on developing techniques that allow to estimate the quality of the translation hypotheses produced by an MT system. Most efforts in MT quality estimation (MTQE) are aimed at evaluating the quality of whole translated segments, in terms of post-editing time, number of editions needed, and other related metrics (Blatz et al., 2004). Our work is focused on the sub-field of word-level MTQE. The main advantage of word-level MTQE is that it allows not only to estimate the effort needed to post-edit the output of an MT"
2015.eamt-1.4,J93-2003,0,0.0476226,"othesis T of the SL sentence S to help the interactive MT system to choose the translation suggestions to be made to the user. Ueffing and Ney (2005) extend this application to word-level MTQE also to automatically reject those target words t with low confidence scores from the translation proposals. This second approach incorporates the use of probabilistic lexicons as a source of translation information. Blatz et al. (2003) introduce a more complex collection of features for word-level MTQE, using semantic features based on WordNet (Miller, 1995), translation probabilities from IBM model 1 (Brown et al., 1993), word posterior probabilities (Blatz et al., 2003), and alignment templates from statistical MT (SMT) models. All the features they use are combined to train a binary classifier which is used to determine the confidence scores. Ueffing and Ney (2007) divide the features used 20 by their approach in two types: those which are independent of the MT system used for translation (system-independent), and those which are extracted from internal data of the SMT system they use for translation (system-dependent). These features are obtained by comparing the output of an SMT system T1 to a collection"
2015.eamt-1.4,W14-3340,0,0.0728255,"Missing"
2015.eamt-1.4,2011.mtsummit-papers.18,1,0.898234,"Missing"
2015.eamt-1.4,W03-0413,0,0.139643,"e fly for new translations. The rest of the paper is organised as follows. Section 2 briefly reviews the state of the art in word-level MTQE. Section 3 describes our binaryclassification approach, the sources of information, and the collection of features used. Section 4 describes the experimental setting used for our experiments, whereas Section 5 reports and discusses the results obtained. The paper ends with some concluding remarks and the description of ongoing and possible future work. 2 Related work Some of the early work on word-level MTQE can be found in the context of interactive MT (Gandrabur and Foster, 2003; Ueffing and Ney, 2005). Gandrabur and Foster (2003) obtain confidence scores for each word t in a given translation hypothesis T of the SL sentence S to help the interactive MT system to choose the translation suggestions to be made to the user. Ueffing and Ney (2005) extend this application to word-level MTQE also to automatically reject those target words t with low confidence scores from the translation proposals. This second approach incorporates the use of probabilistic lexicons as a source of translation information. Blatz et al. (2003) introduce a more complex collection of features f"
2015.eamt-1.4,2013.tc-1.10,0,0.0781078,"to the adoption of this technology by many companies and institutions all around the world in order to bypass the linguistic barriers and reach out to broader audiences. Unfortunately, we are still far from the point of having MT systems able to produce translations with the level of quality required for dissemination in formal scenarios, where human supervision and MT post-editing are unavoidable. It therefore becomes critical to minimise the cost of this human post-editing. This has motivated a growing interest in the field of MT quality estimation (Blatz et al., 2004; Specia et al., 2010; Specia and Soricut, 2013), which is the field that focuses on developing techniques that allow to estimate the quality of the translation hypotheses produced by an MT system. Most efforts in MT quality estimation (MTQE) are aimed at evaluating the quality of whole translated segments, in terms of post-editing time, number of editions needed, and other related metrics (Blatz et al., 2004). Our work is focused on the sub-field of word-level MTQE. The main advantage of word-level MTQE is that it allows not only to estimate the effort needed to post-edit the output of an MT system, but also to guide post-editors on which"
2015.eamt-1.4,P13-4014,0,0.0759888,"Missing"
2015.eamt-1.4,2005.eamt-1.35,0,0.0718838,"The rest of the paper is organised as follows. Section 2 briefly reviews the state of the art in word-level MTQE. Section 3 describes our binaryclassification approach, the sources of information, and the collection of features used. Section 4 describes the experimental setting used for our experiments, whereas Section 5 reports and discusses the results obtained. The paper ends with some concluding remarks and the description of ongoing and possible future work. 2 Related work Some of the early work on word-level MTQE can be found in the context of interactive MT (Gandrabur and Foster, 2003; Ueffing and Ney, 2005). Gandrabur and Foster (2003) obtain confidence scores for each word t in a given translation hypothesis T of the SL sentence S to help the interactive MT system to choose the translation suggestions to be made to the user. Ueffing and Ney (2005) extend this application to word-level MTQE also to automatically reject those target words t with low confidence scores from the translation proposals. This second approach incorporates the use of probabilistic lexicons as a source of translation information. Blatz et al. (2003) introduce a more complex collection of features for word-level MTQE, usin"
2015.eamt-1.4,J07-1003,0,0.0834112,"low confidence scores from the translation proposals. This second approach incorporates the use of probabilistic lexicons as a source of translation information. Blatz et al. (2003) introduce a more complex collection of features for word-level MTQE, using semantic features based on WordNet (Miller, 1995), translation probabilities from IBM model 1 (Brown et al., 1993), word posterior probabilities (Blatz et al., 2003), and alignment templates from statistical MT (SMT) models. All the features they use are combined to train a binary classifier which is used to determine the confidence scores. Ueffing and Ney (2007) divide the features used 20 by their approach in two types: those which are independent of the MT system used for translation (system-independent), and those which are extracted from internal data of the SMT system they use for translation (system-dependent). These features are obtained by comparing the output of an SMT system T1 to a collection of alternative T translations {Ti }N i=2 obtained by using the N -best list from the same SMT system. Several distance metrics are then used to check how often word tj , the word in position j of T , is found in each translation alternative Ti , and h"
2015.eamt-1.4,W14-3302,0,\N,Missing
2015.eamt-1.5,C04-1046,0,0.314325,"ensed under a Creative Commons 3.0 licence, no derivative works, attribution, CCBY-ND. 27 Machine translation: You could, for instance, use machine translation (MT) to get a draft of the translation of each segment, MT(si ); vendors and experts tell you that you will save effort by postediting MT(si ) into your desired translation ti .1 Machine translation output MT(si ) may just be text, but it could come with annotations to help you make the most of it; for instance, words could be color-coded according to how confident the system is about them (Ueffing and Ney, 2007; Ueffing and Ney, 2005; Blatz et al., 2004), or unknown words that come out untranslated may be marked so that you spot them clearly. Machine-translated segments could even be accompanied by indicators of their estimated quality (Specia and Soricut, 2013; Specia et al., 2010; Blatz et al., 2004) which may be used to ascertain whether the output of the MT system is worth being post-edited or not. If someone measured your post-editing effort (in time, in number of keystrokes, in number of words changed, in money you would have to pay another translator to do it, etc.), when turning MT(si ) into ti , they could call that effort eMT i . Tr"
2015.eamt-1.5,2011.eamt-1.28,1,0.850282,"ually as a percentage called fuzzy match score that accounts for the amount of text that is common to both segments2 — and even marks for you the words in s?i that do not match those in si . Let’s call all this information TM(si ): your job is to use it to turn t?i into the final translation ti . If the fuzzy match is good, you will spend less effort than if you started from scratch. Let us call eTM the effort to turn the t?i provided by i TM(si ) into the desired translation ti .3 Mixing them up: You could even have available another technology, fuzzy-match repair (FMR; (Ortega et al., 2014; Dandapat et al., 2011; Hewavitharana et al., 2005; Kranias and Samiotou, 2004)), that integrates the two technologies just mentioned: after a suitable fuzzy match is found, machine translation (or another source of bilingual information) is used to repair, i.e. edit some parts of t?i , to take into account what changes from s?i to si to try to save even more effort; it tells you all that TM(si ) tells you, but also marks the parts that have been repaired. Fuzzy-match repair is one of the technologies that TAUS, the Translation Automation User Society, calls advanced leveraging;4 commercial examples of these are De"
2015.eamt-1.5,W10-1751,0,0.0550422,"Missing"
2015.eamt-1.5,2012.amta-papers.6,0,0.0136534,"., 2002), using reference translations in a development set. Most of these automatic 9 The measurements of effort that one can find in literature vary from simple scores for “perceived” post-editing effort (usually scores taking 3 or 4 values) to actual post-editing time (see, for instance, the quality estimation task in WMT 2014 (Bojar et al., 2014)) measures are measures of similarity (or dissimilarity) between raw and reference translations. Researchers hope that their use during tuning will lead to a reduction in translation effort, although this is not currently guaranteed —for instance, Denkowski and Lavie (2012) found that BLEU could not distinguish between raw and post-edited machine translation. Generally, an automatic evaluation measure for technology X may have the form i i eˆX (X(si ; ~λX ), {tij }nj=1 ;µ ~ X ), where {tij }nj=1 is the set of reference translations for segment si in the development set and µ ~ X is a set of tunable X i parameters. Ideally, eˆ (X(si ; ~λX ), {tij }nj=1 ;µ ~X) should approximate eX (X(si ; ~λX )), but tuning of µ ~ X is surprisingly absent from current MT practice (with some exceptions, see Denkowski and Lavie i (2010)). In fact, eˆX (X(si ; ~λX ), {tij }nj=1 ;µ ~"
2015.eamt-1.5,P10-1064,0,0.0407252,"Missing"
2015.eamt-1.5,2010.amta-papers.27,0,0.0336096,"Missing"
2015.eamt-1.5,2005.eamt-1.18,0,0.0141542,"alled fuzzy match score that accounts for the amount of text that is common to both segments2 — and even marks for you the words in s?i that do not match those in si . Let’s call all this information TM(si ): your job is to use it to turn t?i into the final translation ti . If the fuzzy match is good, you will spend less effort than if you started from scratch. Let us call eTM the effort to turn the t?i provided by i TM(si ) into the desired translation ti .3 Mixing them up: You could even have available another technology, fuzzy-match repair (FMR; (Ortega et al., 2014; Dandapat et al., 2011; Hewavitharana et al., 2005; Kranias and Samiotou, 2004)), that integrates the two technologies just mentioned: after a suitable fuzzy match is found, machine translation (or another source of bilingual information) is used to repair, i.e. edit some parts of t?i , to take into account what changes from s?i to si to try to save even more effort; it tells you all that TM(si ) tells you, but also marks the parts that have been repaired. Fuzzy-match repair is one of the technologies that TAUS, the Translation Automation User Society, calls advanced leveraging;4 commercial examples of these are DeepMiner in Atril’s D´ej`a Vu"
2015.eamt-1.5,J10-4005,0,0.0235175,"on a development set made of bilingual segments and translation effort measurements eMT (MT(si ; ~λMT )).9 X? where ei i is the effort expended in translating segment si using the best technology Xi? for that segment, that is, the one that minimizes that effort. To minimize the translation effort on a specific task, designers have to work in two main areas: Improving each technology: One is to improve the output of each technology X, ideally focusing on those cases when X is going to be selected. Some such technologies have tunable parameters; for instance, feature weights in statistical MT (Koehn, 2010, p. 255); for other technologies, this is not usually reported, but it is not impossible to think, for instance, of fuzzy-match scores that give different weights to different kinds of edit operations. Let us call ~λX the vector of tunable parameters for technology X; as the output of technology X varies with these parameters, we can write its output like this: X(si ; ~λX ). Learning to select the best technology: The other one is that the CAT environment needs a way to select the best technology Xi? for each segment si , obviously without measuring the actual effort. To do this, CAT designer"
2015.eamt-1.5,kranias-samiotou-2004-automatic,0,0.0269917,"accounts for the amount of text that is common to both segments2 — and even marks for you the words in s?i that do not match those in si . Let’s call all this information TM(si ): your job is to use it to turn t?i into the final translation ti . If the fuzzy match is good, you will spend less effort than if you started from scratch. Let us call eTM the effort to turn the t?i provided by i TM(si ) into the desired translation ti .3 Mixing them up: You could even have available another technology, fuzzy-match repair (FMR; (Ortega et al., 2014; Dandapat et al., 2011; Hewavitharana et al., 2005; Kranias and Samiotou, 2004)), that integrates the two technologies just mentioned: after a suitable fuzzy match is found, machine translation (or another source of bilingual information) is used to repair, i.e. edit some parts of t?i , to take into account what changes from s?i to si to try to save even more effort; it tells you all that TM(si ) tells you, but also marks the parts that have been repaired. Fuzzy-match repair is one of the technologies that TAUS, the Translation Automation User Society, calls advanced leveraging;4 commercial examples of these are DeepMiner in Atril’s D´ej`a Vu,5 and ALTM in MultiCorpora’s"
2015.eamt-1.5,P03-1021,0,0.328022,"nologies and for all segments. Therefore it is in principle not easy to determine the parameters θ~X to get good estimates e˜X (X(si ; ~λX ); θ~X ). We will see a way to do this below. Tuning technologies is also hard: Technologies may have tunable parameters ~λX which determine the output they produce. Obviously, one cannot just repetitively measure the actual effort spent by translators in editing their output for a wide variety of values of ~λX , as this is clearly impracticable; therefore, an alternative is needed. When X = MT, this is usually done by means of an algorithm that optimizes (Och, 2003; Chiang, 2012) automatic evaluation measures, such as BLEU (Papineni et al., 2002), using reference translations in a development set. Most of these automatic 9 The measurements of effort that one can find in literature vary from simple scores for “perceived” post-editing effort (usually scores taking 3 or 4 values) to actual post-editing time (see, for instance, the quality estimation task in WMT 2014 (Bojar et al., 2014)) measures are measures of similarity (or dissimilarity) between raw and reference translations. Researchers hope that their use during tuning will lead to a reduction in tr"
2015.eamt-1.5,2014.amta-researchers.4,1,0.890202,"Missing"
2015.eamt-1.5,P02-1040,0,0.103881,"o determine the parameters θ~X to get good estimates e˜X (X(si ; ~λX ); θ~X ). We will see a way to do this below. Tuning technologies is also hard: Technologies may have tunable parameters ~λX which determine the output they produce. Obviously, one cannot just repetitively measure the actual effort spent by translators in editing their output for a wide variety of values of ~λX , as this is clearly impracticable; therefore, an alternative is needed. When X = MT, this is usually done by means of an algorithm that optimizes (Och, 2003; Chiang, 2012) automatic evaluation measures, such as BLEU (Papineni et al., 2002), using reference translations in a development set. Most of these automatic 9 The measurements of effort that one can find in literature vary from simple scores for “perceived” post-editing effort (usually scores taking 3 or 4 values) to actual post-editing time (see, for instance, the quality estimation task in WMT 2014 (Bojar et al., 2014)) measures are measures of similarity (or dissimilarity) between raw and reference translations. Researchers hope that their use during tuning will lead to a reduction in translation effort, although this is not currently guaranteed —for instance, Denkowsk"
2015.eamt-1.5,2013.mtsummit-papers.21,0,0.0138473,"egment si in the development set and µ ~ X is a set of tunable X i parameters. Ideally, eˆ (X(si ; ~λX ), {tij }nj=1 ;µ ~X) should approximate eX (X(si ; ~λX )), but tuning of µ ~ X is surprisingly absent from current MT practice (with some exceptions, see Denkowski and Lavie i (2010)). In fact, eˆX (X(si ; ~λX ), {tij }nj=1 ;µ ~ X ) can be seen as a special estimator of effort, much like e˜X (X(si ; ~λX ); θ~X ), but informed with reference i translations {tij }nj=1 when they are available. This is similar to the use of pseudo-reference translations in machine translation quality estimation (Shah et al., 2013; Soricut and Narsale, 2012; Soricut et al., 2012; Soricut and Echihabi, 2010), but with actual references. Table 1 summarizes the main concepts and the notation used along the paper. 3 A general framework for training the whole CAT environment We describe a possible workflow to tune simultaneously the different technologies that may be used in a CAT environment and the estimators used to select them on a segment basis: 1. Design automatic evaluation measures eˆX (X(si , ~λX ), {tij }; µ ~ X ) and estimators X X X e˜ (X(si ; ~λ ); θ~ ) for each technology X ∈ X , based on a series of relevant"
2015.eamt-1.5,2009.mtsummit-papers.14,0,0.0271238,"ven better, couldn’t the decision of selecting the best technology Xi? , that is, the one that minimizes your effort for each segment si , be made automatically? It is therefore clear that a framework that allows to seamlessly integrate all the translation technologies available in the CAT system is very much needed to make the most of all of them and minimize translation effort as much as possible. Previous work on technology selection: The specific case of automatically choosing between machine translation output and translation memory fuzzy matches has received attention in the last years. Simard and Isabelle (2009) proposed a simple approach called β-combination, which simply selects machine translation when there is no translation memory proposal with a fuzzy match score above a given threshold β, which can be tuned. He et al. (2010a) and He et al. (2010b) approach this problem, which they call translation recommendation, by training a classifier which selects which of the two, TM(si ) or MT(si ), gets the lowest value for an approximate indicator of effort, called translation error rate (TER, (Snover et al., 2006)). Their training compares outputs to preexisting reference translations; their ideas are"
2015.eamt-1.5,2006.amta-papers.25,0,0.034682,"put and translation memory fuzzy matches has received attention in the last years. Simard and Isabelle (2009) proposed a simple approach called β-combination, which simply selects machine translation when there is no translation memory proposal with a fuzzy match score above a given threshold β, which can be tuned. He et al. (2010a) and He et al. (2010b) approach this problem, which they call translation recommendation, by training a classifier which selects which of the two, TM(si ) or MT(si ), gets the lowest value for an approximate indicator of effort, called translation error rate (TER, (Snover et al., 2006)). Their training compares outputs to preexisting reference translations; their ideas are generalized in the approach proposed in this paper. The next section explains two ways to minimize the effort needed to perform a translation job in a CAT environment integrating different technologies. Section 3 then describes our proposal for a general framework for training the whole CAT environment. Finally, we discuss the implications of having such a framework. 2 to come up with a set of estimators e˜X , one for each technology. These estimators should be trained to give the best possible estimate o"
2015.eamt-1.5,P10-1063,0,0.0244939,"arameters. Ideally, eˆ (X(si ; ~λX ), {tij }nj=1 ;µ ~X) should approximate eX (X(si ; ~λX )), but tuning of µ ~ X is surprisingly absent from current MT practice (with some exceptions, see Denkowski and Lavie i (2010)). In fact, eˆX (X(si ; ~λX ), {tij }nj=1 ;µ ~ X ) can be seen as a special estimator of effort, much like e˜X (X(si ; ~λX ); θ~X ), but informed with reference i translations {tij }nj=1 when they are available. This is similar to the use of pseudo-reference translations in machine translation quality estimation (Shah et al., 2013; Soricut and Narsale, 2012; Soricut et al., 2012; Soricut and Echihabi, 2010), but with actual references. Table 1 summarizes the main concepts and the notation used along the paper. 3 A general framework for training the whole CAT environment We describe a possible workflow to tune simultaneously the different technologies that may be used in a CAT environment and the estimators used to select them on a segment basis: 1. Design automatic evaluation measures eˆX (X(si , ~λX ), {tij }; µ ~ X ) and estimators X X X e˜ (X(si ; ~λ ); θ~ ) for each technology X ∈ X , based on a series of relevant features that can easily be extracted from si and X(si ), and which will depen"
2015.eamt-1.5,W12-3121,0,0.0187232,"velopment set and µ ~ X is a set of tunable X i parameters. Ideally, eˆ (X(si ; ~λX ), {tij }nj=1 ;µ ~X) should approximate eX (X(si ; ~λX )), but tuning of µ ~ X is surprisingly absent from current MT practice (with some exceptions, see Denkowski and Lavie i (2010)). In fact, eˆX (X(si ; ~λX ), {tij }nj=1 ;µ ~ X ) can be seen as a special estimator of effort, much like e˜X (X(si ; ~λX ); θ~X ), but informed with reference i translations {tij }nj=1 when they are available. This is similar to the use of pseudo-reference translations in machine translation quality estimation (Shah et al., 2013; Soricut and Narsale, 2012; Soricut et al., 2012; Soricut and Echihabi, 2010), but with actual references. Table 1 summarizes the main concepts and the notation used along the paper. 3 A general framework for training the whole CAT environment We describe a possible workflow to tune simultaneously the different technologies that may be used in a CAT environment and the estimators used to select them on a segment basis: 1. Design automatic evaluation measures eˆX (X(si , ~λX ), {tij }; µ ~ X ) and estimators X X X e˜ (X(si ; ~λ ); θ~ ) for each technology X ∈ X , based on a series of relevant features that can easily be"
2015.eamt-1.5,W12-3118,0,0.0179831,"a set of tunable X i parameters. Ideally, eˆ (X(si ; ~λX ), {tij }nj=1 ;µ ~X) should approximate eX (X(si ; ~λX )), but tuning of µ ~ X is surprisingly absent from current MT practice (with some exceptions, see Denkowski and Lavie i (2010)). In fact, eˆX (X(si ; ~λX ), {tij }nj=1 ;µ ~ X ) can be seen as a special estimator of effort, much like e˜X (X(si ; ~λX ); θ~X ), but informed with reference i translations {tij }nj=1 when they are available. This is similar to the use of pseudo-reference translations in machine translation quality estimation (Shah et al., 2013; Soricut and Narsale, 2012; Soricut et al., 2012; Soricut and Echihabi, 2010), but with actual references. Table 1 summarizes the main concepts and the notation used along the paper. 3 A general framework for training the whole CAT environment We describe a possible workflow to tune simultaneously the different technologies that may be used in a CAT environment and the estimators used to select them on a segment basis: 1. Design automatic evaluation measures eˆX (X(si , ~λX ), {tij }; µ ~ X ) and estimators X X X e˜ (X(si ; ~λ ); θ~ ) for each technology X ∈ X , based on a series of relevant features that can easily be extracted from si and"
2015.eamt-1.5,2013.tc-1.10,0,0.103188,"segment, MT(si ); vendors and experts tell you that you will save effort by postediting MT(si ) into your desired translation ti .1 Machine translation output MT(si ) may just be text, but it could come with annotations to help you make the most of it; for instance, words could be color-coded according to how confident the system is about them (Ueffing and Ney, 2007; Ueffing and Ney, 2005; Blatz et al., 2004), or unknown words that come out untranslated may be marked so that you spot them clearly. Machine-translated segments could even be accompanied by indicators of their estimated quality (Specia and Soricut, 2013; Specia et al., 2010; Blatz et al., 2004) which may be used to ascertain whether the output of the MT system is worth being post-edited or not. If someone measured your post-editing effort (in time, in number of keystrokes, in number of words changed, in money you would have to pay another translator to do it, etc.), when turning MT(si ) into ti , they could call that effort eMT i . Translation memory: You could also use a translation memory (TM; (Somers, 2003)), where previously translated segments s are stored together with their translations t in pairs called translation units (s, t). The"
2015.eamt-1.5,H05-1096,0,0.0348615,"rs. This article is licensed under a Creative Commons 3.0 licence, no derivative works, attribution, CCBY-ND. 27 Machine translation: You could, for instance, use machine translation (MT) to get a draft of the translation of each segment, MT(si ); vendors and experts tell you that you will save effort by postediting MT(si ) into your desired translation ti .1 Machine translation output MT(si ) may just be text, but it could come with annotations to help you make the most of it; for instance, words could be color-coded according to how confident the system is about them (Ueffing and Ney, 2007; Ueffing and Ney, 2005; Blatz et al., 2004), or unknown words that come out untranslated may be marked so that you spot them clearly. Machine-translated segments could even be accompanied by indicators of their estimated quality (Specia and Soricut, 2013; Specia et al., 2010; Blatz et al., 2004) which may be used to ascertain whether the output of the MT system is worth being post-edited or not. If someone measured your post-editing effort (in time, in number of keystrokes, in number of words changed, in money you would have to pay another translator to do it, etc.), when turning MT(si ) into ti , they could call t"
2015.eamt-1.5,J07-1003,0,0.0180096,"; they c 2015 The authors. This article is licensed under a Creative Commons 3.0 licence, no derivative works, attribution, CCBY-ND. 27 Machine translation: You could, for instance, use machine translation (MT) to get a draft of the translation of each segment, MT(si ); vendors and experts tell you that you will save effort by postediting MT(si ) into your desired translation ti .1 Machine translation output MT(si ) may just be text, but it could come with annotations to help you make the most of it; for instance, words could be color-coded according to how confident the system is about them (Ueffing and Ney, 2007; Ueffing and Ney, 2005; Blatz et al., 2004), or unknown words that come out untranslated may be marked so that you spot them clearly. Machine-translated segments could even be accompanied by indicators of their estimated quality (Specia and Soricut, 2013; Specia et al., 2010; Blatz et al., 2004) which may be used to ascertain whether the output of the MT system is worth being post-edited or not. If someone measured your post-editing effort (in time, in number of keystrokes, in number of words changed, in money you would have to pay another translator to do it, etc.), when turning MT(si ) into"
2016.amta-researchers.3,2011.eamt-1.28,1,0.956104,"to determine the potential of the method. The paper ends with some remarks and a description of future research lines. 2 Related Work In the literature, one can ﬁnd many papers addressing the combination of machine translation and translation memories, most of which explore different ways of integrating sub-segments from the translation memory into the decoding process of a phrase-based statistical machine translation system (Bic¸ici and Dymetman, 2008; Simard and Isabelle, 2009; Zhechev and Genabith, 2010; Koehn and Senellart, 2010; Li et al., 2016). Alternative approaches, such as those by Dandapat et al. (2011), Hewavitharana et al. (2005) and Kranias and Samiotou (2004), use instead the target segment t in a translation unit (s, t) as the backbone or the basis of the translation to be produced and describe ways to repair it by modifying those sub-segments in t that are the translation of the mismatched sub-segments in s. The method proposed here, which extends that of Ortega et al. (2014), belongs to this second group. Dandapat et al. (2011)’s method ﬁrst aligns, in a way similar to ours, the words in s and s using the (word-based) edit distance (Levenshtein, 1966) and marks the mismatched subsegm"
2016.amta-researchers.3,2011.mtsummit-papers.18,1,0.846391,"Missing"
2016.amta-researchers.3,2005.eamt-1.18,0,0.757637,"ial of the method. The paper ends with some remarks and a description of future research lines. 2 Related Work In the literature, one can ﬁnd many papers addressing the combination of machine translation and translation memories, most of which explore different ways of integrating sub-segments from the translation memory into the decoding process of a phrase-based statistical machine translation system (Bic¸ici and Dymetman, 2008; Simard and Isabelle, 2009; Zhechev and Genabith, 2010; Koehn and Senellart, 2010; Li et al., 2016). Alternative approaches, such as those by Dandapat et al. (2011), Hewavitharana et al. (2005) and Kranias and Samiotou (2004), use instead the target segment t in a translation unit (s, t) as the backbone or the basis of the translation to be produced and describe ways to repair it by modifying those sub-segments in t that are the translation of the mismatched sub-segments in s. The method proposed here, which extends that of Ortega et al. (2014), belongs to this second group. Dandapat et al. (2011)’s method ﬁrst aligns, in a way similar to ours, the words in s and s using the (word-based) edit distance (Levenshtein, 1966) and marks the mismatched subsegments in s and s for translat"
2016.amta-researchers.3,J10-4005,0,0.249192,"ed) edit distance (Levenshtein, 1966) and marks the mismatched subsegments in s and s for translation. It then aligns the mismatch sub-segments in s with their 3 The translation memory of the Directorate General for Translation of the European Comission, https://ec. europa.eu/jrc/en/language-technologies/dgt-translation-memory 3URFHHGLQJVRI$07$YRO075HVHDUFKHUV 7UDFN $XVWLQ2FW1RY_S counterparts in t by using a sub-segmental translation memory built on the user’s translation memory following the standard method to obtain phrase tables in statistical MT (Koehn, 2010). Finally the sub-segments in t aligned to mismatched sub-segments in s are replaced by the translations of the corresponding sub-segments in s as they are found in the sub-segmental translation memory. The main differences with the approach described here are (a) that Dandapat et al. (2011) do not take into account the context words around the mismatches —which may lead to incorrect translations due to boundary friction problems such as incorrect agreement or incomplete word reorderings— and (b) that they rely on the user’s translation memory (which may be small) rather than on an external S"
2016.amta-researchers.3,2010.jec-1.4,0,0.66572,"ss the experimental settings and the results of an oracle evaluation we have conducted to determine the potential of the method. The paper ends with some remarks and a description of future research lines. 2 Related Work In the literature, one can ﬁnd many papers addressing the combination of machine translation and translation memories, most of which explore different ways of integrating sub-segments from the translation memory into the decoding process of a phrase-based statistical machine translation system (Bic¸ici and Dymetman, 2008; Simard and Isabelle, 2009; Zhechev and Genabith, 2010; Koehn and Senellart, 2010; Li et al., 2016). Alternative approaches, such as those by Dandapat et al. (2011), Hewavitharana et al. (2005) and Kranias and Samiotou (2004), use instead the target segment t in a translation unit (s, t) as the backbone or the basis of the translation to be produced and describe ways to repair it by modifying those sub-segments in t that are the translation of the mismatched sub-segments in s. The method proposed here, which extends that of Ortega et al. (2014), belongs to this second group. Dandapat et al. (2011)’s method ﬁrst aligns, in a way similar to ours, the words in s and s using"
2016.amta-researchers.3,kranias-samiotou-2004-automatic,0,0.791791,"s with some remarks and a description of future research lines. 2 Related Work In the literature, one can ﬁnd many papers addressing the combination of machine translation and translation memories, most of which explore different ways of integrating sub-segments from the translation memory into the decoding process of a phrase-based statistical machine translation system (Bic¸ici and Dymetman, 2008; Simard and Isabelle, 2009; Zhechev and Genabith, 2010; Koehn and Senellart, 2010; Li et al., 2016). Alternative approaches, such as those by Dandapat et al. (2011), Hewavitharana et al. (2005) and Kranias and Samiotou (2004), use instead the target segment t in a translation unit (s, t) as the backbone or the basis of the translation to be produced and describe ways to repair it by modifying those sub-segments in t that are the translation of the mismatched sub-segments in s. The method proposed here, which extends that of Ortega et al. (2014), belongs to this second group. Dandapat et al. (2011)’s method ﬁrst aligns, in a way similar to ours, the words in s and s using the (word-based) edit distance (Levenshtein, 1966) and marks the mismatched subsegments in s and s for translation. It then aligns the mismatch"
2016.amta-researchers.3,2009.mtsummit-papers.14,0,0.721024,"om working on the same mismatch. Sections 5 and 6 discuss the experimental settings and the results of an oracle evaluation we have conducted to determine the potential of the method. The paper ends with some remarks and a description of future research lines. 2 Related Work In the literature, one can ﬁnd many papers addressing the combination of machine translation and translation memories, most of which explore different ways of integrating sub-segments from the translation memory into the decoding process of a phrase-based statistical machine translation system (Bic¸ici and Dymetman, 2008; Simard and Isabelle, 2009; Zhechev and Genabith, 2010; Koehn and Senellart, 2010; Li et al., 2016). Alternative approaches, such as those by Dandapat et al. (2011), Hewavitharana et al. (2005) and Kranias and Samiotou (2004), use instead the target segment t in a translation unit (s, t) as the backbone or the basis of the translation to be produced and describe ways to repair it by modifying those sub-segments in t that are the translation of the mismatched sub-segments in s. The method proposed here, which extends that of Ortega et al. (2014), belongs to this second group. Dandapat et al. (2011)’s method ﬁrst aligns,"
2016.amta-researchers.3,2013.tc-1.10,0,0.0586535,"Missing"
2016.amta-researchers.3,W10-3806,0,0.0208675,"atch. Sections 5 and 6 discuss the experimental settings and the results of an oracle evaluation we have conducted to determine the potential of the method. The paper ends with some remarks and a description of future research lines. 2 Related Work In the literature, one can ﬁnd many papers addressing the combination of machine translation and translation memories, most of which explore different ways of integrating sub-segments from the translation memory into the decoding process of a phrase-based statistical machine translation system (Bic¸ici and Dymetman, 2008; Simard and Isabelle, 2009; Zhechev and Genabith, 2010; Koehn and Senellart, 2010; Li et al., 2016). Alternative approaches, such as those by Dandapat et al. (2011), Hewavitharana et al. (2005) and Kranias and Samiotou (2004), use instead the target segment t in a translation unit (s, t) as the backbone or the basis of the translation to be produced and describe ways to repair it by modifying those sub-segments in t that are the translation of the mismatched sub-segments in s. The method proposed here, which extends that of Ortega et al. (2014), belongs to this second group. Dandapat et al. (2011)’s method ﬁrst aligns, in a way similar to ours, t"
2020.coling-main.349,W19-4208,0,0.0284079,"e that BPE segmentation does not allow the system to learn a general mapping between tags and word endings from the training data. Another explanation could be related to the fact that predicting the morphological gender for German, Czech and Spanish forces the tag prediction task to be aware of TL lexical information, preventing an optimum division of labour between tag and surface form predictions. In conclusion, the prediction of TL morphological information needs to be factorised differently in order not to harm part-of-speech prediction. For instance, in the morphological analysis field, Chaudhary et al. (2019) and Straka et al. (2019) predict part of speech and each morphological attribute independently. 7 Concluding remarks In this paper, we have studied the effects of using linguistic annotations of SL and TL words in underresourced NMT by interleaving linguistic tags for different language pairs, architectures, training data sizes and types of linguistic information (part of speech and morpho-syntactic descriptions). We have shown that both SL and TL linguistic annotations are useful, in line with previous works in the literature (Wagner, 2017). SL linguistic annotations lead to more accurate SL"
2020.coling-main.349,D17-1304,0,0.0519678,"Missing"
2020.coling-main.349,W17-4727,0,0.0559986,"Missing"
2020.coling-main.349,D18-1037,0,0.0417741,"Missing"
2020.coling-main.349,W04-3250,0,0.133025,"re in bold means that the system outperforms the baseline (labelled as None) by a statistically significant margin. A bullet (•) next to the score of a system with interleaved POS or MSD tags means that it outperforms the system with DUM tags in the same language side (SL or TL) by a statistically significant margin.6 A dagger (†) next to the score of a system with POS or MSD tags means that it outperforms the system with the opposite tag (either MSD or POS) in the same language side by a statistically significant margin. Statistical significance was assessed with paired bootstrap resampling (Koehn, 2004) (p = 0.05; 1 000 iterations). As the four languages paired with English are morphologically richer than English, we split the analysis of the results we describe next into two groups: translation into a TL morphologically richer than the SL (pairs with English as SL), and translation from a morphologically richer SL (pairs with English as TL). It is also worth mentioning that, in all the scenarios evaluated, when a system was trained with interleaved TL tags, the decoder alternately produced TL tags and surface forms at test time as expected. Translation into a morphologically rich language."
2020.coling-main.349,J10-4005,0,0.0249903,"of automatic evaluation metrics, even though the use of morpho-syntactic description tags improves the grammaticality of the output. We provide a detailed analysis of the reasons behind this result. 1 Introduction Training neural machine translation (NMT) systems for under-resourced language pairs, for which the amount of parallel corpora is orders of magnitude smaller than those available for prevailing language pairs, may be challenging. Recently, Sennrich and Zhang (2019) have shown that even under these circumstances, NMT surpasses classical approaches such as phrase-based statistical MT (Koehn, 2010). In these under-resourced scenarios, the use of relevant linguistic word-level annotations has proved to improve translation performance (Sennrich and Haddow, 2016; Nadejde et al., 2017). Linguistic annotations can be used to label source-language (SL) or target-language (TL) words. The former lead to more accurate representations of the SL sentence (Sennrich and Haddow, 2016) and usually require changes in the encoder but not in the training loss (Chen et al., 2017; Li et al., 2018). The latter involve producing probability distributions for both TL words and TL linguistic annotations, which"
2020.coling-main.349,W19-4203,0,0.0280961,"r datasets for the same language pairs; and (iii) linguistic annotations can be obtained with the same state-of-the-art morphological analyser, minimising the potential distortions introduced by differences in the morphological analyser technology and in performance between the languages. The POS and MSD tags were obtained by means of the StandfordNLP tagger (Qi et al., 2018). In any case, the approaches described in this paper could be applied to truly under-resourced language pairs as transfer learning allows to obtain morphological analysers even from scarce morphologically annotated data (Kondratyuk, 2019). Corpora. The training, development and test sets used belong to the news domain. For training, we used texts from the News Commentary v14 corpus,2 except for Turkish, for which we used texts from the SETimes corpus (Tyers and Alperen, 2010). For development and testing we used evaluation sets from the WMT 2019 Conference on Machine Translation, each of which contains around 3,000 parallel sentences.3 To see if the conclusions drawn on the under-resourced settings hold in a richer-resourced scenario, we trained English–German systems (in both directions) on the concatenation of the parallel d"
2020.coling-main.349,W17-4707,0,0.220081,"behind this result. 1 Introduction Training neural machine translation (NMT) systems for under-resourced language pairs, for which the amount of parallel corpora is orders of magnitude smaller than those available for prevailing language pairs, may be challenging. Recently, Sennrich and Zhang (2019) have shown that even under these circumstances, NMT surpasses classical approaches such as phrase-based statistical MT (Koehn, 2010). In these under-resourced scenarios, the use of relevant linguistic word-level annotations has proved to improve translation performance (Sennrich and Haddow, 2016; Nadejde et al., 2017). Linguistic annotations can be used to label source-language (SL) or target-language (TL) words. The former lead to more accurate representations of the SL sentence (Sennrich and Haddow, 2016) and usually require changes in the encoder but not in the training loss (Chen et al., 2017; Li et al., 2018). The latter involve producing probability distributions for both TL words and TL linguistic annotations, which can be seen as a form of multi-task learning. Multi-task learning architectures explored in the literature include: independent decoders for words and linguistic annotations (Zhou et al."
2020.coling-main.349,P02-1040,0,0.107731,"tegy by Toral and S´anchez-Cartagena (2017), who used the tool Hjerson (Popovi´c, 2011) to classify word errors into five categories: inflection, reordering, missing words, extra words and incorrect lexical choices. As it is difficult to automatically distinguish between the latter three categories (Popovi´c and Ney, 2011), we grouped them into a unique category named lexical errors. Hjerson works on the surface form and lemma of the words in the reference translations and MT outputs. The lemmas were obtained again with the StandfordNLP tagger. 4 Results and discussion Table 2 shows the BLEU (Papineni et al., 2002) scores obtained by the different systems. A score in bold means that the system outperforms the baseline (labelled as None) by a statistically significant margin. A bullet (•) next to the score of a system with interleaved POS or MSD tags means that it outperforms the system with DUM tags in the same language side (SL or TL) by a statistically significant margin.6 A dagger (†) next to the score of a system with POS or MSD tags means that it outperforms the system with the opposite tag (either MSD or POS) in the same language side by a statistically significant margin. Statistical significance"
2020.coling-main.349,J11-4002,0,0.084782,"Missing"
2020.coling-main.349,E17-2025,0,0.0328302,"Missing"
2020.coling-main.349,K18-2016,0,0.0214488,"uly under-resourced language pairs: (i) we can choose languages from different families and evaluate them on standard, high-quality test sets; (ii) we can confirm whether conclusions hold for richer-resource scenarios by training on larger datasets for the same language pairs; and (iii) linguistic annotations can be obtained with the same state-of-the-art morphological analyser, minimising the potential distortions introduced by differences in the morphological analyser technology and in performance between the languages. The POS and MSD tags were obtained by means of the StandfordNLP tagger (Qi et al., 2018). In any case, the approaches described in this paper could be applied to truly under-resourced language pairs as transfer learning allows to obtain morphological analysers even from scarce morphologically annotated data (Kondratyuk, 2019). Corpora. The training, development and test sets used belong to the news domain. For training, we used texts from the News Commentary v14 corpus,2 except for Turkish, for which we used texts from the SETimes corpus (Tyers and Alperen, 2010). For development and testing we used evaluation sets from the WMT 2019 Conference on Machine Translation, each of whic"
2020.coling-main.349,W16-2209,0,0.135225,"led analysis of the reasons behind this result. 1 Introduction Training neural machine translation (NMT) systems for under-resourced language pairs, for which the amount of parallel corpora is orders of magnitude smaller than those available for prevailing language pairs, may be challenging. Recently, Sennrich and Zhang (2019) have shown that even under these circumstances, NMT surpasses classical approaches such as phrase-based statistical MT (Koehn, 2010). In these under-resourced scenarios, the use of relevant linguistic word-level annotations has proved to improve translation performance (Sennrich and Haddow, 2016; Nadejde et al., 2017). Linguistic annotations can be used to label source-language (SL) or target-language (TL) words. The former lead to more accurate representations of the SL sentence (Sennrich and Haddow, 2016) and usually require changes in the encoder but not in the training loss (Chen et al., 2017; Li et al., 2018). The latter involve producing probability distributions for both TL words and TL linguistic annotations, which can be seen as a form of multi-task learning. Multi-task learning architectures explored in the literature include: independent decoders for words and linguistic a"
2020.coling-main.349,P19-1021,0,0.0175114,"ntrary, when words are annotated in the target language, part-of-speech tags systematically outperform morpho-syntactic description tags in terms of automatic evaluation metrics, even though the use of morpho-syntactic description tags improves the grammaticality of the output. We provide a detailed analysis of the reasons behind this result. 1 Introduction Training neural machine translation (NMT) systems for under-resourced language pairs, for which the amount of parallel corpora is orders of magnitude smaller than those available for prevailing language pairs, may be challenging. Recently, Sennrich and Zhang (2019) have shown that even under these circumstances, NMT surpasses classical approaches such as phrase-based statistical MT (Koehn, 2010). In these under-resourced scenarios, the use of relevant linguistic word-level annotations has proved to improve translation performance (Sennrich and Haddow, 2016; Nadejde et al., 2017). Linguistic annotations can be used to label source-language (SL) or target-language (TL) words. The former lead to more accurate representations of the SL sentence (Sennrich and Haddow, 2016) and usually require changes in the encoder but not in the training loss (Chen et al.,"
2020.coling-main.349,W16-2323,0,0.167396,"s the reasons behind the poor performance of systems with TL morpho-syntactic description tags. The paper ends with some concluding remarks. 2 Interleaving in neural machine translation The interleaving approach for integrating linguistic annotations into NMT (Nadejde et al., 2017) annotates each word with a single tag which is interleaved in the sentence before the word, i.e. introduced in the sentence as if it were another word. In our experiments, tags can represent either the part of speech (POS) of the word or its morpho-syntactic description (MSD). As corpora are pre-processed with BPE (Sennrich et al., 2016b), the tag is introduced just once, before the first sub-word unit. To study if the effect of using tags is related to the fact that input and output sequences get longer and word boundaries are explicitly defined, and not to the information provided by tags, we also tried with a dummy tag (DUM) conveying no linguistic information at all, and used the same dummy tag for every word (Wagner, 2017). Interleaved TL tags are removed from the final translation generated by the system before computing the automatic evaluation metrics. The example below shows the result of interleaving MSD tags in th"
2020.coling-main.349,P16-1162,0,0.162144,"s the reasons behind the poor performance of systems with TL morpho-syntactic description tags. The paper ends with some concluding remarks. 2 Interleaving in neural machine translation The interleaving approach for integrating linguistic annotations into NMT (Nadejde et al., 2017) annotates each word with a single tag which is interleaved in the sentence before the word, i.e. introduced in the sentence as if it were another word. In our experiments, tags can represent either the part of speech (POS) of the word or its morpho-syntactic description (MSD). As corpora are pre-processed with BPE (Sennrich et al., 2016b), the tag is introduced just once, before the first sub-word unit. To study if the effect of using tags is related to the fact that input and output sequences get longer and word boundaries are explicitly defined, and not to the information provided by tags, we also tried with a dummy tag (DUM) conveying no linguistic information at all, and used the same dummy tag for every word (Wagner, 2017). Interleaved TL tags are removed from the final translation generated by the system before computing the automatic evaluation metrics. The example below shows the result of interleaving MSD tags in th"
2020.coling-main.349,W17-4739,0,0.0478108,"Missing"
2020.coling-main.349,W19-4212,0,0.0259463,"Missing"
2020.coling-main.349,W17-4704,0,0.057383,"Missing"
2020.coling-main.349,tiedemann-2012-parallel,0,0.0371198,"plus the synthetic parallel data obtained through back-translation released by Sennrich et al. (2016a). All corpora were tokenised and truecased with the Moses scripts5 and parallel sentences longer than 100 words in either side were discarded. Table 1 provides information about the training corpora after their pre-processing. We trained translation models on these corpora and on random sub-sets of them containing 50k parallel sentences (except for the WMT training data). The token counts depicted in Table 1 for the under-resourced scenario are similar to those listed in the OPUS collection (Tiedemann, 2012) for under-resourced language pairs such as English–Kurdish or English–Igbo; token counts for the 50k subsets match other pairs with even smaller resources available in OPUS, such as English–Kazakh. Translation models. We tested the performance of the recurrent-neural-network encoder-decoder with attention (hereafter, recurrent; Bahdanau et al., 2015) and the Transformer (Vaswani et al., 2017) architectures when the different types of tags introduced in Section 2 are interleaved in the SL input sequence, in the TL output sequence, and in both of them. For each architecture, we also trained a b"
2020.coling-main.349,E17-1100,1,0.89493,"Missing"
2020.coling-main.349,D18-1509,0,0.0196288,"el source-language (SL) or target-language (TL) words. The former lead to more accurate representations of the SL sentence (Sennrich and Haddow, 2016) and usually require changes in the encoder but not in the training loss (Chen et al., 2017; Li et al., 2018). The latter involve producing probability distributions for both TL words and TL linguistic annotations, which can be seen as a form of multi-task learning. Multi-task learning architectures explored in the literature include: independent decoders for words and linguistic annotations (Zhou et al., 2017; Wu et al., 2018; G¯u et al., 2018; Wang et al., 2018; Yang et al., 2019); independent output layers in the same decoder (Garc´ıa-Mart´ınez et al., 2016; Gr¨onroos et al., 2017; Feng et al., 2019); and even sharing the same network for both tasks (Nadejde et al., 2017; Tamchyna et al., 2017; Wagner, 2017) and alternatively produce linguistic annotations and words. The latter approach is usually referred to as interleaving. We can also classify the approaches according to the type of linguistic annotations used: part-of-speech tags (Feng et al., 2019; Yang et al., 2019); morpho-syntactic description tags, which comprise part of speech and morphol"
2020.coling-main.349,W18-6459,0,0.022668,"stic annotations can be used to label source-language (SL) or target-language (TL) words. The former lead to more accurate representations of the SL sentence (Sennrich and Haddow, 2016) and usually require changes in the encoder but not in the training loss (Chen et al., 2017; Li et al., 2018). The latter involve producing probability distributions for both TL words and TL linguistic annotations, which can be seen as a form of multi-task learning. Multi-task learning architectures explored in the literature include: independent decoders for words and linguistic annotations (Zhou et al., 2017; Wu et al., 2018; G¯u et al., 2018; Wang et al., 2018; Yang et al., 2019); independent output layers in the same decoder (Garc´ıa-Mart´ınez et al., 2016; Gr¨onroos et al., 2017; Feng et al., 2019); and even sharing the same network for both tasks (Nadejde et al., 2017; Tamchyna et al., 2017; Wagner, 2017) and alternatively produce linguistic annotations and words. The latter approach is usually referred to as interleaving. We can also classify the approaches according to the type of linguistic annotations used: part-of-speech tags (Feng et al., 2019; Yang et al., 2019); morpho-syntactic description tags, whic"
2020.coling-main.349,D19-1072,0,0.21418,"(SL) or target-language (TL) words. The former lead to more accurate representations of the SL sentence (Sennrich and Haddow, 2016) and usually require changes in the encoder but not in the training loss (Chen et al., 2017; Li et al., 2018). The latter involve producing probability distributions for both TL words and TL linguistic annotations, which can be seen as a form of multi-task learning. Multi-task learning architectures explored in the literature include: independent decoders for words and linguistic annotations (Zhou et al., 2017; Wu et al., 2018; G¯u et al., 2018; Wang et al., 2018; Yang et al., 2019); independent output layers in the same decoder (Garc´ıa-Mart´ınez et al., 2016; Gr¨onroos et al., 2017; Feng et al., 2019); and even sharing the same network for both tasks (Nadejde et al., 2017; Tamchyna et al., 2017; Wagner, 2017) and alternatively produce linguistic annotations and words. The latter approach is usually referred to as interleaving. We can also classify the approaches according to the type of linguistic annotations used: part-of-speech tags (Feng et al., 2019; Yang et al., 2019); morpho-syntactic description tags, which comprise part of speech and morphological inflection in"
2020.coling-main.349,P17-2092,0,0.0150347,"al., 2017). Linguistic annotations can be used to label source-language (SL) or target-language (TL) words. The former lead to more accurate representations of the SL sentence (Sennrich and Haddow, 2016) and usually require changes in the encoder but not in the training loss (Chen et al., 2017; Li et al., 2018). The latter involve producing probability distributions for both TL words and TL linguistic annotations, which can be seen as a form of multi-task learning. Multi-task learning architectures explored in the literature include: independent decoders for words and linguistic annotations (Zhou et al., 2017; Wu et al., 2018; G¯u et al., 2018; Wang et al., 2018; Yang et al., 2019); independent output layers in the same decoder (Garc´ıa-Mart´ınez et al., 2016; Gr¨onroos et al., 2017; Feng et al., 2019); and even sharing the same network for both tasks (Nadejde et al., 2017; Tamchyna et al., 2017; Wagner, 2017) and alternatively produce linguistic annotations and words. The latter approach is usually referred to as interleaving. We can also classify the approaches according to the type of linguistic annotations used: part-of-speech tags (Feng et al., 2019; Yang et al., 2019); morpho-syntactic descr"
2020.eamt-1.32,D18-1399,0,0.0443274,"Missing"
2020.eamt-1.32,W19-6721,1,0.873671,"Missing"
2020.eamt-1.32,W18-6320,1,0.800242,"el iter. backt. 1 iter. backt. 2 iter. backt. 3 iter. backt. 4 + NewsCrawl 4 + NewsCrawl + tags 4 Google Translate sw→en only parallel iter. backt. 1 iter. backt. 2 iter. backt. 3 iter. backt. 4 + tags 4 Google Translate - BLEU chrF++ 22.23 25.59 26.22 26.36 26.58 26.77 27.42 23.24 46.34 50.08 50.91 51.09 51.39 51.46 52.11 48.80 22.66 29.29 29.70 29.99 30.19 30.55 30.36 44.62 51.19 51.82 51.98 52.10 52.72 53.32 en→sw. This corresponds to the content creation use case which will use translation predominantly in this direction, and where the correctness of the translation is key. • Gap filling (Forcada et al., 2018) (GF) is used to test sw→en. This corresponds to the media monitoring use case which will use translation almost exclusively in this direction and where getting the gist of the meaning of a sentence is enough to fulfil the use-case, perfect translation of sentence structure is less important. Custom interfaces were created to support both evaluations; see figures 2 and 3 for DA and GF, respectively. Table 5: Automatic evaluation results obtained for the difEvaluators were recruited from within the media ferent development steps of the MT systems: only parallel partner organisations to complete"
2020.eamt-1.32,W08-0509,0,0.0194309,"arallel data are identified. Bitextor supports two strategies for document alignment: one based on bilingual lexicons and another based on MT. The last option was not feasible in this work as no high-quality MT system between sw and en was available; therefore, the first one was used. This method combines information from bilingual lexicons, the HTML structure of the documents, and the URL to obtain a confidence score for every pair of documents to be aligned (Espl`a-Gomis and Forcada, 2010). The bilingual lexicon used was automatically obtained from the word alignments obtained with mgiza++ (Gao and Vogel, 2008) for the following corpora: EUBookshop v2, Ubuntu and Tanzil (see Table 1). A total of 180 520 pairs of documents were obtained by using this method. • A collection of pairs of segments that are wrongly aligned to train a language model: following Bicleaner’s documentation, this collection was obtained from the raw parallel corpus by applying the “hard rules” implemented in Bicleaner. Sentence alignment. In this step, aligned documents are segmented and aligned at the sentence level. Two sentence-alignment tools are supported by Bitextor: Hunalign (Varga et al., 2007) and BLEUalign (Sennrich a"
2020.eamt-1.32,C16-1294,0,0.0246662,"Missing"
2020.eamt-1.32,W18-2703,0,0.020172,"24 821 40 000 000 en tokens 7 536 537 796 199 072 sw tokens 6 191 959 - existing data for the en→sw direction and the MT system was re-trained. 414 598 - 8 377 157 5.4 5 687 000 - 174 867 482 2 000 1 863 1 969 41 726 41 097 43 149 42 037 41 188 43 174 Table 4: Size of the corpora used to build the NMT systems after preprocesing. For the en NewsCrawl corpus, only the size of the subset that has been used for training is displayed. Token counts were calculated before BPE splitting. correlated with the quality of the system that translates the TL monolingual corpus into the source language (SL) (Hoang et al., 2018, Sec. 3). We took advantage of the fact that we are building systems for both the en→sw and sw→en directions and applied an iterative back-translation (Hoang et al., 2018) algorithm that simultaneously leverages monolingual sw and monolingual en data. It can be outlined as follows: 1. With the best identified hyper-parameters for each direction we built a system using only parallel data. 2. en and sw monolingual data were backtranslated with the systems built in the previous step. 3. Systems in both directions were trained on the combination of the back-translated data and the parallel data."
2020.eamt-1.32,P18-4020,0,0.0147447,"balVoicesv2015 and GlobalVoices-v2017q3, together with the other parallel corpora listed in Table 1 were deduplicated to obtain the final parallel corpus used to train the NMT systems. All corpora were tokenised with the Moses tokeniser (Koehn et al., 2007) and truecased. Parallel sentences with more than 100 tokens in either side were removed. Words were split in sub-word units with byte pair encoding (BPE; Sennrich et al. (2016c)). Table 4 reports the size of the corpora after this pre-processing. 5.2 Neural machine translation architecture We trained the NMT models with the Marian toolkit (Junczys-Dowmunt et al., 2018). Since training hyper-parameters can have a large impact in the quality of the resulting system (Lim et al., 2018), we carried out a grid search in order to find the best hyper-parameters for each translation direction. We explored both the Transformer (Vaswani et al., 2017) and recurrent neural network (RNN) with attention (Bahdanau et al., 2014) architectures. Our starting points were the Transformer hyperparameters15 described by Sennrich et al. (2017) and the RNN hyper-parameters16 described by Sennrich et al. (2016a). For each translation direction and architecture, we explored the follo"
2020.eamt-1.32,P07-2045,0,0.00737981,"e concatenation of GlobalVoices-v2015 and GlobalVoices-v2017q3, and split them into two halves (with 2 000 sentences each), which were used respectively as development and test corpora. The half reserved to be used as test corpus was further filtered to remove the sentences that could be found in any of the monolingual corpora. The remaining sentences from GlobalVoicesv2015 and GlobalVoices-v2017q3, together with the other parallel corpora listed in Table 1 were deduplicated to obtain the final parallel corpus used to train the NMT systems. All corpora were tokenised with the Moses tokeniser (Koehn et al., 2007) and truecased. Parallel sentences with more than 100 tokens in either side were removed. Words were split in sub-word units with byte pair encoding (BPE; Sennrich et al. (2016c)). Table 4 reports the size of the corpora after this pre-processing. 5.2 Neural machine translation architecture We trained the NMT models with the Marian toolkit (Junczys-Dowmunt et al., 2018). Since training hyper-parameters can have a large impact in the quality of the resulting system (Lim et al., 2018), we carried out a grid search in order to find the best hyper-parameters for each translation direction. We expl"
2020.eamt-1.32,W17-4707,0,0.0216419,"5 million sentences. Since the sw NewsCrawl corpus was made available near the end of the development of our MT systems, it could not be used during the iterative back-translation process. Nevertheless, we added it afterwards: the sw NewsCrawl was back-translated with the last available sw→en system obtained after completing all the iterations, concatenated to the Integrating linguistic information In addition to the corpora described above, linguistic information encoded in a more explicit representation was also employed to build the MT systems. In particular, we explored the interleaving (Nadejde et al., 2017) of linguistic tags in the TL side of the training corpus with the aim of enhancing the grammatical correctness of the translations. Morphological taggers were used to obtain the interleaved tags added to the training corpus. The sw text was tagged with TreeTagger (Schmid, 2013). We used a model17 trained on the Helsinki Corpus of Swahili.18 The en text was tagged with the Stanford tagger (Qi et al., 2018), which was trained on the English Web Treebank (Silveira et al., 2014). Figure 1 shows examples of en→sw and sw→en training parallel sentences with interleaved tags. While the tags returned"
2020.eamt-1.32,P02-1040,0,0.107107,"ading what?’) Change in word order, use of auxiliaries Comparative form of adjective (‘-er’) or ‘more’ ’have’ No change in word order Amesoma (‘He has read’); Amesoma? (‘Has he read?’) Absolute form of adjective Virusi ni ndogo (‘A virus is small’) Virusi ni ndogo kuliko bakteria (‘A virus is smaller than a bacterium’, lit. ‘A virus is small where there is a bacterium’) Nina swali (‘I have a question’, lit. ‘I-am-with question’) Comparative Predicative Possession Mainly prefixing conjunctional (‘to be with’) Table 3: A summary of linguistic contrasts between English and Swahili. highest BLEU (Papineni et al., 2002) score on the development set. We obtained the highest test BLEU scores for en→sw with an RNN architecture, 30 000 BPE operations, tied embeddings and single GPU, while the highest ones for sw→en were obtained with a Transformer architecture, 30 000 BPE operations, tied embeddings and two GPUs. 5.3 Leveraging monolingual data Once the best hyper-parameters were identified, we tried to improve the systems by making use of the monolingual corpora via back-translation. Backtranslation (Sennrich et al., 2016b) is a widespread method for integrating target-language (TL) monolingual corpora into NMT"
2020.eamt-1.32,E17-2025,0,0.0141716,"n. We explored both the Transformer (Vaswani et al., 2017) and recurrent neural network (RNN) with attention (Bahdanau et al., 2014) architectures. Our starting points were the Transformer hyperparameters15 described by Sennrich et al. (2017) and the RNN hyper-parameters16 described by Sennrich et al. (2016a). For each translation direction and architecture, we explored the following hyper-parameters: • Number of BPE operations: 15 000, 30 000, or 85 000. • Batch size: 8 000 tokens (trained on one GPU) or 16 000 tokens (trained on two GPUs). • Whether to tie the embeddings for both languages (Press and Wolf, 2017) We trained a system for each combination of hyper-parameters, using only the parallel data described above. Early stopping was based on perplexity on the development set and patience was set to 5. We selected the checkpoint that obtained the 15 https://github.com/marian-nmt/ marian-examples/tree/master/ wmt2017-transformer 16 https://github.com/marian-nmt/ marian-examples/tree/master/ training-basics Feature Coding of plurality in nouns Number of categories encoded in a single-word verb Value in English Plural suffix Value in Swahili Plural prefix Few (number, person, tense) Definite articles"
2020.eamt-1.32,K18-2016,0,0.0620134,"Missing"
2020.eamt-1.32,W18-6488,1,0.855142,"Missing"
2020.eamt-1.32,2010.amta-papers.14,0,0.0157459,"gel, 2008) for the following corpora: EUBookshop v2, Ubuntu and Tanzil (see Table 1). A total of 180 520 pairs of documents were obtained by using this method. • A collection of pairs of segments that are wrongly aligned to train a language model: following Bicleaner’s documentation, this collection was obtained from the raw parallel corpus by applying the “hard rules” implemented in Bicleaner. Sentence alignment. In this step, aligned documents are segmented and aligned at the sentence level. Two sentence-alignment tools are supported by Bitextor: Hunalign (Varga et al., 2007) and BLEUalign (Sennrich and Volk, 2010). We used Hunalign because BLEUalign requires an MT system to be available. The same bilingual dictionary used for document alignment was provided to Hunalign in order to improve the accuracy of the alignment. After applying Hunalign, 2 051 678 unique segment pairs were obtained. Cleaning. Bicleaner11 (S´anchez-Cartagena et al., 2018) was used to clean the raw corpora obtained after sentence alignment. Cleaning implies removing the noisy sentence pairs that are either incorrectly aligned or not in the expected languages.12 Bicleaner cleaning models require some languagedependent resources: • T"
2020.eamt-1.32,P16-1009,0,0.496034,"168 8 269 266 sw tokens 2 981 699 1 206 757 1 734 247 546 107 476 478 2 655 228 170 6 948 341 Table 1: Parallel English–Swahili corpora used to train the NMT systems described in this work. GV stands for the GlobalVoices corpus. automatic evaluation measures, describes a manual evaluation we are conducting and provides preliminary results. The paper ends with some concluding remarks. 2 Monolingual and bilingual corpora Parallel data is the basic resource required to train NMT. Additionally, it is common practice to use synthetic parallel corpora obtained by back-translating monolingual data (Sennrich et al., 2016b). This section describes the corpora we used to train the NMT systems described in Section 5. Tables 1 and 2 describe the parallel and monolingual corpora we used, respectively. As regards parallel corpora, with the exception of GoURMET and SAWA, all of them were downloaded from the OPUS website,2 one of the largest repositories of parallel data on the Internet.3 We used two additional parallel corpora: the SAWA corpus (De Pauw et al., 2011), that was kindly provided by their editors, and the GoURMET corpus, that was crawled from the web following the method described in Section 3. As regard"
2020.eamt-1.32,P16-1162,0,0.700793,"168 8 269 266 sw tokens 2 981 699 1 206 757 1 734 247 546 107 476 478 2 655 228 170 6 948 341 Table 1: Parallel English–Swahili corpora used to train the NMT systems described in this work. GV stands for the GlobalVoices corpus. automatic evaluation measures, describes a manual evaluation we are conducting and provides preliminary results. The paper ends with some concluding remarks. 2 Monolingual and bilingual corpora Parallel data is the basic resource required to train NMT. Additionally, it is common practice to use synthetic parallel corpora obtained by back-translating monolingual data (Sennrich et al., 2016b). This section describes the corpora we used to train the NMT systems described in Section 5. Tables 1 and 2 describe the parallel and monolingual corpora we used, respectively. As regards parallel corpora, with the exception of GoURMET and SAWA, all of them were downloaded from the OPUS website,2 one of the largest repositories of parallel data on the Internet.3 We used two additional parallel corpora: the SAWA corpus (De Pauw et al., 2011), that was kindly provided by their editors, and the GoURMET corpus, that was crawled from the web following the method described in Section 3. As regard"
2020.eamt-1.32,silveira-etal-2014-gold,0,0.0658922,"Missing"
2020.eamt-1.8,D18-1549,0,0.0199032,"n though the problem can be partially mitigated with accurate hyper-parameter tuning (Sennrich and Zhang, 2019), taking advantage of additional resources can help to further improve the quality of the system. Monolingual texts in both languages can be leveraged with the help of back-translation (Sennrich et al., 2016a; Hoang et al., 2018) to generate synthetic c 2020 The authors. This article is licensed under a Creative Commons 3.0 licence, no derivative works, attribution, CCBY-ND. parallel corpora. It is also possible to use only monolingual corpora and follow an unsupervised NMT approach (Artetxe et al., 2018). Parallel corpora from related language pairs can also be leveraged thanks to multilingual NMT (Johnson et al., 2017) and other forms of transfer learning (Kocmi and Bojar, 2018). In addition to the use of corpora, linguistic resources can also be used to improve NMT. If morphological analysers or syntactic parsers are available, they can be used to build a richer representation of the words being translated (Sennrich and Haddow, 2016; Nadejde et al., 2017). Even full rule-based MT (RBMT) systems can be combined with NMT in order to build hybrid systems (Huang et al., 2020). In this work, we"
2020.eamt-1.8,D16-1162,0,0.0268629,"gical analyzers, monolingual and bilingual dictionaries. As for bilingual text corpora, today OPUS2 contains about 400,000 sentence pairs, most of them very specialized, in the field of computer science. The Apertium Breton–French system: The Apertium platform3 contains an MT system designed to allow French-speaking readers to access written Breton content (gisting).4 This MT system 1 Integration of bilingual segments into NMT. The integration of bilingual segments, which could be produced by an RBMT system, into an NMT system has received some attention recently. One of the first approaches (Arthur et al., 2016), which can Breton–French machine translation https://www.freelang.com/enligne/breton. php 2 http://opus.nlpl.eu 3 http://www.apertium.org 4 Developers deliberately chose not develop French–Breton MT, deeming it too risky in terms of the socio-linguistic situation, as users would assume the machine-translated Breton to be us, nuine en-kk data augmentation  Joint SL-TL BPE applied after segmentation without ata* ed data and U chrF++ 6 2 6 1 0 27.80 39.93 39.67 42.87 42.86 3 44.46 6 44.99 9 45.24 ): 2nd n (no cluster) (Tyers, 2010), the only one in the world for Breton, was released in May 2009"
2020.eamt-1.8,P17-1175,0,0.0210054,"Miceli Barone et al., 2017, Sec. 4.2) and a deep output that combines the context vector, the recurrent hidden state and the embedding of the previous symbol. The multi-source recurrent NMT system contains two encoders (one for each input) which do not share parameters. The modifications in the decoder that allow it to accommodate the two encoders are the following: • The initial state of the decoder is obtained after concatenating the averaged encoder states of the two input sequences. • The conditional GRU (cGRU) unit with attention in the decoder is replaced by a doublyattentive cGRU cell (Calixto et al., 2017) featuring two independent attention mechanisms. • The context vector used in the deep output is replaced by the concatenation of the context vectors of the two inputs. For further details, the reader is referred to JunczysDowmunt and Grundkiewicz (2017). Our transformer models follow the architecture proposed by Vaswani et al. (2017). A transformer model contains an encoder and a decoder. The encoder is made of stacked layers, each containing a self-attention unit and a feed-forward unit. The decoder is also made of stacked layers, each containing a self-attention unit, an encoder–decoder att"
2020.eamt-1.8,W17-4716,0,0.020906,"r NMT that could contain multiple-token bilingual segments. They modelled decoding as a mixture of two processes: generating a word with the standard NMT model, or introducing a phrase from the phrase memory. Zhang et al. (2017) formalised the strategy of Tang et al. (2016) as a posterior regularization approach (Ganchev et al., 2010). Feng et al. (2018) designed a phrase attention mechanism that could be used either without additional supervision or with an external bilingual lexicon. Another related line of research modifies the beam search algorithm to meet some terminological constraints (Chatterjee et al., 2017; Post and Vilar, 2018). 2 The Breton language (Brezhoneg in Breton) is a Celtic language of the Brittonic group that is spoken in the west of Brittany (Breizh Izel or “Lower Brittany”) in France, and the main language with which it has contact is French, the only official language; in fact, Breton, spoken by about 200,000 people, has virtually no legal recognition in France. Resources for Breton: Programs like Firefox, Google applications and some Microsoft programs have been localized and there is a 70,000-page Breton Wikipedia. There is little software dedicated to Breton; most of it free/o"
2020.eamt-1.8,A92-1018,0,0.181783,"Missing"
2020.eamt-1.8,W08-0328,0,0.0570623,"e concluding remarks. Hybrid systems combining rule-based and corpus-based approaches. The creation of hybrid systems combining RBMT and statistical MT (SMT) has been explored by many authors. The most relevant approach for this work (Tyers, 2009) enlarged the training corpus of an SMT system with 116,500 sentence pairs made up of all possible inflected Breton forms and their inflected French translations as present in an earlier version of the Apertium Breton–French system we are using. Schwenk et al. (2009) followed a similar approach for other language pairs. More sophisticated approaches (Eisele et al., 2008; Enache et al., 2012; S´anchez-Cartagena et al., 2016) involve modifying the SMT architecture. Concerning the combination of RBMT and NMT, a relevant line of research involves choosing the best output (either RBMT or NMT) for each source sentence. For instance, Huang et al. (2020) propose training an automatic classifier for this task and use some features to help predict how difficult is the source sentence for each system: for instance, the degree of morphological and syntactic ambiguity is useful to estimate how difficult is the sentence for the RBMT system, while the token frequency on th"
2020.eamt-1.8,P18-4020,0,0.015036,"fore, RBMT systems that perform full syntactic analysis are more effective than Apertium when dealing, for instance, with long-range reorderings. Corpus train dev test • A post-generator which performs inter-word orthographic operations: contractions, elisions marked by apostrophes, etc.8 3.2 Multi-source neural machine translation We experimented with the transformer (Vaswani et al., 2017) and the recurrent attentional encoder– decoder (Bahdanau et al., 2015, hereinafter recurrent) NMT architectures. In both cases, we followed the multi-source architectures implemented in the Marian toolkit (Junczys-Dowmunt et al., 2018), which are described next. Our recurrent NMT systems follow the same architecture as Nematus (Sennrich et al., 2017b), namely a bidirectional gated recurrent unit (GRU) encoder, a conditional GRU decoder with attention (Miceli Barone et al., 2017, Sec. 4.2) and a deep output that combines the context vector, the recurrent hidden state and the embedding of the previous symbol. The multi-source recurrent NMT system contains two encoders (one for each input) which do not share parameters. The modifications in the decoder that allow it to accommodate the two encoders are the following: • The init"
2020.eamt-1.8,2012.eamt-1.61,0,0.0289606,"Hybrid systems combining rule-based and corpus-based approaches. The creation of hybrid systems combining RBMT and statistical MT (SMT) has been explored by many authors. The most relevant approach for this work (Tyers, 2009) enlarged the training corpus of an SMT system with 116,500 sentence pairs made up of all possible inflected Breton forms and their inflected French translations as present in an earlier version of the Apertium Breton–French system we are using. Schwenk et al. (2009) followed a similar approach for other language pairs. More sophisticated approaches (Eisele et al., 2008; Enache et al., 2012; S´anchez-Cartagena et al., 2016) involve modifying the SMT architecture. Concerning the combination of RBMT and NMT, a relevant line of research involves choosing the best output (either RBMT or NMT) for each source sentence. For instance, Huang et al. (2020) propose training an automatic classifier for this task and use some features to help predict how difficult is the source sentence for each system: for instance, the degree of morphological and syntactic ambiguity is useful to estimate how difficult is the sentence for the RBMT system, while the token frequency on the training corpus can"
2020.eamt-1.8,W18-6325,0,0.0179531,"ove the quality of the system. Monolingual texts in both languages can be leveraged with the help of back-translation (Sennrich et al., 2016a; Hoang et al., 2018) to generate synthetic c 2020 The authors. This article is licensed under a Creative Commons 3.0 licence, no derivative works, attribution, CCBY-ND. parallel corpora. It is also possible to use only monolingual corpora and follow an unsupervised NMT approach (Artetxe et al., 2018). Parallel corpora from related language pairs can also be leveraged thanks to multilingual NMT (Johnson et al., 2017) and other forms of transfer learning (Kocmi and Bojar, 2018). In addition to the use of corpora, linguistic resources can also be used to improve NMT. If morphological analysers or syntactic parsers are available, they can be used to build a richer representation of the words being translated (Sennrich and Haddow, 2016; Nadejde et al., 2017). Even full rule-based MT (RBMT) systems can be combined with NMT in order to build hybrid systems (Huang et al., 2020). In this work, we focus on an under-resourced language pair: Breton–French, and study mechanisms to build a hybrid system by combining NMT with the Breton–French system built with the Apertium RBMT"
2020.eamt-1.8,W18-6459,0,0.0264215,"f a recurrent attentional encoder–decoder (Bahdanau et al., 2015) model to decide the target language (TL) word translation probabilities that needed to be boosted in the final softmax layer. Tang et al. (2016) and Wang et al. (2017) relied on a phrase memory for NMT that could contain multiple-token bilingual segments. They modelled decoding as a mixture of two processes: generating a word with the standard NMT model, or introducing a phrase from the phrase memory. Zhang et al. (2017) formalised the strategy of Tang et al. (2016) as a posterior regularization approach (Ganchev et al., 2010). Feng et al. (2018) designed a phrase attention mechanism that could be used either without additional supervision or with an external bilingual lexicon. Another related line of research modifies the beam search algorithm to meet some terminological constraints (Chatterjee et al., 2017; Post and Vilar, 2018). 2 The Breton language (Brezhoneg in Breton) is a Celtic language of the Brittonic group that is spoken in the west of Brittany (Breizh Izel or “Lower Brittany”) in France, and the main language with which it has contact is French, the only official language; in fact, Breton, spoken by about 200,000 people,"
2020.eamt-1.8,W17-3204,0,0.0221557,"ural MT (NMT) for the Breton–French under-resourced language pair in an attempt to study to what extent the RBMT resources help improve the translation quality of the NMT system. We combine both translation approaches in a multi-source NMT architecture and find out that, even though the RBMT system has a low performance according to automatic evaluation metrics, using it leads to improved translation quality. 1 Introduction Corpus-based approaches to machine translation (MT), such as neural MT (NMT), struggle when the size of the available parallel corpora for a given language pair is scarce (Koehn and Knowles, 2017). Even though the problem can be partially mitigated with accurate hyper-parameter tuning (Sennrich and Zhang, 2019), taking advantage of additional resources can help to further improve the quality of the system. Monolingual texts in both languages can be leveraged with the help of back-translation (Sennrich et al., 2016a; Hoang et al., 2018) to generate synthetic c 2020 The authors. This article is licensed under a Creative Commons 3.0 licence, no derivative works, attribution, CCBY-ND. parallel corpora. It is also possible to use only monolingual corpora and follow an unsupervised NMT appro"
2020.eamt-1.8,W17-4710,0,0.133921,"ons marked by apostrophes, etc.8 3.2 Multi-source neural machine translation We experimented with the transformer (Vaswani et al., 2017) and the recurrent attentional encoder– decoder (Bahdanau et al., 2015, hereinafter recurrent) NMT architectures. In both cases, we followed the multi-source architectures implemented in the Marian toolkit (Junczys-Dowmunt et al., 2018), which are described next. Our recurrent NMT systems follow the same architecture as Nematus (Sennrich et al., 2017b), namely a bidirectional gated recurrent unit (GRU) encoder, a conditional GRU decoder with attention (Miceli Barone et al., 2017, Sec. 4.2) and a deep output that combines the context vector, the recurrent hidden state and the embedding of the previous symbol. The multi-source recurrent NMT system contains two encoders (one for each input) which do not share parameters. The modifications in the decoder that allow it to accommodate the two encoders are the following: • The initial state of the decoder is obtained after concatenating the averaged encoder states of the two input sequences. • The conditional GRU (cGRU) unit with attention in the decoder is replaced by a doublyattentive cGRU cell (Calixto et al., 2017) feat"
2020.eamt-1.8,W18-2703,0,0.0136508,"ation metrics, using it leads to improved translation quality. 1 Introduction Corpus-based approaches to machine translation (MT), such as neural MT (NMT), struggle when the size of the available parallel corpora for a given language pair is scarce (Koehn and Knowles, 2017). Even though the problem can be partially mitigated with accurate hyper-parameter tuning (Sennrich and Zhang, 2019), taking advantage of additional resources can help to further improve the quality of the system. Monolingual texts in both languages can be leveraged with the help of back-translation (Sennrich et al., 2016a; Hoang et al., 2018) to generate synthetic c 2020 The authors. This article is licensed under a Creative Commons 3.0 licence, no derivative works, attribution, CCBY-ND. parallel corpora. It is also possible to use only monolingual corpora and follow an unsupervised NMT approach (Artetxe et al., 2018). Parallel corpora from related language pairs can also be leveraged thanks to multilingual NMT (Johnson et al., 2017) and other forms of transfer learning (Kocmi and Bojar, 2018). In addition to the use of corpora, linguistic resources can also be used to improve NMT. If morphological analysers or syntactic parsers a"
2020.eamt-1.8,Q17-1024,0,0.0454399,"Missing"
2020.eamt-1.8,I17-1013,0,0.0400098,"Missing"
2020.eamt-1.8,W18-6467,0,0.0202696,"ins an encoder and a decoder. The encoder is made of stacked layers, each containing a self-attention unit and a feed-forward unit. The decoder is also made of stacked layers, each containing a self-attention unit, an encoder–decoder attention unit and feed-forward unit. The multisource transformer systems contain two encoders and two encoder–decoder attention units in each decoder layer. This transformer multi-source architecture was also used in the winning submission to the 2018 WMT automatic post-editing shared task (Chatterjee et al., 2018). For further details, the reader is referred to Junczys-Dowmunt and Grundkiewicz (2018). 8 In French: a` + lequel → auquel; de + hˆotels → d’hˆotels, etc. # sent. 139,489 2,000 3,000 # br tokens 1,096,311 25,291 37,054 # fr tokens 1,116,100 24,835 36,346 Table 1: Number of parallel sentences and tokens in Breton and French for the corpora used for train/dev/test corpora. 4 Experiments and results For the experiments we used the following corpora available at OPUS:9 Tatoeba, GNOME, OfisPublik, KDE4, wikimedia, Ubuntu and OpenSubtitles. For development and testing we used the same portions of the OfisPublik corpus used by S´anchezCartagena et al. (2015), the rest of corpora, after"
2020.eamt-1.8,W17-4707,0,0.0213213,"rks, attribution, CCBY-ND. parallel corpora. It is also possible to use only monolingual corpora and follow an unsupervised NMT approach (Artetxe et al., 2018). Parallel corpora from related language pairs can also be leveraged thanks to multilingual NMT (Johnson et al., 2017) and other forms of transfer learning (Kocmi and Bojar, 2018). In addition to the use of corpora, linguistic resources can also be used to improve NMT. If morphological analysers or syntactic parsers are available, they can be used to build a richer representation of the words being translated (Sennrich and Haddow, 2016; Nadejde et al., 2017). Even full rule-based MT (RBMT) systems can be combined with NMT in order to build hybrid systems (Huang et al., 2020). In this work, we focus on an under-resourced language pair: Breton–French, and study mechanisms to build a hybrid system by combining NMT with the Breton–French system built with the Apertium RBMT platform (Forcada et al., 2011). We aim at producing sentences that combine knowledge extracted from the parallel corpus and from the RBMT system. Hence, we go beyond approaches that simply choose the best system (either RBMT or NMT) for each input sentence (see below). We use mult"
2020.eamt-1.8,J11-4002,0,0.0942918,"Missing"
2020.eamt-1.8,N18-1119,0,0.0132948,"multiple-token bilingual segments. They modelled decoding as a mixture of two processes: generating a word with the standard NMT model, or introducing a phrase from the phrase memory. Zhang et al. (2017) formalised the strategy of Tang et al. (2016) as a posterior regularization approach (Ganchev et al., 2010). Feng et al. (2018) designed a phrase attention mechanism that could be used either without additional supervision or with an external bilingual lexicon. Another related line of research modifies the beam search algorithm to meet some terminological constraints (Chatterjee et al., 2017; Post and Vilar, 2018). 2 The Breton language (Brezhoneg in Breton) is a Celtic language of the Brittonic group that is spoken in the west of Brittany (Breizh Izel or “Lower Brittany”) in France, and the main language with which it has contact is French, the only official language; in fact, Breton, spoken by about 200,000 people, has virtually no legal recognition in France. Resources for Breton: Programs like Firefox, Google applications and some Microsoft programs have been localized and there is a 70,000-page Breton Wikipedia. There is little software dedicated to Breton; most of it free/open-source, such as the"
2020.eamt-1.8,K18-2016,0,0.128736,"t attentional encoder–decoder (Bahdanau et al., 2015) model to decide the target language (TL) word translation probabilities that needed to be boosted in the final softmax layer. Tang et al. (2016) and Wang et al. (2017) relied on a phrase memory for NMT that could contain multiple-token bilingual segments. They modelled decoding as a mixture of two processes: generating a word with the standard NMT model, or introducing a phrase from the phrase memory. Zhang et al. (2017) formalised the strategy of Tang et al. (2016) as a posterior regularization approach (Ganchev et al., 2010). Feng et al. (2018) designed a phrase attention mechanism that could be used either without additional supervision or with an external bilingual lexicon. Another related line of research modifies the beam search algorithm to meet some terminological constraints (Chatterjee et al., 2017; Post and Vilar, 2018). 2 The Breton language (Brezhoneg in Breton) is a Celtic language of the Brittonic group that is spoken in the west of Brittany (Breizh Izel or “Lower Brittany”) in France, and the main language with which it has contact is French, the only official language; in fact, Breton, spoken by about 200,000 people,"
2020.eamt-1.8,W19-5339,1,0.75374,"rning the combination of RBMT and NMT, a relevant line of research involves choosing the best output (either RBMT or NMT) for each source sentence. For instance, Huang et al. (2020) propose training an automatic classifier for this task and use some features to help predict how difficult is the source sentence for each system: for instance, the degree of morphological and syntactic ambiguity is useful to estimate how difficult is the sentence for the RBMT system, while the token frequency on the training corpus can help to assess how difficult it is for the NMT system. Similarly, Singh et al. (2019) use confidence scores computed for each system to choose the best alternative for each source sentence. Torregrosa et al. (2019) experimented with the integration of RBMT bilingual dictionaries and syntactic parsers into NMT without success. Finally, the multi-source architecture studied in this paper has been preliminary explored by S´anchez-Cartagena et al. (2019). The main differences with this work are: i) they did not study the impact of the different components of the RBMT system; and ii) they did not perform a hyper-parameter search, which could explain the poor performance of their tr"
2020.eamt-1.8,W09-0423,0,0.0335454,"Section 4 presents the experiments carried out and discusses the results obtained. The paper ends with some concluding remarks. Hybrid systems combining rule-based and corpus-based approaches. The creation of hybrid systems combining RBMT and statistical MT (SMT) has been explored by many authors. The most relevant approach for this work (Tyers, 2009) enlarged the training corpus of an SMT system with 116,500 sentence pairs made up of all possible inflected Breton forms and their inflected French translations as present in an earlier version of the Apertium Breton–French system we are using. Schwenk et al. (2009) followed a similar approach for other language pairs. More sophisticated approaches (Eisele et al., 2008; Enache et al., 2012; S´anchez-Cartagena et al., 2016) involve modifying the SMT architecture. Concerning the combination of RBMT and NMT, a relevant line of research involves choosing the best output (either RBMT or NMT) for each source sentence. For instance, Huang et al. (2020) propose training an automatic classifier for this task and use some features to help predict how difficult is the source sentence for each system: for instance, the degree of morphological and syntactic ambiguity"
2020.eamt-1.8,W16-2209,0,0.0212494,"0 licence, no derivative works, attribution, CCBY-ND. parallel corpora. It is also possible to use only monolingual corpora and follow an unsupervised NMT approach (Artetxe et al., 2018). Parallel corpora from related language pairs can also be leveraged thanks to multilingual NMT (Johnson et al., 2017) and other forms of transfer learning (Kocmi and Bojar, 2018). In addition to the use of corpora, linguistic resources can also be used to improve NMT. If morphological analysers or syntactic parsers are available, they can be used to build a richer representation of the words being translated (Sennrich and Haddow, 2016; Nadejde et al., 2017). Even full rule-based MT (RBMT) systems can be combined with NMT in order to build hybrid systems (Huang et al., 2020). In this work, we focus on an under-resourced language pair: Breton–French, and study mechanisms to build a hybrid system by combining NMT with the Breton–French system built with the Apertium RBMT platform (Forcada et al., 2011). We aim at producing sentences that combine knowledge extracted from the parallel corpus and from the RBMT system. Hence, we go beyond approaches that simply choose the best system (either RBMT or NMT) for each input sentence ("
2020.eamt-1.8,P19-1021,0,0.0181827,"sources help improve the translation quality of the NMT system. We combine both translation approaches in a multi-source NMT architecture and find out that, even though the RBMT system has a low performance according to automatic evaluation metrics, using it leads to improved translation quality. 1 Introduction Corpus-based approaches to machine translation (MT), such as neural MT (NMT), struggle when the size of the available parallel corpora for a given language pair is scarce (Koehn and Knowles, 2017). Even though the problem can be partially mitigated with accurate hyper-parameter tuning (Sennrich and Zhang, 2019), taking advantage of additional resources can help to further improve the quality of the system. Monolingual texts in both languages can be leveraged with the help of back-translation (Sennrich et al., 2016a; Hoang et al., 2018) to generate synthetic c 2020 The authors. This article is licensed under a Creative Commons 3.0 licence, no derivative works, attribution, CCBY-ND. parallel corpora. It is also possible to use only monolingual corpora and follow an unsupervised NMT approach (Artetxe et al., 2018). Parallel corpora from related language pairs can also be leveraged thanks to multilingua"
2020.eamt-1.8,P16-1009,0,0.0390644,"rding to automatic evaluation metrics, using it leads to improved translation quality. 1 Introduction Corpus-based approaches to machine translation (MT), such as neural MT (NMT), struggle when the size of the available parallel corpora for a given language pair is scarce (Koehn and Knowles, 2017). Even though the problem can be partially mitigated with accurate hyper-parameter tuning (Sennrich and Zhang, 2019), taking advantage of additional resources can help to further improve the quality of the system. Monolingual texts in both languages can be leveraged with the help of back-translation (Sennrich et al., 2016a; Hoang et al., 2018) to generate synthetic c 2020 The authors. This article is licensed under a Creative Commons 3.0 licence, no derivative works, attribution, CCBY-ND. parallel corpora. It is also possible to use only monolingual corpora and follow an unsupervised NMT approach (Artetxe et al., 2018). Parallel corpora from related language pairs can also be leveraged thanks to multilingual NMT (Johnson et al., 2017) and other forms of transfer learning (Kocmi and Bojar, 2018). In addition to the use of corpora, linguistic resources can also be used to improve NMT. If morphological analysers"
2020.eamt-1.8,P16-1162,0,0.0324381,"rding to automatic evaluation metrics, using it leads to improved translation quality. 1 Introduction Corpus-based approaches to machine translation (MT), such as neural MT (NMT), struggle when the size of the available parallel corpora for a given language pair is scarce (Koehn and Knowles, 2017). Even though the problem can be partially mitigated with accurate hyper-parameter tuning (Sennrich and Zhang, 2019), taking advantage of additional resources can help to further improve the quality of the system. Monolingual texts in both languages can be leveraged with the help of back-translation (Sennrich et al., 2016a; Hoang et al., 2018) to generate synthetic c 2020 The authors. This article is licensed under a Creative Commons 3.0 licence, no derivative works, attribution, CCBY-ND. parallel corpora. It is also possible to use only monolingual corpora and follow an unsupervised NMT approach (Artetxe et al., 2018). Parallel corpora from related language pairs can also be leveraged thanks to multilingual NMT (Johnson et al., 2017) and other forms of transfer learning (Kocmi and Bojar, 2018). In addition to the use of corpora, linguistic resources can also be used to improve NMT. If morphological analysers"
2020.eamt-1.8,E17-3017,0,0.0648516,"Missing"
2020.eamt-1.8,E17-1100,1,0.902103,"Missing"
2020.eamt-1.8,W19-6725,0,0.0191191,"MT or NMT) for each source sentence. For instance, Huang et al. (2020) propose training an automatic classifier for this task and use some features to help predict how difficult is the source sentence for each system: for instance, the degree of morphological and syntactic ambiguity is useful to estimate how difficult is the sentence for the RBMT system, while the token frequency on the training corpus can help to assess how difficult it is for the NMT system. Similarly, Singh et al. (2019) use confidence scores computed for each system to choose the best alternative for each source sentence. Torregrosa et al. (2019) experimented with the integration of RBMT bilingual dictionaries and syntactic parsers into NMT without success. Finally, the multi-source architecture studied in this paper has been preliminary explored by S´anchez-Cartagena et al. (2019). The main differences with this work are: i) they did not study the impact of the different components of the RBMT system; and ii) they did not perform a hyper-parameter search, which could explain the poor performance of their transformer systems. In addition, we conduct an automatic analysis of the errors produced by our hybrid approach. only be applied t"
2020.eamt-1.8,2009.eamt-1.29,0,0.720817,"ms, including approaches for integrating external bilingual segments into NMT. Section 2 then explains the resources available for Breton– French and the challenges of translating between Breton and French. Section 3 describes the hybrid architecture chosen. Section 4 presents the experiments carried out and discusses the results obtained. The paper ends with some concluding remarks. Hybrid systems combining rule-based and corpus-based approaches. The creation of hybrid systems combining RBMT and statistical MT (SMT) has been explored by many authors. The most relevant approach for this work (Tyers, 2009) enlarged the training corpus of an SMT system with 116,500 sentence pairs made up of all possible inflected Breton forms and their inflected French translations as present in an earlier version of the Apertium Breton–French system we are using. Schwenk et al. (2009) followed a similar approach for other language pairs. More sophisticated approaches (Eisele et al., 2008; Enache et al., 2012; S´anchez-Cartagena et al., 2016) involve modifying the SMT architecture. Concerning the combination of RBMT and NMT, a relevant line of research involves choosing the best output (either RBMT or NMT) for e"
2020.eamt-1.8,2010.eamt-1.13,0,0.0336913,"d some attention recently. One of the first approaches (Arthur et al., 2016), which can Breton–French machine translation https://www.freelang.com/enligne/breton. php 2 http://opus.nlpl.eu 3 http://www.apertium.org 4 Developers deliberately chose not develop French–Breton MT, deeming it too risky in terms of the socio-linguistic situation, as users would assume the machine-translated Breton to be us, nuine en-kk data augmentation  Joint SL-TL BPE applied after segmentation without ata* ed data and U chrF++ 6 2 6 1 0 27.80 39.93 39.67 42.87 42.86 3 44.46 6 44.99 9 45.24 ): 2nd n (no cluster) (Tyers, 2010), the only one in the world for Breton, was released in May 2009 as the result of the joint efforts of the Ofis ar Brezhoneg,5 the Spanish company Prompsit Language Engineering, and the Universitat d’Alacant and is based on the Apertium platform (Forcada et al., 2011). Dictionary development started with the free dictionaries for Breton in Lexilogos.6 Development of the Apertium Breton– French MT system slowly continues. The quality of the French generated is not suitable for publishing, but may be used to get a rough idea of the meaning of a Breton text. transformer br text RNN + RBMT weighte"
2020.eamt-1.8,D17-1149,0,0.0194142,"are: i) they did not study the impact of the different components of the RBMT system; and ii) they did not perform a hyper-parameter search, which could explain the poor performance of their transformer systems. In addition, we conduct an automatic analysis of the errors produced by our hybrid approach. only be applied to single-token bilingual segments, used the attention weights of a recurrent attentional encoder–decoder (Bahdanau et al., 2015) model to decide the target language (TL) word translation probabilities that needed to be boosted in the final softmax layer. Tang et al. (2016) and Wang et al. (2017) relied on a phrase memory for NMT that could contain multiple-token bilingual segments. They modelled decoding as a mixture of two processes: generating a word with the standard NMT model, or introducing a phrase from the phrase memory. Zhang et al. (2017) formalised the strategy of Tang et al. (2016) as a posterior regularization approach (Ganchev et al., 2010). Feng et al. (2018) designed a phrase attention mechanism that could be used either without additional supervision or with an external bilingual lexicon. Another related line of research modifies the beam search algorithm to meet some"
2020.eamt-1.8,P17-1139,0,0.0444111,"Missing"
2020.eamt-1.8,N16-1004,0,0.0186629,"Missing"
2020.wmt-1.107,W13-3520,0,0.0187456,"ommon defects that arise when crawling parallel corpora from the web. These elements were detected by means of regular expressions. 4.3 Tokenisation and word segmentation Tokenization and subword segmentation have shown to improve the recall of the probabilistic dictionaries used to obtain the lexical features described in Section 2.1. We experimented with the following tokenisation and subword segmentation methods, which were applied to the clean data as well as to the raw sentences to be scored: • Rule-based tokenisation (tok) for Pashto, Khmer and English, as provided by the tool Polyglot (Al-Rfou et al., 2013); • Rule-based tokenisation plus word morphological segmentation with Morfessor (tok-morph). For this we used, after tokenisation, the pre-trained models for Morfessor (Virpioja et al., 2013) included in Polyglot. Training Bicleaner As previously mentioned, the probabilistic bilingual dictionaries were obtained from the same parallel corpus used to train the classifier. This strategy has an important drawback. While almost all words would be found in the bilingual dictionaries when training the classifier, the coverage would be much smaller when classifying the raw sentences because of the sma"
2020.wmt-1.107,W19-5433,0,0.0161103,"on parallel data, without any kind of transfer learning from other language pairs. The approach we follow to detect sentences that are mutual translations is similar to the one by Munteanu and Marcu (2005) for detecting parallel sentences in comparable corpora. However, we use a larger set of shallow features not related to lexical similarity and follow a more sophisticated method for generating negative samples. Concerning our re-scoring strategy for including information about fluency and diversity, participants from past editions also used these attributes to score sentences. For instance, Axelrod et al. (2019) and V´azquez et al. (2019) devised a scoring strategy under the assumption that parallel sentences should have similar monolingual language model perplexities, and many other submissions included a penalty for repetitive sentences (Gonz´alez-Rubio, 2019; Erdmann and Gwinnup, 2019; Bernier-Colborne and Lo, 2019). Nevertheless, to the best of our knowledge, our approach is the first one that directly optimises the weight of these attributes towards an automatic translation evaluation metric. Concluding remarks We described the joint submission of Universitat d’Alacant and Prompsit Language Engi"
2020.wmt-1.107,P09-5002,0,0.0107725,"information gain; for the size of the sub-set of feap tures we tried with |F |, log2 |F |and |F |; for the number of trees we tried with 100, 200, 300, 400 and 500. The features we used can be split in two groups: those that account for the lexical similarity of the two sentences, and those based on shallow properties of the sentences. 2.1 Lexical features Bilingual lexical similarity is assessed by means of the lexical feature Qmax(S, Θ, d), which was first described by S´anchez-Cartagena et al. (2018) and is inspired by the translation probabilities used in statistical machine translation (Koehn, 2009). It is defined as: ! 1 |Θ| Y Qmax(S, Θ, d) = max p(t|s; d) t∈Θ s∈S∪{NULL} where, S is a source-language (SL) sentence, S is a set with the tokens in S, Θ is a set with the tokens in the target-language (TL) sentence T that appear at least once in the SL-to-TL probabilistic bilingual dictionary d, and p(t|s; d) stands for the translation probability of the target token t given the source token s according to the bilingual dictionary d. A smoothing is applied if, for a token t, maxs∈S∪{NULL} p(t|s; d) equals zero; in that case, this expression is set to the value of the smallest probability in"
2020.wmt-1.107,W19-5404,0,0.12872,"Missing"
2020.wmt-1.107,W19-5434,0,0.0167357,"atures not related to lexical similarity and follow a more sophisticated method for generating negative samples. Concerning our re-scoring strategy for including information about fluency and diversity, participants from past editions also used these attributes to score sentences. For instance, Axelrod et al. (2019) and V´azquez et al. (2019) devised a scoring strategy under the assumption that parallel sentences should have similar monolingual language model perplexities, and many other submissions included a penalty for repetitive sentences (Gonz´alez-Rubio, 2019; Erdmann and Gwinnup, 2019; Bernier-Colborne and Lo, 2019). Nevertheless, to the best of our knowledge, our approach is the first one that directly optimises the weight of these attributes towards an automatic translation evaluation metric. Concluding remarks We described the joint submission of Universitat d’Alacant and Prompsit Language Engineering to the parallel corpus filtering shared task at the Fifth Conference on Machine Translation (WMT 2020). Our submission is based on Bicleaner, an open source tool based on a classifier that uses lexical similarity features inspired in the translation probabilities used in statistical machine translation."
2020.wmt-1.107,W19-5435,0,0.0738375,"Missing"
2020.wmt-1.107,W19-5436,0,0.0540063,"a larger set of shallow features not related to lexical similarity and follow a more sophisticated method for generating negative samples. Concerning our re-scoring strategy for including information about fluency and diversity, participants from past editions also used these attributes to score sentences. For instance, Axelrod et al. (2019) and V´azquez et al. (2019) devised a scoring strategy under the assumption that parallel sentences should have similar monolingual language model perplexities, and many other submissions included a penalty for repetitive sentences (Gonz´alez-Rubio, 2019; Erdmann and Gwinnup, 2019; Bernier-Colborne and Lo, 2019). Nevertheless, to the best of our knowledge, our approach is the first one that directly optimises the weight of these attributes towards an automatic translation evaluation metric. Concluding remarks We described the joint submission of Universitat d’Alacant and Prompsit Language Engineering to the parallel corpus filtering shared task at the Fifth Conference on Machine Translation (WMT 2020). Our submission is based on Bicleaner, an open source tool based on a classifier that uses lexical similarity features inspired in the translation probabilities used in s"
2020.wmt-1.107,W19-5437,0,0.030482,"Missing"
2020.wmt-1.107,W11-2123,0,0.0549022,"Missing"
2020.wmt-1.107,W18-6478,0,0.0332909,"because of differences in the GPU hardware or random initialization seed. System LASER (baseline) Bicleaner 2018 tok Bicleaner 2020 tok Bicleaner 2020 tok-morph Bicleaner 2020 tok + re-score Khmer–English fairseq MBART 6.80 10.33 7.45 10.16 7.76 10.66 7.33 10.56 8.25 11.18 Pashto–English fairseq MBART 9.55 11.50 10.11 11.85 10.10 12.35 8.64 10.94 10.53 12.80 Table 1: BLEU scores obtained by the different configurations evaluated for Khmer–English and Pashto–English on the development environment provided by the organisers. 6 TL sentence given an SL sentence, emerged as the dominant approach (Junczys-Dowmunt, 2018). Last year’s edition was focused on a lowresource scenario (Koehn et al., 2019), where parallel data big enough to build NMT models that provide reliable TL probability distributions was not available. The best performing model was LASER, a method based on multilingual sentence embeddings (Chaudhary et al., 2019) that takes advantage of the data available for multiple language pairs. In fact, a LASER model trained on 93 languages is the baseline model published by the organisers for this edition of the shared task. Unlike LASER, our submission is mainly based on lexical similarity scores anal"
2020.wmt-1.107,W18-6488,1,0.874862,"Missing"
2020.wmt-1.107,W17-2619,0,0.026062,"mainly belonging to narrow domains like 1 https://github.com/bitextor/bicleaner Wikipedia: https://en.wikipedia.org/wiki/ Khmer_language 3 http://opus.nlpl.eu 2 software products and religion. Pashto (ps) is spoken by around 40 million people in Pakistan and in Afghanistan, where it is official together with Persian.4 There are around 100k English–Pastho parallel sentence in OPUS, most of which belong to the software domain. Detecting noisy parallel sentences for underresourced language pairs, like those addressed in this shared task, is challenging. Pastho is not directly supported by LASER (Schwenk and Douze, 2017), although it supports other Iranian languages, and there are few bilingual resources for building the Bicleaner’s models. Bicleaner is based on a classifier that assesses whether a pair of sentences are mutual translations or not. It is trained on a parallel corpus (positive samples) and on an automatically corrupted version of the same corpus (negative samples). The most important features used by the classifier are lexical similarity scores obtained with the help of probabilistic bilingual dictionaries, which are also extracted from the parallel corpus. Our submission improves the performan"
2020.wmt-1.107,W19-5441,0,0.0290207,"Missing"
2021.emnlp-main.669,W17-4715,0,0.0186508,"come less informative and force the system to pay more attention to the encoder. This is the effect envisaged by word dropout when preventing posterior collapse in variational autoencoders (Bowman et al., 2016). source: The target sentence becomes a copy of the source sentence. In this way, the most efficient way of emitting the right output is checking the encoder representation to copy from the source. Some authors have identified such training instances as harmful for NMT (Ott et al., 2018; Khayrallah and Koehn, 2018), and only copying in the inverse direction has been proved to be useful (Currey et al., 2017). However, the MTL framework may allow us to leverage such synthetic training data. We propose a simple MTL approach that consists of using a vanilla NMT system —in our experiments, it is a transformer system as defined by Vaswani et al. (2017)— where all (main and auxreverse: The order of the words in the target seniliary) tasks share the encoder and the decoder. In tence is reversed. Voita et al. (2021) suggest that order to avoid harmful interferences by the out-ofthe influence of the encoder decreases along the distribution target data generated for the auxiliary target sentence; therefore"
2021.emnlp-main.669,P15-1166,0,0.0395903,"Missing"
2021.emnlp-main.669,D18-1045,0,0.0142751,"their training samples by infrequent words in order to improve the performance of the NMT model when dealing with them at translation time. Words to be replaced are identified using a large source language model. Once the source words to be replaced are identified, a word-alignment model and a probabilistic dictionary are used to also replace the corresponding counterpart by the most probable translation of the new source word. In our MTL DA framework, the replace transformation, which is similar to Fadaee et al. (2017)’s work, does not require any language model. Regarding back-translation, Edunov et al. (2018) apply several simple transformations (word deletion, replacement, swapping) to back-translated data reporting a noticeable improvement. In relation with the special token we use to prevent negative transfer between tasks, Caswell et al. (2019) propose a similar strategy to identify synthetic samples when combining actual parallel data and backtranslated data for training. Yang et al. (2019) extends this work by including forward-translated data for training using two different special tokens to distinguish the two types of synthetic data. 7 line system (on average around 1.6 BLEU points) and"
2021.emnlp-main.669,P17-2090,0,0.0247884,"e compression of parallel corpora. By making the alignment between source and target words monotonous, the target sentences become less fluent, so we expect the system to pay more attention to the encoder. replace: α · t source–target aligned words are selected at random and replaced by random entries in a bilingual lexicon obtained from the training corpus; to this end, one-to-one word alignments are used.1 This transformation is likely to introduce words that are difficult to produce by relying only on the target language prefix, thus forcing the system to pay attention to the source words. Fadaee et al. (2017) followed a similar approach; however, they constrained the replacements to produce only fluent target sentences. 3 Experimental settings We have conducted experiments for the translation from English to German, Hebrew and Vietnamese, and for the translation in the reverse direction, using corpora commonly used for evaluating DA techniques in low-resource scenarios. We evaluated the effect of using each of the MTL DA auxiliary tasks, as well as the combination of the best performing ones. We also evaluated two strong DA methods that aim at extending the support of the empirical data distributi"
2021.emnlp-main.669,W18-6325,0,0.0207292,"ge amounts of parallel sen- out-of-distribution target data generated, we follow tences —sentence pairs in two languages that are a simple multi-task learning (MTL) approach that mutual translations— are needed, which constitutes does not require changes to the model architecture. a critical barrier for low-resource language pairs. We call the proposed framework multi-task learnThis problem has been addressed through different ing data augmentation (MTL DA) to stress the approaches, such as transfer-learning from high- fact that the augmented data, which do not follow resource language pairs (Kocmi and Bojar, 2018), the distribution of parallel sentences in the trainusing linguistic annotations (Sennrich and Had- ing corpus, constitute different auxiliary tasks that dow, 2016), training multilingual systems (John- nevertheless produce a positive transfer to the main son et al., 2017) and applying data augmentation task. 8502 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 8502–8516 c November 7–11, 2021. 2021 Association for Computational Linguistics Our framework does not require elaborated preprocessing steps, training additional systems, or data besides t"
2021.emnlp-main.669,W18-6459,0,0.125091,"stematic improvements in translation quality and enhanced domain robustness are related to the encoder being exposed during training to more situations where a good source representation is crucial, we carried out an analysis of the relative source and target contributions to the generation decisions of the NMT system. According to Voita et al. (2021), systems trained with more data tend to rely more on source information; we expect MTL DA to produce the same effect. Another aspect that will account for the positive impact of MTL DA in the system’s encoder is the generation of hallucinations (Lee et al., 2018): completely inadequate translations that usually occur under domain shift (Müller et al., 2020), due to the system relying too much on the target context (Voita et al., 2021). We expect systems trained with MTL DA to produce less hallucinations. To validate this last hypothesis, we carried out an hallucination analysis on the results of the domain shift experiments. Relative source and target contributions. We used layer-wise relevance propagation (LRP), as adapted to transformers by Voita et al. (2021), to compute the relative contribution of source and target tokens to each prediction made"
2021.emnlp-main.669,D19-1570,0,0.0229028,"he systems but not on the other.14 Hallucinations. To estimate the number of hallucinations produced by the systems evaluated, we 8508 14 As an example, the evaluation corpus contains the pair 6 Related work The back-translation (Sennrich et al., 2016b) approach for leveraging additional target monolingual data to produce additional training samples is probably the most popular DA approach for NMT. The set of related approaches covered in this sec(a) English–German tion, however, mainly focus on methods that, as MTL DA, do not require additional resources besides the training parallel corpus. Li et al. (2019) evaluate back- and forwardtranslation in such a setting. They train forward and backward NMT systems on the available parallel data and use them to produce new synthetic (b) German–English samples by translating either the target side (SenFigure 2: Number of disjoint hallucinations of the nrich et al., 2016b) or the source side (Zhang and baseline system (in grey) and the systems trained Zong, 2016) of the original training corpus. with different DA methods. Our MTL DA approach The approaches we have evaluated in our exper(in blue) corresponds to the system labeled as reiments, RAML (Norouzi"
2021.emnlp-main.669,2020.amta-research.14,0,0.0225289,"e encoder being exposed during training to more situations where a good source representation is crucial, we carried out an analysis of the relative source and target contributions to the generation decisions of the NMT system. According to Voita et al. (2021), systems trained with more data tend to rely more on source information; we expect MTL DA to produce the same effect. Another aspect that will account for the positive impact of MTL DA in the system’s encoder is the generation of hallucinations (Lee et al., 2018): completely inadequate translations that usually occur under domain shift (Müller et al., 2020), due to the system relying too much on the target context (Voita et al., 2021). We expect systems trained with MTL DA to produce less hallucinations. To validate this last hypothesis, we carried out an hallucination analysis on the results of the domain shift experiments. Relative source and target contributions. We used layer-wise relevance propagation (LRP), as adapted to transformers by Voita et al. (2021), to compute the relative contribution of source and target tokens to each prediction made by the system. LRP allows us to compute Rt (xi ) and Rt (yj ), the relative contribution of sour"
2021.emnlp-main.669,J03-1002,0,0.0234299,"h (2020) for English–German. Corpora were tokenised and truecased with the Moses scripts;5 then, sentences with more than 100 or less than 5 tokens were removed from the training corpora. Afterwards, byte-pair encoding (BPE) with 10,000 merge operations (Sennrich et al., 2016c) was applied on the concatenation of the source and target sides of the training corpora to obtain the vocabulary. Finally, those sentence pairs in the training corpora with more than 100 BPE tokens were removed. One-to-many word alignments in both translation directions were obtained using mgiza++ (Gao and Vogel, 2008; Och and Ney, 2003).6 Source-to-target word alignments were used for the mono transformations; the one-toone word alignments required by the replace transformation were obtained by computing the intersection between the one-to-many word alignments in both translation directions. The bilingual lexicon for the replace transformation was built by associating to each source word the target word it is most frequently aligned with in the one-to-one word alignments. Training. Our neural model is a transformerbased model as defined by Vaswani et al. (2017), with the exception of the amount of warmup_steps, which was set"
2021.emnlp-main.669,N19-4009,0,0.0209406,"ani et al. (2017), with the exception of the amount of warmup_steps, which was set to 8,000. All the experiments were carried out on a single GPU with mini-batches made of 4,000 tokens. Validation was done every 1,000 updates, and the patience based on the BLEU score on the development set was set to 6 4 https://github.com/ZurichNLP/ domain-robustness 5 https://github.com/moses-smt/ mosesdecoder/tree/master/scripts 6 https://github.com/moses-smt/mgiza validation cycles; we then kept the intermediate model performing best on the development set. We trained the systems with the fairseq toolkit (Ott et al., 2019). For RAML and SwitchOut, we integrated into fairseq the sampling function released by Wang et al. (2018).7 Systems trained with MTL DA were fine-tuned on the main (translation) task after being trained on the combination of the main and auxiliary tasks. When combining different auxiliary tasks, a different special token was used for each one. DA hyperparameters. The proportion of words affected by the swap, token and replace transformations is controlled by a hyperparameter α, whereas RAML and SwitchOut are governed by a temperature τ . For each language pair, we explored values of α in [0.1,"
2021.emnlp-main.669,P02-1040,0,0.109367,"y a temperature τ . For each language pair, we explored values of α in [0.1, 0.9] at intervals of 0.1, and values of τ around the best values reported by Wang et al. (2018).8 The results reported are those obtained with the model that maximizes BLEU on the development set. The best hyperparameters obtained for the experiments with the IWSLT parallel data were reused for the experiments with the training set extended with back-translated data. 4 Results and discussion IWSLT parallel data. Table 3 reports the mean and standard deviation of the translation performance, measured in terms of BLEU (Papineni et al., 2002),9 of three different executions for each of the systems trained on the IWSLT parallel data. chrF++ scores (Popovi´c, 2017), that show the same trend, are available in Appendix B. The results show that our MTL DA approach consistently outperforms the baseline system in all language pairs and translation directions. In general, the auxiliary tasks reverse (translation into the target language but in the reverse order) and replace (random replacement of target words and the source words they are aligned with) are the best performing ones. swap (random swapping of words) and source (copying the s"
2021.emnlp-main.669,W17-4770,0,0.0384828,"Missing"
2021.emnlp-main.669,W18-6319,0,0.0196099,"Missing"
2021.emnlp-main.669,2021.naacl-main.92,0,0.0236786,"ing of decoding, but remain throughout the sentence. MTL DA achieves, only with artificially augmented data, an increase in source influence comparable to that reported by Voita et al. (2021, Fig. 6) when the size of genuine parallel data increases. Finally, no consistent differences in source influence could be found between the reverse and replace auxiliary tasks. The systems combining multiple auxiliary tasks, however, are consistently the ones with the highest source influence, thus confirming the complementarity of the tasks. follow the procedure proposed by Lee et al. (2018) and used by Raunak et al. (2021). Although their interest was in detecting those sentences that induced the generation of hallucinations after introducing spurious tokens in the input, we adapted it to automatically measure the number of input sentences in a test set for which the corresponding output seems to be an hallucination. To this end, we use an adjusted version of BLEU which only takes into account the precision of unigrams and bigrams with weights 0.8 and 0.2, respectively, as proposed by Lee et al. (2018). If the sentence-level adjusted BLEU of the lowercased emitted translation is below a certain threshold (10 in"
2021.emnlp-main.669,2021.acl-long.91,0,0.250778,"DA stratecoder. Experiments carried out on six lowgies to produce synthetic target sentences aimed resource translation tasks show consistent imat strengthening the encoder. These strategies exprovements over the baseline and over DA pose the network during training to new situations methods aiming at extending the support of where the target-language context is not sufficient the empirical data distribution. The systems trained with our approach rely more on the to achieve a low loss, and the burden is passed on to source tokens, are more robust against domain the encoder. Recent findings by Voita et al. (2021) shift and suffer less hallucinations. further motivate our approach: they claim that the influence of source tokens in the output predictions 1 Introduction of an NMT system decreases as decoding advances. In order to train reliable neural machine transla- Moreover, to avoid harmful interferences by the tion (NMT) systems, large amounts of parallel sen- out-of-distribution target data generated, we follow tences —sentence pairs in two languages that are a simple multi-task learning (MTL) approach that mutual translations— are needed, which constitutes does not require changes to the model arc"
2021.emnlp-main.669,2020.acl-main.326,0,0.102385,"2016) and SwitchOut (Wang et al., 2018)— and that it can be combined with synthetic corpora generated through back-translation (Sennrich et al., 2016b) to get further improvements. In the context of explainable deep learning models, we perform an analysis of the relevance of the encoder and decoder representations in the NMT system output, which shows that, thanks to the auxiliary tasks, MTL DA increases the contribution of the source tokens to the decisions made by the NMT system. Moreover, systems trained with MTL DA are much more robust against domain shift and produce less hallucinations (Wang and Sennrich, 2020). The remainder of the paper is organised as follows. Next section briefly describes our MTL DA approach and the different auxiliary tasks we evaluated. After that, Sec. 3 describes the experimental settings, whereas Sec. 4 provides and discusses the results obtained. Sec. 5 then presents an analysis of the changes in the transformer dynamics induced by our auxiliary tasks as a way of explaining the improvement in translation quality. The paper ends with a review of the most relevant works in the area of DA for NMT in Sec. 6, followed by some concluding remarks in Sec. 7. 2 Multi-task learning"
2021.emnlp-main.669,D18-1100,0,0.208361,"M. Sánchez-Cartagena, Miquel Esplà-Gomis Juan Antonio Pérez-Ortiz, Felipe Sánchez-Martínez Dep. de Llenguatges i Sistemes Informàtics Universitat d’Alacant E-03690 Sant Vicent del Raspeig (Spain) {vmsanchez,mespla,japerez,fsanchez}@dlsi.ua.es Abstract strategies (Li et al., 2019; Feng et al., 2021), i.e., artificially generating additional parallel sentences. In the context of neural machine translation, Data augmentation (DA) is formalised by many data augmentation (DA) techniques may be authors as a solution to a data distribution mismatch used for generating additional training samproblem (Wang et al., 2018; Wei et al., 2020). The ples when the available parallel data are scarce. Many DA approaches aim at expanding the data distribution of the sentence pairs observed support of the empirical data distribution by in the training corpus, pˆ(x, y), differs from the generating new sentence pairs that contain intrue data distribution, p(x, y). Hence, the system frequent words, thus making it closer to the should be trained on a training set that follows true data distribution of parallel sentences. In q(x, y), an augmented version of pˆ(x, y) with a this paper, we propose to follow a completely wider"
2021.emnlp-main.669,2020.emnlp-main.216,0,0.0322212,"a, Miquel Esplà-Gomis Juan Antonio Pérez-Ortiz, Felipe Sánchez-Martínez Dep. de Llenguatges i Sistemes Informàtics Universitat d’Alacant E-03690 Sant Vicent del Raspeig (Spain) {vmsanchez,mespla,japerez,fsanchez}@dlsi.ua.es Abstract strategies (Li et al., 2019; Feng et al., 2021), i.e., artificially generating additional parallel sentences. In the context of neural machine translation, Data augmentation (DA) is formalised by many data augmentation (DA) techniques may be authors as a solution to a data distribution mismatch used for generating additional training samproblem (Wang et al., 2018; Wei et al., 2020). The ples when the available parallel data are scarce. Many DA approaches aim at expanding the data distribution of the sentence pairs observed support of the empirical data distribution by in the training corpus, pˆ(x, y), differs from the generating new sentence pairs that contain intrue data distribution, p(x, y). Hence, the system frequent words, thus making it closer to the should be trained on a training set that follows true data distribution of parallel sentences. In q(x, y), an augmented version of pˆ(x, y) with a this paper, we propose to follow a completely wider support. In this w"
2021.emnlp-main.669,2020.coling-main.379,0,0.0376397,"lucinapositional behaviour: replaced words are selected tions of the baseline that are not labeled as such from another sentence and not from the vocabulary. by the corresponding DA system; consequently, Some of our auxiliary tasks have already been higher values are better. Additionally, the color used for DA, but mostly on the source side and bars represent the number of hallucinations of the rarely in an MTL framework. Replacing tokens DA system that cannot be labeled as such in the with placeholders (as we do in token) has already baseline system (shorter bars are better). been applied by Zhang et al. (2020) to the source As can be seen, disjoint hallucinations barely language, in combination with auxiliary tasks inhappen with in-domain data, but they can be easily volving detecting replaced and dropped tokens. Xie found when considering out-of-domain data. In et al. (2017) also evaluate the impact of replaceevery domain and in both translation directions, ments on the target data, but do not follow an MTL the blue bar (the one representing our MTL DA approach. Word dropout (Sennrich et al., 2016b; method) is clearly the shortest one in almost all Gal and Ghahramani, 2016; Shen et al., 2020) can"
2021.emnlp-main.669,D16-1160,0,0.0504573,"Missing"
R11-1013,P03-1021,0,0.249448,"he most of such resources. We focus on alleviating the data sparseness problem suffered by phrase-based statistical machine translation (PBSMT) systems (Koehn, 2010, ch. 5) when trained on small parallel corpora. We present a new hybrid approach which enriches a PBSMT system with resources from 2 2.1 Translation Approaches Phrase-Based Statistical Machine Translation PBSMT systems (Koehn, 2010, ch. 5) translate sentences by maximising the translation probability as defined by the log-linear combination of a number of feature functions, whose weights are chosen to optimise translation quality (Och, 2003). A core component of every PBSMT system is the phrase table, which contains bilingual phrase pairs extracted from a bilingual corpus after word alignment (Och and Ney, 2003). The set of translations 90 Proceedings of Recent Advances in Natural Language Processing, pages 90–96, Hissar, Bulgaria, 12-14 September 2011. from which the most probable one is chosen is built by segmenting the source sentence in all possible ways and then combining the translation of the different source segments according to the phrase table. Common feature functions are: source-totarget and target-to-source phrase t"
R11-1013,P02-1040,0,0.0809279,"mprovement in BLEU when adding dictionaries to the training corpus. In a personal communication, the author stated that in Tyers (2009) a baseline in which the feature weights were optimised with MERT was compared to a system enriched with the Apertium dictionaries using the default (not optimised) feature weights. Incidentally, not optimising the feature weights provided better results. If the feature weights are optimised in both cases the results obtained are in the line of those reported in this paper. Results and Discussion Table 3 reports the translation performance as measured by BLEU (Papineni et al., 2002) for the dif4 Revisions 24177, 22150 and 28674, respectively. 94 2k baseline 20.74 alignment 19.31 es-en phrase-dict 24.29 phrase-rules 24.68 Apertium baseline 18.86 alignment 17.53 br-fr phrase-dict 21.57 phrase-rules 22.67 Apertium 5k 24.24 23.71 26.39 26.81 24.17 23.56 26.39 26.42 In-domain 10k 20k ≈ 27k 26.46 28.45 25.89 28.10 27.93 29.30 28.28 29.40 18.00 28.26 33.17 34.69 27.82 32.17 34.76 29.66 33.42 35.50 29.60 33.14 35.83 17.56 40k 29.86 29.73 30.36 30.41 80k 30.88 30.83 31.14 31.02 2k 12.59 12.06 19.76 20.97 5k 14.90 14.55 20.48 21.36 - - - - Out-of-domain 10k 20k ≈ 27k 16.92 18.63 1"
R11-1013,P07-2045,0,0.00956078,"s randomly chosen from the Q4/2000 portion of Europarl v5 corpus (Koehn, 2005) and evaluating them with 2 000 random parallel sentences from the same corpus; special care was taken to avoid the overlapping between the test and development sets. The outof-domain evaluation was performed by using the newstest2008 set for tuning and the newstest2010 test for testing; both sets belong to the news domain and are distributed as part of the WMT 2010 shared translation task.2 Tables 1 and 2 summarise the data about the corpora used in the experiments. We used the free/open-source PBSMT system Moses3 (Koehn et al., 2007) together with the Experimental Settings We evaluated our RBMT–SMT hybridisation approach on two different language pairs, namely Breton–French and Spanish–English, and with different small training corpus sizes. While the Breton–French language pair suffers from actual resource scarceness (there are only around 30 000 parallel sentences available), Spanish–English was chosen because it has a wide range of parallel corpora available, which allows us to perform both in-domain and out-of-domain evaluations. SMT systems for Spanish–English were trained from the Europarl v5 parallel corpus (Koehn,"
R11-1013,W09-0423,0,0.451516,"ransfer allows performing operations between words which are distant in the source sentence, shallowtransfer RBMT systems are less powerful that the ones which perform full parsing. 3 Related Work Bilingual dictionaries are the most reused resource from RBMT. They have been added to SMT systems since its early days (Brown et al., 1993). One of the simplest strategies, which has already been put into practice with the Apertium bilingual dictionaries (Tyers, 2009), consists of adding the dictionary entries directly to the parallel corpus. In addition to the obvious increase in lexical coverage, Schwenk et al. (2009) state that the quality of the alignments obtained is also improved when the words in the bilingual dictionary appear in other sentences of the parallel corpus. However, it is not guaranteed that, following this strategy, multi-word expressions from the bilingual dictionary that appear in the SL sentences are translated as such by the SMT decoder because they may be split into smaller units by the phrase-extraction algorithm. Our strategy differs from these approaches in that we ensure the proper translation of multi-word expressions, but also add the dictionary entries to the training corpus"
R11-1013,2005.mtsummit-papers.11,0,0.01157,"ation task.1 The weights of the different feature functions were optimised by means of minimum error rate training (MERT; Och, 2003). Breton–French systems were tuned using the tuning section of the parallel corpus by Tyers (2009) and evaluated using the devtest section of the same corpus. Note that we can only perform in-domain evaluation for this language pair. Regarding Spanish–English, we have carried out both in-domain and out-of-domain evaluations. The former was performed by tuning the systems with 2 000 parallel sentences randomly chosen from the Q4/2000 portion of Europarl v5 corpus (Koehn, 2005) and evaluating them with 2 000 random parallel sentences from the same corpus; special care was taken to avoid the overlapping between the test and development sets. The outof-domain evaluation was performed by using the newstest2008 set for tuning and the newstest2010 test for testing; both sets belong to the news domain and are distributed as part of the WMT 2010 shared translation task.2 Tables 1 and 2 summarise the data about the corpora used in the experiments. We used the free/open-source PBSMT system Moses3 (Koehn et al., 2007) together with the Experimental Settings We evaluated our R"
R11-1013,J10-4005,0,0.288565,"ical machine translation system with bilingual phrase pairs matching structural transfer rules and dictionary entries from a shallowtransfer rule-based machine translation system. We have tested this approach on different small parallel corpora scenarios, where pure statistical machine translation systems suffer from data sparseness. The results obtained show an improvement in translation quality, specially when translating out-of-domain texts that are well covered by the shallow-transfer rule-based machine translation system we have used. 1 Introduction Statistical machine translation (SMT) (Koehn, 2010) is currently the leading paradigm in machine translation research. SMT systems are very attractive because they may be built with little human effort when enough monolingual and bilingual corpora are available. However, bilingual corpora large enough to build competitive SMT systems are not always easy to harvest, and they may not even exist for some language pairs. On the contrary, rule-based machine translation systems (RBMT) may be built without any parallel corpus; however, they need an explicit representation of linguistic information whose coding by human experts requires a considerable"
R11-1013,W02-1405,0,0.031853,", it is not guaranteed that, following this strategy, multi-word expressions from the bilingual dictionary that appear in the SL sentences are translated as such by the SMT decoder because they may be split into smaller units by the phrase-extraction algorithm. Our strategy differs from these approaches in that we ensure the proper translation of multi-word expressions, but also add the dictionary entries to the training corpus with the aim of improving word alignment. Other approaches go beyond adding a dictionary to the parallel corpus: dictionary entries may constrain the decoding process (Langlais, 2002), or may be used in conjunction with handcrafted rules to reorder the SL sentences to match the structure of the TL (Popovi´c and Ney, 2006). Although RBMT transfer rules have also been reused in hybrid systems, they have been mostly used implicitly as part of a complete RBMT engine. For instance, Dugast et al. (2008) show how a PBSMT system can be bootstrapped using only monolingual data and an RBMT engine. Another remarkable study (Eisele et al., 2008) presents a strategy based on the augmentation of the phrase table to include information provided by an RBMT system. In this approach, the se"
R11-1013,2009.mtsummit-posters.21,0,0.0450645,"ve because they may be built with little human effort when enough monolingual and bilingual corpora are available. However, bilingual corpora large enough to build competitive SMT systems are not always easy to harvest, and they may not even exist for some language pairs. On the contrary, rule-based machine translation systems (RBMT) may be built without any parallel corpus; however, they need an explicit representation of linguistic information whose coding by human experts requires a considerable amount of time. When both parallel corpora and linguistic information exist, hybrid approaches (Thurmair, 2009) may be followed in order to make the most of such resources. We focus on alleviating the data sparseness problem suffered by phrase-based statistical machine translation (PBSMT) systems (Koehn, 2010, ch. 5) when trained on small parallel corpora. We present a new hybrid approach which enriches a PBSMT system with resources from 2 2.1 Translation Approaches Phrase-Based Statistical Machine Translation PBSMT systems (Koehn, 2010, ch. 5) translate sentences by maximising the translation probability as defined by the log-linear combination of a number of feature functions, whose weights are chose"
R11-1013,2006.amta-papers.11,0,0.0226968,"gless phrases, such as el ni˜no inal´ambrico – the wireless boy. In addition, the number of combinations to deal with would become unmanageable as the length of the rule grows. Enhancing Phrase-Based SMT With Shallow-Transfer Linguistic Resources Our hybridisation strategy modifies two elements of a standard PBSMT system: the word alignments and the phrase translation model. 4.1 Enriching the Phrase Translation Model Improving Word Alignment with RBMT Bilingual Dictionaries As improving the quality of the word alignments in a PBSMT system could lead to improvements in translation performance (Lopez and Resnik, 2006), in our approach we add to the original corpus all the entries, after suitably inflecting them, from the Apertium bilingual dictionary, to help the word aligment process. Recall that some multi-word expressions are encoded as single lexical forms in the Apertium dictionaries; therefore, the entries generated from Apertium may contain multi-word parallel segments. Once word alignments have been computed and the probabilistic bilingual dictionary used to compute the lexical weightings of the phrase pairs has been learned, dictionary entries are ignored and no phrase pair are extracted from them"
R11-1013,2009.eamt-1.29,0,0.435725,"ms inter chunk operations; and the third one generates a sequence of lexical forms from each chunk. Note that, although this multi-stage shallow transfer allows performing operations between words which are distant in the source sentence, shallowtransfer RBMT systems are less powerful that the ones which perform full parsing. 3 Related Work Bilingual dictionaries are the most reused resource from RBMT. They have been added to SMT systems since its early days (Brown et al., 1993). One of the simplest strategies, which has already been put into practice with the Apertium bilingual dictionaries (Tyers, 2009), consists of adding the dictionary entries directly to the parallel corpus. In addition to the obvious increase in lexical coverage, Schwenk et al. (2009) state that the quality of the alignments obtained is also improved when the words in the bilingual dictionary appear in other sentences of the parallel corpus. However, it is not guaranteed that, following this strategy, multi-word expressions from the bilingual dictionary that appear in the SL sentences are translated as such by the SMT decoder because they may be split into smaller units by the phrase-extraction algorithm. Our strategy di"
R11-1013,D09-1040,0,0.0537036,"Missing"
R11-1013,zhang-etal-2004-interpreting,0,0.0137147,"from https: //mosesdecoder.svn.sourceforge.net/ svnroot/mosesdecoder/trunk. 93 Corpus Language model 2k 5k 10k Training 20k 40k 80k In-domain tuning In-domain test Out-of-domain tuning Out-of-domain test Origin Europarl Europarl Europarl Europarl Europarl Europarl Europarl Europarl Europarl WMT 2010 WMT 2010 Sentences 1 650 152 2 000 5 000 10 000 20 000 40 000 80 000 2 000 2 000 2 051 2 489 ferent configurations and language pairs described in section 5. Statistical significance of the differences between systems has been computed by performing 1 000 iterations of paired bootstrap resampling (Zhang et al., 2004) with a p-level of 0.05. In addition, table 4 presents the optimal weight obtained with MERT for the feature function that flags whether a phrase pair has been obtained from the Apertium bilingual resources (dictionaries and rules). Table 5 shows the proportion of RBMTgenerated phrases used to perform each translation. The results show that our hybrid approach outperforms both pure RBMT and PBSMT systems in terms of BLEU. However, the difference is statistically significant only under certain circumstances. The in-domain evaluation shows that the statistical significance only holds in the smal"
R11-1013,J03-1002,0,0.0139717,"ch. 5) when trained on small parallel corpora. We present a new hybrid approach which enriches a PBSMT system with resources from 2 2.1 Translation Approaches Phrase-Based Statistical Machine Translation PBSMT systems (Koehn, 2010, ch. 5) translate sentences by maximising the translation probability as defined by the log-linear combination of a number of feature functions, whose weights are chosen to optimise translation quality (Och, 2003). A core component of every PBSMT system is the phrase table, which contains bilingual phrase pairs extracted from a bilingual corpus after word alignment (Och and Ney, 2003). The set of translations 90 Proceedings of Recent Advances in Natural Language Processing, pages 90–96, Hissar, Bulgaria, 12-14 September 2011. from which the most probable one is chosen is built by segmenting the source sentence in all possible ways and then combining the translation of the different source segments according to the phrase table. Common feature functions are: source-totarget and target-to-source phrase translation probabilities, source-to-target and target-to-source lexical weightings (calculated by using a probabilistic bilingual dictionary), reordering costs, number of wor"
R11-1013,H93-1039,0,\N,Missing
R11-1013,W08-0328,0,\N,Missing
R11-1013,W08-0327,0,\N,Missing
S12-1065,P06-1114,0,0.0607265,"oduction Cross-lingual textual entailment (CLTE) detection (Mehdad et al., 2010) is an extension of the textual entailment (TE) detection (Dagan et al., 2006) problem. TE detection consists of finding out, for two text fragments T and H in the same language, whether T entails H from a semantic point of view or not. CLTE presents a similar problem, but with T and H written in different languages. During the last years, many authors have focused on resolving TE detection, as solutions to this problem have proved to be useful in many natural language processing tasks, such as question answering (Harabagiu and Hickl, 2006) or machine translation (MT) (Mirkin et al., 2009; Pad´o et al., 2009). Therefore, CLTE may also be useful for related tasks in which more than one language is involved, such as cross-lingual question answering or cross-lingual information retrieval. Although CLTE detection is a relatively new problem, it has already been tackled. Mehdad et al. (2010) propose to use machine translation (MT) to translate H from LH , the language of H, into LT , the language of T , and then use any of the state-of-the-art TE approaches. In a later work (Mehdad et al., 2011), the authors use MT, but in a more ela"
S12-1065,N03-1017,0,0.00633838,"Missing"
S12-1065,P07-2045,0,0.00420345,"ding feature function computes, for the total number of sub-segments of a given length l ∈ [1, L] obtained from a text fragment S, the fraction of them which appear in a sub-segment link. It is applied both to H and T and is defined as: Fl0 (S) = Linkedl (S)/(|S |− l + 1) where Linkedl is the number of sub-segments from S with length l which appear in a sub-segment link. 3 (both sentences entail each other); and no entailment (neither of the sentences entails each other). For the whole data set, both sentences in each instance were tokenized using the scripts1 included in the Moses MT system (Koehn et al., 2007). Each sentence was segmented to get all possible sub-segments which were then translated into the other language. Experimental settings The experiments designed for this task are aimed at evaluating the features proposed in Section 2. We evaluate our CLTE approach using the English– Spanish data sets provided in the task 8 of SemEval 2012 (Negri et al., 2012). • Google Translate:4 an online MT system by Google Inc. • Microsoft Translator:5 an online MT system by Microsoft. External resources were also used for the extended features described in Section 2.2. We used the stemmer6 and the stopwo"
S12-1065,N10-1045,0,0.550758,"n them, which can be used to determine whether T entails H or not. Note that MT is used as a black box, i.e. sub-segment translations may be collected from any MT system, and that our approach could even use any other sources of bilingual sub-sentential information. It is even possible to combine different MT systems as we do in our experiments. This is a key point of our work, since 472 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 472–476, c Montr´eal, Canada, June 7-8, 2012. 2012 Association for Computational Linguistics it uses MT in a more elaborate way than Mehdad et al. (2010), and it does not depend on a specific MT approach. Another important difference between this work and that of Mehdad et al. (2011) is the set of features used for classification. The paper is organized as follows: Section 2 describes the method used to collect the MT information and obtain the features; Section 3 explains the experimental framework; Section 4 shows the results obtained for the different features combination proposed; the paper ends with concluding remarks. 2 Features from machine translation Our approach uses MT as a black box to detect parallelisms between the text fragments"
S12-1065,P11-1134,0,0.502404,"sks, such as question answering (Harabagiu and Hickl, 2006) or machine translation (MT) (Mirkin et al., 2009; Pad´o et al., 2009). Therefore, CLTE may also be useful for related tasks in which more than one language is involved, such as cross-lingual question answering or cross-lingual information retrieval. Although CLTE detection is a relatively new problem, it has already been tackled. Mehdad et al. (2010) propose to use machine translation (MT) to translate H from LH , the language of H, into LT , the language of T , and then use any of the state-of-the-art TE approaches. In a later work (Mehdad et al., 2011), the authors use MT, but in a more elaborate way. They train a phrase-based statistical MT (PBSMT) system (Koehn et al., 2003) translating from LH to LT , and use the translation table obtained as a by-product of the training process to extract a set of features which are processed by a support vector machine classifier (Theodoridis and Koutroumbas, 2009, Sect. 3.7) to decide whether T entails H or not. Castillo (2011) discusses another machine learning approach in which the features are obtained from semantic similarity measures based on WordNet (Miller, 1995). In this work we present a new"
S12-1065,P09-1089,0,0.0187436,"on (Mehdad et al., 2010) is an extension of the textual entailment (TE) detection (Dagan et al., 2006) problem. TE detection consists of finding out, for two text fragments T and H in the same language, whether T entails H from a semantic point of view or not. CLTE presents a similar problem, but with T and H written in different languages. During the last years, many authors have focused on resolving TE detection, as solutions to this problem have proved to be useful in many natural language processing tasks, such as question answering (Harabagiu and Hickl, 2006) or machine translation (MT) (Mirkin et al., 2009; Pad´o et al., 2009). Therefore, CLTE may also be useful for related tasks in which more than one language is involved, such as cross-lingual question answering or cross-lingual information retrieval. Although CLTE detection is a relatively new problem, it has already been tackled. Mehdad et al. (2010) propose to use machine translation (MT) to translate H from LH , the language of H, into LT , the language of T , and then use any of the state-of-the-art TE approaches. In a later work (Mehdad et al., 2011), the authors use MT, but in a more elaborate way. They train a phrase-based statistical"
S12-1065,D11-1062,0,0.173937,"Missing"
S12-1065,S12-1053,0,0.0318287,"gment link. 3 (both sentences entail each other); and no entailment (neither of the sentences entails each other). For the whole data set, both sentences in each instance were tokenized using the scripts1 included in the Moses MT system (Koehn et al., 2007). Each sentence was segmented to get all possible sub-segments which were then translated into the other language. Experimental settings The experiments designed for this task are aimed at evaluating the features proposed in Section 2. We evaluate our CLTE approach using the English– Spanish data sets provided in the task 8 of SemEval 2012 (Negri et al., 2012). • Google Translate:4 an online MT system by Google Inc. • Microsoft Translator:5 an online MT system by Microsoft. External resources were also used for the extended features described in Section 2.2. We used the stemmer6 and the stopwords list provided by the SnowBall project for Spanish7 and English.8 Classifier. We used the implementation of support vector machine included in the WEKA v.3.6.6 data mining software package (Hall et al., 2009) for multiclass classification, and a polynomial kernel. 4 Results and discussion We tried the different features proposed in Section 2 in isolation, a"
S12-1065,P09-1034,0,0.041275,"Missing"
S12-1065,W07-1401,0,\N,Missing
W11-2157,W05-0909,0,0.121488,"ictionary, 106 first-level structural transfer rules, and 31 second-level rules. As entries in the bilingual dictionary contain mappings between SL and TL lemmas, when phrase pairs matching the bilingual dictionary are generated all the possible inflections of these lemmas are produced. We used the free/open-source PBSMT system Moses (Koehn et al., 2007), together with the IRSTLM language modelling toolkit (Federico et al., 2008), which was used to train a 5-gram lanTable 2 reports the translation performance as measured by BLEU (Papineni et al., 2002), GTM (Melamed et al., 2003) and METEOR2 (Banerjee and Lavie, 2005) for Apertium and the three systems presented in the previous section, as well as the size of the phrase table and the amount of unknown words in the test set. The hybrid approach outperforms the baseline PBSMT system in terms of the three evaluation metrics. The confidence interval of the difference between them, computed by doing 1 000 iterations of paired 1 The corpora can be downloaded from http://www. statmt.org/wmt11/translation-task.html. 2 Modules exact, stem, synonym and paraphrase (Denkowski and Lavie, 2010) were used. 461 Training Tuning Test Table 1: Size of the corpora used in the"
W11-2157,H93-1039,0,0.543406,"{ to<pr> } INF{ come<vblex><inf> } Third-level structural transfer removes chunk encapsulations so that a plain sequence of lexical forms is generated: on the other hand<adv> my<det><pos><pl> american<adj> friend<n><pl> have<vbhaver><pres> decide<vblex><pp> to<pr> come<vblex><inf> Finally, the translation into TL is generated from the TL lexical forms: On the other hand my American friends have decided to come. 3 Related work Linguistic data from RBMT have already been used to enrich SMT systems in different ways. Bilingual 459 dictionaries have been added to SMT systems since its early days (Brown et al., 1993); one of the simplest strategies involves adding the dictionary entries directly to the training parallel corpus (Tyers, 2009; Schwenk et al., 2009). Other approaches go beyond that. Eisele et al. (2008) first translate the sentences in the test set with an RBMT system, then apply the usual phrase-extraction algorithm over the resulting small parallel corpus, and finally add the obtained phrase pairs to the original phrase table. It is worth noting that neither of these two strategies guarantee that the multi-word expressions in the RBMT bilingual dictionary appearing in the sentences to trans"
W11-2157,W10-1751,0,0.0224233,"ured by BLEU (Papineni et al., 2002), GTM (Melamed et al., 2003) and METEOR2 (Banerjee and Lavie, 2005) for Apertium and the three systems presented in the previous section, as well as the size of the phrase table and the amount of unknown words in the test set. The hybrid approach outperforms the baseline PBSMT system in terms of the three evaluation metrics. The confidence interval of the difference between them, computed by doing 1 000 iterations of paired 1 The corpora can be downloaded from http://www. statmt.org/wmt11/translation-task.html. 2 Modules exact, stem, synonym and paraphrase (Denkowski and Lavie, 2010) were used. 461 Training Tuning Test Table 1: Size of the corpora used in the experiments. The bilingual training corpora has been cleaned to remove empty parallel sentences and those which contain more than 40 tokens. guage model using interpolated Kneser-Ney discounting (Goodman and Chen, 1998). Word alignments from the training parallel corpus were computed by means of GIZA++ (Och and Ney, 2003). The cube pruning (Huang and Chiang, 2007) decoding algorithm was chosen in order to speed-up the tuning step and the translation of the test set. 6 Results and discussion system baseline UA-dict UA"
W11-2157,W08-0328,0,0.379859,"dj> friend<n><pl> have<vbhaver><pres> decide<vblex><pp> to<pr> come<vblex><inf> Finally, the translation into TL is generated from the TL lexical forms: On the other hand my American friends have decided to come. 3 Related work Linguistic data from RBMT have already been used to enrich SMT systems in different ways. Bilingual 459 dictionaries have been added to SMT systems since its early days (Brown et al., 1993); one of the simplest strategies involves adding the dictionary entries directly to the training parallel corpus (Tyers, 2009; Schwenk et al., 2009). Other approaches go beyond that. Eisele et al. (2008) first translate the sentences in the test set with an RBMT system, then apply the usual phrase-extraction algorithm over the resulting small parallel corpus, and finally add the obtained phrase pairs to the original phrase table. It is worth noting that neither of these two strategies guarantee that the multi-word expressions in the RBMT bilingual dictionary appearing in the sentences to translate will be translated as such because they may be split into smaller units by the phrase-extraction algorithm. Our approach overcomes this issue by adding the data obtained from the RBMT system directl"
W11-2157,P07-1019,0,0.012347,"ations of paired 1 The corpora can be downloaded from http://www. statmt.org/wmt11/translation-task.html. 2 Modules exact, stem, synonym and paraphrase (Denkowski and Lavie, 2010) were used. 461 Training Tuning Test Table 1: Size of the corpora used in the experiments. The bilingual training corpora has been cleaned to remove empty parallel sentences and those which contain more than 40 tokens. guage model using interpolated Kneser-Ney discounting (Goodman and Chen, 1998). Word alignments from the training parallel corpus were computed by means of GIZA++ (Och and Ney, 2003). The cube pruning (Huang and Chiang, 2007) decoding algorithm was chosen in order to speed-up the tuning step and the translation of the test set. 6 Results and discussion system baseline UA-dict UA Apertium BLEU 28.06 28.58 28.73 23.89 GTM 52.40 52.55 52.66 50.71 METEOR 47.27 47.41 47.51 45.65 # of unknown words 1 447 1 274 1 274 4 064 phrase table size 254 693 494 255 860 346 255 872 094 - Table 2: Case-insensitive BLEU, GTM, and METEOR scores obtained by the hybrid approach submitted to the WMT 2011 shared translation task (UA), a reduced version of it whose phrase table is enriched using only bilingual dictionary entries (UA-dict)"
W11-2157,N03-1017,0,0.00552563,"e in our submission. Section 3 outlines related hybrid approaches, whereas our approach is described in Section 4. Sections 5 and 6 describe, respectively, the resources we used to build our submission and the results achieved for the Spanish–English language pair. The paper ends with some concluding remarks. 2 Translation approaches We briefly describe the rationale behind the PBSMT (section 2.1) and the shallow-transfer RBMT (section 2.2) systems we have used in our hybridisation approach. 2.1 Phrase-based statistical machine translation Phrase-based statistical machine translation systems (Koehn et al., 2003) translate sentences by maximising the translation probability as defined by the log-linear combination of a number of feature functions, whose weights are chosen to opti457 Proceedings of the 6th Workshop on Statistical Machine Translation, pages 457–463, c Edinburgh, Scotland, UK, July 30–31, 2011. 2011 Association for Computational Linguistics mise translation quality (Och, 2003). A core component of every PBSMT system is the phrase table, which contains bilingual phrase pairs extracted from a bilingual corpus after word alignment (Och and Ney, 2003). The set of translations from which the"
W11-2157,P07-2045,0,0.0191797,"se table was enriched only with dictionary entries. The Apertium (Forcada et al., 2011) engine and the linguistic resources for Spanish–English were downloaded from the Apertium Subversion repository.The linguistic data contains 326 228 entries in the bilingual dictionary, 106 first-level structural transfer rules, and 31 second-level rules. As entries in the bilingual dictionary contain mappings between SL and TL lemmas, when phrase pairs matching the bilingual dictionary are generated all the possible inflections of these lemmas are produced. We used the free/open-source PBSMT system Moses (Koehn et al., 2007), together with the IRSTLM language modelling toolkit (Federico et al., 2008), which was used to train a 5-gram lanTable 2 reports the translation performance as measured by BLEU (Papineni et al., 2002), GTM (Melamed et al., 2003) and METEOR2 (Banerjee and Lavie, 2005) for Apertium and the three systems presented in the previous section, as well as the size of the phrase table and the amount of unknown words in the test set. The hybrid approach outperforms the baseline PBSMT system in terms of the three evaluation metrics. The confidence interval of the difference between them, computed by doi"
W11-2157,2005.mtsummit-papers.11,0,0.0549306,"of 1.0. Figure 1 shows the alignment between the words in the running example. Task Language model 5 System training Corpus Europarl News Crawl Total Europarl News Commentary United Nations Total Total clean newstest2008 newstest2011 Sentences 2 015 440 112 905 708 114 921 148 1 786 594 132 571 10 662 993 12 582 158 8 992 751 2 051 3 003 We submitted a hybrid system for the Spanish– English language pair built by following the strategy described above. The initial phrase table was built from all the parallel corpora distributed as part of the WMT 2011 shared translation task, namely Europarl (Koehn, 2005), News Commentary and United Nations. In a similar way, the language model was built from the the Europarl (Koehn, 2005) and the News Crawl monolingual English corpora. The weights of the different feature functions were optimised by means of minimum error rate training (Och, 2003) on the 2008 test set.1 Table 1 summarises the data about the corpora used to build our submission. We also built a baseline PBSMT system trained on the same corpora and a reduced version of our system whose phrase table was enriched only with dictionary entries. The Apertium (Forcada et al., 2011) engine and the lin"
W11-2157,J10-4005,0,0.356122,"ETEOR, a standard phrase-based statistical MT system trained on the same corpus, and received the second best BLEU score in the automatic evaluation. 1 Introduction This paper describes the system submitted by the Transducens Research Group (Universitat d’Alacant, Spain) to the shared translation task of the EMNLP 2011 Sixth Workshop on Statistical Machine Translation (WMT 2011). We participated in the Spanish–English task with a hybrid system that combines, in a phrase-based statistical machine translation (PBSMT) system, bilingual information obtained from parallel corpora in the usual way (Koehn, 2010, ch. 5), and bilingual information from the Spanish–English language pair in the Apertium (Forcada et al., 2011) rule-based machine translation (RMBT) platform. A wide range of hybrid approaches (Thurmair, 2009) may be taken in order to build a machine translation system which takes advantage of a parallel corpus and explicit linguistic information from RBMT. In particular, our hybridisation approach directly enriches the phrase table of a PBSMT system with phrase pairs generated from the explicit linguistic resources from an Apertium-based shallowtransfer RBMT system. Apertium, which is desc"
W11-2157,N03-2021,0,0.0354762,"326 228 entries in the bilingual dictionary, 106 first-level structural transfer rules, and 31 second-level rules. As entries in the bilingual dictionary contain mappings between SL and TL lemmas, when phrase pairs matching the bilingual dictionary are generated all the possible inflections of these lemmas are produced. We used the free/open-source PBSMT system Moses (Koehn et al., 2007), together with the IRSTLM language modelling toolkit (Federico et al., 2008), which was used to train a 5-gram lanTable 2 reports the translation performance as measured by BLEU (Papineni et al., 2002), GTM (Melamed et al., 2003) and METEOR2 (Banerjee and Lavie, 2005) for Apertium and the three systems presented in the previous section, as well as the size of the phrase table and the amount of unknown words in the test set. The hybrid approach outperforms the baseline PBSMT system in terms of the three evaluation metrics. The confidence interval of the difference between them, computed by doing 1 000 iterations of paired 1 The corpora can be downloaded from http://www. statmt.org/wmt11/translation-task.html. 2 Modules exact, stem, synonym and paraphrase (Denkowski and Lavie, 2010) were used. 461 Training Tuning Test T"
W11-2157,J03-1002,0,0.0148419,"tatistical machine translation systems (Koehn et al., 2003) translate sentences by maximising the translation probability as defined by the log-linear combination of a number of feature functions, whose weights are chosen to opti457 Proceedings of the 6th Workshop on Statistical Machine Translation, pages 457–463, c Edinburgh, Scotland, UK, July 30–31, 2011. 2011 Association for Computational Linguistics mise translation quality (Och, 2003). A core component of every PBSMT system is the phrase table, which contains bilingual phrase pairs extracted from a bilingual corpus after word alignment (Och and Ney, 2003). The set of translations from which the most probable one is chosen is built by segmenting the source-language (SL) sentence in all possible ways and then combining the translation of the different source segments according to the phrase table. Common feature functions are: source-to-target and target-to-source phrase translation probabilities, source-to-target and target-to-source lexical weightings (calculated by using a probabilistic bilingual dictionary), reordering costs, number of words in the output (word penalty), number of phrase pairs used (phrase penalty), and likelihood of the out"
W11-2157,P03-1021,0,0.156636,"n 2.1) and the shallow-transfer RBMT (section 2.2) systems we have used in our hybridisation approach. 2.1 Phrase-based statistical machine translation Phrase-based statistical machine translation systems (Koehn et al., 2003) translate sentences by maximising the translation probability as defined by the log-linear combination of a number of feature functions, whose weights are chosen to opti457 Proceedings of the 6th Workshop on Statistical Machine Translation, pages 457–463, c Edinburgh, Scotland, UK, July 30–31, 2011. 2011 Association for Computational Linguistics mise translation quality (Och, 2003). A core component of every PBSMT system is the phrase table, which contains bilingual phrase pairs extracted from a bilingual corpus after word alignment (Och and Ney, 2003). The set of translations from which the most probable one is chosen is built by segmenting the source-language (SL) sentence in all possible ways and then combining the translation of the different source segments according to the phrase table. Common feature functions are: source-to-target and target-to-source phrase translation probabilities, source-to-target and target-to-source lexical weightings (calculated by using"
W11-2157,P02-1040,0,0.086998,".The linguistic data contains 326 228 entries in the bilingual dictionary, 106 first-level structural transfer rules, and 31 second-level rules. As entries in the bilingual dictionary contain mappings between SL and TL lemmas, when phrase pairs matching the bilingual dictionary are generated all the possible inflections of these lemmas are produced. We used the free/open-source PBSMT system Moses (Koehn et al., 2007), together with the IRSTLM language modelling toolkit (Federico et al., 2008), which was used to train a 5-gram lanTable 2 reports the translation performance as measured by BLEU (Papineni et al., 2002), GTM (Melamed et al., 2003) and METEOR2 (Banerjee and Lavie, 2005) for Apertium and the three systems presented in the previous section, as well as the size of the phrase table and the amount of unknown words in the test set. The hybrid approach outperforms the baseline PBSMT system in terms of the three evaluation metrics. The confidence interval of the difference between them, computed by doing 1 000 iterations of paired 1 The corpora can be downloaded from http://www. statmt.org/wmt11/translation-task.html. 2 Modules exact, stem, synonym and paraphrase (Denkowski and Lavie, 2010) were used"
W11-2157,W09-0423,0,0.340076,"ated: on the other hand<adv> my<det><pos><pl> american<adj> friend<n><pl> have<vbhaver><pres> decide<vblex><pp> to<pr> come<vblex><inf> Finally, the translation into TL is generated from the TL lexical forms: On the other hand my American friends have decided to come. 3 Related work Linguistic data from RBMT have already been used to enrich SMT systems in different ways. Bilingual 459 dictionaries have been added to SMT systems since its early days (Brown et al., 1993); one of the simplest strategies involves adding the dictionary entries directly to the training parallel corpus (Tyers, 2009; Schwenk et al., 2009). Other approaches go beyond that. Eisele et al. (2008) first translate the sentences in the test set with an RBMT system, then apply the usual phrase-extraction algorithm over the resulting small parallel corpus, and finally add the obtained phrase pairs to the original phrase table. It is worth noting that neither of these two strategies guarantee that the multi-word expressions in the RBMT bilingual dictionary appearing in the sentences to translate will be translated as such because they may be split into smaller units by the phrase-extraction algorithm. Our approach overcomes this issue b"
W11-2157,2009.mtsummit-posters.21,0,0.0348512,"y the Transducens Research Group (Universitat d’Alacant, Spain) to the shared translation task of the EMNLP 2011 Sixth Workshop on Statistical Machine Translation (WMT 2011). We participated in the Spanish–English task with a hybrid system that combines, in a phrase-based statistical machine translation (PBSMT) system, bilingual information obtained from parallel corpora in the usual way (Koehn, 2010, ch. 5), and bilingual information from the Spanish–English language pair in the Apertium (Forcada et al., 2011) rule-based machine translation (RMBT) platform. A wide range of hybrid approaches (Thurmair, 2009) may be taken in order to build a machine translation system which takes advantage of a parallel corpus and explicit linguistic information from RBMT. In particular, our hybridisation approach directly enriches the phrase table of a PBSMT system with phrase pairs generated from the explicit linguistic resources from an Apertium-based shallowtransfer RBMT system. Apertium, which is described in detail below, does not perform a complete syntactic analysis of the input sentences, but rather works with simpler linear intermediate representations. The rest of the paper is organised as follows. Next"
W11-2157,2009.eamt-1.29,0,0.229508,"orms is generated: on the other hand<adv> my<det><pos><pl> american<adj> friend<n><pl> have<vbhaver><pres> decide<vblex><pp> to<pr> come<vblex><inf> Finally, the translation into TL is generated from the TL lexical forms: On the other hand my American friends have decided to come. 3 Related work Linguistic data from RBMT have already been used to enrich SMT systems in different ways. Bilingual 459 dictionaries have been added to SMT systems since its early days (Brown et al., 1993); one of the simplest strategies involves adding the dictionary entries directly to the training parallel corpus (Tyers, 2009; Schwenk et al., 2009). Other approaches go beyond that. Eisele et al. (2008) first translate the sentences in the test set with an RBMT system, then apply the usual phrase-extraction algorithm over the resulting small parallel corpus, and finally add the obtained phrase pairs to the original phrase table. It is worth noting that neither of these two strategies guarantee that the multi-word expressions in the RBMT bilingual dictionary appearing in the sentences to translate will be translated as such because they may be split into smaller units by the phrase-extraction algorithm. Our approach"
W11-2157,zhang-etal-2004-interpreting,0,0.029589,"Missing"
W11-2157,W07-0734,0,\N,Missing
W14-3319,D11-1033,0,0.199736,"nt sets, to obtain our final SMT system. Our submission for the English to French translation task was ranked second amongst nine teams and a total of twenty submissions. 1 To train the LMs, monolingual corpora and the target side of the parallel corpora are first used individually to train models. Then the individual models are interpolated according to perplexity minimisation on the development sets. To train the TMs, first a baseline is built using the News Commentary parallel corpus. Second, each remaining parallel corpus is processed individually using bilingual cross-entropy difference (Axelrod et al., 2011) in order to separate pseudo in-domain and out-of-domain sentence pairs, and filtering the pseudo out-ofdomain instances with the vocabulary saturation approach (Lewis and Eetemadi, 2013). Third, synthetic translation rules are automatically extracted from the development set and used to train another translation model following a novel approach (S´anchez-Cartagena et al., 2014). Finally, we interpolate the four translation models (baseline, in-domain, filtered out-of-domain and rules) by minimising the perplexity obtained on the development sets and investigate the best tuning and decoding pa"
W14-3319,P11-1105,0,0.180288,"encodes a different degree of generalisation over the particular example it has been extracted from. Finally, the minimum set of rules which correctly reproduces all the bilingual phrases is found based on integer linear programming search (Garfinkel and Nemhauser, 1972). Once the rules have been inferred, the phrase table is built from them and the original rulebased MT dictionaries, following the method by S´anchez-Cartagena et al. (2011), which was one of winning systems4 (together with two online SMT systems) in the pairwise manual evaluation of the WMT11 English–Spanish translation task (Callison-Burch et al., 2011). This phrasetable is then interpolated with the baseline TM and the results are presented in Table 5. A slight improvement over the baseline is observed, which motivates the use of synthetic rules in our final MT system. This small improvement may be related to the small coverage of the Apertium dictionaries: the English–French bilingual dictionary has a low number of entries compared to more mature language pairs in Apertium which have around 20 times more bilingual entries. System the number of n-bests used by MERT. Results obtained on the development set newstest2013 are reported in Table"
W14-3319,W13-2212,0,0.0160957,"except when explicitly said. The decoder used to generate translations is M OSES using features weights optimised with MERT (Och, 2003). As our approach relies on training individual TMs, one for each parallel corpus, our final TM is obtained by linearly interpolating the individual ones. The interpolation of TMs is performed using the script tmcombine.py, minimising the cross-entropy between the TM and the concatenated development sets from 2008 to 2012 (noted newstest2008-2012), as described in Sennrich (2012). Finally, we make use of the findings from WMT 2013 brought by the winning team (Durrani et al., 2013) and decide to use the Operation Sequence Model (OSM), based on minimal translation units and Markov chains over sequences of operations, implemented in M OSES Sentences (k) Words (M) Monolingual Data – English Europarl v7 2,218.2 News Commentary v8 304.2 News Shuffled 2007 3,782.5 News Shuffled 2008 12,954.5 News Shuffled 2009 14,680.0 News Shuffled 2010 6,797.2 News Shuffled 2011 15,437.7 News Shuffled 2012 14,869.7 News Shuffled 2013 21,688.4 LDC afp 7,184.9 LDC apw 8,829.4 LDC cna 618.4 LDC ltw 986.9 LDC nyt 5,327.7 LDC wpb 108.8 LDC xin 5,121.9 59.9 7.4 90.2 308.1 347.0 157.8 358.1 345.5"
W14-3319,W08-0509,0,0.170818,"test sets from previous years of WMT, from 2008 to 2013 (newstest2008-2013). Finally, the training parallel corpora are cleaned using the script clean-corpus-n.perl, keeping the sentences longer than 1 word, shorter than 80 words, and with a length ratio between sentence pairs lower than 4.2 The statistics about the corpora used in our experiments after pre-processing are presented in Table 1. For training LMs we use K EN LM (Heafield et al., 2013) and the SRILM tool-kit (Stolcke et al., 2011). For training TMs, we use M OSES (Koehn et al., 2007) version 2.1 with MGIZA ++ (Och and Ney, 2003; Gao and Vogel, 2008). These tools are used with default parameters for our experiments except when explicitly said. The decoder used to generate translations is M OSES using features weights optimised with MERT (Och, 2003). As our approach relies on training individual TMs, one for each parallel corpus, our final TM is obtained by linearly interpolating the individual ones. The interpolation of TMs is performed using the script tmcombine.py, minimising the cross-entropy between the TM and the concatenated development sets from 2008 to 2012 (noted newstest2008-2012), as described in Sennrich (2012). Finally, we ma"
W14-3319,P13-2121,0,0.0606903,"Missing"
W14-3319,P07-2045,0,0.0136523,"with the focus on the English to French direction. Language models (LMs) and translation 171 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 171–177, c Baltimore, Maryland USA, June 26–27, 2014. 2014 Association for Computational Linguistics 2 Datasets and Tools Corpus We use all the monolingual and parallel datasets in English and French provided by the shared task organisers, as well as the LDC Gigaword for the same languages1 . For each language, a true-case model is trained using all the data, using the traintruecaser.perl script included in the M OSES toolkit (Koehn et al., 2007). Punctuation marks of all the monolingual and parallel corpora are then normalised using the script normalize-punctuation.perl provided by the organisers, before being tokenised and true-cased using the scripts distributed with the M OSES toolkit. The same pre-processing steps are applied to the development and test sets. As development sets, we used all the test sets from previous years of WMT, from 2008 to 2013 (newstest2008-2013). Finally, the training parallel corpora are cleaned using the script clean-corpus-n.perl, keeping the sentences longer than 1 word, shorter than 80 words, and wit"
W14-3319,W13-2235,0,0.294214,"he LMs, monolingual corpora and the target side of the parallel corpora are first used individually to train models. Then the individual models are interpolated according to perplexity minimisation on the development sets. To train the TMs, first a baseline is built using the News Commentary parallel corpus. Second, each remaining parallel corpus is processed individually using bilingual cross-entropy difference (Axelrod et al., 2011) in order to separate pseudo in-domain and out-of-domain sentence pairs, and filtering the pseudo out-ofdomain instances with the vocabulary saturation approach (Lewis and Eetemadi, 2013). Third, synthetic translation rules are automatically extracted from the development set and used to train another translation model following a novel approach (S´anchez-Cartagena et al., 2014). Finally, we interpolate the four translation models (baseline, in-domain, filtered out-of-domain and rules) by minimising the perplexity obtained on the development sets and investigate the best tuning and decoding parameters. Introduction The reminder of this paper is organised as follows: the datasets and tools used in our experiments are described in Section 2. Then, details about the LMs and TMs a"
W14-3319,P10-2041,0,0.0671112,"ces and (ii) reduce the total amount of out-of-domain data. Second, a novel approach for the automatic extraction of translation rules and their use to enrich the phrase table is detailed. 10 Parallel Data Filtering and Vocabulary Saturation Bilingual Cross-Entropy Difference 4.1 Amongst the parallel corpora provided by the shared task organisers, only News Commentary can be considered as in-domain regarding the development and test sets. We use this training corpus to build our baseline SMT system. The other parallel corpora are individually filtered using bilingual cross-entropy difference (Moore and Lewis, 2010; Axelrod et al., 2011). This data filtering method relies on four LMs, two in the source and two in the target language, which aim to model particular features of in and out-ofdomain sentences. We build the in-domain LMs using the source and target sides of the News Commentary parallel corpus. Out-of-domain LMs are trained on a vocabulary-constrained subset of each remaining parallel corpus individually using the SRILM toolkit, which leads to eight models (four in the source language and four in the target language).3 8 6 4 2 0 Common Crawl Europarl 10^9 UN -2 -4 0 2k 4k 6k Sentence Pairs 8k"
W14-3319,J03-1002,0,0.01001,"ts, we used all the test sets from previous years of WMT, from 2008 to 2013 (newstest2008-2013). Finally, the training parallel corpora are cleaned using the script clean-corpus-n.perl, keeping the sentences longer than 1 word, shorter than 80 words, and with a length ratio between sentence pairs lower than 4.2 The statistics about the corpora used in our experiments after pre-processing are presented in Table 1. For training LMs we use K EN LM (Heafield et al., 2013) and the SRILM tool-kit (Stolcke et al., 2011). For training TMs, we use M OSES (Koehn et al., 2007) version 2.1 with MGIZA ++ (Och and Ney, 2003; Gao and Vogel, 2008). These tools are used with default parameters for our experiments except when explicitly said. The decoder used to generate translations is M OSES using features weights optimised with MERT (Och, 2003). As our approach relies on training individual TMs, one for each parallel corpus, our final TM is obtained by linearly interpolating the individual ones. The interpolation of TMs is performed using the script tmcombine.py, minimising the cross-entropy between the TM and the concatenated development sets from 2008 to 2012 (noted newstest2008-2012), as described in Sennrich"
W14-3319,P03-1021,0,0.0469427,"rter than 80 words, and with a length ratio between sentence pairs lower than 4.2 The statistics about the corpora used in our experiments after pre-processing are presented in Table 1. For training LMs we use K EN LM (Heafield et al., 2013) and the SRILM tool-kit (Stolcke et al., 2011). For training TMs, we use M OSES (Koehn et al., 2007) version 2.1 with MGIZA ++ (Och and Ney, 2003; Gao and Vogel, 2008). These tools are used with default parameters for our experiments except when explicitly said. The decoder used to generate translations is M OSES using features weights optimised with MERT (Och, 2003). As our approach relies on training individual TMs, one for each parallel corpus, our final TM is obtained by linearly interpolating the individual ones. The interpolation of TMs is performed using the script tmcombine.py, minimising the cross-entropy between the TM and the concatenated development sets from 2008 to 2012 (noted newstest2008-2012), as described in Sennrich (2012). Finally, we make use of the findings from WMT 2013 brought by the winning team (Durrani et al., 2013) and decide to use the Operation Sequence Model (OSM), based on minimal translation units and Markov chains over se"
W14-3319,P02-1040,0,0.0911366,"d the in-domain LMs using the source and target sides of the News Commentary parallel corpus. Out-of-domain LMs are trained on a vocabulary-constrained subset of each remaining parallel corpus individually using the SRILM toolkit, which leads to eight models (four in the source language and four in the target language).3 8 6 4 2 0 Common Crawl Europarl 10^9 UN -2 -4 0 2k 4k 6k Sentence Pairs 8k 10k Figure 1: Sample of ranked sentence-pairs (10k) of each of the out-of-domain parallel corpora with bilingual cross-entropy difference The results obtained using the pseudo indomain data show B LEU (Papineni et al., 2002) scores superior or equal to the baseline score. Only the Europarl subset is slightly lower than the baseline, while the subset taken from the 109 corpus reaches the highest B LEU compared to the other systems (30.29). This is mainly due to the 3 The subsets contain the same number of sentences and the same vocabulary as News Commentary. 173 size of this subset which is ten times larger than the one taken from Europarl. The last row of Table 3 shows the B LEU score obtained after interpolating the four pseudo in-domain translation models. This system outperforms the best pseudo indomain one by"
W14-3319,2011.mtsummit-papers.64,1,0.889124,"Missing"
W14-3319,E12-1055,0,0.0325549,"Ney, 2003; Gao and Vogel, 2008). These tools are used with default parameters for our experiments except when explicitly said. The decoder used to generate translations is M OSES using features weights optimised with MERT (Och, 2003). As our approach relies on training individual TMs, one for each parallel corpus, our final TM is obtained by linearly interpolating the individual ones. The interpolation of TMs is performed using the script tmcombine.py, minimising the cross-entropy between the TM and the concatenated development sets from 2008 to 2012 (noted newstest2008-2012), as described in Sennrich (2012). Finally, we make use of the findings from WMT 2013 brought by the winning team (Durrani et al., 2013) and decide to use the Operation Sequence Model (OSM), based on minimal translation units and Markov chains over sequences of operations, implemented in M OSES Sentences (k) Words (M) Monolingual Data – English Europarl v7 2,218.2 News Commentary v8 304.2 News Shuffled 2007 3,782.5 News Shuffled 2008 12,954.5 News Shuffled 2009 14,680.0 News Shuffled 2010 6,797.2 News Shuffled 2011 15,437.7 News Shuffled 2012 14,869.7 News Shuffled 2013 21,688.4 LDC afp 7,184.9 LDC apw 8,829.4 LDC cna 618.4 L"
W14-3319,2006.amta-papers.25,0,0.0323304,"r detruecasing and de-tokenising using the scripts distributed with M OSES. This setup allowed us to compare our results with the participants of the translation shared task last year. We pick the decoding parameters leading to the best results in terms of B LEU and decode the official test set of WMT14 newstest2014. The results are reported in Table 7. Results on newstest2013 show that the decoding parameters investigation leads to an overall improvement of 0.1 B LEU absolute. The results on newstest2014 show that adding synthetic rules did not help improving B LEU and degraded slightly TER (Snover et al., 2006) scores. In addition to our English→French submission, we submitted a French→English translation. Our French→English MT system is built on the alignments obtained from the English→French direction. The training processes between the two sys27.76 28.06 Table 5: BLEU scores reported by MERT on English–French newstest2013 for the baseline SMT system standalone and with automatically extracted translation rules. 5 27.76 31.93 31.90 32.21 32.10 Table 6: B LEU scores reported by MERT on English–French newstest2013 development set. BLEUdev Baseline Baseline+Rules BLEUdev Tuning and Decoding We presen"
W14-3319,D07-1080,0,0.0658355,"experiments. Adding the synthetic translation rules degrades B LEU (as indicated by the last row in the Table), thus we decide to submit two systems to the shared task: one without and one with synthetic rules. By submitting a system without synthetic rules, we also ensure that our SMT system is constrained according to the shared task guidelines. System Baseline + pseudo in + pseudo out + OSM + MERT 200-best + Rules As MERT is not suitable when a large number of features are used (our system uses 19 fetures), we switch to the Margin Infused Relaxed Algorithm (MIRA) for our submitted systems (Watanabe et al., 2007). The development set used is newstest2012, as we aim to select the best decoding parameters according to the scores obtained when decoding the newstest2013 corpus, after detruecasing and de-tokenising using the scripts distributed with M OSES. This setup allowed us to compare our results with the participants of the translation shared task last year. We pick the decoding parameters leading to the best results in terms of B LEU and decode the official test set of WMT14 newstest2014. The results are reported in Table 7. Results on newstest2013 show that the decoding parameters investigation lea"
W14-3319,W14-3320,1,\N,Missing
W14-3320,W05-0909,0,0.060513,"e δ are then discarded before solving the minimisation problem. The value of δ is chosen so that it maximises, on the development corpus, the BLEU score (Papineni et al., 2002) obtained by an Apertium-based system which uses the inferred rules; in our submission δ = 0.15. In addition, rules that do not correctly reproduce at least 100 bilingual phrase pairs were also discarded in order to make the minimisation problem computationally feasible. 6 Results and discussion Table 2 reports the translation performance as measured by BLEU (Papineni et al., 2002), TER (Snover et al., 2006) and METEOR (Banerjee and Lavie, 2005) achieved by the baseline PBSMT, our submission (UA-Prompsit), Apertium when it uses the set of inferred rules, and Apertium when it uses no rules at all (word-for-word translation). The size of the phrase table and the amount of unknown words in the test set are also reported when applicable. According to the three evaluation metrics, the translation performance of our submission is very close to that of the PBSMT baseline (slightly better according to BLEU and TER, and slightly worse according to METEOR). The difference between both systems computed by paired bootstrap system baseline UA-Pro"
W14-3320,W08-0319,0,0.04399,"Missing"
W14-3320,W11-2103,0,0.0214179,"followed the path we started with our submission to the Spanish–English WMT 2011 shared translation task (S´anchez-Cartagena et al., 2011b) which consisted of enriching the phrase table of a PBSMT system with phrase pairs generated using the dictionaries and rules in the Apertium (Forcada et al., 2011) Spanish–English RBMT system; our approach was one of the winners1 (together with two online SMT systems that were not submitted for the task but were included in the evaluation by the organisers and a system by Systran) in the pairwise manual evaluation of the English–Spanish translation task (Callison-Burch et al., 2011). In this submission, however, we only borrow the dictionaries from the Apertium English–French RBMT system and use them to automatically infer the rules from a parallel corpus. We therefore avoid the need for human-written rules, which are usually written by trained experts, and explore a novel way to add morphological information to PBSMT. The rules inferred from corpora and used to enlarge the phrase table are shallow-transfer rules that build their output with the help of the bilingual dictionary and work on flat intermediate representations (see section 3.1); no syntactic parsing is conse"
W14-3320,W08-0328,0,0.0176764,"ed as follows. The following section outlines related hybrid approaches. Section 3 formally defines the RBMT paradigm and summarises the method followed to automatically infer the shallow-transfer rules, whereas the enrichment of the phrase table is described in section 4. Sections 5 and 6 describe, respectively, the resources we used to build our submission and the results achieved for the English– French language pair. The paper ends with some concluding remarks. 2 Related work Linguistic data from RBMT systems have already been used to enrich SMT systems (Tyers, 2009; Schwenk et al., 2009; Eisele et al., 2008; S´anchezCartagena et al., 2011a). We have already proved 1 No other system was found statistically significantly better using the sign test at p ≤ 0.10. 178 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 178–185, c Baltimore, Maryland USA, June 26–27, 2014. 2014 Association for Computational Linguistics that using hand-written rules and dictionaries from RBMT yields better results than using only dictionaries (S´anchez-Cartagena et al., 2011a). However, in the approach we present in this paper, rules are automatically inferred from a parallel corpus after convert"
W14-3320,2010.iwslt-papers.9,0,0.0274603,"Missing"
W14-3320,P13-2121,0,0.0242619,"phrase pairs obtained from the RBMT resources is added to the phrase table in order to conveniently weight the synthetic RBMT phrase pairs. 5 Translation model System training Language model We built a baseline PBSMT Moses (Koehn et al., 2007) system6 from a subset of the parallel corpora distributed as part of the WMT 2014 shared translation task, namely Europarl (Koehn, 2005), News Commentary and Common Crawl, and a subset of the French monolingual corpora, namely Common Crawl, Europarl, News Commentary and News Crawl. The language model was built with the KenLM language modelling toolkit (Heafield et al., 2013), which was used to train a 5-gram language model using interpolated Kneser-Ney discounting (Goodman and Chen, 1998). Word alignments were computed by means of GIZA++ (Och and Ney, 2003). The weights of the different feature functions were optimised by means of minimum error rate training (Och, 2003) on the 2013 WMT test set.7 The phrase table of this baseline system was then enriched with phrase pairs generated from rules automatically inferred from the concatenation of the test corpora distributed for the WMT 2008–2012 shared translation tasks, and from the English–French bilingual dictionar"
W14-3320,D07-1091,0,0.0348951,"ranslation, pages 178–185, c Baltimore, Maryland USA, June 26–27, 2014. 2014 Association for Computational Linguistics that using hand-written rules and dictionaries from RBMT yields better results than using only dictionaries (S´anchez-Cartagena et al., 2011a). However, in the approach we present in this paper, rules are automatically inferred from a parallel corpus after converting it into the intermediate representation used by the Apertium RBMT platform (see section 3.3). It can be therefore seen as a novel method to add morphological information to SMT, as factored translation models do (Koehn and Hoang, 2007; Graham and van Genabith, 2010). Unlike factored models, we do not estimate independent statistical models for the translation of the different factors (lemmas, lexical categories, morphological inflection attributes, etc.) and for the generation of the final surface forms. Instead, we first infer a set of rules that deal with the grammatical divergences between the languages involved by performing operations such as reorderings, gender and number agreements, etc. Afterwards, we add synthetic phrase pairs generated from these rules and the Apertium dictionaries to the data from which the well"
W14-3320,P07-2045,0,0.0187495,"orpus-extracted phrase pairs, and then the phrase translation probabilities are computed by relative frequency as usual (Koehn, 2010, sec. 5.2.5). A boolean feature for our rule inferring algorithm there are fewer bilingual phrases from which to infer them, and consequently fewer evidence from which to extract the right reference attributes. Task function to flag bilingual phrase pairs obtained from the RBMT resources is added to the phrase table in order to conveniently weight the synthetic RBMT phrase pairs. 5 Translation model System training Language model We built a baseline PBSMT Moses (Koehn et al., 2007) system6 from a subset of the parallel corpora distributed as part of the WMT 2014 shared translation task, namely Europarl (Koehn, 2005), News Commentary and Common Crawl, and a subset of the French monolingual corpora, namely Common Crawl, Europarl, News Commentary and News Crawl. The language model was built with the KenLM language modelling toolkit (Heafield et al., 2013), which was used to train a 5-gram language model using interpolated Kneser-Ney discounting (Goodman and Chen, 1998). Word alignments were computed by means of GIZA++ (Och and Ney, 2003). The weights of the different featu"
W14-3320,W04-3250,0,0.0292718,"0.5781 0.7767 0.8368 METEOR 0.5441 0.5432 0.3168 0.2617 # of unknown words 870 861 4 743 4 743 phrase table size 100 530 734 100 585 182 - Table 2: Case-insensitive BLEU, TER, and METEOR scores obtained, on the newstest2014 test set, by the baseline PBSMT system (baseline), the hybrid system submitted to the WMT 2014 shared translation task (UA-Prompsit), Apertium when it uses the set of inferred rules (Apertium-rules), and Apertium when it uses no rules at all (Apertium-word-for-word). The number of unknown words and the size of the phrase table are also reported when applicable. resampling (Koehn, 2004) is not statistically significant for any of the three evaluation metrics (1 000 iterations, p = 0.05). An inspection of the 86 rules inferred shows that they encode some of the transformations that one would expect from a set of English–French rules, such as gender and number agreements between nouns, determiners and adjectives, preposition changes, and the introduction of the auxiliary verb avoir for the past tense. In addition, the improvement over word-for-word translation achieved when they are used by Apertium is statistically significant for the three evaluation metrics. One of the reas"
W14-3320,2005.mtsummit-papers.11,0,0.0185538,"). A boolean feature for our rule inferring algorithm there are fewer bilingual phrases from which to infer them, and consequently fewer evidence from which to extract the right reference attributes. Task function to flag bilingual phrase pairs obtained from the RBMT resources is added to the phrase table in order to conveniently weight the synthetic RBMT phrase pairs. 5 Translation model System training Language model We built a baseline PBSMT Moses (Koehn et al., 2007) system6 from a subset of the parallel corpora distributed as part of the WMT 2014 shared translation task, namely Europarl (Koehn, 2005), News Commentary and Common Crawl, and a subset of the French monolingual corpora, namely Common Crawl, Europarl, News Commentary and News Crawl. The language model was built with the KenLM language modelling toolkit (Heafield et al., 2013), which was used to train a 5-gram language model using interpolated Kneser-Ney discounting (Goodman and Chen, 1998). Word alignments were computed by means of GIZA++ (Och and Ney, 2003). The weights of the different feature functions were optimised by means of minimum error rate training (Och, 2003) on the 2013 WMT test set.7 The phrase table of this basel"
W14-3320,J10-4005,0,0.178128,"ot written by humans, but automatically inferred from a parallel corpus. 1 Introduction This paper describes the system jointly submitted by the Departament de Llenguatges i Sistemes Inform`atics at Universitat d’Alacant and the Prompsit Language Engineering company to the shared translation task of the ACL 2014 Ninth Workshop on Statistical Machine Translation (WMT 2014). We participated in the English–French translation task with a hybrid system that combines, in a phrase-based statistical machine translation (PBSMT) system, bilingual phrases obtained from parallel corpora in the usual way (Koehn, 2010, ch. 5), and also bilingual phrases obtained from the existing dictionaries in the Apertium rule-based machine translation (RBMT) platform (Forcada et al., 2011) and a number of shallow-transfer machine translation rules automatically inferred from a small subset of the training corpus. Among the different approaches for adding linguistic information to SMT systems (Costa-Juss`a and Farr´us, 2014), we followed the path we started with our submission to the Spanish–English WMT 2011 shared translation task (S´anchez-Cartagena et al., 2011b) which consisted of enriching the phrase table of a PBS"
W14-3320,J03-1002,0,0.0135605,"integrated into the PBSMT system’s phrase table, encoded with the formalism presented in the previous section, are obtained from the parallel corpus by applying the steps described in this section. They are a subset of the steps followed by S´anchez-Cartagena et al. (2014) to infer shallow-transfer rules to be used in Apertium from small parallel corpora. First, both sides of the parallel corpus are morphologically analysed and converted into the intermediate representations used by Apertium. Word alignments are then obtained by symmetrising (using the refined intersection method proposed by Och and Ney (2003)) the set of alignments provided by GIZA++ (Och and Ney, 2003) when it is run on both translations directions. Afterwards, the bilingual phrase pairs compatible with the alignments are extracted as it is usually done in SMT (Koehn, 2010, Sec. 5.2.3), and those that are not compatible with the bilingual dictionary of the Apertium English–French RBMT system3 or 2 In addition to that criterion, our formalism also permits restricting the application of a rule to the SL lexical forms that, after being looked up in the bilingual dictionary, the TL lexical forms obtained from them have specific morph"
W14-3320,P03-1021,0,0.0359887,"the WMT 2014 shared translation task, namely Europarl (Koehn, 2005), News Commentary and Common Crawl, and a subset of the French monolingual corpora, namely Common Crawl, Europarl, News Commentary and News Crawl. The language model was built with the KenLM language modelling toolkit (Heafield et al., 2013), which was used to train a 5-gram language model using interpolated Kneser-Ney discounting (Goodman and Chen, 1998). Word alignments were computed by means of GIZA++ (Och and Ney, 2003). The weights of the different feature functions were optimised by means of minimum error rate training (Och, 2003) on the 2013 WMT test set.7 The phrase table of this baseline system was then enriched with phrase pairs generated from rules automatically inferred from the concatenation of the test corpora distributed for the WMT 2008–2012 shared translation tasks, and from the English–French bilingual dictionary in the Apertium platform.8 Since the minimisation problem which needs to be solved in order to obtain the rules is very time-consuming, we chose a small rule inference corpus similar to this year’s test set. The bilingual dictionary, which contains mappings between SL and TL lemmas, consists of 13"
W14-3320,P02-1040,0,0.0927506,"more than 40 tokens. was split into two parts: the larger one (4/5 of the corpus) was used for actual rule inference as described in section 3.3; the remaining corpus was used as a development corpus as explained next. For each rule z, first the proportion r(z) of bilingual phrase pairs correctly reproduced by the rule divided by the number of bilingual phrases it matches is computed. Rules whose proportion r(z) is lower than a threshold value δ are then discarded before solving the minimisation problem. The value of δ is chosen so that it maximises, on the development corpus, the BLEU score (Papineni et al., 2002) obtained by an Apertium-based system which uses the inferred rules; in our submission δ = 0.15. In addition, rules that do not correctly reproduce at least 100 bilingual phrase pairs were also discarded in order to make the minimisation problem computationally feasible. 6 Results and discussion Table 2 reports the translation performance as measured by BLEU (Papineni et al., 2002), TER (Snover et al., 2006) and METEOR (Banerjee and Lavie, 2005) achieved by the baseline PBSMT, our submission (UA-Prompsit), Apertium when it uses the set of inferred rules, and Apertium when it uses no rules at a"
W14-3320,N06-1032,0,0.0438341,"Missing"
W14-3320,2011.mtsummit-papers.64,1,0.894664,"Missing"
W14-3320,W11-2157,1,0.793196,"Missing"
W14-3320,W09-0423,0,0.13984,"f the paper is organised as follows. The following section outlines related hybrid approaches. Section 3 formally defines the RBMT paradigm and summarises the method followed to automatically infer the shallow-transfer rules, whereas the enrichment of the phrase table is described in section 4. Sections 5 and 6 describe, respectively, the resources we used to build our submission and the results achieved for the English– French language pair. The paper ends with some concluding remarks. 2 Related work Linguistic data from RBMT systems have already been used to enrich SMT systems (Tyers, 2009; Schwenk et al., 2009; Eisele et al., 2008; S´anchezCartagena et al., 2011a). We have already proved 1 No other system was found statistically significantly better using the sign test at p ≤ 0.10. 178 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 178–185, c Baltimore, Maryland USA, June 26–27, 2014. 2014 Association for Computational Linguistics that using hand-written rules and dictionaries from RBMT yields better results than using only dictionaries (S´anchez-Cartagena et al., 2011a). However, in the approach we present in this paper, rules are automatically inferred from a parallel"
W14-3320,2006.amta-papers.25,0,0.0291398,"z) is lower than a threshold value δ are then discarded before solving the minimisation problem. The value of δ is chosen so that it maximises, on the development corpus, the BLEU score (Papineni et al., 2002) obtained by an Apertium-based system which uses the inferred rules; in our submission δ = 0.15. In addition, rules that do not correctly reproduce at least 100 bilingual phrase pairs were also discarded in order to make the minimisation problem computationally feasible. 6 Results and discussion Table 2 reports the translation performance as measured by BLEU (Papineni et al., 2002), TER (Snover et al., 2006) and METEOR (Banerjee and Lavie, 2005) achieved by the baseline PBSMT, our submission (UA-Prompsit), Apertium when it uses the set of inferred rules, and Apertium when it uses no rules at all (word-for-word translation). The size of the phrase table and the amount of unknown words in the test set are also reported when applicable. According to the three evaluation metrics, the translation performance of our submission is very close to that of the PBSMT baseline (slightly better according to BLEU and TER, and slightly worse according to METEOR). The difference between both systems computed by p"
W14-3320,2009.eamt-1.29,0,0.043133,"d. The rest of the paper is organised as follows. The following section outlines related hybrid approaches. Section 3 formally defines the RBMT paradigm and summarises the method followed to automatically infer the shallow-transfer rules, whereas the enrichment of the phrase table is described in section 4. Sections 5 and 6 describe, respectively, the resources we used to build our submission and the results achieved for the English– French language pair. The paper ends with some concluding remarks. 2 Related work Linguistic data from RBMT systems have already been used to enrich SMT systems (Tyers, 2009; Schwenk et al., 2009; Eisele et al., 2008; S´anchezCartagena et al., 2011a). We have already proved 1 No other system was found statistically significantly better using the sign test at p ≤ 0.10. 178 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 178–185, c Baltimore, Maryland USA, June 26–27, 2014. 2014 Association for Computational Linguistics that using hand-written rules and dictionaries from RBMT yields better results than using only dictionaries (S´anchez-Cartagena et al., 2011a). However, in the approach we present in this paper, rules are automatically in"
W15-3036,C04-1046,0,0.174452,"te) and the bilingual concordancer Reverso Context. After obtaining the subsegment correspondences, a collection of features is extracted from them, which are then used by a binary classifer to obtain the final “GOOD” or “BAD” word-level quality labels. We prepared two submissions for this year’s edition of WMT 2015: one using the features produced by our system, and one combining them with the baseline features published by the organisers of the task, which were ranked third and first for the sub-task, respectively. 1 translation for dissemination. Consequently, MT quality estimation (MTQE) (Blatz et al., 2004; Specia et al., 2010; Specia and Soricut, 2013) has emerged as a mean to minimise the post-editing effort by developing techniques that allow to estimate the quality of the translation hypotheses produced by an MT system. In order to boost the scientific efforts on this problem, the WMT 2015 MTQE shared task proposes three tasks that allow to compare different approaches at three different levels: segment-level (sub-task 1), word-level (sub-task 2), and document-level (sub-task 3). Our submissions tackle the word-level MTQE sub-task, which proposes a framework for evaluating and comparing dif"
W15-3036,S12-1065,1,0.85218,"Missing"
W15-3036,W15-4903,1,0.387153,"Missing"
W15-3036,2013.tc-1.10,0,0.0629579,"o Context. After obtaining the subsegment correspondences, a collection of features is extracted from them, which are then used by a binary classifer to obtain the final “GOOD” or “BAD” word-level quality labels. We prepared two submissions for this year’s edition of WMT 2015: one using the features produced by our system, and one combining them with the baseline features published by the organisers of the task, which were ranked third and first for the sub-task, respectively. 1 translation for dissemination. Consequently, MT quality estimation (MTQE) (Blatz et al., 2004; Specia et al., 2010; Specia and Soricut, 2013) has emerged as a mean to minimise the post-editing effort by developing techniques that allow to estimate the quality of the translation hypotheses produced by an MT system. In order to boost the scientific efforts on this problem, the WMT 2015 MTQE shared task proposes three tasks that allow to compare different approaches at three different levels: segment-level (sub-task 1), word-level (sub-task 2), and document-level (sub-task 3). Our submissions tackle the word-level MTQE sub-task, which proposes a framework for evaluating and comparing different approaches. This year, the sub-task used"
W15-3036,2015.eamt-1.4,1,\N,Missing
W15-4903,W14-3339,0,0.11879,"Missing"
W15-4903,W13-2242,0,0.24993,"Missing"
W15-4903,W11-2131,0,0.0732552,"Missing"
W15-4903,C04-1046,0,0.168128,"ﬁeld of machine translation (MT) have led to the adoption of this technology by many companies and institutions all around the world in order to bypass the linguistic barriers and reach out to broader audiences. Unfortunately, we are still far from the point of having MT systems able to produce translations with the level of quality required for dissemination in formal scenarios, where human supervision and MT post-editing are unavoidable. It therefore becomes critical to minimise the cost of this human post-editing. This has motivated a growing interest in the ﬁeld of MT quality estimation (Blatz et al., 2004; Specia et al., 2010; Specia and Soricut, 2013), which is the ﬁeld that focuses on developing techniques that allow to estimate the quality of the translation hypotheses produced by an MT system. Most efforts in MT quality estimation (MTQE) are aimed at evaluating the quality of whole translated segments, in terms of post-editing time, number of editions needed, and other related metrics (Blatz et al., 2004). Our work is focused on the sub-ﬁeld of word-level MTQE. The main advantage of word-level MTQE is that it allows not only to estimate the effort needed to post-edit the output of an MT sy"
W15-4903,J93-2003,0,0.0449269,"ypothesis T of the SL sentence S to help the interactive MT system to choose the translation suggestions to be made to the user. Uefﬁng and Ney (2005) extend this application to word-level MTQE also to automatically reject those target words t with low conﬁdence scores from the translation proposals. This second approach incorporates the use of probabilistic lexicons as a source of translation information. Blatz et al. (2003) introduce a more complex collection of features for word-level MTQE, using semantic features based on WordNet (Miller, 1995), translation probabilities from IBM model 1 (Brown et al., 1993), word posterior probabilities (Blatz et al., 2003), and alignment templates from statistical MT (SMT) models. All the features they use are combined to train a binary classiﬁer which is used to determine the conﬁdence scores. Uefﬁng and Ney (2007) divide the features used 20 by their approach in two types: those which are independent of the MT system used for translation (system-independent), and those which are extracted from internal data of the SMT system they use for translation (system-dependent). These features are obtained by comparing the output of an SMT system T1 to a collection of"
W15-4903,W14-3340,0,0.0649819,"Missing"
W15-4903,2011.mtsummit-papers.18,1,0.897823,"Missing"
W15-4903,W03-0413,0,0.113451,"the ﬂy for new translations. The rest of the paper is organised as follows. Section 2 brieﬂy reviews the state of the art in word-level MTQE. Section 3 describes our binaryclassiﬁcation approach, the sources of information, and the collection of features used. Section 4 describes the experimental setting used for our experiments, whereas Section 5 reports and discusses the results obtained. The paper ends with some concluding remarks and the description of ongoing and possible future work. 2 Related work Some of the early work on word-level MTQE can be found in the context of interactive MT (Gandrabur and Foster, 2003; Uefﬁng and Ney, 2005). Gandrabur and Foster (2003) obtain conﬁdence scores for each word t in a given translation hypothesis T of the SL sentence S to help the interactive MT system to choose the translation suggestions to be made to the user. Uefﬁng and Ney (2005) extend this application to word-level MTQE also to automatically reject those target words t with low conﬁdence scores from the translation proposals. This second approach incorporates the use of probabilistic lexicons as a source of translation information. Blatz et al. (2003) introduce a more complex collection of features for w"
W15-4903,2013.tc-1.10,0,0.0690461,"d to the adoption of this technology by many companies and institutions all around the world in order to bypass the linguistic barriers and reach out to broader audiences. Unfortunately, we are still far from the point of having MT systems able to produce translations with the level of quality required for dissemination in formal scenarios, where human supervision and MT post-editing are unavoidable. It therefore becomes critical to minimise the cost of this human post-editing. This has motivated a growing interest in the ﬁeld of MT quality estimation (Blatz et al., 2004; Specia et al., 2010; Specia and Soricut, 2013), which is the ﬁeld that focuses on developing techniques that allow to estimate the quality of the translation hypotheses produced by an MT system. Most efforts in MT quality estimation (MTQE) are aimed at evaluating the quality of whole translated segments, in terms of post-editing time, number of editions needed, and other related metrics (Blatz et al., 2004). Our work is focused on the sub-ﬁeld of word-level MTQE. The main advantage of word-level MTQE is that it allows not only to estimate the effort needed to post-edit the output of an MT system, but also to guide post-editors on which wo"
W15-4903,P13-4014,0,0.0715472,"Missing"
W15-4903,2005.eamt-1.35,0,0.057781,". The rest of the paper is organised as follows. Section 2 brieﬂy reviews the state of the art in word-level MTQE. Section 3 describes our binaryclassiﬁcation approach, the sources of information, and the collection of features used. Section 4 describes the experimental setting used for our experiments, whereas Section 5 reports and discusses the results obtained. The paper ends with some concluding remarks and the description of ongoing and possible future work. 2 Related work Some of the early work on word-level MTQE can be found in the context of interactive MT (Gandrabur and Foster, 2003; Uefﬁng and Ney, 2005). Gandrabur and Foster (2003) obtain conﬁdence scores for each word t in a given translation hypothesis T of the SL sentence S to help the interactive MT system to choose the translation suggestions to be made to the user. Uefﬁng and Ney (2005) extend this application to word-level MTQE also to automatically reject those target words t with low conﬁdence scores from the translation proposals. This second approach incorporates the use of probabilistic lexicons as a source of translation information. Blatz et al. (2003) introduce a more complex collection of features for word-level MTQE, using s"
W15-4903,J07-1003,0,0.0732554,"th low conﬁdence scores from the translation proposals. This second approach incorporates the use of probabilistic lexicons as a source of translation information. Blatz et al. (2003) introduce a more complex collection of features for word-level MTQE, using semantic features based on WordNet (Miller, 1995), translation probabilities from IBM model 1 (Brown et al., 1993), word posterior probabilities (Blatz et al., 2003), and alignment templates from statistical MT (SMT) models. All the features they use are combined to train a binary classiﬁer which is used to determine the conﬁdence scores. Uefﬁng and Ney (2007) divide the features used 20 by their approach in two types: those which are independent of the MT system used for translation (system-independent), and those which are extracted from internal data of the SMT system they use for translation (system-dependent). These features are obtained by comparing the output of an SMT system T1 to a collection of alternative T translations {Ti }N i=2 obtained by using the N -best list from the same SMT system. Several distance metrics are then used to check how often word tj , the word in position j of T , is found in each translation alternative Ti , and h"
W15-4903,W14-3302,0,\N,Missing
W15-4904,C04-1046,0,0.309248,"censed under a Creative � Commons 3.0 licence, no derivative works, attribution, CCBY-ND. 27 Machine translation: You could, for instance, use machine translation (MT) to get a draft of the translation of each segment, MT(si ); vendors and experts tell you that you will save effort by postediting MT(si ) into your desired translation ti .1 Machine translation output MT(si ) may just be text, but it could come with annotations to help you make the most of it; for instance, words could be color-coded according to how conﬁdent the system is about them (Uefﬁng and Ney, 2007; Uefﬁng and Ney, 2005; Blatz et al., 2004), or unknown words that come out untranslated may be marked so that you spot them clearly. Machine-translated segments could even be accompanied by indicators of their estimated quality (Specia and Soricut, 2013; Specia et al., 2010; Blatz et al., 2004) which may be used to ascertain whether the output of the MT system is worth being post-edited or not. If someone measured your post-editing effort (in time, in number of keystrokes, in number of words changed, in money you would have to pay another translator to do it, etc.), when turning MT(si ) into ti , they could call that effort eMT i . Tr"
W15-4904,2011.eamt-1.28,1,0.853803,"sually as a percentage called fuzzy match score that accounts for the amount of text that is common to both segments2 — and even marks for you the words in s�i that do not match those in si . Let’s call all this information TM(si ): your job is to use it to turn t�i into the ﬁnal translation ti . If the fuzzy match is good, you will spend less effort than if you started from scratch. Let us call eTM the effort to turn the t�i provided by i TM(si ) into the desired translation ti .3 Mixing them up: You could even have available another technology, fuzzy-match repair (FMR; (Ortega et al., 2014; Dandapat et al., 2011; Hewavitharana et al., 2005; Kranias and Samiotou, 2004)), that integrates the two technologies just mentioned: after a suitable fuzzy match is found, machine translation (or another source of bilingual information) is used to repair, i.e. edit some parts of t�i , to take into account what changes from s�i to si to try to save even more effort; it tells you all that TM(si ) tells you, but also marks the parts that have been repaired. Fuzzy-match repair is one of the technologies that TAUS, the Translation Automation User Society, calls advanced leveraging;4 commercial examples of these are De"
W15-4904,W10-1751,0,0.122711,"Missing"
W15-4904,2012.amta-papers.6,0,0.0132858,"l., 2002), using reference translations in a development set. Most of these automatic 9 The measurements of effort that one can ﬁnd in literature vary from simple scores for “perceived” post-editing effort (usually scores taking 3 or 4 values) to actual post-editing time (see, for instance, the quality estimation task in WMT 2014 (Bojar et al., 2014)) measures are measures of similarity (or dissimilarity) between raw and reference translations. Researchers hope that their use during tuning will lead to a reduction in translation effort, although this is not currently guaranteed —for instance, Denkowski and Lavie (2012) found that BLEU could not distinguish between raw and post-edited machine translation. Generally, an automatic evaluation measure for technology X may have the form i i eˆX (X(si ; �λX ), {tij }nj=1 ;µ � X ), where {tij }nj=1 is the set of reference translations for segment si in the development set and µ � X is a set of tunable X i ;µ �X) parameters. Ideally, eˆ (X(si ; �λX ), {tij }nj=1 should approximate eX (X(si ; �λX )), but tuning of µ � X is surprisingly absent from current MT practice (with some exceptions, see Denkowski and Lavie i (2010)). In fact, eˆX (X(si ; �λX ), {tij }nj=1 ;µ �"
W15-4904,P10-1064,0,0.170309,"Missing"
W15-4904,2010.amta-papers.27,0,0.0353888,"Missing"
W15-4904,2005.eamt-1.18,0,0.0159116,"called fuzzy match score that accounts for the amount of text that is common to both segments2 — and even marks for you the words in s�i that do not match those in si . Let’s call all this information TM(si ): your job is to use it to turn t�i into the ﬁnal translation ti . If the fuzzy match is good, you will spend less effort than if you started from scratch. Let us call eTM the effort to turn the t�i provided by i TM(si ) into the desired translation ti .3 Mixing them up: You could even have available another technology, fuzzy-match repair (FMR; (Ortega et al., 2014; Dandapat et al., 2011; Hewavitharana et al., 2005; Kranias and Samiotou, 2004)), that integrates the two technologies just mentioned: after a suitable fuzzy match is found, machine translation (or another source of bilingual information) is used to repair, i.e. edit some parts of t�i , to take into account what changes from s�i to si to try to save even more effort; it tells you all that TM(si ) tells you, but also marks the parts that have been repaired. Fuzzy-match repair is one of the technologies that TAUS, the Translation Automation User Society, calls advanced leveraging;4 commercial examples of these are DeepMiner in Atril’s D´ej`a Vu"
W15-4904,J10-4005,0,0.0421896,"d on a development set made of bilingual segments and translation effort measurements eMT (MT(si ; �λMT )).9 X� where ei i is the effort expended in translating segment si using the best technology Xi� for that segment, that is, the one that minimizes that effort. To minimize the translation effort on a speciﬁc task, designers have to work in two main areas: Improving each technology: One is to improve the output of each technology X, ideally focusing on those cases when X is going to be selected. Some such technologies have tunable parameters; for instance, feature weights in statistical MT (Koehn, 2010, p. 255); for other technologies, this is not usually reported, but it is not impossible to think, for instance, of fuzzy-match scores that give different weights to different kinds of edit operations. Let us call �λX the vector of tunable parameters for technology X; as the output of technology X varies with these parameters, we can write its output like this: X(si ; �λX ). Learning to select the best technology: The other one is that the CAT environment needs a way to select the best technology Xi� for each segment si , obviously without measuring the actual effort. To do this, CAT designer"
W15-4904,kranias-samiotou-2004-automatic,0,0.46605,"t accounts for the amount of text that is common to both segments2 — and even marks for you the words in s�i that do not match those in si . Let’s call all this information TM(si ): your job is to use it to turn t�i into the ﬁnal translation ti . If the fuzzy match is good, you will spend less effort than if you started from scratch. Let us call eTM the effort to turn the t�i provided by i TM(si ) into the desired translation ti .3 Mixing them up: You could even have available another technology, fuzzy-match repair (FMR; (Ortega et al., 2014; Dandapat et al., 2011; Hewavitharana et al., 2005; Kranias and Samiotou, 2004)), that integrates the two technologies just mentioned: after a suitable fuzzy match is found, machine translation (or another source of bilingual information) is used to repair, i.e. edit some parts of t�i , to take into account what changes from s�i to si to try to save even more effort; it tells you all that TM(si ) tells you, but also marks the parts that have been repaired. Fuzzy-match repair is one of the technologies that TAUS, the Translation Automation User Society, calls advanced leveraging;4 commercial examples of these are DeepMiner in Atril’s D´ej`a Vu,5 and ALTM in MultiCorpora’s"
W15-4904,P03-1021,0,0.330667,"nologies and for all segments. Therefore it is in principle not easy to determine the parameters θ�X to get good estimates e˜X (X(si ; �λX ); θ�X ). We will see a way to do this below. Tuning technologies is also hard: Technologies may have tunable parameters �λX which determine the output they produce. Obviously, one cannot just repetitively measure the actual effort spent by translators in editing their output for a wide variety of values of �λX , as this is clearly impracticable; therefore, an alternative is needed. When X = MT, this is usually done by means of an algorithm that optimizes (Och, 2003; Chiang, 2012) automatic evaluation measures, such as BLEU (Papineni et al., 2002), using reference translations in a development set. Most of these automatic 9 The measurements of effort that one can ﬁnd in literature vary from simple scores for “perceived” post-editing effort (usually scores taking 3 or 4 values) to actual post-editing time (see, for instance, the quality estimation task in WMT 2014 (Bojar et al., 2014)) measures are measures of similarity (or dissimilarity) between raw and reference translations. Researchers hope that their use during tuning will lead to a reduction in tra"
W15-4904,2014.amta-researchers.4,1,0.842404,"Missing"
W15-4904,P02-1040,0,0.109824,"o determine the parameters θ�X to get good estimates e˜X (X(si ; �λX ); θ�X ). We will see a way to do this below. Tuning technologies is also hard: Technologies may have tunable parameters �λX which determine the output they produce. Obviously, one cannot just repetitively measure the actual effort spent by translators in editing their output for a wide variety of values of �λX , as this is clearly impracticable; therefore, an alternative is needed. When X = MT, this is usually done by means of an algorithm that optimizes (Och, 2003; Chiang, 2012) automatic evaluation measures, such as BLEU (Papineni et al., 2002), using reference translations in a development set. Most of these automatic 9 The measurements of effort that one can ﬁnd in literature vary from simple scores for “perceived” post-editing effort (usually scores taking 3 or 4 values) to actual post-editing time (see, for instance, the quality estimation task in WMT 2014 (Bojar et al., 2014)) measures are measures of similarity (or dissimilarity) between raw and reference translations. Researchers hope that their use during tuning will lead to a reduction in translation effort, although this is not currently guaranteed —for instance, Denkowski"
W15-4904,2013.mtsummit-papers.21,0,0.0142137,"egment si in the development set and µ � X is a set of tunable X i ;µ �X) parameters. Ideally, eˆ (X(si ; �λX ), {tij }nj=1 should approximate eX (X(si ; �λX )), but tuning of µ � X is surprisingly absent from current MT practice (with some exceptions, see Denkowski and Lavie i (2010)). In fact, eˆX (X(si ; �λX ), {tij }nj=1 ;µ � X ) can be seen as a special estimator of effort, much like e˜X (X(si ; �λX ); θ�X ), but informed with reference i translations {tij }nj=1 when they are available. This is similar to the use of pseudo-reference translations in machine translation quality estimation (Shah et al., 2013; Soricut and Narsale, 2012; Soricut et al., 2012; Soricut and Echihabi, 2010), but with actual references. Table 1 summarizes the main concepts and the notation used along the paper. 3 A general framework for training the whole CAT environment We describe a possible workﬂow to tune simultaneously the different technologies that may be used in a CAT environment and the estimators used to select them on a segment basis: 1. Design automatic evaluation measures eˆX (X(si , �λX ), {tij }; µ � X ) and estimators X X X e˜ (X(si ; �λ ); θ� ) for each technology X ∈ X , based on a series of relevant f"
W15-4904,2009.mtsummit-papers.14,0,0.192959,"even better, couldn’t the decision of selecting the best technology Xi� , that is, the one that minimizes your effort for each segment si , be made automatically? It is therefore clear that a framework that allows to seamlessly integrate all the translation technologies available in the CAT system is very much needed to make the most of all of them and minimize translation effort as much as possible. Previous work on technology selection: The speciﬁc case of automatically choosing between machine translation output and translation memory fuzzy matches has received attention in the last years. Simard and Isabelle (2009) proposed a simple approach called β-combination, which simply selects machine translation when there is no translation memory proposal with a fuzzy match score above a given threshold β, which can be tuned. He et al. (2010a) and He et al. (2010b) approach this problem, which they call translation recommendation, by training a classiﬁer which selects which of the two, TM(si ) or MT(si ), gets the lowest value for an approximate indicator of effort, called translation error rate (TER, (Snover et al., 2006)). Their training compares outputs to preexisting reference translations; their ideas are"
W15-4904,2006.amta-papers.25,0,0.102842,"tput and translation memory fuzzy matches has received attention in the last years. Simard and Isabelle (2009) proposed a simple approach called β-combination, which simply selects machine translation when there is no translation memory proposal with a fuzzy match score above a given threshold β, which can be tuned. He et al. (2010a) and He et al. (2010b) approach this problem, which they call translation recommendation, by training a classiﬁer which selects which of the two, TM(si ) or MT(si ), gets the lowest value for an approximate indicator of effort, called translation error rate (TER, (Snover et al., 2006)). Their training compares outputs to preexisting reference translations; their ideas are generalized in the approach proposed in this paper. The next section explains two ways to minimize the effort needed to perform a translation job in a CAT environment integrating different technologies. Section 3 then describes our proposal for a general framework for training the whole CAT environment. Finally, we discuss the implications of having such a framework. 2 Minimizing translation effort to come up with a set of estimators e˜X , one for each technology. These estimators should be trained to giv"
W15-4904,P10-1063,0,0.0247347,"µ �X) parameters. Ideally, eˆ (X(si ; �λX ), {tij }nj=1 should approximate eX (X(si ; �λX )), but tuning of µ � X is surprisingly absent from current MT practice (with some exceptions, see Denkowski and Lavie i (2010)). In fact, eˆX (X(si ; �λX ), {tij }nj=1 ;µ � X ) can be seen as a special estimator of effort, much like e˜X (X(si ; �λX ); θ�X ), but informed with reference i translations {tij }nj=1 when they are available. This is similar to the use of pseudo-reference translations in machine translation quality estimation (Shah et al., 2013; Soricut and Narsale, 2012; Soricut et al., 2012; Soricut and Echihabi, 2010), but with actual references. Table 1 summarizes the main concepts and the notation used along the paper. 3 A general framework for training the whole CAT environment We describe a possible workﬂow to tune simultaneously the different technologies that may be used in a CAT environment and the estimators used to select them on a segment basis: 1. Design automatic evaluation measures eˆX (X(si , �λX ), {tij }; µ � X ) and estimators X X X e˜ (X(si ; �λ ); θ� ) for each technology X ∈ X , based on a series of relevant features that can easily be extracted from si and X(si ), and which will depend"
W15-4904,W12-3121,0,0.0184337,"velopment set and µ � X is a set of tunable X i ;µ �X) parameters. Ideally, eˆ (X(si ; �λX ), {tij }nj=1 should approximate eX (X(si ; �λX )), but tuning of µ � X is surprisingly absent from current MT practice (with some exceptions, see Denkowski and Lavie i (2010)). In fact, eˆX (X(si ; �λX ), {tij }nj=1 ;µ � X ) can be seen as a special estimator of effort, much like e˜X (X(si ; �λX ); θ�X ), but informed with reference i translations {tij }nj=1 when they are available. This is similar to the use of pseudo-reference translations in machine translation quality estimation (Shah et al., 2013; Soricut and Narsale, 2012; Soricut et al., 2012; Soricut and Echihabi, 2010), but with actual references. Table 1 summarizes the main concepts and the notation used along the paper. 3 A general framework for training the whole CAT environment We describe a possible workﬂow to tune simultaneously the different technologies that may be used in a CAT environment and the estimators used to select them on a segment basis: 1. Design automatic evaluation measures eˆX (X(si , �λX ), {tij }; µ � X ) and estimators X X X e˜ (X(si ; �λ ); θ� ) for each technology X ∈ X , based on a series of relevant features that can easily be"
W15-4904,W12-3118,0,0.0163317,"a set of tunable X i ;µ �X) parameters. Ideally, eˆ (X(si ; �λX ), {tij }nj=1 should approximate eX (X(si ; �λX )), but tuning of µ � X is surprisingly absent from current MT practice (with some exceptions, see Denkowski and Lavie i (2010)). In fact, eˆX (X(si ; �λX ), {tij }nj=1 ;µ � X ) can be seen as a special estimator of effort, much like e˜X (X(si ; �λX ); θ�X ), but informed with reference i translations {tij }nj=1 when they are available. This is similar to the use of pseudo-reference translations in machine translation quality estimation (Shah et al., 2013; Soricut and Narsale, 2012; Soricut et al., 2012; Soricut and Echihabi, 2010), but with actual references. Table 1 summarizes the main concepts and the notation used along the paper. 3 A general framework for training the whole CAT environment We describe a possible workﬂow to tune simultaneously the different technologies that may be used in a CAT environment and the estimators used to select them on a segment basis: 1. Design automatic evaluation measures eˆX (X(si , �λX ), {tij }; µ � X ) and estimators X X X e˜ (X(si ; �λ ); θ� ) for each technology X ∈ X , based on a series of relevant features that can easily be extracted from si and"
W15-4904,2013.tc-1.10,0,0.102852,"ach segment, MT(si ); vendors and experts tell you that you will save effort by postediting MT(si ) into your desired translation ti .1 Machine translation output MT(si ) may just be text, but it could come with annotations to help you make the most of it; for instance, words could be color-coded according to how conﬁdent the system is about them (Uefﬁng and Ney, 2007; Uefﬁng and Ney, 2005; Blatz et al., 2004), or unknown words that come out untranslated may be marked so that you spot them clearly. Machine-translated segments could even be accompanied by indicators of their estimated quality (Specia and Soricut, 2013; Specia et al., 2010; Blatz et al., 2004) which may be used to ascertain whether the output of the MT system is worth being post-edited or not. If someone measured your post-editing effort (in time, in number of keystrokes, in number of words changed, in money you would have to pay another translator to do it, etc.), when turning MT(si ) into ti , they could call that effort eMT i . Translation memory: You could also use a translation memory (TM; (Somers, 2003)), where previously translated segments s are stored together with their translations t in pairs called translation units (s, t). The"
W15-4904,H05-1096,0,0.0369622,"rs. This article is licensed under a Creative � Commons 3.0 licence, no derivative works, attribution, CCBY-ND. 27 Machine translation: You could, for instance, use machine translation (MT) to get a draft of the translation of each segment, MT(si ); vendors and experts tell you that you will save effort by postediting MT(si ) into your desired translation ti .1 Machine translation output MT(si ) may just be text, but it could come with annotations to help you make the most of it; for instance, words could be color-coded according to how conﬁdent the system is about them (Uefﬁng and Ney, 2007; Uefﬁng and Ney, 2005; Blatz et al., 2004), or unknown words that come out untranslated may be marked so that you spot them clearly. Machine-translated segments could even be accompanied by indicators of their estimated quality (Specia and Soricut, 2013; Specia et al., 2010; Blatz et al., 2004) which may be used to ascertain whether the output of the MT system is worth being post-edited or not. If someone measured your post-editing effort (in time, in number of keystrokes, in number of words changed, in money you would have to pay another translator to do it, etc.), when turning MT(si ) into ti , they could call t"
W15-4904,J07-1003,0,0.0199256,"they c 2015 The authors. This article is licensed under a Creative � Commons 3.0 licence, no derivative works, attribution, CCBY-ND. 27 Machine translation: You could, for instance, use machine translation (MT) to get a draft of the translation of each segment, MT(si ); vendors and experts tell you that you will save effort by postediting MT(si ) into your desired translation ti .1 Machine translation output MT(si ) may just be text, but it could come with annotations to help you make the most of it; for instance, words could be color-coded according to how conﬁdent the system is about them (Uefﬁng and Ney, 2007; Uefﬁng and Ney, 2005; Blatz et al., 2004), or unknown words that come out untranslated may be marked so that you spot them clearly. Machine-translated segments could even be accompanied by indicators of their estimated quality (Specia and Soricut, 2013; Specia et al., 2010; Blatz et al., 2004) which may be used to ascertain whether the output of the MT system is worth being post-edited or not. If someone measured your post-editing effort (in time, in number of keystrokes, in number of words changed, in money you would have to pay another translator to do it, etc.), when turning MT(si ) into"
W15-4919,J96-1002,0,0.236061,"s actually lexical selection in MT; they used a parser to identify syntactic relations such as subject–object or subject–verb. After generating all the possible translations for a given input sentence using an ambiguous bilingual dictionary, they extract the syntactic tuples from the TL and count the frequency in a previously-trained TL model of tuples. They use maximum-likelihood estimation to calculate the probability that a given 1 145 Such as a morphological or syntactic analyser. TL tuple is the translation of a given SL tuple, with an automatically determined conﬁdence threshold. Later, Berger et al. (1996) illustrated the use of maximum-entropy classiﬁers on the speciﬁc problem of lexical selection in IBM-style word-based statistical MT. Other authors (Melero et al., 2007) have used TL models to rank the translations resulting from all possible combinations of lexical selections. Nowadays, in state-of-the-art phrasebased statistical MT (Koehn, 2010), lexical selection is taken care of by a combination of the translation model and the language model. The translation model provides probabilities of translation between words or word sequences (often referred to as phrases) in the source and target"
W15-4919,J94-4003,0,0.496098,"putational cost. 1 Mikel L. Forcada Dept. Lleng. i Sist. Inform., Universitat d’Alacant, E-03071 Alacant 1.1 Introduction Corpus-based machine translation (MT) has been the primary research direction in the ﬁeld of MT in recent years. However, rule-based MT (RBMT) systems are still being developed, and there are many successful commercial and non-commercial systems. One reason for the continued development of RBMT systems is that in order to be successful, c 2015 The authors. This article is licensed under a Creative � Commons 3.0 licence, no derivative works, attribution, CCBY-ND. Prior work Dagan and Itai (1994) used the term word sense disambiguation to refer to what is actually lexical selection in MT; they used a parser to identify syntactic relations such as subject–object or subject–verb. After generating all the possible translations for a given input sentence using an ambiguous bilingual dictionary, they extract the syntactic tuples from the TL and count the frequency in a previously-trained TL model of tuples. They use maximum-likelihood estimation to calculate the probability that a given 1 145 Such as a morphological or syntactic analyser. TL tuple is the translation of a given SL tuple, wi"
W15-4919,W04-3250,0,0.066516,"tion module returns more than one translation, Apertium will select the default one if marked or the ﬁrst one of not.12 The table in Figure 2 gives an overview of the inputs.In the description it is assumed that the reference translation has been annotated by hand. However, hand annotation is a time-consuming process, and was not possible. A description of how the reference was built is given in Section 3.4. diﬀ(Tr (si ), Tt (si )) = 3.3.2 3.3.3 Conﬁdence intervals Conﬁdence intervals for both metrics will be calculated through bootstrap resampling (Efron and Tibshirani, 1994) as described by Koehn (2004). In all cases, bootstrap resampling will be carried out for 1,000 iterations. Where the p = 0.05 conﬁdence intervals overlap, we will also perform paired bootstrap resampling (Koehn, 2004). 3.4 For creating the test corpora, providing a SL corpus for training, and a TL corpus for scoring, we used four parallel corpora: • Oﬁs ar Brezhoneg (OAB): This parallel corpus of Breton and French has been collected speciﬁcally for lexical-selection experiments from translations produced by Oﬁs ar Brezhoneg ‘The Ofﬁce of the Breton language’. The corpus has recently been made available online through OPU"
W15-4919,2005.mtsummit-papers.11,0,0.0323006,"Oﬁs ar Brezhoneg ‘The Ofﬁce of the Breton language’. The corpus has recently been made available online through OPUS.13 • South-East European Times (SETimes): Described in Tyers and Alperen (2010), this corpus is a multilingual corpus of the Balkan languages (and English) in the news domain. The Macedonian and English part will be used. • Open Data Euskadi (OpenData): This is a Basque–Spanish parallel corpus made from the translation memories of the Herri Arduralaritzaren Euskal Erakundea ‘Basque Institute of Public Administration’.14 • European Parliament Proceedings (EuroParl): Described by Koehn (2005), this is a multilingual corpus of the European Union ofﬁcial languages. We are using the English–Spanish data from version 7.15 Machine translation performance This is an extrinsic evaluation, which ideally would test how much the system improves as regards an approximate measurement of ﬁnal translation quality in a real system. For this task, we use the widely-used BLEU metric (Papineni et al., 2002). This is not ideal for evaluating the task of a lexical selection module as the performance of the module will depend greatly on (a) the coverage of the bilingual dictionaries of the RBMT system"
W15-4919,J10-4005,0,0.0125822,"ey use maximum-likelihood estimation to calculate the probability that a given 1 145 Such as a morphological or syntactic analyser. TL tuple is the translation of a given SL tuple, with an automatically determined conﬁdence threshold. Later, Berger et al. (1996) illustrated the use of maximum-entropy classiﬁers on the speciﬁc problem of lexical selection in IBM-style word-based statistical MT. Other authors (Melero et al., 2007) have used TL models to rank the translations resulting from all possible combinations of lexical selections. Nowadays, in state-of-the-art phrasebased statistical MT (Koehn, 2010), lexical selection is taken care of by a combination of the translation model and the language model. The translation model provides probabilities of translation between words or word sequences (often referred to as phrases) in the source and target language. The TL model provides probabilities of word sequences in the TL. Mareˇcek et al. (2010) trained a maximum-entropy lexical selector for their dependency-grammar-based transfer system TectoMT using a bilingual corpus. More recently, Tyers et al. (2012) presented a method of lexical selection for RBMT based on rules which select or remove t"
W15-4919,P07-2045,0,0.0115657,"Missing"
W15-4919,W10-1730,0,0.0432849,"Missing"
W15-4919,P14-2123,0,0.0130906,"di follows arrain    0 otherwise (1) This feature considers a context of zero words to the left of the problem word and one word (+ handi) to the right of it. As a result of training, each of the nF features hsk (t, c) in the classiﬁer is assigned a weight λsk . Combining these weights of active features as in equation (2) yields the probability of a translation t for word s in context c. n ps (t|c) = (2) k=1 In this equation, Z s (c) is a normalising constant. Thus, the most probable translation t� can be found using t� = arg max ps (t|c) = arg max 2 The work by Ravi and Knight (2011) and Nuhn and Ney (2014), who decipher word-ciphered text using monolingual corpora only may be seen as a generalised version of the problem of lexical selection without parallel corpora. F � 1 λsk hsk (t, c) exp Z s (c) t∈Ts (s) 3 146 nF � t∈Ts (s) k=1 We follow the notation of Berger et al. (1996) λsk hsk (t, c), (3) S→ preposti=|G| → ({gi }i=1 , S) → lexsel → (g � , S) → → τ (g � , S) lexsel lexsel Figure 1: A schema of the lexical selection process: source sentence S has |G |lexical selection paths gi : lexsel selects one of them g � , which is used to generate translation τ (g � , S). where Ts (s) is the set of"
W15-4919,J03-1002,0,0.007507,"hough for all pairs there is a parallel corpus available for evaluation (see Section 3.3).5 Breton–French (Tyers, 2010): Bilingual dictionaries were not built with polysemy in mind from the outset, but some entries were added later to start work on lexical selection.6 Macedonian–English: The Macedonian–English pair in Apertium was created speciﬁcally for the purposes of running lexical-selection experiments. The lexical resources for the pair were tuned to the SETimes parallel corpus (Tyers and Alperen, 2010). The most probable entry from automatic word alignment of this corpus using GIZA ++ (Och and Ney, 2003) was checked to ensure that it was an adequate translation, and if so marked as the default.7 As a result of attempting to include all possible translations, the average number of translations per word is much higher than in other pairs.8 Basque–Spanish (Ginest´ı-Rosell et al., 2009): alternative translations were included in the bilingual dictionary.9 English–Spanish: The English–Spanish pair was developed from a combination of the English– Catalan and Spanish–Catalan pairs, and contains a number of entries in the bilingual dictionary with more than one translation.10 3.3 Performance measures"
W15-4919,P02-1040,0,0.0984287,"Missing"
W15-4919,P11-1002,0,0.0208438,"rrain  +handi (t, c) = handi follows arrain    0 otherwise (1) This feature considers a context of zero words to the left of the problem word and one word (+ handi) to the right of it. As a result of training, each of the nF features hsk (t, c) in the classiﬁer is assigned a weight λsk . Combining these weights of active features as in equation (2) yields the probability of a translation t for word s in context c. n ps (t|c) = (2) k=1 In this equation, Z s (c) is a normalising constant. Thus, the most probable translation t� can be found using t� = arg max ps (t|c) = arg max 2 The work by Ravi and Knight (2011) and Nuhn and Ney (2014), who decipher word-ciphered text using monolingual corpora only may be seen as a generalised version of the problem of lexical selection without parallel corpora. F � 1 λsk hsk (t, c) exp Z s (c) t∈Ts (s) 3 146 nF � t∈Ts (s) k=1 We follow the notation of Berger et al. (1996) λsk hsk (t, c), (3) S→ preposti=|G| → ({gi }i=1 , S) → lexsel → (g � , S) → → τ (g � , S) lexsel lexsel Figure 1: A schema of the lexical selection process: source sentence S has |G |lexical selection paths gi : lexsel selects one of them g � , which is used to generate translation τ (g � , S). whe"
W15-4919,2010.eamt-1.13,1,0.828127,"uild RBMT systems. Translation is implemented as a pipeline consisting of the following modules: morphological analysis, morphological disambiguation, lexical transfer, lexical selection, structural transfer and morphological generation. 3.2 Language pairs Evaluation will be performed using four Apertium (Forcada et al., 2011) language pairs. These pairs have been selected as they include languages with different morphological complexity, and different amounts of resources available — although for all pairs there is a parallel corpus available for evaluation (see Section 3.3).5 Breton–French (Tyers, 2010): Bilingual dictionaries were not built with polysemy in mind from the outset, but some entries were added later to start work on lexical selection.6 Macedonian–English: The Macedonian–English pair in Apertium was created speciﬁcally for the purposes of running lexical-selection experiments. The lexical resources for the pair were tuned to the SETimes parallel corpus (Tyers and Alperen, 2010). The most probable entry from automatic word alignment of this corpus using GIZA ++ (Och and Ney, 2003) was checked to ensure that it was an adequate translation, and if so marked as the default.7 As a re"
W15-4919,2012.eamt-1.54,1,0.760313,"Missing"
W15-4919,H05-1097,0,0.0938983,"Missing"
W16-2383,C04-1046,0,0.163519,"on have been used: machine translation (Lucy LT KWIK Translator and Google Translate) and the bilingual concordancer Reverso Context. Building upon the word-level approach implemented for WMT 2015, a method for phrase-based MTQE is proposed which builds on the probabilities obtained for word-level MTQE. For each sub-task we have submitted two systems: one using the features produced exclusively based on online sources of bilingual information, and one combining them with the baseline features provided by the organisers of the task. 1 Introduction Machine translation quality estimation (MTQE) (Blatz et al., 2004; Specia et al., 2010; Specia and Soricut, 2013) has aroused the interest of both the scientific community and translation companies on account of its noticeable advantages: it can be used to help professional translators in post-editing, to estimate the translation productivity for different translation technologies, or even for budgeting translation projects. In this context, the WMT 2016 MTQE shared task becomes one of the best scenarios in which different approaches to MTQE can be evaluated and compared for different granularities: segment-level (sub-task 1), phrase-level and word-level (s"
W16-2383,W15-3036,1,0.568476,"Missing"
W16-2383,2013.tc-1.10,0,0.0306421,"ucy LT KWIK Translator and Google Translate) and the bilingual concordancer Reverso Context. Building upon the word-level approach implemented for WMT 2015, a method for phrase-based MTQE is proposed which builds on the probabilities obtained for word-level MTQE. For each sub-task we have submitted two systems: one using the features produced exclusively based on online sources of bilingual information, and one combining them with the baseline features provided by the organisers of the task. 1 Introduction Machine translation quality estimation (MTQE) (Blatz et al., 2004; Specia et al., 2010; Specia and Soricut, 2013) has aroused the interest of both the scientific community and translation companies on account of its noticeable advantages: it can be used to help professional translators in post-editing, to estimate the translation productivity for different translation technologies, or even for budgeting translation projects. In this context, the WMT 2016 MTQE shared task becomes one of the best scenarios in which different approaches to MTQE can be evaluated and compared for different granularities: segment-level (sub-task 1), phrase-level and word-level (sub-task 2), and document-level (sub-task 3). For"
W18-6464,W13-2242,0,0.328437,"Missing"
W18-6464,W17-4760,0,0.030293,"ghlight four groups of contributions: 801 • To estimate the sentence-level quality of MT output for a source segment S, Bic¸ici (2013) chooses sentence pairs from a parallel corpus which are close to S, and builds an SMT system whose internals when translating S are examined to extract features. • MULTILIZER, one of the participants in the sentence-level MT QE task at WMT 2014 (Bojar et al., 2014) uses other MT systems to translate S into the target language (TL) and T into the source language (SL). The results are compared to the original SL and TL segments to obtain indicators of quality. • Blain et al. (2017) use bilexical embeddings (obtained from SL and TL word embeddings Proceedings of the Third Conference on Machine Translation (WMT), Volume 2: Shared Task Papers, pages 801–808 c Belgium, Brussels, October 31 - Novermber 1, 2018. 2018 Association for Computational Linguistics https://doi.org/10.18653/v1/W18-64091 and word-aligned parallel corpora) to model the strength of the relationship between SL and TL words, in order to estimate sentencelevel and word-level MT quality. • Finally, Espl`a-Gomis et al. (2015a,b), and Espl`a-Gomis et al. (2016) perform word-level MT QE by using other MT syste"
W18-6464,W15-4903,1,0.868199,"Missing"
W18-6464,C04-1046,0,0.0918984,"o estimate quality at the sentence level. The paper is organized as follows: section 2 briefly reviews previous work on word-level MT QE; section 3 describes the method used to label words and gaps, paying special attention to the features extracted (sections 3.1 and 3.2) and the neural network (NN) architecture and its training (section 3.3); section 4 describes the datasets used; section 5 shows the main results; and, finally, section 6 closes the paper with concluding remarks. Related work Pioneering work on word-level MT QE dealt with predictive/interactive MT (Gandrabur and Foster, 2003; Blatz et al., 2004; Ueffing and Ney, 2005, 2007), often under the name of confidence estimation. Estimations relied on the internals of the actual MT system —for instance, studying the nbest translations (Ueffing and Ney, 2007)— or used external sources of bilingual information; for instance, both Blatz et al. (2004) and Ueffing and Ney (2005) used probabilistic dictionaries; in the case of Blatz et al. (2004), as one of many features in a binary classifier for each word. The last decade has witnessed an explosion of work in word-level MT QE, with most of the recent advances made by participants in the shared t"
W18-6464,W16-2383,1,0.89035,"Missing"
W18-6464,W03-0413,0,0.0996183,"redicted at the word level to estimate quality at the sentence level. The paper is organized as follows: section 2 briefly reviews previous work on word-level MT QE; section 3 describes the method used to label words and gaps, paying special attention to the features extracted (sections 3.1 and 3.2) and the neural network (NN) architecture and its training (section 3.3); section 4 describes the datasets used; section 5 shows the main results; and, finally, section 6 closes the paper with concluding remarks. Related work Pioneering work on word-level MT QE dealt with predictive/interactive MT (Gandrabur and Foster, 2003; Blatz et al., 2004; Ueffing and Ney, 2005, 2007), often under the name of confidence estimation. Estimations relied on the internals of the actual MT system —for instance, studying the nbest translations (Ueffing and Ney, 2007)— or used external sources of bilingual information; for instance, both Blatz et al. (2004) and Ueffing and Ney (2005) used probabilistic dictionaries; in the case of Blatz et al. (2004), as one of many features in a binary classifier for each word. The last decade has witnessed an explosion of work in word-level MT QE, with most of the recent advances made by particip"
W18-6464,W15-3036,1,0.889532,"Missing"
W18-6464,W17-4763,0,0.0602735,"d (wordaligned) SL words into feature vectors — extended with the baseline features provided by WMT15 (Bojar et al., 2015) organizers— to perform word-level MT QE. • Martins et al. (2016) achieved the best results in the word-level MT QE shared task at WMT 2016 (Bojar et al., 2016) by combining a feedforward NN with two recurrent NNs whose predictions were fed into a linear sequential model together with the baseline features provided by the organizers of the task. An extension (Martins et al., 2017) uses the output of an automatic post-editing tool, with a clear improvement in performance. • Kim et al. (2017a,b) obtained in WMT 2017 (Bojar et al., 2017) results which were better or comparable to those by Martins et al. (2017), using a three-level stacked architecture trained in a multi-task fashion, combining a neural word prediction model trained on large-scale parallel corpora, and word- and sentence-level MT QE models. Our approach uses a much simpler architecture than the last two approaches, containing no recurrent NNs, but just feed-forward NNs applied to a fixedlength context window around the word or gap about which a decision is being made (similarly to a convolutional approach). This ma"
W18-6464,W15-3037,0,0.151616,"l MT quality. • Finally, Espl`a-Gomis et al. (2015a,b), and Espl`a-Gomis et al. (2016) perform word-level MT QE by using other MT systems to translate sub-segments of S and T and extracting features describing the way in which these translated sub-segments match sub-segments of T . This is the work most related to the one presented in this paper. Only the last two groups of work actually tackle the problem of word-level MT QE, and none of them are able to identify the gaps where insertions are needed. As regards the use of neural networks (NN) in MT QE, we can highlight a few contributions: • Kreutzer et al. (2015) use a deep feed-forward NN to process the concatenated vector embeddings of neighbouring TL words and (wordaligned) SL words into feature vectors — extended with the baseline features provided by WMT15 (Bojar et al., 2015) organizers— to perform word-level MT QE. • Martins et al. (2016) achieved the best results in the word-level MT QE shared task at WMT 2016 (Bojar et al., 2016) by combining a feedforward NN with two recurrent NNs whose predictions were fed into a linear sequential model together with the baseline features provided by the organizers of the task. An extension (Martins et al.,"
W18-6464,W16-2387,0,0.0826382,"s the work most related to the one presented in this paper. Only the last two groups of work actually tackle the problem of word-level MT QE, and none of them are able to identify the gaps where insertions are needed. As regards the use of neural networks (NN) in MT QE, we can highlight a few contributions: • Kreutzer et al. (2015) use a deep feed-forward NN to process the concatenated vector embeddings of neighbouring TL words and (wordaligned) SL words into feature vectors — extended with the baseline features provided by WMT15 (Bojar et al., 2015) organizers— to perform word-level MT QE. • Martins et al. (2016) achieved the best results in the word-level MT QE shared task at WMT 2016 (Bojar et al., 2016) by combining a feedforward NN with two recurrent NNs whose predictions were fed into a linear sequential model together with the baseline features provided by the organizers of the task. An extension (Martins et al., 2017) uses the output of an automatic post-editing tool, with a clear improvement in performance. • Kim et al. (2017a,b) obtained in WMT 2017 (Bojar et al., 2017) results which were better or comparable to those by Martins et al. (2017), using a three-level stacked architecture trained"
W18-6464,Q17-1015,0,0.0142485,"r et al. (2015) use a deep feed-forward NN to process the concatenated vector embeddings of neighbouring TL words and (wordaligned) SL words into feature vectors — extended with the baseline features provided by WMT15 (Bojar et al., 2015) organizers— to perform word-level MT QE. • Martins et al. (2016) achieved the best results in the word-level MT QE shared task at WMT 2016 (Bojar et al., 2016) by combining a feedforward NN with two recurrent NNs whose predictions were fed into a linear sequential model together with the baseline features provided by the organizers of the task. An extension (Martins et al., 2017) uses the output of an automatic post-editing tool, with a clear improvement in performance. • Kim et al. (2017a,b) obtained in WMT 2017 (Bojar et al., 2017) results which were better or comparable to those by Martins et al. (2017), using a three-level stacked architecture trained in a multi-task fashion, combining a neural word prediction model trained on large-scale parallel corpora, and word- and sentence-level MT QE models. Our approach uses a much simpler architecture than the last two approaches, containing no recurrent NNs, but just feed-forward NNs applied to a fixedlength context wind"
W18-6464,2005.eamt-1.35,0,0.0493499,"t the sentence level. The paper is organized as follows: section 2 briefly reviews previous work on word-level MT QE; section 3 describes the method used to label words and gaps, paying special attention to the features extracted (sections 3.1 and 3.2) and the neural network (NN) architecture and its training (section 3.3); section 4 describes the datasets used; section 5 shows the main results; and, finally, section 6 closes the paper with concluding remarks. Related work Pioneering work on word-level MT QE dealt with predictive/interactive MT (Gandrabur and Foster, 2003; Blatz et al., 2004; Ueffing and Ney, 2005, 2007), often under the name of confidence estimation. Estimations relied on the internals of the actual MT system —for instance, studying the nbest translations (Ueffing and Ney, 2007)— or used external sources of bilingual information; for instance, both Blatz et al. (2004) and Ueffing and Ney (2005) used probabilistic dictionaries; in the case of Blatz et al. (2004), as one of many features in a binary classifier for each word. The last decade has witnessed an explosion of work in word-level MT QE, with most of the recent advances made by participants in the shared tasks on MT QE at the di"
W18-6464,J07-1003,0,0.0388073,"special attention to the features extracted (sections 3.1 and 3.2) and the neural network (NN) architecture and its training (section 3.3); section 4 describes the datasets used; section 5 shows the main results; and, finally, section 6 closes the paper with concluding remarks. Related work Pioneering work on word-level MT QE dealt with predictive/interactive MT (Gandrabur and Foster, 2003; Blatz et al., 2004; Ueffing and Ney, 2005, 2007), often under the name of confidence estimation. Estimations relied on the internals of the actual MT system —for instance, studying the nbest translations (Ueffing and Ney, 2007)— or used external sources of bilingual information; for instance, both Blatz et al. (2004) and Ueffing and Ney (2005) used probabilistic dictionaries; in the case of Blatz et al. (2004), as one of many features in a binary classifier for each word. The last decade has witnessed an explosion of work in word-level MT QE, with most of the recent advances made by participants in the shared tasks on MT QE at the different editions of the Conference on Statistical Machine Translation (WMT). Therefore, we briefly review those papers related to our approach: those using an external bilingual source s"
W18-6464,2015.eamt-1.4,1,\N,Missing
W19-5339,W17-4755,0,0.0441326,"Missing"
W19-5339,P17-2061,0,0.0114301,"iterations. The objective of the 3rd iteration, in which only English-to-Kazakh systems were trained, was building the submissions, and the corresponding details are described in Section 6. We explored different ways of training NMT systems with backtranslated data. First, we carried out transfer learning from the multilingual models described in Section 3.1. In this case, the child model was trained on a parallel corpus built from the concatenation of the genuine parallel data and the backtranslated data. The genuine parallel data was oversampled to match the size of the backtranslated data (Chu et al., 2017). As an alternative to transfer learning, we experimented with corpus concatenation and finetuning. For the English-to-Kazakh direction, we concatenated the backtranslated data to the pivotbacktranslated corpus and the genuine parallel corpora, trained a model from scratch, and fine-tuned it only on the genuine parallel data. For the opposite direction, we trained a system only on the concatenation of the backtranslated and the genuine parallel data, and fine-tuned it on the latter (note that in this set-up we dispensed with parallel data from other language pairs). Table 4 shows the automatic"
W19-5339,W11-2123,0,0.0158863,"le 1 for other language pairs, we applied the transfer learning approach proposed by Kocmi and Bojar (2018). We experimented with the parent models listed next (models trained on other high-resource language pairs) and used the concatenation of the genuine English–Kazakh parallel data as the child corpus (corpus of a lowresource language pair used to continue training a parent model):6 Wikititles parallel corpus and extracted the positive and negative training examples from News Commentary. We kept those sentences with a classifier score above 0.6. 2 The language model was trained with KenLM (Heafield, 2011) with modified Kneser-Ney smoothing (Ney et al., 1994). 3 https://github.com/marian-nmt/ marian-examples/tree/master/ wmt2017-transformer 4 https://github.com/marian-nmt/ marian-examples/tree/master/ training-basics 357 • A Russian-to-Kazakh model trained on the crawled parallel corpus depicted in Table 1. • An English-to-Russian model trained on all the available parallel data for the English– Russian language pair in this year’s news translation task (depicted in Table 1). • A multilingual system (Johnson et al., 2017) trained on the concatenation of the corpora of the two previous models. T"
W19-5339,W18-2703,0,0.0340878,"learning brings a clear improvement over training only on the genuine parallel data, and the best parent model is the multilingual one. 3.2 Monolingual data: iterative backtranslation Backtranslation (Sennrich et al., 2016b) is a widespread method for integrating TL monolingual corpora into NMT systems. In order to integrate the available Kazakh monolingual data into our submission, we need a Kazakh-to-English MT system as competitive as possible, since the quality of a system trained on backtranslated data is usually correlated with the quality of the system that perform the backtranslation (Hoang et al., 2018, Sec. 3). We followed the iterative backtranslation algorithm (Hoang et al., 2018) outlined below with the aim of obtaining strong English-to-Kazakh and Kazakh-to-English systems using monolingual English and monolingual Kazakh corpora: 1. The best strategies from Section 3.1 were applied to build systems in both directions without backtranslated monolingual data. 2. English and Kazakh monolingual data were backtranslated with the previous systems. 3. Systems in both directions were trained on the combination of the backtranslated data and the parallel data. 4. Steps 2–3 were re-executed 2 mo"
W19-5339,2012.amta-papers.8,0,0.0440778,"ransformer 4 https://github.com/marian-nmt/ marian-examples/tree/master/ training-basics 357 • A Russian-to-Kazakh model trained on the crawled parallel corpus depicted in Table 1. • An English-to-Russian model trained on all the available parallel data for the English– Russian language pair in this year’s news translation task (depicted in Table 1). • A multilingual system (Johnson et al., 2017) trained on the concatenation of the corpora of the two previous models. This strategy aims at making the most of the data available for related language pairs. We also explored pivot backtranslation (Huck and Ney, 2012): we translated the Russian side of the crawled Kazakh–Russian parallel corpus with 5 Following Popovi´c (2017), we set β to 2. In the 3 set-ups evaluated, BPE models were trained from the concatenation of the parent and the child corpora. 6 a Russian-to-English NMT system to produce a synthetic English–Kazakh parallel corpus. The NMT system was a Transformer trained on the English–Russian parallel data depicted in Table 1. We concatenated the pivot-backtranslated corpus to the genuine English–Kazakh parallel data and fine-tuned the resulting system only on the latter. The results of the evalu"
W19-5339,W17-4706,0,0.0615079,"News Crawl. Finally, the repetition of steps 2–3 helped to further improve translation quality. 4 Morphological segmentation Morphological segmentation is a strategy for segmeting words into sub-word units that consists in splitting them into a stem, that carries out the meaning of the word, and a suffix or sequence of suffixes that contain morphological and syntatic information. When that strategy has been followed to segment the training corpus for an NMT system, it has been reported to outperform BPE for highly inflected languages such as Finnish (SánchezCartagena and Toral, 2016), German (Huck et al., 2017) or Basque (Sánchez-Cartagena, 2018). In our submissions, we morphologically segmented the Kazakh text with the Apertium Kazakh morphological analyzer.7 For each word, the analyzer provides a set of candidate analyses made of a lemma and morphological information. Those analyses in which the lemma is a prefix of the word are considered valid analyses for segmentation and involve that the word can be morphologically segmented into the lemma and the remainder of the word.8 When there are multiple valid analyses for a word, they are disambiguated as explained below. When a word has no valid analy"
W19-5339,W18-6414,0,0.0344641,"Missing"
W19-5339,W18-6467,0,0.112053,"Missing"
W19-5339,P18-4020,0,0.0366647,"Missing"
W19-5339,W18-6325,0,0.0489303,", 2017), as is the case. Where reported, we assess whether differences between systems’ outputs are statistically significant for p < 0.05 with 1 000 iterations of paired bootstrap resampling (Koehn, 2004). 3 Data augmentation This section describes the process followed to select the best strategy to take advantage of parallel corpora from other language pairs (Section 3.1) and monolingual corpora (Section 3.2). 3.1 Data from other language pairs In order to take advantage of the parallel corpora listed in Table 1 for other language pairs, we applied the transfer learning approach proposed by Kocmi and Bojar (2018). We experimented with the parent models listed next (models trained on other high-resource language pairs) and used the concatenation of the genuine English–Kazakh parallel data as the child corpus (corpus of a lowresource language pair used to continue training a parent model):6 Wikititles parallel corpus and extracted the positive and negative training examples from News Commentary. We kept those sentences with a classifier score above 0.6. 2 The language model was trained with KenLM (Heafield, 2011) with modified Kneser-Ney smoothing (Ney et al., 1994). 3 https://github.com/marian-nmt/ mar"
W19-5339,W04-3250,0,0.350722,"Missing"
W19-5339,P07-2045,0,0.0121386,"2 describes how corpora were filtered and preprocessed, and the steps followed to train NMT systems from them. Section 3 outlines the process followed to obtain synthetic training data. Sections 4 and 5 describe respectively morphological segmentation and hybridization with Apertium. The model ensembles we submitted are then presented in Section 6. The paper ends with some concluding remarks. 2 Data preparation and training details In our submissions, we only used the corpora allowed in the constrained task. Parallel corpora were cleaned with the script clean-corpus-n.perl shipped with Moses (Koehn et al., 2007), that removes unbalanced sentence pairs and those with at least one side longer than 80 tokens. Additional filtering steps, described below, were applied to the web crawled corpora. Tables 1 and 2 depict the number of segments in the parallel and monolingual corpora used, and their sizes after cleaning. The English–Kazakh web crawled corpus allowed in the constrained task presented a high proportion of parallel segments that were not translation of each other. We filtered it with Bicleaner (Sánchez-Cartagena et al., 2018). We applied the hardrules and the detection of misaligned sentences des"
W19-5339,W18-6326,0,0.0318805,"Missing"
W19-5339,P02-1040,0,0.103762,"Missing"
W19-5339,W17-4770,0,0.0296756,"Missing"
W19-5339,W18-6488,1,0.888579,"Missing"
W19-5339,W16-2322,1,0.896761,"Missing"
W19-5339,P16-1009,0,0.175488,"us. In addition, inspired by Iranzo-Sánchez et al. (2018), we ranked its sentences by perplexity computed by a character-based 7-gram language model and discarded the half of the corpus with the highest perplexity. The language model was trained2 on the high-quality Kazakh monolingual News Commentary corpus. Training corpora were tokenized and truecased with the Moses scripts. Truecaser models were learned independently for each trained system from the very same training parallel corpus. Unless otherwise specified, for each trained system, words were split with 50 000 byte pair encoding (BPE; Sennrich et al., 2016c) operations learned from the concatenation of the source-language (SL) and target-language (TL) training corpora. As described in Section 6, our submissions were ensembles of Transformer (Vaswani et al., 2017) and recurrent neural network (RNN; Bahdanau et al., 2015) NMT models trained with the Marian toolkit (Junczys-Dowmunt et al., 2018). We used the Transformer hyperparameters3 described by Sennrich et al. (2017) and the RNN hyperparameters4 described by Sennrich et al. (2016a). Early stopping was based on perplexity and patience was set to 5. We selected the checkpoint that obtained the"
W19-5339,P16-1162,0,0.411139,"us. In addition, inspired by Iranzo-Sánchez et al. (2018), we ranked its sentences by perplexity computed by a character-based 7-gram language model and discarded the half of the corpus with the highest perplexity. The language model was trained2 on the high-quality Kazakh monolingual News Commentary corpus. Training corpora were tokenized and truecased with the Moses scripts. Truecaser models were learned independently for each trained system from the very same training parallel corpus. Unless otherwise specified, for each trained system, words were split with 50 000 byte pair encoding (BPE; Sennrich et al., 2016c) operations learned from the concatenation of the source-language (SL) and target-language (TL) training corpora. As described in Section 6, our submissions were ensembles of Transformer (Vaswani et al., 2017) and recurrent neural network (RNN; Bahdanau et al., 2015) NMT models trained with the Marian toolkit (Junczys-Dowmunt et al., 2018). We used the Transformer hyperparameters3 described by Sennrich et al. (2017) and the RNN hyperparameters4 described by Sennrich et al. (2016a). Early stopping was based on perplexity and patience was set to 5. We selected the checkpoint that obtained the"
W19-5339,N16-1004,0,0.0421554,"aluated for integrating the Apertium English-toKazakh rule-based machine translation system into an NMT system. Scores of hybrid systems are shown in bold if they outperform the corresponding pure NMT system by a statistically significant margin. 5 Hybridization with rule-based machine translation The Apertium platform contains an English-toKazakh RBMT system (Sundetova et al., 2015) that may encode knowledge that is not present in the corpora available in the constrained task. In order to take advantage of that knowledge, we built a hybrid system by means of multi-source machine translation (Zoph and Knight, 2016). Our hybrid system is a multi-source NMT system with two inputs: the English sentence to be translated, and its translation into Kazakh provided by Apertium. This very same set-up has been successfully followed in the WMT automated post-editing task (JunczysDowmunt and Grundkiewicz, 2018). In order to assess the viability of this approach, we trained and automatically evaluated multisource and single-source English-to-Kazakh systems on the concatenation of the genuine English– Kazakh parallel corpora and the backtranslation of the Kazakh monolingual corpora News Crawl and Wiki dumps.9 Results"
W19-6625,2011.mtsummit-papers.35,0,0.110811,"Missing"
W19-6625,P15-2026,1,0.852334,"g from the seminal work by (Simard et al., 2007), the problem has been tackled as a “monolingual translation” task in which the MT output must be translated into an improved text in the target language. Under this definition, the “parallel data” used for training an APE system consist of triplets of the form (source, target, post-edited target) rather than the (source, target) pairs normally used in MT. Following the translation-based approach, initial solutions relied on the phrase-based paradigm (Simard et al., 2007; Dugast et al., 2007; Terumasa, 2007; Pilevar, 2011; B´echara et al., 2011; Chatterjee et al., 2015; Chatterjee et al., 2016). However, in the past couple of years, top results have been achieved by neural architectures (Pal et al., 2016; Junczys-Dowmunt and Grundkiewicz, 2016; Chatterjee et al., 2017; Junczys-Dowmunt and Grundkiewicz, 2017; Tebbifakhr et al., 2018). Recent advancements made by participants in the APE shared task organized within the Conference on Machine Translation (WMT) have shown the capability of APE systems to significantly improve the performance of a black-box MT system gaining up to seven BLEU points (Bojar et al., 2017; Chatterjee et al., 2018a). The neural approa"
W19-6625,W17-4773,1,0.870702,"ques (Isabel, 2017). In many cases, the state-of-the-art techniques are applied to improve translation proposals from a translation memory (TM) or directly produced by a mac 2019 The authors. This article is licensed under a Creative Commons 4.0 licence, no derivative works, attribution, CCBY-ND. Proceedings of MT Summit XVII, volume 1 chine translation (MT) system. Post-editing techniques can be automated and seamlessly integrated into the typical translation pipeline for productivity gains. Two such techniques: fuzzy-match repair (FMR) (Ortega et al., 2016) and automatic post-editing (APE) (Chatterjee et al., 2017) have shown to be effective without the initial intervention of the translator by offering a repaired translation proposal from a TM in the case of FMR, and an improved MT output in the case of APE. FMR is an automatic post-editing technique typically used with TM-based computer-aided translation (CAT) tools. In TM-based CAT, the translator is offered a translation proposal that comes from a translation unit (a pair of parallel segments) whose source segment is similar to the segment to be translated. When the source segment in the translation unit and the segment to be translated are not iden"
W19-6625,W18-1804,1,0.873753,"echara et al., 2011; Chatterjee et al., 2015; Chatterjee et al., 2016). However, in the past couple of years, top results have been achieved by neural architectures (Pal et al., 2016; Junczys-Dowmunt and Grundkiewicz, 2016; Chatterjee et al., 2017; Junczys-Dowmunt and Grundkiewicz, 2017; Tebbifakhr et al., 2018). Recent advancements made by participants in the APE shared task organized within the Conference on Machine Translation (WMT) have shown the capability of APE systems to significantly improve the performance of a black-box MT system gaining up to seven BLEU points (Bojar et al., 2017; Chatterjee et al., 2018a). The neural approaches proposed share common traits such as using multi encoders (one for the source and one for the MT segments) and leveraging artificial data (round-trip translations) to maximize results. The APE system used in this paper proposes a novel approach extending the original technology implemented by the best performing system at the WMT 2016 APE shared task (Chatterjee et al., 2017). 2.3 Combination of approaches We briefly describe a few combinations of approaches and systems that are usually used in different scenarios, such as FMR and APE, and that could be considered nov"
W19-6625,2011.eamt-1.28,0,0.0241052,"se should be postedited by the translator. The idea of FMR points back to papers by Kranias and Samiotou (2004) and Hewavitharana et al. (2005) whose approaches were based on the location of anchor points via alignment of words and relied heavily on the inner workings of the MT system they used. Improvements over time led way to advances that used phrase-based MT (Simard and Isabelle, 2009; Koehn and Senellart, 2010). Work has gradually advanced and various FMR methods have been proposed that share one common theme: locating and repairing sub-segments in the translation proposal. Later works (Dandapat et al., 2011; Ortega et al., 2016), on the other hand, can use any MT system as a black-box. Knowles et al. (2018) recently performed a comparison of the nature of MT systems for their use in FMR. In particular, they contrast the quality of FMR output using neural MT and phrase-based MT. Most importantly, they show that neural MT may not be appropriate if it is not trained on indomain data. Other novel works, like the work by Bult´e et al. (2018), include FMR as a primary part of a system integrating MT and TM. Lastly, Ortega et al. (2018) have found a statistical way to select the best MT system to use i"
W19-6625,2015.iwslt-evaluation.11,0,0.0142639,"els are optimized using Adagrad (Duchi et al., 2011) and every 10,000 mini-batches they are evaluated with BLEU on the 500-sentence-pairs development set. 4.3 APE settings The APE system is trained on the eSCAPE corpus (Negri et al., 2018), a collection of ∼7M triplets (source, MT output and reference), where the MT outputs have been created by a phrasebased MT system. It consists of datasets belonging to different domains and it is filtered by removing duplicates and too short (3 words) or too long (60 words) segments. To adapt the generic APE system to the FMR task, the model is fine-tuned (Luong and Manning, 2015) on 2,500 triplets (see Section 4.1), where the source input is paired with the repaired translation proposal produced by FMR. Similar to the neural MT system, the APE system is trained on sub-word units by using BPE. The APE vocabulary is created by selecting 50k most frequent sub-words. Word embedding and GRU hidden state size is set to 1024. Network parameters are optimized with Adagrad with a learning rate of 0.01. Source and target dropout is set to 6 All available at opus.lingfil.uu.se. www.statmt.org/wmt13/ training-parallel-commoncrawl.tgz 8 www.statmt.org/europarl/ 7 Dublin, Aug. 19-2"
W19-6625,W07-0732,0,0.0399822,"urring errors from an MT system by learning from human corrections. Starting from the seminal work by (Simard et al., 2007), the problem has been tackled as a “monolingual translation” task in which the MT output must be translated into an improved text in the target language. Under this definition, the “parallel data” used for training an APE system consist of triplets of the form (source, target, post-edited target) rather than the (source, target) pairs normally used in MT. Following the translation-based approach, initial solutions relied on the phrase-based paradigm (Simard et al., 2007; Dugast et al., 2007; Terumasa, 2007; Pilevar, 2011; B´echara et al., 2011; Chatterjee et al., 2015; Chatterjee et al., 2016). However, in the past couple of years, top results have been achieved by neural architectures (Pal et al., 2016; Junczys-Dowmunt and Grundkiewicz, 2016; Chatterjee et al., 2017; Junczys-Dowmunt and Grundkiewicz, 2017; Tebbifakhr et al., 2018). Recent advancements made by participants in the APE shared task organized within the Conference on Machine Translation (WMT) have shown the capability of APE systems to significantly improve the performance of a black-box MT system gaining up to seve"
W19-6625,L18-1004,1,0.86203,"Missing"
W19-6625,2005.eamt-1.18,0,0.0746382,"ld be “glued” together to form a new system that is added in a modular way to a traditional CAT pipeline. Third, in Section 4 we describe our exProceedings of MT Summit XVII, volume 1 Related work Fuzzy-match repair FMR aims to reduce the post-editing effort of translation proposals retrieved from a TM. To do so FMR techniques rely on a source of bilingual information, usually MT, to automatically repair a translation proposal by modifying those parts of the proposal that otherwise should be postedited by the translator. The idea of FMR points back to papers by Kranias and Samiotou (2004) and Hewavitharana et al. (2005) whose approaches were based on the location of anchor points via alignment of words and relied heavily on the inner workings of the MT system they used. Improvements over time led way to advances that used phrase-based MT (Simard and Isabelle, 2009; Koehn and Senellart, 2010). Work has gradually advanced and various FMR methods have been proposed that share one common theme: locating and repairing sub-segments in the translation proposal. Later works (Dandapat et al., 2011; Ortega et al., 2016), on the other hand, can use any MT system as a black-box. Knowles et al. (2018) recently performed"
W19-6625,W17-4775,0,0.0132537,", volume 1 MT QE to choose between the original MT output and its post-edited version. Additionally, Tan et al. (2017) attempts to correct a common problem in APE known as “overcorrection” (i.e. systems’ tendency to completely re-translate the MT output, also rephrasing parts that are already correct). They do this by specifying two models (called neural post-editing models). Then, they use MT and QE to help select one of the models for the translation. This by no means is related to fuzzy-match repair; however, the idea of combining several systems around APE is similar to what we are doing. Hokamp (2017) includes word-level MT QE features as additional inputs to an APE system and trains several neural models using different input representations, but sharing the same output space. These models are finally ensembled together and tuned for APE and MT QE. 3 TM repairing through FMR and APE Our system is a two-step process that can be added to any TM-based CAT tool that has access to a source of bilingual information (SBI), such as a black-box MT system. The first step of our process is to use the translation unit whose source segment is most similar to the segment to be translated as input to FM"
W19-6625,W16-2378,0,0.013358,"n improved text in the target language. Under this definition, the “parallel data” used for training an APE system consist of triplets of the form (source, target, post-edited target) rather than the (source, target) pairs normally used in MT. Following the translation-based approach, initial solutions relied on the phrase-based paradigm (Simard et al., 2007; Dugast et al., 2007; Terumasa, 2007; Pilevar, 2011; B´echara et al., 2011; Chatterjee et al., 2015; Chatterjee et al., 2016). However, in the past couple of years, top results have been achieved by neural architectures (Pal et al., 2016; Junczys-Dowmunt and Grundkiewicz, 2016; Chatterjee et al., 2017; Junczys-Dowmunt and Grundkiewicz, 2017; Tebbifakhr et al., 2018). Recent advancements made by participants in the APE shared task organized within the Conference on Machine Translation (WMT) have shown the capability of APE systems to significantly improve the performance of a black-box MT system gaining up to seven BLEU points (Bojar et al., 2017; Chatterjee et al., 2018a). The neural approaches proposed share common traits such as using multi encoders (one for the source and one for the MT segments) and leveraging artificial data (round-trip translations) to maximi"
W19-6625,I17-1013,0,0.0157202,"e “parallel data” used for training an APE system consist of triplets of the form (source, target, post-edited target) rather than the (source, target) pairs normally used in MT. Following the translation-based approach, initial solutions relied on the phrase-based paradigm (Simard et al., 2007; Dugast et al., 2007; Terumasa, 2007; Pilevar, 2011; B´echara et al., 2011; Chatterjee et al., 2015; Chatterjee et al., 2016). However, in the past couple of years, top results have been achieved by neural architectures (Pal et al., 2016; Junczys-Dowmunt and Grundkiewicz, 2016; Chatterjee et al., 2017; Junczys-Dowmunt and Grundkiewicz, 2017; Tebbifakhr et al., 2018). Recent advancements made by participants in the APE shared task organized within the Conference on Machine Translation (WMT) have shown the capability of APE systems to significantly improve the performance of a black-box MT system gaining up to seven BLEU points (Bojar et al., 2017; Chatterjee et al., 2018a). The neural approaches proposed share common traits such as using multi encoders (one for the source and one for the MT segments) and leveraging artificial data (round-trip translations) to maximize results. The APE system used in this paper proposes a novel ap"
W19-6625,2016.amta-researchers.3,1,0.873894,"Missing"
W19-6625,P16-2046,0,0.0336612,"Missing"
W19-6625,P02-1040,0,0.115866,"Missing"
W19-6625,W18-2108,1,0.888247,"ou (2004) and Hewavitharana et al. (2005) whose approaches were based on the location of anchor points via alignment of words and relied heavily on the inner workings of the MT system they used. Improvements over time led way to advances that used phrase-based MT (Simard and Isabelle, 2009; Koehn and Senellart, 2010). Work has gradually advanced and various FMR methods have been proposed that share one common theme: locating and repairing sub-segments in the translation proposal. Later works (Dandapat et al., 2011; Ortega et al., 2016), on the other hand, can use any MT system as a black-box. Knowles et al. (2018) recently performed a comparison of the nature of MT systems for their use in FMR. In particular, they contrast the quality of FMR output using neural MT and phrase-based MT. Most importantly, they show that neural MT may not be appropriate if it is not trained on indomain data. Other novel works, like the work by Bult´e et al. (2018), include FMR as a primary part of a system integrating MT and TM. Lastly, Ortega et al. (2018) have found a statistical way to select the best MT system to use in black-box FMR. Dublin, Aug. 19-23, 2019 |p. 257 2.2 Automatic post-editing Automatic post-editing is"
W19-6625,2012.eamt-1.34,0,0.0603209,"Missing"
W19-6625,2010.jec-1.4,0,0.124755,"anslation proposal that comes from a translation unit (a pair of parallel segments) whose source segment is similar to the segment to be translated. When the source segment in the translation unit and the segment to be translated are not identical, which happens very often, the translation proposal needs to be post-edited in order to create the final translation. FMR aims to provide repaired translation hypotheses to reduce the postediting effort of the original translation proposals by using another source of bilingual information such as an MT system. Some approaches to FMR, like the one by Koehn and Senellart (2010), heavily depend on the specific MT system type being used for repairing. Others, such as the one by Ortega et al. (2016) use an agnostic, black-box, MT system in such a way that the user would only choose from several repaired hypothesis proposals. APE aims to correct the errors present in a machine-translated text before showing it to the translator or post-editor. As motivated by Parton et al. (2012), an APE system can help to improve MT output by exploiting information that is not available during translation, or by performing a deeper text analysis, and by adapting the output of Dublin, A"
W19-6625,P07-2045,0,0.00759509,"Missing"
W19-6625,P16-1162,0,0.0261162,"0 sentences selected at random from the DGT TM, 2,500 are randomly selected and used to fine-tune the APE system (see Section 4.3), 500 are used for development, and 1,000 for testing. Altogether, about 350 sentences are not successfully repaired by FMR; in those cases, we used the output of Moses. 4.2 Machine translation systems We use the phrase-based statistical MT system Moses (Koehn et al., 2007) as a SBI for FMR; it has shown to perform well in previous experiments and in the black-box setting (Knowles et al., 2018). As a term of comparison we use Moses and the neural MT system Nematus (Sennrich et al., 2016) as baselines; we leave for future work the inclusion of a neural MT system as a SBI for FMR. It is worth noting that the phrase-based MT system performed better on the APE module than the neural MT system (see Table 2). With Moses we use pre-trained models downloaded from www.statmt.org/moses/ RELEASE-3.0/models/. By using pretrained models, we try to replicate what most users in a corporate setting would choose, at least as a first iteration, in absence of advanced knowledge to build the MT models by their own. Nematus is trained on a collection of datasets belonging to different domains. Th"
W19-6625,J10-4005,0,0.035949,"e words in s and s0 obtained as a by-product of the computation of the word-based edit distance (Levenshtein, 1966) between s and 1 Other SBIs that could be used are sub-segment translation memories, bilingual dictionaries or phrase tables. Dublin, Aug. 19-23, 2019 |p. 258 s0 : mismatched words are left unaligned. SBIs are then used to translate into the target language subsegment pairs of s and s0 containing mismatched words. The sub-segments pairs to be translated are obtained by using the phrase-pair extraction algorithm used in phrase-based statistical MT to obtain bilingual phrase pairs (Koehn, 2010, section 5.2.3). The translations obtained for the subsegments of s are used to identify the sub-segment in t that needs to be modified, and the translation of the sub-segments of s0 to identify the way they should be modified. In this way, a set of patching operators is built. Each patching operator consists of a sub-segment σ of s, a sub-segment σ 0 of s0 aligned with σ, a sub-segment τ of t to be repaired, and a sub-segment τ 0 , the translation of σ 0 , to be used for repairing. By combining these patching operators, a set of fuzzy-match repaired hypothesis is generated. For a detailed de"
W19-6625,kranias-samiotou-2004-automatic,0,0.0936348,"how how the two technologies could be “glued” together to form a new system that is added in a modular way to a traditional CAT pipeline. Third, in Section 4 we describe our exProceedings of MT Summit XVII, volume 1 Related work Fuzzy-match repair FMR aims to reduce the post-editing effort of translation proposals retrieved from a TM. To do so FMR techniques rely on a source of bilingual information, usually MT, to automatically repair a translation proposal by modifying those parts of the proposal that otherwise should be postedited by the translator. The idea of FMR points back to papers by Kranias and Samiotou (2004) and Hewavitharana et al. (2005) whose approaches were based on the location of anchor points via alignment of words and relied heavily on the inner workings of the MT system they used. Improvements over time led way to advances that used phrase-based MT (Simard and Isabelle, 2009; Koehn and Senellart, 2010). Work has gradually advanced and various FMR methods have been proposed that share one common theme: locating and repairing sub-segments in the translation proposal. Later works (Dandapat et al., 2011; Ortega et al., 2016), on the other hand, can use any MT system as a black-box. Knowles e"
W19-6625,2009.mtsummit-papers.14,0,0.0474348,"effort of translation proposals retrieved from a TM. To do so FMR techniques rely on a source of bilingual information, usually MT, to automatically repair a translation proposal by modifying those parts of the proposal that otherwise should be postedited by the translator. The idea of FMR points back to papers by Kranias and Samiotou (2004) and Hewavitharana et al. (2005) whose approaches were based on the location of anchor points via alignment of words and relied heavily on the inner workings of the MT system they used. Improvements over time led way to advances that used phrase-based MT (Simard and Isabelle, 2009; Koehn and Senellart, 2010). Work has gradually advanced and various FMR methods have been proposed that share one common theme: locating and repairing sub-segments in the translation proposal. Later works (Dandapat et al., 2011; Ortega et al., 2016), on the other hand, can use any MT system as a black-box. Knowles et al. (2018) recently performed a comparison of the nature of MT systems for their use in FMR. In particular, they contrast the quality of FMR output using neural MT and phrase-based MT. Most importantly, they show that neural MT may not be appropriate if it is not trained on indo"
W19-6625,N07-1064,0,0.0532993,"output using neural MT and phrase-based MT. Most importantly, they show that neural MT may not be appropriate if it is not trained on indomain data. Other novel works, like the work by Bult´e et al. (2018), include FMR as a primary part of a system integrating MT and TM. Lastly, Ortega et al. (2018) have found a statistical way to select the best MT system to use in black-box FMR. Dublin, Aug. 19-23, 2019 |p. 257 2.2 Automatic post-editing Automatic post-editing is the task of correcting recurring errors from an MT system by learning from human corrections. Starting from the seminal work by (Simard et al., 2007), the problem has been tackled as a “monolingual translation” task in which the MT output must be translated into an improved text in the target language. Under this definition, the “parallel data” used for training an APE system consist of triplets of the form (source, target, post-edited target) rather than the (source, target) pairs normally used in MT. Following the translation-based approach, initial solutions relied on the phrase-based paradigm (Simard et al., 2007; Dugast et al., 2007; Terumasa, 2007; Pilevar, 2011; B´echara et al., 2011; Chatterjee et al., 2015; Chatterjee et al., 2016"
W19-6625,2006.amta-papers.25,0,0.174736,"Missing"
W19-6625,W18-6471,1,0.828095,"PE system consist of triplets of the form (source, target, post-edited target) rather than the (source, target) pairs normally used in MT. Following the translation-based approach, initial solutions relied on the phrase-based paradigm (Simard et al., 2007; Dugast et al., 2007; Terumasa, 2007; Pilevar, 2011; B´echara et al., 2011; Chatterjee et al., 2015; Chatterjee et al., 2016). However, in the past couple of years, top results have been achieved by neural architectures (Pal et al., 2016; Junczys-Dowmunt and Grundkiewicz, 2016; Chatterjee et al., 2017; Junczys-Dowmunt and Grundkiewicz, 2017; Tebbifakhr et al., 2018). Recent advancements made by participants in the APE shared task organized within the Conference on Machine Translation (WMT) have shown the capability of APE systems to significantly improve the performance of a black-box MT system gaining up to seven BLEU points (Bojar et al., 2017; Chatterjee et al., 2018a). The neural approaches proposed share common traits such as using multi encoders (one for the source and one for the MT segments) and leveraging artificial data (round-trip translations) to maximize results. The APE system used in this paper proposes a novel approach extending the origi"
W19-6625,2007.mtsummit-wpt.4,0,0.0992745,"MT system by learning from human corrections. Starting from the seminal work by (Simard et al., 2007), the problem has been tackled as a “monolingual translation” task in which the MT output must be translated into an improved text in the target language. Under this definition, the “parallel data” used for training an APE system consist of triplets of the form (source, target, post-edited target) rather than the (source, target) pairs normally used in MT. Following the translation-based approach, initial solutions relied on the phrase-based paradigm (Simard et al., 2007; Dugast et al., 2007; Terumasa, 2007; Pilevar, 2011; B´echara et al., 2011; Chatterjee et al., 2015; Chatterjee et al., 2016). However, in the past couple of years, top results have been achieved by neural architectures (Pal et al., 2016; Junczys-Dowmunt and Grundkiewicz, 2016; Chatterjee et al., 2017; Junczys-Dowmunt and Grundkiewicz, 2017; Tebbifakhr et al., 2018). Recent advancements made by participants in the APE shared task organized within the Conference on Machine Translation (WMT) have shown the capability of APE systems to significantly improve the performance of a black-box MT system gaining up to seven BLEU points (B"
