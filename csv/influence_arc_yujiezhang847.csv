2005.mtsummit-invited.7,J03-1002,0,0.00441278,"Missing"
2005.mtsummit-invited.7,2003.mtsummit-papers.51,0,0.0620587,"Missing"
2005.mtsummit-invited.7,H93-1040,0,0.0440308,"Missing"
2005.mtsummit-papers.10,J97-2004,0,0.152501,"d (&c&&)). (3) Posstra expresses the possibility estimated by and Aaug as alignment results. using the traditional Chinese characters. 5.1.2 Algorithm for broadening coverage Let WJ ( WC ) denote the list of words Bilingual Dictionary j ∈ WJ (c ∈ WC ) that are still not aligned. In this A translation dictionary can help to identify the translation relations. Let C j denote the Chinese phase, we only consider one to one alignment. ~ ~ For j (∈ WJ ) , we estimate the possibility of j translation set of j . We can estimate the possibility of j being aligned with &c&& using the following formula (Ker and Chang, 1997). Possdic ( j ,&c&&) = max Sim(c' ,&c&&). c '∈C j being aligned with c~ (∈ WC ) as follows. ~ For an alignment candidate ( j , c~ ) , we estimate its likelihood by taking the established alignments into account. Here we consider four established alignments: the two alignments that are the nearest ~ to j on the left and right and the two alignments that are the nearest to c~ on the left and right. Null0 , Null0 ) and First, add ( (4) Possdic expresses the possibility estimated by using a translation dictionary. An automatically built Japanese-Chinese dictionary is used here, which was built fro"
2005.mtsummit-papers.10,maekawa-etal-2000-spontaneous,1,0.906267,"Missing"
2005.mtsummit-papers.10,P01-1067,0,0.0308519,"Missing"
2005.mtsummit-papers.10,C94-2209,0,0.0409686,"Annotation on Chinese Sentences For Chinese morphological analysis, we used the analyser developed by Peking University, where the research on definition of Chinese words and the criteria of word segmentation has been conducted for over ten years. The achievements include a grammatical knowledge base of contemporary Chinese, an automatic morphological analyser, and an annotated People’s Daily Corpus. Since the definition and tagset are widely used in Chinese language processing, we also took the criteria as the basis of our guidelines. A morphological analyzer developed by Peking University (Zhou and Yu, 1994) was applied for automatic annotation of the Chinese sentences and then the automatically tagged sentences were revised by humans. An annotated sentence is illustrated in Figure 3, which is the Chinese sentence in Ex. 1 in Section 2. The interface of the tool is shown in Figure 4 and Figure 5. S-ID: 950104141-008 这些/r 俄军/j 士兵/n 均/d 为/v 十九/m 岁/q 左右/m 的/u 年青人/n ，/w 他们/r 甚至/d 连/p 回答/v 问题/n 的/u 气力/n 也/d 没有/v 。/w Figure 3 An annotated Chinese sentence 4.3 Tool for Manual Revision We developed a tool to assist annotators in revision. The tool has both Japanese and Chinese versions. Here, we introduc"
2005.mtsummit-papers.10,W04-2208,1,\N,Missing
2005.mtsummit-papers.18,J93-2003,0,0.0071722,"of the multi-aligner. 1 Introduction In a parallel corpus, automatic word alignment is to identify the translation relations between the words in a source sentence and those in a target sentence. A word-aligned parallel corpus has many applications, such as machine translation, machineaided translation, bilingual lexicography, and wordsense disambiguation. For these applications, much research on automatic word alignment has been conducted and reported. The statistics-based approach is widely studied (Och and Ney, 2003), and is mainly based on the research of statistical machine translation (Brown et al., 1993). However, this approach incorrectly aligns less frequently occurring words when statistically significant evidence is not available. Instead of word-based statistics, Ker proposed a class-based approach by using lexicon resources (Ker and Chang, 1997). Based on this idea, various 2 Japanese-Chinese Parallel Corpus The corpus we used in this study consists of 38,383 Japanese sentences from Mainichi newspaper and their Chinese translations. The corpus has been morphological annotated (word segmented and part-of-speech tagged) in the first phase of the project. For Japanese morphological 133 spe"
2005.mtsummit-papers.18,P00-1050,0,0.0436575,"Missing"
2005.mtsummit-papers.18,J97-2004,0,0.207324,"such as machine translation, machineaided translation, bilingual lexicography, and wordsense disambiguation. For these applications, much research on automatic word alignment has been conducted and reported. The statistics-based approach is widely studied (Och and Ney, 2003), and is mainly based on the research of statistical machine translation (Brown et al., 1993). However, this approach incorrectly aligns less frequently occurring words when statistically significant evidence is not available. Instead of word-based statistics, Ker proposed a class-based approach by using lexicon resources (Ker and Chang, 1997). Based on this idea, various 2 Japanese-Chinese Parallel Corpus The corpus we used in this study consists of 38,383 Japanese sentences from Mainichi newspaper and their Chinese translations. The corpus has been morphological annotated (word segmented and part-of-speech tagged) in the first phase of the project. For Japanese morphological 133 special case when k = 0 . Actually, the case of more-to-one has also been considered in the study. For simplicity of describtion, however, only the case of one-to-more is described here. Three kinds of lexical resources used for the estimation are describ"
2005.mtsummit-papers.18,maekawa-etal-2000-spontaneous,1,0.796201,"Missing"
2005.mtsummit-papers.18,J03-1002,0,0.0088417,"statistics-based aligner at the same time. Quantitative results confirmed the effectiveness of the multi-aligner. 1 Introduction In a parallel corpus, automatic word alignment is to identify the translation relations between the words in a source sentence and those in a target sentence. A word-aligned parallel corpus has many applications, such as machine translation, machineaided translation, bilingual lexicography, and wordsense disambiguation. For these applications, much research on automatic word alignment has been conducted and reported. The statistics-based approach is widely studied (Och and Ney, 2003), and is mainly based on the research of statistical machine translation (Brown et al., 1993). However, this approach incorrectly aligns less frequently occurring words when statistically significant evidence is not available. Instead of word-based statistics, Ker proposed a class-based approach by using lexicon resources (Ker and Chang, 1997). Based on this idea, various 2 Japanese-Chinese Parallel Corpus The corpus we used in this study consists of 38,383 Japanese sentences from Mainichi newspaper and their Chinese translations. The corpus has been morphological annotated (word segmented and"
2005.mtsummit-papers.18,C94-2209,0,0.0449107,"ir Chinese translations. The corpus has been morphological annotated (word segmented and part-of-speech tagged) in the first phase of the project. For Japanese morphological 133 special case when k = 0 . Actually, the case of more-to-one has also been considered in the study. For simplicity of describtion, however, only the case of one-to-more is described here. Three kinds of lexical resources used for the estimation are described below . annotation, the definition of the Corpus of Spontaneous Japanese was adopted (Maekawa, 2000). For Chinese, the definition of Peking University was adopted (Zhou and Yu, 1994). The average lengths of the sentences on both sides are about 30 words. The study, word alignment, aims to assist to word alignment annotation, which is a task in the second phase of the project. 3 Orthography About half of Japanese words contain kanji, the Chinese characters used in Japanese writing. We call them kanji words. Japanese words may also contain hiragana or katakana, which are phonetic characters. Because some kanji words were adapted directly from China, their Chinese translations are the same as the words themselves. For example, the Chinese translations for the Japanese words"
2007.mtsummit-papers.73,C94-2209,0,0.128581,"Missing"
2020.ccl-1.21,P17-1152,0,0.0764269,"Missing"
2020.ccl-1.21,N19-1423,0,0.0666584,"Missing"
2020.ccl-1.21,C18-1166,0,0.051011,"Missing"
2020.ccl-1.21,P18-1130,0,0.0583507,"Missing"
2020.ccl-1.21,P16-2022,0,0.0779458,"Missing"
2020.ccl-1.76,D19-1422,0,0.0276345,"of the POS information in the shared encoding layers, the POS tagging task cannot provide the predicted information for word segmentation and dependency parsing. Therefore, we introduce the vector representation of the POS tag and propose the tag attention mechanism (TAM) to integrate the POS information of contextual characters into the vector representation of each character, so that the POS information of the contextual character can also be used in the word segmentation and dependency parsing. This structure is similar to the hierarchicallyrefined label attention network (LAN) proposed by Cui and Zhang (2019), but we use it to obtain POS information of each layer for subsequent character-level dependency parsing tasks. LAN differs from TAM in that LAN only predicts at the last layer while TAM predicts at each layer. We have tried to predict only at the last layer, but the result of segmentation and dependency parsing is slightly lower than predicting at each layer. The model is shown in figure 3. Firstly, we vectorize the POS tags. Each POS tag is represented by a vector eti , and the represents of the set of POS tags denoted as E t = {et1 , ..., etm }, which is randomly initialized before model t"
2020.ccl-1.76,P12-1110,0,0.395791,"al method is usually following pipline way: word segmentation, POS tagging and dependency parsing. However, there are two problems of the pipline way, one is error propagation:incorrect word segmentation directly affects POS tagging and dependency parsing, another is information sharing: the tree tasks are strongly related, the label information of one task can help others, but the pipline way cannot exploit the correlations among the three tasks. Using joint model for Chinese word segmentation, POS tagging and dependency parsing is a solution to these two problems. The previous joint models (Hatori et al., 2012; Zhang et al., 2014; Kurita et al., 2017) mainly adopted a transition-based framework to integrate the three tasks. Based on the standard sequential shift-reduce transitions, they design some extra actions for word segmentation and POS tagging. Although these transition-based models maintained the best performance of word segmentation, POS tagging and dependency parsing, its local decision problem led to the low precision of long-distance dependency parsing, which limited the precision of dependency parsing. Different from the transition-based framework, the graph-based framework has the abil"
2020.ccl-1.76,P08-1102,0,0.0991163,"Missing"
2020.ccl-1.76,Q16-1023,0,0.0275425,"ering research on the three tasks, and realized the synchronous processing of the three tasks. Zhang et al. (2014) annotated the internal structure of words, and regarded the word segmentation task as dependency parsing within characters to jointly process with three tasks. Kurita et al. (2017) firstly applied neural network to the charater-level dependency parsing. Although these transition-based joint models achieved best accuracy in dependency parsing, they still suffer from the limitation of local decision. With the development of neural network, the graph-based dependency parsing models (Kiperwasser and Goldberg, 2016; Dozat and Manning, 2016) using neural networks have developed rapidly. those model fully exploit the ability of the bidirectional long short-term memory network (Bi-LSTM) (Hochreiter and Schmidhuber, 1997) and attention mechanism (Bahdanau et al., 2014; Vaswani et al., 2017) to capture the interactions of words in a sentence. Different from transition-based models, the graph-based model can make global decision when predicting dependency arcs, but few joint model adopted this framework. Yan et al. (2019) firstly proposed a joint model adopting graph-based framework with neural network for Ch"
2020.ccl-1.76,P17-1111,0,0.196875,"y: word segmentation, POS tagging and dependency parsing. However, there are two problems of the pipline way, one is error propagation:incorrect word segmentation directly affects POS tagging and dependency parsing, another is information sharing: the tree tasks are strongly related, the label information of one task can help others, but the pipline way cannot exploit the correlations among the three tasks. Using joint model for Chinese word segmentation, POS tagging and dependency parsing is a solution to these two problems. The previous joint models (Hatori et al., 2012; Zhang et al., 2014; Kurita et al., 2017) mainly adopted a transition-based framework to integrate the three tasks. Based on the standard sequential shift-reduce transitions, they design some extra actions for word segmentation and POS tagging. Although these transition-based models maintained the best performance of word segmentation, POS tagging and dependency parsing, its local decision problem led to the low precision of long-distance dependency parsing, which limited the precision of dependency parsing. Different from the transition-based framework, the graph-based framework has the ability to make global decisions. Before the a"
2020.ccl-1.76,N18-2028,0,0.0160097,"ollowing previous works (Jiang et al., 2008; Kurita et al., 2017; Yan et al., 2019), we use standard measures of word-level F1 score to evaluate word segmentation, POS tagging and dependency parsing. F1 score is calculated according to the precision P and the recall R as F = 2P R/(P + R) (Jiang et al., 2008). Dependency parsing task is evaluated with the unlabeled attachment scores excluding punctuations. The output of POS tags and dependency arcs cannot be correct unless the corresponding words are correctly segmented. 3.2 Model Configuration We use the same Tencent’s pre-trained embeddings (Song et al., 2018) and configuration as Yan et al. (2019), and the dimension of character vectors is 200. The dimension of POS tag vectors is also 200. We use with 400 units for each Bi-LSTM layer and the layer numbers is 3. Dependency arc MLP output size is 500 and the label MLP output size is 100.The dropout rates are all 0.33. The models are trained with Adam algorithm (Kingma and Ba, 2014) to minimize the total loss of the cross-entropy of arc predictions, label predictions and POS tag predictions, which using uncertainty weights to combine losses. The initial learning rate is 0.002 annealed by multiplying"
2020.ccl-1.76,P14-1125,0,0.309796,"following pipline way: word segmentation, POS tagging and dependency parsing. However, there are two problems of the pipline way, one is error propagation:incorrect word segmentation directly affects POS tagging and dependency parsing, another is information sharing: the tree tasks are strongly related, the label information of one task can help others, but the pipline way cannot exploit the correlations among the three tasks. Using joint model for Chinese word segmentation, POS tagging and dependency parsing is a solution to these two problems. The previous joint models (Hatori et al., 2012; Zhang et al., 2014; Kurita et al., 2017) mainly adopted a transition-based framework to integrate the three tasks. Based on the standard sequential shift-reduce transitions, they design some extra actions for word segmentation and POS tagging. Although these transition-based models maintained the best performance of word segmentation, POS tagging and dependency parsing, its local decision problem led to the low precision of long-distance dependency parsing, which limited the precision of dependency parsing. Different from the transition-based framework, the graph-based framework has the ability to make global d"
2020.coling-main.209,D10-1059,0,0.0355734,"ax(˜ ot ), o˜t = (ot + η)/τ, η = − log(− log u) (5) where η is the Gumbel noise calculated from a uniform random variable u ∼ U (0, 1), τ is temperature. When τ → 0, the sample generated from the vocabulary is similar to the argmax operation, and when τ → ∞, the sample is gradually closing uniform distribution. Increasing the temperature increases the use of infrequent words (Holtzman et al., 2019), which has the implicit effect of weakening the tail distribution, making the model to explore more diverse generation. Finally, according to pvocab (yt |Y1:t−1 , X), we apply multinomial sampling (Chatterjee and Cancedda, 2010) to generate sentence Yˆ for computing rewards, which produces each word one by one through multinomial sampling over the model’s output distribution. The sampling terminate the expansion of a candidate sentence when an end of sentence (&lt;EOS&gt;) token is met. 2.4 Reinforcement Learning with Explored Paraphrase We adopt reinforcement learning (RL) (Sutton and Barto, 1998) to train our paraphrase generator by using the sampled sentences. Our paraphrase generator can be viewed as an “agent” that interacts with an external “environment” (original input or reference). The parameters of the agent defi"
2020.coling-main.209,P11-1020,0,0.0558942,"ora dataset. 4.3 Automatic Evaluation Metrics Following previous work (Prakash et al., 2016; Hasan et al., 2016) on paraphrase generation, we adopted well-known automatic evaluation metrics BLEU (B) (Papineni et al., 2002), ROUGE (R) (Lin, 2004) and METEOR (MET) (Lavie and Agarwal, 2007) to compute lexical similarity with reference. Pervious studies have shown that these metrics perform well in evaluating generated paraphrases. These n-gram-based matching may obtain low score for predictions with highly lexical and syntactical variation, but these predictions are not necessarily poor quality (Chen and Dolan, 2011; Wang et al., 2019). We further used Embedding Similarity (Sharma et al., 2017) to evaluate generated paraphrases. This metric measures the semantic similarity between the reference and prediction based on the cosine similarity of their embeddings on word and sentence level. Following previous work (Park et al., 2019; Egonmwan and Chali, 2019), we used average, extreme, and greedy (A/E/G) embedding similarities. Besides, we hope to generate more diverse paraphrases when preserving meaning. However, previous work (Miao et al., 2019) has shown that it is insufficient when only comparing with re"
2020.coling-main.209,P19-1599,0,0.148187,"y a reward function. Li et al. (2019) suppose a sentence-level paraphrase can be decomposed to word/phrase-level paraphrase and learn to generate paraphrases at different levels of granularity. More recent works also focus on generating diverse paraphrases, which is important for improving model generalization capability and robustness of downstream applications. Gupta et al. (2018) use a variational autoencoder framework to generate diverse paraphrases by introducing random noise as input. Iyyer et al. (2018) harness syntactic-tree template information for controllable paraphrase generation. Chen et al. (2019) use sentences as exemplars to graft their syntax style to generated paraphrases. Qian et al. (2019) uses multiple generators trained by reinforcement learning to generate diverse paraphrases. Similar to these works, we also adopt Seq2Seq model for paraphrase generation. However, significantly different from them, our work extend the Seq2Seq model to use explored paraphrases for model training via deep reinforcement learning. We further introduce evaluation metrics in terms of expressive diversity and semantic similarity for model learning. Finally, our model can effectively generate paraphras"
2020.coling-main.209,D14-1179,0,0.0154494,"Missing"
2020.coling-main.209,D17-1091,0,0.0615809,"-based algorithm that automatically balances these training objectives. Experiments and analyses on Quora and Twitter datasets demonstrate that our proposed method not only gains a significant increase in diversity but also improves generation quality over several state-of-the-art baselines. 1 Introduction Paraphrase generation (PG) creates different expressions that share the same meaning (e.g., “how far is Earth from Sun” and “what is the distance between Sun and Earth”). It is a crucial technology in many downstream natural language processing (NLP) applications such as question answering (Dong et al., 2017), machine translation (Zhou et al., 2019), and text summarization (Zhao et al., 2018). Diversity is an essential characteristic of human language, as the meaning of a text can often have multiple different expressions. A good paraphrase generation system is often required to conform to two desired properties (Xu et al., 2018b). The first is diversity, capturing a wide range of linguistic variations. The second is fidelity, preserving semantic meanings while paraphrasing. Therefore, we hope to generate diverse paraphrases while ensuring same meaning, which is important for enhancing generalizat"
2020.coling-main.209,D19-5627,0,0.053267,"us studies have shown that these metrics perform well in evaluating generated paraphrases. These n-gram-based matching may obtain low score for predictions with highly lexical and syntactical variation, but these predictions are not necessarily poor quality (Chen and Dolan, 2011; Wang et al., 2019). We further used Embedding Similarity (Sharma et al., 2017) to evaluate generated paraphrases. This metric measures the semantic similarity between the reference and prediction based on the cosine similarity of their embeddings on word and sentence level. Following previous work (Park et al., 2019; Egonmwan and Chali, 2019), we used average, extreme, and greedy (A/E/G) embedding similarities. Besides, we hope to generate more diverse paraphrases when preserving meaning. However, previous work (Miao et al., 2019) has shown that it is insufficient when only comparing with reference because simply copying the input sentence itself yields the highest BLEU-ref score. To evaluate the variation of generated paraphrases, following Miao et al. (2019), we used BLEU-ori (B-ori) metric that against the original input sentence, in which the lower n-gram overlaps, the better variation and diversity. 4.4 Baselines We compared"
2020.coling-main.209,N18-1170,0,0.241258,"ization (Zhao et al., 2018). Diversity is an essential characteristic of human language, as the meaning of a text can often have multiple different expressions. A good paraphrase generation system is often required to conform to two desired properties (Xu et al., 2018b). The first is diversity, capturing a wide range of linguistic variations. The second is fidelity, preserving semantic meanings while paraphrasing. Therefore, we hope to generate diverse paraphrases while ensuring same meaning, which is important for enhancing generalization capability and robustness of downstream applications (Iyyer et al., 2018). As shown in Table 1, we give some examples. These examples express the same meaning but with different diversities. Most recent state-of-the-art approaches to PG (Prakash et al., 2016; Hasan et al., 2016; Gupta et al., 2018) employ neural sequence-to-sequence (Seq2Seq) models, which mainly uses one given reference for model learning, while the nature of paraphrasing indicates that we can paraphrase one sentence into several different sentences. Meanwhile these methods usually adopt the cross-entropy loss which requires a strict pairwise matching at the word level between the predicted senten"
2020.coling-main.209,P19-1607,0,0.126208,"guage? there is a possible way to improve my English language? what is the best way to increase my English knowledge? Table 1: Paraphrases of an original sentence with increasing diversity. night.” and the reference “I saw a film last night.”, the cross-entropy loss lacks the ability to properly optimize model to generate a diverse paraphrase even with only one changed token at word level. In recent years, there are also growing interests in generating lexically and syntactically diverse paraphrases (Gupta et al., 2018; Xu et al., 2018b; Xu et al., 2018a; Park et al., 2019; Qian et al., 2019; Kajiwara, 2019). For Seq2Seq models, the techniques of generating diverse paraphrases mainly include two categories: i) applying decoding methods such as using beam search or multiple decoders; ii) introducing random noise as model input. Park et al. (2019) use multi-time decoding to diverse generation by considering those generated sentences previously. Qian et al. (2019) use multiple generators to generate a variety of different paraphrases. Gupta et al. (2018) employ a variational auto-encoder framework to produce multiple paraphrases according to different noise inputs. Although these methods can improve"
2020.coling-main.209,D17-1126,0,0.0136537,"ning. 4 Experiment In this section, we described the datasets, experimental setup, evaluation metrics and the results of our experiments. 4.1 Datasets We conducted experiments on two standard datasets Quora and Twitter to evaluate the proposed model. Quora Dataset This dataset is a paired paraphrase dataset in question domain. It consists of 150K paraphrase pairs. Following previous work (Li et al., 2018; Qian et al., 2019), we used 30K pairs and 4K pairs as test set and validation set, and 100K pairs for training, respectively. Twitter Dataset This dataset is Twitter URL paraphrasing corpus (Lan et al., 2017) that contains two subsets, one is labelled by human annotators while the other is labelled automatically by algorithm. Following previous work (Li et al., 2018; Qian et al., 2019), we sampled 5K pairs as the test set and 1K pairs as validation set from the labeled subset, while using the remaining 110K pairs as training set. 4.2 Model Configuration We used the following experimental setting for our model. Following Li et al. (2018), we maintained a fixed-size vocabulary of 5K shared by the words in input and output, and truncate all the sentences longer than 20 words. For paraphrase generator"
2020.coling-main.209,W07-0734,0,0.076928,"Work 59.27 32.00 30.49 61.81 35.27 34.21 63.84 36.97 34.78 59.96 33.93 33.32 63.08 36.61 34.83 64.66 38.22 35.02 Emb(A/E/G)↑ B-ori-1↓ 80.61/-/64.81 81.50/-/65.52 - - 92.58/71.94/82.63 93.35/75.22/92.11 93.73/76.10/92.12 93.10/74.13/90.61 93.81/76.34/92.12 93.74/76.33/92.31 60.57 67.91 70.54 66.63 71.06 67.60 Table 2: Performance on Quora dataset. 4.3 Automatic Evaluation Metrics Following previous work (Prakash et al., 2016; Hasan et al., 2016) on paraphrase generation, we adopted well-known automatic evaluation metrics BLEU (B) (Papineni et al., 2002), ROUGE (R) (Lin, 2004) and METEOR (MET) (Lavie and Agarwal, 2007) to compute lexical similarity with reference. Pervious studies have shown that these metrics perform well in evaluating generated paraphrases. These n-gram-based matching may obtain low score for predictions with highly lexical and syntactical variation, but these predictions are not necessarily poor quality (Chen and Dolan, 2011; Wang et al., 2019). We further used Embedding Similarity (Sharma et al., 2017) to evaluate generated paraphrases. This metric measures the semantic similarity between the reference and prediction based on the cosine similarity of their embeddings on word and sentenc"
2020.coling-main.209,D18-1421,0,0.633688,"hood of the ground-truth reference and (b) it makes our model has the ability to explore unseen paraphrases beyond one single reference. 3.1 Rewards for Multi-Objective Learning ROUGE Reward with Reference The first basic reward is based on the primary evaluation metric of ROUGE package (Lin, 2004). We compare a sampled sentence Yˆ s1 with ground-truth reference Y ref with ROUGE score (namely ROUGE-ref), and then takes the score as a reward. The loss function is given by: ∇θ Lrl1 (θ) ≈ −(ROUGE−ref(Yˆ s1 , Y ref )−b)∇θ log pθ (Yˆ s1 ), b = ROUGE−ref(Yˆ b , Y ref ) (9) Similar to previous work (Li et al., 2018), we find that ROUGE-ref score as a reward works better compared to only using cross-entropy loss. This reward can be taken as sentence-level learning signal, which overcomes the full token-level matching issue of cross-entropy loss at training stage. On the other hand, as pointed in Kajiwara (2019), paraphrase generation rewrites only a limited portion of an original input and the reference often includes some words occurred in the original input, thus a sentence with higher ROUGE-ref score may have low diversity (Miao et al., 2019). Therefore, the reward based on reference do not focus on th"
2020.coling-main.209,P19-1332,0,0.400494,"ual LSTM RbM-SL RbM-IRL DEPD 33.90 44.67 45.74 34.23 Seq2SeqAttn PNet PNet + LE-ROUGE-ref PNet + LE-ROUGE-ori PNet + LE-SEM-ori PNet + all 30.86 31.10 33.45 28.48 28.64 32.84 R-1↑ R-2↑ MET↑ Previous Work 32.50 16.86 13.65 41.87 24.23 19.97 42.15 24.73 20.18 24.29 This Work 40.65 28.35 20.68 43.21 28.94 23.91 45.89 31.20 25.60 39.67 26.30 21.93 40.36 26.43 21.87 45.51 30.61 25.20 Emb(A/E/G)↑ B-ori-1↓ - - 82.91/53.71/87.36 84.04/54.66/90.16 84.92/56.86/92.28 81.52/51.43/88.42 82.68/52.37/90.81 85.57/57.46/93.81 48.31 52.87 57.16 46.39 49.19 53.21 Table 3: Performance on Twitter dataset. • DPNG (Li et al., 2019): This is a Transformer-based model that can learn and generate paraphrases of a sentence at different levels of granularity (word or phrase) in a disentangled way. 4.5 Results Baseline Cross-Entropy Model Results Our paraphrase generation model has attention mechanism (Seq2SeqAtt) and pointer-generator network (PNet). For better observing model behaviour, we first trained two baselines by applying cross-entropy optimization. As we can see, the model with pointergenerator network effectively improves performance in all metrics related to reference. And it is also natural for PNet to obtain hig"
2020.coling-main.209,P02-1040,0,0.106418,"5.84 64.39 38.11 32.84 64.02 37.72 31.97 29.28 63.73 37.75 This Work 59.27 32.00 30.49 61.81 35.27 34.21 63.84 36.97 34.78 59.96 33.93 33.32 63.08 36.61 34.83 64.66 38.22 35.02 Emb(A/E/G)↑ B-ori-1↓ 80.61/-/64.81 81.50/-/65.52 - - 92.58/71.94/82.63 93.35/75.22/92.11 93.73/76.10/92.12 93.10/74.13/90.61 93.81/76.34/92.12 93.74/76.33/92.31 60.57 67.91 70.54 66.63 71.06 67.60 Table 2: Performance on Quora dataset. 4.3 Automatic Evaluation Metrics Following previous work (Prakash et al., 2016; Hasan et al., 2016) on paraphrase generation, we adopted well-known automatic evaluation metrics BLEU (B) (Papineni et al., 2002), ROUGE (R) (Lin, 2004) and METEOR (MET) (Lavie and Agarwal, 2007) to compute lexical similarity with reference. Pervious studies have shown that these metrics perform well in evaluating generated paraphrases. These n-gram-based matching may obtain low score for predictions with highly lexical and syntactical variation, but these predictions are not necessarily poor quality (Chen and Dolan, 2011; Wang et al., 2019). We further used Embedding Similarity (Sharma et al., 2017) to evaluate generated paraphrases. This metric measures the semantic similarity between the reference and prediction base"
2020.coling-main.209,C18-1230,0,0.0262193,"opying the input sentence itself yields the highest BLEU-ref score. To evaluate the variation of generated paraphrases, following Miao et al. (2019), we used BLEU-ori (B-ori) metric that against the original input sentence, in which the lower n-gram overlaps, the better variation and diversity. 4.4 Baselines We compared our model with several state-of-the-art models in the paraphrase generation field. • Residual LSTM (Prakash et al., 2016): This implements stacked residual LSTM networks. • VAE-SVG-eq (Gupta et al., 2018): This employs a variational autoencoder as its main component. • EDD-LG (Patro et al., 2018): This introduces semantic discriminator to learn encoder and decoder. • TRANS and TRANSSEQ (Egonmwan and Chali, 2019): This integrates Transformer model (Vaswani et al., 2017) and Recurrent Neural Network GRU (Cho et al., 2014) as encoder. • RbM-SL and RbM-IRL (Li et al., 2018): This is a generator-evaluator framework with the matching-based semantic evaluator trained by reinforcement learning. • DEPD (Qian et al., 2019): This uses multiple generators trained by reinforcement learning to generate a variety of different paraphrases. 2316 Model B-2↑ Residual LSTM RbM-SL RbM-IRL DEPD 33.90 44.67"
2020.coling-main.209,C16-1275,0,0.0429483,"Missing"
2020.coling-main.209,D19-1313,0,0.409468,"edge in English language? there is a possible way to improve my English language? what is the best way to increase my English knowledge? Table 1: Paraphrases of an original sentence with increasing diversity. night.” and the reference “I saw a film last night.”, the cross-entropy loss lacks the ability to properly optimize model to generate a diverse paraphrase even with only one changed token at word level. In recent years, there are also growing interests in generating lexically and syntactically diverse paraphrases (Gupta et al., 2018; Xu et al., 2018b; Xu et al., 2018a; Park et al., 2019; Qian et al., 2019; Kajiwara, 2019). For Seq2Seq models, the techniques of generating diverse paraphrases mainly include two categories: i) applying decoding methods such as using beam search or multiple decoders; ii) introducing random noise as model input. Park et al. (2019) use multi-time decoding to diverse generation by considering those generated sentences previously. Qian et al. (2019) use multiple generators to generate a variety of different paraphrases. Gupta et al. (2018) employ a variational auto-encoder framework to produce multiple paraphrases according to different noise inputs. Although these me"
2020.coling-main.209,P17-1099,0,0.0602623,"enote the sequence generated by the model. Our proposed model mainly contains three components: paraphrase generator, sample-based exploring algorithm and reinforcement learning with explored paraphrase. Figure 1 gives an overview of our framework. Basically the generator can generate paraphrases of a given sentence, and the evaluator measures the quality of explored paraphrases in term of expressive diversity and semantic fidelity. 2.2 Paraphrase Generator We frame paraphrase generation as a sequence-to-sequence (Seq2Seq) problem. We adopt the encoderdecoder framework (Bahdanau et al., 2014; See et al., 2017), both of which are implemented as recurrent neural networks (RNN). All RNNs use LSTM cells (Hochreiter and Schmidhuber, 1997). Given an input sentence X, the goal is to learn a model p(θ) that can generate a sentence Yˆ = pθ (X) as its paraphrase. Traditionally the parameters θ are learned by maximizing the likelihood of the predicted sentence. Finally, model estimates the conditional probability p(Y |X) via directly mapping the input sentence X to its target paraphrase Y. The learning objective is to minimize the cross-entropy loss: Lce (θ) = − T X log pθ (yt |Y1:t−1 , X) (1) t=1 We choose t"
2020.coling-main.209,D18-1397,0,0.0181518,"e Optimization Our objective function combines the maximum-likelihood cross-entropy loss (Lce ) with rewards from policy gradient reinforcement learning to jointly optimize our model. Finally, the over learning objective is to minimize the following combined losses: Lall (θ) = α0 ∗ Lce + α1 ∗ Lrl1 + α2 ∗ Lrl2 + α3 ∗ Lrl3 (13) where α is the the weights to combine these losses. Optimizing multiple objectives at the same time is important for final performance, in which one objective can easily dominate the learning of a shared model, leading the other objectives are ineffective. Previous work (Wu et al., 2018) choose fixed weights α from manual experience for RL learning. Different from them, we use an adaptive method GradNorm (Chen et al., 2017), and the αi is vary at each training step t: αi = αi (t). The GradNorm algorithm controls gradient magnitudes through tuning of the multi-objective loss function. To optimize the weights αi (t) for gradient balancing, following Chen et al. (2017), we penalize the network when back-propagated gradients from any task are too large or too small. If objective i is training relatively quickly, then its weight αi (t) should decrease relative to other objective w"
2020.coling-main.209,D18-1355,0,0.0957376,"nd analyses on Quora and Twitter datasets demonstrate that our proposed method not only gains a significant increase in diversity but also improves generation quality over several state-of-the-art baselines. 1 Introduction Paraphrase generation (PG) creates different expressions that share the same meaning (e.g., “how far is Earth from Sun” and “what is the distance between Sun and Earth”). It is a crucial technology in many downstream natural language processing (NLP) applications such as question answering (Dong et al., 2017), machine translation (Zhou et al., 2019), and text summarization (Zhao et al., 2018). Diversity is an essential characteristic of human language, as the meaning of a text can often have multiple different expressions. A good paraphrase generation system is often required to conform to two desired properties (Xu et al., 2018b). The first is diversity, capturing a wide range of linguistic variations. The second is fidelity, preserving semantic meanings while paraphrasing. Therefore, we hope to generate diverse paraphrases while ensuring same meaning, which is important for enhancing generalization capability and robustness of downstream applications (Iyyer et al., 2018). As sho"
2020.coling-main.209,P19-2015,0,0.10207,"ces these training objectives. Experiments and analyses on Quora and Twitter datasets demonstrate that our proposed method not only gains a significant increase in diversity but also improves generation quality over several state-of-the-art baselines. 1 Introduction Paraphrase generation (PG) creates different expressions that share the same meaning (e.g., “how far is Earth from Sun” and “what is the distance between Sun and Earth”). It is a crucial technology in many downstream natural language processing (NLP) applications such as question answering (Dong et al., 2017), machine translation (Zhou et al., 2019), and text summarization (Zhao et al., 2018). Diversity is an essential characteristic of human language, as the meaning of a text can often have multiple different expressions. A good paraphrase generation system is often required to conform to two desired properties (Xu et al., 2018b). The first is diversity, capturing a wide range of linguistic variations. The second is fidelity, preserving semantic meanings while paraphrasing. Therefore, we hope to generate diverse paraphrases while ensuring same meaning, which is important for enhancing generalization capability and robustness of downstre"
2021.ccl-1.52,A97-1029,0,0.47131,"Missing"
2021.ccl-1.52,D18-1017,0,0.0274672,"Missing"
2021.ccl-1.52,N19-1423,0,0.0251454,"Missing"
2021.ccl-1.52,doddington-etal-2004-automatic,0,0.491706,"Missing"
2021.ccl-1.52,E17-2113,0,0.0599206,"Missing"
2021.ccl-1.52,C02-1054,0,0.507814,"Missing"
2021.ccl-1.52,C16-1087,0,0.0468943,"Missing"
2021.ccl-1.52,N16-1030,0,0.157278,"Missing"
2021.ccl-1.52,li-etal-2014-comparison,0,0.0649409,"Missing"
2021.ccl-1.52,P18-1074,0,0.0423974,"Missing"
2021.ccl-1.52,L16-1138,0,0.0212176,"现有研究已经开始探索命名实体识别任务与其他任务的关联，并提出基于多任务学习的方 法。Collobert et al. (2011)训练了一个window/sentence方法网络来共同执行NER、POS、组块 分析(Chunk)和语义角色标注(SRL)任务。这种多任务机制让训练算法发现并学习对所有任务都 感兴趣的有用的内部表示。Rei (2017)发现，通过在训练过程中加入无监督语言建模目标，可实 现序列标注模型性能的改进。Lin et al. (2018)提出了一种基于低资源的多语种多任务架构，可 有效转移不同类型的知识改善主模型。除了考虑命名实体识别连同其他序列标注任务，多任务 学习框架还可以应用到联合提取实体和关系，或将命名实体识别模型分为实体切分和实体类别 预测，进一步提升效果(Crichton et al., 2017; Wang et al., 2018)。 中文命名实体识别 CC L 现有的中文命名实体识别方法包括基于字的命名实体识别、基于词的命名实体识别、基 于字词联合的命名实体识别。Li et al. (2014)通过字级别和词级别统计方法的对比，表明基 于字符的命名实体识别方法一般有更好的表现。Huang et al. (2015)利用双向长短期记忆网 络(BiLSTM)提取特征，将拼写、上下文、词嵌入和地名词典四种类型的特征融于命名实体识 别任务。部分研究者(Lu et al., 2016; Dong et al., 2016)在基于神经网络的命名实体识别模型中 采用基于字的命名实体识别方案。Zhang and Yang (2018)提出的Lattice LSTM网络结构效果 较好，其将传统的LSTM单元改进为网格LSTM，在字模型的基础上显性利用词与词序信息， 避免了分词错误传递的问题。Zhou et al. (2017)将中文命名实体识别视为一项联合识别和分类 任务。Wang et al. (2018)提出了一种适用于中文命名实体识别的门控卷积神经网络(GCNN)模 型。在多任务学习方面，Wang et al. (2013)通过将分词和命名实体识别联合学习来融合分词信 息。Peng and Dredze (2015)与He and Sun (2017)在字级别的命名实体识别方案中又融入了词 的信息，将分词信息作为soft feature来增强识别效果。Peng and Dredze (2016)提出了基于分词 任务和中文命名实体识别任务联合训练的模型。 然而，现有的中文命名实体识别方法一般只考虑了单纯将中文命名实体识别与中文分词两 种任务相结合，并且没有在学习过程中考虑标签的一致性信息，这可能只会得到次优的结果。 针对以上问题，本文提出一种新的针对中文命名实体识别的多任务学习框架，融入分词及词性 等词汇信息，同时考虑多任务解码一致性的信息，实现更为有效的多任务学"
2021.ccl-1.52,N16-1028,0,0.0583117,"Missing"
2021.ccl-1.52,P16-1101,0,0.0714434,"Missing"
2021.ccl-1.52,P12-1000,0,0.0650116,"Missing"
2021.ccl-1.52,P16-2025,0,0.0489384,"Missing"
2021.ccl-1.52,P17-1194,0,0.060708,"Missing"
2021.ccl-1.52,W17-2630,0,0.0295765,"Missing"
2021.ccl-1.52,P13-1106,0,0.190973,"考虑命名实体识别连同其他序列标注任务，多任务 学习框架还可以应用到联合提取实体和关系，或将命名实体识别模型分为实体切分和实体类别 预测，进一步提升效果(Crichton et al., 2017; Wang et al., 2018)。 中文命名实体识别 CC L 现有的中文命名实体识别方法包括基于字的命名实体识别、基于词的命名实体识别、基 于字词联合的命名实体识别。Li et al. (2014)通过字级别和词级别统计方法的对比，表明基 于字符的命名实体识别方法一般有更好的表现。Huang et al. (2015)利用双向长短期记忆网 络(BiLSTM)提取特征，将拼写、上下文、词嵌入和地名词典四种类型的特征融于命名实体识 别任务。部分研究者(Lu et al., 2016; Dong et al., 2016)在基于神经网络的命名实体识别模型中 采用基于字的命名实体识别方案。Zhang and Yang (2018)提出的Lattice LSTM网络结构效果 较好，其将传统的LSTM单元改进为网格LSTM，在字模型的基础上显性利用词与词序信息， 避免了分词错误传递的问题。Zhou et al. (2017)将中文命名实体识别视为一项联合识别和分类 任务。Wang et al. (2018)提出了一种适用于中文命名实体识别的门控卷积神经网络(GCNN)模 型。在多任务学习方面，Wang et al. (2013)通过将分词和命名实体识别联合学习来融合分词信 息。Peng and Dredze (2015)与He and Sun (2017)在字级别的命名实体识别方案中又融入了词 的信息，将分词信息作为soft feature来增强识别效果。Peng and Dredze (2016)提出了基于分词 任务和中文命名实体识别任务联合训练的模型。 然而，现有的中文命名实体识别方法一般只考虑了单纯将中文命名实体识别与中文分词两 种任务相结合，并且没有在学习过程中考虑标签的一致性信息，这可能只会得到次优的结果。 针对以上问题，本文提出一种新的针对中文命名实体识别的多任务学习框架，融入分词及词性 等词汇信息，同时考虑多任务解码一致性的信息，实现更为有效的多任务学习过程。 3 方法 图1展示了本文方法的框架，主要包含以下三个部分，下面将对每一模块进行具体介绍。 • NER数 数据集预标 注模块 利用现有的NER数据集进行预标注处理，通过数据预标注融入额 外的与原始数据具有一致性的分词、词性标签，在数据层面即实现一定程度的信息共享， 用于指导后续多任务一致性学习。 第二十届中国计算语言学大会论文集，第576页-第588页，呼和浩特，中国，2021年8月13日至15日。 (c) 2021 中国中文信息学会计算语言学专业委员会 计算语言学 • 多 任 务 共 享 学 习 模 块 基于多任务学习的命名实体识别方"
2021.ccl-1.52,C18-1327,0,0.0296758,"Missing"
2021.ccl-1.52,P18-1144,0,0.0121422,"一个window/sentence方法网络来共同执行NER、POS、组块 分析(Chunk)和语义角色标注(SRL)任务。这种多任务机制让训练算法发现并学习对所有任务都 感兴趣的有用的内部表示。Rei (2017)发现，通过在训练过程中加入无监督语言建模目标，可实 现序列标注模型性能的改进。Lin et al. (2018)提出了一种基于低资源的多语种多任务架构，可 有效转移不同类型的知识改善主模型。除了考虑命名实体识别连同其他序列标注任务，多任务 学习框架还可以应用到联合提取实体和关系，或将命名实体识别模型分为实体切分和实体类别 预测，进一步提升效果(Crichton et al., 2017; Wang et al., 2018)。 中文命名实体识别 CC L 现有的中文命名实体识别方法包括基于字的命名实体识别、基于词的命名实体识别、基 于字词联合的命名实体识别。Li et al. (2014)通过字级别和词级别统计方法的对比，表明基 于字符的命名实体识别方法一般有更好的表现。Huang et al. (2015)利用双向长短期记忆网 络(BiLSTM)提取特征，将拼写、上下文、词嵌入和地名词典四种类型的特征融于命名实体识 别任务。部分研究者(Lu et al., 2016; Dong et al., 2016)在基于神经网络的命名实体识别模型中 采用基于字的命名实体识别方案。Zhang and Yang (2018)提出的Lattice LSTM网络结构效果 较好，其将传统的LSTM单元改进为网格LSTM，在字模型的基础上显性利用词与词序信息， 避免了分词错误传递的问题。Zhou et al. (2017)将中文命名实体识别视为一项联合识别和分类 任务。Wang et al. (2018)提出了一种适用于中文命名实体识别的门控卷积神经网络(GCNN)模 型。在多任务学习方面，Wang et al. (2013)通过将分词和命名实体识别联合学习来融合分词信 息。Peng and Dredze (2015)与He and Sun (2017)在字级别的命名实体识别方案中又融入了词 的信息，将分词信息作为soft feature来增强识别效果。Peng and Dredze (2016)提出了基于分词 任务和中文命名实体识别任务联合训练的模型。 然而，现有的中文命名实体识别方法一般只考虑了单纯将中文命名实体识别与中文分词两 种任务相结合，并且没有在学习过程中考虑标签的一致性信息，这可能只会得到次优的结果。 针对以上问题，本文提出一种新的针对中文命名实体识别的多任务学习框架，融入分词及词性 等词汇信息，同时考虑多任务解码一致性的信息，实现更为有效的多任务学习过程。 3 方法 图1展示了本文方法的框架，主要包含以下三个部分，下面将对每一模块进行具体介绍。 • NER数 数据集预标 注模块 利用现有的NE"
2021.ccl-1.68,P18-1009,0,0.0487086,"Missing"
2021.ccl-1.68,D10-1041,0,0.0738363,"Missing"
2021.ccl-1.68,D12-1058,0,0.0666017,"Missing"
2021.ccl-1.68,C92-2082,0,0.396955,"Missing"
2021.ccl-1.68,W03-1608,0,0.358073,"Missing"
2021.ccl-1.68,N03-1024,0,0.418133,"Missing"
2021.ccl-1.68,D14-1162,0,0.0856298,"Missing"
2021.ccl-1.68,D12-1084,0,0.0857588,"Missing"
2021.ccl-1.68,I05-5011,0,0.202769,"Missing"
2021.ccl-1.68,P06-2096,0,0.177842,"Missing"
2021.ccl-1.68,W16-0109,0,0.0618183,"Missing"
2021.ccl-1.68,2020.emnlp-main.523,0,0.0473212,"Missing"
2021.ccl-1.68,P09-1094,0,0.134181,"Missing"
2021.ccl-1.68,D18-1355,0,0.0441114,"Missing"
2021.emnlp-main.203,P19-1602,0,0.0696387,"r. They use the attentional seq2seq framework to build a parse generator and a paraphrase generator. A two-stage generation process is used. In the first stage, they generate full parse trees from syntactic templates, and then produce final generations in the second stage. Both parse and paraphrase generator require parallel data for training. Significantly different from their method, our model based on conditional VAE is an unsupervised method that does not require any parallel data for training. Conditional Variational Autoencoder Our work is also related to syntax-infused text generation (Bao et al., 2019; Zhang et al., 2019). Their models use two variational autoencoders to introduce two latent variables which are designed to capture semantic and syntactic information. The variational autoencoder (VAE) network is proposed by Kingma and Welling (2014) for image generation. Bowman et al. (2016) successfully apply VAE in fluent sentence generation from a latent space. The conditional VAE is a modification of VAE to generate diverse images conditioned on certain attributes, e.g. generating different human faces given skin color (Sohn et al., 2015; Yan et al., 2016). Inspired by conditional VAE, w"
2021.emnlp-main.203,K16-1002,0,0.620027,"araphrase generator require parallel data for training. Significantly different from their method, our model based on conditional VAE is an unsupervised method that does not require any parallel data for training. Conditional Variational Autoencoder Our work is also related to syntax-infused text generation (Bao et al., 2019; Zhang et al., 2019). Their models use two variational autoencoders to introduce two latent variables which are designed to capture semantic and syntactic information. The variational autoencoder (VAE) network is proposed by Kingma and Welling (2014) for image generation. Bowman et al. (2016) successfully apply VAE in fluent sentence generation from a latent space. The conditional VAE is a modification of VAE to generate diverse images conditioned on certain attributes, e.g. generating different human faces given skin color (Sohn et al., 2015; Yan et al., 2016). Inspired by conditional VAE, we view the syntactic structure as the conditional attribute and adopt conditional VAE to generate syntactic paraphrases. Furthermore, to improve the syntactic controllability and semantic consistency of generated sentences, we use syntax controlling and cycle reconstruction objective functions"
2021.emnlp-main.203,P19-1599,0,0.217606,"g, China 2 College of Intelligence and Computing, Tianjin University, Tianjin, China 3 Lenovo Research AI Lab, Beijing, China {egyang,mingtongliu,yjzhang,jaxu,chenyf}@bjtu.edu.cn, dyxiong@tju.edu.cn, {yaomeng1,hucj1}@lenovo.com Abstract Recent years have witnessed that learning controllable paraphrase generation (CPG) with specPrevious works on syntactically controlled ified styles is attracting intense research interests, paraphrase generation heavily rely on largee.g., satisfying particular syntactic templates (Iyyer scale parallel paraphrase data that are not easet al., 2018) or exemplars (Chen et al., 2019; Kuily available for many languages and domains. mar et al., 2020). As CPG can produce diverse In this paper, we take this research direction to the extreme and investigate whether paraphrases by exposing syntactic control, it can it is possible to learn syntactically controlled be also employed for adversarial example generaparaphrase generation with non-parallel data. tion (Iyyer et al., 2018). We propose a syntactically-informed unsuperExisting syntactically controlled paraphrase netvised paraphrasing model based on conditional works (Iyyer et al., 2018) rely on large paraphrase variationa"
2021.emnlp-main.203,P19-1601,0,0.0794332,"and adopt conditional VAE to generate syntactic paraphrases. Furthermore, to improve the syntactic controllability and semantic consistency of generated sentences, we use syntax controlling and cycle reconstruction objective functions to fine-tune the model. Adversarial Example Generation To generate Controlled Text Generation Recent works on adversarial examples for NLP models, most precontrolled generation aim at controlling attributes vious works rely on injecting noise either at the such as sentiment (Hu et al., 2017; John et al., 2019; character level (Ebrahimi et al., 2018; Gao et al., Dai et al., 2019). These works use a categorical 2018) or at the word level by adding and deleting 2595 Figure 1: Architecture of the proposed syntactically-informed unsupervised paraphrasing model. Stage 1: Training a Conditional VAE model by reconstructing the input sentence given the sentence itself and its syntax structure. Here we simply take x = {x1 , x2 } as an example. Stage 2: Fine-tuning the model using novel objective functions. x, s, s0 (different from s), and y denote the input sentence, its syntactic structure, other syntactic structure, and output sentence, respectively. L∗ denote the loss terms"
2021.emnlp-main.203,D17-1091,0,0.0496685,"Missing"
2021.emnlp-main.203,P18-2006,0,0.0135699,"ucture as the conditional attribute and adopt conditional VAE to generate syntactic paraphrases. Furthermore, to improve the syntactic controllability and semantic consistency of generated sentences, we use syntax controlling and cycle reconstruction objective functions to fine-tune the model. Adversarial Example Generation To generate Controlled Text Generation Recent works on adversarial examples for NLP models, most precontrolled generation aim at controlling attributes vious works rely on injecting noise either at the such as sentiment (Hu et al., 2017; John et al., 2019; character level (Ebrahimi et al., 2018; Gao et al., Dai et al., 2019). These works use a categorical 2018) or at the word level by adding and deleting 2595 Figure 1: Architecture of the proposed syntactically-informed unsupervised paraphrasing model. Stage 1: Training a Conditional VAE model by reconstructing the input sentence given the sentence itself and its syntax structure. Here we simply take x = {x1 , x2 } as an example. Stage 2: Fine-tuning the model using novel objective functions. x, s, s0 (different from s), and y denote the input sentence, its syntactic structure, other syntactic structure, and output sentence, respect"
2021.emnlp-main.203,2020.emnlp-main.498,0,0.0467006,"Missing"
2021.emnlp-main.203,2020.acl-main.22,0,0.016592,"1 and 2). ESM-H denotes the percentage that generations follow given syntactic templates. KL-weight Original 0.1 0.3 0.5 0.7 1.0 1.3 1.5 BLEU-ref 31.13 22.71 22.46 22.37 22.43 22.09 21.87 21.41 BLEU-ori 100 53.89 50.55 48.66 47.07 44.49 41.96 39.29 ESM 56.5 70.8 73.9 75.7 77.1 78.3 79.9 80.7 S-BERT 0.845 0.750 0.738 0.731 0.723 0.707 0.691 0.673 Table 4: BLEU-ref, BLEU-ori, ESM, and S-BERT score with varying KL weights on the Quora test set. 4.1.6 Human Evaluation We also conducted human evaluation to measure paraphrase quality in a blind fashion. Following previous work (Iyyer et al., 2018; Goyal and Durrett, 2020), Three annotators were asked to evaluate the 100 randomly selected generations from the Quora test set according to a three-point scale scoring system: 0 denotes that the generated sentence is not a paraphrase at all; 1 means that the generated sentence is a paraphrase containing grammatical errors; 2 indicates that the generated sentence is a grammatically good paraphrase. Additionally, we also asked annotators to evaluate syntactic controllability (ESM-H): whether generations follow given syntactic templates. Table 3 shows the results of human evaluation which are somewhat consistent with t"
2021.emnlp-main.203,N18-1170,0,0.373524,"g intense research interests, paraphrase generation heavily rely on largee.g., satisfying particular syntactic templates (Iyyer scale parallel paraphrase data that are not easet al., 2018) or exemplars (Chen et al., 2019; Kuily available for many languages and domains. mar et al., 2020). As CPG can produce diverse In this paper, we take this research direction to the extreme and investigate whether paraphrases by exposing syntactic control, it can it is possible to learn syntactically controlled be also employed for adversarial example generaparaphrase generation with non-parallel data. tion (Iyyer et al., 2018). We propose a syntactically-informed unsuperExisting syntactically controlled paraphrase netvised paraphrasing model based on conditional works (Iyyer et al., 2018) rely on large paraphrase variational auto-encoder (VAE) which can genparallel data for training. Unfortunately, paraphrase erate texts in a specified syntactic structure. Particularly, we design a two-stage learning parallel corpora are not easily available for many method to effectively train the model using languages, and are expensive to build. Conversely, non-parallel data. The conditional VAE is non-parallel data is much easi"
2021.emnlp-main.203,P19-1041,0,0.020059,"ional VAE, we view the syntactic structure as the conditional attribute and adopt conditional VAE to generate syntactic paraphrases. Furthermore, to improve the syntactic controllability and semantic consistency of generated sentences, we use syntax controlling and cycle reconstruction objective functions to fine-tune the model. Adversarial Example Generation To generate Controlled Text Generation Recent works on adversarial examples for NLP models, most precontrolled generation aim at controlling attributes vious works rely on injecting noise either at the such as sentiment (Hu et al., 2017; John et al., 2019; character level (Ebrahimi et al., 2018; Gao et al., Dai et al., 2019). These works use a categorical 2018) or at the word level by adding and deleting 2595 Figure 1: Architecture of the proposed syntactically-informed unsupervised paraphrasing model. Stage 1: Training a Conditional VAE model by reconstructing the input sentence given the sentence itself and its syntax structure. Here we simply take x = {x1 , x2 } as an example. Stage 2: Fine-tuning the model using novel objective functions. x, s, s0 (different from s), and y denote the input sentence, its syntactic structure, other syntactic"
2021.emnlp-main.203,2020.coling-main.209,1,0.799898,"encoder to effectively model structure information and a syntax controlling learning objective to further improve syntactic controllability. Meanwhile, we also introduce a cycle reconstruction learning objective to preserve the semantics of the input sentence. • Experiments show that our model can successfully generate syntactically adversarial examples. By augmenting training data with such examples, we can improve the robustness of target neural models. 2 Related Work Paraphrase Generation The task of paraphrase generation has recently received significant attention (Li et al., 2018, 2019; Liu et al., 2020a). Previous works mainly explore supervised paraphrasing methods, which require large corpora of parallel sentences for training. Due to the lack of parallel data, unsupervised paraphrasing has become an emerging research direction (Miao et al., 2018; Liu et al., 2020c). However, these methods mainly rely on lexical changes to generate paraphrases. Compared to these approaches, our work focus primarily on the syntactically controlled paraphrase generation, which is able to generate a paraphrase according to a given syntactic structure. feature as a controlling signal. Different from them, we"
2021.emnlp-main.203,2020.acl-main.28,0,0.033019,"encoder to effectively model structure information and a syntax controlling learning objective to further improve syntactic controllability. Meanwhile, we also introduce a cycle reconstruction learning objective to preserve the semantics of the input sentence. • Experiments show that our model can successfully generate syntactically adversarial examples. By augmenting training data with such examples, we can improve the robustness of target neural models. 2 Related Work Paraphrase Generation The task of paraphrase generation has recently received significant attention (Li et al., 2018, 2019; Liu et al., 2020a). Previous works mainly explore supervised paraphrasing methods, which require large corpora of parallel sentences for training. Due to the lack of parallel data, unsupervised paraphrasing has become an emerging research direction (Miao et al., 2018; Liu et al., 2020c). However, these methods mainly rely on lexical changes to generate paraphrases. Compared to these approaches, our work focus primarily on the syntactically controlled paraphrase generation, which is able to generate a paraphrase according to a given syntactic structure. feature as a controlling signal. Different from them, we"
2021.emnlp-main.203,P14-5010,0,0.00322884,"nce and the reference sentence. For syntactic evaluation, we evaluated how often generated paraphrases completely conform to the target syntactic templates by computing the rate of exact syntactic match (ESM): a paraphrase g is deemed as an exact syntactic match to reference r only if the top three levels of its parse tree pg exactly matches those of pr . The tuning of all hyper-parameter was based on the BLEU-ref score on the validation set. 4.1.4 Implementation Details We parsed all sentences in the training set, the reference sentences in the validation and test set using Stanford CoreNLP (Manning et al., 2014). We used the Adam optimizer (Kingma and Ba, 2014) for optimization. For the training of Stage 1 and Stage 2, we set the learning rate to 5e-4 and 1e-4, respectively. The word embedding layer was initialized by the publicly available GloVe 300-dimensional 3 We used the paraphrase-distilroberta-base-v1, which is trained on large-scale paraphrase data. Available at: https: //public.ukp.informatik.tu-darmstadt.de/ reimers/sentence-transformers/v0.2/ 2599 embeddings.4 We adopted the tricks of KL annealing and word dropout following (Bowman et al., 2016). We set λres to 5, λbow to 0.5, λsc to 2.5,"
2021.emnlp-main.203,P02-1040,0,0.10998,"ference sentences from the ParaNMT-50M dataset to train unsupervised methods. The manually annotated 800 sentence pairs created by Chen et al. (2019) were used as our test set, and 500 for the development set. For the supervised method SCPN (Iyyer et al., 2018), we used their trained model.2 4.1.3 Evaluation Metrics We employed original sentences and syntactic templates (or full parse trees) obtained from references as input, which is convenient for evaluation. But in the application scenario, we can give any syntactic templates to the trained model. For semantic evaluation, we computed BLEU (Papineni et al., 2002) scores against the reference and original sentence, denoted as BLEU-ref and BLEU-ori, respectively. Addition2 https://github.com/miyyer/scpn ally, we used i-BLEU (Sun and Zhou, 2012) to measure the diversity of expressions. We also used the embedding-based evaluation method Sentence-BERT3 (Reimers and Gurevych, 2019) to evaluate the semantic similarity between the generated sentence and the reference sentence. For syntactic evaluation, we evaluated how often generated paraphrases completely conform to the target syntactic templates by computing the rate of exact syntactic match (ESM): a parap"
2021.emnlp-main.203,2020.tacl-1.22,0,0.0311238,"Missing"
2021.emnlp-main.203,D19-1410,0,0.0153167,"ics We employed original sentences and syntactic templates (or full parse trees) obtained from references as input, which is convenient for evaluation. But in the application scenario, we can give any syntactic templates to the trained model. For semantic evaluation, we computed BLEU (Papineni et al., 2002) scores against the reference and original sentence, denoted as BLEU-ref and BLEU-ori, respectively. Addition2 https://github.com/miyyer/scpn ally, we used i-BLEU (Sun and Zhou, 2012) to measure the diversity of expressions. We also used the embedding-based evaluation method Sentence-BERT3 (Reimers and Gurevych, 2019) to evaluate the semantic similarity between the generated sentence and the reference sentence. For syntactic evaluation, we evaluated how often generated paraphrases completely conform to the target syntactic templates by computing the rate of exact syntactic match (ESM): a paraphrase g is deemed as an exact syntactic match to reference r only if the top three levels of its parse tree pg exactly matches those of pr . The tuning of all hyper-parameter was based on the BLEU-ref score on the validation set. 4.1.4 Implementation Details We parsed all sentences in the training set, the reference s"
2021.emnlp-main.203,D18-1421,0,0.0181783,"we propose a novel tree encoder to effectively model structure information and a syntax controlling learning objective to further improve syntactic controllability. Meanwhile, we also introduce a cycle reconstruction learning objective to preserve the semantics of the input sentence. • Experiments show that our model can successfully generate syntactically adversarial examples. By augmenting training data with such examples, we can improve the robustness of target neural models. 2 Related Work Paraphrase Generation The task of paraphrase generation has recently received significant attention (Li et al., 2018, 2019; Liu et al., 2020a). Previous works mainly explore supervised paraphrasing methods, which require large corpora of parallel sentences for training. Due to the lack of parallel data, unsupervised paraphrasing has become an emerging research direction (Miao et al., 2018; Liu et al., 2020c). However, these methods mainly rely on lexical changes to generate paraphrases. Compared to these approaches, our work focus primarily on the syntactically controlled paraphrase generation, which is able to generate a paraphrase according to a given syntactic structure. feature as a controlling signal."
2021.emnlp-main.203,P17-1099,0,0.123846,"Missing"
2021.emnlp-main.203,P19-1332,0,0.0283947,"Missing"
2021.emnlp-main.203,D13-1170,0,0.0036219,". contradictory to each other. Usually, a smaller KL weight makes the autoencoder less “variational” but more “deterministic,” leading to a lower syntactic match but better content preservation. In this experiment, to trade-off the content preservation and syntactic controllability, we set the KL weight to 0.3. 4.2 Adversarial Example Generation We further examined the utility of controlled paraphrase generation for adversarial example generation. Following previous work (Iyyer et al., 2018), we evaluated our syntactically adversarial examples on the Stanford Sentiment Treebank Dataset (SST) (Socher et al., 2013). We generated 10 syntactically different paraphrases for each instance using the top 10 frequent syntactic templates and add them to the SST training set. Since we cannot generate a valid paraphrase for each syntactic template, we filtered generated paraphrases using a threshold (BLEU, 1-3gram) to remove nonsensical outputs. In this experiment, we set the threshold to 0.5. 4.2.1 Evaluation Metrics We evaluated this task with the following metrics: 1. Dev Failure (Failure). We assume a development instance x as a prediction failure if the original prediction is correct, but the prediction for"
2021.emnlp-main.203,P12-2008,0,0.0164569,"or the development set. For the supervised method SCPN (Iyyer et al., 2018), we used their trained model.2 4.1.3 Evaluation Metrics We employed original sentences and syntactic templates (or full parse trees) obtained from references as input, which is convenient for evaluation. But in the application scenario, we can give any syntactic templates to the trained model. For semantic evaluation, we computed BLEU (Papineni et al., 2002) scores against the reference and original sentence, denoted as BLEU-ref and BLEU-ori, respectively. Addition2 https://github.com/miyyer/scpn ally, we used i-BLEU (Sun and Zhou, 2012) to measure the diversity of expressions. We also used the embedding-based evaluation method Sentence-BERT3 (Reimers and Gurevych, 2019) to evaluate the semantic similarity between the generated sentence and the reference sentence. For syntactic evaluation, we evaluated how often generated paraphrases completely conform to the target syntactic templates by computing the rate of exact syntactic match (ESM): a paraphrase g is deemed as an exact syntactic match to reference r only if the top three levels of its parse tree pg exactly matches those of pr . The tuning of all hyper-parameter was base"
2021.emnlp-main.203,P15-1150,0,0.0609342,"Missing"
2021.emnlp-main.203,P19-1199,0,0.0582223,"17), machine translation (Zhou et al., 2019) input sentence and a different syntactic structure, and text summarization (Zhao et al., 2018). the model can generate a paraphrase according to ∗ Corresponding Authors. the given structure. 2594 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 2594–2604 c November 7–11, 2021. 2021 Association for Computational Linguistics We evaluate SUP on both syntactic paraphrase generation and adversarial example generation tasks. Experiments show that SUP outperforms previous unsupervised paraphrasing method SIVAE (Zhang et al., 2019). It is also capable of generating syntactically adversarial examples that have a significant impact on the performance of attacked neural models. We further show that augmenting training data with such examples can improve the robustness of target neural models. In summary, the major contributions of this paper are as follows: • We propose a syntactically-informed unsupervised paraphrasing model based on conditional VAE framework and use it to generate syntactically adversarial examples. • To enable the model to generate syntacticallycontrolled paraphrases, we propose a novel tree encoder to"
2021.emnlp-main.203,D18-1355,0,0.0241989,"rent surface realization. and semantic consistency of generated sentences, Paraphrase generation (PG) is a key technology of we fine-tune the model trained at stage 1 using automatically generating a restatement for a given carefully-designed objective functions involving text, which has the potential use in many down- syntax controlling and cycle reconstruction. After stream tasks, such as question answering (Dong the conditional VAE model is fine-tuned, given an et al., 2017), machine translation (Zhou et al., 2019) input sentence and a different syntactic structure, and text summarization (Zhao et al., 2018). the model can generate a paraphrase according to ∗ Corresponding Authors. the given structure. 2594 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 2594–2604 c November 7–11, 2021. 2021 Association for Computational Linguistics We evaluate SUP on both syntactic paraphrase generation and adversarial example generation tasks. Experiments show that SUP outperforms previous unsupervised paraphrasing method SIVAE (Zhang et al., 2019). It is also capable of generating syntactically adversarial examples that have a significant impact on the performance"
2021.emnlp-main.203,P19-2015,0,0.0219605,"conveying the At stage 2, to improve the syntactic controllability same meaning but with different surface realization. and semantic consistency of generated sentences, Paraphrase generation (PG) is a key technology of we fine-tune the model trained at stage 1 using automatically generating a restatement for a given carefully-designed objective functions involving text, which has the potential use in many down- syntax controlling and cycle reconstruction. After stream tasks, such as question answering (Dong the conditional VAE model is fine-tuned, given an et al., 2017), machine translation (Zhou et al., 2019) input sentence and a different syntactic structure, and text summarization (Zhao et al., 2018). the model can generate a paraphrase according to ∗ Corresponding Authors. the given structure. 2594 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 2594–2604 c November 7–11, 2021. 2021 Association for Computational Linguistics We evaluate SUP on both syntactic paraphrase generation and adversarial example generation tasks. Experiments show that SUP outperforms previous unsupervised paraphrasing method SIVAE (Zhang et al., 2019). It is also capable of g"
C02-1056,1993.tmi-1.20,0,0.0631249,"lly says it again using other words, i.e., he paraphrases. In a Chinese-Japanese spoken language translation system, the pre-processing of Chinese utterances is involved and we attempt to apply a paraphrasing approach. This paper ∗ This work was done when the author stayed at ATR Spoken Language Translation Research Laboratories. is focused on the paraphrasing of Chinese utterances. Some cases of paraphrasing research with certain targets have been reported. For example, there has been work on rewriting the source language in machine translation with a focus on reducing syntactic ambiguities (Shirai et al., 1993), research on paraphrasing paper titles with a focus on transforming syntactic structures to achieve readability (Sato, 1999), and research on paraphrasing Japanese in summarization with a focus on transforming a noun modiﬁer into a noun phrase (Kataoka et al., 1999). We have reported some research on Chinese paraphrasing (Zhang and Yamamoto, 2001; Zhang et al., 2001; Zong et al., 2001). The techniques of paraphrasing natural language can be applied not only to the pre-processing of machine translation but also to information retrieval and summarization. 2 Goals and Approach In the pre-process"
C02-1056,C02-1163,1,0.849824,"In addition, a pattern construction method is described through which paraphrasing patterns can be eﬃciently learned from a paraphrase corpus and human experience. Using the implemented paraphraser and the obtained patterns, a paraphrasing experiment was conducted and the results were evaluated. 1 Introduction In spoken language translation one of the key issues is how to deal with unrestricted expressions in spontaneous utterances. To resolve this problem, we have proposed a paraphrasing approach in which the utterances are automatically paraphrased prior to transfer (Yamamoto et al., 2001; Yamamoto, 2002). The paraphrasing process aims to bridge the gap between the unrestricted expressions in the input and the limited expressions that the transfer can translate. In fact, paraphrasing actions are often seen in daily communication. When a listener cannot understand what a speaker said, the speaker usually says it again using other words, i.e., he paraphrases. In a Chinese-Japanese spoken language translation system, the pre-processing of Chinese utterances is involved and we attempt to apply a paraphrasing approach. This paper ∗ This work was done when the author stayed at ATR Spoken Language Tr"
D07-1122,P06-2013,1,0.896796,"Missing"
D07-1122,W07-2416,0,0.0297342,"stage. For four languages with different values of ROOT, we design some special features for the ROOT labeler. Then we present evaluation results and error analyses focusing on Chinese. 1 2 Two-Stage Parsing 2.1 The Unlabeled Parser Introduction The CoNLL-2007 shared tasks include two tracks: the Multilingual Track and Domain Adaptation Track(Nivre et al., 2007). We took part the Multilingual Track of all ten languages provided by the CoNLL-2007 shared task organizers(Hajiˇc et al., 2004; Aduriz et al., 2003; Mart´ı et al., 2007; Chen et al., 2003; B¨ohmov´a et al., 2003; Marcus et al., 1993; Johansson and Nugues, 2007; Prokopidis et al., 2005; Csendes et al., 2005; Montemagni et al., 2003; Oﬂazer et al., 2003) . In this paper, we describe a two-stage parsing system consisting of an unlabeled parser and a sequence labeler, which was submitted to the Multilingual Track. At the ﬁrst stage, we use the parsing model proposed by (Nivre, 2003) to assign the arcs between the words. Then we obtain a dependency parsing tree based on the arcs. At the second stage, we use a SVM-based approach(Kudo and The unlabeled parser predicts unlabeled directed dependencies. This parser is primarily based on the parsing models de"
D07-1122,N01-1025,0,0.0441799,"duced the dependency tree y at the ﬁrst stage. In this stage, we assign a label l(i,j) to each pair. As described in (McDonald et al., 2006), we treat the labeling of dependencies as a sequence labeling problem. Suppose that we consider a head xi with dependents xj1 , ..., xjM . We then consider the labels of (i, j1), ..., (i, jM ) as a sequence. We use the model to ﬁnd the solution: lmax = arg max s(l, i, y, x) l (1) And we consider a ﬁrst-order Markov chain of labels. We used the package YamCha (V0.33)2 to implement the SVM model for labeling. YamCha is a powerful tool for sequence labeling(Kudo and Matsumoto, 2001). 2.2.2 Features for Labeling After the ﬁrst stage, we know the unlabeled dependency parsing tree for the input sentence. This information forms the basis for part of the features of the second stage. For the sequence labeler, we deﬁne the individual features, the pair features, the verb features, the neighbor features, and the position features. All the features are listed as follows: • The individual features: the FORM, the LEMMA, the CPOSTAG, the POSTAG, and the FEATS of the parent and child node. • The pair features: the direction of dependency, the combination of lemmata of the parent and"
D07-1122,W06-2932,0,0.02086,"AG of TOP and NEXT, the POSTAG of next three tokens after NEXT, the POSTAG of the token immediately before NEXT in original input string, the POSTAG of the token immediately below TOP, and the POSTAG of the token immediately after rightmost dependent of TOP. • The FEATS features: the FEATS of TOP and NEXT. But note that the ﬁelds LEMMA and FEATS are not available for all languages. with a set of ordered pairs (i, j) ∈ y in which xj is a dependent and xi is the head. We have produced the dependency tree y at the ﬁrst stage. In this stage, we assign a label l(i,j) to each pair. As described in (McDonald et al., 2006), we treat the labeling of dependencies as a sequence labeling problem. Suppose that we consider a head xi with dependents xj1 , ..., xjM . We then consider the labels of (i, j1), ..., (i, jM ) as a sequence. We use the model to ﬁnd the solution: lmax = arg max s(l, i, y, x) l (1) And we consider a ﬁrst-order Markov chain of labels. We used the package YamCha (V0.33)2 to implement the SVM model for labeling. YamCha is a powerful tool for sequence labeling(Kudo and Matsumoto, 2001). 2.2.2 Features for Labeling After the ﬁrst stage, we know the unlabeled dependency parsing tree for the input sen"
D07-1122,W06-2933,0,0.0231911,"cies. This parser is primarily based on the parsing models described by (Nivre, 2003). The algorithm makes a dependency parsing tree in one leftto-right pass over the input, and uses a stack to store the processed tokens. The behaviors of the parser are deﬁned by four elementary actions (where TOP is the token on top of the stack and NEXT is the next token in the original input string): • Left-Arc(LA): Add an arc from NEXT to TOP; pop the stack. • Right-Arc(RA): Add an arc from TOP to NEXT; push NEXT onto the stack. • Reduce(RE): Pop the stack. • Shift(SH): Push NEXT onto the stack. Although (Nivre et al., 2006) used the pseudoprojective approach to process non-projective dependencies, here we only derive projective dependency tree. We use MaltParser(Nivre et al., 2006) 1129 Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007, pp. 1129–1133, c Prague, June 2007. 2007 Association for Computational Linguistics V0.41 to implement the unlabeled parser, and use the SVM model as the classiﬁer. More speciﬁcally, the MaltParser use LIBSVM(Chang and Lin, 2001) with a quadratic kernel and the built-in one-versusall strategy for multi-class classiﬁcation. 2.1.1 Features for Parsing The MaltParser i"
D07-1122,W03-3017,0,0.0404162,"Track(Nivre et al., 2007). We took part the Multilingual Track of all ten languages provided by the CoNLL-2007 shared task organizers(Hajiˇc et al., 2004; Aduriz et al., 2003; Mart´ı et al., 2007; Chen et al., 2003; B¨ohmov´a et al., 2003; Marcus et al., 1993; Johansson and Nugues, 2007; Prokopidis et al., 2005; Csendes et al., 2005; Montemagni et al., 2003; Oﬂazer et al., 2003) . In this paper, we describe a two-stage parsing system consisting of an unlabeled parser and a sequence labeler, which was submitted to the Multilingual Track. At the ﬁrst stage, we use the parsing model proposed by (Nivre, 2003) to assign the arcs between the words. Then we obtain a dependency parsing tree based on the arcs. At the second stage, we use a SVM-based approach(Kudo and The unlabeled parser predicts unlabeled directed dependencies. This parser is primarily based on the parsing models described by (Nivre, 2003). The algorithm makes a dependency parsing tree in one leftto-right pass over the input, and uses a stack to store the processed tokens. The behaviors of the parser are deﬁned by four elementary actions (where TOP is the token on top of the stack and NEXT is the next token in the original input strin"
D07-1122,J93-2004,0,\N,Missing
D07-1122,D07-1096,0,\N,Missing
D11-1007,D08-1092,0,0.318764,"design a set of effective bilingual features for parsing models based on the verified results. The experimental results show that our new parsers significantly outperform state-of-theart baselines. Moreover, our approach is still able to provide improvement when we use a larger monolingual treebank that results in a much stronger baseline. Especially notable is that our approach can be used in a purely monolingual setting with the help of SMT. 1 Introduction Recently there have been several studies aiming to improve the performance of parsing bilingual texts (bitexts) (Smith and Smith, 2004; Burkett and Klein, 2008; Huang et al., 2009; Zhao et al., 2009; Chen et al., 2010). In bitext parsing, we can use the information based on “bilingual constraints” (Burkett and Klein, 2008), which do not exist in monolingual sentences. More accurate bitext parsing results can be effectively used in the training of syntax-based machine translation systems (Liu and Huang, 2010). Most previous studies rely on bilingual treebanks to provide bilingual constraints for bitext parsing. Burkett and Klein (2008) proposed joint models on bitexts to improve the performance on either or both sides. Their method uses bilingual tre"
D11-1007,D07-1101,0,0.144195,"sponding to “技巧(jiqiao)/skill” is a grandchild of the word “play” corresponding to “发挥(fahui)/demonstrate”. This is a positive evidence for supporting “发 挥(fahui)/demonstrate” as being the head of “技 巧(jiqiao)/skill”. From this example, although the sentences and parse trees on the target side are not perfect, we still can explore useful information to improve bitext parsing. In this paper, we focus on how to design a method to verify such unreliable bilingual constraints. 3 Parsing model In this paper, we implement our approach based on graph-based parsing models (McDonald and Pereira, 2006; Carreras, 2007). Note that our approach can also be applied to transition-based parsing models (Nivre, 2003; Yamada and Matsumoto, 2003). The graph-based parsing model is to search for the maximum spanning tree (MST) in a graph (McDonald and Pereira, 2006). The formulation defines the score of a dependency tree to be the sum of edge scores, s(x, y) = X g∈y score(w, x, g) = X g∈y 4.2 Bilingual constraint functions w ·f (x, g) (1) where x is an input sentence, y is a dependency tree for x, and g is a spanning subgraph of y. f (x, g) can be based on arbitrary features of the subgraph and the input sequence x an"
D11-1007,D09-1060,1,0.927841,"list of the target monolingual subtrees 1 For the second order features, Dir is the combination of the directions of two dependencies. or bilingual subtrees, this constraint will probably be reliable. We first parse the large-scale unannotated monolingual and bilingual data. Subsequently, we extract the monolingual and bilingual subtrees from the parsed data. We then verify the bilingual constraints using the extracted subtrees. Finally, we generate the bilingual features based on the verified results for the parsing models. 5.1 Verified constraint functions 5.1.1 Monolingual target subtrees Chen et al. (2009) proposed a simple method to extract subtrees from large-scale monolingual data and used them as features to improve monolingual parsing. Following their method, we parse large unannotated data with the Parsert and obtain the subtree list (STt ) on the target side. We extract two types of subtrees: bigram (two words) subtree and trigram (three words) subtree. 5.1.2 Verified target constraint function: Fvt (rtk ) We use the extracted target subtrees to verify the rtk of the bilingual constraints. In fact, rtk is a candidate subtree. If the rtk is included in STt , function Fvt (rtk ) = T ype(rt"
D11-1007,P10-1003,1,0.243719,"based on the verified results. The experimental results show that our new parsers significantly outperform state-of-theart baselines. Moreover, our approach is still able to provide improvement when we use a larger monolingual treebank that results in a much stronger baseline. Especially notable is that our approach can be used in a purely monolingual setting with the help of SMT. 1 Introduction Recently there have been several studies aiming to improve the performance of parsing bilingual texts (bitexts) (Smith and Smith, 2004; Burkett and Klein, 2008; Huang et al., 2009; Zhao et al., 2009; Chen et al., 2010). In bitext parsing, we can use the information based on “bilingual constraints” (Burkett and Klein, 2008), which do not exist in monolingual sentences. More accurate bitext parsing results can be effectively used in the training of syntax-based machine translation systems (Liu and Huang, 2010). Most previous studies rely on bilingual treebanks to provide bilingual constraints for bitext parsing. Burkett and Klein (2008) proposed joint models on bitexts to improve the performance on either or both sides. Their method uses bilingual treebanks that have human-annotated tree structures on both si"
D11-1007,P07-1003,0,0.126522,"ale unannotated data. 4.1 Auto-generated bilingual treebank Assuming that we have monolingual treebanks on the source side, an SMT system that can translate the source sentences into the target language, and a Parsert trained on the target monolingual treebank. We first translate the sentences of the source monolingual treebank into the target language using the SMT system. Usually, SMT systems can output the word alignment links directly. If they can not, we perform word alignment using some publicly available tools, such as Giza++ (Och and Ney, 2003) or Berkeley Aligner (Liang et al., 2006; DeNero and Klein, 2007). The translated sentences are parsed by the Parsert . Then, we have a newly auto-generated bilingual treebank. 76 In this paper, we focus on the first- and secondorder graph models (McDonald and Pereira, 2006; Carreras, 2007). Thus we produce the constraints for bigram (a single edge) and trigram (adjacent edges) dependencies in the graph model. For the trigram dependencies, we consider the parent-sibling and parent-child-grandchild structures described in McDonald and Pereira (2006) and Carreras (2007). We leave the third-order models (Koo and Collins, 2010) for a future study. Suppose that"
D11-1007,D09-1127,0,0.0657088,"Missing"
D11-1007,N03-1017,0,0.0160094,"Missing"
D11-1007,P10-1001,0,0.0121108,"ley Aligner (Liang et al., 2006; DeNero and Klein, 2007). The translated sentences are parsed by the Parsert . Then, we have a newly auto-generated bilingual treebank. 76 In this paper, we focus on the first- and secondorder graph models (McDonald and Pereira, 2006; Carreras, 2007). Thus we produce the constraints for bigram (a single edge) and trigram (adjacent edges) dependencies in the graph model. For the trigram dependencies, we consider the parent-sibling and parent-child-grandchild structures described in McDonald and Pereira (2006) and Carreras (2007). We leave the third-order models (Koo and Collins, 2010) for a future study. Suppose that we have a (candidate) dependency relation rs that can be a bigram or trigram dependency. We examine whether the corresponding words of the source words of rs have a dependency relation rt in the target trees. We also consider the direction of the dependency relation. The corresponding word of the head should also be the head in rt . We define a binary function for this bilingual constraint: Fbn (rsn : rtk ), where n and k refers to the types of the dependencies (2 for bigram and 3 for trigram). For example, in rs2 : rt3 , rs2 is a bigram dependency on the sour"
D11-1007,P09-1058,1,0.820133,"//www.itl.nist.gov/iad/mig//tests/mt/2008/ 4 data. To extract English subtrees, we used the BLLIP corpus (Charniak et al., 2000) that contains about 43 million words of WSJ texts. We used the MXPOST tagger (Ratnaparkhi, 1996) trained on training data to assign POS tags and used the first-order Parsert to process the sentences of the BLLIP corpus. To extract bilingual subtrees, we used the FBIS corpus and an additional bilingual corpus containing 800,000 sentence pairs from the training data of NIST MT08 evaluation campaign. On the Chinese side, we used the morphological analyzer described in (Kruengkrai et al., 2009) trained on the training data of CTBtp to perform word segmentation and POS tagging and used the first-order Parsers to parse all the sentences in the data. On the English side, we used the same procedure as we did for the BLLIP corpus. Word alignment was performed using the Berkeley Aligner. We reported the parser quality by the UAS, i.e., the percentage of tokens (excluding all punctuation tokens) with correct HEADs. 6.1 Experimental settings For baseline systems, we used the monolingual features mentioned in Section 3. We called these features basic features. To compare the results of (Burk"
D11-1007,N06-1014,0,0.079112,"ed by using large-scale unannotated data. 4.1 Auto-generated bilingual treebank Assuming that we have monolingual treebanks on the source side, an SMT system that can translate the source sentences into the target language, and a Parsert trained on the target monolingual treebank. We first translate the sentences of the source monolingual treebank into the target language using the SMT system. Usually, SMT systems can output the word alignment links directly. If they can not, we perform word alignment using some publicly available tools, such as Giza++ (Och and Ney, 2003) or Berkeley Aligner (Liang et al., 2006; DeNero and Klein, 2007). The translated sentences are parsed by the Parsert . Then, we have a newly auto-generated bilingual treebank. 76 In this paper, we focus on the first- and secondorder graph models (McDonald and Pereira, 2006; Carreras, 2007). Thus we produce the constraints for bigram (a single edge) and trigram (adjacent edges) dependencies in the graph model. For the trigram dependencies, we consider the parent-sibling and parent-child-grandchild structures described in McDonald and Pereira (2006) and Carreras (2007). We leave the third-order models (Koo and Collins, 2010) for a fu"
D11-1007,P10-5002,0,0.0237031,"able is that our approach can be used in a purely monolingual setting with the help of SMT. 1 Introduction Recently there have been several studies aiming to improve the performance of parsing bilingual texts (bitexts) (Smith and Smith, 2004; Burkett and Klein, 2008; Huang et al., 2009; Zhao et al., 2009; Chen et al., 2010). In bitext parsing, we can use the information based on “bilingual constraints” (Burkett and Klein, 2008), which do not exist in monolingual sentences. More accurate bitext parsing results can be effectively used in the training of syntax-based machine translation systems (Liu and Huang, 2010). Most previous studies rely on bilingual treebanks to provide bilingual constraints for bitext parsing. Burkett and Klein (2008) proposed joint models on bitexts to improve the performance on either or both sides. Their method uses bilingual treebanks that have human-annotated tree structures on both sides. Huang et al. (2009) presented a method to train a source-language parser by using the reordering information on words between the sentences on two sides. It uses another type of bilingual treebanks that have tree structures on the source sentences and their human-translated sentences. Chen"
D11-1007,J93-2004,0,0.0430468,"ingual treebanks that have tree structures on the source sentences and their human-translated sentences. Chen et al. (2010) also used bilingual treebanks and made use of tree structures on the target side. However, the bilingual treebanks are hard to obtain, partly because of the high cost of human translation. Thus, in their experiments, they applied their methods to a small data set, the manually translated portion of the Chinese Treebank (CTB) which contains only about 3,000 sentences. On the other hand, many large-scale monolingual treebanks exist, such as the Penn English Treebank (PTB) (Marcus et al., 1993) (about 40,000 sentences in Version 3) and the latest version of CTB (over 50,000 sentences in Version 7). In this paper, we propose a bitext parsing approach in which we produce the bilingual constraints on existing monolingual treebanks with the help of SMT systems. In other words, we aim to improve source-language parsing with the help of automatic translations. In our approach, we first use an SMT system to translate the sentences of a source monolingual treebank into the target language. Then, the target sentences are parsed by a parser trained on a target monolingual treebank. We then ob"
D11-1007,E06-1011,0,0.17547,"ure, the word “skills” corresponding to “技巧(jiqiao)/skill” is a grandchild of the word “play” corresponding to “发挥(fahui)/demonstrate”. This is a positive evidence for supporting “发 挥(fahui)/demonstrate” as being the head of “技 巧(jiqiao)/skill”. From this example, although the sentences and parse trees on the target side are not perfect, we still can explore useful information to improve bitext parsing. In this paper, we focus on how to design a method to verify such unreliable bilingual constraints. 3 Parsing model In this paper, we implement our approach based on graph-based parsing models (McDonald and Pereira, 2006; Carreras, 2007). Note that our approach can also be applied to transition-based parsing models (Nivre, 2003; Yamada and Matsumoto, 2003). The graph-based parsing model is to search for the maximum spanning tree (MST) in a graph (McDonald and Pereira, 2006). The formulation defines the score of a dependency tree to be the sum of edge scores, s(x, y) = X g∈y score(w, x, g) = X g∈y 4.2 Bilingual constraint functions w ·f (x, g) (1) where x is an input sentence, y is a dependency tree for x, and g is a spanning subgraph of y. f (x, g) can be based on arbitrary features of the subgraph and the in"
D11-1007,W03-3017,0,0.0373708,"monstrate”. This is a positive evidence for supporting “发 挥(fahui)/demonstrate” as being the head of “技 巧(jiqiao)/skill”. From this example, although the sentences and parse trees on the target side are not perfect, we still can explore useful information to improve bitext parsing. In this paper, we focus on how to design a method to verify such unreliable bilingual constraints. 3 Parsing model In this paper, we implement our approach based on graph-based parsing models (McDonald and Pereira, 2006; Carreras, 2007). Note that our approach can also be applied to transition-based parsing models (Nivre, 2003; Yamada and Matsumoto, 2003). The graph-based parsing model is to search for the maximum spanning tree (MST) in a graph (McDonald and Pereira, 2006). The formulation defines the score of a dependency tree to be the sum of edge scores, s(x, y) = X g∈y score(w, x, g) = X g∈y 4.2 Bilingual constraint functions w ·f (x, g) (1) where x is an input sentence, y is a dependency tree for x, and g is a spanning subgraph of y. f (x, g) can be based on arbitrary features of the subgraph and the input sequence x and the feature weight vector w are the parameters to be learned by using MIRA (Crammer and Si"
D11-1007,J03-1002,0,0.00306085,"ased on the bilingual constraints verified by using large-scale unannotated data. 4.1 Auto-generated bilingual treebank Assuming that we have monolingual treebanks on the source side, an SMT system that can translate the source sentences into the target language, and a Parsert trained on the target monolingual treebank. We first translate the sentences of the source monolingual treebank into the target language using the SMT system. Usually, SMT systems can output the word alignment links directly. If they can not, we perform word alignment using some publicly available tools, such as Giza++ (Och and Ney, 2003) or Berkeley Aligner (Liang et al., 2006; DeNero and Klein, 2007). The translated sentences are parsed by the Parsert . Then, we have a newly auto-generated bilingual treebank. 76 In this paper, we focus on the first- and secondorder graph models (McDonald and Pereira, 2006; Carreras, 2007). Thus we produce the constraints for bigram (a single edge) and trigram (adjacent edges) dependencies in the graph model. For the trigram dependencies, we consider the parent-sibling and parent-child-grandchild structures described in McDonald and Pereira (2006) and Carreras (2007). We leave the third-order"
D11-1007,W96-0213,0,0.0608525,"e trained first-order and second-order Parsert on the training data. The unlabeled attachment score (UAS) of the second-order Parsert was 91.92, indicating state-of-the-art accuracy on the test data. We used the second-order Parsert to parse the autotranslated/human-made target sentences in the CTB 3 http://www.statmt.org/moses/ http://www.speech.sri.com/projects/srilm/download.html 5 http://www.itl.nist.gov/iad/mig//tests/mt/2008/ 4 data. To extract English subtrees, we used the BLLIP corpus (Charniak et al., 2000) that contains about 43 million words of WSJ texts. We used the MXPOST tagger (Ratnaparkhi, 1996) trained on training data to assign POS tags and used the first-order Parsert to process the sentences of the BLLIP corpus. To extract bilingual subtrees, we used the FBIS corpus and an additional bilingual corpus containing 800,000 sentence pairs from the training data of NIST MT08 evaluation campaign. On the Chinese side, we used the morphological analyzer described in (Kruengkrai et al., 2009) trained on the training data of CTBtp to perform word segmentation and POS tagging and used the first-order Parsers to parse all the sentences in the data. On the English side, we used the same proced"
D11-1007,W04-3207,0,0.0134497,"ify the constraints and design a set of effective bilingual features for parsing models based on the verified results. The experimental results show that our new parsers significantly outperform state-of-theart baselines. Moreover, our approach is still able to provide improvement when we use a larger monolingual treebank that results in a much stronger baseline. Especially notable is that our approach can be used in a purely monolingual setting with the help of SMT. 1 Introduction Recently there have been several studies aiming to improve the performance of parsing bilingual texts (bitexts) (Smith and Smith, 2004; Burkett and Klein, 2008; Huang et al., 2009; Zhao et al., 2009; Chen et al., 2010). In bitext parsing, we can use the information based on “bilingual constraints” (Burkett and Klein, 2008), which do not exist in monolingual sentences. More accurate bitext parsing results can be effectively used in the training of syntax-based machine translation systems (Liu and Huang, 2010). Most previous studies rely on bilingual treebanks to provide bilingual constraints for bitext parsing. Burkett and Klein (2008) proposed joint models on bitexts to improve the performance on either or both sides. Their"
D11-1007,W03-3023,0,0.0472776,"his is a positive evidence for supporting “发 挥(fahui)/demonstrate” as being the head of “技 巧(jiqiao)/skill”. From this example, although the sentences and parse trees on the target side are not perfect, we still can explore useful information to improve bitext parsing. In this paper, we focus on how to design a method to verify such unreliable bilingual constraints. 3 Parsing model In this paper, we implement our approach based on graph-based parsing models (McDonald and Pereira, 2006; Carreras, 2007). Note that our approach can also be applied to transition-based parsing models (Nivre, 2003; Yamada and Matsumoto, 2003). The graph-based parsing model is to search for the maximum spanning tree (MST) in a graph (McDonald and Pereira, 2006). The formulation defines the score of a dependency tree to be the sum of edge scores, s(x, y) = X g∈y score(w, x, g) = X g∈y 4.2 Bilingual constraint functions w ·f (x, g) (1) where x is an input sentence, y is a dependency tree for x, and g is a spanning subgraph of y. f (x, g) can be based on arbitrary features of the subgraph and the input sequence x and the feature weight vector w are the parameters to be learned by using MIRA (Crammer and Singer, 2003) during training."
D11-1007,P09-1007,0,0.0415887,"for parsing models based on the verified results. The experimental results show that our new parsers significantly outperform state-of-theart baselines. Moreover, our approach is still able to provide improvement when we use a larger monolingual treebank that results in a much stronger baseline. Especially notable is that our approach can be used in a purely monolingual setting with the help of SMT. 1 Introduction Recently there have been several studies aiming to improve the performance of parsing bilingual texts (bitexts) (Smith and Smith, 2004; Burkett and Klein, 2008; Huang et al., 2009; Zhao et al., 2009; Chen et al., 2010). In bitext parsing, we can use the information based on “bilingual constraints” (Burkett and Klein, 2008), which do not exist in monolingual sentences. More accurate bitext parsing results can be effectively used in the training of syntax-based machine translation systems (Liu and Huang, 2010). Most previous studies rely on bilingual treebanks to provide bilingual constraints for bitext parsing. Burkett and Klein (2008) proposed joint models on bitexts to improve the performance on either or both sides. Their method uses bilingual treebanks that have human-annotated tree s"
D19-1267,D15-1075,0,0.606858,"xt to enhance attention-aware representation within each sentence. Experiment results on three sentence matching benchmark datasets SNLI, SciTail and Quora show that OSOA-DFN has the ability to model sentence matching more precisely. 1 Introduction Natural language sentence matching is a key technique of comparing two sentences and identifying the semantic relationship between them, which is usually viewed as a classification problem (Wang et al., 2017). The technique has applications in natural language inference to judge whether a hypothesis sentence can be inferred from a premise sentence (Bowman et al., 2015) and in paraphrase identification to determine whether two sentences express the equivalent meaning or not (Yin et al., 2015). The core issue for sentence matching is to model the relatedness between two sentences (Rockt¨aschel et al., 2015; Parikh et al., 2016; Wang et al., 2017; Duan et al., 2018). Recently, neural network-based models for sentence matching have attracted more attention for their powerful ability to learn sentence representation (Bowman et al., 2015; Wang et al., 2017; Duan et al., 2018). There are mainly two types of frameworks: sentence encoding based framework and attenti"
D19-1267,P17-1152,0,0.527609,"is (Q) Premise (P) (a) The General Model ℎ ?? ? Q(1) –&gt; P Attention 1 ??0 Input Encoding Layer ℎ?? ? Self-Attention Self-Attention Attention-based Matching Layer ℎ?? ? ℎ?t−1 ? Hypothesis (Q) (c) A Cross Attention Unit Q(t)→P (b) The Architecture of Our Proposed OSOA-DFN Figure 1: (a) is the general model for sentence matching. (b) is an overview architecture of our proposed OSOADFN. (c) is an original semantics-oriented cross attention unit that learns the interaction information from the original semantic representation of another sentence. two sentences. It can be formulated as: gether as (Chen et al., 2017; Duan et al., 2018). VP = Match(H0P , H0Q ), VQ = Match(H0Q , H0P ) (4) where Match(·) is a neural attention-based matching function, VP = [vp1 , · · · , vpi , · · · , vpm ] and VQ = [vq1 , · · · , vqj , · · · , vqn ] are new attention-aware representations for P and Q, respectively. This layer is the core layer for sentence matching. Match(·) is mainly focused by researches and some effective frameworks are proposed (Rockt¨aschel et al., 2015; Wang et al., 2017; Duan et al., 2018). In this paper, we also focus on this layer, and propose an original semanticsoriented attention and deep fusion"
D19-1267,D16-1053,0,0.0363324,"Missing"
D19-1267,N13-1092,0,0.0561703,"Missing"
D19-1267,N16-1170,0,0.361994,"on from Q. Given the representations of P and Q: Ht−1 = P t−1 t−1 t−1 0 0 [hp1 , · · · , hpi , · · · , hpm ] and HQ = [hq1 , · · · , h0qj , · · · , h0qn ], each cross attention P(t)→Q will use the original semantics H0Q of Q for interaction, where t = {1, · · · , T} and t = 1 represents P using the original representation H0P . We first compute the unnormalized attention weights as the similarity of P(t) and Q, the alignment matrix At ∈ Rm×n is defined as follows: where [·; ·; ·; ·] refers to the concatenation operation. In matching operation, the concatenation can retain all the information (Wang and Jiang, 2016a; Chen et al., 2017). We use a neural nonlinear transformation ReLU (Glorot et al., 2011) as local comparison function. This operation helps the model to better fuse the attention information and also reduce the complexity of vector representation. Since the understanding of some word-level alignments may rely on the contextual matching information, we then apply a BiLSTM to incorporate the sequential matching information, which further gathers interactive features between two sentences. We design the deep fusion layer as follows. A gated connection layer is used to learn adaptively controlli"
D19-1267,D16-1244,0,0.2613,"Missing"
D19-1267,C16-1127,0,0.057475,"Missing"
D19-1267,P18-2086,0,0.0376876,"Missing"
D19-1267,D14-1162,0,0.0900765,"M (ensemble) ESIM (ensemble) DIIN (ensemble) AF-DMN (ensemble) OSOA-DFN (ensemble) Train 85.3 92.0 88.5 89.5 90.7 88.6 89.6 90.9 92.6 91.2 94.5 92.3 93.2 93.5 92.3 94.9 93.5 Test 83.5 86.1 86.3 86.8 87.5 87.6 86.3 87.5 88.0 88.0 88.6 88.8 88.8 88.6 88.9 89.0 89.3 Models Majority class Ngram DecompAtt ESIM DGEM DEISTE CAFE AF-DMN (re-imp) OSOA-DFN The detailed statistical information of the three datasets is shown in Table 1. Implementation Details We set word embeddings and all of the hidden states of BiLSTMs and MLPs to 300 dimensions. Pre-trained word vectors are 300-dimensional Glove 840B (Pennington et al., 2014) and without updating during training. The learnable word vectors and POS vectors have 30 dimensions. For all datasets, there are 3 cross sentence attention layers and 1 self-attention layer. The batch size is set to 64 for SNLI and Quora, 32 for SciTail. We use the Adam method (Kingma and Ba, 2014) for model training. We set the initial learning rate to 5e-4 with a decay ratio of 0.95 for each epoch, and l2 regularizer strength to 6e-5. To prevent overfitting, we use dropout regularization (Srivastava et al., 2014) with a drop rate of 0.2 for all MLPs. 5.3 Ensemble The ensemble strategy has b"
D19-1267,N18-1202,0,0.0745232,"Missing"
D19-1267,C16-1270,0,0.0269313,"Missing"
I05-2015,J97-2004,0,0.0332403,"he annotated data to improve the performance of automatic word alignment. We will also investigate a method to automatically identify phrase alignments from the annotated word alignment and a method to automatically discover the syntactic structures on the Chinese side from the annotated phrase alignments. Annotation of word alignment Since automatic word alignment techniques cannot reach as high a level as the morphological analyses, we adopt a practical method of using multiple aligners. One aligner is a lexical knowledge-based approach, which was implemented by us based on the work of Ker (Ker and Chang, 1997). Another aligner is the well-known GIZA++ toolkit, which is a statistics-based approach. For GIZA++, two directions were adopted: the Chinese sentences were used as source sentences and the Japanese sentences as target sentences, and vice versa. The results produced by the lexical knowledgebased aligner, C → J of GIZA++, and J → C of GIZA++ were selected in a majority decision. If an alignment result was produced by two or three aligners at the same time, the result was accepted. Otherwise, was abandoned. In this way, we aimed to utilize the results of each aligner and maintain high precision"
I05-2015,maekawa-etal-2000-spontaneous,1,0.909793,"Missing"
I05-2015,P01-1067,0,0.0246433,"Missing"
I05-2015,C94-2209,0,0.0395746,"Annotation on Chinese Sentences For Chinese morphological analysis, we used the analyser developed by Peking University, where the research on definition of Chinese words and the criteria of word segmentation has been conducted for over ten years. The achievements include a grammatical knowledge base of contemporary Chinese, an automatic morphological analyser, and an annotated People’s Daily Corpus. Since the definition and tagset are widely used in Chinese language processing, we also took the criteria as the basis of our guidelines. A morphological analyzer developed by Peking University (Zhou and Yu, 1994) was applied for automatic annotation of the Chinese sentences and then the automatically tagged sentences were revised by humans. An annotated sentence is illustrated in Figure 3, which is the Chinese sentence in Ex. 1 in Section 2. The interface of the tool is shown in Figure 4 and Figure 5. S-ID: 950104141-008 这些/r 俄军/j 士兵/n 均/d 为/v 十九/m 岁/q 左右/m 的/u 年青人/n ，/w 他们/r 甚至/d 连/p 回答/v 问题/n 的/u 气力/n 也/d 没有/v 。/w Figure 3 An annotated Chinese sentence 4.3 Tool for Manual Revision We developed a tool to assist annotators in revision. The tool has both Japanese and Chinese versions. Here, we introduc"
I05-2015,W04-2208,1,\N,Missing
I08-1012,A00-1031,0,0.0519108,"2 in our experiments. We used the same rules for conversion and created the same data split as (Wang et al., 2007): ﬁles 1-270 and 400-931 as training, 271-300 as testing and ﬁles 301-325 as development. We used the gold standard segmentation and POS tags in the CTB. For unlabeled data, we used the PFR corpus 3 . It includes the documents from People’s Daily at 1998 (12 months). There are about 290 thousand sentences and 15 million words in the PFR corpus. To simplify, we used its segmentation. And we discarded the POS tags because PFR and CTB used different POS sets. We used the package TNT (Brants, 2000), a very efﬁcient statistical part-of-speech tagger, to train a POS tagger4 on training data of the CTB. We measured the quality of the parser by the unlabeled attachment score (UAS), i.e., the percentage of tokens with correct HEAD. We reported two types of scores: “UAS without p” is the UAS score without all punctuation tokens and “UAS with p” is the one with all punctuation tokens. 4.1 Experimental results In the experiments, we trained the parsers on training data and tuned the parameters on development data. In the following sessions, “baseline” refers to Basic Parser (the model with basi"
I08-1012,D07-1097,0,0.013766,"case frames. And we represent additional information as the features for learning models while they use the case frames as one component for a probabilistic model. 3 Our Approach In this section, we describe our approach of exploiting reliable features from unlabeled data, which is parsed by a basic parser. We then train another parser based on new feature space. 3.1 Training a basic parser In this paper, we implement a deterministic parser based on the model described by (Nivre, 2003). This model is simple and works very well in the shared-tasks of CoNLL2006(Nivre et al., 2006) and CoNLL2007(Hall et al., 2007). In fact, our approach 90 can also be applied to other parsers, such as (Yamada and Matsumoto, 2003)’s parser, (McDonald et al., 2006)’s parser, and so on. 3.1.1 The parser The parser predicts unlabeled directed dependencies between words in sentences. The algorithm (Nivre, 2003) makes a dependency parsing tree in one left-to-right pass over the input, and uses a stack to store the processed tokens. The behaviors of the parser are deﬁned by four elementary actions (where TOP is the token on top of the stack and NEXT is the next token in the original input string): • Left-Arc(LA): Add an arc f"
I08-1012,N06-1023,1,0.793343,"successful instance of parsing with self-training by using a re-ranker. As Figure 1 suggests, the dependency parser performs bad for parsing the words with long distances. In our approach, we choose partial reliable information which comes from short dependency relations for the dependency parser. (Smith and Eisner, 2006) presents an approach to improve the accuracy of a dependency grammar induction models by EM from unlabeled data. They obtain consistent improvements by penalizing dependencies between two words that are farther apart in the string. The study most relevant to ours is done by (Kawahara and Kurohashi, 2006). They present an integrated probabilistic model for Japanese parsing. They also use partial information after current parser parses the sentences. Our work differs in that we consider general dependency relations while they only consider case frames. And we represent additional information as the features for learning models while they use the case frames as one component for a probabilistic model. 3 Our Approach In this section, we describe our approach of exploiting reliable features from unlabeled data, which is parsed by a basic parser. We then train another parser based on new feature sp"
I08-1012,P06-1043,0,0.0775317,"mance. Our study is relative to incorporating unlabeled data into a model for parsing. There are several other studies relevant to ours as described below. A simple method is self-training in which the existing model ﬁrst labels unlabeled data and then the newly labeled data is then treated as hand annotated data for training a new model. But it seems that selftraining is not so effective. (Steedman et al., 2003) reports minor improvement by using self-training for syntactic parsing on small labeled data. The reason may be that errors in the original model would be ampliﬁed in the new model. (McClosky et al., 2006) presents a successful instance of parsing with self-training by using a re-ranker. As Figure 1 suggests, the dependency parser performs bad for parsing the words with long distances. In our approach, we choose partial reliable information which comes from short dependency relations for the dependency parser. (Smith and Eisner, 2006) presents an approach to improve the accuracy of a dependency grammar induction models by EM from unlabeled data. They obtain consistent improvements by penalizing dependencies between two words that are farther apart in the string. The study most relevant to ours"
I08-1012,D07-1013,0,0.091155,"Missing"
I08-1012,E06-1011,0,0.0577863,"Missing"
I08-1012,W06-2932,0,0.0408924,"onent for a probabilistic model. 3 Our Approach In this section, we describe our approach of exploiting reliable features from unlabeled data, which is parsed by a basic parser. We then train another parser based on new feature space. 3.1 Training a basic parser In this paper, we implement a deterministic parser based on the model described by (Nivre, 2003). This model is simple and works very well in the shared-tasks of CoNLL2006(Nivre et al., 2006) and CoNLL2007(Hall et al., 2007). In fact, our approach 90 can also be applied to other parsers, such as (Yamada and Matsumoto, 2003)’s parser, (McDonald et al., 2006)’s parser, and so on. 3.1.1 The parser The parser predicts unlabeled directed dependencies between words in sentences. The algorithm (Nivre, 2003) makes a dependency parsing tree in one left-to-right pass over the input, and uses a stack to store the processed tokens. The behaviors of the parser are deﬁned by four elementary actions (where TOP is the token on top of the stack and NEXT is the next token in the original input string): • Left-Arc(LA): Add an arc from NEXT to TOP; pop the stack. • Right-Arc(RA): Add an arc from TOP to NEXT; push NEXT onto the stack. • Reduce(RE): Pop the stack. •"
I08-1012,W06-2933,0,0.0192393,"elations while they only consider case frames. And we represent additional information as the features for learning models while they use the case frames as one component for a probabilistic model. 3 Our Approach In this section, we describe our approach of exploiting reliable features from unlabeled data, which is parsed by a basic parser. We then train another parser based on new feature space. 3.1 Training a basic parser In this paper, we implement a deterministic parser based on the model described by (Nivre, 2003). This model is simple and works very well in the shared-tasks of CoNLL2006(Nivre et al., 2006) and CoNLL2007(Hall et al., 2007). In fact, our approach 90 can also be applied to other parsers, such as (Yamada and Matsumoto, 2003)’s parser, (McDonald et al., 2006)’s parser, and so on. 3.1.1 The parser The parser predicts unlabeled directed dependencies between words in sentences. The algorithm (Nivre, 2003) makes a dependency parsing tree in one left-to-right pass over the input, and uses a stack to store the processed tokens. The behaviors of the parser are deﬁned by four elementary actions (where TOP is the token on top of the stack and NEXT is the next token in the original input stri"
I08-1012,W03-3017,0,0.0749943,"t parser parses the sentences. Our work differs in that we consider general dependency relations while they only consider case frames. And we represent additional information as the features for learning models while they use the case frames as one component for a probabilistic model. 3 Our Approach In this section, we describe our approach of exploiting reliable features from unlabeled data, which is parsed by a basic parser. We then train another parser based on new feature space. 3.1 Training a basic parser In this paper, we implement a deterministic parser based on the model described by (Nivre, 2003). This model is simple and works very well in the shared-tasks of CoNLL2006(Nivre et al., 2006) and CoNLL2007(Hall et al., 2007). In fact, our approach 90 can also be applied to other parsers, such as (Yamada and Matsumoto, 2003)’s parser, (McDonald et al., 2006)’s parser, and so on. 3.1.1 The parser The parser predicts unlabeled directed dependencies between words in sentences. The algorithm (Nivre, 2003) makes a dependency parsing tree in one left-to-right pass over the input, and uses a stack to store the processed tokens. The behaviors of the parser are deﬁned by four elementary actions (w"
I08-1012,P06-1072,0,0.0195353,"model. But it seems that selftraining is not so effective. (Steedman et al., 2003) reports minor improvement by using self-training for syntactic parsing on small labeled data. The reason may be that errors in the original model would be ampliﬁed in the new model. (McClosky et al., 2006) presents a successful instance of parsing with self-training by using a re-ranker. As Figure 1 suggests, the dependency parser performs bad for parsing the words with long distances. In our approach, we choose partial reliable information which comes from short dependency relations for the dependency parser. (Smith and Eisner, 2006) presents an approach to improve the accuracy of a dependency grammar induction models by EM from unlabeled data. They obtain consistent improvements by penalizing dependencies between two words that are farther apart in the string. The study most relevant to ours is done by (Kawahara and Kurohashi, 2006). They present an integrated probabilistic model for Japanese parsing. They also use partial information after current parser parses the sentences. Our work differs in that we consider general dependency relations while they only consider case frames. And we represent additional information as"
I08-1012,E03-1008,0,0.0855318,"ta for dependency parsing. We use a parser to parse the sentences in unlabeled data. Then another parser makes use of the information on short dependency relations in the newly parsed data to improve performance. Our study is relative to incorporating unlabeled data into a model for parsing. There are several other studies relevant to ours as described below. A simple method is self-training in which the existing model ﬁrst labels unlabeled data and then the newly labeled data is then treated as hand annotated data for training a new model. But it seems that selftraining is not so effective. (Steedman et al., 2003) reports minor improvement by using self-training for syntactic parsing on small labeled data. The reason may be that errors in the original model would be ampliﬁed in the new model. (McClosky et al., 2006) presents a successful instance of parsing with self-training by using a re-ranker. As Figure 1 suggests, the dependency parser performs bad for parsing the words with long distances. In our approach, we choose partial reliable information which comes from short dependency relations for the dependency parser. (Smith and Eisner, 2006) presents an approach to improve the accuracy of a dependen"
I08-1012,W05-1516,0,0.0438281,"Missing"
I08-1012,P06-1054,0,0.0376974,"Missing"
I08-1012,N07-3002,0,0.0308211,"Missing"
I08-1012,W03-3023,0,0.171228,"le they use the case frames as one component for a probabilistic model. 3 Our Approach In this section, we describe our approach of exploiting reliable features from unlabeled data, which is parsed by a basic parser. We then train another parser based on new feature space. 3.1 Training a basic parser In this paper, we implement a deterministic parser based on the model described by (Nivre, 2003). This model is simple and works very well in the shared-tasks of CoNLL2006(Nivre et al., 2006) and CoNLL2007(Hall et al., 2007). In fact, our approach 90 can also be applied to other parsers, such as (Yamada and Matsumoto, 2003)’s parser, (McDonald et al., 2006)’s parser, and so on. 3.1.1 The parser The parser predicts unlabeled directed dependencies between words in sentences. The algorithm (Nivre, 2003) makes a dependency parsing tree in one left-to-right pass over the input, and uses a stack to store the processed tokens. The behaviors of the parser are deﬁned by four elementary actions (where TOP is the token on top of the stack and NEXT is the next token in the original input string): • Left-Arc(LA): Add an arc from NEXT to TOP; pop the stack. • Right-Arc(RA): Add an arc from TOP to NEXT; push NEXT onto the stac"
I08-1012,D07-1096,0,\N,Missing
I11-1035,D08-1017,0,0.0345737,"t. 3.2 New Features for POS Tagging We generate n-gram and lexicon features for POS tagging as well. In addition, the features that incorporate word clusters derived from a large autoanalyzed corpus (referred to as cluster features) are introduced. • For the development and test sets, we collect a lexicon using the entire training corpus and use it for feature generation. Because the lexicon is extracted from other sets, the weights for this feature will not be overestimated by the learning algorithm. This kind of cross-validation-like techniques are used in studies such as Collins (2002) and Martins et al. (2008) to avoid over-fitting to the training data. Our method can be considered as its application to lexicon extraction. Using the extracted lexicon, we generate lexicon features as follows. If a character sequence starting with character c0 matches some words in the lexicon, we greedily choose the longest such matching word w. Letting LEN (w) be the length (the number of characters) of w, we add the following feature for each character ck in c0 , c1 , ..., cLEN (w) : (b) P (ck )/LEN (w)-P OSs(w) Here, P (ck ) is the position number (i.e., k) of the character ck in w and P OSs(w) represents the POS"
I11-1035,P09-1058,1,0.736673,"Missing"
I11-1035,W03-1719,0,0.0122318,"81 0.9112 CTB7 0.8996 0.9017 0.9020 0.9019 0.9046 Table 7: Results of word segmentation POS tag method Baseline +(c) n-gram +(d) cluster +(e) lexicon +(c)+(d)+(e) CTB5 0.9318 0.9333 0.9350 0.9346 0.9359 CTB6 0.8999 0.9014 0.9026 0.9015 0.9048 CTB7 0.8937 0.8958 0.8959 0.8959 0.8985 POS tag method Baseline +(c) n-gram +(d) cluster +(e) lexicon +(c)+(d)+(e) Table 8: F1 results of segmentation and POS tagging (baseline model for word segmentation) Table 9: F1 results of segmentation and POS tagging (our best model for word segmentation) the words in the test set that are not in the training set (Sproat and Emerson, 2003). The development sets were used to obtain the optimal values of tunable parameters and feature configurations. The unlabeled data for our experiments were taken from the XIN_CMN portion of Chinese Gigaword Version 2.0 (LDC2009T14), which has approximately 311 million words. Some of CTB data and Chinese Gigaword data are from the same source: Xinhua newswire between 1994 and 1998. In order to avoid overlap between the CTB data and the unlabeled data, we used only the articles published in 1991- 1993 and 1999-2004 as unlabeled data, with 204 million words.8 Note that we only used one million wo"
I11-1035,Y06-1012,0,0.122089,"(and labeled) data into the above baseline models through features. We preprocess unlabeled data with our baseline models and obtain wordsegmented sentences with POS tags, and generate new features from the auto-analyzed data. Although the focus of the paper is semi-supervised learning, we also extract a lexicon from the training corpus and use it to generate features. Figure 1 shows an overview of our approach. The rest of this section describes our features in detail. Segmentation and POS tagging Models We implement our approach using sequential tagging models. Following the previous work (Zhao et al., 2006; Zhao et al., 2010), we employ the linear chain CRFs (Lafferty et al., 2001) as our learning model. Specifically, we use its CRF++ (version 0.54) implementation by Taku Kudo. 1 2.1 Baseline Segmentation Model 3.1 New features for Word Segmentation We employ character-based sequence labeling for word segmentation. In addition to its simplicity, the advantage of a character-based model is its robustness to the unknown word problem (Xue, 2003). In a character-based Chinese word segmentation task, a character in a given sequence is labeled by a tag that stands for its position in the word that th"
I11-1035,P08-1068,0,0.0436031,"Missing"
I11-1035,P07-2055,0,0.142272,"ry is correctly identified. For Seg &Tag, a word is considered correct only when both the word boundary and its POS tag are correctly identified. Table 13 summarizes the results on test sets. These tests suggest that although the difference from K 09b for CTB5 data is not statistically significant, all other differences are clearly statistically significant (p < 10−5 ). 4.4 Comparative Results In this section, we compare our approach with the best previous approaches reported in the literature. The performance scores of previous studies are directly taken from their papers, except for N&U 07 (Nakagawa and Uchimoto, 2007), which is taken from Kruengkrai et al. (2009b). Z&C 10 refers to Zhang and Clark (2010). Two methods in Kruengkrai et al. (2009a; 2009b) are referred to as K 09a and K 09b. Jiang 08a and Jiang 08b refer to Jiang et al. (2008a; 2008b). Table 10 compares F1 results on CTB5.0. The best score in each column is in boldface. The results of our approach are superior to those of previous studies for both 4.6 Comparison with Self-Training An alternative method of incorporating unlabeled data is self-training, so we also compared our results to the self-training method. Because no existing research was"
I11-1035,P08-1102,0,0.21839,"rence from K 09b for CTB5 data is not statistically significant, all other differences are clearly statistically significant (p < 10−5 ). 4.4 Comparative Results In this section, we compare our approach with the best previous approaches reported in the literature. The performance scores of previous studies are directly taken from their papers, except for N&U 07 (Nakagawa and Uchimoto, 2007), which is taken from Kruengkrai et al. (2009b). Z&C 10 refers to Zhang and Clark (2010). Two methods in Kruengkrai et al. (2009a; 2009b) are referred to as K 09a and K 09b. Jiang 08a and Jiang 08b refer to Jiang et al. (2008a; 2008b). Table 10 compares F1 results on CTB5.0. The best score in each column is in boldface. The results of our approach are superior to those of previous studies for both 4.6 Comparison with Self-Training An alternative method of incorporating unlabeled data is self-training, so we also compared our results to the self-training method. Because no existing research was found concerning the selftraining method on word segmentation and POS 9 We used the version with Yates’ correction, using correction factor 0.5 315 Sentences added 0(Baseline) 5k 10k 30k 150k 300k 600k Segmentation F1 0.9498"
I11-1035,C08-1049,0,0.103149,"rence from K 09b for CTB5 data is not statistically significant, all other differences are clearly statistically significant (p < 10−5 ). 4.4 Comparative Results In this section, we compare our approach with the best previous approaches reported in the literature. The performance scores of previous studies are directly taken from their papers, except for N&U 07 (Nakagawa and Uchimoto, 2007), which is taken from Kruengkrai et al. (2009b). Z&C 10 refers to Zhang and Clark (2010). Two methods in Kruengkrai et al. (2009a; 2009b) are referred to as K 09a and K 09b. Jiang 08a and Jiang 08b refer to Jiang et al. (2008a; 2008b). Table 10 compares F1 results on CTB5.0. The best score in each column is in boldface. The results of our approach are superior to those of previous studies for both 4.6 Comparison with Self-Training An alternative method of incorporating unlabeled data is self-training, so we also compared our results to the self-training method. Because no existing research was found concerning the selftraining method on word segmentation and POS 9 We used the version with Yates’ correction, using correction factor 0.5 315 Sentences added 0(Baseline) 5k 10k 30k 150k 300k 600k Segmentation F1 0.9498"
I11-1035,I08-1012,1,0.81753,"gging by incorporating large unlabeled data. We first preprocess unlabeled data with our baseline models. We then extract various items of dictionary information from the auto-analyzed data. Finally, we generate new features that incorporate the extracted information for both word segmentation and POS tagging. We also perform word clustering on the auto-segmented data and use word clusters as features in POS tagging. In addition, we introduce lexicon features by using a crossvalidation technique. The use of sub-structures from the autoannotated data has been presented previously (Noord, 2007; Chen et al., 2008; Chen et al., 2009). Chen et al. (2009) extracted different types of subtrees from the auto-parsed data and used them as new features in standard learning methods. They showed this simple method greatly improves the accuracy of dependency parsing. The idea of combining word clusters with discriminative learning has been previously reported in the context of named entity recognition (Miller et al., 2004; Kazama and Torisawa, 2008) and dependency parsing (Koo et al., 2008). We adapt and extend these techniques to Chinese word segmentation and POS tagging, and demonstrate their effectiveness in"
I11-1035,D09-1060,1,0.381889,"Missing"
I11-1035,I08-4029,0,0.134356,"be a character n-gram (e.g., uni-gram ci , bi-gram ci ci+1 , trigram ci−1 ci ci+1 and so on)2 , and seg be a segmentation profile for n-gram g observed at each position. The segmentation profile can be tag ti or the combination of tags. Take a bi-gram for example, seg may be ti or ti ti+1 . Then, 2.2 Baseline POS Tagging Model Since we employ a pipelined method, the POS tagging can be performed as a word labeling task, where the input is a sequence of segmented words. We use a CRF here as well. The feature set of our baseline POS tagger, is listed in Table 3. These are adopted from Wu et al. (2008). 1 2 Note that there are several alternative ways for extracting n-grams at position i, for example ci−1 ci for a bi-gram. In this paper, we used the way as explained here. Available from http://crfpp.sourceforge.net/ 310 Feature Type Word Unigram Nearing Word Bigram Jump Word Bigram First Character Last Character Length Context Position w−2 ,w−1 ,w0 ,w1 ,w2 (w−2 w−1 ),(w−1 w0 ),(w1 w0 ),(w1 w2 ) (w−1 ,w1 ) F c(w0 ) Lc(w0 ) Len(w0 ) Description Word unigram Word bigram Previous word and next word First character of current word Last character of current word Length of current word Table 3: Fe"
I11-1035,D09-1058,0,0.0118399,"r-level NLP tasks such as parsing and information extraction. Although the performance of Chinese word segmentation and POS tagging has been greatly improved over the past years, the task is still challenging. To improve the accuracy of NLP systems, one of the current trends is semi-supervised learning, which utilizes large unlabeled data in supervised learning. Several studies have demonstrated that the use of unlabeled data can improve the performance of NLP tasks, such as text chunking (Ando and Zhang, 2005), POS tagging and named entity recognition (Suzuki and Isozaki, 2008), and parsing (Suzuki et al., 2009; Chen et al., 2009; Koo et al., 2008). Therefore, it is attractive to consider adopting semi-supervised learning in Chinese word segmentation and POS tagging tasks. 309 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 309–317, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP Word Length Tags 1 S 2 BE 3 BB2 E 4 BB2 B3 E 5 BB2 B3 M E 6 BB2 B3 M M E 7 or more BB2 B3 M...M E Table 1: Word representation with a 6-tag tagset : S, B, B2 , B3 , M, E Type Character Unigram Nearing Character Bigram Jump Character Bigram Punctuation Character Type Feat"
I11-1035,D10-1082,0,0.4394,"boundary and its POS tag are correctly identified. Table 13 summarizes the results on test sets. These tests suggest that although the difference from K 09b for CTB5 data is not statistically significant, all other differences are clearly statistically significant (p < 10−5 ). 4.4 Comparative Results In this section, we compare our approach with the best previous approaches reported in the literature. The performance scores of previous studies are directly taken from their papers, except for N&U 07 (Nakagawa and Uchimoto, 2007), which is taken from Kruengkrai et al. (2009b). Z&C 10 refers to Zhang and Clark (2010). Two methods in Kruengkrai et al. (2009a; 2009b) are referred to as K 09a and K 09b. Jiang 08a and Jiang 08b refer to Jiang et al. (2008a; 2008b). Table 10 compares F1 results on CTB5.0. The best score in each column is in boldface. The results of our approach are superior to those of previous studies for both 4.6 Comparison with Self-Training An alternative method of incorporating unlabeled data is self-training, so we also compared our results to the self-training method. Because no existing research was found concerning the selftraining method on word segmentation and POS 9 We used the ver"
I11-1035,N04-1043,0,\N,Missing
I11-1035,W07-2201,0,\N,Missing
I11-1035,P08-1047,1,\N,Missing
I11-1035,P08-1076,0,\N,Missing
I11-1035,O03-4002,0,\N,Missing
I11-1035,I05-3025,0,\N,Missing
I11-1035,W03-1726,0,\N,Missing
I11-1035,P02-1062,0,\N,Missing
I11-1035,N06-1020,0,\N,Missing
I13-1120,C10-1015,0,0.0359044,"between words whose similarity is vague. To overcome this problem, this paper proposes an approach of hybrid hierarchical structure computing Chinese word similarity to achieve fine-grained similarity results with HowNet 2008. The experimental results prove that the method has a better effect on computing similarity of synonyms and antonyms including nouns, verbs and adjectives. Besides, it performs stably on standard data provided by SemEval 2012. 1 Introduction Word similarity computing plays an important role in various fields, such as Natural Language Understanding and Cognitive Science (Bunescu and Huang, 2010b; Mohler et al., 2011; Wang and Wan, 2011;). Moreover, it is a pivotal method in Word Sense Disambiguation (WSD). Two main types of word similarity computing methods have been proposed. One is usually based on the thesaurus. The methods of this type utilize the structure of thesaurus (Liu and Li, 2002; Ge et al., 2010) with the advantages of preciseness and deep usage of word semantics, but a relatively complete semantic dictionary is required in order to ensure the presence of words in thesaurus. The other methods are based on large-scale corpora with some inevitable disadvantages, such as t"
I13-1120,P11-1076,0,0.0167387,"arity is vague. To overcome this problem, this paper proposes an approach of hybrid hierarchical structure computing Chinese word similarity to achieve fine-grained similarity results with HowNet 2008. The experimental results prove that the method has a better effect on computing similarity of synonyms and antonyms including nouns, verbs and adjectives. Besides, it performs stably on standard data provided by SemEval 2012. 1 Introduction Word similarity computing plays an important role in various fields, such as Natural Language Understanding and Cognitive Science (Bunescu and Huang, 2010b; Mohler et al., 2011; Wang and Wan, 2011;). Moreover, it is a pivotal method in Word Sense Disambiguation (WSD). Two main types of word similarity computing methods have been proposed. One is usually based on the thesaurus. The methods of this type utilize the structure of thesaurus (Liu and Li, 2002; Ge et al., 2010) with the advantages of preciseness and deep usage of word semantics, but a relatively complete semantic dictionary is required in order to ensure the presence of words in thesaurus. The other methods are based on large-scale corpora with some inevitable disadvantages, such as the frequent need of la"
I13-1120,C10-1028,0,0.0263332,"pe utilize the structure of thesaurus (Liu and Li, 2002; Ge et al., 2010) with the advantages of preciseness and deep usage of word semantics, but a relatively complete semantic dictionary is required in order to ensure the presence of words in thesaurus. The other methods are based on large-scale corpora with some inevitable disadvantages, such as the frequent need of large-scale corpora, noise, low search efficiency etc. (Nakov and Hearst, 2008). Therefore, it is fine to create a refined thesaurus with Internet resource or largescale corpora (Morita et al., 2011; Navigli and Ponzetto, 2010; Davidov and Rappoport, 2010) as an interim for computing word similarity. WordNet is deemed to be very valuable thesaurus. Since Chinese that belongs to isolated Yujie Zhang Beijing Jiaotong Universtity yjzhang@bjtu.edu.cn language is different from English that belongs to inflected language and the complex Chinese grammar is highly ambiguous, computing Chinese words similarity is more difficult than English under the same lack of systematic resource. HowNet is also a valuable bilingual knowledge thesaurus organized by Zhongdong Dong. HowNet uses a markup language called KDML to describe word’s concept which facilitates"
I13-1120,P08-1052,0,\N,Missing
I13-1120,S12-1049,0,\N,Missing
N07-1005,2001.mtsummit-papers.3,0,0.0334423,"ere, the term Qj corresponds to the rate of accomplishment of the sub-goal having the i-th ID, and λQj is a weight for the rate of accomplishment. The  term Qj corresponds to the rate of unaccomplishment of the sub-goal having the i-th ID, and λQ is a j weight for the rate of unaccomplishment. The value n indicates the number of types of sub-goals. The term λ is constant. The term Si indicates a similarity between a translated sentence and its reference translation, and λSi is a weight for the similarity. Many methods for calculating the similarity have been proposed (Niessen et al., 2000; Akiba et al., 2001; Papineni et al., 2002; NIST, 2002; Leusch et al., 2003; Turian et al., 2003; Babych and Hartley, 2004; Lin and Och, 2004; Banerjee and Lavie, 2005; Gime´nez et al., 2005). In our research, 23 scores, namely BLEU (Papineni et al., 2002) with maximum n-gram lengths of 1, 2, 3, and 4, NIST (NIST, 2002) with maximum n-gram lengths of 1, 2, 3, 4, and 5, GTM (Turian et al., 2003) with exponents of 1.0, 2.0, and 3.0, METEOR (exact) (Banerjee and Lavie, 2005), WER (Niessen et 37 Automatic Estimation of Rate of Accomplishment of Sub-goals The rate of accomplishment of sub-goals is estimated by determ"
N07-1005,P04-1079,0,0.130077,"ine Translation Based on Rate of Accomplishment of Sub-goals Kiyotaka Uchimoto and Katsunori Kotani and Yujie Zhang and Hitoshi Isahara National Institute of Information and Communications Technology 3-5, Hikari-dai, Seika-cho, Soraku-gun, Kyoto, 619-0289, Japan {uchimoto,yujie,isahara}@nict.go.jp, kat@khn.nict.go.jp Abstract issue. In recent years, many researchers have tried to automatically evaluate the quality of MT and improve the performance of automatic MT evaluations (Niessen et al., 2000; Akiba et al., 2001; Papineni et al., 2002; NIST, 2002; Leusch et al., 2003; Turian et al., 2003; Babych and Hartley, 2004; Lin and Och, 2004; Banerjee and Lavie, 2005; Gime´nez et al., 2005) because improving the performance of automatic MT evaluation is expected to enable us to use and improve MT systems efficiently. For example, Och reported that the quality of MT results was improved by using automatic MT evaluation measures for the parameter tuning of an MT system (Och, 2003). This report shows that the quality of MT results improves as the performance of automatic MT evaluation improves. The quality of a sentence translated by a machine translation (MT) system is difficult to evaluate. We propose a method f"
N07-1005,W05-0909,0,0.126772,"nt of Sub-goals Kiyotaka Uchimoto and Katsunori Kotani and Yujie Zhang and Hitoshi Isahara National Institute of Information and Communications Technology 3-5, Hikari-dai, Seika-cho, Soraku-gun, Kyoto, 619-0289, Japan {uchimoto,yujie,isahara}@nict.go.jp, kat@khn.nict.go.jp Abstract issue. In recent years, many researchers have tried to automatically evaluate the quality of MT and improve the performance of automatic MT evaluations (Niessen et al., 2000; Akiba et al., 2001; Papineni et al., 2002; NIST, 2002; Leusch et al., 2003; Turian et al., 2003; Babych and Hartley, 2004; Lin and Och, 2004; Banerjee and Lavie, 2005; Gime´nez et al., 2005) because improving the performance of automatic MT evaluation is expected to enable us to use and improve MT systems efficiently. For example, Och reported that the quality of MT results was improved by using automatic MT evaluation measures for the parameter tuning of an MT system (Och, 2003). This report shows that the quality of MT results improves as the performance of automatic MT evaluation improves. The quality of a sentence translated by a machine translation (MT) system is difficult to evaluate. We propose a method for automatically evaluating the quality of ea"
N07-1005,2005.iwslt-1.26,0,0.0249315,"Missing"
N07-1005,1995.mtsummit-1.35,1,0.83169,"nguistic phenomena that are difficult to automatically translate. Recently, MT evaluation campaigns such 34 as the International Workshop on Spoken Language Translation 1 , NIST Machine Translation Evaluation 2 , and HTRDP Evaluation 3 were organized to support the improvement of MT techniques. The data used in the evaluation campaigns were arbitrarily collected from newspaper articles or travel conversation data for fair evaluation. They are classified as the former type of data mentioned above. On the other hand, the data provided by NTT (Ikehara et al., 1994) and that constructed by JEIDA (Isahara, 1995) are classified as the latter type. Almost all the data mentioned above consist of only parallel translations in two languages. Data with information for evaluating MT results, such as JEIDA’s are rarely found. In this paper, we call data that consist of parallel translations collected for MT evaluation and that the information for MT evaluation is assigned to, a test set. The most characteristic information assigned to the JEIDA test set is the yes/no question for assessing the translation results. For example, a yes/no question such as “Is ‘for’ translated into an expression representing a c"
N07-1005,W04-3250,0,0.0286266,"ach question. The third procedure is combining a measure based on the questions and conventional measures. We also present a method for automatically generating sub-goals in the form of yes/no questions and estimating the rate of accomplishment of the sub-goals. Promising results are shown. 1 Introduction In machine translation (MT) research, appropriately evaluating the quality of MT results is an important MT systems can be ranked if a set of MT results for each system and their reference translations are given. Usually, about 300 or more sentences are used to automatically rank MT systems (Koehn, 2004). However, the quality of a sentence translated by an MT system is difficult to evaluate. For example, the results of five MTs into Japanese of the sentence “The percentage of stomach cancer among the workers appears to be the highest for any asbestos workers.” are shown in Table 1. A conventional automatic evaluation method ranks the fifth MT result first although its human subjective evaluation is the lowest. This is because conventional methods are based on the similarity between a translated sentence and its reference translation, and they give the translated sentence a high score when the"
N07-1005,2003.mtsummit-papers.32,0,0.19761,"Zealand. The personal pronoun “they” is omitted in a translation like “nyuujiilando de wa eigo wo hanasu”? The answer is yes if the pattern [karera wa|sore ra wa] is not included in a translation. Otherwise, the answer is no. ear regression model as follows using the rate of accomplishment of the sub-goals and the similarities between a given translation and its reference translation. The best-fitted line for the observed data is calculated by the method of least-squares (Draper and Smith, 1981). A = m  i=1 +  Qj =   Qj = λSi × Si n  (1)  (λQj × Qj + λQ × Qj ) + λ j=1 al., 2000), PER (Leusch et al., 2003), and ROUGE (Lin, 2004) with n-gram lengths of 1, 2, 3, and 4 and 4 variants (LCS, S∗, SU∗, W-1.2), were used to calculate each similarity Si . Therefore, the value of m in Eq. (1) was 23. Japanese word segmentation was performed by using JUMAN 4 in our experiments. As you can see, the definition of our new measure is based on a combination of an evaluation measure focusing on local information and that focusing on global information. j 1 : if subgoal is accomplished 0 : otherwise 3.2 (2) 1 : if subgoal is unaccomplished (3) 0 : otherwise Here, the term Qj corresponds to the rate of accomplish"
N07-1005,C04-1072,0,0.0727122,"ate of Accomplishment of Sub-goals Kiyotaka Uchimoto and Katsunori Kotani and Yujie Zhang and Hitoshi Isahara National Institute of Information and Communications Technology 3-5, Hikari-dai, Seika-cho, Soraku-gun, Kyoto, 619-0289, Japan {uchimoto,yujie,isahara}@nict.go.jp, kat@khn.nict.go.jp Abstract issue. In recent years, many researchers have tried to automatically evaluate the quality of MT and improve the performance of automatic MT evaluations (Niessen et al., 2000; Akiba et al., 2001; Papineni et al., 2002; NIST, 2002; Leusch et al., 2003; Turian et al., 2003; Babych and Hartley, 2004; Lin and Och, 2004; Banerjee and Lavie, 2005; Gime´nez et al., 2005) because improving the performance of automatic MT evaluation is expected to enable us to use and improve MT systems efficiently. For example, Och reported that the quality of MT results was improved by using automatic MT evaluation measures for the parameter tuning of an MT system (Och, 2003). This report shows that the quality of MT results improves as the performance of automatic MT evaluation improves. The quality of a sentence translated by a machine translation (MT) system is difficult to evaluate. We propose a method for automatically ev"
N07-1005,W04-1013,0,0.00357986,"ey” is omitted in a translation like “nyuujiilando de wa eigo wo hanasu”? The answer is yes if the pattern [karera wa|sore ra wa] is not included in a translation. Otherwise, the answer is no. ear regression model as follows using the rate of accomplishment of the sub-goals and the similarities between a given translation and its reference translation. The best-fitted line for the observed data is calculated by the method of least-squares (Draper and Smith, 1981). A = m  i=1 +  Qj =   Qj = λSi × Si n  (1)  (λQj × Qj + λQ × Qj ) + λ j=1 al., 2000), PER (Leusch et al., 2003), and ROUGE (Lin, 2004) with n-gram lengths of 1, 2, 3, and 4 and 4 variants (LCS, S∗, SU∗, W-1.2), were used to calculate each similarity Si . Therefore, the value of m in Eq. (1) was 23. Japanese word segmentation was performed by using JUMAN 4 in our experiments. As you can see, the definition of our new measure is based on a combination of an evaluation measure focusing on local information and that focusing on global information. j 1 : if subgoal is accomplished 0 : otherwise 3.2 (2) 1 : if subgoal is unaccomplished (3) 0 : otherwise Here, the term Qj corresponds to the rate of accomplishment of the sub-goal ha"
N07-1005,niessen-etal-2000-evaluation,0,0.0410402,"ed (3) 0 : otherwise Here, the term Qj corresponds to the rate of accomplishment of the sub-goal having the i-th ID, and λQj is a weight for the rate of accomplishment. The  term Qj corresponds to the rate of unaccomplishment of the sub-goal having the i-th ID, and λQ is a j weight for the rate of unaccomplishment. The value n indicates the number of types of sub-goals. The term λ is constant. The term Si indicates a similarity between a translated sentence and its reference translation, and λSi is a weight for the similarity. Many methods for calculating the similarity have been proposed (Niessen et al., 2000; Akiba et al., 2001; Papineni et al., 2002; NIST, 2002; Leusch et al., 2003; Turian et al., 2003; Babych and Hartley, 2004; Lin and Och, 2004; Banerjee and Lavie, 2005; Gime´nez et al., 2005). In our research, 23 scores, namely BLEU (Papineni et al., 2002) with maximum n-gram lengths of 1, 2, 3, and 4, NIST (NIST, 2002) with maximum n-gram lengths of 1, 2, 3, 4, and 5, GTM (Turian et al., 2003) with exponents of 1.0, 2.0, and 3.0, METEOR (exact) (Banerjee and Lavie, 2005), WER (Niessen et 37 Automatic Estimation of Rate of Accomplishment of Sub-goals The rate of accomplishment of sub-goals is"
N07-1005,P03-1021,0,0.0239749,"ed to automatically evaluate the quality of MT and improve the performance of automatic MT evaluations (Niessen et al., 2000; Akiba et al., 2001; Papineni et al., 2002; NIST, 2002; Leusch et al., 2003; Turian et al., 2003; Babych and Hartley, 2004; Lin and Och, 2004; Banerjee and Lavie, 2005; Gime´nez et al., 2005) because improving the performance of automatic MT evaluation is expected to enable us to use and improve MT systems efficiently. For example, Och reported that the quality of MT results was improved by using automatic MT evaluation measures for the parameter tuning of an MT system (Och, 2003). This report shows that the quality of MT results improves as the performance of automatic MT evaluation improves. The quality of a sentence translated by a machine translation (MT) system is difficult to evaluate. We propose a method for automatically evaluating the quality of each translation. In general, when translating a given sentence, one or more conditions should be satisfied to maintain a high translation quality. In EnglishJapanese translation, for example, prepositions and infinitives must be appropriately translated. We show several procedures that enable evaluating the quality of"
N07-1005,P02-1040,0,0.0808433,"responds to the rate of accomplishment of the sub-goal having the i-th ID, and λQj is a weight for the rate of accomplishment. The  term Qj corresponds to the rate of unaccomplishment of the sub-goal having the i-th ID, and λQ is a j weight for the rate of unaccomplishment. The value n indicates the number of types of sub-goals. The term λ is constant. The term Si indicates a similarity between a translated sentence and its reference translation, and λSi is a weight for the similarity. Many methods for calculating the similarity have been proposed (Niessen et al., 2000; Akiba et al., 2001; Papineni et al., 2002; NIST, 2002; Leusch et al., 2003; Turian et al., 2003; Babych and Hartley, 2004; Lin and Och, 2004; Banerjee and Lavie, 2005; Gime´nez et al., 2005). In our research, 23 scores, namely BLEU (Papineni et al., 2002) with maximum n-gram lengths of 1, 2, 3, and 4, NIST (NIST, 2002) with maximum n-gram lengths of 1, 2, 3, 4, and 5, GTM (Turian et al., 2003) with exponents of 1.0, 2.0, and 3.0, METEOR (exact) (Banerjee and Lavie, 2005), WER (Niessen et 37 Automatic Estimation of Rate of Accomplishment of Sub-goals The rate of accomplishment of sub-goals is estimated by determining the answer to eac"
N07-1005,C04-1046,0,\N,Missing
P06-2013,J95-4004,0,0.083355,"n an n window. • POS: uni-gram and bi-grams of POS in an n window. • WORD+POS: Both the features of WORD and POS. where n is a predefined number to denote window size. For instance, the WORD features at the 3rd position (北 京-NR) in Example 1 (set n as 2): ”他 L2 到 达 L1 北 京 0 机 场 R1 。 R2”(unigram) and ”他 到达 LB1 到达 北京 B0 北京 机 场 RB1 机场 。 RB2”(bi-gram). Thus features of WORD have 9 items(5 from uni-gram and 4 from bi-grams). In the similar way, features of POS also have 9 items and features of WORD+POS have 18 items(9+9). 3.1.3 TBL Transformation based learning(TBL), first introduced by Eric Brill(Brill, 1995), is mainly based on the idea of successively transforming the data in order to correct the error. The transformation rules obtained are usually few , yet powerful. TBL was applied to Chinese chunking by Li et al.(Li et al., 2004) and TBL provided good performance on their corpus. In this paper, we used fnTBL (V1.0)7 to implement the TBL model. 4 Tag-Extension In Chinese chunking, there are some difficult problems, which are related to Special Terms, NounNoun Compounds, Named Entities Tagging and Coordination. In this section, we propose an approach to resolve these problems by extending the c"
P06-2013,W00-0730,0,0.0141815,"tances are based(Walter et al., 1999). The similarity between the new instance X and example Y in memory is computed using a distance metric. Tjong Kim Sang(Sang, 2002) applied memorybased learning(MBL) to English chunking. MBL performs well for a variety of shallow parsing tasks, often yielding good results. In this paper, we used TiMBL8 (Daelemans et al., 2004) to implement the MBL model. 3.1.1 SVMs Support Vector Machines (SVMs) is a powerful supervised learning paradigm based on the Structured Risk Minimization principle from computational learning theory(Vapnik, 1995). Kudo and Matsumoto(Kudo and Matsumoto, 2000) applied SVMs to English chunking and achieved the best performance in the CoNLL00 shared task(Sang and Buchholz, 2000). They created 231 SVMs classifiers to predict the unique pairs of chunk tags.The final decision was given by their weighted voting. Then the label sequence was chosen using a dynamic programming algorithm. Tan et al. (Tan et al., 2004) applied SVMs to Chinese chunking. They used sigmoid functions to extract probabilities from SVMs outputs as the post-processing of classification. In this paper, we used Yamcha (V0.33)5 in our experiments. 3.2 Features The observations are base"
P06-2013,N01-1025,0,0.289957,"fficult to tell whether ”最低” is a shared modifier or not, even for people. We extend the tags with COO for Coordination: B-NP-COO and I-NP-COO. 3) Named Entities Tagging: Named Entities(NE)(Sang and Meulder, 2003) are not distinguished in CTB4, and they are all tagged as ”NR”. However, they play different roles in chunks, especial in noun phrases. For instance, ”澳 门-NR(Macau)/ 机 场-NN(Airport)” and ”香 港-NR(Hong Kong)/ 机场-NN(Airport)” vs ”邓小 平-NR(Deng Xiaoping)/ 先生-NN(Mr.)” and ”宋 卫 平-NR(Song Weiping) 主 席-NN(President)”. Here ”澳门” and ”香港” are LOCATION, while 5 Voting Methods Kudo and Matsumoto(Kudo and Matsumoto, 2001) reported that they achieved higher accuracy by applying voting of systems that were trained using different data representations. Tjong Kim Sang et al.(Sang and Buchholz, 2000) reported similar results by combining different systems. In order to provide better results, we also apply the voting of basic systems, including SVMs, CRFs, MBL and TBL. Depending on the characteristics in the chunking task, we propose two new voting methods. In these two voting methods, we consider long distance information. In the weighted voting method, we can assign different weights to the results of the individu"
P06-2013,O05-1017,0,0.0624368,"Linguistics Group National Institute of Information and Communications Technology 3-5 Hikari-dai, Seika-cho, Soraku-gun, Kyoto, Japan, 619-0289 {chenwl, yujie, isahara}@nict.go.jp Abstract Conditional Random Fields (CRFs)(Lafferty et al., 2001), Memory-based Learning (MBL)(Park and Zhang, 2003), Transformation-based Learning (TBL)(Brill, 1995), and Hidden Markov Models (HMMs)(Zhou et al., 2000), have been applied to text chunking(Sang and Buchholz, 2000; Hammerton et al., 2002). Chinese chunking is a difficult task, and much work has been done on this topic(Li et al., 2003a; Tan et al., 2005; Wu et al., 2005; Zhao et al., 2000). However, there are many different Chinese chunk definitions, which are derived from different data sets(Li et al., 2004; Zhang and Zhou, 2002). Therefore, comparing the performance of previous studies in Chinese chunking is very difficult. Furthermore, compared with the other languages, there are some special problems for Chinese chunking(Li et al., 2004). In this paper, we extracted the chunking corpus from UPENN Chinese Treebank-4(CTB4). We presented an empirical study of Chinese chunking on this corpus. First, we made an evaluation on the corpus to clarify the performa"
P06-2013,W04-1107,0,0.131808,"89 {chenwl, yujie, isahara}@nict.go.jp Abstract Conditional Random Fields (CRFs)(Lafferty et al., 2001), Memory-based Learning (MBL)(Park and Zhang, 2003), Transformation-based Learning (TBL)(Brill, 1995), and Hidden Markov Models (HMMs)(Zhou et al., 2000), have been applied to text chunking(Sang and Buchholz, 2000; Hammerton et al., 2002). Chinese chunking is a difficult task, and much work has been done on this topic(Li et al., 2003a; Tan et al., 2005; Wu et al., 2005; Zhao et al., 2000). However, there are many different Chinese chunk definitions, which are derived from different data sets(Li et al., 2004; Zhang and Zhou, 2002). Therefore, comparing the performance of previous studies in Chinese chunking is very difficult. Furthermore, compared with the other languages, there are some special problems for Chinese chunking(Li et al., 2004). In this paper, we extracted the chunking corpus from UPENN Chinese Treebank-4(CTB4). We presented an empirical study of Chinese chunking on this corpus. First, we made an evaluation on the corpus to clarify the performance of stateof-the-art models in Chinese chunking. Then we proposed two approaches in order to improve the performance of Chinese chunking. 1"
P06-2013,W02-1818,0,0.0174475,", isahara}@nict.go.jp Abstract Conditional Random Fields (CRFs)(Lafferty et al., 2001), Memory-based Learning (MBL)(Park and Zhang, 2003), Transformation-based Learning (TBL)(Brill, 1995), and Hidden Markov Models (HMMs)(Zhou et al., 2000), have been applied to text chunking(Sang and Buchholz, 2000; Hammerton et al., 2002). Chinese chunking is a difficult task, and much work has been done on this topic(Li et al., 2003a; Tan et al., 2005; Wu et al., 2005; Zhao et al., 2000). However, there are many different Chinese chunk definitions, which are derived from different data sets(Li et al., 2004; Zhang and Zhou, 2002). Therefore, comparing the performance of previous studies in Chinese chunking is very difficult. Furthermore, compared with the other languages, there are some special problems for Chinese chunking(Li et al., 2004). In this paper, we extracted the chunking corpus from UPENN Chinese Treebank-4(CTB4). We presented an empirical study of Chinese chunking on this corpus. First, we made an evaluation on the corpus to clarify the performance of stateof-the-art models in Chinese chunking. Then we proposed two approaches in order to improve the performance of Chinese chunking. 1) We proposed an approa"
P06-2013,W00-1211,0,0.0256491,"National Institute of Information and Communications Technology 3-5 Hikari-dai, Seika-cho, Soraku-gun, Kyoto, Japan, 619-0289 {chenwl, yujie, isahara}@nict.go.jp Abstract Conditional Random Fields (CRFs)(Lafferty et al., 2001), Memory-based Learning (MBL)(Park and Zhang, 2003), Transformation-based Learning (TBL)(Brill, 1995), and Hidden Markov Models (HMMs)(Zhou et al., 2000), have been applied to text chunking(Sang and Buchholz, 2000; Hammerton et al., 2002). Chinese chunking is a difficult task, and much work has been done on this topic(Li et al., 2003a; Tan et al., 2005; Wu et al., 2005; Zhao et al., 2000). However, there are many different Chinese chunk definitions, which are derived from different data sets(Li et al., 2004; Zhang and Zhou, 2002). Therefore, comparing the performance of previous studies in Chinese chunking is very difficult. Furthermore, compared with the other languages, there are some special problems for Chinese chunking(Li et al., 2004). In this paper, we extracted the chunking corpus from UPENN Chinese Treebank-4(CTB4). We presented an empirical study of Chinese chunking on this corpus. First, we made an evaluation on the corpus to clarify the performance of stateof-the-a"
P06-2013,P03-1063,0,0.0370304,"Missing"
P06-2013,W00-0737,0,0.089585,"Missing"
P06-2013,W95-0107,0,0.0145654,"opose two novel voting methods based on the characteristics of chunking task. Compared with traditional voting methods, the proposed voting methods consider long distance information. The experimental results show that the SVMs model outperforms the other models and that our proposed approaches can improve performance significantly. 1 Introduction Chunking identifies the non-recursive cores of various types of phrases in text, possibly as a precursor to full parsing or information extraction. Steven P. Abney was the first person to introduce chunks for parsing(Abney, 1991). Ramshaw and Marcus(Ramshaw and Marcus, 1995) first represented base noun phrase recognition as a machine learning problem. In 2000, CoNLL-2000 introduced a shared task to tag many kinds of phrases besides noun phrases in English(Sang and Buchholz, 2000). Additionally, many machine learning approaches, such as Support Vector Machines (SVMs)(Vapnik, 1995), 97 Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 97–104, c Sydney, July 2006. 2006 Association for Computational Linguistics Each chunk type could be extended with I or B tags. For instance, NP could be represented as two types of tags, B-NP or I-NP. Therefor"
P06-2013,W00-0726,0,0.168927,"Missing"
P06-2013,N03-1028,0,0.563936,"inside a chunk), O (outside a chunk), and B (inside a chunk, but also the first word of the chunk). 3 Chinese Chunking 3.1 Models for Chinese Chunking In this paper, we applied four models, including SVMs, CRFs, TBL, and MBL, which have 1 More detailed information at achieved good performance in other languages. http://www.cis.upenn.edu/ chinese/. 2 Tool is available at We only describe these models briefly since full http://www.nlplab.cn/chenwl/tools/chunklinkctb.txt. details are presented elsewhere(Kudo and Mat3 Tool is available at http://ilk.uvt.nl/software.html#chunklink. 4 sumoto, 2001; Sha and Pereira, 2003; Ramshaw There are 15 types in the Upenn Chinese TreeBank. The other chunk types are FRAG, PRN, and UCP. and Marcus, 1995; Sang, 2002). 98 3.1.4 MBL Memory-based Learning (also called instance based learning) is a non-parametric inductive learning paradigm that stores training instances in a memory structure on which predictions of new instances are based(Walter et al., 1999). The similarity between the new instance X and example Y in memory is computed using a distance metric. Tjong Kim Sang(Sang, 2002) applied memorybased learning(MBL) to English chunking. MBL performs well for a variety of"
P06-2013,P98-1081,0,0.0903512,"Missing"
P06-2013,W99-0707,0,\N,Missing
P06-2013,C98-1078,0,\N,Missing
P06-2013,W03-0419,0,\N,Missing
P14-2026,W13-2239,0,0.0189713,"-ordering rules. They are: (b) Stanford typed dependency parse tree Figure 1: A constituent parse tree and its corresponding Stanford typed dependency parse tree for the same Chinese sentence. spent more than two months discovering the rules introduced in this paper. By applying our rules and Wang et al.’s rules, one can use both dependency and constituency parsers for pre-ordering in Chinese-English PBSMT. This is especially important on the point of the system combination of PBSMT systems, because the diversity of outputs from machine translation systems is important for system combination (Cer et al., 2013). By using both our rules and Wang et al.’s rules, one can obtain diverse machine translation results because the pre-ordering results of these two rule sets are generally different. Another similar work is that of (Xu et al., 2009). They created a pre-ordering rule set for dependency parsers from English to several SOV languages. In contrast, our rule set is for ChineseEnglish PBSMT. That is, the direction of translation is opposite. Because there are a lot of language specific decisions that reflect specific aspects of the source language and the language pair combination, our rule set provi"
P14-2026,C10-1071,0,0.0152835,"procedure into phrase-based SMT (PBSMT) were effective. These pre-ordering approaches first parse the source language sentences to create parse trees. Then, syntactic reordering rules are applied to these parse trees with the goal of reordering the source language sentences into the word order of the target language. Syntax-based pre-ordering by employing constituent parsing have demonstrated effectiveness in many language pairs, such as English-French (Xia and McCord, 2004), German-English (Collins et al., 2005), Chinese-English (Wang et al., 2007; Zhang et al., 2008), and English-Japanese (Lee et al., 2010). ∗ This work was done when the first author was on an internship in NICT. 155 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 155–160, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics Figure 2: An example of a preposition phrase with a plmod structure. The phrase translates into “in front of the US embassy”. (a) A constituent parse tree Chinese sentence. As shown in the figure, the number of nodes in the dependency parse tree (i.e. 9) is much fewer than that in its corresponding constituen"
P14-2026,W09-2307,0,0.0252693,"parse tree Chinese sentence. As shown in the figure, the number of nodes in the dependency parse tree (i.e. 9) is much fewer than that in its corresponding constituent parse tree (i.e. 17). Because dependency parse trees are generally more concise than the constituent ones, they can conduct longdistance reorderings in a finer way. Thus, we attempted to conduct pre-ordering based on dependency parsing. There are two widely-used dependency systems – Stanford typed dependencies and CoNLL typed dependencies. For Chinese, there are 45 types of grammatical relations for Stanford typed dependencies (Chang et al., 2009) and 25 for CoNLL typed dependencies. As we thought that Stanford typed dependencies could describe language phenomena more meticulously owing to more types of grammatical relations, we preferred to use it for searching candidate preordering rules. We designed two types of formats in our dependency-based pre-ordering rules. They are: (b) Stanford typed dependency parse tree Figure 1: A constituent parse tree and its corresponding Stanford typed dependency parse tree for the same Chinese sentence. spent more than two months discovering the rules introduced in this paper. By applying our rules a"
P14-2026,P02-1040,0,0.0985432,",752 547,084 Table 1: The comparison of four systems, including the performance (BLEU) on the test set, the total count of each rule set and the number of sentences they were applied to on the training set. tracted from the Linguistic Data Consortium’s parallel news corpora. Our development set was the official NIST MT evaluation data from 2002 to 2005, consisting of 4476 Chinese-English sentences pairs. Our test set was the NIST 2006 MT evaluation data, consisting of 1664 sentence pairs. We employed the Stanford Segmenter1 to segment all of the data sets. For evaluation, we used BLEU scores (Papineni et al., 2002). We implemented the constituent-based preordering rule set in Wang et al. (2007) for comparison, which is called WR07 below. The Berkeley Parser (Petrov et al., 2006) was employed for parsing the Chinese sentences. For training the Berkeley Parser, we used Chinese Treebank (CTB) 7.0. We conducted our dependency-based preordering experiments on the Berkeley Parser and the Mate Parser (Bohnet, 2010), which were shown to be the two best parsers for Stanford typed dependencies (Che et al., 2012). First, we converted the constituent parse trees in the results of the Berkeley Parser into dependency"
P14-2026,P06-1055,0,0.00873215,"were applied to on the training set. tracted from the Linguistic Data Consortium’s parallel news corpora. Our development set was the official NIST MT evaluation data from 2002 to 2005, consisting of 4476 Chinese-English sentences pairs. Our test set was the NIST 2006 MT evaluation data, consisting of 1664 sentence pairs. We employed the Stanford Segmenter1 to segment all of the data sets. For evaluation, we used BLEU scores (Papineni et al., 2002). We implemented the constituent-based preordering rule set in Wang et al. (2007) for comparison, which is called WR07 below. The Berkeley Parser (Petrov et al., 2006) was employed for parsing the Chinese sentences. For training the Berkeley Parser, we used Chinese Treebank (CTB) 7.0. We conducted our dependency-based preordering experiments on the Berkeley Parser and the Mate Parser (Bohnet, 2010), which were shown to be the two best parsers for Stanford typed dependencies (Che et al., 2012). First, we converted the constituent parse trees in the results of the Berkeley Parser into dependency parse trees by employing a tool in the Stanford Parser (Klein and Manning, 2003). For the Mate Parser, POS tagged inputs are required both in training and in inferenc"
P14-2026,P12-2003,0,0.0130816,"yed the Stanford Segmenter1 to segment all of the data sets. For evaluation, we used BLEU scores (Papineni et al., 2002). We implemented the constituent-based preordering rule set in Wang et al. (2007) for comparison, which is called WR07 below. The Berkeley Parser (Petrov et al., 2006) was employed for parsing the Chinese sentences. For training the Berkeley Parser, we used Chinese Treebank (CTB) 7.0. We conducted our dependency-based preordering experiments on the Berkeley Parser and the Mate Parser (Bohnet, 2010), which were shown to be the two best parsers for Stanford typed dependencies (Che et al., 2012). First, we converted the constituent parse trees in the results of the Berkeley Parser into dependency parse trees by employing a tool in the Stanford Parser (Klein and Manning, 2003). For the Mate Parser, POS tagged inputs are required both in training and in inference. Thus, we then extracted the POS information from the results of the Berkeley Parser and used these as the pre-specified POS tags for the Mate Parser. Finally, we applied our dependency-based pre-ordering rule set to the dependency parse trees created from the converted Berkeley Parser and the Mate Parser, respectively. Table"
P14-2026,P05-1066,0,0.155941,"pairs. Previous work has shown that the approaches tackling the problem by introducing a pre-ordering procedure into phrase-based SMT (PBSMT) were effective. These pre-ordering approaches first parse the source language sentences to create parse trees. Then, syntactic reordering rules are applied to these parse trees with the goal of reordering the source language sentences into the word order of the target language. Syntax-based pre-ordering by employing constituent parsing have demonstrated effectiveness in many language pairs, such as English-French (Xia and McCord, 2004), German-English (Collins et al., 2005), Chinese-English (Wang et al., 2007; Zhang et al., 2008), and English-Japanese (Lee et al., 2010). ∗ This work was done when the first author was on an internship in NICT. 155 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 155–160, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics Figure 2: An example of a preposition phrase with a plmod structure. The phrase translates into “in front of the US embassy”. (a) A constituent parse tree Chinese sentence. As shown in the figure, the number of n"
P14-2026,D07-1077,0,0.265653,"approaches tackling the problem by introducing a pre-ordering procedure into phrase-based SMT (PBSMT) were effective. These pre-ordering approaches first parse the source language sentences to create parse trees. Then, syntactic reordering rules are applied to these parse trees with the goal of reordering the source language sentences into the word order of the target language. Syntax-based pre-ordering by employing constituent parsing have demonstrated effectiveness in many language pairs, such as English-French (Xia and McCord, 2004), German-English (Collins et al., 2005), Chinese-English (Wang et al., 2007; Zhang et al., 2008), and English-Japanese (Lee et al., 2010). ∗ This work was done when the first author was on an internship in NICT. 155 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 155–160, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics Figure 2: An example of a preposition phrase with a plmod structure. The phrase translates into “in front of the US embassy”. (a) A constituent parse tree Chinese sentence. As shown in the figure, the number of nodes in the dependency parse tree (i"
P14-2026,W12-4207,0,0.229959,"Missing"
P14-2026,I11-1004,0,0.148624,"Missing"
P14-2026,2007.mtsummit-papers.29,0,0.39493,"Missing"
P14-2026,W10-1736,0,0.108857,"Missing"
P14-2026,C04-1073,0,0.257529,"in SMT systems between distant language pairs. Previous work has shown that the approaches tackling the problem by introducing a pre-ordering procedure into phrase-based SMT (PBSMT) were effective. These pre-ordering approaches first parse the source language sentences to create parse trees. Then, syntactic reordering rules are applied to these parse trees with the goal of reordering the source language sentences into the word order of the target language. Syntax-based pre-ordering by employing constituent parsing have demonstrated effectiveness in many language pairs, such as English-French (Xia and McCord, 2004), German-English (Collins et al., 2005), Chinese-English (Wang et al., 2007; Zhang et al., 2008), and English-Japanese (Lee et al., 2010). ∗ This work was done when the first author was on an internship in NICT. 155 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 155–160, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics Figure 2: An example of a preposition phrase with a plmod structure. The phrase translates into “in front of the US embassy”. (a) A constituent parse tree Chinese sentence."
P14-2026,D11-1017,0,0.0265976,"Missing"
P14-2026,N09-1028,0,0.380798,"munications Technology joycetsai99@gmail.com {mutiyama, eiichiro.sumita}@nict.go.jp yjzhang@bjtu.edu.cn Abstract As a kind of constituent structure, HPSG (Pollard and Sag, 1994) parsing-based pre-ordering showed improvements in SVO-SOV translations, such as English-Japanese (Isozaki et al., 2010; Wu et al., 2011) and Chinese-Japanese (Han et al., 2012). Since dependency parsing is more concise than constituent parsing in describing sentences, some research has used dependency parsing in pre-ordering approaches for language pairs such as Arabic-English (Habash, 2007), and EnglishSOV languages (Xu et al., 2009; Katz-Brown et al., 2011). The pre-ordering rules can be made manually (Collins et al., 2005; Wang et al., 2007; Han et al., 2012) or extracted automatically from a parallel corpus (Xia and McCord, 2004; Habash, 2007; Zhang et al., 2007; Wu et al., 2011). The purpose of this paper is to introduce a novel dependency-based pre-ordering approach through creating a pre-ordering rule set and applying it to the Chinese-English PBSMT system. Experiment results showed that our pre-ordering rule set improved the BLEU score on the NIST 2006 evaluation data by 1.61. Moreover, this rule set substantially"
P14-2026,C08-1137,0,0.0190934,"g the problem by introducing a pre-ordering procedure into phrase-based SMT (PBSMT) were effective. These pre-ordering approaches first parse the source language sentences to create parse trees. Then, syntactic reordering rules are applied to these parse trees with the goal of reordering the source language sentences into the word order of the target language. Syntax-based pre-ordering by employing constituent parsing have demonstrated effectiveness in many language pairs, such as English-French (Xia and McCord, 2004), German-English (Collins et al., 2005), Chinese-English (Wang et al., 2007; Zhang et al., 2008), and English-Japanese (Lee et al., 2010). ∗ This work was done when the first author was on an internship in NICT. 155 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 155–160, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics Figure 2: An example of a preposition phrase with a plmod structure. The phrase translates into “in front of the US embassy”. (a) A constituent parse tree Chinese sentence. As shown in the figure, the number of nodes in the dependency parse tree (i.e. 9) is much fewer"
P14-2026,P03-1054,0,0.00382741,"et in Wang et al. (2007) for comparison, which is called WR07 below. The Berkeley Parser (Petrov et al., 2006) was employed for parsing the Chinese sentences. For training the Berkeley Parser, we used Chinese Treebank (CTB) 7.0. We conducted our dependency-based preordering experiments on the Berkeley Parser and the Mate Parser (Bohnet, 2010), which were shown to be the two best parsers for Stanford typed dependencies (Che et al., 2012). First, we converted the constituent parse trees in the results of the Berkeley Parser into dependency parse trees by employing a tool in the Stanford Parser (Klein and Manning, 2003). For the Mate Parser, POS tagged inputs are required both in training and in inference. Thus, we then extracted the POS information from the results of the Berkeley Parser and used these as the pre-specified POS tags for the Mate Parser. Finally, we applied our dependency-based pre-ordering rule set to the dependency parse trees created from the converted Berkeley Parser and the Mate Parser, respectively. Table 1 presents a comparison of the system without pre-ordering, the constituent system using WR07 and two dependency systems employing the converted Berkeley Parser and the Mate Parser, re"
P14-2026,P07-2045,0,0.00682969,"Missing"
P14-2026,W07-0401,0,\N,Missing
W03-1714,J93-2003,0,\N,Missing
W03-1714,C02-1060,1,\N,Missing
W03-1714,C88-1016,0,\N,Missing
W03-1714,C92-2101,0,\N,Missing
W03-1714,P95-1033,0,\N,Missing
W03-1714,P93-1004,0,\N,Missing
W03-1714,C02-1032,0,\N,Missing
W04-2208,C00-1007,0,0.0337696,"h expressions corresponding to a Japanese expression is 1.3 as shown in Table 2. Even when there are two or more possible English expressions, an appropriate English expression can be chosen by selecting a Japanese expression by referring to dependencies in extracted translation pairs. Therefore, in many cases, English sentences can be generated just by reordering the selected expressions. The English word order was estimated manually in this experiment. However, we can automatically estimate English word order by using a language model or an English surface sentence generator such as FERGUS (Bangalore and Rambow, 2000). Unnatural or ungrammatical parallel translations are sometimes generated in the above steps. However, comprehensible translations can be generated as shown in Figure 4. The biggest advantage of this framework is that comprehensible target sentences can be generated basically by referring only to source sentences. Although it is costly to search and select appropriate translation pairs, we believe that human labor can be reduced by developing a human interface. For example, when we use a Japanese text generation system from keywords (Uchimoto et al., 2002), users should only select appropriat"
W04-2208,P01-1030,0,0.0114299,"o a given sentence can be semiautomatically generated. In this paper we show that the framework can be achieved by using our aligned parallel treebank corpus. 1 ‡ New York University 715 Broadway, 7th floor 3-5 Hikari-dai, Seika-cho, Soraku-gun, New York, NY 10003, USA Kyoto 619-0289, Japan {sudo,sekine}@cs.nyu.edu {uchimoto,yujie,murata,isahara}@nict.go.jp Abstract pora and do not have bilingual or multilinNational Institute of Information and Communications Technology Introduction Recently, accurate machine translation systems can be constructed by using parallel corpora (Och and Ney, 2000; Germann et al., 2001). However, almost all existing machine translation systems do not consider the problem of translating a given sentence into a natural sentence reﬂecting its contextual information in the target language. One of the main reasons for this is that we had many problems that had to be solved by one-sentence to one-sentence machine translation before we could solve the contextual problem. Another reason is that it was diﬃcult to simply investigate the inﬂuence of the context on the translation because sentence correspondences of the existing bilingual documents are rarely one-to-one, and are usually"
W04-2208,2002.tmi-papers.9,0,0.015857,"and Japanese-English machine translation. We can directly compare various methods of machine translation by using this corpus. It can be summarized as follows in terms of the characteristics of the corpus. One-sentence to one-sentence translation can be simply used for the evaluation of various methods of machine translation. Morphological and syntactic information can be used for the evaluation of methods that actively use morphological and syntactic information, such as methods for examplebased machine translation (Nagao, 1981; Watanabe et al., 2003), or transfer-based machine translation (Imamura, 2002). Phrasal alignment is used for the evaluation of automatically acquired translation knowledge (Yamamoto and Matsumoto, 2003). An actual comparison and evaluation is our future work. 3.2 Analysis of Translation One-sentence to one-sentence translation reﬂects contextual information. Therefore, it is suitable to investigate the inﬂuence of the context on the translation. For example, we can investigate the diﬀerence in the use of demonstratives and pronouns between English and Japanese. We can also investigate the diﬀerence in the use of anaphora. Morphological and syntactic information and phr"
W04-2208,J93-2004,0,0.0235999,"ng a given sentence into a natural sentence reﬂecting its contextual information in the target language. One of the main reasons for this is that we had many problems that had to be solved by one-sentence to one-sentence machine translation before we could solve the contextual problem. Another reason is that it was diﬃcult to simply investigate the inﬂuence of the context on the translation because sentence correspondences of the existing bilingual documents are rarely one-to-one, and are usually one-to-many or many-to-many. On the other hand, high-quality treebanks such as the Penn Treebank (Marcus et al., 1993) and the Kyoto University text corpus (Kurohashi and Nagao, 1997) have contributed to improving the accuracies of fundamental techniques for natural language processing such as morphological analysis and syntactic structure analysis. However, almost all of these highquality treebanks are based on monolingual corgual information. There are few high-quality bilingual or multilingual treebank corpora because parallel corpora have mainly been actively used for machine translation between related languages such as English and French, therefore their syntactic structures are not required so much for"
W04-2208,P00-1056,0,0.126608,"ntence is similar to a given sentence can be semiautomatically generated. In this paper we show that the framework can be achieved by using our aligned parallel treebank corpus. 1 ‡ New York University 715 Broadway, 7th floor 3-5 Hikari-dai, Seika-cho, Soraku-gun, New York, NY 10003, USA Kyoto 619-0289, Japan {sudo,sekine}@cs.nyu.edu {uchimoto,yujie,murata,isahara}@nict.go.jp Abstract pora and do not have bilingual or multilinNational Institute of Information and Communications Technology Introduction Recently, accurate machine translation systems can be constructed by using parallel corpora (Och and Ney, 2000; Germann et al., 2001). However, almost all existing machine translation systems do not consider the problem of translating a given sentence into a natural sentence reﬂecting its contextual information in the target language. One of the main reasons for this is that we had many problems that had to be solved by one-sentence to one-sentence machine translation before we could solve the contextual problem. Another reason is that it was diﬃcult to simply investigate the inﬂuence of the context on the translation because sentence correspondences of the existing bilingual documents are rarely one-"
W04-2208,C02-1064,1,0.816552,"tence generator such as FERGUS (Bangalore and Rambow, 2000). Unnatural or ungrammatical parallel translations are sometimes generated in the above steps. However, comprehensible translations can be generated as shown in Figure 4. The biggest advantage of this framework is that comprehensible target sentences can be generated basically by referring only to source sentences. Although it is costly to search and select appropriate translation pairs, we believe that human labor can be reduced by developing a human interface. For example, when we use a Japanese text generation system from keywords (Uchimoto et al., 2002), users should only select appropriate keywords. We are investigating whether or not we can generate similar parallel translations to all of the Japanese sentences appearing on January 17, 1995. So far, we found that we can generate similar parallel translations to 691 out of 840 sentences (the average number of bunsetsus is about 10.3) including the 102 sentences described in Section 3.3. We found that we could not generate similar parallel translations to 149 out of 840 sentences. In the proposed framework of similar parallel translation generation, the language appearing in a corpus corresp"
W04-2208,P01-1067,0,0.102382,"erefore their syntactic structures are not required so much for aligning words or phrases. However, syntactic structures are necessary for machine translation between languages whose syntactic structures are diﬀerent from each other, such as in Japanese-English, Japanese-Chinese, and Chinese-English machine translations, because it is more diﬃcult to automatically align words or phrases between two unrelated languages than between two related languages. Actually, it has been reported that syntactic structures contribute to improving the accuracy of word alignment between Japanese and English (Yamada and Knight, 2001). Therefore, if we had a high-quality parallel treebank corpus, the accuracies of machine translation between languages whose syntactic structures are diﬀerent from each other would improve. Furthermore, if the parallel treebank corpus had word or phrase alignment, the accuracy of automatic word or phrase alignment would increase by using the parallel treebank corpus as training data. However, so far, there is no aligned parallel treebank corpus whose domain is not restricted. For example, the Japanese Electronics Industry Development Association’s (JEIDA’s) bilingual corpus (Isahara and Harun"
W04-2208,A00-2018,0,\N,Missing
W06-0116,N03-1028,0,0.101057,"eatures via doing statistics in training corpus. Our system incorporates basic features and additional features based on Conditional Random Fields (CRFs). In order to correct inconsistently results, we perform the postprocessing procedure according to n-best results given by the CRFs model. Our final system achieved a F-score of 85.14 at MSRA, 89.03 at CityU, and 76.27 at LDC. 1 2 Conditional Random Fields 2.1 The model Conditional Random Fields(CRFs), a statistical sequence modeling framework, was first introduced by Lafferty et al(Lafferty et al., 2001). The model has been used for chunking(Sha and Pereira, 2003). We only describe the model briefly since full details are presented in the paper(Lafferty et al., 2001). In this paper, we regard Chinese NER as a sequence labeling problem. For our sequence labeling problem, we create a linear-chain CRFs based on an undirected graph G = (V, E), where V is the set of random variables Y = {Yi |1 ≤ i ≤ n}, for each of n tokens in an input sentence and E = {(Yi−1 , Yi )|1 ≤ i ≤ n} is the set of n − 1 edges forming a linear chain. For each sentence x, we define two non-negative factors: P exp( K λk fk (yi−1 , yi , x)) for each edge k=1 PK 0 0 0 exp( k=1 λk fk (y"
W14-7005,D07-1077,0,0.0617104,"Missing"
W14-7005,C04-1073,0,0.013054,"entences which have similar word order with target language sentences. Finally, the reordered source language sentences are used in the SMT system, not only in training, but also in tuning and translating. In other words, all of the sentences in the training set, the development set and the test set should be reordered while applying these kinds of approaches. Since parsing is required in pre-ordering approaches, two popular parsing are often considered, i.e. constituent parsing and dependency parsing. Constituent parsing has been employed in pre-ordering for the translation of EnglishFrench (Xia and McCord, 2004), GermanyEnglish (Collins et al., 2005), Chinese-English Section 2 describes some issues in JapaneseChinese translation and proposes our dependencybased pre-ordering approach. Section 3 reports on our experiment results on a Japanese-Chinese phrase-based SMT (PBSMT) system. Section 4 concludes this paper. 2 Dependency-based Pre-ordering Approach Japanese is a kind of SOV language and Chinese is a kind of SVO language. Therefore, great difference exists in their word order. Moreover, both of them have very free word order, i.e. a sentence may have several expressions by only changing its word o"
W14-7005,N09-1028,0,0.0182076,"atu” to obtain an SVO word order sentence. After operating this pre-ordering, the Japanese sentence can be translated into “我/I 读/read 书/book 。/.” in Chinese, whose word order is the same as its Chinese counterpart. We investigate the phenomena of word order exchanging, such as “wo” case and “ni” case, on the ASPEC Japanese-Chinese paper excerpt corpus1 . We take statistics of the various cases and observe the samples of the cases with high frequency. Based on the observed results, we build pre-ordering rules, which consist of global preordering and local pre-ordering. Inspired by the work of Xu et al. (2009), we obtain the global pre-ordering rules in the following way. We focus on verbs and classify the cases that directly depend on verbs into several groups. We then set sequence numbers to the groups by considering the word orders of their Chinese translations in a Chinese sentence, as we observe from Issues in Japanese-Chinese Translation As is known to all, Japanese is a kind of SOV language, in which the verb (V) occurs after the object (O), and Chinese is a kind of SVO language, in which the verb (V) occurs before the object (O). This is the most common difference between Japanese and Chine"
W14-7005,P14-2026,1,0.838827,"Missing"
W14-7005,P05-1066,0,0.0498072,"with target language sentences. Finally, the reordered source language sentences are used in the SMT system, not only in training, but also in tuning and translating. In other words, all of the sentences in the training set, the development set and the test set should be reordered while applying these kinds of approaches. Since parsing is required in pre-ordering approaches, two popular parsing are often considered, i.e. constituent parsing and dependency parsing. Constituent parsing has been employed in pre-ordering for the translation of EnglishFrench (Xia and McCord, 2004), GermanyEnglish (Collins et al., 2005), Chinese-English Section 2 describes some issues in JapaneseChinese translation and proposes our dependencybased pre-ordering approach. Section 3 reports on our experiment results on a Japanese-Chinese phrase-based SMT (PBSMT) system. Section 4 concludes this paper. 2 Dependency-based Pre-ordering Approach Japanese is a kind of SOV language and Chinese is a kind of SVO language. Therefore, great difference exists in their word order. Moreover, both of them have very free word order, i.e. a sentence may have several expressions by only changing its word order. This fact causes troubles while t"
W14-7005,2007.mtsummit-papers.29,0,0.104971,"Missing"
W14-7005,D10-1092,0,0.131553,"the “wo” case, i.e. all of the tokens occurring before “を”, is moved to the position after the “bunmatu” case. Then, local pre-ordering is conducted. The particle “を” and “に” are moved to the front of the subtree of their cases, respectively. The reordered Japanese sentence can be translated 3 Experiments Section 2 describes our dependency-based preordering approach for the translation of JapaneseChinese. This section reports on our experiments and evaluation results. We use MOSES PBSMT system (Koehn et al., 2007) in our experiments and use BLEU scores (Papineni et al., 2002) and RIBES score (Isozaki et al., 2010) for evaluation. The data sets are from the ASPEC JapaneseChinese paper excerpt corpus. The training data contains 672,315 sentences, the development data contains 2,090 sentences and the test data contains 2,107 sentences. We use a bilingual-based approach proposed in Su et al. (2013) for Chinese word segmentation, in which n-gram feature from the raw Chinese part and word alignment from the parallel corpus are introduced to augment the conventional model based on annotation data. We employ the KNP parser2 for Japanese dependency parsing. The KNP parser can create dependency tree for a Japane"
W14-7005,P07-2045,0,0.00575557,"Japanese sentence is shown in Figure 4 (b). First, global pre-ordering is conducted. The subtree of the “wo” case, i.e. all of the tokens occurring before “を”, is moved to the position after the “bunmatu” case. Then, local pre-ordering is conducted. The particle “を” and “に” are moved to the front of the subtree of their cases, respectively. The reordered Japanese sentence can be translated 3 Experiments Section 2 describes our dependency-based preordering approach for the translation of JapaneseChinese. This section reports on our experiments and evaluation results. We use MOSES PBSMT system (Koehn et al., 2007) in our experiments and use BLEU scores (Papineni et al., 2002) and RIBES score (Isozaki et al., 2010) for evaluation. The data sets are from the ASPEC JapaneseChinese paper excerpt corpus. The training data contains 672,315 sentences, the development data contains 2,090 sentences and the test data contains 2,107 sentences. We use a bilingual-based approach proposed in Su et al. (2013) for Chinese word segmentation, in which n-gram feature from the raw Chinese part and word alignment from the parallel corpus are introduced to augment the conventional model based on annotation data. We employ t"
W14-7005,P02-1040,0,0.0941746,"e-ordering is conducted. The subtree of the “wo” case, i.e. all of the tokens occurring before “を”, is moved to the position after the “bunmatu” case. Then, local pre-ordering is conducted. The particle “を” and “に” are moved to the front of the subtree of their cases, respectively. The reordered Japanese sentence can be translated 3 Experiments Section 2 describes our dependency-based preordering approach for the translation of JapaneseChinese. This section reports on our experiments and evaluation results. We use MOSES PBSMT system (Koehn et al., 2007) in our experiments and use BLEU scores (Papineni et al., 2002) and RIBES score (Isozaki et al., 2010) for evaluation. The data sets are from the ASPEC JapaneseChinese paper excerpt corpus. The training data contains 672,315 sentences, the development data contains 2,090 sentences and the test data contains 2,107 sentences. We use a bilingual-based approach proposed in Su et al. (2013) for Chinese word segmentation, in which n-gram feature from the raw Chinese part and word alignment from the parallel corpus are introduced to augment the conventional model based on annotation data. We employ the KNP parser2 for Japanese dependency parsing. The KNP parser"
W15-3503,P03-2041,0,0.0333191,"aining the Hiero robustness qualities and avoiding computational explosions. We obtain significant improvements over a strong HPB baseline in the Japanese-toChinese task. We will try to improve the performance of our system with soft constraint or hard constraint using case frame rules, and we will challenge to resolve the problem of tense, aspect and some special grammatical sentences of Japanese to Chinese translation. Related Work Recently linguistically-motivated models have been intensively investigated in MT. In particular, source tree-based models (Liu et al., 2006; Huang et al., 2006; Eisner, 2003; Zhang et al., 2008; Liu et al., 2009a; Xie et al., 2011) have received growing interest due to their excellent ability to model source language syntax for better lexical selection and reordering. Alternatively, the hierarchical phrase-based approach (Chiang, 2005) considers the underlying hierarchical structures of sentences but does not require linguistically syntactic trees on either language′ s side. There are several lines of work for augmenting hierarchical phrase-based systems with the use of source language linguistic information. Xiao (2014) incorporates source syntax into the hierar"
W15-3503,2011.mtsummit-papers.34,0,0.0878711,"Missing"
W15-3503,P08-1064,0,0.0245813,"ro robustness qualities and avoiding computational explosions. We obtain significant improvements over a strong HPB baseline in the Japanese-toChinese task. We will try to improve the performance of our system with soft constraint or hard constraint using case frame rules, and we will challenge to resolve the problem of tense, aspect and some special grammatical sentences of Japanese to Chinese translation. Related Work Recently linguistically-motivated models have been intensively investigated in MT. In particular, source tree-based models (Liu et al., 2006; Huang et al., 2006; Eisner, 2003; Zhang et al., 2008; Liu et al., 2009a; Xie et al., 2011) have received growing interest due to their excellent ability to model source language syntax for better lexical selection and reordering. Alternatively, the hierarchical phrase-based approach (Chiang, 2005) considers the underlying hierarchical structures of sentences but does not require linguistically syntactic trees on either language′ s side. There are several lines of work for augmenting hierarchical phrase-based systems with the use of source language linguistic information. Xiao (2014) incorporates source syntax into the hierarchical phrase-based"
W15-3503,P09-1063,0,0.0531936,"Missing"
W15-3503,D11-1020,0,0.0226104,"mputational explosions. We obtain significant improvements over a strong HPB baseline in the Japanese-toChinese task. We will try to improve the performance of our system with soft constraint or hard constraint using case frame rules, and we will challenge to resolve the problem of tense, aspect and some special grammatical sentences of Japanese to Chinese translation. Related Work Recently linguistically-motivated models have been intensively investigated in MT. In particular, source tree-based models (Liu et al., 2006; Huang et al., 2006; Eisner, 2003; Zhang et al., 2008; Liu et al., 2009a; Xie et al., 2011) have received growing interest due to their excellent ability to model source language syntax for better lexical selection and reordering. Alternatively, the hierarchical phrase-based approach (Chiang, 2005) considers the underlying hierarchical structures of sentences but does not require linguistically syntactic trees on either language′ s side. There are several lines of work for augmenting hierarchical phrase-based systems with the use of source language linguistic information. Xiao (2014) incorporates source syntax into the hierarchical phrase-based model. They develop procedures for joi"
W15-3503,P12-3004,0,0.0465611,"Missing"
W15-3503,C02-1122,0,0.136868,"Missing"
W15-3503,kawahara-kurohashi-2006-case,0,0.0839682,"Missing"
W15-3503,W09-1201,0,0.0622702,"Missing"
W15-3503,P06-1077,0,0.0356119,"e translation search space while maintaining the Hiero robustness qualities and avoiding computational explosions. We obtain significant improvements over a strong HPB baseline in the Japanese-toChinese task. We will try to improve the performance of our system with soft constraint or hard constraint using case frame rules, and we will challenge to resolve the problem of tense, aspect and some special grammatical sentences of Japanese to Chinese translation. Related Work Recently linguistically-motivated models have been intensively investigated in MT. In particular, source tree-based models (Liu et al., 2006; Huang et al., 2006; Eisner, 2003; Zhang et al., 2008; Liu et al., 2009a; Xie et al., 2011) have received growing interest due to their excellent ability to model source language syntax for better lexical selection and reordering. Alternatively, the hierarchical phrase-based approach (Chiang, 2005) considers the underlying hierarchical structures of sentences but does not require linguistically syntactic trees on either language′ s side. There are several lines of work for augmenting hierarchical phrase-based systems with the use of source language linguistic information. Xiao (2014) incorpor"
W15-3503,2006.amta-papers.8,0,\N,Missing
W15-3503,J04-4002,0,\N,Missing
W15-3503,C14-1195,0,\N,Missing
W15-3910,J03-1002,0,0.0109206,"Missing"
W15-3910,P11-2094,0,0.0699392,"Missing"
W15-3910,W09-3519,0,0.0393993,"Missing"
W15-3910,P12-3004,0,\N,Missing
W15-3910,N03-1017,0,\N,Missing
W15-3910,J98-4003,0,\N,Missing
W15-5010,P05-1033,0,0.192811,"Missing"
W15-5010,P07-1019,0,0.0334133,"erses each internal node n of T in post-order. And we process it as follows. 1) If n is a leaf node, it checks the rule set for matched translation rules H and uses the rules to generate candidate translation; 2) If n is a internal node, it enumerates all instances of the related sentence, clauses or phrases of the HDR fragment rooted at n, and checks the translation rule set for matched translation rules. If there is no matched rules, we construct a pseudo translation rule according to the word order of the HDR fragment in the source side; 3) Make use of Cube Pruning algorithm (Chiang, 2007; Huang and Chiang, 2007) to generate the candidate translation for the node n. To balance the decoder’s performance and speed, we use four constraints as follows: 1) Beam-threshold: we get the score threshold from the best score in the current stack multiplied by a fixed ratio. The candidate translations with a score worse than the score threshold will be discarded; 2) beam-limit: the maximum number of candidate translations in the beam; 3) rule-threshold: we get the rule score threshold from the best score multiplied by a fixed ratio in the rule table queue. The rules with a score worse than rule score threshold wil"
W15-5010,D11-1020,0,0.0391198,"Missing"
W15-5010,P07-2045,0,\N,Missing
W15-5010,P02-1038,0,\N,Missing
W16-4608,D14-1179,0,0.0330016,"Missing"
W16-4608,W14-4012,0,0.111024,"Missing"
W16-4608,N16-1101,0,0.0195249,"). Most of the existing NMT models are built based on Encoder-Decoder framework (Sutskever et al., 2014; Luong et al., 2014). The encoder network encodes the source sentence into a vector, the decoder generates a target sentence. While early models encode the source sentence into a fixed-length vector. For instance, Bahdanau et al. advocate the attention mechanism to dynamically generate a context vector of the whole source sentence (Bahdanau et al., 2014) for improving the performance of the NMT. Recently, a large amount of research works focus on the attention mechanism (Cheng et al., 2015; Firat et al., 2016). In this paper, we adopt RNN, GRU and attention mechanism to build an Encoder-Decoder network as our machine translation system. Figure 1 shows the framework of our NMT. ... xTx x2 x1 Encoder h1 h2 ... hT x h1 h2 ... hT x Attention Mechanism ai2 ai1 S1 ... aiTx ci Si-1 Si yi-1 yi Decoder y1 This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http://creativecommons.org/licenses/by/4.0/ 104 Proceedings of the 3rd Workshop on Asian Translation, pages 104–110, Osaka, Japan, December 11-17 2016. Figure 1: The framework of NMT. Where x and y denote"
W16-4608,D13-1176,0,0.060454,"Missing"
W16-4608,W16-2209,0,0.049796,"Missing"
W16-4608,J03-1005,0,0.141711,"Missing"
W16-4608,L16-1350,0,0.0225278,"Missing"
W16-4608,W14-7001,0,\N,Missing
Y04-1018,C02-1011,0,0.01304,"words is an important problem related to the construction of translation dictionaries. On the one hand, the published paper bilingual dictionaries usually collect translations for simple words but not for compound words. On the other hand, new compound words emerge every day. It is reported that most compound words are nominal words. In natural language comprehension, entities or concepts to be recognized are usually described by nouns or compound nouns. Therefore, acquiring translations for compound nouns is extremely important in machine translation and cross-language information retrieval [Cao and Li, 2002; Nakagawa, 2001]. In resolving this problem, we propose a self-learning mechanism that can dynamically and automatically acquire compound words translations from the web and corpora. Here we take Japanese-Chinese language pairs as an instance for illustration and report initial experiment results. The proposed method can also be applied to other language pairs since it is language-independent. 2 Overview of the self-learning mechanism The self-learning mechanism aims at automatically and dynamically expanding the translation dictionary of compound word. It is implemented in two parts: collect"
Y04-1018,W01-1413,0,0.0314006,"osition patterns. Two examples of the obtained compound words are listed below. Ex.1. Ex.2. アーバンソシオロジー (urban sociology) Part-Of-Speech: noun Composition: ”アーバン(urban) + ソシオロジー (sociology)” Composition pattern: ”noun + noun” アーク電流 (arc current) Part-Of-Speech: noun Composition: ”アーク(arc)+ 電流(current)” Composition pattern: ”noun + noun” Table 1 Extracted composition patterns within the top 4 Composition Pattern Noun + Noun Noun + Suffix Noun + Verb Verb + Verb 3 Count 17,481 8089 6838 6322 Approaches to acquiring translations There are two points underlying our approach: (1) exploring the web [Nagata, et al., 2001] because most new compound words or terminology appear there before they appear in corpora, and (2) utilizing English translations to link the Japanese word and corresponding Chinese translations because there is an abundance of resources for Japanese-English and English-Chinese translation. For a given Japanese compound word, we can try the following approach in turn. Approach 1 First, get an English translation by looking it up in the Japanese-English dictionary or by searching the Japanese web where the Japanese compound word is followed by an English expression, which might be its English"
Y99-1026,Y98-1022,1,0.613475,"elation among clauses [3]. Then dependency structure candidates are ordered by using heuristic dependency scores between clauses. Finally the segmentation points are determined in accordance with the top candidate for the dependency structure [2]. These techniques have been reported effective. However, the problem with these conventional methods is that the segmentation patterns or the heuristic dependency scores must be given manually, hence no guarantee for optimality. This paper proposes a new method of automatic segmentation of long compound sentences using a classification tree technique [4,5,6,7] based on the surface information obtained by morphological analysis. In this method, optimal segmentation patterns and the optimal order of their application are automatically acquired from training data, linguistic phenomena together with their occurrence frequencies being taken into account. The rest of the paper describes the details of the method, and reports the experimental results on an EDR corpus, including the effects of pruning. 2. CLASSIFICATION TREE The classification tree employed in this work is of the following type: (1) It is a binary tree: each intermediate node has two child"
zhang-etal-2008-word,W04-2208,1,\N,Missing
zhang-etal-2008-word,I05-2015,1,\N,Missing
zhang-etal-2008-word,2005.mtsummit-papers.10,1,\N,Missing
zhang-etal-2008-word,maekawa-etal-2000-spontaneous,1,\N,Missing
