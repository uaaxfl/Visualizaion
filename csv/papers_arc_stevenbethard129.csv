2021.wnut-1.36,Detection of Puffery on the {E}nglish {W}ikipedia,2021,-1,-1,2,0,223,amanda bertsch,Proceedings of the Seventh Workshop on Noisy User-generated Text (W-NUT 2021),0,"On Wikipedia, an online crowdsourced encyclopedia, volunteers enforce the encyclopedia{'}s editorial policies. Wikipedia{'}s policy on maintaining a neutral point of view has inspired recent research on bias detection, including {``}weasel words{''} and {``}hedges{''}. Yet to date, little work has been done on identifying {``}puffery,{''} phrases that are overly positive without a verifiable source. We demonstrate that collecting training data for this task requires some care, and construct a dataset by combining Wikipedia editorial annotations and information retrieval techniques. We compare several approaches to predicting puffery, and achieve 0.963 f1 score by incorporating citation features into a RoBERTa model. Finally, we demonstrate how to integrate our model with Wikipedia{'}s public infrastructure to give back to the Wikipedia editor community."
2021.semeval-1.42,{S}em{E}val-2021 Task 10: Source-Free Domain Adaptation for Semantic Processing,2021,-1,-1,6,0,1744,egoitz laparra,Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021),0,"This paper presents the Source-Free Domain Adaptation shared task held within SemEval-2021. The aim of the task was to explore adaptation of machine-learning models in the face of data sharing constraints. Specifically, we consider the scenario where annotations exist for a domain but cannot be shared. Instead, participants are provided with models trained on that (source) data. Participants also receive some labeled data from a new (development) domain on which to explore domain adaptation algorithms. Participants are then tested on data representing a new (target) domain. We explored this scenario with two different semantic tasks: negation detection (a text classification task) and time expression recognition (a sequence tagging task)."
2021.semeval-1.56,"The {U}niversity of {A}rizona at {S}em{E}val-2021 Task 10: Applying Self-training, Active Learning and Data Augmentation to Source-free Domain Adaptation",2021,-1,-1,3,0,1745,xin su,Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021),0,"This paper describes our systems for negation detection and time expression recognition in SemEval 2021 Task 10, Source-Free Domain Adaptation for Semantic Processing. We show that self-training, active learning and data augmentation techniques can improve the generalization ability of the model on the unlabeled target domain data without accessing source domain data. We also perform detailed ablation studies and error analyses for our time expression recognition systems to identify the source of the performance improvement and give constructive feedback on the temporal normalization annotation guidelines."
2021.naacl-main.97,Explainable Multi-hop Verbal Reasoning Through Internal Monologue,2021,-1,-1,2,0,3539,zhengzhong liang,Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"Many state-of-the-art (SOTA) language models have achieved high accuracy on several multi-hop reasoning problems. However, these approaches tend to not be interpretable because they do not make the intermediate reasoning steps explicit. Moreover, models trained on simpler tasks tend to fail when directly tested on more complex problems. We propose the Explainable multi-hop Verbal Reasoner (EVR) to solve these limitations by (a) decomposing multi-hop reasoning problems into several simple ones, and (b) using natural language to guide the intermediate reasoning hops. We implement EVR by extending the classic reasoning paradigm General Problem Solver (GPS) with a SOTA generative language model to generate subgoals and perform inference in natural language at each reasoning step. Evaluation of EVR on the RuleTaker synthetic question answering (QA) dataset shows that EVR achieves SOTA performance while being able to generate all reasoning steps in natural language. Furthermore, EVR generalizes better than other strong methods when trained on simpler tasks or less training data (up to 35.7{\%} and 7.7{\%} absolute improvement respectively)."
2021.naacl-main.363,If You Want to Go Far Go Together: Unsupervised Joint Candidate Evidence Retrieval for Multi-hop Question Answering,2021,-1,-1,2,1,4336,vikas yadav,Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"Multi-hop reasoning requires aggregation and inference from multiple facts. To retrieve such facts, we propose a simple approach that retrieves and reranks set of evidence facts jointly. Our approach first generates unsupervised clusters of sentences as candidate evidence by accounting links between sentences and coverage with the given query. Then, a RoBERTa-based reranker is trained to bring the most representative evidence cluster to the top. We specifically emphasize on the importance of retrieving evidence jointly by showing several comparative analyses to other methods that retrieve and rerank evidence sentences individually. First, we introduce several attention- and embedding-based analyses, which indicate that jointly retrieving and reranking approaches can learn compositional knowledge required for multi-hop reasoning. Second, our experiments show that jointly retrieving candidate evidence leads to substantially higher evidence retrieval performance when fed to the same supervised reranker. In particular, our joint retrieval and then reranking approach achieves new state-of-the-art evidence retrieval performance on two multi-hop question answering (QA) datasets: 30.5 Recall@2 on QASC, and 67.6{\%} F1 on MultiRC. When the evidence text from our joint retrieval approach is fed to a RoBERTa-based answer selection classifier, we achieve new state-of-the-art QA performance on MultiRC and second best result on QASC."
2021.law-1.11,Simplifying annotation of intersections in time normalization annotation: exploring syntactic and semantic validation,2021,-1,-1,2,0,5445,peiwen su,Proceedings of The Joint 15th Linguistic Annotation Workshop (LAW) and 3rd Designing Meaning Representations (DMR) Workshop,0,"While annotating normalized times in food security documents, we found that the semantically compositional annotation for time normalization (SCATE) scheme required several near-duplicate annotations to get the correct semantics for expressions like Nov. 7th to 11th 2021. To reduce this problem, we explored replacing SCATE{'}s Sub-Interval property with a Super-Interval property, that is, making the smallest units (e.g., 7th and 11th) rather than the largest units (e.g., 2021) the heads of the intersection chains. To ensure that the semantics of annotated time intervals remained unaltered despite our changes to the syntax of the annotation scheme, we applied several different techniques to validate our changes. These validation techniques detected and allowed us to resolve several important bugs in our automated translation from Sub-Interval to Super-Interval syntax."
2021.conll-1.6,Do pretrained transformers infer telicity like humans?,2021,-1,-1,4,1,1746,yiyun zhao,Proceedings of the 25th Conference on Computational Natural Language Learning,0,"Pretrained transformer-based language models achieve state-of-the-art performance in many NLP tasks, but it is an open question whether the knowledge acquired by the models during pretraining resembles the linguistic knowledge of humans. We present both humans and pretrained transformers with descriptions of events, and measure their preference for telic interpretations (the event has a natural endpoint) or atelic interpretations (the event does not have a natural endpoint). To measure these preferences and determine what factors influence them, we design an English test and a novel-word test that include a variety of linguistic cues (noun phrase quantity, resultative structure, contextual information, temporal units) that bias toward certain interpretations. We find that humans{'} choice of telicity interpretation is reliably influenced by theoretically-motivated cues, transformer models (BERT and RoBERTa) are influenced by some (though not all) of the cues, and transformer models often rely more heavily on temporal units than humans do."
2021.bionlp-1.2,Triplet-Trained Vector Space and Sieve-Based Search Improve Biomedical Concept Normalization,2021,-1,-1,2,1,12138,dongfang xu,Proceedings of the 20th Workshop on Biomedical Language Processing,0,"Concept normalization, the task of linking textual mentions of concepts to concepts in an ontology, is critical for mining and analyzing biomedical texts. We propose a vector-space model for concept normalization, where mentions and concepts are encoded via transformer networks that are trained via a triplet objective with online hard triplet mining. The transformer networks refine existing pre-trained models, and the online triplet mining makes training efficient even with hundreds of thousands of concepts by sampling training triples within each mini-batch. We introduce a variety of strategies for searching with the trained vector-space model, including approaches that incorporate domain-specific synonyms at search time with no model retraining. Across five datasets, our models that are trained only once on their corresponding ontologies are within 3 points of state-of-the-art models that are retrained for each new domain. Our models can also be trained for each domain, achieving new state-of-the-art on multiple datasets."
2021.bionlp-1.21,{E}ntity{BERT}: Entity-centric Masking Strategy for Model Pretraining for the Clinical Domain,2021,-1,-1,4,1,12173,chen lin,Proceedings of the 20th Workshop on Biomedical Language Processing,0,"Transformer-based neural language models have led to breakthroughs for a variety of natural language processing (NLP) tasks. However, most models are pretrained on general domain data. We propose a methodology to produce a model focused on the clinical domain: continued pretraining of a model with a broad representation of biomedical terminology (PubMedBERT) on a clinical corpus along with a novel entity-centric masking strategy to infuse domain knowledge in the learning process. We show that such a model achieves superior results on clinical extraction tasks by comparing our entity-centric masking strategy with classic random masking on three clinical NLP tasks: cross-domain negation detection, document time relation (DocTimeRel) classification, and temporal relation extraction. We also evaluate our models on the PubMedQA dataset to measure the models{'} performance on a non-entity-centric task in the biomedical domain. The language addressed in this work is English."
2021.adaptnlp-1.11,Domain adaptation in practice: Lessons from a real-world information extraction pipeline,2021,-1,-1,3,0.707766,1747,timothy miller,Proceedings of the Second Workshop on Domain Adaptation for NLP,0,"Advances in transfer learning and domain adaptation have raised hopes that once-challenging NLP tasks are ready to be put to use for sophisticated information extraction needs. In this work, we describe an effort to do just that {--} combining state-of-the-art neural methods for negation detection, document time relation extraction, and aspectual link prediction, with the eventual goal of extracting drug timelines from electronic health record text. We train on the THYME colon cancer corpus and test on both the THYME brain cancer corpus and an internal corpus, and show that performance of the combined systems is unacceptable despite good performance of individual systems. Although domain adaptation shows improvements on each individual system, the model selection problem is a barrier to improving overall pipeline performance."
2020.semeval-1.240,{TTUI} at {S}em{E}val-2020 Task 11: Propaganda Detection with Transfer Learning and Ensembles,2020,-1,-1,2,0,15342,moonsung kim,Proceedings of the Fourteenth Workshop on Semantic Evaluation,0,"In this paper, we describe our approaches and systems for the SemEval-2020 Task 11 on propaganda technique detection. We fine-tuned BERT and RoBERTa pre-trained models then merged them with an average ensemble. We conducted several experiments for input representations dealing with long texts and preserving context as well as for the imbalanced class problem. Our system ranked 20th out of 36 teams with 0.398 F1 in the SI task and 14th out of 31 teams with 0.556 F1 in the TC task."
2020.louhi-1.12,Defining and Learning Refined Temporal Relations in the Clinical Narrative,2020,-1,-1,4,0,17864,kristin wrightbettner,Proceedings of the 11th International Workshop on Health Text Mining and Information Analysis,0,"We present refinements over existing temporal relation annotations in the Electronic Medical Record clinical narrative. We refined the THYME corpus annotations to more faithfully represent nuanced temporality and nuanced temporal-coreferential relations. The main contributions are in re-defining CONTAINS and OVERLAP relations into CONTAINS, CONTAINS-SUBEVENT, OVERLAP and NOTED-ON. We demonstrate that these refinements lead to substantial gains in learnability for state-of-the-art transformer models as compared to previously reported results on the original THYME corpus. We thus establish a baseline for the automatic extraction of these refined temporal relations. Although our study is done on clinical narrative, we believe it addresses far-reaching challenges that are corpus- and domain- agnostic."
2020.coling-main.81,A Dataset and Evaluation Framework for Complex Geographical Description Parsing,2020,-1,-1,2,0.57061,1744,egoitz laparra,Proceedings of the 28th International Conference on Computational Linguistics,0,"Much previous work on geoparsing has focused on identifying and resolving individual toponyms in text like Adrano, S.Maria di Licodia or Catania. However, geographical locations occur not only as individual toponyms, but also as compositions of reference geolocations joined and modified by connectives, e.g., {``}. . . between the towns of Adrano and S.Maria di Licodia, 32 kilometres northwest of Catania{''}. Ideally, a geoparser should be able to take such text, and the geographical shapes of the toponyms referenced within it, and parse these into a geographical shape, formed by a set of coordinates, that represents the location described. But creating a dataset for this complex geoparsing task is difficult and, if done manually, would require a huge amount of effort to annotate the geographical shapes of not only the geolocation described but also the reference toponyms. We present an approach that automates most of the process by combining Wikipedia and OpenStreetMap. As a result, we have gathered a collection of 360,187 uncurated complex geolocation descriptions, from which we have manually curated 1,000 examples intended to be used as a test set. To accompany the data, we define a new geoparsing evaluation framework along with a scoring methodology and a set of baselines."
2020.bionlp-1.7,A {BERT}-based One-Pass Multi-Task Model for Clinical Temporal Relation Extraction,2020,-1,-1,5,1,12173,chen lin,Proceedings of the 19th SIGBioMed Workshop on Biomedical Language Processing,0,"Recently BERT has achieved a state-of-the-art performance in temporal relation extraction from clinical Electronic Medical Records text. However, the current approach is inefficient as it requires multiple passes through each input sequence. We extend a recently-proposed one-pass model for relation classification to a one-pass model for relation extraction. We augment this framework by introducing global embeddings to help with long-distance relation inference, and by multi-task learning to increase model performance and generalizability. Our proposed model produces results on par with the state-of-the-art in temporal relation extraction on the THYME corpus and is much {``}greener{''} in computational cost."
2020.bea-1.11,Assisting Undergraduate Students in Writing {S}panish Methodology Sections,2020,-1,-1,2,0,22292,samuel gonzalezlopez,Proceedings of the Fifteenth Workshop on Innovative Use of NLP for Building Educational Applications,0,"In undergraduate theses, a good methodology section should describe the series of steps that were followed in performing the research. To assist students in this task, we develop machine-learning models and an app that uses them to provide feedback while students write. We construct an annotated corpus that identifies sentences representing methodological steps and labels when a methodology contains a logical sequence of such steps. We train machine-learning models based on language modeling and lexical features that can identify sentences representing methodological steps with 0.939 f-measure, and identify methodology sections containing a logical sequence of steps with an accuracy of 87{\%}. We incorporate these models into a Microsoft Office Add-in, and show that students who improved their methodologies according to the model feedback received better grades on their methodologies."
2020.alw-1.4,Fine-tuning for multi-domain and multi-label uncivil language detection,2020,-1,-1,6,0,22384,kadir ozler,Proceedings of the Fourth Workshop on Online Abuse and Harms,0,"Incivility is a problem on social media, and it comes in many forms (name-calling, vulgarity, threats, etc.) and domains (microblog posts, online news comments, Wikipedia edits, etc.). Training machine learning models to detect such incivility must handle the multi-label and multi-domain nature of the problem. We present a BERT-based model for incivility detection and propose several approaches for training it for multi-label and multi-domain datasets. We find that individual binary classifiers outperform a joint multi-label classifier, and that simply combining multiple domains of training data outperforms other recently-proposed fine tuning strategies. We also establish new state-of-the-art performance on several incivility detection datasets."
2020.acl-main.414,Unsupervised Alignment-based Iterative Evidence Retrieval for Multi-hop Question Answering,2020,44,0,2,1,4336,vikas yadav,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,1,"Evidence retrieval is a critical stage of question answering (QA), necessary not only to improve performance, but also to explain the decisions of the QA method. We introduce a simple, fast, and unsupervised iterative evidence retrieval method, which relies on three ideas: (a) an unsupervised alignment approach to soft-align questions and answers with justification sentences using only GloVe embeddings, (b) an iterative process that reformulates queries focusing on terms that are not covered by existing justifications, which (c) stops when the terms in the given question and candidate answers are covered by the retrieved justifications. Despite its simplicity, our approach outperforms all the previous methods (including supervised methods) on the evidence selection task on two datasets: MultiRC and QASC. When these evidence sentences are fed into a RoBERTa answer classification component, we achieve state-of-the-art QA performance on these two datasets."
2020.acl-main.429,How does {BERT}{'}s attention change when you fine-tune? An analysis methodology and a case study in negation scope,2020,-1,-1,2,1,1746,yiyun zhao,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,1,"Large pretrained language models like BERT, after fine-tuning to a downstream task, have achieved high performance on a variety of NLP problems. Yet explaining their decisions is difficult despite recent work probing their internal representations. We propose a procedure and analysis methods that take a hypothesis of how a transformer-based model might encode a linguistic phenomenon, and test the validity of that hypothesis based on a comparison between knowledge-related downstream tasks with downstream control tasks, and measurement of cross-dataset consistency. We apply this methodology to test BERT and RoBERTa on a hypothesis that some attention heads will consistently attend from a word in negation scope to the negation cue. We find that after fine-tuning BERT and RoBERTa on a negation scope task, the average attention head improves its sensitivity to negation and its attention consistency across negation datasets compared to the pre-trained models. However, only the base models (not the large models) improve compared to a control task, indicating there is evidence for a shallow encoding of negation only in the base models."
2020.acl-main.748,A Generate-and-Rank Framework with Semantic Type Regularization for Biomedical Concept Normalization,2020,-1,-1,3,1,12138,dongfang xu,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,1,"Concept normalization, the task of linking textual mentions of concepts to concepts in an ontology, is challenging because ontologies are large. In most cases, annotated datasets cover only a small sample of the concepts, yet concept normalizers are expected to predict all concepts in the ontology. In this paper, we propose an architecture consisting of a candidate generator and a list-wise ranker based on BERT. The ranker considers pairings of concept mentions and candidate concepts, allowing it to make predictions for any concept, not just those seen during training. We further enhance this list-wise approach with a semantic type regularizer that allows the model to incorporate semantic type information from the ontology during training. Our proposed concept normalization framework achieves state-of-the-art performance on multiple datasets."
W19-2506,Inferring missing metadata from environmental policy texts,2019,0,0,1,1,224,steven bethard,"Proceedings of the 3rd Joint {SIGHUM} Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature",0,"The National Environmental Policy Act (NEPA) provides a trove of data on how environmental policy decisions have been made in the United States over the last 50 years. Unfortunately, there is no central database for this information and it is too voluminous to assess manually. We describe our efforts to enable systematic research over US environmental policy by extracting and organizing metadata from the text of NEPA documents. Our contributions include collecting more than 40,000 NEPA-related documents, and evaluating rule-based baselines that establish the difficulty of three important tasks: identifying lead agencies, aligning document versions, and detecting reused text."
W19-1908,A {BERT}-based Universal Model for Both Within- and Cross-sentence Clinical Temporal Relation Extraction,2019,-1,-1,4,1,12173,chen lin,Proceedings of the 2nd Clinical Natural Language Processing Workshop,0,"Classic methods for clinical temporal relation extraction focus on relational candidates within a sentence. On the other hand, break-through Bidirectional Encoder Representations from Transformers (BERT) are trained on large quantities of arbitrary spans of contiguous text instead of sentences. In this study, we aim to build a sentence-agnostic framework for the task of CONTAINS temporal relation extraction. We establish a new state-of-the-art result for the task, 0.684F for in-domain (0.055-point improvement) and 0.565F for cross-domain (0.018-point improvement), by fine-tuning BERT and pre-training domain-specific BERT models on sentence-agnostic temporal relation instances with WordPiece-compatible encodings, and augmenting the labeled data with automatically generated {``}silver{''} instances."
S19-2232,{U}niversity of {A}rizona at {S}em{E}val-2019 Task 12: Deep-Affix Named Entity Recognition of Geolocation Entities,2019,0,2,5,1,4336,vikas yadav,Proceedings of the 13th International Workshop on Semantic Evaluation,0,"We present the Named Entity Recognition (NER) and disambiguation model used by the University of Arizona team (UArizona) for the SemEval 2019 task 12. We achieved fourth place on tasks 1 and 3. We implemented a deep-affix based LSTM-CRF NER model for task 1, which utilizes only character, word, pre- fix and suffix information for the identification of geolocation entities. Despite using just the training data provided by task organizers and not using any lexicon features, we achieved 78.85{\%} strict micro F-score on task 1. We used the unsupervised population heuristics for task 3 and achieved 52.99{\%} strict micro-F1 score in this task."
S19-1008,Pre-trained Contextualized Character Embeddings Lead to Major Improvements in Time Normalization: a Detailed Analysis,2019,0,0,3,1,12138,dongfang xu,Proceedings of the Eighth Joint Conference on Lexical and Computational Semantics (*{SEM} 2019),0,"Recent studies have shown that pre-trained contextual word embeddings, which assign the same word different vectors in different contexts, improve performance in many tasks. But while contextual embeddings can also be trained at the character level, the effectiveness of such embeddings has not been studied. We derive character-level contextual embeddings from Flair (Akbik et al., 2018), and apply them to a time normalization task, yielding major performance improvements over the previous state-of-the-art: 51{\%} error reduction in news and 33{\%} in clinical notes. We analyze the sources of these improvements, and find that pre-trained contextual character embeddings are more robust to term variations, infrequent terms, and cross-domain changes. We also quantify the size of context that pre-trained contextual character embeddings take advantage of, and show that such embeddings capture features like part-of-speech and capitalization."
S19-1031,Incivility Detection in Online Comments,2019,0,1,6,1,22246,farig sadeque,Proceedings of the Eighth Joint Conference on Lexical and Computational Semantics (*{SEM} 2019),0,"Incivility in public discourse has been a major concern in recent times as it can affect the quality and tenacity of the discourse negatively. In this paper, we present neural models that can learn to detect name-calling and vulgarity from a newspaper comment section. We show that in contrast to prior work on detecting toxic language, fine-grained incivilities like namecalling cannot be accurately detected by simple models like logistic regression. We apply the models trained on the newspaper comments data to detect uncivil comments in a Russian troll dataset, and find that despite the change of domain, the model makes accurate predictions."
N19-4008,"Eidos, {INDRA}, {\\&} Delphi: From Free Text to Executable Causal Models",2019,0,0,14,0.681818,473,rebecca sharp,Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics (Demonstrations),0,"Building causal models of complicated phenomena such as food insecurity is currently a slow and labor-intensive manual process. In this paper, we introduce an approach that builds executable probabilistic models from raw, free text. The proposed approach is implemented through three systems: Eidos, INDRA, and Delphi. Eidos is an open-domain machine reading system designed to extract causal relations from natural language. It is rule-based, allowing for rapid domain transfer, customizability, and interpretability. INDRA aggregates multiple sources of causal information and performs assembly to create a coherent knowledge base and assess its reliability. This assembled knowledge serves as the starting point for modeling. Delphi is a modeling framework that assembles quantified causal fragments and their contexts into executable probabilistic models that respect the semantics of the original text, and can be used to support decision making."
N19-1274,Alignment over Heterogeneous Embeddings for Question Answering,2019,0,5,2,1,4336,vikas yadav,"Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",0,"We propose a simple, fast, and mostly-unsupervised approach for non-factoid question answering (QA) called Alignment over Heterogeneous Embeddings (AHE). AHE simply aligns each word in the question and candidate answer with the most similar word in the retrieved supporting paragraph, and weighs each alignment score with the inverse document frequency of the corresponding question/answer term. AHE{'}s similarity function operates over embeddings that model the underlying text at different levels of abstraction: character (FLAIR), word (BERT and GloVe), and sentence (InferSent), where the latter is the only supervised component in the proposed approach. Despite its simplicity and lack of supervision, AHE obtains a new state-of-the-art performance on the {``}Easy{''} partition of the AI2 Reasoning Challenge (ARC) dataset (64.6{\%} accuracy), top-two performance on the {``}Challenge{''} partition of ARC (34.1{\%}), and top-three performance on the WikiQA dataset (74.08{\%} MRR), outperforming many other complex, supervised approaches. Our error analysis indicates that alignments over character, word, and sentence embeddings capture substantially different semantic information. We exploit this with a simple meta-classifier that learns how much to trust the predictions over each representation, which further improves the performance of unsupervised AHE."
D19-1260,Quick and (not so) Dirty: Unsupervised Selection of Justification Sentences for Multi-hop Question Answering,2019,60,3,2,1,4336,vikas yadav,Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),0,"We propose an unsupervised strategy for the selection of justification sentences for multi-hop question answering (QA) that (a) maximizes the relevance of the selected sentences, (b) minimizes the overlap between the selected facts, and (c) maximizes the coverage of both question and answer. This unsupervised sentence selection can be coupled with any supervised QA model. We show that the sentences selected by our method improve the performance of a state-of-the-art supervised QA model on two multi-hop QA datasets: AI2{'}s Reasoning Challenge (ARC) and Multi-Sentence Reading Comprehension (MultiRC). We obtain new state-of-the-art performance on both datasets among systems that do not use external resources for training the QA system: 56.82{\%} F1 on ARC (41.24{\%} on Challenge and 64.49{\%} on Easy) and 26.1{\%} EM0 on MultiRC. Our justification sentences have higher quality than the justifications selected by a strong information retrieval baseline, e.g., by 5.4{\%} F1 in MultiRC. We also show that our unsupervised selection of justification sentences is more stable across domains than a state-of-the-art supervised sentence selection method."
W18-5619,Self-training improves Recurrent Neural Networks performance for Temporal Relation Extraction,2018,0,2,5,1,12173,chen lin,Proceedings of the Ninth International Workshop on Health Text Mining and Information Analysis,0,"Neural network models are oftentimes restricted by limited labeled instances and resort to advanced architectures and features for cutting edge performance. We propose to build a recurrent neural network with multiple semantically heterogeneous embeddings within a self-training framework. Our framework makes use of labeled, unlabeled, and social media data, operates on basic features, and is scalable and generalizable. With this method, we establish the state-of-the-art result for both in- and cross-domain for a clinical temporal relation extraction task."
S18-2021,Deep Affix Features Improve Neural Named Entity Recognizers,2018,0,5,3,1,4336,vikas yadav,Proceedings of the Seventh Joint Conference on Lexical and Computational Semantics,0,"We propose a practical model for named entity recognition (NER) that combines word and character-level information with a specific learned representation of the prefixes and suffixes of the word. We apply this approach to multilingual and multi-domain NER and show that it achieves state of the art results on the CoNLL 2002 Spanish and Dutch and CoNLL 2003 German NER datasets, consistently achieving 1.5-2.3 percent over the state of the art without relying on any dictionary features. Additionally, we show improvement on SemEval 2013 task 9.1 DrugNER, achieving state of the art results on the MedLine dataset and the second best results overall (-1.3{\%} from state of the art). We also establish a new benchmark on the I2B2 2010 Clinical NER dataset with 84.70 F-score."
S18-1011,{S}em{E}val 2018 Task 6: Parsing Time Normalizations,2018,0,1,4,0.582021,1744,egoitz laparra,Proceedings of The 12th International Workshop on Semantic Evaluation,0,"This paper presents the outcomes of the Parsing Time Normalization shared task held within SemEval-2018. The aim of the task is to parse time expressions into the compositional semantic graphs of the Semantically Compositional Annotation of Time Expressions (SCATE) schema, which allows the representation of a wider variety of time expressions than previous approaches. Two tracks were included, one to evaluate the parsing of individual components of the produced graphs, in a classic information extraction way, and another one to evaluate the quality of the time intervals resulting from the interpretation of those graphs. Though 40 participants registered for the task, only one team submitted output, achieving 0.55 F1 in Track 1 (parsing) and 0.70 F1 in Track 2 (intervals)."
Q18-1025,From Characters to Time Intervals: New Paradigms for Evaluation and Neural Parsing of Time Normalizations,2018,4,4,3,0.582021,1744,egoitz laparra,Transactions of the Association for Computational Linguistics,0,"This paper presents the first model for time normalization trained on the SCATE corpus. In the SCATE schema, time expressions are annotated as a semantic composition of time entities. This novel schema favors machine learning approaches, as it can be viewed as a semantic parsing task. In this work, we propose a character level multi-output neural network that outperforms previous state-of-the-art built on the TimeML schema. To compare predictions of systems that follow both SCATE and TimeML, we present a new scoring metric for time intervals. We also apply this new metric to carry out a comparative analysis of the annotations of both schemes in the same corpus."
C18-1182,A Survey on Recent Advances in Named Entity Recognition from Deep Learning models,2018,0,59,2,1,4336,vikas yadav,Proceedings of the 27th International Conference on Computational Linguistics,0,"Named Entity Recognition (NER) is a key component in NLP systems for question answering, information retrieval, relation extraction, etc. NER systems have been studied and developed widely for decades, but accurate systems using deep neural networks (NN) have only been introduced in the last few years. We present a comprehensive survey of deep neural network architectures for NER, and contrast them with previous approaches to NER based on feature engineering and other supervised or semi-supervised learning algorithms. Our results highlight the improvements achieved by neural networks, and show how incorporating some of the lessons learned from past work on feature-based NER systems can yield further improvements."
W17-2320,Unsupervised Domain Adaptation for Clinical Negation Detection,2017,13,3,2,1,1747,timothy miller,{B}io{NLP} 2017,0,"Detecting negated concepts in clinical texts is an important part of NLP information extraction systems. However, generalizability of negation systems is lacking, as cross-domain experiments suffer dramatic performance losses. We examine the performance of multiple unsupervised domain adaptation algorithms on clinical negation detection, finding only modest gains that fall well short of in-domain performance."
W17-2341,Representations of Time Expressions for Temporal Relation Extraction with Convolutional Neural Networks,2017,16,10,4,1,12173,chen lin,{B}io{NLP} 2017,0,"Token sequences are often used as the input for Convolutional Neural Networks (CNNs) in natural language processing. However, they might not be an ideal representation for time expressions, which are long, highly varied, and semantically complex. We describe a method for representing time expressions with single pseudo-tokens for CNNs. With this method, we establish a new state-of-the-art result for a clinical temporal relation extraction task."
S17-2093,{S}em{E}val-2017 Task 12: Clinical {T}emp{E}val,2017,6,23,1,1,224,steven bethard,Proceedings of the 11th International Workshop on Semantic Evaluation ({S}em{E}val-2017),0,"Clinical TempEval 2017 aimed to answer the question: how well do systems trained on annotated timelines for one medical condition (colon cancer) perform in predicting timelines on another medical condition (brain cancer)? Nine sub-tasks were included, covering problems in time expression identification, event expression identification and temporal relation identification. Participant systems were evaluated on clinical and pathology notes from Mayo Clinic cancer patients, annotated with an extension of TimeML for the clinical domain. 11 teams participated in the tasks, with the best systems achieving F1 scores above 0.55 for time expressions, above 0.70 for event expressions, and above 0.40 for temporal relations. Most tasks observed about a 20 point drop over Clinical TempEval 2016, where systems were trained and evaluated on the same domain (colon cancer)."
I17-1010,Improving Implicit Semantic Role Labeling by Predicting Semantic Frame Arguments,2017,0,1,2,1,12371,quynh do,Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers),0,"Implicit semantic role labeling (iSRL) is the task of predicting the semantic roles of a predicate that do not appear as explicit arguments, but rather regard common sense knowledge or are mentioned earlier in the discourse. We introduce an approach to iSRL based on a predictive recurrent neural semantic frame model (PRNSFM) that uses a large unannotated corpus to learn the probability of a sequence of semantic arguments given a predicate. We leverage the sequence probabilities predicted by the PRNSFM to estimate selectional preferences for predicates and their arguments. On the NomBank iSRL test set, our approach improves state-of-the-art performance on implicit semantic role labeling with less reliance than prior work on manually constructed language resources."
E17-2118,Neural Temporal Relation Extraction,2017,17,25,4,0.424813,12174,dmitriy dligach,"Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 2, Short Papers",0,"We experiment with neural architectures for temporal relation extraction and establish a new state-of-the-art for several scenarios. We find that neural models with only tokens as input outperform state-of-the-art hand-engineered feature-based models, that convolutional neural networks outperform LSTM models, and that encoding relation arguments with XML tags outperforms a traditional position-based encoding."
W16-6203,Why Do They Leave: Modeling Participation in Online Depression Forums,2016,14,7,6,1,22246,farig sadeque,Proceedings of The Fourth International Workshop on Natural Language Processing for Social Media,0,None
W16-6105,Analysis of Anxious Word Usage on Online Health Forums,2016,16,2,4,0,33400,nicolas reyvillamizar,Proceedings of the Seventh International Workshop on Health Text Mining and Information Analysis,0,None
W16-6009,Visualizing the Content of a Children{'}s Story in a Virtual World: Lessons Learned,2016,0,0,2,1,12371,quynh do,Proceedings of the Workshop on Uphill Battles in Language Processing: Scaling Early Achievements to Robust Methods,0,None
W16-2914,Improving Temporal Relation Extraction with Training Instance Augmentation,2016,18,8,4,1,12173,chen lin,Proceedings of the 15th Workshop on Biomedical Natural Language Processing,0,"Temporal relation extraction is important for understanding the ordering of events in narrative text. We describe a method for increasing the number of high-quality training instances available to a temporal relation extraction task, with an adaptation to different annotation styles in the clinical domain by taking advantage of the Unified Medical Language System (UMLS). This method notably improves clinical temporal relation extraction, works beyond featurizing or duplicating the same information, can generalize between-argument signals in a more effective and robust fashion. We also report a new state-of-the-art result, which is a two point improvement over the best Clinical TempEval 2016 system."
W16-0322,Semi-supervised {CLP}sych 2016 Shared Task System Submission,2016,2,1,5,0,33400,nicolas reyvillamizar,Proceedings of the Third Workshop on Computational Linguistics and Clinical Psychology,0,None
S16-1099,{DLS}@{CU} at {S}em{E}val-2016 Task 1: Supervised Models of Sentence Similarity,2016,15,0,2,1,19708,md sultan,Proceedings of the 10th International Workshop on Semantic Evaluation ({S}em{E}val-2016),0,None
S16-1165,{S}em{E}val-2016 Task 12: Clinical {T}emp{E}val,2016,15,60,1,1,224,steven bethard,Proceedings of the 10th International Workshop on Semantic Evaluation ({S}em{E}val-2016),0,"Clinical TempEval 2016 evaluated temporal information extraction systems on the clinical domain. Nine sub-tasks were included, covering problems in time expression identification, event expression identification and temporal relation identification. Participant systems were trained and evaluated on a corpus of clinical and pathology notes from the Mayo Clinic, annotated with an extension of TimeML for the clinical domain. 14 teams submitted a total of 40 system runs, with the best systems achieving near-human performance on identifying events and times. On identifying temporal relations, there was a gap between the best systems and human performance, but the gap was less than half the gap of Clinical TempEval 2015."
P16-1210,Domain Adaptation for Authorship Attribution: Improved Structural Correspondence Learning,2016,12,5,4,1,34541,upendra sapkota,Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,None
L16-1541,Age and Gender Prediction on Health Forum Data,2016,6,3,5,0,11296,prasha shrestha,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"Health support forums have become a rich source of data that can be used to improve health care outcomes. A user profile, including information such as age and gender, can support targeted analysis of forum data. But users might not always disclose their age and gender. It is desirable then to be able to automatically extract this information from users{'} content. However, to the best of our knowledge there is no such resource for author profiling of health forum data. Here we present a large corpus, with close to 85,000 users, for profiling and also outline our approach and benchmark results to automatically detect a user{'}s age and gender from their forum posts. We use a mix of features from a user{'}s text as well as forum specific features to obtain accuracy well above the baseline, thus showing that both our dataset and our method are useful and valid."
L16-1599,A Semantically Compositional Annotation Scheme for Time Normalization,2016,9,2,1,1,224,steven bethard,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"We present a new annotation scheme for normalizing time expressions, such as {``}three days ago{''}, to computer-readable forms, such as 2016-03-07. The annotation scheme addresses several weaknesses of the existing TimeML standard, allowing the representation of time expressions that align to more than one calendar unit (e.g., {``}the past three summers{''}), that are defined relative to events (e.g., {``}three weeks postoperative{''}), and that are unions or intersections of smaller time expressions (e.g., {``}Tuesdays and Thursdays{''}). It achieves this by modeling time expression interpretation as the semantic composition of temporal operators like UNION, NEXT, and AFTER. We have applied the annotation scheme to 34 documents so far, producing 1104 annotations, and achieving inter-annotator agreement of 0.821."
C16-1121,Facing the most difficult case of Semantic Role Labeling: A collaboration of word embeddings and co-training,2016,15,0,2,1,12371,quynh do,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers",0,"We present a successful collaboration of word embeddings and co-training to tackle in the most difficult test case of semantic role labeling: predicting out-of-domain and unseen semantic frames. Despite the fact that co-training is a successful traditional semi-supervised method, its application in SRL is very limited especially when a huge amount of labeled data is available. In this work, co-training is used together with word embeddings to improve the performance of a system trained on a large training dataset. We also introduce a semantic role labeling system with a simple learning architecture and effective inference that is easily adaptable to semi-supervised settings with new training data and/or new features. On the out-of-domain testing set of the standard benchmark CoNLL 2009 data our simple approach achieves high performance and improves state-of-the-art results."
W15-3809,Extracting Time Expressions from Clinical Text,2015,18,6,2,1,1747,timothy miller,Proceedings of {B}io{NLP} 15,0,"Temporal information extraction is important to understanding text in clinical documents. Temporal expression extraction provides explicit grounding of events in a narrative. In this work we provide a direct comparison of various ways of extracting temporal expressions, using similar features as much as possible to explore the advantages of the methods themselves. We evaluate these systems on both the THYME (Temporal History of Your Medical Events) and i2b2 Challenge corpora. Our main findings are that simple sequence taggers outperform conditional random fields on the new data, and higher-level syntactic features do not seem to improve performance."
W15-2602,Predicting Continued Participation in Online Health Forums,2015,14,4,5,1,22246,farig sadeque,Proceedings of the Sixth International Workshop on Health Text Mining and Information Analysis,0,"Online health forums provide advice and emotional solace to their users from a social network of people who have faced similar conditions. Continued participation of users is thus critical to their success. In this paper, we develop machine learning models for predicting whether or not a user will continue to participate in an online health forum. The prediction models are trained and tested over a large dataset collected from the support group based social networking site dailystrength.org. We find that our models can predict continued participation with over 83% accuracy after as little as 1 month observing the userxe2x80x99s activities, and that performance increases rapidly up to 1 year of observation. We also show that features such as the time since a userxe2x80x99s last activity are consistently predictive regardless of the length of the observation period, while other features, such as the number of times a user replies to others, decrease in predictiveness as the observation period grows."
W15-1608,Developing Language-tagged Corpora for Code-switching Tweets,2015,20,9,3,0.952381,15228,suraj maharjan,Proceedings of The 9th Linguistic Annotation Workshop,0,"Code-switching, where a speaker switches between languages mid-utterance, is frequently used by multilingual populations worldwide. Despite its prevalence, limited effort has been devoted to develop computational approaches or even basic linguistic resources to support research into the processing of such mixedlanguage data. We present a user-centric approach to collecting code-switched utterances from social media posts, and develop language universal guidelines for the annotation of codeswitched data. We also present results for several baseline language identification models on our corpora and demonstrate that language identification in code-switched text is a difficult task that calls for deeper investigation."
S15-2027,{DLS}@{CU}: Sentence Similarity from Word Alignment and Semantic Vector Composition,2015,19,62,2,1,19708,md sultan,Proceedings of the 9th International Workshop on Semantic Evaluation ({S}em{E}val 2015),0,"We describe a set of top-performing systems at the SemEval 2015 English Semantic Textual Similarity (STS) task. Given two English sentences, each system outputs the degree of their semantic similarity. Our unsupervised system, which is based on word alignments across the two input sentences, ranked 5th among 73 submitted system runs with a mean correlation of 79.19% with human annotations. We also submitted two runs of a supervised system which uses word alignments and similarities between compositional sentence vectors as its features. Our best supervised run ranked 1st with a mean correlation of 80.15%."
S15-2072,{CUAB}: Supervised Learning of Disorders and their Attributes using Relations,2015,7,1,3,0,24476,james gung,Proceedings of the 9th International Workshop on Semantic Evaluation ({S}em{E}val 2015),0,"We implemented an end-to-end system for disorder identification and slot filling. For identifying spans for both disorders and their attributes, we used a linear chain conditional random field (CRF) approach coupled with cTAKES for pre-processing. For combining disjoint disorder spans, finding relations between attributes and disorders, and attribute normalization, we used l2-regularized l2-loss linear support vector machine (SVM) classification. Disorder CUIs were identified using a back-off approach to YTEX lookup (CUAB1) or NLM UTS API (CUAB2) if the target text was not found in the training data. Our best system utilized UMLS semantic type features for disorder/attribute span identification and the NLM UTS API for normalization. It was ranked 12th in Task 1 (disorder identification) and 6th in Task 2b (disorder identification and slot filling) with a weighted F Measure of 0.711."
S15-2136,{S}em{E}val-2015 Task 6: Clinical {T}emp{E}val,2015,7,73,1,1,224,steven bethard,Proceedings of the 9th International Workshop on Semantic Evaluation ({S}em{E}val 2015),0,"Clinical TempEval 2015 brought the temporal information extraction tasks of past TempEval campaigns to the clinical domain. Nine sub-tasks were included, covering problems in time expression identification, event expression identification and temporal relation identification. Participant systems were trained and evaluated on a corpus of clinical notes and pathology reports from the Mayo Clinic, annotated with an extension of TimeML for the clinical domain. Three teams submitted a total of 13 system runs, with the best systems achieving near-human performance on identifying events and times, but with a large performance gap still remaining for temporal relations."
N15-1010,Not All Character N-grams Are Created Equal: A Study in Authorship Attribution,2015,20,59,2,1,34541,upendra sapkota,Proceedings of the 2015 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"Character n-grams have been identified as the most successful feature in both singledomain and cross-domain Authorship Attribution (AA), but the reasons for their discriminative value were not fully understood. We identify subgroups of charactern-grams that correspond to linguistic aspects commonly claimed to be covered by these features: morphosyntax, thematic content and style. We evaluate the predictiveness of each of these groups in two AA settings: a single domain setting and a cross-domain setting where multiple topics are present. We demonstrate that characterngrams that capture information about affixes and punctuation account for almost all of the power of character n-grams as features. Our study contributes new insights into the use of n-grams for future AA work and other classification tasks."
D15-1111,Feature-Rich Two-Stage Logistic Regression for Monolingual Alignment,2015,36,0,2,1,19708,md sultan,Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,0,"Monolingual alignment is the task of pairing semantically similar units from two pieces of text. We report a top-performing supervised aligner that operates on short text snippets. We employ a large feature set to (1) encode similarities among semantic units (words and named entities) in context, and (2) address cooperation and competition for alignment among units in the same snippet. These features are deployed in a two-stage logistic regression framework for alignment. On two benchmark data sets, our aligner achieves F1 scores of 92.1% and 88.5%, with statistically significant error reductions of 4.8% and 7.3% over the previous best aligner. It produces top results in extrinsic evaluation as well."
D15-1271,Adapting Coreference Resolution for Narrative Processing,2015,18,0,2,1,12371,quynh do,Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,0,"Domain adaptation is a challenge for supervised NLP systems because of expensive and time-consuming manual annotated resources. We present a novel method to adapt a supervised coreference resolution system trained on newswire to short narrative stories without retraining the system. The idea is to perform inference via an Integer Linear Programming (ILP) formulation with the features of narratives adopted as soft constraints. When testing on the UMIREC 1 and N2 2 corpora with the-stateof-the-art Berkeley coreference resolution system trained on OntoNotes 3 , our inference substantially outperforms the original inference on the CoNLL 2011 metric."
W14-3907,Overview for the First Shared Task on Language Identification in Code-Switched Data,2014,16,46,4,0.418203,136,thamar solorio,Proceedings of the First Workshop on Computational Approaches to Code Switching,0,"We present an overview of the first shared task on language identification on codeswitched data. The shared task included code-switched data from four language pairs: Modern Standard ArabicDialectal Arabic (MSA-DA), MandarinEnglish (MAN-EN), Nepali-English (NEPEN), and Spanish-English (SPA-EN). A total of seven teams participated in the task and submitted 42 system runs. The evaluation showed that language identification at the token level is more difficult when the languages present are closely related, as in the case of MSA-DA, where the prediction performance was the lowest among all language pairs. In contrast, the language pairs with the higest F-measure where SPA-EN and NEP-EN. The task made evident that language identification in code-switched data is still far from solved and warrants further research."
S14-2039,{DLS}@{CU}: Sentence Similarity from Word Alignment,2014,15,45,2,1,19708,md sultan,Proceedings of the 8th International Workshop on Semantic Evaluation ({S}em{E}val 2014),0,"We present an algorithm for computing the semantic similarity between two sentences. It adopts the hypothesis that semantic similarity is a monotonically increasing function of the degree to which (1) the two sentences contain similar semantic units, and (2) such units occur in similar semantic contexts. With a simplistic operationalization of the notion of semantic units with individual words, we experimentally show that this hypothesis can lead to state-of-the-art results for sentencelevel semantic similarity. At the SemEval 2014 STS task (task 10), our system demonstrated the best performance (measured by correlation with human annotations) among 38 system runs."
Q14-1012,Temporal Annotation in the Clinical Domain,2014,32,79,2,0,39080,william iv,Transactions of the Association for Computational Linguistics,0,"This article discusses the requirements of a formal specification for the annotation of temporal information in clinical narratives. We discuss the implementation and extension of ISO-TimeML for annotating a corpus of clinical notes, known as the THYME corpus. To reflect the information task and the heavily inference-based reasoning demands in the domain, a new annotation guideline has been developed, {``}the THYME Guidelines to ISO-TimeML (THYME-TimeML){''}. To clarify what relations merit annotation, we distinguish between linguistically-derived and inferentially-derived temporal orderings in the text. We also apply a top performing TempEval 2013 system against this new resource to measure the difficulty of adapting systems to the clinical domain. The corpus is available to the community and has been proposed for use in a SemEval 2015 task."
Q14-1018,Back to Basics for Monolingual Alignment: Exploiting Word Similarity and Contextual Evidence,2014,25,68,2,1,19708,md sultan,Transactions of the Association for Computational Linguistics,0,"We present a simple, easy-to-replicate monolingual aligner that demonstrates state-of-the-art performance while relying on almost no supervision and a very small number of external resources. Based on the hypothesis that words with similar meanings represent potential pairs for alignment if located in similar contexts, we propose a system that operates by finding such pairs. In two intrinsic evaluations on alignment test data, our system achieves F1 scores of 88{--}92{\%}, demonstrating 1{--}3{\%} absolute improvement over the previous best system. Moreover, in two extrinsic evaluations our aligner outperforms existing aligners, and even a naive application of the aligner approaches state-of-the-art performance in each extrinsic task."
Q14-1022,Dense Event Ordering with a Multi-Pass Architecture,2014,24,58,4,0.231496,980,nathanael chambers,Transactions of the Association for Computational Linguistics,0,"The past 10 years of event ordering research has focused on learning partial orderings over document events and time expressions. The most popular corpus, the TimeBank, contains a small subset of the possible ordering graph. Many evaluations follow suit by only testing certain pairs of events (e.g., only main verbs of neighboring sentences). This has led most research to focus on specific learners for partial labelings. This paper attempts to nudge the discussion from identifying some relations to all relations. We present new experiments on strongly connected event graphs that contain â¼10 times more relations per document than the TimeBank. We also describe a shift away from the single learner to a sieve-based architecture that naturally blends multiple learners into a precision-ranked cascade of sieves. Each sieve adds labels to the event graph one at a time, and earlier sieves inform later ones through transitive closure. This paper thus describes innovations in both approach and task. We experiment on the densest event graphs to date and show a 14{\%} gain over state-of-the-art."
P14-5010,The {S}tanford {C}ore{NLP} Natural Language Processing Toolkit,2014,14,2880,5,0,1411,christopher manning,Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations,0,"We describe the design and use of the Stanford CoreNLP toolkit, an extensible pipeline that provides core natural language analysis. This toolkit is quite widely used, both in the research NLP community and also among commercial and government users of open source NLP technology. We suggest that this follows from a simple, approachable design, straightforward interfaces, the inclusion of robust and good quality analysis components, and not requiring use of a large amount of associated baggage."
P14-2014,Descending-Path Convolution Kernel for Syntactic Structures,2014,15,3,4,1,12173,chen lin,Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"Convolution tree kernels are an efficient and effective method for comparing syntactic structures in NLP methods. However, current kernel methods such as subset tree kernel and partial tree kernel understate the similarity of very similar tree structures. Although soft-matching approaches can improve the similarity scores, they are corpusdependent and match relaxations may be task-specific. We propose an alternative approach called descending path kernel which gives intuitive similarity scores on comparable structures. This method is evaluated on two temporal relation extraction tasks and demonstrates its advantage over rich syntactic representations."
P14-2082,An Annotation Framework for Dense Event Ordering,2014,10,29,4,0,34445,taylor cassidy,Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"Todayxe2x80x99s event ordering research is heavily dependent on annotated corpora. Current corpora influence shared evaluations and drive algorithm development. Partly due to this dependence, most research focuses on partial orderings of a documentxe2x80x99s events. For instance, the TempEval competitions and the TimeBank only annotate small portions of the event graph, focusing on the most salient events or on specific types of event pairs (e.g., only events in the same sentence). Deeper temporal reasoners struggle with this sparsity because the entire temporal picture is not represented. This paper proposes a new annotation process with a mechanism to force annotators to label connected graphs. It generates 10 times more relations per document than the TimeBank, and our TimeBank-Dense corpus is larger than all current corpora. We hope this process and its dense corpus encourages research on new global models with deeper reasoning."
bethard-etal-2014-cleartk,{C}lear{TK} 2.0: Design Patterns for Machine Learning in {UIMA},2014,6,15,1,1,224,steven bethard,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"ClearTK adds machine learning functionality to the UIMA framework, providing wrappers to popular machine learning libraries, a rich feature extraction library that works across different classifiers, and utilities for applying and evaluating machine learning models. Since its inception in 2008, ClearTK has evolved in response to feedback from developers and the community. This evolution has followed a number of important design principles including: conceptually simple annotator interfaces, readable pipeline descriptions, minimal collection readers, type system agnostic code, modules organized for ease of import, and assisting user comprehension of the complex UIMA framework."
C14-1116,Cross-Topic Authorship Attribution: Will Out-Of-Topic Data Help?,2014,25,12,4,1,34541,upendra sapkota,"Proceedings of {COLING} 2014, the 25th International Conference on Computational Linguistics: Technical Papers",0,"Most previous research on authorship attribution (AA) assumes that the training and test data are drawn from same distribution. But in real scenarios, this assumption is too strong. The goal of this study is to improve the prediction results in cross-topic AA (CTAA), where the training data comes from one topic but the test data comes from another. Our proposed idea is to build a predictive model for one topic using documents from all other available topics. In addition to improving the performance of CTAA, we also make a thorough analysis of the sensitivity to changes in topic of four most commonly used feature types in AA. We empirically illustrate that our proposed framework is significantly better than the one trained on a single out-of-domain topic and is as effective, in some cases, as same-topic setting."
W13-1903,Discovering Temporal Narrative Containers in Clinical Text,2013,11,13,2,1,1747,timothy miller,Proceedings of the 2013 Workshop on Biomedical Natural Language Processing,0,None
S13-2002,{C}lear{TK}-{T}ime{ML}: A minimalist approach to {T}emp{E}val 2013,2013,8,70,1,1,224,steven bethard,"Second Joint Conference on Lexical and Computational Semantics (*{SEM}), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation ({S}em{E}val 2013)",0,"The ClearTK-TimeML submission to TempEval 2013 competed in all English tasks: identifying events, identifying times, and identifying temporal relations. The system is a pipeline of machine-learning models, each with a small set of features from a simple morpho-syntactic annotation pipeline, and where temporal relations are only predicted for a small set of syntactic constructions and relation types. ClearTKTimeML ranked 1 st for temporal relation F1, time extent strict F1 and event tense accuracy."
S13-2044,{S}em{E}val-2013 Task 3: Spatial Role Labeling,2013,15,17,4,1,41194,oleksandr kolomiyets,"Second Joint Conference on Lexical and Computational Semantics (*{SEM}), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation ({S}em{E}val 2013)",0,"Many NLP applications require information about locations of objects referenced in text, or relations between them in space. For example, the phrase a book on the desk contains information about the location of the object book, as trajector, with respect to another object desk, as landmark. Spatial Role Labeling (SpRL) is an evaluation task in the information extraction domain which sets a goal to automatically process text and identify objects of spatial scenes and relations between them. This paper describes the task in Semantic Evaluations 2013, annotation schema, corpora, participants, methods and results obtained by the participants."
S13-2101,{CU} : Computational Assessment of Short Free Text Answers - A Tool for Evaluating Students{'} Understanding,2013,8,6,2,0,41252,ifeyinwa okoye,"Second Joint Conference on Lexical and Computational Semantics (*{SEM}), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation ({S}em{E}val 2013)",0,"Assessing student understanding by evaluating their free text answers to posed questions is a very important task. However, manually, it is time-consuming and computationally, it is difficult. This paper details our shallow NLP approach to computationally assessing student free text answers when a reference answer is provided. For four out of the five test sets, our system achieved an overall accuracy above the median and mean."
S13-1025,{DLS}@{CU}-{CORE}: A Simple Machine Learning Model of Semantic Textual Similarity,2013,8,0,2,1,19708,md sultan,"Second Joint Conference on Lexical and Computational Semantics (*{SEM}), Volume 1: Proceedings of the Main Conference and the Shared Task: Semantic Textual Similarity",0,"We present a system submitted in the Semantic Textual Similarity (STS) task at the Second Joint Conference on Lexical and Computational Semantics (*SEM 2013). Given two short text fragments, the goal of the system is to determine their semantic similarity. Our system makes use of three different measures of text similarity: word n-gram overlap, character n-gram overlap and semantic overlap. Using these measures as features, it trains a support vector regression model on SemEval STS 2012 data. This model is then applied on the STS 2013 data to compute textual similarities. Two different selections of training data result in very different performance levels: while a correlation of 0.4135 with gold standards was observed in the official evaluation (ranked 63 among all systems) for one selection, the other resulted in a correlation of 0.5352 (that would rank 21)."
D13-1078,A Synchronous Context Free Grammar for Time Normalization,2013,7,17,1,1,224,steven bethard,Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,0,"We present an approach to time normalization (e.g. the day before yesterday)2013-04-12) based on a synchronous context free grammar. Synchronous rules map the source language to formally defined operators for manipulating times (FINDENCLOSED, STARTATENDOF, etc.). Time expressions are then parsed using an extended CYK algorithm, and converted to a normalized form by applying the operators recursively. For evaluation, a small set of synchronous rules for English time expressions were developed. Our model outperforms HeidelTime, the best time normalization system in TempEval 2013, on four different time normalization corpora."
W12-2002,Identifying science concepts and student misconceptions in an interactive essay writing tutor,2012,12,7,1,1,224,steven bethard,Proceedings of the Seventh Workshop on Building Educational Applications Using {NLP},0,"We present initial steps towards an interactive essay writing tutor that improves science knowledge by analyzing student essays for misconceptions and recommending science webpages that help correct those misconceptions. We describe the five components in this system: identifying core science concepts, determining appropriate pedagogical sequences for the science concepts, identifying student misconceptions in essays, aligning student misconceptions to science concepts, and recommending webpages to address misconceptions. We provide initial models and evaluations of the models for each component."
S12-1048,{S}em{E}val-2012 Task 3: Spatial Role Labeling,2012,20,36,2,1,1061,parisa kordjamshidi,"*{SEM} 2012: The First Joint Conference on Lexical and Computational Semantics {--} Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation ({S}em{E}val 2012)",0,"This SemEval2012 shared task is based on a recently introduced spatial annotation scheme called Spatial Role Labeling. The Spatial Role Labeling task concerns the extraction of main components of the spatial semantics from natural language: trajectors, landmarks and spatial indicators. In addition to these major components, the links between them and the general-type of spatial relationships including region, direction and distance are targeted. The annotated dataset contains about 1213 sentences which describe 612 images of the CLEF IAPR TC-12 Image Benchmark. We have one participant system with two runs. The participant's runs are compared to the system in (Kordjamshidi et al., 2011c) which is provided by task organizers."
P12-1010,Extracting Narrative Timelines as Temporal Dependency Structures,2012,33,26,2,1,41194,oleksandr kolomiyets,Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"We propose a new approach to characterizing the timeline of a text: temporal dependency structures, where all the events of a narrative are linked via partial ordering relations like BEFORE, AFTER, OVERLAP and IDENTITY. We annotate a corpus of children's stories with temporal dependency trees, achieving agreement (Krippendorff's Alpha) of 0.856 on the event words, 0.822 on the links between events, and of 0.700 on the ordering relation labels. We compare two parsing models for temporal dependency structures, and show that a deterministic non-projective dependency parser outperforms a graph-based maximum spanning tree parser, achieving labeled attachment accuracy of 0.647 and labeled tree edit distance of 0.596. Our analysis of the dependency parser errors gives some insights into future research directions."
bethard-etal-2012-annotating,Annotating Story Timelines as Temporal Dependency Structures,2012,12,12,1,1,224,steven bethard,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"We present an approach to annotating timelines in stories where events are linked together by temporal relations into a temporal dependency tree. This approach avoids the disconnected timeline problems of prior work, and results in timelines that are more suitable for temporal reasoning. We show that annotating timelines as temporal dependency trees is possible with high levels of inter-annotator agreement - Krippendorff's Alpha of 0.822 on selecting event pairs, and of 0.700 on selecting temporal relation labels - even with the moderately sized relation set of BEFORE, AFTER, INCLUDES, IS-INCLUDED, IDENTITY and OVERLAP. We also compare several annotation schemes for identifying story events, and show that higher inter-annotator agreement can be reached by focusing on only the events that are essential to forming the timeline, skipping words in negated contexts, modal contexts and quoted speech."
E12-1034,Skip N-grams and Ranking Functions for Predicting Script Events,2012,12,34,2,0,43590,bram jans,Proceedings of the 13th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"In this paper, we extend current state-of-the-art research on unsupervised acquisition of scripts, that is, stereotypical and frequently observed sequences of events. We design, evaluate and compare different methods for constructing models for script event prediction: given a partial chain of events in a script, predict other events that are likely to belong to the script. Our work aims to answer key questions about how best to (1) identify representative event chains from a source text, (2) gather statistics from the event chains, and (3) choose ranking functions for predicting new script events. We make several contributions, introducing skip-grams for collecting event statistics, designing improved methods for ranking event predictions, defining a more reliable evaluation metric for measuring predictiveness, and providing a systematic analysis of the various event prediction models."
W11-0116,Using Query Patterns to Learn the Duration of Events,2011,18,23,5,0,14606,andrey gusev,Proceedings of the Ninth International Conference on Computational Semantics ({IWCS} 2011),0,"We present the first approach to learning the durations of events without annotated training data, employing web query patterns to infer duration distributions. For example, we learn that war lasts years or decades, while look lasts seconds or minutes. Learning aspectual information is an important goal for computational semantics and duration information may help enable rich document understanding. We first describe and improve a supervised baseline that relies on event duration annotations. We then show how web queries for linguistic patterns can help learn the duration of events without labeled data, producing fine-grained duration judgments that surpass the supervised system. We evaluate on the TimeBank duration corpus, and also investigate how an event's participants (arguments) effect its duration using a corpus collected through Amazon's Mechanical Turk. We make available a new database of events and their duration distributions for use in research involving the temporal and aspectual properties of events."
P11-2047,Model-Portability Experiments for Textual Temporal Analysis,2011,15,16,2,1,41194,oleksandr kolomiyets,Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,0,"We explore a semi-supervised approach for improving the portability of time expression recognition to non-newswire domains: we generate additional training examples by substituting temporal expression words with potential synonyms. We explore using synonyms both from WordNet and from the Latent Words Language Model (LWLM), which predicts synonyms in context using an unsupervised approach. We evaluate a state-of-the-art time expression recognition system trained both with and without the additional training examples using data from TempEval 2010, Reuters and Wikipedia. We find that the LWLM provides substantial improvements on the Reuters corpus, and smaller improvements on the Wikipedia corpus. We find that WordNet alone never improves performance, though intersecting the examples from the LWLM and WordNet provides more stable results for Wikipedia."
W10-0719,Crowdsourcing and language studies: the new generation of linguistic data,2010,19,95,2,0,20214,robert munro,Proceedings of the {NAACL} {HLT} 2010 Workshop on Creating Speech and Language Data with {A}mazon{'}s Mechanical Turk,0,"We present a compendium of recent and current projects that utilize crowdsourcing technologies for language studies, finding that the quality is comparable to controlled laboratory experiments, and in some cases superior. While crowdsourcing has primarily been used for annotation in recent language studies, the results here demonstrate that far richer data may be generated in a range of linguistic disciplines from semantics to psycholinguistics. For these, we report a number of successful methods for evaluating data quality in the absence of a 'correct' response for any given data point."
W09-2002,Topic Model Analysis of Metaphor Frequency for Psycholinguistic Stimuli,2009,16,11,1,1,224,steven bethard,Proceedings of the Workshop on Computational Approaches to Linguistic Creativity,0,"Psycholinguistic studies of metaphor processing must control their stimuli not just for word frequency but also for the frequency with which a term is used metaphorically. Thus, we consider the task of metaphor frequency estimation, which predicts how often target words will be used metaphorically. We develop metaphor classifiers which represent metaphorical domains through Latent Dirichlet Allocation, and apply these classifiers to the target words, aggregating their decisions to estimate the metaphorical frequencies. Training on only 400 sentences, our models are able to achieve 61.3% accuracy on metaphor classification and 77.8% accuracy on High vs. Low metaphorical frequency estimation."
W09-1501,Building Test Suites for {UIMA} Components,2009,1,19,2,0,39476,philip ogren,"Proceedings of the Workshop on Software Engineering, Testing, and Quality Assurance for Natural Language Processing ({SETQA}-{NLP} 2009)",0,"We summarize our experiences building a comprehensive suite of tests for a statistical natural language processing toolkit, ClearTK. We describe some of the challenges we encountered, introduce a software project that emerged from these efforts, summarize our resulting test suite, and discuss some of the lessons learned."
P08-2045,Learning Semantic Links from a Corpus of Parallel Temporal and Causal Relations,2008,10,49,1,1,224,steven bethard,"Proceedings of ACL-08: HLT, Short Papers",0,"Finding temporal and causal relations is crucial to understanding the semantic structure of a text. Since existing corpora provide no parallel temporal and causal annotations, we annotated 1000 conjoined event pairs, achieving inter-annotator agreement of 81.2% on temporal relations and 77.8% on causal relations. We trained machine learning models using features derived from WordNet and the Google N-gram corpus, and they outperformed a variety of baselines, achieving an F-measure of 49.0 for temporals and 52.4 for causals. Analysis of these models suggests that additional data will improve performance, and that temporal information is crucial to causal relation identification."
bethard-etal-2008-building,Building a Corpus of Temporal-Causal Structure,2008,18,33,1,1,224,steven bethard,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"While recent corpus annotation efforts cover a wide variety of semantic structures, work on temporal and causal relations is still in its early stages. Annotation efforts have typically considered either temporal relations or causal relations, but not both, and no corpora currently exist that allow the relation between temporals and causals to be examined empirically. We have annotated a corpus of 1000 event pairs for both temporal and causal relations, focusing on a relatively frequent construction in which the events are conjoined by the word ÂandÂ. Temporal relations were annotated using an extension of the BEFORE and AFTER scheme used in the TempEval competition, and causal relations were annotated using a scheme based on connective phrases like Âand as a resultÂ. The annotators achieved 81.2{\%} agreement on temporal relations and 77.8{\%} agreement on causal relations. Analysis of the resulting corpus revealed some interesting findings, for example, that over 30{\%} of CAUSAL relations do not have an underlying BEFORE relation. The corpus was also explored using machine learning methods, and while model performance exceeded all baselines, the results suggested that simple grammatical cues may be insufficient for identifying the more difficult temporal and causal relations."
S07-1025,{CU}-{TMP}: Temporal Relation Classification Using Syntactic and Semantic Features,2007,5,60,1,1,224,steven bethard,Proceedings of the Fourth International Workshop on Semantic Evaluations ({S}em{E}val-2007),0,"We approached the temporal relation identification tasks of TempEval 2007 as pair-wise classification tasks. We introduced a variety of syntactically and semantically motivated features, including temporal-logic-based features derived from running our Task B system on the Task A and C data. We trained support vector machine models and achieved the second highest accuracies on the tasks: 61% on Task A, 75% on Task B and 54% on Task C."
W06-1618,Identification of Event Mentions and their Semantic Class,2006,11,55,1,1,224,steven bethard,Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing,0,"Complex tasks like question answering need to be able to identify events in text and the relations among those events. We show that this event identification task and a related task, identifying the semantic class of these events, can both be formulated as classification problems in a word-chunking paradigm. We introduce a variety of linguistically motivated features for this task and then train a system that is able to identify events with a precision of 82% and a recall of 71%. We then show a variety of analyses of this model, and their implications for the event identification task."
