2007.mtsummit-papers.32,J84-3009,0,0.701915,"Missing"
bies-etal-2006-linguistic,A00-2018,0,\N,Missing
bies-etal-2006-linguistic,graff-bird-2000-many,0,\N,Missing
bies-etal-2006-linguistic,N01-1016,0,\N,Missing
bies-etal-2006-linguistic,W02-1007,0,\N,Missing
bies-etal-2006-linguistic,N04-4032,0,\N,Missing
bies-etal-2006-linguistic,J03-4003,0,\N,Missing
bies-etal-2006-linguistic,N06-1024,1,\N,Missing
bies-etal-2006-linguistic,P04-1005,0,\N,Missing
bies-etal-2006-linguistic,cieri-etal-2004-fisher,0,\N,Missing
chen-etal-2004-evaluating,eickeler-etal-2002-creation,0,\N,Missing
D07-1065,P04-1082,0,0.0605586,"fall into three categories. Dienes and Dubey (2003) recover empty nodes as a preprocessing step and pass strings with gaps to their parser. Their performance was comparable to (Johnson, 2002); however, they did not evaluate the impact of the gaps on parser performance. Collins (1999) directly incorporated wh-traces into his Model 3 parser, but he did not evaluate gap insertion accuracy directly. Most of the research belongs to the third category, i.e., post-processing of parser output. Johnson (2002) used corpus-induced patterns to insert gaps into both gold standard trees and parser output. Campbell (2004) developed a set of linguistically motivated hand-written rules for gap insertion. Machine learning methods were employed by (Higgins, 2003; Levy and Manning, 2004; Gabbard et al., 2006). In this paper, we develop a probabilistic model that uses a set of patterns and tree matching to guide the insertion of WH-traces. We only insert traces of non-null WH-phrases, as they are most relevant for our goals. Our effort differs from the previous approaches in that we have developed an algorithm for the insertion of gaps that combines a small set of expressive patterns with a probabilistic grammar-bas"
D07-1065,A00-2018,0,0.016053,"ue to the span requirement. To overcome the latter issue with Campbell’s metric, we propose to use a third metric that evaluates gaps with respect to correctness of their lexical head, type of the mother node, and the type of the coindexed wh-phrase. This metric differs from that used by Levy and Manning (2004) in that it counts only the dependencies involving gaps, and so it represents performance of the gap insertion algorithm more directly. We evaluate gap insertion on gold trees from section 23 of the Wall Street Journal Penn Treebank (WSJ) and parse trees automatically produced using the Charniak (2000) and Bikel (2004) parsers. These parsers were trained using sections 00 through 22 of the WSJ with section 24 as the development set. Because our algorithm inserts only traces of nonempty WH phrases, to fairly compare to Johnson’s and Gabbard’s performance on WH-traces alone, we 6 Johnson’s source code is publicly available, and Ryan Gabbard kindly provided us with output trees produced by his system. 626 remove the other gap types from both the gold trees and the output of their algorithms. Note that Gabbard et al.’s algorithm requires the use of function tags, which are produced using a modi"
D07-1065,W03-1005,0,0.016245,"ns, e.g., mapping parse trees to logical representations and structured representations for language modeling. For example, SuperARV language models (LMs) (Wang and Harper, 2002; Wang et al., 2003), which tightly integrate lexical features and syntactic constraints, have been found to significantly reduce word error in English speech recognition tasks. In order to generate SuperARV LM training, a state-ofthe-art parser is used to parse training material and then a rule-based transformer converts the parses to Approaches applied to the problem of empty node recovery fall into three categories. Dienes and Dubey (2003) recover empty nodes as a preprocessing step and pass strings with gaps to their parser. Their performance was comparable to (Johnson, 2002); however, they did not evaluate the impact of the gaps on parser performance. Collins (1999) directly incorporated wh-traces into his Model 3 parser, but he did not evaluate gap insertion accuracy directly. Most of the research belongs to the third category, i.e., post-processing of parser output. Johnson (2002) used corpus-induced patterns to insert gaps into both gold standard trees and parser output. Campbell (2004) developed a set of linguistically mo"
D07-1065,N06-1024,0,0.166055,"hnson, 2002); however, they did not evaluate the impact of the gaps on parser performance. Collins (1999) directly incorporated wh-traces into his Model 3 parser, but he did not evaluate gap insertion accuracy directly. Most of the research belongs to the third category, i.e., post-processing of parser output. Johnson (2002) used corpus-induced patterns to insert gaps into both gold standard trees and parser output. Campbell (2004) developed a set of linguistically motivated hand-written rules for gap insertion. Machine learning methods were employed by (Higgins, 2003; Levy and Manning, 2004; Gabbard et al., 2006). In this paper, we develop a probabilistic model that uses a set of patterns and tree matching to guide the insertion of WH-traces. We only insert traces of non-null WH-phrases, as they are most relevant for our goals. Our effort differs from the previous approaches in that we have developed an algorithm for the insertion of gaps that combines a small set of expressive patterns with a probabilistic grammar-based model. 620 Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational c Natural Language Learning, pp. 620–629, Prague, June 2007."
D07-1065,E03-1049,0,0.0154376,"Their performance was comparable to (Johnson, 2002); however, they did not evaluate the impact of the gaps on parser performance. Collins (1999) directly incorporated wh-traces into his Model 3 parser, but he did not evaluate gap insertion accuracy directly. Most of the research belongs to the third category, i.e., post-processing of parser output. Johnson (2002) used corpus-induced patterns to insert gaps into both gold standard trees and parser output. Campbell (2004) developed a set of linguistically motivated hand-written rules for gap insertion. Machine learning methods were employed by (Higgins, 2003; Levy and Manning, 2004; Gabbard et al., 2006). In this paper, we develop a probabilistic model that uses a set of patterns and tree matching to guide the insertion of WH-traces. We only insert traces of non-null WH-phrases, as they are most relevant for our goals. Our effort differs from the previous approaches in that we have developed an algorithm for the insertion of gaps that combines a small set of expressive patterns with a probabilistic grammar-based model. 620 Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational c Natural Lang"
D07-1065,P02-1018,0,0.102055,"(LMs) (Wang and Harper, 2002; Wang et al., 2003), which tightly integrate lexical features and syntactic constraints, have been found to significantly reduce word error in English speech recognition tasks. In order to generate SuperARV LM training, a state-ofthe-art parser is used to parse training material and then a rule-based transformer converts the parses to Approaches applied to the problem of empty node recovery fall into three categories. Dienes and Dubey (2003) recover empty nodes as a preprocessing step and pass strings with gaps to their parser. Their performance was comparable to (Johnson, 2002); however, they did not evaluate the impact of the gaps on parser performance. Collins (1999) directly incorporated wh-traces into his Model 3 parser, but he did not evaluate gap insertion accuracy directly. Most of the research belongs to the third category, i.e., post-processing of parser output. Johnson (2002) used corpus-induced patterns to insert gaps into both gold standard trees and parser output. Campbell (2004) developed a set of linguistically motivated hand-written rules for gap insertion. Machine learning methods were employed by (Higgins, 2003; Levy and Manning, 2004; Gabbard et a"
D07-1065,levy-andrew-2006-tregex,0,0.0381651,"ilable, and Ryan Gabbard kindly provided us with output trees produced by his system. 626 remove the other gap types from both the gold trees and the output of their algorithms. Note that Gabbard et al.’s algorithm requires the use of function tags, which are produced using a modified version of the Bikel parser (Gabbard et al., 2006) and a separate software tool (Blaheta, 2003) for the Charniak parser output. For our algorithm, we do not utilize function tags, but we automatically replace the tags of auxiliary verbs in tensed constructions with AUX prior to inserting gaps using tree surgeon (Levy and Andrew, 2006). We found that Johnson’s algorithm more accurately inserts gaps when operating on auxified trees, and so we evaluate his algorithm using these modified trees. In order to assess robustness of our algorithm, we evaluate it on a corpus of a different genre – Broadcast News Penn Treebank (BN), and compare the result with Johnson’s and Gabbard’s algorithms. The BN corpus uses a modified version of annotation guidelines, with some of the modifications affecting gap placement. Treebank 2 guidelines (WSJ style): (SBAR (WHNP-2 (WP whom)) (S (NP-SBJ (PRP they)) (VP (VBD called) (S (NP-SBJ (-NONE- *T*-"
D07-1065,P04-1042,0,0.0617911,"ce was comparable to (Johnson, 2002); however, they did not evaluate the impact of the gaps on parser performance. Collins (1999) directly incorporated wh-traces into his Model 3 parser, but he did not evaluate gap insertion accuracy directly. Most of the research belongs to the third category, i.e., post-processing of parser output. Johnson (2002) used corpus-induced patterns to insert gaps into both gold standard trees and parser output. Campbell (2004) developed a set of linguistically motivated hand-written rules for gap insertion. Machine learning methods were employed by (Higgins, 2003; Levy and Manning, 2004; Gabbard et al., 2006). In this paper, we develop a probabilistic model that uses a set of patterns and tree matching to guide the insertion of WH-traces. We only insert traces of non-null WH-phrases, as they are most relevant for our goals. Our effort differs from the previous approaches in that we have developed an algorithm for the insertion of gaps that combines a small set of expressive patterns with a probabilistic grammar-based model. 620 Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational c Natural Language Learning, pp. 620–6"
D07-1065,W02-1031,1,0.823901,"parser output using three different metrics. Our method compares favorably with state-of-the-art algorithms that recover WH-traces. 1 Introduction In this paper, we describe a new algorithm for recovering WH-trace empty nodes in gold parse trees in the Penn Treebank and, more importantly, in automatically generated parses. This problem has only been investigated by a handful of researchers and yet it is important for a variety of applications, e.g., mapping parse trees to logical representations and structured representations for language modeling. For example, SuperARV language models (LMs) (Wang and Harper, 2002; Wang et al., 2003), which tightly integrate lexical features and syntactic constraints, have been found to significantly reduce word error in English speech recognition tasks. In order to generate SuperARV LM training, a state-ofthe-art parser is used to parse training material and then a rule-based transformer converts the parses to Approaches applied to the problem of empty node recovery fall into three categories. Dienes and Dubey (2003) recover empty nodes as a preprocessing step and pass strings with gaps to their parser. Their performance was comparable to (Johnson, 2002); however, the"
D07-1065,J03-4003,0,\N,Missing
D07-1117,A00-1031,0,0.0307159,"Missing"
D07-1117,P05-1022,0,0.035713,"Missing"
D07-1117,A00-2018,0,0.0433678,"NN), and (prefix, D, X, 0, NN) for every tag X not in {NN, VV}, where 1 and 0 are indicator values. Features are extracted in the similar way for the suffix ‹. The n-gram and morphological features are easy to compute, however, they have difficulty in capturing the long distance information related to syntactic relationships that might help POS tagging accuracy. In order to examine the effectiveness of utilizing syntactic information in tagging, we have also experimented with dependency features that are extracted based on automatic parse trees. First a bracketing parser (the Charniak parser (Charniak, 2000) in our case) is used to generate the parse tree of a sentence, then the const2dep tool developed by Hwa was utilized to convert the bracketing tree to a dependency tree based on the head percolation table developed by the second author. The dependency tree is comprised of a set of dependency relations among word pairs. A dependency relation is a triple hword-a, word-b, relationi, in which word-a is governed by word-b with grammatical relation denoted as relation. For example, in the sentence “• Ï(Tibet) Ï N(economy) ú (construction) Ö —(achieves) > W(significant) é(accomplishments)”, one exa"
D07-1117,W02-2236,0,0.0538824,"Missing"
D07-1117,J05-1003,0,0.248035,"ti ti+1 ) in Equation 4 by two time slices3 (i.e., by replacing P (wi |ti ti+1 ) with P (wi−2 |ti−2 ti−1 )), we are able to compute τ (w1N ) in Equation 4 with the same asymptotic time complexity of decoding as in Equation 2. 3 where 2.3 This corresponds to a mixture model of two generation paths, one from the left and one from the right, to approximate τ (w1N ) in Equation 1 in a different way. Discriminative Reranking In this section, we describe our use of the RankBoost-based (Freund and Schapire, 1997; Freund et al., 1998) discriminative reranking approach that was originally developed by Collins and Koo (2005) for parsing. It provides an additional avenue for improving tagging accuracy, and also allows us to investigate the impact of various features on Mandarin tagging performance. The reranking algorithm takes as input a list of candidates produced by some probabilistic model, in our case the HMM tagger, and reranks these candidates based on a set of features. We first introduce Collins’ reranking algorithm in Subsection 3.1, and then describe two modifications in Subsections 3.2 and 3.3 that were designed to improve the generalization performance of the reranking algorithm for our POS tagging ta"
D07-1117,W06-3607,0,0.0379472,"Missing"
D07-1117,W96-0213,0,0.189405,"provement. However, the relatively poorer performance of existing methods on Mandarin POS tagging makes reranking a much more compelling technique to evaluate. In this paper, we use reranking to improve tagging performance of an HMM tagger adapted to 1093 Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational c Natural Language Learning, pp. 1093–1102, Prague, June 2007. 2007 Association for Computational Linguistics Mandarin. Hidden Markov models are simple and effective, but unlike discriminative models, such as Maximum Entropy models (Ratnaparkhi, 1996) and Conditional Random Fields (John Lafferty, 2001), they have more difficulty utilizing a rich set of conditionally dependent features. This limitation can be overcome by utilizing reranking approaches, which are able to make use of the features extracted from the tagging hypotheses produced by the HMM tagger. Reranking also has advantages over MaxEnt and CRF models. It is able to use any features extracted from entire labeled sentences, including those that cannot be incorporated into MaxEnt and CRF models due to inference difficulties. In addition, reranking methods are able to utilize the"
D07-1117,P99-1023,1,0.233445,"ications to better handle unknown words in Mandarin and to enrich the word emission probabilities through the combination of bi-directional estimations. In Section 3, we first describe the reranking algorithm and then propose two methods to improve its performance. We also describe the features that will be used for Mandarin POS reranking in Section 3. Experimental results are given in Section 4. Conclusions and future work appear in Section 5. 2 The HMM Model 2.1 Porting English Tagger to Mandarin The HMM tagger used in this work is a secondorder HMM tagger initially developed for English by Thede and Harper (1999). This state-of-the-art second-order HMM tagger uses trigram transition probability estimations, P (ti |ti−2 ti−1 ), and trigram emission probability estimations, P (wi |ti−1 ti ). Let ti1 denote the tag sequence t1 , · · · , ti , and w1i denote the word sequence w1 , · · · , wi . The tagging problem can be formally defined as finding the best tag sequence τ (w1N ) for the word sequence w1N of length N as follows1 : τ (w1N ) = N arg max P (tN 1 |w1 ) = arg max = N arg max P (tN 1 w1 ) tN 1 = arg max tN 1 tN 1 tN 1 Y N P (tN 1 w1 ) N P (w1 ) (1) i−1 P (ti |ti−1 )P (wi |ti1 w1i−1 ) 1 w1 i 1 We a"
D07-1117,I05-3005,0,0.252107,"ing Features A reranking model has the flexibility of incorporating any type of feature extracted from N-best candidates. For the work presented in this paper, we examine three types of features. For each window of three word/tag pairs, we extract all the n-grams, except those that are comprised of only one word/tag pair, or only tags, or only words, or do not include either the word or tag in the center word/tag pair. These constitute the n-gram feature set. In order to better handle unknown words, we also extract the two most important types of morphological features5 that were utilized in (Tseng et al., 2005) for those words that appear no more than seven times (following their convention) in the training set: Affixation features: we use character n-gram prefixes and suffixes for n up to 4. For example, for word/tag pair D ™ ‹/NN (InformationBag, i.e., folder), we add the following features: (prefix1, D, NN), (prefix2, D™, NN), (prefix3, D™‹, NN), (suffix1, ‹, NN), (suffix2, ™‹, NN), (suffix3, D™‹, NN). AffixPOS features6 : we used the training set to build a prefix/POS and suffix/POS dictionary associating possible tags with each prefix and 5 Tseng at el. also used other morphological features th"
D07-1117,C02-1145,0,0.106075,"Missing"
D09-1087,W00-1201,0,0.0206989,"Missing"
D09-1087,E03-1008,0,0.150483,"Missing"
D09-1087,J82-2005,0,0.78178,"Missing"
D09-1087,P05-1022,0,0.488887,"erent NP-i→PRP-j rules. The expansion probabilities of these split rules are the parameters of a PCFG-LA grammar. was trained on a small labeled set. Reichart and Rappoport (2007) obtained significant gains using Collins lexicalized parser with a different selftraining protocol, but again they only looked at small labeled sets. McClosky et al. (2006) effectively utilized unlabeled data to improve parsing accuracy on the standard WSJ training set, but they used a two-stage parser comprised of Charniak’s lexicalized probabilistic parser with n-best parsing and a discriminative reranking parser (Charniak and Johnson, 2005), and thus it would be better categorized as “co-training” (McClosky et al., 2008). It is worth noting that their attempts at selftraining Charniak’s lexicalized parser directly resulted in no improvement. There are other successful semi-supervised training approaches for dependency parsing, such as (Koo et al., 2008; Wang et al., 2008), and it would be interesting to investigate how they could be applied to constituency parsing. We show in this paper, for the first time, that self-training is able to significantly improve the performance of the PCFG-LA parser, a single generative parser, on b"
D09-1087,P08-1061,0,0.0198229,"2006) effectively utilized unlabeled data to improve parsing accuracy on the standard WSJ training set, but they used a two-stage parser comprised of Charniak’s lexicalized probabilistic parser with n-best parsing and a discriminative reranking parser (Charniak and Johnson, 2005), and thus it would be better categorized as “co-training” (McClosky et al., 2008). It is worth noting that their attempts at selftraining Charniak’s lexicalized parser directly resulted in no improvement. There are other successful semi-supervised training approaches for dependency parsing, such as (Koo et al., 2008; Wang et al., 2008), and it would be interesting to investigate how they could be applied to constituency parsing. We show in this paper, for the first time, that self-training is able to significantly improve the performance of the PCFG-LA parser, a single generative parser, on both small and large amounts of labeled training data, for both English and Chinese. With self-training, a fraction of the WSJ or CTB6 treebank training data is sufficient to train a PCFG-LA parser that is able to achieve or even beat the accuracies obtained using a single parser trained on the entire treebank without selftraining. We co"
D09-1087,D07-1117,1,0.613034,"ater, both of these methods significantly improve parsing performance. 2.1 probabilities: c(t, w) c(tx , w) = cr (tx , unk) · cr (t, unk) X c(tx , w) p(w|tx ) = c(tx , w)/ w 2.2 Chinese Unknown Word Handling The Berkeley parser utilizes statistics associated with rare words (e.g., suffix, capitalization) to estimate the emission probabilities of unknown words at decoding time. This is adequate for for English, however, only a limited number of classes of unknown words, such as digits and dates, are handled for Chinese. In this paper, we develop a characterbased unknown word model inspired by (Huang et al., 2007) that reflects the fact that characters in any position (prefix, infix, or suffix) can be predictive of the part-of-speech (POS) type for Chinese words. In our model, the word emission probability, p(w|tx ), of an unknown word w given the latent state tx of POS tag t is estimated by the geometric average of the emission probability of the characters ck in the word: rY P (w|tx ) = n P (ck |t) Rare Word Handling Whereas rule expansions are frequently observed in the treebank, word-tag co-occurrences are sparser and more likely to suffer from over-fitting. Although the lexicon smoothing method in"
D09-1087,P08-1067,0,0.0553762,"than lexicalized generative parsers. We show for the first time that self-training is able to significantly improve the performance of a PCFG-LA parser, a single generative parser, on both small and large amounts of labeled training data. We conjecture based on our analysis that the EM training algorithm is able to exploit the information available in both gold and automatically labeled data with more complex grammars while being less affected by over-fitting. Better results would be expected by combining the PCFG-LA parser with discriminative reranking approaches (Charniak and Johnson, 2005; Huang, 2008) for self training. Self-training should also benefit other discriminatively trained parsers with latent annotations (Petrov and Klein, 2008), although training would be much slower compared to using generative models, as in our case. In future work, we plan to scale up the training process with more unlabeled training data (e.g., gigaword) and investigate automatic selection of materials that are most suitable for self-training. We also plan to investigate domain adaptation and apply the model to other languages with modest treebank resources. Finally, it is also important to explore other wa"
D09-1087,P08-1068,0,0.041962,"McClosky et al. (2006) effectively utilized unlabeled data to improve parsing accuracy on the standard WSJ training set, but they used a two-stage parser comprised of Charniak’s lexicalized probabilistic parser with n-best parsing and a discriminative reranking parser (Charniak and Johnson, 2005), and thus it would be better categorized as “co-training” (McClosky et al., 2008). It is worth noting that their attempts at selftraining Charniak’s lexicalized parser directly resulted in no improvement. There are other successful semi-supervised training approaches for dependency parsing, such as (Koo et al., 2008; Wang et al., 2008), and it would be interesting to investigate how they could be applied to constituency parsing. We show in this paper, for the first time, that self-training is able to significantly improve the performance of the PCFG-LA parser, a single generative parser, on both small and large amounts of labeled training data, for both English and Chinese. With self-training, a fraction of the WSJ or CTB6 treebank training data is sufficient to train a PCFG-LA parser that is able to achieve or even beat the accuracies obtained using a single parser trained on the entire treebank without"
D09-1087,levy-andrew-2006-tregex,0,0.0131704,"Missing"
D09-1087,P03-1056,0,0.0610328,"Missing"
D09-1087,P05-1010,0,0.252321,"better handling of rare words across languages, as well as unknown Chinese words. The parser is able to process difficult sentences robustly using adaptive beam expansion. The training algorithm was updated to support a wide range of self-training experiments (e.g., posterior-weighted unlabeled data, introducing self-training in later iterations) and to make use of multiple processors to parallelize EM training. The parallelization is crucial Parsing Model The Berkeley parser (Petrov et al., 2006; Petrov and Klein, 2007) is an efficient and effective parser that introduces latent annotations (Matsuzaki et al., 2005) to refine syntactic categories to learn better PCFG grammars. In the example parse tree in Figure 1(a), each syntactic category is split into 1 A major motivation for this implementation was to support some algorithms we are developing. Most of our enhancements will be merged with a future release of the Berkeley parser. 833 for training a model with large volumes of data in a reasonable amount of time2 . We next describe the language-independent method to handle rare words, which is important for training better PCFG-LA grammars especially when the training data is limited in size, and our u"
D09-1087,N06-1020,0,0.366516,"ACL and AFNLP multiple latent subcategories, and accordingly the original parse tree is decomposed into many parse trees with latent annotations. Figure 1(b) depicts one of such trees. The grammar and lexical rules are split accordingly, e.g., NP→PRP is split into different NP-i→PRP-j rules. The expansion probabilities of these split rules are the parameters of a PCFG-LA grammar. was trained on a small labeled set. Reichart and Rappoport (2007) obtained significant gains using Collins lexicalized parser with a different selftraining protocol, but again they only looked at small labeled sets. McClosky et al. (2006) effectively utilized unlabeled data to improve parsing accuracy on the standard WSJ training set, but they used a two-stage parser comprised of Charniak’s lexicalized probabilistic parser with n-best parsing and a discriminative reranking parser (Charniak and Johnson, 2005), and thus it would be better categorized as “co-training” (McClosky et al., 2008). It is worth noting that their attempts at selftraining Charniak’s lexicalized parser directly resulted in no improvement. There are other successful semi-supervised training approaches for dependency parsing, such as (Koo et al., 2008; Wang"
D09-1087,C08-1071,0,0.0523468,"ters of a PCFG-LA grammar. was trained on a small labeled set. Reichart and Rappoport (2007) obtained significant gains using Collins lexicalized parser with a different selftraining protocol, but again they only looked at small labeled sets. McClosky et al. (2006) effectively utilized unlabeled data to improve parsing accuracy on the standard WSJ training set, but they used a two-stage parser comprised of Charniak’s lexicalized probabilistic parser with n-best parsing and a discriminative reranking parser (Charniak and Johnson, 2005), and thus it would be better categorized as “co-training” (McClosky et al., 2008). It is worth noting that their attempts at selftraining Charniak’s lexicalized parser directly resulted in no improvement. There are other successful semi-supervised training approaches for dependency parsing, such as (Koo et al., 2008; Wang et al., 2008), and it would be interesting to investigate how they could be applied to constituency parsing. We show in this paper, for the first time, that self-training is able to significantly improve the performance of the PCFG-LA parser, a single generative parser, on both small and large amounts of labeled training data, for both English and Chinese"
D09-1087,N07-1051,0,0.267641,"likelihood. For this paper, we implemented1 our own version of Berkeley parser. Updates include better handling of rare words across languages, as well as unknown Chinese words. The parser is able to process difficult sentences robustly using adaptive beam expansion. The training algorithm was updated to support a wide range of self-training experiments (e.g., posterior-weighted unlabeled data, introducing self-training in later iterations) and to make use of multiple processors to parallelize EM training. The parallelization is crucial Parsing Model The Berkeley parser (Petrov et al., 2006; Petrov and Klein, 2007) is an efficient and effective parser that introduces latent annotations (Matsuzaki et al., 2005) to refine syntactic categories to learn better PCFG grammars. In the example parse tree in Figure 1(a), each syntactic category is split into 1 A major motivation for this implementation was to support some algorithms we are developing. Most of our enhancements will be merged with a future release of the Berkeley parser. 833 for training a model with large volumes of data in a reasonable amount of time2 . We next describe the language-independent method to handle rare words, which is important for"
D09-1087,D08-1091,0,0.0685497,"e of a PCFG-LA parser, a single generative parser, on both small and large amounts of labeled training data. We conjecture based on our analysis that the EM training algorithm is able to exploit the information available in both gold and automatically labeled data with more complex grammars while being less affected by over-fitting. Better results would be expected by combining the PCFG-LA parser with discriminative reranking approaches (Charniak and Johnson, 2005; Huang, 2008) for self training. Self-training should also benefit other discriminatively trained parsers with latent annotations (Petrov and Klein, 2008), although training would be much slower compared to using generative models, as in our case. In future work, we plan to scale up the training process with more unlabeled training data (e.g., gigaword) and investigate automatic selection of materials that are most suitable for self-training. We also plan to investigate domain adaptation and apply the model to other languages with modest treebank resources. Finally, it is also important to explore other ways to exploit the use of unlabeled data. P P p(a → b|e, t) p(a → b|e, t) + α t∈Tu e∈t t∈Tl e∈t P P P P P p(a → b|e, t)) p(a → b|e, t) + α ( P"
D09-1087,P06-1055,0,0.799003,"to increase training likelihood. For this paper, we implemented1 our own version of Berkeley parser. Updates include better handling of rare words across languages, as well as unknown Chinese words. The parser is able to process difficult sentences robustly using adaptive beam expansion. The training algorithm was updated to support a wide range of self-training experiments (e.g., posterior-weighted unlabeled data, introducing self-training in later iterations) and to make use of multiple processors to parallelize EM training. The parallelization is crucial Parsing Model The Berkeley parser (Petrov et al., 2006; Petrov and Klein, 2007) is an efficient and effective parser that introduces latent annotations (Matsuzaki et al., 2005) to refine syntactic categories to learn better PCFG grammars. In the example parse tree in Figure 1(a), each syntactic category is split into 1 A major motivation for this implementation was to support some algorithms we are developing. Most of our enhancements will be merged with a future release of the Berkeley parser. 833 for training a model with large volumes of data in a reasonable amount of time2 . We next describe the language-independent method to handle rare word"
D09-1087,P07-1078,0,0.0868515,"treebanked Chinese materials are more 832 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 832–841, c Singapore, 6-7 August 2009. 2009 ACL and AFNLP multiple latent subcategories, and accordingly the original parse tree is decomposed into many parse trees with latent annotations. Figure 1(b) depicts one of such trees. The grammar and lexical rules are split accordingly, e.g., NP→PRP is split into different NP-i→PRP-j rules. The expansion probabilities of these split rules are the parameters of a PCFG-LA grammar. was trained on a small labeled set. Reichart and Rappoport (2007) obtained significant gains using Collins lexicalized parser with a different selftraining protocol, but again they only looked at small labeled sets. McClosky et al. (2006) effectively utilized unlabeled data to improve parsing accuracy on the standard WSJ training set, but they used a two-stage parser comprised of Charniak’s lexicalized probabilistic parser with n-best parsing and a discriminative reranking parser (Charniak and Johnson, 2005), and thus it would be better categorized as “co-training” (McClosky et al., 2008). It is worth noting that their attempts at selftraining Charniak’s le"
D09-1087,A00-2018,0,\N,Missing
D09-1087,J03-4003,0,\N,Missing
D09-1087,W08-0336,0,\N,Missing
D09-1116,N03-2002,0,0.0440209,"useful for designing tagsets, e.g., for a new language, because it allows us to assess relative performance of tagsets without having to train a full model. 3 We used one-count smoothing (Chen and Goodman, 1996). While the tree construction algorithm is fairly standard – to recursively select binary questions about the history optimizing some function – there are important decisions to make in terms of which questions to ask and which function to optimize. In the remainder of this section, we discuss the decisions we made regarding these issues. 3.2 Factors The Factored Language Model (FLM) (Bilmes and Kirchhoff, 2003) offers a convenient view of the input data: it represents every word in a sentence as a tuple of factors. This allows us to extend the language model with additional parameters. In an FLM, however, all factors have to be deterministically computed in a joint model; whereas, we need to distinguish between the factors that are given or computed and the factors that the model must predict stochastically. We call these types of factors overt and hidden, respectively. Examples of overt factors include surface words, morphological features such as suffixes, case information when available, etc., an"
D09-1116,J92-4003,0,0.0501089,"ameter space can be quite sparse and requires sophisticated techniques for reliable probability estimation (Chen and Goodman, 1996). While the ngram models perform fairly well, they are only capable of capturing very shallow knowledge of the language. There is extensive literature on a variety of methods that have been used to imbue models with syntactic and semantic information in different ways. These methods can be broadly categorized into two types: • The first method uses surface words within its context, sometimes organizing them into deterministic classes. Models of this type include: (Brown et al., 1992; Zitouni, 2007), which use semantic word clustering, and (Bahl et al., 1990), which uses variablelength context. • The other method adds stochastic variables to express the ambiguous nature of surface words2 . To obtain the probability of the next argmax p(w1n |A) = argmax p(A|w1n ) · p(w1n ) w1n i=1 p(wi |w1i−1 ) Approximation is required to keep the parameter space tractable. Most commonly the context is reduced to just a few immediately preceding words. This type of model is called an ngram model: Introduction w1n n Y Real applications use argmaxwn p(A|w1n )·p(w1n )α ·nβ 1 instead of Eq. 1"
D09-1116,P96-1041,0,0.0202521,"the problem of selecting the best sequence of words from multiple hypotheses. This problem stems from the noisy channel approach to these applications. The noisy channel model states that the observed data, e.g., the acoustic signal, is the result of some input translated by some unknown stochastic process. Then the problem of finding the best sequence of words given the acoustic input, not approachable directly, is transformed into two separate models: Even with limited context, the parameter space can be quite sparse and requires sophisticated techniques for reliable probability estimation (Chen and Goodman, 1996). While the ngram models perform fairly well, they are only capable of capturing very shallow knowledge of the language. There is extensive literature on a variety of methods that have been used to imbue models with syntactic and semantic information in different ways. These methods can be broadly categorized into two types: • The first method uses surface words within its context, sometimes organizing them into deterministic classes. Models of this type include: (Brown et al., 1992; Zitouni, 2007), which use semantic word clustering, and (Bahl et al., 1990), which uses variablelength context."
D09-1116,W99-0617,0,0.758505,"coarser POS model, as well as the ngram baseline, in Section 5. (2) i−1 i−1 p(wi ti |w1i−1 ti−1 1 )p(w1 t1 ) P t1 ...ti−1 p(w1i−1 ti−1 1 ) Models of this type, which we call joint models since they essentially predict joint events of words and some random variable(s), include (Chelba and Jelinek, 2000) which used POS tags in combination with “parser instructions” for constructing a full parse tree in a left-to-right manner; (Wang et al., 2003) used SuperARVs (complex tuples of dependency information) without resolving the dependencies, thus called almost parsing; (Niesler and Woodland, 1996; Heeman, 1999) utilize part of speech (POS) tags. Note that some models reduce the context by making the following approximation: • Address the challenges that arise in a joint language model with fine-grain tags. While the idea of using joint language modeling is not novel (Chelba and Jelinek, 2000; Heeman, 1999), nor is the idea of using fine-grain tags (Bangalore, 1996; Wang et al., 2003), none of prior papers focus on the issues that arise from the combination of joint language modeling with fine-grain tags, both in terms of reliable parameter estimation and scalability in the face of the increased comp"
D09-1116,D09-1087,1,0.187415,"f topics than the Wall Street Journal, we eliminated the most irrelevant stories based on their trigram coverage by sections 00-22 of WSJ. We also eliminated sentences over 120 words, because the parser’s performance drops significantly on long sentences. After parsing the corpus, we deleted sentences that were assigned a very low probability by the parser. Overall we removed only a few percent of the data; however, we believe that such a rigorous approach to data cleaning is important for building discriminating models. Parse trees were produced by an extended version of the Berkeley parser (Huang and Harper, 2009). We trained the parser on a combination of the BN and WSJ treebanks, preprocessed to make them more consistent with each other. We also modified the trees for the speech recognition task by replacing numbers and abbreviations with their verbalized forms. We pre-processed the NYT corpus in the same way, and parsed it. After that, we removed punctuation and downcased words. For the ngram model, we used text processed in the same way. In head and parent models, tag vocabularies contain approximately 1,500 tags each, while the SuperARV model has approximately 1,400 distinct SuperARVs, most of whi"
D09-1116,P99-1023,1,0.641455,"n exactly, thus we can create a projection of the more reliable the distribution p˜A (wi ti ) is, and the clustering function H in Eq. 4 to the plane hence αA is lower. The formula we use is as foli−1 wi−n+1 = const, i.e., where words in the context lows: are fixed to be whatever is observed in the history: 1 αA = p 1 + distanceT oRoot(A) ˆ i−1 5 H(wi−1 ti−1 ) ⇒ H (ti−1 ) We use this distribution rather than uniform joint distrip˜n (wi ti ) = p˜(wi ti |q1 . . . q(n′ )′ qn′ ) 1 because we do not want to allow word-tag pairs bution |V ||T | that have never been observed. The idea is similar to (Thede and Harper, 1999). 6 To avoid a large number of zeros due to the product, we set a minimum for λ to be 10−7 . 7 The lower order model is constructed by the same algorithm, although with smaller context. Note that the lower order model can back off on words or tags, or both. In this paper i−1 i−1 ti−2 ) we backoff both on words and tags, i.e., p(wi ti |wi−2 backs off to p(wi ti |wi−1 ti−1 ), which in turn backs off to the unigram p(wi ti ). i−n+1 i−n+1 wi−n+1 =const i−n+1 (6) The number of distinct clusters in the projection ˆ depends on the decision tree configuration and H i−1 can vary greatly for different w"
D09-1116,W02-1031,1,0.904505,"an be directly derived from its syntactic parse. SuperARVs encode lexical information as well as syntactic and semantic constraints in a uniform representation that is much more fine-grained than POS. It is a four-tuple (C; F ; R+; D), where C is the lexical category of the word, F is a vector of lexical features for the word, R+ is a set of governor and need labels that indicate the function of the word in the sentence and the types of words it needs, and D represents the relative position of the word and its dependents. We refer the reader to the literature for further details on SuperARVs (Wang and Harper, 2002; Wang et al., 2003). SuperARVs can be produced from parse trees by applying deterministic rules. In this work we use SuperARVs as individual tags and do not cluster them based of their structure. While SuperARVs are very attractive for language modeling, developing such a rich set of annotations for a new language would require a large amount of human effort. We propose two other types of tags which have not been applied to this task, although similar information has been used in parsing. This tag is a combination of the word’s POS tag with its immediate parent in the parse tree, along with t"
D09-1116,W04-3242,0,\N,Missing
D10-1002,W08-2102,0,0.357336,"Missing"
D10-1002,N01-1016,0,0.010271,"Test Unlabeled Newswire # sentences # words length Avg./Std. 39.8k 950.0k 28.9/11.2 1.7k 40.1k 25.1/11.8 2.4k 56.7k 25.1/12.0 1,769.1k 43,057.0k 24.3/10.9 Broadcast News # sentences # words length Avg./Std. 59.0k 1,281.1k 17.3/11.3 1.0k 17.1k 17.4/11.3 1.1k 19.4k 17.7/11.4 4,386.5k 77,687.9k 17.7/12.8 Table 1: The number of words and sentences, together with average (Avg.) sentence length and its standard deviation (Std.), for the data sets used in our experiments. duces scores that are identical to those produced by EVALB for WSJ. For Broadcast News, SParseval applies Charniak and Johnson’s (Charniak and Johnson, 2001) scoring method for EDITED nodes3 . Using this method, BN scores were slightly (.05-.1) lower than if EDITED constituents were treated like any other, as in EVALB. 2.3 Latent Variable Grammars We use the latent variable grammar (Matsuzaki et al., 2005; Petrov et al., 2006) implementation of Huang and Harper (2009) in this work. Latent variable grammars augment the observed parse trees in the treebank with a latent variable at each tree node. This effectively splits each observed category into a set of latent subcategories. An EM-algorithm is used to fit the model by maximizing the joint likeli"
D10-1002,P05-1022,0,0.104018,"ccuracies can be achieved by training grammars on disjoint sets of automatically labeled data. Two primary factors appear to be determining the efficacy of our self-training approach. First, the accuracy of the model used for parsing the unlabeled data is important for the accuracy of the resulting single self-trained grammars. Second, the diversity of the individual grammars controls the gains that can be obtained by combining multiple grammars into a product model. Our most accurate single grammar achieves an F score of 91.6 on the WSJ test set, rivaling discriminative reranking approaches (Charniak and Johnson, 2005) and products of latent variable grammars (Petrov, 2010), despite being a single generative PCFG. Our most accurate product model achieves an F score of 92.5 without the use of discriminative reranking and comes close to the best known numbers on this test set (Zhang et al., 2009). In future work, we plan to investigate additional methods for increasing the diversity of our selftrained models. One possibility would be to utilize more unlabeled data or to identify additional ways to bias the models. It would also be interesting to determine whether further increasing the accuracy of the model u"
D10-1002,A00-2018,0,0.220478,"Missing"
D10-1002,D09-1116,1,0.56437,"thod for injecting some additional diversity into the individual grammars to determine whether a product model is most successful when there is more variance among the individual models. Our initial experiments and analysis will focus on the development set of WSJ. We will then follow up with an analysis of broadcast news (BN) to determine whether the findings generalize to a second, less structured type of data. It is important to construct grammars capable of parsing this type of data accurately and consistently in order to support structured language modeling (e.g., (Wang and Harper, 2002; Filimonov and Harper, 2009)). 4 Newswire Experiments In this section, we compare single grammars and their products that are trained in the standard way with gold WSJ training data, as well as the three self-training scenarios discussed in Section 3. We 15 4.1 Regular Training We begin by training ten latent variable models initialized with different random seeds using the gold WSJ training set. Results are presented in Table 2. The best F score attained by the individual SM6 grammars on the development set is 90.8, with an average score of 90.5. The product of grammars achieves a significantly improved accuracy at 92.0"
D10-1002,N09-2064,0,0.115286,"Missing"
D10-1002,D09-1087,1,0.925811,"cently, Petrov (2010) showed that substantial differences between the learned grammars remain, even if the hierarchical splitting reduces the variance across independent runs of EM. In order to counteract the overfitting behavior, Petrov et al. (2006) introduced a linear smoothing procedure that allows training grammars for 6 splitmerge (SM) rounds without overfitting. The increased expressiveness of the model, combined with the more robust parameter estimates provided by the smoothing, results in a nice increase in parsing accuracy on a held-out set. However, as reported by Petrov (2009) and Huang and Harper (2009), an additional 7th SM round actually hurts performance. Huang and Harper (2009) addressed the issue of data sparsity and overfitting from a different angle. They showed that self-training latent variable grammars on their own output can mitigate data sparsity issues and improve parsing accuracy. Because the capacity of the model can grow with the size of the training data, latent variable grammars are able to benefit from the additional training data, even though it is not perfectly labeled. Consequently, they also found that a 7th round of SM training was beneficial in the presence of large"
D10-1002,P08-1067,0,0.309603,"Missing"
D10-1002,D07-1072,1,0.65979,". A hierarchical split-and-merge algorithm introduces grammar complexity gradually, iteratively splitting (and potentially merging back) each observed treebank category into a number of increasingly refined latent subcategories. The Expectation Maximization (EM) algorithm is used to train the model, guaranteeing that each EM iteration will increase the training likelihood. However, because the latent variable grammars are not explicitly regularized, EM keeps fitSlav Petrov∗ Google Research 76 Ninth Avenue New York, NY slav@google.com ∗ ting the training data and eventually begins overfitting (Liang et al., 2007). Moreover, EM is a local method, making no promises regarding the final point of convergence when initialized from different random seeds. Recently, Petrov (2010) showed that substantial differences between the learned grammars remain, even if the hierarchical splitting reduces the variance across independent runs of EM. In order to counteract the overfitting behavior, Petrov et al. (2006) introduced a linear smoothing procedure that allows training grammars for 6 splitmerge (SM) rounds without overfitting. The increased expressiveness of the model, combined with the more robust parameter est"
D10-1002,P05-1010,0,0.398785,"k 19.4k 17.7/11.4 4,386.5k 77,687.9k 17.7/12.8 Table 1: The number of words and sentences, together with average (Avg.) sentence length and its standard deviation (Std.), for the data sets used in our experiments. duces scores that are identical to those produced by EVALB for WSJ. For Broadcast News, SParseval applies Charniak and Johnson’s (Charniak and Johnson, 2001) scoring method for EDITED nodes3 . Using this method, BN scores were slightly (.05-.1) lower than if EDITED constituents were treated like any other, as in EVALB. 2.3 Latent Variable Grammars We use the latent variable grammar (Matsuzaki et al., 2005; Petrov et al., 2006) implementation of Huang and Harper (2009) in this work. Latent variable grammars augment the observed parse trees in the treebank with a latent variable at each tree node. This effectively splits each observed category into a set of latent subcategories. An EM-algorithm is used to fit the model by maximizing the joint likelihood of parse trees and sentences. To allocate the grammar complexity only where needed, a simple split-and-merge procedure is applied. In every splitmerge (SM) round, each latent variable is first split in two and the model is re-estimated. A likelih"
D10-1002,N06-1020,0,0.854687,"odels’ mistakes are independent to some extent, multiple grammars can be effectively combined into an unweighted product model of much higher accuracy. We build upon this line of work and investigate methods to exploit products of latent variable grammars in the context of self-training. 3 Self-training Methodology Different types of parser self-training have been proposed in the literature over the years. All of them involve parsing a set of unlabeled sentences with a baseline parser and then estimating a new parser by combining this automatically parsed data with the original training data. McClosky et al. (2006) presented a very effective method for self-training a two-stage parsing system consisting of a first-stage generative lexicalized parser and a second-stage discriminative reranker. In their approach, a large amount of unlabeled text is parsed by the two-stage system and the parameters of the first-stage lexicalized parser are then re-estimated taking the counts from the automatically parsed data into consideration. More recently Huang and Harper (2009) presented a self-training procedure based on an EMalgorithm. They showed that the EM-algorithm that is typically used to fit a latent variable"
D10-1002,N07-1051,1,0.885056,"Missing"
D10-1002,P06-1055,1,0.639362,"the latent variable grammars are not explicitly regularized, EM keeps fitSlav Petrov∗ Google Research 76 Ninth Avenue New York, NY slav@google.com ∗ ting the training data and eventually begins overfitting (Liang et al., 2007). Moreover, EM is a local method, making no promises regarding the final point of convergence when initialized from different random seeds. Recently, Petrov (2010) showed that substantial differences between the learned grammars remain, even if the hierarchical splitting reduces the variance across independent runs of EM. In order to counteract the overfitting behavior, Petrov et al. (2006) introduced a linear smoothing procedure that allows training grammars for 6 splitmerge (SM) rounds without overfitting. The increased expressiveness of the model, combined with the more robust parameter estimates provided by the smoothing, results in a nice increase in parsing accuracy on a held-out set. However, as reported by Petrov (2009) and Huang and Harper (2009), an additional 7th SM round actually hurts performance. Huang and Harper (2009) addressed the issue of data sparsity and overfitting from a different angle. They showed that self-training latent variable grammars on their own o"
D10-1002,N10-1003,1,0.0899128,"into a number of increasingly refined latent subcategories. The Expectation Maximization (EM) algorithm is used to train the model, guaranteeing that each EM iteration will increase the training likelihood. However, because the latent variable grammars are not explicitly regularized, EM keeps fitSlav Petrov∗ Google Research 76 Ninth Avenue New York, NY slav@google.com ∗ ting the training data and eventually begins overfitting (Liang et al., 2007). Moreover, EM is a local method, making no promises regarding the final point of convergence when initialized from different random seeds. Recently, Petrov (2010) showed that substantial differences between the learned grammars remain, even if the hierarchical splitting reduces the variance across independent runs of EM. In order to counteract the overfitting behavior, Petrov et al. (2006) introduced a linear smoothing procedure that allows training grammars for 6 splitmerge (SM) rounds without overfitting. The increased expressiveness of the model, combined with the more robust parameter estimates provided by the smoothing, results in a nice increase in parsing accuracy on a held-out set. However, as reported by Petrov (2009) and Huang and Harper (200"
D10-1002,roark-etal-2006-sparseval,1,0.786696,"Missing"
D10-1002,N06-2033,0,0.301023,"Missing"
D10-1002,P05-1003,0,0.00841046,"The power of the product model comes directly from the diversity in log p(r|s, G) among individual grammars. If there is little diversity, the individual grammars would make similar predictions and there would be little or no benefit from using a product model. We use the average empirical variance of the log posterior probabilities of the rules among the learned grammars over a held-out set S as a proxy of the diversity among the grammars: P P P p(r|s, G)VAR(log(p(r|s, G))) s∈S G∈G r∈R(G,s) P P Diversity From the perspective of Products of Experts (Hinton, 1999) or Logarithmic Opinion Pools (Smith et al., 2005), each individual expert learns complementary aspects of the training data and the veto power of product models enforces that the joint prediction of their product has to be licensed by all individual experts. One possible explanation of the observation in the previous subsection is that with the addition of more latent variables, the individual grammars become more deeply specialized on certain aspects of the training data. This specialization leads to greater diversity in their prediction preferences, especially in the presence of a small training set. On the other hand, the self-labeled tra"
D10-1002,W02-1031,1,0.777259,"iment investigates a method for injecting some additional diversity into the individual grammars to determine whether a product model is most successful when there is more variance among the individual models. Our initial experiments and analysis will focus on the development set of WSJ. We will then follow up with an analysis of broadcast news (BN) to determine whether the findings generalize to a second, less structured type of data. It is important to construct grammars capable of parsing this type of data accurately and consistently in order to support structured language modeling (e.g., (Wang and Harper, 2002; Filimonov and Harper, 2009)). 4 Newswire Experiments In this section, we compare single grammars and their products that are trained in the standard way with gold WSJ training data, as well as the three self-training scenarios discussed in Section 3. We 15 4.1 Regular Training We begin by training ten latent variable models initialized with different random seeds using the gold WSJ training set. Results are presented in Table 2. The best F score attained by the individual SM6 grammars on the development set is 90.8, with an average score of 90.5. The product of grammars achieves a significan"
D10-1002,D09-1161,0,0.175583,"resulting single self-trained grammars. Second, the diversity of the individual grammars controls the gains that can be obtained by combining multiple grammars into a product model. Our most accurate single grammar achieves an F score of 91.6 on the WSJ test set, rivaling discriminative reranking approaches (Charniak and Johnson, 2005) and products of latent variable grammars (Petrov, 2010), despite being a single generative PCFG. Our most accurate product model achieves an F score of 92.5 without the use of discriminative reranking and comes close to the best known numbers on this test set (Zhang et al., 2009). In future work, we plan to investigate additional methods for increasing the diversity of our selftrained models. One possibility would be to utilize more unlabeled data or to identify additional ways to bias the models. It would also be interesting to determine whether further increasing the accuracy of the model used for automatically labeling the unlabeled data can enhance performance even more. A simple but computationally expensive way to do this would be to parse the data with an SM7 product model. Finally, for this work, we always used products of 10 grammars, but we sometimes observe"
D10-1080,bies-etal-2006-linguistic,1,0.839346,", all the models have high accuracy on newswire data, but the Stanford bidirectional tagger significantly outperforms the other models with the exception of the HMM-LA-Bidir model on this task.1 1 Statistically significant improvements are calculated using the sign test (p &lt; 0.05). 823 Accuracy 96.58 97.05 97.16 97.28 97.07 96.81 Table 2: Tagging accuracy on WSJ 3 Experimental Setup In the rest of this paper, we evaluate the tagging models described in Section 2 on conversational speech. We chose to utilize the Penn Switchboard (Godfrey et al., 1992) and Fisher treebanks (Harper et al., 2005; Bies et al., 2006) because they provide gold standard tags for conversational speech and we have access to corresponding automatically generated ToBI break indexes provided by (Dreyer and Shafran, 2007; Harper et al., 2005)2 . We utilized the Fisher dev1 and dev2 sets containing 16,519 sentences (112,717 words) as the primary training data and the entire Penn Switchboard treebank containing 110,504 sentences (837,863 words) as an additional training source3 . The treebanks were preprocessed as follows: the tags of auxiliary verbs were replaced with the AUX tag, empty nodes 2 A small fraction of words in the Swi"
D10-1080,D09-1116,1,0.848465,"that misalignments between automatic prosodic breaks and true phrase boundaries have on the model. This paper investigates methods for using stateof-the-art taggers on conversational speech transcriptions and the effect that prosody has on tagging accuracy. Improving POS tagging performance of speech transcriptions has implications for improving downstream applications that rely on accurate POS tags, including sentence boundary detection (Liu et al., 2005), automatic punctuation (Hillard et al., 2006), information extraction from speech, parsing, and syntactic language modeling (Heeman, 1999; Filimonov and Harper, 2009). While there have been several attempts to integrate prosodic information to improve parse accuracy of speech transcripts, to the best of our knowledge there has been little work on using this type of information for POS tagging. Furthermore, most of the parsing work has involved generative models and rescoring/reranking of hypotheses from the generative models. In this work, we will analyze several factors related to effective POS tagging of conversational speech: • discriminative versus generative POS tagging models (Section 2) • prosodic features in the form of simplified ToBI break indexe"
D10-1080,N10-1060,0,0.0185146,"oves tagging performance of models on conversation sides, but has much less impact on smaller segments. We conclude that, although the use of break indexes can indeed significantly improve performance over baseline models without them on conversation sides, tagging accuracy improves more by using smaller segments, for which the impact of the break indexes is marginal. 1 Introduction Natural language processing technologies, such as parsing and tagging, often require reconfiguration when they are applied to challenging domains that differ significantly from newswire, e.g., blogs, twitter text (Foster, 2010), or speech. In contrast to text, conversational speech represents a significant challenge because the transcripts are not segmented into sentences. Furthermore, the transcripts are often disfluent and lack punctuation and case information. On the other hand, speech provides additional information, beyond simply the sequence of words, which could be exploited to more accurately assign each word in the transcript a part-of-speech (POS) tag. One potentially beneficial type of information is prosody (Cutler et al., 1997). Prosody provides cues for lexical disambiguation, sentence segmentation and"
D10-1080,N04-1011,0,0.0317301,"t al., 2001; Taylor and Black, 1998), as well as in speech recognition (Gallwitz et al., 2002; Hasegawa-Johnson et al., 2005; Ostendorf et al., 2003). Additionally, prosodic features such as pause length, duration of words and phones, pitch contours, energy contours, and their normalized values have been used for speech processing tasks like sentence boundary detection (Liu et al., 2005). Linguistic encoding schemes like ToBI (Silverman et al., 1992) have also been used for sentence boundary detection (Roark et al., 2006; Harper et al., 2005), as well as for parsing (Dreyer and Shafran, 2007; Gregory et al., 2004; Kahn et al., 2005). In the ToBI scheme, aspects of prosody such as tone, prominence, and degree of juncture between words are represented symbolically. For instance, Dreyer and Shafran (2007) use three classes of automatically detected ToBI break indexes, indicating major intonational breaks with a 4, hesitation with a p, and all other breaks with a 1. Recently, Huang and Harper (2010) found that they could effectively integrate prosodic informa821 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 821–831, c MIT, Massachusetts, USA, 9-11 October 20"
D10-1080,W99-0617,0,0.0383367,"gative effect that misalignments between automatic prosodic breaks and true phrase boundaries have on the model. This paper investigates methods for using stateof-the-art taggers on conversational speech transcriptions and the effect that prosody has on tagging accuracy. Improving POS tagging performance of speech transcriptions has implications for improving downstream applications that rely on accurate POS tags, including sentence boundary detection (Liu et al., 2005), automatic punctuation (Hillard et al., 2006), information extraction from speech, parsing, and syntactic language modeling (Heeman, 1999; Filimonov and Harper, 2009). While there have been several attempts to integrate prosodic information to improve parse accuracy of speech transcripts, to the best of our knowledge there has been little work on using this type of information for POS tagging. Furthermore, most of the parsing work has involved generative models and rescoring/reranking of hypotheses from the generative models. In this work, we will analyze several factors related to effective POS tagging of conversational speech: • discriminative versus generative POS tagging models (Section 2) • prosodic features in the form of"
D10-1080,N10-1005,1,0.785102,"05). Linguistic encoding schemes like ToBI (Silverman et al., 1992) have also been used for sentence boundary detection (Roark et al., 2006; Harper et al., 2005), as well as for parsing (Dreyer and Shafran, 2007; Gregory et al., 2004; Kahn et al., 2005). In the ToBI scheme, aspects of prosody such as tone, prominence, and degree of juncture between words are represented symbolically. For instance, Dreyer and Shafran (2007) use three classes of automatically detected ToBI break indexes, indicating major intonational breaks with a 4, hesitation with a p, and all other breaks with a 1. Recently, Huang and Harper (2010) found that they could effectively integrate prosodic informa821 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 821–831, c MIT, Massachusetts, USA, 9-11 October 2010. 2010 Association for Computational Linguistics tion in the form of this simplified three class ToBI encoding when parsing spontaneous speech by using a prosodically enriched PCFG model with latent annotations (PCFG-LA) (Matsuzaki et al., 2005; Petrov and Klein, 2007) to rescore n-best parses produced by a baseline PCFG-LA model without prosodic enrichment. However, the prosodically e"
D10-1080,N09-2054,1,0.867501,"ntation (Section 5) 2 Models In order to fully evaluate the difficulties inherent in tagging conversational speech, as well as the possible benefits of prosodic information, we conducted experiments with six different POS tagging models. The models can be broadly separated into two classes: generative and discriminative. As the first of our generative models, we used a Hidden Markov 822 Model (HMM) trigram tagger (Thede and Harper, 1999), which serves to establish a baseline and to gauge the difficulty of the task at hand. Our second model, HMM-LA, was the latent variable bigram HMM tagger of Huang et al. (2009), which achieved state-of-the-art tagging performance by introducing latent tags to weaken the stringent Markov independence assumptions that generally hinder tagging performance in generative models. For the third model, we implemented a bidirectional variant of the HMM-LA (HMM-LA-Bidir) that combines evidence from two HMM-LA taggers, one trained left-to-right and the other right-toleft. For decoding, we use a product model (Petrov, 2010). The intuition is that the context information from the left and the right of the current position is complementary for predicting the current tag and thus,"
D10-1080,H05-1030,0,0.163045,"d Black, 1998), as well as in speech recognition (Gallwitz et al., 2002; Hasegawa-Johnson et al., 2005; Ostendorf et al., 2003). Additionally, prosodic features such as pause length, duration of words and phones, pitch contours, energy contours, and their normalized values have been used for speech processing tasks like sentence boundary detection (Liu et al., 2005). Linguistic encoding schemes like ToBI (Silverman et al., 1992) have also been used for sentence boundary detection (Roark et al., 2006; Harper et al., 2005), as well as for parsing (Dreyer and Shafran, 2007; Gregory et al., 2004; Kahn et al., 2005). In the ToBI scheme, aspects of prosody such as tone, prominence, and degree of juncture between words are represented symbolically. For instance, Dreyer and Shafran (2007) use three classes of automatically detected ToBI break indexes, indicating major intonational breaks with a 4, hesitation with a p, and all other breaks with a 1. Recently, Huang and Harper (2010) found that they could effectively integrate prosodic informa821 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 821–831, c MIT, Massachusetts, USA, 9-11 October 2010. 2010 Association"
D10-1080,P05-1056,1,0.928045,"xical disambiguation, sentence segmentation and classification, phrase structure and attachment, discourse structure, speaker affect, etc. Prosody has been found to play an important role in speech synthesis systems (Batliner et al., 2001; Taylor and Black, 1998), as well as in speech recognition (Gallwitz et al., 2002; Hasegawa-Johnson et al., 2005; Ostendorf et al., 2003). Additionally, prosodic features such as pause length, duration of words and phones, pitch contours, energy contours, and their normalized values have been used for speech processing tasks like sentence boundary detection (Liu et al., 2005). Linguistic encoding schemes like ToBI (Silverman et al., 1992) have also been used for sentence boundary detection (Roark et al., 2006; Harper et al., 2005), as well as for parsing (Dreyer and Shafran, 2007; Gregory et al., 2004; Kahn et al., 2005). In the ToBI scheme, aspects of prosody such as tone, prominence, and degree of juncture between words are represented symbolically. For instance, Dreyer and Shafran (2007) use three classes of automatically detected ToBI break indexes, indicating major intonational breaks with a 4, hesitation with a p, and all other breaks with a 1. Recently, Hua"
D10-1080,P05-1010,0,0.038619,"ally detected ToBI break indexes, indicating major intonational breaks with a 4, hesitation with a p, and all other breaks with a 1. Recently, Huang and Harper (2010) found that they could effectively integrate prosodic informa821 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 821–831, c MIT, Massachusetts, USA, 9-11 October 2010. 2010 Association for Computational Linguistics tion in the form of this simplified three class ToBI encoding when parsing spontaneous speech by using a prosodically enriched PCFG model with latent annotations (PCFG-LA) (Matsuzaki et al., 2005; Petrov and Klein, 2007) to rescore n-best parses produced by a baseline PCFG-LA model without prosodic enrichment. However, the prosodically enriched models by themselves did not perform significantly better than the baseline PCFG-LA model without enrichment, due to the negative effect that misalignments between automatic prosodic breaks and true phrase boundaries have on the model. This paper investigates methods for using stateof-the-art taggers on conversational speech transcriptions and the effect that prosody has on tagging accuracy. Improving POS tagging performance of speech transcrip"
D10-1080,N07-1051,0,0.03165,"indexes, indicating major intonational breaks with a 4, hesitation with a p, and all other breaks with a 1. Recently, Huang and Harper (2010) found that they could effectively integrate prosodic informa821 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 821–831, c MIT, Massachusetts, USA, 9-11 October 2010. 2010 Association for Computational Linguistics tion in the form of this simplified three class ToBI encoding when parsing spontaneous speech by using a prosodically enriched PCFG model with latent annotations (PCFG-LA) (Matsuzaki et al., 2005; Petrov and Klein, 2007) to rescore n-best parses produced by a baseline PCFG-LA model without prosodic enrichment. However, the prosodically enriched models by themselves did not perform significantly better than the baseline PCFG-LA model without enrichment, due to the negative effect that misalignments between automatic prosodic breaks and true phrase boundaries have on the model. This paper investigates methods for using stateof-the-art taggers on conversational speech transcriptions and the effect that prosody has on tagging accuracy. Improving POS tagging performance of speech transcriptions has implications fo"
D10-1080,N10-1003,0,0.0146931,"serves to establish a baseline and to gauge the difficulty of the task at hand. Our second model, HMM-LA, was the latent variable bigram HMM tagger of Huang et al. (2009), which achieved state-of-the-art tagging performance by introducing latent tags to weaken the stringent Markov independence assumptions that generally hinder tagging performance in generative models. For the third model, we implemented a bidirectional variant of the HMM-LA (HMM-LA-Bidir) that combines evidence from two HMM-LA taggers, one trained left-to-right and the other right-toleft. For decoding, we use a product model (Petrov, 2010). The intuition is that the context information from the left and the right of the current position is complementary for predicting the current tag and thus, the combination should serve to improve performance over the HMM-LA tagger. Since prior work on parsing speech with prosody has relied on generative models, it was necessary to modify equations of the model in order to incorporate the prosodic information, and then perform rescoring in order to achieve gains. However, it is far simpler to directly integrate prosody as features into the model by using a discriminative approach. Hence, we a"
D10-1080,P99-1023,1,0.636861,"sational speech: • discriminative versus generative POS tagging models (Section 2) • prosodic features in the form of simplified ToBI break indexes (Section 4) • type of speech segmentation (Section 5) 2 Models In order to fully evaluate the difficulties inherent in tagging conversational speech, as well as the possible benefits of prosodic information, we conducted experiments with six different POS tagging models. The models can be broadly separated into two classes: generative and discriminative. As the first of our generative models, we used a Hidden Markov 822 Model (HMM) trigram tagger (Thede and Harper, 1999), which serves to establish a baseline and to gauge the difficulty of the task at hand. Our second model, HMM-LA, was the latent variable bigram HMM tagger of Huang et al. (2009), which achieved state-of-the-art tagging performance by introducing latent tags to weaken the stringent Markov independence assumptions that generally hinder tagging performance in generative models. For the third model, we implemented a bidirectional variant of the HMM-LA (HMM-LA-Bidir) that combines evidence from two HMM-LA taggers, one trained left-to-right and the other right-toleft. For decoding, we use a product"
D10-1080,W00-1308,0,0.0731247,"er 2nd 1st 1st √ √ 2nd 2nd 2nd Table 1: Description of tagging models The objective we need to maximize then becomes : Model Trigram HMM HMM-LA HMM-LA-Bidir Stanford Bidir Stanford Left5 CRF   X X kλk2  L= λj Fj (tn , wn ) − log Zλ (xn ) − 2σ 2 n j where we use a spherical Gaussian prior to prevent overfitting of the model (Chen and Rosenfeld, 1999) and the wide-spread quasi-Newtonian L-BFGS method to optimize the model parameters (Liu and Nocedal, 1989). Decoding is performed with the Viterbi algorithm. We also evaluate state-of-the-art Maximum Entropy taggers: the Stanford Left5 tagger (Toutanova and Manning, 2000) and the Stanford bidirectional tagger (Toutanova et al., 2003), with the former using only left context and the latter bidirectional dependencies. Table 1 summarizes the major differences between the models along several dimensions: (1) generative versus discriminative, (2) directionality of decoding, (3) the presence or absence of latent annotations, (4) the availability of n-best extraction, and (5) the model order. In order to assess the quality of our models, we evaluate them on the section 23 test set of the standard newswire WSJ tagging task after training all models on sections 0-22. R"
D10-1080,N03-1033,0,0.0266465,"s The objective we need to maximize then becomes : Model Trigram HMM HMM-LA HMM-LA-Bidir Stanford Bidir Stanford Left5 CRF   X X kλk2  L= λj Fj (tn , wn ) − log Zλ (xn ) − 2σ 2 n j where we use a spherical Gaussian prior to prevent overfitting of the model (Chen and Rosenfeld, 1999) and the wide-spread quasi-Newtonian L-BFGS method to optimize the model parameters (Liu and Nocedal, 1989). Decoding is performed with the Viterbi algorithm. We also evaluate state-of-the-art Maximum Entropy taggers: the Stanford Left5 tagger (Toutanova and Manning, 2000) and the Stanford bidirectional tagger (Toutanova et al., 2003), with the former using only left context and the latter bidirectional dependencies. Table 1 summarizes the major differences between the models along several dimensions: (1) generative versus discriminative, (2) directionality of decoding, (3) the presence or absence of latent annotations, (4) the availability of n-best extraction, and (5) the model order. In order to assess the quality of our models, we evaluate them on the section 23 test set of the standard newswire WSJ tagging task after training all models on sections 0-22. Results appear in Table 2. Clearly, all the models have high acc"
D11-1064,N03-2002,0,0.0133078,"gram models for decision tree LMs and briefly describe a generalized interpolation for such models. The generalized interpolation method allows the addition of any number of trees to the model, and thus raises the question: what is the best way to create diverse decision trees so that their combination results in a stronger model, while at the same time keeping the total number of trees in the model relatively low for computational practicality. In Section 4, we explore and evaluate a variety 3 For example, morphological features can be very helpful for modeling highly inflectional languages (Bilmes and Kirchhoff, 2003). 692 of methods for creating different trees. To support our findings, we evaluate several of the models on an ASR rescoring task in Section 5. Finally, we discuss our findings in Section 6. 2 Joint Syntactic Decision Tree LM A decision tree provides us with a clustering funci−1 i−1 tion Φ(wi−n+1 ti−n+1 ) → {Φ1 , . . . , ΦN }, where N is the number of clusters, and clusters Φk are disjoint subsets of the context space. The probability estimation for a joint decision tree model is approximated as follows: i−1 i−1 i−1 p(wi ti |wi−n+1 ti−1 i−n+1 ) ≈ p(wi ti |Φ(wi−n+1 ti−n+1 )) (4) In the remaind"
D11-1064,P96-1041,0,0.0351341,"l is a simple Markov chain lacking any notion of syntax. It is widely accepted that languages do have some structure. Moreover, it has been shown that incorporating syntax into a language model can improve its performance (Bangalore, 1996; Heeman, 1998; Chelba and Jelinek, 2000; Filimonov and Harper, 2009). A straightforward way of incorporating syntax into a language model is by assigning a tag to each word and modeling them jointly; then to obtain the proba1 O(|V |n−1 ) in n-gram model with typical order n = 3 . . . 5, and a vocabulary size of |V |= 104 . . . 106 . 2 We refer the reader to (Chen and Goodman, 1996) for a survey of the discounting methods for n-gram models. 691 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 691–699, c Edinburgh, Scotland, UK, July 27–31, 2011. 2011 Association for Computational Linguistics bility of a word sequence, the tags must be marginalized out: p(w1m ) = X t1 ...tm p(w1m tm 1 )= m X Y t1 ...tm i=1 p(wi ti |w1i−1 ti−1 1 ) An independence assumption similar to Eq. 1 can be made: i−1 i−1 p(wi ti |w1i−1 ti−1 1 ) ≈ p(wi ti |wi−n+1 ti−n+1 ) (3) A primary goal of our research is to build strong syntactic language models and p"
D11-1064,D09-1116,1,0.86304,"wm (denoted as w1m ), assuming that higher probability corresponds to more fluent hypotheses. LMs are often represented in the following generative form: p(w1m ) = m Y i=1 (1) p(wi |w1i−1 ) i−1 = ρ(wi |wi−n+1 )+ i−1 γ(wi−n+1 ) · (2) i−1 p˜(wi |wi−n+2 ) where ρ is a discounted probability2 . Note that this type of model is a simple Markov chain lacking any notion of syntax. It is widely accepted that languages do have some structure. Moreover, it has been shown that incorporating syntax into a language model can improve its performance (Bangalore, 1996; Heeman, 1998; Chelba and Jelinek, 2000; Filimonov and Harper, 2009). A straightforward way of incorporating syntax into a language model is by assigning a tag to each word and modeling them jointly; then to obtain the proba1 O(|V |n−1 ) in n-gram model with typical order n = 3 . . . 5, and a vocabulary size of |V |= 104 . . . 106 . 2 We refer the reader to (Chen and Goodman, 1996) for a survey of the discounting methods for n-gram models. 691 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 691–699, c Edinburgh, Scotland, UK, July 27–31, 2011. 2011 Association for Computational Linguistics bility of a word sequence"
D11-1064,P11-2109,1,0.85087,"imilar to JelinekMercer smoothing for n-gram models (Jelinek and Mercer, 1980) has been applied to decision tree models: i−1 p˜n (wi |wi−n+1 ) = λn (φn ) · pn (wi |φn ) + (8) i−1 (1 − λn (φn )) · p˜n−1 (wi |wi−n+2 ) i−1 Φn (wi−n+1 ), where φn ≡ and λn (φn ) ∈ [0, 1] are assigned to each cluster and are optimized on a heldout set using EM. pn (wi |φn ) is the probability distribution at the cluster φn in the tree of order n. This interpolation method is particularly useful as, unlike count-based discounting methods (e.g., KneserNey), it can be applied to already smoothed distributions pn . In (Filimonov and Harper, 2011), we observed that because of the violation of Property 7 in decision tree models, the interpolation method of Eq. 8 is not appropriate for such models. Instead we proposed the following generalized form of linear interpolation: i−1 p˜n (wi |wi−n+1 )= Pn λm (φm ) · pm (wi |φm ) m=1P n m=1 λm (φm ) (9) Note that the recursive interpolation of Eq. 8 can be represented in this form with the additional conP straint nm=1 λm (φm ) = 1, which is not required in the generalized interpolation of Eq. 9; thus, the generalized interpolation, albeit having the same number of parameters, has more degrees of"
D11-1064,W98-1121,0,0.176183,"rate the word sequence w1 , w2 , . . . , wm (denoted as w1m ), assuming that higher probability corresponds to more fluent hypotheses. LMs are often represented in the following generative form: p(w1m ) = m Y i=1 (1) p(wi |w1i−1 ) i−1 = ρ(wi |wi−n+1 )+ i−1 γ(wi−n+1 ) · (2) i−1 p˜(wi |wi−n+2 ) where ρ is a discounted probability2 . Note that this type of model is a simple Markov chain lacking any notion of syntax. It is widely accepted that languages do have some structure. Moreover, it has been shown that incorporating syntax into a language model can improve its performance (Bangalore, 1996; Heeman, 1998; Chelba and Jelinek, 2000; Filimonov and Harper, 2009). A straightforward way of incorporating syntax into a language model is by assigning a tag to each word and modeling them jointly; then to obtain the proba1 O(|V |n−1 ) in n-gram model with typical order n = 3 . . . 5, and a vocabulary size of |V |= 104 . . . 106 . 2 We refer the reader to (Chen and Goodman, 1996) for a survey of the discounting methods for n-gram models. 691 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 691–699, c Edinburgh, Scotland, UK, July 27–31, 2011. 2011 Association"
D11-1064,D09-1087,1,0.766874,"4-96 from LDC2008T13. The text was converted into speech-like form, namely numbers and abbreviations were verbalized, text was downcased, punctuation was removed, and contractions and possessives were joined with the previous word (i.e., they ’ll becomes they’ll). For the syntactic modeling, we used tags comprised of the POS tags of the word and it’s head. Parsing of the text for tag extraction occurred after verbalization of numbers and abbreviations but 4 pm . Note that (Xu, 2005) used lower order models to estimate before any further processing; we used a latent variable PCFG parser as in (Huang and Harper, 2009). For reference, we include an n-gram model with modified interpolated KN discounting. All models use the same vocabulary of approximately 50k words. Perplexity numbers reported in Tables 1, 2, 3, and 4 are computed on WSJ section 23 (tokenized in the same way)5 . In Table 1, we show results reported in (Filimonov and Harper, 2011), which we use as the baseline for further experiments. We constructed two sets of decision trees (a joint syntactic model and a word-tree model) as described in Section 2. Each set was comprised of a fourgram tree with backoff trigram, bigram, and unigram trees. We"
huang-etal-2006-open,W04-3209,1,\N,Missing
I11-1025,P08-1068,0,0.0455415,"per 80.5 82.7 78.9 81.2 79.7 81.9 Table 5: Final test set accuracies. feature-rich lexical model significantly outperform the standard PCFG-LA grammars of (Petrov and Klein, 2007) for all of the three languages, especially on Chinese (+1.6 F) and Arabic (+2.2 F). Other Features Our model supports any local features that can be extracted from the pair (tx , w), including the language-dependent features studied in (Attia et al., 2010). In addition, features related to word semantics (e.g., using WordNet (Fellbaum, 1998)) or word clusters (e.g., using unsupervised clustering (Brown et al., 1992; Koo et al., 2008; Goyal and Daume, 2011)) might also be beneficial for modeling Pφ (tx |t, w) and/or Pγ (t|w). Features extracted from (t, w) could also be helpful for providing some smoothing effect across the latent tags. Moreover, it might be beneficial to perform feature selection prior to training. We leave this to future work. 5.6 TB CTB6 that end with the character 国 (country) in the Chinese treebank. These names appear in similar contexts and would be expected to favor certain latent tag or tags; however, when training using the wid feature set, this is only true for the frequent names as shown in Fig"
I11-1025,W10-1408,0,0.414855,"Missing"
I11-1025,N10-1083,0,0.0747349,"Missing"
I11-1025,J92-4003,0,0.01643,"Klein (2007) This Paper 80.5 82.7 78.9 81.2 79.7 81.9 Table 5: Final test set accuracies. feature-rich lexical model significantly outperform the standard PCFG-LA grammars of (Petrov and Klein, 2007) for all of the three languages, especially on Chinese (+1.6 F) and Arabic (+2.2 F). Other Features Our model supports any local features that can be extracted from the pair (tx , w), including the language-dependent features studied in (Attia et al., 2010). In addition, features related to word semantics (e.g., using WordNet (Fellbaum, 1998)) or word clusters (e.g., using unsupervised clustering (Brown et al., 1992; Koo et al., 2008; Goyal and Daume, 2011)) might also be beneficial for modeling Pφ (tx |t, w) and/or Pγ (t|w). Features extracted from (t, w) could also be helpful for providing some smoothing effect across the latent tags. Moreover, it might be beneficial to perform feature selection prior to training. We leave this to future work. 5.6 TB CTB6 that end with the character 国 (country) in the Chinese treebank. These names appear in similar contexts and would be expected to favor certain latent tag or tags; however, when training using the wid feature set, this is only true for the frequent nam"
I11-1025,P05-1010,0,0.035064,"into generative models, we propose to use a feature-rich log-linear lexical model to train PCFG-LA grammars that are more robust to rare and OOV words. The proposed lexical model has three advantages: over-fitting is alleviated via regularization, OOV words are modeled using rich features, and lexical features are exploited for grammar induction. Our approach results in significantly more accurate PCFG-LA grammars that are flexible to train for different languages (with test F scores of 90.5, 85.0, and 81.9 on WSJ, CTB6, and ATB, respectively). 1 Introduction The latent variable approach of (Matsuzaki et al., 2005; Petrov et al., 2006) is capable of learning high accuracy context-free grammars directly from a raw treebank, and has achieved state-of-the-art parsing accuracies on multiple languages, outperforming many other parsers that are engineered for performance in a particular language (Petrov, 2009; Green and Manning, 2010). However, the lexical model of PCFG-LA grammars (responsible for emitting words from latent POS tags) is not designed to effectively handle OOV words universally. In fact, hand-crafted rules designed for English OOV words were used in the multi-language study of (Petrov, 2009)"
I11-1025,A00-2018,0,0.201833,"ble to features are discussed and the final test results are use rich features. This is less of an issue when regcompared with the literature. ularization takes effect as it favors common discriminative features to reduce the penalty term. 5.1 Data & Setup The second approach, which was found to We experiment with three languages: English, outperform the EM-based approach in (BergChinese, and Arabic. For English, we used the Kirkpatrick et al., 2010), optimizes on the regWSJ Penn Treebank (Marcus et al., 1999) and ularized log-likelihood (Equation 7) directly by the commonly used data splits (Charniak, 2000). updating both ψ and φ using a gradient descent For Chinese, we used the Penn Chinese Treebank approach. In order to convert this to an un5 6.0 (CTB6) (Xue et al., 2005) and the preparation constrained optimization problem , we set each steps and data splits in (Huang and Harper, 2009). phrasal rule expansion probability ψi as the output 0 For Arabic, we used the Penn Arabic Treebank of a log-linear model, i.e., ψi = exp(ψi )/Z with (ATB) (Maamouri et al., 2009) and the preparaZ being the normalization factor, and treat ψ 0 as tion steps6 and data splits in (Green and Manning, the parameter f"
I11-1025,N07-1051,0,0.0798346,"to where it is most needed, Petrov et al. (2006) developed a simple split-and-merge procedure. In every split-merge (SM) round, each latent category is first split into two, and the model is reestimated using several rounds of EM iterations. A likelihood criterion is then used to merge back the least useful splits. The result is that categories, such as NP and VB, that occur frequently in different syntactic environments, are split more heavily than categories such as UH (interjection). This approach also creates a hierarchy of latent categories that enables efficient coarse-to-fine parsing (Petrov and Klein, 2007). We next discuss two important issues related to the lexical model of PCFG-LA grammars: overfitting and OOV word handling. S−1 S NP which results in the following update formula for lexical rule probability θtx →w = Pθ (w|tx ): et ,w Pθ (w|tx ) = P x (3) w0 etx ,w0 NP . NP−2 . PRP−3 VBD−5 She DT NN the noise .−1 VP−4 heard (a) . NP−6 DT−2 NN−6 the noise (b) Figure 1: (a) treebank tree (b) derivation tree The objective of PCFG-LA training is to induce a grammar with latent variables that maximizes the probability of the training trees. Given a PCFG-LA grammar with model parameter θ, R denotes"
I11-1025,E06-1047,0,0.084853,"cent For Chinese, we used the Penn Chinese Treebank approach. In order to convert this to an un5 6.0 (CTB6) (Xue et al., 2005) and the preparation constrained optimization problem , we set each steps and data splits in (Huang and Harper, 2009). phrasal rule expansion probability ψi as the output 0 For Arabic, we used the Penn Arabic Treebank of a log-linear model, i.e., ψi = exp(ψi )/Z with (ATB) (Maamouri et al., 2009) and the preparaZ being the normalization factor, and treat ψ 0 as tion steps6 and data splits in (Green and Manning, the parameter for the phrasal rules to be optimized. 2010; Chiang et al., 2006). Table 1 provides gross The gradient of L(θ) with respect to φ turns out to statistics for each treebank. As we can see, CTB6 be the same as in the first approach (Salakhutdinov and ATB both have a higher OOV rate than WSJ, et al., 2003). The gradient of L(θ) with respect to ∆tx ,w (φ) = f (tx , w) − x0 5 The elements of ψ are constrained to form proper probability distributions. 6 Except that clitic marks were removed, which results in about 0.3 degradation in F score (p.c.). 223 5.3 and hence have greater need for effective OOV handling. Statistics Train Dev Test English (WSJ) #sents #token"
I11-1025,D08-1091,0,0.234171,"or Latent Variable PCFG Grammars Zhongqiang Huang and Mary Harper Department of Computer Science University of Maryland, College Park {zqhuang,mharper}@umd.edu Abstract Huang and Harper (2009) and Attia et al. (2010) studied the impact of rare and OOV word handling for parsing with PCFG-LA grammars, especially for non-English languages. They both found that language-specific handling of OOV words significantly improves parsing performance. However, hand tailoring of the language-specific module with expert knowledge may produce suboptimal results, and would not be applicable to new languages. Petrov and Klein (2008) presented a discriminatively trained PCFG-LA model that makes use of rich morphological features for handling OOV words and obtained improved performance on some languages; however, this method was considerably less accurate than its strong generative counterpart on English WSJ. Berg-Kirkpatrick et al. (2010) demonstrated that each generation step of a generative process can be modeled as a locally normalized log-linear model so that rich features can be incorporated for learning unsupervised models, e.g., POS induction. Inspired by their work, we propose a log-linear lexical model for genera"
I11-1025,P08-1109,0,0.0656919,"Missing"
I11-1025,P06-1055,0,0.291799,"we propose to use a feature-rich log-linear lexical model to train PCFG-LA grammars that are more robust to rare and OOV words. The proposed lexical model has three advantages: over-fitting is alleviated via regularization, OOV words are modeled using rich features, and lexical features are exploited for grammar induction. Our approach results in significantly more accurate PCFG-LA grammars that are flexible to train for different languages (with test F scores of 90.5, 85.0, and 81.9 on WSJ, CTB6, and ATB, respectively). 1 Introduction The latent variable approach of (Matsuzaki et al., 2005; Petrov et al., 2006) is capable of learning high accuracy context-free grammars directly from a raw treebank, and has achieved state-of-the-art parsing accuracies on multiple languages, outperforming many other parsers that are engineered for performance in a particular language (Petrov, 2009; Green and Manning, 2010). However, the lexical model of PCFG-LA grammars (responsible for emitting words from latent POS tags) is not designed to effectively handle OOV words universally. In fact, hand-crafted rules designed for English OOV words were used in the multi-language study of (Petrov, 2009) for nonEnglish languag"
I11-1025,D11-1023,0,0.0142334,"81.2 79.7 81.9 Table 5: Final test set accuracies. feature-rich lexical model significantly outperform the standard PCFG-LA grammars of (Petrov and Klein, 2007) for all of the three languages, especially on Chinese (+1.6 F) and Arabic (+2.2 F). Other Features Our model supports any local features that can be extracted from the pair (tx , w), including the language-dependent features studied in (Attia et al., 2010). In addition, features related to word semantics (e.g., using WordNet (Fellbaum, 1998)) or word clusters (e.g., using unsupervised clustering (Brown et al., 1992; Koo et al., 2008; Goyal and Daume, 2011)) might also be beneficial for modeling Pφ (tx |t, w) and/or Pγ (t|w). Features extracted from (t, w) could also be helpful for providing some smoothing effect across the latent tags. Moreover, it might be beneficial to perform feature selection prior to training. We leave this to future work. 5.6 TB CTB6 that end with the character 国 (country) in the Chinese treebank. These names appear in similar contexts and would be expected to favor certain latent tag or tags; however, when training using the wid feature set, this is only true for the frequent names as shown in Figure 3. For the rare name"
I11-1025,N10-1003,0,0.0121782,"res as discussed in Subsection 5.5. Table 3 lists the templates we used to extract predicates on words. For the log-linear OOV model, we use the full feature set, i.e., (t, pred) pairs extracted using all of the predicates. For the log-linear latent lexical model, we experiment with two feature sets: 1) the wid feature set containing only (tx , wid) pairs, which are the same as those used in the standard PCFG-LA grammars, 2) the full feature set using all of the predicates. Table 1: Gross Statistics of the treebanks. Due to the variability (caused by random initialization) among the grammars (Petrov, 2010), we train 10 grammars with different seeds in each experiment and report their average F score on the development set. The best grammar selected using the development set is used for evaluation on the test set. 5.2 We first study the effect of rare word smoothing and OOV handling on the standard PCFGLA grammars using our reimplementation of the Berkeley parser. The no+simple row in Table 2 represents the baseline, for which the grammars are trained without rare word smoothing described in Subsection 2.1 and OOV words are handled by the simple method described in Subsection 2.2. Each language-"
I11-1025,C10-1045,0,0.0377778,"ploited for grammar induction. Our approach results in significantly more accurate PCFG-LA grammars that are flexible to train for different languages (with test F scores of 90.5, 85.0, and 81.9 on WSJ, CTB6, and ATB, respectively). 1 Introduction The latent variable approach of (Matsuzaki et al., 2005; Petrov et al., 2006) is capable of learning high accuracy context-free grammars directly from a raw treebank, and has achieved state-of-the-art parsing accuracies on multiple languages, outperforming many other parsers that are engineered for performance in a particular language (Petrov, 2009; Green and Manning, 2010). However, the lexical model of PCFG-LA grammars (responsible for emitting words from latent POS tags) is not designed to effectively handle OOV words universally. In fact, hand-crafted rules designed for English OOV words were used in the multi-language study of (Petrov, 2009) for nonEnglish languages, leaving room for further improvement for each of the languages studied. 2 PCFG-LA Grammar PCFG grammars with latent annotations (Matsuzaki et al., 2005; Petrov et al., 2006) augment 219 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 219–227, c Chiang"
I11-1025,D09-1087,1,0.937443,"ining, leading to over-fitting. In order to counteract this behavior, Petrov et al. (2006) introduced a linear smoothing method to smooth lexical emission probabilities: X ¯ = 1 P Pθ (w|tx ) |t |x ¯ + (1 − )Pθ (w|tx ) Pθ (w|tx ) ← P Pθ (r) D∈D(T ) r∈R(D) An EM-algorithm is used to optimize θ based on the training likelihood. The E-step computes the expected count er of rule r over the training set T under the current model parameter θ0 : er ← X X T ∈T r0 ∈R(T ) δ(r0 , r)Pθ0 (r0 |T ) A similar smoothing method was used for phrasal rules. While the above method has been found to be effective, Huang and Harper (2009) observed that rare words suffer more from over-fitting than frequent words and suggested tying rare words together when estimating their emission probabilities. Using their approach, all words with a frequency less than a threshold τ are mapped to symbol rare1 , and their emission probability Pθ (w|tx ) is set in proportion to their co-occurrences with the surface POS tag: ct,w Pθ (rare|tx ) Pθ (w|tx ) = P w0 :c 0 <τ ct,w0 (1) where δ(·, ·) is an indicator function that returns 1 if the two operands are identical and 0 otherwise, and Pθ0 (r0 |T ) is the posterior probability of having (latent"
I11-1025,D10-1002,1,0.868903,"Missing"
J92-4002,P89-1004,0,0.0369065,"Missing"
J92-4002,T87-1035,0,0.0602802,"Missing"
J92-4002,P83-1009,0,0.0215586,"fier scope ambiguity and how this representation conforms to our computational constraints. Then, we introduce our LF representations for pronouns, singular definite NPs, and singular indefinite NPs, considering how to update each representation when new information becomes available (though not how to determine that information). We also discuss an implementation that uses our representations and compare our approach with related work. 2. Quantifier Scope Ambiguity in Logical Form Quantifier scope ambiguity has been handled by some researchers by using an intermediate scope-neutral LF (e.g., Hobbs 1983; Schubert and Pelletier 1984; Allen 1987) for the initial representation of sentences. Hence, sentences like Someone loves everyone are initially represented without committing to one particular meaning, as shown in Example 1. Example 1 Someone loves everyone. Possible Meanings: 1. ~x VY (love x y) 2. Vy 3x (love x y) Scope-neutral Form: (love [3x x] [Vy y]) Initially, in scope-neutral LF, the quantifiers are stored in the predicate argumentstructure with no scoping preference indicated, hence the representation does not commit to a specific meaning for the sentence; it is simply a compact wa"
J92-4002,P88-1010,0,0.02065,"68 contains no free variables and is compatible in voice to the elided VP in 67. Hence, the program replaces the elided VP with the trigger VP as shown in 69, and the meaning of the elided sentence is determined. Example 69 Sentence: George did too. Meaning: George saw George&apos;s mother&apos;s picture. ((def7) ] (name (def7) George)), A(y)(see (subject y) (object ((def4 y) I (and (picture (def4 y)) (possess ((def3 y) ] (and (mother (def3 y)) (possess y (def3 y)))) (def4 y)))))) 7. Related Work Other researchers have developed an intermediate representation for a sentence from syntactic information (Pollack and Pereira 1988; Alshawi and van Eijck 1989). These approaches agree that in order to determine the meaning of a sentence, it is useful to build a partial meaning that is augmented once contextual information becomes available. These approaches, however, use a different scheme for indicating the final meaning of a sentence and do not handle VPE. In the rest of this section, we review past representations of pronouns, definite NPs, and indefinite NPs. We emphasize VPE research because it considers not only the representation of sentences in general, but also the representation of trigger sentences. Each appro"
J92-4002,J82-1003,0,\N,Missing
J94-4006,J90-3001,0,0.0215555,"approach to m a n a g e the ambiguity of a sentence. The efficiency of this approach can be i m p r o v e d by resolving each indeterminacy in the parse as soon as it arises to prevent backtracking (Briscoe 1987). However, this requires that e n o u g h information be available at that point in the parse to select a m o n g the alternatives. In m a n y cases, this requirement cannot be met; words occurring later in the sentence or possibly in subsequent sentences m a y be n e e d e d to resolve the ambiguity. A slightly different alternative is to work with the highest preference choice only (Alshawi 1990; Briscoe and Carroll 1993). Although this approach is efficient, it provides only the most likely parse (independently of context), not necessarily the correct parse. An alternative scheme for coping with syntactic ambiguity is to change the grammar rules so that they provide a single parse tree for a syntactically ambiguous sentence and then wait for the semantic routines to pinpoint the parse. To illustrate this strategy, consider a c o m m o n w a y to write a rule for an N P with n o u n modifiers: NP --+ DET N1 N1 --+ N O U N N1 --+ N1 N1 This g r a m m a r generates a very large n u m b"
J94-4006,P92-1005,0,0.043912,"Missing"
J94-4006,J93-1002,0,0.0138696,"a n a g e the ambiguity of a sentence. The efficiency of this approach can be i m p r o v e d by resolving each indeterminacy in the parse as soon as it arises to prevent backtracking (Briscoe 1987). However, this requires that e n o u g h information be available at that point in the parse to select a m o n g the alternatives. In m a n y cases, this requirement cannot be met; words occurring later in the sentence or possibly in subsequent sentences m a y be n e e d e d to resolve the ambiguity. A slightly different alternative is to work with the highest preference choice only (Alshawi 1990; Briscoe and Carroll 1993). Although this approach is efficient, it provides only the most likely parse (independently of context), not necessarily the correct parse. An alternative scheme for coping with syntactic ambiguity is to change the grammar rules so that they provide a single parse tree for a syntactically ambiguous sentence and then wait for the semantic routines to pinpoint the parse. To illustrate this strategy, consider a c o m m o n w a y to write a rule for an N P with n o u n modifiers: NP --+ DET N1 N1 --+ N O U N N1 --+ N1 N1 This g r a m m a r generates a very large n u m b e r of possible structures"
J94-4006,J80-2002,0,0.0213599,"rent interpretive characteristics. In the first, the syntactic underspecification is matched by a semantic underspecification; attaching the PP to the NP or VP does not alter the truth value of the sentence. However, the second sentence has two distinct interpretations depending on the resolution of the attachment ambiguity. Hence, for D-theory to be useful in a system that does semantic interpretation, it would need to be coupled with a semantic interpreter that recognizes the difference between these two examples. The approach taken in this paper is to combine an all-path parsing algorithm (Chester 1980; Earley 1970; Kay 1980; Seo and Simmons 1989; Tomita 1985) with routines for generating logical representations in order to create a shared-packed parse forest annotated with the logical representations for the constituents in the forest (i.e., an annotated shared-packed parse forest). Before discussing the benefits of this approach, we describe the properties of a shared-packed parse forest (Seo and Simmons 1989; Tomita 1985, 1987). A shared-packed parse forest is a data structure that stores all parses of a sentence in a compact form. Consider the packed parse forest produced by an implemen"
J94-4006,J82-3004,0,0.0509541,"Missing"
J94-4006,P81-1022,0,0.0605651,"ive characteristics. In the first, the syntactic underspecification is matched by a semantic underspecification; attaching the PP to the NP or VP does not alter the truth value of the sentence. However, the second sentence has two distinct interpretations depending on the resolution of the attachment ambiguity. Hence, for D-theory to be useful in a system that does semantic interpretation, it would need to be coupled with a semantic interpreter that recognizes the difference between these two examples. The approach taken in this paper is to combine an all-path parsing algorithm (Chester 1980; Earley 1970; Kay 1980; Seo and Simmons 1989; Tomita 1985) with routines for generating logical representations in order to create a shared-packed parse forest annotated with the logical representations for the constituents in the forest (i.e., an annotated shared-packed parse forest). Before discussing the benefits of this approach, we describe the properties of a shared-packed parse forest (Seo and Simmons 1989; Tomita 1985, 1987). A shared-packed parse forest is a data structure that stores all parses of a sentence in a compact form. Consider the packed parse forest produced by an implementation of Tom"
J94-4006,J92-4002,1,0.891521,"Missing"
J94-4006,P83-1020,0,0.0636405,"Missing"
J94-4006,P91-1014,0,0.0135317,"redpacked parse forest annotated with the logical form developed by Harper (1990, 1992). We augmented a Tomita-style LR parser with the necessary routines for constructing the logical form representation. Tomita's parser is a bottom-up LR(k)-based parser that constructs a forest of all possible parses while using a graph-structured stack and breadth-first search to handle non-determinism in the parse. In the next section, we describe three methods for interfacing our logical form routines with Tomita's parser. The conclusions we draw can also be applied to more efficient parsers (Earley 1970; Schabes 1991) that produce other logical representations (e.g., Alshawi and Crouch 1992; Hirst 1987; Weischedel 1989) in more compact forests (Nederhof 1993). 3. Combining Logical Form with Forests: A Case Study Previously, our logical form routines were interfaced with a one-parse-tree-at-a-time, top-down ATN parser (Harper 1990, 1992). This made it relatively easy to create compositional logical form routines and interface them with the parser. These routines were developed to construct and store the logical forms for each major type of constituent. Some routines created logical representations for the b"
J94-4006,J89-1002,0,0.157525,"the first, the syntactic underspecification is matched by a semantic underspecification; attaching the PP to the NP or VP does not alter the truth value of the sentence. However, the second sentence has two distinct interpretations depending on the resolution of the attachment ambiguity. Hence, for D-theory to be useful in a system that does semantic interpretation, it would need to be coupled with a semantic interpreter that recognizes the difference between these two examples. The approach taken in this paper is to combine an all-path parsing algorithm (Chester 1980; Earley 1970; Kay 1980; Seo and Simmons 1989; Tomita 1985) with routines for generating logical representations in order to create a shared-packed parse forest annotated with the logical representations for the constituents in the forest (i.e., an annotated shared-packed parse forest). Before discussing the benefits of this approach, we describe the properties of a shared-packed parse forest (Seo and Simmons 1989; Tomita 1985, 1987). A shared-packed parse forest is a data structure that stores all parses of a sentence in a compact form. Consider the packed parse forest produced by an implementation of Tomita's parser (Tomita 1985) for t"
J94-4006,J87-1004,0,0.0368424,"Missing"
J94-4006,P89-1024,0,0.0129245,"a Tomita-style LR parser with the necessary routines for constructing the logical form representation. Tomita's parser is a bottom-up LR(k)-based parser that constructs a forest of all possible parses while using a graph-structured stack and breadth-first search to handle non-determinism in the parse. In the next section, we describe three methods for interfacing our logical form routines with Tomita's parser. The conclusions we draw can also be applied to more efficient parsers (Earley 1970; Schabes 1991) that produce other logical representations (e.g., Alshawi and Crouch 1992; Hirst 1987; Weischedel 1989) in more compact forests (Nederhof 1993). 3. Combining Logical Form with Forests: A Case Study Previously, our logical form routines were interfaced with a one-parse-tree-at-a-time, top-down ATN parser (Harper 1990, 1992). This made it relatively easy to create compositional logical form routines and interface them with the parser. These routines were developed to construct and store the logical forms for each major type of constituent. Some routines created logical representations for the basic constituents like nouns and verbs, whereas routines for more complex constituents, like VPs, NPs, a"
J94-4006,E93-1036,0,\N,Missing
N09-2054,A00-1031,0,0.369984,"Missing"
N09-2054,W03-0407,0,0.110572,"ned tags to better model the training data. Liang and Klein (2008) analyzed the errors of unsupervised learning using EM and found that both estimation and optimization errors decrease as the amount of unlabeled data increases. In our case, the learning of latent annotations through EM may also benefit from a large set of automatically labeled data to improve tagging performance. Semi-supervised, self-labeled data has been effectively used to train acoustic models for speech recognition (Ma and Schwartz, 2008); however, early investigations of self-training on POS tagging have mixed outcomes. Clark et al. (2003) reported positive results with little labeled training data but negative results when the amount of labeled training data increases. Wang et al. (2007) reported that self-training improves a trigram tagger’s accuracy, but this tagger was trained with only a small amount of in-domain labeled data. In this paper, we will investigate whether the performance of a simple bigram HMM tagger can be improved by introducing latent annotations and whether self-training can further improve its performance. To the best of our knowledge, this is the first attempt to use latent annotations with self-trainin"
N09-2054,D07-1117,1,0.926304,"ore likely to suffer from over-fitting. To handle this problem, we map all words with frequency less than threshold3 λ to symbol unk and for each latent tag accumulate the word tag P statistics of these rare words to cr (ax , unk) = w:c(w)<λ c(ax , w). These statistics are redistributed among the rare words (w : c(w) < λ) to compute their emission probabilities: = cr (ax , unk) · c(a, w)/cr (a, unk) X p(w|ax ) = c(ax , w)/ c(ax , w) c(ax , w) w The impact of this rare word handling method will be investigated in Section 3. A character-based unknown word model, similar to the one described in (Huang et al., 2007), is used to handle unknown Chinese words during tagging. A decoding method similar to the max-rule-product method in (Petrov and Klein, 2007) is used to tag sentences using our model. 3 Experiments The Penn Chinese Treebank 6.0 (CTB6) (Xue et al., 2005) is used as the labeled data in our study. CTB6 2 3 c(·) represents the count of the event. The value of λ is tuned on the development set. contains news articles, which are used as the primary source of labeled data in our experiments, as well as broadcast news transcriptions. Since the news articles were collected during different time period"
N09-2054,P08-1100,0,0.0946829,"Missing"
N09-2054,P05-1010,0,0.144767,"ord w from tag a. A simple HMM tagger is trained by pulling counts from labeled data and normalizing to get the conditional probabilities. It is well know that the independence assumption of a bigram tagger is too strong in many cases. A common practice for weakening the independence assumption is to use a second-order Markov assumption, i.e., a trigram tagger. This is similar to explicitly annotating each POS tag with the preceding tag. Rather than explicit annotation, we could use latent annotations to split the POS tags, similarly to the introduction of latent annotations to PCFG grammars (Matsuzaki et al., 2005; Petrov et al., 2006). For example, the NR tag may be split into NR-1 and NR-2, and correspondingly the POS tag sequence of “Mr./NR Smith/NR saw/VV Ms./NR Smith/NR” could be refined as: “Mr./NR-2 Smith/NR-1 saw/VV-2 Ms./NR-2 Smith/NR-1”. The objective of training a bigram tagger with latent annotations is to find the transition and emission probabilities associated with the latent tags such that the likelihood of the training data is maximized. Unlike training a standard bigram tagger where the POS tags are observed, in the latent case, the latent tags are not observable, and so a variant of"
N09-2054,N07-1051,0,0.0231031,"or each latent tag accumulate the word tag P statistics of these rare words to cr (ax , unk) = w:c(w)<λ c(ax , w). These statistics are redistributed among the rare words (w : c(w) < λ) to compute their emission probabilities: = cr (ax , unk) · c(a, w)/cr (a, unk) X p(w|ax ) = c(ax , w)/ c(ax , w) c(ax , w) w The impact of this rare word handling method will be investigated in Section 3. A character-based unknown word model, similar to the one described in (Huang et al., 2007), is used to handle unknown Chinese words during tagging. A decoding method similar to the max-rule-product method in (Petrov and Klein, 2007) is used to tag sentences using our model. 3 Experiments The Penn Chinese Treebank 6.0 (CTB6) (Xue et al., 2005) is used as the labeled data in our study. CTB6 2 3 c(·) represents the count of the event. The value of λ is tuned on the development set. contains news articles, which are used as the primary source of labeled data in our experiments, as well as broadcast news transcriptions. Since the news articles were collected during different time periods from different sources with a diversity of topics, in order to obtain a representative split of train-testdevelopment sets, we divide them i"
N09-2054,P06-1055,0,0.202896,"le HMM tagger is trained by pulling counts from labeled data and normalizing to get the conditional probabilities. It is well know that the independence assumption of a bigram tagger is too strong in many cases. A common practice for weakening the independence assumption is to use a second-order Markov assumption, i.e., a trigram tagger. This is similar to explicitly annotating each POS tag with the preceding tag. Rather than explicit annotation, we could use latent annotations to split the POS tags, similarly to the introduction of latent annotations to PCFG grammars (Matsuzaki et al., 2005; Petrov et al., 2006). For example, the NR tag may be split into NR-1 and NR-2, and correspondingly the POS tag sequence of “Mr./NR Smith/NR saw/VV Ms./NR Smith/NR” could be refined as: “Mr./NR-2 Smith/NR-1 saw/VV-2 Ms./NR-2 Smith/NR-1”. The objective of training a bigram tagger with latent annotations is to find the transition and emission probabilities associated with the latent tags such that the likelihood of the training data is maximized. Unlike training a standard bigram tagger where the POS tags are observed, in the latent case, the latent tags are not observable, and so a variant of EM algorithm is used t"
N09-2054,P99-1023,1,0.79267,"Missing"
N09-2054,I05-3005,0,0.211356,"Missing"
N09-2054,I05-3027,0,\N,Missing
N09-2067,lamel-etal-2008-question,0,0.0603513,"Missing"
N09-2067,N06-4010,0,0.0664508,"Missing"
N10-1005,bies-etal-2006-linguistic,1,0.770925,"he baseline PCFGLA parser to avoid egregious parse trees. The effectiveness of our rescoring method suggests that the reranking approach of (Kahn et al., 2005) was successful not only because of their prosodic feature design, but also because they restrict the search space for reranking to n-best lists generated by a syntactic model alone. 2 Experimental Setup Due to our goal of investigating the effect of prosodic information on the accuracy of state of the art parsing of conversational speech, we utilize both Penn Switchboard (Godfrey et al., 1992) and Fisher treebanks (Harper et al., 2005; Bies et al., 2006), for which we also had automatically generated break indexes from (Dreyer and Shafran, 2007; Harper et al., 2005)1 . The Fisher treebank is a higher quality parsing resource than Switchboard due to its greater use of audio and refined specifications for sentence segmentation and disfluency markups, and so we utilize its eval set for our parser evaluation; the first 1,020 trees (7,184 words) were used for development and the remaining 3,917 trees (29,173 words) for evaluation. We utilized the Fisher dev1 and dev2 sets containing 16,519 trees (112,717 words) as the main training data source and"
N10-1005,A00-2018,0,0.106381,"ted into the parse trees deterministically for all of the three enrichment methods (B RK I NSERT, B RK P OS, and B RK P HRASE), training a basic PCFG is straightforward; we simply pull the counts of grammar rules, lexical rewrites, or prosodic rewrites from the treebank and normalize them to obtain their probabilities. As is well known in the parsing community, the basic PCFG does not provide state-of-the-art performance due to its strong independence assumptions. We can relax these assumptions by explicitly incorporating more information into the conditional history, as in Charniak’s parser (Charniak, 2000); however, this would require sophisticated engineering efforts to decide what to include in the history and how to smooth probabilities appropriately due to data sparsity. In this paper, we utilize PCFG-LA models (Matsuzaki et al., 2005; Petrov and Klein, 2007) that split each nonterminal into a set of latent tags and learn complex dependencies among the latent tags automatically during training. The resulting model is still a PCFG, but it is probabilistically context free on the latent tags, and the interaction among the latent tags is able to implicitly capture higher order dependencies amo"
N10-1005,N04-1011,0,0.756447,"trast, other researchers use linguistic encoding schemes like ToBI (Silverman et al., 1992), which encodes tones, the degree of juncture between words, and prominence symbolically. For example, a simplified ToBI encoding scheme uses the symbol 4 for major intonational breaks, p for hesitation, and 1 for all other breaks (Dreyer and Shafran, 2007). In the literature, there have been several attempts to integrate prosodic information to improve parse accuracy of speech transcripts. These studies have used either quantized acoustic measurements of prosody or automatically detected break indexes. Gregory et al. (2004) attempted to integrate quantized prosodic features as additional tokens in the same manner that punctuation marks are added into text. Although punctuation marks can significantly improve parse accuracy of newswire text, the quantized prosodic tokens were found harm37 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 37–45, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics ful to parse accuracy when inserted into humangenerated speech transcripts of the Switchboard corpus. The authors hypothesized that th"
N10-1005,D09-1087,1,0.924588,"o words in the middle of a sentence and *4* to words that end a sentence. S S treebank containing 110,504 trees (837,863 words) as an additional training source to evaluate the effect of training data size on parsing performance. The treebank trees are normalized by downcasing all terminal strings and deleting punctuation, empty nodes, and nonterminal-yield unary rules that are not related to edits. We will compare2 three prosodically enriched PCFG models described in the next section, with a baseline PCFG parser. We will also utilize a state of the art PCFG-LA parser (Petrov and Klein, 2007; Huang and Harper, 2009) to examine the effect of prosodic enrichment3 . Unlike (Kahn et al., 2005), we do not remove EDITED regions prior to parsing because parsing of EDITED regions is likely to benefit from prosodic information. Also, parses from all models are compared with the gold standard parses in the Fisher evaluation set using SParseval bracket scoring (Harper et al., 2005; Roark et al., 2006) without flattening the EDITED constituents. 3 Methods of Integrating Breaks Rather than using quantized raw acoustic features as in (Gregory et al., 2004), we use automatically generated ToBI break indexes as in (Drey"
N10-1005,H05-1030,0,0.664388,"es, which are predicted based only on acoustic cues and could misalign with phrase boundaries. We illustrate that these prosodically enriched models are in fact highly effective if we systematically eliminate bad phrase and hesitation breaks given their projection onto the reference parse trees. Inspired by this, we propose two alternative rescoring methods to restrict the search space of the prosodically enriched parser models to the n-best parses from the baseline PCFGLA parser to avoid egregious parse trees. The effectiveness of our rescoring method suggests that the reranking approach of (Kahn et al., 2005) was successful not only because of their prosodic feature design, but also because they restrict the search space for reranking to n-best lists generated by a syntactic model alone. 2 Experimental Setup Due to our goal of investigating the effect of prosodic information on the accuracy of state of the art parsing of conversational speech, we utilize both Penn Switchboard (Godfrey et al., 1992) and Fisher treebanks (Harper et al., 2005; Bies et al., 2006), for which we also had automatically generated break indexes from (Dreyer and Shafran, 2007; Harper et al., 2005)1 . The Fisher treebank is"
N10-1005,P05-1056,1,0.870632,"ttention to important information through contrastive pitch or duration patterns associated words or phrases. In addition, prosodic cues can help one to segment speech into chunks that are hypothesized to have a hierarchical structure, although not necessarily identical to that of syntax. This suggests that prosodic cues may help in the parsing of speech inputs, the topic of this paper. Prosodic information such as pause length, duration of words and phones, pitch contours, energy contours, and their normalized values have been used in speech processing tasks like sentence boundary detection (Liu et al., 2005). In contrast, other researchers use linguistic encoding schemes like ToBI (Silverman et al., 1992), which encodes tones, the degree of juncture between words, and prominence symbolically. For example, a simplified ToBI encoding scheme uses the symbol 4 for major intonational breaks, p for hesitation, and 1 for all other breaks (Dreyer and Shafran, 2007). In the literature, there have been several attempts to integrate prosodic information to improve parse accuracy of speech transcripts. These studies have used either quantized acoustic measurements of prosody or automatically detected break i"
N10-1005,P05-1010,0,0.310534,"pkins University, Baltimore, MD USA {zqhuang,mharper}@umiacs.umd.edu Abstract This paper investigates using prosodic information in the form of ToBI break indexes for parsing spontaneous speech. We revisit two previously studied approaches, one that hurt parsing performance and one that achieved minor improvements, and propose a new method that aims to better integrate prosodic breaks into parsing. Although these approaches can improve the performance of basic probabilistic context free grammar (PCFG) parsers, they all fail to produce fine-grained PCFG models with latent annotations (PCFGLA) (Matsuzaki et al., 2005; Petrov and Klein, 2007) that perform significantly better than the baseline PCFG-LA model that does not use break indexes, partially due to mis-alignments between automatic prosodic breaks and true phrase boundaries. We propose two alternative ways to restrict the search space of the prosodically enriched parser models to the nbest parses from the baseline PCFG-LA parser to avoid egregious parses caused by incorrect breaks. Our experiments show that all of the prosodically enriched parser models can then achieve significant improvement over the baseline PCFG-LA parser. 1 Introduction Speech"
N10-1005,N07-1051,0,0.569215,"ore, MD USA {zqhuang,mharper}@umiacs.umd.edu Abstract This paper investigates using prosodic information in the form of ToBI break indexes for parsing spontaneous speech. We revisit two previously studied approaches, one that hurt parsing performance and one that achieved minor improvements, and propose a new method that aims to better integrate prosodic breaks into parsing. Although these approaches can improve the performance of basic probabilistic context free grammar (PCFG) parsers, they all fail to produce fine-grained PCFG models with latent annotations (PCFGLA) (Matsuzaki et al., 2005; Petrov and Klein, 2007) that perform significantly better than the baseline PCFG-LA model that does not use break indexes, partially due to mis-alignments between automatic prosodic breaks and true phrase boundaries. We propose two alternative ways to restrict the search space of the prosodically enriched parser models to the nbest parses from the baseline PCFG-LA parser to avoid egregious parses caused by incorrect breaks. Our experiments show that all of the prosodically enriched parser models can then achieve significant improvement over the baseline PCFG-LA parser. 1 Introduction Speech conveys more than a seque"
N10-1005,roark-etal-2006-sparseval,1,0.85885,"elated to edits. We will compare2 three prosodically enriched PCFG models described in the next section, with a baseline PCFG parser. We will also utilize a state of the art PCFG-LA parser (Petrov and Klein, 2007; Huang and Harper, 2009) to examine the effect of prosodic enrichment3 . Unlike (Kahn et al., 2005), we do not remove EDITED regions prior to parsing because parsing of EDITED regions is likely to benefit from prosodic information. Also, parses from all models are compared with the gold standard parses in the Fisher evaluation set using SParseval bracket scoring (Harper et al., 2005; Roark et al., 2006) without flattening the EDITED constituents. 3 Methods of Integrating Breaks Rather than using quantized raw acoustic features as in (Gregory et al., 2004), we use automatically generated ToBI break indexes as in (Dreyer and Shafran, 2007; Kahn et al., 2005) as the prosodic cues, and investigate three alternative methods of modeling prosodic breaks. Figure 1 shows parse trees for the four models for processing the spontaneous speech transcription she’s she would do, where the speaker hesitated after saying she’s and then resumed with another utterance she would do. Each word input into the par"
P05-1056,W04-3209,1,0.68097,"rion more closely related to classification performance. Second, the N-gram LM underlying the HMM transition model makes it difficult to use features that are highly correlated (such as words and POS labels) without greatly increasing the number of model parameters, which in turn would make robust estimation difficult. More details about using textual information in the HMM system are provided in Section 3. 1.2 Sentence Segmentation Using Maxent A maximum entropy (Maxent) posterior classification method has been evaluated in an attempt to overcome some of the shortcomings of the HMM approach (Liu et al., 2004; Huang and Zweig, 2002). For a boundary position i, the Maxent model takes the exponential form: ( ij P E i T ;F i) = 1  (Ti ; Fi ) Z e P k Fi Oi In the HMM, the forward-backward algorithm is used to determine the event with the highest posterior probability for each interword boundary: Ei Wi+1 Ei+1 i?1 Ei?1 ) W 1 E1 : : : W E Ei k gk (Ei ;Ti ;Fi ) (2) where Z (Ti ; Fi ) is a normalization term and Ti represents textual information. The indicator functions gk (Ei ; Ti ; Fi ) correspond to features defined over events, words, and prosody. The parameters in 2 In the prosody model implementat"
P05-1056,W03-0430,0,0.0544118,"tection task. The rest of the paper is organized as follows. Section 2 describes the CRF model and discusses how it differs from the HMM and Maxent models. Section 3 describes the data and features used in the models to be compared. Section 4 summarizes the experimental results for the sentence boundary detection task. Conclusions and future work appear in Section 5. 2 CRF Model Description E1 A CRF is a random field that is globally conditioned on an observation sequence O . CRFs have been successfully used for a variety of text processing tasks (Lafferty et al., 2001; Sha and Pereira, 2003; McCallum and Li, 2003), but they have not been widely applied to a speech-related task with both acoustic and textual knowledge sources. The top graph in Figure 2 is a general CRF model. The states of the model correspond to event labels E . The observations O are composed of the textual features, as well as the ^ prosodic features. The most likely event sequence E for the given input sequence (observations) O is ^ = arg max E P e E k k Gk (E;O) Z  (O ) (3) where the functions G are potential functions over the events and the observations, and Z is the normalization term:  (O ) Z = X P G e k k E;O) k( (4) E Ev"
P05-1056,H89-1002,0,0.36263,"Missing"
P05-1056,N03-1028,0,0.0216611,"he sentence boundary detection task. The rest of the paper is organized as follows. Section 2 describes the CRF model and discusses how it differs from the HMM and Maxent models. Section 3 describes the data and features used in the models to be compared. Section 4 summarizes the experimental results for the sentence boundary detection task. Conclusions and future work appear in Section 5. 2 CRF Model Description E1 A CRF is a random field that is globally conditioned on an observation sequence O . CRFs have been successfully used for a variety of text processing tasks (Lafferty et al., 2001; Sha and Pereira, 2003; McCallum and Li, 2003), but they have not been widely applied to a speech-related task with both acoustic and textual knowledge sources. The top graph in Figure 2 is a general CRF model. The states of the model correspond to event labels E . The observations O are composed of the textual features, as well as the ^ prosodic features. The most likely event sequence E for the given input sequence (observations) O is ^ = arg max E P e E k k Gk (E;O) Z  (O ) (3) where the functions G are potential functions over the events and the observations, and Z is the normalization term:  (O ) Z = X P"
P06-1021,N01-1016,0,0.406874,"rally lack appropriate rules for analyzing these constructions. One possible response to this mismatch between grammatical resources and the brute facts of disfluent speech is to make one look more like the other, for the purpose of parsing. In this separate-processing approach, reparanda are located through a variety of acoustic, lexical or string-based techniques, then excised before submission to a parser (Stolcke and Shriberg, 1996; Heeman and Allen, 1999; Spilker et al., 2000; Johnson and Charniak, 2004). The resulting parse tree then has the reparandum re-attached in a standardized way (Charniak and Johnson, 2001). An alternative strategy, adopted in this paper, is to use the same grammar to model fluent speech, disfluent speech, and their interleaving. Such an integrated approach can use syntactic properties of the reparandum itself. For instance, in example (1) the reparandum is an unfinished noun phrase, the repair a finished noun phrase. This sort of phrasal correspondence, while not absolute, is strong in conversational speech, and cannot be exploited on the separate-processing approach. Section 3 applies metarules (Weischedel and Sondheimer, 1983; McKelvie, 1998a; Core and Schubert, 1999) in reco"
P06-1021,A00-2018,0,0.0169867,"ation × √ both × √ EDIT F none Parseval F Break index POST (Ratnaparkhi, 1996) which was itself trained on Switchboard. Finally, as described in section 2 these tags were augmented with a special prosodic break symbol if the decision tree rated the probability a ToBI ‘p’ symbol higher than the threshold value of 0.75. Annotation speech repairs. The first two use the CYK algorithm to find the most likely parse tree on a grammar read-off from example trees annotated as in Figures 2 and 4. The third experiment measures the benefit from syntactic indicators alone in Charniak’s lexicalized parser (Charniak, 2000). The tables in subsections 4.1, 4.2, and 4.3 summarize the accuracy of output parse trees on two measures. One is the standard Parseval F-measure, which tracks the precision and recall for all labeled constituents as compared to a gold-standard parse. The other measure, EDIT-finding F, restricts consideration to just constituents that are reparanda. It measures the per-word performance identifying a word as dominated by EDITED or not. As in previous studies, reference transcripts were used in all √ cases. A check ( ) indicates an experiment where prosodic breaks where automatically inferred b"
P06-1021,P99-1053,0,0.591037,"way (Charniak and Johnson, 2001). An alternative strategy, adopted in this paper, is to use the same grammar to model fluent speech, disfluent speech, and their interleaving. Such an integrated approach can use syntactic properties of the reparandum itself. For instance, in example (1) the reparandum is an unfinished noun phrase, the repair a finished noun phrase. This sort of phrasal correspondence, while not absolute, is strong in conversational speech, and cannot be exploited on the separate-processing approach. Section 3 applies metarules (Weischedel and Sondheimer, 1983; McKelvie, 1998a; Core and Schubert, 1999) in recognizing these correspondences using standard context-free grammars. At the same time as it defies parsing, conversational speech offers the possibility of leveraging prosodic cues to speech repairs. SecA grammatical method of combining two kinds of speech repair cues is presented. One cue, prosodic disjuncture, is detected by a decision tree-based ensemble classifier that uses acoustic cues to identify where normal prosody seems to be interrupted (Lickley, 1996). The other cue, syntactic parallelism, codifies the expectation that repairs continue a syntactic category that was left unfi"
P06-1021,N04-1011,0,0.0312862,"Missing"
P06-1021,H05-1030,1,0.8458,"ecision and recall trade-off on its detection can be adjusted using a threshold on the posterior probability of predicting “p”, as shown in Figure 3. In essence, the large number of acoustic and prosodic features related to disfluency are encoded via the ToBI label ‘p’, and provided as additional observations to the PCFG. This is unlike previous work on incorporating prosodic information (Gre0.6 0.5 Probability of Miss 0.4 0.3 0.2 0.1 0 0 0.1 0.2 0.3 Probability of False Alarm 0.4 0.5 0.6 Figure 3: DET curve for detecting disfluent breaks from acoustics. gory et al., 2004; Lease et al., 2005; Kahn et al., 2005) as described further in Section 6. 3 Syntactic parallelism The other striking property of speech repairs is their parallel character: subsequent repair regions ‘line up’ with preceding reparandum regions. This property can be harnessed to better estimate the length of the reparandum by considering parallelism from the perspective of syntax. For instance, in Figure 4(a) the unfinished reparandum noun phrase is repaired by another noun phrase – the syntactic categories are parallel. 3.1 Levelt’s WFR and Conjunction The idea that the reparandum is syntactically parallel to the repair can be trac"
P06-1021,J93-2004,0,0.0291288,"ion of Levelt’s WFR can be applied to Treebanks by systematically recoding the annotations to indicate which phrases are unfinished and to distinguish matching from nonmatching repairs. (2) If, as schema (2) suggests, conjunction does favor like-categories, and, as Levelt suggests, wellformed repairs are conjoinable with finished versions of their reparanda, then the syntactic categories of repairs ought to match the syntactic categories of (finished versions of) reparanda. 3.2 (3) 3.3.1 Unfinished phrases Some Treebanks already mark unfinished phrases. For instance, the Penn Treebank policy (Marcus et al., 1993; Marcus et al., 1994) is to annotate the lowest node that is unfinished with an -UNF tag as in Figure 4(a). It is straightforward to propagate this mark upwards in the tree from wherever it is annotated to the nearest enclosing EDITED node, just as -BRK is propagated upwards from disjuncture marks on individual words. This percolation simulates the action of McKelvie’s [abort = true]. The resulting PCFG is one in which distributions on phrase structure rules with ‘missing’ daughters are segregated from distributions on ‘complete’ rules. A WFR for grammars Levelt’s WFR imposes two requirements"
P06-1021,C00-2169,0,0.0526183,"Missing"
P06-1021,H94-1020,0,0.0752404,"an be applied to Treebanks by systematically recoding the annotations to indicate which phrases are unfinished and to distinguish matching from nonmatching repairs. (2) If, as schema (2) suggests, conjunction does favor like-categories, and, as Levelt suggests, wellformed repairs are conjoinable with finished versions of their reparanda, then the syntactic categories of repairs ought to match the syntactic categories of (finished versions of) reparanda. 3.2 (3) 3.3.1 Unfinished phrases Some Treebanks already mark unfinished phrases. For instance, the Penn Treebank policy (Marcus et al., 1993; Marcus et al., 1994) is to annotate the lowest node that is unfinished with an -UNF tag as in Figure 4(a). It is straightforward to propagate this mark upwards in the tree from wherever it is annotated to the nearest enclosing EDITED node, just as -BRK is propagated upwards from disjuncture marks on individual words. This percolation simulates the action of McKelvie’s [abort = true]. The resulting PCFG is one in which distributions on phrase structure rules with ‘missing’ daughters are segregated from distributions on ‘complete’ rules. A WFR for grammars Levelt’s WFR imposes two requirements on a grammar • distin"
P06-1021,J83-3003,0,0.415711,"has the reparandum re-attached in a standardized way (Charniak and Johnson, 2001). An alternative strategy, adopted in this paper, is to use the same grammar to model fluent speech, disfluent speech, and their interleaving. Such an integrated approach can use syntactic properties of the reparandum itself. For instance, in example (1) the reparandum is an unfinished noun phrase, the repair a finished noun phrase. This sort of phrasal correspondence, while not absolute, is strong in conversational speech, and cannot be exploited on the separate-processing approach. Section 3 applies metarules (Weischedel and Sondheimer, 1983; McKelvie, 1998a; Core and Schubert, 1999) in recognizing these correspondences using standard context-free grammars. At the same time as it defies parsing, conversational speech offers the possibility of leveraging prosodic cues to speech repairs. SecA grammatical method of combining two kinds of speech repair cues is presented. One cue, prosodic disjuncture, is detected by a decision tree-based ensemble classifier that uses acoustic cues to identify where normal prosody seems to be interrupted (Lickley, 1996). The other cue, syntactic parallelism, codifies the expectation that repairs conti"
P06-1021,W05-1519,0,0.0261845,"Missing"
P06-1021,H91-1073,0,0.273826,"onstituents labeled EDITED. Such NP NP NP Prosodic disjuncture Everyday experience as well as acoustic analysis suggests that the syntactic interruption in speech repairs is typically accompanied by a change in prosody (Nakatani and Hirschberg, 1994; Shriberg, 1994). For instance, the spectrogram corresponding to example (2), shown in Figure 1, (2) DT NNP the jehovah NNP POS witness EDITED CC NP CC−BRK or NNPS or~+ mormons ’s Figure 2: Propagating BRK, the evidence of disfluent juncture, from acoustics to syntax. disjuncture symbols are identified in the ToBI labeling scheme as break indices (Price et al., 1991; Silverman et al., 1992). The availability of a corpus annotated with ToBI labels makes it possible to design a break index classifier via supervised training. The corpus is a subset of the Switchboard corpus, consisting of sixty-four telephone conversations manually annotated by an experienced linguist according to a simplified ToBI labeling scheme (Ostendorf et al., 2001). In ToBI, degree of disjuncture is indicated by integer values from 0 to 4, where a value of 0 corresponds to clitic and 4 to a major phrase break. In addition, a suffix p denotes perceptually disfluent events reflecting,"
P06-1021,W96-0213,0,\N,Missing
P06-1021,P83-1019,0,\N,Missing
P06-1021,J99-4003,0,\N,Missing
P06-1021,P04-1005,0,\N,Missing
P09-1048,P98-1013,0,0.0125282,"Missing"
P09-1048,W05-0620,0,0.0201316,"Missing"
P09-1048,P01-1017,0,0.0402229,"Missing"
P09-1048,W03-1006,0,0.020803,"Missing"
P09-1048,erk-pado-2006-shalmaneser,0,0.021266,"Missing"
P09-1048,J02-3001,0,0.10105,"Missing"
P09-1048,P02-1031,0,0.0346015,"Missing"
P09-1048,P07-1098,0,0.019224,"Missing"
P09-1048,N07-1051,0,0.0240934,"Missing"
P09-1048,C04-1127,1,0.822126,"Missing"
P09-1048,N04-1032,0,0.0335295,"Missing"
P09-1048,W05-0623,0,0.0539303,"Missing"
P09-1048,W04-3212,0,0.049316,"Missing"
P09-1048,W01-1511,1,0.848003,"Missing"
P09-1048,D07-1077,0,0.0941353,"Missing"
P09-1048,N10-1005,1,\N,Missing
P09-1048,C98-1013,0,\N,Missing
P11-2109,D09-1116,1,0.930087,"e model would benefit from merging them. Therefore, it is reasonable to believe that arbitrary (i.e., unconstrained) context clustering such as a decision tree should be able to outperform the n-gram model. A decision tree provides us with a clustering funci−1 tion Φ(wi−n+1 ) → {Φ1 , . . . , ΦN }, where N is the number of clusters (leaves in the DT), and clusters Φk are disjoint subsets of the context space; the probability estimation is approximated as follows: i−1 i−1 p(wi |wi−n+1 ) ≈ p(wi |Φ(wi−n+1 )) (2) Methods of DT construction and probability estimation used in this work are based on (Filimonov and Harper, 2009); therefore, we refer the reader to that paper for details. Another advantage of using decision trees is the ease of adding parameters such as syntactic tags: p(w1m ) = X p(w1m tm 1 ) = t1 ...tm ≈ m X Y p(wi ti |w1i−1 ti−1 1 ) t1 ...tm i=1 m X Y i−1 p(wi ti |Φ(wi−n+1 ti−1 i−n+1 )) (3) t1 ...tm i=1 In this case, the decision tree would cluster the coni−1 text space wi−n+1 ti−1 i−n+1 based on information theoretic metrics, without utilizing heuristics for which order the context attributes are to be backed off (cf. Eq. 1). In subsequent discussion, we will write equations for word models (Eq. 2)"
P11-2109,W99-0617,0,0.107249,"Missing"
P11-2109,D09-1087,1,0.814248,"n 35M words of WSJ 94-96 from LDC2008T13. The text was converted into speech-like form, namely numbers and abbreviations were verbalized, text was downcased, punctuation was removed, and contractions and possessives were joined with the previous word (i.e., they ’ll becomes they’ll). For syntactic modeling, we used tags comprised of POS tags of the word and its head, as in (Filimonov and Harper, 2009). Parsing of the text for tag extraction occurred after verbalization of numbers and abbreviations but before any further processing; we used an appropriately trained latent variable PCFG parser (Huang and Harper, 2009). For reference, we include n-gram models 623 Conclusion The main contribution of this paper is the insight that in the standard recursive backoff there is an implied relation between the backoff and the higher order models, which is essential for adequate performance. When this relation is not satisfied other interpolation methods should be employed; hence, we propose a generalization of linear interpolation that significantly outperforms the standard form in such a scenario. 3 We refer the reader to (Filimonov and Harper, 2009) for details on the tree construction algorithm. 4 The higher ord"
P11-2109,W04-3242,0,\N,Missing
P86-1003,P85-1003,0,0.175425,"lap. 3. allow the exact time of event to be unfixed until it is pinpointed based on contextual information or adverbial modification. 4. allow reference to points and intervals of time (eg. precisely at 3 PM VS. for 5 hours). This work has been supported in part by the National Science Foundation under grants IST 8416034 and IST 8515005, and Office of Naval Research under grant N00014-79-C-0529. 5. allow parsing of temporal information in sentences to be simple and compositional. These criteria were used to judge previous temporal representation research (Bruce (1972), Hornstein (1977, 1981), Yip (1985)). None fulfilled all five criteria. The criteria will also be used to judge the representations developed here. Tense The representations for tense, adverbs, and temporal connectives developed here is based on McDermott's (1982) temporal logic. McDermott's ""point-based"" temporal logic was chosen because it is not unusual to talk about the beginning and end points of a period of time or an event. In fact, the semantics of tense developed here relate the endpoints o f events in sentences. This representation of tense provides a flexibility not found in many other representations of tense (eg. ("
P90-1009,P88-1010,0,0.0710724,"Missing"
P99-1023,J97-3003,0,0.00904641,"etail by Rabiner (1989) and will not be repeated here. 2.2 Calculating Probabilities for U n k n o w n Words In a standard HMM, when a word does not occur in the training data, the emit probability for the unknown word is 0.0 in the B matrix (i.e., bj(k) = 0.0 if wk is unknown). Being able to accurately tag unknown words is important, as they are frequently encountered when tagging sentences in applications. Most work in the area of unknown words and tagging deals with predicting part-of-speech information based on word endings and affixation information, as shown by work in (Mikheev, 1996), (Mikheev, 1997), (Weischedel et al., 1993), and (Thede, 1998). This section highlights a method devised for HMMs, which differs slightly from previous approaches. To create an HMM to accurately tag unknown words, it is necessary to determine an estimate of the probability P(wklti) for use in the tagger. The probability P(word contains sjl tag is ti) is estimated, where sj is some &quot;suffix&quot; (a more appropriate term would be word ending, since the sj&apos;s are not necessarily morphologically significant, but this terminology is unwieldy). This new probability is stored in a matrix C = {cj(k)), where cj(k) = P(word"
P99-1023,W96-0213,0,0.126147,") and verb tense. The Penn Treebank documentation (Marcus et al., 1993) defines a commonly used set of tags. Part-of-speech tagging is an important research topic in Natural Language Processing (NLP). Taggers are often preprocessors in NLP systems, making accurate performance especially important. Much research has been done to improve tagging accuracy using several different models and methods, including: hidden Markov models (HMMs) (Kupiec, 1992), (Charniak et al., 1993); rule-based systems (Brill, 1994), (Brill, 1995); memory-based systems (Daelemans et al., 1996); maximum-entropy systems (Ratnaparkhi, 1996); path voting constraint systems (Tiir and Oflazer, 1998); linear separator systems (Roth and Zelenko, 1998); and majority voting systems (van Halteren et al., 1998). This paper describes various modifications to an HMM tagger that improve the performance to an accuracy comparable to or better than the best current single classifier taggers. This improvement comes from using secondorder approximations of the Markov assumptions. Section 2 discusses a basic first-order hidden Markov model for part-of-speech tagging and extensions to that model to handle out-oflexicon words. The new second-order"
P99-1023,J95-4004,0,0.0944564,"ccasionally include additional feature information, such as number (singular or plural) and verb tense. The Penn Treebank documentation (Marcus et al., 1993) defines a commonly used set of tags. Part-of-speech tagging is an important research topic in Natural Language Processing (NLP). Taggers are often preprocessors in NLP systems, making accurate performance especially important. Much research has been done to improve tagging accuracy using several different models and methods, including: hidden Markov models (HMMs) (Kupiec, 1992), (Charniak et al., 1993); rule-based systems (Brill, 1994), (Brill, 1995); memory-based systems (Daelemans et al., 1996); maximum-entropy systems (Ratnaparkhi, 1996); path voting constraint systems (Tiir and Oflazer, 1998); linear separator systems (Roth and Zelenko, 1998); and majority voting systems (van Halteren et al., 1998). This paper describes various modifications to an HMM tagger that improve the performance to an accuracy comparable to or better than the best current single classifier taggers. This improvement comes from using secondorder approximations of the Markov assumptions. Section 2 discusses a basic first-order hidden Markov model for part-of-spee"
P99-1023,W96-0102,0,0.00784413,"re information, such as number (singular or plural) and verb tense. The Penn Treebank documentation (Marcus et al., 1993) defines a commonly used set of tags. Part-of-speech tagging is an important research topic in Natural Language Processing (NLP). Taggers are often preprocessors in NLP systems, making accurate performance especially important. Much research has been done to improve tagging accuracy using several different models and methods, including: hidden Markov models (HMMs) (Kupiec, 1992), (Charniak et al., 1993); rule-based systems (Brill, 1994), (Brill, 1995); memory-based systems (Daelemans et al., 1996); maximum-entropy systems (Ratnaparkhi, 1996); path voting constraint systems (Tiir and Oflazer, 1998); linear separator systems (Roth and Zelenko, 1998); and majority voting systems (van Halteren et al., 1998). This paper describes various modifications to an HMM tagger that improve the performance to an accuracy comparable to or better than the best current single classifier taggers. This improvement comes from using secondorder approximations of the Markov assumptions. Section 2 discusses a basic first-order hidden Markov model for part-of-speech tagging and extensions to that model to hand"
P99-1023,J93-2004,0,0.027286,"ger to state of the art levels. These approximations make use of more contextual information than standard statistical systems. New methods of smoothing the estimated probabilities are also introduced to address the sparse data problem. 1 Introduction Part-of-speech tagging is the act of assigning each word in a sentence a tag that describes how that word is used in the sentence. Typically, these tags indicate syntactic categories, such as noun or verb, and occasionally include additional feature information, such as number (singular or plural) and verb tense. The Penn Treebank documentation (Marcus et al., 1993) defines a commonly used set of tags. Part-of-speech tagging is an important research topic in Natural Language Processing (NLP). Taggers are often preprocessors in NLP systems, making accurate performance especially important. Much research has been done to improve tagging accuracy using several different models and methods, including: hidden Markov models (HMMs) (Kupiec, 1992), (Charniak et al., 1993); rule-based systems (Brill, 1994), (Brill, 1995); memory-based systems (Daelemans et al., 1996); maximum-entropy systems (Ratnaparkhi, 1996); path voting constraint systems (Tiir and Oflazer, 1"
P99-1023,P96-1043,0,0.0137354,"is explained in detail by Rabiner (1989) and will not be repeated here. 2.2 Calculating Probabilities for U n k n o w n Words In a standard HMM, when a word does not occur in the training data, the emit probability for the unknown word is 0.0 in the B matrix (i.e., bj(k) = 0.0 if wk is unknown). Being able to accurately tag unknown words is important, as they are frequently encountered when tagging sentences in applications. Most work in the area of unknown words and tagging deals with predicting part-of-speech information based on word endings and affixation information, as shown by work in (Mikheev, 1996), (Mikheev, 1997), (Weischedel et al., 1993), and (Thede, 1998). This section highlights a method devised for HMMs, which differs slightly from previous approaches. To create an HMM to accurately tag unknown words, it is necessary to determine an estimate of the probability P(wklti) for use in the tagger. The probability P(word contains sjl tag is ti) is estimated, where sj is some &quot;suffix&quot; (a more appropriate term would be word ending, since the sj&apos;s are not necessarily morphologically significant, but this terminology is unwieldy). This new probability is stored in a matrix C = {cj(k)), wher"
P99-1023,P98-2186,0,0.00962812,"tags. Part-of-speech tagging is an important research topic in Natural Language Processing (NLP). Taggers are often preprocessors in NLP systems, making accurate performance especially important. Much research has been done to improve tagging accuracy using several different models and methods, including: hidden Markov models (HMMs) (Kupiec, 1992), (Charniak et al., 1993); rule-based systems (Brill, 1994), (Brill, 1995); memory-based systems (Daelemans et al., 1996); maximum-entropy systems (Ratnaparkhi, 1996); path voting constraint systems (Tiir and Oflazer, 1998); linear separator systems (Roth and Zelenko, 1998); and majority voting systems (van Halteren et al., 1998). This paper describes various modifications to an HMM tagger that improve the performance to an accuracy comparable to or better than the best current single classifier taggers. This improvement comes from using secondorder approximations of the Markov assumptions. Section 2 discusses a basic first-order hidden Markov model for part-of-speech tagging and extensions to that model to handle out-oflexicon words. The new second-order HMM is described in Section 3, and Section 4 presents experimental results and conclusions. 2 Hidden Markov"
P99-1023,C98-2246,1,0.785445,"here. 2.2 Calculating Probabilities for U n k n o w n Words In a standard HMM, when a word does not occur in the training data, the emit probability for the unknown word is 0.0 in the B matrix (i.e., bj(k) = 0.0 if wk is unknown). Being able to accurately tag unknown words is important, as they are frequently encountered when tagging sentences in applications. Most work in the area of unknown words and tagging deals with predicting part-of-speech information based on word endings and affixation information, as shown by work in (Mikheev, 1996), (Mikheev, 1997), (Weischedel et al., 1993), and (Thede, 1998). This section highlights a method devised for HMMs, which differs slightly from previous approaches. To create an HMM to accurately tag unknown words, it is necessary to determine an estimate of the probability P(wklti) for use in the tagger. The probability P(word contains sjl tag is ti) is estimated, where sj is some &quot;suffix&quot; (a more appropriate term would be word ending, since the sj&apos;s are not necessarily morphologically significant, but this terminology is unwieldy). This new probability is stored in a matrix C = {cj(k)), where cj(k) = P(word has suffix ski tag is tj), replaces bj(k) in t"
P99-1023,P98-2208,0,0.00842832,"arcus et al., 1993) defines a commonly used set of tags. Part-of-speech tagging is an important research topic in Natural Language Processing (NLP). Taggers are often preprocessors in NLP systems, making accurate performance especially important. Much research has been done to improve tagging accuracy using several different models and methods, including: hidden Markov models (HMMs) (Kupiec, 1992), (Charniak et al., 1993); rule-based systems (Brill, 1994), (Brill, 1995); memory-based systems (Daelemans et al., 1996); maximum-entropy systems (Ratnaparkhi, 1996); path voting constraint systems (Tiir and Oflazer, 1998); linear separator systems (Roth and Zelenko, 1998); and majority voting systems (van Halteren et al., 1998). This paper describes various modifications to an HMM tagger that improve the performance to an accuracy comparable to or better than the best current single classifier taggers. This improvement comes from using secondorder approximations of the Markov assumptions. Section 2 discusses a basic first-order hidden Markov model for part-of-speech tagging and extensions to that model to handle out-oflexicon words. The new second-order HMM is described in Section 3, and Section 4 presents exp"
P99-1023,W96-0101,0,0.0159135,"o a w o r d / t a g pair t h a t does not appear in the training data. This prevents the tagger from trying every possible combination of word and tag, something which both increases running time and decreases the accuracy. We believe the low accuracy of the original smoothing scheme emerges from the fact t h a t smoothing the lexical probabilities too far allows the contextual information to dominate at the expense of the lexical information. A better smoothing approach for lexical information could possibly be created by using some sort of word class idea, such as the genotype idea used in (Tzoukermann and Radev, 1996), to improve our /5 estimate. 178 In addition to choosing the above approach for smoothing the C matrix for unknown words, there is an additional issue of choosing which suffix to use when predicting the part of speech. There are many possible answers, some of which are considered by (Thede, 1998): use the longest matching suffix, use an entropy measure to determine the &quot;best&quot; affix to use, or use an average. A voting technique for c i j ( k ) was determined that is similar to that used for contextual smoothing but is based on different length suffixes. Let s4 be the length four suffix of the"
P99-1023,P98-1081,0,0.0687059,"Missing"
P99-1023,J93-2006,0,0.041653,"(1989) and will not be repeated here. 2.2 Calculating Probabilities for U n k n o w n Words In a standard HMM, when a word does not occur in the training data, the emit probability for the unknown word is 0.0 in the B matrix (i.e., bj(k) = 0.0 if wk is unknown). Being able to accurately tag unknown words is important, as they are frequently encountered when tagging sentences in applications. Most work in the area of unknown words and tagging deals with predicting part-of-speech information based on word endings and affixation information, as shown by work in (Mikheev, 1996), (Mikheev, 1997), (Weischedel et al., 1993), and (Thede, 1998). This section highlights a method devised for HMMs, which differs slightly from previous approaches. To create an HMM to accurately tag unknown words, it is necessary to determine an estimate of the probability P(wklti) for use in the tagger. The probability P(word contains sjl tag is ti) is estimated, where sj is some &quot;suffix&quot; (a more appropriate term would be word ending, since the sj&apos;s are not necessarily morphologically significant, but this terminology is unwieldy). This new probability is stored in a matrix C = {cj(k)), where cj(k) = P(word has suffix ski tag is tj),"
P99-1023,H94-1049,0,\N,Missing
P99-1023,P98-2251,1,\N,Missing
P99-1023,C98-2203,0,\N,Missing
P99-1023,C98-1078,0,\N,Missing
P99-1023,C98-2181,0,\N,Missing
roark-etal-2006-sparseval,A00-2018,1,\N,Missing
roark-etal-2006-sparseval,J93-2004,0,\N,Missing
roark-etal-2006-sparseval,P97-1003,0,\N,Missing
roark-etal-2006-sparseval,N01-1016,1,\N,Missing
roark-etal-2006-sparseval,N04-4032,1,\N,Missing
roark-etal-2006-sparseval,J01-2004,1,\N,Missing
W02-1031,P01-1017,0,0.252317,"Missing"
W02-1031,P96-1025,0,0.0307547,"Missing"
W02-1031,1997.iwpt-1.13,0,0.113915,"Missing"
W02-1031,A00-2014,0,0.167767,"Missing"
W02-1031,W98-1121,0,0.451008,"Missing"
W02-1031,P01-1042,0,0.0633577,"Missing"
W02-1031,J93-2004,0,0.0347107,"Missing"
W02-1031,P90-1005,0,0.118146,"Missing"
W02-1031,J01-2004,0,0.169437,"Missing"
W02-1031,P01-1010,0,\N,Missing
W04-0307,A00-2018,0,0.884176,"g incrementally and evaluate it on the Wall Street Journal Penn Treebank. Using a tight integration of multiple knowledge sources, together with distance modeling and synergistic dependencies, this parser achieves a parsing accuracy comparable to several state-of-the-art context-free grammar (CFG) based statistical parsers using a dependency-based evaluation metric. Factors contributing to the SCDG parser’s performance are analyzed. 1 Introduction Statistical parsing has been an important focus of recent research (Magerman, 1995; Eisner, 1996; Charniak, 1997; Collins, 1999; Ratnaparkhi, 1999; Charniak, 2000). Several of these parsers generate constituents by conditioning probabilities on non-terminal labels, part-of-speech (POS) tags, and some headword information (Collins, 1999; Ratnaparkhi, 1999; Charniak, 2000). They utilize non-terminals that go beyond the level of a single word and do not explicitly use lexical features. Collins’ Model 2 parser (1999) learns the distinction between complements and adjuncts by using heuristics during training, distinguishes complement and adjunct non-terminals, and includes a probabilistic choice of left and right subcategorization frames, while his Model 3 p"
W04-0307,P01-1017,0,0.0169473,"aparkhi, 1999; Charniak, 2000). They utilize non-terminals that go beyond the level of a single word and do not explicitly use lexical features. Collins’ Model 2 parser (1999) learns the distinction between complements and adjuncts by using heuristics during training, distinguishes complement and adjunct non-terminals, and includes a probabilistic choice of left and right subcategorization frames, while his Model 3 parser uses gap features to model wh-movement. Charniak (Charniak, 2000) developed a state-of-the-art statistical CFG parser and then built an effective language model based on it (Charniak, 2001). But his parser and language model were originally designed to analyze complete sentences. Among the statistical dependency grammar parsers, Eisner’s (1996) best probabilistic dependency model used unlabeled links between words and their heads, as Mary P. Harper Electrical and Computer Engineering Purdue University West Lafayette, IN 47907-1285, U.S.A., harper@ecn.purdue.edu well as between words and their complements and adjuncts. However, the parser does not distinguish between complements and adjuncts or model whmovement. Collins’ bilexical dependency grammar parser (1999) used head-modifi"
W04-0307,P95-1037,0,0.0600699,"In this paper, we describe a statistical CDG (SCDG) parser that performs parsing incrementally and evaluate it on the Wall Street Journal Penn Treebank. Using a tight integration of multiple knowledge sources, together with distance modeling and synergistic dependencies, this parser achieves a parsing accuracy comparable to several state-of-the-art context-free grammar (CFG) based statistical parsers using a dependency-based evaluation metric. Factors contributing to the SCDG parser’s performance are analyzed. 1 Introduction Statistical parsing has been an important focus of recent research (Magerman, 1995; Eisner, 1996; Charniak, 1997; Collins, 1999; Ratnaparkhi, 1999; Charniak, 2000). Several of these parsers generate constituents by conditioning probabilities on non-terminal labels, part-of-speech (POS) tags, and some headword information (Collins, 1999; Ratnaparkhi, 1999; Charniak, 2000). They utilize non-terminals that go beyond the level of a single word and do not explicitly use lexical features. Collins’ Model 2 parser (1999) learns the distinction between complements and adjuncts by using heuristics during training, distinguishes complement and adjunct non-terminals, and includes a pro"
W04-0307,J01-2004,0,0.0275618,"tagging accuracy and parse accuracy improve in tandem, as can be seen in Tables 2 and 3. These results are consistent with the observations of (Collins, 1999) and (Eisner, 1996). It is important to note that each of the factors contributing to improved parse accuracy in these two experiments also improved the word prediction capability of the corresponding parser-based LM (Wang and Harper, 2003). 5.2 Comparing to Other Parsers Charniak’s state-of-the-art PCFG parser (Charniak, 2000) has achieved the highest PARSEVAL LP/LR when compared to Collins’ Model 2 and Model 3 (Collins, 1999), Roark’s (Roark, 2001), Ratnaparkhi’s (Ratnaparkhi, 1999), and Xu & Chelba’s (Xu et al., 2002) parsers. Hence, we will compare our best loosely integrated and tightly integrated SCDG parsers to Charniak’s parser. Additionally, we will compare with Collins’ Model ≤ 40 words (2,245 sentences) Tagging governor only all roles Acc. RLP RLR RLP RLR 96.2 91.5 91.2 88.0 87.4 96.7 91.9 91.5 88.3 87.7 96.9 92.2 91.7 88.6 88.1 97.2 92.4 92.3 89.1 88.6 97.4 93.2 92.9 89.8 89.2 ≤ 100 words (2,416 sentences) Tagging governor only all roles Acc. RLP RLR RLP RLR 95.4 90.9 90.5 87.5 86.8 95.8 91.3 90.8 87.7 87.0 96.0 91.7 91.2 88.0"
W04-0307,W02-1031,1,0.950132,"[Need3] < MX[Need1] < MX[Need2] MC need role constraints The SuperARV of the word ""did"": C Figure 1: An example of a CDG parse and the SuperARV of the word did in the sentence what did you learn. PX and MX([R]) represent the position of a word and its modifiee (for role R), respectively. Note that CDG parse information can be easily lexicalized at the word level. This lexicalization is able to include not only lexical category and syntactic constraints, but also a rich set of lexical features to model subcategorization and wh-movement without a combinatorial explosion of the parametric space (Wang and Harper, 2002). CDG can distinguish between adjuncts and complements due to the use of need roles (Harper and Helzerman, 1995), is more powerful than CFG, and has the ability to model languages with crossing dependencies and free word ordering (hence, this research could be applicable to a wide variety of languages). An almost-parsing LM based on CDG has been developed in (Wang and Harper, 2002). The underlying hidden event of this LM is a SuperARV. A SuperARV is formally defined as a four-tuple for a word, hC, F , (R, L, U C, M C)+, DCi, where C is the lexical category of the word, F = {F name1 = F value1"
W04-0307,P02-1025,0,0.0125207,"n in Tables 2 and 3. These results are consistent with the observations of (Collins, 1999) and (Eisner, 1996). It is important to note that each of the factors contributing to improved parse accuracy in these two experiments also improved the word prediction capability of the corresponding parser-based LM (Wang and Harper, 2003). 5.2 Comparing to Other Parsers Charniak’s state-of-the-art PCFG parser (Charniak, 2000) has achieved the highest PARSEVAL LP/LR when compared to Collins’ Model 2 and Model 3 (Collins, 1999), Roark’s (Roark, 2001), Ratnaparkhi’s (Ratnaparkhi, 1999), and Xu & Chelba’s (Xu et al., 2002) parsers. Hence, we will compare our best loosely integrated and tightly integrated SCDG parsers to Charniak’s parser. Additionally, we will compare with Collins’ Model ≤ 40 words (2,245 sentences) Tagging governor only all roles Acc. RLP RLR RLP RLR 96.2 91.5 91.2 88.0 87.4 96.7 91.9 91.5 88.3 87.7 96.9 92.2 91.7 88.6 88.1 97.2 92.4 92.3 89.1 88.6 97.4 93.2 92.9 89.8 89.2 ≤ 100 words (2,416 sentences) Tagging governor only all roles Acc. RLP RLR RLP RLR 95.4 90.9 90.5 87.5 86.8 95.8 91.3 90.8 87.7 87.0 96.0 91.7 91.2 88.0 87.4 96.3 91.8 91.5 88.5 87.8 96.6 92.6 92.2 89.1 88.5 2 since it makes"
W04-0307,J03-4003,0,\N,Missing
W04-3209,J96-1002,0,0.159614,"2 Maxent Posterior Probability Model As observed, HMM training does not maximize the posterior probabilities of the correct labels. This mismatch between training and use of the model as a classifier would not arise if the model directly estimated the posterior boundary label probabilities P (ei jW; F ). A second problem with HMMs is that the underlying N-gram sequence model does not cope well with multiple representations (features) of the word sequence (words, POS, etc.) short of building a joint model of all variables. This type of situation is well-suited to a maximum entropy formulation (Berger et al., 1996), which allows conditioning features to apply simultaneously, and therefore gives greater freedom in choosing representations. Another desirable characteristic of maxent models is that they do not split the data recursively to condition their probability estimates, which makes them more robust than decision trees when training data is limited. 4.2.1 Model Formulation and Training We built a posterior probability model for sentence boundary classification in the maxent framework. Such a model takes the familiar exponential form4 P (ejW; F ) = 1 Z (W; F ) e P k k gk (e;W;F ) (3) where Z (W; F"
W04-3209,A00-1031,0,0.0267287,"condition allows use of the correct word transcripts. This condition allows us to study the segmentation task without the confounding effect of speech recognition errors, using perfect lexical information. 3 Features and Knowledge Sources Words and sentence boundaries are mutually constrained via syntactic structure. Therefore, the word identities themselves (from automatic recognition or human transcripts) constitute a primary knowledge source for the sentence segmentation task. We also make use of various automatic taggers that map the word sequence to other representations. The TnT tagger (Brants, 2000) is used to obtain part-ofspeech (POS) tags. A TBL chunker trained on Wall Street Journal corpus (Ngai and Florian, 2001) maps each word to an associated chunk tag, encoding chunk type and relative word position (beginning of an NP, inside a VP, etc.). The tagged versions of the word stream are provided to allow generalizations based on syntactic structure and to smooth out possibly undertrained word-based probability esti1 This is the same as simple per-event classification accuracy, except that the denominator counts only the “marked” events, thereby yielding error rates that are much higher"
W04-3209,J92-4003,0,0.145144,"k type and relative word position (beginning of an NP, inside a VP, etc.). The tagged versions of the word stream are provided to allow generalizations based on syntactic structure and to smooth out possibly undertrained word-based probability esti1 This is the same as simple per-event classification accuracy, except that the denominator counts only the “marked” events, thereby yielding error rates that are much higher than if one uses all potential boundary locations. mates. For the same reasons we also generate word class labels that are automatically induced from bigram word distributions (Brown et al., 1992). To model the prosodic structure of sentence boundaries, we extract several hundred features around each word boundary. These are based on the acoustic alignments produced by a speech recognizer (or forced alignments of the true words when given). The features capture duration, pitch, and energy patterns associated with the word boundaries. Informative features include the pause duration at the boundary, the difference in pitch before and after the boundary, and so on. A crucial aspect of many of these features is that they are highly correlated (e.g., by being derived from the same raw measu"
W04-3209,W02-1002,0,0.0243102,"Missing"
W04-3209,A94-1013,0,0.242823,"s in the maximum entropy (maxent) framework. Both models combine lexical, syntactic, and prosodic information. We develop a technique for integrating pretrained probability models into the maxent framework, and show that this approach can improve on an HMM-based state-of-the-art system for the sentence-boundary detection task. An even more substantial improvement is obtained by combining the posterior probabilities of the two systems. 1 Introduction Sentence boundary detection is a problem that has received limited attention in the text-based computational linguistics community (Schmid, 2000; Palmer and Hearst, 1994; Reynar and Ratnaparkhi, 1997), but which has recently acquired renewed importance through an effort by the DARPA EARS program (DARPA Information Processing Technology Office, 2003) to improve automatic speech transcription technology. Since standard speech recognizers output an unstructured stream of words, improving transcription means not only that word accuracy must be improved, but also that commonly used structural features such as sentence boundaries need to be recognized. The task is thus fundamentally based on both acoustic and textual (via automatic word recognition) information. Fr"
W04-3209,A97-1004,0,0.274153,"(maxent) framework. Both models combine lexical, syntactic, and prosodic information. We develop a technique for integrating pretrained probability models into the maxent framework, and show that this approach can improve on an HMM-based state-of-the-art system for the sentence-boundary detection task. An even more substantial improvement is obtained by combining the posterior probabilities of the two systems. 1 Introduction Sentence boundary detection is a problem that has received limited attention in the text-based computational linguistics community (Schmid, 2000; Palmer and Hearst, 1994; Reynar and Ratnaparkhi, 1997), but which has recently acquired renewed importance through an effort by the DARPA EARS program (DARPA Information Processing Technology Office, 2003) to improve automatic speech transcription technology. Since standard speech recognizers output an unstructured stream of words, improving transcription means not only that word accuracy must be improved, but also that commonly used structural features such as sentence boundaries need to be recognized. The task is thus fundamentally based on both acoustic and textual (via automatic word recognition) information. From a computational linguistics"
W04-3209,N01-1006,0,\N,Missing
W09-3019,P03-1010,0,0.0268446,"Missing"
W09-3019,P01-1017,0,0.0441748,"our grammars of Japanese and Chinese do not currently.; (5) a logic2 label (L2) for Chinese and English, which represents PropBank, NomBank and Penn Discourse Treebank relations; and (6) Asterisks (*) indicate transparent relations, relations where the functor inherits semantic properties of certain special arguments (*CONJ, *OBJ, *PRD, *COMP). GLARF relations are generated from treebank and parses for English, Chinese and Japanese. Our evaluation of system output for these input types requires consideration of multiple correct answers.1 1 Introduction Systems, such as treebank-based parsers (Charniak, 2001; Collins, 1999) and semantic role labelers (Gildea and Jurafsky, 2002; Xue, 2008), are trained and tested on hand-annotated data. Evaluation is based on differences between system output and test data. Other systems use these programs to perform tasks unrelated to the original annotation. For example, participating systems in CONLL (Surdeanu et al., 2008; Hajiˇc et al., 2009), ACE and GALE tasks merged the results of several processors (parsers, named entity recognizers, etc.) not initially designed for the task at hand. This paper discusses differences between handannotated data and automati"
W09-3019,J02-3001,0,0.015508,"a logic2 label (L2) for Chinese and English, which represents PropBank, NomBank and Penn Discourse Treebank relations; and (6) Asterisks (*) indicate transparent relations, relations where the functor inherits semantic properties of certain special arguments (*CONJ, *OBJ, *PRD, *COMP). GLARF relations are generated from treebank and parses for English, Chinese and Japanese. Our evaluation of system output for these input types requires consideration of multiple correct answers.1 1 Introduction Systems, such as treebank-based parsers (Charniak, 2001; Collins, 1999) and semantic role labelers (Gildea and Jurafsky, 2002; Xue, 2008), are trained and tested on hand-annotated data. Evaluation is based on differences between system output and test data. Other systems use these programs to perform tasks unrelated to the original annotation. For example, participating systems in CONLL (Surdeanu et al., 2008; Hajiˇc et al., 2009), ACE and GALE tasks merged the results of several processors (parsers, named entity recognizers, etc.) not initially designed for the task at hand. This paper discusses differences between handannotated data and automatically generated data with respect to our GLARFers, systems for generat"
W09-3019,J08-2004,1,0.84166,"inese and English, which represents PropBank, NomBank and Penn Discourse Treebank relations; and (6) Asterisks (*) indicate transparent relations, relations where the functor inherits semantic properties of certain special arguments (*CONJ, *OBJ, *PRD, *COMP). GLARF relations are generated from treebank and parses for English, Chinese and Japanese. Our evaluation of system output for these input types requires consideration of multiple correct answers.1 1 Introduction Systems, such as treebank-based parsers (Charniak, 2001; Collins, 1999) and semantic role labelers (Gildea and Jurafsky, 2002; Xue, 2008), are trained and tested on hand-annotated data. Evaluation is based on differences between system output and test data. Other systems use these programs to perform tasks unrelated to the original annotation. For example, participating systems in CONLL (Surdeanu et al., 2008; Hajiˇc et al., 2009), ACE and GALE tasks merged the results of several processors (parsers, named entity recognizers, etc.) not initially designed for the task at hand. This paper discusses differences between handannotated data and automatically generated data with respect to our GLARFers, systems for generating Grammati"
W09-3019,W09-1201,1,0.825301,"Missing"
W09-3019,D09-1087,1,0.855247,"Missing"
W09-3019,P06-2055,1,0.900802,"Missing"
W09-3019,W08-2121,1,\N,Missing
W09-3019,J03-4003,0,\N,Missing
W09-3019,W04-0413,1,\N,Missing
W09-3019,W09-2423,1,\N,Missing
W10-0732,P07-1073,0,0.0419445,"n between two textual entity mentions. Slot filling, a general form of relation extraction, includes relations between nonentities, such as a person and an occupation, age, or cause of death (McNamee and Dang, 2009). RE annotated data, such as ACE (2008), is expensive to produce so systems take different approaches to minimizing data needs. For example, tree kernels can reduce feature sparsity and generalize across many examples (GuoDong et al., 2007; Zhou et al., 2009). Distant supervision automatically generates noisy training examples from a knowledge base (KB) without needing annotations (Bunescu and Mooney, 2007; Mintz et al., 2009). While this method can quickly generate training data, it also generates many false examples. We reduce the noise in such examples by using Amazon Mechanical Turk (MTurk), which has been shown to produce Automatic generation of noisy examples To create noisy examples we use a similar approach to Mintz et al. (2009). We extract relations from a KB in the form of tuples, (e, r, v), where e is an entity, v is a value, and r is a relation that holds between them; for example (J.R.R. Tolkien, occupation, author). Our KB is Freebase1 , an online database of structured informati"
W10-0732,D07-1076,0,0.0340232,"e also present results on inter-annotator agreement. 1 2 Method 2.1 Introduction Relation extraction (RE) is the task of determining the existence and type of relation between two textual entity mentions. Slot filling, a general form of relation extraction, includes relations between nonentities, such as a person and an occupation, age, or cause of death (McNamee and Dang, 2009). RE annotated data, such as ACE (2008), is expensive to produce so systems take different approaches to minimizing data needs. For example, tree kernels can reduce feature sparsity and generalize across many examples (GuoDong et al., 2007; Zhou et al., 2009). Distant supervision automatically generates noisy training examples from a knowledge base (KB) without needing annotations (Bunescu and Mooney, 2007; Mintz et al., 2009). While this method can quickly generate training data, it also generates many false examples. We reduce the noise in such examples by using Amazon Mechanical Turk (MTurk), which has been shown to produce Automatic generation of noisy examples To create noisy examples we use a similar approach to Mintz et al. (2009). We extract relations from a KB in the form of tuples, (e, r, v), where e is an entity, v i"
W10-0732,P09-1113,0,0.0861627,"ty mentions. Slot filling, a general form of relation extraction, includes relations between nonentities, such as a person and an occupation, age, or cause of death (McNamee and Dang, 2009). RE annotated data, such as ACE (2008), is expensive to produce so systems take different approaches to minimizing data needs. For example, tree kernels can reduce feature sparsity and generalize across many examples (GuoDong et al., 2007; Zhou et al., 2009). Distant supervision automatically generates noisy training examples from a knowledge base (KB) without needing annotations (Bunescu and Mooney, 2007; Mintz et al., 2009). While this method can quickly generate training data, it also generates many false examples. We reduce the noise in such examples by using Amazon Mechanical Turk (MTurk), which has been shown to produce Automatic generation of noisy examples To create noisy examples we use a similar approach to Mintz et al. (2009). We extract relations from a KB in the form of tuples, (e, r, v), where e is an entity, v is a value, and r is a relation that holds between them; for example (J.R.R. Tolkien, occupation, author). Our KB is Freebase1 , an online database of structured information, and our corpus is"
W10-0732,D08-1027,0,0.0515835,"Missing"
W97-0124,H94-1049,0,0.0127931,"nd have focused on various aspects of the unknown words. Statistical methods are most commonly used in part-of-speech tagging. Charniak&apos;s paper [5] outlines the use of statistical equations in part-of-speech tagging. Tagging systems make only limited use of the syntactic knowledge inherent in the sentence, in contrast to parsers. An ngram tagger concentrates on the n neighbors of a word (where n tends to be 2 or 3), ignoring the global sentence structure. Also, many part-of-speech tagging systems are only concerned with resolving ambiguity, not dealing with unknown words. Kupiec [8] and Brill [3] make use of morphology to handle unknown words during part-ofspeech tagging. Brill&apos;s tagger begins by tagging unknown words as proper nouns if capitalized, common nouns if not. Then the tagger learns various transformational rules by training on a tagged corpus. It applies these rules to unknown words to tag them with the appropriate part-ofspeech information. Kupiec&apos;s hidden Markov model uses a set of suffixes to assign probabilities and state transformations to unknown words. Both these methods work well, but they ignore the global syntactic content of the sentence. We will examine the effe"
W97-0124,P96-1005,0,0.0251327,"tanding how people learn a language. For example, Muysken [11] studied the classification of affixes according to their order of application for a theoretical discussion on morphology. Badecker and Caramazza [2] discussed the distinction between inflectional and derivational morphology as it applies to acquired language deficit disorder, and, in general, to the theory of language learning. Baayen and Lieber [1] studied the productivity of certain English affixes in the CELEX lexical database, in an effort to study the differences between frequency of appearance and productivity. Viegas, et al [14] show that the use of lexical rules and morphological generation can greatly aid in the task of lexical acquisition. However, morphological generation involves the construction of new word forms by applying rules of afiixation to base forms, and so it is only indirectly helpful in the analysis of unknown words. Morphological reconstruction researchers process an unknown word by using knowledge of the root stem and affixes of that word. For example, Milne [10] makes use of morphological reconstruction to resolve lexical ambiguity while parsing. Light [9] uses morphological cues to determine sem"
W97-0124,J93-2006,0,0.427344,"words should help to refine the possibilities for an unknown word and enhance the information provided by the syntactic knowledge. Morphological recognition can also be helpful in predicting possible parts of speech for many unknown words. We expect that these three knowledge sources will greatly improve our parser&apos;s ability to process and cope with words that are not in the system lexicon. 2 Problem Description A major problem that can occur when parsing sentences is the appearance of unknown words-words that are not contained in the lexicon of the system. As mentioned in Weischedel, et al. [15], the best-performing system at the Second Message Understanding Conference (MUC-2) simply 261 halted parsing when an unknown word was encountered. Clearly, for a parser to be considered robust it must have mechanisms to process unknown words. The need to cope with unknown words will continue to grow as new words are coined, and words associated with sub-cultures leak into the main-stream vocabulary. In the case of a large corpus, especially one with no specific domain, a comprehensive lexicon is prohibitive. Thus, it is important that parsers have the ability to cope with new words. There are"
W97-0124,J86-1001,0,\N,Missing
