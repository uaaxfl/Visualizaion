2021.socialnlp-1.3,A Case Study of In-House Competition for Ranking Constructive Comments in a News Service,2021,-1,-1,11,0,1077,hayato kobayashi,Proceedings of the Ninth International Workshop on Natural Language Processing for Social Media,0,"Ranking the user comments posted on a news article is important for online news services because comment visibility directly affects the user experience. Research on ranking comments with different metrics to measure the comment quality has shown {``}constructiveness{''} used in argument analysis is promising from a practical standpoint. In this paper, we report a case study in which this constructiveness is examined in the real world. Specifically, we examine an in-house competition to improve the performance of ranking constructive comments and demonstrate the effectiveness of the best obtained model for a commercial service."
2021.findings-emnlp.383,Co-Teaching Student-Model through Submission Results of Shared Task,2021,-1,-1,5,0,7365,kouta nakayama,Findings of the Association for Computational Linguistics: EMNLP 2021,0,"Shared tasks have a long history and have become the mainstream of NLP research. Most of the shared tasks require participants to submit only system outputs and descriptions. It is uncommon for the shared task to request submission of the system itself because of the license issues and implementation differences. Therefore, many systems are abandoned without being used in real applications or contributing to better systems. In this research, we propose a scheme to utilize all those systems which participated in the shared tasks. We use all participated system outputs as task teachers in this scheme and develop a new model as a student aiming to learn the characteristics of each system. We call this scheme {``}Co-Teaching.{''} This scheme creates a unified system that performs better than the task{'}s single best system. It only requires the system outputs, and slightly extra effort is needed for the participants and organizers. We apply this scheme to the {``}SHINRA2019-JP{''} shared task, which has nine participants with various output accuracies, confirming that the unified system outperforms the best system. Moreover, the code used in our experiments has been released."
2020.lrec-1.150,Multi-class Multilingual Classification of {W}ikipedia Articles Using Extended Named Entity Tag Set,2020,-1,-1,2,0,8886,hassan shavarani,Proceedings of the 12th Language Resources and Evaluation Conference,0,"Wikipedia is a great source of general world knowledge which can guide NLP models better understand their motivation to make predictions. Structuring Wikipedia is the initial step towards this goal which can facilitate fine-grain classification of articles. In this work, we introduce the Shinra 5-Language Categorization Dataset (SHINRA-5LDS), a large multi-lingual and multi-labeled set of annotated Wikipedia articles in Japanese, English, French, German, and Farsi using Extended Named Entity (ENE) tag set. We evaluate the dataset using the best models provided for ENE label set classification and show that the currently available classification models struggle with large datasets using fine-grained tag sets."
W19-4804,Can Neural Networks Understand Monotonicity Reasoning?,2019,33,1,5,0,7510,hitomi yanaka,Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP,0,"Monotonicity reasoning is one of the important reasoning skills for any intelligent natural language inference (NLI) model in that it requires the ability to capture the interaction between lexical and syntactic structures. Since no test set has been developed for monotonicity reasoning with wide coverage, it is still unclear whether neural models can perform monotonicity reasoning in a proper way. To investigate this issue, we introduce the Monotonicity Entailment Dataset (MED). Performance by state-of-the-art NLI models on the new test set is substantially worse, under 55{\%}, especially on downward reasoning. In addition, analysis using a monotonicity-driven data augmentation method showed that these models might be limited in their generalization ability in upward and downward reasoning."
W19-4433,Analytic Score Prediction and Justification Identification in Automated Short Answer Scoring,2019,0,0,6,0,22506,tomoya mizumoto,Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications,0,"This paper provides an analytical assessment of student short answer responses with a view to potential benefits in pedagogical contexts. We first propose and formalize two novel analytical assessment tasks: analytic score prediction and justification identification, and then provide the first dataset created for analytic short answer scoring research. Subsequently, we present a neural baseline model and report our extensive empirical results to demonstrate how our dataset can be used to explore new and intriguing technical challenges in short answer scoring. The dataset is publicly available for research purposes."
S19-1027,{HELP}: A Dataset for Identifying Shortcomings of Neural Models in Monotonicity Reasoning,2019,24,3,5,0,7510,hitomi yanaka,Proceedings of the Eighth Joint Conference on Lexical and Computational Semantics (*{SEM} 2019),0,"Large crowdsourced datasets are widely used for training and evaluating neural models on natural language inference (NLI). Despite these efforts, neural models have a hard time capturing logical inferences, including those licensed by phrase replacements, so-called monotonicity reasoning. Since no large dataset has been developed for monotonicity reasoning, it is still unclear whether the main obstacle is the size of datasets or the model architectures themselves. To investigate this issue, we introduce a new dataset, called HELP, for handling entailments with lexical and logical phenomena. We add it to training data for the state-of-the-art neural models and evaluate them on test sets for monotonicity phenomena. The results showed that our data augmentation improved the overall accuracy. We also find that the improvement is better on monotonicity inferences with lexical replacements than on downward inferences with disjunction and modification. This suggests that some types of inferences can be improved by our data augmentation while others are immune to it."
D19-1054,Select and Attend: Towards Controllable Content Selection in Text Generation,2019,0,4,6,0,5981,xiaoyu shen,Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),0,"Many text generation tasks naturally contain two steps: content selection and surface realization. Current neural encoder-decoder models conflate both steps into a black-box architecture. As a result, the content to be described in the text cannot be explicitly controlled. This paper tackles this problem by decoupling content selection from the decoder. The decoupled content selection is human interpretable, whose value can be manually manipulated to control the content of generated text. The model can be trained end-to-end without human annotations by maximizing a lower bound of the marginal likelihood. We further propose an effective way to trade-off between performance and controllability with a single adjustable hyperparameter. In both data-to-text and headline generation tasks, our model achieves promising results, paving the way for controllable content selection in text generation."
D19-1357,Bridging the Defined and the Defining: Exploiting Implicit Lexical Semantic Relations in Definition Modeling,2019,0,0,2,0,12295,koki washio,Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),0,"Definition modeling includes acquiring word embeddings from dictionary definitions and generating definitions of words. While the meanings of defining words are important in dictionary definitions, it is crucial to capture the lexical semantic relations between defined words and defining words. However, thus far, the utilization of such relations has not been explored for definition modeling. In this paper, we propose definition modeling methods that use lexical semantic relations. To utilize implicit semantic relations in definitions, we use unsupervisedly obtained pattern-based word-pair embeddings that represent semantic relations of word pairs. Experimental results indicate that our methods improve the performance in learning embeddings from definitions, as well as definition generation."
D18-1453,What Makes Reading Comprehension Questions Easier?,2018,0,26,3,0,5181,saku sugawara,Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,0,"A challenge in creating a dataset for machine reading comprehension (MRC) is to collect questions that require a sophisticated understanding of language to answer beyond using superficial cues. In this work, we investigate what makes questions easier across recent 12 MRC datasets with three question styles (answer extraction, description, and multiple choice). We propose to employ simple heuristics to split each dataset into easy and hard subsets and examine the performance of two baseline models for each of the subsets. We then manually annotate questions sampled from each subset with both validity and requisite reasoning skills to investigate which skills explain the difference between easy and hard questions. From this study, we observed that (i) the baseline performances for the hard subsets remarkably degrade compared to those of entire datasets, (ii) hard questions require knowledge inference and multiple-sentence reasoning in comparison with easy questions, and (iii) multiple-choice questions tend to require a broader range of reasoning skills than answer extraction and description questions. These results suggest that one might overestimate recent advances in MRC."
C18-1060,An Empirical Study on Fine-Grained Named Entity Recognition,2018,0,3,7,0,30768,khai mai,Proceedings of the 27th International Conference on Computational Linguistics,0,"Named entity recognition (NER) has attracted a substantial amount of research. Recently, several neural network-based models have been proposed and achieved high performance. However, there is little research on fine-grained NER (FG-NER), in which hundreds of named entity categories must be recognized, especially for non-English languages. It is still an open question whether there is a model that is robust across various settings or the proper model varies depending on the language, the number of named entity categories, and the size of training datasets. This paper first presents an empirical comparison of FG-NER models for English and Japanese and demonstrates that LSTM+CNN+CRF (Ma and Hovy, 2016), one of the state-of-the-art methods for English NER, also works well for English FG-NER but does not work well for Japanese, a language that has a large number of character types. To tackle this problem, we propose a method to improve the neural network-based Japanese FG-NER performance by removing the CNN layer and utilizing dictionary and category embeddings. Experiment results show that the proposed method improves Japanese FG-NER F-score from 66.76{\%} to 75.18{\%}."
P17-4007,Extended Named Entity Recognition {API} and Its Applications in Language Education,2017,5,4,8,0,712,tuan nguyen,"Proceedings of {ACL} 2017, System Demonstrations",0,None
Y16-3027,Neural Joint Learning for Classifying {W}ikipedia Articles into Fine-grained Named Entity Types,2016,20,3,3,0,33283,masatoshi suzuki,"Proceedings of the 30th Pacific Asia Conference on Language, Information and Computation: Posters",0,None
W16-4405,An Entity-Based approach to Answering Recurrent and Non-Recurrent Questions with Past Answers,2016,-1,-1,3,0,5382,anietie andy,Proceedings of the Open Knowledge Base and Question Answering Workshop ({OKBQA} 2016),0,"An Entity-based approach to Answering recurrent and non-recurrent questions with Past Answers Abstract Community question answering (CQA) systems such as Yahoo! Answers allow registered-users to ask and answer questions in various question categories. However, a significant percentage of asked questions in Yahoo! Answers are unanswered. In this paper, we propose to reduce this percentage by reusing answers to past resolved questions from the site. Specifically, we propose to satisfy unanswered questions in entity rich categories by searching for and reusing the best answers to past resolved questions with shared needs. For unanswered questions that do not have a past resolved question with a shared need, we propose to use the best answer to a past resolved question with similar needs. Our experiments on a Yahoo! Answers dataset shows that our approach retrieves most of the past resolved questions that have shared and similar needs to unanswered questions."
W16-3909,Name Variation in Community Question Answering Systems,2016,2,0,2,0,5382,anietie andy,Proceedings of the 2nd Workshop on Noisy User-generated Text ({WNUT}),0,"Name Variation in Community Question Answering Systems Abstract Community question answering systems are forums where users can ask and answer questions in various categories. Examples are Yahoo! Answers, Quora, and Stack Overflow. A common challenge with such systems is that a significant percentage of asked questions are left unanswered. In this paper, we propose an algorithm to reduce the number of unanswered questions in Yahoo! Answers by reusing the answer to the most similar past resolved question to the unanswered question, from the site. Semantically similar questions could be worded differently, thereby making it difficult to find questions that have shared needs. For example, {``}Who is the best player for the Reds?{''} and {``}Who is currently the biggest star at Manchester United?{''} have a shared need but are worded differently; also, {``}Reds{''} and {``}Manchester United{''} are used to refer to the soccer team Manchester United football club. In this research, we focus on question categories that contain a large number of named entities and entity name variations. We show that in these categories, entity linking can be used to identify relevant past resolved questions with shared needs as a given question by disambiguating named entities and matching these questions based on the disambiguated entities, identified entities, and knowledge base information related to these entities. We evaluated our algorithm on a new dataset constructed from Yahoo! Answers. The dataset contains annotated question pairs, (Qgiven, [Qpast, Answer]). We carried out experiments on several question categories and show that an entity-based approach gives good performance when searching for similar questions in entity rich categories."
W15-2907,Utilizing review analysis to suggest product advertisement improvements,2015,27,1,3,0,36875,takaaki tsunoda,"Proceedings of the 6th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis",0,"On an e-commerce site, product blurbs (short promotional statements) and user reviews give us a lot of information about products. While a blurb should be appealing to encourage more users to click on a product link, sometimes sellers may miss or misunderstand which aspects of the product are important to their users. We therefore propose a novel task: suggesting aspects of products for an advertisement improvement. As reviews have a lot of information about aspects from the perspective of users, review analysis enables us to suggest aspects that could attract more users. To achieve this, we break this task into the following two subtasks: aspect grouping and aspect group ranking. Aspect grouping enables us to treat product aspects at the semantic level rather than expression level. Aspect group ranking allows us to show users only aspects important for them. On the basis of experimental results using travel domain hotel data, we show that our proposed solution accomplishes NDCG@3 score of 0.739, which shows our solution is effective in achieving our goal."
C14-2009,Lightweight Client-Side {C}hinese/{J}apanese Morphological Analyzer Based on Online Learning,2014,9,4,2,1,752,masato hagiwara,"Proceedings of {COLING} 2014, the 25th International Conference on Computational Linguistics: System Demonstrations",0,"As mobile devices and Web applications become popular, lightweight, client-side language analysis is more important than ever. We propose Rakuten MA, a Chinese/Japanese morphological analyzer written in JavaScript. It employs an online learning algorithm SCW, which enables client-side model update and domain adaptation. We have achieved a compact model size (5MB) while maintaining the state-of-the-art performance, via techniques such as feature hashing, FOBOS, and feature quantization."
P13-2033,Accurate Word Segmentation using Transliteration and Language Model Projection,2013,20,2,2,1,752,masato hagiwara,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"Transliterated compound nouns not separated by whitespaces pose difficulty on word segmentation (WS). Offline approaches have been proposed to split them using word statistics, but they rely on static lexicon, limiting their use. We propose an online approach, integrating source LM, and/or, back-transliteration and English LM. The experiments on Japanese and Chinese WS have shown that the proposed models achieve significant improvement over state-of-the-art, reducing 16% errors in Japanese."
I13-1190,Unsupervised Extraction of Attributes and Their Values from Product Description,2013,18,12,2,0,15889,keiji shinzato,Proceedings of the Sixth International Joint Conference on Natural Language Processing,0,None
W12-4805,phloat : Integrated Writing Environment for {ESL} learners,2012,12,0,3,0,13659,yuta hayashibe,Proceedings of the Second Workshop on Advances in Text Input Methods,0,"present (xe8xb4x88xe3x82x8b;okuru) send (xe9x80x81xe3x82x8b;okuru) live (xe9x80x81xe3x82x8b;okuru) After the user chooses one of the candidate phrases, the system subsequently suggests candidates to fill in the slots of the phrase. These slot candidates are generated from the context of the suggested English phrase and its Japanese translation. This enables users to complete a long expression just by choosing a phrase and filling out the blanks in it. Phrase Suggestion"
W12-4404,Latent Semantic Transliteration using {D}irichlet Mixture,2012,15,4,2,1,752,masato hagiwara,Proceedings of the 4th Named Entity Workshop ({NEWS}) 2012,0,"Transliteration has been usually recognized by spelling-based supervised models. However, a single model cannot deal with mixture of words with different origins, such as get in piaget and target. Li et al. (2007) propose a class transliteration method, which explicitly models the source language origins and switches them to address this issue. In contrast to their model which requires an explicitly tagged training corpus with language origins, Hagiwara and Sekine (2011) have proposed the latent class transliteration model, which models language origins as latent classes and train the transliteration table via the EM algorithm. However, this model, which can be formulated as unigram mixture, is prone to overfitting since it is based on maximum likelihood estimation. We propose a novel latent semantic transliteration model based on Dirichlet mixture, where a Dirichlet mixture prior is introduced to mitigate the overfitting problem. We have shown that the proposed method considerably outperform the conventional transliteration models."
N12-3004,{S}urf{S}hop: combing a product ontology with topic model results for online window-shopping.,2012,5,2,2,0,42792,zofia stankiewicz,Proceedings of the Demonstration Session at the Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"At present, online shopping is typically a search-oriented activity where a user gains access to products which best match their query. Instead, we propose a surf-oriented online shopping paradigm, which links associated products allowing users to wander around the online store and enjoy browsing a variety of items. As an initial step in creating this experience, we constructed a prototype of an online shopping interface which combines product ontology information with topic model results to allow users to explore items from the food and kitchen domain. As a novel task for topic model application, we also discuss possible approaches to the task of selecting the best product categories to illustrate the hidden topics discovered for our product domain."
P11-2010,Latent Class Transliteration based on Source Language Origin,2011,7,8,2,1,752,masato hagiwara,Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,0,"Transliteration, a rich source of proper noun spelling variations, is usually recognized by phonetic- or spelling-based models. However, a single model cannot deal with different words from different language origins, e.g., get in piaget and target. Li et al. (2007) propose a method which explicitly models and classifies the source language origins and switches transliteration models accordingly. This model, however, requires an explicitly tagged training set with language origins. We propose a novel method which models language origins as latent classes. The parameters are learned from a set of transliterated word pairs via the EM algorithm. The experimental results of the transliteration task of Western names to Japanese show that the proposed model can achieve higher accuracy compared to the conventional models without latent classes."
P11-1053,Semi-supervised Relation Extraction with Large-scale Word Clustering,2011,29,80,3,0,40578,ang sun,Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,1,"We present a simple semi-supervised relation extraction system with large-scale word clustering. We focus on systematically exploring the effectiveness of different cluster-based features. We also propose several statistical methods for selecting clusters at an appropriate level of granularity. When training on different sizes of data, our semi-supervised approach consistently outperformed a state-of-the-art supervised baseline system."
sekine-dalwani-2010-ngram,"Ngram Search Engine with Patterns Combining Token, {POS}, Chunk and {NE} Information",2010,5,3,1,1,1087,satoshi sekine,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"We developed a search tool for ngrams extracted from a very large corpus (the current system uses the entire Wikipedia, which has 1.7 billion tokens). The tool supports queries with an arbitrary number of wildcards and/or specification by a combination of token, POS, chunk (such as NP, VP, PP) and Named Entity (NE). The previous system (Sekine 08) can only handle tokens and unrestricted wildcards in the query, such as Â* was established in *Â. However, being able to constrain the wildcards by POS, chunk or NE is quite useful to filter out noise. For example, the new system can search for ÂNE=COMPANY was established in POS=CDÂ. This finer specification reduces the number of outputs to less than half and avoids the ngrams which have a comma or a common noun at the first position or location information at the last position. It outputs the matched ngrams with their frequencies as well as all the contexts (i.e. sentences, KWIC lists and document ID information) where the matched ngrams occur in the corpus. It takes a fraction of a second for a search on a single CPU Linux-PC (1GB memory and 500GB disk) environment."
lin-etal-2010-new,New Tools for Web-Scale N-grams,2010,32,68,4,0,30549,dekang lin,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"While the web provides a fantastic linguistic resource, collecting and processing data at web-scale is beyond the reach of most academic laboratories. Previous research has relied on search engines to collect online information, but this is hopelessly inefficient for building large-scale linguistic resources, such as lists of named-entity types or clusters of distributionally similar words. An alternative to processing web-scale text directly is to use the information provided in an N-gram corpus. An N-gram corpus is an efficient compression of large amounts of text. An N-gram corpus states how often each sequence of words (up to length N) occurs. We propose tools for working with enhanced web-scale N-gram corpora that include richer levels of source annotation, such as part-of-speech tags. We describe a new set of search tools that make use of these tags, and collectively lower the barrier for lexical learning and ambiguity resolution at web-scale. They will allow novel sources of information to be applied to long-standing natural language challenges."
sekine-2008-extended,Extended Named Entity Ontology with Attribute Information,2008,10,48,1,1,1087,satoshi sekine,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"Named Entities (NE) are regarded as an important type of semantic knowledge in many natural language processing (NLP) applications. Originally, a limited number of NE categories were proposed. In MUC, it was 7 categories - people, organization, location, time, date, money and percentage expressions. However, it was noticed that such a limited number of NE categories is too small for many applications. The author has proposed Extended Named Entity (ENE), which has about 200 categories (Sekine and Nobata 04). During the development of ENE, we noticed that many ENE categories have specific attributes, and those provide very important information for the entities. For example, ÂriversÂ have attributes like Âsource locationÂ, ÂoutflowÂ, and ÂlengthÂ. Some such information is essential to Âknowing aboutÂ the river, while the name is only a label which can be used to refer to the river. Also, such attributes are important information for many NLP applications. In this paper, we report on the design of a set of attributes for ENE categories. We used a bottom up approach to creating the knowledge using a Japanese encyclopedia, which contains abundant descriptions of ENE instances."
sadamitsu-etal-2008-sentiment,Sentiment Analysis Based on Probabilistic Models Using Inter-Sentence Information,2008,11,11,2,0,32868,kugatsu sadamitsu,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"This paper proposes a new method of the sentiment analysis utilizing inter-sentence structures especially for coping with reversal phenomenon of word polarity such as quotation of otherÂs opinions on an opposite side. We model these phenomenon using Hidden Conditional Random Fields(HCRFs) with three kinds of features: transition features, polarity features and reversal (of polarity) features. Polarity features and reversal features are doubly added to each word, and each weight of the features are trained by the common structure of positive and negative corpus in, for example, assuming that reversal phenomenon occured for the same reason (features) in both polarity corpus. Our method achieved better accuracy than the Naive Bayes method and as good as SVMs."
C08-3010,A Linguistic Knowledge Discovery Tool: Very Large Ngram Database Search with Arbitrary Wildcards,2008,7,12,1,1,1087,satoshi sekine,Coling 2008: Companion volume: Demonstrations,0,"In this paper, we will describe a search tool for a huge set of ngrams. The tool supports queries with an arbitrary number of wildcards. It takes a fraction of a second for a search, and can provide the fillers of the wildcards. The system runs on a single Linux PC with reasonable size memory (less than 4GB) and disk space (less than 400GB). This system can be a very useful tool for linguistic knowledge discovery and other NLP tasks."
S07-1012,The {S}em{E}val-2007 {W}e{PS} Evaluation: Establishing a benchmark for the Web People Search Task,2007,5,203,3,0,45710,javier artiles,Proceedings of the Fourth International Workshop on Semantic Evaluations ({S}em{E}val-2007),0,"This paper presents the task definition, resources, participation, and comparative results for the Web People Search task, which was organized as part of the SemEval-2007 evaluation exercise. This task consists of clustering a set of documents that mention an ambiguous person name according to the actual entities referred to using that name."
P07-2005,System Demonstration of On-Demand Information Extraction,2007,4,2,1,1,1087,satoshi sekine,Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions,0,"In this paper, we will describe ODIE, the On-Demand Information Extraction system. Given a user's query, the system will produce tables of the salient information about the topic in structured form. It produces the tables in less than one minute without any knowledge engineering by hand, i.e. pattern creation or paraphrase knowledge creation, which was the largest obstacle in traditional IE. This demonstration is based on the idea and technologies reported in (Sekine 06). A substantial speed-up over the previous system (which required about 15 minutes to analyze one year of newspaper) was achieved through a new approach to handling pattern candidates; now less than one minute is required when using 11 years of newspaper corpus. In addition, functionality was added to facilitate investigation of the extracted information."
P06-2094,On-Demand Information Extraction,2006,14,131,1,1,1087,satoshi sekine,Proceedings of the {COLING}/{ACL} 2006 Main Conference Poster Sessions,0,"At present, adapting an Information Extraction system to new topics is an expensive and slow process, requiring some knowledge engineering for each new topic. We propose a new paradigm of Information Extraction which operates 'on demand' in response to a user's query. On-demand Information Extraction (ODIE) aims to completely eliminate the customization effort. Given a user's query, the system will automatically create patterns to extract salient relations in the text of the topic, and build tables from the extracted information using paraphrase discovery technology. It relies on recent advances in pattern discovery, paraphrase discovery, and extended named entity tagging. We report on experimental results in which the system created useful tables for many topics, demonstrating the feasibility of this approach."
N06-2034,Using Phrasal Patterns to Identify Discourse Relations,2006,2,33,3,0,50069,manami saito,"Proceedings of the Human Language Technology Conference of the {NAACL}, Companion Volume: Short Papers",0,"This paper describes a system which identifies discourse relations between two successive sentences in Japanese. On top of the lexical information previously proposed, we used phrasal pattern information. Adding phrasal information improves the system's accuracy 12%, from 53% to 65%."
N06-1039,Preemptive Information Extraction using Unrestricted Relation Discovery,2006,10,195,2,1,48966,yusuke shinyama,"Proceedings of the Human Language Technology Conference of the {NAACL}, Main Conference",0,We are trying to extend the boundary of Information Extraction (IE) systems. Existing IE systems require a lot of time and human effort to tune for a new scenario. Preemptive Information Extraction is an attempt to automatically create all feasible IE systems in advance without human intervention. We propose a technique called Unrestricted Relation Discovery that discovers all possible relations from texts and presents them as tables. We present a preliminary system that obtains reasonably good results.
I05-5011,Automatic Paraphrase Discovery based on Context and Keywords between {NE} Pairs,2005,9,66,1,1,1087,satoshi sekine,Proceedings of the Third International Workshop on Paraphrasing ({IWP}2005),0,"Automatic paraphrase discovery is an important but challenging task. We propose an unsupervised method to discover paraphrases from a large untagged corpus, without requiring any seed phrase or other cue. We focus on phrases which connect two Named Entities (NEs), and proceed in two stages. The first stage identifies a keyword in each phrase and joins phrases with the same keyword into sets. The second stage links sets which involve the same pairs of individual NEs. A total of 13,976 phrases were grouped. The accuracy of the sets in representing paraphrase ranged from 73% to 99%, depending on the NE categories and set sizes; the accuracy of the links for two evaluated domains was 73% and 86%."
I05-2008,A System to Solve Language Tests for Second Grade Students,2005,-1,-1,3,0,50069,manami saito,Companion Volume to the Proceedings of Conference including Posters/Demos and tutorial abstracts,0,None
W04-2208,Multilingual Aligned Parallel Treebank Corpus Reflecting Contextual Information and Its Applications,2004,13,18,5,0.969278,30019,kiyotaka uchimoto,Proceedings of the Workshop on Multilingual Linguistic Resources,0,"This paper describes Japanese-English-Chinese aligned parallel treebank corpora of newspaper articles. They have been constructed by translating each sentence in the Penn Treebank and the Kyoto University text corpus into a corresponding natural sentence in a target language. Each sentence is translated so as to reflect its contextual information and is annotated with morphological and syntactic structures and phrasal alignment. This paper also describes the possible applications of the parallel corpus and proposes a new framework to aid in translation. In this framework, parallel translations whose source language sentence is similar to a given sentence can be semi-automatically generated. In this paper we show that the framework can be achieved by using our aligned parallel treebank corpus."
P04-1053,Discovering Relations among Named Entities from Large Corpora,2004,6,327,2,0,44785,takaaki hasegawa,Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics ({ACL}-04),1,"Discovering the significant relations embedded in documents would be very useful not only for information retrieval but also for question answering and summarization. Prior methods for relation discovery, however, needed large annotated corpora which cost a great deal of time and effort. We propose an unsupervised method for relation discovery from large corpora. The key idea is clustering pairs of named entities according to the similarity of context words intervening between the named entities. Our experiments using one year of newspapers reveals not only that the relations among named entities could be detected with high recall and precision, but also that appropriate labels could be automatically provided for the relations."
sekine-nobata-2004-definition,"Definition, Dictionaries and Tagger for Extended Named Entity Hierarchy",2004,3,146,1,1,1087,satoshi sekine,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,"The tagging of Named Entities, the names of particular things or classes, is regarded as an important component technology for many NLP applications. The first Named Entity set had 7 types, organization, location, person, date, time, money and percent expressions. Later, in the IREX project artifact was added and ACE added two, GPE and facility, to pursue the generalization of the technology. However, 7 or 8 kinds of NE are not broad enough to cover general applications. We proposed about 150 categories of NE (Sekine et al. 2002) and now we have extended it again to 200 categories. Also we have developed dictionaries and an automatic tagger for NEs in Japanese."
ando-etal-2004-automatic,Automatic Extraction of Hyponyms from {J}apanese Newspapers. Using Lexico-syntactic Patterns,2004,4,10,2,0,42195,maya ando,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,"We describe a method to automatically extract hyponyms from Japanese newspapers. First, we discover patterns which can extract hyponyms of a noun, such as A nado-no B (B such as A), then we apply the patterns to the newspaper corpus to extract instances. The procedure works best to extract hyponyms of concrete things in the middle of the word hierarchies. The precision is 49-87 percent depending on the patterns. We compare the extracted hyponyms and those associated by humans. We find that the popular words in the associative concept dictionary are likely to be found in the corpus but also many additional hyponyms can be extracted from 32 years of newspaper articles."
C04-1122,Named Entity Discovery Using Comparable News Articles,2004,7,73,2,1,48966,yusuke shinyama,{COLING} 2004: Proceedings of the 20th International Conference on Computational Linguistics,0,"In this paper we describe a way to discover Named Entities by using the distribution of words in news articles. Named Entity recognition is an important task for today's natural language applications, but it still suffers from data sparseness. We used an observation that a Named Entity is likely to appear synchronously in several news articles, whereas a common noun is less likely. Exploiting this characteristic, we successfully obtained rare Named Entities with 90% accuracy just by comparing time series distributions of a word in two newspapers. Although the achieved recall is not sufficient yet, we believe that this method can be used to strengthen the lexical knowledge of a Named Entity tagger."
C04-1127,Cross-lingual Information Extraction System Evaluation,2004,8,20,2,1,51460,kiyoshi sudo,{COLING} 2004: Proceedings of the 20th International Conference on Computational Linguistics,0,"In this paper, we discuss the performance of cross-lingual information extraction systems employing an automatic pattern acquisition module. This module, which creates extraction patterns starting from a user's narrative task description, allows rapid customization to new extraction tasks. We compare two approaches: (1) acquiring patterns in the source language, performing source language extraction, and then translating the resulting templates to the target language, and (2) translating the texts and performing pattern discovery and extraction in the target language. We demonstrate an average of 8--10% more recall using the first approach. We discuss some of the problems with machine translation and their effect on pattern discovery which lead to this difference in performance."
C04-1176,Automatic Construction of {J}apanese {KATAKANA} Variant List from Large Corpus,2004,3,13,2,0,1084,takeshi masuyama,{COLING} 2004: Proceedings of the 20th International Conference on Computational Linguistics,0,"This paper presents a method to construct Japanese KATAKANA variant list from large corpus. Our method is useful for information retrieval, information extraction, question answering, and so on, because KATAKANA words tend to be used as loan words and the transliteration causes several variations of spelling. Our method consists of three steps. At step 1, our system collects KATAKANA words from large corpus. At step 2, our system collects candidate pairs of KATAKANA variants from the collected KATAKANA words using a spelling similarity which is based on the edit distance. At step 3, our system selects variant pairs from the candidate pairs using a semantic similarity which is calculated by a vector space model of a context of each KATAKANA word. We conducted experiments using 38 years of Japanese newspaper articles and constructed Japanese KATAKANA variant list with the performance of 97.4% recall and 89.1% precision. Estimating from this precision, our system can extract 178,569 variant pairs from the corpus."
W03-1609,Paraphrase Acquisition for Information Extraction,2003,9,65,2,1,48966,yusuke shinyama,Proceedings of the Second International Workshop on Paraphrasing,0,"We are trying to find paraphrases from Japanese news articles which can be used for Information Extraction. We focused on the fact that a single event can be reported in more than one article in different ways. However, certain kinds of noun phrases such as names, dates and numbers behave as anchors which are unlikely to change across articles. Our key idea is to identify these anchors among comparable articles and extract portions of expressions which share the anchors. This way we can extract expressions which convey the same information. Obtained paraphrases are generalized as templates and stored for future use.In this paper, first we describe our basic idea of paraphrase acquisition. Our method is divided into roughly four steps, each of which is explained in turn. Then we illustrate several issues which we encounter in real texts. To solve these problems, we introduce two techniques: coreference resolution and structural restriction of possible portions of expressions. Finally we discuss the experimental results and conclusions."
W03-1204,Evaluation of Features for Sentence Extraction on Different Types of Corpora,2003,9,6,2,1,51944,chikashi nobata,Proceedings of the {ACL} 2003 Workshop on Multilingual Summarization and Question Answering,0,"We report evaluation results for our summarization system and analyze the resulting summarization data for three different types of corpora. To develop a robust summarization system, we have created a system based on sentence extraction and applied it to summarize Japanese and English newspaper articles, obtained some of the top results at two evaluation workshops. We have also created sentence extraction data from Japanese lectures and evaluated our system with these data. In addition to the evaluation results, we analyze the relationships between key sentences and the features used in sentence extraction. We find that discrete combinations of features match distributions of key sentences better than sequential combinations."
W03-0509,A survey for Multi-Document Summarization,2003,6,20,1,1,1087,satoshi sekine,Proceedings of the {HLT}-{NAACL} 03 Text Summarization Workshop,0,"Automatic Multi-Document summarization is still hard to realize. Under such circumstances, we believe, it is important to observe how humans are doing the same task, and look around for different strategies.We prepared 100 document sets similar to the ones used in the DUC multi-document summarization task. For each document set, several people prepared the following data and we conducted a survey.A) Free style summarizationB) Sentence Extraction type summarizationC) Axis (type of main topic)D) Table style summaryIn particular, we will describe the last two in detail, as these could lead to a new direction for multi-summarization research."
P03-1029,An Improved Extraction Pattern Representation Model for Automatic {IE} Pattern Acquisition,2003,8,106,2,1,51460,kiyoshi sudo,Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,1,"Several approaches have been described for the automatic unsupervised acquisition of patterns for information extraction. Each approach is based on a particular model for the patterns to be acquired, such as a predicate-argument structure or a dependency chain. The effect of these alternative models has not been previously studied. In this paper, we compare the prior models and introduce a new model, the Subtree model, based on arbitrary subtrees of dependency trees. We describe a discovery procedure for this model and demonstrate experimentally an improvement in recall using Subtree patterns."
P03-1061,Morphological Analysis of a Large Spontaneous Speech Corpus in {J}apanese,2003,13,9,4,1,30019,kiyotaka uchimoto,Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,1,"This paper describes two methods for detecting word segments and their morphological information in a Japanese spontaneous speech corpus, and describes how to tag a large spontaneous speech corpus accurately by using the two methods. The first method is used to detect any type of word segments. The second method is used when there are several definitions for word segments and their POS categories, and when one type of word segments includes another type of word segments. In this paper, we show that by using semi-automatic analysis we achieve a precision of better than 99% for detecting and tagging short words and 97% for long words; the two types of words that comprise the corpus. We also show that better accuracy is achieved by using both methods than by using only the first."
N03-4013,pre-{CODIE}{--}Crosslingual On-Demand Information Extraction,2003,1,2,2,1,51460,kiyoshi sudo,Companion Volume of the Proceedings of {HLT}-{NAACL} 2003 - Demonstrations,0,"Our research addresses two central issues of information extraction --- portability and multilinguality. We are creating information extraction (IE) systems that take foreign-language input and generate English tables of extracted information, and that can be easily adapted to new extraction tasks. We want to minimize the human intervention required for customization to a new scenario (type of facts or events of interest), and allow the user to interact with the system entirely in English. As a prototype, we have developed the pre-CODIE system, an experimental Crosss-lingual On-Demand Information Extraction system that extracts facts or events of interest from Japanese source text without requiring user knowledge of Japanese."
nobata-etal-2002-summarization,Summarization System Integrated with Named Entity Tagging and {IE} pattern Discovery,2002,7,31,2,1,51944,chikashi nobata,Proceedings of the Third International Conference on Language Resources and Evaluation ({LREC}{'}02),0,"We have introduced information extraction technique such as named entity tagging and pattern discovery to a summarization system based on sentence extraction technique, and evaluated the performance in the Document Understanding Conference 2001 (DUC-2001). We participated in the Single Document Summarization task in DUC-2001 and achieved one of the best performance in subjective evaluation of summarization results."
sekine-etal-2002-extended,Extended Named Entity Hierarchy,2002,1,196,1,1,1087,satoshi sekine,Proceedings of the Third International Conference on Language Resources and Evaluation ({LREC}{'}02),0,"The tagging of Named Entities (NE), the names of particular things or classes and numeric expressions, is regarded as an important component technology for many NLP applications. These applications include Information Extraction, from which it was born, QuestionAnswering, Summarization and Information Retrieval. However, up to now, the number of NE types has been quite limited, 7 in MUC, 8 in IREX and 5 in the ACE program. Many more kinds of things have proper names or proper classes of expressions, and also finer distinctions are needed for some applications. We now propose a Named Entity hierarchy which contains about 150 NE types. The focus of this paper is the design of the hierarchy and we would like to provide this resource for any application. We report the design and development procedure of the hierarchy."
C02-2019,Morphological Analysis of the Spontaneous Speech Corpus,2002,7,7,4,1,30019,kiyotaka uchimoto,{COLING} 2002: The 17th International Conference on Computational Linguistics: Project Notes,0,"This paper describes a project tagging a spontaneous speech corpus with morphological information such as word segmentation and parts-of-speech. We use a morphological analysis system based on a maximum entropy model, which is independent of the domain of corpora. In this paper we show the tagging accuracy achieved by using the model and discuss problems in tagging the spontaneous speech corpus. We also show that a dictionary developed for a corpus on a certain domain is helpful for improving accuracy in analyzing a corpus on another domain."
C02-1064,Text Generation from Keywords,2002,15,23,2,1,30019,kiyotaka uchimoto,{COLING} 2002: The 19th International Conference on Computational Linguistics,0,"We describe a method for generating sentences from keywords or headwords. This method consists of two main parts, candidate-text construction and evaluation. The construction part generates text sentences in the form of dependency trees by using complementary information to replace information that is missing because of a knowledge gap and other missing function words to generate natural text sentences based on a particular monolingual corpus. The evaluation part consists of a model for generating an appropriate text when given keywords. This model considers not only word n-gram information, but also dependency information between words. Furthermore, it considers both string information and morphological information."
W01-0512,The Unknown Word Problem: a Morphological Analysis of {J}apanese Using Maximum Entropy Aided by a Dictionary,2001,0,45,2,1,30019,kiyotaka uchimoto,Proceedings of the 2001 Conference on Empirical Methods in Natural Language Processing,0,None
S01-1038,Word Translation Based on Machine Learning Models Using Translation Memory and Corpora,2001,1,0,2,1,30019,kiyotaka uchimoto,Proceedings of {SENSEVAL}-2 Second International Workshop on Evaluating Word Sense Disambiguation Systems,0,"Senseval-2 was held in Spring, 2001. It consisted of several tasks in various languages. In this paper, we describe our system used for one of these tasks: the Japanese translation task. With an accuracy of 63.4%, our system was the third best system in the contest among nine systems developed by seven groups."
H01-1009,Automatic Pattern Acquisition for {J}apanese Information Extraction,2001,4,47,2,1,51460,kiyoshi sudo,Proceedings of the First International Conference on Human Language Technology Research,0,"One of the central issues for information extraction is the cost of customization from one scenario to another. Research on the automated acquisition of patterns is important for portability and scalability. In this paper, we introduce Tree-Based Pattern representation where a pattern is denoted as a path in the dependency tree of a sentence. We outline the procedure to acquire Tree-Based Patterns in Japanese from un-annotated text. The system extracts the relevant sentences from the training data based on TF/IDF scoring and the common paths in the parse tree of relevant sentences are taken as extracted patterns."
P00-1044,Difficulty Indices for the Named Entity Task in {J}apanese,2000,2,3,2,1,51944,chikashi nobata,Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics,1,"We propose indices to measure the difficulty of the named entity (NE) task by looking at test corpora, based on expressions inside and outside the NEs. These indices are intended to estimate the difficulty of each task without actually using an NE system and to be unbiased towards a specific system. The values of the indices are compared with the systems' performance in Japanese documents. We also discuss the difference between NE classes with the indices and show useful clues which will make it easier to recognize NEs."
sekine-isahara-2000-irex,{IREX}: {IR} {\\&} {IE} Evaluation Project in {J}apanese,2000,0,51,1,1,1087,satoshi sekine,Proceedings of the Second International Conference on Language Resources and Evaluation ({LREC}{'}00),0,"We will report on the IREX (Information Retrieval and Extraction Exercise) project. It is an evaluation-based project for Information Retrieval and Information Extraction in Japanese. The project started in May 1998 and concluded in September 1999 with the IREX workshop held in Tokyo with more than 150 attendance (IREX Commettee, 1999). There is a homepage of the project at (IREX, Homepage) and anyone can download almost all the data and the tools produced by the project for free."
moreno-etal-2000-treebank,A Treebank of {S}panish and its Application to Parsing,2000,8,22,5,0,28753,antonio moreno,Proceedings of the Second International Conference on Language Resources and Evaluation ({LREC}{'}00),0,"This paper presents joint research between a Spanish team and an American one on the development and exploitation of a Spanish treebank. Such treebanks for other languages have proven valuable for the development of high-quality parsers and for a wide variety of language studies. However, when the project started, at the end of 1997, there was no syntactically annotated corpus for Spanish. This paper describes the design of such a treebank and its initial application to parser construction. 1. Constructing a Spanish treebank 1.1. Preliminary considerations This paper presents joint research between a Spanish team and an American one on the development and exploitation of a Spanish treebank. Such treebanks for other languages have proven valuable for the development of high-quality parsers and for a wide variety of language studies. As there was no previous experience in building a syntactically annotated corpus for Spanish, the first effort consisted necessarily in writing a set of annotation guidelines. The starting point was the existing documentation at that time, especially the Penn Treebank project (Marcus, Santorini and Marcinkiewicz, 1993; Bies et al., 1995), the EAGLES preliminary recommendations (EAGLES, 1996), and the Negra corpus (Skut et al., 1997). Our experience in developing Spanish NLP systems told us that a pure phrase structure annotation (typical of the English treebanks) would not be enough for inducing relevant rules for Spanish. At the least, information about agreement and syntactic functions is necessary for Spanish, and we wanted to incorporate that information in our trees in the form of features. The treebank has been created mostly by hand, although some automatic pre-tagging of the data is performed, as described below, to speed treebank creation."
C00-2109,Backward Beam Search Algorithm for Dependency Analysis of {J}apanese,2000,8,19,1,1,1087,satoshi sekine,{COLING} 2000 Volume 2: The 18th International Conference on Computational Linguistics,0,"Backward beam search for dependency analysis of Japanese is proposed. As dependencies normally go from left to right in Japanese, it is effective to analyze sentences backwards (from right to left). The analysis is based on a statistical method and employs a beam search strategy. Based on experiments varying the beam search width, we found that the accuracy is not sensitive to the beam width and even the analysis with a beam width of 1 gets almost the same dependency accuracy as the best accuracy using a wider beam width. This suggested a deterministic algorithm for backwards Japanese dependency analysis, although still the beam search is effective as the N-best sentence accuracy is quite high. The time of analysis is observed to be quadratic in the sentence length."
C00-2110,{J}apanese Dependency Analysis using a Deterministic Finite State Transducer,2000,7,6,1,1,1087,satoshi sekine,{COLING} 2000 Volume 2: The 18th International Conference on Computational Linguistics,0,"A deterministic finite state transducer is a fast device for analyzing strings. It takes O(n) time to analyze a string of length n. In this paper, an application of this technique to Japanese dependency analysis will be described. We achieved the speed at a small cost in accuracy. It takes about 0.17 millisecond to analyze one sentence (average length is 10 bunsetsu, based on PentiumIII 650MHz PC, Linux) and we actually observed the analysis time to be proportional to the sentence length. The accuracy is about 81% even though very little lexical information is used. This is about 17% and 9% better than the default and a simple system, respectively. We believe the gap between our performance and the best current performance on the same task, about 7%, can be filled by introducing lexical or semantic information."
C00-2126,Word Order Acquisition from Corpora,2000,2,18,4,1,30019,kiyotaka uchimoto,{COLING} 2000 Volume 2: The 18th International Conference on Computational Linguistics,0,"In this paper we describe a method of acquiring word order from corpora. Word order is defined as the order of modifiers, or the order of phrasal units called 'bunsetsu' which depend on the same modifiee. The method uses a model which automatically discovers what the tendency of the word order in Japanese is by using various kinds of information in and around the target bunsetsus. This model shows us to what extent each piece of information contributes to deciding the word order and which word order tends to be selected when several kinds of information conflict. The contribution rate of each piece of information in deciding word order is efficiently learned by a model within a maximum entropy framework. The performance of this trained model can be evaluated by checking how many instances of word order selected by the model agree with those in the original text. In this paper, we show that even a raw corpus that has not been tagged can be used to train the model, if it is first analyzed by a parser. This is possible because the word order of the text in the corpus is correct."
C00-2167,{J}apanese Named Entity Extraction Evaluation - Analysis of Results -,2000,1,31,1,1,1087,satoshi sekine,{COLING} 2000 Volume 2: The 18th International Conference on Computational Linguistics,0,"We will report on one of the two tasks in the IREX (Information Retrieval and Extraction Exercise) project, an evaluation-based project for Information Retrieval and Information Extraction in Japanese (Sekine and Isahara, 2000) (IREX Committee, 1999). The project started in 1998 and concluded in September 1999 with many participants and collaborators (45 groups in total from Japan and the US). In this paper, the Named Entity (NE) task is reported. It is a task to extract NE's, such as names of organizations, persons, locations and artifacts, time expressions and numeric expressions from newspaper articles. First, we will explain the task and the definition, as well as the data we created and the results. Second, the analyses of the results will be described, which include analysis of task difficulty across the NE types and system types, analysis of domain dependency and comparison to human performance."
2000.iwpt-1.43,Dependency Model using Posterior Context,2000,-1,-1,3,1,30019,kiyotaka uchimoto,Proceedings of the Sixth International Workshop on Parsing Technologies,0,"We describe a new model for dependency structure analysis. This model learns the relationship between two phrasal units called bunsetsus as three categories; {`}between{'}, {`}dependent{'}, and {`}beyond{'}, and estimates the dependency likelihood by considering not only the relationship between two bunsetsus but also the relationship between the left bunsetsu and all of the bunsetsus to its right. We implemented this model based on the maximum entropy model. When using the Kyoto University corpus, the dependency accuracy of our model was 88{\%}, which is about 1{\%} higher than that of the conventional model using exactly the same features."
W99-0510,Statistical Matching of Two Ontologies,1999,-1,-1,1,1,1087,satoshi sekine,{SIGLEX}99: Standardizing Lexical Resources,0,None
E99-1026,{J}apanese Dependency Structure Analysis Based on Maximum Entropy Models,1999,8,51,2,1,30019,kiyotaka uchimoto,Ninth Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,This paper describes a dependency structure analysis of Japanese sentences based on the maximum entropy models. Our model is created by learning the weights of some features from a training corpus to predict the dependency between bunsetsus or phrasal units. The dependency accuracy of our system is 87.2% using the Kyoto University corpus. We discuss the contribution of each feature set and the relationship between the number of training data and the accuracy.
X98-1015,{J}apanese {IE} System and Customization Tool,1998,-1,-1,2,1,51944,chikashi nobata,"TIPSTER TEXT PROGRAM PHASE III: Proceedings of a Workshop held at Baltimore, {M}aryland, October 13-15, 1998",0,None
W98-1120,A Decision Tree Method for Finding and Classifying Names in {J}apanese Texts,1998,0,108,1,1,1087,satoshi sekine,Sixth Workshop on Very Large Corpora,0,None
M98-1019,Description of the {J}apanese {NE} System Used for {MET}-2,1998,0,28,1,1,1087,satoshi sekine,"Seventh Message Understanding Conference ({MUC}-7): Proceedings of a Conference Held in Fairfax, Virginia, {A}pril 29 - May 1, 1998",0,None
A97-1015,The Domain Dependence of Parsing,1997,8,82,1,1,1087,satoshi sekine,Fifth Conference on Applied Natural Language Processing,0,"A major concern in corpus based approaches is that the applicability of the acquired knowledge may be limited by some feature of the corpus, in particular, the notion of text 'domain'. In order to examine the domain dependence of parsing, in this paper, we report 1) Comparison of structure distributions across domains; 2) Examples of domain specific structures; and 3) Parsing experiment using some domain dependent grammars. The observations using the Brown corpus demonstrate domain dependence and idiosyncrasy of syntactic structure. The parsing results show that the best accuracy is obtained using the grammar acquired from the same domain or the same class (fiction or nonfiction). We will also discuss the relationship between parsing accuracy and the size of training corpus."
C96-2154,Modeling Topic Coherence for Speech Recognition,1996,7,3,1,1,1087,satoshi sekine,{COLING} 1996 Volume 2: The 16th International Conference on Computational Linguistics,0,"Statistical language models play a major role in current speech recognition systems. Most of these models have focussed on relatively local interactions between words. Recently, however, there have been several attempts to incorporate other knowledge sources, in particular longer-range word dependencies, in order to improve speech recognizers. We will present one such method, which tries to antomatically utilize properties of topic continuity. When a base-line speech recognition system generates alternative hypotheses for a sentence, we will utilize the word preferences based on topic coherence to select the best hypothesis. In our experiment, we achieved a 0.65% improvement in the word error rate on top of the base-line system. It corresponds to 10.40% of the possible word error improvement."
1995.iwpt-1.26,A Corpus-based Probabilistic Grammar with Only Two Non-terminals,1995,-1,-1,1,1,1087,satoshi sekine,Proceedings of the Fourth International Workshop on Parsing Technologies,0,"The availability of large, syntactically-bracketed corpora such as the Penn Tree Bank affords us the opportunity to automatically build or train broad-coverage grammars, and in particular to train probabilistic grammars. A number of recent parsing experiments have also indicated that grammars whose production probabilities are dependent on the context can be more effective than context-free grammars in selecting a correct parse. To make maximal use of context, we have automatically constructed, from the Penn Tree Bank version 2, a grammar in which the symbols S and NP are the only real nonterminals, and the other non-terminals or grammatical nodes are in effect embedded into the right-hand-sides of the S and NP rules. For example, one of the rules extracted from the tree bank would be S -{\textgreater} NP VBX JJ CC VBX NP [1] ( where NP is a non-terminal and the other symbols are terminals {--} part-of-speech tags of the Tree Bank). The most common structure in the Tree Bank associated with this expansion is (S NP (VP (VP VBX (ADJ JJ) CC (VP VBX NP)))) [2]. So if our parser uses rule [1] in parsing a sentence, it will generate structure [2] for the corresponding part of the sentence. Using 94{\%} of the Penn Tree Bank for training, we extracted 32,296 distinct rules ( 23,386 for S, and 8,910 for NP). We also built a smaller version of the grammar based on higher frequency patterns for use as a back-up when the larger grammar is unable to produce a parse due to memory limitation. We applied this parser to 1,989 Wall Street Journal sentences (separate from the training set and with no limit on sentence length). Of the parsed sentences (1,899), the percentage of no-crossing sentences is 33.9{\%}, and Parseval recall and precision are 73.43{\%} and 72 .61{\%}."
C92-2085,Linguistic Knowledge Generator,1992,8,18,1,1,1087,satoshi sekine,{COLING} 1992 Volume 2: The 14th {I}nternational {C}onference on {C}omputational {L}inguistics,0,"The difficulties in current NLP applications are seldom due to the lack of appropriate frameworks for encoding our linguistic or extra-linguistic knowledge, hut rather to the fact that we do not know in advance what actual znstances of knowledge should be, even though we know in advance what types of knowledge are required. It normally takes a long time and requires painful trial and error processes to adapt knowledge, for example, in existing MT systems in order to translate documents of a new text-type and of a new subject domain. Semantic classification schemes for words, for example, usually reflect ontologies of subject domains so that we cannot expect a single classification scheme to be effective across different domains. To treat different suhlanguages requires different word classification schemes. We have to construct appropriate schemes for given sublanguages from scratch [1]. It has also been reported that not only knowledge concerned with extra-linguistic domains but also syntactic knowledge, such as subcategorization frames of verbs (which is usually conceived as a par t of general language knowledge), often varies from one sublanguage to another [2]. Though re-usability of linguistic knowledge is currently and intensively prescribed [3], our contention is that the adaptation of existing knowledge requires processes beyond mere re-use. Tha t is,"
A92-1014,Automatic Learning for Semantic Collocation,1992,9,59,1,1,1087,satoshi sekine,Third Conference on Applied Natural Language Processing,0,"The real difficulty in development of practical NLP systems comes from the fact that we do not have effective means for gathering knowledge. In this paper, we propose an algorithm which acquires automatically knowledge of semantic collocations among words from sample corpora.The algorithm proposed in this paper tries to discover semantic collocations which will be useful for disambiguating structurally ambiguous sentences, by a statistical approach. The algorithm requires a corpus and minimum linguistic knowledge (parts-of-speech of words, simple inflection rules, and a small number of general syntactic rules).We conducted two experiments of applying the algorithm to diferent corpora to extract different types of semantic collocations. Though there are some unsolved problems, the results showed the effectiveness of the proposed algorithm."
