2014.iwslt-papers.13,2005.iwslt-1.20,0,0.0317057,"stems for spontaneous, conversational, human-human speech. In contrast to machine directed or scripted conversations (broadcast news), most conversational speech has by nature, variability in recording environment and vocal registers and a high number of disfluencies and out-of-vocabulary words. It also exhibits difficult challenges associated with code switching and regional dialects. This directly relates to an increase of difficulty for both ASR and SMT systems. Since SLT systems are generally built by feeding the output of the ASR system to an SMT system, each trained on separate datasets [1, 2], errors produced by the systems compound. With respect to Egyptian Arabic specifically, unscripted, spontaneous, telephone conversations have been available through the Callhome Egyptian Arabic corpus (speech and transcripts) since 1997. However, since this dataset did not come with translations for the transcriptions in Arabic, researchers had to resort to using out-of-domain data to train the SMT systems. Transcripts for spontaneous conversations (speech), vary significantly from transcripts for scripted conversations and informal written conversations (web, forum, SMS, chat). To bridge thi"
2014.iwslt-papers.13,N10-1024,1,0.808571,"ble 3. A sample of the special symbols using in the Arabic transcripts. These represent non-conventional speech segments such as non-verbal vocalizations, disfluencies, background noise and distortion. orthographic representation of words in the LDC romanization scheme and Arabic script along with morphological, phonological, stress, source, and frequency information. 3. TRANSLATION METHODOLOGY The translations for the Egyptian Arabic Callhome corpus were obtained using crowd-sourcing techniques. Crowdsourcing has become a standard technique in the collection and annotation of scientific data [3, 4, 5, 6, 7, 8] including data for natural language processing tasks like machine translation [9]. We use the crowdsourcing platform, Amazon Mechanical Turk (MTurk) to obtain translations. We follow the best practices suggested by [9] in this process. 3.1. Pre-processing Each transcript was pre-processed to remove markup, including the special symbols described in Section 2.4. Some special symbols contain text in a foreign language (mostly, English). These were retained so that they could be passed through to the translation. Utterances that comprised only of markup and the special symbols were removed. Each"
2014.iwslt-papers.13,D08-1027,0,0.0103059,"ble 3. A sample of the special symbols using in the Arabic transcripts. These represent non-conventional speech segments such as non-verbal vocalizations, disfluencies, background noise and distortion. orthographic representation of words in the LDC romanization scheme and Arabic script along with morphological, phonological, stress, source, and frequency information. 3. TRANSLATION METHODOLOGY The translations for the Egyptian Arabic Callhome corpus were obtained using crowd-sourcing techniques. Crowdsourcing has become a standard technique in the collection and annotation of scientific data [3, 4, 5, 6, 7, 8] including data for natural language processing tasks like machine translation [9]. We use the crowdsourcing platform, Amazon Mechanical Turk (MTurk) to obtain translations. We follow the best practices suggested by [9] in this process. 3.1. Pre-processing Each transcript was pre-processed to remove markup, including the special symbols described in Section 2.4. Some special symbols contain text in a foreign language (mostly, English). These were retained so that they could be passed through to the translation. Utterances that comprised only of markup and the special symbols were removed. Each"
2014.iwslt-papers.13,D09-1030,1,0.792712,"ble 3. A sample of the special symbols using in the Arabic transcripts. These represent non-conventional speech segments such as non-verbal vocalizations, disfluencies, background noise and distortion. orthographic representation of words in the LDC romanization scheme and Arabic script along with morphological, phonological, stress, source, and frequency information. 3. TRANSLATION METHODOLOGY The translations for the Egyptian Arabic Callhome corpus were obtained using crowd-sourcing techniques. Crowdsourcing has become a standard technique in the collection and annotation of scientific data [3, 4, 5, 6, 7, 8] including data for natural language processing tasks like machine translation [9]. We use the crowdsourcing platform, Amazon Mechanical Turk (MTurk) to obtain translations. We follow the best practices suggested by [9] in this process. 3.1. Pre-processing Each transcript was pre-processed to remove markup, including the special symbols described in Section 2.4. Some special symbols contain text in a foreign language (mostly, English). These were retained so that they could be passed through to the translation. Utterances that comprised only of markup and the special symbols were removed. Each"
2014.iwslt-papers.13,P11-1122,1,0.929626,"ntional speech segments such as non-verbal vocalizations, disfluencies, background noise and distortion. orthographic representation of words in the LDC romanization scheme and Arabic script along with morphological, phonological, stress, source, and frequency information. 3. TRANSLATION METHODOLOGY The translations for the Egyptian Arabic Callhome corpus were obtained using crowd-sourcing techniques. Crowdsourcing has become a standard technique in the collection and annotation of scientific data [3, 4, 5, 6, 7, 8] including data for natural language processing tasks like machine translation [9]. We use the crowdsourcing platform, Amazon Mechanical Turk (MTurk) to obtain translations. We follow the best practices suggested by [9] in this process. 3.1. Pre-processing Each transcript was pre-processed to remove markup, including the special symbols described in Section 2.4. Some special symbols contain text in a foreign language (mostly, English). These were retained so that they could be passed through to the translation. Utterances that comprised only of markup and the special symbols were removed. Each utterance in the corpus contains channel and segment information. These were inco"
2020.acl-main.149,N18-2085,1,0.921528,"of the candidate translation our model actually produces, e.g. under approximate decoding. However, an information-theoretic evaluation 1641 is much more suitable for measuring the more abstract notion of which language pairs are hardest to translate to and from, which is our purpose here. 3 Disentangling Translation Difficulty and Monolingual Complexity We contend that simply reporting cross-entropies is not enough. A second issue in performing a controlled, cross-lingual MT comparison is that the language generation component (without translation) is not equally difficult across languages (Cotterell et al., 2018). We claim that the difficulty of translation corresponds more closely to the mutual information MI(S; T ) between the source and target language, which tells us how much easier it becomes to predict T when S is given (see Figure 1). But what is the appropriate analogue of mutual information for cross-entropy? One such natural generalization is a novel quantity that we term cross-mutual information, defined as: XMI(S → T ) = HqLM (T ) − HqMT (T |S) (3) where HqLM (T ) denotes the cross-entropy of the target sentence T under the model qLM . As in §2, this can, analogously, be approximated by th"
2020.acl-main.149,D18-1312,0,0.025153,"Missing"
2020.acl-main.149,D10-1092,0,0.0433027,"rich language is harder than translating into a morphologically impoverished one. In fact, the only significant correlate of MT difficulty we find is source-side type–token ratio. 2 Cross-Linguistic Comparability through Likelihoods, not BLEU Human evaluation will always be the gold standard of MT evaluation. However, it is both timeconsuming and expensive to perform. To help researchers and practitioners quickly deploy and evaluate new systems, automatic metrics that correlate fairly well with human evaluations have been proposed over the years (Banerjee and Lavie, 2005; Snover et al., 2006; Isozaki et al., 2010; Lo, 2019). BLEU (Papineni et al., 2002), however, has remained the most common metric to report the performance of MT systems. BLEU is a precisionbased metric: a BLEU score is proportional to the geometric average of the number of n-grams in the candidate translation that also appear in the reference translation for 1 ≤ n ≤ 4.1 In the context of our study, we take issue with two shortcomings of BLEU scores that prevent a cross-linguistically comparable study. First, it is not possible to directly compare BLEU scores across languages because different languages might express the same meaning"
2020.acl-main.149,W04-3250,0,0.537126,"Missing"
2020.acl-main.149,2005.mtsummit-papers.11,0,0.165253,"tely estimate the difficulty of translation for a given architecture in a controlled way. In summary, by looking at XMI, we can effectively decouple the language generation component, whose difficulties have been investigated by Cotterell et al. 2018 and Mielke et al. 2019, from the translation component. This gives us a measure of how rich and useful the information extracted from the source language is for the target-language generation component. 4 Experiments In order to measure which pairs of languages are harder to translate to and from, we make use of the latest release v7 of Europarl (Koehn, 2005): a corpus of the proceedings of the European Parliament containing parallel sentences between English (en) and 20 other European languages: Bulgarian (bg), Czech (cs), Danish (da), German (de), Greek (el), Spanish (es), Estonian (et), Finnish (fi), French (fr), Hungarian (hu), Italian (it), Lithuanian (lt), Latvian (lv), Dutch (nl), Polish (pl), Portuguese (pt), Romanian (ro), Slovak (sk), Slovene (sl) and Swedish (sv). Pre-processing steps In order to precisely effect a fully controlled experiment, we enforce a fair comparison by selecting the set of parallel sentences available across all 2"
2020.acl-main.149,P09-5002,0,0.0271803,"but the relation between H(S) and H(T ) can be arbitrary. Right: estimating cross-entropies using models qMT and qLM invalidates relations between bars, except that Hq· (·) ≥ H(·). XMI, our proposed metric, is no longer purely a symmetric measure of language, but now an asymmetric measure that mostly highlights models’ shortcomings. Introduction Machine translation (MT) is one of the core research areas in natural language processing. Current state-of-the-art MT systems are based on neural networks (Sutskever et al., 2014; Bahdanau et al., 2015), which generally surpass phrase-based systems (Koehn, 2009) in a variety of domains and languages (Bentivogli et al., 2016; Toral and S´anchez-Cartagena, 2017; Castilho et al., 2017; Bojar et al., 2018; Barrault et al., 2019). Using phrase-based MT systems, various controlled studies to understand where the translation difficulties lie for different language pairs were conducted (Birch et al., 2008; Koehn et al., 2009). However, comparable studies have yet to be performed for neural machine translation (NMT). As a result, it is still unclear whether all translation directions are equally easy (or hard) to model for NMT. This paper hence aims at fillin"
2020.acl-main.149,2009.mtsummit-papers.7,0,0.0241981,"translation (MT) is one of the core research areas in natural language processing. Current state-of-the-art MT systems are based on neural networks (Sutskever et al., 2014; Bahdanau et al., 2015), which generally surpass phrase-based systems (Koehn, 2009) in a variety of domains and languages (Bentivogli et al., 2016; Toral and S´anchez-Cartagena, 2017; Castilho et al., 2017; Bojar et al., 2018; Barrault et al., 2019). Using phrase-based MT systems, various controlled studies to understand where the translation difficulties lie for different language pairs were conducted (Birch et al., 2008; Koehn et al., 2009). However, comparable studies have yet to be performed for neural machine translation (NMT). As a result, it is still unclear whether all translation directions are equally easy (or hard) to model for NMT. This paper hence aims at filling this gap: Ceteris paribus, is it easier to translate from English into Finnish or into Hungarian? And how much easier is it? Conversely, is it equally hard to translate Finnish and Hungarian into another language? Based on BLEU (Papineni et al., 2002) scores, previous work (Belinkov et al., 2017) suggests that translating into morphologically rich languages,"
2020.acl-main.149,W18-1819,0,0.0425158,"Missing"
2020.acl-main.149,P02-1040,0,\N,Missing
2020.acl-main.149,W05-0909,0,\N,Missing
2020.acl-main.149,D08-1078,0,\N,Missing
2020.acl-main.149,W17-0230,0,\N,Missing
2020.acl-main.149,E17-2002,0,\N,Missing
2020.acl-main.149,W18-6401,0,\N,Missing
2020.acl-main.149,P16-1162,0,\N,Missing
2020.acl-main.149,W19-5358,0,\N,Missing
2020.acl-main.149,2020.acl-main.615,1,\N,Missing
2020.acl-main.415,2020.lrec-1.520,0,0.0419634,"on requires access to large amounts of cross-linguistic data. Previous cross-linguistic phonetic studies have been limited to a small number of languages with available data (Disner, 1983; Cho and Ladefoged, 1999), or have relied on previously reported measures from many studies (Whalen and Levitt, 1995; Becker-Kristal, 2010; Gordon and Roettger, 2017; Chodroff et al., 2019). Existing multilingual speech corpora have similar restrictions, with data too limited for many tasks (Engstrand and Cunningham-Andersson, 1988; Ladefoged and Maddieson, 2007) or approximately 20 to 30 recorded languages (Ardila et al., 2020; Harper, 2011; Schultz, 2002). The recently developed CMU Wilderness corpus (Black, 2019) constitutes an exception to this rule with over 600 languages. This makes it the largest and most typologically diverse speech corpus to date. In addition to its coverage, the CMU Wilderness corpus is unique in two additional aspects: cleanly recorded, read speech exists for all languages in the corpus, and the same content (modulo translation) exists across all languages. However, this massively multilingual speech corpus is challenging to work with directly. Copyright, computational restrictions, and s"
2020.acl-main.415,2020.lrec-1.521,0,0.334554,"e mean mel cepstral distortion score (see §3.1.3) converges. Baum-Welch does not change the predicted phoneme labels, but obtains a language-specific, reading-specific, contextual (triphone) acoustic model for each phoneme type in the language. We then use Viterbi alignment to identify an audio segment for each phoneme token. 3.1.2 High-Resource Languages A subset of the languages in our corpus are supported by existing pronunciation resources. Two such resources are Epitran (Mortensen et al., 2018), a G2P tool based on language-specific rules, available in both IPA and X-SAMPA, and WikiPron (Lee et al., 2020), a collection of crowd-sourced pronunciations scraped from Wiktionary. These are mapped from IPA to X-SAMPA for label consistency across our corpus. Epitran covers 29 of our languages (39 readings), while WikiPron’s ‘phonemic’ annotations7 provide partial coverage of 13 additional languages (18 readings). We use Epitran for languages with regular orthographies where it provides high-quality support, and WikiPron for other languages covered by WikiPron annotations. While Unitran and Epitran provide a single pronunciation for a word from the orthography, WikiPron may include multiple pronunciat"
2020.acl-main.415,qian-etal-2010-python,0,0.477559,"for each utterance. A pronunciation is predicted from the text alone using some graphemeto-phoneme (G2P) method. Each word’s predicted pronunciation is a sequence of categorical labels, which are ‘phoneme-level’ in the sense that they are usually intended to distinguish the words of the language. We then align this predicted sequence of ‘phonemes’ to the corresponding audio. 3.1.1 All Languages Most of our languages have neither existing pronunciation lexicons nor G2P resources. To provide coverage for all languages, we generate pronunciations using the simple ‘universal’ G2P system Unitran (Qian et al., 2010, as extended by Black, 2019), which deterministically expands each grapheme to a fixed sequence of phones in the Extended Speech Assessment Methods Phonetic Alphabet (XSAMPA) (Wells, 1995/2000). This naive process is error-prone for languages with opaque orthographies, as we show in §3.1.3 below and discuss further in §3.4 (Caveat B). Even so, it provides a starting point for exploring low-resource languages: after some manual inspection, a linguist may be 6 able to correct the labels in a given language by a combination of manual and automatic methods. For each reading, to align the pronunci"
2020.acl-main.420,N19-1112,0,0.270272,"tions. Many speculate that these representations encode a continuous analogue of discrete linguistic properties, e.g., part-of-speech tags, due to the networks’ impressive performance on many NLP tasks (Belinkov et al., 2017). As a result of this Our analysis also provides insight into how to choose a probe family: We show that choosing the highest-performing probe, independent of its complexity, is optimal for achieving the best estimate of mutual information (MI). This contradicts the received wisdom that one should always select simple probes over more complex ones (Alain and Bengio, 2017; Liu et al., 2019; Hewitt and Manning, 2019). In this context, we also discuss the recent work of Hewitt and Liang (2019) who proposes selectivity as a criterion for choosing families of probes. Hewitt and Liang (2019) defines selectivity as the performance difference between a probe on the target task and a control task, writing “[t]he selectivity of a probe puts linguistic task accuracy in context with the probe’s capacity to memorize from word types.” They further ponder: “when a probe achieves high accuracy on a linguistic task using a representation, can we conclude that the representation encodes linguis"
2020.acl-main.420,C18-1198,0,0.0577837,"Missing"
2020.acl-main.420,N18-1202,0,0.220027,"Missing"
2020.acl-main.420,D18-1179,0,0.125431,"Missing"
2020.acl-main.420,petrov-etal-2012-universal,0,0.108893,"Missing"
2020.acl-main.420,W18-5448,0,0.143865,"on operationalizing ease of extraction more rigorously—even though we do not attempt this ourselves. As previously argued by Saphra and Lopez (2019, §5), the advantage of simple probes is that they may reveal something about the structure of the encoded information—i.e., is it structured in such a way that it can be easily taken advantage of by downstream consumers of the contextualized embeddings? Many researchers who are interested in less complex probes have, either implicitly or explicitly, had this in mind. 5 A Critique of Control Tasks We agree with Hewitt and Liang (2019)—and with both Zhang and Bowman (2018) and Tenney et al. (2019)—that we should have controlled baselines when probing for linguistic properties. However, we disagree with parts of their methodology for constructing control tasks. We present these disagreements here. 5.1 Structure and Randomness Hewitt and Liang (2019) introduces control tasks to evaluate the effectiveness of probes. We draw 4614 6 Xu et al. (2020) is a possible exception. inspiration from this technique as evidenced by our introduction of control functions. However, we take issue with the suggestion that controls should have structure and randomness, to use the te"
2020.acl-main.420,P13-1057,0,\N,Missing
2020.acl-main.420,Q17-1010,0,\N,Missing
2020.acl-main.420,N19-1329,0,\N,Missing
2020.acl-main.420,N19-1419,0,\N,Missing
2020.acl-main.420,I17-1001,0,\N,Missing
2020.acl-main.420,N19-1423,0,\N,Missing
2020.acl-main.420,D19-1275,0,\N,Missing
2020.acl-main.597,W11-1301,0,0.0340707,"s. We exhibit examples in §3. 2.2 Distributional Lexical Semantics We adopt a distributional approach to lexical semantics (Harris 1954; Mitchell and Lapata 2010; Turney and Pantel 2010; Bernardi et al. 2015; Clark 2015; inter alia) that relies on pretrained word embeddings for this paper. We do this for multiple reasons: First, distributional semantic approaches to create word vectors, such as WORD 2 VEC (Mikolov et al., 2013), have been shown to do well at extracting lexical features such as animacy and taxonomic information (Rubinstein et al., 2015) and can also recognize semantic anomaly (Vecchi et al., 2011). Second, the distributional approach to lexical meaning yields a straightforward procedure for extracting “meaning” from text corpora at scale. 2.3 Controlling for grammatical gender? Grammatical gender has been found to interact with lexical semantics (Schwichtenberg and Schiller, 2004; Williams et al., 2019, 2020), and often can be determined from form (Brooks et al., 1993; Dobrin, 1998; Frigo and McDonald, 1998; Starreveld and La Heij, 2004). This means that it cannot be ignored in the present study. While the precise nature of the relationship between declension class and gender is far fr"
2020.acl-main.597,D19-1577,1,0.538907,"Missing"
2020.acl-main.597,P15-2119,0,\N,Missing
2020.acl-main.597,J92-1002,0,\N,Missing
2020.acl-main.597,P16-1140,0,\N,Missing
2020.acl-main.597,P19-1171,1,\N,Missing
2020.acl-main.615,2012.eamt-1.60,0,0.0221516,"the development set for the respective task.10 We additionally do a more fine-grained grid search over β for J0 (confidence penalty) and J1 (label smoothing) for completeness. All other model hyperparameters are held constant. We run experiments on multiple architectures and across several data sets to ensure trends are general. 4.1 Neural Machine Translation We explore performance of the regularizer DJα on NMT systems using three language pairs and corpora of two different sizes on the following tasks: WMT’14 German-to-English (De-En) (Bojar et al., 2014), IWSLT’14 German-to-English (De-En) (Cettolo et al., 2012), and Multitarget TED Talks Task (MTTT) French-to-English (Fr–En) and Japanese-to-English (Ja-En) tasks (Duh, 2018). For the larger WMT data set, we train fewer models using coarser-grained α and β ranges. We perform experiments for both Transformers (Vaswani et al., 2017) and convolutional sequence-to-sequence models (Gehring et al., 2017). For reproducibility and comparability, we use the data pre-processing scripts provided by fairseq (Ott et al., 2019) and follow recommended hyperparameter settings from previous work (Vaswani et al., 2017; Gehring et al., 2017) for baseline models. We use"
2020.acl-main.615,P18-1008,0,0.0196625,", resulting in peaky (low-entropy) probability distributions over the vocabulary. Specifically for language generation tasks, this behavior leads to the output of repetitive or frequently occurring but unrelated text, which is detrimental to the generalization abilities of the model (Chorowski and Jaitly, 2017; Holtzman et al., 2020). A natural regularizer to consider is, therefore, one that penalizes overconfidence, encouraging higher entropy in the learned distribution. Indeed, the literature has ascribed gains of ≈ 1 BLEU point in machine translation to label smoothing, one such technique (Chen et al., 2018). Despite the clear relationship between low entropy and overfitting, only a handful of distinct entropy regularizers have been explored. To fill this gap, we introduce generalized entropy regularization (GER), a unified framework for understanding and exploring a broad range of entropyinducing regularizers. GER is based on the skewJensen family of divergences Jα,G (Nielsen and Boltz, 2011) and thus may be generalized to any Bregman divergence through the choice of generator function G. For the negative entropy generator function, GER recovers label smoothing (Szegedy et al., 2015) as α → 1, a"
2020.acl-main.615,D18-1045,0,0.0170391,"U. Results of our experiments are shown in Table 2 and Figure 3. We see the same relation between model entropy and BLEU with both Transformer and convolutional architectures and between different language pairs. We show results for the Transformer architectures inline as they are the current standard for many NLP tasks; results for convolutional architectures are in App. H. Our results show better performance is achieved with values of α and β other than those that correspond to label smoothing with γ = 0.1, which is the commonly used value for the strength coefficient (Vaswani et al., 2017; Edunov et al., 2018). Moreover, the relationship between model entropy and evaluation performance is strong, following the same trend for all values of α, which suggests tuning a model for a specific entropy rather than α, β may be a better method in practice. We discuss trends in §4.3. 4.2 Abstractive Summarization We fine-tune BART (Lewis et al., 2019) on the CNN/DailyMail abstractive summarization task (Hermann et al., 2015) with regularizer DJα . Data pre-processing and other hyperparameter settings follow Lewis et al. (2019). Results in Table 3 show that optimal values of ROUGE -L (Lin, 2004), the evaluation"
2020.acl-main.615,W04-1013,0,0.0126571,"2017; Edunov et al., 2018). Moreover, the relationship between model entropy and evaluation performance is strong, following the same trend for all values of α, which suggests tuning a model for a specific entropy rather than α, β may be a better method in practice. We discuss trends in §4.3. 4.2 Abstractive Summarization We fine-tune BART (Lewis et al., 2019) on the CNN/DailyMail abstractive summarization task (Hermann et al., 2015) with regularizer DJα . Data pre-processing and other hyperparameter settings follow Lewis et al. (2019). Results in Table 3 show that optimal values of ROUGE -L (Lin, 2004), the evaluation metric, can be achieved by regularizing with DJα for different values of α. Notably, the entropy is virtually the same for the models that achieve top performance, demonstrating the closer relationship of performance with model entropy than with α, discussed further in §4.3. 4.3 Significance of α and Model Entropy We look at the strength of the relationship between the evaluation metrics and both α and the model’s entropy. Figure 3 shows a quadratic relationship between model entropy and BLEU. On the other hand, the relationship between α (coloring of points) and BLEU is not a"
2020.acl-main.615,D11-1139,0,0.0959788,"Missing"
2020.acl-main.615,N19-4009,0,0.0156882,"two different sizes on the following tasks: WMT’14 German-to-English (De-En) (Bojar et al., 2014), IWSLT’14 German-to-English (De-En) (Cettolo et al., 2012), and Multitarget TED Talks Task (MTTT) French-to-English (Fr–En) and Japanese-to-English (Ja-En) tasks (Duh, 2018). For the larger WMT data set, we train fewer models using coarser-grained α and β ranges. We perform experiments for both Transformers (Vaswani et al., 2017) and convolutional sequence-to-sequence models (Gehring et al., 2017). For reproducibility and comparability, we use the data pre-processing scripts provided by fairseq (Ott et al., 2019) and follow recommended hyperparameter settings from previous work (Vaswani et al., 2017; Gehring et al., 2017) for baseline models. We use SacreBLEU (Post, 2018) to calculate BLEU scores (Papineni et al., 2002). Specific data pre-processing steps and model hyperparameter details are provided in App. G. Decoding is performed with length-normalized beam search with a beam size of 5 unless otherwise stated. Early stopping was used during training; model parame10 We only report results with generator function G = −H as results using G(z) = ||z||22 were consistently worse and often did not improve"
2020.acl-main.615,P02-1040,0,0.106575,"-English (Fr–En) and Japanese-to-English (Ja-En) tasks (Duh, 2018). For the larger WMT data set, we train fewer models using coarser-grained α and β ranges. We perform experiments for both Transformers (Vaswani et al., 2017) and convolutional sequence-to-sequence models (Gehring et al., 2017). For reproducibility and comparability, we use the data pre-processing scripts provided by fairseq (Ott et al., 2019) and follow recommended hyperparameter settings from previous work (Vaswani et al., 2017; Gehring et al., 2017) for baseline models. We use SacreBLEU (Post, 2018) to calculate BLEU scores (Papineni et al., 2002). Specific data pre-processing steps and model hyperparameter details are provided in App. G. Decoding is performed with length-normalized beam search with a beam size of 5 unless otherwise stated. Early stopping was used during training; model parame10 We only report results with generator function G = −H as results using G(z) = ||z||22 were consistently worse and often did not improve on the baseline; these results may be seen in App. H. 6874 ˆ θ ) vs. BLEU on IWSLT’14 German to English (De-En) and Multitarget TED Talks Figure 3: Model entropy H(p Task French to English (Fr-En) using a Trans"
2020.acl-main.615,W18-1819,0,0.105107,"Missing"
2020.acl-main.615,W18-6319,0,0.0187951,"target TED Talks Task (MTTT) French-to-English (Fr–En) and Japanese-to-English (Ja-En) tasks (Duh, 2018). For the larger WMT data set, we train fewer models using coarser-grained α and β ranges. We perform experiments for both Transformers (Vaswani et al., 2017) and convolutional sequence-to-sequence models (Gehring et al., 2017). For reproducibility and comparability, we use the data pre-processing scripts provided by fairseq (Ott et al., 2019) and follow recommended hyperparameter settings from previous work (Vaswani et al., 2017; Gehring et al., 2017) for baseline models. We use SacreBLEU (Post, 2018) to calculate BLEU scores (Papineni et al., 2002). Specific data pre-processing steps and model hyperparameter details are provided in App. G. Decoding is performed with length-normalized beam search with a beam size of 5 unless otherwise stated. Early stopping was used during training; model parame10 We only report results with generator function G = −H as results using G(z) = ||z||22 were consistently worse and often did not improve on the baseline; these results may be seen in App. H. 6874 ˆ θ ) vs. BLEU on IWSLT’14 German to English (De-En) and Multitarget TED Talks Figure 3: Model entropy"
2020.acl-main.615,P16-1162,0,0.0416353,"in such models pθ (y |x) is factored as: pθ (y |x) = pθ (y1 |x) · · · pθ (yn |x, y&lt;n ) (1) where pθ (yi |x, y&lt;i ) is defined by a softmax over the output of the final fully connected layer of the network. Generation is performed using greedy search, beam search or a sampling scheme. Of the candidate sequences generated, the one with the highest probability under the model pθ is returned as the model’s prediction. One way of selecting the parameters θ is to minimize the KL-divergence between the empirical 1 Targets yi may also be characters or subwords; our experiments use byte-pair encoding (Sennrich et al., 2016) (2) = H(˜ p, p ) − H(˜ p) |{z θ} |{z } cross-entropy loss (3) constant w.r.t. θ However, fitting a model that perfectly approximates the empirical distribution is, in general, fraught with problems (Hastie et al., 2001). The goal of learning is to generalize beyond the observed data. Exactly fitting the empirical distribution, often termed overfitting, is therefore not an ideal goal and for language generation models specifically, does not go hand-in-hand with the ability of a model to generate desirable text (Bengio et al., 2015). Consequently, it is advisable to minimize a regularized objec"
2020.acl-main.615,D07-1070,0,0.0588537,"he mathematical relationship to entropy regularization becomes more convoluted. Therefore, we leave the application of GER to other distributions as a topic for future work. 6 Related Work Entropy regularization has a long history in reinforcement learning (Williams and Peng, 1991; Mnih et al., 2016; Fox et al., 2016; Haarnoja et al., 2018) where it has provided substantial improvements in exploration. Such methods have since been adapted for supervised learning where they have proven to be reliable forms of regularization for various probabilistic modeling tasks (Grandvalet and Bengio, 2005; Smith and Eisner, 2007). More recently, interpolating between exclusive and inclusive KL divergences has been explored in NMT by Xiao et al. (2019). However, this method was used for the objective function (i.e. between p˜ and pθ ) and not as a regularization technique (i.e. between a baseline distribution q and pθ ). Li et al. (2020) construct a baseline distribution q as a function of word embedding distances to to use in place of the uniform distribution u in the label smoothing equation. This work is complementary to ours, as q can similarly be used in place of u with GER. Finally, our work is closest to that of"
2020.acl-main.615,W14-3302,0,\N,Missing
2020.acl-main.659,J93-2004,0,0.0693292,"ese had the second and third largest sets of training data for BERT respectively (refer to Table 1 in the appendices). We speculate that this may be because in the treebanks used, Japanese and Arabic have a longer average sentence length than other languages. DSprP +F W Results Following the postprocessing step, the difference in DSpr (shown in Figure 3) is far less stark than previously suggested—the mean difference between the two across all nine languages is just 0.0006 (in favour of the parser). Notice in particular the improvement 6 We used the UD treebanks rather than the Penn-Treebank (Marcus et al., 1993), and experimented on the final hidden layer of multilingual BERT using a subset of 12,000 sentences from the larger treebanks. This renders our numbers incomparable to those found in Hewitt and Manning (2019). 7392 Figure 3: DSprP +F W results—DSpr following the application of Prim’s then Floyd–Warshall to D. for both Arabic and Japanese—where previously (in the vanilla DSpr) the structured perceptron vastly underperformed, the post-processing step closes the gap almost entirely. Though Prop. 1 implies that we do not need to consider the full pairwise output of dB to account for global proper"
2020.acl-main.659,P18-1198,0,0.0368841,"troduction Recently, unsupervised sentence encoders such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019) have become popular within NLP. These pre-trained models boast impressive performance when used in many language-related tasks, but this gain has come at the cost of interpretability. A natural question to ask, then, is whether these models encode the traditional linguistic structures one might expect, such as part-of-speech tags or dependency trees. To this end, researchers have invested in the design of diagnostic tools commonly referred to as probes (Alain and Bengio, 2017; Conneau et al., 2018; Hupkes et al., 2018; Poliak et al., 2018; Marvin and Linzen, 2018; Niven and Kao, 2019). Probes are supervised models designed to extract a target linguistic structure from the output representation learned by another model. Based on the authors’ reading of the probing literature, there is little consensus on where to draw the line between probes and models for performing a target task (e.g. a part-of-speech tagger versus a probe for identifying parts of speech). The main distinction appears to be one of researcher intent: probes are, in essence, a visualisation method (Hupkes et al., 2018)."
2020.acl-main.659,D18-1151,0,0.0390836,"(Peters et al., 2018) and BERT (Devlin et al., 2019) have become popular within NLP. These pre-trained models boast impressive performance when used in many language-related tasks, but this gain has come at the cost of interpretability. A natural question to ask, then, is whether these models encode the traditional linguistic structures one might expect, such as part-of-speech tags or dependency trees. To this end, researchers have invested in the design of diagnostic tools commonly referred to as probes (Alain and Bengio, 2017; Conneau et al., 2018; Hupkes et al., 2018; Poliak et al., 2018; Marvin and Linzen, 2018; Niven and Kao, 2019). Probes are supervised models designed to extract a target linguistic structure from the output representation learned by another model. Based on the authors’ reading of the probing literature, there is little consensus on where to draw the line between probes and models for performing a target task (e.g. a part-of-speech tagger versus a probe for identifying parts of speech). The main distinction appears to be one of researcher intent: probes are, in essence, a visualisation method (Hupkes et al., 2018). Their goal is not to best the state of the art, but rather to indi"
2020.acl-main.659,N19-1423,0,0.548927,"syntactic probes would do better to make use of existing techniques, we compare the structural probe to a more traditional parser with an identical lightweight parameterisation. The parser outperforms structural probe on UUAS in seven of nine analysed languages, often by a substantial amount (e.g. by 11.1 points in English). Under a second less common metric, however, there is the opposite trend—the structural probe outperforms the parser. This begs the question: which metric should we prefer? 1 Introduction Recently, unsupervised sentence encoders such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019) have become popular within NLP. These pre-trained models boast impressive performance when used in many language-related tasks, but this gain has come at the cost of interpretability. A natural question to ask, then, is whether these models encode the traditional linguistic structures one might expect, such as part-of-speech tags or dependency trees. To this end, researchers have invested in the design of diagnostic tools commonly referred to as probes (Alain and Bengio, 2017; Conneau et al., 2018; Hupkes et al., 2018; Poliak et al., 2018; Marvin and Linzen, 2018; Niven and Kao, 2019). Probes"
2020.acl-main.659,D19-1275,0,0.122757,"on where to draw the line between probes and models for performing a target task (e.g. a part-of-speech tagger versus a probe for identifying parts of speech). The main distinction appears to be one of researcher intent: probes are, in essence, a visualisation method (Hupkes et al., 2018). Their goal is not to best the state of the art, but rather to indicate whether certain information is readily available in a model—probes should not “dig” for information, they should just expose what is already present. Indeed, a sufficiently expressive probe with enough training data could learn any task (Hewitt and Liang, 2019), but this tells us nothing about a representation, so it is beside the point. For this reason, probes are made “simple” (Liu et al., 2019), which usually means they are minimally parameterised.1 Syntactic probes, then, are designed to measure the extent to which a target model encodes syntax. A popular example is the structural probe (Hewitt and Manning, 2019), used to compare the syntax that is decodable from different contextualised word embeddings. Rather than adopting methodology from the parsing literature, this probe utilises a novel approach for syntax extraction. However, the precise"
2020.acl-main.659,N19-1419,0,0.285711,"whether certain information is readily available in a model—probes should not “dig” for information, they should just expose what is already present. Indeed, a sufficiently expressive probe with enough training data could learn any task (Hewitt and Liang, 2019), but this tells us nothing about a representation, so it is beside the point. For this reason, probes are made “simple” (Liu et al., 2019), which usually means they are minimally parameterised.1 Syntactic probes, then, are designed to measure the extent to which a target model encodes syntax. A popular example is the structural probe (Hewitt and Manning, 2019), used to compare the syntax that is decodable from different contextualised word embeddings. Rather than adopting methodology from the parsing literature, this probe utilises a novel approach for syntax extraction. However, the precise motivation for this novel approach is not immediately clear, since it has nothing to do with model complexity, and appears orthogonal to the goal of a probe. Probes are designed to help researchers understand what information exists in a model, and unfamiliar ways of measuring this information may obscure whether we are actually gaining an insight about the rep"
2020.acl-main.659,P19-1459,0,0.0226516,"d BERT (Devlin et al., 2019) have become popular within NLP. These pre-trained models boast impressive performance when used in many language-related tasks, but this gain has come at the cost of interpretability. A natural question to ask, then, is whether these models encode the traditional linguistic structures one might expect, such as part-of-speech tags or dependency trees. To this end, researchers have invested in the design of diagnostic tools commonly referred to as probes (Alain and Bengio, 2017; Conneau et al., 2018; Hupkes et al., 2018; Poliak et al., 2018; Marvin and Linzen, 2018; Niven and Kao, 2019). Probes are supervised models designed to extract a target linguistic structure from the output representation learned by another model. Based on the authors’ reading of the probing literature, there is little consensus on where to draw the line between probes and models for performing a target task (e.g. a part-of-speech tagger versus a probe for identifying parts of speech). The main distinction appears to be one of researcher intent: probes are, in essence, a visualisation method (Hupkes et al., 2018). Their goal is not to best the state of the art, but rather to indicate whether certain i"
2020.acl-main.659,L16-1262,0,0.032545,"Missing"
2020.acl-main.659,N18-1202,0,0.0372858,"ly obvious. To explore whether syntactic probes would do better to make use of existing techniques, we compare the structural probe to a more traditional parser with an identical lightweight parameterisation. The parser outperforms structural probe on UUAS in seven of nine analysed languages, often by a substantial amount (e.g. by 11.1 points in English). Under a second less common metric, however, there is the opposite trend—the structural probe outperforms the parser. This begs the question: which metric should we prefer? 1 Introduction Recently, unsupervised sentence encoders such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019) have become popular within NLP. These pre-trained models boast impressive performance when used in many language-related tasks, but this gain has come at the cost of interpretability. A natural question to ask, then, is whether these models encode the traditional linguistic structures one might expect, such as part-of-speech tags or dependency trees. To this end, researchers have invested in the design of diagnostic tools commonly referred to as probes (Alain and Bengio, 2017; Conneau et al., 2018; Hupkes et al., 2018; Poliak et al., 2018; Marvin and Linzen, 201"
2020.acl-main.659,2020.acl-main.420,1,0.811013,"is probe utilises a novel approach for syntax extraction. However, the precise motivation for this novel approach is not immediately clear, since it has nothing to do with model complexity, and appears orthogonal to the goal of a probe. Probes are designed to help researchers understand what information exists in a model, and unfamiliar ways of measuring this information may obscure whether we are actually gaining an insight about the representation we wish to examine, or the tool of measurement itself. 1 An information-theoretic take on probe complexity is the subject of concurrent work; see Pimentel et al. (2020). 7389 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7389–7395 c July 5 - 10, 2020. 2020 Association for Computational Linguistics ∆t (wi , wj ), is defined as the shortest path from wi to wj in the tree t where each edge has weight 1. Note that ∆t (·, ·) is a distance in the technical sense of the word: it is non-negative, symmetric, and satisfies the triangle inequality. My displeasure in everything displeases me 1 2 3 4 5 6 Figure 1: Example of an undirected dependency tree. We observe that the syntactic distance between displeases and everyt"
2020.acl-main.659,W18-5441,0,0.0318421,"Missing"
2020.acl-main.659,H05-1066,0,\N,Missing
2020.acl-main.659,D18-1007,0,\N,Missing
2020.acl-main.695,E17-2067,0,0.0305779,"en a corpus and lexicon. First, we cluster lexicon forms into cells. Then we cluster forms into paradigms given their fixed cell membership. To maintain tractability, clustering assumes a one-to-one mapping of forms to slots. Following cell and paradigm clustering, we predict forms to realize empty slots given one of the lexicon forms assigned to a cell in Clustering into Cells We use a heuristic method to determine the number of cells and what lexicon forms to assign to each. Inspired by work on inductive biases in word embeddings (Pennington et al., 2014; Trask et al., 2015; Goldberg, 2016; Avraham and Goldberg, 2017; Tu et al., 2017), we train morphosyntactically biased embeddings on the corpus and use them to k-means cluster lexicon forms into cells. Following Erdmann et al. (2018), we emphasize morphosyntactically salient dimensions in embedding space by manipulating hyperparameters in fastText (Bojanowski et al., 2017). Specifically, to encourage grouping of morphologically related words, fastText computes a word’s embedding as the sum of its subword embeddings for all subword sequences between 3 and 6 characters long (Schütze, 1993). We shorten this range to 2 to 4 to bias the grouping toward shared"
2020.acl-main.695,E17-1032,0,0.183936,"Missing"
2020.acl-main.695,P19-1376,0,0.0196105,"y. Some cognitive works suggest the PCFP cannot be too difficult for any language (Dale et al., 1998; Ackerman and Malouf, 2013, 2015; Blevins et al., 2017; Cotterell et al., 2019). Neural models can test and extend such proposals (Cotterell et al., 2018a; Silfverberg and Hulden, 2018). A related vein of work discusses how speakers inflect nonce words (Berko, 1958; Plunkett and Juola, 1999; Yang, 2015), e.g., is the past tense of sping, spinged or spung? There is a long tradition of modeling past-tense generation with neural networks (Rumelhart and McClelland, 1986; Kirov and Cotterell, 2018; Corkery et al., 2019). On the engineering side, Durrett and DeNero (2013) inspired much recent work, which has since benefited from large inflectional datasets (Kirov et al., 2018) and advances in neural sequence modeling (Bahdanau et al., 2015). Shared tasks have drawn extra attention to the PCFP (Cotterell et al., 2016a, 2017, 2018c; McCarthy et al., 2019). 3 The Paradigm Discovery Problem Paradigm discovery is a natural next step in computational morphology, building on related minimally or indirectly supervised works (§2.2) to bridge the gap between unsupervised traditions (§2.1) and supervised work on the PCF"
2020.acl-main.695,Q19-1021,1,0.85462,"dels can guess a word’s plural having only seen its singular, but the child must bootstrap morphological knowledge from scratch, first learning that singular–plural is a relevant distinction. Thus, the PDP must be at least partially solved before the PCFP can be attempted. Yet, as a supervised task, the PCFP is more easily studied, and has received much attention on its own, especially from the word-and-paradigm camp of morphological theory. Some cognitive works suggest the PCFP cannot be too difficult for any language (Dale et al., 1998; Ackerman and Malouf, 2013, 2015; Blevins et al., 2017; Cotterell et al., 2019). Neural models can test and extend such proposals (Cotterell et al., 2018a; Silfverberg and Hulden, 2018). A related vein of work discusses how speakers inflect nonce words (Berko, 1958; Plunkett and Juola, 1999; Yang, 2015), e.g., is the past tense of sping, spinged or spung? There is a long tradition of modeling past-tense generation with neural networks (Rumelhart and McClelland, 1986; Kirov and Cotterell, 2018; Corkery et al., 2019). On the engineering side, Durrett and DeNero (2013) inspired much recent work, which has since benefited from large inflectional datasets (Kirov et al., 2018)"
2020.acl-main.695,N18-2087,1,0.928989,"must bootstrap morphological knowledge from scratch, first learning that singular–plural is a relevant distinction. Thus, the PDP must be at least partially solved before the PCFP can be attempted. Yet, as a supervised task, the PCFP is more easily studied, and has received much attention on its own, especially from the word-and-paradigm camp of morphological theory. Some cognitive works suggest the PCFP cannot be too difficult for any language (Dale et al., 1998; Ackerman and Malouf, 2013, 2015; Blevins et al., 2017; Cotterell et al., 2019). Neural models can test and extend such proposals (Cotterell et al., 2018a; Silfverberg and Hulden, 2018). A related vein of work discusses how speakers inflect nonce words (Berko, 1958; Plunkett and Juola, 1999; Yang, 2015), e.g., is the past tense of sping, spinged or spung? There is a long tradition of modeling past-tense generation with neural networks (Rumelhart and McClelland, 1986; Kirov and Cotterell, 2018; Corkery et al., 2019). On the engineering side, Durrett and DeNero (2013) inspired much recent work, which has since benefited from large inflectional datasets (Kirov et al., 2018) and advances in neural sequence modeling (Bahdanau et al., 2015). Shared"
2020.acl-main.695,K18-3001,1,0.853843,"rvised morphological paradigm completion. Given only a small corpus and lexicon of verbal lemmata, participating systems must propose full paradigms for each lemma. By contrast, our framework does not reveal how many paradigms should be generated, nor do we privilege a specific form as the lemma, but we do use a larger lexicon of exclusively verbal or nominal forms. Their proposed baseline uses distributional context for POS tagging and features, but does not train embeddings as the corpus is small. 2.2 Subtasks of Paradigm Discovery A few works address subtasks of the PDP. Erdmann and Habash (2018) learn paradigm membership from raw text, but do not sort paradigms into cells. Boyé and Schalchli (2019) discuss the paradigm cell finding problem, identifying the cell (but not paradigm) realized by a given form. Lee (2015) clusters forms into cells across inflection classes. Beniamine et al. (2018) group paradigms into inflection classes, and Eskander et al. (2013) induce inflection classes and lemmata from cell labels. 2.3 The Paradigm Cell Filling Problem The PCFP is the task of predicting unseen inflected forms given morphologically labeled input. PCFP models can guess a word’s plural ha"
2020.acl-main.695,K17-2001,1,0.898483,"Missing"
2020.acl-main.695,P16-1156,1,0.930366,"i.e., they realize the same morphosyntactic properties 3. SG . PRES, but in different paradigms. Acquiring such paradigmatic knowledge enables us to produce unseen inflectional variants of new vocabulary items, i.e. to complete morphological paradigms. Much work has addressed this task, which Ackerman et al. (2009) call the paradigm cell filling problem (PCFP),1 but few have discussed inducing paradigmatic knowledge from scratch, which we call the paradigm discovery problem (PDP).2 1 In the NLP literature, this task is called morphological reinflection or morphological inflection generation (Cotterell et al., 2016a); this is only a difference in nomenclature. 2 Elsner et al. (2019) call the task the paradigm cell discovery problem; we drop cell to distinguish our task from As an unsupervised task, the PDP poses challenges for modeling and evaluation and has yet to be attempted in its full form (Elsner et al., 2019). However, we contend there is much to be gained from formalizing and studying the PDP. There are insights for cognitive modeling to be won (Pinker, 2001; Goldwater, 2007) and intuitions on combating sparse data for language generation (King and White, 2018) to be accrued. Unsupervised langua"
2020.acl-main.695,J98-3001,0,0.192601,"Missing"
2020.acl-main.695,D11-1057,0,0.266604,"(e.g., bring 6= br + ing) with high accuracy, they do not attempt to solve the PDP. They do, however, reveal which forms take the same affixes (e.g., walked, talked), not which forms occupy the same cell (e.g., walked, brought). Indeed, they explicitly struggle with irregular morphology. Segmenters also cannot easily model non-concatenative phenomena like ablaut, vowel harmony and templatic processes. Two works have proposed tasks which can be considered alternative formulations of the PDP, using either minimal or indirect supervision to bootstrap their models. We discuss each in turn. First, Dreyer and Eisner (2011) use a generative model to cluster forms into paradigms and cells with a Bayesian non-parametric mixture of weighted finite-state transducers. They present a PDP framework which, in principle, could be fully unsupervised, but their model requires a small seed of labeled data to get key information like the number of cells distinguished, making it less relevant cognitively. In contrast, our task is not directly supervised and focuses on distributional context. Second, contemporaneous to our work, Jin et al. (2020) propose a similar framework for SIGMORPHON 2020’s shared task on unsupervised mor"
2020.acl-main.695,N13-1138,0,0.166655,"be too difficult for any language (Dale et al., 1998; Ackerman and Malouf, 2013, 2015; Blevins et al., 2017; Cotterell et al., 2019). Neural models can test and extend such proposals (Cotterell et al., 2018a; Silfverberg and Hulden, 2018). A related vein of work discusses how speakers inflect nonce words (Berko, 1958; Plunkett and Juola, 1999; Yang, 2015), e.g., is the past tense of sping, spinged or spung? There is a long tradition of modeling past-tense generation with neural networks (Rumelhart and McClelland, 1986; Kirov and Cotterell, 2018; Corkery et al., 2019). On the engineering side, Durrett and DeNero (2013) inspired much recent work, which has since benefited from large inflectional datasets (Kirov et al., 2018) and advances in neural sequence modeling (Bahdanau et al., 2015). Shared tasks have drawn extra attention to the PCFP (Cotterell et al., 2016a, 2017, 2018c; McCarthy et al., 2019). 3 The Paradigm Discovery Problem Paradigm discovery is a natural next step in computational morphology, building on related minimally or indirectly supervised works (§2.2) to bridge the gap between unsupervised traditions (§2.1) and supervised work on the PCFP (§2.3). In the PCFP, 7779 Corpus The cat watched m"
2020.acl-main.695,W18-5806,1,0.852738,"ared task on unsupervised morphological paradigm completion. Given only a small corpus and lexicon of verbal lemmata, participating systems must propose full paradigms for each lemma. By contrast, our framework does not reveal how many paradigms should be generated, nor do we privilege a specific form as the lemma, but we do use a larger lexicon of exclusively verbal or nominal forms. Their proposed baseline uses distributional context for POS tagging and features, but does not train embeddings as the corpus is small. 2.2 Subtasks of Paradigm Discovery A few works address subtasks of the PDP. Erdmann and Habash (2018) learn paradigm membership from raw text, but do not sort paradigms into cells. Boyé and Schalchli (2019) discuss the paradigm cell finding problem, identifying the cell (but not paradigm) realized by a given form. Lee (2015) clusters forms into cells across inflection classes. Beniamine et al. (2018) group paradigms into inflection classes, and Eskander et al. (2013) induce inflection classes and lemmata from cell labels. 2.3 The Paradigm Cell Filling Problem The PCFP is the task of predicting unseen inflected forms given morphologically labeled input. PCFP models can guess a word’s plural ha"
2020.acl-main.695,W19-4214,1,0.815752,"cell. Our algorithm greedily builds paradigms cell by cell. To gauge the quality of a candidate paradigm, we first identify its base and exponents. Following Beniamine et al. (2018), we define π’s base, bπ , as the longest common subsequence shared by all forms in π.56 For each form f in π, we define the exponent xf as the subsequences of f that remain after removing bπ , i.e., xf is a tuple of affixes. For example, if π contains words wxyxz and axx, bπ is xx and the exponents are (&lt;w, y, z&gt;) and (&lt;a), respectively.7 Inspired by unsupervised maximum matching in greedy tokenization (Guo, 1997; Erdmann et al., 2019), we define the following paradigm score function: score(π) = X   |bπ |− |xf | (2) 1: 2: 3: 4: 5: 6: 7: 8: fj0 ∈Cj 9: 10: 11: 12: 13: 5 The fact that we use a subsequence, instead of a substring, means that we can handle non-concatenative morphology. 6 We note that the longest common subsequence may be found with a polynomial-time dynamic program; however, there will not exist an algorithm whose runtime is polynomial in the number of strings unless P = NP (Maier, 1978). 7 We use word start (&lt;) and end (&gt;) tokens to distinguish exponents; they do not count as exponent characters in eq. (2). 8"
2020.acl-main.695,P18-2089,1,0.840933,"ng assumes a one-to-one mapping of forms to slots. Following cell and paradigm clustering, we predict forms to realize empty slots given one of the lexicon forms assigned to a cell in Clustering into Cells We use a heuristic method to determine the number of cells and what lexicon forms to assign to each. Inspired by work on inductive biases in word embeddings (Pennington et al., 2014; Trask et al., 2015; Goldberg, 2016; Avraham and Goldberg, 2017; Tu et al., 2017), we train morphosyntactically biased embeddings on the corpus and use them to k-means cluster lexicon forms into cells. Following Erdmann et al. (2018), we emphasize morphosyntactically salient dimensions in embedding space by manipulating hyperparameters in fastText (Bojanowski et al., 2017). Specifically, to encourage grouping of morphologically related words, fastText computes a word’s embedding as the sum of its subword embeddings for all subword sequences between 3 and 6 characters long (Schütze, 1993). We shorten this range to 2 to 4 to bias the grouping toward shared affixes rather than (usually longer) shared stems. This helps recognize that the same affix is likely to realize the same cell, e.g., watch +ed and follow +ed. We limit t"
2020.acl-main.695,D13-1105,1,0.825814,"nominal forms. Their proposed baseline uses distributional context for POS tagging and features, but does not train embeddings as the corpus is small. 2.2 Subtasks of Paradigm Discovery A few works address subtasks of the PDP. Erdmann and Habash (2018) learn paradigm membership from raw text, but do not sort paradigms into cells. Boyé and Schalchli (2019) discuss the paradigm cell finding problem, identifying the cell (but not paradigm) realized by a given form. Lee (2015) clusters forms into cells across inflection classes. Beniamine et al. (2018) group paradigms into inflection classes, and Eskander et al. (2013) induce inflection classes and lemmata from cell labels. 2.3 The Paradigm Cell Filling Problem The PCFP is the task of predicting unseen inflected forms given morphologically labeled input. PCFP models can guess a word’s plural having only seen its singular, but the child must bootstrap morphological knowledge from scratch, first learning that singular–plural is a relevant distinction. Thus, the PDP must be at least partially solved before the PCFP can be attempted. Yet, as a supervised task, the PCFP is more easily studied, and has received much attention on its own, especially from the word-"
2020.acl-main.695,W99-0904,0,0.49395,"Missing"
2020.acl-main.695,J01-2001,0,0.284034,"Missing"
2020.acl-main.695,K19-1014,1,0.855013,"onal data. The use of orthographic data for morphological tasks is problematic, but standard in the field, due to scarcity of phonologically transcribed data (Malouf et al., 2020). 7780 Predictions paradigm 1 paradigm 2 paradigm 3 paradigm 4 cell 1 watched followed «seed» «seened» cell 2 watching «following» «seeing» «seening» cell 3 «watches» follows «sees» «seens» cell 4 «watch» «follow» see seen Table 2: Toy predictions made from the corpus and lexicon in Table 1, to be evaluated against the toy gold grid. Again, bracketed «forms» are those not occurring in the lexicon. in UD and UniMorph (Gorman et al., 2019; Malouf et al., 2020). Unlike the gold grid, the lexicon retains overabundant realizations, requiring systems to handle such phenomena. For each language, the raw sentences used to augment the corpus add over 1 million additional words. For German and Russian, we sample sentences from OpenSubtitles (Lison and Tiedemann, 2016), for Latin, the Latin Library (Johnson et al., 2016), and for English and Arabic, Gigaword (Parker et al., 2011a,b). Supplementary sentences are preprocessed via Moses (Koehn et al., 2007) to split punctuation, and, for supported languages, clitics. Table 3 shows corpus"
2020.acl-main.695,J97-4004,0,0.109447,"m the same cell. Our algorithm greedily builds paradigms cell by cell. To gauge the quality of a candidate paradigm, we first identify its base and exponents. Following Beniamine et al. (2018), we define π’s base, bπ , as the longest common subsequence shared by all forms in π.56 For each form f in π, we define the exponent xf as the subsequences of f that remain after removing bπ , i.e., xf is a tuple of affixes. For example, if π contains words wxyxz and axx, bπ is xx and the exponents are (&lt;w, y, z&gt;) and (&lt;a), respectively.7 Inspired by unsupervised maximum matching in greedy tokenization (Guo, 1997; Erdmann et al., 2019), we define the following paradigm score function: score(π) = X   |bπ |− |xf | (2) 1: 2: 3: 4: 5: 6: 7: 8: fj0 ∈Cj 9: 10: 11: 12: 13: 5 The fact that we use a subsequence, instead of a substring, means that we can handle non-concatenative morphology. 6 We note that the longest common subsequence may be found with a polynomial-time dynamic program; however, there will not exist an algorithm whose runtime is polynomial in the number of strings unless P = NP (Maier, 1978). 7 We use word start (&lt;) and end (&gt;) tokens to distinguish exponents; they do not count as exponent c"
2020.acl-main.695,2020.acl-main.598,0,0.247281,"ect supervision to bootstrap their models. We discuss each in turn. First, Dreyer and Eisner (2011) use a generative model to cluster forms into paradigms and cells with a Bayesian non-parametric mixture of weighted finite-state transducers. They present a PDP framework which, in principle, could be fully unsupervised, but their model requires a small seed of labeled data to get key information like the number of cells distinguished, making it less relevant cognitively. In contrast, our task is not directly supervised and focuses on distributional context. Second, contemporaneous to our work, Jin et al. (2020) propose a similar framework for SIGMORPHON 2020’s shared task on unsupervised morphological paradigm completion. Given only a small corpus and lexicon of verbal lemmata, participating systems must propose full paradigms for each lemma. By contrast, our framework does not reveal how many paradigms should be generated, nor do we privilege a specific form as the lemma, but we do use a larger lexicon of exclusively verbal or nominal forms. Their proposed baseline uses distributional context for POS tagging and features, but does not train embeddings as the corpus is small. 2.2 Subtasks of Paradig"
2020.acl-main.695,W18-3605,0,0.0295523,"phological inflection generation (Cotterell et al., 2016a); this is only a difference in nomenclature. 2 Elsner et al. (2019) call the task the paradigm cell discovery problem; we drop cell to distinguish our task from As an unsupervised task, the PDP poses challenges for modeling and evaluation and has yet to be attempted in its full form (Elsner et al., 2019). However, we contend there is much to be gained from formalizing and studying the PDP. There are insights for cognitive modeling to be won (Pinker, 2001; Goldwater, 2007) and intuitions on combating sparse data for language generation (King and White, 2018) to be accrued. Unsupervised language processing also has natural applications in the documentation of endangered languages (Zamaraeva et al., 2019) where a lot of annotated data is never likely to exist. Our formalization of the PDP offers a starting point for future work on unsupervised morphological paradigm completion. Our paper presents a concrete formalization of the PDP. Then, as a baseline for future work, we introduce a heuristic benchmark system. Our benchmark system takes an unannotated text corpus and a lexicon of words from the corpus to be analyzed. It first clusters the lexicon"
2020.acl-main.695,L18-1293,1,0.928002,"erell et al., 2019). Neural models can test and extend such proposals (Cotterell et al., 2018a; Silfverberg and Hulden, 2018). A related vein of work discusses how speakers inflect nonce words (Berko, 1958; Plunkett and Juola, 1999; Yang, 2015), e.g., is the past tense of sping, spinged or spung? There is a long tradition of modeling past-tense generation with neural networks (Rumelhart and McClelland, 1986; Kirov and Cotterell, 2018; Corkery et al., 2019). On the engineering side, Durrett and DeNero (2013) inspired much recent work, which has since benefited from large inflectional datasets (Kirov et al., 2018) and advances in neural sequence modeling (Bahdanau et al., 2015). Shared tasks have drawn extra attention to the PCFP (Cotterell et al., 2016a, 2017, 2018c; McCarthy et al., 2019). 3 The Paradigm Discovery Problem Paradigm discovery is a natural next step in computational morphology, building on related minimally or indirectly supervised works (§2.2) to bridge the gap between unsupervised traditions (§2.1) and supervised work on the PCFP (§2.3). In the PCFP, 7779 Corpus The cat watched me watching it . I followed the show but she had n’t seen it . Let ’s see who follows your logic . Lexicon w"
2020.acl-main.695,P07-2045,0,0.00621406,"ain, bracketed «forms» are those not occurring in the lexicon. in UD and UniMorph (Gorman et al., 2019; Malouf et al., 2020). Unlike the gold grid, the lexicon retains overabundant realizations, requiring systems to handle such phenomena. For each language, the raw sentences used to augment the corpus add over 1 million additional words. For German and Russian, we sample sentences from OpenSubtitles (Lison and Tiedemann, 2016), for Latin, the Latin Library (Johnson et al., 2016), and for English and Arabic, Gigaword (Parker et al., 2011a,b). Supplementary sentences are preprocessed via Moses (Koehn et al., 2007) to split punctuation, and, for supported languages, clitics. Table 3 shows corpus and lexicon sizes. 3.3 Metrics A system attemping the PDP is expected to output a morphologically organized grid in which rows and columns are arbitrarily ordered, but ideally, each row corresponds to a gold paradigm and each column to a gold cell. Aligning rows to paradigms and columns to cells is non-trivial, making it difficult to simply compute accuracy over gold grid slots. Furthermore, cluster-based metrics (Rosenberg and Hirschberg, 2007) are difficult to apply as forms can appear in multiple columns or r"
2020.acl-main.695,N15-2022,0,0.0209498,"digms should be generated, nor do we privilege a specific form as the lemma, but we do use a larger lexicon of exclusively verbal or nominal forms. Their proposed baseline uses distributional context for POS tagging and features, but does not train embeddings as the corpus is small. 2.2 Subtasks of Paradigm Discovery A few works address subtasks of the PDP. Erdmann and Habash (2018) learn paradigm membership from raw text, but do not sort paradigms into cells. Boyé and Schalchli (2019) discuss the paradigm cell finding problem, identifying the cell (but not paradigm) realized by a given form. Lee (2015) clusters forms into cells across inflection classes. Beniamine et al. (2018) group paradigms into inflection classes, and Eskander et al. (2013) induce inflection classes and lemmata from cell labels. 2.3 The Paradigm Cell Filling Problem The PCFP is the task of predicting unseen inflected forms given morphologically labeled input. PCFP models can guess a word’s plural having only seen its singular, but the child must bootstrap morphological knowledge from scratch, first learning that singular–plural is a relevant distinction. Thus, the PDP must be at least partially solved before the PCFP ca"
2020.acl-main.695,L16-1147,0,0.0156269,"eening» cell 3 «watches» follows «sees» «seens» cell 4 «watch» «follow» see seen Table 2: Toy predictions made from the corpus and lexicon in Table 1, to be evaluated against the toy gold grid. Again, bracketed «forms» are those not occurring in the lexicon. in UD and UniMorph (Gorman et al., 2019; Malouf et al., 2020). Unlike the gold grid, the lexicon retains overabundant realizations, requiring systems to handle such phenomena. For each language, the raw sentences used to augment the corpus add over 1 million additional words. For German and Russian, we sample sentences from OpenSubtitles (Lison and Tiedemann, 2016), for Latin, the Latin Library (Johnson et al., 2016), and for English and Arabic, Gigaword (Parker et al., 2011a,b). Supplementary sentences are preprocessed via Moses (Koehn et al., 2007) to split punctuation, and, for supported languages, clitics. Table 3 shows corpus and lexicon sizes. 3.3 Metrics A system attemping the PDP is expected to output a morphologically organized grid in which rows and columns are arbitrarily ordered, but ideally, each row corresponds to a gold paradigm and each column to a gold cell. Aligning rows to paradigms and columns to cells is non-trivial, making it diffi"
2020.acl-main.695,2020.scil-1.52,0,0.0134249,"i.e., slots realized by multiple forms, and remove all but the most frequently attested realization in UD. While some languages permit overabundance (Thornton, 2010), it often indicates typographical or annotation errors 3 Aligning UniMorph and UD requires removing diacritics in (Latin and Arabic) UniMorph corpora to match UD. This can obscure some morphosyntactic distinctions but is more consistent with natural orthography in distributional data. The use of orthographic data for morphological tasks is problematic, but standard in the field, due to scarcity of phonologically transcribed data (Malouf et al., 2020). 7780 Predictions paradigm 1 paradigm 2 paradigm 3 paradigm 4 cell 1 watched followed «seed» «seened» cell 2 watching «following» «seeing» «seening» cell 3 «watches» follows «sees» «seens» cell 4 «watch» «follow» see seen Table 2: Toy predictions made from the corpus and lexicon in Table 1, to be evaluated against the toy gold grid. Again, bracketed «forms» are those not occurring in the lexicon. in UD and UniMorph (Gorman et al., 2019; Malouf et al., 2020). Unlike the gold grid, the lexicon retains overabundant realizations, requiring systems to handle such phenomena. For each language, the"
2020.acl-main.695,W18-6011,1,0.85496,"lar to the corpus created by Vylomova et al. (2019). As a system does not know which lexicon forms will be evaluated in the gold grid, it must model the entire lexicon, which should contain a realistic distribution over rare words and inflection classes having been directly extracted from distributional data (Bybee, 2003; Lignos and Yang, 2018). To ensure the gold grid is reasonably clean, we take all word–lemma–feature tuples from the UD portion of the corpus matching the specified POS and convert the features to a morphosyntactic cell identifier compatible with UniMorph representation as in McCarthy et al. (2018).3 Then we check which word–lemma–cell tuples also occur in UniMorph. For each unique lemma in this intersection, the full paradigm is added as a row to the gold grid. To filter typos and annotation discrepancies, we identify any overabundant slots, i.e., slots realized by multiple forms, and remove all but the most frequently attested realization in UD. While some languages permit overabundance (Thornton, 2010), it often indicates typographical or annotation errors 3 Aligning UniMorph and UD requires removing diacritics in (Latin and Arabic) UniMorph corpora to match UD. This can obscure some"
2020.acl-main.695,W19-4226,1,0.679152,"ect nonce words (Berko, 1958; Plunkett and Juola, 1999; Yang, 2015), e.g., is the past tense of sping, spinged or spung? There is a long tradition of modeling past-tense generation with neural networks (Rumelhart and McClelland, 1986; Kirov and Cotterell, 2018; Corkery et al., 2019). On the engineering side, Durrett and DeNero (2013) inspired much recent work, which has since benefited from large inflectional datasets (Kirov et al., 2018) and advances in neural sequence modeling (Bahdanau et al., 2015). Shared tasks have drawn extra attention to the PCFP (Cotterell et al., 2016a, 2017, 2018c; McCarthy et al., 2019). 3 The Paradigm Discovery Problem Paradigm discovery is a natural next step in computational morphology, building on related minimally or indirectly supervised works (§2.2) to bridge the gap between unsupervised traditions (§2.1) and supervised work on the PCFP (§2.3). In the PCFP, 7779 Corpus The cat watched me watching it . I followed the show but she had n’t seen it . Let ’s see who follows your logic . Lexicon watching, seen, follows, watched, followed, see Gold Grid paradigm 1 paradigm 2 paradigm 3 cell 1 «watch» «follow» see cell 2 «watches» follows «sees» cell 3 watching «following» «s"
2020.acl-main.695,Q15-1012,0,0.338073,"Missing"
2020.acl-main.695,L16-1262,0,0.0610659,"Missing"
2020.acl-main.695,D14-1162,0,0.0838528,"ark system for proposing a morphologically organized grid given a corpus and lexicon. First, we cluster lexicon forms into cells. Then we cluster forms into paradigms given their fixed cell membership. To maintain tractability, clustering assumes a one-to-one mapping of forms to slots. Following cell and paradigm clustering, we predict forms to realize empty slots given one of the lexicon forms assigned to a cell in Clustering into Cells We use a heuristic method to determine the number of cells and what lexicon forms to assign to each. Inspired by work on inductive biases in word embeddings (Pennington et al., 2014; Trask et al., 2015; Goldberg, 2016; Avraham and Goldberg, 2017; Tu et al., 2017), we train morphosyntactically biased embeddings on the corpus and use them to k-means cluster lexicon forms into cells. Following Erdmann et al. (2018), we emphasize morphosyntactically salient dimensions in embedding space by manipulating hyperparameters in fastText (Bojanowski et al., 2017). Specifically, to encourage grouping of morphologically related words, fastText computes a word’s embedding as the sum of its subword embeddings for all subword sequences between 3 and 6 characters long (Schütze, 1993). We"
2020.acl-main.695,D07-1043,0,0.0730775,"d (Parker et al., 2011a,b). Supplementary sentences are preprocessed via Moses (Koehn et al., 2007) to split punctuation, and, for supported languages, clitics. Table 3 shows corpus and lexicon sizes. 3.3 Metrics A system attemping the PDP is expected to output a morphologically organized grid in which rows and columns are arbitrarily ordered, but ideally, each row corresponds to a gold paradigm and each column to a gold cell. Aligning rows to paradigms and columns to cells is non-trivial, making it difficult to simply compute accuracy over gold grid slots. Furthermore, cluster-based metrics (Rosenberg and Hirschberg, 2007) are difficult to apply as forms can appear in multiple columns or rows. Thus, we propose novel metrics that are lexical, based on analogical relationships between forms. We propose a set of PDP metrics, to measure how well organized lexicon forms are in the grid, and a set of PCFP metrics, to measure how well the system anticipates unattested inflectional variants. All metrics support non-canonical phenomena such as defective paradigms and overabundant slots. 3.3.1 PDP Metrics A form f ’s paradigm mates are all those forms that co-occur in at least one paradigm with f . f ’s paradigm F-score"
2020.acl-main.695,D18-1315,0,0.0493291,"ical knowledge from scratch, first learning that singular–plural is a relevant distinction. Thus, the PDP must be at least partially solved before the PCFP can be attempted. Yet, as a supervised task, the PCFP is more easily studied, and has received much attention on its own, especially from the word-and-paradigm camp of morphological theory. Some cognitive works suggest the PCFP cannot be too difficult for any language (Dale et al., 1998; Ackerman and Malouf, 2013, 2015; Blevins et al., 2017; Cotterell et al., 2019). Neural models can test and extend such proposals (Cotterell et al., 2018a; Silfverberg and Hulden, 2018). A related vein of work discusses how speakers inflect nonce words (Berko, 1958; Plunkett and Juola, 1999; Yang, 2015), e.g., is the past tense of sping, spinged or spung? There is a long tradition of modeling past-tense generation with neural networks (Rumelhart and McClelland, 1986; Kirov and Cotterell, 2018; Corkery et al., 2019). On the engineering side, Durrett and DeNero (2013) inspired much recent work, which has since benefited from large inflectional datasets (Kirov et al., 2018) and advances in neural sequence modeling (Bahdanau et al., 2015). Shared tasks have drawn extra attention"
2020.acl-main.695,N19-1203,1,0.854484,"DP For a given language and POS, we create a corpus, lexicon, and gold grid based on a Universal Dependencies (UD) corpus (Nivre et al., 2016). At a high level, the corpus includes raw, non-UD sentences, and UD sentences stripped of annotations. The lexicon includes all forms occurring in the UD sentences with the specified POS (potentially including variant spellings and typographical errors). The gold grid consists of full paradigms for every word which co-occurs in UD and the UniMorph lexicon (Kirov et al., 2018) with a matching lemma–cell analysis; this is similar to the corpus created by Vylomova et al. (2019). As a system does not know which lexicon forms will be evaluated in the gold grid, it must model the entire lexicon, which should contain a realistic distribution over rare words and inflection classes having been directly extracted from distributional data (Bybee, 2003; Lignos and Yang, 2018). To ensure the gold grid is reasonably clean, we take all word–lemma–feature tuples from the UD portion of the corpus matching the specified POS and convert the features to a morphosyntactic cell identifier compatible with UniMorph representation as in McCarthy et al. (2018).3 Then we check which word–l"
2020.acl-main.695,D18-1268,0,0.0934499,"Missing"
2020.acl-main.695,W19-6005,0,0.0193758,"radigm cell discovery problem; we drop cell to distinguish our task from As an unsupervised task, the PDP poses challenges for modeling and evaluation and has yet to be attempted in its full form (Elsner et al., 2019). However, we contend there is much to be gained from formalizing and studying the PDP. There are insights for cognitive modeling to be won (Pinker, 2001; Goldwater, 2007) and intuitions on combating sparse data for language generation (King and White, 2018) to be accrued. Unsupervised language processing also has natural applications in the documentation of endangered languages (Zamaraeva et al., 2019) where a lot of annotated data is never likely to exist. Our formalization of the PDP offers a starting point for future work on unsupervised morphological paradigm completion. Our paper presents a concrete formalization of the PDP. Then, as a baseline for future work, we introduce a heuristic benchmark system. Our benchmark system takes an unannotated text corpus and a lexicon of words from the corpus to be analyzed. It first clusters the lexicon by cell and then by paradigm making use of distributional semantics and string similarity. Finally, it uses this clustering as silver-standard super"
2020.acl-main.695,W17-2632,0,0.0231066,"st, we cluster lexicon forms into cells. Then we cluster forms into paradigms given their fixed cell membership. To maintain tractability, clustering assumes a one-to-one mapping of forms to slots. Following cell and paradigm clustering, we predict forms to realize empty slots given one of the lexicon forms assigned to a cell in Clustering into Cells We use a heuristic method to determine the number of cells and what lexicon forms to assign to each. Inspired by work on inductive biases in word embeddings (Pennington et al., 2014; Trask et al., 2015; Goldberg, 2016; Avraham and Goldberg, 2017; Tu et al., 2017), we train morphosyntactically biased embeddings on the corpus and use them to k-means cluster lexicon forms into cells. Following Erdmann et al. (2018), we emphasize morphosyntactically salient dimensions in embedding space by manipulating hyperparameters in fastText (Bojanowski et al., 2017). Specifically, to encourage grouping of morphologically related words, fastText computes a word’s embedding as the sum of its subword embeddings for all subword sequences between 3 and 6 characters long (Schütze, 1993). We shorten this range to 2 to 4 to bias the grouping toward shared affixes rather tha"
2020.acl-main.695,Q17-1010,0,\N,Missing
2020.coling-main.256,D16-1250,0,0.0680169,"night, 2002). Apart from the distributional signal, the early approaches make use of other monolingual clues, e.g. word spelling, cognates or word frequency.3 More recent approaches leverage the distributional signal in word embeddings without any explicit linguistic clues. Many current models (Mikolov et al., 2013; Ruder et al., 2019) learn a linear transformation between two monolingual word embedding spaces, often guided by an initial set of seed translations. This seed dictionary frequently spans several thousand word pairs (Mikolov et al., 2013; Xing et al., 2015; Lazaridou et al., 2015; Artetxe et al., 2016) but one can also provide weaker supervision, through listing only identical strings or shared numerals (Artetxe et al., 2017; Søgaard et al., 2018). For unsupervised BLI, the initial translations may also be induced automatically through exploiting the structure of the monolingual embedding spaces (Zhang et al., 2017; Conneau et al., 2018; Artetxe et al., 2018b). We focus on supervised and weakly supervised BLI which outperform unsupervised approaches (Glavaˇs et al., 2019). The BLI models are typically evaluated using the precision@k metric, which tells us how many times the correct translat"
2020.coling-main.256,P17-1042,0,0.0271259,"ng, cognates or word frequency.3 More recent approaches leverage the distributional signal in word embeddings without any explicit linguistic clues. Many current models (Mikolov et al., 2013; Ruder et al., 2019) learn a linear transformation between two monolingual word embedding spaces, often guided by an initial set of seed translations. This seed dictionary frequently spans several thousand word pairs (Mikolov et al., 2013; Xing et al., 2015; Lazaridou et al., 2015; Artetxe et al., 2016) but one can also provide weaker supervision, through listing only identical strings or shared numerals (Artetxe et al., 2017; Søgaard et al., 2018). For unsupervised BLI, the initial translations may also be induced automatically through exploiting the structure of the monolingual embedding spaces (Zhang et al., 2017; Conneau et al., 2018; Artetxe et al., 2018b). We focus on supervised and weakly supervised BLI which outperform unsupervised approaches (Glavaˇs et al., 2019). The BLI models are typically evaluated using the precision@k metric, which tells us how many times the correct translation of a source form is among the k-best candidates returned by the model. In this work we exclusively consider the precision"
2020.coling-main.256,P18-1073,0,0.0227704,"ion between two monolingual word embedding spaces, often guided by an initial set of seed translations. This seed dictionary frequently spans several thousand word pairs (Mikolov et al., 2013; Xing et al., 2015; Lazaridou et al., 2015; Artetxe et al., 2016) but one can also provide weaker supervision, through listing only identical strings or shared numerals (Artetxe et al., 2017; Søgaard et al., 2018). For unsupervised BLI, the initial translations may also be induced automatically through exploiting the structure of the monolingual embedding spaces (Zhang et al., 2017; Conneau et al., 2018; Artetxe et al., 2018b). We focus on supervised and weakly supervised BLI which outperform unsupervised approaches (Glavaˇs et al., 2019). The BLI models are typically evaluated using the precision@k metric, which tells us how many times the correct translation of a source form is among the k-best candidates returned by the model. In this work we exclusively consider the precision@1 metric, which is the least forgiving. 2.3 Morphological Inflection: A Challenge for BLI Most datasets for BLI operate at the level of inflected forms and impose no restriction on the morphosyntactic category of translated words. From a"
2020.coling-main.256,P13-1133,0,0.0642117,"ska et al. (2019). As discussed in §2.3, given that inflectional morphology is present in the induced lexicon, BLI models should be trained and evaluated on datasets which list a range of compatible inflected form pairs for every source-target lexeme pair. At this time, the dictionaries of Czarnowska et al. (2019) are the only publicly available resource that meets this criterion, and, for this reason, they are the most important evaluation benchmark used in this work. The dictionaries were generated based on Open Multilingual WordNet (Bond and Paik, 2012), Extended Open Multilingual WordNet (Bond and Foster, 2013) and UniMorph8 (Kirov et al., 2016; McCarthy et al., 2020), a resource comprised of inflectional word paradigms for 107 languages. The dictionaries only list parts of speech that undergo inflection in either the source or the target language; these are nouns, adjectives and verbs in the Romance languages. Conneau et al. (2018). MUSE (Conneau et al., 2018) was generated using an “internal translation tool” and is one of the few other resources which covers pairs of Romance languages. However, it is skewed towards most frequent forms: The vast majority of forms in MUSE are ranked in the top 10k"
2020.coling-main.256,J90-2002,0,0.81584,"ected form that lexicographers have chosen to be representative of the lexeme. For example, the lexeme RUN’s lemma is run. In many languages, the infinitive is the verbal lemma and the nominative singular is the nominal lemma. We consider a lexicon of a language to be a set of inflected forms.2 2.2 Bilingual Lexicon Induction In the NLP literature, the BLI task is to translate a given list of source-side word forms into the most appropriate corresponding target-side word forms. It dates back to 1990s. The first data-driven experiments on parallel corpora made use of word-alignment techniques (Brown et al., 1990; Kupiec, 1993; Smadja and McKeown, 1994). Such approaches were later extended to operate on non-parallel or even unrelated texts by leveraging the correlation between word co-occurrence patterns in different languages (Rapp, 1995; Fung and Lo, 1998; Fung, 1998; Koehn and Knight, 2002). Apart from the distributional signal, the early approaches make use of other monolingual clues, e.g. word spelling, cognates or word frequency.3 More recent approaches leverage the distributional signal in word embeddings without any explicit linguistic clues. Many current models (Mikolov et al., 2013; Ruder et"
2020.coling-main.256,K17-2001,1,0.898886,"Missing"
2020.coling-main.256,D19-1090,1,0.853852,"odules have no means of handling irregular morphology, beyond the irregular forms they have been exposed to during training. Guided by this insight, we propose an alternative version of our model, which employs a special treatment for forms likely to have irregular morphology—the most frequent forms (Bybee, 1995a; Baayen and Lieber, 1996; Wu et al., 2019a). We term this extension the hybrid model. It employs a frequency-based heuristic and translates the source form through its lemma only if the lemma is more frequent.6 Otherwise, it translates the inflected form 4 Indeed, the dictionaries of Czarnowska et al. (2019) on which we experiment also make this assumption. Note that the translator’s distribution, as defined in eq. (3) and eq. (4), is over all inflected forms in the target lexicon. An alternative would be to define a distribution over lemmata only. However, this would require filtering out all non-lemma forms from the target embedding matrix, which is not trivial. In our preliminary experiments, we observed that this can lead to a further performance increase. 6 We rely on the order of FAST T EXT embeddings for the relative ranking of inflected forms. 5 2850 directly, using only the translator co"
2020.coling-main.256,P19-1070,1,0.883225,"Missing"
2020.coling-main.256,L18-1550,0,0.0423771,"to supervised and semi-supervised approaches. 5.3 Skyline We also consider a version of our model which uses an oracle analyzer—the source lemma λs and tag τs are known a priori. The skyline provides an upper-bound of performance—to wit, what performance would be achievable if the model had had access to more information about the translated source form. 5.4 Experimental Details We implemented all models in PyTorch (Paszke et al., 2019), adapting the code of Wu et al. (2019b) for the transducers (analyzer and inflector). Throughout our experiments we used the Wikipedia FAST T EXT embeddings (Grave et al., 2018), which we length-normalized and mean-centered before training the models. As is standard, we trained all translators on the top 200k most frequent word forms in the vocabularies of both languages. To evaluate on very rare forms present in the dictionaries of Czarnowska et al. (2019) which are out-of-vocabulary (OOV) for FAST T EXT, we created an OOV FAST T EXT embedding for every OOV form that appears in a union of WordNet and UniMorph and appended those representations to the original embedding matrices.10 We evaluated all models using precision@1 as a metric, which is equivalent to accuracy"
2020.coling-main.256,D19-1328,0,0.0227454,"the source or the target language; these are nouns, adjectives and verbs in the Romance languages. Conneau et al. (2018). MUSE (Conneau et al., 2018) was generated using an “internal translation tool” and is one of the few other resources which covers pairs of Romance languages. However, it is skewed towards most frequent forms: The vast majority of forms in MUSE are ranked in the top 10k of the vocabularies in their respective languages, causing it to omit many morphological variants of words. The dataset also suffers from other issues, such as a high level of noise coming from proper nouns (Kementchedjhieva et al., 2019). Thus, we do not view this resource as a reasonable benchmark for BLI. 5.2 Baselines Artetxe et al. (2016). They learn an orthogonal linear transformation matrix between the source language space and the target language space, after length-normalizing and mean-centering the monolingual embedding matrices. Their method is fully supervised and works best with large amounts of training data (several thousand translation pairs). Ruder et al. (2018). They introduce a weakly supervised, self-learning model, which can induce a dictionary given only a handful of initial, seed translations. This is ac"
2020.coling-main.256,L16-1498,0,0.0605189,"3, given that inflectional morphology is present in the induced lexicon, BLI models should be trained and evaluated on datasets which list a range of compatible inflected form pairs for every source-target lexeme pair. At this time, the dictionaries of Czarnowska et al. (2019) are the only publicly available resource that meets this criterion, and, for this reason, they are the most important evaluation benchmark used in this work. The dictionaries were generated based on Open Multilingual WordNet (Bond and Paik, 2012), Extended Open Multilingual WordNet (Bond and Foster, 2013) and UniMorph8 (Kirov et al., 2016; McCarthy et al., 2020), a resource comprised of inflectional word paradigms for 107 languages. The dictionaries only list parts of speech that undergo inflection in either the source or the target language; these are nouns, adjectives and verbs in the Romance languages. Conneau et al. (2018). MUSE (Conneau et al., 2018) was generated using an “internal translation tool” and is one of the few other resources which covers pairs of Romance languages. However, it is skewed towards most frequent forms: The vast majority of forms in MUSE are ranked in the top 10k of the vocabularies in their respe"
2020.coling-main.256,W02-0902,0,0.391248,"lected forms.2 2.2 Bilingual Lexicon Induction In the NLP literature, the BLI task is to translate a given list of source-side word forms into the most appropriate corresponding target-side word forms. It dates back to 1990s. The first data-driven experiments on parallel corpora made use of word-alignment techniques (Brown et al., 1990; Kupiec, 1993; Smadja and McKeown, 1994). Such approaches were later extended to operate on non-parallel or even unrelated texts by leveraging the correlation between word co-occurrence patterns in different languages (Rapp, 1995; Fung and Lo, 1998; Fung, 1998; Koehn and Knight, 2002). Apart from the distributional signal, the early approaches make use of other monolingual clues, e.g. word spelling, cognates or word frequency.3 More recent approaches leverage the distributional signal in word embeddings without any explicit linguistic clues. Many current models (Mikolov et al., 2013; Ruder et al., 2019) learn a linear transformation between two monolingual word embedding spaces, often guided by an initial set of seed translations. This seed dictionary frequently spans several thousand word pairs (Mikolov et al., 2013; Xing et al., 2015; Lazaridou et al., 2015; Artetxe et a"
2020.coling-main.256,P93-1003,0,0.087534,"cographers have chosen to be representative of the lexeme. For example, the lexeme RUN’s lemma is run. In many languages, the infinitive is the verbal lemma and the nominative singular is the nominal lemma. We consider a lexicon of a language to be a set of inflected forms.2 2.2 Bilingual Lexicon Induction In the NLP literature, the BLI task is to translate a given list of source-side word forms into the most appropriate corresponding target-side word forms. It dates back to 1990s. The first data-driven experiments on parallel corpora made use of word-alignment techniques (Brown et al., 1990; Kupiec, 1993; Smadja and McKeown, 1994). Such approaches were later extended to operate on non-parallel or even unrelated texts by leveraging the correlation between word co-occurrence patterns in different languages (Rapp, 1995; Fung and Lo, 1998; Fung, 1998; Koehn and Knight, 2002). Apart from the distributional signal, the early approaches make use of other monolingual clues, e.g. word spelling, cognates or word frequency.3 More recent approaches leverage the distributional signal in word embeddings without any explicit linguistic clues. Many current models (Mikolov et al., 2013; Ruder et al., 2019) le"
2020.coling-main.256,P15-1027,0,0.0406645,"Fung, 1998; Koehn and Knight, 2002). Apart from the distributional signal, the early approaches make use of other monolingual clues, e.g. word spelling, cognates or word frequency.3 More recent approaches leverage the distributional signal in word embeddings without any explicit linguistic clues. Many current models (Mikolov et al., 2013; Ruder et al., 2019) learn a linear transformation between two monolingual word embedding spaces, often guided by an initial set of seed translations. This seed dictionary frequently spans several thousand word pairs (Mikolov et al., 2013; Xing et al., 2015; Lazaridou et al., 2015; Artetxe et al., 2016) but one can also provide weaker supervision, through listing only identical strings or shared numerals (Artetxe et al., 2017; Søgaard et al., 2018). For unsupervised BLI, the initial translations may also be induced automatically through exploiting the structure of the monolingual embedding spaces (Zhang et al., 2017; Conneau et al., 2018; Artetxe et al., 2018b). We focus on supervised and weakly supervised BLI which outperform unsupervised approaches (Glavaˇs et al., 2019). The BLI models are typically evaluated using the precision@k metric, which tells us how many tim"
2020.coling-main.256,W19-4226,1,0.850478,"ctored into two parts. The first part, the inflector, produces an inflected form ιt given a lemma λt and a morphological tag τt . This problem has been well studied in the NLP literature (Cotterell et al., 2016; Cotterell et al., 2017). The second part, tag translator, determines the possible target-side morphological tags that are compatible with the features present in the source tag. In principle, our model is compatible with any probabilistic inflector. In this paper, we employ the model of Wu et al. (2019b), which obtained the single-model state of the art at the time of experimentation (McCarthy et al., 2019). The model has a latent character-level monotonic alignment between the source and target inflections that is jointly learned with the transducer and is, in effect, a neuralized version of a hidden Markov model for translation (Vogel et al., 1996). 2849 In this work we focus on closely related languages and make a simplifying assumption that there exists a single most-plausible translation for each inflected form.4 We formalize the tag translator using an indicator function: ( 1 if τt = τs p(τt |τs ) = 0 if τt 6= τs For experiments with more distant language pairs one can define p(τt |τs ) to"
2020.coling-main.256,W16-1614,0,0.0589174,"ms belonging to different morpho-syntactic categories for French–Spanish. was a success, but, on the other, our more nuanced conclusion is that the task of BLI, as currently researched in NLP, is ill-defined with respect to inflectional morphology. Indeed, the authors suggest that BLI needs redirection going forward. The recent trend in BLI research is to remain data-driven and to avoid specialist linguistic annotation. Current projection-based approaches to BLI depend heavily on the assumption that the lexicons of different languages are approximately isomorphic (Mikolov et al., 2013; Miceli Barone, 2016). However, given the immense variation in morphological systems of worlds’ languages, this assumption is prima fascie false. Consider the simple contradiction of Spanish and English, where the first exhibits much more morphological inflection than the latter; there can be no one-to-one alignment between the words in those two lexicons. The failure of the isomorphism assumption has been discussed and addressed in many recent works on cross-lingual word embeddings (Søgaard et al., 2018; Nakashole and Flauger, 2018; Ormazabal et al., 2019; Vuli´c et al., 2019; Patra et al., 2019). However, none o"
2020.coling-main.256,P18-2036,0,0.0223414,"t the lexicons of different languages are approximately isomorphic (Mikolov et al., 2013; Miceli Barone, 2016). However, given the immense variation in morphological systems of worlds’ languages, this assumption is prima fascie false. Consider the simple contradiction of Spanish and English, where the first exhibits much more morphological inflection than the latter; there can be no one-to-one alignment between the words in those two lexicons. The failure of the isomorphism assumption has been discussed and addressed in many recent works on cross-lingual word embeddings (Søgaard et al., 2018; Nakashole and Flauger, 2018; Ormazabal et al., 2019; Vuli´c et al., 2019; Patra et al., 2019). However, none of those studies directly target inflectional morphology. In this work we highlight that inflectional morphology complicates BLI and NLP researchers should strive to develop a cleaner way to integrate it into their models. We contend the models we present make progress in this direction but there is still a long way to go. We now make three concrete suggestions for BLI going forward. The first two involve engaging with morphology more seriously and are extensions to the ideas in this paper. The third focuses on b"
2020.coling-main.256,P19-1492,0,0.0209653,"anguages are approximately isomorphic (Mikolov et al., 2013; Miceli Barone, 2016). However, given the immense variation in morphological systems of worlds’ languages, this assumption is prima fascie false. Consider the simple contradiction of Spanish and English, where the first exhibits much more morphological inflection than the latter; there can be no one-to-one alignment between the words in those two lexicons. The failure of the isomorphism assumption has been discussed and addressed in many recent works on cross-lingual word embeddings (Søgaard et al., 2018; Nakashole and Flauger, 2018; Ormazabal et al., 2019; Vuli´c et al., 2019; Patra et al., 2019). However, none of those studies directly target inflectional morphology. In this work we highlight that inflectional morphology complicates BLI and NLP researchers should strive to develop a cleaner way to integrate it into their models. We contend the models we present make progress in this direction but there is still a long way to go. We now make three concrete suggestions for BLI going forward. The first two involve engaging with morphology more seriously and are extensions to the ideas in this paper. The third focuses on backing away from morphol"
2020.coling-main.256,P19-1018,0,0.0207411,"v et al., 2013; Miceli Barone, 2016). However, given the immense variation in morphological systems of worlds’ languages, this assumption is prima fascie false. Consider the simple contradiction of Spanish and English, where the first exhibits much more morphological inflection than the latter; there can be no one-to-one alignment between the words in those two lexicons. The failure of the isomorphism assumption has been discussed and addressed in many recent works on cross-lingual word embeddings (Søgaard et al., 2018; Nakashole and Flauger, 2018; Ormazabal et al., 2019; Vuli´c et al., 2019; Patra et al., 2019). However, none of those studies directly target inflectional morphology. In this work we highlight that inflectional morphology complicates BLI and NLP researchers should strive to develop a cleaner way to integrate it into their models. We contend the models we present make progress in this direction but there is still a long way to go. We now make three concrete suggestions for BLI going forward. The first two involve engaging with morphology more seriously and are extensions to the ideas in this paper. The third focuses on backing away from morphology. More Fine-Grained Lexicons. Our first"
2020.coling-main.256,P95-1050,0,0.688785,"a lexicon of a language to be a set of inflected forms.2 2.2 Bilingual Lexicon Induction In the NLP literature, the BLI task is to translate a given list of source-side word forms into the most appropriate corresponding target-side word forms. It dates back to 1990s. The first data-driven experiments on parallel corpora made use of word-alignment techniques (Brown et al., 1990; Kupiec, 1993; Smadja and McKeown, 1994). Such approaches were later extended to operate on non-parallel or even unrelated texts by leveraging the correlation between word co-occurrence patterns in different languages (Rapp, 1995; Fung and Lo, 1998; Fung, 1998; Koehn and Knight, 2002). Apart from the distributional signal, the early approaches make use of other monolingual clues, e.g. word spelling, cognates or word frequency.3 More recent approaches leverage the distributional signal in word embeddings without any explicit linguistic clues. Many current models (Mikolov et al., 2013; Ruder et al., 2019) learn a linear transformation between two monolingual word embedding spaces, often guided by an initial set of seed translations. This seed dictionary frequently spans several thousand word pairs (Mikolov et al., 2013;"
2020.coling-main.256,P18-2062,0,0.0349516,"edictable items (e.g. Prasada and Pinker (1993) Pinker and Prince (1994)). 3 For more references we refer the reader to the survey of Ruder et al. (2019). 2848 evaluated on datasets that contain a more representative range of morphological inflections. We use the term morphologically enriched dictionary for such bilingual lexicons (see §5.1). To our knowledge, we are the first to explicitly model inflectional morphology in BLI. Closest to our endeavor, Yang et al. (2019) address morphology in BLI by incorporating grammatical information learned by a pre-trained denoising language model, while Riley and Gildea (2018) enhance the projection-based approach of Artetxe et al. (2017) with orthographic features to improve performance on BLI for related languages. 3 A Joint Model for Morphologically Aware Word-level Translation The primary contribution of this work is a morphologically aware probabilistic model for word-level translation. Our model exploits a simple intuition: Because the core unit of meaning is the lexeme, one should translate through the lexeme and then inflect the word according to the target language’s morphology. Notation. In the task of BLI, we consider a source language s and a target lan"
2020.coling-main.256,D18-1042,1,0.886316,"n for both the source and the target language, although in practice these look-up functions are distinct. The model has a single matrix of parameters: Ω ∈ RNt ×Ns where Ns is the source embedding dimensionality and Nt the target embedding dimensionality. Our translator is defined as the following conditional model p(λt |λs ) =  1 exp e(λt )> Ω e(λs ) Z(λs ) (3) X (4) where the normalizer is defined as Z(λs ) = exp e(λ0t )> Ω e(λs )  λ0t ∈Lt Note that this log-bilinear model differs from most embedding-based bilingual lexicon inducers which predict embeddings, rather than words. For example, Ruder et al. (2018)’s approach contains a multivariate Gaussian over the target-language’s embedding space.5 Orthogonal Regularization. During training we employ a special regularization term on the parameter matrix Ω. Specifically, we use R(Ω) = α Ω> Ω − I (5) F with a tunable “strength” hyperparameter α ∈ R≥0 . This term encourages the translation matrix to be orthogonal, which has led to consistent gains in past work (Xing et al., 2015; Artetxe et al., 2016; Ruder et al., 2018). 3.3 The Analyzer: p(λs , τs |ιs ) For our probabilistic analyzer we use the same hard attention model as in the inflector. The model"
2020.coling-main.256,H94-1027,0,0.381259,"e chosen to be representative of the lexeme. For example, the lexeme RUN’s lemma is run. In many languages, the infinitive is the verbal lemma and the nominative singular is the nominal lemma. We consider a lexicon of a language to be a set of inflected forms.2 2.2 Bilingual Lexicon Induction In the NLP literature, the BLI task is to translate a given list of source-side word forms into the most appropriate corresponding target-side word forms. It dates back to 1990s. The first data-driven experiments on parallel corpora made use of word-alignment techniques (Brown et al., 1990; Kupiec, 1993; Smadja and McKeown, 1994). Such approaches were later extended to operate on non-parallel or even unrelated texts by leveraging the correlation between word co-occurrence patterns in different languages (Rapp, 1995; Fung and Lo, 1998; Fung, 1998; Koehn and Knight, 2002). Apart from the distributional signal, the early approaches make use of other monolingual clues, e.g. word spelling, cognates or word frequency.3 More recent approaches leverage the distributional signal in word embeddings without any explicit linguistic clues. Many current models (Mikolov et al., 2013; Ruder et al., 2019) learn a linear transformation"
2020.coling-main.256,P18-1072,1,0.891487,"Missing"
2020.coling-main.256,C96-2141,0,0.765278,"t, tag translator, determines the possible target-side morphological tags that are compatible with the features present in the source tag. In principle, our model is compatible with any probabilistic inflector. In this paper, we employ the model of Wu et al. (2019b), which obtained the single-model state of the art at the time of experimentation (McCarthy et al., 2019). The model has a latent character-level monotonic alignment between the source and target inflections that is jointly learned with the transducer and is, in effect, a neuralized version of a hidden Markov model for translation (Vogel et al., 1996). 2849 In this work we focus on closely related languages and make a simplifying assumption that there exists a single most-plausible translation for each inflected form.4 We formalize the tag translator using an indicator function: ( 1 if τt = τs p(τt |τs ) = 0 if τt 6= τs For experiments with more distant language pairs one can define p(τt |τs ) to be a multi-label classifier. 3.2 The Translator: p(λt |λt ) As our translator, we construct a log-bilinear model that yields a distribution over all elements in the target lexicon. We assume the existence of both source- and target-side embeddings"
2020.coling-main.256,D19-1449,0,0.0245645,"Missing"
2020.coling-main.256,P19-1505,1,0.888004,"ns of handling irregular morphology, beyond the irregular forms they have been exposed to during training. Guided by this insight, we propose an alternative version of our model, which employs a special treatment for forms likely to have irregular morphology—the most frequent forms (Bybee, 1995a; Baayen and Lieber, 1996; Wu et al., 2019a). We term this extension the hybrid model. It employs a frequency-based heuristic and translates the source form through its lemma only if the lemma is more frequent.6 Otherwise, it translates the inflected form 4 Indeed, the dictionaries of Czarnowska et al. (2019) on which we experiment also make this assumption. Note that the translator’s distribution, as defined in eq. (3) and eq. (4), is over all inflected forms in the target lexicon. An alternative would be to define a distribution over lemmata only. However, this would require filtering out all non-lemma forms from the target embedding matrix, which is not trivial. In our preliminary experiments, we observed that this can lead to a further performance increase. 6 We rely on the order of FAST T EXT embeddings for the relative ranking of inflected forms. 5 2850 directly, using only the translator co"
2020.coling-main.256,P19-1148,1,0.891112,"ιt ,τt i∈π t hιt ,τt i∈π t inflector tag translator The joint distribution over forms and tags is factored into two parts. The first part, the inflector, produces an inflected form ιt given a lemma λt and a morphological tag τt . This problem has been well studied in the NLP literature (Cotterell et al., 2016; Cotterell et al., 2017). The second part, tag translator, determines the possible target-side morphological tags that are compatible with the features present in the source tag. In principle, our model is compatible with any probabilistic inflector. In this paper, we employ the model of Wu et al. (2019b), which obtained the single-model state of the art at the time of experimentation (McCarthy et al., 2019). The model has a latent character-level monotonic alignment between the source and target inflections that is jointly learned with the transducer and is, in effect, a neuralized version of a hidden Markov model for translation (Vogel et al., 1996). 2849 In this work we focus on closely related languages and make a simplifying assumption that there exists a single most-plausible translation for each inflected form.4 We formalize the tag translator using an indicator function: ( 1 if τt ="
2020.coling-main.256,N15-1104,0,0.0421115,"Fung and Lo, 1998; Fung, 1998; Koehn and Knight, 2002). Apart from the distributional signal, the early approaches make use of other monolingual clues, e.g. word spelling, cognates or word frequency.3 More recent approaches leverage the distributional signal in word embeddings without any explicit linguistic clues. Many current models (Mikolov et al., 2013; Ruder et al., 2019) learn a linear transformation between two monolingual word embedding spaces, often guided by an initial set of seed translations. This seed dictionary frequently spans several thousand word pairs (Mikolov et al., 2013; Xing et al., 2015; Lazaridou et al., 2015; Artetxe et al., 2016) but one can also provide weaker supervision, through listing only identical strings or shared numerals (Artetxe et al., 2017; Søgaard et al., 2018). For unsupervised BLI, the initial translations may also be induced automatically through exploiting the structure of the monolingual embedding spaces (Zhang et al., 2017; Conneau et al., 2018; Artetxe et al., 2018b). We focus on supervised and weakly supervised BLI which outperform unsupervised approaches (Glavaˇs et al., 2019). The BLI models are typically evaluated using the precision@k metric, whi"
2020.coling-main.256,P19-1308,0,0.0385215,"975), McClelland et al. (1987) and Bybee (1995b), and stands in opposition to alternative views of the lexicon being comprised of only the unpredictable items (e.g. Prasada and Pinker (1993) Pinker and Prince (1994)). 3 For more references we refer the reader to the survey of Ruder et al. (2019). 2848 evaluated on datasets that contain a more representative range of morphological inflections. We use the term morphologically enriched dictionary for such bilingual lexicons (see §5.1). To our knowledge, we are the first to explicitly model inflectional morphology in BLI. Closest to our endeavor, Yang et al. (2019) address morphology in BLI by incorporating grammatical information learned by a pre-trained denoising language model, while Riley and Gildea (2018) enhance the projection-based approach of Artetxe et al. (2017) with orthographic features to improve performance on BLI for related languages. 3 A Joint Model for Morphologically Aware Word-level Translation The primary contribution of this work is a morphologically aware probabilistic model for word-level translation. Our model exploits a simple intuition: Because the core unit of meaning is the lexeme, one should translate through the lexeme and"
2020.coling-main.256,P17-1179,0,0.0330424,"r et al., 2019) learn a linear transformation between two monolingual word embedding spaces, often guided by an initial set of seed translations. This seed dictionary frequently spans several thousand word pairs (Mikolov et al., 2013; Xing et al., 2015; Lazaridou et al., 2015; Artetxe et al., 2016) but one can also provide weaker supervision, through listing only identical strings or shared numerals (Artetxe et al., 2017; Søgaard et al., 2018). For unsupervised BLI, the initial translations may also be induced automatically through exploiting the structure of the monolingual embedding spaces (Zhang et al., 2017; Conneau et al., 2018; Artetxe et al., 2018b). We focus on supervised and weakly supervised BLI which outperform unsupervised approaches (Glavaˇs et al., 2019). The BLI models are typically evaluated using the precision@k metric, which tells us how many times the correct translation of a source form is among the k-best candidates returned by the model. In this work we exclusively consider the precision@1 metric, which is the least forgiving. 2.3 Morphological Inflection: A Challenge for BLI Most datasets for BLI operate at the level of inflected forms and impose no restriction on the morphosy"
2020.emnlp-main.15,J92-1002,0,0.64528,"Missing"
2020.emnlp-main.15,W19-4828,0,0.0274929,"t al., 2018) is one prominent method, which consists of using a lightly parameterized model to predict linguistic phenomena from intermediate representations, albeit recent work has raised concerns on how model parameterization and evaluation metrics may affect the effectiveness of this approach (Hewitt and Liang, 2019; Pimentel et al., 2020b; Maudslay et al., 2020; Pimentel et al., 2020a). Most work in intrinsic probing has focused in the identification of individual neurons that are important for a task (Li et al., 2016; Kádár et al., 2017; Li et al., 2017; Lakretz et al., 2019). Similarly, Clark et al. (2019) and Voita et al. (2019) use probing to analyze BERT’s attention heads, finding some interpretable heads that attend to positional and syntactic features. However, there has also been some work investigating collections of neurons. For example, Shi et al. (2016) observe that different training objectives can affect how focal an intermediate representation is. Recently, Dalvi et al. (2019) use the magnitude of the weights learned by a linear probe as a proxy for dimension informativeness, 204 0.5 0.6 rus lav 0.4 hin 0.4 0.2 est ara Dimension 80 0 0.3 −0.2 −0.4 0.2 −0.6 −0.8 0.1 −1 −1.2 −0.4 −0."
2020.emnlp-main.15,D19-1588,0,0.0189933,"oncentrating its linguistic structure more than BERT.1 1 1 Dimension 179 0.5 0 −0.5 −1 −1.5 −2 −1.5 −1 −0.5 0 0.5 1 Dimension 477 Figure 1: Scatter plot of the two most informative BERT dimensions for English present and past tense. The contours belong to our probe. Introduction Natural language processing (NLP) is enamored of contextual word representations—and for good reason! Contextual word-embedders, e.g. BERT (Devlin et al., 2019) and ELMo (Peters et al., 2018), have bolstered NLP model performance on myriad tasks, such as syntactic parsing (Kitaev et al., 2019), coreference resolution (Joshi et al., 2019), morphological tagging (Kondratyuk, 2019) and text generation (Zellers et al., 2019). Given the large empirical gains observed when they are employed, it is all but certain that word representations derived from neural networks encode some continuous analogue of linguistic structures. Exactly what these representations encode about linguistic structure, however, remains little understood. Researchers have studied this question by attributing function to specific network cells with visualization methods (Karpathy et al., 2015; Li et al., 2016) and by probing (Alain and Bengio, 2017; Belinkov a"
2020.emnlp-main.15,K19-1001,0,0.0150657,"rk There has been a growing interest in understanding what information is in NLP models’ internal representations. Studies vary widely, from detailed analyses of particular scenarios and linguistic phenomena (Linzen et al., 2016; Gulordava et al., 2018; Ravfogel et al., 2018; KrasnowskaKiera´s and Wróblewska, 2019; Wallace et al., 2019; Warstadt et al., 2019; Sorodoc et al., 2020) to extensive investigations across a wealth of tasks (Tenney et al., 2018; Conneau et al., 2018; Liu et al., 2019). A plethora of methods have been designed and applied (e.g. Li et al., 2016; Saphra and Lopez, 2019; Jumelet et al., 2019) to answer this question. Probing (Adi et al., 2017; Hupkes et al., 2018; Figure 5: Scatter graph of two most informative fastText (above) and BERT (below) dimensions for English present and past tense. Contours belong to our probe. Conneau et al., 2018) is one prominent method, which consists of using a lightly parameterized model to predict linguistic phenomena from intermediate representations, albeit recent work has raised concerns on how model parameterization and evaluation metrics may affect the effectiveness of this approach (Hewitt and Liang, 2019; Pimentel et al., 2020b; Maudslay et"
2020.emnlp-main.15,S19-1026,0,0.0537808,"Missing"
2020.emnlp-main.15,L18-1293,1,0.922891,"erate Gaussians when there are more dimensions under consideration than training datapoints (Srivastava et al., 2007). Under the Bayesian framework, we seek to compute the posterior distribution over the probe’s parameters given our training data, p(θ v |D(v) ) ∝ p(θ v ) × p(D(v) |θ v ) 5 Crucially, some words may have different morphosyntactic tags depending on their context. For example, the number attribute of “make” could be either singular (“I make”) or plural (“They make”). 6 “Universal” here refers to the set of all UniMorph dimensions and their possible features (Sylak-Glassman, 2016; Kirov et al., 2018). (9) (10) where p(θ v ) is our Bayesian prior. The prior encodes our a priori belief about the parameters in 200 7 UniMorph’s most varied attribute is C ASE, with 32 values, though most languages do not exhibit all of them. the absence of any data, and p(D(v) |θ v ) is the likelihood of the data under our model given a parameterization θ v . In the case of a Gaussian– inverse-Wishart prior,8 due to the model overfitting in certain dimensions. In principle, if a model was able to achieve a higher score using fewer dimensions, then there exists a model that can be at least as effective using a"
2020.emnlp-main.15,P19-1340,0,0.0220394,"encoded by only a few neurons, with fastText concentrating its linguistic structure more than BERT.1 1 1 Dimension 179 0.5 0 −0.5 −1 −1.5 −2 −1.5 −1 −0.5 0 0.5 1 Dimension 477 Figure 1: Scatter plot of the two most informative BERT dimensions for English present and past tense. The contours belong to our probe. Introduction Natural language processing (NLP) is enamored of contextual word representations—and for good reason! Contextual word-embedders, e.g. BERT (Devlin et al., 2019) and ELMo (Peters et al., 2018), have bolstered NLP model performance on myriad tasks, such as syntactic parsing (Kitaev et al., 2019), coreference resolution (Joshi et al., 2019), morphological tagging (Kondratyuk, 2019) and text generation (Zellers et al., 2019). Given the large empirical gains observed when they are employed, it is all but certain that word representations derived from neural networks encode some continuous analogue of linguistic structures. Exactly what these representations encode about linguistic structure, however, remains little understood. Researchers have studied this question by attributing function to specific network cells with visualization methods (Karpathy et al., 2015; Li et al., 2016) and b"
2020.emnlp-main.15,W19-4203,0,0.0186479,"han BERT.1 1 1 Dimension 179 0.5 0 −0.5 −1 −1.5 −2 −1.5 −1 −0.5 0 0.5 1 Dimension 477 Figure 1: Scatter plot of the two most informative BERT dimensions for English present and past tense. The contours belong to our probe. Introduction Natural language processing (NLP) is enamored of contextual word representations—and for good reason! Contextual word-embedders, e.g. BERT (Devlin et al., 2019) and ELMo (Peters et al., 2018), have bolstered NLP model performance on myriad tasks, such as syntactic parsing (Kitaev et al., 2019), coreference resolution (Joshi et al., 2019), morphological tagging (Kondratyuk, 2019) and text generation (Zellers et al., 2019). Given the large empirical gains observed when they are employed, it is all but certain that word representations derived from neural networks encode some continuous analogue of linguistic structures. Exactly what these representations encode about linguistic structure, however, remains little understood. Researchers have studied this question by attributing function to specific network cells with visualization methods (Karpathy et al., 2015; Li et al., 2016) and by probing (Alain and Bengio, 2017; Belinkov and Glass, 2019), which seeks to extract st"
2020.emnlp-main.15,P19-1573,0,0.0402951,"Missing"
2020.emnlp-main.15,N19-1002,0,0.205816,"he multivariate Gaussian distribution’s inherent decomposability, and evaluate its performance on a large-scale, multi-lingual, morphosyntactic probing task (§3). We experiment on 36 languages2 from the Universal Dependencies treebanks (Nivre et al., 2017). We find that all the morphosyntactic features we considered are encoded by a relatively small selection of neurons. In some cases, very few neurons are needed; for instance, for multilingual BERT English representations, we see that, with two neurons, we can largely separate past and present tense in Fig. 1. In this, our work is closest to Lakretz et al. (2019), except that we extend the investigation beyond individual neurons—a move which is only made tractable by decomposable probing. We also provide analyses on morphological features beyond number and tense. Across all languages, 35 out of 768 neurons on average suffice to reach a reasonable amount of encoded information, and adding more yields diminishing returns (see Fig. 2). Interestingly, in our head-to-head comparison of BERT and fastText, we find that fastText almost always encodes information about morphosyntactic 2 properties using fewer dimensions. 2 Probing through Dimension Selection T"
2020.emnlp-main.15,N16-1082,0,0.0914877,"(Kitaev et al., 2019), coreference resolution (Joshi et al., 2019), morphological tagging (Kondratyuk, 2019) and text generation (Zellers et al., 2019). Given the large empirical gains observed when they are employed, it is all but certain that word representations derived from neural networks encode some continuous analogue of linguistic structures. Exactly what these representations encode about linguistic structure, however, remains little understood. Researchers have studied this question by attributing function to specific network cells with visualization methods (Karpathy et al., 2015; Li et al., 2016) and by probing (Alain and Bengio, 2017; Belinkov and Glass, 2019), which seeks to extract structure from the representations. Recent work has probed various representations for correlates of morphological (Belinkov et al., 2017; Giulianelli et al., 2018), syntactic (Hupkes et al., 2018; Zhang and Bowman, 2018; Hewitt and Manning, 2019; Lin et al., 2019), and semantic (Kim et al., 2019) structure. 1 Code and data are available at https://github. com/rycolab/intrinsic-probing. Most current probing efforts focus on what we term extrinsic probing, where the goal is to determine whether the posite"
2020.emnlp-main.15,W19-4825,0,0.0353696,"Missing"
2020.emnlp-main.15,Q16-1037,0,0.0634468,"ERT. Each bar is broken up into three components, which denote the LBNMI after selecting 2, 10 and 50 dimensions. of information for any given subset of dimensions. However, we expect that better modeling of the embedding distribution should improve our bound on the mutual information and thus yield a better probe (Pimentel et al., 2020b). 7 −0.1 −0.2 Dimension 179 Afro-Asiatic Related Work There has been a growing interest in understanding what information is in NLP models’ internal representations. Studies vary widely, from detailed analyses of particular scenarios and linguistic phenomena (Linzen et al., 2016; Gulordava et al., 2018; Ravfogel et al., 2018; KrasnowskaKiera´s and Wróblewska, 2019; Wallace et al., 2019; Warstadt et al., 2019; Sorodoc et al., 2020) to extensive investigations across a wealth of tasks (Tenney et al., 2018; Conneau et al., 2018; Liu et al., 2019). A plethora of methods have been designed and applied (e.g. Li et al., 2016; Saphra and Lopez, 2019; Jumelet et al., 2019) to answer this question. Probing (Adi et al., 2017; Hupkes et al., 2018; Figure 5: Scatter graph of two most informative fastText (above) and BERT (below) dimensions for English present and past tense. Cont"
2020.emnlp-main.15,N19-1112,0,0.10788,"en be used to evaluate the log-likelihood of any subset of dimensions in constant or near-constant time. To the best of our knowledge, no probes in the literature exhibit this property; the primary technical contribution of the paper is the development of such a probe in §3. Other Selection Criteria. Our exposition above uses the log-likelihood of held-out data as a selection criterion for a subset of dimensions; however, any function that scores a subset of dimensions is suitable. For example, much of the current probing literature relies on accuracy to evaluate probes (Conneau et al., 2018; Liu et al., 2019, inter alia), and two recent papers motivate a probabilistic evaluation with information theory (Pimentel et al., 2020b; Voita and Titov, 2020). One could select based on accuracy, mutual information, or anything else within our framework. In fact, recent work in intrinsic probing by Dalvi et al. (2019) could be recast into our framework if we chose a dimension selection criterion based on the magnitude of the weights of a linear probe. However, we suspect that a performance-based dimension selection criterion (e.g., log-likelihood) should be more robust given that a weight-based approach is"
2020.emnlp-main.15,2020.acl-main.659,1,0.827125,"t al., 2019) to answer this question. Probing (Adi et al., 2017; Hupkes et al., 2018; Figure 5: Scatter graph of two most informative fastText (above) and BERT (below) dimensions for English present and past tense. Contours belong to our probe. Conneau et al., 2018) is one prominent method, which consists of using a lightly parameterized model to predict linguistic phenomena from intermediate representations, albeit recent work has raised concerns on how model parameterization and evaluation metrics may affect the effectiveness of this approach (Hewitt and Liang, 2019; Pimentel et al., 2020b; Maudslay et al., 2020; Pimentel et al., 2020a). Most work in intrinsic probing has focused in the identification of individual neurons that are important for a task (Li et al., 2016; Kádár et al., 2017; Li et al., 2017; Lakretz et al., 2019). Similarly, Clark et al. (2019) and Voita et al. (2019) use probing to analyze BERT’s attention heads, finding some interpretable heads that attend to positional and syntactic features. However, there has also been some work investigating collections of neurons. For example, Shi et al. (2016) observe that different training objectives can affect how focal an intermediate repre"
2020.emnlp-main.15,W18-6011,1,0.844276,"mensions. In practice, we report lower-bound normalized MI (LBNMI), which normalizes LBMI by the entropy of Va , because normalizing MI estimates drawn from different samples enables them to be compared (Gates et al., 2019). 5 Experimental Setup In this section we outline our experimental setup. Selection Criterion. We use log-likelihood as our greedy selection criterion. We select 50 dimensions, and keep selecting even if the estimate has decreased.11 Data. We map the UD v2.1 treebanks (Nivre et al., 2017) to the UniMorph schema (Kirov et al., 2018; Sylak-Glassman, 2016) using the mapping by McCarthy et al. (2018). We keep only the “main” treebank for a language (e.g. UD_Portuguese as opposed to UD_Portuguese_PUD). We remove any sentences that would have a sub-token length greater than 512, the maximum allowed for our BERT model.12 We assign any tags from the constituents of a contraction to the contracted word form (e.g., for Portuguese, we copy annotations from de and a to the contracted word form da). We use the UD train split to train a probe for each attribute, the validation split to choose which dimensions to select using our greedy scheme, and the test split to evaluate the performance of the p"
2020.emnlp-main.15,N18-1202,0,0.65979,"for various morphosyntactic attributes across 36 languages. We find that most attributes are reliably encoded by only a few neurons, with fastText concentrating its linguistic structure more than BERT.1 1 1 Dimension 179 0.5 0 −0.5 −1 −1.5 −2 −1.5 −1 −0.5 0 0.5 1 Dimension 477 Figure 1: Scatter plot of the two most informative BERT dimensions for English present and past tense. The contours belong to our probe. Introduction Natural language processing (NLP) is enamored of contextual word representations—and for good reason! Contextual word-embedders, e.g. BERT (Devlin et al., 2019) and ELMo (Peters et al., 2018), have bolstered NLP model performance on myriad tasks, such as syntactic parsing (Kitaev et al., 2019), coreference resolution (Joshi et al., 2019), morphological tagging (Kondratyuk, 2019) and text generation (Zellers et al., 2019). Given the large empirical gains observed when they are employed, it is all but certain that word representations derived from neural networks encode some continuous analogue of linguistic structures. Exactly what these representations encode about linguistic structure, however, remains little understood. Researchers have studied this question by attributing funct"
2020.emnlp-main.15,P19-1171,1,0.803355,"ayesian inference as we found a maximum a posteriori (MAP) estimate to be sufficient for our purposes.9 MAP estimation uses the parameters at the posterior mode θ ?v = argmax p(θ v |D(v) ) 4.2 Recent work has advocated for informationtheoretic metrics in probing (Voita and Titov, 2020; Pimentel et al., 2020b). One such metric, mutual information (MI), measures how predictable the occurrence of one random variable is given another. (12) θv = argmax GIW(µv , Σv |µn , kn , Λn , νn ) We estimate the MI between representations and particular attributes using a method similar to the one proposed by Pimentel et al. (2019) (refer to App. D for an extended derivation). Let Va be a Va -valued random variable denoting the value of a morphosyntactic attribute, and H be a Rd -valued random variable for the word representation. µv ,Σv which are (Murphy, 2012, Chapter 4) µ?v = µn Σ?v = 1 Λn νn + d + 2 (13) (14) The mutual information between Va and H is where d is the dimensionality of the Gaussian. 4 Probing Metrics MI(Va ; H) = H(Va ) − H(Va |H) In this section, we describe the metrics that we compute. We track both accuracy (§4.1) and mutual information (§4.2). 4.1 Mutual Information Accuracy As with most probes in"
2020.emnlp-main.15,2020.emnlp-main.254,1,0.831367,"st of our knowledge, no probes in the literature exhibit this property; the primary technical contribution of the paper is the development of such a probe in §3. Other Selection Criteria. Our exposition above uses the log-likelihood of held-out data as a selection criterion for a subset of dimensions; however, any function that scores a subset of dimensions is suitable. For example, much of the current probing literature relies on accuracy to evaluate probes (Conneau et al., 2018; Liu et al., 2019, inter alia), and two recent papers motivate a probabilistic evaluation with information theory (Pimentel et al., 2020b; Voita and Titov, 2020). One could select based on accuracy, mutual information, or anything else within our framework. In fact, recent work in intrinsic probing by Dalvi et al. (2019) could be recast into our framework if we chose a dimension selection criterion based on the magnitude of the weights of a linear probe. However, we suspect that a performance-based dimension selection criterion (e.g., log-likelihood) should be more robust given that a weight-based approach is sensitive to feature collinearity, variance and regularization. As we mentioned before, performance-based selection req"
2020.emnlp-main.15,2020.acl-main.420,1,0.846708,"st of our knowledge, no probes in the literature exhibit this property; the primary technical contribution of the paper is the development of such a probe in §3. Other Selection Criteria. Our exposition above uses the log-likelihood of held-out data as a selection criterion for a subset of dimensions; however, any function that scores a subset of dimensions is suitable. For example, much of the current probing literature relies on accuracy to evaluate probes (Conneau et al., 2018; Liu et al., 2019, inter alia), and two recent papers motivate a probabilistic evaluation with information theory (Pimentel et al., 2020b; Voita and Titov, 2020). One could select based on accuracy, mutual information, or anything else within our framework. In fact, recent work in intrinsic probing by Dalvi et al. (2019) could be recast into our framework if we chose a dimension selection criterion based on the magnitude of the weights of a linear probe. However, we suspect that a performance-based dimension selection criterion (e.g., log-likelihood) should be more robust given that a weight-based approach is sensitive to feature collinearity, variance and regularization. As we mentioned before, performance-based selection req"
2020.emnlp-main.15,W18-5412,0,0.019993,"nts, which denote the LBNMI after selecting 2, 10 and 50 dimensions. of information for any given subset of dimensions. However, we expect that better modeling of the embedding distribution should improve our bound on the mutual information and thus yield a better probe (Pimentel et al., 2020b). 7 −0.1 −0.2 Dimension 179 Afro-Asiatic Related Work There has been a growing interest in understanding what information is in NLP models’ internal representations. Studies vary widely, from detailed analyses of particular scenarios and linguistic phenomena (Linzen et al., 2016; Gulordava et al., 2018; Ravfogel et al., 2018; KrasnowskaKiera´s and Wróblewska, 2019; Wallace et al., 2019; Warstadt et al., 2019; Sorodoc et al., 2020) to extensive investigations across a wealth of tasks (Tenney et al., 2018; Conneau et al., 2018; Liu et al., 2019). A plethora of methods have been designed and applied (e.g. Li et al., 2016; Saphra and Lopez, 2019; Jumelet et al., 2019) to answer this question. Probing (Adi et al., 2017; Hupkes et al., 2018; Figure 5: Scatter graph of two most informative fastText (above) and BERT (below) dimensions for English present and past tense. Contours belong to our probe. Conneau et al., 2018)"
2020.emnlp-main.15,D19-1286,0,0.0213575,"or any given subset of dimensions. However, we expect that better modeling of the embedding distribution should improve our bound on the mutual information and thus yield a better probe (Pimentel et al., 2020b). 7 −0.1 −0.2 Dimension 179 Afro-Asiatic Related Work There has been a growing interest in understanding what information is in NLP models’ internal representations. Studies vary widely, from detailed analyses of particular scenarios and linguistic phenomena (Linzen et al., 2016; Gulordava et al., 2018; Ravfogel et al., 2018; KrasnowskaKiera´s and Wróblewska, 2019; Wallace et al., 2019; Warstadt et al., 2019; Sorodoc et al., 2020) to extensive investigations across a wealth of tasks (Tenney et al., 2018; Conneau et al., 2018; Liu et al., 2019). A plethora of methods have been designed and applied (e.g. Li et al., 2016; Saphra and Lopez, 2019; Jumelet et al., 2019) to answer this question. Probing (Adi et al., 2017; Hupkes et al., 2018; Figure 5: Scatter graph of two most informative fastText (above) and BERT (below) dimensions for English present and past tense. Contours belong to our probe. Conneau et al., 2018) is one prominent method, which consists of using a lightly parameterized model to pr"
2020.emnlp-main.170,D18-1045,0,0.0246029,"am search is one of the few NLP algorithms that has stood the test of time: It has remained a cornerstone of NLP systems since the 1970s (Reddy, 1977). As such, it became the natural choice for decoding neural probabilistic text generators—whose design makes evaluating the full search space impossible (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Vinyals and Le, 2015; Yin et al., 2016). While there is no formal guarantee that beam search will return— or even approximate—the highest-scoring candidate under a model, it has repeatedly proven its merit in practice (Serban et al., 2017; Edunov et al., 2018; Yang et al., 2019) and, thus, has largely been tolerated—even embraced—as NLP’s go-to search heuristic. However, in the context of neural machine translation (NMT), a shocking empirical finding has emerged: Using beam search to decode sentences from neural text generators almost invariably leads to better text than using exact search (or beam search with a very large beam size). In fact, Stahlberg and Byrne (2019) report that exact search 2173 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 2173–2185, c November 16–20, 2020. 2020 Association for"
2020.emnlp-main.170,W13-3214,0,0.0176519,"m search (k = 5 and k = 100) are included. Sub-graph shows the explicit relationship between BLEU and σ. λ and σ axes are log-scaled. Introduction As a simple search heuristic, beam search has been used to decode models developed by the NLP community for decades. Indeed, it is noteworthy that beam search is one of the few NLP algorithms that has stood the test of time: It has remained a cornerstone of NLP systems since the 1970s (Reddy, 1977). As such, it became the natural choice for decoding neural probabilistic text generators—whose design makes evaluating the full search space impossible (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Vinyals and Le, 2015; Yin et al., 2016). While there is no formal guarantee that beam search will return— or even approximate—the highest-scoring candidate under a model, it has repeatedly proven its merit in practice (Serban et al., 2017; Edunov et al., 2018; Yang et al., 2019) and, thus, has largely been tolerated—even embraced—as NLP’s go-to search heuristic. However, in the context of neural machine translation (NMT), a shocking empirical finding has emerged: Using beam search to decode sentences from neural text generators almost invariably leads to better text t"
2020.emnlp-main.170,koen-2004-pharaoh,0,0.14752,"f iterations3 nmax and the set Ynmax is returned. We overload pθ (· |x) to take a set of hypotheses as an argument instead of just Qa single hypothesis. In this case, pθ (Y |x) := y∈Y pθ (y | x).4 Using a similar schema, the argmax may also operate over a different objective, e.g., logprobabilities combined with various rewards or penaties, such as those discussed in §2.2. Beam search has a long history in sequence transduction. For example, many of the decoding strategies used in statistical machine translation (SMT) systems were variants of beam search (Och et al., 1999; Koehn et al., 2003; Koehn, 2004). As language generation systems moved away from phrase-based statistical approaches and towards neural models, beam search remained the de-facto decoding algorithm (Sutskever et al., 2014; Vinyals and Le, 2015). However, it has been observed that when used as a decoding algorithm for neural text generation, beam search (for small beams) typically has a large percentage of search errors 3 If all hypotheses in Yt end in EOS for some t &lt; nmax , then we may terminate beam search early as it is then gauranteed that Yt = Ynmax . We do not consider further earlystopping methods for beam search (Huan"
2020.emnlp-main.170,N01-1021,0,0.123907,"Hypothesis 4.1. “Within the bounds defined by grammar, speakers prefer utterances that distribute information uniformly across the signal (information density). Where speakers have a choice between several variants to encode their message, they prefer the variant with more uniform information density (ceteris paribus)” (Jaeger, 2010). At its core, the theory seeks to explain various aspects of human language processing in terms of information theory; it is often applied to an area of psycholinguistics known as sentence processing where the UID hypothesis is used to explain experimental data (Hale, 2001). As the UID hypothesis concerns a cognitive process (virtually) independent of the language in use, the theory should hold across languages (Jaeger and Tily, 2011). To see the hypothesis in action, consider the classic case of syntactic reduction from Levy and Jaeger (2007): (1) How big is [NP the familyi [RC (that) you cook for −i ]]? In the above example, the sentence does not require the relativizer that at the start of the relative clause (denoted by RC); it would also be syntactically correct without it. However, many would agree that the relativizer makes the text qualitatively better."
2020.emnlp-main.170,W17-3204,0,0.54135,"or beam search (Huang et al., 2017; Yang et al., 2018; Meister et al., 2020) as they generally should not affect the quality of the decoded set. 4 There do exist objectives that take into account interactions between hypotheses in a set, e.g., diverse beam search (Vijayakumar et al., 2018), but we do not consider those here. (Stahlberg and Byrne, 2019). Counterintuitively, it is widely known that increasing the beam size beyond 5 can hurt model performance in terms of downstream evaluation metrics (e.g., BLEU, ROUGE ); while a number of prior works have referred to this phenomenon as a curse (Koehn and Knowles, 2017; Yang et al., 2018; Cohen and Beck, 2019), it should perhaps be seen as a blessing. Beam search typically generates well-formed and coherent text from probabilistic models, whose global optimum in many cases is the empty string, when they otherwise might fail to produce text at all. As we demonstrate in §4, this text also tends to be human-like. We will subsequently explore possible reasons as to why beam search leads to desirable text from models that are otherwise poorly calibrated, i.e., poor representations of the true distribution p(y |x) (Guo et al., 2017). 2.2 Alternative Decoding Obje"
2020.emnlp-main.170,N03-1017,0,0.199814,"Missing"
2020.emnlp-main.170,D17-1227,0,0.0240534,"004). As language generation systems moved away from phrase-based statistical approaches and towards neural models, beam search remained the de-facto decoding algorithm (Sutskever et al., 2014; Vinyals and Le, 2015). However, it has been observed that when used as a decoding algorithm for neural text generation, beam search (for small beams) typically has a large percentage of search errors 3 If all hypotheses in Yt end in EOS for some t &lt; nmax , then we may terminate beam search early as it is then gauranteed that Yt = Ynmax . We do not consider further earlystopping methods for beam search (Huang et al., 2017; Yang et al., 2018; Meister et al., 2020) as they generally should not affect the quality of the decoded set. 4 There do exist objectives that take into account interactions between hypotheses in a set, e.g., diverse beam search (Vijayakumar et al., 2018), but we do not consider those here. (Stahlberg and Byrne, 2019). Counterintuitively, it is widely known that increasing the beam size beyond 5 can hurt model performance in terms of downstream evaluation metrics (e.g., BLEU, ROUGE ); while a number of prior works have referred to this phenomenon as a curse (Koehn and Knowles, 2017; Yang et a"
2020.emnlp-main.170,W18-4605,0,0.0184464,"gnitive science. As a final step, we next provide operationalizations of UID—in the form of regularizers within our regularized decoding framework—through which we can empirically test the validity of this hypothesis. 5 Generalized UID Decoding If beam search is trying to optimize for UID, can we beat it at its own game? This section develops a battery of possible sentence-level UID measures, which can be used as regularizers in our regularized decoding framework and compared experimentally on downstream evaluation metrics. Variance Regularizer. We first consider the variance regularizer from Jain et al. (2018). In essence, UID concerns the distribution of information over the course (i.e., time steps) of a sentence. A natural 2178 measure for this is variance of the surprisals. Rvar (y) = |y| 2 1 X ut (yt ) − µ |y| (14) t=1 P|y| where µ = 1/|y |t=1 ut (yt ). This regularizer, in contrast to Eq. (11), is a much more straightforward encoding of the UID: it directly operationalizes UID through variance. Local Consistency. Next we consider a local consistency regularizer, also taken from Jain et al. (2018), that encourages adjacent surprisals to have similar magnitude: |y| 2 1 X Rlocal (y) = ut (yt"
2020.emnlp-main.170,W15-3014,0,0.170404,"e human-like. We will subsequently explore possible reasons as to why beam search leads to desirable text from models that are otherwise poorly calibrated, i.e., poor representations of the true distribution p(y |x) (Guo et al., 2017). 2.2 Alternative Decoding Objectives When the MAP objective (Eq. (3)) is used for decoding neural text generators, the results are generally not satisfactory. Among other problems, the generated texts are often short and defaults to highfrequency words (Cho et al., 2014; Vinyals and Le, 2015; Shen et al., 2016). Methods such as length and coverage normalization (Jean et al., 2015; Tu et al., 2016; Murray and Chiang, 2018), which augment the MAP objective with an additive term or multiplicative factor, have been adopted to alleviate these issues. For example, two such forms of length5 and coverage normalization use the following modified MAP objective respectively during decoding to produce higher-quality output: log pθ (y |x) + λ|y| (7)   |y| |x| X X  log pθ (y |x)+λ log min 1, αij  (8) i=1 j=1 where λ &gt; 0 is the (tunable) strength of the reward and αij is the attention weight (Bahdanau et al., 2015) from the j th decoding step over the ith input. Eq. (7) directly"
2020.emnlp-main.170,N16-1014,0,0.0324796,"tandard and lengthnormalized, i.e. score divided by sequence length, beam search with noticeable improvements for larger beams. Search details and parameter settings may be found in App. B. Notably, combining multiple UID regularizers does not lead to as great an increase in performance as one might expect, which hints that a single method for enforcing UID is sufficient for promoting quality in generated text. 2180 7 Related Work Acknowledgments Neural probabilistic text generators are far from perfect; prior work has shown that they often generate text that is generic (Vinyals and Le, 2015; Li et al., 2016), unnatural (Holtzman et al., 2020), and sometimes even non-existent (Stahlberg and Byrne, 2019). In the context of the degenerate behavior of these models, the beam search curse—a specific phenomenon where using a larger beam size leads to worse performance—has been analyzed by a number of authors (Koehn and Knowles, 2017; Murray and Chiang, 2018; Yang et al., 2018; Stahlberg and Byrne, 2019; Jean et al., 2015; Tu et al., 2016; He et al., 2016; Cohen and Beck, 2019). Many of these authors attribute the performance drop (as search becomes better) to an inherent bias in neural sequence models t"
2020.emnlp-main.170,2020.tacl-1.51,1,0.881597,"ved away from phrase-based statistical approaches and towards neural models, beam search remained the de-facto decoding algorithm (Sutskever et al., 2014; Vinyals and Le, 2015). However, it has been observed that when used as a decoding algorithm for neural text generation, beam search (for small beams) typically has a large percentage of search errors 3 If all hypotheses in Yt end in EOS for some t &lt; nmax , then we may terminate beam search early as it is then gauranteed that Yt = Ynmax . We do not consider further earlystopping methods for beam search (Huang et al., 2017; Yang et al., 2018; Meister et al., 2020) as they generally should not affect the quality of the decoded set. 4 There do exist objectives that take into account interactions between hypotheses in a set, e.g., diverse beam search (Vijayakumar et al., 2018), but we do not consider those here. (Stahlberg and Byrne, 2019). Counterintuitively, it is widely known that increasing the beam size beyond 5 can hurt model performance in terms of downstream evaluation metrics (e.g., BLEU, ROUGE ); while a number of prior works have referred to this phenomenon as a curse (Koehn and Knowles, 2017; Yang et al., 2018; Cohen and Beck, 2019), it should"
2020.emnlp-main.170,W18-6322,0,0.184448,"xplore possible reasons as to why beam search leads to desirable text from models that are otherwise poorly calibrated, i.e., poor representations of the true distribution p(y |x) (Guo et al., 2017). 2.2 Alternative Decoding Objectives When the MAP objective (Eq. (3)) is used for decoding neural text generators, the results are generally not satisfactory. Among other problems, the generated texts are often short and defaults to highfrequency words (Cho et al., 2014; Vinyals and Le, 2015; Shen et al., 2016). Methods such as length and coverage normalization (Jean et al., 2015; Tu et al., 2016; Murray and Chiang, 2018), which augment the MAP objective with an additive term or multiplicative factor, have been adopted to alleviate these issues. For example, two such forms of length5 and coverage normalization use the following modified MAP objective respectively during decoding to produce higher-quality output: log pθ (y |x) + λ|y| (7)   |y| |x| X X  log pθ (y |x)+λ log min 1, αij  (8) i=1 j=1 where λ &gt; 0 is the (tunable) strength of the reward and αij is the attention weight (Bahdanau et al., 2015) from the j th decoding step over the ith input. Eq. (7) directly rewards longer outputs (He et al., 2016) w"
2020.emnlp-main.170,W19-5333,0,0.0197304,"y rewards longer outputs (He et al., 2016) while Eq. (8) aims to reward coverage of input words in a prediction using the attention mechanism of an encoder–decoder model as an oracle (Tu 5 The predominant form of length normalization divides (log) sequence probability by the length of the hypothesis rather than using an additive reward as in (He et al., 2016). We present results from the former in our experiments as we find it empirically leads to better performance. 2175 et al., 2016). While such methods help obtain stateof-the-art results in neural MT (Wu et al., 2016; Gehring et al., 2017; Ng et al., 2019), we view them as a patch to the observed problems. The fact that text quality still degrades with increased beam sizes when these rewards are used (Koehn and Knowles, 2017; Ott et al., 2018a) suggests that they do not address the inherent issues with text generation systems. We subsequently hypothesize about the nature of these issues and provide a set of linguistically motivated regularizers—inspired by beam search—that appear to alleviate them. 3 Deriving Beam Search We introduce a regularized decoding framework. The idea is simple; we seek to solve the regularized optimization problem to d"
2020.emnlp-main.170,D19-1331,0,0.559265,", beam search (for small beams) typically has a large percentage of search errors 3 If all hypotheses in Yt end in EOS for some t &lt; nmax , then we may terminate beam search early as it is then gauranteed that Yt = Ynmax . We do not consider further earlystopping methods for beam search (Huang et al., 2017; Yang et al., 2018; Meister et al., 2020) as they generally should not affect the quality of the decoded set. 4 There do exist objectives that take into account interactions between hypotheses in a set, e.g., diverse beam search (Vijayakumar et al., 2018), but we do not consider those here. (Stahlberg and Byrne, 2019). Counterintuitively, it is widely known that increasing the beam size beyond 5 can hurt model performance in terms of downstream evaluation metrics (e.g., BLEU, ROUGE ); while a number of prior works have referred to this phenomenon as a curse (Koehn and Knowles, 2017; Yang et al., 2018; Cohen and Beck, 2019), it should perhaps be seen as a blessing. Beam search typically generates well-formed and coherent text from probabilistic models, whose global optimum in many cases is the empty string, when they otherwise might fail to produce text at all. As we demonstrate in §4, this text also tends"
2020.emnlp-main.170,W99-0604,0,0.544754,"ithm terminates after a fixed number of iterations3 nmax and the set Ynmax is returned. We overload pθ (· |x) to take a set of hypotheses as an argument instead of just Qa single hypothesis. In this case, pθ (Y |x) := y∈Y pθ (y | x).4 Using a similar schema, the argmax may also operate over a different objective, e.g., logprobabilities combined with various rewards or penaties, such as those discussed in §2.2. Beam search has a long history in sequence transduction. For example, many of the decoding strategies used in statistical machine translation (SMT) systems were variants of beam search (Och et al., 1999; Koehn et al., 2003; Koehn, 2004). As language generation systems moved away from phrase-based statistical approaches and towards neural models, beam search remained the de-facto decoding algorithm (Sutskever et al., 2014; Vinyals and Le, 2015). However, it has been observed that when used as a decoding algorithm for neural text generation, beam search (for small beams) typically has a large percentage of search errors 3 If all hypotheses in Yt end in EOS for some t &lt; nmax , then we may terminate beam search early as it is then gauranteed that Yt = Ynmax . We do not consider further earlystop"
2020.emnlp-main.170,D17-2005,0,0.0255743,"on the IWSLT’14 De-En (Cettolo et al., 2012) and WMT’14 En-Fr (Bojar et al., 2014) datasets. For reproducibility, we use the model provided by fairseq (Ott et al., 2019) for the WMT’14 task;9 we use the data pre-processing scripts and recommended hyperparameter settings provided by fairseq for training a model on the IWSLT’14 De-En dataset. We use the Newstest’14 dataset as the test set for the WMT’14 model. All model and data information can be found on the fairseq NMT repository. 10 6.1 To perform exact decoding of neural probabilistic text generators, we build on the decoding framework of Stahlberg et al. (2017), albeit using Dijkstra’s algorithm (Dijkstra, 1959) instead of depthfirst search as we find it decreases decoding time. Note that Dijkstra’s algorithm is guaranteed to find the global optimum when path cost is monotoni8 (17) t=1 Experimentally, we expect to see the following: If encouraging decoded text to exhibit UID is Exact Decoding By optimal beam search, we mean beam search using the beam width that empirically leads to the best results. 9 This model uses a transformer architecture (Vaswani et al., 2017) and was trained as in Ott et al. (2018b). 10 https://github.com/pytorch/fairseq/ tre"
2020.emnlp-main.170,N19-4009,0,0.0194356,"perform exact decoding for a range of λ and observe how text quality (quantified by BLEU (Papineni et al., 2002) using the SacreBLEU (Post, 2018) system) and the distribution of surprisal changes. We additionally evaluate our regularizers under the beam search decoding strategy to see if penalizing violations of UID alleviates the text-quality degradation typically seen with increased beam widths. Experiments are performed using models trained on the IWSLT’14 De-En (Cettolo et al., 2012) and WMT’14 En-Fr (Bojar et al., 2014) datasets. For reproducibility, we use the model provided by fairseq (Ott et al., 2019) for the WMT’14 task;9 we use the data pre-processing scripts and recommended hyperparameter settings provided by fairseq for training a model on the IWSLT’14 De-En dataset. We use the Newstest’14 dataset as the test set for the WMT’14 model. All model and data information can be found on the fairseq NMT repository. 10 6.1 To perform exact decoding of neural probabilistic text generators, we build on the decoding framework of Stahlberg et al. (2017), albeit using Dijkstra’s algorithm (Dijkstra, 1959) instead of depthfirst search as we find it decreases decoding time. Note that Dijkstra’s algor"
2020.emnlp-main.170,W18-6301,0,0.0994708,"he predominant form of length normalization divides (log) sequence probability by the length of the hypothesis rather than using an additive reward as in (He et al., 2016). We present results from the former in our experiments as we find it empirically leads to better performance. 2175 et al., 2016). While such methods help obtain stateof-the-art results in neural MT (Wu et al., 2016; Gehring et al., 2017; Ng et al., 2019), we view them as a patch to the observed problems. The fact that text quality still degrades with increased beam sizes when these rewards are used (Koehn and Knowles, 2017; Ott et al., 2018a) suggests that they do not address the inherent issues with text generation systems. We subsequently hypothesize about the nature of these issues and provide a set of linguistically motivated regularizers—inspired by beam search—that appear to alleviate them. 3 Deriving Beam Search We introduce a regularized decoding framework. The idea is simple; we seek to solve the regularized optimization problem to decode   y? = argmax log pθ (y |x) − λ · R(y) (9) y∈Y for a strategically chosen R(·). Clearly, for certain R(·), we recover the decoding objectives discussed in §2.2. The question we ask i"
2020.emnlp-main.170,P02-1040,0,0.106551,"connection between UID and high-quality text; comparable performance of optimal beam search8 and exact search under our regularized objective would provide explicit evidence for our declarative explanation of the inductive bias in beam search. 6 Experiments We explore how encouraging uniform information density in text generated by neural probabalistic text generators affects its downstream quality. To this end, we decode NMT models using the regularized objective (Eq. (9)) with our UID regularizers. We perform exact decoding for a range of λ and observe how text quality (quantified by BLEU (Papineni et al., 2002) using the SacreBLEU (Post, 2018) system) and the distribution of surprisal changes. We additionally evaluate our regularizers under the beam search decoding strategy to see if penalizing violations of UID alleviates the text-quality degradation typically seen with increased beam widths. Experiments are performed using models trained on the IWSLT’14 De-En (Cettolo et al., 2012) and WMT’14 En-Fr (Bojar et al., 2014) datasets. For reproducibility, we use the model provided by fairseq (Ott et al., 2019) for the WMT’14 task;9 we use the data pre-processing scripts and recommended hyperparameter se"
2020.emnlp-main.170,W18-6319,0,0.0355621,"t; comparable performance of optimal beam search8 and exact search under our regularized objective would provide explicit evidence for our declarative explanation of the inductive bias in beam search. 6 Experiments We explore how encouraging uniform information density in text generated by neural probabalistic text generators affects its downstream quality. To this end, we decode NMT models using the regularized objective (Eq. (9)) with our UID regularizers. We perform exact decoding for a range of λ and observe how text quality (quantified by BLEU (Papineni et al., 2002) using the SacreBLEU (Post, 2018) system) and the distribution of surprisal changes. We additionally evaluate our regularizers under the beam search decoding strategy to see if penalizing violations of UID alleviates the text-quality degradation typically seen with increased beam widths. Experiments are performed using models trained on the IWSLT’14 De-En (Cettolo et al., 2012) and WMT’14 En-Fr (Bojar et al., 2014) datasets. For reproducibility, we use the model provided by fairseq (Ott et al., 2019) for the WMT’14 task;9 we use the data pre-processing scripts and recommended hyperparameter settings provided by fairseq for tr"
2020.emnlp-main.170,P16-1159,0,0.0385869,"produce text at all. As we demonstrate in §4, this text also tends to be human-like. We will subsequently explore possible reasons as to why beam search leads to desirable text from models that are otherwise poorly calibrated, i.e., poor representations of the true distribution p(y |x) (Guo et al., 2017). 2.2 Alternative Decoding Objectives When the MAP objective (Eq. (3)) is used for decoding neural text generators, the results are generally not satisfactory. Among other problems, the generated texts are often short and defaults to highfrequency words (Cho et al., 2014; Vinyals and Le, 2015; Shen et al., 2016). Methods such as length and coverage normalization (Jean et al., 2015; Tu et al., 2016; Murray and Chiang, 2018), which augment the MAP objective with an additive term or multiplicative factor, have been adopted to alleviate these issues. For example, two such forms of length5 and coverage normalization use the following modified MAP objective respectively during decoding to produce higher-quality output: log pθ (y |x) + λ|y| (7)   |y| |x| X X  log pθ (y |x)+λ log min 1, αij  (8) i=1 j=1 where λ &gt; 0 is the (tunable) strength of the reward and αij is the attention weight (Bahdanau et al.,"
2020.emnlp-main.170,D16-1158,0,0.0900994,"ls, the beam search curse—a specific phenomenon where using a larger beam size leads to worse performance—has been analyzed by a number of authors (Koehn and Knowles, 2017; Murray and Chiang, 2018; Yang et al., 2018; Stahlberg and Byrne, 2019; Jean et al., 2015; Tu et al., 2016; He et al., 2016; Cohen and Beck, 2019). Many of these authors attribute the performance drop (as search becomes better) to an inherent bias in neural sequence models to pefer shorter sentences. Other authors have ascribed fault to the model architectures, or how they are trained (Cho et al., 2014; Bengio et al., 2015; Sountsov and Sarawagi, 2016; Vinyals et al., 2017; Ott et al., 2018a; Kumar and Sarawagi, 2019). To remedy the problem, a large number of regularized decoding objectives and modified training techniques have been proposed. In contrast, this work analyzes the behavior of neural text generators from a different angle: We provide a plausible answer—inspired by psycholinguistic theory—as to why beam search (with small beams) leads to high-quality text, rather than another explanation of why exact search performs so badly. 8 Conclusion We analyze beam search as a decoding strategy for text generation models by framing it as"
2020.emnlp-main.170,P16-1008,0,0.0961302,"ll subsequently explore possible reasons as to why beam search leads to desirable text from models that are otherwise poorly calibrated, i.e., poor representations of the true distribution p(y |x) (Guo et al., 2017). 2.2 Alternative Decoding Objectives When the MAP objective (Eq. (3)) is used for decoding neural text generators, the results are generally not satisfactory. Among other problems, the generated texts are often short and defaults to highfrequency words (Cho et al., 2014; Vinyals and Le, 2015; Shen et al., 2016). Methods such as length and coverage normalization (Jean et al., 2015; Tu et al., 2016; Murray and Chiang, 2018), which augment the MAP objective with an additive term or multiplicative factor, have been adopted to alleviate these issues. For example, two such forms of length5 and coverage normalization use the following modified MAP objective respectively during decoding to produce higher-quality output: log pθ (y |x) + λ|y| (7)   |y| |x| X X  log pθ (y |x)+λ log min 1, αij  (8) i=1 j=1 where λ &gt; 0 is the (tunable) strength of the reward and αij is the attention weight (Bahdanau et al., 2015) from the j th decoding step over the ith input. Eq. (7) directly rewards longer o"
2020.emnlp-main.170,D18-1342,0,0.208142,"Missing"
2020.emnlp-main.170,W16-0106,0,0.0252547,"onship between BLEU and σ. λ and σ axes are log-scaled. Introduction As a simple search heuristic, beam search has been used to decode models developed by the NLP community for decades. Indeed, it is noteworthy that beam search is one of the few NLP algorithms that has stood the test of time: It has remained a cornerstone of NLP systems since the 1970s (Reddy, 1977). As such, it became the natural choice for decoding neural probabilistic text generators—whose design makes evaluating the full search space impossible (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Vinyals and Le, 2015; Yin et al., 2016). While there is no formal guarantee that beam search will return— or even approximate—the highest-scoring candidate under a model, it has repeatedly proven its merit in practice (Serban et al., 2017; Edunov et al., 2018; Yang et al., 2019) and, thus, has largely been tolerated—even embraced—as NLP’s go-to search heuristic. However, in the context of neural machine translation (NMT), a shocking empirical finding has emerged: Using beam search to decode sentences from neural text generators almost invariably leads to better text than using exact search (or beam search with a very large beam siz"
2020.emnlp-main.232,N19-1423,0,0.0115755,"ion Pre-trained word representations are a necessity for strong performance on modern NLP tasks. These embeddings now serve as input to neural methods (Goldberg, 2017), which recently have become the standard models in the field. However, because these representations are constructed from large, human-created corpora, they naturally contain societal biases encoded in that data; gender bias is among the most well studied of these biases (Caliskan et al., 2017). Both a-contextual word embeddings (Mikolov et al., 2013; Pennington et al., 2014) and contextual word embeddings (Peters et al., 2018; Devlin et al., 2019) have been shown to encode gender bias (Bolukbasi et al., 2016; Caliskan et al., 2017; Zhao et al., 2019; May et al., 2019; Karve et al., 2019). More importantly, the bias in those embeddings has been shown to influence models for downstream tasks where they are used as input, e.g. coreference resolution (Rudinger et al., 2018; Zhao et al., 2018). Bolukbasi et al. (2016) present one of the first methods for detecting and mitigating gender bias in word embeddings. They provide a novel linear-algebraic approach that post-processes word embeddings in order to partially remove gender bias. Under t"
2020.emnlp-main.232,W19-3621,0,0.454633,"Karve et al., 2019). More importantly, the bias in those embeddings has been shown to influence models for downstream tasks where they are used as input, e.g. coreference resolution (Rudinger et al., 2018; Zhao et al., 2018). Bolukbasi et al. (2016) present one of the first methods for detecting and mitigating gender bias in word embeddings. They provide a novel linear-algebraic approach that post-processes word embeddings in order to partially remove gender bias. Under their evaluation, they find they can nearly perfectly remove bias in an analogical reasoning task. However, subsequent work (Gonen and Goldberg, 2019; Hall Maudslay et al., 2019) has indicated that gender bias still lingers in the embeddings, despite Bolukbasi et al. (2016)’s strong experimental results. In the development of their method, Bolukbasi et al. (2016) make a critical and unstated assumption: Gender bias forms a linear subspace of word embedding space. In mathematics, linearity is a strong assumption and there is no reason a-priori why one should expect complex and nuanced societal phenomena, such as gender bias, should be represented by a linear subspace. In this work, we present the first non-linear gender bias mitigation tech"
2020.emnlp-main.232,D19-1530,1,0.803782,"tantly, the bias in those embeddings has been shown to influence models for downstream tasks where they are used as input, e.g. coreference resolution (Rudinger et al., 2018; Zhao et al., 2018). Bolukbasi et al. (2016) present one of the first methods for detecting and mitigating gender bias in word embeddings. They provide a novel linear-algebraic approach that post-processes word embeddings in order to partially remove gender bias. Under their evaluation, they find they can nearly perfectly remove bias in an analogical reasoning task. However, subsequent work (Gonen and Goldberg, 2019; Hall Maudslay et al., 2019) has indicated that gender bias still lingers in the embeddings, despite Bolukbasi et al. (2016)’s strong experimental results. In the development of their method, Bolukbasi et al. (2016) make a critical and unstated assumption: Gender bias forms a linear subspace of word embedding space. In mathematics, linearity is a strong assumption and there is no reason a-priori why one should expect complex and nuanced societal phenomena, such as gender bias, should be represented by a linear subspace. In this work, we present the first non-linear gender bias mitigation technique for a-contextual word e"
2020.emnlp-main.232,J15-4004,0,0.0949007,"l., 2016; Gonen and Goldberg, 2019) that measures how word embeddings representing different professions are potentially genderstereotyped. Again, as with the WEAT evaluation, we find that our non-linear bias mitigation technique performs on par with the linear method. We also consider whether the non-linear gender mitigation technique removes indirect bias from the vectors (Gonen and Goldberg, 2019); yet again, we find the non-linear method performs on par with the linear methods. As a final evaluation, we evaluate whether non-linear bias mitigation hurts semantic performance. On Simlex-999 (Hill et al., 2015), we show that similarity estimates between the vectors remain on par with the linear methods. We conclude that much of the gender bias in word embeddings is indeed captured by a linear subspace, answering this paper’s titular question. 2 Bias as a Linear Subspace The first step of Bolukbasi et al. (2016)’s technique is the discovery of a subspace B ⊂ Rd that captures most of the gender bias. Specifically, they stipulate that this space is linear. Given word embeddings that live in Rd , they provide a spectral method for isolating the bias subspace. In this section, we review their approach an"
2020.emnlp-main.232,W19-3806,0,0.107485,"ethods (Goldberg, 2017), which recently have become the standard models in the field. However, because these representations are constructed from large, human-created corpora, they naturally contain societal biases encoded in that data; gender bias is among the most well studied of these biases (Caliskan et al., 2017). Both a-contextual word embeddings (Mikolov et al., 2013; Pennington et al., 2014) and contextual word embeddings (Peters et al., 2018; Devlin et al., 2019) have been shown to encode gender bias (Bolukbasi et al., 2016; Caliskan et al., 2017; Zhao et al., 2019; May et al., 2019; Karve et al., 2019). More importantly, the bias in those embeddings has been shown to influence models for downstream tasks where they are used as input, e.g. coreference resolution (Rudinger et al., 2018; Zhao et al., 2018). Bolukbasi et al. (2016) present one of the first methods for detecting and mitigating gender bias in word embeddings. They provide a novel linear-algebraic approach that post-processes word embeddings in order to partially remove gender bias. Under their evaluation, they find they can nearly perfectly remove bias in an analogical reasoning task. However, subsequent work (Gonen and Goldberg,"
2020.emnlp-main.232,N19-1063,0,0.170478,"input to neural methods (Goldberg, 2017), which recently have become the standard models in the field. However, because these representations are constructed from large, human-created corpora, they naturally contain societal biases encoded in that data; gender bias is among the most well studied of these biases (Caliskan et al., 2017). Both a-contextual word embeddings (Mikolov et al., 2013; Pennington et al., 2014) and contextual word embeddings (Peters et al., 2018; Devlin et al., 2019) have been shown to encode gender bias (Bolukbasi et al., 2016; Caliskan et al., 2017; Zhao et al., 2019; May et al., 2019; Karve et al., 2019). More importantly, the bias in those embeddings has been shown to influence models for downstream tasks where they are used as input, e.g. coreference resolution (Rudinger et al., 2018; Zhao et al., 2018). Bolukbasi et al. (2016) present one of the first methods for detecting and mitigating gender bias in word embeddings. They provide a novel linear-algebraic approach that post-processes word embeddings in order to partially remove gender bias. Under their evaluation, they find they can nearly perfectly remove bias in an analogical reasoning task. However, subsequent work"
2020.emnlp-main.232,D14-1162,0,0.0918804,"ar subspace, justifying the assumption of Bolukbasi et al. (2016). 1 Introduction Pre-trained word representations are a necessity for strong performance on modern NLP tasks. These embeddings now serve as input to neural methods (Goldberg, 2017), which recently have become the standard models in the field. However, because these representations are constructed from large, human-created corpora, they naturally contain societal biases encoded in that data; gender bias is among the most well studied of these biases (Caliskan et al., 2017). Both a-contextual word embeddings (Mikolov et al., 2013; Pennington et al., 2014) and contextual word embeddings (Peters et al., 2018; Devlin et al., 2019) have been shown to encode gender bias (Bolukbasi et al., 2016; Caliskan et al., 2017; Zhao et al., 2019; May et al., 2019; Karve et al., 2019). More importantly, the bias in those embeddings has been shown to influence models for downstream tasks where they are used as input, e.g. coreference resolution (Rudinger et al., 2018; Zhao et al., 2018). Bolukbasi et al. (2016) present one of the first methods for detecting and mitigating gender bias in word embeddings. They provide a novel linear-algebraic approach that post-p"
2020.emnlp-main.232,N18-1202,0,0.0671837,". (2016). 1 Introduction Pre-trained word representations are a necessity for strong performance on modern NLP tasks. These embeddings now serve as input to neural methods (Goldberg, 2017), which recently have become the standard models in the field. However, because these representations are constructed from large, human-created corpora, they naturally contain societal biases encoded in that data; gender bias is among the most well studied of these biases (Caliskan et al., 2017). Both a-contextual word embeddings (Mikolov et al., 2013; Pennington et al., 2014) and contextual word embeddings (Peters et al., 2018; Devlin et al., 2019) have been shown to encode gender bias (Bolukbasi et al., 2016; Caliskan et al., 2017; Zhao et al., 2019; May et al., 2019; Karve et al., 2019). More importantly, the bias in those embeddings has been shown to influence models for downstream tasks where they are used as input, e.g. coreference resolution (Rudinger et al., 2018; Zhao et al., 2018). Bolukbasi et al. (2016) present one of the first methods for detecting and mitigating gender bias in word embeddings. They provide a novel linear-algebraic approach that post-processes word embeddings in order to partially remov"
2020.emnlp-main.232,N18-2002,0,0.0688168,"Missing"
2020.emnlp-main.232,N19-1064,1,0.880333,"Missing"
2020.emnlp-main.232,N18-2003,0,0.178397,"l biases encoded in that data; gender bias is among the most well studied of these biases (Caliskan et al., 2017). Both a-contextual word embeddings (Mikolov et al., 2013; Pennington et al., 2014) and contextual word embeddings (Peters et al., 2018; Devlin et al., 2019) have been shown to encode gender bias (Bolukbasi et al., 2016; Caliskan et al., 2017; Zhao et al., 2019; May et al., 2019; Karve et al., 2019). More importantly, the bias in those embeddings has been shown to influence models for downstream tasks where they are used as input, e.g. coreference resolution (Rudinger et al., 2018; Zhao et al., 2018). Bolukbasi et al. (2016) present one of the first methods for detecting and mitigating gender bias in word embeddings. They provide a novel linear-algebraic approach that post-processes word embeddings in order to partially remove gender bias. Under their evaluation, they find they can nearly perfectly remove bias in an analogical reasoning task. However, subsequent work (Gonen and Goldberg, 2019; Hall Maudslay et al., 2019) has indicated that gender bias still lingers in the embeddings, despite Bolukbasi et al. (2016)’s strong experimental results. In the development of their method, Bolukba"
2020.emnlp-main.254,I17-1001,0,0.0429719,"rly complex model, we might ascribe high accuracy on the probing task to the probe itself, meaning the probe has “learned the task” to a large extent. In this section, we argue that a probing framework that does not explicitly take into account the accuracy–complexity trade-off may be easily gamed. Indeed, we demonstrate how to game both accuracy and complexity respectively below. 2.1 The Nature of Probing Tasks Most probing tasks are relatively “toy” in nature (Hupkes et al., 2018).2 For instance, two of the most common probing tasks are part-of-speech labeling (POSL; Hewitt and Liang, 2019; Belinkov et al., 2017) and dependency arc labeling (DAL; Tenney et al., 2019a,b; Voita and Titov, 2020). Both tasks are treated as multi-way classification problems. POSL requires a model to assign a part-of-speech tag to a word in context without 2 Not all though, several people have looked into e.g. parse tree reconstruction tasks (Jawahar et al., 2019; Hewitt and Manning, 2019; Vilares et al., 2020) 3139 modeling the entire sequence of part-of-speech tags. Likewise, DAL requires a model to assign a dependency-arc label to an arc independently of the larger dependency tree. These word-oriented probing approaches"
2020.emnlp-main.254,Q17-1010,0,0.0493573,"dependency labels in the language, but we predict these labels from pairs of representations—the two words composing the arc. We analyze the contextual representations from BERT (Devlin et al., 2019), ALBERT (Lan et al., 2020) and RoBERTa (Liu et al., 2019)—noting that ALBERT and RoBERTa are trained in English alone, so we only evaluate their performance on that language.7 For each of these models, we feed it a sentence and average the output word piece (Wu et al., 2016) representations for each word, as tokenized in the treebank. We further analyze fastText’s non-contextual representations (Bojanowski et al., 2017) as well as one-hot and random representations such as those considered by Pimentel et al. (2020). One-hot and random representations map each word type in the training data to a vector we sample from a standard normal distribution (zero mean and unit variance). New representations are sampled on the spot (untrained) for any out of vocabulary words. All representations are kept fixed during training, except for one-hot, which are learned with the other network parameters. 6.1 Linear Probes with Norm Constraints For each language–representation–task triple, we train 100 linear probes, 50 optimi"
2020.emnlp-main.254,petrov-etal-2012-universal,0,0.0403554,"probes with the explicit reasoning that linear models are simpler than non-linear ones (Alain and Bengio, 2017; Hewitt and Manning, 2019; Hall Maudslay et al., 2020). 2.3 Reducing a Probe’s Complexity In § 2.2, we argued that solely optimizing for accuracy does not lead to a reasonable probing framework. Less commonly discussed, however, is that we also cannot directly optimize for simplicity. Let us consider the POSL probing task and the case where we are using a linear model as our probabilistic probe: p(t |h) = softmax (Wh) (1) where t ∈ T is the target, e.g. a universal partof-speech tag (Petrov et al., 2012), h ∈ Rd is a contextual embedding and W ∈ R|T |×d is a linear projection matrix. A natural measure of probe complexity in this framework is the rank of the projection matrix: rank(W). Indeed, this complexity metric was considered in one of the experiments in Hewitt and Manning (2019) to show that BERT representations strictly dominate ELMo representations for all ranks in their analyzed task. That experiment, though, left out some important baselines—the simplest of which is the encoding of words as one-hot representations. We take inspiration from those experiments and expand upon them (but"
2020.emnlp-main.254,2020.acl-main.420,1,0.835368,"0 Conference on Empirical Methods in Natural Language Processing, pages 3138–3153, c November 16–20, 2020. 2020 Association for Computational Linguistics saliently, Hewitt and Liang (2019) attempts to operationalize complexity in terms of control tasks, which constrain a probe’s capacity for memorization.1 Voita and Titov (2020) follow in this vein with an information-theoretic estimate of complexity: a model’s minimum description length. In opposition to the complexity of a probe is its accuracy, i.e., its ability to perform the target probing task. From an information-theoretic perspective, Pimentel et al. (2020) argues for the use of more complex probes, since they better estimate the amount of mutual information between a representation and the target linguistic property. From a different perspective, Saphra and Lopez (2019) also criticize the indiscriminate use of simple probes, because most neural representations are not estimated with the explicit aim of making information linearly separable; thus, it is unlikely that they will naturally do so, and foolish, perhaps, to expect them to. This paper proposes to directly acknowledge the existence of a trade-off between the two when considering the dev"
2020.emnlp-main.254,P19-1356,0,0.0478585,"oth accuracy and complexity respectively below. 2.1 The Nature of Probing Tasks Most probing tasks are relatively “toy” in nature (Hupkes et al., 2018).2 For instance, two of the most common probing tasks are part-of-speech labeling (POSL; Hewitt and Liang, 2019; Belinkov et al., 2017) and dependency arc labeling (DAL; Tenney et al., 2019a,b; Voita and Titov, 2020). Both tasks are treated as multi-way classification problems. POSL requires a model to assign a part-of-speech tag to a word in context without 2 Not all though, several people have looked into e.g. parse tree reconstruction tasks (Jawahar et al., 2019; Hewitt and Manning, 2019; Vilares et al., 2020) 3139 modeling the entire sequence of part-of-speech tags. Likewise, DAL requires a model to assign a dependency-arc label to an arc independently of the larger dependency tree. These word-oriented probing approaches force models to rely on information about context indirectly encoded in the feature vectors generated by the probed model. Importantly, both are simplified versions of their structured prediction cousins—part-of-speech tagging and dependency parsing—which require the modeling of entire sentences. Accuracy on POSL and DAL is then con"
2020.emnlp-main.254,W17-7623,0,0.207902,"our experimental findings on the previously discussed part-of-speech labeling (POSL) and dependency arc labeling (DAL) probing tasks using both our parametric complexity metrics (§ 4) and the non-parametric ones (§ 5). For both tasks, we use data from Universal Dependencies Treebanks version 2.5 (Zeman et al., 2019) and we probe a set of 5 typologically diverse languages: Basque (Aranzabe et al. 2015; BDT licensed under CC BY-NC-SA 3.0), English (Bies et al. 2012; Silveira et al. 2014; EWT licensed under CC BY-SA 4.0), Finnish (Haverinen et al. 2014, TDT licensed under CC BY-SA 4.0), Marathi (Ravishankar 2017, UFAL licensed under CC BY-SA 4.0) and Turkish (Sulubacak et al. 2016, IMST licensed under CC BY-NC-SA 3.0). When investigating POSL, we take the target space T to be the set of universal part-of-speech tags for a specific language. We then train a classifier to predict these POS tags from word representations obtained from the 6 Another non-parametric method, online coding MDL (Voita and Titov, 2020) can likewise be compared across arbitrary model classes, because its complexity metric is based on probabilities produced and not probe parameters. analyzed model (e.g., BERT). Similarly, for DA"
2020.emnlp-main.328,W09-0106,0,0.0215859,"rue distribution p(m |w), so we will need to approximate this entropy. This is discussed in §5.1. A unique feature of this operationalisation of lexical ambiguity is that it is language independent.5 However, the quality of a possible approximation will vary from language to language, depending on the models and the data available in that language. A final note is that mutual information between M and W as a function of w is equivalent, up to an additive constant, to the conditional entropy I(M ; W = w) = H(M ) − H(M |W = w) (4) 5 We acknowledge the abuse of this bigram in the NLP literature (Bender, 2009), and use it in the following specific sense: the operationalisation may be applied to any language independent of its typological profile. We take this as our operationalisation of contextual uncertainty. We note that this definition is different to typical uses of surprisal in computational psycholinguistics (Hale, 2001; Levy, 2008; Seyfarth, 2014; Piantadosi et al., 2011; Pimentel et al., 2020). Most work in this vein attempts to maintain cognitive plausibility, usually calculating surprisal based on only the unidirectional left piece of the context, as − log p(w |c← ). Although surprisal i"
2020.emnlp-main.328,J06-1003,0,0.29319,"Missing"
2020.emnlp-main.328,J82-3004,0,0.213849,"ion, and find significant negative correlations on five of them. We then extend our evaluation, using our BERT-based measure, to cover a much more representative set of 18 typologically diverse languages: Afrikaans, Arabic, Bengali, English, Estonian, Finnish, Hebrew, Indonesian, Icelandic, Kannada, Malayalam, Marathi, Persian, Portuguese, Tagalog, Turkish, Tatar, and Yoruba.2 In this set, we find significant negative correlations for all languages (see Figure 1). 2 Ambiguity in Language While the pervasiveness of ambiguity in language encumbers the algorithmic processing of natural language (Church and Patil, 1982; Manning and Sch¨utze, 1999), people seamlessly overcome ambiguity through both linguistic and non-linguistic means. World knowledge, pragmatic inferences, and expectations about discourse coherence all contribute to rapidly decoding the intended message out of potentially ambiguous signals (Wasow, 2015). While sometimes ambiguity might indeed result in an observed processing burden (Frazier, 1985), which could lead communication astray, individuals can in response retrace and reanalyse their inferences (as it has been famously shown in garden-path sentences like “The horse raced past the bar"
2020.emnlp-main.328,N19-1423,0,0.0606477,"ly be modestly modified (e.g. by choosing clipped forms when available; Mahowald et al., 2013). However, contexts can be enriched or demoted dynamically, so as to complement a word with the evidence needed for disambiguation. To investigate whether it is the case that the contexts in which a word appears are systematically adapted to enable disambiguation, we first provide an operationalisation of lexical ambiguity, grounded in information theory. We then provide two methods for estimating it, one using WordNet (Miller, 1995), and the other using multilingual BERT’s contextualised embeddings (Devlin et al., 2019), which allows us to explore a large set of languages. We validate our lexical ambiguity measurements by comparing one to the other in six highresource languages from four language families (Afro-Asiatic: Arabic; Austronesian: Indonesian; Indo-European: English, Persian and Portuguese; Uralic: Finnish), and find significant correlations between the number of synsets in WordNet and our BERT estimate (e.g. ρ = 0.40 in English), indicating that our annotation-free method for measuring lexical ambiguity is useful. We then test our main hypothesis—that the con1 We refer to overdetermination with re"
2020.emnlp-main.328,E17-1065,0,0.0162073,"e unidirectional left piece of the context, as − log p(w |c← ). Although surprisal is the operationalisation we are interested in here, we note that a word may have low surprisal if it is frequent across many contexts and not just in a specific one under consideration. Sticking with our notion of half-pointwiseness, we define contextual informativeness as I(W = w; C) = (7) H(W = w) − H(W = w |C) where we define a word’s pointwise entropy (also known as surprisal) as H(W = w) = − log2 p(w) (8) The mutual information between a word and its context was studied before by Bicknell and Levy (2011), Futrell and Levy (2017) and Futrell et al. (2020)—although only using the unidirectional left piece of the context. Eq. (7) again asserts something trivial: low contextual uncertainty implies in an informative context. This informativeness itself is upper-bounded by the word’s absolute negative log-probabiliy (i.e. the unigram surprisal). 4007 4 Hypothesis: Why Should Ambiguity Correlate with Uncertainty? 5.1 As discussed in §1, we expect the linguistic signal to be on average somewhat overdetermined or redundant—such redundancy leads to robustness in noisy situations, when part of the signal may be lost during its"
2020.emnlp-main.328,N01-1021,0,0.398319,"ilable in that language. A final note is that mutual information between M and W as a function of w is equivalent, up to an additive constant, to the conditional entropy I(M ; W = w) = H(M ) − H(M |W = w) (4) 5 We acknowledge the abuse of this bigram in the NLP literature (Bender, 2009), and use it in the following specific sense: the operationalisation may be applied to any language independent of its typological profile. We take this as our operationalisation of contextual uncertainty. We note that this definition is different to typical uses of surprisal in computational psycholinguistics (Hale, 2001; Levy, 2008; Seyfarth, 2014; Piantadosi et al., 2011; Pimentel et al., 2020). Most work in this vein attempts to maintain cognitive plausibility, usually calculating surprisal based on only the unidirectional left piece of the context, as − log p(w |c← ). Although surprisal is the operationalisation we are interested in here, we note that a word may have low surprisal if it is frequent across many contexts and not just in a specific one under consideration. Sticking with our notion of half-pointwiseness, we define contextual informativeness as I(W = w; C) = (7) H(W = w) − H(W = w |C) where we"
2020.emnlp-main.328,E17-1009,0,0.0543798,"Missing"
2020.emnlp-main.328,2020.tacl-1.1,1,0.555066,"between M and W as a function of w is equivalent, up to an additive constant, to the conditional entropy I(M ; W = w) = H(M ) − H(M |W = w) (4) 5 We acknowledge the abuse of this bigram in the NLP literature (Bender, 2009), and use it in the following specific sense: the operationalisation may be applied to any language independent of its typological profile. We take this as our operationalisation of contextual uncertainty. We note that this definition is different to typical uses of surprisal in computational psycholinguistics (Hale, 2001; Levy, 2008; Seyfarth, 2014; Piantadosi et al., 2011; Pimentel et al., 2020). Most work in this vein attempts to maintain cognitive plausibility, usually calculating surprisal based on only the unidirectional left piece of the context, as − log p(w |c← ). Although surprisal is the operationalisation we are interested in here, we note that a word may have low surprisal if it is frequent across many contexts and not just in a specific one under consideration. Sticking with our notion of half-pointwiseness, we define contextual informativeness as I(W = w; C) = (7) H(W = w) − H(W = w |C) where we define a word’s pointwise entropy (also known as surprisal) as H(W = w) = −"
2020.emnlp-main.329,Q17-1010,0,0.0270169,"such as “. . . . . . .” annotated as a sequence of adjectives. Finally, MUSE fastText embeddings are only released as word–embedding dictionaries, unlike standard fastText embeddings which are built from substrings of characters. Thus, unlike conventional fastText embeddings, they are unable to infer embeddings for unseen words. And so, we 10,000 1,000 11,000 2,695 806 2,786 Split by Type # Phrases # Adj Types Training Testing Total Multi-lingual Word Embeddings In order to predict adjective order across languages, we need a joint model for word representations. We use multi-lingual fastText (Bojanowski et al., 2017) Wikipedia supervized word embeddings of dimensionality d = 300 aligned in a single vector space (MUSE), provided by Conneau et al. (2018). 4 Split by Token # Phrases # Adj Types 9,165 1,835 11,000 2,514 890 2,786 Table 1: English dataset summary. need to disqualify all noun phrases which include adjectives not in these dictionaries. We then randomly select 12,000 phrases. Of these, 1,000 are set aside as a development set. The remaining 11,000 phrases are split in two different ways: by token and by type. Splitting by token is done by randomly picking 10,000 phrases to form the training set a"
2020.emnlp-main.329,W11-2707,0,0.0261417,"yle”. Subjectivity. The subjectivity theory (Hill, 2012; Scontras et al., 2017; Hahn et al., 2018) ranks adjectives by subjectivity on a continuous scale and posits that the less subjective an adjective is, the closer it should be placed to the noun. 2.2 Binomial Ordering A closely related phenomenon to adjective ordering is binomial ordering. Binomials are pairs of words joined by a conjunction, such as “salt and pepper” or “ball and chain”. Adjective ordering and binomial ordering have been studied in similar ways, and have in many cases been found to behave similarly (Benor and Levy, 2006; Copestake and Herbelot, 2011; Ivanova and Levy, 2018). 3 A Latent-Variable Model A natural mathematical formalization of adjective ordering is as a latent-variable model. A latentvariable model relates a set of observable variables to a set of unobservable (latent) ones. Here, we observe how adjectives are ordered in corpus data and from this infer an ordered set of latent adjective classes. This allows us to determine the ordering of an arbitrary set of adjectives by referencing their class memberships and the class order. Like other latent-variable models, such as latent semantic analysis (Dumais et al., 1988) and late"
2020.emnlp-main.329,P18-1128,0,0.0201377,"d = 300 4020 Token split Type split EL EF Random 0.843 0.836 0.823 0.829 0.483 0.482 Scrambled Unscrambled EL EF Random 0.791 0.784 0.797 0.797 0.483 0.483 Table 4: English accuracy on different data splits. Comparing the two models on the same data split, the results do not differ significantly. Table 5: English accuracy with scrambled and unscrambled fastText vectors. Comparing different vectors for the same model, the results do not differ significantly. for all models. We report the exact expectation of the random baseline. All significance testing is done with permutation tests following Dror et al. (2018), using 10,000 random permutations and significance at α = 0.05. All differences between model performance and the corresponding random baselines are significant with p < 0.01. sentences where strings of consecutive adjectives have been randomly scrambled. We then retrain the EL and EF models on the token split data with both the scrambled and unscrambled fastText vectors. Results are detailed in Tab. 5. That neither pair of scrambled and unscrambled results differs significantly indicates that adjective ordering information is not coming from the fastText embeddings. Otherwise, the unscramble"
2020.emnlp-main.329,N10-1085,0,0.0223514,"the most popular being hierarchical tendencies based on semantic categories of adjectives (Dixon, 1982; Sproat and Shih, 1991; Cinque, 1994, 2010). For instance, Sproat and Shih (1991) and Cinque (2010) note that adjectives describing SIZE tend to be placed further from the noun than those describing COLOR in most languages. However, most of these studies have relied primarily on the judgment of native speakers rather than on corpus data, and those corpus-based models that do exist have focused exclusively on English (Shaw and Hatzivassiloglou, 1999; Malouf, 2000; Wulff, 2003; Mitchell, 2009; Dunlop et al., 2010; Mitchell et al., 2011; Hill, 2012; Scontras et al., 2017; Hahn et al., 2018; Futrell et al., 2020). In this paper, we make use of tools and techniques from statistical modeling to provide strong converging evidence supporting a hierarchical theory of cross-linguistic adjective ordering. Specifically, we present a novel interpretable, multi-lingual, latent-variable model of adjective ordering that directly enforces a hierarchy of semantic classes and is trained entirely using corpus data. We empirically show that our model accurately orders adjectives across 24 different languages, even when"
2020.emnlp-main.329,2020.acl-main.181,0,0.248119,"1982; Sproat and Shih, 1991; Cinque, 1994, 2010). For instance, Sproat and Shih (1991) and Cinque (2010) note that adjectives describing SIZE tend to be placed further from the noun than those describing COLOR in most languages. However, most of these studies have relied primarily on the judgment of native speakers rather than on corpus data, and those corpus-based models that do exist have focused exclusively on English (Shaw and Hatzivassiloglou, 1999; Malouf, 2000; Wulff, 2003; Mitchell, 2009; Dunlop et al., 2010; Mitchell et al., 2011; Hill, 2012; Scontras et al., 2017; Hahn et al., 2018; Futrell et al., 2020). In this paper, we make use of tools and techniques from statistical modeling to provide strong converging evidence supporting a hierarchical theory of cross-linguistic adjective ordering. Specifically, we present a novel interpretable, multi-lingual, latent-variable model of adjective ordering that directly enforces a hierarchy of semantic classes and is trained entirely using corpus data. We empirically show that our model accurately orders adjectives across 24 different languages, even when tested on languages that it has not been trained on. In doing so, we demonstrate the existence of un"
2020.emnlp-main.329,N12-2003,0,0.105326,"es based on semantic categories of adjectives (Dixon, 1982; Sproat and Shih, 1991; Cinque, 1994, 2010). For instance, Sproat and Shih (1991) and Cinque (2010) note that adjectives describing SIZE tend to be placed further from the noun than those describing COLOR in most languages. However, most of these studies have relied primarily on the judgment of native speakers rather than on corpus data, and those corpus-based models that do exist have focused exclusively on English (Shaw and Hatzivassiloglou, 1999; Malouf, 2000; Wulff, 2003; Mitchell, 2009; Dunlop et al., 2010; Mitchell et al., 2011; Hill, 2012; Scontras et al., 2017; Hahn et al., 2018; Futrell et al., 2020). In this paper, we make use of tools and techniques from statistical modeling to provide strong converging evidence supporting a hierarchical theory of cross-linguistic adjective ordering. Specifically, we present a novel interpretable, multi-lingual, latent-variable model of adjective ordering that directly enforces a hierarchy of semantic classes and is trained entirely using corpus data. We empirically show that our model accurately orders adjectives across 24 different languages, even when tested on languages that it has not"
2020.emnlp-main.329,P00-1012,0,0.870044,"ve ordering in the linguistics literature, the most popular being hierarchical tendencies based on semantic categories of adjectives (Dixon, 1982; Sproat and Shih, 1991; Cinque, 1994, 2010). For instance, Sproat and Shih (1991) and Cinque (2010) note that adjectives describing SIZE tend to be placed further from the noun than those describing COLOR in most languages. However, most of these studies have relied primarily on the judgment of native speakers rather than on corpus data, and those corpus-based models that do exist have focused exclusively on English (Shaw and Hatzivassiloglou, 1999; Malouf, 2000; Wulff, 2003; Mitchell, 2009; Dunlop et al., 2010; Mitchell et al., 2011; Hill, 2012; Scontras et al., 2017; Hahn et al., 2018; Futrell et al., 2020). In this paper, we make use of tools and techniques from statistical modeling to provide strong converging evidence supporting a hierarchical theory of cross-linguistic adjective ordering. Specifically, we present a novel interpretable, multi-lingual, latent-variable model of adjective ordering that directly enforces a hierarchy of semantic classes and is trained entirely using corpus data. We empirically show that our model accurately orders ad"
2020.emnlp-main.329,W09-0608,0,0.0349336,"ics literature, the most popular being hierarchical tendencies based on semantic categories of adjectives (Dixon, 1982; Sproat and Shih, 1991; Cinque, 1994, 2010). For instance, Sproat and Shih (1991) and Cinque (2010) note that adjectives describing SIZE tend to be placed further from the noun than those describing COLOR in most languages. However, most of these studies have relied primarily on the judgment of native speakers rather than on corpus data, and those corpus-based models that do exist have focused exclusively on English (Shaw and Hatzivassiloglou, 1999; Malouf, 2000; Wulff, 2003; Mitchell, 2009; Dunlop et al., 2010; Mitchell et al., 2011; Hill, 2012; Scontras et al., 2017; Hahn et al., 2018; Futrell et al., 2020). In this paper, we make use of tools and techniques from statistical modeling to provide strong converging evidence supporting a hierarchical theory of cross-linguistic adjective ordering. Specifically, we present a novel interpretable, multi-lingual, latent-variable model of adjective ordering that directly enforces a hierarchy of semantic classes and is trained entirely using corpus data. We empirically show that our model accurately orders adjectives across 24 different"
2020.emnlp-main.329,P11-2041,0,0.0334463,"g hierarchical tendencies based on semantic categories of adjectives (Dixon, 1982; Sproat and Shih, 1991; Cinque, 1994, 2010). For instance, Sproat and Shih (1991) and Cinque (2010) note that adjectives describing SIZE tend to be placed further from the noun than those describing COLOR in most languages. However, most of these studies have relied primarily on the judgment of native speakers rather than on corpus data, and those corpus-based models that do exist have focused exclusively on English (Shaw and Hatzivassiloglou, 1999; Malouf, 2000; Wulff, 2003; Mitchell, 2009; Dunlop et al., 2010; Mitchell et al., 2011; Hill, 2012; Scontras et al., 2017; Hahn et al., 2018; Futrell et al., 2020). In this paper, we make use of tools and techniques from statistical modeling to provide strong converging evidence supporting a hierarchical theory of cross-linguistic adjective ordering. Specifically, we present a novel interpretable, multi-lingual, latent-variable model of adjective ordering that directly enforces a hierarchy of semantic classes and is trained entirely using corpus data. We empirically show that our model accurately orders adjectives across 24 different languages, even when tested on languages tha"
2020.emnlp-main.329,L16-1262,0,0.0899939,"Missing"
2020.emnlp-main.329,P99-1018,0,0.298471,"counts of crosslinguistic adjective ordering in the linguistics literature, the most popular being hierarchical tendencies based on semantic categories of adjectives (Dixon, 1982; Sproat and Shih, 1991; Cinque, 1994, 2010). For instance, Sproat and Shih (1991) and Cinque (2010) note that adjectives describing SIZE tend to be placed further from the noun than those describing COLOR in most languages. However, most of these studies have relied primarily on the judgment of native speakers rather than on corpus data, and those corpus-based models that do exist have focused exclusively on English (Shaw and Hatzivassiloglou, 1999; Malouf, 2000; Wulff, 2003; Mitchell, 2009; Dunlop et al., 2010; Mitchell et al., 2011; Hill, 2012; Scontras et al., 2017; Hahn et al., 2018; Futrell et al., 2020). In this paper, we make use of tools and techniques from statistical modeling to provide strong converging evidence supporting a hierarchical theory of cross-linguistic adjective ordering. Specifically, we present a novel interpretable, multi-lingual, latent-variable model of adjective ordering that directly enforces a hierarchy of semantic classes and is trained entirely using corpus data. We empirically show that our model accura"
2020.emnlp-main.329,D13-1015,0,0.0129349,"osite order as do prenominal ones. To illustrate, consider the following phrase in both English and Spanish: (5) An ugly black shirt (6) Una camisa negra fea a shirt black ugly Inherentness. The inherentness theory (Whorf, 1945) posits that adjectives fall into two broad categories: adjectives that describe inherent properties of nouns—such as color, material, physical state, provenance, breed, nationality, function, use, etc.— and adjectives that describe non-inherent properties, and that inherent adjectives are usually placed closer to the noun than non-inherent ones. Modification strength. Vecchi et al. (2013) apply a compositional distributional semantics approach to studying English adjective–adjective– noun phrases, and note that in correctly ordered phrases, the adjective closer to the noun contributes more to the meaning of the phrase than does the adjective further from the noun. For instance, “different architectural style” is more similar to “architectural style” than it is to “different style”. Subjectivity. The subjectivity theory (Hill, 2012; Scontras et al., 2017; Hahn et al., 2018) ranks adjectives by subjectivity on a continuous scale and posits that the less subjective an adjective i"
2020.emnlp-main.390,D07-1015,0,0.804058,"w G to be a multigraph, i.e., we allow multiple edges between pairs of nodes. Multi-graphs are a natural encoding of labeled dependency relations where possible labels between words are captured by multiple edges be5 We note exact match metrics, which consider the entire arborescence, do penalize root constraint violations 6 There is one exception: Corro et al. (2016) mention Gabow and Tarjan (1984)’s algorithm in a footnote. 7 Much like this paper, efficient root-constrained marginal inference is also possible without picking up an extra factor of n, but it requires some attention to detail (Koo et al., 2007; Zmigrod et al., 2020). 8 When there is no ambiguity, we may abuse notation using G to refer to either its node or edge set, e.g., we may write (i − A j) ∈ G to mean (i − A j) ∈ E, and i ∈ G to mean i ∈ V . (C2) Each non-root node has exactly one incoming edge (thus, |E 0 |= |V |−1); A has no cycles. A dependency tree of G is an arborescence that additionally satisfies (C3) |{(ρ − A ) ∈ E 0 } |= 1 In words, (C3) says A contains exactly one out-edge from ρ. Let A(G) and A† (G) denote the sets of arborescences and dependency trees, respectively. The weight of a graph or subgraph is defined as X"
2020.emnlp-main.456,S07-1012,0,0.0655164,"of nouns clustered by their gender, with the same nouns clustered by the adjectives that modify them or the verbs that take them as arguments. Although we adopt information theoretic measures, here there are two other major classes of cluster evaluation measures: set-matching measures, and pair-counting measures, which tally which pairs of items are in the same or different communities. One popular set-matching measure in information retrieval, purity (Manning et al., 2008), is asymmetric and biased by the size and number of communities (Danon et al., 2005). Its symmetric form, the F-measure (Artiles et al., 2007), has clear bounds but gives no indication of average-case performance. The adjusted Rand index (ARI; Hubert and Arabie, 1985) is the preeminent pair-counting measure. It is related to AMI, adjusting the Rand index in the same way that AMI adjusts MI. ARI also computes an expectation, which can be computed over the proper distribution (Gates and Ahn, 2017), but it Dataset Measure Score St. Dev. Swadesh MI VI AMI Random MI VI AMI Random 344 312 344 1184 1231 1164 1548 2531 133.4 209.6 NorthEuraLex Table 1: Distances of generated trees from gold tree. is empirically better suited to large, balan"
2020.emnlp-main.456,N19-1415,1,0.845587,"in our languages, with measurable success. Separate Indo-European branches are no more similar than chance. We emphasize that our methods are not specifically tailored to gender systems. One could apply them more broadly other aspects of the lexicon, e.g. to Indo-European verb classes, Bantu noun classes, or diachronic time slices of a single language’s gender system, data permitting. A related challenge is East and Southeast Asian numeral classifier systems, which associate nouns with classifiers based largely on the semantic properties of the nouns (Kuo and Sera, 2009; Zhan and Levy, 2018; Liu et al., 2019). They display more idiolectal variation, and often more than one classifier can accompany a given noun (Hu, 1993), unlike for gender (where this is rare). We note that we could further extend our measures to fuzzy partitions, which remain less explored in community detection, but are a promising avenue for future work. 5671 Acknowledgments We thank Tongfei Chen for comments on the Slavic languages, Jean-Gabriel Young for suggesting that we consider Variation of Information, and Johannes Bjerva for providing us with code to compute the tree distance. We also thank Tiago Pimentel for his help w"
2020.emnlp-main.456,D13-1032,0,0.0377147,"Missing"
2020.emnlp-main.456,D09-1142,0,0.0348282,"r Grammatical gender is a highly fixed classification system for nouns. Native speakers rarely make errors in gender recall, which might tentatively argue against tremendous arbitrary variation (Corbett, 1991). Some regularity can surely be found in the associations between gender and various features of the noun, such as orthographic or phonological form, or semantics. With respect to form-based regularities, Cucerzan and Yarowsky (2003a) devise a system for inferring noun gender (masculine or feminine) from contextual clues and character representations, even in inflected forms of the noun. Nastase and Popescu (2009) also find that phonological form can lead to predictability of gender in two three-gender systems. With respect to word semantics, (Williams et al., 2019) quantify the relationship between the gender on inanimate nouns and their distributional word vectors. We can’t rely on form. Using phonological or orthographic form to derive gender is fraught with complications: particular to our study, epicene nouns (i.e., words that can appear in multiple genders) can pose issues. In German, only gender concord on the definite article and adjectives can disambiguate the gender of some nouns; the same wo"
2020.emnlp-main.456,P17-1049,0,0.0515973,"ections surface between Romanian and both Slovene and Ukrainian, but the majority of the Balto-Slavic languages are quite distant from it. 6.2 Phylogeny Inspired by the findings in the previous section (especially the high similarity among Romance languages), we further validate our measure, asking whether the resulting similarities reflect known phylogenetic ground truth—namely, the developmental history of Indo-European languages. Obviously, there are many more facets to languages’ relatedness than their gender systems, so it is interesting to find signal this strong from a single category. Rabinovich et al. (2017) cluster languages based on simple features of their translations into a common 4 This claim can be debated (Bateman and Polinsky, 2010): The neuter gender manifests as masculine when singular and feminine when plural (Corbett, 1991). target language to craft phylogenetic trees. We take a similar approach, asking whether the pairwise similarities of gender systems are enough to reveal phylogenetic truth or some other relationship. We create phylogenetic trees through agglomerative hierarchical clustering, using both VI and one minus the AMI as distance measures. We use the weighted pair group"
2020.emnlp-main.456,N18-1181,0,0.0126966,"logenic relationships in our languages, with measurable success. Separate Indo-European branches are no more similar than chance. We emphasize that our methods are not specifically tailored to gender systems. One could apply them more broadly other aspects of the lexicon, e.g. to Indo-European verb classes, Bantu noun classes, or diachronic time slices of a single language’s gender system, data permitting. A related challenge is East and Southeast Asian numeral classifier systems, which associate nouns with classifiers based largely on the semantic properties of the nouns (Kuo and Sera, 2009; Zhan and Levy, 2018; Liu et al., 2019). They display more idiolectal variation, and often more than one classifier can accompany a given noun (Hu, 1993), unlike for gender (where this is rare). We note that we could further extend our measures to fuzzy partitions, which remain less explored in community detection, but are a promising avenue for future work. 5671 Acknowledgments We thank Tongfei Chen for comments on the Slavic languages, Jean-Gabriel Young for suggesting that we consider Variation of Information, and Johannes Bjerva for providing us with code to compute the tree distance. We also thank Tiago Pime"
2020.emnlp-main.456,D19-1577,1,0.741496,"Missing"
2020.emnlp-main.456,2020.acl-main.597,1,0.832162,"i.a.) and on phonology (Bidot, 1925; Tucker et al., 1977; Newman, 1979; Hayward and Corbett, 1988; Marchese, 1988). Intensional approaches, particularly those with typological leanings, contribute very fine grained research on particular pairwise similarities for particular languages and dialects. Although we cannot survey these in detail here, we would love for our measures to contribute findings that can complement these approaches. Relatedly, other recent works have investigated grammatical gender and other types of noun classification systems with information theoretic tools. For example, Williams et al. 2020b uses mutual information to quantify the strength of the relationships between declension class, grammatical gender, distributional semantics, and orthographic form respectively in several languages. Williams et al. 2020a, which is arguably closest to this work, measures the strength of semantic relationships between inanimate nouns and verbs or adjectives that takes those nouns as arguments, and that work can be seen as comparing the similarity of nouns clustered by their gender, with the same nouns clustered by the adjectives that modify them or the verbs that take them as arguments. Althou"
2020.figlang-1.30,D17-1169,0,0.0327174,"aining Details We train in batches of 32 sentences, and employ early stopping after 20 stable steps (based on F1 on dev). As an optimizer, we use AdamW (Loshchilov and Hutter, 2017). We experimented with three fine-tuning options: (1) unfreezing the whole network and training it all at once, (2) freezing BERT and training until early stopping activates, then unfreezing BERT and training until early stopping again, and (3) freezing BERT and training until early stopping, then sequentially unfreezing and training a single layer of BERT at a time, and finally the whole model at once (inspired by Felbo et al., 2017). We used option (2) in the end, since it offered a large improvement over (1) when we used a lower learning rate for the second phase. We found that (3) offered no additional advantage. To find hyperparameters, we performed a random search over the parameter space; final hyperparameters are reported in Table 2. Threshold Shifting The ratio of metaphors to non-metaphors in the entire VUA dataset was not the same as that of the verb and all-pos subsets used by the Shared Task. Having trained the model on all the data, we then adjust it to each different distribution. To do this, we find the thr"
2020.figlang-1.30,W15-1402,0,0.399931,"the extent to which a word denotes something that can be experienced by the senses, and is gener221 Proceedings of the Second Workshop on Figurative Language Processing, pages 221–226 c July 9, 2020. 2020 Association for Computational Linguistics https://doi.org/10.18653/v1/P17 ally measured by asking annotators to rate words on a numeric scale (Paivio et al., 1968; Spreen and Schulz, 1966); abstractness is then the inverse of concreteness. Using concreteness ratings for metaphor identification is clearly well motivated, as evidenced by previous work (e.g. Tsvetkov et al., 2014, 2013; Beigman Klebanov et al., 2015). For a word to be metaphorical in a particular context, then, it needs to have a concrete sense and an abstract sense, with the abstract sense activated in that context. The concrete sense would belong to the source domain, and the abstract sense to the target domain. For instance, the meaning of the word attacked in “she attacked the soldier” is concrete, but in “she attacked the problem” it is abstract—and thus that usage is metaphorical. Polysemy of the word is a necessary condition; the existence of an abstract sense is not enough, otherwise a monosemously abstract word such as considered"
2020.figlang-1.30,N19-1423,0,0.0881981,"embeddings, to provide information about the source domain. Since these static type-level embeddings will clearly contain information about both source and target, we compliment them with type-level concreteness ratings. Such ratings should reflect the concreteness of the most concrete sense of the word, thus allowing the network to differentiate between the left and right columns of Figure 1. Figure 2 shows an overview of our architecture. In the following paragraphs, we detail each individual component of the model. Contextual Word Embedding For contextualised embeddings, we fine-tune BERT (Devlin et al., 2019). BERT is a sentence encoder which utilises a transformer architecture (Vaswani et al., 2017), and is trained with two separate tasks— masked language modelling (a cloze task), and next-sentence prediction. The latent space (the final hidden state of the encoder) contains vector representations of each input token, which change in different contexts. Several pre-trained BERT models are available—we use BERT large.2 Model Architecture We now describe a model which uses semantic representations of a word in and out of context to predict metaphoricity. Ideally, we would only provide the model wit"
2020.figlang-1.30,D18-1060,0,0.135355,"s a device which allows one to project structure from a source domain to a target domain (Lakoff and Johnson, 1980). For instance, in the sentence “he attacked the government”, attacked can be seen as a conventional metaphor, which applies structure from the source domain of war to the target domain of argument. Intuitively, it seems that the context in which a word appears tells us about the target domain, whilst the word itself (and some knowledge about how it is used nonmetaphorically) tells us about the source. Several existing models have exploited this difference (e.g. Mao et al., 2019; Gao et al., 2018). Usually, the target domain is something intangible, whilst the source domain relates more closely to our real-world experience. Concreteness refers to the extent to which a word denotes something that can be experienced by the senses, and is gener221 Proceedings of the Second Workshop on Figurative Language Processing, pages 221–226 c July 9, 2020. 2020 Association for Computational Linguistics https://doi.org/10.18653/v1/P17 ally measured by asking annotators to rate words on a numeric scale (Paivio et al., 1968; Spreen and Schulz, 1966); abstractness is then the inverse of concreteness. Us"
2020.figlang-1.30,2020.figlang-1.3,0,0.446354,"ate whether or not it is metaphorical. Some metaphors occur so frequently as to be considered word senses in their own right (so-called conventional metaphors), whilst others are creative, and involve the use of words in unexpected ways (novel metaphors). Sometimes whole phrases or even sentences can lend themselves to metaphorical or literal interpretations.1 For these reasons and others, human annotators might disagree about what constitutes a metaphor—computational metaphor detection is no doubt a challenging problem. In this work, we participate in the 2020 Metaphor Detection Shared Task (Leong et al., 2020). First, we offer a description of metaphoricity, framing it in terms of the concreteness of a word in different contexts. Concreteness of a word in context is not a quantity for which there exists large-scale annotated data. In lieu of this, we train a metaphor detection model using input features which we expect to 1 Consider drowning student, which could refer to students submerged in water, or students struggling with coursework (Tsvetkov et al., 2014), or the more idiomatic phrase, they stabbed him in the back, which could be taken literally or (more likely) metaphorically, depending on i"
2020.figlang-1.30,W18-0907,0,0.566273,"quantity for which there exists large-scale annotated data. In lieu of this, we train a metaphor detection model using input features which we expect to 1 Consider drowning student, which could refer to students submerged in water, or students struggling with coursework (Tsvetkov et al., 2014), or the more idiomatic phrase, they stabbed him in the back, which could be taken literally or (more likely) metaphorically, depending on its context. contain the information needed to derive this contextual concreteness. This model outperforms the highest performing system of the previous shared task (Leong et al., 2018), and finishes 4th in the two subtasks in which we participate. 2 Concreteness and Context Metaphor is a device which allows one to project structure from a source domain to a target domain (Lakoff and Johnson, 1980). For instance, in the sentence “he attacked the government”, attacked can be seen as a conventional metaphor, which applies structure from the source domain of war to the target domain of argument. Intuitively, it seems that the context in which a word appears tells us about the target domain, whilst the word itself (and some knowledge about how it is used nonmetaphorically) tells"
2020.figlang-1.30,P19-1378,0,0.504792,"Context Metaphor is a device which allows one to project structure from a source domain to a target domain (Lakoff and Johnson, 1980). For instance, in the sentence “he attacked the government”, attacked can be seen as a conventional metaphor, which applies structure from the source domain of war to the target domain of argument. Intuitively, it seems that the context in which a word appears tells us about the target domain, whilst the word itself (and some knowledge about how it is used nonmetaphorically) tells us about the source. Several existing models have exploited this difference (e.g. Mao et al., 2019; Gao et al., 2018). Usually, the target domain is something intangible, whilst the source domain relates more closely to our real-world experience. Concreteness refers to the extent to which a word denotes something that can be experienced by the senses, and is gener221 Proceedings of the Second Workshop on Figurative Language Processing, pages 221–226 c July 9, 2020. 2020 Association for Computational Linguistics https://doi.org/10.18653/v1/P17 ally measured by asking annotators to rate words on a numeric scale (Paivio et al., 1968; Spreen and Schulz, 1966); abstractness is then the inverse"
2020.figlang-1.30,P14-1024,0,0.421066,"r—computational metaphor detection is no doubt a challenging problem. In this work, we participate in the 2020 Metaphor Detection Shared Task (Leong et al., 2020). First, we offer a description of metaphoricity, framing it in terms of the concreteness of a word in different contexts. Concreteness of a word in context is not a quantity for which there exists large-scale annotated data. In lieu of this, we train a metaphor detection model using input features which we expect to 1 Consider drowning student, which could refer to students submerged in water, or students struggling with coursework (Tsvetkov et al., 2014), or the more idiomatic phrase, they stabbed him in the back, which could be taken literally or (more likely) metaphorically, depending on its context. contain the information needed to derive this contextual concreteness. This model outperforms the highest performing system of the previous shared task (Leong et al., 2018), and finishes 4th in the two subtasks in which we participate. 2 Concreteness and Context Metaphor is a device which allows one to project structure from a source domain to a target domain (Lakoff and Johnson, 1980). For instance, in the sentence “he attacked the government”"
2020.figlang-1.30,W13-0906,0,0.200661,"Missing"
2020.figlang-1.30,D11-1063,0,0.133213,"sentations of a word in and out of context to predict metaphoricity. Ideally, we would only provide the model with a representation of the concreteness of a word in context (since we believe that would do most of the lifting), but to our knowledge, no large-scale annotated datasets exist for contextdependent concreteness. In most popular datasets of concreteness annotation (e.g. Coltheart, 1981; Brysbaert et al., 2014), concreteness is a property assigned to each word type—but we would need the concreteness of a word instance. In this respect, our work resembles the abstractness classifier in Turney et al. (2011)—although this work uses word senses Concreteness Model We define a simple model which represents the concreteness of a word as a linear interpolation between two vectors, representing maximal concreteness and abstractness, vcon and vabs respectively. For each word w we obtain a real number estimate of its concreteness, c, from Brysbaert et al. (2014), where c = 5 indicates maximum abstractness, and c = 0 indicates maximum 222 2 BERT accepts WordPiece units (Wu et al., 2016) as tokens, rather than words. There is not a single accepted way of converting multiple WordPiece unit vector representa"
2020.figlang-1.30,W18-0913,0,0.125319,"Missing"
2020.lrec-1.483,P19-1310,0,0.0958144,"Missing"
2020.lrec-1.483,C12-2009,1,0.843263,"Missing"
2020.lrec-1.483,P19-1156,0,0.050149,"Missing"
2020.lrec-1.483,P16-1156,1,0.881371,"Missing"
2020.lrec-1.483,K17-2001,1,0.904106,"Missing"
2020.lrec-1.483,N07-1048,0,0.0363457,"Missing"
2020.lrec-1.483,P08-1115,0,0.0323664,"Missing"
2020.lrec-1.483,K19-1014,1,0.901227,"Missing"
2020.lrec-1.483,N12-1032,0,0.0791982,"Missing"
2020.lrec-1.483,D19-1328,0,0.0253837,"Missing"
2020.lrec-1.483,S19-1026,1,0.819906,"Missing"
2020.lrec-1.483,L16-1498,1,0.928068,"Missing"
2020.lrec-1.483,L18-1293,1,0.890225,"Missing"
2020.lrec-1.483,W18-6011,1,0.883111,"Missing"
2020.lrec-1.483,W19-4226,1,0.885665,"Missing"
2020.lrec-1.483,D14-1095,0,0.0393603,"Missing"
2020.lrec-1.483,2020.lrec-1.488,1,0.822461,"Missing"
2020.lrec-1.483,L16-1262,0,0.126329,"Missing"
2020.lrec-1.483,Q15-1026,0,0.0701072,"Missing"
2020.lrec-1.483,W18-1813,1,0.887977,"Missing"
2020.lrec-1.483,P15-2111,1,0.855194,"Missing"
2020.lrec-1.483,A94-1008,0,0.317255,"Missing"
2020.lrec-1.483,N01-1026,1,0.569711,"Missing"
2020.sigmorphon-1.1,K18-3001,1,0.899071,"logical reinflection, we specifically focus on typological diversity and aim to investigate systems’ ability to generalize across typologically distinct languages many of which are low-resource. For example, if a neural network architecture works well for a sample of IndoEuropean languages, should the same architecture also work well for Tupi–Guarani languages (where nouns are “declined” for tense) or Austronesian languages (where verbal morphology is frequently prefixing)? 2 Task Description The 2020 iteration of our task is similar to CoNLL-SIGMORPHON 2017 (Cotterell et al., 2017) and 2018 (Cotterell et al., 2018) in that participants are required to design a model that learns to generate inflected forms from a lemma and a set of morphosyntactic features that derive the desired target form. For each language we provide a separate training, development, and test set. More historically, all of these tasks resemble the classic “wug”-test that Berko (1958) developed to test child and human knowledge of English nominal morphology. Unlike the task from earlier years, this year’s task proceeds in three phases: a Development Phase, a Generalization Phase, and an Evaluation Phase, in which each phase introduces"
2020.sigmorphon-1.1,K17-2001,1,0.928876,"e SIGMORPHON shared task on morphological reinflection, we specifically focus on typological diversity and aim to investigate systems’ ability to generalize across typologically distinct languages many of which are low-resource. For example, if a neural network architecture works well for a sample of IndoEuropean languages, should the same architecture also work well for Tupi–Guarani languages (where nouns are “declined” for tense) or Austronesian languages (where verbal morphology is frequently prefixing)? 2 Task Description The 2020 iteration of our task is similar to CoNLL-SIGMORPHON 2017 (Cotterell et al., 2017) and 2018 (Cotterell et al., 2018) in that participants are required to design a model that learns to generate inflected forms from a lemma and a set of morphosyntactic features that derive the desired target form. For each language we provide a separate training, development, and test set. More historically, all of these tasks resemble the classic “wug”-test that Berko (1958) developed to test child and human knowledge of English nominal morphology. Unlike the task from earlier years, this year’s task proceeds in three phases: a Development Phase, a Generalization Phase, and an Evaluation Pha"
2020.sigmorphon-1.1,W09-0106,0,0.0385713,"n variably surface as prefixes, suffixes, infixes, or circumfixes (Dryer, 2013). Most Eurasian and Australian languages strongly favor suffixation, and the same holds true, but to a lesser extent, for South American and New Guinean languages (Dryer, 2013). In Mesoamerican languages and African languages spoken below the Sahara, prefixation is dominant instead. These are just three dimensions of variation in morphology, and the cross-linguistic variation is already considerable. Such cross-lingual variation makes the development of natural language processing (NLP) applications challenging. As Bender (2009, 2016) notes, many current architectures and training and tuning algorithms still present language-specific biases. The most commonly used language for developing NLP applications is English. Along the above dimensions, English is productively concatenative, a mixture of analytic and synthetic, and largely suffixing in its inflectional morphology. With respect to languages that exhibit inflectional morphology, English is relatively impoverished.1 Importantly, English is just one morphological system among many. A larger goal of natural language processing is that the system work for any prese"
2020.sigmorphon-1.1,2020.lrec-1.344,1,0.878264,"Missing"
2020.sigmorphon-1.1,2020.sigmorphon-1.15,0,0.0565092,"Missing"
2020.sigmorphon-1.1,2020.sigmorphon-1.14,0,0.0439232,"Missing"
2020.sigmorphon-1.1,L16-1379,0,0.0190826,"Missing"
2020.sigmorphon-1.1,K17-2010,1,0.837279,"l baselines were based on a neural transducer (Wu and Cotterell, 2019), which is essentially a hard monotonic attention model (mono-*). The second baseline is a transformer (Vaswani et al., 2017) adopted for character-level tasks that currently holds the state-of-the-art on the 2017 SIGMORPHON shared task data (Wu et al., 2020, trm-*). Both models take the lemma and morphological tags as input and output the target inflection. The baseline is further expanded to include the data augmentation technique used by Anastasopoulos and Neubig (2019, -aug-) (conceptually similar to the one proposed by Silfverberg et al. (2017)). Relying on a simple characterlevel alignment between lemma and form, this technique replaces shared substrings of length &gt; 3 with random characters from the language’s alphabet, producing hallucinated lemma–tag–form triples. Both neural baselines were trained in mono- (*-single) and multilingual (shared parameters among the same family, *-shared) settings. 6 Many teams based their models on the transformer architecture. NYU-CUBoulder experimented with a vanilla transformer model (NYU-CUBoulder-04-0), a pointer-generator transformer that allows for a copy mechanism (NYU-CUBoulder-02-0), and"
2020.sigmorphon-1.1,2020.sigmorphon-1.4,0,0.0612223,"Missing"
2020.sigmorphon-1.1,W19-4207,0,0.0125147,"c attention model with improved alignment strategy. This model is further improved (flexica-03-1) by introducing a data hallucination technique which is based on phonotactic modelling of extremely low-resource languages (Shcherbakov et al., 2016). LTI focused on their earlier model (Anastasopoulos and Neubig, 2019), a neural multi-source encoder–decoder with two-step attention architecture, training it with hallucinated data, cross-lingual transfer, and romanization of scripts to improve performance on low-resource languages. DeepSpin reimplemented gated sparse two-headed attention model from Peters and Martins (2019) and trained it on all languages at once (massively multilingual). The team experimented with two modifications of the softmax function: sparsemax (Martins and Astudillo, 2016, deepspin-02-1) and 1.5-entmax (Peters et al., 2019, deepspin-01-1). Neural Neural baselines were based on a neural transducer (Wu and Cotterell, 2019), which is essentially a hard monotonic attention model (mono-*). The second baseline is a transformer (Vaswani et al., 2017) adopted for character-level tasks that currently holds the state-of-the-art on the 2017 SIGMORPHON shared task data (Wu et al., 2020, trm-*). Both"
2020.sigmorphon-1.1,P19-1146,0,0.0351308,"Missing"
2020.sigmorphon-1.1,P19-1148,1,0.838088,"lti-source encoder–decoder with two-step attention architecture, training it with hallucinated data, cross-lingual transfer, and romanization of scripts to improve performance on low-resource languages. DeepSpin reimplemented gated sparse two-headed attention model from Peters and Martins (2019) and trained it on all languages at once (massively multilingual). The team experimented with two modifications of the softmax function: sparsemax (Martins and Astudillo, 2016, deepspin-02-1) and 1.5-entmax (Peters et al., 2019, deepspin-01-1). Neural Neural baselines were based on a neural transducer (Wu and Cotterell, 2019), which is essentially a hard monotonic attention model (mono-*). The second baseline is a transformer (Vaswani et al., 2017) adopted for character-level tasks that currently holds the state-of-the-art on the 2017 SIGMORPHON shared task data (Wu et al., 2020, trm-*). Both models take the lemma and morphological tags as input and output the target inflection. The baseline is further expanded to include the data augmentation technique used by Anastasopoulos and Neubig (2019, -aug-) (conceptually similar to the one proposed by Silfverberg et al. (2017)). Relying on a simple characterlevel alignme"
2020.sigmorphon-1.1,2020.sigmorphon-1.5,0,0.0487999,"Missing"
2020.sigtyp-1.1,Q19-1038,0,0.0255088,"o Edoardo M. Ponti‡ Ekaterina Vylomova] Ryan Cotterell? Isabelle Augenstein/ • Aalborg University / University of Copenhagen  Johns Hopkins University ] University of Melbourne 2 Carnegie Mellon University Leipzig University ? ETH Z¨urich ‡ University of Cambridge jbjerva@cs.aau.dk, augenstein@di.ku.dk Abstract of reasons. The field has seen considerable advances in cross-lingual transfer learning, whereby stable cross-lingual representations can be learned on massive amounts of data in an unsupervised way, be it for words (Ammar et al., 2016; Wada et al., 2019) or, more recently, sentences (Artetxe and Schwenk, 2019; Devlin et al., 2019; Conneau and Lample, 2019; Conneau et al., 2020). This naturally raises the question of what these representations encode, and some have turned to typology for potential answers (Choenni and Shutova, 2020; Zhao et al., 2020). In a similar vein, research has shown that these learned representations can be fine-tuned for supervised tasks, then applied to new languages in a few- or even zero-shot fashion with surprisingly high performance. This has raised the question of what causes such surprisingly high results, and to what degree typological similarities are exploited by"
2020.sigtyp-1.1,N18-1083,1,0.844133,"l., 2019; Conneau and Lample, 2019; Conneau et al., 2020). This naturally raises the question of what these representations encode, and some have turned to typology for potential answers (Choenni and Shutova, 2020; Zhao et al., 2020). In a similar vein, research has shown that these learned representations can be fine-tuned for supervised tasks, then applied to new languages in a few- or even zero-shot fashion with surprisingly high performance. This has raised the question of what causes such surprisingly high results, and to what degree typological similarities are exploited by such models (Bjerva and Augenstein, 2018a; Nooralahzadeh et al., 2020; Zhao et al., 2020). In addition to using typology for diagnostic purposes, prior work has also found that typology can guide cross-lingual sharing (de Lhoneux et al., 2018). Finally, the relationship between typological knowledge bases (KBs) such as WALS (Dryer and Haspelmath, 2013) and language representations has been studied, which has shown that knowledge base population methods can be used to complete typological KBs (Malaviya et al., 2017; Murawaki, 2017; Bjerva and Augenstein, 2018a; Bjerva et al., 2019c), and that implications can be discovered in typolog"
2020.sigtyp-1.1,W18-0207,1,0.747207,"l., 2019; Conneau and Lample, 2019; Conneau et al., 2020). This naturally raises the question of what these representations encode, and some have turned to typology for potential answers (Choenni and Shutova, 2020; Zhao et al., 2020). In a similar vein, research has shown that these learned representations can be fine-tuned for supervised tasks, then applied to new languages in a few- or even zero-shot fashion with surprisingly high performance. This has raised the question of what causes such surprisingly high results, and to what degree typological similarities are exploited by such models (Bjerva and Augenstein, 2018a; Nooralahzadeh et al., 2020; Zhao et al., 2020). In addition to using typology for diagnostic purposes, prior work has also found that typology can guide cross-lingual sharing (de Lhoneux et al., 2018). Finally, the relationship between typological knowledge bases (KBs) such as WALS (Dryer and Haspelmath, 2013) and language representations has been studied, which has shown that knowledge base population methods can be used to complete typological KBs (Malaviya et al., 2017; Murawaki, 2017; Bjerva and Augenstein, 2018a; Bjerva et al., 2019c), and that implications can be discovered in typolog"
2020.sigtyp-1.1,N19-1156,1,0.642604,"al similarities are exploited by such models (Bjerva and Augenstein, 2018a; Nooralahzadeh et al., 2020; Zhao et al., 2020). In addition to using typology for diagnostic purposes, prior work has also found that typology can guide cross-lingual sharing (de Lhoneux et al., 2018). Finally, the relationship between typological knowledge bases (KBs) such as WALS (Dryer and Haspelmath, 2013) and language representations has been studied, which has shown that knowledge base population methods can be used to complete typological KBs (Malaviya et al., 2017; Murawaki, 2017; Bjerva and Augenstein, 2018a; Bjerva et al., 2019c), and that implications can be discovered in typological KBs (Daum´e III and Campbell, 2007; Bjerva et al., 2019b). The latter stream of work has provided the inspiration for this shared task on typological feature prediction. As knowledge bases are notoriously incomplete and require manual labour from (in this case, linguistic) domain experts to create, populate and maintain, high-performance methods for autoTypological knowledge bases (KBs) such as WALS (Dryer and Haspelmath, 2013) contain information about linguistic properties of the world’s languages. They have been shown to be useful f"
2020.sigtyp-1.1,P19-1382,1,0.844402,"al similarities are exploited by such models (Bjerva and Augenstein, 2018a; Nooralahzadeh et al., 2020; Zhao et al., 2020). In addition to using typology for diagnostic purposes, prior work has also found that typology can guide cross-lingual sharing (de Lhoneux et al., 2018). Finally, the relationship between typological knowledge bases (KBs) such as WALS (Dryer and Haspelmath, 2013) and language representations has been studied, which has shown that knowledge base population methods can be used to complete typological KBs (Malaviya et al., 2017; Murawaki, 2017; Bjerva and Augenstein, 2018a; Bjerva et al., 2019c), and that implications can be discovered in typological KBs (Daum´e III and Campbell, 2007; Bjerva et al., 2019b). The latter stream of work has provided the inspiration for this shared task on typological feature prediction. As knowledge bases are notoriously incomplete and require manual labour from (in this case, linguistic) domain experts to create, populate and maintain, high-performance methods for autoTypological knowledge bases (KBs) such as WALS (Dryer and Haspelmath, 2013) contain information about linguistic properties of the world’s languages. They have been shown to be useful f"
2020.sigtyp-1.1,P07-1009,0,0.847176,"Missing"
2020.sigtyp-1.1,C16-1298,0,0.0351469,"Missing"
2020.sigtyp-1.1,N19-1423,0,0.0121619,"na Vylomova] Ryan Cotterell? Isabelle Augenstein/ • Aalborg University / University of Copenhagen  Johns Hopkins University ] University of Melbourne 2 Carnegie Mellon University Leipzig University ? ETH Z¨urich ‡ University of Cambridge jbjerva@cs.aau.dk, augenstein@di.ku.dk Abstract of reasons. The field has seen considerable advances in cross-lingual transfer learning, whereby stable cross-lingual representations can be learned on massive amounts of data in an unsupervised way, be it for words (Ammar et al., 2016; Wada et al., 2019) or, more recently, sentences (Artetxe and Schwenk, 2019; Devlin et al., 2019; Conneau and Lample, 2019; Conneau et al., 2020). This naturally raises the question of what these representations encode, and some have turned to typology for potential answers (Choenni and Shutova, 2020; Zhao et al., 2020). In a similar vein, research has shown that these learned representations can be fine-tuned for supervised tasks, then applied to new languages in a few- or even zero-shot fashion with surprisingly high performance. This has raised the question of what causes such surprisingly high results, and to what degree typological similarities are exploited by such models (Bjerva a"
2020.sigtyp-1.1,J19-2006,1,0.848772,"al similarities are exploited by such models (Bjerva and Augenstein, 2018a; Nooralahzadeh et al., 2020; Zhao et al., 2020). In addition to using typology for diagnostic purposes, prior work has also found that typology can guide cross-lingual sharing (de Lhoneux et al., 2018). Finally, the relationship between typological knowledge bases (KBs) such as WALS (Dryer and Haspelmath, 2013) and language representations has been studied, which has shown that knowledge base population methods can be used to complete typological KBs (Malaviya et al., 2017; Murawaki, 2017; Bjerva and Augenstein, 2018a; Bjerva et al., 2019c), and that implications can be discovered in typological KBs (Daum´e III and Campbell, 2007; Bjerva et al., 2019b). The latter stream of work has provided the inspiration for this shared task on typological feature prediction. As knowledge bases are notoriously incomplete and require manual labour from (in this case, linguistic) domain experts to create, populate and maintain, high-performance methods for autoTypological knowledge bases (KBs) such as WALS (Dryer and Haspelmath, 2013) contain information about linguistic properties of the world’s languages. They have been shown to be useful f"
2020.sigtyp-1.1,W19-4208,1,0.845374,"ographic proximity is a challenging one. We expect that further exploration of unconstrained systems to have the most potential for predicting features in such cases, where little or nothing is known about a language. 7 Typologically Informed Sharing Cross-lingual sharing informed by typology has been investigated for, among others, parsing (Naseem et al., 2012; T¨ackstr¨om et al., 2013; Zhang and Barzilay, 2015; de Lhoneux et al., 2018), language modeling (Tsvetkov et al., 2016; Ponti et al., 2019b), machine translation (Daiber et al., 2016; Ponti et al., 2018), and morphological inflection (Chaudhary et al., 2019). Many of these approaches use language embeddings with sparse features encoding WALS feature values. Oncevay et al. (2020) find that combining information from typological databases with embeddings learned during training of an NMT model can be beneficial for multilingual NMT. 6.3 Conclusions Acknowledgments This research has received funding from the Swedish Research Council under grant agreement No 2019-04129, as well as the German Research Foundation (DFG project number 408121292). Typological Probing Several recent papers study typological feature prediction as a probing task for evaluati"
2020.sigtyp-1.1,D18-1029,1,0.907351,"Missing"
2020.sigtyp-1.1,2020.sigtyp-1.3,0,0.0348564,"ure values for non-ancestral languages can be inferred individually by rerooting the tree to a related language. NUIG (Choudhary (2020), NUI Galway) submitted a constrained system with independent classifiers to predict each WALS feature. The outputs of independent classifiers are then fed into a shared encoder with feed-forward and self-attention layers in order to make use of feature correlations. Their model does not use other known features for WALS feature prediction at inference time, relying only on the 5-dimensional inputs of longitude, latitude, genus, family, and country-code. NEMO (Gutkin and Sproat (2020), Google London and Tokyo) submitted constrained systems which first computed probabilities of represented feature values across each language’s genetic (genus and family), and areal (features from languages within a 2,500 kilometre radius, computed from provided latitude and longitude with the Haversine formula), and implicational universals or rather, priors for certain features given commonly associated feature-value pairs in the data. Figure 2: Macro-averaged rankings of all submissions They compared several classifiers’ performance using these sparse features, ultimately submitting system"
2020.sigtyp-1.1,2020.acl-main.747,0,0.0779042,"Missing"
2020.sigtyp-1.1,2020.sigtyp-1.5,0,0.0676552,"Missing"
2020.sigtyp-1.1,2020.sigtyp-1.2,0,0.0945652,"Missing"
2020.sigtyp-1.1,N18-2085,1,0.860078,"Missing"
2020.sigtyp-1.1,D18-1543,1,0.916053,"Missing"
2020.sigtyp-1.1,E17-2002,0,0.0174388,"m 5 teams, out of which the most successful methods make use of such feature correlations. However, our error analysis reveals that even the strongest submitted systems struggle with predicting feature values for languages where few features are known. 1 Introduction Linguistic typology is the study of structural properties of languages (Comrie, 1988; Croft, 2002; Velupillai, 2012). Approaches to the categorisation of the languages of the world according to their linguistic properties are represented by, e.g., typological features in databases such as WALS (Dryer and Haspelmath, 2013), URIEL (Littell et al., 2017), and AUTOTYP (Nichols et al., 2013), e.g. in terms of their syntax, morphology, and phonology. One example of such a typological feature is the basic word order feature in WALS. For instance, English is best described as a subject-verb-object (SVO) language, whereas Japanese is best described as a subject-object-verb (SOV) language. Once a relatively niche topic in the NLP community, studying typological features has recently risen in popularity and importance for a number 1 Proceedings of the Second Workshop on Computational Research in Linguistic Typology, pages 1–11 c Online, November 19,"
2020.sigtyp-1.1,P18-1142,1,0.887137,"Missing"
2020.sigtyp-1.1,D17-1268,0,0.394103,"t causes such surprisingly high results, and to what degree typological similarities are exploited by such models (Bjerva and Augenstein, 2018a; Nooralahzadeh et al., 2020; Zhao et al., 2020). In addition to using typology for diagnostic purposes, prior work has also found that typology can guide cross-lingual sharing (de Lhoneux et al., 2018). Finally, the relationship between typological knowledge bases (KBs) such as WALS (Dryer and Haspelmath, 2013) and language representations has been studied, which has shown that knowledge base population methods can be used to complete typological KBs (Malaviya et al., 2017; Murawaki, 2017; Bjerva and Augenstein, 2018a; Bjerva et al., 2019c), and that implications can be discovered in typological KBs (Daum´e III and Campbell, 2007; Bjerva et al., 2019b). The latter stream of work has provided the inspiration for this shared task on typological feature prediction. As knowledge bases are notoriously incomplete and require manual labour from (in this case, linguistic) domain experts to create, populate and maintain, high-performance methods for autoTypological knowledge bases (KBs) such as WALS (Dryer and Haspelmath, 2013) contain information about linguistic prope"
2020.sigtyp-1.1,D19-1288,1,0.883931,"Missing"
2020.sigtyp-1.1,I17-1046,0,0.0877507,"gly high results, and to what degree typological similarities are exploited by such models (Bjerva and Augenstein, 2018a; Nooralahzadeh et al., 2020; Zhao et al., 2020). In addition to using typology for diagnostic purposes, prior work has also found that typology can guide cross-lingual sharing (de Lhoneux et al., 2018). Finally, the relationship between typological knowledge bases (KBs) such as WALS (Dryer and Haspelmath, 2013) and language representations has been studied, which has shown that knowledge base population methods can be used to complete typological KBs (Malaviya et al., 2017; Murawaki, 2017; Bjerva and Augenstein, 2018a; Bjerva et al., 2019c), and that implications can be discovered in typological KBs (Daum´e III and Campbell, 2007; Bjerva et al., 2019b). The latter stream of work has provided the inspiration for this shared task on typological feature prediction. As knowledge bases are notoriously incomplete and require manual labour from (in this case, linguistic) domain experts to create, populate and maintain, high-performance methods for autoTypological knowledge bases (KBs) such as WALS (Dryer and Haspelmath, 2013) contain information about linguistic properties of the wor"
2020.sigtyp-1.1,N13-1126,0,0.0591155,"Missing"
2020.sigtyp-1.1,P12-1066,0,0.170714,"Missing"
2020.sigtyp-1.1,N16-1161,0,0.0680928,"Missing"
2020.sigtyp-1.1,2020.emnlp-main.368,1,0.920027,"2019; Conneau et al., 2020). This naturally raises the question of what these representations encode, and some have turned to typology for potential answers (Choenni and Shutova, 2020; Zhao et al., 2020). In a similar vein, research has shown that these learned representations can be fine-tuned for supervised tasks, then applied to new languages in a few- or even zero-shot fashion with surprisingly high performance. This has raised the question of what causes such surprisingly high results, and to what degree typological similarities are exploited by such models (Bjerva and Augenstein, 2018a; Nooralahzadeh et al., 2020; Zhao et al., 2020). In addition to using typology for diagnostic purposes, prior work has also found that typology can guide cross-lingual sharing (de Lhoneux et al., 2018). Finally, the relationship between typological knowledge bases (KBs) such as WALS (Dryer and Haspelmath, 2013) and language representations has been studied, which has shown that knowledge base population methods can be used to complete typological KBs (Malaviya et al., 2017; Murawaki, 2017; Bjerva and Augenstein, 2018a; Bjerva et al., 2019c), and that implications can be discovered in typological KBs (Daum´e III and Camp"
2020.sigtyp-1.1,2020.emnlp-main.187,0,0.0638701,"al for predicting features in such cases, where little or nothing is known about a language. 7 Typologically Informed Sharing Cross-lingual sharing informed by typology has been investigated for, among others, parsing (Naseem et al., 2012; T¨ackstr¨om et al., 2013; Zhang and Barzilay, 2015; de Lhoneux et al., 2018), language modeling (Tsvetkov et al., 2016; Ponti et al., 2019b), machine translation (Daiber et al., 2016; Ponti et al., 2018), and morphological inflection (Chaudhary et al., 2019). Many of these approaches use language embeddings with sparse features encoding WALS feature values. Oncevay et al. (2020) find that combining information from typological databases with embeddings learned during training of an NMT model can be beneficial for multilingual NMT. 6.3 Conclusions Acknowledgments This research has received funding from the Swedish Research Council under grant agreement No 2019-04129, as well as the German Research Foundation (DFG project number 408121292). Typological Probing Several recent papers study typological feature prediction as a probing task for evaluating cross-lingual sentence encoders (Choenni and Shutova, 2020; Bjerva and Augenstein, 2018a; Nooralahzadeh et al., 2020; Zh"
2020.sigtyp-1.1,P19-1300,0,0.0206958,"J. Mielke Aditi Chaudhary2 Giuseppe G. A. Celano Edoardo M. Ponti‡ Ekaterina Vylomova] Ryan Cotterell? Isabelle Augenstein/ • Aalborg University / University of Copenhagen  Johns Hopkins University ] University of Melbourne 2 Carnegie Mellon University Leipzig University ? ETH Z¨urich ‡ University of Cambridge jbjerva@cs.aau.dk, augenstein@di.ku.dk Abstract of reasons. The field has seen considerable advances in cross-lingual transfer learning, whereby stable cross-lingual representations can be learned on massive amounts of data in an unsupervised way, be it for words (Ammar et al., 2016; Wada et al., 2019) or, more recently, sentences (Artetxe and Schwenk, 2019; Devlin et al., 2019; Conneau and Lample, 2019; Conneau et al., 2020). This naturally raises the question of what these representations encode, and some have turned to typology for potential answers (Choenni and Shutova, 2020; Zhao et al., 2020). In a similar vein, research has shown that these learned representations can be fine-tuned for supervised tasks, then applied to new languages in a few- or even zero-shot fashion with surprisingly high performance. This has raised the question of what causes such surprisingly high results, and t"
2020.sigtyp-1.1,E17-2102,0,0.458015,"t with 5k samples each. 3.2 Baselines We provide two baselines. The first is a simple lower-bound baseline based on observing feature frequencies in WALS (Baseline frequency in Figure 2). For each unobserved feature in the test set, we predict the most frequent feature value from the training set. The second uses the k-nearest neighbours (kNN) algorithm with a simple feature set to predict each unobserved feature, with k = 1 (Baseline knn-imputation in Figure 2). Each language is represented by a language vector (~l ∈ R64 ) trained as a part of a multilingual character-based language ¨ model (Ostling and Tiedemann, 2017). During inference, for a language l and unobserved feature y, we find the nearest neighbour to ~l for which y has been observed, similar to Bjerva and Augenstein (2018a,b). 1 3 Distances calculated with WALS language locations. 3.3 Submissions We received eight submissions from five teams across the constrained and unconstrained subtasks, as described below. ´ (Vastl et al. (2020), Charles University) UFAL submitted a constrained system which ensembled two approaches: first, estimating the correlation of feature values within languages enables missing feature prediction, and second, using a n"
2020.sigtyp-1.1,D15-1213,0,0.0604974,"Missing"
2020.tacl-1.1,J92-1002,0,0.824584,"Missing"
2020.tacl-1.1,C65-1001,0,0.259592,"lasi, 2014). To the extent that this is interpreted as being a compensatory relation, this would indicate that word length is being taken as an implicit measure of complexity. Alternatively, word length has a natural interpretation in terms of information rate, 2.3 Phonotactics Beyond characterizing the complexity of phonemes in isolation or the number of syllables, one can also look at the system determining how phonemes combine to form longer sequences in order to create words. The study of which sequences of phonemes constitute natural-sounding words is called phonotactics. For example, as Chomsky and Halle (1965) point out in their oftcited example, brick is an actual word in English;5 3 Note that by examining negative correlations between word length and inventory size within the context of complexity compensation, word length is also being taken implicitly as a complexity measure, as we shortly make explicit. 4 McWhorter (2001) was one of the first to offer a quantitative treatment of linguistic complexity at all levels. Note, however, he rejects the equal complexity hypothesis, arguing that creoles are simpler than other languages. As our data contain no creole languages, we cannot address this hyp"
2020.tacl-1.1,P17-1109,1,0.80852,"Missing"
2020.tacl-1.1,N01-1021,0,0.682633,"g language, which Miestamo (2006) points out may vary depending on the individual (hence, is relative to the individual being considered). For example, vowel harmony, which we will touch upon later in the paper, may make vowels more predictable for a native speaker, hence less difficult to process; for a second language learner, however, vowel harmony may increase difficulty of learning and speaking. Absolute complexity measures, in contrast, assess the number of parts of a linguistic (sub-)system (e.g., number of phonemes or licit syllables). In the sentence processing literature, surprisal (Hale, 2001; Levy, 2008) is a widely used measure of processing difficulty, defined as the negative log probability of a word given the preceding words. Words that are highly predictable from the preceding context have low surprisal, and those that are not predictable have high surprisal. The phonotactic measure we advocate for in §3 is related to surprisal, though at the phoneme level rather than the word level, and over words rather than sentences. Measures related to phonotactic probability have been used in a range of psycholinguistic studies—see §2.4—though generally to characterize single words wit"
2020.tacl-1.1,Q17-1006,0,0.053281,"Missing"
2020.tacl-1.1,P82-1020,0,0.84283,"Missing"
2020.tacl-1.1,J86-2003,0,0.156571,"et al., 1992). 1 Introduction One prevailing view on system wide phonological complexity is that as one aspect increases in complexity (e.g., size of phonemic inventory), another reduces in complexity (e.g., degree of phonotactic interactions). Underlying this claim— the so-called compensation hypothesis (Martinet, 1955; Moran and Blasi, 2014)—is the conjecture that languages are, generally speaking, of roughly equivalent complexity, that is, no language is overall inherently more complex than another. This conjecture is widely accepted in the literature and dates back at least to the work of Hockett (1958). Because along any one axis, a language may be more complex than another, this conjecture has a corollary that compensatory relationships between different types of complexity must exist. Such compensation has been hypothesized to be the result of natural processes of historical change, 1 Transactions of the Association for Computational Linguistics, vol. 8, pp. 1–18, 2020. https://doi.org/10.1162/tacl a 00296 Action Editor: Eric Fosler-Lussier. Submission batch: 5/2019; Revision batch: 9/2019; Published 1/2020. c 2020 Association for Computational Linguistics. Distributed under a CC-BY 4.0 l"
2020.tacl-1.1,J94-3001,0,0.469028,"lobstruent devoicing reduces phonological complexity under our information-theoretic metric. The reason is simple: There are fewer valid syllables as all those with voiced final obstruents are ruled out. Indeed, this point is also true of the syllable counting metric discussed in §2.2. One computational notion of complexity might say that the complexity of the phonology is equal to the number of states required to encode the transduction from an underlying form to a surface form in a minimal finite-state transduction. Note that all Sound Pattern of English (SPE)-style rules may be so encoded (Kaplan and Kay, 1994). Thus, the complexity of the phonotactics could be said to be related to the number of SPE-style rules that operate. In contrast, under our metric, any process that constrains the number of possibilities will, inherently, reduce complexity. The studies in §5.3 allow us to examine the magnitude of such a reduction, and validate our models with respect to this expected behavior. We create two artificial datasets without finalobstruent devoicing based on the German and Dutch portions of NorthEuraLex. We reverse the 8 Most of the concepts in the dataset do not contain function words and verbs are"
2020.tacl-1.1,P18-1027,0,0.0172519,"th black immediately portuguese finnish north karelian veps northern sami hill mari olho korva antua hambaz c˘ a´ hppes t¨op¨ok /oLu/ /kOrVA/ /AntUA/ /hAmbAz/ &gt; /Ùaahppes/ /tørøk/ corresponding embedding representation z (k) . A phoneme embedding will, then, be composed by the element-wise average of each of its features lookup embedding Recurrent Neural LM. Recurrent neural networks excel in language modeling, being able to capture complex distributions p(xi |x<i ) (Mikolov et al., 2010; Sundermeyer et al., 2012). Empirically, recent work has observed dependencies on up to around 200 tokens (Khandelwal et al., 2018). We use a characterlevel Long Short-Term Memory (LSTM, Hochreiter and Schmidhuber, 1997) language model, which is the state of the art for character-level language modeling (Merity et al., 2018). Our architecture receives a sequence of tokens x ∈ Σ∗ and embeds each token xi ∈ Σ using a dictionary-lookup embedding table. This results in vectors zi ∈ Rd which are fed into an LSTM. This LSTM produces a high-dimensional representation of the sequence, often termed hidden states zi = P (k ) (k ) k ai z P (k ) k ai (11) (j ) where ai is 1 if phoneme i presents attribute j and z (j ) is the lookup e"
2020.tacl-1.1,P19-1491,1,0.847512,"ther than a /d/. We conclude that /x/ is in the consonant 3.3 A Variational Upper Bound If we want to compute Equation (1), we are immediately faced with two problems. First, we do not know plex : we simply assume the existence of such a distribution from which the words of the lexicon were drawn. Second, even if we did know plex , computation of the H (plex ) would 7 3.4 A Note on Types and Tokens be woefully intractable, as it involves an infinite sum. Following Brown et al. (1992), we tackle both of these issues together. Note that this line of reasoning follows Cotterell et al. (2018) and Mielke et al. (2019), who use a similar technique for measuring language complexity at the sentence level. We start with a basic inequality from information theory. For any distribution qlex with the same support as plex , the cross-entropy provides an upper bound on the entropy, that is H (plex ) ≤ H (plex , qlex ) To make the implicit explicit, in this work we will exclusively be modeling types, rather than tokens. We briefly justify this discussion from both theoretical and practical concerns. From a theoretical side, a token-based model is unlikely to correctly model an out of vocabulary distribution as very"
2020.tacl-1.1,L18-1293,1,0.863836,"Missing"
2020.tacl-1.1,N18-2085,1,\N,Missing
2021.acl-long.106,J05-1003,0,0.205866,"are subject to a root constraint.1 1 K = 50 K=1 35 30 25 20 15 10 5 0 102 103 Training set size (log-scale) 104 Figure 1: Violation rate of the root constraint when using regular K-best decoding (Camerini et al., 1980) on pre-trained models of Qi et al. (2020) for languages with varying training set sizes. Introduction Non-projective, graph-based dependency parsers are widespread in the NLP literature. (McDonald et al., 2005; Dozat and Manning, 2017; Qi et al., 2020). However, despite the prevalence of K-best dependency parsing for other parsing formalisms— often in the context of re-ranking (Collins and Koo, 2005; Sangati et al., 2009; Zhu et al., 2015; Do and Rehbein, 2020) and other areas of NLP (Shen et al., 2004; Huang and Chiang, 2005; Pauls and Klein, 2009; Zhang et al., 2009), we have only found three works that consider K-best non-projective 1 Our implementation is available at https://github. com/rycolab/spanningtrees. dependency parsing (Hall, 2007; Hall et al., 2007; Agi´c, 2012). All three papers utilize the K-best spanning tree algorithm of Camerini et al. (1980). Despite the general utility of K-best methods in NLP, we suspect that the relative lack of interest in K-best non-projective d"
2021.acl-long.106,2020.acl-main.379,0,0.011897,"10 5 0 102 103 Training set size (log-scale) 104 Figure 1: Violation rate of the root constraint when using regular K-best decoding (Camerini et al., 1980) on pre-trained models of Qi et al. (2020) for languages with varying training set sizes. Introduction Non-projective, graph-based dependency parsers are widespread in the NLP literature. (McDonald et al., 2005; Dozat and Manning, 2017; Qi et al., 2020). However, despite the prevalence of K-best dependency parsing for other parsing formalisms— often in the context of re-ranking (Collins and Koo, 2005; Sangati et al., 2009; Zhu et al., 2015; Do and Rehbein, 2020) and other areas of NLP (Shen et al., 2004; Huang and Chiang, 2005; Pauls and Klein, 2009; Zhang et al., 2009), we have only found three works that consider K-best non-projective 1 Our implementation is available at https://github. com/rycolab/spanningtrees. dependency parsing (Hall, 2007; Hall et al., 2007; Agi´c, 2012). All three papers utilize the K-best spanning tree algorithm of Camerini et al. (1980). Despite the general utility of K-best methods in NLP, we suspect that the relative lack of interest in K-best non-projective dependency parsing is due to the implementation complexity and n"
2021.acl-long.106,P07-1050,0,0.0835718,"sers are widespread in the NLP literature. (McDonald et al., 2005; Dozat and Manning, 2017; Qi et al., 2020). However, despite the prevalence of K-best dependency parsing for other parsing formalisms— often in the context of re-ranking (Collins and Koo, 2005; Sangati et al., 2009; Zhu et al., 2015; Do and Rehbein, 2020) and other areas of NLP (Shen et al., 2004; Huang and Chiang, 2005; Pauls and Klein, 2009; Zhang et al., 2009), we have only found three works that consider K-best non-projective 1 Our implementation is available at https://github. com/rycolab/spanningtrees. dependency parsing (Hall, 2007; Hall et al., 2007; Agi´c, 2012). All three papers utilize the K-best spanning tree algorithm of Camerini et al. (1980). Despite the general utility of K-best methods in NLP, we suspect that the relative lack of interest in K-best non-projective dependency parsing is due to the implementation complexity and nuances of Camerini et al. (1980)’s algorithm.2 We make a few changes to Camerini et al. (1980)’s algorithm, which result in both a simpler algorithm and simpler proof of correctness.3 Firstly, both algorithms follow the key property that we can find the second-best tree of a graph by remo"
2021.acl-long.38,W08-0212,0,0.0225083,"simple form of morphological agreement: verbs agree with their subjects in number (singular or plural). This introduces an element of long-term dependencies into our languages – if a language model is to correctly predict a verb form, it must carry information about the number of the subject. In order to enforce this agreement in our grammar, non-terminals are subscripted with their number (where applicable). Assigning Probabilities. Weights given to each production were chosen manually through experimentation. Some principles for choosing weights for a grammar in this manner are described by Eisner and Smith (2008). An automated method of assigning weights could be explored in future work. 3.3 Our end goal is to construct a grammar parameterized by a binary vector of K switches. We denote such a vector of switches b ∈ {0, 1}K . Toggling an individual switch in the grammar reverses the order of the right-hand sides of a set of production rules. For example, the switch that we term the S switch reverses the order of the production S → NP VP to create S → VP NP.3 2K different grammars are possible from K binary switches. In the following paragraphs, we describe each of the switches we consider in this work"
2021.acl-long.38,N18-2085,1,0.870616,"nese also differ in many other typological dimensions, such as how subjects are marked, the extent of subject–verb agreement and use of postpositions or prepositions, which could contribute to the difference in performance. Indeed, recent correlational studies have failed to find an effect 454 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 454–463 August 1–6, 2021. ©2021 Association for Computational Linguistics between language model performance and typological features (Cotterell et al., 2018; Mielke et al., 2019). Moreover, the sentences used for training and testing may differ in content, style or information density, which could further contribute to differences in performance. Thus, we offer a study investigating the inductive biases of language models through the construction of artificial languages. Our approach involves creating small context-free grammars resembling subsets of attested languages, which we then use to train and evaluate language models. In an approach inspired by Chomsky’s (1981) framework of principles and parameters, we imbue our grammars with “switches”"
2021.acl-long.38,P19-1491,1,0.942299,"other typological dimensions, such as how subjects are marked, the extent of subject–verb agreement and use of postpositions or prepositions, which could contribute to the difference in performance. Indeed, recent correlational studies have failed to find an effect 454 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 454–463 August 1–6, 2021. ©2021 Association for Computational Linguistics between language model performance and typological features (Cotterell et al., 2018; Mielke et al., 2019). Moreover, the sentences used for training and testing may differ in content, style or information density, which could further contribute to differences in performance. Thus, we offer a study investigating the inductive biases of language models through the construction of artificial languages. Our approach involves creating small context-free grammars resembling subsets of attested languages, which we then use to train and evaluate language models. In an approach inspired by Chomsky’s (1981) framework of principles and parameters, we imbue our grammars with “switches” that indicate how to p"
2021.acl-long.38,N19-4009,0,0.0124832,"010101 010110 010111 011000 011001 011010 011011 011100 011101 011110 011111 100000 100001 100010 100011 100100 100101 100110 100111 101000 101001 101010 101011 101100 101101 101110 101111 110000 110001 110010 110011 110100 110101 110110 110111 111000 111001 111010 111011 111100 111101 111110 111111 10 Ordering Figure 3: All scores achieved by LSTM- and transformer-based models 4 Experiments 5 Architectures and Data. In order to compare inductive biases across architectures, two neural architectures were tested: transformers and LSTMs. We used the implementation available as part of Fairseq (Ott et al., 2019). Our base grammar has K = 6 switches, i.e. 6 binary choice points as described in §3.3. This results in 26 = 64 possible grammars. For each of these grammars we generated 100,000 sentences, which were divided into 10 splits of 10,000.6 The sentences generated for each grammar differed only in the designated choice points, i.e. in the ordering of their constituents. This meant that each sentence appeared in an equivalent form in each grammar. As such, for each sentence, we can compare the perplexity of the 64 variants of the sentence as calculated by language models trained on the correspondin"
2021.acl-long.38,N19-1356,0,0.0332329,"Missing"
2021.acl-long.38,Q16-1035,0,0.0188981,"at has not been clearly stated in the NLP literature. 2 2.1 Why Artificial Languages? respect to specific phenomenon, such as their ability to acquire hierarchical generalizations (McCoy et al., 2018) and whether they can use systematic composition skills to make generalizations (Lake and Baroni, 2018). Bowman et al. (2015) also used artificial languages to investigate the ability of LSTMs to learn compositional structure, and compare their ability to that of tree-structured models. The work most closely related to ours is that of Ravfogel et al. (2019). Taking methodological inspiration from Wang and Eisner (2016), they create artificial versions of English with modified word order and case systems, including a version with object–verb agreement. They use the task of predicting the number of the subject and object of a missing verb to examine language model performance across these variations. They find that the models perform better on this task for the language with SVO word order. What they leave unchanged in their experiment, however, is the original English ordering within the constituents, e.g. the adjective–noun ordering in a noun phrase. However, constituent order correlates with ordering of ot"
2021.acl-long.404,2020.aacl-main.25,0,0.0415154,"Missing"
2021.acl-long.404,2020.lrec-1.297,0,0.0576653,"Missing"
2021.acl-long.404,2020.acl-main.615,1,0.926749,"ate u(·) via eq. (4) using our model pθ . Local Consistency. Next, we consider a local consistency regularizer that encourages the surprisals of adjacent words to have similar magnitude: |w|−1  2 X 1 R(θ) = u(wt ) − u(wt+1 ) (7) |w|−1 t=1 This regularizer is also a reasonable operationalization of UID—if every surprisal is similar to its neighbor, then the density of information in the sequence will be close to uniform. Though we focus on these two regularizers, other operationalizations of UID certainly exist. For example, a similar variant of the above regularizers is the max regularizer (Meister et al., 2020a), which penalizes the highest surprisal in a sentence.3 Furthermore, UID may also be defined in terms of parse steps (Hale, 2001) or structural integrations (Gibson, 2000), as well as in spoken language in the form of filler words like uh and um or word repetition during challenging lexical retrieval. We consider these operationalizations (as well as the broader discussion of how to operationalize UID) as future work. 3 We also tried this operationalization in preliminary experiments, but results were not as strong as the variance or local consistency regularizers. 5 Experimental Setup To em"
2021.acl-long.404,P19-1491,1,0.754097,"re work. 3 We also tried this operationalization in preliminary experiments, but results were not as strong as the variance or local consistency regularizers. 5 Experimental Setup To empirically evaluate UID regularization, we train various language models with the UIDregularized objective (eq. (5)) using the following experimental setup. Datasets. We employ datasets from multiple languages and of varying sizes. We use the EuroParl corpus (Koehn, 2005)—a multi-lingual dataset of discussions from the European Parliament that has been commonly used for language modeling (Cotterell et al., 2018; Mielke et al., 2019)—since it is roughly semantically controlled in that all utterances are presumably about the same topics. We use EuroParl v7 download from the ACL 2014 SMT Workshop4 and perform a 80–10–10 traindev-test split on all five languages—Czech, English, French, German, and Spanish—which yields 46.7, 42.2, 47.2, 51.3, and 12.4 million training tokens for each language respectively. Moreover, we experiment on languages from several language families; the five languages in Europarl that we consider are all Indo-European, and so we look to Wiki-40B (Guo et al., 2020), which contains Wikipedia dumps of a"
2021.acl-long.404,N19-4009,0,0.0284883,"l: Finnish (a Uralic language; 59.3M training tokens), Indonesian (an Austronesian language; 45.7M training tokens), and Turkish (a Turkic language; 38.1M training tokens). To explore performance on lower-resource languages, we additionally experiment with Swahili5 (a Niger-Congo language; 6.3M training tokens) and Tagalog (an Austronesian language; 4.2M training tokens). For all languages, we performed tokenization using the MosesTokenizer.6 Train, dev, and test set splits are shown in Table 5 in the Appendix. Model Framework and Architecture. For our experiments, we use the fairseq library (Ott et al., 2019), a standard sequence modeling toolkit in PyTorch. As our model, we use fairseq’s default transformer (with six decoder layers and eight 4 http://statmt.org/wmt14/ translation-task.html 5 Since there are no Niger-Congo languages in Wiki-40B, we perform a 80-10-10 split on Swahili Wikidumps (see https://github.com/google-research/bert/ blob/master/multilingual.md). 6 https://pypi.org/project/ mosestokenizer/ 5194 attention heads), which achieves competitive7 language modeling performance (although the purpose of our paper is not to achieve or compare with the state of the art). For all experime"
2021.acl-long.404,N16-1110,0,0.0189394,"Missing"
2021.acl-long.414,Q19-1004,0,0.0271769,"ke perplexity—in order to understand what attributes of human language these models are learning. Some use task-based approaches, i.e., they design a set of tasks that require a specific subset of linguistic knowledge then evaluate model performance on these tasks (Linzen et al., 2016; Gulordava et al., 2018; Jiang et al., 2020, inter alia). Others use model-based approaches, where a separate model is trained to perform some auxiliary task on representations learned by the model under test (Blevins et al., 2018; Giulianelli et al., 2018; Sorodoc et al., 2020, inter alia). We direct readers to Belinkov and Glass (2019) for a full survey of probing methods. These approaches have drawbacks; for example, introducing a secondary model to determine what the original model has learned presents confounding factors (Hewitt and Liang, 2019). The designing of auxiliary tasks for assessing linguistic knowledge requires large manual effort and lends itself to implicit bias about how linguistic phenomena should manifest. In contrast, our work allows us to take a hands-off approach to analyzing language models. We see the benefit of this in §5, where our results without an assumed model of statistical tendencies give us"
2021.acl-long.414,P18-2003,0,0.129605,"STMs reflects the natural language distributions over length, stopwords, and symbols surprisingly well. 1 Figure 1: Average number of unique words vs. document length, i.e., type–token, in text sampled from language models. Values from models’ test set are plotted for reference. and Glass, 2019), i.e., determining whether models encode linguistic phenomena. For the most part, these works have been limited to analyses of sentence-level phenomenon, such as subject–verb agreement (Gulordava et al., 2018) and garden path effects (van Schijndel and Linzen, 2018) among a myriad of other properties (Blevins et al., 2018; Chowdhury and Zamparelli, 2018, inter alia). Introduction Neural language models1 have become shockingly good at modeling natural language data in recent years (Merity et al., 2017; Conneau and Lample, 2019; Radford et al., 2019). Thus, to test just how well neural language models capture language NLP researchers have started to look beyond standard evaluation metrics such as perplexity, endeavoring to understand which underlying attributes of human language these models are learning. To this end, a nascent literature has emerged that focuses on probing language models (Belinkov 1 In this wo"
2021.acl-long.414,C18-1012,0,0.0145645,"ral language distributions over length, stopwords, and symbols surprisingly well. 1 Figure 1: Average number of unique words vs. document length, i.e., type–token, in text sampled from language models. Values from models’ test set are plotted for reference. and Glass, 2019), i.e., determining whether models encode linguistic phenomena. For the most part, these works have been limited to analyses of sentence-level phenomenon, such as subject–verb agreement (Gulordava et al., 2018) and garden path effects (van Schijndel and Linzen, 2018) among a myriad of other properties (Blevins et al., 2018; Chowdhury and Zamparelli, 2018, inter alia). Introduction Neural language models1 have become shockingly good at modeling natural language data in recent years (Merity et al., 2017; Conneau and Lample, 2019; Radford et al., 2019). Thus, to test just how well neural language models capture language NLP researchers have started to look beyond standard evaluation metrics such as perplexity, endeavoring to understand which underlying attributes of human language these models are learning. To this end, a nascent literature has emerged that focuses on probing language models (Belinkov 1 In this work, we do not use the term langu"
2021.acl-long.414,N19-1423,0,0.0179706,"ecome shockingly good at modeling natural language data in recent years (Merity et al., 2017; Conneau and Lample, 2019; Radford et al., 2019). Thus, to test just how well neural language models capture language NLP researchers have started to look beyond standard evaluation metrics such as perplexity, endeavoring to understand which underlying attributes of human language these models are learning. To this end, a nascent literature has emerged that focuses on probing language models (Belinkov 1 In this work, we do not use the term language model to refer to cloze language models such as BERT (Devlin et al., 2019), which do not give us a distribution over strings. In this work, we attempt to understand which macro-level phenomena of human language today’s language models reflect. That is, we pose the question: Do neural language models exhibit the statistical tendencies of human language? Phenomena that can be measured at this level provide an alternate view of a model’s comprehension; for example, rather than exploring whether morphological agreement is captured, we look at whether our models learn the trends across a corpus as a whole, e.g., the token rank–frequency (Zipf’s) relationship. In comparis"
2021.acl-long.414,P18-1128,0,0.0276081,"and Tanaka-Ishii (2017, 2019) who use model generated text to visually analyze whether language models reflect well-established statistical tendencies. In contrast, our work provides a quantitative framework, along with appropriate significance tests,16 for evaluating distribution fits. We additionally assess the fit of language models to our test set directly, rather than solely to established laws. Further, our analysis includes different generation strategies, multiple neural architectures, and a wider variety of empirical language distributions. 16 In this respect, our work is similar to Dror et al. (2018), whom also present statistical tests for use in NLP. 7 Conclusion and Future Directions In this work, we present a framework for determining the linguistic properties learned by language models through analysis of statistical trends in generated text. We find that neural language models accurately capture only a subset of natural language distributions and that this subset is highly dependent on both model architecture and generation strategy; no one configuration stands out as capturing all linguistic distributions. Ultimately, we see this analysis framework as a means for a more finegrained"
2021.acl-long.414,2020.coling-main.398,0,0.0218727,"rameters: both in their generally close fit to the natural language type–token distribution and in their visible fall-off for longer length sequences. The latter observation reveals a deficiency that is seemingly specific to the transformer architecture—one that may be linked to observations in natural language generation tasks. More specifically, we take this as quantitative evidence for recent qualitative observations that when left to generate lots of text, neural language models based on the transformer architecture tend to babble repetitively (Holtzman et al., 2020; Cohen and Beck, 2019; Eikema and Aziz, 2020). To provide a more mathematically rigorous analysis, we compute KS metrics,14 again presenting three values: Dp✓ , Dpˆ, and Dp . In Fig. 4, we can see that model-generated text follows a NHPP parameterized by Heaps’ law moderately well (Dp✓ ); there are larger divergences at the tails of document length. However, most do not follow an NHPP with the same parameters as our test set (Dpˆ). Further, in contrast to rank–frequency, the type–token distribution is more disparate from the empirical natural language distribution than our parameterized ones, as shown by high values of Dp . While both tr"
2021.acl-long.414,W18-5426,0,0.0122809,"s (Wood and Altavela, 1978)) for all KS metrics are ⌧ 0.001. evaluation metrics—like perplexity—in order to understand what attributes of human language these models are learning. Some use task-based approaches, i.e., they design a set of tasks that require a specific subset of linguistic knowledge then evaluate model performance on these tasks (Linzen et al., 2016; Gulordava et al., 2018; Jiang et al., 2020, inter alia). Others use model-based approaches, where a separate model is trained to perform some auxiliary task on representations learned by the model under test (Blevins et al., 2018; Giulianelli et al., 2018; Sorodoc et al., 2020, inter alia). We direct readers to Belinkov and Glass (2019) for a full survey of probing methods. These approaches have drawbacks; for example, introducing a secondary model to determine what the original model has learned presents confounding factors (Hewitt and Liang, 2019). The designing of auxiliary tasks for assessing linguistic knowledge requires large manual effort and lends itself to implicit bias about how linguistic phenomena should manifest. In contrast, our work allows us to take a hands-off approach to analyzing language models. We see the benefit of this i"
2021.acl-long.414,N18-1108,0,0.104115,"to the type– token relationship of natural language than text produced using standard ancestral sampling; text from LSTMs reflects the natural language distributions over length, stopwords, and symbols surprisingly well. 1 Figure 1: Average number of unique words vs. document length, i.e., type–token, in text sampled from language models. Values from models’ test set are plotted for reference. and Glass, 2019), i.e., determining whether models encode linguistic phenomena. For the most part, these works have been limited to analyses of sentence-level phenomenon, such as subject–verb agreement (Gulordava et al., 2018) and garden path effects (van Schijndel and Linzen, 2018) among a myriad of other properties (Blevins et al., 2018; Chowdhury and Zamparelli, 2018, inter alia). Introduction Neural language models1 have become shockingly good at modeling natural language data in recent years (Merity et al., 2017; Conneau and Lample, 2019; Radford et al., 2019). Thus, to test just how well neural language models capture language NLP researchers have started to look beyond standard evaluation metrics such as perplexity, endeavoring to understand which underlying attributes of human language these models are lear"
2021.acl-long.414,D19-1275,0,0.017275,"hen evaluate model performance on these tasks (Linzen et al., 2016; Gulordava et al., 2018; Jiang et al., 2020, inter alia). Others use model-based approaches, where a separate model is trained to perform some auxiliary task on representations learned by the model under test (Blevins et al., 2018; Giulianelli et al., 2018; Sorodoc et al., 2020, inter alia). We direct readers to Belinkov and Glass (2019) for a full survey of probing methods. These approaches have drawbacks; for example, introducing a secondary model to determine what the original model has learned presents confounding factors (Hewitt and Liang, 2019). The designing of auxiliary tasks for assessing linguistic knowledge requires large manual effort and lends itself to implicit bias about how linguistic phenomena should manifest. In contrast, our work allows us to take a hands-off approach to analyzing language models. We see the benefit of this in §5, where our results without an assumed model of statistical tendencies give us a much different sense of which empirical properties of human-generated text our models have learned. Our work is closest to that of Takahashi and Tanaka-Ishii (2017, 2019) who use model generated text to visually ana"
2021.acl-long.414,P82-1020,0,0.613147,"Missing"
2021.acl-long.414,2020.tacl-1.28,0,0.0228391,"037 0.205 0.252 0.213 0.271 0.061 Table 3: KS metrics (Dp ) between empirical length, stopword, and symbol distributions of test set and model generated text. p-values (estimated using Monte Carlo simulations (Wood and Altavela, 1978)) for all KS metrics are ⌧ 0.001. evaluation metrics—like perplexity—in order to understand what attributes of human language these models are learning. Some use task-based approaches, i.e., they design a set of tasks that require a specific subset of linguistic knowledge then evaluate model performance on these tasks (Linzen et al., 2016; Gulordava et al., 2018; Jiang et al., 2020, inter alia). Others use model-based approaches, where a separate model is trained to perform some auxiliary task on representations learned by the model under test (Blevins et al., 2018; Giulianelli et al., 2018; Sorodoc et al., 2020, inter alia). We direct readers to Belinkov and Glass (2019) for a full survey of probing methods. These approaches have drawbacks; for example, introducing a secondary model to determine what the original model has learned presents confounding factors (Hewitt and Liang, 2019). The designing of auxiliary tasks for assessing linguistic knowledge requires large ma"
2021.acl-long.414,2020.acl-main.66,0,0.0267968,"raining? We posit that corpus preprocessing should perhaps be more carefully considered in light of these results. 5.5 Consistent Trends Across results, we observe that text generated using the nucleus sampling decoding scheme often aligns with natural language more closely than text produced using other generation strategies. This suggests that nucleus sampling performs a helpful alteration to a standard distribution learned via MLE, which may in turn provide motivation for recent efforts to employ truncated or sparse probability distributions directly at training time, e.g., truncated loss (Kang and Hashimoto, 2020) or ↵entmax loss (Peters et al., 2019). We additionally observe large discrepancies in both §5.1 and §5.2 between the results when using empirical natural language cdfs vs. parametric ones. We take this as a warning that assumptions about the forms of linguistic distributions—such as the ones employed by challenge tasks in probing—can have significant effects on results. 6 Related Work In the last few years, a number of works have extended language model analysis beyond simple 5335 Model Random Transformer Transformer (AS) CNN LSTM Trigram 0.031 0.037 0.034 0.014 0.093 Length Nucleus 0.034 0.0"
2021.acl-long.414,P07-2045,0,0.00688333,"ampling (Nucleus), and beam sampling (Beam).10 In ancestral random sampling, y(i) are constructed iteratively according to the distribution Experiments We use the above framework to assess the degree to which language models learn various distributions of natural language, i.e., we report metrics outlined in §4 measured over the distributions and quantities defined in §3. We compare samples generated from language models to a reserved test set taken from the same corpus as the model’s training data. Each set contains 1 million samples.8 We tokenize all samples using the Moses decoder toolkit (Koehn et al., 2007). All text is lower-cased and only complete unigrams are considered, i.e., when BPE is used, only the detokenized unigram is considered. Length of a string is computed as the number of tokens separated by whitespace. Note that when reporting the KS metric (D), we always report the metric between (a) an empirical cdf computed over the respective model-generated samples and (b) a reference cdf, where Dp indicates direct comparison with empirical cdf of the test set. Dp✓ and Dpˆ indicate comparison with cdfs of a parametric distribution, whose parameters are estimated on the model and test set, r"
2021.acl-long.414,Q16-1037,0,0.032964,"ymbol Nucleus Beam 0.065 0.072 0.054 0.048 0.037 0.205 0.252 0.213 0.271 0.061 Table 3: KS metrics (Dp ) between empirical length, stopword, and symbol distributions of test set and model generated text. p-values (estimated using Monte Carlo simulations (Wood and Altavela, 1978)) for all KS metrics are ⌧ 0.001. evaluation metrics—like perplexity—in order to understand what attributes of human language these models are learning. Some use task-based approaches, i.e., they design a set of tasks that require a specific subset of linguistic knowledge then evaluate model performance on these tasks (Linzen et al., 2016; Gulordava et al., 2018; Jiang et al., 2020, inter alia). Others use model-based approaches, where a separate model is trained to perform some auxiliary task on representations learned by the model under test (Blevins et al., 2018; Giulianelli et al., 2018; Sorodoc et al., 2020, inter alia). We direct readers to Belinkov and Glass (2019) for a full survey of probing methods. These approaches have drawbacks; for example, introducing a secondary model to determine what the original model has learned presents confounding factors (Hewitt and Liang, 2019). The designing of auxiliary tasks for asse"
2021.acl-long.414,W18-6322,0,0.022445,"n inductive bias that is helpful for capturing these distributions. On the other hand, using beam sampling leads to strong divergence from natural language distributions across the board. Results for differences in distribution means in the permutation testing framework can be found in App. D. With respect to the length distribution, these results are perhaps surprising: the localnormalization scheme used by the majority of language generation models (and by those in these experiments) has been claimed to result in models that favor shorter than typical sequences (Sountsov and Sarawagi, 2016; Murray and Chiang, 2018). The results in Tab. 3 and Fig. 5 suggest otherwise. 15 We observe this empirically; calculating TVD between distributions truncated to the (union of the) first 1000 ranked unigrams lead to almost the exact same result. Figure 5: Boxplots showing the distribution of sample length per model and generation scheme. Distribution of test set is repeated in each group for reference. Specifically, we see that our models fit the natural language length distribution of our corpus quite closely, in terms of both overall distributions and means (see App. D). Rather, it appears that the generation strate"
2021.acl-long.414,P19-1146,0,0.0174416,"should perhaps be more carefully considered in light of these results. 5.5 Consistent Trends Across results, we observe that text generated using the nucleus sampling decoding scheme often aligns with natural language more closely than text produced using other generation strategies. This suggests that nucleus sampling performs a helpful alteration to a standard distribution learned via MLE, which may in turn provide motivation for recent efforts to employ truncated or sparse probability distributions directly at training time, e.g., truncated loss (Kang and Hashimoto, 2020) or ↵entmax loss (Peters et al., 2019). We additionally observe large discrepancies in both §5.1 and §5.2 between the results when using empirical natural language cdfs vs. parametric ones. We take this as a warning that assumptions about the forms of linguistic distributions—such as the ones employed by challenge tasks in probing—can have significant effects on results. 6 Related Work In the last few years, a number of works have extended language model analysis beyond simple 5335 Model Random Transformer Transformer (AS) CNN LSTM Trigram 0.031 0.037 0.034 0.014 0.093 Length Nucleus 0.034 0.041 0.051 0.036 0.084 Beam Random 0.481"
2021.acl-long.414,W98-1218,0,0.599848,"t adhere very closely to a standard Zipfian distribution (as shown by Dp✓ and Dpˆ 0), despite appearing to at a superficial level (see App. D). However, the same is true for our test (Dpˆ = 0.148), which suggests that our models fit a Zipfian distribution perhaps no more poorly than natural language does. Rather, the model produces qualitatively worst text (see App. E)—a trigram model under the beam sampling generation strategy—follows a power law trend the most closely of any of our samples. On the other hand, the small values of Dp suggest our 13 s is known to vary with the corpus size |C |(Powers, 1998), however |C |is the same for all sets, so this should not affect our analysis. 5333 Model Transformer Transformer (AS) CNN LSTM Trigram R D p✓ N B 0.150 0.145 0.145 0.147 0.151 0.145 0.142 0.142 0.143 0.148 0.170 0.150 0.167 0.175 0.119 Rank–Frequency Dpˆ R N B 0.142 0.142 0.142 0.142 0.146 0.150 0.143 0.144 0.144 0.154 Unigram 0.170 0.142 0.167 0.178 0.152 R Dp N B R N B 3.7e-3 0.013 0.013 0.016 4.9e-3 0.029 0.041 0.039 0.043 0.020 0.024 0.046 0.022 0.034 0.251 6.9e-3 0.014 6.9e-3 3.4e-3 2.9e-3 6.9e-3 0.014 6.9e-3 0.010 3.0e-3 6.9e-3 0.038 8.6e-3 9.2e-3 0.075 TVD Table 2: KS metrics (lower i"
2021.acl-long.414,D18-1499,0,0.0376044,"Missing"
2021.acl-long.414,P16-1162,0,0.0105954,"s framework is a valuable tool for gaining a deeper understanding of where today’s language models are succeeding and failing at capturing human language. 2 Language Models Language models are probability distributions over natural language sentences. We define the support of a language model p✓ with parameters ✓ as Y := {BOS v EOS |v 2 V ⇤ } (1) where V is the model’s vocabulary and tokens EOS and BOS demarcate the beginning and end of a string, respectively, and V ⇤ is the Kleene closure of V. In this paper, we term vocabularies consisting of words closed and those consisting of BPE tokens (Sennrich et al., 2016) open. In the case when p✓ is locally normalized, which is the predominant case for language models, p✓ is defined as the product of probability distributions: p✓ (y) = |y| Y t=1 p✓ (yt |y&lt;t ) (2) where each p✓ (· |y&lt;t ) is a distribution with support over V¯ := V [{EOS} and y&lt;1 = y0 := BOS. To estimate model parameters ✓, one typically optimizes the log-likelihood function over a corpus Ctrain : X L(✓ |Ctrain ) = log p✓ (y) (3) y2Ctrain where we call each string y a document. To determine the goodness of fit of a model to the 2 Such biases are naturally introduced by many probing techniques t"
2021.acl-long.414,2020.acl-main.384,0,0.0243866,")) for all KS metrics are ⌧ 0.001. evaluation metrics—like perplexity—in order to understand what attributes of human language these models are learning. Some use task-based approaches, i.e., they design a set of tasks that require a specific subset of linguistic knowledge then evaluate model performance on these tasks (Linzen et al., 2016; Gulordava et al., 2018; Jiang et al., 2020, inter alia). Others use model-based approaches, where a separate model is trained to perform some auxiliary task on representations learned by the model under test (Blevins et al., 2018; Giulianelli et al., 2018; Sorodoc et al., 2020, inter alia). We direct readers to Belinkov and Glass (2019) for a full survey of probing methods. These approaches have drawbacks; for example, introducing a secondary model to determine what the original model has learned presents confounding factors (Hewitt and Liang, 2019). The designing of auxiliary tasks for assessing linguistic knowledge requires large manual effort and lends itself to implicit bias about how linguistic phenomena should manifest. In contrast, our work allows us to take a hands-off approach to analyzing language models. We see the benefit of this in §5, where our result"
2021.acl-long.414,D16-1158,0,0.0262804,", suggesting LSTMs may have an inductive bias that is helpful for capturing these distributions. On the other hand, using beam sampling leads to strong divergence from natural language distributions across the board. Results for differences in distribution means in the permutation testing framework can be found in App. D. With respect to the length distribution, these results are perhaps surprising: the localnormalization scheme used by the majority of language generation models (and by those in these experiments) has been claimed to result in models that favor shorter than typical sequences (Sountsov and Sarawagi, 2016; Murray and Chiang, 2018). The results in Tab. 3 and Fig. 5 suggest otherwise. 15 We observe this empirically; calculating TVD between distributions truncated to the (union of the) first 1000 ranked unigrams lead to almost the exact same result. Figure 5: Boxplots showing the distribution of sample length per model and generation scheme. Distribution of test set is repeated in each group for reference. Specifically, we see that our models fit the natural language length distribution of our corpus quite closely, in terms of both overall distributions and means (see App. D). Rather, it appears"
2021.acl-long.414,J19-3003,0,0.0580401,"Missing"
2021.acl-long.512,W19-5301,0,0.0276635,"Missing"
2021.acl-long.512,D18-1045,0,0.0181169,"al beam search as a diverse decoding strategy for language generation. 5.1 Baselines Various diverse decoding strategies exist in the NLP literature. We first discuss those strategies that we employ as baselines in our experiments. Standard Beam Search. Beam search is one of the most widely used decoding algorithms in NLP, where many problems require efficient strategies for decoding solutions from structured predictors. Specifically, for language generation tasks, beam search has repeatedly proved its effectiveness at decoding state-of-the-art solutions (Wu et al., 2016; Serban et al., 2017; Edunov et al., 2018; Yang et al., 2019). We refer back to §2.1 for the algorithm. Stochastic Beam Search. Kool et al. (2019) propose stochastic beam search (SBS), a decoding technique that samples without replacement from sequence models according to their distribution over the entire space Y. For random sampling methods such as SBS, it is customary to use a sampling temperature T &gt; 0 at generation time to control for the peakiness of the sampling distribution. This results in the generalized softmax: pT (y |y<t , x) (13)   exp log p(y |y<t , x)/T   0 |y , x)/T exp log p(y <t y 0 ∈V =P where larger T may lea"
2021.acl-long.512,2020.coling-main.398,0,0.0175905,"e further define the set decoding problem as the search for a set Y ? of a specified cardinality k among all valid subsets {Y 0 ⊆ Y ||Y 0 |= k} that has the highest score where, by overloading, we define Y def p(Y |x) = p(y |x) (3) Degenerate Objective. It is important to note that the highest-probability solutions under neural sequence models are not always high-quality; specifically for tasks involving language generation, e.g., machine translation, prior work has shown the tendency for MAP decoding to lead to generic or degenerate solutions (Stahlberg and Byrne, 2019; Meister et al., 2020; Eikema and Aziz, 2020) while superior solutions assigned only slightly lower probability are often overlooked (Holtzman et al., 2020). Consequently, heuristic search methods or alternative objectives are frequently employed for decoding language generators. 2.1 Beam Search A common heuristic to approximate the decoding problem in Eq. (2) is to sequentially choose the token yt at each time step t that maximizes p(yt |y<t , x) until the EOS token is generated or the maximum sequence length nmax is reached. This procedure is known as greedy search. Beam search is an oft-employed generalization of greedy search that re"
2021.acl-long.512,D09-1005,0,0.13528,"Missing"
2021.acl-long.512,2020.emnlp-main.170,1,0.835348,"Missing"
2021.acl-long.512,W19-5333,0,0.0235078,"Missing"
2021.acl-long.512,N19-4009,0,0.0233986,"ult 11 The diversity term has coefficient w to determine the strength of the penalty. When this weight is 0 or sufficiently small, all groups will return the same solution(s). 6555 Figure 1: Averaged n-gram diversity vs. minimum, median, and maximum BLEU score for beam sizes k = 5, 10, 20 on WMT’14 En–Fr and WMT’19 De–En newstest using various decoding strategies. The free parameter for each strategy is either the softmax temperature or the weight of the diversity parameter (see §5.2). et al., 2019) De–En datasets; for reproducibility, we use the pretrained models made available by fairseq12 (Ott et al., 2019). We evaluate on the newstest set from the respective datasets, each containing 3003 sentences. Further details can be found in App. B. For determinantal beam search (DetBS), we perform a hyperparameter search (precise details likewise in App. B) over λ and n, the decay factor and subsequence length, respectively. Search is performed for fixed w = 0.1 and k = 10 on validation sets for both languages; we omit a search over the entire space of w, k, λ, n so as to not create an unfair advantage for DetBS in comparison with the other decoding strategies, for which no hyperparameters are tuned. We"
2021.acl-long.512,D19-1331,0,0.0195277,"e the log transform of p is used by convention. We further define the set decoding problem as the search for a set Y ? of a specified cardinality k among all valid subsets {Y 0 ⊆ Y ||Y 0 |= k} that has the highest score where, by overloading, we define Y def p(Y |x) = p(y |x) (3) Degenerate Objective. It is important to note that the highest-probability solutions under neural sequence models are not always high-quality; specifically for tasks involving language generation, e.g., machine translation, prior work has shown the tendency for MAP decoding to lead to generic or degenerate solutions (Stahlberg and Byrne, 2019; Meister et al., 2020; Eikema and Aziz, 2020) while superior solutions assigned only slightly lower probability are often overlooked (Holtzman et al., 2020). Consequently, heuristic search methods or alternative objectives are frequently employed for decoding language generators. 2.1 Beam Search A common heuristic to approximate the decoding problem in Eq. (2) is to sequentially choose the token yt at each time step t that maximizes p(yt |y<t , x) until the EOS token is generated or the maximum sequence length nmax is reached. This procedure is known as greedy search. Beam search is an oft-em"
2021.acl-short.17,2020.emnlp-main.263,1,0.912862,"sparsity can be tuned via the hyperparameter 2 ( 1, 1); a larger encourages more sparsity in the minimizing solution. 3 Model Interpretability Model interpretability and explainability have been framed in different ways (Gehrmann et al., 2019)— as model understanding tasks, where (spurious) features learned by a model are identified, or as decision understanding tasks, where explanations for particular instances are produced. We consider the latter in this paper. Such tasks can be framed as generative, where models generate free text explanations (Camburu et al., 2018; Kotonya and Toni, 2020; Atanasova et al., 2020b), or as post-hoc interpretability methods, where salient portions of the input are highlighted (Lipton, 2018; DeYoung et al., 2020; Atanasova et al., 2020a). As there does not exist a clearly superior choice for framing decision understanding for NLP tasks (Miller, 2019; Carton et al., 2020; Jacovi and 123 Goldberg, 2021), we follow a substantial body of prior work in considering the post-hoc definition of interpretability based on local methods proposed by Lipton (2018). This definition is naturally operationalized through feature importance metrics and meta models (Jacovi and Goldberg, 202"
2021.acl-short.17,2020.acl-main.656,1,0.929314,"sparsity can be tuned via the hyperparameter 2 ( 1, 1); a larger encourages more sparsity in the minimizing solution. 3 Model Interpretability Model interpretability and explainability have been framed in different ways (Gehrmann et al., 2019)— as model understanding tasks, where (spurious) features learned by a model are identified, or as decision understanding tasks, where explanations for particular instances are produced. We consider the latter in this paper. Such tasks can be framed as generative, where models generate free text explanations (Camburu et al., 2018; Kotonya and Toni, 2020; Atanasova et al., 2020b), or as post-hoc interpretability methods, where salient portions of the input are highlighted (Lipton, 2018; DeYoung et al., 2020; Atanasova et al., 2020a). As there does not exist a clearly superior choice for framing decision understanding for NLP tasks (Miller, 2019; Carton et al., 2020; Jacovi and 123 Goldberg, 2021), we follow a substantial body of prior work in considering the post-hoc definition of interpretability based on local methods proposed by Lipton (2018). This definition is naturally operationalized through feature importance metrics and meta models (Jacovi and Goldberg, 202"
2021.acl-short.17,2020.emnlp-main.747,0,0.0299302,"ures learned by a model are identified, or as decision understanding tasks, where explanations for particular instances are produced. We consider the latter in this paper. Such tasks can be framed as generative, where models generate free text explanations (Camburu et al., 2018; Kotonya and Toni, 2020; Atanasova et al., 2020b), or as post-hoc interpretability methods, where salient portions of the input are highlighted (Lipton, 2018; DeYoung et al., 2020; Atanasova et al., 2020a). As there does not exist a clearly superior choice for framing decision understanding for NLP tasks (Miller, 2019; Carton et al., 2020; Jacovi and 123 Goldberg, 2021), we follow a substantial body of prior work in considering the post-hoc definition of interpretability based on local methods proposed by Lipton (2018). This definition is naturally operationalized through feature importance metrics and meta models (Jacovi and Goldberg, 2020). Further, we acknowledge the specific requirement that an interpretable model obeys some set of structural constraints of the domain in which it is used, such as monotonicity or physical constraints (Rudin, 2019). For NLP tasks such as sentiment analysis or topic classification, such const"
2021.acl-short.17,2020.lrec-1.220,0,0.1378,"of a recurrent network or contextual embeddings of a Transformer (see Fig. 1). Importantly, these internal representations do not solely encode information from the input token they are co-indexed with (Salehinejad et al., 2017; Brunner et al., 2020), but rather from a range of inputs. This presents the question: if internal representations themselves may not be interpretable, can we actually deduce anything from “interpretable” attention weights? We build on the recent line of work challenging the validity of attention-as-explanation methods (Jain and Wallace, 2019; Serrano and Smith, 2019; Grimsley et al., 2020, inter alia) and specifically examine how sparsity affects their observations. To this end, we introduce a novel entropy-based metric to measure the dispersion of inputs’ influence, rather than just their magnitudes. Through experiments on three text classification tasks, utilizing both LSTM and Transformer-based models, we observe how sparse attention affects the results of Jain and Wallace (2019) and Wiegreffe and Pinter (2019), additionally exploring whether it allows us to identify a core set of inputs that are important to models’ decisions. We find we are unable to identify such a set w"
2021.acl-short.17,2020.acl-main.386,0,0.071391,"; Atanasova et al., 2020b), or as post-hoc interpretability methods, where salient portions of the input are highlighted (Lipton, 2018; DeYoung et al., 2020; Atanasova et al., 2020a). As there does not exist a clearly superior choice for framing decision understanding for NLP tasks (Miller, 2019; Carton et al., 2020; Jacovi and 123 Goldberg, 2021), we follow a substantial body of prior work in considering the post-hoc definition of interpretability based on local methods proposed by Lipton (2018). This definition is naturally operationalized through feature importance metrics and meta models (Jacovi and Goldberg, 2020). Further, we acknowledge the specific requirement that an interpretable model obeys some set of structural constraints of the domain in which it is used, such as monotonicity or physical constraints (Rudin, 2019). For NLP tasks such as sentiment analysis or topic classification, such constraints may logically include the utilization of only a few key words in the input when making a decision, in which case, knowing the magnitude of the influence each input token has on a model’s prediction through, e.g., feature importance metrics, may suffice to verify the model obeys such constraints. While"
2021.acl-short.17,N19-1357,0,0.34537,"ion internal to the model, e.g. the hidden states of a recurrent network or contextual embeddings of a Transformer (see Fig. 1). Importantly, these internal representations do not solely encode information from the input token they are co-indexed with (Salehinejad et al., 2017; Brunner et al., 2020), but rather from a range of inputs. This presents the question: if internal representations themselves may not be interpretable, can we actually deduce anything from “interpretable” attention weights? We build on the recent line of work challenging the validity of attention-as-explanation methods (Jain and Wallace, 2019; Serrano and Smith, 2019; Grimsley et al., 2020, inter alia) and specifically examine how sparsity affects their observations. To this end, we introduce a novel entropy-based metric to measure the dispersion of inputs’ influence, rather than just their magnitudes. Through experiments on three text classification tasks, utilizing both LSTM and Transformer-based models, we observe how sparse attention affects the results of Jain and Wallace (2019) and Wiegreffe and Pinter (2019), additionally exploring whether it allows us to identify a core set of inputs that are important to models’ decisions"
2021.acl-short.17,2020.coling-main.474,0,0.0285215,"(3) where the degree of sparsity can be tuned via the hyperparameter 2 ( 1, 1); a larger encourages more sparsity in the minimizing solution. 3 Model Interpretability Model interpretability and explainability have been framed in different ways (Gehrmann et al., 2019)— as model understanding tasks, where (spurious) features learned by a model are identified, or as decision understanding tasks, where explanations for particular instances are produced. We consider the latter in this paper. Such tasks can be framed as generative, where models generate free text explanations (Camburu et al., 2018; Kotonya and Toni, 2020; Atanasova et al., 2020b), or as post-hoc interpretability methods, where salient portions of the input are highlighted (Lipton, 2018; DeYoung et al., 2020; Atanasova et al., 2020a). As there does not exist a clearly superior choice for framing decision understanding for NLP tasks (Miller, 2019; Carton et al., 2020; Jacovi and 123 Goldberg, 2021), we follow a substantial body of prior work in considering the post-hoc definition of interpretability based on local methods proposed by Lipton (2018). This definition is naturally operationalized through feature importance metrics and meta models ("
2021.acl-short.17,P18-2059,0,0.0550343,"Missing"
2021.acl-short.17,N18-1100,0,0.0232581,"natural language processing (NLP) is becoming increasingly important as complex models are applied to more and more downstream decision making tasks. In light of this, many researchers have turned to the attention mechanism, which has not only led to impressive performance improvements in neural models, but has also been claimed to offer insights into how models make decisions. Specifically, a number of works imply or directly state that one may inspect the attention distribution to determine the amount of influence each input token has in a model’s decision-making process (Xie et al., 2017; Mullenbach et al., 2018; Niculae et al., 2018, inter alia). Many lines of work have gone on to exploit this assumption when building their own “interpretable” models or analysis tools (Yang et al., 2016; Tu et al., 2016; De-Arteaga et al., 2019); one subset has even tried to make models with attention more interpretable by inducing sparsity—a common attribute of interpretable models (Lipton, 2018; Rudin, 2019)—in attention weights, with the motivation that this allows model decisions to be mapped to a limited number of items (Martins and Astudillo, 2016; Malaviya et al., 2018; Zhang et al., 2019). Yet, there lacks c"
2021.acl-short.17,P18-1032,0,0.0196337,"Missing"
2021.acl-short.17,2020.acl-main.432,0,0.0410922,"on may simply lead to the dispersion of information to different intermediate representations, a behavior similar to that seen when constraining attention for divergence from another distribution, i.e., in the adversarial experiments of Wiegreffe and Pinter (2019) compared to those of Jain and Wallace (2019). 5 unique optimal attention weights.3 Subsequently, other studies arrived at similar results: Grimsley et al. (2020) found evidence that causal explanations are not attainable from attention layers over text data; Jacovi and Goldberg (2020) explored the faithfulness of attention heatmaps; Pruthi et al. (2020) showed that attention masks can be trained to give deceptive explanations. We view this work as another such study, exploring attention’s innate interpretability on a different axis. Further, this work fits into the context of a larger body of interpretability research in NLP, which has challenged the informal use of terms such as faithfulness, plausibility, and explainability (Lipton, 2018; Arrieta et al., 2020; Jacovi and Goldberg, 2021, inter alia) and tried to quantify the reliability of current definitions (Atanasova et al., 2020a). While we consider their findings in our experimental de"
2021.acl-short.17,P19-1282,0,0.0567097,"l, e.g. the hidden states of a recurrent network or contextual embeddings of a Transformer (see Fig. 1). Importantly, these internal representations do not solely encode information from the input token they are co-indexed with (Salehinejad et al., 2017; Brunner et al., 2020), but rather from a range of inputs. This presents the question: if internal representations themselves may not be interpretable, can we actually deduce anything from “interpretable” attention weights? We build on the recent line of work challenging the validity of attention-as-explanation methods (Jain and Wallace, 2019; Serrano and Smith, 2019; Grimsley et al., 2020, inter alia) and specifically examine how sparsity affects their observations. To this end, we introduce a novel entropy-based metric to measure the dispersion of inputs’ influence, rather than just their magnitudes. Through experiments on three text classification tasks, utilizing both LSTM and Transformer-based models, we observe how sparse attention affects the results of Jain and Wallace (2019) and Wiegreffe and Pinter (2019), additionally exploring whether it allows us to identify a core set of inputs that are important to models’ decisions. We find we are unable t"
2021.acl-short.17,P16-1008,0,0.0284336,"attention mechanism, which has not only led to impressive performance improvements in neural models, but has also been claimed to offer insights into how models make decisions. Specifically, a number of works imply or directly state that one may inspect the attention distribution to determine the amount of influence each input token has in a model’s decision-making process (Xie et al., 2017; Mullenbach et al., 2018; Niculae et al., 2018, inter alia). Many lines of work have gone on to exploit this assumption when building their own “interpretable” models or analysis tools (Yang et al., 2016; Tu et al., 2016; De-Arteaga et al., 2019); one subset has even tried to make models with attention more interpretable by inducing sparsity—a common attribute of interpretable models (Lipton, 2018; Rudin, 2019)—in attention weights, with the motivation that this allows model decisions to be mapped to a limited number of items (Martins and Astudillo, 2016; Malaviya et al., 2018; Zhang et al., 2019). Yet, there lacks concrete reasoning or evidence that sparse attention weights leads to more interpretable models: customarily, attention is not directly over the model’s inputs, but rather over some representation"
2021.acl-short.17,2020.repl4nlp-1.17,0,0.0187892,"k | Inputs and Intermediate Representations are not Interchangeable. We first explore how strongly-related inputs are to their co-indexed intermediate representations. A strong relationship on its own may validate the use of sparse attention, as the ability to identify a subset of influential intermediate representations would then directly translate to a set of influential inputs. Previous works show that the “contribution” of a token xi to its intermediate representation hi is often quite low for various model architectures (Salehinejad et al., 2017; Ming et al., 2017; Brunner et al., 2020; Tutek and Snajder, 2020). In the context of attention, we find this property to be evinced by the adversarial experiments of Wiegreffe and Pinter (2019) (§4) and Jain and Wallace (2019) (§4), which we verify in App. C. They construct adversarial attention distributions by optimizing for divergence from a baseline 124 Figure 2: Entropy of gradient-based gyˆ(x) and LOO Dyˆ(x) FI distributions. Results are from models with full spectrum of projection functions. Figure 1: Correlation between the attention distribution and gradient-based FI measures. We see a notably stronger correlation between attention and FI of interm"
2021.acl-short.17,D19-1002,0,0.219077,"able” attention weights? We build on the recent line of work challenging the validity of attention-as-explanation methods (Jain and Wallace, 2019; Serrano and Smith, 2019; Grimsley et al., 2020, inter alia) and specifically examine how sparsity affects their observations. To this end, we introduce a novel entropy-based metric to measure the dispersion of inputs’ influence, rather than just their magnitudes. Through experiments on three text classification tasks, utilizing both LSTM and Transformer-based models, we observe how sparse attention affects the results of Jain and Wallace (2019) and Wiegreffe and Pinter (2019), additionally exploring whether it allows us to identify a core set of inputs that are important to models’ decisions. We find we are unable to identify such a set when using sparse attention; rather, it appears that encouraging sparsity may simultaneously encourage a higher degree of 122 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Short Papers), pages 122–129 August 1–6, 2021. ©2021 Association for Computational Linguistics contextualization in intermediate representations."
2021.acl-short.17,P17-1088,0,0.0286273,"bility research in natural language processing (NLP) is becoming increasingly important as complex models are applied to more and more downstream decision making tasks. In light of this, many researchers have turned to the attention mechanism, which has not only led to impressive performance improvements in neural models, but has also been claimed to offer insights into how models make decisions. Specifically, a number of works imply or directly state that one may inspect the attention distribution to determine the amount of influence each input token has in a model’s decision-making process (Xie et al., 2017; Mullenbach et al., 2018; Niculae et al., 2018, inter alia). Many lines of work have gone on to exploit this assumption when building their own “interpretable” models or analysis tools (Yang et al., 2016; Tu et al., 2016; De-Arteaga et al., 2019); one subset has even tried to make models with attention more interpretable by inducing sparsity—a common attribute of interpretable models (Lipton, 2018; Rudin, 2019)—in attention weights, with the motivation that this allows model decisions to be mapped to a limited number of items (Martins and Astudillo, 2016; Malaviya et al., 2018; Zhang et al.,"
2021.acl-short.17,N16-1174,0,0.0366465,"have turned to the attention mechanism, which has not only led to impressive performance improvements in neural models, but has also been claimed to offer insights into how models make decisions. Specifically, a number of works imply or directly state that one may inspect the attention distribution to determine the amount of influence each input token has in a model’s decision-making process (Xie et al., 2017; Mullenbach et al., 2018; Niculae et al., 2018, inter alia). Many lines of work have gone on to exploit this assumption when building their own “interpretable” models or analysis tools (Yang et al., 2016; Tu et al., 2016; De-Arteaga et al., 2019); one subset has even tried to make models with attention more interpretable by inducing sparsity—a common attribute of interpretable models (Lipton, 2018; Rudin, 2019)—in attention weights, with the motivation that this allows model decisions to be mapped to a limited number of items (Martins and Astudillo, 2016; Malaviya et al., 2018; Zhang et al., 2019). Yet, there lacks concrete reasoning or evidence that sparse attention weights leads to more interpretable models: customarily, attention is not directly over the model’s inputs, but rather over som"
2021.acl-short.32,Q15-1031,1,0.825957,"A2 N 4 ) time where A is the alphabet size and N is the number of states. Our algorithm is significantly faster than prior algorithms. Additionally, our approach leads to a significantly faster algorithm for computing second-order expectations, such as covariance matrices and gradients of first-order expectations. 1 Introduction Weighted finite-state machines (WFSMs) have a storied role in NLP. They are a useful formalism for speech recognition (Mohri et al., 2002), machine transliteration (Knight and Graehl, 1998), morphology (Geyken and Hanneforth, 2005; Lind´en et al., 2009) and phonology (Cotterell et al., 2015) inter alia. Indeed, WFSMs have been “neuralized” (Rastogi et al., 2016; Hannun et al., 2020; Schwartz et al., 2018) and are still of practical use to the NLP modeler. Moreover, many popular sequence models, e.g., conditional random fields for part-ofspeech tagging (Lafferty et al., 2001), are naturally viewed as special cases of WFSMs. For this reason, we consider the study of algorithms for the WFSMs of interest in se for the NLP community. This paper considers inference algorithms for WSFMs. When WFSMs are acyclic, there exist simple linear-time dynamic programs, e.g., the forward algorithm"
2021.acl-short.32,D09-1005,0,0.399164,"iterature beyond a generalpurpose method from automatic differentiation. In contrast to many presentations of WFSMs (Mohri, 1997), our work provides a purely linear-algebraic take on them. And, indeed, it is this connection that allows us to develop our general algorithm. We provide a thorough analysis of the soundness, runtime, and space complexity of our algorithm. In the special case of second-order derivatives, our algorithm runs optimally in O(A2 N 4 ) time and space where A is the size of the alphabet, and N is the number of states.1 In contrast, the second-order expectation semiring of Li and Eisner (2009) provides an O(A2 N 7 ) solution and automatic differentiation (Griewank, 1989) yields a slightly faster O(AN 5+A2 N 4 ) solution. Additionally, we provide a speed-up for the general family of second-order expectations. Indeed, we believe our algorithm is the fastest known for computing common quantities, e.g., a covariance matrix.2 2 Weighted Finite-State Machines In this section we briefly provide important notation for WFSMs and a classic result that efficiently finds the normalization constant for the probability distribution of a WFSM. 1 Our implementation is available at https://github."
2021.acl-short.32,J97-2003,0,0.591039,"s inference algorithms for WSFMs. When WFSMs are acyclic, there exist simple linear-time dynamic programs, e.g., the forward algorithm (Rabiner, 1989), for inference. However, in general, WFSMs may contain cycles and such approaches are not applicable. Our work considers this general case and provides a method for efficient computation of mth -order derivatives over a cyclic WFSM. To the best of our knowledge, no algorithm for higher-order derivatives has been presented in the literature beyond a generalpurpose method from automatic differentiation. In contrast to many presentations of WFSMs (Mohri, 1997), our work provides a purely linear-algebraic take on them. And, indeed, it is this connection that allows us to develop our general algorithm. We provide a thorough analysis of the soundness, runtime, and space complexity of our algorithm. In the special case of second-order derivatives, our algorithm runs optimally in O(A2 N 4 ) time and space where A is the size of the alphabet, and N is the number of states.1 In contrast, the second-order expectation semiring of Li and Eisner (2009) provides an O(A2 N 7 ) solution and automatic differentiation (Griewank, 1989) yields a slightly faster O(AN"
2021.acl-short.32,N16-1076,1,0.833757,"Our algorithm is significantly faster than prior algorithms. Additionally, our approach leads to a significantly faster algorithm for computing second-order expectations, such as covariance matrices and gradients of first-order expectations. 1 Introduction Weighted finite-state machines (WFSMs) have a storied role in NLP. They are a useful formalism for speech recognition (Mohri et al., 2002), machine transliteration (Knight and Graehl, 1998), morphology (Geyken and Hanneforth, 2005; Lind´en et al., 2009) and phonology (Cotterell et al., 2015) inter alia. Indeed, WFSMs have been “neuralized” (Rastogi et al., 2016; Hannun et al., 2020; Schwartz et al., 2018) and are still of practical use to the NLP modeler. Moreover, many popular sequence models, e.g., conditional random fields for part-ofspeech tagging (Lafferty et al., 2001), are naturally viewed as special cases of WFSMs. For this reason, we consider the study of algorithms for the WFSMs of interest in se for the NLP community. This paper considers inference algorithms for WSFMs. When WFSMs are acyclic, there exist simple linear-time dynamic programs, e.g., the forward algorithm (Rabiner, 1989), for inference. However, in general, WFSMs may contain"
2021.eacl-main.118,2020.coling-main.398,0,0.0781379,"dy or beam search (Reddy, 1977), since performing exact search can be computationally expensive, if not impossible.2 While for a deterministic task, greedy search is optimal under a Bayes optimal model,3 most text generation tasks benefit from using beam search. However, text quality almost invariably decreases for beam sizes larger than k = 5. This phenomenon is sometimes referred to as the beam search curse, and has been investigated in detail by a number of scholarly works (Koehn and Knowles, 2017; Murray and Chiang, 2018; Yang et al., 2018; Stahlberg and Byrne, 2019; Cohen and Beck, 2019; Eikema and Aziz, 2020). 1 While there are cases where there exist multiple inflected forms of a lemma, e.g., in English the past tense of dream can be realized as either dreamed or dreamt, these cases (termed “overabundance”) are rare (Thornton, 2019). 2 The search space is exponential in the sequence length and due to the non-Markov nature of (typical) neural transducers, dynamic-programming techniques are not helpful. 3 Under such a model, the correct token yi at time step i will be assigned all probability mass. 1389 Overall Low-resource High-resource k=1 Transformer k = 10 k = 100 Dijkstra k=1 HMM k = 10 k = 10"
2021.eacl-main.118,N16-1077,0,0.105004,"and Chiang, 2018; Yang et al., 2018; Cohen and Beck, 2019). Further, in the context of NMT, it has been shown that the empty string is frequently the most-probable solution under the model (Stahlberg and Byrne, 2019). Some suggest this is a manifestation of the general inadequacy of neural models for language generation tasks (Koehn and Knowles, 2017; Kumar and Sarawagi, 2019; Holtzman et al., 2020; Stahlberg, 2020); in this work, we find evidence demonstrating otherwise. Sequence-to-sequence transducers for characterlevel tasks often follow the architectures of their word-level counterparts (Faruqui et al., 2016; Lee et al., 2017), and have likewise achieved state-of-the-art performance on e.g., morphological inflection generation (Wu et al., 2020) and grapheme-to-phoneme conversion (Yolchuyeva et al., 2019). Given prior findings, we might expect to see the same degenerate behavior in these models—however, we do not. We run a series of experiments on morphological inflection (MI) generators to explore whether neural transducers for this task are similarly poorly calibrated, i.e. are far from the true distribution p(y |x). We evaluate the performance of two character-level sequenceto-sequence transduc"
2021.eacl-main.118,L18-1293,1,0.799649,"Missing"
2021.eacl-main.118,W17-3204,0,0.0842234,"ewis et al., 2019). Yet, an undesirable property of these models has been repeatedly observed in word-level tasks: When using beam search as the decoding strategy, increasing the beam width beyond a size of k = 5 often leads to a drop in the quality of solutions (Murray and Chiang, 2018; Yang et al., 2018; Cohen and Beck, 2019). Further, in the context of NMT, it has been shown that the empty string is frequently the most-probable solution under the model (Stahlberg and Byrne, 2019). Some suggest this is a manifestation of the general inadequacy of neural models for language generation tasks (Koehn and Knowles, 2017; Kumar and Sarawagi, 2019; Holtzman et al., 2020; Stahlberg, 2020); in this work, we find evidence demonstrating otherwise. Sequence-to-sequence transducers for characterlevel tasks often follow the architectures of their word-level counterparts (Faruqui et al., 2016; Lee et al., 2017), and have likewise achieved state-of-the-art performance on e.g., morphological inflection generation (Wu et al., 2020) and grapheme-to-phoneme conversion (Yolchuyeva et al., 2019). Given prior findings, we might expect to see the same degenerate behavior in these models—however, we do not. We run a series of e"
2021.eacl-main.118,Q17-1026,0,0.0330683,"et al., 2018; Cohen and Beck, 2019). Further, in the context of NMT, it has been shown that the empty string is frequently the most-probable solution under the model (Stahlberg and Byrne, 2019). Some suggest this is a manifestation of the general inadequacy of neural models for language generation tasks (Koehn and Knowles, 2017; Kumar and Sarawagi, 2019; Holtzman et al., 2020; Stahlberg, 2020); in this work, we find evidence demonstrating otherwise. Sequence-to-sequence transducers for characterlevel tasks often follow the architectures of their word-level counterparts (Faruqui et al., 2016; Lee et al., 2017), and have likewise achieved state-of-the-art performance on e.g., morphological inflection generation (Wu et al., 2020) and grapheme-to-phoneme conversion (Yolchuyeva et al., 2019). Given prior findings, we might expect to see the same degenerate behavior in these models—however, we do not. We run a series of experiments on morphological inflection (MI) generators to explore whether neural transducers for this task are similarly poorly calibrated, i.e. are far from the true distribution p(y |x). We evaluate the performance of two character-level sequenceto-sequence transducers using different"
2021.eacl-main.118,2020.tacl-1.51,1,0.715886,"an be computationally expensive, we can employ efficient search strategies due to some properties of pθ . Specifically, from Eq. (1), we can see that the scoring function for sequences y is monotonically decreasing in t. We can therefore find the provably optimal solution with Dijkstra’s algorithm (Dijkstra, 1959), which terminates and returns the global optimum the first time it encounters an EOS. Additionally, to prevent a large memory footprint, we can lower-bound the search using any complete hypothesis, e.g., the empty string or a solution found by beam search (Stahlberg and Byrne, 2019; Meister et al., 2020). That is, we can prematurely stop exploring solutions whose scores become less than these hypotheses at any point in time. Although exact search is an exponential-time method in this setting, we see that, in practice, it terminates quickly due to the peakiness of pθ (see App. A). While the effects of exact decoding and beam search decoding with large beam widths have been explored for a number of word-level tasks (Stahlberg and Byrne, 2019; Cohen and Beck, 2019; Eikema and Aziz, 2020), to the best of our knowledge, they have not yet been explored for any character-level sequence-to-sequence t"
2021.eacl-main.118,W18-6322,0,0.0777679,"(k). MI results are averaged across languages. Introduction Neural sequence-to-sequence models are omnipresent in the field of natural language processing due to their impressive performance. They hold state of the art on a myriad of tasks, e.g., neural machine translation (NMT; Ott et al., 2018b) and abstractive summarization (AS; Lewis et al., 2019). Yet, an undesirable property of these models has been repeatedly observed in word-level tasks: When using beam search as the decoding strategy, increasing the beam width beyond a size of k = 5 often leads to a drop in the quality of solutions (Murray and Chiang, 2018; Yang et al., 2018; Cohen and Beck, 2019). Further, in the context of NMT, it has been shown that the empty string is frequently the most-probable solution under the model (Stahlberg and Byrne, 2019). Some suggest this is a manifestation of the general inadequacy of neural models for language generation tasks (Koehn and Knowles, 2017; Kumar and Sarawagi, 2019; Holtzman et al., 2020; Stahlberg, 2020); in this work, we find evidence demonstrating otherwise. Sequence-to-sequence transducers for characterlevel tasks often follow the architectures of their word-level counterparts (Faruqui et al.,"
2021.eacl-main.118,W18-6301,0,0.0486667,"Missing"
2021.eacl-main.118,W19-4207,0,0.0509727,"Missing"
2021.eacl-main.118,2020.eamt-1.1,0,0.0237003,"repeatedly observed in word-level tasks: When using beam search as the decoding strategy, increasing the beam width beyond a size of k = 5 often leads to a drop in the quality of solutions (Murray and Chiang, 2018; Yang et al., 2018; Cohen and Beck, 2019). Further, in the context of NMT, it has been shown that the empty string is frequently the most-probable solution under the model (Stahlberg and Byrne, 2019). Some suggest this is a manifestation of the general inadequacy of neural models for language generation tasks (Koehn and Knowles, 2017; Kumar and Sarawagi, 2019; Holtzman et al., 2020; Stahlberg, 2020); in this work, we find evidence demonstrating otherwise. Sequence-to-sequence transducers for characterlevel tasks often follow the architectures of their word-level counterparts (Faruqui et al., 2016; Lee et al., 2017), and have likewise achieved state-of-the-art performance on e.g., morphological inflection generation (Wu et al., 2020) and grapheme-to-phoneme conversion (Yolchuyeva et al., 2019). Given prior findings, we might expect to see the same degenerate behavior in these models—however, we do not. We run a series of experiments on morphological inflection (MI) generators to explore w"
2021.eacl-main.118,D19-1331,0,0.216578,"hold state of the art on a myriad of tasks, e.g., neural machine translation (NMT; Ott et al., 2018b) and abstractive summarization (AS; Lewis et al., 2019). Yet, an undesirable property of these models has been repeatedly observed in word-level tasks: When using beam search as the decoding strategy, increasing the beam width beyond a size of k = 5 often leads to a drop in the quality of solutions (Murray and Chiang, 2018; Yang et al., 2018; Cohen and Beck, 2019). Further, in the context of NMT, it has been shown that the empty string is frequently the most-probable solution under the model (Stahlberg and Byrne, 2019). Some suggest this is a manifestation of the general inadequacy of neural models for language generation tasks (Koehn and Knowles, 2017; Kumar and Sarawagi, 2019; Holtzman et al., 2020; Stahlberg, 2020); in this work, we find evidence demonstrating otherwise. Sequence-to-sequence transducers for characterlevel tasks often follow the architectures of their word-level counterparts (Faruqui et al., 2016; Lee et al., 2017), and have likewise achieved state-of-the-art performance on e.g., morphological inflection generation (Wu et al., 2020) and grapheme-to-phoneme conversion (Yolchuyeva et al., 2"
2021.eacl-main.118,2021.eacl-main.163,1,0.807019,"Missing"
2021.eacl-main.118,D18-1342,0,0.0385877,"Missing"
2021.eacl-main.118,D17-2005,0,0.060858,"Missing"
2021.eacl-main.118,P19-1148,1,0.76358,"y estimates are representative of the true likelihood that a solution y is correct. Morphological Inflection. In the task of morphological inflection, x is an encoding of the lemma concatenated with a flattened morphosyntactic description (MSD) and y is the target inflection. As a concrete example, consider inflecting the German word Bruder into the genitive plural, as shown in Tab. 2. Then, x is the string hB r u d e r GEN PLi and y is the string hB r ¨ u d e ri. As this demonstrates, morphological inflection generation is, by its nature, modeled at the character level (Faruqui et al., 2016; Wu and Cotterell, 2019), i.e., our target vocabulary V is a set of characters in the language. Note that y ∈ V ∗ , but x 6∈ V ∗ due to the additional encoding of the MSD. This stands in contrast to NMT, which is typically performed on a (sub)word level, making the vocabulary size orders of magnitude larger. This problem is also known as maximum-aposteriori (MAP) inference. Decoding is often performed with a heuristic search method such as greedy or beam search (Reddy, 1977), since performing exact search can be computationally expensive, if not impossible.2 While for a deterministic task, greedy search is optimal un"
2021.eacl-main.163,D19-1091,0,0.274868,"Missing"
2021.eacl-main.163,W19-5301,0,0.048882,"Missing"
2021.eacl-main.163,K17-2002,0,0.146749,"Missing"
2021.eacl-main.163,P17-1183,0,0.0427771,"Missing"
2021.eacl-main.163,K18-3001,1,0.94323,"Missing"
2021.eacl-main.163,K17-2001,1,0.951744,"Missing"
2021.eacl-main.163,N19-1423,0,0.0323939,"gure 1: Development set accuracy for 5 languages on morphological inflection with different batch sizes. We evince our two primary contributions: (1) we set the new state of the art morphological inflection using the transformer and (2) we demonstrate the transformer’s dependence on the batch size. Introduction The transformer (Vaswani et al., 2017) has become a popular architecture for sequence-to-sequence transduction in NLP. It has achieved state-of-theart performance on a range of common word-level transduction tasks: neural machine translation (Barrault et al., 2019), question answering (Devlin et al., 2019) and abstractive summarization (Dong et al., 2019). In addition, the transformer forms the backbone of the widely-used BERT (Devlin et al., 2019). Yet for character-level transduction tasks like morphological inflection, the dominant model has remained a recurrent neural network-based sequenceCode will be available at https://github.com/ shijie-wu/neural-transducer. to-sequence model with attention (Cotterell et al., 2018). This is not for lack of effort—but rather, it is the case that the transformer has consistently underperformed in experiments on average (Tang et al., 2018b).1 As anecdotal"
2021.eacl-main.163,P19-1157,0,0.0491436,"Missing"
2021.eacl-main.163,D15-1166,0,0.113722,"a dropout of 0.3 yields a slightly better CERi . G2P and Transliteration. Tab. 4 shows that the transformer outperforms previously published strong recurrent models on two tasks despite having fewer parameters. A dropout rate of 0.3 yields 1904 significantly better performance on the transliteration task while a dropout rate of 0.1 is stronger on the g2p task. This shows that transformers can and do outperform recurrent transducers on common character-level tasks when properly tuned. 4 Related Work Character-level transduction is largely dominated by attention-based LSTM sequence-to-sequence (Luong et al., 2015) models (Cotterell et al., 2018). Character-level transduction tasks usually involve input-output pairs that share large substrings and alignments between these are often monotonic. Models that address the task tend to focus on exploiting such structural bias. Instead of learning the alignments, Aharoni and Goldberg (2017) use external monotonic alignments from the SIGMORPHON 2016 shared task baseline Cotterell et al. (2016). Makarov et al. (2017) use this approach to win the CoNLL-SIGMORPHON 2017 shared task on morphological inflection (Cotterell et al., 2017). Wu et al. (2018) shows that exp"
2021.eacl-main.163,W19-4226,1,0.884602,"uction tasks like morphological inflection, the dominant model has remained a recurrent neural network-based sequenceCode will be available at https://github.com/ shijie-wu/neural-transducer. to-sequence model with attention (Cotterell et al., 2018). This is not for lack of effort—but rather, it is the case that the transformer has consistently underperformed in experiments on average (Tang et al., 2018b).1 As anecdotal evidence of this, we note that in the 2019 SIGMORPHON shared task on cross-lingual transfer for morphological inflection, no participating system was based on the transformer (McCarthy et al., 2019). Character-level transduction models are often trained with less data than their word-level counterparts: In contrast to machine translation, where millions of training samples are available, the 2018 SIGMORPHON shared task (Cotterell et al., 2018) high-resource setting only provides ≈ 10k training examples per language. It is also not obvious that non-recurrent architectures such as the transformer 1 This claim is also based on the authors’ personal communication with other researchers in morphology in the corridors of conferences and through email. 1901 Proceedings of the 16th Conference of"
2021.eacl-main.163,K19-1014,1,0.909908,"Missing"
2021.eacl-main.163,W13-2409,0,0.0320945,"et al., 2017) with 52 languages. The performance is evaluated by accuracy (ACC) and edit distance (Dist). For the g2p task, we use the unstressed CMUDict (Weide, 1998) and NETtalk (Sejnowski and Rosenberg, 1987) resources. We use the splits from Wu et al. (2018). We evaluate under word error rate (WER) and phoneme error rate (PER). For transliteration, we use the NEWS 2015 shared task data (Zhang et al., 2015).4 For historical text normalization, we follow Bollmann (2019) and use datasets for Spanish (S´anchez-Mart´ınez et al., 2013), Icelandic and Swedish (Pettersson et al., 2013), Slovene (Scherrer and Erjavec, 2013, 2016; Ljubeˇsic et al., 2016), Hungarian and German (Pettersson, 2016).5 We evaluate using accuracy (ACC) and character error rate of incorrect prediction (CERi ). Optimization. We use Adam (Kingma and Ba, 2014) with a learning rate of 0.001 and an inverse 3 While the features could be encoded with a binary vector followed by MLP, it introduces a representation bottleneck for encoding features. 4 We do not have access to the test set. 5 We do not include English due to licensing issues. Figure 3: Distribution of incorrectly inflected forms in the test set of the inflection task over all 52 l"
2021.eacl-main.163,K17-2010,0,0.324347,"Missing"
2021.eacl-main.163,C18-1112,0,0.0162749,"on answering (Devlin et al., 2019) and abstractive summarization (Dong et al., 2019). In addition, the transformer forms the backbone of the widely-used BERT (Devlin et al., 2019). Yet for character-level transduction tasks like morphological inflection, the dominant model has remained a recurrent neural network-based sequenceCode will be available at https://github.com/ shijie-wu/neural-transducer. to-sequence model with attention (Cotterell et al., 2018). This is not for lack of effort—but rather, it is the case that the transformer has consistently underperformed in experiments on average (Tang et al., 2018b).1 As anecdotal evidence of this, we note that in the 2019 SIGMORPHON shared task on cross-lingual transfer for morphological inflection, no participating system was based on the transformer (McCarthy et al., 2019). Character-level transduction models are often trained with less data than their word-level counterparts: In contrast to machine translation, where millions of training samples are available, the 2018 SIGMORPHON shared task (Cotterell et al., 2018) high-resource setting only provides ≈ 10k training examples per language. It is also not obvious that non-recurrent architectures such"
2021.eacl-main.163,D18-1458,0,0.0513814,"Missing"
2021.eacl-main.163,P19-1148,1,0.942093,"nce of the transformer on character-level tasks, and we show that with a large enough batch size, the transformer does indeed outperform recurrent models. We also introduce a simple technique to handle feature-guided character-level transduction that further improves performance. With these insights, we achieve state-of-the-art performance on morphological inflection and historical text normalization. We also show that the transformer outperforms a strong baseline on two other character-level transduction tasks: grapheme-to-phoneme conversion and transliteration. 1 86 ACC 84 82 80 78 76 16 32 Wu and Cotterell (2019) Wu and Cotterell (2019) (Our Eval) Wu and Cotterell (2019) + LR Warmup Vanilla Transformer Feature Invariant Transformer 64 128 256 512 Batch Size Figure 1: Development set accuracy for 5 languages on morphological inflection with different batch sizes. We evince our two primary contributions: (1) we set the new state of the art morphological inflection using the transformer and (2) we demonstrate the transformer’s dependence on the batch size. Introduction The transformer (Vaswani et al., 2017) has become a popular architecture for sequence-to-sequence transduction in NLP. It has achieved st"
2021.eacl-main.163,D18-1473,1,0.886594,"Missing"
2021.eacl-main.3,C65-1001,0,0.231066,"Missing"
2021.eacl-main.3,2020.tacl-1.1,1,0.622989,"recognition. In this paper, we point out the confounds in existing methods for comparing the informativeness of segments early in the word versus later in the word, and present several new measures that avoid these confounds. When controlling for these confounds, we still find evidence across hundreds of languages that indeed there is a cross-linguistic tendency to front-load information in words.1 1 celex wikipedia northeuralex 4 2 Backward 0 2 4 6 8 4 2 0 Position from Start celex wikipedia northeuralex 4 2 8 6 Position from End Figure 1: Forward and Backward Surprisals with LSTM model from Pimentel et al. (2020). The bottom plot has been flipped horizontally such that it visually corresponds to the normal string direction. evidence of increased levels of phonological reduction in word endings (van Son and Pols, 2003b). To analyse this front-loading effect, researchers have investigated the information provided by segments in words. van Son and Pols (2003a,b) showed that, in Dutch, a segment’s position in a word is a very strong predictor of its conditional surprisal, with later segments being more predictable than earlier ones—a result which we show to arise directly from its definition in §3.3.1. Re"
2021.eacl-main.3,P82-1020,0,0.796646,"Missing"
2021.emnlp-main.229,2021.findings-emnlp.71,0,0.036081,"Missing"
2021.emnlp-main.229,2021.ccl-1.108,0,0.0422911,"Missing"
2021.emnlp-main.229,2021.acl-long.70,0,0.0306159,"Missing"
2021.emnlp-main.229,P19-1171,1,0.834497,"ariables X and Y , both with c classes and uniformly distributed. 1 1 p(x) = , p(x |y) = c c (12) We further assume a Bayesian agent with two categorical beliefs {pθ (x) = Cat(θ), pθ (x | y) = Cat(θ + y)}—where y is assumed to be encoded as a one hot vector—and a Dirichlet prior 4 We put this in contrast to eq. (5)—which takes this expectation over the belief itself—since the instances are in practice encountered with this true frequency. This distinction has been explicitly noted before, by e.g. Bartlett (1953). 5 Cross mutual information (XMI) has been used in several previous work such as (Pimentel et al., 2019, 2020b; Bugliarello et al., 2020; McAllester and Stratos, 2020; Torroba Hennigen et al., 2020; Fernandes et al., 2021; O’Connor and Andreas, 2021). In those works, though, it was usually interpreted as a computational approximation to the truth-MI (or to V-information (Xu et al., 2020), which is discussed later in the paper). In this work, we highlight the Bayesian MI’s (and XMI’s) relevance as a generalisation of Shannon’s MI. 2871 pθ (θ) = Dir(α) with concentration parameters α = 1. Note that this (biased) agent believes that y and x are more likely than chance to share a class. Given no da"
2021.emnlp-main.229,2020.emnlp-main.254,1,0.863473,"our paper formulates an information-theoretic framework that is compatible both with model misspecification and the finite data assumption. In his seminal work, Shannon (1948) occupied himself with the limit of communication. Indeed, mutual information can be described as the theoretical limit (or upper-bound) of how much information can be extracted from one random variable about another. However, this limit is only achievable when one has full knowledge of these random 1 Introduction variables, including the true probability distribution according to which they are distributed. In practice, Pimentel et al. (2020b) recently undertook an information-theoretic analysis of probing. They ar- we will not have access to such information and it may be difficult to approximate. It follows that gue that probing may be viewed as approximating the mutual information between a linguistic prop- any system with imperfect knowledge of the random variable’s true distribution will only be able erty (e.g., part-of-speech tags) and a contextual representation (e.g., BERT). Counter-intuitively, to extract a subset of this information. however, due to the data-processing inequality, With this in mind, we propose and motiv"
2021.emnlp-main.229,2020.acl-main.420,1,0.915036,"our paper formulates an information-theoretic framework that is compatible both with model misspecification and the finite data assumption. In his seminal work, Shannon (1948) occupied himself with the limit of communication. Indeed, mutual information can be described as the theoretical limit (or upper-bound) of how much information can be extracted from one random variable about another. However, this limit is only achievable when one has full knowledge of these random 1 Introduction variables, including the true probability distribution according to which they are distributed. In practice, Pimentel et al. (2020b) recently undertook an information-theoretic analysis of probing. They ar- we will not have access to such information and it may be difficult to approximate. It follows that gue that probing may be viewed as approximating the mutual information between a linguistic prop- any system with imperfect knowledge of the random variable’s true distribution will only be able erty (e.g., part-of-speech tags) and a contextual representation (e.g., BERT). Counter-intuitively, to extract a subset of this information. however, due to the data-processing inequality, With this in mind, we propose and motiv"
2021.emnlp-main.229,2020.tacl-1.48,0,0.0820355,"Missing"
2021.emnlp-main.52,D18-1045,0,0.0479253,"Missing"
2021.emnlp-main.52,2020.coling-main.398,0,0.0230579,"tistical Estimators for Language Generation Models Estimators have a large number of applications in machine learning. For example, the REINFORCE algorithm (Williams, 1992) constructs an estimator for the value of the score function; minimum-Bayes risk decoding (Kumar and Byrne, 2004) uses an estimate of risk in its optimization problem. In this section, we compare estimators for sentence-level BLEU score and conditional model entropy for NMT models. Notably, NMT models that are trained to minimize cross-entropy with the empirical distribution12 are not peaky distributions (Ott et al., 2018a; Eikema and Aziz, 2020); thus, standard estimation techniques, e.g., Monte Carlo, should generally provide good results. However, we can vary the annealing parameter of our model in order to observe the behavior of our estimator with both high- and low-entropy distributions, making this a comprehensive case study. Here the annealed model distribution is computed as t 5 1 pτ (yt |y&lt;t ) ∝ p (yt |y&lt;t ) τ where we should expect a standard Monte Carlo estimator to provide good results at τ close to 1 when p is naturally high entropy. We test our estimator in this setting so as to give a comparison in a competitive settin"
2021.emnlp-main.52,P16-1159,0,0.0825003,"Missing"
2021.emnlp-main.613,W17-2709,0,0.0223791,"Missing"
2021.emnlp-main.613,2020.findings-emnlp.211,0,0.0323516,"Missing"
2021.emnlp-main.613,N19-1167,0,0.0304035,"Missing"
2021.emnlp-main.613,D12-1006,0,0.090199,"Missing"
2021.emnlp-main.613,W12-4102,0,0.0978953,"Missing"
2021.emnlp-main.613,N16-1180,0,0.0301825,"). Structural balance thefeatures we extracted from Wikipedia correlate, to ory (Heider, 1946; Cartwright and Harary, 1956) some extent, with whether a pair of entities are has been extended to status theory (Leskovec et al., allies or enemies. Indeed, some preliminary ex- 2010) and studied in online discussions by comperiments show that the tf-idf representations of bining signed graphs with sentiment analysis (Hasarticles and sections are more similar among allies san et al., 2012a,b). Friend and enemy relations than enemies (for an example, see Fig. 6 where the have been studied in novels (Iyyer et al., 2016; 7781 Gender equality Society Efforts Area of opportunity Social factors Health Education Demographics Education Largest cities in Mali Demographics Society Health Attacks Iraq Late 1990s September 11 attacks Europe 1992 States operations AfterUnited May 2011 Leadership Philosophy Science and technology Sport Culture Allegations of Financing Qatari support Command structure Organization Religion War on Terror Fatwas Law Ideology Contemporary period (1914 present) Languages Revolutionary France (1789 Language Strategy Alleged CIA1799) involvement Cinema Sports Music Music History HighAgricultu"
2021.emnlp-main.613,W17-2705,0,0.0252463,"pedia article is not simply using superficial linguistic cues to regurgitate these relationships? After all, if this is all that a model does, we could hardly attribute its success to whether it is dyadic or systemic! Therefore, for a fair comparison, we need to ensure that our data does not contain explicit mentions of such relationships.8 The second challenge is that we must not inadvertently provide more information to our models than the information in our features (§2). For example, pre-trained representations have been shown to encode a plethora of information that may skew predictions (Kutuzov et al., 2017; Petroni et al., 2019; Bouraoui et al., 2020). Hence, for our exper7 as of 14 September 2021 We believe that a model that exploits explicit mentions of ally–enemy relationships in the text is better analyzed through the lens of information extraction and machine reading. 8 6 Most graph processing is done with the help of the networkX library (Hagberg et al., 2008). Niger 7778 Conflict Top 10 unigrams Mali War soldier, town, attack, troop, rebel, group, city, conflict, northern, hostage Mali woman, country, coup, population, president, align, region, control, popular, rate France world, countr"
2021.emnlp-main.613,P16-1105,0,0.0315378,"irs shown in green, and enemy ones in red. We find that distance is, on average, lower between allies (mean distance: 0.905, standard deviation: 0.063) than between enemies (mean distance: 0.912, standard deviation: 0.060) at a significance level of p < 0.05 under a t-test. This means that entities with similar articles are statistically less likely to appear as enemies in a conflict. 6 Related Work Entity relationship classification. Most work on entity relationship classification is focused on modeling multi-dimensional relations in knowledge bases and ontologies (e.g., Riedel et al., 2010; Miwa and Bansal, 2016). The focus of our work is more similar to person-to-person sentiment analysis (West et al., 2014) since dyadic relationships are binary. There exist expert-based 5.2 Analysis of Textual Similarity conflict-cooperation scales such as the Goldstein Our results suggest that the dyadic and systemic Scale (Goldstein, 1992). Structural balance thefeatures we extracted from Wikipedia correlate, to ory (Heider, 1946; Cartwright and Harary, 1956) some extent, with whether a pair of entities are has been extended to status theory (Leskovec et al., allies or enemies. Indeed, some preliminary ex- 2010) a"
2021.emnlp-main.613,P13-1108,0,0.0245587,"Missing"
2021.emnlp-main.613,D19-1250,0,0.012452,"imply using superficial linguistic cues to regurgitate these relationships? After all, if this is all that a model does, we could hardly attribute its success to whether it is dyadic or systemic! Therefore, for a fair comparison, we need to ensure that our data does not contain explicit mentions of such relationships.8 The second challenge is that we must not inadvertently provide more information to our models than the information in our features (§2). For example, pre-trained representations have been shown to encode a plethora of information that may skew predictions (Kutuzov et al., 2017; Petroni et al., 2019; Bouraoui et al., 2020). Hence, for our exper7 as of 14 September 2021 We believe that a model that exploits explicit mentions of ally–enemy relationships in the text is better analyzed through the lens of information extraction and machine reading. 8 6 Most graph processing is done with the help of the networkX library (Hagberg et al., 2008). Niger 7778 Conflict Top 10 unigrams Mali War soldier, town, attack, troop, rebel, group, city, conflict, northern, hostage Mali woman, country, coup, population, president, align, region, control, popular, rate France world, country, large, region, terr"
2021.emnlp-main.613,2020.aespen-1.7,0,0.0488063,"Missing"
2021.emnlp-main.613,P17-1072,0,0.0163112,"f centralSummary History Summary Northern Mali conflict Factionalism Mali France Summary Religious compatibility Attacks on Civilians Death of Osama bin Laden Al-Qaeda Azawad Liberation Movement Conflict with Islamist Independence fightgroups History Figure 6: Top 2 principal components of tf-idf representations of the four entity articles involved in the Mali War; each belligerent is shown with the same symbol. We observe that the allies Mali and France are semantically more similar than enemies. Srivastava et al., 2016) and international relations extracted from news (O’Connor et al., 2013; Tan et al., 2017; Han et al., 2019). lated militarized conflict analyses focus on news coverage (West and Pfeffer, 2017), interpretable topic models (Mueller and Rauh, 2018) and graph neural networks for event detection (Nguyen and Grishman, 2018; Cui et al., 2020). Quantitative conflict studies. Consistent with our work, existing empirical studies find evidence for coalescing dyadic and systemic conflict causes 7 Conclusion (de Mesquita and Lalman, 1988; Midlarsky, 1990; Geller, 1993). However, empirical studies are lim- This work explores the extent to which dyadic and systemic features correlate with wheth"
2021.emnlp-main.613,Q14-1024,1,0.797291,"(mean distance: 0.905, standard deviation: 0.063) than between enemies (mean distance: 0.912, standard deviation: 0.060) at a significance level of p < 0.05 under a t-test. This means that entities with similar articles are statistically less likely to appear as enemies in a conflict. 6 Related Work Entity relationship classification. Most work on entity relationship classification is focused on modeling multi-dimensional relations in knowledge bases and ontologies (e.g., Riedel et al., 2010; Miwa and Bansal, 2016). The focus of our work is more similar to person-to-person sentiment analysis (West et al., 2014) since dyadic relationships are binary. There exist expert-based 5.2 Analysis of Textual Similarity conflict-cooperation scales such as the Goldstein Our results suggest that the dyadic and systemic Scale (Goldstein, 1992). Structural balance thefeatures we extracted from Wikipedia correlate, to ory (Heider, 1946; Cartwright and Harary, 1956) some extent, with whether a pair of entities are has been extended to status theory (Leskovec et al., allies or enemies. Indeed, some preliminary ex- 2010) and studied in online discussions by comperiments show that the tf-idf representations of bining si"
2021.emnlp-main.653,P96-1041,0,0.581535,"hrough n-gram, or in this case n-phone, modelling. Specifically, we can estimate the probability of observing some phone wt given the previous n − 1 phones by computing the proportion of times this phone follows those previous n − 1 phones in a corpus. By this definition, sequences not present in the corpus will be assigned 0 probability under the model. This, among other factors, contributes to the often poor generalisation abilities of basic n-gram models. Indeed, there exists an entire literature on smoothing and regularisation techniques for n-gram modelling (Katz, 1987; Ney et al., 1994; Chen and Goodman, 1996). Laplace smoothing is a popular choice, being used in a number of recent works in computational linguistics (e.g. Dautriche et al., 2017a; Trott and Bergen, 2020). However, it is perhaps the simplest of such regularisation techniques, and usually leads to much weaker empirical performances than, e.g., Kneyser–Ney (Ney et al., 1994). It is therefore natural to question whether an n-gram model with simple Laplace smoothing can provide a good representation of the true phonotactic distribution of a language. In our experiments, we follow Trott and Bergen in using a 5-gram model with Laplace smoo"
2021.emnlp-main.653,C65-1001,0,0.244338,"ted lexicon would contain more homophony than natural ones; as we will show, overfit distributions will likely produce more collisions. Thus, it is not entirely clear what we can conclude from their experiments. 3 Quantifying Homophony tion over wordforms. In this section, we first provide a definition of a language’s phonotactic distribution. We then present both the R´enyi collision entropy and the sample R´enyi entropy as new measures of homophony. 3.1 Phonotactics and Wordforms Formally, phonotactics defines a language’s set of plausible wordforms. Its classic exemplification, provided by Chomsky and Halle (1965), is that while the unattested wordform blick would be plausible in English, *bnick would not. Under a probabilistic interpretation (Hayes and Wilson, 2008; Gorman, 2013), this can be re-stated as blick having high phonotactic probability, while *bnick has low phonotactic probability. Notedly, a language’s phonotactics highly constrains its set of possible wordforms (Dautriche et al., 2017a) and, cross-linguistically, the size of these sets seems to be roughly constant (Pimentel et al., 2020b). Further, phonotactics has a tight relationship with word frequency; more phonotactically likely word"
2021.emnlp-main.653,2020.emnlp-main.328,1,0.779516,"onotactics defines a language’s set of plausible wordforms. Its classic exemplification, provided by Chomsky and Halle (1965), is that while the unattested wordform blick would be plausible in English, *bnick would not. Under a probabilistic interpretation (Hayes and Wilson, 2008; Gorman, 2013), this can be re-stated as blick having high phonotactic probability, while *bnick has low phonotactic probability. Notedly, a language’s phonotactics highly constrains its set of possible wordforms (Dautriche et al., 2017a) and, cross-linguistically, the size of these sets seems to be roughly constant (Pimentel et al., 2020b). Further, phonotactics has a tight relationship with word frequency; more phonotactically likely words are more frequent (Mahowald et al., 2018). We model the phonotactic distribution over possible wordforms as a language model: p(w) = |w| Y p(wt |w<t ) (1) t=1 whose support is the infinite set W—defined here as the Kleene closure of a phonetic alphabet Σ∗ , albeit where all w ∈ W are padded with beginningof- and end-of-word symbols. Under this definition, highly plausible wordforms would be assigned high probability, and vice-versa. 3.2 Entropy as a Measure of Homophony The R´enyi entropy"
2021.emnlp-main.653,P19-1171,1,0.892198,"Missing"
2021.emnlp-main.653,2020.tacl-1.1,1,0.837036,"onotactics defines a language’s set of plausible wordforms. Its classic exemplification, provided by Chomsky and Halle (1965), is that while the unattested wordform blick would be plausible in English, *bnick would not. Under a probabilistic interpretation (Hayes and Wilson, 2008; Gorman, 2013), this can be re-stated as blick having high phonotactic probability, while *bnick has low phonotactic probability. Notedly, a language’s phonotactics highly constrains its set of possible wordforms (Dautriche et al., 2017a) and, cross-linguistically, the size of these sets seems to be roughly constant (Pimentel et al., 2020b). Further, phonotactics has a tight relationship with word frequency; more phonotactically likely words are more frequent (Mahowald et al., 2018). We model the phonotactic distribution over possible wordforms as a language model: p(w) = |w| Y p(wt |w<t ) (1) t=1 whose support is the infinite set W—defined here as the Kleene closure of a phonetic alphabet Σ∗ , albeit where all w ∈ W are padded with beginningof- and end-of-word symbols. Under this definition, highly plausible wordforms would be assigned high probability, and vice-versa. 3.2 Entropy as a Measure of Homophony The R´enyi entropy"
2021.emnlp-main.73,2020.lrec-1.521,0,0.0781886,"amantis. While Unitran is particularly error-prone for languages with opaque orthographies (Salesky et al., 2020), we filter out the languages with lower-quality alignments (as we detail below). This original dataset has 690 doculects from 635 languages in 70 language families. In order to study the trade-off hypothesis we require two measurements: phone durations and phone-level surprisals. As mentioned above, phone 4 These texts were crawled from bible.is and utterancealigned by Black (2019) for the CMU Wilderness dataset. 5 A list of all languages can be found in App. C. 6 We set Wikipron (Lee et al., 2020) alignments aside because we could not obtain word position information for them. 7 The term doculect refers to a dialect as recorded in a specific document, in this case a Bible reading. Figure 2: The languages of the VoxClamantis corpus geo-located and coloured by language family. durations are readily available in VoxClamantis. Phone-level surprisals, on the other hand, are not, so we employed phone-level language models in order to estimate them (as detailed in §3). Given both these values, we can now perform our crosslinguistic analysis. First, though, we will describe some data quality c"
2021.emnlp-main.73,W18-0102,0,0.0142434,"ne st appears. Unfortunately, this surprisal is not readily available, since we would need access to the true distribution p(st |s<t ) to compute it. We will use an approximation pθ (st |s<t ) instead, i.e. a phone-level model with estimated parameters θ. 3.1 Approximating p(st |s<t ). While much of the original psycholinguistic work on surprisal estimated pθ using n-gram models (Levy and Jaeger, 2007; Coupé et al., 2019, inter alia), recent work has shown that a language model’s psychometric predictive power correlates directly with its quality, measured by its crossentropy in held-out data (Goodkind and Bicknell, 2018; Wilcox et al., 2020). We will thus make use of LSTMs in this work, since they have been shown to outperform n-grams on phone-level language modelling tasks (Pimentel et al., 2020). We first encode each phone st into a high-dimensional lookup embedding et ∈ d1 , where d1 is its embedding size. We then process these embeddings using an LSTM (Hochreiter and Schmidhuber, 1997), which outputs contextualised hidden state vectors: R ht = LSTM(ht−1 , et−1 ) ∈ Rd 2 (2) bol. The hidden states are then linearly transformed and projected onto ∆|S|+1 , the probability simplex, via a softmax to compute th"
2021.emnlp-main.73,N01-1021,0,0.184808,"ve channel’s capacity 950 (Frank and Jaeger, 2008; Piantadosi et al., 2011). If we assume this channel’s capacity to be constant across languages, we may derive a cross-linguistic version of UID. Such a hypothesis would predict, for instance, that speakers of languages with less informative phones will make them faster. Under this specific interpretation, our study can be seen as evidence of UID as a cross-linguistic phenomenon. 3 Measuring Surprisal To formalise our approach, we first present a standard measure of information content: surprisal. In the context of natural language, surprisal (Hale, 2001) measures the Shannon information content a linguistic unit conveys in context, which can be measured as its negative log-probability: H(St = st |S <t = s<t ) = − log p(st |s<t ) (1) In this equation, S is a sentence-level random variable, with instances s ∈ S ∗ , and t indexes a position in the sentence. Accordingly, we define S as the set of phones in a given phonetic alphabet, and we use s<t to indicate the context in which phone st appears. Unfortunately, this surprisal is not readily available, since we would need access to the true distribution p(st |s<t ) to compute it. We will use an a"
2021.emnlp-main.73,P82-1020,0,0.737253,"Missing"
2021.emnlp-main.73,J86-2003,0,0.474383,"ive effect, duration is multiplied by y per bit of information. Sorted dots represent languages in Unitran; ‘+’ are languages in Epitran. the uniform information density hypothesis (UID; Fenk and Fenk, 1980; Aylett and Turk, 2004; Levy and Jaeger, 2007), which predicts that speakers smooth the information rate in a linguistic signal so as to keep it roughly constant; by smoothing their information rate, natural languages can stay close to a (hypothetical) channel capacity. Across languages, a unified channel capacity allows us to derive a specific instantiation of the compensation hypothesis (Hockett, 1958), with information density (measured in, e.g., bits per phone) being compensated by utterance speed (in, e.g., milliseconds per phone). We may thus predict a trade-off between surprisal1 and duration both within and across the world’s languages. This trade-off has been studied amply within high resource languages (Genzel and Charniak, 2002; Bell et al., 2003; Mahowald et al., 2018, inter alia). Cross-linguistically, however, this trade-off has received comparatively little attention, with a few notable exceptions such as Pellegrino During the course of human evolution, countless languages have"
2021.emnlp-main.73,2021.emnlp-main.74,1,0.682102,"n trade-off. Indeed, Pellegrino et al. (2011) and Coupé et al. (2019) present initial evidence of this trade-off across languages. Analogously, the UID hypothesis posits that, within a language, users balance the amount of information per linguistic unit with the duration of its utterance. This hypothesis has been used to explain a range of experimental data in psycholinguistics, including syntactic reduction (Levy and Jaeger, 2007) and contractions, such as are vs ’re (Frank and Jaeger, 2008). While this theory is somewhat under-specified with respect to its causal mechanisms, as we argue in Meister et al. (2021), one of its typical interpretations is that users are maximising a communicative channel’s capacity 950 (Frank and Jaeger, 2008; Piantadosi et al., 2011). If we assume this channel’s capacity to be constant across languages, we may derive a cross-linguistic version of UID. Such a hypothesis would predict, for instance, that speakers of languages with less informative phones will make them faster. Under this specific interpretation, our study can be seen as evidence of UID as a cross-linguistic phenomenon. 3 Measuring Surprisal To formalise our approach, we first present a standard measure of"
2021.emnlp-main.73,qian-etal-2010-python,0,0.0180848,"gned using either multilingual acoustic models (Wiesner et al., 2019; Povey et al., 2011) or language-specific acoustic models (Black, 2019; Anumanchipalli et al., 2011). VoxClamantis offers its phonetic measurements under three G2P models, which trade-off language coverage and quality. We will focus on two:6 • Epitran (Mortensen et al., 2018). This is a collection of high quality G2P models based on language-specific rules. Phonetic measurements produced with Epitran are available for a collection of 39 doculects7 from 29 languages (as defined by ISO codes) in 8 language families. • Unitran (Qian et al., 2010). This is a naïve and deterministic G2P model, but its derived measurements are available for all languages in VoxClamantis. While Unitran is particularly error-prone for languages with opaque orthographies (Salesky et al., 2020), we filter out the languages with lower-quality alignments (as we detail below). This original dataset has 690 doculects from 635 languages in 70 language families. In order to study the trade-off hypothesis we require two measurements: phone durations and phone-level surprisals. As mentioned above, phone 4 These texts were crawled from bible.is and utterancealigned b"
2021.emnlp-main.73,2020.acl-main.415,1,0.888884,"ugh for this analysis. Further, as we have a specific hypothesis for why this trade-off should arise (humans’ information processing capacity), we are not interested in simply finding any correlation between surprisal and duration. Several confounds could drive such a correlation, but most of these are either trivially true or uninteresting from our perspective. Therefore, a thorough analysis of this trade-off needs to control for these potential confounds. In this work, we investigate the surprisal– duration trade-off by analysing a massively multi-lingual dataset of more than 600 languages (Salesky et al., 2020). We present an experimental framework, controlling for several possible confounds, and evaluate the surprisal–duration trade-off at the phone level. We find evidence of a trade-off across languages: languages with more surprising phones compensate by making utterances longer. We also confirm mono-lingual trade-offs in 319 languages, out of 600;2 within these languages, more surprising phones are pronounced with a significantly longer duration. This is the most representative evidence of the uniform information density hypothesis to date. Moreover, we did not find evidence of a single language"
2021.emnlp-main.73,2021.eacl-main.3,1,0.730146,"Nooteboom, 1981). Under an information-theoretic analysis, it has 8 Probabilities must sum to 1. This finite probability mass means average probability must go down with more classes. 9 Concatenative languages, for instance, would have both longer and less predictable words. Take the German word Hauptbahnhof which can be translated into English as central train station. Predicting this single (and long) German word is equivalent to predicting three words in English. been observed that earlier segments in a word are more surprising than later ones (van Son and Pols, 2003; King and Wedel, 2020; Pimentel et al., 2021). Word-initial segments are both lengthened and more surprising, potentially for unrelated reasons. An analysis which does not control for such word-positioning is thus doomed to find trivial correlations. To account for this word-initial and word-final lengthening, we include three word position fixed effects (initial, middle, or final) in our mixed effects models. Sentential Context. The amount of context that a model conditions on when estimating probabilities will undoubtedly have an impact on a study of this nature. For example, a model that cannot look back beyond the current word, such"
2021.emnlp-main.73,2020.tacl-1.1,1,0.907824,"egrino et al. (2011) recently analysed the speech rate of 8 languages using a semantically controlled corpus. They found strong evidence towards non-uniform speech rates across these languages. This result is not surprising, however, given that natural languages vary widely in their phonology, morphology, and syntax. Despite these differences, researchers have hypothesised that there exist compensatory relationships between the complexity of these components (Hockett, 1958; Martinet, 1955). For instance, a larger phonemic diversity could be compensated by shorter words (Moran and Blasi, 2014; Pimentel et al., 2020) or a larger number of irregular inflected forms could lead to less complex morphological paradigms (Cotterell et al., 2019). Such a compensation can be thus seen as a type of balance, where languages compromise reliability versus effort in communication (Zipf, 1949; Martinet, 1962). One natural avenue for creating this balance would be a language’s information rate. If this were kept roughly constant, the needs of both speakers (who prefer shorter utterances) and listeners (who value easier comprehension) could be accommodated. Speech rate would then be compensated by information density, res"
2021.emnlp-main.74,W18-4605,0,0.0218524,"hesis in language production span levels of linguistic structure: from phonetics (Aylett and Turk, 2004) to lexical choice (Mahowald et al., 2013), to syntax (Jaeger, 2010), and to discourse (Torabi Asr and Demberg 2015) (though see Zhan and Levy 2018, 2019). Despite this evidence, there are several aspects of the UID hypothesis that lack clarity or unity. For example, there is a dearth of converging evidence from studies in language comprehension. Furthermore, multiple candidate operationalizations of UID have been proposed, each without formal justification for their choices (Collins, 2014; Jain et al., 2018; Meister et al., 2020; Wei et al., 2021). In this work, we attempt to shed light on these issues: we first study the relationship between the dis1 Introduction tribution of information content throughout a senThe uniform information density (UID) hypothesis tence and native speakers’ (i) sentence-level read(Fenk and Fenk, 1980; Levy and Jaeger, 2007) ing times and (ii) sentence acceptability judgments. states that language users prefer when information While our results for sentence-level reading times content (measured information-theoretically as do not contradict previous word-level readin"
2021.emnlp-main.74,2021.acl-long.405,0,0.0226906,"quadratic test might be too restrictive. Our approach, which explores a more fine-grained range of k, is potentially more comprehensive, and indeed we find that values of k slightly greater than 1 often fit the data at least as well as k = 1, and can certainly not be ruled out. Other potential virtues of our analysis are (1) Our analysis is performed at the sentence- (rather than word-) level. This is arguably a better method for analyzing a sequence-level phenomenon, i.e., UID, and (2) specifically for eye movement data, we include re-reading times after the first pass. called into question (Kuribayashi et al., 2021). As such, while we find convincing preliminary evidence in our analyzed languages, we are not able to fully test the hypothesis that the pressure for UID is at the language-level. Further, we have no evidence as to whether there may be pressure towards a cross-linguistic µc , which would be relevant to cross-linguistic interpretations of UID (Pimentel et al., 2021). Another important limitation of this work is the restriction to psychometric data from the written domain. To fully grasp the effects of the distribution of information in linguistic signals on language comprehension, spoken langu"
2021.emnlp-main.74,P16-1162,0,0.013764,"Missing"
2021.emnlp-main.74,N18-1181,1,0.785228,"oefficient between (negative) sum of surprisals raised to the k th power and linguistic acceptability judgments of a sentence. The higher correlation when k > 1 implies sentences with a more uniform distribution of information are more acceptable. surprisal) is distributed as smoothly as possible throughout an utterance. The studies adduced in support of this hypothesis in language production span levels of linguistic structure: from phonetics (Aylett and Turk, 2004) to lexical choice (Mahowald et al., 2013), to syntax (Jaeger, 2010), and to discourse (Torabi Asr and Demberg 2015) (though see Zhan and Levy 2018, 2019). Despite this evidence, there are several aspects of the UID hypothesis that lack clarity or unity. For example, there is a dearth of converging evidence from studies in language comprehension. Furthermore, multiple candidate operationalizations of UID have been proposed, each without formal justification for their choices (Collins, 2014; Jain et al., 2018; Meister et al., 2020; Wei et al., 2021). In this work, we attempt to shed light on these issues: we first study the relationship between the dis1 Introduction tribution of information content throughout a senThe uniform information"
2021.emnlp-main.824,D12-1032,0,0.0301932,"ctic dependency structure (Kübler et al., 2009). Additionally, probabilistic models over spanning trees are common in the NLP literature with applications primarily in non-projective dependency parsing (Pei et al., 2015; Wang and Chang, 2016; Dozat and Manning, 2017; Ma and Hovy, 2017), but also in recovering 1 Our implementation of these algorithms is publicly available at https://github.com/rycolab/treesample. 2 Directed spanning trees are known as arborescences in the graph theory literature (Williamson, 1985). However, we will simply refer to them as spanning trees. phylogenic structures (Andrews et al., 2012), and event extraction (McClosky et al., 2011). Given the prevalence of such probabilistic models, efficient dependency tree sampling algorithms deserve study. Indeed, some work has been done in transition-based dependency parsing (Keith et al., 2018) as well as graph-based dependency parsing (Nakagawa, 2007; Mareˇcek and Žabokrtský, 2011). Sampling has also been utilized in an abundance of NLP tasks, such as text generation (Clark et al., 2018; Fedus et al., 2018), co-reference resolution (Singh et al., 2012), and language modeling (Mnih and Hinton, 2007; Logan IV et al., 2020). The theoretic"
2021.emnlp-main.824,N18-1204,0,0.019637,"Missing"
2021.emnlp-main.824,N18-1084,0,0.0399814,"Missing"
2021.emnlp-main.824,D07-1015,0,0.296336,"random walks through Markov chains have been used to sample spanning trees from both undirected (Broder, 1989; Aldous, 1990) and directed graphs (Wilson, 1996). The algorithm of Wilson (1996) is linear in the mean hitting time of the graph and is currently the fastest sampling algorithm for directed spanning trees. It has been used in dependency parsing inference by Zhang et al. (2014a,b). Second, several algorithms have leveraged the matrix–tree theorem (MTT; Kirchhoff, 1847; Tutte, 1984). The MTT has been frequently used to perform inference on non-projective graphbased dependency parsers (Koo et al., 2007; McDonald and Satta, 2007; Smith and Smith, 2007; Zmigrod et al., 2021). This theorem was first used for sampling by Guénoche (1983) who gave an O(N 5 ) algorithm which was then improved by Kulkarni (1990) and Colbourn et al. (1996). Colbourn et al. (1996) give an O(N 3 ) algorithm to sample spanning trees from an unweighted directed graph. We generalize their algorithm to the weighted case. While directed spanning tree sampling algorithms exist, an important constraint of many dependency tree schemes, such as the Universal De10558 Proceedings of the 2021 Conference on Empirical Methods in Na"
2021.emnlp-main.824,2020.acl-main.196,0,0.0753032,"Missing"
2021.emnlp-main.824,I17-1007,0,0.0603687,") time. To the best of our knowledge, no algorithm has been given for sampling spanning trees without replacement from a directed graph.1 1 Introduction Spanning trees in directed graphs2 are fundamental combinatorial structures in natural language processing where they are used to represent dependency structures—especially syntactic dependency structure (Kübler et al., 2009). Additionally, probabilistic models over spanning trees are common in the NLP literature with applications primarily in non-projective dependency parsing (Pei et al., 2015; Wang and Chang, 2016; Dozat and Manning, 2017; Ma and Hovy, 2017), but also in recovering 1 Our implementation of these algorithms is publicly available at https://github.com/rycolab/treesample. 2 Directed spanning trees are known as arborescences in the graph theory literature (Williamson, 1985). However, we will simply refer to them as spanning trees. phylogenic structures (Andrews et al., 2012), and event extraction (McClosky et al., 2011). Given the prevalence of such probabilistic models, efficient dependency tree sampling algorithms deserve study. Indeed, some work has been done in transition-based dependency parsing (Keith et al., 2018) as well as gr"
2021.emnlp-main.824,W11-3901,0,0.0577847,"Missing"
2021.emnlp-main.824,W11-1806,0,0.0773456,"9). Additionally, probabilistic models over spanning trees are common in the NLP literature with applications primarily in non-projective dependency parsing (Pei et al., 2015; Wang and Chang, 2016; Dozat and Manning, 2017; Ma and Hovy, 2017), but also in recovering 1 Our implementation of these algorithms is publicly available at https://github.com/rycolab/treesample. 2 Directed spanning trees are known as arborescences in the graph theory literature (Williamson, 1985). However, we will simply refer to them as spanning trees. phylogenic structures (Andrews et al., 2012), and event extraction (McClosky et al., 2011). Given the prevalence of such probabilistic models, efficient dependency tree sampling algorithms deserve study. Indeed, some work has been done in transition-based dependency parsing (Keith et al., 2018) as well as graph-based dependency parsing (Nakagawa, 2007; Mareˇcek and Žabokrtský, 2011). Sampling has also been utilized in an abundance of NLP tasks, such as text generation (Clark et al., 2018; Fedus et al., 2018), co-reference resolution (Singh et al., 2012), and language modeling (Mnih and Hinton, 2007; Logan IV et al., 2020). The theoretical computer science literature has several eff"
2021.emnlp-main.824,W07-2216,0,0.0612357,"ugh Markov chains have been used to sample spanning trees from both undirected (Broder, 1989; Aldous, 1990) and directed graphs (Wilson, 1996). The algorithm of Wilson (1996) is linear in the mean hitting time of the graph and is currently the fastest sampling algorithm for directed spanning trees. It has been used in dependency parsing inference by Zhang et al. (2014a,b). Second, several algorithms have leveraged the matrix–tree theorem (MTT; Kirchhoff, 1847; Tutte, 1984). The MTT has been frequently used to perform inference on non-projective graphbased dependency parsers (Koo et al., 2007; McDonald and Satta, 2007; Smith and Smith, 2007; Zmigrod et al., 2021). This theorem was first used for sampling by Guénoche (1983) who gave an O(N 5 ) algorithm which was then improved by Kulkarni (1990) and Colbourn et al. (1996). Colbourn et al. (1996) give an O(N 3 ) algorithm to sample spanning trees from an unweighted directed graph. We generalize their algorithm to the weighted case. While directed spanning tree sampling algorithms exist, an important constraint of many dependency tree schemes, such as the Universal De10558 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,"
2021.emnlp-main.824,D07-1100,0,0.0183795,"implementation of these algorithms is publicly available at https://github.com/rycolab/treesample. 2 Directed spanning trees are known as arborescences in the graph theory literature (Williamson, 1985). However, we will simply refer to them as spanning trees. phylogenic structures (Andrews et al., 2012), and event extraction (McClosky et al., 2011). Given the prevalence of such probabilistic models, efficient dependency tree sampling algorithms deserve study. Indeed, some work has been done in transition-based dependency parsing (Keith et al., 2018) as well as graph-based dependency parsing (Nakagawa, 2007; Mareˇcek and Žabokrtský, 2011). Sampling has also been utilized in an abundance of NLP tasks, such as text generation (Clark et al., 2018; Fedus et al., 2018), co-reference resolution (Singh et al., 2012), and language modeling (Mnih and Hinton, 2007; Logan IV et al., 2020). The theoretical computer science literature has several efficient algorithms for sampling directed spanning trees. These algorithms come in two flavors. First, random walks through Markov chains have been used to sample spanning trees from both undirected (Broder, 1989; Aldous, 1990) and directed graphs (Wilson, 1996). T"
2021.emnlp-main.824,2020.emnlp-main.390,1,0.791375,"Missing"
2021.emnlp-main.824,D12-1101,0,0.03482,"). However, we will simply refer to them as spanning trees. phylogenic structures (Andrews et al., 2012), and event extraction (McClosky et al., 2011). Given the prevalence of such probabilistic models, efficient dependency tree sampling algorithms deserve study. Indeed, some work has been done in transition-based dependency parsing (Keith et al., 2018) as well as graph-based dependency parsing (Nakagawa, 2007; Mareˇcek and Žabokrtský, 2011). Sampling has also been utilized in an abundance of NLP tasks, such as text generation (Clark et al., 2018; Fedus et al., 2018), co-reference resolution (Singh et al., 2012), and language modeling (Mnih and Hinton, 2007; Logan IV et al., 2020). The theoretical computer science literature has several efficient algorithms for sampling directed spanning trees. These algorithms come in two flavors. First, random walks through Markov chains have been used to sample spanning trees from both undirected (Broder, 1989; Aldous, 1990) and directed graphs (Wilson, 1996). The algorithm of Wilson (1996) is linear in the mean hitting time of the graph and is currently the fastest sampling algorithm for directed spanning trees. It has been used in dependency parsing inference by"
2021.emnlp-main.824,D07-1014,0,0.0780501,"n used to sample spanning trees from both undirected (Broder, 1989; Aldous, 1990) and directed graphs (Wilson, 1996). The algorithm of Wilson (1996) is linear in the mean hitting time of the graph and is currently the fastest sampling algorithm for directed spanning trees. It has been used in dependency parsing inference by Zhang et al. (2014a,b). Second, several algorithms have leveraged the matrix–tree theorem (MTT; Kirchhoff, 1847; Tutte, 1984). The MTT has been frequently used to perform inference on non-projective graphbased dependency parsers (Koo et al., 2007; McDonald and Satta, 2007; Smith and Smith, 2007; Zmigrod et al., 2021). This theorem was first used for sampling by Guénoche (1983) who gave an O(N 5 ) algorithm which was then improved by Kulkarni (1990) and Colbourn et al. (1996). Colbourn et al. (1996) give an O(N 3 ) algorithm to sample spanning trees from an unweighted directed graph. We generalize their algorithm to the weighted case. While directed spanning tree sampling algorithms exist, an important constraint of many dependency tree schemes, such as the Universal De10558 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 10558–10569 c No"
2021.emnlp-main.824,2021.emnlp-main.823,0,0.071317,"Missing"
2021.emnlp-main.824,P16-1218,0,0.0311665,"e K trees without replacement in O(KN 3 + K 2 N ) time. To the best of our knowledge, no algorithm has been given for sampling spanning trees without replacement from a directed graph.1 1 Introduction Spanning trees in directed graphs2 are fundamental combinatorial structures in natural language processing where they are used to represent dependency structures—especially syntactic dependency structure (Kübler et al., 2009). Additionally, probabilistic models over spanning trees are common in the NLP literature with applications primarily in non-projective dependency parsing (Pei et al., 2015; Wang and Chang, 2016; Dozat and Manning, 2017; Ma and Hovy, 2017), but also in recovering 1 Our implementation of these algorithms is publicly available at https://github.com/rycolab/treesample. 2 Directed spanning trees are known as arborescences in the graph theory literature (Williamson, 1985). However, we will simply refer to them as spanning trees. phylogenic structures (Andrews et al., 2012), and event extraction (McClosky et al., 2011). Given the prevalence of such probabilistic models, efficient dependency tree sampling algorithms deserve study. Indeed, some work has been done in transition-based dependen"
2021.emnlp-main.824,D14-1109,0,0.0258485,"and language modeling (Mnih and Hinton, 2007; Logan IV et al., 2020). The theoretical computer science literature has several efficient algorithms for sampling directed spanning trees. These algorithms come in two flavors. First, random walks through Markov chains have been used to sample spanning trees from both undirected (Broder, 1989; Aldous, 1990) and directed graphs (Wilson, 1996). The algorithm of Wilson (1996) is linear in the mean hitting time of the graph and is currently the fastest sampling algorithm for directed spanning trees. It has been used in dependency parsing inference by Zhang et al. (2014a,b). Second, several algorithms have leveraged the matrix–tree theorem (MTT; Kirchhoff, 1847; Tutte, 1984). The MTT has been frequently used to perform inference on non-projective graphbased dependency parsers (Koo et al., 2007; McDonald and Satta, 2007; Smith and Smith, 2007; Zmigrod et al., 2021). This theorem was first used for sampling by Guénoche (1983) who gave an O(N 5 ) algorithm which was then improved by Kulkarni (1990) and Colbourn et al. (1996). Colbourn et al. (1996) give an O(N 3 ) algorithm to sample spanning trees from an unweighted directed graph. We generalize their algorith"
2021.emnlp-main.824,P14-1019,0,0.0273782,"and language modeling (Mnih and Hinton, 2007; Logan IV et al., 2020). The theoretical computer science literature has several efficient algorithms for sampling directed spanning trees. These algorithms come in two flavors. First, random walks through Markov chains have been used to sample spanning trees from both undirected (Broder, 1989; Aldous, 1990) and directed graphs (Wilson, 1996). The algorithm of Wilson (1996) is linear in the mean hitting time of the graph and is currently the fastest sampling algorithm for directed spanning trees. It has been used in dependency parsing inference by Zhang et al. (2014a,b). Second, several algorithms have leveraged the matrix–tree theorem (MTT; Kirchhoff, 1847; Tutte, 1984). The MTT has been frequently used to perform inference on non-projective graphbased dependency parsers (Koo et al., 2007; McDonald and Satta, 2007; Smith and Smith, 2007; Zmigrod et al., 2021). This theorem was first used for sampling by Guénoche (1983) who gave an O(N 5 ) algorithm which was then improved by Kulkarni (1990) and Colbourn et al. (1996). Colbourn et al. (1996) give an O(N 3 ) algorithm to sample spanning trees from an unweighted directed graph. We generalize their algorith"
2021.findings-emnlp.322,W11-2903,0,0.0860434,"Missing"
2021.findings-emnlp.322,P96-1025,0,0.201479,"Missing"
2021.findings-emnlp.322,P14-2102,1,0.671127,"• Linear index-grammar parsing: O n7 in Vijay Shanker and Weir (1989), sped up to O n6 by Vijay-Shanker and Weir (1993). • Lexicalized tree adjoining grammar parsing:  O n8 in Vijay-Shankar and Joshi (1985), sped  up to O n7 by Eisner and Satta (2000). Algorithmic research in natural language processing (NLP) has focused—in large part—on developing dynamic programming solutions to combinatorial problems that arise in the field (Huang, 2009). Such algorithms have been introduced over the years for countless linguistic formalisms, such as finite-state transduction (Mohri, 1997; Eisner, 2002; Cotterell et al., 2014), context-free parsing (Stolcke, 1995; Goodman, 1999), dependency parsing (Eisner, 1996; Koo and Collins, 2010; Ma and Zhao, 2012) and mildly context-sensitive parsing (Vijay-Shanker and Weir, 1989, 1990; Kuhlmann et al., 2018). In recent years, the same algorithms have often been used for deep structured prediction, 1 Many of these examples were brought to our attention using a neural scoring function that decomposes in the works of Eisner and Blatz (2007) and Gildea (2011); over the structure (Durrett and Klein, 2015; Rastogi further discussion can be found therein. 3812 Findings of the Asso"
2021.findings-emnlp.322,N09-1026,0,0.040575,"ived using our methodology assume that relations are dense. Often relations are statically known to be sparse. Many low-level details affect actual execution time, but do not matter for asymptotic complexity. For example, memory layouts (e.g., row-order or column-order layout of a dense array in memory), sparse vs. dense representations of relations (e.g., hash tables vs. arrays), and indexes on relations (including sorted order) can have a dramatic effect on the running time in practice. However, they will not manifest in the degree analysis (e.g., Bilmes et al. (1997); Dunlop et al. (2011); DeNero et al. (2009); Lopez (2007)). Such choices are out of the control of our specific search space, but they may interact with the program in ways that are not represented in the degree. An obvious alternative cost function would be the empirical execution time of executing the transformed program on a workload of representative inputs (e.g., running a transformed parser on actual sentences from the Penn Treebank (Marcus et al., 1993)). But as we noted earlier, such a cost function might be impractically expensive. For example, evaluating the degree of a degree-1000 program is linear in the size of the program"
2021.findings-emnlp.322,C12-2077,0,0.012458,"tree adjoining grammar parsing:  O n8 in Vijay-Shankar and Joshi (1985), sped  up to O n7 by Eisner and Satta (2000). Algorithmic research in natural language processing (NLP) has focused—in large part—on developing dynamic programming solutions to combinatorial problems that arise in the field (Huang, 2009). Such algorithms have been introduced over the years for countless linguistic formalisms, such as finite-state transduction (Mohri, 1997; Eisner, 2002; Cotterell et al., 2014), context-free parsing (Stolcke, 1995; Goodman, 1999), dependency parsing (Eisner, 1996; Koo and Collins, 2010; Ma and Zhao, 2012) and mildly context-sensitive parsing (Vijay-Shanker and Weir, 1989, 1990; Kuhlmann et al., 2018). In recent years, the same algorithms have often been used for deep structured prediction, 1 Many of these examples were brought to our attention using a neural scoring function that decomposes in the works of Eisner and Blatz (2007) and Gildea (2011); over the structure (Durrett and Klein, 2015; Rastogi further discussion can be found therein. 3812 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 3812–3830 November 7–11, 2021. ©2021 Association for Computational Lingui"
2021.findings-emnlp.322,J93-2004,0,0.074266,"orted order) can have a dramatic effect on the running time in practice. However, they will not manifest in the degree analysis (e.g., Bilmes et al. (1997); Dunlop et al. (2011); DeNero et al. (2009); Lopez (2007)). Such choices are out of the control of our specific search space, but they may interact with the program in ways that are not represented in the degree. An obvious alternative cost function would be the empirical execution time of executing the transformed program on a workload of representative inputs (e.g., running a transformed parser on actual sentences from the Penn Treebank (Marcus et al., 1993)). But as we noted earlier, such a cost function might be impractically expensive. For example, evaluating the degree of a degree-1000 program is linear in the size of the program, whereas evalu 1000 ating the wallclock time is O η . Optimizing the program degree is a crucial design choice as it enables a more exhaustive search in practice. Additionally, it sidesteps the need to optimize for a specific workload. However, in future work, we would like to investigate hybrid search algorithms (e.g., Song et al. (2019)) that do attempt to minimize empirical execution time, but replace some of the"
2021.findings-emnlp.322,P17-1076,0,0.0118469,"mations to improve its running time as much as possible. To this end, we describe a set of program transformations, a simple metric for assessing the efficiency of a transformed program, and a heuristic search procedure to improve this metric. We show that in practice, automated search—like the mental search performed by human programmers—can find substantial improvements to the initial program. Empirically, we show that many speed-ups described in the NLP literature could have been discovered automatically by our system. 1 Introduction et al., 2016; Lee et al., 2016; Dozat and Manning, 2017; Stern et al., 2017; Kim et al., 2017; Hong and Huang, 2018; Wu et al., 2018; Wu and Cotterell, 2019; Qi et al., 2020; Rush, 2020). When a dynamic programming algorithm for a new problem is first introduced in the literature, its runtime may not be optimal—faster versions are often published over time. Indeed, the process of introducing a first algorithm and subsequently finding improvements is common throughout computer science. In the case of dynamic programming, there are program transformations that may be exploited to derive algorithms with a faster runtime (Eisner and Blatz, 2007). These transformations ma"
2021.findings-emnlp.322,J95-2002,0,0.670937,"Shanker and Weir (1989), sped up to O n6 by Vijay-Shanker and Weir (1993). • Lexicalized tree adjoining grammar parsing:  O n8 in Vijay-Shankar and Joshi (1985), sped  up to O n7 by Eisner and Satta (2000). Algorithmic research in natural language processing (NLP) has focused—in large part—on developing dynamic programming solutions to combinatorial problems that arise in the field (Huang, 2009). Such algorithms have been introduced over the years for countless linguistic formalisms, such as finite-state transduction (Mohri, 1997; Eisner, 2002; Cotterell et al., 2014), context-free parsing (Stolcke, 1995; Goodman, 1999), dependency parsing (Eisner, 1996; Koo and Collins, 2010; Ma and Zhao, 2012) and mildly context-sensitive parsing (Vijay-Shanker and Weir, 1989, 1990; Kuhlmann et al., 2018). In recent years, the same algorithms have often been used for deep structured prediction, 1 Many of these examples were brought to our attention using a neural scoring function that decomposes in the works of Eisner and Blatz (2007) and Gildea (2011); over the structure (Durrett and Klein, 2015; Rastogi further discussion can be found therein. 3812 Findings of the Association for Computational Linguistics"
2021.findings-emnlp.322,P85-1011,0,0.399947,"Missing"
2021.findings-emnlp.322,W89-0218,0,0.0916546,"and Joshi (1985), sped  up to O n7 by Eisner and Satta (2000). Algorithmic research in natural language processing (NLP) has focused—in large part—on developing dynamic programming solutions to combinatorial problems that arise in the field (Huang, 2009). Such algorithms have been introduced over the years for countless linguistic formalisms, such as finite-state transduction (Mohri, 1997; Eisner, 2002; Cotterell et al., 2014), context-free parsing (Stolcke, 1995; Goodman, 1999), dependency parsing (Eisner, 1996; Koo and Collins, 2010; Ma and Zhao, 2012) and mildly context-sensitive parsing (Vijay-Shanker and Weir, 1989, 1990; Kuhlmann et al., 2018). In recent years, the same algorithms have often been used for deep structured prediction, 1 Many of these examples were brought to our attention using a neural scoring function that decomposes in the works of Eisner and Blatz (2007) and Gildea (2011); over the structure (Durrett and Klein, 2015; Rastogi further discussion can be found therein. 3812 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 3812–3830 November 7–11, 2021. ©2021 Association for Computational Linguistics fold(1, [1, 2]) elim(4) start β(X,I,K) += γ(X,Y,Z) * β(Y,I,J)"
2021.findings-emnlp.322,P90-1001,0,0.419344,"Missing"
2021.findings-emnlp.322,J93-4002,0,0.57893,"thmist. Consider the following instances1 of published dynamic programs whose runtime bounds were later improved using specific applications of the program transformations mentioned above. • Projective dependency parsing: Collins (1996)  5 gave an  O n algorithm that was sped up to O n4 by Eisner and Satta (1999). • Split-head-factored dependency  parsing: imple5 ; with some efmented na¨ıvely runs in O n  fort, an O n3 algorithm can be derived (Eisner, 1996; Johnson, 2007; Eisner and Blatz, 2007).  • Linear index-grammar parsing: O n7 in Vijay Shanker and Weir (1989), sped up to O n6 by Vijay-Shanker and Weir (1993). • Lexicalized tree adjoining grammar parsing:  O n8 in Vijay-Shankar and Joshi (1985), sped  up to O n7 by Eisner and Satta (2000). Algorithmic research in natural language processing (NLP) has focused—in large part—on developing dynamic programming solutions to combinatorial problems that arise in the field (Huang, 2009). Such algorithms have been introduced over the years for countless linguistic formalisms, such as finite-state transduction (Mohri, 1997; Eisner, 2002; Cotterell et al., 2014), context-free parsing (Stolcke, 1995; Goodman, 1999), dependency parsing (Eisner, 1996; Koo and"
2021.naacl-main.11,P19-1356,0,0.0238359,"al., 2018; Hupkes and Zuidema, 2018); if the probe is able to perform the task well, then it is argued that the model has learnt to implicitly encode that structure in its representation.1 Work from this inchoate probing literature is frequently cited to support the claim that models like BERT encode a large amount of syntactic knowledge. For instance, consider the two excerpts below demonstrating how a couple of syntactic probing papers have been interpreted:2 [The training objectives of BERT/GPT2/XLNet] have shown great abilities to capture dependency between words and syntactic structures (Jawahar et al., 2019) (Tian et al., 2020) Recently, unsupervised language models like Further work has found impressive deBERT (Devlin et al., 2019) have become popular grees of syntactic structure in Transwithin natural language processing (NLP). These former encodings (Hewitt and Manning, pre-trained sentence encoders, known affection2019) (Soulos et al., 2020) ately as BERToids (Rogers et al., 2020), have 1 pushed forward the state of the art in many NLP Methods which analyse stimuli are also sometimes termed tasks. Given their impressive performance, a nat- ‘probes’ (e.g. Niven and Kao, 2019), but in this pape"
2021.naacl-main.11,W19-0112,0,0.0167855,"bility by Kharkwal (2014). We conjugate each of these words using hand-written rules assuming they obey the standard English morphology and graphotactics. This results in 1361 word types—a total of 2377 varieties when we annotate these regular forms with several possible fine-grained part-of-speech realisations. To build sentences, we take the test portion of the English EWT Universal Dependency (UD; Nivre et al., 2016) treebank and substitute words (randomly) with our pseudowords whenever we have one available with matching fine-grained part-ofspeech annotation.4 Our method closely resembles Kasai and Frank (2019), except they do so to analyse parsers in place of syntactic probes. An example of one of our Jabberwocky sentences is shown in Figure 2, along with its unlabeled undirected parse (used by the probes) which is taken from the vanilla sentence’s annotation in the treebank. 4 Two Syntactic Probes Here we briefly introduce two syntactic probes, each designed to learn the syntactic distance between a pair of words in a sentence, which is the number of steps between them in an undirected parse tree (example in Figure 2). Hewitt and Manning (2019) first introduced syntactic distance, and propose the"
2021.naacl-main.11,Q16-1037,0,0.0839249,"Missing"
2021.naacl-main.11,2021.ccl-1.108,0,0.0783637,"Missing"
2021.naacl-main.11,D18-1151,0,0.0234175,"onclusions about any BERToids’ syntactic knowledge, but instead to urge caution when drawing conclusions from probing results. In our discussion, we contend that evaluating BERToids’ syntactic knowledge requires more nuanced experimentation than simply training a syntactic probe as if it were a parser (Hall Maudslay et al., 2020), and call for the separation of syntax and semantics in future probing work. 2 Syntax and Semantics When investigating whether a particular model encodes syntax, those who have opted for stimulianalysis have been careful to isolate syntactic phenomena from semantics (Marvin and Linzen, 2018; Gulordava et al., 2018; Goldberg, 2019), but the same cannot be said of most syntactic probing work, which conflates the two. To see how the two can be separated, consider the famous utterance of Chomsky (1957): (1) Colourless green ideas sleep furiously root amod amod nsubj advmod Colourless green ideas sleep furiously Figure 1: Chomsky’s classic, albeit with the spelling corrected. Syntactic probes are typically evaluated on realworld data, not on Chomsky-style sentences of (1)’s ilk. The same is true for parsers, but from a machine-learning point of view this is not problematic, since the"
2021.naacl-main.11,H05-1066,0,0.145022,"Missing"
2021.naacl-main.11,P19-1459,0,0.0285533,"ctic structures (Jawahar et al., 2019) (Tian et al., 2020) Recently, unsupervised language models like Further work has found impressive deBERT (Devlin et al., 2019) have become popular grees of syntactic structure in Transwithin natural language processing (NLP). These former encodings (Hewitt and Manning, pre-trained sentence encoders, known affection2019) (Soulos et al., 2020) ately as BERToids (Rogers et al., 2020), have 1 pushed forward the state of the art in many NLP Methods which analyse stimuli are also sometimes termed tasks. Given their impressive performance, a nat- ‘probes’ (e.g. Niven and Kao, 2019), but in this paper we use the term to refer specifically to supervised models. ural question to ask is whether models like these 2 Jawahar et al. (2019) and Hewitt and Manning (2019) are implicitly learn to encode linguistic structures, such more reserved about their claims; these examples merely show as part-of-speech tags or dependency trees. how such work is frequently interpreted, regardless of intent. 124 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 124–131 June 6–11, 2021. ©2021 Asso"
2021.naacl-main.11,L16-1262,0,0.0335183,"Missing"
2021.naacl-main.11,2020.acl-main.420,1,0.776485,"on The main distinction between syntactic probes and the representations at multiple layers, and choose dependency parsers is one of researcher intent— whichever layers result in the best performance probes are not meant to best the state of the art, but on the development set. For each Transformer are a visualisation method (Hupkes and Zuidema, model, we also train probes on the layer 0 em2018). As such, probes are typically minimally beddings; we can treat these layer 0 embeddings parameterised so they do not “dig” for information as baselines since they are uncontextualised, with (but see Pimentel et al., 2020). If a syntactic probe knowledge only of a single word and where it sits performs well using a model’s representations, it is in a sentence, but no knowledge of the other words. argued that that model implicitly encodes syntax. As an additional baseline representation to probe, 4 we use FastText embeddings (Bojanowski et al., More specifically, for nouns we treat elements annotated (in UD notation) with Number=Sing or Number=Plur; 2017) appended with BERT position embeddings for verbs we treat VerbForm=Inf, VerbForm=Fin (Fast+Pos). We emphasise that none of these |Mood=Ind |Number=Sing |Person"
2021.naacl-main.11,2020.tacl-1.54,0,0.021938,"w demonstrating how a couple of syntactic probing papers have been interpreted:2 [The training objectives of BERT/GPT2/XLNet] have shown great abilities to capture dependency between words and syntactic structures (Jawahar et al., 2019) (Tian et al., 2020) Recently, unsupervised language models like Further work has found impressive deBERT (Devlin et al., 2019) have become popular grees of syntactic structure in Transwithin natural language processing (NLP). These former encodings (Hewitt and Manning, pre-trained sentence encoders, known affection2019) (Soulos et al., 2020) ately as BERToids (Rogers et al., 2020), have 1 pushed forward the state of the art in many NLP Methods which analyse stimuli are also sometimes termed tasks. Given their impressive performance, a nat- ‘probes’ (e.g. Niven and Kao, 2019), but in this paper we use the term to refer specifically to supervised models. ural question to ask is whether models like these 2 Jawahar et al. (2019) and Hewitt and Manning (2019) are implicitly learn to encode linguistic structures, such more reserved about their claims; these examples merely show as part-of-speech tags or dependency trees. how such work is frequently interpreted, regardless of"
2021.naacl-main.11,2020.blackboxnlp-1.23,0,0.0175792,"instance, consider the two excerpts below demonstrating how a couple of syntactic probing papers have been interpreted:2 [The training objectives of BERT/GPT2/XLNet] have shown great abilities to capture dependency between words and syntactic structures (Jawahar et al., 2019) (Tian et al., 2020) Recently, unsupervised language models like Further work has found impressive deBERT (Devlin et al., 2019) have become popular grees of syntactic structure in Transwithin natural language processing (NLP). These former encodings (Hewitt and Manning, pre-trained sentence encoders, known affection2019) (Soulos et al., 2020) ately as BERToids (Rogers et al., 2020), have 1 pushed forward the state of the art in many NLP Methods which analyse stimuli are also sometimes termed tasks. Given their impressive performance, a nat- ‘probes’ (e.g. Niven and Kao, 2019), but in this paper we use the term to refer specifically to supervised models. ural question to ask is whether models like these 2 Jawahar et al. (2019) and Hewitt and Manning (2019) are implicitly learn to encode linguistic structures, such more reserved about their claims; these examples merely show as part-of-speech tags or dependency trees. how such work"
2021.naacl-main.11,2020.acl-main.374,0,0.0179497,"uidema, 2018); if the probe is able to perform the task well, then it is argued that the model has learnt to implicitly encode that structure in its representation.1 Work from this inchoate probing literature is frequently cited to support the claim that models like BERT encode a large amount of syntactic knowledge. For instance, consider the two excerpts below demonstrating how a couple of syntactic probing papers have been interpreted:2 [The training objectives of BERT/GPT2/XLNet] have shown great abilities to capture dependency between words and syntactic structures (Jawahar et al., 2019) (Tian et al., 2020) Recently, unsupervised language models like Further work has found impressive deBERT (Devlin et al., 2019) have become popular grees of syntactic structure in Transwithin natural language processing (NLP). These former encodings (Hewitt and Manning, pre-trained sentence encoders, known affection2019) (Soulos et al., 2020) ately as BERToids (Rogers et al., 2020), have 1 pushed forward the state of the art in many NLP Methods which analyse stimuli are also sometimes termed tasks. Given their impressive performance, a nat- ‘probes’ (e.g. Niven and Kao, 2019), but in this paper we use the term to"
2021.naacl-main.11,D19-1286,0,0.0175733,"case of BERT for one probe. This begs the question: what empirical scores constitute knowing syntax? 1 ’Twas Brillig, and the Slithy Toves There are two strains of research that investigate this question. On one hand, stimuli-analysis compares the relative probabilities a language model assigns to words which could fill a gap in a clozestyle task. This allows the experimenter to test whether neural models do well at capturing specific linguistic phenomena, such as subject–verb agreement (Linzen et al., 2016; Gulordava et al., 2018) or negative-polarity item licensing (Marvin and Linzen, 2018; Warstadt et al., 2019). Another strain of research directly analyses the neural network’s representations; this is called probing. Probes are supervised models which attempt to predict a target linguistic structure using a model’s representation as its input (e.g. Alain and Bengio, 2017; Conneau et al., 2018; Hupkes and Zuidema, 2018); if the probe is able to perform the task well, then it is argued that the model has learnt to implicitly encode that structure in its representation.1 Work from this inchoate probing literature is frequently cited to support the claim that models like BERT encode a large amount of sy"
2021.naacl-main.11,P19-1230,0,0.0135847,"re 1: Chomsky’s classic, albeit with the spelling corrected. Syntactic probes are typically evaluated on realworld data, not on Chomsky-style sentences of (1)’s ilk. The same is true for parsers, but from a machine-learning point of view this is not problematic, since the goal of a statistical parser is to parse well the data that one may encounter in the real world. The probing literature, however, is inherently making a epistemological claim: whether BERT knows syntax.3 Indeed, we already know that BERT significantly improves the performance of statistical parsing models on real-world data (Zhou and Zhao, 2019); there is no reason to develop specialist probes to reinforce that claim. As probing consider a scientific qustion, it follows that the probing literature needs to consider syntax from a linguistic point of view and, thus, it requires a linguistic definition of syntax. At least in the generative tradition, it taken as definitional that grammaticality, i.e. syntactic well-formedness, is distinct from the meaning of the sentence. It is this distinction that the nascent syntactic probing literature has overlooked. 3 Generating Jabberwocky Sentences To tease apart syntax and semantics when evalua"
2021.naacl-main.12,W19-7703,0,0.0129964,"the RBF kernel must be responsible. In this self-attention mechanism; allowing BERT to pay section, we argue that the reason that the RBF attention to syntactically close words when solving kernel serves as such a boon to probing is that it the cloze language modeling task. Being attentive resembles BERT’s attention mechanism; recall to syntactically close words would also be supthat BERT’s attention mechanism is defined as ported by recent linguistic research, since words   &gt; sharing syntactic dependencies have higher mutual (Khi ) (Qhj ) √ att(hi , hj ) ∝ exp (12) information on average (Futrell et al., 2019). d2 The representations we analyze, though, are where K and Q are linear transformations and taken from BERT’s final layer; as such, they are d2 is the dimension vectors are projected into. not trained to be used in any self-attention layer— 5 so why should such a resemblance be relevant? Significance was established using paired permutation tests with 10,000 samples, to the level of p &lt; 0.05. BERT’s architecture is based on the Transformer 135 (Vaswani et al., 2017), and uses skip connections between each self-attention layer. Such skip connections create an incentive for residual learning,"
2021.naacl-main.12,2020.acl-main.659,1,0.804901,"similar to the original linear case in eq. (2), but with a scaling term − 2σ1 2 and a non-linearity exp(·). Finally, we consider, the sigmoid kernel, which is defined as 2 The trace norm regularizer is the matrix analogue of the L1 regularizer and it encourages the matrix A to be low rank. As Jain et al. (2010) point out, using a low-rank transformation in conjunction with a kernel corresponds to a supervised kernel dimensionality reduction method. 5 Experiments We experiment with Hewitt and Manning’s (2019) probe on 6 typologically diverse languages, following the experimental design of Hall Maudslay et al. (2020). Our data comes from the Universal Dependency 2.4 Treebank (Nivre et al., 2019), providing sentences and their dependency trees, annotated using the Universal Dependencies annotation scheme.3 For each sentence we calculate contextual representations using multilingual BERT. For all languages, we took the first 12,000 sentences (or the maximum number thereof) in the train portion of the treebank and created new 80–10–10 train–test–dev splits.4 3 κsig (hi , hj ) = tanh (a(Bhi )&gt; (Bhj ) + b) (9) this syntax tree reconstruction task—selectivity control tasks work at the word type level, as oppose"
2021.naacl-main.12,2020.emnlp-main.13,0,0.0229171,"we took the first 12,000 sentences (or the maximum number thereof) in the train portion of the treebank and created new 80–10–10 train–test–dev splits.4 3 κsig (hi , hj ) = tanh (a(Bhi )&gt; (Bhj ) + b) (9) this syntax tree reconstruction task—selectivity control tasks work at the word type level, as opposed to the sentence one. 2 Lin and Lin (2003) observe that it is difficult to effectively tune a and b in the sigmoid kernel. They also note that although this kernel is not in fact PSD, it is PSD when a and b are both positive, which we enforce in this work. (11) It was recently demonstrated by Kuznetsov and Gurevych (2020) that choice of linguistic formalism may have an impact on probing results. In this work, we investigate using only one formalism, so we cannot be sure that our results would not differ if an alternative formalism were used. Nonetheless, we believe that the results that we find most interesting, which are discussed in §6, should be robust to a change in formalism, since their explanation lies in the way attention is calculated in the transformer architecture. 4 We cap the maximum number of sentences analyzed as a naïve control for our multilingual analysis. 134 Basque English Finnish Korean Ta"
2021.naacl-main.12,N18-1202,0,0.116149,"Missing"
2021.naacl-main.12,2020.emnlp-main.254,1,0.844818,"Missing"
2021.naacl-main.12,2020.acl-main.420,1,0.882185,"Missing"
2021.naacl-main.12,N19-1329,1,0.929075,"simply learning to perform an ysis of BERT’s attention, asserting it is a rough NLP task (Hewitt and Liang, 2019; Zhang and Bow- approximation to an RBF kernel. As such, it is not man, 2018; Voita and Titov, 2020) This preference surprising that the syntactic information in BERT for simplicity has often led researchers to place re- representations is more accessible with this spestrictions on probe designs that may not allow them cific non-linear transformation. We conclude that to fully exploit the structure in which information is kernelization is a useful tool for analyzing contexencoded (Saphra and Lopez, 2019; Pimentel et al., tual representations—enabling us to run controlled 132 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 132–138 June 6–11, 2021. ©2021 Association for Computational Linguistics experiments and investigate the structure in which information is encoded. 2 The Structural Probe Hewitt and Manning (2019) introduce the structural probe, a novel model designed to probe for syntax in contextual word representations. We review their formulation here and build upon it in §4. A sentence"
2021.naacl-main.12,P19-1580,0,0.0217565,"distance increases (while an examination of the unkernelized loss function reveals the opposite behavior). This means that it will be less sensitive to the distances between syntactically distant words and focus more on words with small distances. This may partially explain its better performance on UUAS, and comparably worse performance as measured by correlation (which counts pairwise differences between all words, not just those which are directly attached in the tree). Furthermore, our probe’s focus on nearby words resembles the general attentional bias towards syntactically close words (Voita et al., 2019). The direct resemblance between self-attention mechanisms and our proposed probe metric poses a new way of understanding results from more complex probes. While Reif et al. (2019) understood the Euclidean-squared distance of Hewitt and Manning as an isometric tree embedding, their geometric interpretation did not factor in the rest of BERT’s architecture. Such simplified contextless probes cannot tell us how linguistic properties are processed by a sequence of learned modules (Saphra and Lopez, 2019). However, we consider representations in the context of the model which is expected to employ"
2021.naacl-main.12,2020.emnlp-main.14,0,0.0121838,"lin et al., 2019) and ELMo (Peters non-linearity helps—a structural probe based on a et al., 2018). These probes tend to be designed with radial-basis function (RBF) kernel improves perforsimplicity in mind and with the intent of revealing mance significantly in all 6 languages tested over what linguistic structure is encoded in an embed- a linear structural probe. We then perform an analding, rather than simply learning to perform an ysis of BERT’s attention, asserting it is a rough NLP task (Hewitt and Liang, 2019; Zhang and Bow- approximation to an RBF kernel. As such, it is not man, 2018; Voita and Titov, 2020) This preference surprising that the syntactic information in BERT for simplicity has often led researchers to place re- representations is more accessible with this spestrictions on probe designs that may not allow them cific non-linear transformation. We conclude that to fully exploit the structure in which information is kernelization is a useful tool for analyzing contexencoded (Saphra and Lopez, 2019; Pimentel et al., tual representations—enabling us to run controlled 132 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Hum"
2021.naacl-main.12,W18-5448,0,0.040564,"Missing"
2021.naacl-main.181,2020.acl-main.463,0,0.0199868,"of that primary right. While there might be a precedent present for the secondary Articles, the probability is high that our models will not have the chance to train on them because they appear late and because our method truncates text due to computational complexity reasons. This could explain why for these Articles, all our models trained on the precedent cases underperform when compared to the models trained on the facts of the case alone. 6.3 Model Limitations BERT being able to reason, it is merely very good at utilising the artefacts in the data when compared to previous approaches. As Bender and Koller (2020) contend a system can’t ever learn meaning from form alone. According to their view, description of the case facts alone will never fully capture the reality of the world the claimant inhabits. On the other hand, there is some evidence towards transformers being able to reason over simple sentences Clark et al. (2020). While this is encouraging, legal documents are far more complicated than the simple sentences considered in the study above. Either way, the models’ ability to reason in the way a human lawyer would is certainly limited and could explain the diminished performance for the more c"
2021.naacl-main.181,P19-1424,0,0.267634,"eated as operating under precedential law, in the vein of common law countries. This is not a given, as the ECtHR is an international court of highest appeal without a formal doctrine of stare decisis (Jacob, 2014), but there is nevertheless strong evidence that it is precedential. This evidence comes from the court’s own guidelines (ECtHR, 2014), but can also be found in the writings of a former judge of the ECtHR (Zupancic, 2016) and of legal scholars (Lupu and Voeten, 2010). Second, there is existing research on the neural modeling of ECtHR case law we can build upon (Aletras et al., 2016; Chalkidis et al., 2019, 2020). Third, the documents of the ECtHR 3 Nats are computed with ln, while bits use log2 . case law, unlike those of most other courts, textually separate the facts from the arguments, which is crucial for our experiments. Case facts are descriptions of what had happened to the claimant before they went to the court; they include domestic proceedings of their case before it was appealed to the ECtHR as a form of a last resort. They do not contain any reference to European Convention of Human Rights (ECHR) Articles or ECtHR case law. Arguments on the other hand contain judges’ discussion of"
2021.naacl-main.181,N19-1423,0,0.0139735,"y for this, approximating them as the difference between two cross-entropies: MI(O; H |F ) ≈ Hθ (O |F ) − Hθ (O |H, F ) MI(O; G |F ) ≈ Hθ (O |F ) − Hθ (O |G, F ) Indeed, although several estimates for the mutual information exist, McAllester and Stratos (2020) argues that estimating it as this difference is the most statistically justified way. These conditional entropies are themselves approximated through their sample estimate. For instance, we compute: 1 X Hθ (O |G, F ) ≈ − log pθ (oc |gc , fc ) |C| L ONGFORMER is built on the same T RANS FORMER (Vaswani et al., 2017) architecture as BERT (Devlin et al., 2019), but allows for up to 4,096 tokens, using an attention mechanism which scales linearly, instead of quadratically. We choose this architecture in particular as it achieves stateof-the-art performance in tasks similar to ours, e.g. on the IMDB sentiment classification (Maas et al., 2011) and Hyperpartisan news detection (Kiesel et al., 2019). To find the probability of violation of the K Articles we compute: h = L ONGFORMER(g, f ) (14) pθ (o |g, f ) = σ(W (1) ReLU(W (2) h)) c∈C (10) which is exact as |C |→ ∞. We note that the crossentropy is an upper bound on the entropy, which uses a model pθ"
2021.naacl-main.181,N18-1175,0,0.0273371,"cle 7: No punishment without law, our precedent models fail to learn any additional information from the precedent facts or arguments. This might simply be the result of an insufficient representation of Article 7 in training cases, or of its appearance truncated out of the input. However it also raises the question of what a T RANS FORMER model can learn. The nascent field of BERTology has explored exactly this question (Rogers et al., 2020; Pimentel et al., 2020). In particular the work of Niven and Kao (2019), examining BERT performance on the English Argument Reasoning Comprehension Task (Habernal et al., 2018), suggest that instead of 2282 10 Caselaw Access Project:, https://case.law mation retrieval for the first time by Rabelo et al. (2020). This improved access to a high quality common law datasets has opened up a potential for new work in the field of legal AI. Particularly similar to our work is the study done by Sim et al. (2016), who have considered the influence of petitioners and responders (amicus) briefs on the US Supreme Court decision and opinions. 8 Conclusion In this paper, we have shifted the focus of legal AI research from practical tasks such as precedent retrieval or outcome pred"
2021.naacl-main.181,S19-2145,0,0.0228148,"way. These conditional entropies are themselves approximated through their sample estimate. For instance, we compute: 1 X Hθ (O |G, F ) ≈ − log pθ (oc |gc , fc ) |C| L ONGFORMER is built on the same T RANS FORMER (Vaswani et al., 2017) architecture as BERT (Devlin et al., 2019), but allows for up to 4,096 tokens, using an attention mechanism which scales linearly, instead of quadratically. We choose this architecture in particular as it achieves stateof-the-art performance in tasks similar to ours, e.g. on the IMDB sentiment classification (Maas et al., 2011) and Hyperpartisan news detection (Kiesel et al., 2019). To find the probability of violation of the K Articles we compute: h = L ONGFORMER(g, f ) (14) pθ (o |g, f ) = σ(W (1) ReLU(W (2) h)) c∈C (10) which is exact as |C |→ ∞. We note that the crossentropy is an upper bound on the entropy, which uses a model pθ (o |•) for its estimate. The better this model, the tighter our estimates will be. The only thing left to do now, is to obtain these probability estimates. We thus model Halsbury’s view as a classification task (see Figure 2) estimating the probability: pθ (o |h, f ) = K Y pθ (ok |h, f ) (11) k=1 We analogously model Goodhart’s view as: pθ"
2021.naacl-main.181,P11-1015,0,0.00562464,"this difference is the most statistically justified way. These conditional entropies are themselves approximated through their sample estimate. For instance, we compute: 1 X Hθ (O |G, F ) ≈ − log pθ (oc |gc , fc ) |C| L ONGFORMER is built on the same T RANS FORMER (Vaswani et al., 2017) architecture as BERT (Devlin et al., 2019), but allows for up to 4,096 tokens, using an attention mechanism which scales linearly, instead of quadratically. We choose this architecture in particular as it achieves stateof-the-art performance in tasks similar to ours, e.g. on the IMDB sentiment classification (Maas et al., 2011) and Hyperpartisan news detection (Kiesel et al., 2019). To find the probability of violation of the K Articles we compute: h = L ONGFORMER(g, f ) (14) pθ (o |g, f ) = σ(W (1) ReLU(W (2) h)) c∈C (10) which is exact as |C |→ ∞. We note that the crossentropy is an upper bound on the entropy, which uses a model pθ (o |•) for its estimate. The better this model, the tighter our estimates will be. The only thing left to do now, is to obtain these probability estimates. We thus model Halsbury’s view as a classification task (see Figure 2) estimating the probability: pθ (o |h, f ) = K Y pθ (ok |h, f"
2021.naacl-main.181,P19-1459,0,0.0157045,"loped a satisfying legal test for them, our models simply aren’t able to learn it. For example for Article 7: No punishment without law, our precedent models fail to learn any additional information from the precedent facts or arguments. This might simply be the result of an insufficient representation of Article 7 in training cases, or of its appearance truncated out of the input. However it also raises the question of what a T RANS FORMER model can learn. The nascent field of BERTology has explored exactly this question (Rogers et al., 2020; Pimentel et al., 2020). In particular the work of Niven and Kao (2019), examining BERT performance on the English Argument Reasoning Comprehension Task (Habernal et al., 2018), suggest that instead of 2282 10 Caselaw Access Project:, https://case.law mation retrieval for the first time by Rabelo et al. (2020). This improved access to a high quality common law datasets has opened up a potential for new work in the field of legal AI. Particularly similar to our work is the study done by Sim et al. (2016), who have considered the influence of petitioners and responders (amicus) briefs on the US Supreme Court decision and opinions. 8 Conclusion In this paper, we hav"
2021.naacl-main.181,C18-1041,0,0.0278454,"mance by the early 2000’s (Ashley, 2017). These systems however proved too brittle to keep up with the everchanging legal landscape and never transitioned from research into industry. More recently, a new wave of deep learning methods has reinvigorated the research interest in legal AI. The majority of this new work has been conducted on statutory legal systems which do not rely on the doctrine of precedent to nearly the same extent as their common law counterparts. For instance, in Chinese law the use of neural models for case outcome classification has already been investigated extensively (Hu et al., 2018; Zhong et al., 2018; Xu et al., 2020). In the precedential legal domain, smaller corpora of annotated cases have been investigated over the years (Grover et al., 2003; Valvoda et al., 2018). However, large-scale corpora necessary for deep learning architectures have become available only recently. The Caselaw Access Project10 introduced a large dataset of American case law in 2018. Aletras et al. (2016) have introduced the ECtHR corpus, and Chalkidis et al. (2019) have run deep neural networks on it in order to predict outcome. Similarly, the Canadian Supreme Court Case corpus has been used i"
2021.naacl-main.181,P19-1171,1,0.920019,"uncated to 512 tokens. Outcome of the precedent is concatenated with either the precedent facts or arguments, and both are jointly truncated at 512 tokens. Finally, these are concatenated together and embedded in 768 dimensions before being fed into the L ONG FORMER . analogy; hence it is the good alignment between the facts of the two cases that leads to consistent outcomes. 3 Operationalising Halsbury and Goodhart. In this work, we intend to measure the use of Halsbury’s and Goodhart’s views in practice, which we operationalise information-theoretically following the methodology proposed by Pimentel et al. (2019). To test the hypothesis, we construct two collections of random variables, which we denote H and G. We define an instance hc of random variable H as the union of arguments S and outcomes for all precedent cases of c, i.e. c0 ∈Pc {ac0 , oc0 }. We will denote the instance h when referring to it in the abstract (without referring to a particular case). We analogously Sdefine instances of random variable G as gc = c0 ∈Pc {fc0 , oc0 }. While the set-theoretic notation may seem tedious, it encompasses the essence of the distinction between Halsbury’s and Goodhart’s view: Each view hypothesises a di"
2021.naacl-main.181,2021.naacl-main.349,1,0.752052,"Missing"
2021.naacl-main.181,2020.acl-main.420,1,0.833816,"3 counterpart. So while the judges might have developed a satisfying legal test for them, our models simply aren’t able to learn it. For example for Article 7: No punishment without law, our precedent models fail to learn any additional information from the precedent facts or arguments. This might simply be the result of an insufficient representation of Article 7 in training cases, or of its appearance truncated out of the input. However it also raises the question of what a T RANS FORMER model can learn. The nascent field of BERTology has explored exactly this question (Rogers et al., 2020; Pimentel et al., 2020). In particular the work of Niven and Kao (2019), examining BERT performance on the English Argument Reasoning Comprehension Task (Habernal et al., 2018), suggest that instead of 2282 10 Caselaw Access Project:, https://case.law mation retrieval for the first time by Rabelo et al. (2020). This improved access to a high quality common law datasets has opened up a potential for new work in the field of legal AI. Particularly similar to our work is the study done by Sim et al. (2016), who have considered the influence of petitioners and responders (amicus) briefs on the US Supreme Court decision"
2021.naacl-main.181,2020.tacl-1.54,0,0.0161054,"g than their Article 3 counterpart. So while the judges might have developed a satisfying legal test for them, our models simply aren’t able to learn it. For example for Article 7: No punishment without law, our precedent models fail to learn any additional information from the precedent facts or arguments. This might simply be the result of an insufficient representation of Article 7 in training cases, or of its appearance truncated out of the input. However it also raises the question of what a T RANS FORMER model can learn. The nascent field of BERTology has explored exactly this question (Rogers et al., 2020; Pimentel et al., 2020). In particular the work of Niven and Kao (2019), examining BERT performance on the English Argument Reasoning Comprehension Task (Habernal et al., 2018), suggest that instead of 2282 10 Caselaw Access Project:, https://case.law mation retrieval for the first time by Rabelo et al. (2020). This improved access to a high quality common law datasets has opened up a potential for new work in the field of legal AI. Particularly similar to our work is the study done by Sim et al. (2016), who have considered the influence of petitioners and responders (amicus) briefs on the US"
2021.naacl-main.181,D16-1178,0,0.0263346,"RMER model can learn. The nascent field of BERTology has explored exactly this question (Rogers et al., 2020; Pimentel et al., 2020). In particular the work of Niven and Kao (2019), examining BERT performance on the English Argument Reasoning Comprehension Task (Habernal et al., 2018), suggest that instead of 2282 10 Caselaw Access Project:, https://case.law mation retrieval for the first time by Rabelo et al. (2020). This improved access to a high quality common law datasets has opened up a potential for new work in the field of legal AI. Particularly similar to our work is the study done by Sim et al. (2016), who have considered the influence of petitioners and responders (amicus) briefs on the US Supreme Court decision and opinions. 8 Conclusion In this paper, we have shifted the focus of legal AI research from practical tasks such as precedent retrieval or outcome prediction, to a theoretical question: which aspect of the precedent is most important in forming the law? To this end, we trained a similar neural modeling approach as Chalkidis et al. (2019) to predict the outcome of a case on the ECtHR dataset, and inspected the difference in the mutual information between our operationalisations o"
2021.naacl-main.181,2020.acl-main.597,1,0.842025,"no uncertainty left. Conversely, if the variables are independent, then H(O) = H(O |G), where H(O) denotes the unconditional entropy of the outcomes O. We now note a common decomposition of mutual information that will help with the approximation: MI(O; H |F ) = H(O |F ) − H(O |H, F ) (5) MI(O; G |F ) = H(O |F ) − H(O |G, F ) (6) In this work, we consider the conditional probabilities p(o |•) as the independent QK product of each Article’s probability, i.e. k=1 p(ok |•). Information-theoretically, then, they are related through the following equation: H(O |•) = K X H(Ok |•) (7) k=1 Following Williams et al. (2020), we further calculate the uncertainty coefficient (Theil, 1970) of each of these mutual informations. These coefficients are easier to interpret, representing the percentage of uncertainty reduced by the knowledge of a random variable: MI(O; H |F ) H(O |F ) MI(O; G |F ) U(O |G; F ) = H(O |F ) U(O |H; F ) = 4 (8) (9) Experimental Setup We choose to work with the ECtHR corpus for three reasons. First, it can be treated as operating under precedential law, in the vein of common law countries. This is not a given, as the ECtHR is an international court of highest appeal without a formal doctrine"
2021.naacl-main.181,2020.acl-main.280,0,0.0210805,"17). These systems however proved too brittle to keep up with the everchanging legal landscape and never transitioned from research into industry. More recently, a new wave of deep learning methods has reinvigorated the research interest in legal AI. The majority of this new work has been conducted on statutory legal systems which do not rely on the doctrine of precedent to nearly the same extent as their common law counterparts. For instance, in Chinese law the use of neural models for case outcome classification has already been investigated extensively (Hu et al., 2018; Zhong et al., 2018; Xu et al., 2020). In the precedential legal domain, smaller corpora of annotated cases have been investigated over the years (Grover et al., 2003; Valvoda et al., 2018). However, large-scale corpora necessary for deep learning architectures have become available only recently. The Caselaw Access Project10 introduced a large dataset of American case law in 2018. Aletras et al. (2016) have introduced the ECtHR corpus, and Chalkidis et al. (2019) have run deep neural networks on it in order to predict outcome. Similarly, the Canadian Supreme Court Case corpus has been used in inforAnother possible explanation fo"
2021.naacl-main.349,P82-1020,0,0.688361,"Missing"
2021.naacl-main.349,2020.tacl-1.1,1,0.891824,"ngth (Blasi et al., 2016). While useful, the estimates emerging from this type of study can be regarded as lower bounds to the total amount of non-arbitrary associations found in the vocabulary. Recent efforts have resulted in datasets with thousands of languages (Wichmann et al., 2020), with which linguists can look for universal statistical patterns (Wichmann et al., 2010; Blasi et al., 2016). These studies, though, only looked at the presence (or not) of individual phones in words, not accounting for their connections. Our methods rely on neural phonotactic models, similar to those used by Pimentel et al. (2020), thus capturing a broader range of potential correspondences. 3 Data An exceptional resource with substantial crosslinguistic representation is provided in the Automated Similarity Judgment Program, better known by its acronym ASJP (Wichmann et al., 2020). ASJP is a collection of basic vocabulary wordlists, i.e. lists of words with referents that are expected to be widely attested across human societies. It involves body parts, some colour terms, lower nu3 merals, general properties (such as big or round), See §3 for a discussion on potential biases in the dataset that likely influence the ac"
2021.naacl-main.349,P19-1171,1,0.588723,"euristic approaches (Bergen, 2004; Wichmann et al., 2010; Johansson and Zlatev, 2013; Haynie et al., 2014; Gutierrez et al., 2016; Blasi et al., 2016; Joo, 2019). Previous studies differ from each other along (at least) three axes: (i) which unit is used to measure wordform similarity (e.g., phonemes, sub-phonemic features or arbitrary sequences); (ii) how they deploy a baseline for statistical comparison (e.g. permute forms with meanings, or propose a generative model that yields wordforms uninformed by their meaning) and (iii) whether they study non-arbitrariness within or across languages. Pimentel et al. (2019) provide the first holistic measure of non-arbitrariness (in a large vocabulary sample of a single language) using tools from information theory, and apply their measure to discover phonesthemes.2 Our work extends their approach to the problem of discovering and estimating the strength of frequent cross-linguistic form–meaning associations (e.g. iconicity and systematicity) in individual concepts. We do this by adapting Pimentel et al.’s (2019) approach, modelling 1 See §2 below for a brief literature review and Dingemanse et al. (2015) for a more comprehensive one. 2 Phonesthemes are sub-morp"
2021.naacl-main.350,P82-1020,0,0.725061,"Missing"
2021.naacl-main.350,2021.eacl-main.3,1,0.722208,"al codes context dependent (Piantadosi et al., 2012), since context often disambiguates words (Dautriche et al., 2018; Pimentel et al., 2020a). Additionally, the meaning or message being conveyed by a given word might bias its form. Within languages, there seems to be a pressure for more semantically similar words to also be more phonologically similar (Monaghan et al., 2014; Dingemanse et al., 2015; Dautriche et al., 2017; Pimentel et al., 2019). Across languages, words for the same referents exhibit detectable patterns in term of their phonological makeup (Blasi et al., 2016), phonotactics (Pimentel et al., 2021b), as well as word length (Lewis and Frank, 2016)—this is driven by semantic features such as size, quality or complexity. Finally, there is a cross-linguistic tendency for lexicons to place higher surprisal in word-initial segments (van Son and Pols, 2003a,b; King and Wedel, 2020; Pimentel et al., 2021a) making words more constrained in their choice of final segments. These aspects of language might also collide with a purely Zipfian conception of lexicon optimality. As stated above, our notion of optimality is derived In this work, however, we consider optimality from Zipf’s principle of le"
2021.naacl-main.350,2020.emnlp-main.328,1,0.929211,"bsence (including morphology and phonotactics/graphotactics). We thus define an upper bound for the compressibility of a lexicon optimized purely for word length efficiency. 2 (Non-)Optimality in the Lexicon Jurafsky et al., 2001; Gahl, 2008). Additionally, words which are typically predictable in context are shorter than words which are less predictable in context (Piantadosi et al., 2011). On another note, a purely coding-theoretically efficient language could make the lexical codes context dependent (Piantadosi et al., 2012), since context often disambiguates words (Dautriche et al., 2018; Pimentel et al., 2020a). Additionally, the meaning or message being conveyed by a given word might bias its form. Within languages, there seems to be a pressure for more semantically similar words to also be more phonologically similar (Monaghan et al., 2014; Dingemanse et al., 2015; Dautriche et al., 2017; Pimentel et al., 2019). Across languages, words for the same referents exhibit detectable patterns in term of their phonological makeup (Blasi et al., 2016), phonotactics (Pimentel et al., 2021b), as well as word length (Lewis and Frank, 2016)—this is driven by semantic features such as size, quality or complex"
2021.naacl-main.350,P19-1171,1,0.828382,"e shorter than words which are less predictable in context (Piantadosi et al., 2011). On another note, a purely coding-theoretically efficient language could make the lexical codes context dependent (Piantadosi et al., 2012), since context often disambiguates words (Dautriche et al., 2018; Pimentel et al., 2020a). Additionally, the meaning or message being conveyed by a given word might bias its form. Within languages, there seems to be a pressure for more semantically similar words to also be more phonologically similar (Monaghan et al., 2014; Dingemanse et al., 2015; Dautriche et al., 2017; Pimentel et al., 2019). Across languages, words for the same referents exhibit detectable patterns in term of their phonological makeup (Blasi et al., 2016), phonotactics (Pimentel et al., 2021b), as well as word length (Lewis and Frank, 2016)—this is driven by semantic features such as size, quality or complexity. Finally, there is a cross-linguistic tendency for lexicons to place higher surprisal in word-initial segments (van Son and Pols, 2003a,b; King and Wedel, 2020; Pimentel et al., 2021a) making words more constrained in their choice of final segments. These aspects of language might also collide with a pure"
2021.naacl-main.350,2020.tacl-1.1,1,0.938766,"bsence (including morphology and phonotactics/graphotactics). We thus define an upper bound for the compressibility of a lexicon optimized purely for word length efficiency. 2 (Non-)Optimality in the Lexicon Jurafsky et al., 2001; Gahl, 2008). Additionally, words which are typically predictable in context are shorter than words which are less predictable in context (Piantadosi et al., 2011). On another note, a purely coding-theoretically efficient language could make the lexical codes context dependent (Piantadosi et al., 2012), since context often disambiguates words (Dautriche et al., 2018; Pimentel et al., 2020a). Additionally, the meaning or message being conveyed by a given word might bias its form. Within languages, there seems to be a pressure for more semantically similar words to also be more phonologically similar (Monaghan et al., 2014; Dingemanse et al., 2015; Dautriche et al., 2017; Pimentel et al., 2019). Across languages, words for the same referents exhibit detectable patterns in term of their phonological makeup (Blasi et al., 2016), phonotactics (Pimentel et al., 2021b), as well as word length (Lewis and Frank, 2016)—this is driven by semantic features such as size, quality or complex"
2021.naacl-main.350,2021.naacl-main.349,1,0.749579,"Missing"
2021.naacl-main.350,P16-1162,0,0.377441,"grapheme alphabet Σ.4 When the exact index is unnecessary in context, we will drop the subscripted n; and we make use of uppercase letters to refer to random variables (e.g. M or W ) where necessary. We will write meanings in typewriter font, e.g. cat, and wordforms in italics: cat (English), kissa (Finnish). Viewing the lexicon from a coding-theoretic perspective, we consider the mapping from meaning to form as a code: C : M → Σ∗ . Every language comes endowed with a natural code Cnat , which 3 We also present results using the additional sub-word tokenizers: byte pair encoding (Gage, 1994; Sennrich et al., 2016) and word piece (Schuster and Nakajima, 2012). See Bostrom and Durrett (2020) for a discussion of the tradeoffs of these schemes, in terms of performance and compressibility. 4 This alphabet is augmented with an end-of-word symbol. Words as Meanings The unigram distribution represents the frequency of each wordform in a text, i.e. the probability of a token without conditioning on context p(W = kissa). In this work, though, we assume the unigram distribution is a distribution over M, e.g. p(M = cat)—this way we can analyze how changing the code C would affect its efficiency. As stated above, t"
2021.naacl-main.350,E14-2006,0,0.0687955,"Missing"
2021.naacl-main.350,2020.emnlp-main.448,0,0.0155628,"and phonotactics) as well This section reviews a few technical results as regards the construction of codes from a probability distribution. If p had finite support—i.e. there were a finite set of possible meanings or wordforms—a simple Huffman encoding (Huffman, 1952) would give us an optimal code for our lexicon. However, this is not the case—p(w) has support on all of Σ∗ —so we might need a more complex strategy to get such a code. Linder et al. (1997) proved the 6 Under this assumption our language model is also conexistence of an optimal encoding for a distribution sistent, as defined by Welleck et al. (2020)—sequences with with infinite support, given that it has finite entropy. infinite length have asymptotically zero probability mass. 4429 as a more abundant prevalence of otherwise rare segments, such as the voiced and voiceless dental fricatives (orthographically expressed with th). These rare segments would be overrepresented in such a na¨ıve training regime, making it hard for the character-level model to correctly represent the language’s graphotactics. In order to address the problem of skewed frequencies, we use a novel neuralization of Goldwater et al.’s (2011) two-stage model to capture"
2021.naacl-main.350,P06-1124,0,0.178205,"= zn |z&lt;n ) ( (z ) c&lt;nn − a ∝ a · K&lt;n + b (10) 1 ≤ zn ≤ K&lt;n (old cluster) zn = K&lt;n + 1 (new cluster) 8 We note two subscripts are used here: k refers to the kth wordform, while t indexes the tth character in the wordform. 9 This generative process allows the same wordform to be sampled multiple times, as they are generated i.i.d. 4430 In this equation, K&lt;n is the current number of (z ) populated clusters; while c&lt;nn is the number of instances currently assigned to cluster zn . The PYCRP has two hyperparameters: 0 ≤ a &lt; 1 and b ≥ 0. The parameter a controls the rate in which the clusters grow (Teh, 2006), while b controls an initial preference for dispersion. Together, these ensure the formation of a long-tail—concocting a power-law distribution for the cluster frequencies. This property allows a cluster with wordform make, for example, to have an exponentially larger frequency than its graphotactic neighbor cake. Modeling Word Tokens. Finally, given the set of wordforms and the cluster assignments, defining the form associated with a token is deterministic. Since each cluster only contains instances of one wordform, the form of a token is defined looking up the label of the cluster it was as"
2021.sigmorphon-1.25,K18-3001,1,0.679933,"l marker used for expressing case, familiarity, plurality, and (sometimes) gender within animate nouns. Pronouns are marked for different cases and honorificity levels. These paradigms are generated on the basis of a manually annotated corpus of Magahi folktales. We used a raw dataset from the literary domain. First, we annotated the dataset with the Universal Dependency morphological feature tags at token level using the CoNLL-U editor (Heinecke, 2019). We then converted the annotated dataset into the UniMorph schema using the script available for converting UD data into the UniMorph tagset (McCarthy et al., 2018). To finalize the data, we manually validated the dataset against the UniMorph schema (Sylak-Glassman et al., 2015a). Brajbhasha, or Braj is one of the Indo-Aryan languages spoken in the Western Indian states of Uttar Pradesh, Madhya Pradesh, and Rajasthan. Grierson (1908) groups Brajbhasha under Western Hindi of the Central Group in the Indo-Aryan family, along with other languages like Hindustani, Bangaru, Kannauji, and Bundeli. Braj is not generally used in education or for any official purposes in any Braj spoken state, but it has a very rich literary tradition. Also in order to preserve,"
2021.sigmorphon-1.25,K17-2001,1,0.858355,"Missing"
2021.sigmorphon-1.25,U19-1001,1,0.893844,"nje-ng ‘1/3PL-again-wrong-BENmeat-cook-PP’ (“I cooked the wrong meat for them again”). As shown, the form has several prefixes and suffixes attached to the stem. As in other Australian languages, long vowels are typically represented by double characters, and trills with “rr”.3 According to Evans’ (2003) analysis, the verb template contains 12 affix slots which include two incorporated noun classes, and derivational affixes such as the benefactive and comitative. The data included in this set are verbs extracted from the Kunwinjku translation of the Bible using the morphological analyzer from Lane and Bird (2019) and manually verified by human annotators. 3.2 Afro-Asiatic The Afro-Asiatic language family is represented by the Semitic subgroup. 3.2.1 Semitic: Classical Syriac Classical Syriac is a dialect of the Aramaic language and is attested as early as the 1st century CE. As with most Semitic languages, it displays non-concatenative morphology involving primarily tri-consonantal roots. Syriac nouns and adjectives are conventionally classified into three ‘states’— Emphatic, Absolute, Construct—which loosely correlate with the syntactic features of definiteness, indeterminacy and the genitive. There"
2021.sigmorphon-1.25,U19-1005,1,0.782664,"respect to morphology and realized in the UniMorph schema (Sylak-Glassman et al., 2015b). Morphosyntactic features (such as “the dative case” or “the past tense”) in the UniMorph occupy an intermediate position between the descriptive categories and comparative concepts. The set of features was initially established on the basis of analysis of typological literature, and refined with the addition of new languages to the UniMorph database (Kirov et al., 2018; McCarthy et al., 2020). Since 2016, SIGMORPHON organized shared tasks on morphological reinflection (Cotterell et al., 2016, 2017, 2018; McCarthy et al., 2019; Vylomova et al., 2020) that aimed at evaluating contemporary systems. Parallel to that, they also served as a platform for enriching the UniMorph database with new languages. For instance, the 2020 shared task (Vylomova et al., 2020) featured 90 typologically diverse languages derived from various linguistic resources. This year, we are bringing many under-resourced languages (languages of Peru, Russia, India, Australia, Papua New Guinea) and dialects (e.g., for Arabic and Kurdish). The sample is highly diverse: it contains languages with templatic, concatenative (fusional and agglutinative)"
2021.sigmorphon-1.25,W02-0604,0,0.0878472,"Missing"
2021.sigmorphon-1.25,U08-1018,0,0.0643919,"through affixation, compounding, or reduplication. The four types of Indonesian affixes are prefixes, suffixes, circumfixes (combination of prefixes and suffixes), and infixes (inside the base form). Indonesian uses both full and partial reduplication processes to form words. Full reduplication is often used to express the plural forms of nouns, while partial reduplication is typically used to derive forms that might have a different category than their base forms. Unlike English, the distinction between inflectional and derivational morphological processes in Indonesian is not always clear (Pisceldo et al., 2008). In this shared task, the Indonesian data is created by bootstrapping the data from an Indonesian Wikipedia dump. Using a list of possible Indonesian affixes, we collect unique word forms from Wikipedia and analyze them using MorphInd (Larasati et al., 2011), a morphological analyzer tool for Indonesian based on an FST. We manually create a mapping between the MorphInd tagset and the UniMorph schema. We then use this mapping and apply some additional rule-based formulas created by Indonesian linguists to build the final dataset (Table 9). 3.9.2 Malayo-Polynesian: Kodi/Kodhi Kodi or Kodhi [koâ"
2021.sigmorphon-1.25,N19-1119,0,0.0213266,"ugmentation technique presented by Anastasopoulos and Neubig (2019). More specifically, the team implemented an encoder–decoder model with an attention mechanism. The encoder processes a character sequence using an LSTM-based RNN with attention. Tags are encoded with a selfattention (Vaswani et al., 2017) position-invariant module. The decoder is an LSTM with separate attention mechanisms for the lemma and the tags. GUClasp focus their efforts on exploring strategies for training a multilingual model, in particular, they implement the following strategies: curriculum learning with competence (Platanios et al., 2019) based on character frequency and L BME GUClasp afb amh ara arz heb syc ame cni ind kod aym ckt itl gup bra bul ces ckb deu kmr mag nld pol por rus spa see ail evn sah tyv krl lud olo vep 92.39 98.16 99.76 95.27 97.46 21.71 82.46 99.5 81.31 94.62 99.98 44.74 32.4 14.75 58.52 98.9 98.03 99.46 97.98 98.21 70.2 98.28 99.54 99.85 98.07 99.82 78.28 6.84 51.9 99.95 99.97 99.88 59.46 99.72 99.72 81.71 93.81 94.86 87.12 89.93 10.57 55.94 93.36 55.68 87.1 99.97 52.63 31.28 21.31 56.91 96.46 94.00 96.60 91.94 98.09 72.24 94.91 98.52 99.11 94.32 97.65 40.97 6.46 51.5 99.69 99.78 98.50 59.46 98.2 97.05 sj"
2021.sigmorphon-1.25,2020.acl-main.597,1,0.915066,"arget inflected form—and removed all forms other than verbs, nouns, or adjectives. We then capped the dataset sizes to a maximum of 100,000 instances per language, subsampling when necessary. Finally, we create a 70–10–20 train–dev–test split per language, splitting the data across these sets at the instance level (as opposed to, e.g., the lemma one). As such, the information about a lemma’s declension or inflection class is spread out across these train, dev and test sets, making this task much simpler than if one had to predict the entire class from the lemma’s form alone, as done by, e.g., Williams et al. (2020) and Liu and Hulden (2021). 5 Baseline Systems The organizers provide four neural systems as baselines, a product of two models and optional data augmentation. The first model is a transformer (Vaswani et al., 2017, TRM), and the second model is an adaption of the transformer to character-level transduction tasks (Wu et al., 2021, CHR-TRM), which holds the state-of-the-art on the 2017 SIGMORPHON shared task data. Both models follow the hyperparameters of Wu et al. (2021). The optional data augmentation follows the technique proposed by Anastasopoulos and Neubig (2019). Rely14 The new languages"
2021.sigtyp-1.11,2021.sigtyp-1.11,1,0.0530913,"Missing"
2021.sigtyp-1.11,2020.lrec-1.520,0,0.132584,"Missing"
2021.sigtyp-1.11,2021.sigtyp-1.12,0,0.156907,"Missing"
2021.sigtyp-1.11,2021.sigtyp-1.13,0,0.183854,"Missing"
2021.sigtyp-1.11,2020.lrec-1.800,0,0.133837,"Missing"
2021.sigtyp-1.11,2020.acl-main.560,0,0.102083,"Missing"
2021.sigtyp-1.11,P05-1064,0,0.85596,"Missing"
cotterell-callison-burch-2014-multi,N12-1006,1,\N,Missing
cotterell-callison-burch-2014-multi,P11-2007,1,\N,Missing
cotterell-callison-burch-2014-multi,W10-0701,1,\N,Missing
cotterell-callison-burch-2014-multi,D13-1007,0,\N,Missing
cotterell-callison-burch-2014-multi,W13-2317,0,\N,Missing
cotterell-callison-burch-2014-multi,elfardy-diab-2012-simplified,0,\N,Missing
cotterell-callison-burch-2014-multi,al-sabbagh-girju-2012-yadac,0,\N,Missing
cotterell-callison-burch-2014-multi,P13-2081,0,\N,Missing
cotterell-callison-burch-2014-multi,al-sabbagh-girju-2010-mining,0,\N,Missing
cotterell-callison-burch-2014-multi,W13-1102,0,\N,Missing
D15-1108,P03-1006,0,0.0606744,"2). To improve the method, recall that subproblem k considers only variables X k . It is indifferent to the value of Xi if Xi ∈ / X k , so we just leave xki undefined in the subproblem’s solution. We treat that as automatically satisfying the equality constraint; thus we do not need any Lagrange multiplier λki to force equality. Our final solution x ignores undefined values, and sets xi to the value agreed on by the subproblems that did consider Xi .7 4.2 Gki (xk ) = λki · γ(xki ) def (4) These unary factors penalize strings according to the Lagrange multipliers. They can be encoded as WFSAs (Allauzen et al., 2003; Cotterell and Eisner, 2015, Appendices B.1–B.5), allowing us to solve the subproblem by max-product BP as usual. The topology of the WFSA for Gki depends only on Wi , while its weights come from λki . 4.3 Projected Subgradient Method We aim to adjust the collection λ of Lagrange multipliers to minimize the upper bound (3). Following Komodakis et al. (2007), we solve this convex dual problem using a projected subgradient method. We initialize λ = 0 and compute (3) by solving the K subproblems. Then we take a step to adjust λ, and repeat in hopes of eventually satisfying the equality condition"
D15-1108,D07-1093,0,0.086226,"Missing"
D15-1108,D10-1125,0,0.0361387,"inite-state methods to decode a composition of several finite-state noisy channels (Pereira and Riley, 1997; Knight and Graehl, 1998) can be regarded as BP on a graphical model over strings that has a linear-chain topology. 919 4 Dual Decomposition rεzɪgn eɪʃən rεzɪgn#eɪʃən rizajn z rizajn#z dæmn eɪʃən dæmn#eɪʃən r,εzɪgn’eɪʃn riz’ajnz d,æmn’eɪʃn Subproblem 1 Subproblem 2 Subproblem 3 dæmn z Dual decomposition is a general technique for solving constrained optimization problems. It has been widely used for MAP inference in graphical models (Komodakis et al., 2007; Komodakis and Paragios, 2009; Koo et al., 2010; Martins et al., 2011; Sontag et al., 2011; Rush and Collins, 2014). However, previous work has focused on variables Xi whose values are in R or a small finite set; we will consider the infinite set Σ∗ . dæmn#z d’æmz Subproblem 4 Figure 2: To apply dual decomposition, we choose to decompose 1 into one subproblem per surface word. Dashed lines connect two or more variables from different subproblems that correspond to the same variable in the original graph. The method of Lagrange multipliers is used to force these variables to have identical values. An additional unary factor attached to each"
D15-1108,N15-1094,1,0.87548,"work, we assume that a degree-d factor is a d-way rational relation, i.e., a function of d strings that can be computed by a d-tape weighted finite-state machine (WFSM) (Mohri et al., 2002; Kempe et al., 2004). Such a machine is called an acceptor (WFSA) if d = 1 or a transducer (WFST) if d = 2.3 Past work has shown how to approximately sample from the distribution over x defined by such a model (Bouchard-Cˆot´e et al., 2007), or approximately compute the distribution’s marginals using variants of sum-product belief propagation (BP) (Dreyer and Eisner, 2009) and expectation propagation (EP) (Cotterell and Eisner, 2015). 2.2 2.3 def score(x) = X F (x). (1) F ∈F The String Case Graphical models over strings have enjoyed some attention in the NLP community. Tree-shaped graphical models naturally model the evolutionary tree of word forms (Bouchard-Cˆot´e et al., 2007; Bouchard-Cˆot´e et al., 2008; Hall and Klein, 2010; Hall and Klein, 2011). Cyclic graphical Finite-State Belief Propagation BP iteratively updates messages between factors and variables. Each message is a vector whose elements score the possible values of a variable. Murphy et al. (1999) discusses BP on cyclic (“loopy”) graphs. For pedagogical rea"
D15-1108,P14-2102,1,0.822861,"ability of copying the next character of the underlying word as it is transduced to the surface word. The remaining probability mass 1−θ is apportioned equally among insertion, substitution and deletion operations.11 This models phonology as “noisy concatenation”—the minimum necessary to account for the fact that surface words cannot quite be obtained as simple concatenations of their shared underlying morphemes. Model 2 is a replication of the much more complicated parametric model of Cotterell et al. (2015), which can handle linguistic phonology. Here the factor Sθ is a contextual edit FST (Cotterell et al., 2014). The probabilities of competing edits in a given context are determined by a loglinear model with weight vector θ and features that are meant to pick up on phonological phenomena. E XERCISE Small datasets of Catalan, English, Maori, and Tangale, drawn from phonology textbooks. Each dataset contains 55 to 106 surface words, formed from a collection of 16 to 55 morphemes. CELEX Larger datasets of German, English, and Dutch, drawn from the CELEX database (Baayen et al., 1995). Each dataset contains 1000 surface words, formed from 341 to 381 underlying morphemes. 5.2 Evaluation Scheme We compared"
D15-1108,Q15-1031,1,0.10354,"noisy forms; contemporary or historical forms; underlying or surface forms; source or target language forms. Such relationships arise in domains such as morphology, phonology, historical linguistics, translation between related languages, and social media text analysis. In this paper, we assume a given graphical model, whose factors evaluate the relationships among observed and unobserved strings.1 We present a dual decomposition algorithm for MAP inference, which returns a certifiably optimal solution when it converges. We demonstrate our method on a graphical model for phonology proposed by Cotterell et al. (2015). We show that the method generally converges and that it achieves better results than alternatives. The rest of the paper is arranged as follows: We will review graphical models over strings in section 2, and briefly introduce our sample problem in section 3. Section 4 develops dual decomposition inference for graphical models over strings. Then our experimental setup and results are presented in sections 5 and 6, with some discussion. We investigate dual decomposition for joint MAP inference of many strings. Given an arbitrary graphical model, we decompose it into small acyclic sub-models, w"
D15-1108,D09-1011,1,0.883132,"Thus, a degree d-factor scores some length-d subtuple of x. The score of the whole joint assignment simply sums over all factors: We seek the x of maximum score that is consistent with our partial observation of x. This is a generic constraint satisfaction problem with soft constraints. While our algorithm does not depend on a probabilistic interpretation of the factor graph,2 it can be regarded as peforming maximum a posteriori (MAP) inference of the unobserved variables, under the probability distribution def p(x) = (1/Z) exp score(x). models have been used to model morphological paradigms (Dreyer and Eisner, 2009; Dreyer and Eisner, 2011) and to reconstruct phonological underlying forms of words (Cotterell et al., 2015). The variables in such a model are strings of unbounded length: each variable Xi is permitted to range over Σ∗ where Σ is some fixed, finite alphabet. As in previous work, we assume that a degree-d factor is a d-way rational relation, i.e., a function of d strings that can be computed by a d-tape weighted finite-state machine (WFSM) (Mohri et al., 2002; Kempe et al., 2004). Such a machine is called an acceptor (WFSA) if d = 1 or a transducer (WFST) if d = 2.3 Past work has shown how to"
D15-1108,D11-1057,1,0.881701,"cores some length-d subtuple of x. The score of the whole joint assignment simply sums over all factors: We seek the x of maximum score that is consistent with our partial observation of x. This is a generic constraint satisfaction problem with soft constraints. While our algorithm does not depend on a probabilistic interpretation of the factor graph,2 it can be regarded as peforming maximum a posteriori (MAP) inference of the unobserved variables, under the probability distribution def p(x) = (1/Z) exp score(x). models have been used to model morphological paradigms (Dreyer and Eisner, 2009; Dreyer and Eisner, 2011) and to reconstruct phonological underlying forms of words (Cotterell et al., 2015). The variables in such a model are strings of unbounded length: each variable Xi is permitted to range over Σ∗ where Σ is some fixed, finite alphabet. As in previous work, we assume that a degree-d factor is a d-way rational relation, i.e., a function of d strings that can be computed by a d-tape weighted finite-state machine (WFSM) (Mohri et al., 2002; Kempe et al., 2004). Such a machine is called an acceptor (WFSA) if d = 1 or a transducer (WFST) if d = 2.3 Past work has shown how to approximately sample from"
D15-1108,N12-1024,1,0.896936,"g Count Features But what do we do if the variables are strings? The Lagrangian term λki ·xki in (3) is now ill-typed. We replace it with λki · γ(xki ), where γ(·) extracts a real-valued feature vector from a string, and λki is a vector of Lagrange multipliers. This corresponds to changing the constraint in (2). Instead of requiring x1i = · · · = xK i for each 1 i, we are now requiring γ(xi ) = · · · = γ(xK i ), i.e., these strings must agree in their features. We want each possible string to have a unique feature vector, so that matching features forces the actual strings to match. We follow Paul and Eisner (2012) and use a substring count feature for each w ∈ Σ∗ . In other words, γ(x) is an infinitely long vector, which maps each w to the number of times that w appears in x as a substring.8 Computing λki · γ(xki ) in (3) remains possible because in practice, λki will have only finitely many nonzeros. This is so because our feature vector γ(x) has only finitely many nonzeros for any string x, and the subgradient algorithm in section 4.3 below always updates λki by adding multiples of such γ(x) vectors. We will use a further trick below to prevent rapid growth of this finite set of nonzeros. Each variab"
D15-1108,P10-1105,0,0.0677345,"has shown how to approximately sample from the distribution over x defined by such a model (Bouchard-Cˆot´e et al., 2007), or approximately compute the distribution’s marginals using variants of sum-product belief propagation (BP) (Dreyer and Eisner, 2009) and expectation propagation (EP) (Cotterell and Eisner, 2015). 2.2 2.3 def score(x) = X F (x). (1) F ∈F The String Case Graphical models over strings have enjoyed some attention in the NLP community. Tree-shaped graphical models naturally model the evolutionary tree of word forms (Bouchard-Cˆot´e et al., 2007; Bouchard-Cˆot´e et al., 2008; Hall and Klein, 2010; Hall and Klein, 2011). Cyclic graphical Finite-State Belief Propagation BP iteratively updates messages between factors and variables. Each message is a vector whose elements score the possible values of a variable. Murphy et al. (1999) discusses BP on cyclic (“loopy”) graphs. For pedagogical reasons, suppose momentarily that all factors have degree ≤ 2 (this loses no power). Then BP manipulates only vectors and matrices—whose dimensionality depends on the number of possible values of the vari2 E.g., it could be used for exactly computing the separation oracle when training a structural SVM"
D15-1108,D11-1032,0,0.0133363,"oximately sample from the distribution over x defined by such a model (Bouchard-Cˆot´e et al., 2007), or approximately compute the distribution’s marginals using variants of sum-product belief propagation (BP) (Dreyer and Eisner, 2009) and expectation propagation (EP) (Cotterell and Eisner, 2015). 2.2 2.3 def score(x) = X F (x). (1) F ∈F The String Case Graphical models over strings have enjoyed some attention in the NLP community. Tree-shaped graphical models naturally model the evolutionary tree of word forms (Bouchard-Cˆot´e et al., 2007; Bouchard-Cˆot´e et al., 2008; Hall and Klein, 2010; Hall and Klein, 2011). Cyclic graphical Finite-State Belief Propagation BP iteratively updates messages between factors and variables. Each message is a vector whose elements score the possible values of a variable. Murphy et al. (1999) discusses BP on cyclic (“loopy”) graphs. For pedagogical reasons, suppose momentarily that all factors have degree ≤ 2 (this loses no power). Then BP manipulates only vectors and matrices—whose dimensionality depends on the number of possible values of the vari2 E.g., it could be used for exactly computing the separation oracle when training a structural SVM (Tsochantaridis et al.,"
D15-1108,W10-2902,0,0.0304375,"test11 That is, probability mass of (1 − θ)/3 is divided equally among the |Σ |possible insertions; another (1 − θ)/3 is divided equally among the |Σ|−1 possible substitutions; and the final (1 − θ)/3 is allocated to deletion. 10 The model also has a three-way factor, connecting layers 1 and 2 of Figure 1. This represents deterministic concatenation (appropriate for these languages) and has no parameters. 923 ing. This gives us three approximations to EM, based on DD, SP and MP. Note that DD specifically gives the Viterbi approximation to EM— which sometimes gets better results than true EM (Spitkovsky et al., 2010). For MP (but not SP), we extract only the 1-best predictions for the E step, since we study MP as an approximation to DD. As initialization, our first E step uses the trained version of Model 1 for the same inference method. 5.5 (a) Tangale (b) Catalan (c) Maori (d) English Inference Details We run SP and MP for 20 iterations (usually the predictions converge within 10 iterations). We run DD to convergence (usually &lt; 600 iterations). DD iterations are much faster since each variable considers d strings, not d distributions over strings. Hence DD does not intersect distributions, and many part"
D15-1108,J98-4003,0,\N,Missing
D15-1272,C10-3009,0,0.0148955,"Missing"
D15-1272,P14-5010,0,0.00199005,"Missing"
D15-1272,chrupala-etal-2008-learning,0,0.0925021,"Missing"
D15-1272,J93-2004,0,0.0488165,"G models not using morphology (+dict) (×) or both (+ ×) are marked. More baseline numbers in the appendix (Table A2). including OOVs, and only requires the same training corpus as a generic tagger (containing tags and lemmata), a resource that is available for many languages. 5 Experiments Datasets. We present experiments on the joint task of lemmatization and tagging in six diverse languages: English, German, Czech, Hungarian, Latin and Spanish. We use the same data sets as in M¨uller and Sch¨utze (2015), but do not use the out-of-domain test sets. The English data is from the Penn Treebank (Marcus et al., 1993), Latin from PROIEL (Haug and Jøhndal, 2008), German and Hungarian from SPMRL 2013 (Seddah et al., 2013), and Spanish and Czech from CoNLL 2009 (Hajiˇc et al., 2009). For German, Hungarian, Spanish and Czech we use the splits from the shared tasks; for English the split from SANCL (Petrov and McDonald, 2012); and for Latin a 8/1/1 split into train/dev/test. For all languages we limit our training data to the first 100,000 tokens. Dataset statistics can be found in Table A4 of the appendix. The lemma of Spanish se is set to be consistent. Baselines. We compare our model to three baselines. (i)"
D15-1272,N15-1055,1,0.894932,"Missing"
D15-1272,W02-1001,0,0.0542005,"g/gnu/aspell/dict Example: for the Spanish noun medidas “measures” with attributes N OUN, C OMMON, P LURAL and F EMININE, we conjoin each feature above with N OUN, N OUN +C OMMON, N OUN +P LURAL and N OUN +F EMININE. 4 candidates by extracting edit operation sequences between lemmata and surface forms (Chrupała, 2006), and then trains two maximum entropy Markov models (Ratnaparkhi, 1996) for morphological tagging and lemmatization, which are queried using a beam search decoder. In our experiments we use the latest version5 of M ORFETTE. This version is based on structured perceptron learning (Collins, 2002) and edit trees (Chrupała, 2008). Models similar to M ORFETTE include those of Bj¨orkelund et al. (2010) and Gesmundo and Samardzic (2012) and have also been used for generation (Duˇsek and Jurˇc´ıcˇ ek, 2013). Wicentowski (2002) similarly treats lemmatization as classification over a deterministically chosen candidate set, but uses distributional information extracted from large corpora as a key source of information. Toutanova and Cherry (2009)’s joint morphological analyzer predicts the set of possible lemmata and coarse-grained POS for a word type. This is different from our problem of lem"
D15-1272,D13-1032,1,0.912146,"Missing"
D15-1272,P14-2102,1,0.754648,"Missing"
D15-1272,D08-1113,0,0.0505869,"Missing"
D15-1272,P13-3023,0,0.0421399,"Missing"
D15-1272,E12-1068,1,0.283993,"isambiguate the lemma of a form, which explains why many NLP systems (Manning et al., 2014; Padr´o and Stanilovsky, 2012) apply a pipeline approach of tagging followed by lemmatization. Conversely, knowing the lemma of a form is often beneficial for tagging, for instance in the presence of syncretism; e.g., since German plural noun phrases do not mark gender, it is important to know the lemma (singular form) to correctly tag gender on the noun. Introduction Lemmatization is important for many NLP tasks, including parsing (Bj¨orkelund et al., 2010; Seddah et al., 2010) and machine translation (Fraser et al., 2012). Lemmata are required whenever we want to map words to lexical resources and establish the relation between inflected forms, particularly critical for morphologically rich languages to address the sparsity of unlemmatized forms. This strongly motivates work on language-independent tokenbased lemmatization, but until now there has been little work (Chrupała et al., 2008). Many regular transformations can be described by simple replacement rules, but lemmatization of unknown words requires more than this. For instance the Spanish paradigms for verbs ending in ir and er share the same 3rd person"
D15-1272,P12-2072,0,0.0321468,"e conjoin each feature above with N OUN, N OUN +C OMMON, N OUN +P LURAL and N OUN +F EMININE. 4 candidates by extracting edit operation sequences between lemmata and surface forms (Chrupała, 2006), and then trains two maximum entropy Markov models (Ratnaparkhi, 1996) for morphological tagging and lemmatization, which are queried using a beam search decoder. In our experiments we use the latest version5 of M ORFETTE. This version is based on structured perceptron learning (Collins, 2002) and edit trees (Chrupała, 2008). Models similar to M ORFETTE include those of Bj¨orkelund et al. (2010) and Gesmundo and Samardzic (2012) and have also been used for generation (Duˇsek and Jurˇc´ıcˇ ek, 2013). Wicentowski (2002) similarly treats lemmatization as classification over a deterministically chosen candidate set, but uses distributional information extracted from large corpora as a key source of information. Toutanova and Cherry (2009)’s joint morphological analyzer predicts the set of possible lemmata and coarse-grained POS for a word type. This is different from our problem of lemmatization and fine-grained morphological tagging of tokens in context. Despite the superficial similarity of the two problems, direct com"
D15-1272,padro-stanilovsky-2012-freeling,0,0.0187324,"Missing"
D15-1272,W96-0213,0,0.203493,"e results of the joint model by initializing it with the parameters of a pretrained tagging model. 4 Related Work In functionality, our system resembles M ORFETTE (Chrupała et al., 2008), which generates lemma 3 ftp://ftp.gnu.org/gnu/aspell/dict Example: for the Spanish noun medidas “measures” with attributes N OUN, C OMMON, P LURAL and F EMININE, we conjoin each feature above with N OUN, N OUN +C OMMON, N OUN +P LURAL and N OUN +F EMININE. 4 candidates by extracting edit operation sequences between lemmata and surface forms (Chrupała, 2006), and then trains two maximum entropy Markov models (Ratnaparkhi, 1996) for morphological tagging and lemmatization, which are queried using a beam search decoder. In our experiments we use the latest version5 of M ORFETTE. This version is based on structured perceptron learning (Collins, 2002) and edit trees (Chrupała, 2008). Models similar to M ORFETTE include those of Bj¨orkelund et al. (2010) and Gesmundo and Samardzic (2012) and have also been used for generation (Duˇsek and Jurˇc´ıcˇ ek, 2013). Wicentowski (2002) similarly treats lemmatization as classification over a deterministically chosen candidate set, but uses distributional information extracted from"
D15-1272,W10-1410,0,0.0704151,"Missing"
D15-1272,P08-1103,0,0.0390074,"int morphological analyzer predicts the set of possible lemmata and coarse-grained POS for a word type. This is different from our problem of lemmatization and fine-grained morphological tagging of tokens in context. Despite the superficial similarity of the two problems, direct comparison is not possible. TC’s model is best thought of as inducing a tagging dictionary for OOV types, mapping them to a set of tag and lemma pairs, whereas L EM MING is a token-level, context-based morphological tagger. We do, however, use TC’s model of lemmatization, a string-to-string transduction model based on Jiampojamarn et al. (2008) (JCK), as a standalone baseline. Our tagging-in-context model is faced with higher complexity of learning and inference since it addresses a more difficult task; thus, while we could in principle use JCK as a replacement for our candidate selection, the edit tree approach – which has high coverage at a low average number of lemma candidates (cf. Section 5) – allows us to train and apply L EMMING efficiently. Smith et al. (2005) proposed a log-linear model for the context-based disambiguation of a morphological dictionary. This has the effect of joint tagging, morphological segmentation and le"
D15-1272,H05-1060,0,0.0361191,"L EM MING is a token-level, context-based morphological tagger. We do, however, use TC’s model of lemmatization, a string-to-string transduction model based on Jiampojamarn et al. (2008) (JCK), as a standalone baseline. Our tagging-in-context model is faced with higher complexity of learning and inference since it addresses a more difficult task; thus, while we could in principle use JCK as a replacement for our candidate selection, the edit tree approach – which has high coverage at a low average number of lemma candidates (cf. Section 5) – allows us to train and apply L EMMING efficiently. Smith et al. (2005) proposed a log-linear model for the context-based disambiguation of a morphological dictionary. This has the effect of joint tagging, morphological segmentation and lemmatization, but, critically, is limited to the entries in the morphological dictionary (without which the approach cannot be used), causing problems of recall. In contrast, L EMMING can analyze any word, 5 https://github.com/ gchrupala/morfette/commit/ ca886556916b6cc1e808db4d32daf720664d17d6 2270 cs 5 6 7 JCK L EMMING -P 4 9 10 11 12 13 L EMMING -J 8 tag+lemma +mrph +dict 3 lemma +dict 2 M AR M OT tag lemma tag+lemma lemma tag"
D15-1272,P09-1055,0,0.102223,"e been used in different previous models. All features are extracted given a form-lemma pair hw, li created with an edit tree e. We use the following three edit tree features of Chrupała (2008). (i) The edit tree e. (ii) The pair he, wi. This feature is crucial for the model to memorize irregular forms, e.g., the lemma of was is be. (iii) For each form affix (of maximum length 10): its conjunction with e. These features are useful in learning orthographic and phonological regularities, e.g., the lemma of signalling is signal, not signall. We define the following alignment features. Similar to Toutanova and Cherry (2009) (TC), we define an alignment between w and l. Our alignments can be read from an edit tree by aligning the characters in LCS nodes character by character and characters in substitution nodes block-wise. Thus the alignment of umgeschaut - umschauen is: u-u, m-m, ge-, s-s, c-c, h-h, a-a, u-u, t-en. Each alignment pair constitutes a feature in our model. These features allow the model to learn that the substitution t/en is likely in German. We also concatenate each alignment pair with its form and lemma character context (of up to length 6) to learn, e.g., that ge is often deleted after um. We"
D15-1272,P09-1054,0,0.0122241,"apitalization, digits, hyphens) and the immediate lexical context. We combine lemmatization and higher-order CRF components in a treestructured CRF. Given a sequence of forms w with lemmata l and morphological+POS tags m, we define a globally normalized model: Q T p(l, m |w) ∝ i hwi (li ) exp(f (li , wi , mi ) θ +g(mi , mi−1 , mi−2 , w, i)T λ), where f and g are the features associated with lemma and tag cliques respectively and θ and λ are weight vectors. The graphical model is shown in Figure 2. We perform inference with belief propagation (Pearl, 1988) and estimate the parameters with SGD (Tsuruoka et al., 2009). We greatly improved the results of the joint model by initializing it with the parameters of a pretrained tagging model. 4 Related Work In functionality, our system resembles M ORFETTE (Chrupała et al., 2008), which generates lemma 3 ftp://ftp.gnu.org/gnu/aspell/dict Example: for the Spanish noun medidas “measures” with attributes N OUN, C OMMON, P LURAL and F EMININE, we conjoin each feature above with N OUN, N OUN +C OMMON, N OUN +P LURAL and N OUN +F EMININE. 4 candidates by extracting edit operation sequences between lemmata and surface forms (Chrupała, 2006), and then trains two maximum"
D15-1272,C00-2137,0,0.0183128,"xperiments showed that correct morphological attributes would substantially improve lemmatization as they help in cases of ambiguity. As an example, number helps to lemmatize the singular German noun Raps “canola”, which looks like the plural of Rap “rap”. Numbers can be found in Table A3 of the appendix. This motivates the necessity of joint tagging and lemmatization. For the final experiments, we run pipeline models on tags predicted by M AR M OT (M¨uller et al., 2013) and compare them to L EMMING -J, the 7 8 Unknown word accuracies in the appendix (Table A1). We use the randomization test (Yeh, 2000) and p = .05. joint model described in Section 3. All L EMMING versions use exactly the same features. Table 2 shows that L EMMING -J outperforms L EMMING P in three measures (see bold tag, lemma & joint (tag+lemma) accuracies) except for English, where we observe a tie in lemma accuracy and a small drop in tag and tag+lemma accuracy. Coupling morphological attributes and lemmatization (lines 8–10 vs 11–13) improves tag+lemma prediction for five languages. Improvements in lemma accuracy of the joint over the best pipeline systems range from .1 (Spanish), over >.3 (German, Hungarian) to ≥.96 (C"
D16-1097,W14-4012,0,0.188779,"Missing"
D16-1097,D14-1179,0,0.0413197,"Missing"
D16-1097,P11-1004,0,0.1291,"Missing"
D16-1097,K15-1017,1,0.890809,"Missing"
D16-1097,N16-1080,1,0.852389,"Missing"
D16-1097,W02-0603,0,0.445604,"el’s ability to embed morphemes is important for morphological segmentation since we often have strong corpus-level signals. The reranker also takes into account the characterlevel information through the score of the encoderdecoder model. Due to this combination we expect stronger performance. 3 Related Work Various approaches to morphological segmentation have been proposed in the literature. In the unsupervised realm, most work has been based on the principle of minimum description length (Cover and Thomas, 2012), e.g., L INGUISTICA (Goldsmith, 2001; Lee and Goldsmith, 2016) or M ORFESSOR (Creutz and Lagus, 2002; Creutz et al., 2007; Poon et al., 2009). M ORFESSOR was later extended to a semi-supervised version by Kohonen et al. (2010). Supervised approaches have also been considered. Most notably, Ruokolainen et al. (2013) developed a supervised approach for morphological segmentation based on conditional random fields (CRFs) which they later extended to work also in a semisupervised way (Ruokolainen et al., 2014) using letter successor variety features (Hafer and Weiss, 1974). Similarly, Cotterell et al. (2015) improved performance with a semi-Markov CRF. More recently, Wang et al. (2016) achieved"
D16-1097,D08-1113,0,0.059286,"tation. Baselines. Our first baseline is the joint transduction and segmentation model (JOINT) of Cotterell et al. (2016). It is the current state of the art on the datasets we use and the task of canonical segmentation in general. This model uses a jointly trained, separate transduction and segmentation component. Importantly, the joint model of Cotterell et al. (2016) already contains segment-level features. Thus, reranking this baseline would not provide a similar boost. Our second baseline is a weighted finite-state transducer (WFST) (Mohri et al., 2002) with a loglinear parameterization (Dreyer et al., 2008), again, taken from Cotterell et al. (2016). The WFST baseline is particularly relevant because, like our encoder-decoder, it formulates the problem directly as a string-to-string transduction. Evaluation Metrics. We follow Cotterell et al. (2016) and use the following evaluation measures: error rate, edit distance and morpheme F1 . Error rate is defined as 1 minus the proportion of guesses that are completely correct. Edit distance is the Levenshtein distance between guess and gold standard. For this, guess and gold are each represented as one string with a distinguished character denoting th"
D16-1097,J01-2001,0,0.0390897,"raves et al., 2013). 4 Experiments k∈Sw The reranking model’s ability to embed morphemes is important for morphological segmentation since we often have strong corpus-level signals. The reranker also takes into account the characterlevel information through the score of the encoderdecoder model. Due to this combination we expect stronger performance. 3 Related Work Various approaches to morphological segmentation have been proposed in the literature. In the unsupervised realm, most work has been based on the principle of minimum description length (Cover and Thomas, 2012), e.g., L INGUISTICA (Goldsmith, 2001; Lee and Goldsmith, 2016) or M ORFESSOR (Creutz and Lagus, 2002; Creutz et al., 2007; Poon et al., 2009). M ORFESSOR was later extended to a semi-supervised version by Kohonen et al. (2010). Supervised approaches have also been considered. Most notably, Ruokolainen et al. (2013) developed a supervised approach for morphological segmentation based on conditional random fields (CRFs) which they later extended to work also in a semisupervised way (Ruokolainen et al., 2014) using letter successor variety features (Hafer and Weiss, 1974). Similarly, Cotterell et al. (2015) improved performance wit"
D16-1097,P16-2090,1,0.887592,"Missing"
D16-1097,W10-2210,0,0.569431,"e reranker also takes into account the characterlevel information through the score of the encoderdecoder model. Due to this combination we expect stronger performance. 3 Related Work Various approaches to morphological segmentation have been proposed in the literature. In the unsupervised realm, most work has been based on the principle of minimum description length (Cover and Thomas, 2012), e.g., L INGUISTICA (Goldsmith, 2001; Lee and Goldsmith, 2016) or M ORFESSOR (Creutz and Lagus, 2002; Creutz et al., 2007; Poon et al., 2009). M ORFESSOR was later extended to a semi-supervised version by Kohonen et al. (2010). Supervised approaches have also been considered. Most notably, Ruokolainen et al. (2013) developed a supervised approach for morphological segmentation based on conditional random fields (CRFs) which they later extended to work also in a semisupervised way (Ruokolainen et al., 2014) using letter successor variety features (Hafer and Weiss, 1974). Similarly, Cotterell et al. (2015) improved performance with a semi-Markov CRF. More recently, Wang et al. (2016) achieved stateof-the-art results on surface morphological segmentation using a window LSTM. Even though Wang et al. (2016) also employ"
D16-1097,N16-3005,0,0.0128112,"3). 4 Experiments k∈Sw The reranking model’s ability to embed morphemes is important for morphological segmentation since we often have strong corpus-level signals. The reranker also takes into account the characterlevel information through the score of the encoderdecoder model. Due to this combination we expect stronger performance. 3 Related Work Various approaches to morphological segmentation have been proposed in the literature. In the unsupervised realm, most work has been based on the principle of minimum description length (Cover and Thomas, 2012), e.g., L INGUISTICA (Goldsmith, 2001; Lee and Goldsmith, 2016) or M ORFESSOR (Creutz and Lagus, 2002; Creutz et al., 2007; Poon et al., 2009). M ORFESSOR was later extended to a semi-supervised version by Kohonen et al. (2010). Supervised approaches have also been considered. Most notably, Ruokolainen et al. (2013) developed a supervised approach for morphological segmentation based on conditional random fields (CRFs) which they later extended to work also in a semisupervised way (Ruokolainen et al., 2014) using letter successor variety features (Hafer and Weiss, 1974). Similarly, Cotterell et al. (2015) improved performance with a semi-Markov CRF. More"
D16-1097,D14-1095,0,0.268843,"Missing"
D16-1097,N09-1024,0,0.209303,"for morphological segmentation since we often have strong corpus-level signals. The reranker also takes into account the characterlevel information through the score of the encoderdecoder model. Due to this combination we expect stronger performance. 3 Related Work Various approaches to morphological segmentation have been proposed in the literature. In the unsupervised realm, most work has been based on the principle of minimum description length (Cover and Thomas, 2012), e.g., L INGUISTICA (Goldsmith, 2001; Lee and Goldsmith, 2016) or M ORFESSOR (Creutz and Lagus, 2002; Creutz et al., 2007; Poon et al., 2009). M ORFESSOR was later extended to a semi-supervised version by Kohonen et al. (2010). Supervised approaches have also been considered. Most notably, Ruokolainen et al. (2013) developed a supervised approach for morphological segmentation based on conditional random fields (CRFs) which they later extended to work also in a semisupervised way (Ruokolainen et al., 2014) using letter successor variety features (Hafer and Weiss, 1974). Similarly, Cotterell et al. (2015) improved performance with a semi-Markov CRF. More recently, Wang et al. (2016) achieved stateof-the-art results on surface morpho"
D16-1097,W13-3504,0,0.319601,"the encoderdecoder model. Due to this combination we expect stronger performance. 3 Related Work Various approaches to morphological segmentation have been proposed in the literature. In the unsupervised realm, most work has been based on the principle of minimum description length (Cover and Thomas, 2012), e.g., L INGUISTICA (Goldsmith, 2001; Lee and Goldsmith, 2016) or M ORFESSOR (Creutz and Lagus, 2002; Creutz et al., 2007; Poon et al., 2009). M ORFESSOR was later extended to a semi-supervised version by Kohonen et al. (2010). Supervised approaches have also been considered. Most notably, Ruokolainen et al. (2013) developed a supervised approach for morphological segmentation based on conditional random fields (CRFs) which they later extended to work also in a semisupervised way (Ruokolainen et al., 2014) using letter successor variety features (Hafer and Weiss, 1974). Similarly, Cotterell et al. (2015) improved performance with a semi-Markov CRF. More recently, Wang et al. (2016) achieved stateof-the-art results on surface morphological segmentation using a window LSTM. Even though Wang et al. (2016) also employ a recurrent neural network, we distinguish our approach, in that we focus on canonical mor"
D16-1097,Q15-1026,0,0.180686,"Missing"
D16-1097,P13-1118,0,0.0302509,"Missing"
D16-1206,P03-1006,0,0.0582063,"longest suffix of s = yt−k . . . yt that appears in W—and thus w(hyt ) = w(s) ∈ W and provides sufficient information to compute f (x, t, s).2 For a given x of length n and given parameters θ, the log-normalizer log Zθ (x)—which will be needed to compute the log-probability in eq. (1) below—can be found in time O(|W |n) by dynamic programming. Concise pseudocode is in Alg. 1. In effect, this 2 Our DFA construction is essentially that of Cotterell and Eisner (2015, Appendix B.5). However, Appendix B of that paper also gives a construction that obtains an even smaller DFA by using failure arcs (Allauzen et al., 2003), which remove the requirement that W be closed under last-character substitution. This would yield a further speedup to our Alg. 1 (replacing it with the efficient backward algorithm in footnote 16 of that paper) and similarly to our Alg. 2 (by differentiating the new Alg. 1). runs the forward algorithm on the lattice of taggings given by length-n paths through the DFA. For finding the parameters θ that minimize eq. (1) below, we want the gradient ∇θ log Zθ (x). By applying algorithmic differentiation to Alg. 1, we obtain Alg. 2, which uses back-propagation to compute the gradient (asymptotic"
D16-1206,N15-1094,1,0.920904,"with state set H and alphabet Y . If this DFA is used to read any tag sequence y ∈ Y ∗ , then the arc that reads yt comes from a state h such that hyt is the longest suffix of s = yt−k . . . yt that appears in W—and thus w(hyt ) = w(s) ∈ W and provides sufficient information to compute f (x, t, s).2 For a given x of length n and given parameters θ, the log-normalizer log Zθ (x)—which will be needed to compute the log-probability in eq. (1) below—can be found in time O(|W |n) by dynamic programming. Concise pseudocode is in Alg. 1. In effect, this 2 Our DFA construction is essentially that of Cotterell and Eisner (2015, Appendix B.5). However, Appendix B of that paper also gives a construction that obtains an even smaller DFA by using failure arcs (Allauzen et al., 2003), which remove the requirement that W be closed under last-character substitution. This would yield a further speedup to our Alg. 1 (replacing it with the efficient backward algorithm in footnote 16 of that paper) and similarly to our Alg. 2 (by differentiating the new Alg. 1). runs the forward algorithm on the lattice of taggings given by length-n paths through the DFA. For finding the parameters θ that minimize eq. (1) below, we want the g"
D16-1206,W16-5901,1,0.806645,"omain experts or “one size fits all” strategies (e.g., k-CRF). Our goal is to choose θ—and thus W—so that inference is accurate and fast. Our approach is to modify the usual L2 regularized log-likelihood training criterion with a carefully defined runtime penalty scaled by a parameter γ to balance competing objectives: likelihood on the data {(x(i) , y (i) )}m i=1 vs. efficiency (small W). m X − log pθ (y (i) |x(i) ) + λ||θ||22 + γR(θ) (1) | {z } |{z } |{z } i=1 loss generalization runtime Recall that the runtime of inference on a given sentence is proportional to the size of W, the closure 3 Eisner (2016) explains the connection between algorithmic differentiation and the forward-backward algorithm. 4 Extensions to richer sets of higher-order features are possible, such as conjunctions with properties of the words at position t. 1975 ε N NN G&quot; V NV VN VV GV Figure 2: A visual depiction of the tree-structured group lasso penalty. Each node represents a tag string feature. The group indexed by a node’s tag string is defined as the set of features that are proper descendants of the node. For example, the lavender box indicates the largest group Gε and the aubergine box indicates a smaller group G"
D16-1206,D13-1152,1,0.904177,"Missing"
D16-1206,D11-1139,0,0.0487638,"Missing"
D16-1206,D13-1024,0,0.0201484,"ing the online proximal gradient algorithm SPOM (Martins et al., 2011b) and Adagrad (Duchi et al., 2011) with η = 0.01 and 15 inner epochs. We limited to 3 active set iterations, and as a result, our final W contained at most tag trigrams. 4 Related Work Our paper can be seen as transferring methods of Cotterell and Eisner (2015) to the CRF setting. They too used tree-structured group lasso and active set to select variable-order n-gram features W for globally-normalized sequence models (in their case, to rapidly and accurately approximate beliefs during message-passing inference). Similarly, Nelakanti et al. (2013) used tree-structured group lasso to regularize a variable-order language model (though their focus was training speed). Here we apply these techniques to conditional models for tagging. Our work directly builds on the variable-order CRF of Cuong et al. (2014), with a speedup in Alg. 2, but our approach also learns the VoCRF structure. Our method is also related to the generative variable-order tagger of Sch¨utze and Singer (1994). Our static feature selection chooses a single model that permits fast exact marginal inference, similar to learning a low-treewidth graphical model (Bach and 5 Each"
D16-1206,petrov-etal-2012-universal,0,0.0521156,"Missing"
D16-1206,P94-1025,0,0.581918,"Missing"
D16-1206,P15-1015,0,0.015011,"is also related to the generative variable-order tagger of Sch¨utze and Singer (1994). Our static feature selection chooses a single model that permits fast exact marginal inference, similar to learning a low-treewidth graphical model (Bach and 5 Each gradient computation in this inner optimization takes time O(|Wactive |n), which is especially fast at early iterations. 1976 Jordan, 2001; Elidan and Gould, 2008). This contrasts with recent papers that learn to do approximate 1-best inference using a sequence of models, whether by dynamic feature selection within a greedy inference algorithm (Strubell et al., 2015), or by gradually increasing the feature set of a 1-best global inference algorithm and pruning its hypothesis space after each increase (Weiss and Taskar, 2010; He et al., 2013). Schmidt (2010) explores the use of group lasso penalties and the active set method for learning the structure of a graphical model, but does not consider learning repeated structures (in our setting, W defines a structure that is reused at each position). Steinhardt and Liang (2015) jointly modeled the amount of context to use in a variable-order model that dynamically determines how much context to use in a beam sea"
D16-1206,N03-1033,0,0.0744387,"tures over the output structure are limited. For example, an order-k CRF (or “k-CRF” for short, with k &gt; 1 being “higher-order”) allows expressive features over a window of k+1 adjacent tags (as well as the input), and then inference takes time O(n·|Y |k+1 ), where Y is the set of tags and n is the length of the input. How large does k need to be? Typically k = 2 works well, with big gains from 0 → 1 and modest ∗ 95 Equal contribution gains from 1 → 2 (Fig. 1). Small k may be sufficient when there is enough training data to allow the model to attend to many fine-grained features of the input (Toutanova et al., 2003; Liang et al., 2008). For example, when predicting POS tags in morphologicallyrich languages, certain words are easily tagged based on their spelling without considering the context (k = 0). In fact, such languages tend to have a more free word order, making tag context less useful. We investigate a hybrid approach that gives the accuracy of higher-order models while reducing runtime. We build on variable-order CRFs (Ye et al., 2009) (VoCRF), which support features on tag subsequences of mixed orders. Since only modest gains are obtained from moving to higher-order models, we posit that only"
D16-1256,P99-1037,0,0.586811,"Missing"
D16-1256,D13-1034,0,0.020932,"oding We also decode by importance sampling. Given w, we sample canonical forms u and then run the CKY algorithm to get the highest scoring tree. 5 Related Work We believe our attempt to train discriminative grammars for morphology is novel. Nevertheless, other researchers have described parsers for morphology. Most of this work is unsupervised: Johnson et al. (2007) applied a Bayesian PCFG to unsupervised morphological segmentation. Similarly, Adaptor Grammars (Johnson et al., 2006), a non-parametric Bayesian generalization of PCFGs, have been applied to the unsupervised version of the task (Botha and Blunsom, 2013; Sirts and Goldwater, 2013). Relatedly, Schmid (2005) performed unsupervised disambiguation of a German morphological analyzer (Schmid et al., 2004) using a PCFG, using the inside-outside algorithm (Baker, 1979). Also, discriminative parsing approaches have been applied to the related problem of Chinese word segmentation (Zhang et al., 2014). 6 Morphological Treebank Supervised morphological segmentation has historically been treated as a segmentation problem, devoid of hierarchical structure. A core reason behind this is that—to the best of our knowledge—there are no hierarchically annotated"
D16-1256,P11-1004,0,0.175378,"s, which are typically taken to be the smallest meaning-bearing units in language. This work concerns itself with modeling hierarchical structure over these morphemes. Note a simple flat 1 We found post publication that CELEX (Baayen et al., 1993) has annotated words for hierarchical morphological segmentation as well. morphological segmentation can also be straightforwardly derived from the CFG parse tree. Segmentations have found use in a diverse set of NLP applications, e.g., automatic speech recognition (Afify et al., 2006), keyword spotting (Narasimhan et al., 2014), machine translation (Clifton and Sarkar, 2011) and parsing (Seeker and C¸etino˘glu, 2015). In contrast to prior work, we focus on canonical segmentation, i.e., we seek to jointly model orthographic changes and segmentation. For instance, the canonical segmentation of untestably is un+test+able+ly, where we map ably to able+ly, restoring the letters le. We make two contributions: (i) We introduce a joint model for canonical segmentation with a CFG backbone. We experimentally show that this model outperforms a semi-Markov model on flat segmentation. (ii) We release the first morphology treebank, consisting of 7454 English word types, each a"
D16-1256,P14-2102,1,0.861633,"Dreyer et al. (2008) for a thorough exposition. For ease of computation, we can encode this function as a weighted finite-state machine (WFST) (Mohri et al., 2002). This requires, however, that the feature function g factors over the topology of the finite-state encoding. Since our model conditions on the word w, the feature function g can extract features from any part of this string. Features on the output string, u, however, are more restricted. In this work, we employ a bigram model over output characters. This implies that each state remembers exactly one character: the previous one. See Cotterell et al. (2014) for details. We can compute the score for two strings u and w using a weighted generalization of the Levenshtein algorithm. Computing the partition function requires a different dynamic program, which runs in O(|w|2 · |Σ|2 ) time. Note that since |Σ |≈ 26 (lower case English letters), it takes roughly 262 = 676 times longer to compute the partition function than to score a pair of strings. Our model includes several simple feature templates, including features that fire on individual edit actions as well as conjunctions of edit actions and 2326 ROOT W ORD W ORD W ORD P REFIX S UFFIX → → → → →"
D16-1256,N16-1080,1,0.873925,"Missing"
D16-1256,D08-1113,0,0.0295802,"fore describing the joint model 2 For efficiency, we assume u ∈ Σ|w|+k , k = 5. in detail, we first consider its pieces individually. 3.1 Restoring Orthographic Changes To extract a canonical segmentation (Naradowsky and Goldwater, 2009; Cotterell et al., 2016), we restore orthographic changes that occur during word formation. To this end, we define the score function   scoreη (u, a, w) = exp g(u, a, w)> η (1) where a is a monotonic alignment between the strings u and w. The goal is for scoreη to assign higher values to better matched pairs, e.g., (w=untestably, u=untestablely). We refer to Dreyer et al. (2008) for a thorough exposition. For ease of computation, we can encode this function as a weighted finite-state machine (WFST) (Mohri et al., 2002). This requires, however, that the feature function g factors over the topology of the finite-state encoding. Since our model conditions on the word w, the feature function g can extract features from any part of this string. Features on the output string, u, however, are more restricted. In this work, we employ a bigram model over output characters. This implies that each state remembers exactly one character: the previous one. See Cotterell et al. (20"
D16-1256,N16-1024,0,0.0345825,"llel construction), we make use of an importancesampling estimator, derived by Cotterell et al. (2016), which is faster in practice. Roughly speaking, we approximate the hard-to-samplefrom distribution pθ by taking samples from an easy-to-sample-from proposal distribution q. Specifically, we employ a pipeline model for q consisting of WFST and then a WCFG sampled from consecutively. We then reweight the samples using the unnormalized score from pθ . Importance sampling has found many uses in NLP ranging from language modeling (Bengio et al., 2003) and neural MT (Jean et al., 2015) to parsing (Dyer et al., 2016). Due to a lack of space, we omit the derivation of the importance-sampled approximate gradient. 4.2 Decoding We also decode by importance sampling. Given w, we sample canonical forms u and then run the CKY algorithm to get the highest scoring tree. 5 Related Work We believe our attempt to train discriminative grammars for morphology is novel. Nevertheless, other researchers have described parsers for morphology. Most of this work is unsupervised: Johnson et al. (2007) applied a Bayesian PCFG to unsupervised morphological segmentation. Similarly, Adaptor Grammars (Johnson et al., 2006), a non-"
D16-1256,D14-1095,0,0.0766574,"ose words into smaller units, known as morphemes, which are typically taken to be the smallest meaning-bearing units in language. This work concerns itself with modeling hierarchical structure over these morphemes. Note a simple flat 1 We found post publication that CELEX (Baayen et al., 1993) has annotated words for hierarchical morphological segmentation as well. morphological segmentation can also be straightforwardly derived from the CFG parse tree. Segmentations have found use in a diverse set of NLP applications, e.g., automatic speech recognition (Afify et al., 2006), keyword spotting (Narasimhan et al., 2014), machine translation (Clifton and Sarkar, 2011) and parsing (Seeker and C¸etino˘glu, 2015). In contrast to prior work, we focus on canonical segmentation, i.e., we seek to jointly model orthographic changes and segmentation. For instance, the canonical segmentation of untestably is un+test+able+ly, where we map ably to able+ly, restoring the letters le. We make two contributions: (i) We introduce a joint model for canonical segmentation with a CFG backbone. We experimentally show that this model outperforms a semi-Markov model on flat segmentation. (ii) We release the first morphology treeban"
D16-1256,H05-1065,0,0.0395242,"canonical forms u and then run the CKY algorithm to get the highest scoring tree. 5 Related Work We believe our attempt to train discriminative grammars for morphology is novel. Nevertheless, other researchers have described parsers for morphology. Most of this work is unsupervised: Johnson et al. (2007) applied a Bayesian PCFG to unsupervised morphological segmentation. Similarly, Adaptor Grammars (Johnson et al., 2006), a non-parametric Bayesian generalization of PCFGs, have been applied to the unsupervised version of the task (Botha and Blunsom, 2013; Sirts and Goldwater, 2013). Relatedly, Schmid (2005) performed unsupervised disambiguation of a German morphological analyzer (Schmid et al., 2004) using a PCFG, using the inside-outside algorithm (Baker, 1979). Also, discriminative parsing approaches have been applied to the related problem of Chinese word segmentation (Zhang et al., 2014). 6 Morphological Treebank Supervised morphological segmentation has historically been treated as a segmentation problem, devoid of hierarchical structure. A core reason behind this is that—to the best of our knowledge—there are no hierarchically annotated corpora for the task. To remedy this, we provide tree"
D16-1256,schmid-etal-2004-smor,0,0.0439404,"ted Work We believe our attempt to train discriminative grammars for morphology is novel. Nevertheless, other researchers have described parsers for morphology. Most of this work is unsupervised: Johnson et al. (2007) applied a Bayesian PCFG to unsupervised morphological segmentation. Similarly, Adaptor Grammars (Johnson et al., 2006), a non-parametric Bayesian generalization of PCFGs, have been applied to the unsupervised version of the task (Botha and Blunsom, 2013; Sirts and Goldwater, 2013). Relatedly, Schmid (2005) performed unsupervised disambiguation of a German morphological analyzer (Schmid et al., 2004) using a PCFG, using the inside-outside algorithm (Baker, 1979). Also, discriminative parsing approaches have been applied to the related problem of Chinese word segmentation (Zhang et al., 2014). 6 Morphological Treebank Supervised morphological segmentation has historically been treated as a segmentation problem, devoid of hierarchical structure. A core reason behind this is that—to the best of our knowledge—there are no hierarchically annotated corpora for the task. To remedy this, we provide tree annotations for a subset of the English portion of CELEX (Baayen et al., 1993). We reannotated"
D16-1256,P08-1109,0,0.0868813,"Missing"
D16-1256,Q15-1026,0,0.186647,"Missing"
D16-1256,P14-1022,0,0.15414,"g the hierarchical structure allows us to select the correct reading; note there are even cases of true ambiguity; e.g., unlockable has two readings: “unable to be locked” and “able to be unlocked.” We also note that theoretical linguists often implicitly assume a context-free treatment of word formation, e.g., by employing brackets to indicate different levels of affixation. Others have explicitly modeled word-internal structure with grammars (Selkirk, 1982; Marvin, 2002). 3 Parsing the Lexicon A novel component of this work is the development of a discriminative parser (Finkel et al., 2008; Hall et al., 2014) for morphology. The goal is to define a probability distribution over all trees that could arise from the input word, after reversal of orthographic and phonological processes. We employ the simple grammar shown in Table 1. Despite its simplicity, it models the order in which morphemes are attached. More formally, our goal is to map a surface form w (e.g., w=untestably) into its underlying canonical form u (e.g., u=untestablely) and then into a parse tree t over its morphemes. We assume u, w ∈ Σ∗ , for some discrete alphabet Σ.2 Note that a parse tree over the string implicitly defines a flat"
D16-1256,P15-1001,0,0.0375468,"olynomial time (using the BarHillel construction), we make use of an importancesampling estimator, derived by Cotterell et al. (2016), which is faster in practice. Roughly speaking, we approximate the hard-to-samplefrom distribution pθ by taking samples from an easy-to-sample-from proposal distribution q. Specifically, we employ a pipeline model for q consisting of WFST and then a WCFG sampled from consecutively. We then reweight the samples using the unnormalized score from pθ . Importance sampling has found many uses in NLP ranging from language modeling (Bengio et al., 2003) and neural MT (Jean et al., 2015) to parsing (Dyer et al., 2016). Due to a lack of space, we omit the derivation of the importance-sampled approximate gradient. 4.2 Decoding We also decode by importance sampling. Given w, we sample canonical forms u and then run the CKY algorithm to get the highest scoring tree. 5 Related Work We believe our attempt to train discriminative grammars for morphology is novel. Nevertheless, other researchers have described parsers for morphology. Most of this work is unsupervised: Johnson et al. (2007) applied a Bayesian PCFG to unsupervised morphological segmentation. Similarly, Adaptor Grammars"
D16-1256,N07-1018,0,0.0454708,"ing has found many uses in NLP ranging from language modeling (Bengio et al., 2003) and neural MT (Jean et al., 2015) to parsing (Dyer et al., 2016). Due to a lack of space, we omit the derivation of the importance-sampled approximate gradient. 4.2 Decoding We also decode by importance sampling. Given w, we sample canonical forms u and then run the CKY algorithm to get the highest scoring tree. 5 Related Work We believe our attempt to train discriminative grammars for morphology is novel. Nevertheless, other researchers have described parsers for morphology. Most of this work is unsupervised: Johnson et al. (2007) applied a Bayesian PCFG to unsupervised morphological segmentation. Similarly, Adaptor Grammars (Johnson et al., 2006), a non-parametric Bayesian generalization of PCFGs, have been applied to the unsupervised version of the task (Botha and Blunsom, 2013; Sirts and Goldwater, 2013). Relatedly, Schmid (2005) performed unsupervised disambiguation of a German morphological analyzer (Schmid et al., 2004) using a PCFG, using the inside-outside algorithm (Baker, 1979). Also, discriminative parsing approaches have been applied to the related problem of Chinese word segmentation (Zhang et al., 2014)."
D16-1256,Q13-1021,0,0.0222585,"mportance sampling. Given w, we sample canonical forms u and then run the CKY algorithm to get the highest scoring tree. 5 Related Work We believe our attempt to train discriminative grammars for morphology is novel. Nevertheless, other researchers have described parsers for morphology. Most of this work is unsupervised: Johnson et al. (2007) applied a Bayesian PCFG to unsupervised morphological segmentation. Similarly, Adaptor Grammars (Johnson et al., 2006), a non-parametric Bayesian generalization of PCFGs, have been applied to the unsupervised version of the task (Botha and Blunsom, 2013; Sirts and Goldwater, 2013). Relatedly, Schmid (2005) performed unsupervised disambiguation of a German morphological analyzer (Schmid et al., 2004) using a PCFG, using the inside-outside algorithm (Baker, 1979). Also, discriminative parsing approaches have been applied to the related problem of Chinese word segmentation (Zhang et al., 2014). 6 Morphological Treebank Supervised morphological segmentation has historically been treated as a segmentation problem, devoid of hierarchical structure. A core reason behind this is that—to the best of our knowledge—there are no hierarchically annotated corpora for the task. To re"
D16-1256,J07-4003,0,0.0364486,"a parse tree (e.g., t=[[un[[test]able]]ly]). Thus, we define the parser score with the following function  scoreω (t, u) = exp   X f (π, u)> ω  (2) π∈Π(t) where Π(t) is the set of anchored productions in the tree t. An anchored production π is a grammar rule in Chomsky normal form attached to a span, e.g., Ai,k → Bi,j Cj,k . Each π is then assigned a weight by the linear function f (π, u)> ω, where the function f extracts relevant features from the anchored production as well as the corresponding span of the underlying form u. This model is typically referred to as a weighted CFG (WCFG) (Smith and Johnson, 2007) or a CRF parser. For f , we define three span features: (i) indicator features on the span’s segment, (ii) an indicator feature that fires if the segment appears in an external corpus3 and (iii) the conjunction of the segment with the label (e.g., P REFIX) of the subtree root. Following Hall et al. (2014), we employ an indicator feature for each production as well as production backoff features. 4 A Joint Model where T (u) is the set of all parse trees for the string u. This involves a sum over all possible underlying orthographic forms and all parse trees for those forms. The joint approach"
D16-1256,P14-1125,0,0.0321234,"Johnson et al. (2007) applied a Bayesian PCFG to unsupervised morphological segmentation. Similarly, Adaptor Grammars (Johnson et al., 2006), a non-parametric Bayesian generalization of PCFGs, have been applied to the unsupervised version of the task (Botha and Blunsom, 2013; Sirts and Goldwater, 2013). Relatedly, Schmid (2005) performed unsupervised disambiguation of a German morphological analyzer (Schmid et al., 2004) using a PCFG, using the inside-outside algorithm (Baker, 1979). Also, discriminative parsing approaches have been applied to the related problem of Chinese word segmentation (Zhang et al., 2014). 6 Morphological Treebank Supervised morphological segmentation has historically been treated as a segmentation problem, devoid of hierarchical structure. A core reason behind this is that—to the best of our knowledge—there are no hierarchically annotated corpora for the task. To remedy this, we provide tree annotations for a subset of the English portion of CELEX (Baayen et al., 1993). We reannotated 7454 English types with a full constituency parse.4 The resource will be freely available for future research. 6.1 Annotation Guidelines The annotation of the morphology treebank was guided by t"
D17-1074,N15-1107,0,0.0573578,"literature can also break down derivationally complex forms into their morphs, provided preaffix F1 affix F1 affix F1 -ly -er -ation -or 1.0 0.86 0.78 0.59 -ity -ment -ist -ness 0.54 0.45 0.43 0.40 -ence -ure -ee -age 0.32 0.22 0.20 0.20 Table 4: F1 for various suffix attachments with the sequenceto-sequence model segmented and labeled data is available for training (Ruokolainen et al., 2013; Cotterell et al., 2015; Cotterell and Sch¨utze, 2017). Our work, however, builds directly upon recent efforts in the generation of inflectional morphology (Durrett and DeNero, 2013; Nicolai et al., 2015; Ahlberg et al., 2015; Rastogi et al., 2016; Faruqui et al., 2016). We differ in that we focus on derivational morphology. In another recent line of work, Vylomova et al. (2017) predict derivationally complex forms using sentential context. Our work differs from their approach in that we attempt to generate derivational forms divorced from the context, but the underlying neural sequence-to-sequence architecture is quite similar. 6 Conclusion We have presented a statistical model for the generation of derivationally complex forms, a task that has gone essentially unexplored in the literature. Viewing derivational m"
D17-1074,W14-4012,0,0.107262,"Missing"
D17-1074,K17-2001,1,0.914766,"t -ness 0.54 0.45 0.43 0.40 -ence -ure -ee -age 0.32 0.22 0.20 0.20 Table 4: F1 for various suffix attachments with the sequenceto-sequence model segmented and labeled data is available for training (Ruokolainen et al., 2013; Cotterell et al., 2015; Cotterell and Sch¨utze, 2017). Our work, however, builds directly upon recent efforts in the generation of inflectional morphology (Durrett and DeNero, 2013; Nicolai et al., 2015; Ahlberg et al., 2015; Rastogi et al., 2016; Faruqui et al., 2016). We differ in that we focus on derivational morphology. In another recent line of work, Vylomova et al. (2017) predict derivationally complex forms using sentential context. Our work differs from their approach in that we attempt to generate derivational forms divorced from the context, but the underlying neural sequence-to-sequence architecture is quite similar. 6 Conclusion We have presented a statistical model for the generation of derivationally complex forms, a task that has gone essentially unexplored in the literature. Viewing derivational morphology as paradigmatic, where slots refer to semantic categories, e.g., corrode+RESULT7→corrosion, we draw upon recent advances in the generation of infl"
D17-1074,K15-1017,1,0.908059,"Missing"
D17-1074,N13-1138,0,0.0477791,"ervised segmentation and analysis models in the literature can also break down derivationally complex forms into their morphs, provided preaffix F1 affix F1 affix F1 -ly -er -ation -or 1.0 0.86 0.78 0.59 -ity -ment -ist -ness 0.54 0.45 0.43 0.40 -ence -ure -ee -age 0.32 0.22 0.20 0.20 Table 4: F1 for various suffix attachments with the sequenceto-sequence model segmented and labeled data is available for training (Ruokolainen et al., 2013; Cotterell et al., 2015; Cotterell and Sch¨utze, 2017). Our work, however, builds directly upon recent efforts in the generation of inflectional morphology (Durrett and DeNero, 2013; Nicolai et al., 2015; Ahlberg et al., 2015; Rastogi et al., 2016; Faruqui et al., 2016). We differ in that we focus on derivational morphology. In another recent line of work, Vylomova et al. (2017) predict derivationally complex forms using sentential context. Our work differs from their approach in that we attempt to generate derivational forms divorced from the context, but the underlying neural sequence-to-sequence architecture is quite similar. 6 Conclusion We have presented a statistical model for the generation of derivationally complex forms, a task that has gone essentially unexplor"
D17-1074,N16-1077,0,0.0470827,"y complex forms into their morphs, provided preaffix F1 affix F1 affix F1 -ly -er -ation -or 1.0 0.86 0.78 0.59 -ity -ment -ist -ness 0.54 0.45 0.43 0.40 -ence -ure -ee -age 0.32 0.22 0.20 0.20 Table 4: F1 for various suffix attachments with the sequenceto-sequence model segmented and labeled data is available for training (Ruokolainen et al., 2013; Cotterell et al., 2015; Cotterell and Sch¨utze, 2017). Our work, however, builds directly upon recent efforts in the generation of inflectional morphology (Durrett and DeNero, 2013; Nicolai et al., 2015; Ahlberg et al., 2015; Rastogi et al., 2016; Faruqui et al., 2016). We differ in that we focus on derivational morphology. In another recent line of work, Vylomova et al. (2017) predict derivationally complex forms using sentential context. Our work differs from their approach in that we attempt to generate derivational forms divorced from the context, but the underlying neural sequence-to-sequence architecture is quite similar. 6 Conclusion We have presented a statistical model for the generation of derivationally complex forms, a task that has gone essentially unexplored in the literature. Viewing derivational morphology as paradigmatic, where slots refer"
D17-1074,P16-2090,0,0.122677,"Missing"
D17-1074,P96-1004,0,0.25946,"ional morphology. For example, English is labeled as morphologically impoverished, whereas German and Russian are considered morphologically rich, e.g., see the introduction of Tsarfaty et al. (2010). As regards derivation, English is quite complex and even similar in richness to German or Russian as it contains productive formations from two substrata: Germanic and Latinate. From this perspective, English is very much a morphologically rich language. Indeed, a corpus study on the Brown Corpus showed that the majority of English words are morphologically complex when derivation is considered (Light, 1996). Note that there many languages that have exhibit neither rich inflection, nor rich derivational morphology, e.g., Chinese, which most commonly employs compounding for word word formation (Chung et al., 2014). 3 Task and Models We discuss our two systems for derivational paradigm completion and the results they achieve. 3.1 Data We experiment on English derivational triples extracted from NomBank (Meyers et al., 2004).4 Each triple consists of a base form, the semantics of the derivation and a corresponding derived form e.g., hameliorate, RESULT, ameliorationi. Note that in this task we do no"
D17-1074,W04-2705,0,0.179042,"Missing"
D17-1074,Q15-1012,0,0.0186166,"advancely from advance, rather than inadvance, as well as in PATIENT nominalizations, e.g., the model produces containee in place of content—this last distinction is unpredictable. 5 Related Work Previous work in unsupervised morphological segmentation and has implicitly incorporated derivational morphology. Such systems attempt to segment words into all constituent morphs, treating inflectional and derivational affixes as equivalent. The popular Morfessor tool (Creutz and Lagus, 2007) is one example of such an unsupervised segmentation system, but many others exist, e.g., Poon et al. (2009), Narasimhan et al. (2015) inter alia. Supervised segmentation and analysis models in the literature can also break down derivationally complex forms into their morphs, provided preaffix F1 affix F1 affix F1 -ly -er -ation -or 1.0 0.86 0.78 0.59 -ity -ment -ist -ness 0.54 0.45 0.43 0.40 -ence -ure -ee -age 0.32 0.22 0.20 0.20 Table 4: F1 for various suffix attachments with the sequenceto-sequence model segmented and labeled data is available for training (Ruokolainen et al., 2013; Cotterell et al., 2015; Cotterell and Sch¨utze, 2017). Our work, however, builds directly upon recent efforts in the generation of inflectio"
D17-1074,N15-1093,0,0.0263326,"nalysis models in the literature can also break down derivationally complex forms into their morphs, provided preaffix F1 affix F1 affix F1 -ly -er -ation -or 1.0 0.86 0.78 0.59 -ity -ment -ist -ness 0.54 0.45 0.43 0.40 -ence -ure -ee -age 0.32 0.22 0.20 0.20 Table 4: F1 for various suffix attachments with the sequenceto-sequence model segmented and labeled data is available for training (Ruokolainen et al., 2013; Cotterell et al., 2015; Cotterell and Sch¨utze, 2017). Our work, however, builds directly upon recent efforts in the generation of inflectional morphology (Durrett and DeNero, 2013; Nicolai et al., 2015; Ahlberg et al., 2015; Rastogi et al., 2016; Faruqui et al., 2016). We differ in that we focus on derivational morphology. In another recent line of work, Vylomova et al. (2017) predict derivationally complex forms using sentential context. Our work differs from their approach in that we attempt to generate derivational forms divorced from the context, but the underlying neural sequence-to-sequence architecture is quite similar. 6 Conclusion We have presented a statistical model for the generation of derivationally complex forms, a task that has gone essentially unexplored in the literature."
D17-1074,N09-1024,0,0.0345515,", e.g., we generate advancely from advance, rather than inadvance, as well as in PATIENT nominalizations, e.g., the model produces containee in place of content—this last distinction is unpredictable. 5 Related Work Previous work in unsupervised morphological segmentation and has implicitly incorporated derivational morphology. Such systems attempt to segment words into all constituent morphs, treating inflectional and derivational affixes as equivalent. The popular Morfessor tool (Creutz and Lagus, 2007) is one example of such an unsupervised segmentation system, but many others exist, e.g., Poon et al. (2009), Narasimhan et al. (2015) inter alia. Supervised segmentation and analysis models in the literature can also break down derivationally complex forms into their morphs, provided preaffix F1 affix F1 affix F1 -ly -er -ation -or 1.0 0.86 0.78 0.59 -ity -ment -ist -ness 0.54 0.45 0.43 0.40 -ence -ure -ee -age 0.32 0.22 0.20 0.20 Table 4: F1 for various suffix attachments with the sequenceto-sequence model segmented and labeled data is available for training (Ruokolainen et al., 2013; Cotterell et al., 2015; Cotterell and Sch¨utze, 2017). Our work, however, builds directly upon recent efforts in t"
D17-1074,N16-1076,1,0.853692,"eak down derivationally complex forms into their morphs, provided preaffix F1 affix F1 affix F1 -ly -er -ation -or 1.0 0.86 0.78 0.59 -ity -ment -ist -ness 0.54 0.45 0.43 0.40 -ence -ure -ee -age 0.32 0.22 0.20 0.20 Table 4: F1 for various suffix attachments with the sequenceto-sequence model segmented and labeled data is available for training (Ruokolainen et al., 2013; Cotterell et al., 2015; Cotterell and Sch¨utze, 2017). Our work, however, builds directly upon recent efforts in the generation of inflectional morphology (Durrett and DeNero, 2013; Nicolai et al., 2015; Ahlberg et al., 2015; Rastogi et al., 2016; Faruqui et al., 2016). We differ in that we focus on derivational morphology. In another recent line of work, Vylomova et al. (2017) predict derivationally complex forms using sentential context. Our work differs from their approach in that we attempt to generate derivational forms divorced from the context, but the underlying neural sequence-to-sequence architecture is quite similar. 6 Conclusion We have presented a statistical model for the generation of derivationally complex forms, a task that has gone essentially unexplored in the literature. Viewing derivational morphology as paradigma"
D17-1074,W13-3504,0,0.024175,"or tool (Creutz and Lagus, 2007) is one example of such an unsupervised segmentation system, but many others exist, e.g., Poon et al. (2009), Narasimhan et al. (2015) inter alia. Supervised segmentation and analysis models in the literature can also break down derivationally complex forms into their morphs, provided preaffix F1 affix F1 affix F1 -ly -er -ation -or 1.0 0.86 0.78 0.59 -ity -ment -ist -ness 0.54 0.45 0.43 0.40 -ence -ure -ee -age 0.32 0.22 0.20 0.20 Table 4: F1 for various suffix attachments with the sequenceto-sequence model segmented and labeled data is available for training (Ruokolainen et al., 2013; Cotterell et al., 2015; Cotterell and Sch¨utze, 2017). Our work, however, builds directly upon recent efforts in the generation of inflectional morphology (Durrett and DeNero, 2013; Nicolai et al., 2015; Ahlberg et al., 2015; Rastogi et al., 2016; Faruqui et al., 2016). We differ in that we focus on derivational morphology. In another recent line of work, Vylomova et al. (2017) predict derivationally complex forms using sentential context. Our work differs from their approach in that we attempt to generate derivational forms divorced from the context, but the underlying neural sequence-to-se"
D17-1074,E17-3017,0,0.0739484,"Missing"
D17-1074,W10-1401,0,0.027253,"e less restricted. A parsimonious model of derivational morphology would describe forms using productive rules when possible, but may store forms with highly restricted patterns directly as full lexical items. A Note On Terminology. We would like to make a subtle, but important point regarding terminology: the phrase morphologically rich in the NLP community almost exclusively refers to inflectional, rather than derivational morphology. For example, English is labeled as morphologically impoverished, whereas German and Russian are considered morphologically rich, e.g., see the introduction of Tsarfaty et al. (2010). As regards derivation, English is quite complex and even similar in richness to German or Russian as it contains productive formations from two substrata: Germanic and Latinate. From this perspective, English is very much a morphologically rich language. Indeed, a corpus study on the Brown Corpus showed that the majority of English words are morphologically complex when derivation is considered (Light, 1996). Note that there many languages that have exhibit neither rich inflection, nor rich derivational morphology, e.g., Chinese, which most commonly employs compounding for word word formatio"
D17-1074,E17-2019,1,0.788873,"9 -ity -ment -ist -ness 0.54 0.45 0.43 0.40 -ence -ure -ee -age 0.32 0.22 0.20 0.20 Table 4: F1 for various suffix attachments with the sequenceto-sequence model segmented and labeled data is available for training (Ruokolainen et al., 2013; Cotterell et al., 2015; Cotterell and Sch¨utze, 2017). Our work, however, builds directly upon recent efforts in the generation of inflectional morphology (Durrett and DeNero, 2013; Nicolai et al., 2015; Ahlberg et al., 2015; Rastogi et al., 2016; Faruqui et al., 2016). We differ in that we focus on derivational morphology. In another recent line of work, Vylomova et al. (2017) predict derivationally complex forms using sentential context. Our work differs from their approach in that we attempt to generate derivational forms divorced from the context, but the underlying neural sequence-to-sequence architecture is quite similar. 6 Conclusion We have presented a statistical model for the generation of derivationally complex forms, a task that has gone essentially unexplored in the literature. Viewing derivational morphology as paradigmatic, where slots refer to semantic categories, e.g., corrode+RESULT7→corrosion, we draw upon recent advances in the generation of infl"
D17-1078,D15-1041,0,0.0168734,"ity of jointly train translation models for a wide variety of languages. Our work addresses a different task, but the undergirding philosophical motivation is similar, i.e., attack low-resource NLP through multi-task transfer learning. Kann et al. (2017) offer a similar method for cross-lingual transfer in morphological inflection generation. 7 Character-level NLP. Our work also follows a recent trend in NLP, whereby traditional word-level neural representations are being replaced by character-level representations for a myriad tasks, e.g., POS tagging dos Santos and Zadrozny (2014), parsing (Ballesteros et al., 2015), language modeling (Ling et al., 2015), sentiment analysis (Zhang et al., 2015) as well as the tagger of Heigold et al. (2017), whose work we build upon. Our work is also related to recent work on character-level morphological generation using neural architectures (Faruqui et al., 2016; Rastogi et al., 2016). 6.3 Neural Cross-lingual Transfer in NLP. In terms of methodology, however, our proposal bears similarity to recent work in speech and machine translation–we discuss each in turn. In speech recognition, Heigold et al. (2013) train a cross-lingual neural acoustic model on five Romance lan"
D17-1078,P16-1184,0,0.233714,"erent languages that have the same syntacto-semantic function have the same bundle of tags (see §2 for a discussion). Potentially, further gains would be possible by using a more 752 universal scheme, e.g., the U NI M ORPH scheme. 4.3 Baselines We consider two baselines in our work. First, we consider the M AR M OT tagger (M¨uller et al., 2013), which is currently the best performing non-neural model. The source code for M AR M OT is freely available online,5 which allows us to perform fully controlled experiments with this model. Second, we consider the alignment-based projection approach of Buys and Botha (2016).6 We discuss each of the two baselines in turn. 4.3.1 Alignment-based Projection The projection approach of Buys and Botha (2016) provides an alternative method for transfer learning. The idea is to construct pseudo-annotations for bitext given cross-lingual alignments (Och and Ney, 2003). Then, one trains a standard tagger using the projected annotations. The specific tagger employed is the WSABIE model of Weston et al. (2011), which—like our approach— is a 0th -order discriminative neural model. In contrast to ours, however, their network is shallow. We compare the two methods in more detai"
D17-1078,K17-2001,1,0.700548,"ture, i.e., between adjacent tags.3 We parameterize the distribution over tags at each time step as pθ (ti |w) = softmax (W ei + b) , (3) (2) In other words, we run a bidirectional LSTM over the character stream. This bidirectional LSTM is the sequence bidirectional recurrent neural network of Plank et al. (2016). Note a concatenation of the sequence of character symbols hci1 , . . . , ciMi i results in the word string wi . Each of the Mi characters cik is a member of the set Σ. We take Σ to be the union of sets of characters in the languages considered. We direct the reader to Heigold et al. (2017) for a more in-depth discussion of this and various additional architectures for the computation of v i ; the architecture we have presented in Equation (5) is competitive with the best performing setting in Heigold et al.’s study. 3.2 2 The parameter vector θ is a vectorization of all the parameters discussed below. 3 As an aside, it is quite interesting that a model with the factorization in Equation (1) outperforms the M AR M OT model (M¨uller et al., 2013), which focused on modeling higher-order interactions between the morphological tags, e.g., they employ up to a (pruned) 3rd order CRF."
D17-1078,N16-1077,0,0.0305972,"ual transfer in morphological inflection generation. 7 Character-level NLP. Our work also follows a recent trend in NLP, whereby traditional word-level neural representations are being replaced by character-level representations for a myriad tasks, e.g., POS tagging dos Santos and Zadrozny (2014), parsing (Ballesteros et al., 2015), language modeling (Ling et al., 2015), sentiment analysis (Zhang et al., 2015) as well as the tagger of Heigold et al. (2017), whose work we build upon. Our work is also related to recent work on character-level morphological generation using neural architectures (Faruqui et al., 2016; Rastogi et al., 2016). 6.3 Neural Cross-lingual Transfer in NLP. In terms of methodology, however, our proposal bears similarity to recent work in speech and machine translation–we discuss each in turn. In speech recognition, Heigold et al. (2013) train a cross-lingual neural acoustic model on five Romance languages. The architecture bears similarity to our multi-language softmax approach. Dependency parsing benefits from cross-lingual learning in a similar fashion (Guo et al., 2015, 2016). In neural machine translation (Sutskever et al., 2014; Bahdanau et al., 2015), recent work (Firat et a"
D17-1078,N16-1101,0,0.0246738,"al., 2016; Rastogi et al., 2016). 6.3 Neural Cross-lingual Transfer in NLP. In terms of methodology, however, our proposal bears similarity to recent work in speech and machine translation–we discuss each in turn. In speech recognition, Heigold et al. (2013) train a cross-lingual neural acoustic model on five Romance languages. The architecture bears similarity to our multi-language softmax approach. Dependency parsing benefits from cross-lingual learning in a similar fashion (Guo et al., 2015, 2016). In neural machine translation (Sutskever et al., 2014; Bahdanau et al., 2015), recent work (Firat et al., 2016; Zoph and Knight, 2016; Johnson et al., Conclusion We have presented three character-level recurrent neural network architectures for multi-task crosslingual transfer of morphological taggers. We provided an empirical evaluation of the technique on 18 languages from four different language families, showing wide-spread applicability of the method. We found that the transfer of morphological taggers is an eminently viable endeavor among related language and, in general, the closer the languages, the easier the transfer of morphology becomes. Our technique outperforms two strong baselines propo"
D17-1078,I05-1075,0,0.231845,"ples 213 Figure 3: Learning Curve for Spanish and Catalan comparing our monolingual model, our joint model and two M AR M OT models. The first M AR M OT model is identical to those trained in the rest of the paper and the second attempts a multi-task approach, which failed so no further experimentation was performed with this model. training the tagger in the low-resource language through annotations projected over aligned bitext with a high-resource language. This method of projection was first introduced by Yarowsky and Ngai (2001) for the projection of POS annotation. While follow-up work (Fossum and Abney, 2005; Das and Petrov, 2011; T¨ackstr¨om et al., 2012) has continually demonstrated the efficacy of projecting simple part-of-speech annotations, Buys and Botha (2016) were the first to show the use of bitext-based projection for the training of a morphological tagger for low-resource languages. 755 As we also discuss the training of a morphological tagger, our work is most closely related to Buys and Botha (2016) in terms of the task itself. We contrast the approaches. The main difference lies therein, that our approach is not projection-based and, thus, does not require the construction of a bili"
D17-1078,N13-1014,0,0.0384871,"our approach to various baselines for low-resource tagging under token-level accuracy. We compare on only those languages in Buys and Botha (2016). Note that tag-level accuracy was not reported in the original B&B paper, but was acquired through personal communication with the first author. All architectures presented in this work are used in their multi-source setting. The B&B and M AR M OT models are single-source. Accuracy annotation of a small number of sentences in the target language for training. We note, however, that this does not necessitate a large number of human annotation hours (Garrette and Baldridge, 2013). Reducing Sample Complexity. Another interesting a point about our model that is best evinced in Figure 3 is the feature-based CRF approach seems to be a better choice for the low-resource setting, i.e., the neural model has greater sample complexity. However, in the multi-task scenario, we find that the neural tagger’s learning curve is even steeper. In other words, if we have to train a tagger on very little data, we are better off using a neural multi-task approach than a feature-based approach; preliminary attempts to develop a multitask version of M AR M OT failed (see Figure 3). 6 Relat"
D17-1078,P15-1119,0,0.0392137,"ork is also related to recent work on character-level morphological generation using neural architectures (Faruqui et al., 2016; Rastogi et al., 2016). 6.3 Neural Cross-lingual Transfer in NLP. In terms of methodology, however, our proposal bears similarity to recent work in speech and machine translation–we discuss each in turn. In speech recognition, Heigold et al. (2013) train a cross-lingual neural acoustic model on five Romance languages. The architecture bears similarity to our multi-language softmax approach. Dependency parsing benefits from cross-lingual learning in a similar fashion (Guo et al., 2015, 2016). In neural machine translation (Sutskever et al., 2014; Bahdanau et al., 2015), recent work (Firat et al., 2016; Zoph and Knight, 2016; Johnson et al., Conclusion We have presented three character-level recurrent neural network architectures for multi-task crosslingual transfer of morphological taggers. We provided an empirical evaluation of the technique on 18 languages from four different language families, showing wide-spread applicability of the method. We found that the transfer of morphological taggers is an eminently viable endeavor among related language and, in general, the cl"
D17-1078,P11-1061,0,0.0955636,"ing Curve for Spanish and Catalan comparing our monolingual model, our joint model and two M AR M OT models. The first M AR M OT model is identical to those trained in the rest of the paper and the second attempts a multi-task approach, which failed so no further experimentation was performed with this model. training the tagger in the low-resource language through annotations projected over aligned bitext with a high-resource language. This method of projection was first introduced by Yarowsky and Ngai (2001) for the projection of POS annotation. While follow-up work (Fossum and Abney, 2005; Das and Petrov, 2011; T¨ackstr¨om et al., 2012) has continually demonstrated the efficacy of projecting simple part-of-speech annotations, Buys and Botha (2016) were the first to show the use of bitext-based projection for the training of a morphological tagger for low-resource languages. 755 As we also discuss the training of a morphological tagger, our work is most closely related to Buys and Botha (2016) in terms of the task itself. We contrast the approaches. The main difference lies therein, that our approach is not projection-based and, thus, does not require the construction of a bilingual lexicon for proj"
D17-1078,P15-1033,0,0.0261944,"lity tools from smaller amounts of data. In this work, we focus on transfer learning—we train a recurrent neural tagger for a low-resource language jointly with a tagger for a related highresource language. Forcing the models to share character-level features among the languages allows large gains in accuracy when tagging the lowresource languages, while maintaining (or even improving) accuracy on the high-resource language. Recurrent neural networks constitute the state of the art for a myriad of tasks in NLP, e.g., multilingual part-of-speech tagging (Plank et al., 2016), syntactic parsing (Dyer et al., 2015; Zeman et al., 2017), morphological paradigm completion (Cotterell et al., 2016, 2017) and language modeling georg.heigold@gmail.com (Sundermeyer et al., 2012; Melis et al., 2017); recently, such models have also improved morphological tagging (Heigold et al., 2016, 2017). In addition to increased performance over classical approaches, neural networks also offer a second advantage: they admit a clean paradigm for multitask learning. If the learned representations for all of the tasks are embedded jointly into a shared vector space, the various tasks reap benefits from each other and often per"
D17-1078,E17-1048,1,0.86218,"Missing"
D17-1078,P82-1020,0,0.829335,"Missing"
D17-1078,Q17-1024,0,0.0694221,"Missing"
D17-1078,P17-1182,1,0.871252,"Missing"
D17-1078,D15-1176,0,0.0360311,"wide variety of languages. Our work addresses a different task, but the undergirding philosophical motivation is similar, i.e., attack low-resource NLP through multi-task transfer learning. Kann et al. (2017) offer a similar method for cross-lingual transfer in morphological inflection generation. 7 Character-level NLP. Our work also follows a recent trend in NLP, whereby traditional word-level neural representations are being replaced by character-level representations for a myriad tasks, e.g., POS tagging dos Santos and Zadrozny (2014), parsing (Ballesteros et al., 2015), language modeling (Ling et al., 2015), sentiment analysis (Zhang et al., 2015) as well as the tagger of Heigold et al. (2017), whose work we build upon. Our work is also related to recent work on character-level morphological generation using neural architectures (Faruqui et al., 2016; Rastogi et al., 2016). 6.3 Neural Cross-lingual Transfer in NLP. In terms of methodology, however, our proposal bears similarity to recent work in speech and machine translation–we discuss each in turn. In speech recognition, Heigold et al. (2013) train a cross-lingual neural acoustic model on five Romance languages. The architecture bears similari"
D17-1078,D15-1272,1,0.853018,"Missing"
D17-1078,D13-1032,0,0.186357,"Missing"
D17-1078,J03-1002,0,0.0099706,"consider the M AR M OT tagger (M¨uller et al., 2013), which is currently the best performing non-neural model. The source code for M AR M OT is freely available online,5 which allows us to perform fully controlled experiments with this model. Second, we consider the alignment-based projection approach of Buys and Botha (2016).6 We discuss each of the two baselines in turn. 4.3.1 Alignment-based Projection The projection approach of Buys and Botha (2016) provides an alternative method for transfer learning. The idea is to construct pseudo-annotations for bitext given cross-lingual alignments (Och and Ney, 2003). Then, one trains a standard tagger using the projected annotations. The specific tagger employed is the WSABIE model of Weston et al. (2011), which—like our approach— is a 0th -order discriminative neural model. In contrast to ours, however, their network is shallow. We compare the two methods in more detail in §6. 4.3.3 Architecture Study Additionally, we perform a thorough study of the neural transfer learner, considering all three architectures. A primary goal of our experiments is to determine which of our three proposed neural transfer techniques is superior. Even though our experiments"
D17-1078,A94-1024,0,0.644736,"Missing"
D17-1078,petrov-etal-2012-universal,0,0.0254411,"trict generalization of POS tagging, where we have significantly refined the set of available tags. All of the experiments in this paper make use of the universal morphological tag set available in the Universal Dependencies (UD) (Nivre et al., 2016). As an example, we have provided a Russian sentence with its UD tagging in Figure 1. Transferring Morphology. The transfer of morphology is arguably more dependent on the relatedness of the languages in question than other annotations in NLP, such as POS and named entity recognition (NER). POS lends itself nicely to a universal annotation scheme (Petrov et al., 2012) and traditional NER is limited to a small number of cross-linguistically compliant categories, e.g., PER SON and PLACE . Even universal dependency arcs employ cross-lingual labels (Nivre et al., 2016). Notation. We will discuss morphological tagging in terms of the following notation. We will consider two (related) languages: a high-resource source language `s and a low-resource target language `t . Each of these languages will have its own (potentially overlapping) set of morphological tags, denoted Ts and Tt , respectively. We will work with the union of both sets T = Ts ∪ Tt . An individua"
D17-1078,P16-2067,0,0.228797,"that allow for the training of high-quality tools from smaller amounts of data. In this work, we focus on transfer learning—we train a recurrent neural tagger for a low-resource language jointly with a tagger for a related highresource language. Forcing the models to share character-level features among the languages allows large gains in accuracy when tagging the lowresource languages, while maintaining (or even improving) accuracy on the high-resource language. Recurrent neural networks constitute the state of the art for a myriad of tasks in NLP, e.g., multilingual part-of-speech tagging (Plank et al., 2016), syntactic parsing (Dyer et al., 2015; Zeman et al., 2017), morphological paradigm completion (Cotterell et al., 2016, 2017) and language modeling georg.heigold@gmail.com (Sundermeyer et al., 2012; Melis et al., 2017); recently, such models have also improved morphological tagging (Heigold et al., 2016, 2017). In addition to increased performance over classical approaches, neural networks also offer a second advantage: they admit a clean paradigm for multitask learning. If the learned representations for all of the tasks are embedded jointly into a shared vector space, the various tasks reap"
D17-1078,N16-1076,1,0.790468,"logical inflection generation. 7 Character-level NLP. Our work also follows a recent trend in NLP, whereby traditional word-level neural representations are being replaced by character-level representations for a myriad tasks, e.g., POS tagging dos Santos and Zadrozny (2014), parsing (Ballesteros et al., 2015), language modeling (Ling et al., 2015), sentiment analysis (Zhang et al., 2015) as well as the tagger of Heigold et al. (2017), whose work we build upon. Our work is also related to recent work on character-level morphological generation using neural architectures (Faruqui et al., 2016; Rastogi et al., 2016). 6.3 Neural Cross-lingual Transfer in NLP. In terms of methodology, however, our proposal bears similarity to recent work in speech and machine translation–we discuss each in turn. In speech recognition, Heigold et al. (2013) train a cross-lingual neural acoustic model on five Romance languages. The architecture bears similarity to our multi-language softmax approach. Dependency parsing benefits from cross-lingual learning in a similar fashion (Guo et al., 2015, 2016). In neural machine translation (Sutskever et al., 2014; Bahdanau et al., 2015), recent work (Firat et al., 2016; Zoph and Knig"
D17-1078,P15-2111,0,0.0258372,"ss-lingual annotation. PAST INDICATIVE SINGULAR PLURAL SINGULAR PLURAL sue˜no sue˜nas sue˜na so˜namos so˜na´ is sue˜nan so˜ne´ so˜naste so˜no´ so˜namos so˜nasteis so˜naron Table 1: Partial inflection table for the Spanish verb so˜nar the given form (in a sentential context). For concreteness, we list a more complete table of Spanish verbal inflections in Table 1. Note that some languages, e.g., Archi, Northeast Caucasian language, display a veritable cornucopia of potential forms with the size of the verbal paradigm exceeding 10,000 (Kibrik, 1998). Standard NLP annotation, e.g., the scheme in Sylak-Glassman et al. (2015), marks forms in terms of universal key-attribute pairs, e.g., the first person present singular is represented as [ pos=V, per=1, num=SG, tns=PRES ]. This bundle of key-attributes pairs is typically termed a morphological tag and we may view the goal of morphological tagging to label each word in its sentential context with the appropriate tag (Oflazer and Kuru¨oz, 1994; Hajiˇc and Hladk´a, 1998). As the part-of-speech (POS) is a component of the tag, we may view morphological tagging as a strict generalization of POS tagging, where we have significantly refined the set of available tags. All"
D17-1078,N12-1052,0,0.172043,"Missing"
D17-1078,N01-1026,0,0.0697308,"60 55 Languages Joint MarMoT Mono MarMoT-Trans 26 27 28 29 210 211 212 Number of Samples 213 Figure 3: Learning Curve for Spanish and Catalan comparing our monolingual model, our joint model and two M AR M OT models. The first M AR M OT model is identical to those trained in the rest of the paper and the second attempts a multi-task approach, which failed so no further experimentation was performed with this model. training the tagger in the low-resource language through annotations projected over aligned bitext with a high-resource language. This method of projection was first introduced by Yarowsky and Ngai (2001) for the projection of POS annotation. While follow-up work (Fossum and Abney, 2005; Das and Petrov, 2011; T¨ackstr¨om et al., 2012) has continually demonstrated the efficacy of projecting simple part-of-speech annotations, Buys and Botha (2016) were the first to show the use of bitext-based projection for the training of a morphological tagger for low-resource languages. 755 As we also discuss the training of a morphological tagger, our work is most closely related to Buys and Botha (2016) in terms of the task itself. We contrast the approaches. The main difference lies therein, that our appr"
D17-1078,N16-1004,0,0.0173951,"et al., 2016). 6.3 Neural Cross-lingual Transfer in NLP. In terms of methodology, however, our proposal bears similarity to recent work in speech and machine translation–we discuss each in turn. In speech recognition, Heigold et al. (2013) train a cross-lingual neural acoustic model on five Romance languages. The architecture bears similarity to our multi-language softmax approach. Dependency parsing benefits from cross-lingual learning in a similar fashion (Guo et al., 2015, 2016). In neural machine translation (Sutskever et al., 2014; Bahdanau et al., 2015), recent work (Firat et al., 2016; Zoph and Knight, 2016; Johnson et al., Conclusion We have presented three character-level recurrent neural network architectures for multi-task crosslingual transfer of morphological taggers. We provided an empirical evaluation of the technique on 18 languages from four different language families, showing wide-spread applicability of the method. We found that the transfer of morphological taggers is an eminently viable endeavor among related language and, in general, the closer the languages, the easier the transfer of morphology becomes. Our technique outperforms two strong baselines proposed in previous work. M"
D17-1078,P98-1080,0,\N,Missing
D17-1078,C98-1077,0,\N,Missing
D17-1078,L16-1262,0,\N,Missing
D17-1078,K17-3001,0,\N,Missing
D17-1078,W16-2002,1,\N,Missing
D18-1042,D16-1250,0,0.30206,"om a one-to-many alignment a, rather than a matching m. Why a Reinterpretation? The reinterpretation of Artetxe et al. (2017) as a probabilistic model yields a clear analytical comparison between our method and theirs. The only difference between the two is the constraint on the bilingual lexicon that the model is allowed to induce. 6 Experiments We first conduct experiments on bilingual dictionary induction and cross-lingual word similarity on three standard language pairs, English–Italian, English–German, and English–Finnish. 462 Mikolov et al. (2013c) Xing et al. (2015) Zhang et al. (2016) Artetxe et al. (2016) Artetxe et al. (2017) Ours (1:1) Ours (1:1, rank constr.) 5,000 English–Italian 25 num iden 5,000 34.93 36.87 36.73 39.27 39.67 41.00 42.47 00.00 0.00 0.07 0.07 37.27 39.63 41.13 1.87 27.13 28.07 31.07 39.97 41.07 41.80 35.00 41.27 40.80 41.87 40.87 42.60 41.93 0.00 0.13 0.27 0.40 39.40 40.47 41.40 English–German 25 num iden 0.00 0.07 0.13 0.13 39.60 42.40 42.40 0.07 0.53 0.87 0.73 40.27 42.60 41.93 19.20 38.13 38.27 41.53 40.67 43.20 41.47 5,000 25.91 28.23 28.16 30.62 28.72 29.78 28.23 English–Finnish 25 num iden 0.00 0.07 0.14 0.21 28.16 0.07 27.04 0.00 0.56 0.42 0.77 26.47 3.02 27.60 7.02"
D18-1042,P17-1042,0,0.0577102,"g a novel discriminative latent-variable model that outperforms previous work. Our proposed model is a bridge between current state-of-the-art methods in bilingual lexicon induction that take advantage of word embeddings, e.g., the embeddings induced by Mikolov et al. (2013b)’s skip-gram objective, and older ideas in the literature that build explicit probabilistic models for the task. We propose a discriminative probability model, inspired by Irvine and Callison-Burch (2013), infused with the bipartite matching dictionary prior of Haghighi et al. (2008). However, like more recent approaches (Artetxe et al., 2017), our model operates directly over pretrained word embeddings, induces a joint cross-lingual embedding space, and scales to large vocabulary sizes. To train our model, we derive a generalized expectationmaximization algorithm (EM; Neal and Hinton, 1998) and employ an efficient matching algorithm. Empirically, we experiment on three standard and three extremely low-resource language pairs. We evaluate intrinsically, comparing the quality of the induced bilingual dictionary, as well as analyzing the resulting bilingual word embeddings themselves. The latent-variable model yields gains over sever"
D18-1042,J93-2003,0,0.197772,"and Hinton, 1998) and employ an efficient matching algorithm. Empirically, we experiment on three standard and three extremely low-resource language pairs. We evaluate intrinsically, comparing the quality of the induced bilingual dictionary, as well as analyzing the resulting bilingual word embeddings themselves. The latent-variable model yields gains over several previous approaches across language pairs. It also enables us to make implicit modeling assumptions explicit. To this end, we provide a reinterpretation of Artetxe et al. (2017) as a latent-variable model with an IBM Model 1–style (Brown et al., 1993) dictionary prior, which allows a clean side-by-side analytical comparison. Viewed in this light, the difference between our approach and Artetxe et al. (2017), the strongest baseline, is whether one-to-one alignments or one-to-many alignments are admitted between the words of the languages’ respective lexicons. Thus, we conclude that our hard constraint on one-to-one alignments is primarily responsible for the improvements over Artetxe et al. (2017). 2 Background: Bilingual Lexicon Induction and Word Embeddings Bilingual lexicon induction2 is the task of finding word-level translations betwee"
D18-1042,P15-2001,0,0.0227503,"h Søgaard et al. (2018), we additionally use a dictionary of identically spelled strings in both vocabularies. Table 2: Spearman correlations on English–Italian and English–German cross-lingual word similarity datasets. 6.1 Experimental Details Datasets For bilingual dictionary induction, we use the English–Italian dataset by Dinu et al. (2015) and the English–German and English–Finnish datasets by Artetxe et al. (2017). For cross-lingual word similarity, we use the RG-65 and WordSim353 cross-lingual datasets for English–German and the WordSim-353 cross-lingual dataset for English– Italian by Camacho-Collados et al. (2015). Monolingual Embeddings We follow Artetxe et al. (2017) and train monolingual embeddings with word2vec, CBOW, and negative sampling (Mikolov et al., 2013a) on a 2.8 billion word corpus for English (ukWaC + Wikipedia + BNC), a 1.6 billion word corpus for Italian (itWaC), a 0.9 billion word corpus for German (SdeWaC), and a 2.8 billion word corpus for Finnish (Common Crawl). Implementation details Similar to Artetxe et al. (2017), we stop training when the improvement on the average cosine similarity for the induced dictionary is below 1 × 10−6 between succeeding iterations. Unless stated other"
D18-1042,W95-0114,0,0.626272,"ilingual lexicon and (bilingual) dictionary synonymously. On the other hand, unmodified lexicon always refers to a word list in a single language. 458 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 458–468 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics equivalent, so the pair Hund–dog should be an entry in a German–English bilingual lexicon. The task itself comes in a variety of flavors. We consider a version of the task that only relies on monolingual corpora in the tradition of Rapp (1995) and Fung (1995). In other words, the goal is to produce a bilingual lexicon primarily from unannotated raw text in each of the respective languages. Importantly, we avoid reliance on bitext, i.e. corpora with parallel sentences that are known translations of each other, e.g., EuroParl (Koehn, 2005). The bitext assumption is quite common in the literature; see Ruder et al. (2018, Table 2) for a survey. Additionally, we will assume the existence of a small seed set of word-level translations obtained from a dictionary; we also experiment with seed sets obtained from heuristics that do not rely on the existence"
D18-1042,P08-1088,0,0.333996,"ova KementchedjhievaZ Anders SøgaardZ @Insight Research Centre, National University of Ireland, Galway, Ireland HAylien Ltd., Dublin, Ireland SThe Computer Laboratory, University of Cambridge, Cambridge, UK PDepartment of Computer Science, Johns Hopkins University, Baltimore, USA ZDepartment of Computer Science, University of Copenhagen, Copenhagen, Denmark sebastian@ruder.io,ryan.cotterell@jhu.com,{yova|soegaard}@di.ku.dk Abstract We introduce a novel discriminative latentvariable model for the task of bilingual lexicon induction. Our model combines the bipartite matching dictionary prior of Haghighi et al. (2008) with a state-of-the-art embeddingbased approach. To train the model, we derive an efficient Viterbi EM algorithm. We provide empirical improvements on six language pairs under two metrics and show that the prior theoretically and empirically helps to mitigate the hubness problem. We also demonstrate how previous work may be viewed as a similarly fashioned latent-variable model, albeit with a different prior.1 1 Introduction Is there a more fundamental bilingual linguistic resource than a dictionary? The task of bilingual lexicon induction seeks to create a dictionary in a datadriven manner di"
D18-1042,N13-1056,0,0.0178519,"ntiev et al., 2012) to cross-lingual named entity recognition (Mayhew et al., 2017). In this work, we offer a probabilistic twist on the task, developing a novel discriminative latent-variable model that outperforms previous work. Our proposed model is a bridge between current state-of-the-art methods in bilingual lexicon induction that take advantage of word embeddings, e.g., the embeddings induced by Mikolov et al. (2013b)’s skip-gram objective, and older ideas in the literature that build explicit probabilistic models for the task. We propose a discriminative probability model, inspired by Irvine and Callison-Burch (2013), infused with the bipartite matching dictionary prior of Haghighi et al. (2008). However, like more recent approaches (Artetxe et al., 2017), our model operates directly over pretrained word embeddings, induces a joint cross-lingual embedding space, and scales to large vocabulary sizes. To train our model, we derive a generalized expectationmaximization algorithm (EM; Neal and Hinton, 1998) and employ an efficient matching algorithm. Empirically, we experiment on three standard and three extremely low-resource language pairs. We evaluate intrinsically, comparing the quality of the induced bil"
D18-1042,K18-1021,1,0.770121,"nking neighbor lists. Lazaridou et al. (2015) proposed a max-marging objective as a solution, while more recent approaches proposed to modify the nearest neighbor retrieval by inverting the softmax (Smi, 2017) or scaling the similarity values (Conneau et al., 2018). 9 Table 5: Example translations for German-English. (2017) in German and seek their nearest neighbours in the English embedding space. P@1 over the German-English test set is 36.38 and 39.18 for Artetxe et al. (2017) and our method respectively. We show examples where nearest neighbours of the methods differ in Table 5. Similar to Kementchedjhieva et al. (2018), we find that morphologically related words are often the source of mistakes. Other common sources of mistakes in this dataset are names that are translated to different names and nearly synonymous words being predicted. Both of these sources indicate that while the learned alignment is generally good, it is often not sufficiently precise. 8 Conclusion We have presented a novel latent-variable model for bilingual lexicon induction, building on the work of Artetxe et al. (2017). Our model combines the prior over bipartite matchings inspired by Haghighi et al. (2008) and the discriminative, rat"
D18-1042,E12-1014,0,0.0237578,"ally helps to mitigate the hubness problem. We also demonstrate how previous work may be viewed as a similarly fashioned latent-variable model, albeit with a different prior.1 1 Introduction Is there a more fundamental bilingual linguistic resource than a dictionary? The task of bilingual lexicon induction seeks to create a dictionary in a datadriven manner directly from monolingual corpora in the respective languages and, perhaps, a small seed set of translations. From a practical point of view, bilingual dictionaries have found uses in a myriad of NLP tasks ranging from machine translation (Klementiev et al., 2012) to cross-lingual named entity recognition (Mayhew et al., 2017). In this work, we offer a probabilistic twist on the task, developing a novel discriminative latent-variable model that outperforms previous work. Our proposed model is a bridge between current state-of-the-art methods in bilingual lexicon induction that take advantage of word embeddings, e.g., the embeddings induced by Mikolov et al. (2013b)’s skip-gram objective, and older ideas in the literature that build explicit probabilistic models for the task. We propose a discriminative probability model, inspired by Irvine and Callison"
D18-1042,2005.mtsummit-papers.11,0,0.0965587,"1 - November 4, 2018. 2018 Association for Computational Linguistics equivalent, so the pair Hund–dog should be an entry in a German–English bilingual lexicon. The task itself comes in a variety of flavors. We consider a version of the task that only relies on monolingual corpora in the tradition of Rapp (1995) and Fung (1995). In other words, the goal is to produce a bilingual lexicon primarily from unannotated raw text in each of the respective languages. Importantly, we avoid reliance on bitext, i.e. corpora with parallel sentences that are known translations of each other, e.g., EuroParl (Koehn, 2005). The bitext assumption is quite common in the literature; see Ruder et al. (2018, Table 2) for a survey. Additionally, we will assume the existence of a small seed set of word-level translations obtained from a dictionary; we also experiment with seed sets obtained from heuristics that do not rely on the existence of linguistic resources. m richtig bird Vogel mother trinken drink Mutter right Wurzel eye werfen ash usrc rot utrg Figure 1: Partial lexicons of German and English shown as a 2.1 bipartite graph. German is the target language and English is the source language. The ntrg = 7 German"
D18-1042,P15-1027,0,0.605353,"results for our approach in comparison to the baselines in Figure 2 for English–Italian using a 5,000 word seed lexicon across vocabularies consisting of different 12 Other recent improvements such as symmetric reweighting (Artetxe et al., 2018) are orthogonal to our method, which is why we do not explicitly compare to them here. 13 Note that results are not directly comparable to (Conneau et al., 2018) due to the use of embeddings trained on different monolingual corpora (WaCKy vs. Wikipedia). Hubness problem We analyze empirically whether the prior helps with the hubness problem. Following Lazaridou et al. (2015), we define the hubness Nk (y) at k of a target word y as follows: Nk (y) = |{x ∈ Q |y ∈ NNk (x, G)}| (14) where Q is a set of query source language words and NNk (x, G) denotes the k nearest neighbors 464 14 We only use the words in the 5,000 word seed lexicon that are contained in the n most frequent words. We do not show results for the 25 word seed lexicon and numerals as they are not contained in the smallest n of most frequent words. (a) English–Italian (b) English–German (c) English–Finnish Figure 3: Bilingual dictionary induction results of our method with different priors using a 5,00"
D18-1042,D17-1269,0,0.108922,"Missing"
D18-1042,P95-1050,0,0.637006,"paper, we use bilingual lexicon and (bilingual) dictionary synonymously. On the other hand, unmodified lexicon always refers to a word list in a single language. 458 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 458–468 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics equivalent, so the pair Hund–dog should be an entry in a German–English bilingual lexicon. The task itself comes in a variety of flavors. We consider a version of the task that only relies on monolingual corpora in the tradition of Rapp (1995) and Fung (1995). In other words, the goal is to produce a bilingual lexicon primarily from unannotated raw text in each of the respective languages. Importantly, we avoid reliance on bitext, i.e. corpora with parallel sentences that are known translations of each other, e.g., EuroParl (Koehn, 2005). The bitext assumption is quite common in the literature; see Ruder et al. (2018, Table 2) for a survey. Additionally, we will assume the existence of a small seed set of word-level translations obtained from a dictionary; we also experiment with seed sets obtained from heuristics that do not rely"
D18-1042,N12-1087,0,0.0176416,"tep. Viterbi EM estimates the parameters by alternating between the two steps until convergence. We give the complete pseudocode in Algorithm 1. 4.1 Viterbi E-Step The E-step asks us to compute the posterior of latent bipartite matchings p(m |S, T ). Computation of this distribution, however, is intractable as it would require a sum over all bipartite matchings, which is #P-hard (Valiant, 1979). Tricks from combinatorial optimization make it possible to maximize over all bipartite matchings in polynomial time. Thus, we fall back on the Viterbi approximation for the E-step (Brown et al., 1993; Samdani et al., 2012). The derivation will follow Haghighi et al. (2008). In order to compute m? = argmax log pθ (m |S, T ) Finding a Maximal Bipartite Matching We frame finding an optimal one-to-one alignment between nsrc source and ntrg words as a combinatorial optimization problem, specifically, a linear assignment problem (LAP; Bertsimas and Tsitsiklis, 1997). In its original formulation, the LAP requires assigning a number of agents (source words) to a number of tasks (target words) at a cost that varies based on each assignment. An optimal solution assigns each source word to exactly one target word and vice"
D18-1042,P18-1072,1,0.88512,"Missing"
D18-1042,P16-1024,0,0.136192,"Missing"
D18-1042,N15-1104,0,0.514311,"source cannot be used more than once.4 (ii) There exists an orthogonal transformation, after which the embedding spaces are more or less equivalent. Assumption (i) may be true for related languages, but is likely false for morphologically rich languages that have a many-to-many relationship between the words in their respective lexicons. We propose to ameliorate this using a rank constraint that only considers the top n most frequent words in both lexicons for matching in §6. In addition, we experiment with priors that express different matchings in §7. As for assumption (ii), previous work (Xing et al., 2015; Artetxe et al., 2017) has achieved some success using an orthogonal transformation; recently, however, Søgaard et al. (2018) demonstrated that monolingual embedding spaces are not approximately isomorphic and that there is a complex relationship between word form and meaning, which is only inadequately modeled by current approaches, which for example cannot model polysemy. Nevertheless, we will show that imbuing our model with these assumptions helps empirically in §6, giving them practical utility. Why it Works: The Hubness Problem Why should we expect the bipartite matching prior to help,"
D18-1042,N16-1156,0,0.309433,"Sa and Ta formed from a one-to-many alignment a, rather than a matching m. Why a Reinterpretation? The reinterpretation of Artetxe et al. (2017) as a probabilistic model yields a clear analytical comparison between our method and theirs. The only difference between the two is the constraint on the bilingual lexicon that the model is allowed to induce. 6 Experiments We first conduct experiments on bilingual dictionary induction and cross-lingual word similarity on three standard language pairs, English–Italian, English–German, and English–Finnish. 462 Mikolov et al. (2013c) Xing et al. (2015) Zhang et al. (2016) Artetxe et al. (2016) Artetxe et al. (2017) Ours (1:1) Ours (1:1, rank constr.) 5,000 English–Italian 25 num iden 5,000 34.93 36.87 36.73 39.27 39.67 41.00 42.47 00.00 0.00 0.07 0.07 37.27 39.63 41.13 1.87 27.13 28.07 31.07 39.97 41.07 41.80 35.00 41.27 40.80 41.87 40.87 42.60 41.93 0.00 0.13 0.27 0.40 39.40 40.47 41.40 English–German 25 num iden 0.00 0.07 0.13 0.13 39.60 42.40 42.40 0.07 0.53 0.87 0.73 40.27 42.60 41.93 19.20 38.13 38.27 41.53 40.67 43.20 41.47 5,000 25.91 28.23 28.16 30.62 28.72 29.78 28.23 English–Finnish 25 num iden 0.00 0.07 0.14 0.21 28.16 0.07 27.04 0.00 0.56 0.42 0.77"
D18-1473,N16-1102,0,0.0690913,"Missing"
D18-1473,K17-2001,1,0.117988,"racters to transduce from. As the non-monotonic alignment is the latent variable, we will face a combinatorial problem: summing over all non-monotonic alignments. The algorithmic contribution of this paper is the derivation of a simple dynamic program for computing this sum in polynomial time that still allows for very rich recurrent neural featurization of the model. With respect to the literature, our paper represents the first instance of exact marginalization for a neural transducer with hard non-monotonic alignment; previous methods, such as Rastogi et al. (2016) and Aharoni and Goldberg (2017), are exclusively monotonic. Non-monotonic methods dominate characterlevel transduction. Indeed, the state of art in classic character-level NLP tasks such as graphemeto-phoneme conversion (Yao and Zweig, 2015), transliteration (Rosca and Breuel, 2016) and morphological inflection generation (Kann and Schütze, 2016) is held by the soft non-monotonic method of Bahdanau et al. (2015). Even though non-monotonicity is more common in word-level tasks, it also exists in character-level transduction tasks, as evidenced by our example in Fig. 1 and the superior performance of non-monotonic methods. Ou"
D18-1473,D15-1166,0,0.823203,"We note that there is no reason why we need to make this independence assumption—we will likely want a targetside language model in transduction. Indeed, subsequent statistical machine translation systems, e.g., M OSES (Koehn et al., 2007), integrate a language model into the decoder. It is of note that many 3.2 4 Algorithmic Analysis: Time Complexity Recurrent Neural Parameterization How do we parameterize p(yi |ai , y&lt;i , x) and αj (i) in our hard, non-monotonic transduction model? We will use a neural network identical to the one proposed in the attention-based sequenceto-sequence model of Luong et al. (2015) without input feeding (a variant of Bahdanau et al. (2015)). 4.1 Encoding the Input All models discussed in this exposition will make use of the same mechanism for mapping a source string x ∈ Σ∗x into a fixed-length representation in Rdh . This mapping will take the form of a bidirectional recurrent neural network encoder, which works as follows: each element of Σx is mapped to an embedding vector of length de through a mapping: e : Σx → Rde . Now, the RNN folds the 4427 x a1 a2 a3 a4 h(dec) 1 h(dec) 2 h(dec) 3 h(dec) 4 y1 y2 y3 y4 Figure 2: Our hard-attention model without input feeding view"
D18-1473,W15-3901,0,0.348629,"rce and target string for each task in Tab. 1. Grapheme-to-Phoneme Conversion. We use the standard grapheme-to-phoneme conversion (G2P) dataset: the Sphinx-compatible version of CMUDict (Weide, 1998) and NetTalk (Sejnowski and Rosenberg, 1987). G2P transduces a word, a string of graphemes, to its pronunciation, a string of phonemes. We evaluate with word error rate (WER) and phoneme error rate (PER) (Yao and Zweig, 2015). PER is equal to the edit distance divided by the length of the string of phonemes. Named-Entity Transliteration. We use the NEWS 2015 shared task on machine transliteration (Zhang et al., 2015) as our named-entity transliteration dataset. It contains 14 language pairs. Transliteration transduces a named entity from its source language to a target language—in other words, from a string in the source orthography to a string in the target orthography. We evaluate with word accuracy in percentage (ACC) and mean F-score soft attention input-fed that generalize IBM Model 1, are easily bolted on to our proposed model as well. If we are willing to perform approximate inference, we may also consider fertility as found in IBM Model 4. In order to extend our method to machine translation (MT)"
D18-1473,N16-1076,1,0.816,"input string out-of-order to determine the characters to transduce from. As the non-monotonic alignment is the latent variable, we will face a combinatorial problem: summing over all non-monotonic alignments. The algorithmic contribution of this paper is the derivation of a simple dynamic program for computing this sum in polynomial time that still allows for very rich recurrent neural featurization of the model. With respect to the literature, our paper represents the first instance of exact marginalization for a neural transducer with hard non-monotonic alignment; previous methods, such as Rastogi et al. (2016) and Aharoni and Goldberg (2017), are exclusively monotonic. Non-monotonic methods dominate characterlevel transduction. Indeed, the state of art in classic character-level NLP tasks such as graphemeto-phoneme conversion (Yao and Zweig, 2015), transliteration (Rosca and Breuel, 2016) and morphological inflection generation (Kann and Schütze, 2016) is held by the soft non-monotonic method of Bahdanau et al. (2015). Even though non-monotonicity is more common in word-level tasks, it also exists in character-level transduction tasks, as evidenced by our example in Fig. 1 and the superior performa"
D18-1473,P17-2091,0,0.108811,"seminal work in statistical machine translation by Brown et al. (1993)—it has been forgotten in recent formulations of hard alignment (Xu et al., 2015), which use stochastic approximation to handle the exponential summands. As we will see in §5, we can compute the soft-attention model of Bahdanau et al. (2015) in O (|x |· |y |+ |y |· |Σy |) time. When Σy is large, for example in case of machine translation with tens of thousands of Σy at least, we can ignore |x |· |y |in soft-attention model, and the exact marginalization has an extra |x|-factor compared to soft-attention model. In practice, Shi and Knight (2017) show the bottleneck of a NMT system is the softmax layer, making the extra |x|-factor practically cumbersome. Relation to IBM Model 1. The derivation above is similar to that of the IBM alignment model 1. We remark, however, two key generalizations that will serve our recurrent neural parameterization well in §4. First, traditionally, derivations of IBM Model 1 omit a prior over alignments p(ai |x), taking it to be uniform. Due to this omission, an additional multiplicative constant ε/|x||y |is introduced to ensure the distribution remains normalized (Koehn, 2009). Second, IBM Model 1 does no"
D18-1473,C96-2141,0,0.725553,"identical set of parameters. Note that we have omitted the dependence of p(a | x) on the appropriate prefix of y; this was done for notational simplicity. Using this bound, Xu et al. (2015) derive an efficient approximation to the gradient using the REINFORCE trick of Williams (1992). This sampling-based gradient estimator is then used for learning, but suffers from high variance. We compare to this model in §8. 6 Future Work Just as Brown et al. (1993) started with IBM Model 1 and build up to richer models, we can do the same. Extensions, resembling those of IBM Model 2 and the HMM aligner (Vogel et al., 1996) 4429 source target Grapheme-to-phoneme conversion a c t i o n AE K SH AH N Named-entity transliteration Morphological inflection A A C H E N N AT+ALL SG l i p u k e 아헨 l i p u k k e e l l e Table 1: Example of source and target string for each task as processed by the model 7 The Tasks The empirical portion of the paper focuses on character-level string-to-string transduction problems. We consider three tasks: G : grapheme-tophoneme conversion, T : named-entity transliteration, and I : morphological inflection. We describe each briefly in turn and we give an example of a source and target str"
D19-1090,P17-1042,0,0.480953,"imental paradigm in which we independently control for four different variables: the word form’s frequency, morphology, the lexeme frequency and the lexeme (a total of 480 experiments). Our comprehensive analysis reveals that BLI models can generalize for frequent morphosyntactic categories, even of infrequent lexemes, but fail to generalize for the more rare categories. This yields a more nuanced picture of the known deficiency of word embeddings to underperform on infrequent words (Gong et al., 2018). Our findings also contradict the strong empirical claims made elsewhere in the literature (Artetxe et al., 2017; Conneau et al., 2018; Ruder et al., 2018; Grave et al., 2018b), as we observe that performance severely degrades when the evaluation includes rare morphological variants of a word and infrequent lexemes. We picture this general trend in Figure 1, which also highlights the skew of existing dictionaries towards more frequent words.2 As our final contribution, we demonstrate that better encoding of morphology is indeed beneficial: enforcing a simple morphological constraint yields consistent performance improvements for all Romance language pairs and many of the Slavic language pairs. 2 2.1 Mor"
D19-1090,P18-1073,0,0.206257,"ˇre moˇre moˇre N ; NOM ; PL N ; DAT; SG N ; NOM ; SG N ; INS ; PL N ; GEN ; PL N ; ESS ; SG N ; DAT; PL Table 1: An example extract from our morphologically complete Polish–Czech dictionary. tions of new and less common forms, not present in the existing resources. In spite of this, most ground truth lexica used for BLI evaluation contain mainly frequent word forms. Many available resources are restricted to the top 200k most frequent words; this applies to the English–Italian dictionary of Dinu et al. (2015), the English–German and English– Finnish dictionaries of Artetxe et al. (2017), and Artetxe et al. (2018a)’s English–Spanish resource. The dictionaries of Irvine and Callison-Burch (2017) contain only the top most frequent 10k words for each language. Zhang et al. (2017) extracted their Spanish–English and Italian–English lexica from Open Multilingual WordNet (Bond and Paik, 2012), a resource which only yields high frequency, lemma level mappings. Another example is the recent MUSE dataset (Conneau et al., 2018), which was generated using an “internal translation tool”, and in which the majority of word pairs consist of forms ranked in the top 10k of the vocabularies of their respective language"
D19-1090,Q17-1010,1,0.332483,"al., 2018) and our morphologically complete dictionary, which contains many rare morphological variants of words. The numbers above the bars correspond to the number of translated source words (a hyphen represents an empty dictionary). 1 The dictionaries are available at https://github. com/pczarnowska/morph_dictionaries. ? Sebastian is now affiliated with DeepMind. humans do. Generalization to rare and novel words is arguably the main point of BLI as a task—most frequent translation pairs are already contained in digital dictionaries. Modern word embeddings encode character-level knowledge (Bojanowski et al., 2017), which should—in principle—enable the models to learn this behaviour; but morphological generalization has never been directly tested. Most existing dictionaries used for BLI evaluation do not account for the full spectrum of linguistic properties of language. Specifically, as we demonstrate in §2, they omit most morphological inflections of even common lexemes. To enable a more thorough evaluation we introduce a new resource: 40 morphologically complete dictionar974 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conferen"
D19-1090,P13-1133,0,0.0277452,"ocus on pairs of genetically-related languages for which we can cleanly map one morphological inflection onto another.3 We selected 5 languages from the Slavic family: Polish, Czech, Russian, Slovak and Ukrainian, and 5 Romance languages: French, Spanish, Italian, Portuguese and Catalan. Table 1 presents an example extract from our resource; every source–target pair is followed by their corresponding lemmata and a shared tag. We generated our dictionaries automatically based on openly available resources: Open Multilingual WordNet (Bond and Paik, 2012) and Extended Open Multilingual WordNet4 (Bond and Foster, 2013), both of which are collections of lexical databases which group words into sets of synonyms (synsets), and UniMorph5 (Kirov et al., 2016)—a resource comprised of inflectional word paradigms for 107 languages, extracted from Wiktionary6 and annotated according to the UniMorph schema (Sylak-Glassman, 2016). For each language pair (L1, L2) we first generated lemma translation pairs by mapping all L1 lemmata to all L2 lemmata for each synset that appeared in both L1 3 One may translate talked, the past tense of talk, into many different Spanish forms, but the Portuguese falavam has, arguably, onl"
D19-1090,L18-1550,1,0.925323,"erent variables: the word form’s frequency, morphology, the lexeme frequency and the lexeme (a total of 480 experiments). Our comprehensive analysis reveals that BLI models can generalize for frequent morphosyntactic categories, even of infrequent lexemes, but fail to generalize for the more rare categories. This yields a more nuanced picture of the known deficiency of word embeddings to underperform on infrequent words (Gong et al., 2018). Our findings also contradict the strong empirical claims made elsewhere in the literature (Artetxe et al., 2017; Conneau et al., 2018; Ruder et al., 2018; Grave et al., 2018b), as we observe that performance severely degrades when the evaluation includes rare morphological variants of a word and infrequent lexemes. We picture this general trend in Figure 1, which also highlights the skew of existing dictionaries towards more frequent words.2 As our final contribution, we demonstrate that better encoding of morphology is indeed beneficial: enforcing a simple morphological constraint yields consistent performance improvements for all Romance language pairs and many of the Slavic language pairs. 2 2.1 Morphological Dictionaries Existing Dictionaries Frequent word fo"
D19-1090,J17-2001,0,0.0525854,"GEN ; PL N ; ESS ; SG N ; DAT; PL Table 1: An example extract from our morphologically complete Polish–Czech dictionary. tions of new and less common forms, not present in the existing resources. In spite of this, most ground truth lexica used for BLI evaluation contain mainly frequent word forms. Many available resources are restricted to the top 200k most frequent words; this applies to the English–Italian dictionary of Dinu et al. (2015), the English–German and English– Finnish dictionaries of Artetxe et al. (2017), and Artetxe et al. (2018a)’s English–Spanish resource. The dictionaries of Irvine and Callison-Burch (2017) contain only the top most frequent 10k words for each language. Zhang et al. (2017) extracted their Spanish–English and Italian–English lexica from Open Multilingual WordNet (Bond and Paik, 2012), a resource which only yields high frequency, lemma level mappings. Another example is the recent MUSE dataset (Conneau et al., 2018), which was generated using an “internal translation tool”, and in which the majority of word pairs consist of forms ranked in the top 10k of the vocabularies of their respective languages. Another problem associated with existing resources is ‘semantic leakage’ between"
D19-1090,L16-1498,0,0.045505,"Missing"
D19-1090,D18-1042,1,0.74662,"but they will only have seen the less frequent, first-person plural future form hablar´amos a few times. Nevertheless, they would have no problem translating the latter. In this paper we ask whether current methods for bilingual lexicon induction (BLI) generalize morphologically as k Re ma 00 50 0 - ini ng OO Vs - –6 k - 00 00 –5 k - 40 0 k 30 0 k - 00 –3 20 0 00 –2 00 - –4 - 10 0 0k 0k –5 –1 50 10 –1 vo ca +O b OV s In Introduction - k - 0.0 Figure 1: The relation between the BLI performance and the frequency of source words in the test dictionary. The graph presents results for the model of Ruder et al. (2018) evaluated on both the MUSE dictionary (Conneau et al., 2018) and our morphologically complete dictionary, which contains many rare morphological variants of words. The numbers above the bars correspond to the number of translated source words (a hyphen represents an empty dictionary). 1 The dictionaries are available at https://github. com/pczarnowska/morph_dictionaries. ? Sebastian is now affiliated with DeepMind. humans do. Generalization to rare and novel words is arguably the main point of BLI as a task—most frequent translation pairs are already contained in digital dictionaries. Modern"
D19-1090,N13-1011,0,0.0656821,"Missing"
D19-1090,P17-1179,0,0.0476665,"olish–Czech dictionary. tions of new and less common forms, not present in the existing resources. In spite of this, most ground truth lexica used for BLI evaluation contain mainly frequent word forms. Many available resources are restricted to the top 200k most frequent words; this applies to the English–Italian dictionary of Dinu et al. (2015), the English–German and English– Finnish dictionaries of Artetxe et al. (2017), and Artetxe et al. (2018a)’s English–Spanish resource. The dictionaries of Irvine and Callison-Burch (2017) contain only the top most frequent 10k words for each language. Zhang et al. (2017) extracted their Spanish–English and Italian–English lexica from Open Multilingual WordNet (Bond and Paik, 2012), a resource which only yields high frequency, lemma level mappings. Another example is the recent MUSE dataset (Conneau et al., 2018), which was generated using an “internal translation tool”, and in which the majority of word pairs consist of forms ranked in the top 10k of the vocabularies of their respective languages. Another problem associated with existing resources is ‘semantic leakage’ between train and evaluation sets. As we demonstrate in §2.3, it is common for a single lex"
D19-1288,W18-5412,0,0.214596,"d stored it in the form of attribute–value features in publicly accessible databases (Croft, 2002; Dryer and Haspelmath, 2013). The usage of such features to inform neural NLP models is still scarce, partly because the evidence in favor of their effectiveness is mixed (Ponti et al., 2018, 2019). In this work, we propose a way to distantly supervise the model with this side information effectively. We extend our non-conditional language models outlined in §3 (BARE) to a series of variants conditioned on language-specific properties, inspired by Östling and Tiedemann (2017) and Platanios et al. (2018). A fundamental difference from these previous works, however, is that they learn such properties in an end-to-end fashion from the data in a joint multilingual learning setting. Obviously, this is not feasible for the zeroshot setting and unreliable for the few-shot setting. Rather, we represent languages with their typological feature vector, which we assume to be readily available both for both training and held-out languages. Let t` ∈ [0, 1]f be a vector of f typological features for language ` ∈ T t E. We reinterpret the conditional language models within the Bayesian framework by estimat"
D19-1288,N16-1161,0,0.0861779,"shing the problem to its most complex formulation, zero-shot inference, and in taking into account the largest sample of languages for language modeling to date. In addition to those considered in our work, there are also alternative methods to condition language models on features. Kalchbrenner and Blunsom (2013) used encoded features as additional biases in recurrent layers. Kiros et al. (2014) put forth a log-bilinear model that allows for a ‘multiplicative interaction’ between hidden representations and input features (such as images). With a similar device, but a different gating method, Tsvetkov et al. (2016) trained a phoneme-level joint multilingual model of words conditioned on typological features from Moran et al. (2014). The use of the Laplace method for neural transfer learning has been proposed by Kirkpatrick et al. (2017), inspired by synaptic consolidation in neuroscience, with the aim to avoid catastrophic forgetting. Kochurov et al. (2018) tackled the problem of continuous learning by approximating the posterior probabilities through stochastic variational inference. Ritter et al. (2018) substitute diagonal Laplace approximation with a Kronecker factored method, leading to better uncer"
D19-1288,Q19-1040,0,0.125021,"rld’s languages, we hope that these findings will help broaden the scope of applications for language technology. 1 Introduction With the success of recurrent neural networks and other black-box models on core NLP tasks, such as language modeling, researchers have turned their attention to the study of the inductive bias such neural models exhibit (Linzen et al., 2016; Marvin and Linzen, 2018; Ravfogel et al., 2018). A number of natural questions have been asked. For example, do recurrent neural language models learn syntax (Marvin and Linzen, 2018)? Do they map onto grammaticality judgments (Warstadt et al., 2019)? However, as Ravfogel et al. (2019) note, “[m]ost of the work so far has focused on English.” Moreover, these studies have almost always focused on training scenarios where a large number of in-language sentences are available. In this work, we aim to find a prior distribution over network parameters that generalize well to new human languages. The recent vein of research on the inductive biases of neural nets implicitly assumes a uniform (unnormalizable) prior over the space of neural network parameters (Ravfogel et al., 2019, inter alia). In contrast, we take a Bayesian-updating approach: F"
D19-1530,Q14-1029,0,0.125379,"Missing"
D19-1530,W19-3621,1,0.676916,"nd two sets of attribute words A and B. We compute Cohen’s d (a measure of the difference in relative similarity of the word sets within each embedding; higher is more biased), and a one-sided p-value which indicates whether the bias detected by WEAT within each embedding is significant (the best outcome being that no such bias is detectable). We do this for three tests proposed by Nosek et al. (2002) which measure the strength of various gender stereotypes: art–maths, arts–sciences, and careers–family.10 Indirect bias To demonstrate indirect gender bias we adapt a pair of methods proposed by Gonen and Goldberg (2019). First, we test whether the most-biased words prior to bias mitigation remain clustered following bias mitigation. To do this, we define a new subspace, ~btest , using the 23 word pairs used in the Google Analogy family test subset (Mikolov et al., 2013) following Bolukbasi et al.’s (2016) method, and determine 10 In the careers–family test the gender dimension is expressed by female and male first names, unlike in the other sets, where pronouns and typical gendered words are used. the 1000 most biased words in each corpus (the 500 words most similar to ~btest and ~btest ) in the unmitigated"
D19-1530,J15-4004,0,0.0791268,"llowing bias mitigation. Second, we test whether a classifier can be trained to reclassify the gender of debiased words. If it succeeds, this would indicate that biasinformation still remains in the embedding. We trained an RBF-kernel SVM classifier on a random sample of 1000 out of the 5000 most biased words from each corpus using ~btest (500 from each gender), then report the classifier’s accuracy when reclassifying the remaining 4000 words. Word similarity The quality of a space is traditionally measured by how well it replicates human judgements of word similarity. The SimLex-999 dataset (Hill et al., 2015) provides a ground-truth measure of similarity produced by 500 native English speakers.11 Similarity scores in an embedding are computed as the cosine angle between wordvector pairs, and Spearman correlation between embedding and human judgements are reported. We measure correlative significance at ↵ = 0.01. Sentiment classification Following Le and Mikolov (2014), we use a standard sentiment classification task to quantify the downstream performance of the embedding spaces when they are used as a pretrained word embedding input (Lau and Baldwin, 2016) to Doc2Vec on the Stanford Large Movie Re"
D19-1530,W16-1609,0,0.0222116,"s of word similarity. The SimLex-999 dataset (Hill et al., 2015) provides a ground-truth measure of similarity produced by 500 native English speakers.11 Similarity scores in an embedding are computed as the cosine angle between wordvector pairs, and Spearman correlation between embedding and human judgements are reported. We measure correlative significance at ↵ = 0.01. Sentiment classification Following Le and Mikolov (2014), we use a standard sentiment classification task to quantify the downstream performance of the embedding spaces when they are used as a pretrained word embedding input (Lau and Baldwin, 2016) to Doc2Vec on the Stanford Large Movie Review dataset. The classification is performed by an SVM classifier using the document embeddings as features, trained on 40,000 labelled reviews and tested on the remaining 10,000 documents, reported as error percentage. Non-biased gender analogies When proposing WED, Bolukbasi et al. (2016) use human raters to class gender-analogies as either biased (woman:housewife :: man:shopkeeper) or appropriate (woman:grandmother :: man::grandfather), and postulate that whilst biased analogies are undesirable, appropriate ones should remain. Our new analogy test"
D19-1530,W12-3018,0,0.0215335,"Missing"
D19-1530,D07-1043,0,0.060778,"the Google Analogy family test subset (Mikolov et al., 2013) following Bolukbasi et al.’s (2016) method, and determine 10 In the careers–family test the gender dimension is expressed by female and male first names, unlike in the other sets, where pronouns and typical gendered words are used. the 1000 most biased words in each corpus (the 500 words most similar to ~btest and ~btest ) in the unmitigated embedding. For each debiased embedding we then project these words into 2D space with tSNE (van der Maaten and Hinton, 2008), compute clusters with k-means, and calculate the clusters’ Vmeasure (Rosenberg and Hirschberg, 2007). Low values of cluster purity indicate that biased words are less clustered following bias mitigation. Second, we test whether a classifier can be trained to reclassify the gender of debiased words. If it succeeds, this would indicate that biasinformation still remains in the embedding. We trained an RBF-kernel SVM classifier on a random sample of 1000 out of the 5000 most biased words from each corpus using ~btest (500 from each gender), then report the classifier’s accuracy when reclassifying the remaining 4000 words. Word similarity The quality of a space is traditionally measured by how w"
D19-1530,N18-2002,0,0.208296,"Missing"
D19-1530,N18-2003,0,0.175913,"Missing"
D19-1531,W19-3805,0,0.0528656,"Missing"
D19-1531,Q17-1010,0,0.0597394,"rections are overlapped to some extent but not identical. This is reasonable because words such as “mujer (woman)”, “doctor (female doctor)” are both semantically and grammatically marked as feminine. To better distinguish between these two directions, we project out the grammatical gender component in the computed gender direction to make the semantic gender direction d~s orthogonal to the grammatical gender direction: 5277 D E d~s = d~P CA − d~P CA , d~g d~g , Visualizing and Analyzing Bias in Spanish We take Spanish as an example and analyze gender bias in Spanish fastText word embeddings (Bojanowski et al., 2017) pre-trained on Spanish Wikipedia. For simplicity, we assume that the embeddings contain all gender forms of the words. To show bias, we randomly select several pairs of gender-definition and occupation words as well as inanimate nouns in Spanish and visualize them on the two gender directions defined above. Figure 1 shows that the inanimate nouns lie near the origin point on the semantic gender direction, while the masculine and feminine forms of the occupation words are on the opposite sides for both directions. However, while projections of occupation words on the grammatical gender directi"
D19-1531,N19-3002,0,0.0797182,"Missing"
D19-1531,S17-2002,0,0.0316142,"iginal Spanish and French embeddings exhibit strong bias and Hybrid Ori significantly reduces the bias in the embedding to to an insignificant level (p-value &gt; 0.05). male and female attribute words for our WEAT experiments and translate them to Spanish and French. For the word pair translation task, we use 7 common adjectives and pair every adjective with each occupation, resulting in 406 pairs for Spanish and 161 for French. 4.2 architect* arquitecta lawyer* abogada enfermera 0.2 Word Similarity We test the quality of the embeddings after mitigation on the SemEval 2017 word similarity task (Camacho-Collados et al., 2017) for monolingual embeddings. This task evaluates how well the cosine similarity between two words correlates with a human-labeled score for which we report the Pearson correlation score. Results Table 1 shows that Hybrid Ori significantly decreases the difference of association between two genders as indicated by MWEAT–Diff and p-value. Other methods only show marginal doctor nurse* 0.1 enfermero 0.0 0.1 Semantic Gender Direction 0.2 0.3 Figure 3: Projections of occupations words on the semantic gender direction after hybrid mitigation with origin as the anchor point. The two forms of occupati"
D19-1531,P19-1166,0,0.0757875,"natural language processing tools. By virtue of their being trained on large, human-written corpora, recent studies have shown that word embeddings, in addition to capturing a word’s semantics, also encode gender bias in society (Bolukbasi et al., 2016; Caliskan et al., 2017). As a result of this bias, the embeddings may cause undesired consequences in the resulting models (Zhao et al., 2018a; Font and Costa-juss`a, 2019). Therefore, extensive effort has been put toward analyzing and mitigating gender bias in word embeddings (Bolukbasi et al., 2016; Zhao et al., 2018b; Dev and Phillips, 2019; Ethayarajh et al., 2019). Existing studies on gender bias almost exclusively focus on English (EN) word embeddings. Unfortunately, the techniques used to mitigate bias in English word embeddings cannot be directly applied to languages with grammatical gender1 , where all nouns are assigned a gender class and the corresponding dependent articles, adjectives, and verbs must agree in gender with the noun (e.g. in Spanish: la buena enfermera the good female nurse, el buen enfermero the good male nurse) (Corbett, 1991, 2006). This is because most existing approaches define bias in word embeddings based on the projection o"
D19-1531,W19-3821,0,0.0439998,"Missing"
D19-1531,N18-2002,0,0.130865,"Missing"
D19-1531,D18-1521,1,0.825445,"ord translation, and word pair translation tasks show that the proposed approaches effectively reduce the gender bias while preserving the utility of the embeddings. 1 Introduction Word embeddings are widely used in modern natural language processing tools. By virtue of their being trained on large, human-written corpora, recent studies have shown that word embeddings, in addition to capturing a word’s semantics, also encode gender bias in society (Bolukbasi et al., 2016; Caliskan et al., 2017). As a result of this bias, the embeddings may cause undesired consequences in the resulting models (Zhao et al., 2018a; Font and Costa-juss`a, 2019). Therefore, extensive effort has been put toward analyzing and mitigating gender bias in word embeddings (Bolukbasi et al., 2016; Zhao et al., 2018b; Dev and Phillips, 2019; Ethayarajh et al., 2019). Existing studies on gender bias almost exclusively focus on English (EN) word embeddings. Unfortunately, the techniques used to mitigate bias in English word embeddings cannot be directly applied to languages with grammatical gender1 , where all nouns are assigned a gender class and the corresponding dependent articles, adjectives, and verbs must agree in gender wit"
D19-1577,Q17-1010,0,0.0756011,"Missing"
D19-1577,P13-1133,0,0.0573204,"Missing"
D19-1577,L18-1550,0,0.0443806,"Missing"
D19-1577,N03-1006,0,0.0320888,"l approaches to semantics (Harris, 1954; Firth, 1957)— and with the copious amounts of text available on the internet, it is now possible to conduct such an investigation. We focus on languages that have either two (masculine–feminine) or three (masculine– feminine–neuter) genders, to which nouns are exhaustively assigned, and investigate whether a correlation exists between grammatical gender and lexical semantics for inanimate nouns—i.e., whether noun–gender assignments are arbitrary or not. In many languages, a noun’s grammatical gender can be predicted from its spelling and pronunciation (Cucerzan and Yarowsky, 2003; Nastase and Popescu, 2009). For example, almost all Spanish nouns ending in -a are feminine, whereas Spanish nouns ending in -o are usually masculine. These assignments are non-arbitrary; indeed, Corbett (1991, Ch. 4) provides a thorough typological description of how phonology pervades gender systems. We emphasize that these assignments are not the subject of our investigation. Rather, we are concerned with the relationship between grammatical gender and lexical semantics—i.e., when asking why the Spanish word casa is feminine, we do not consider that it ends in -a. Finally, our investigati"
D19-1577,D09-1142,0,0.49816,"rris, 1954; Firth, 1957)— and with the copious amounts of text available on the internet, it is now possible to conduct such an investigation. We focus on languages that have either two (masculine–feminine) or three (masculine– feminine–neuter) genders, to which nouns are exhaustively assigned, and investigate whether a correlation exists between grammatical gender and lexical semantics for inanimate nouns—i.e., whether noun–gender assignments are arbitrary or not. In many languages, a noun’s grammatical gender can be predicted from its spelling and pronunciation (Cucerzan and Yarowsky, 2003; Nastase and Popescu, 2009). For example, almost all Spanish nouns ending in -a are feminine, whereas Spanish nouns ending in -o are usually masculine. These assignments are non-arbitrary; indeed, Corbett (1991, Ch. 4) provides a thorough typological description of how phonology pervades gender systems. We emphasize that these assignments are not the subject of our investigation. Rather, we are concerned with the relationship between grammatical gender and lexical semantics—i.e., when asking why the Spanish word casa is feminine, we do not consider that it ends in -a. Finally, our investigation is related to that of Kan"
D19-1577,Q17-1033,0,0.0148635,"sing Common Crawl and Wikipedia data, using CBOW with position weights, with character n-grams of length 5. For more information, see http://fasttext.cc/docs/en/crawl-vectors.html. Canonical Correlation Analysis 3 To establish statistical significance, we follow the approach of Monteiro et al. (2016). We create B = 100, 000 permutations of the columns of 5736 Gtrain ` ; for each permutation b, we then repeat the steps above to obtain ρ`,m ; finally, we compute p= 1+ PB b b=1 δ(ρ`,m B+1 ≥ ρ`,m ) . Because our investigation involves testing 90 different hypotheses, we use Bonferroni correction (Dror et al., 2017)—i.e., we multiply p by 90. If the resulting Bonferroni-corrected p-value is small, then we can reject the null hypothesis that there is no correlation between grammatical gender and lexical semantics for that language pair. Secondarily, we investigate semantic similarities between the 18 languages’ gender systems by analyzing their projections of lexical semantics. For each pair of gendered languages ` and `0 , we compute the correlation (cosine distance) between b?` and b?`0 for each of the 5 genderless languages. 4 Results We find a significant correlation between grammatical gender and lex"
D19-1577,simov-osenova-2010-constructing,0,0.03913,"Missing"
D19-1577,stamou-etal-2004-exploring,0,0.127049,"Missing"
E17-1049,W16-2007,0,0.235502,"Missing"
E17-1049,N15-1107,0,0.0463434,"gically rich languages still constitute a challenge for natural language processing (NLP). The increased data sparsity caused by highly inflected word forms in certain languages causes otherwise state-of-the-art systems to perform worse in standard tasks, e.g., parsing (Ballesteros et al., 2015) and machine translation (Bojar et al., 2016). To create systems whose performance is not deterred by complex morphology, the development of NLP tools for the generation and analysis of morphological forms is crucial. Indeed, these considerations have motivated a great deal of recent work on the topic (Ahlberg et al., 2015; Dreyer, 2011; Nicolai et al., 2015). In the area of generation, the most natural task is morphological inflection—finding an inflected form for a given target tag and lemma. An example for English is as follows: (trg:3rdSgPres, bring) 7→ brings. In this case, the 3rd person singular present tense of bring is generated. One generalization of inflection is morphological reinflection (MRI) (Cotterell et al., 2016a), where we must produce an inflected form from a triple of target tag, source form and source tag. The inflection task is the special case where the source form is the lemma. As an ex"
E17-1049,D15-1041,0,0.0289893,"sense, the verbal paradigm is partitioned into subparadigms. To see why multi-source models could help in this case, starting only from the infinitive treffen makes it difficult to predict subjunctive form tr¨afest, but the additional information of the fellow subjunctive form tr¨afe makes the task easier. Introduction Morphologically rich languages still constitute a challenge for natural language processing (NLP). The increased data sparsity caused by highly inflected word forms in certain languages causes otherwise state-of-the-art systems to perform worse in standard tasks, e.g., parsing (Ballesteros et al., 2015) and machine translation (Bojar et al., 2016). To create systems whose performance is not deterred by complex morphology, the development of NLP tools for the generation and analysis of morphological forms is crucial. Indeed, these considerations have motivated a great deal of recent work on the topic (Ahlberg et al., 2015; Dreyer, 2011; Nicolai et al., 2015). In the area of generation, the most natural task is morphological inflection—finding an inflected form for a given target tag and lemma. An example for English is as follows: (trg:3rdSgPres, bring) 7→ brings. In this case, the 3rd person"
E17-1049,W14-4012,0,0.178924,"Missing"
E17-1049,K15-1017,1,0.918613,"Missing"
E17-1049,Q15-1031,1,0.856892,"transducer for the task, which was later hybridized with an LSTM (Rastogi et al., 2016). Durrett and DeNero (2013) apply a semi-CRF to heuristically extracted rules to generate inflected forms from lemmata using data scraped from Wiktionary. Improved systems for the Wiktionary data were subsequently developed by Hulden et al. (2014), who used a semi-supervised approach, and Faruqui et al. (2016), who used a character-level LSTM. All of the above work has focused on the single input case. Two important exceptions, however, have considered the multi-input case. Both Dreyer and Eisner (2009) and Cotterell et al. (2015b) define a string-valued graphical model over the paradigm and apply the missing values. performing model made use of an attention mechanism (Kann and Sch¨utze, 2016a), first popularized in machine translation (Bahdanau et al., 2015). We generalize this architecture to the multi-source case in this paper for the reinflection task. Besides generation, computational work on morphology has also focused on analysis. In this area, a common task—morphological segmentation—is to break up a word into its sequence of constituent morphs. The unsupervised M ORFESSOR model (Creutz and Lagus, 2002) has ac"
E17-1049,N16-1080,1,0.875264,"Missing"
E17-1049,W02-0603,0,0.06034,"09) and Cotterell et al. (2015b) define a string-valued graphical model over the paradigm and apply the missing values. performing model made use of an attention mechanism (Kann and Sch¨utze, 2016a), first popularized in machine translation (Bahdanau et al., 2015). We generalize this architecture to the multi-source case in this paper for the reinflection task. Besides generation, computational work on morphology has also focused on analysis. In this area, a common task—morphological segmentation—is to break up a word into its sequence of constituent morphs. The unsupervised M ORFESSOR model (Creutz and Lagus, 2002) has achieved widespread adoption. Bayesian methods have also proven themselves successful in unsupervised morphological segmentation (Johnson et al., 2006; Goldwater et al., 2009). When labeled training data for segmentation is available, supervised methods significantly outperform the unsupervised techniques (Ruokolainen et al., 2013; Cotterell et al., 2015a; Cotterell et al., 2016b). As we pointed out in Section 2, morphologically annotated corpora provide an ideal source of data for the multi-source MRI task: they are annotated on the token level with inflectional features and often contai"
E17-1049,D09-1011,0,0.0711054,"orming weighted finite-state transducer for the task, which was later hybridized with an LSTM (Rastogi et al., 2016). Durrett and DeNero (2013) apply a semi-CRF to heuristically extracted rules to generate inflected forms from lemmata using data scraped from Wiktionary. Improved systems for the Wiktionary data were subsequently developed by Hulden et al. (2014), who used a semi-supervised approach, and Faruqui et al. (2016), who used a character-level LSTM. All of the above work has focused on the single input case. Two important exceptions, however, have considered the multi-input case. Both Dreyer and Eisner (2009) and Cotterell et al. (2015b) define a string-valued graphical model over the paradigm and apply the missing values. performing model made use of an attention mechanism (Kann and Sch¨utze, 2016a), first popularized in machine translation (Bahdanau et al., 2015). We generalize this architecture to the multi-source case in this paper for the reinflection task. Besides generation, computational work on morphology has also focused on analysis. In this area, a common task—morphological segmentation—is to break up a word into its sequence of constituent morphs. The unsupervised M ORFESSOR model (Cre"
E17-1049,D08-1113,0,0.275655,"Missing"
E17-1049,N13-1138,0,0.107651,"y transformations to the hidden states of the encoders that are input to the decoder. Firat et al. (2016)’s neural architecture for MT translates from any of N source languages to any of M target languages, using language specific encoders and decoders, but sharing one single attention-mechanism. In contrast to our work, they obtain a single output for each input. Much ink has been spilled on morphological reinflection over recent years. Dreyer et al. (2008) develop a high-performing weighted finite-state transducer for the task, which was later hybridized with an LSTM (Rastogi et al., 2016). Durrett and DeNero (2013) apply a semi-CRF to heuristically extracted rules to generate inflected forms from lemmata using data scraped from Wiktionary. Improved systems for the Wiktionary data were subsequently developed by Hulden et al. (2014), who used a semi-supervised approach, and Faruqui et al. (2016), who used a character-level LSTM. All of the above work has focused on the single input case. Two important exceptions, however, have considered the multi-input case. Both Dreyer and Eisner (2009) and Cotterell et al. (2015b) define a string-valued graphical model over the paradigm and apply the missing values. pe"
E17-1049,D13-1105,0,0.022021,"ods have also proven themselves successful in unsupervised morphological segmentation (Johnson et al., 2006; Goldwater et al., 2009). When labeled training data for segmentation is available, supervised methods significantly outperform the unsupervised techniques (Ruokolainen et al., 2013; Cotterell et al., 2015a; Cotterell et al., 2016b). As we pointed out in Section 2, morphologically annotated corpora provide an ideal source of data for the multi-source MRI task: they are annotated on the token level with inflectional features and often contain several different inflected forms of a lemma. Eskander et al. (2013) develop an algorithm for automatic learning of inflectional classes and associated lemmas from morphologically annotated corpora, an approach that could be usefully combined with our multi-source MRI framework. The SIGMORPHON 2016 Shared Task on Morphological Reinflection (Cotterell et al., 2016a), based on the U NI M ORPH (Sylak-Glassman et al., 2015) data, resulted in the development of numerous methods. RNN encoder-decoder models (Aha¨ roni et al., 2016; Kann and Sch¨utze, 2016a; Ostling, 2016) obtained the strongest performance and are the current state of the art on the task. The best521"
E17-1049,N16-1077,0,0.179268,"echanism. In contrast to our work, they obtain a single output for each input. Much ink has been spilled on morphological reinflection over recent years. Dreyer et al. (2008) develop a high-performing weighted finite-state transducer for the task, which was later hybridized with an LSTM (Rastogi et al., 2016). Durrett and DeNero (2013) apply a semi-CRF to heuristically extracted rules to generate inflected forms from lemmata using data scraped from Wiktionary. Improved systems for the Wiktionary data were subsequently developed by Hulden et al. (2014), who used a semi-supervised approach, and Faruqui et al. (2016), who used a character-level LSTM. All of the above work has focused on the single input case. Two important exceptions, however, have considered the multi-input case. Both Dreyer and Eisner (2009) and Cotterell et al. (2015b) define a string-valued graphical model over the paradigm and apply the missing values. performing model made use of an attention mechanism (Kann and Sch¨utze, 2016a), first popularized in machine translation (Bahdanau et al., 2015). We generalize this architecture to the multi-source case in this paper for the reinflection task. Besides generation, computational work on"
E17-1049,N16-1101,0,0.0232653,"nd P that encode that the target form is 3rd person plural. We omit the tags from the diagram to which the model hardly attends. al., 2015), parsing (Vinyals et al., 2014) and automatic speech recognition (Graves and Schmidhuber, 2005; Graves et al., 2013). The first work on multi-source models was presented for machine translation. Zoph and Knight (2016) made simultaneous use of source sentences in multiple languages in order to find the best match possible in the target language. Unlike our model, they apply transformations to the hidden states of the encoders that are input to the decoder. Firat et al. (2016)’s neural architecture for MT translates from any of N source languages to any of M target languages, using language specific encoders and decoders, but sharing one single attention-mechanism. In contrast to our work, they obtain a single output for each input. Much ink has been spilled on morphological reinflection over recent years. Dreyer et al. (2008) develop a high-performing weighted finite-state transducer for the task, which was later hybridized with an LSTM (Rastogi et al., 2016). Durrett and DeNero (2013) apply a semi-CRF to heuristically extracted rules to generate inflected forms f"
E17-1049,E14-1060,0,0.17837,"cific encoders and decoders, but sharing one single attention-mechanism. In contrast to our work, they obtain a single output for each input. Much ink has been spilled on morphological reinflection over recent years. Dreyer et al. (2008) develop a high-performing weighted finite-state transducer for the task, which was later hybridized with an LSTM (Rastogi et al., 2016). Durrett and DeNero (2013) apply a semi-CRF to heuristically extracted rules to generate inflected forms from lemmata using data scraped from Wiktionary. Improved systems for the Wiktionary data were subsequently developed by Hulden et al. (2014), who used a semi-supervised approach, and Faruqui et al. (2016), who used a character-level LSTM. All of the above work has focused on the single input case. Two important exceptions, however, have considered the multi-input case. Both Dreyer and Eisner (2009) and Cotterell et al. (2015b) define a string-valued graphical model over the paradigm and apply the missing values. performing model made use of an attention mechanism (Kann and Sch¨utze, 2016a), first popularized in machine translation (Bahdanau et al., 2015). We generalize this architecture to the multi-source case in this paper for t"
E17-1049,W16-2010,1,0.877615,"Missing"
E17-1049,P16-2090,1,0.891599,"Missing"
E17-1049,P13-2017,0,0.0266029,"Missing"
E17-1049,D15-1272,1,0.856915,"Missing"
E17-1049,N15-1093,0,0.071106,"ute a challenge for natural language processing (NLP). The increased data sparsity caused by highly inflected word forms in certain languages causes otherwise state-of-the-art systems to perform worse in standard tasks, e.g., parsing (Ballesteros et al., 2015) and machine translation (Bojar et al., 2016). To create systems whose performance is not deterred by complex morphology, the development of NLP tools for the generation and analysis of morphological forms is crucial. Indeed, these considerations have motivated a great deal of recent work on the topic (Ahlberg et al., 2015; Dreyer, 2011; Nicolai et al., 2015). In the area of generation, the most natural task is morphological inflection—finding an inflected form for a given target tag and lemma. An example for English is as follows: (trg:3rdSgPres, bring) 7→ brings. In this case, the 3rd person singular present tense of bring is generated. One generalization of inflection is morphological reinflection (MRI) (Cotterell et al., 2016a), where we must produce an inflected form from a triple of target tag, source form and source tag. The inflection task is the special case where the source form is the lemma. As an example, we may again consider generati"
E17-1049,W16-2003,0,0.0239677,"th inflectional features and often contain several different inflected forms of a lemma. Eskander et al. (2013) develop an algorithm for automatic learning of inflectional classes and associated lemmas from morphologically annotated corpora, an approach that could be usefully combined with our multi-source MRI framework. The SIGMORPHON 2016 Shared Task on Morphological Reinflection (Cotterell et al., 2016a), based on the U NI M ORPH (Sylak-Glassman et al., 2015) data, resulted in the development of numerous methods. RNN encoder-decoder models (Aha¨ roni et al., 2016; Kann and Sch¨utze, 2016a; Ostling, 2016) obtained the strongest performance and are the current state of the art on the task. The best521 6 Conclusion Generation of unknown inflections in morphologically rich languages is an important task that remains unsolved. We provide a new angle on the problem by considering systems that are allowed to have multiple inflected forms as input. To this end, we define the task of multi-source morphological reinflection as a generalization of singlesource MRI (Cotterell et al., 2016a) and present a model that solves the task. We extend an attentionbased RNN encoder-decoder architecture from the sin"
E17-1049,N16-1076,1,0.804979,"ke our model, they apply transformations to the hidden states of the encoders that are input to the decoder. Firat et al. (2016)’s neural architecture for MT translates from any of N source languages to any of M target languages, using language specific encoders and decoders, but sharing one single attention-mechanism. In contrast to our work, they obtain a single output for each input. Much ink has been spilled on morphological reinflection over recent years. Dreyer et al. (2008) develop a high-performing weighted finite-state transducer for the task, which was later hybridized with an LSTM (Rastogi et al., 2016). Durrett and DeNero (2013) apply a semi-CRF to heuristically extracted rules to generate inflected forms from lemmata using data scraped from Wiktionary. Improved systems for the Wiktionary data were subsequently developed by Hulden et al. (2014), who used a semi-supervised approach, and Faruqui et al. (2016), who used a character-level LSTM. All of the above work has focused on the single input case. Two important exceptions, however, have considered the multi-input case. Both Dreyer and Eisner (2009) and Cotterell et al. (2015b) define a string-valued graphical model over the paradigm and a"
E17-1049,W13-3504,0,0.0535729,"for the reinflection task. Besides generation, computational work on morphology has also focused on analysis. In this area, a common task—morphological segmentation—is to break up a word into its sequence of constituent morphs. The unsupervised M ORFESSOR model (Creutz and Lagus, 2002) has achieved widespread adoption. Bayesian methods have also proven themselves successful in unsupervised morphological segmentation (Johnson et al., 2006; Goldwater et al., 2009). When labeled training data for segmentation is available, supervised methods significantly outperform the unsupervised techniques (Ruokolainen et al., 2013; Cotterell et al., 2015a; Cotterell et al., 2016b). As we pointed out in Section 2, morphologically annotated corpora provide an ideal source of data for the multi-source MRI task: they are annotated on the token level with inflectional features and often contain several different inflected forms of a lemma. Eskander et al. (2013) develop an algorithm for automatic learning of inflectional classes and associated lemmas from morphologically annotated corpora, an approach that could be usefully combined with our multi-source MRI framework. The SIGMORPHON 2016 Shared Task on Morphological Reinfl"
E17-1049,P15-2111,0,0.0314828,"ointed out in Section 2, morphologically annotated corpora provide an ideal source of data for the multi-source MRI task: they are annotated on the token level with inflectional features and often contain several different inflected forms of a lemma. Eskander et al. (2013) develop an algorithm for automatic learning of inflectional classes and associated lemmas from morphologically annotated corpora, an approach that could be usefully combined with our multi-source MRI framework. The SIGMORPHON 2016 Shared Task on Morphological Reinflection (Cotterell et al., 2016a), based on the U NI M ORPH (Sylak-Glassman et al., 2015) data, resulted in the development of numerous methods. RNN encoder-decoder models (Aha¨ roni et al., 2016; Kann and Sch¨utze, 2016a; Ostling, 2016) obtained the strongest performance and are the current state of the art on the task. The best521 6 Conclusion Generation of unknown inflections in morphologically rich languages is an important task that remains unsolved. We provide a new angle on the problem by considering systems that are allowed to have multiple inflected forms as input. To this end, we define the task of multi-source morphological reinflection as a generalization of singlesour"
E17-1049,N16-1004,0,0.0178675,"; Bahdanau et Figure 4: Attention heatmap for the multi-source model. The example is for the German verb wiegen ‘to weigh’. The model learns to focus most of its attention on forms that share the irregular subjunctive stem w¨og in addition to the target subtags 3 and P that encode that the target form is 3rd person plural. We omit the tags from the diagram to which the model hardly attends. al., 2015), parsing (Vinyals et al., 2014) and automatic speech recognition (Graves and Schmidhuber, 2005; Graves et al., 2013). The first work on multi-source models was presented for machine translation. Zoph and Knight (2016) made simultaneous use of source sentences in multiple languages in order to find the best match possible in the target language. Unlike our model, they apply transformations to the hidden states of the encoders that are input to the decoder. Firat et al. (2016)’s neural architecture for MT translates from any of N source languages to any of M target languages, using language specific encoders and decoders, but sharing one single attention-mechanism. In contrast to our work, they obtain a single output for each input. Much ink has been spilled on morphological reinflection over recent years. D"
E17-2018,J93-2004,0,0.0579074,"st construct one. Past work (focused on translating out of English into MRLs) assigned a handful of morphological annotations using manually-developed heuristics (Dr´abek and Yarowsky, 2005; Avramidis and Koehn, 2008), but this is hard to scale. We therefore instead look to obtain rich morphological tags by projecting them (Yarowsky et al., 2001) from a language (such as Czech) where such rich tags have already been annotated. We use the Prague Czech–English Dependency Treebank (PCEDT) (Hajiˇc et al., 2012), a complete translation of the Wall Street Journal portion of the Penn Treebank (PTB) (Marcus et al., 1993). Each word on the Czech side of the PCEDT was originally hand-annotated with complex 15-dimensional morphological tags containing positional subtag values for morphological categories specific to Czech.1 We manually mapped these tags to the UniMorph Schema tagset (SylakGlassman et al., 2015), which provides a universal, typologically-informed annotation framework for representing morphological features of inflected words in the world’s languages. UniMorph tags are in principle up to 23-dimensional, but tags are not positionally dependent, and not every dimension needs to be specified. Table 1"
E17-2018,D15-1272,1,0.906538,"Missing"
E17-2018,P08-1087,0,0.038713,"en the feature projected from a UniMorph ‘translation’ of the original PCEDT annotation of Czech matches the feature that would be expected subtag. Note that the core part-of-speech must agree as a precondition for further evaluation. Training a system to tag English text with multidimensional morphological tags requires a corpus of English text annotated with those tags. Since no such corpora exist, we must construct one. Past work (focused on translating out of English into MRLs) assigned a handful of morphological annotations using manually-developed heuristics (Dr´abek and Yarowsky, 2005; Avramidis and Koehn, 2008), but this is hard to scale. We therefore instead look to obtain rich morphological tags by projecting them (Yarowsky et al., 2001) from a language (such as Czech) where such rich tags have already been annotated. We use the Prague Czech–English Dependency Treebank (PCEDT) (Hajiˇc et al., 2012), a complete translation of the Wall Street Journal portion of the Penn Treebank (PTB) (Marcus et al., 1993). Each word on the Czech side of the PCEDT was originally hand-annotated with complex 15-dimensional morphological tags containing positional subtag values for morphological categories specific to"
E17-2018,J03-1002,0,0.00945149,"sal, typologically-informed annotation framework for representing morphological features of inflected words in the world’s languages. UniMorph tags are in principle up to 23-dimensional, but tags are not positionally dependent, and not every dimension needs to be specified. Table 1 shows the subset of UniMorph subtags used here. PTB tags have no formal internal subtag structure. See Figure 1 for a comparison of the PCEDT, UniMorph, and PTB tag systems for a Czech word and its aligned English translation. The PCEDT also contains automatically generated word alignments produced by using GIZA++ (Och and Ney, 2003) to align the Czech and English sides of the treebank. We use these alignments to project morphological tags from the Czech words to their English counterparts through the following process. For every English word, if the word is aligned to a single Czech word, take its tag. If the word is mapped to multiple Czech words, take the annotation from the alignment point belonging to the intersection of the two underlying GIZA++ models used to produce the many-many alignment.2 If no such alignment point is found, take the leftmost aligned word. Unaligned English words get no annotation. 3 Validating"
E17-2018,D14-1162,0,0.0865909,"t or case, under the Universal Dependency features.4 Finally, we use features from CFG parsing: • POS features. A word’s part-of-speech (POS) tag, its parent’s, and its grandparent’s. • Chain features. We compute chains of the tree nodes, starting with its POS tag and moving upward (NN NP S). • The distance to the root. Non-lexical features are treated as real-valued when appropriate (such as in the case of the distance to the root), while all others are treated as binary. For lexical features, we use pretrained GLoVe embeddings, specifically 200-dimensional 400K-vocab uncased embeddings from Pennington et al. (2014). This is an approach similar to Tran et al. (2015), but we additionally augment the pretrained embeddings with randomly initialized embeddings for vocabulary items outside of the 400K lexicon. Neural Morphological Tag Prediction Features With our projections validated, we turn to the prediction model itself. Based on the idea that languages with rich morphology use that morphology to convey similar distinctions in meaning to 4.2 Neural Model In order to take advantage of correlated information between subtags, we present a neural model 3 English also uses morphology to mark the 3rd person sin"
E17-2018,P16-1184,0,0.0647408,"ture and vice versa. We explore the veracity of this claim computationally by asking the following: Can we develop a tagger for English that uses the signal available in English-only syntactic structure to recover the rich semantic distinctions conveyed by morphology in Czech? Can we, for example, accurately detect which English contexts would have a Czech translation that employs the accusative case marker? Traditionally, morphological analysis and tagging is a task that has been limited to morphologically rich languages (MRLs) (Hajiˇc, 2000; Dr´abek and Yarowsky, 2005; M¨uller et al., 2015; Buys and Botha, 2016). In order to build a rich morphological tagger for a morphologically poor language (MPL) like English, we need some way to build a gold standard set of richly tagged English data for training and testing. Our approach is to project the complex morphological tags of Czech words directly onto the English words they align to in a large parallel corpus. After evaluating the validity of these projections, we develop a neural network tagging architecture that takes as input a number of English features derived from off-theshelf dependency parsing and attempts to recover the projected Czech tags. A"
E17-2018,W16-2209,0,0.0413943,"Missing"
E17-2018,W05-0807,0,0.752671,"Missing"
E17-2018,P15-2111,1,0.845556,"Missing"
E17-2018,2015.mtsummit-papers.12,0,0.0185026,"features from CFG parsing: • POS features. A word’s part-of-speech (POS) tag, its parent’s, and its grandparent’s. • Chain features. We compute chains of the tree nodes, starting with its POS tag and moving upward (NN NP S). • The distance to the root. Non-lexical features are treated as real-valued when appropriate (such as in the case of the distance to the root), while all others are treated as binary. For lexical features, we use pretrained GLoVe embeddings, specifically 200-dimensional 400K-vocab uncased embeddings from Pennington et al. (2014). This is an approach similar to Tran et al. (2015), but we additionally augment the pretrained embeddings with randomly initialized embeddings for vocabulary items outside of the 400K lexicon. Neural Morphological Tag Prediction Features With our projections validated, we turn to the prediction model itself. Based on the idea that languages with rich morphology use that morphology to convey similar distinctions in meaning to 4.2 Neural Model In order to take advantage of correlated information between subtags, we present a neural model 3 English also uses morphology to mark the 3rd person singular verb form. 4 114 universaldependencies.org Ot"
E17-2018,hajic-etal-2012-announcing,0,0.0623432,"Missing"
E17-2018,H01-1035,0,0.328922,"cted subtag. Note that the core part-of-speech must agree as a precondition for further evaluation. Training a system to tag English text with multidimensional morphological tags requires a corpus of English text annotated with those tags. Since no such corpora exist, we must construct one. Past work (focused on translating out of English into MRLs) assigned a handful of morphological annotations using manually-developed heuristics (Dr´abek and Yarowsky, 2005; Avramidis and Koehn, 2008), but this is hard to scale. We therefore instead look to obtain rich morphological tags by projecting them (Yarowsky et al., 2001) from a language (such as Czech) where such rich tags have already been annotated. We use the Prague Czech–English Dependency Treebank (PCEDT) (Hajiˇc et al., 2012), a complete translation of the Wall Street Journal portion of the Penn Treebank (PTB) (Marcus et al., 1993). Each word on the Czech side of the PCEDT was originally hand-annotated with complex 15-dimensional morphological tags containing positional subtag values for morphological categories specific to Czech.1 We manually mapped these tags to the UniMorph Schema tagset (SylakGlassman et al., 2015), which provides a universal, typol"
E17-2018,A00-2013,0,0.732801,"Missing"
E17-2019,K15-1017,1,0.899754,"Missing"
E17-2019,N16-1080,1,0.899654,"Missing"
E17-2019,P15-1033,0,0.0145967,"), as well as other nominalisations (e.g., explain 7→ explanation). Nominallemma <s> environmental { d e v a s t a t e } contextR contextL guages such as English where grapheme-phoneme correspondences are opaque. For this reason we consider orthographic rather than phonological representations. In our approach, we test how well models incorporating distributional semantics can capture derivational transformations. Deep learning models capable of learning real-valued word embeddings have been shown to perform well on a range of tasks, from language modelling (Mikolov et al., 2013a) to parsing (Dyer et al., 2015) and machine translation (Bahdanau et al., 2015). Recently, these models have also been successfully applied to morphological reinflection tasks (Kann and Sch¨utze, 2016; Cotterell et al., 2016a). is best exemplified by ... </s> output generation ... encoding d e v a s t a t e } } } d e v a s t a t i o n } Figure 1: The encoder–decoder model, showing the stem devastate in context producing the form devastation. Coloured arrows indicate shared parameters isations have varyingly different meanings from their base verbs, and a key focus of this study is the prediction of which form is most approp"
E17-2019,N16-2002,0,0.0171762,"d a key focus of this study is the prediction of which form is most appropriate depending on the context, in terms of syntactic and semantic concordance. Our model is highly flexible and easily applicable to other related lexical problems. 3 Related Work Although in the last few years many neural morphological models have been proposed, most of them have focused on inflectional morphology (e.g., see Cotterell et al. (2016a)). Focusing on derivational processes, there are three main directions of research. The first deals with the evaluation of word embeddings either using a word analogy task (Gladkova et al., 2016) or binary relation type classification (Vylomova et al., 2016). In this context, it has been shown that, unlike inflectional morphology, most derivational relations cannot be as easily captured using distributional methods. Researchers working on the second type of task attempt to predict derived forms using the embedding of its corresponding base form and a vector encoding a “derivational” shift. Guevara (2011) notes that derivational affixes can be modelled as a geometrical function over the vectors of the base forms. On the other hand, Lazaridou et al. (2013) and Cotterell and Sch¨utze (20"
E17-2019,W11-0115,0,0.0312045,"6a)). Focusing on derivational processes, there are three main directions of research. The first deals with the evaluation of word embeddings either using a word analogy task (Gladkova et al., 2016) or binary relation type classification (Vylomova et al., 2016). In this context, it has been shown that, unlike inflectional morphology, most derivational relations cannot be as easily captured using distributional methods. Researchers working on the second type of task attempt to predict derived forms using the embedding of its corresponding base form and a vector encoding a “derivational” shift. Guevara (2011) notes that derivational affixes can be modelled as a geometrical function over the vectors of the base forms. On the other hand, Lazaridou et al. (2013) and Cotterell and Sch¨utze (2017) represent derivational affixes as vectors and investigate various functions to combine them with base forms. Kisselew et al. 119 (2015) and Pad´o et al. (2016) extend this line of research to model derivational morphology in German. This work demonstrates that various factors such as part of speech, semantic regularity and argument structure (Grimshaw, 1990) influence the predictability of a derived word. The"
E17-2019,N16-1149,1,0.837461,"m, to bias the model to generate a derived form that is morphologically-related to the base 120 baseline biLSTM+BS biLSTM+CTX biLSTM+CTX+BS biLSTM+CTX+BS+POS LSTM+CTX+BS+POS Shared Split 0.63 0.58 0.80 0.83 0.89 0.90 — 0.36 0.45 0.52 0.63 0.66 Table 1: Accuracy for predicted lemmas (bases and derivations) on shared and split lexicons verb. In most cases, the derived form is longer than its stem, and accordingly, when we reach the end of the base form, we continue to input an end-of-word symbol. We provide the model with the context vector o at each decoding step. It has been previously shown (Hoang et al., 2016) that this yields better results than other means of incorporation.4 Finally, we use max pooling to enable the model to switch between copying of a stem or producing a new character. 5.3 Settings We used a 3-layer bidirectional LSTM network, with hidden dimensionality h for both context and base-form stem states of 100, and character embedding cj of 100.5 We used pre-trained 300-dimensional Google News word embeddings (Mikolov et al., 2013a; Mikolov et al., 2013b). During the training of the model, we keep the word embeddings fixed, for greater applicability to unseen test instances. All token"
E17-2019,P16-2090,0,0.0804887,"Missing"
E17-2019,W15-0108,0,0.11611,"Missing"
E17-2019,P13-1149,0,0.0775424,"ther using a word analogy task (Gladkova et al., 2016) or binary relation type classification (Vylomova et al., 2016). In this context, it has been shown that, unlike inflectional morphology, most derivational relations cannot be as easily captured using distributional methods. Researchers working on the second type of task attempt to predict derived forms using the embedding of its corresponding base form and a vector encoding a “derivational” shift. Guevara (2011) notes that derivational affixes can be modelled as a geometrical function over the vectors of the base forms. On the other hand, Lazaridou et al. (2013) and Cotterell and Sch¨utze (2017) represent derivational affixes as vectors and investigate various functions to combine them with base forms. Kisselew et al. 119 (2015) and Pad´o et al. (2016) extend this line of research to model derivational morphology in German. This work demonstrates that various factors such as part of speech, semantic regularity and argument structure (Grimshaw, 1990) influence the predictability of a derived word. The third area of research focuses on the analysis of derivationally complex forms, which differs from this study in that we focus on generation. The goal o"
E17-2019,P96-1004,0,0.165816,"ology is the set of processes through which the word form outwardly displays syntactic information, e.g., verb tense. It follows that an inflectional affix typically neither changes the part-of-speech (POS) nor the semantics of the word. For example, the English verb to run takes various forms: run, runs and ran, all of which convey the concept “moving by foot quickly”, but appear in complementary syntactic contexts. Derivation, on the other hand, deals with the formation of new words that have semantic shifts in meaning (often including POS) and is tightly intertwined with lexical semantics (Light, 1996). Consider the example of the English noun discontentedness, which is derived from the adjective discontented. It is true that both words share a close semantic relationship, but the transformation is clearly more than a simple inflectional marking of syntax. Indeed, we can go one step further and define a chain of words content 7→ contented 7→ discontented 7→ discontentedness. In this work, we deal with the formation of deverbal nouns, i.e., nouns that are formed from verbs. Common examples of this in English include agentives (e.g., explain 7→ explainer), gerunds (e.g., explain 7→ explaining"
E17-2019,C16-1122,0,0.137298,"Missing"
E17-2019,P16-1158,1,0.668374,"most appropriate depending on the context, in terms of syntactic and semantic concordance. Our model is highly flexible and easily applicable to other related lexical problems. 3 Related Work Although in the last few years many neural morphological models have been proposed, most of them have focused on inflectional morphology (e.g., see Cotterell et al. (2016a)). Focusing on derivational processes, there are three main directions of research. The first deals with the evaluation of word embeddings either using a word analogy task (Gladkova et al., 2016) or binary relation type classification (Vylomova et al., 2016). In this context, it has been shown that, unlike inflectional morphology, most derivational relations cannot be as easily captured using distributional methods. Researchers working on the second type of task attempt to predict derived forms using the embedding of its corresponding base form and a vector encoding a “derivational” shift. Guevara (2011) notes that derivational affixes can be modelled as a geometrical function over the vectors of the base forms. On the other hand, Lazaridou et al. (2013) and Cotterell and Sch¨utze (2017) represent derivational affixes as vectors and investigate v"
E17-2028,P15-5005,0,0.0361252,"Missing"
E17-2028,D14-1165,0,0.0106915,"o a baseline that simply embeds lemmas rather than words (equivalent to fixing rk = 1). 6 Related Work Tensor factorization has already found uses in a few corners of NLP research. Van de Cruys et al. (2013) applied tensor factorization to model the compositionality of subject-verb-object triples. Similarly, Hashimoto and Tsuruoka (2015) use an implicit tensor factorization method to learn embeddings for transitive verb phrases. Tensor factorization also appears in semantic-based NLP tasks. Lei et al. (2015) explicitly factorize a tensor based on feature vectors for predicting semantic roles. Chang et al. (2014) use tensor factorization to create knowledge base embeddings optimized for relation extraction. See Bouchard et al. (2015) for a large bibliography. Other researchers have likewise attempted to escape the bag-of-words assumption in word embeddings, e.g., Yatbaz et al. (2012) incorporates morphological and orthographic features into continuous vectors; Cotterell and Sch¨utze (2015) consider a multi-task set-up to force morphological information into embeddings; Cotterell and Sch¨utze (2017) jointly morphologically segment and embed words; Levy and Goldberg (2014a) derive contexts based on depe"
E17-2028,N15-1140,1,0.893374,"Missing"
E17-2028,P16-1156,1,0.0989234,"Missing"
E17-2028,W16-2506,0,0.00454851,"33 28.54 +7.21 de 353 S IM L 44.90 28.39 50.39 68.08 40.09 53.97 +23.18 +11.7 +3.58 RG -65 Z222 29.75 31.11 +1.36 RG -65 353 70.60 64.50 71.71 63.72 +1.11 -0.78 en MEN MTURK 64.33 58.77 66.66 62.64 +2.33 +3.87 S IM L 41.62 49.70 +8.08 S IM V 30.48 29.96 +0.52 RW 40.78 42.40 +1.62 Table 2: Word similarity results comparing the compositional morphology tensor with the standard skip-gram model. Numbers indicate Spearman’s correlation coefficient ρ between human similarity judgements and the cosine distances of vectors. For each language, we compare on several sets of human judgments as listed by Faruqui et al. (2016, Table 2). tribute signal to the embedding of run. We expect these lemma embeddings to be predictive of human judgments of lemma similarity. We evaluate using standard datasets on four languages (French, Italian, German and English). Given a list of pairs of words (always lemmata), multiple native speakers judged (on a scale of 1– 10) how “similar” those words are conceptually. Our model produces a similarity judgment for each pair using the cosine similarity of their lemma embeddings wj . Table 2 shows how well this learned judgment correlates with the average human judgment. Our model does"
E17-2028,N13-1092,1,0.120323,"Missing"
E17-2028,W15-4001,0,0.0184305,"oduces a similarity judgment for each pair using the cosine similarity of their lemma embeddings wj . Table 2 shows how well this learned judgment correlates with the average human judgment. Our model does achieve higher correlation than skip-gram word embeddings. Note we did not compare to a baseline that simply embeds lemmas rather than words (equivalent to fixing rk = 1). 6 Related Work Tensor factorization has already found uses in a few corners of NLP research. Van de Cruys et al. (2013) applied tensor factorization to model the compositionality of subject-verb-object triples. Similarly, Hashimoto and Tsuruoka (2015) use an implicit tensor factorization method to learn embeddings for transitive verb phrases. Tensor factorization also appears in semantic-based NLP tasks. Lei et al. (2015) explicitly factorize a tensor based on feature vectors for predicting semantic roles. Chang et al. (2014) use tensor factorization to create knowledge base embeddings optimized for relation extraction. See Bouchard et al. (2015) for a large bibliography. Other researchers have likewise attempted to escape the bag-of-words assumption in word embeddings, e.g., Yatbaz et al. (2012) incorporates morphological and orthographic"
E17-2028,C92-2082,0,0.0789371,"onsider a multi-task set-up to force morphological information into embeddings; Cotterell and Sch¨utze (2017) jointly morphologically segment and embed words; Levy and Goldberg (2014a) derive contexts based on dependency relations; PPDB (Ganitkevitch et al., 2013) employs a mixed bag of words, parts of speech, and syntax; Rastogi et al. (2015) represent word contexts, morphology, semantic frame relations, syntactic dependency relations, and multilingual bitext counts each as separate matrices, combined via GCCA; and, finally, Schwartz et al. (2016) derived embeddings based on Hearst patterns (Hearst, 1992). Ling et al. (2015) learn position-specific word embeddings (§4.1), but do not factor them as wj rk to share parameters (we did not compare empirically to this). As demonstrated in the experiments, our tensor factorization method enables us to include other syntactic properties besides word order, e.g. morphology. Poliak et al. (2017) also create positional word embeddings. Our research direction is orthogonal to these efforts in that we provide a general purpose procedure for all sorts of higher-order coocurrence. 7 Conclusion We have presented an interpretation of the skipgram model as expo"
E17-2028,N15-1121,0,0.0411841,"Missing"
E17-2028,P14-2050,0,0.0916527,"is passed through some “inverse link” function to obtain the expected feature values under the distribution, which in turn determines j = i XX j i exp (ci · wj ) Xij log P i0 exp (ci0 · wj ) (4) This is the log-likelihood (plus a constant) if we assume that for each word j, the context vector xj was drawn from a multinomial with natural parameter vector C > wj and count parameter P Nj = i Xij . This is the same model as in Figure 1a, but with a different conditional distribution for xj , and with xj taking an additional observed parent Nj (which is the token count of word j). 2.1 Related work Levy and Goldberg (2014b) also interpreted skipgram as matrix factorization. They argued that skipgram estimation by negative sampling implicitly factorizes a shifted matrix of positive empirical pointwise mutual information values. We instead regard the skip-gram objective itself as demanding EPCA-style factorization of the count matrix X: i.e., X arose stochastically from some unknown matrix of log-linear parameters (column j of X generated from parameter column j), and we seek a rank-d estimate C > W of that matrix. pLSI (Hofmann, 1999) similarly factors an unknown matrix of multinomial probabilities, which is mu"
E17-2028,Q15-1016,0,0.0144649,"l case—it is equal to (wj ; 1) (1; rk ), which uses twice as many dimensions to embed each object. 5 Experiments We build HOSG on top of the HYPERWORDS package. All models (both skip-gram and higher-order skip-gram) are trained for 10 epochs and use 5 negative samples. All models for §5.1 are trained on the Sept. 2016 dump of the full Wikipedia. All models for §5.2 were trained on the lemmatized and POS-tagged WaCky corpora (Baroni et al., 2009) for French, Italian, German and English (Joubarne and Inkpen, 2011; Leviant and Reichart, 2015). To ensure controlled and fair experiments, we follow Levy et al. (2015) for all preprocessing. 5.1 Experiment 1: Positional Tensor We postulate that the positional tensor should encode richer notions of syntax than standard bag6 If one wanted to extend the model to decompose the context words i as well, we see at least four approaches. 7 Cotterell et al. (2016) made two further moves that could be applied to extend the present paper. First, they allowed a word to consist of any number of (unordered) morphemes— not necessarily two—whose embeddings were combined (by summation) to get the word embedding. Second, this sum also included word-specific random noise, all"
E17-2028,N15-1142,0,0.0122328,"-task set-up to force morphological information into embeddings; Cotterell and Sch¨utze (2017) jointly morphologically segment and embed words; Levy and Goldberg (2014a) derive contexts based on dependency relations; PPDB (Ganitkevitch et al., 2013) employs a mixed bag of words, parts of speech, and syntax; Rastogi et al. (2015) represent word contexts, morphology, semantic frame relations, syntactic dependency relations, and multilingual bitext counts each as separate matrices, combined via GCCA; and, finally, Schwartz et al. (2016) derived embeddings based on Hearst patterns (Hearst, 1992). Ling et al. (2015) learn position-specific word embeddings (§4.1), but do not factor them as wj rk to share parameters (we did not compare empirically to this). As demonstrated in the experiments, our tensor factorization method enables us to include other syntactic properties besides word order, e.g. morphology. Poliak et al. (2017) also create positional word embeddings. Our research direction is orthogonal to these efforts in that we provide a general purpose procedure for all sorts of higher-order coocurrence. 7 Conclusion We have presented an interpretation of the skipgram model as exponential family princ"
E17-2028,D14-1162,0,0.0855584,"syntax) or morphological information (to share parameters across related words). We experiment on 40 languages and show that our model improves upon skip-gram. 1 2 Introduction Over the past years NLP has witnessed a veritable frenzy on the topic of word embeddings: lowdimensional representations of distributional information. The embeddings, trained on extremely large text corpora such as Wikipedia and Common Crawl, are claimed to encode semantic knowledge extracted from large text corpora. Numerous methods have been proposed—the most popular being skip-gram (Mikolov et al., 2013) and GloVe (Pennington et al., 2014)—for learning these low-dimensional embeddings from a bag of contexts associated with each word type. Natural language text, however, contains richer structure than simple context-word pairs. In this work, we embed n-tuples rather than pairs, allowing us to escape the bag-of-words assumption and encode richer linguistic structures. As a first step, we offer a novel interpretation of the skip-gram model (Mikolov et al., 2013). We show how skip-gram can be viewed as an application of exponential-family principal components analysis (EPCA) (Collins et al., 2001) to an integer matrix of coocurrenc"
E17-2028,petrov-etal-2012-universal,0,0.0290821,"Missing"
E17-2028,E17-2081,1,0.885781,"Missing"
E17-2028,N15-1058,1,0.772914,"Missing"
E17-2028,N16-1060,0,0.0202674,"Missing"
E17-2028,W16-2520,0,0.0416803,"Missing"
E17-2028,N13-1134,0,0.0390591,"Missing"
E17-2028,D12-1086,0,0.0694959,"Missing"
E17-2028,L16-1262,0,\N,Missing
E17-2035,W06-1655,0,0.100025,"Missing"
E17-2035,P11-1004,0,0.0182457,"models this transformation as ( ) ∑ 1 exp pθ (s, l |w) = θ ⊤ f (si , ℓi , ℓi−1 ) , Zθ (w) 3 Morphological Segmentation We first examine the task of morphological segmentation in the Dravidian languages. The task entails breaking a word up into its constituent morphs. For example, the English word joblessness can be segmented as job+less+ness. When processing morphologically-rich languages, this helps reduce the sparsity created by the higher OOV rate due to productive morphology, and, empirically, has shown to be beneficial in a diverse variety of down-stream tasks, e.g., machine translation (Clifton and Sarkar, 2011), speech recognition (Afify et al., 2006), keyword spotting (Narasimhan et al., 2014) and parsing (Seeker and Özlem Çetinoğlu, 2015). Both supervised # Sentences 8600 4034 4550 5679 Segmentation i=1 where s is a segmentation, ℓ a labeling, θ ∈ Rd is the parameter vector, f is a feature function2 and the partition function Zθ (w) ensures the distribution is normalized. Note that each ℓi is taken from a set of labels L. In this work, we take L = {prefix, stem, suffix}. As an extension to the standard S-CRF Model, we allow for higher-order segment interactions (Nguyen et al., 2011). This allows f"
E17-2035,K15-1017,1,0.898953,"93 4730 4445 4183 Table 2: Per language breakdown of size of the POS portion and the morphological segmentation portion of DravMorph. All train / dev / test splits used in the experiments will be released with the corpus. and unsupervised approaches have been successful, but, when annotated data is available, supervised approaches typically greatly outperform unsupervised approaches (Ruokolainen et al., 2013). In light of this, we adopt a fully supervised model here. We apply semi-Markov Conditional Random Fields (S-CRFs) to the problem of morphological segmentation (Sarawagi and Cohen, 2004; Cotterell et al., 2015). S-CRFs have the ability to jointly model both a segmentation and a labeling. For example, consider the following the Malayalam word kūṭṭukāranmāruṭeyēāppam (കൂടടുകാരനമാരുടെയോപപം) (with (male) friends): labeled segmentation kūṭṭukāranmāruṭeyēāppam Z===========⇒ {z } | w [stem kūṭṭukāran] [suf mār] [suf uṭe] [suf yēāppam] . | {z } |{z } |{z } | {z } s1 ,ℓ1 s2 ,ℓ2 s3 ,ℓ3 s4 ,ℓ4 A S-CRF models this transformation as ( ) ∑ 1 exp pθ (s, l |w) = θ ⊤ f (si , ℓi , ℓi−1 ) , Zθ (w) 3 Morphological Segmentation We first examine the task of morphological segmentation in the Dravidian languages. The ta"
E17-2035,D16-1256,1,0.88989,"Missing"
E17-2035,N16-1080,1,0.863416,"Missing"
E17-2035,R15-1041,1,0.844831,"ik (2007) ap220 Marmot Marmot + seg Ka 86.35 88.04 Ma 88.77 90.44 Ta 89.04 91.64 Te 90.50 91.44 Table 4: Tagging results using the Marmot tagger on the four Dravidian languages studied in the paper. The results indicate strongly that morphological segmentation---rather than simple prefix and suffixes n-gram features---is a useful step in handling the agglutinative Dravidian languages. ply linear-chain CRFs for POS tagging of Bengali, Hindi and Telugu. Another approach that applied to POS tagging of Dravidian language is to use part-of-speech tagger of another similar languages. More recently, Kumar et al. (2015) applied adaptor grammars to unsupervised morphological segmentation of Kannada, Malayalam and Tamil. 6 Conclusion In this paper, we presented a higher-order semiCRF model for morphological segmentation for the Dravidian languages of South India. Our results show that the modeling of higher-order dependencies between segments and linguisticallyinspired features can greatly improve system performance. We also showed that segmentation is beneficial to the down-stream task of POS tagging. To promote research on the Dravidian family, we release hand-corrected corpora for both morphological segment"
E17-2035,D13-1032,0,0.0473167,"Missing"
E17-2035,D14-1095,0,0.0212216,"(w) 3 Morphological Segmentation We first examine the task of morphological segmentation in the Dravidian languages. The task entails breaking a word up into its constituent morphs. For example, the English word joblessness can be segmented as job+less+ness. When processing morphologically-rich languages, this helps reduce the sparsity created by the higher OOV rate due to productive morphology, and, empirically, has shown to be beneficial in a diverse variety of down-stream tasks, e.g., machine translation (Clifton and Sarkar, 2011), speech recognition (Afify et al., 2006), keyword spotting (Narasimhan et al., 2014) and parsing (Seeker and Özlem Çetinoğlu, 2015). Both supervised # Sentences 8600 4034 4550 5679 Segmentation i=1 where s is a segmentation, ℓ a labeling, θ ∈ Rd is the parameter vector, f is a feature function2 and the partition function Zθ (w) ensures the distribution is normalized. Note that each ℓi is taken from a set of labels L. In this work, we take L = {prefix, stem, suffix}. As an extension to the standard S-CRF Model, we allow for higher-order segment interactions (Nguyen et al., 2011). This allows for feature functions to look at multiple adjacent segments si , 218 2 Note we have om"
E17-2035,C04-1081,0,0.170602,"Missing"
E17-2035,petrov-etal-2012-universal,0,0.0886232,"Missing"
E17-2035,W96-0213,0,0.293853,"Missing"
E17-2035,W13-3504,0,0.151268,"re, we calculated inter-annotator agreement of two annotators for morphological labels and all datasets have Cohen's κ (Cohen, 1968) > 0.80. POS Tagging Lang Ka Ma Ta Te # Tokens 31364 34300 32400 30625 # Types 3593 4730 4445 4183 Table 2: Per language breakdown of size of the POS portion and the morphological segmentation portion of DravMorph. All train / dev / test splits used in the experiments will be released with the corpus. and unsupervised approaches have been successful, but, when annotated data is available, supervised approaches typically greatly outperform unsupervised approaches (Ruokolainen et al., 2013). In light of this, we adopt a fully supervised model here. We apply semi-Markov Conditional Random Fields (S-CRFs) to the problem of morphological segmentation (Sarawagi and Cohen, 2004; Cotterell et al., 2015). S-CRFs have the ability to jointly model both a segmentation and a labeling. For example, consider the following the Malayalam word kūṭṭukāranmāruṭeyēāppam (കൂടടുകാരനമാരുടെയോപപം) (with (male) friends): labeled segmentation kūṭṭukāranmāruṭeyēāppam Z===========⇒ {z } | w [stem kūṭṭukāran] [suf mār] [suf uṭe] [suf yēāppam] . | {z } |{z } |{z } | {z } s1 ,ℓ1 s2 ,ℓ2 s3 ,ℓ3 s4 ,ℓ4 A S-CR"
E17-2035,E14-4017,0,0.0597902,"Missing"
E17-2035,Q15-1026,0,0.067967,"Missing"
E17-2035,P15-2111,0,0.0232754,"Missing"
E17-2120,W16-2007,0,0.531293,"mpa (i) ) T 1/τ qi (mi ) qi (m0i ) if uniform(0, 1) ≤ a then mi ← m0i . update string to new value if accepted τ ←τ ·d . decay temperature where d ∈ (0, 1) until τ ≤  . repeat until convergence; see Spall (2003, Ch. 8) return m Following previous work (Faruqui et al., 2016), we train our model in the fully observed setting with complete paradigms as training data. As our model is directed, this makes parameter estimation relatively straightforward. We may estimate the parameters of each LSTM independently without performing joint inference during training. We follow the training procedure of Aharoni et al. (2016), using a maximum of 300 epochs of SGD. 4.2 3.1 LSTMs with Hard Monotonic Attention We define the conditional distributions in our Bayesian network p(mi |mpaT (i) ) as LSTMs with hard monotonic attention (Aharoni et al., 2016; Aharoni and Goldberg, 2016), which we briefly overview. These networks map one inflection to another, e.g. mapping the English gerund running to the past tense ran, using an encoder-decoder architecture (Sutskever et al., 2014) run over an augmented alignment alphabet, consisting of copy, substitution, deletion and insertion, as in Dreyer et al. (2008). For strings x, y"
E17-2120,N15-1107,0,0.0262512,"ccur in training corpora. Thus, a necessity for improving NLP on morphologically rich languages is the ability to analyze all inflected forms for any lexical entry. One way to do this is through paradigm completion, which generates all the inflected forms associated with a given lemma. Until recently, paradigm completion has been narrowly construed as the task of generating a full paradigm (e.g. noun declension, verb conjugation) based on a single privileged form—the lemma (i.e. the citation form, such as poner). While recent work (Durrett and DeNero, 2013; Hulden, 2014; Nicolai et al., 2015; Ahlberg et al., 2015; Faruqui et al., 2016) has made tremendous progress on this narrower task, paradigm completion is not only broader in scope, but is better solved without privileging the lemma over other forms. By forcing string-to-string transformations from one inflected form to another to go through the lemma, the transformation problem is often made more complex than by allowing transformations to happen directly or through a different intermediary form. This interpretation is inspired by ideas from linguistics and language pedagogy, namely principal parts morphology, which argues that forms in a paradigm"
E17-2120,chrupala-etal-2008-learning,0,0.24008,"Missing"
E17-2120,N15-1094,1,0.858951,"ed we may sample strings from the conditional p(mi |mpaT (i) ) efficiently using forward sampling. This network stands in contrast to attention models (Bahdanau et al., 2015) in which the alignments are soft and not necessarily monotonic. We refer the reader to Aharoni et al. (2016) for exact implementation details as we use their code out-of-the-box.2 4 Neural Graphical Models over Strings Our Bayesian network defined in Equation (1) is a graphical model defined over multiple stringvalued random variables, a framework formalized in Dreyer and Eisner (2009). In contrast to previous work, e.g. Cotterell and Eisner (2015; Peng et al. (2015), which considered conditional distributions encodable by finite-state machines, we offer the first neural parameterization for such graphical models. With the increased expressivity comes computational challenges—inference becomes intractable. Thus, we fashion an efficient sampling algorithm. 2 https://github.com/roeeaharoni/ morphological-reinflection Parameter Estimation Approximate Joint Decoding In a Bayesian network, the maximum-a-posteriori (MAP) inference problem refers to finding the most probable configuration of the variables given some evidence. In our case, thi"
E17-2120,Q15-1031,1,0.949378,"ns to happen directly or through a different intermediary form. This interpretation is inspired by ideas from linguistics and language pedagogy, namely principal parts morphology, which argues that forms in a paradigm are best derived using a set of citation forms rather than a single form (Finkel and Stump, 2007a; Finkel and Stump, 2007b). Directed graphical models provide a natural formalism for principal parts morphology since a graph topology can represent relations between inflected forms and principal parts. Specifically, we apply string-valued graphical models (Dreyer and Eisner, 2009; Cotterell et al., 2015) to the problem. We develop a novel, neural parameterization of string-valued graphical models where the conditional probabilities in the Bayesian network are given by a sequence-to-sequence model (Sutskever et al., 2014). However, under such a parameterization, exact inference and decoding are intractable. Thus, we derive a sampling-based decoding algorithm. We experiment on 5 languages: Arabic, German, Latin, Russian, and Spanish, showing that our model outperforms a baseline approach that privileges the lemma form. 2 A Generative Model of Principal Parts We first formally define the task of"
E17-2120,W16-2002,1,0.819436,"nimal directed spanning tree. The intuition behind this procedure is that the number of deterministic arcs should be maximized. Gold Network. Finally, for Latin verbs we consider a graph that matches the classic pedagogical derivation of Latin verbs from four principal parts. 3 Inflection Generation with RNNs RNNs have recently achieved state-of-the-art results for many sequence-to-sequence mapping problems and paradigm completion is no exception. Given the success of LSTM-based (Hochreiter and Schmidhuber, 1997) and GRU-based (Cho et al., 2014) morphological inflectors (Faruqui et al., 2016; Cotterell et al., 2016), we choose a neural parameterization for our Bayesian network, i.e. the conditional probability p(mi |mpaT (i) ) is computed using a RNN. Our graphical modeling approach as well as the inference algorithms subsequently discussed in §4.2 are agnostic to the minutiae of any one parameterization, i.e. the encoding p(mi |mpaT (i) ) is a black box. Algorithm 1 Decoding by Simulated Annealing 4.1 1: procedure S IMULATED -A NNEALING(T , d, ) 2: τ ← 10.0 ; m ← [ε, . . . , ε] 3: repeat 4: i ∼ uniform({1, 2, . . . , |m|}) . sample latent node in T 5: m0i ∼ qi "" . sample string from proposal distributi"
E17-2120,D09-1011,0,0.67068,"by allowing transformations to happen directly or through a different intermediary form. This interpretation is inspired by ideas from linguistics and language pedagogy, namely principal parts morphology, which argues that forms in a paradigm are best derived using a set of citation forms rather than a single form (Finkel and Stump, 2007a; Finkel and Stump, 2007b). Directed graphical models provide a natural formalism for principal parts morphology since a graph topology can represent relations between inflected forms and principal parts. Specifically, we apply string-valued graphical models (Dreyer and Eisner, 2009; Cotterell et al., 2015) to the problem. We develop a novel, neural parameterization of string-valued graphical models where the conditional probabilities in the Bayesian network are given by a sequence-to-sequence model (Sutskever et al., 2014). However, under such a parameterization, exact inference and decoding are intractable. Thus, we derive a sampling-based decoding algorithm. We experiment on 5 languages: Arabic, German, Latin, Russian, and Spanish, showing that our model outperforms a baseline approach that privileges the lemma form. 2 A Generative Model of Principal Parts We first fo"
E17-2120,W14-2804,0,0.0366937,"ing since many word forms will not occur in training corpora. Thus, a necessity for improving NLP on morphologically rich languages is the ability to analyze all inflected forms for any lexical entry. One way to do this is through paradigm completion, which generates all the inflected forms associated with a given lemma. Until recently, paradigm completion has been narrowly construed as the task of generating a full paradigm (e.g. noun declension, verb conjugation) based on a single privileged form—the lemma (i.e. the citation form, such as poner). While recent work (Durrett and DeNero, 2013; Hulden, 2014; Nicolai et al., 2015; Ahlberg et al., 2015; Faruqui et al., 2016) has made tremendous progress on this narrower task, paradigm completion is not only broader in scope, but is better solved without privileging the lemma over other forms. By forcing string-to-string transformations from one inflected form to another to go through the lemma, the transformation problem is often made more complex than by allowing transformations to happen directly or through a different intermediary form. This interpretation is inspired by ideas from linguistics and language pedagogy, namely principal parts morph"
E17-2120,D08-1113,0,0.230975,"Missing"
E17-2120,N13-1138,0,0.101554,"blematic for machine learning since many word forms will not occur in training corpora. Thus, a necessity for improving NLP on morphologically rich languages is the ability to analyze all inflected forms for any lexical entry. One way to do this is through paradigm completion, which generates all the inflected forms associated with a given lemma. Until recently, paradigm completion has been narrowly construed as the task of generating a full paradigm (e.g. noun declension, verb conjugation) based on a single privileged form—the lemma (i.e. the citation form, such as poner). While recent work (Durrett and DeNero, 2013; Hulden, 2014; Nicolai et al., 2015; Ahlberg et al., 2015; Faruqui et al., 2016) has made tremendous progress on this narrower task, paradigm completion is not only broader in scope, but is better solved without privileging the lemma over other forms. By forcing string-to-string transformations from one inflected form to another to go through the lemma, the transformation problem is often made more complex than by allowing transformations to happen directly or through a different intermediary form. This interpretation is inspired by ideas from linguistics and language pedagogy, namely princip"
E17-2120,N16-1077,0,0.518227,"ra. Thus, a necessity for improving NLP on morphologically rich languages is the ability to analyze all inflected forms for any lexical entry. One way to do this is through paradigm completion, which generates all the inflected forms associated with a given lemma. Until recently, paradigm completion has been narrowly construed as the task of generating a full paradigm (e.g. noun declension, verb conjugation) based on a single privileged form—the lemma (i.e. the citation form, such as poner). While recent work (Durrett and DeNero, 2013; Hulden, 2014; Nicolai et al., 2015; Ahlberg et al., 2015; Faruqui et al., 2016) has made tremendous progress on this narrower task, paradigm completion is not only broader in scope, but is better solved without privileging the lemma over other forms. By forcing string-to-string transformations from one inflected form to another to go through the lemma, the transformation problem is often made more complex than by allowing transformations to happen directly or through a different intermediary form. This interpretation is inspired by ideas from linguistics and language pedagogy, namely principal parts morphology, which argues that forms in a paradigm are best derived using"
E17-2120,W16-2010,0,0.127069,"Missing"
E17-2120,D16-1097,1,0.859823,"Missing"
E17-2120,N15-1093,0,0.0651139,"word forms will not occur in training corpora. Thus, a necessity for improving NLP on morphologically rich languages is the ability to analyze all inflected forms for any lexical entry. One way to do this is through paradigm completion, which generates all the inflected forms associated with a given lemma. Until recently, paradigm completion has been narrowly construed as the task of generating a full paradigm (e.g. noun declension, verb conjugation) based on a single privileged form—the lemma (i.e. the citation form, such as poner). While recent work (Durrett and DeNero, 2013; Hulden, 2014; Nicolai et al., 2015; Ahlberg et al., 2015; Faruqui et al., 2016) has made tremendous progress on this narrower task, paradigm completion is not only broader in scope, but is better solved without privileging the lemma over other forms. By forcing string-to-string transformations from one inflected form to another to go through the lemma, the transformation problem is often made more complex than by allowing transformations to happen directly or through a different intermediary form. This interpretation is inspired by ideas from linguistics and language pedagogy, namely principal parts morphology, which argues th"
E17-2120,D15-1108,1,0.893311,"Missing"
E17-2120,P15-2111,1,0.858674,"proposed models take the form of Figure 1a, i.e. a graphical model where all leaves connect to the lemma. Unfortunately, in that configuration, observing additional forms cannot help at test time since information must flow through the lemma, which is always observed. We conjecture that principal parts-based topologies will outperform the baseline topology for that reason. We propose a controlled experiment in which we consider identical training and testing conditions and vary only the topology. Data. Data for training, development, and testing is randomly sampled from the UniMorph dataset (Sylak-Glassman et al., 2015).3 We run experiments on Arabic, German, and Russian nominal paradigms and Latin and Spanish verbal paradigms. The sizes of the resulting data splits are given in Table 2. For the development and test splits we always include the lemma (as is standard) while sampling additional observed forms. On average one third of all forms are observed. Evaluation. Evaluation of the held-out sets proceeds as follows: Given the observed forms in the paradigm, we jointly decode the remaining forms as discussed in §4.2; joint decoding is performed without Algorithm 1 for the baseline—instead, we 762 3 http://"
I17-2016,J96-1002,0,0.329562,"Language Processing, pages 91–96, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP to each token: B if the token is the beginning of an entity, or I if the token is inside an entity, or O if the token is outside an entity (see Fig. 1). Following convention, we focus on person (per), location (loc), organization (org), and miscellaneous (misc) entity types, resulting in 9 tags: {B - ORG, I - ORG, B - PER , I - PER , B - LOC , I - LOC , B - MISC , I - MISC }. Conditional Random Fields (CRFs), first introduced in Lafferty et al. (2001), generalize the classical maximum entropy models (Berger et al., 1996) to distributions over structured objects, and are an effective tool for sequence labeling tasks like NER. We briefly overview the formalism here and then discuss its neural parameterization. 2.1 features are conjoined with other indicator features, e.g., is the ith tag I - LOC? We refer the reader to Sha and Pereira (2003) for standard CRF feature functions employed in NER, which we use in this work. The log-linear parameterization yields a convex objective and is extremely efficient to compute as it only involves a sparse dot product, but the representational power of model depends fully on"
I17-2016,Q16-1026,0,0.00598678,". Now, given a low-resource target language τ and a source language σ (potentially, a set of m highresource source languages {σi }m i=1 ). We consider the following training objective X L (θ) = log pθ (t |w, τ ) + (7) X (t,w)∈Dτ µ· log pθ (t |w, σ) , Character-Level Neural Networks. In recent years, many authors have incorporated characterlevel information into taggers using neural networks, e.g., dos Santos and Zadrozny (2014) employed a convolutional network for part-of-speech tagging in morphologically rich languages and Ling et al. (2015) a LSTM for a myriad of different tasks. Relatedly, Chiu and Nichols (2016) approached NER with character-level LSTMs, but without using a CRF. Our work firmly builds upon on this in that we, too, compactly summarize the word form with a recurrent neural component. Neural Transfer Schemes. Previous work has also performed transfer learning using neural networks. The novelty of our work lies in the crosslingual transfer. For example, Peng and Dredze (2017) and Yang et al. (2017), similarly oriented concurrent papers, focus on domain adaptation within the same language. While this is a related problem, cross-lingual transfer is much more involved since the morphology,"
I17-2016,P07-1033,0,0.0792625,"Missing"
I17-2016,P82-1020,0,0.794572,"Missing"
I17-2016,D15-1176,0,0.0395701,"· · c|wi |is shared cross-lingually while e(wi ) is language-specific. Now, given a low-resource target language τ and a source language σ (potentially, a set of m highresource source languages {σi }m i=1 ). We consider the following training objective X L (θ) = log pθ (t |w, τ ) + (7) X (t,w)∈Dτ µ· log pθ (t |w, σ) , Character-Level Neural Networks. In recent years, many authors have incorporated characterlevel information into taggers using neural networks, e.g., dos Santos and Zadrozny (2014) employed a convolutional network for part-of-speech tagging in morphologically rich languages and Ling et al. (2015) a LSTM for a myriad of different tasks. Relatedly, Chiu and Nichols (2016) approached NER with character-level LSTMs, but without using a CRF. Our work firmly builds upon on this in that we, too, compactly summarize the word form with a recurrent neural component. Neural Transfer Schemes. Previous work has also performed transfer learning using neural networks. The novelty of our work lies in the crosslingual transfer. For example, Peng and Dredze (2017) and Yang et al. (2017), similarly oriented concurrent papers, focus on domain adaptation within the same language. While this is a related p"
I17-2016,I13-1183,0,0.0177086,"tures the user selects. 2.3 Modern CRFs, however, try to obviate the handselection of features through deep, non-linear parameterizations of ψ (ti−1 , ti , w; θ). This idea is far from novel and there have been numerous attempts in the literature over the past decade to find effective non-linear parameterizations (Peng et al., 2009; Do and Arti`eres, 2010; Collobert et al., 2011; Vinel et al., 2011; Fujii et al., 2012). Until recently, however, it was not clear that these nonlinear parameterizations of CRFs were worth the non-convexity and the extra computational cost. Indeed, on neural CRFs, Wang and Manning (2013) find that “a nonlinear architecture offers no benefits in a high-dimensional discrete feature space.” However, recently with the application of long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997) recurrent neural networks (RNNs) (Elman, 1990) to CRFs, it has become clear that neural feature extractors are superior to the hand-crafted approaches (Huang et al., 2015; Lample et al., 2016; Ma and Hovy, 2016). As our starting point, we build upon the architecture of Lample et al. (2016), which is currently competitive with the state of the art for NER. CRFs: A Cursory Overview We star"
I17-2016,P16-1101,0,0.00986082,"til recently, however, it was not clear that these nonlinear parameterizations of CRFs were worth the non-convexity and the extra computational cost. Indeed, on neural CRFs, Wang and Manning (2013) find that “a nonlinear architecture offers no benefits in a high-dimensional discrete feature space.” However, recently with the application of long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997) recurrent neural networks (RNNs) (Elman, 1990) to CRFs, it has become clear that neural feature extractors are superior to the hand-crafted approaches (Huang et al., 2015; Lample et al., 2016; Ma and Hovy, 2016). As our starting point, we build upon the architecture of Lample et al. (2016), which is currently competitive with the state of the art for NER. CRFs: A Cursory Overview We start with two discrete alphabets Σ and ∆. In the case of sentence-level sequence tagging, Σ is a set of words (potentially infinite) and ∆ is a set of tags (generally finite; in our case |∆ |= 9). Given t = t1 · · · tn ∈ ∆n and w = w1 · · · wn ∈ Σn , where n is the sentence length. A CRF is a globally normalized conditional probability distribution, n pθ (t |w) = 1 Y ψ (ti−1 , ti , w; θ) , (1) Zθ (w) i=1 where ψ (ti−1 ,"
I17-2016,Q14-1005,0,0.0716736,"better at transferring entity-level abstractions cross-linguistically. Projection-based Transfer Schemes. Projection is a common approach to tag low-resource languages. The strategy involves annotating one side of bitext with a tagger for a high-resource language and then project the annotation the over the bilingual alignments obtained through unsupervised learning (Och and Ney, 2003). Using these projected annotations as weak supervision, one then trains a tagger in the target language. This line of research has a rich history, starting with Yarowsky and Ngai (2001). For a recent take, see Wang and Manning (2014) for projecting NER from English to Chinese. We emphasize that projection-based approaches are incomparable to our proposed method as they make an additional bitext assumption, which is generally not present in the case of low-resource languages. 5 5.1 Data We experiment on 15 languages from the crosslingual named entity dataset described in Pan et al. (2017). We focus on 5 typologically diverse3 target languages: Galician, West Frisian, Ukranian, Marathi and Tagalog. As related source languages, we consider Spanish, Catalan, Italian, French, Romanian, Dutch, Russian, Cebuano, Hindi and Urdu."
I17-2016,J03-1002,0,0.0251075,"linear CRF to dominate in the low-resource settings, the neural CRF to dominate in the highresource setting. The novelty of our paper lies in the consideration of the low-resource with transfer case: we show that neural CRFs are better at transferring entity-level abstractions cross-linguistically. Projection-based Transfer Schemes. Projection is a common approach to tag low-resource languages. The strategy involves annotating one side of bitext with a tagger for a high-resource language and then project the annotation the over the bilingual alignments obtained through unsupervised learning (Och and Ney, 2003). Using these projected annotations as weak supervision, one then trains a tagger in the target language. This line of research has a rich history, starting with Yarowsky and Ngai (2001). For a recent take, see Wang and Manning (2014) for projecting NER from English to Chinese. We emphasize that projection-based approaches are incomparable to our proposed method as they make an additional bitext assumption, which is generally not present in the case of low-resource languages. 5 5.1 Data We experiment on 15 languages from the crosslingual named entity dataset described in Pan et al. (2017). We"
I17-2016,P17-1178,0,0.0397626,"ng (Och and Ney, 2003). Using these projected annotations as weak supervision, one then trains a tagger in the target language. This line of research has a rich history, starting with Yarowsky and Ngai (2001). For a recent take, see Wang and Manning (2014) for projecting NER from English to Chinese. We emphasize that projection-based approaches are incomparable to our proposed method as they make an additional bitext assumption, which is generally not present in the case of low-resource languages. 5 5.1 Data We experiment on 15 languages from the crosslingual named entity dataset described in Pan et al. (2017). We focus on 5 typologically diverse3 target languages: Galician, West Frisian, Ukranian, Marathi and Tagalog. As related source languages, we consider Spanish, Catalan, Italian, French, Romanian, Dutch, Russian, Cebuano, Hindi and Urdu. For the language code abbreviations and linguistic families, see Tab. 1. For each of the target languages, we emulate a truly low-resource condition, creating a 100 sentence split for training. We then create a 10000 sentence superset to be able to compare to a high-resource condition in those same Experiments Fundamentally, we want to show that characterleve"
I17-2016,N01-1026,0,0.0335455,"with transfer case: we show that neural CRFs are better at transferring entity-level abstractions cross-linguistically. Projection-based Transfer Schemes. Projection is a common approach to tag low-resource languages. The strategy involves annotating one side of bitext with a tagger for a high-resource language and then project the annotation the over the bilingual alignments obtained through unsupervised learning (Och and Ney, 2003). Using these projected annotations as weak supervision, one then trains a tagger in the target language. This line of research has a rich history, starting with Yarowsky and Ngai (2001). For a recent take, see Wang and Manning (2014) for projecting NER from English to Chinese. We emphasize that projection-based approaches are incomparable to our proposed method as they make an additional bitext assumption, which is generally not present in the case of low-resource languages. 5 5.1 Data We experiment on 15 languages from the crosslingual named entity dataset described in Pan et al. (2017). We focus on 5 typologically diverse3 target languages: Galician, West Frisian, Ukranian, Marathi and Tagalog. As related source languages, we consider Spanish, Catalan, Italian, French, Rom"
I17-2016,W17-2612,0,0.00845038,"networks, e.g., dos Santos and Zadrozny (2014) employed a convolutional network for part-of-speech tagging in morphologically rich languages and Ling et al. (2015) a LSTM for a myriad of different tasks. Relatedly, Chiu and Nichols (2016) approached NER with character-level LSTMs, but without using a CRF. Our work firmly builds upon on this in that we, too, compactly summarize the word form with a recurrent neural component. Neural Transfer Schemes. Previous work has also performed transfer learning using neural networks. The novelty of our work lies in the crosslingual transfer. For example, Peng and Dredze (2017) and Yang et al. (2017), similarly oriented concurrent papers, focus on domain adaptation within the same language. While this is a related problem, cross-lingual transfer is much more involved since the morphology, syntax and semantics change more radically between two languages than (t,w)∈Dσ where µ is a trade-off parameter, Dτ is the set of training examples for the target language and Dσ is the set of training data for the source language σ. In the case of multiple source languages, we add a summand to the set of source languages used, in which case set have multiple training sets Dσi . 93"
I17-2016,W95-0107,0,0.0196601,"ew years, however, neural approaches that jointly learn their own features have surpassed the feature-based approaches in performance. Despite their empirical success, neural networks have remarkably high sample complexity and still only outperform hand-engineered feature approaches when enough supervised training data is available, leaving effective training of neural networks in the low-resource case a challenge. For most of the world’s languages, there is a very 2 Neural Conditional Random Fields Named entity recognition is typically framed as a sequence labeling task using the BIO scheme (Ramshaw and Marcus, 1995; Baldwin, 2009), i.e., given an input sentence, the goal is to assign a label 91 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 91–96, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP to each token: B if the token is the beginning of an entity, or I if the token is inside an entity, or O if the token is outside an entity (see Fig. 1). Following convention, we focus on person (per), location (loc), organization (org), and miscellaneous (misc) entity types, resulting in 9 tags: {B - ORG, I - ORG, B - PER , I - PER , B - LOC , I - LOC ,"
I17-2016,N03-1028,0,0.0497222,"(org), and miscellaneous (misc) entity types, resulting in 9 tags: {B - ORG, I - ORG, B - PER , I - PER , B - LOC , I - LOC , B - MISC , I - MISC }. Conditional Random Fields (CRFs), first introduced in Lafferty et al. (2001), generalize the classical maximum entropy models (Berger et al., 1996) to distributions over structured objects, and are an effective tool for sequence labeling tasks like NER. We briefly overview the formalism here and then discuss its neural parameterization. 2.1 features are conjoined with other indicator features, e.g., is the ith tag I - LOC? We refer the reader to Sha and Pereira (2003) for standard CRF feature functions employed in NER, which we use in this work. The log-linear parameterization yields a convex objective and is extremely efficient to compute as it only involves a sparse dot product, but the representational power of model depends fully on the quality of the features the user selects. 2.3 Modern CRFs, however, try to obviate the handselection of features through deep, non-linear parameterizations of ψ (ti−1 , ti , w; θ). This idea is far from novel and there have been numerous attempts in the literature over the past decade to find effective non-linear parame"
I17-2016,W02-2024,0,0.0641318,"per, we present a transfer learning scheme, whereby we train character-level neural CRFs to predict named entities for both high-resource languages and low-resource languages jointly. Learning character representations for multiple related languages allows transfer among the languages, improving F1 by up to 9.8 points over a loglinear CRF baseline. 1 O O B-ORG O B-LOC I-LOC I-LOC Figure 1: Example of an English sentence annotated with its typed named entities. limited amount of training data for NER; CoNLL— the standard dataset in the field—only provides annotations for 4 languages (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003). Creating similarly sized datasets for other languages has a prohibitive annotation cost, making the lowresource case an important scenario. To get around this barrier, we develop a cross-lingual solution: given a low-resource target language, we additionally offer large amounts of annotated data in a language that is genetically related to the target language. We show empirically that this improves the quality of the resulting model. In terms of neural modeling, we introduce a novel neural conditional random field (CRF) for cross-lingual NER that allows"
I17-2016,W03-0419,0,\N,Missing
I17-2016,N16-1030,0,\N,Missing
K15-1017,W06-1655,0,0.0677899,"as well as the semi-supervised model of Kohonen et al. (2010a) on English, Finnish and Turkish. Finally, Ruokolainen et al. (2014) get further consistent improvements by using features extracted from large corpora, based on the letter successor variety (LSV) model (Harris, 1995) and on unsupervised segmentation models such as Morfessor CatMAP (Creutz et al., 2007a). The idea behind LSV is that for example talking should be split into talk and ing, because talk can also be followed by different letters then i such as e (talked) and s (talks). Chinese word segmentation (CWS) is related to UMS. Andrew (2006) successfully apply semiCRFs to CWS. The problem of joint CWS and POS tagging (Ng and Low, 2004; Zhang and Clark, 2008) is related to LMS. To our knowledge, joint CWS and POS tagging has not been addressed by a simple single semi-CRF, possibly because POS tagsets typically used in Chinese treebanks are much bigger than our morphotactic tagsets and the morphological poverty of Chinese makes higher-order models necessary and the direct application of semi-CRFs infeasible. 6 English Finnish German Indonesian Turkish Zulu 878k 2,928k 2,338k 88k 617k 123k Train+Tune+Dev Train Tune Dev 800 100 100 8"
K15-1017,W06-2920,0,0.0387228,"e labels are needed for many tasks, for instance in sentiment analysis detecting morphologically encoded negation, as in Turkish, is crucial. In other words, for many applications UMS is insufficient. (ii) The LMS framework allows us to learn a probabilistic model of morphotactics. Working with LMS results in higher UMS accuracy. So even in applications that only need segments and no labels, LMS is beneficial. Note that the concatenation of labels across segments yields a bundle of morphological attributes similar to those found in the CoNLL datasets often used to train morphological taggers (Buchholz and Marsi, 2006)— thus LMS helps to unify UMS and morphological tagging. We believe that LMS is a needed extension of current work in morphological segmentation. Our framework concisely allows the model to capture interdependencies among various morphemes and model relations between entire mor2 Like en in English open, en in German Offen is part of the root. 165 5 4 3 2 1 0 German English P REFIX :D ERIV:V ERB P REFIX :D ERIV:V ERB P REFIX :D ERIV:V ERB P REFIX :D ERIV P REFIX S EGMENT Ent de ROOT:N OUN ROOT:N OUN ROOT:N OUN ROOT ROOT S EGMENT eis frost S UFFIX :D ERIV:N OUN S UFFIX :D ERIV:N OUN S UFFIX :D E"
K15-1017,chrupala-etal-2008-learning,0,0.145492,"Missing"
K15-1017,W02-1001,0,0.168778,"7 Un. Data al. (2009) introduces a Bayesian state-space model with corpus-wide priors. The model resembles a semi-CRF, but dynamic programming is no longer possible due to the priors. They employ the threestate tagset of Creutz and Lagus (2004) (row 1 in Figure 2) for Arabic and Hebrew UMS. Their gradient and objective computation is based on an enumeration of a heuristically chosen subset of the exponentially many segmentations. This limits its applicability to language with complex concatenative morphology, e.g., Turkish and Finnish. Ruokolainen et al. (2013) present an averaged perceptron (Collins, 2002), a discriminative structured prediction method, for UMS. The model outperforms the semi-supervised model of Poon et al. (2009) on Arabic and Hebrew morpheme segmentation as well as the semi-supervised model of Kohonen et al. (2010a) on English, Finnish and Turkish. Finally, Ruokolainen et al. (2014) get further consistent improvements by using features extracted from large corpora, based on the letter successor variety (LSV) model (Harris, 1995) and on unsupervised segmentation models such as Morfessor CatMAP (Creutz et al., 2007a). The idea behind LSV is that for example talking should be sp"
K15-1017,W02-0603,0,0.824231,"present memory-based approaches to discriminative learning of morphological segmentation. This is the previous work most similar to our work. They address the problem of LMS. We distinguish our work from theirs in that we define a cross-lingual schema for defining a hierarchical tagset for LMS. Morever, we tackle the problem with a feature-rich log-linear model, allowing us to easily incorporate disparate sources of knowledge into a single framework, as we show in our extensive evaluation. UMS has been mainly addressed by unsupervised algorithms. L INGUISTICA (Goldsmith, 2001) and M ORFESSOR (Creutz and Lagus, 2002) are built around an idea of optimally encoding the data, in the sense of minimal description length (MDL). M ORFESSOR C AT-MAP (Creutz et al., 2007a) formulates the model as sequence prediction based on HMMs over a morph dictionary and MAP estimation. The model also attempts to induce basic morphotactic categories (P REFIX, ROOT, S UFFIX). Kohonen et al. (2010a,b) and Grönroos et al. (2014) present variations of M OR FESSOR for semi-supervised learning. Poon et 4 A good example of such a resource is en.wiktionary.org/wiki/Category:Turkish_suffixes. 167 Un. Data al. (2009) introduces a Bayesia"
K15-1017,W04-0106,0,0.110462,"ence prediction based on HMMs over a morph dictionary and MAP estimation. The model also attempts to induce basic morphotactic categories (P REFIX, ROOT, S UFFIX). Kohonen et al. (2010a,b) and Grönroos et al. (2014) present variations of M OR FESSOR for semi-supervised learning. Poon et 4 A good example of such a resource is en.wiktionary.org/wiki/Category:Turkish_suffixes. 167 Un. Data al. (2009) introduces a Bayesian state-space model with corpus-wide priors. The model resembles a semi-CRF, but dynamic programming is no longer possible due to the priors. They employ the threestate tagset of Creutz and Lagus (2004) (row 1 in Figure 2) for Arabic and Hebrew UMS. Their gradient and objective computation is based on an enumeration of a heuristically chosen subset of the exponentially many segmentations. This limits its applicability to language with complex concatenative morphology, e.g., Turkish and Finnish. Ruokolainen et al. (2013) present an averaged perceptron (Collins, 2002), a discriminative structured prediction method, for UMS. The model outperforms the semi-supervised model of Poon et al. (2009) on Arabic and Hebrew morpheme segmentation as well as the semi-supervised model of Kohonen et al. (201"
K15-1017,W05-0701,0,0.101986,"Missing"
K15-1017,P08-1115,0,0.064437,"s: (i) morphological segmentation, (ii) stemming and (iii) morphological tag classification. For morphological segmentation our method shows absolute improvements of 2-6 points F1 over a strong baseline. 1 Introduction Morphological processing is often an overlooked problem since many well-studied languages (e.g., Chinese and English) are morphologically impoverished. But for languages with complex morphology (e.g., Finnish and Turkish) morphological processing is essential. A specific form of morphological processing, morphological segmentation, has shown its utility for machine translation (Dyer et al., 2008), sentiment analysis (AbdulMageed et al., 2014), bilingual word alignment (Eyigöz et al., 2013), speech processing (Creutz et al., 2007b) and keyword spotting (Narasimhan et al., 2014), inter alia. We advance the state-of-theart in supervised morphological segmentation by describing a high-performance, data-driven tool for handling complex morphology, even in lowresource settings. In this work, we make the distinction between unlabeled morphological segmentation (UMS ) (often just called “morphological segmentation”) and labeled morphological segmentation (LMS). The labels in our supervised di"
K15-1017,D13-1032,1,0.778252,"y predicting LMS and then discarding the labels. Our primary baseline is the state-of-the-art supervised system CRF-M ORPH of Ruokolainen et al. (2013). We ran the version of the system that the authors published on their website.8 We optimized the model’s two hyperparameters on Tune: the number of epochs and the maximal length of ngram character features. The system also supports Harris’s letter successor variety (LSV) features (Section 5), extracted from large unannotated corpora, our second baseline. For completeness, we also compare C HIPMUNK with a first-order CRF and a higher-order CRF (Müller et al., 2013), both used the same n-gram features as CRF-M ORPH, but without the LSV features.9 We evaluate all models using the traditional macro F1 of the segmentation boundaries. Discussion. The UMS results on held-out data are displayed in Table 5. Our most complex model beats the best baseline by between 1 (German) and 3 (Finnish) points F1 on all six languages. We additionally provide extensive ablation studies to highlight the contribution of our novel features. We find that the properties of each specific language highly influences which features are most effective. For the agglutinative languages,"
K15-1017,N13-1004,0,0.0261228,"For morphological segmentation our method shows absolute improvements of 2-6 points F1 over a strong baseline. 1 Introduction Morphological processing is often an overlooked problem since many well-studied languages (e.g., Chinese and English) are morphologically impoverished. But for languages with complex morphology (e.g., Finnish and Turkish) morphological processing is essential. A specific form of morphological processing, morphological segmentation, has shown its utility for machine translation (Dyer et al., 2008), sentiment analysis (AbdulMageed et al., 2014), bilingual word alignment (Eyigöz et al., 2013), speech processing (Creutz et al., 2007b) and keyword spotting (Narasimhan et al., 2014), inter alia. We advance the state-of-theart in supervised morphological segmentation by describing a high-performance, data-driven tool for handling complex morphology, even in lowresource settings. In this work, we make the distinction between unlabeled morphological segmentation (UMS ) (often just called “morphological segmentation”) and labeled morphological segmentation (LMS). The labels in our supervised discriminative model 1 Terminological notes: We use root to refer to a morpheme with concrete mea"
K15-1017,D14-1095,0,0.0748928,"1 over a strong baseline. 1 Introduction Morphological processing is often an overlooked problem since many well-studied languages (e.g., Chinese and English) are morphologically impoverished. But for languages with complex morphology (e.g., Finnish and Turkish) morphological processing is essential. A specific form of morphological processing, morphological segmentation, has shown its utility for machine translation (Dyer et al., 2008), sentiment analysis (AbdulMageed et al., 2014), bilingual word alignment (Eyigöz et al., 2013), speech processing (Creutz et al., 2007b) and keyword spotting (Narasimhan et al., 2014), inter alia. We advance the state-of-theart in supervised morphological segmentation by describing a high-performance, data-driven tool for handling complex morphology, even in lowresource settings. In this work, we make the distinction between unlabeled morphological segmentation (UMS ) (often just called “morphological segmentation”) and labeled morphological segmentation (LMS). The labels in our supervised discriminative model 1 Terminological notes: We use root to refer to a morpheme with concrete meaning, stem to refer to the concatenation of all roots and derivational affixes, root dete"
K15-1017,J01-2001,0,0.672752,"s (1999) and Marsi et al. (2005) present memory-based approaches to discriminative learning of morphological segmentation. This is the previous work most similar to our work. They address the problem of LMS. We distinguish our work from theirs in that we define a cross-lingual schema for defining a hierarchical tagset for LMS. Morever, we tackle the problem with a feature-rich log-linear model, allowing us to easily incorporate disparate sources of knowledge into a single framework, as we show in our extensive evaluation. UMS has been mainly addressed by unsupervised algorithms. L INGUISTICA (Goldsmith, 2001) and M ORFESSOR (Creutz and Lagus, 2002) are built around an idea of optimally encoding the data, in the sense of minimal description length (MDL). M ORFESSOR C AT-MAP (Creutz et al., 2007a) formulates the model as sequence prediction based on HMMs over a morph dictionary and MAP estimation. The model also attempts to induce basic morphotactic categories (P REFIX, ROOT, S UFFIX). Kohonen et al. (2010a,b) and Grönroos et al. (2014) present variations of M OR FESSOR for semi-supervised learning. Poon et 4 A good example of such a resource is en.wiktionary.org/wiki/Category:Turkish_suffixes. 167"
K15-1017,W04-3236,0,0.00874839,"ish. Finally, Ruokolainen et al. (2014) get further consistent improvements by using features extracted from large corpora, based on the letter successor variety (LSV) model (Harris, 1995) and on unsupervised segmentation models such as Morfessor CatMAP (Creutz et al., 2007a). The idea behind LSV is that for example talking should be split into talk and ing, because talk can also be followed by different letters then i such as e (talked) and s (talks). Chinese word segmentation (CWS) is related to UMS. Andrew (2006) successfully apply semiCRFs to CWS. The problem of joint CWS and POS tagging (Ng and Low, 2004; Zhang and Clark, 2008) is related to LMS. To our knowledge, joint CWS and POS tagging has not been addressed by a simple single semi-CRF, possibly because POS tagsets typically used in Chinese treebanks are much bigger than our morphotactic tagsets and the morphological poverty of Chinese makes higher-order models necessary and the direct application of semi-CRFs infeasible. 6 English Finnish German Indonesian Turkish Zulu 878k 2,928k 2,338k 88k 617k 123k Train+Tune+Dev Train Tune Dev 800 100 100 800 100 100 800 100 100 800 100 100 800 100 100 800 100 100 Test 694 835 751 2500 763 9040 Table"
K15-1017,C14-1111,0,0.0697741,"ate disparate sources of knowledge into a single framework, as we show in our extensive evaluation. UMS has been mainly addressed by unsupervised algorithms. L INGUISTICA (Goldsmith, 2001) and M ORFESSOR (Creutz and Lagus, 2002) are built around an idea of optimally encoding the data, in the sense of minimal description length (MDL). M ORFESSOR C AT-MAP (Creutz et al., 2007a) formulates the model as sequence prediction based on HMMs over a morph dictionary and MAP estimation. The model also attempts to induce basic morphotactic categories (P REFIX, ROOT, S UFFIX). Kohonen et al. (2010a,b) and Grönroos et al. (2014) present variations of M OR FESSOR for semi-supervised learning. Poon et 4 A good example of such a resource is en.wiktionary.org/wiki/Category:Turkish_suffixes. 167 Un. Data al. (2009) introduces a Bayesian state-space model with corpus-wide priors. The model resembles a semi-CRF, but dynamic programming is no longer possible due to the priors. They employ the threestate tagset of Creutz and Lagus (2004) (row 1 in Figure 2) for Arabic and Hebrew UMS. Their gradient and objective computation is based on an enumeration of a heuristically chosen subset of the exponentially many segmentations. Th"
K15-1017,N09-1024,0,0.0623345,"dynamic programming is no longer possible due to the priors. They employ the threestate tagset of Creutz and Lagus (2004) (row 1 in Figure 2) for Arabic and Hebrew UMS. Their gradient and objective computation is based on an enumeration of a heuristically chosen subset of the exponentially many segmentations. This limits its applicability to language with complex concatenative morphology, e.g., Turkish and Finnish. Ruokolainen et al. (2013) present an averaged perceptron (Collins, 2002), a discriminative structured prediction method, for UMS. The model outperforms the semi-supervised model of Poon et al. (2009) on Arabic and Hebrew morpheme segmentation as well as the semi-supervised model of Kohonen et al. (2010a) on English, Finnish and Turkish. Finally, Ruokolainen et al. (2014) get further consistent improvements by using features extracted from large corpora, based on the letter successor variety (LSV) model (Harris, 1995) and on unsupervised segmentation models such as Morfessor CatMAP (Creutz et al., 2007a). The idea behind LSV is that for example talking should be split into talk and ing, because talk can also be followed by different letters then i such as e (talked) and s (talks). Chinese"
K15-1017,W13-3504,0,0.276986,"h Zulu Table 2: Sizes of the various affix gazetteers. 4 119,839 6,690,417 364,564 35,269 80,261 73,525 Table 3: Number of words covered by the respective ASPELL dictionary Features features that fire if a segment is valid for a given spell checker. Spell-check features function effectively as a proxy for a “root detector”. We use the open-source ASPELL dictionaries as they are freely available in 91 languages. Table 3 shows the coverage of these dictionaries. Integrating the Features. Our model uses the features discussed in this section and additionally the simple n-gram context features of Ruokolainen et al. (2013). The n-gram features look at variable length substrings of the word on both the right and left side of each potential boundary. We create conjunctive features from the cross-product between the morphotactic tagset (Section 2) and the features. We introduce several novel features for LMS. We exploit existing resources, e.g., spell checkers and Wiktionary, to create straightforward and effective features and we incorporate ideas from related areas: named-entity recognition (NER) and morphological tagging. Affix Features and Gazetteers. In contrast to syntax and semantics, the morphology of a la"
K15-1017,W10-2210,0,0.124893,"llowing us to easily incorporate disparate sources of knowledge into a single framework, as we show in our extensive evaluation. UMS has been mainly addressed by unsupervised algorithms. L INGUISTICA (Goldsmith, 2001) and M ORFESSOR (Creutz and Lagus, 2002) are built around an idea of optimally encoding the data, in the sense of minimal description length (MDL). M ORFESSOR C AT-MAP (Creutz et al., 2007a) formulates the model as sequence prediction based on HMMs over a morph dictionary and MAP estimation. The model also attempts to induce basic morphotactic categories (P REFIX, ROOT, S UFFIX). Kohonen et al. (2010a,b) and Grönroos et al. (2014) present variations of M OR FESSOR for semi-supervised learning. Poon et 4 A good example of such a resource is en.wiktionary.org/wiki/Category:Turkish_suffixes. 167 Un. Data al. (2009) introduces a Bayesian state-space model with corpus-wide priors. The model resembles a semi-CRF, but dynamic programming is no longer possible due to the priors. They employ the threestate tagset of Creutz and Lagus (2004) (row 1 in Figure 2) for Arabic and Hebrew UMS. Their gradient and objective computation is based on an enumeration of a heuristically chosen subset of the expon"
K15-1017,E14-4017,0,0.108741,". Their gradient and objective computation is based on an enumeration of a heuristically chosen subset of the exponentially many segmentations. This limits its applicability to language with complex concatenative morphology, e.g., Turkish and Finnish. Ruokolainen et al. (2013) present an averaged perceptron (Collins, 2002), a discriminative structured prediction method, for UMS. The model outperforms the semi-supervised model of Poon et al. (2009) on Arabic and Hebrew morpheme segmentation as well as the semi-supervised model of Kohonen et al. (2010a) on English, Finnish and Turkish. Finally, Ruokolainen et al. (2014) get further consistent improvements by using features extracted from large corpora, based on the letter successor variety (LSV) model (Harris, 1995) and on unsupervised segmentation models such as Morfessor CatMAP (Creutz et al., 2007a). The idea behind LSV is that for example talking should be split into talk and ing, because talk can also be followed by different letters then i such as e (talked) and s (talks). Chinese word segmentation (CWS) is related to UMS. Andrew (2006) successfully apply semiCRFs to CWS. The problem of joint CWS and POS tagging (Ng and Low, 2004; Zhang and Clark, 2008"
K15-1017,W10-2211,0,0.0842879,"from Wikipedia and the corpus of Krisnawati and Schulz (2013). Table 4 shows the important statistics of our datasets. In all evaluations, we use variants of the standard MorphoChallenge evaluation approach. Importantly, for word types with multiple correct segmentations, this involves finding the maximum score by comparing our hypothesized segmentation with each correct segmentation, as is standardly done in MorphoChallenge. Experiments We experimented on six languages from diverse language families. The segmentation data for English, Finnish and Turkish was taken from MorphoChallenge 2010 (Kurimo et al., 2010).5 Despite typically being used for UMS tasks, the MorphoChallenge datasets do contain morpheme level 6 https://github.com/desmond86/ Indonesian-English-Bilingual-Corpus 7 We used both Tune and Dev in order to both optimize hyperparameters on held-out data (Tune) and perform qualitative error analysis on separate held-out data (Dev). 5 http://research.ics.aalto.fi/events/ morphochallenge2010/ 168 CRF-M ORPH CRF-M ORPH +LSV First-order CRF Higher-order CRF C HIPMUNK C HIPMUNK +Morph C HIPMUNK +Affix C HIPMUNK +Dict C HIPMUNK +Dict,+Affix,+Morph English 83.23 84.45 84.66 84.66 84.40 83.27 83.81"
K15-1017,W06-2918,0,0.0380945,"resources, e.g., spell checkers and Wiktionary, to create straightforward and effective features and we incorporate ideas from related areas: named-entity recognition (NER) and morphological tagging. Affix Features and Gazetteers. In contrast to syntax and semantics, the morphology of a language is often simple to document and a list of the most common morphs can be found in any good grammar book. Wiktionary, for example, contains affix lists for all the six languages used in our experiments.4 Providing a supervised learner with such a list is a great boon, just as gazetteer features aid NER (Smith and Osborne, 2006)— perhaps even more so since suffixes and prefixes are generally closed-class; hence these lists are likely to be comprehensive. These features are binary and fire if a given substring occurs in the gazetteer list. In this paper, we simply use suffix lists from English Wiktionary, except for Zulu, for which we use a prefix list, see Table 2. We also include a feature that fires on the conjunction of tags and substrings observed in the training data. In the level 5 tagset this allows us to link all allomorphs of a given morpheme. In the lower level tagsets, this links related morphemes. Virpioj"
K15-1017,C10-1115,0,0.0740834,"Missing"
K15-1017,P99-1037,0,0.623548,"Missing"
K15-1017,P08-1101,0,0.0645359,"olainen et al. (2014) get further consistent improvements by using features extracted from large corpora, based on the letter successor variety (LSV) model (Harris, 1995) and on unsupervised segmentation models such as Morfessor CatMAP (Creutz et al., 2007a). The idea behind LSV is that for example talking should be split into talk and ing, because talk can also be followed by different letters then i such as e (talked) and s (talks). Chinese word segmentation (CWS) is related to UMS. Andrew (2006) successfully apply semiCRFs to CWS. The problem of joint CWS and POS tagging (Ng and Low, 2004; Zhang and Clark, 2008) is related to LMS. To our knowledge, joint CWS and POS tagging has not been addressed by a simple single semi-CRF, possibly because POS tagsets typically used in Chinese treebanks are much bigger than our morphotactic tagsets and the morphological poverty of Chinese makes higher-order models necessary and the direct application of semi-CRFs infeasible. 6 English Finnish German Indonesian Turkish Zulu 878k 2,928k 2,338k 88k 617k 123k Train+Tune+Dev Train Tune Dev 800 100 100 800 100 100 800 100 100 800 100 100 800 100 100 800 100 100 Test 694 835 751 2500 763 9040 Table 4: Dataset sizes (numbe"
K17-2001,E14-1060,1,0.855884,"Missing"
K17-2001,P17-1136,0,0.0562959,"Missing"
K17-2001,N15-1107,1,0.861491,"Missing"
K17-2001,chrupala-etal-2008-learning,0,0.152497,"Missing"
K17-2001,W16-2004,0,0.0654088,"Missing"
K17-2001,P14-2102,1,0.910119,"Missing"
K17-2001,Q15-1031,1,0.919531,"Missing"
K17-2001,P16-1156,1,0.87299,"Missing"
K17-2001,K17-2002,0,0.132442,"Missing"
K17-2001,E17-2120,1,0.859022,"Missing"
K17-2001,E17-1049,1,0.894341,"Missing"
K17-2001,N07-1048,0,0.21796,"Missing"
K17-2001,P17-1182,1,0.876831,"Missing"
K17-2001,D09-1011,1,0.888474,"Missing"
K17-2001,D08-1113,1,0.874358,"Missing"
K17-2001,P16-2090,0,0.46072,"Missing"
K17-2001,N13-1138,0,0.147728,"Missing"
K17-2001,P08-1115,0,0.089787,"Missing"
K17-2001,L16-1498,1,0.792213,"Missing"
K17-2001,N16-1077,1,0.812854,"Missing"
K17-2001,W10-2211,0,0.123726,"Missing"
K17-2001,W16-2006,0,0.0673676,"Missing"
K17-2001,P82-1020,0,0.75423,"Missing"
K17-2001,P08-1103,0,0.0541683,"Missing"
K17-2001,D15-1272,1,0.92636,"Missing"
K17-2001,D14-1095,0,0.0938069,"Missing"
K17-2001,K17-2011,0,0.0350181,"Missing"
K17-2001,N15-1093,0,0.19098,"Missing"
K17-2001,K17-2008,0,0.055504,"Missing"
K17-2001,K17-2010,0,0.158704,"Missing"
K17-2001,W16-2007,0,\N,Missing
K17-2001,L16-1497,1,\N,Missing
K17-2001,K17-2012,0,\N,Missing
K17-2001,K17-2003,0,\N,Missing
K17-2001,P17-1029,0,\N,Missing
K18-1021,W16-1614,0,0.151595,"Missing"
K18-1021,E14-1049,0,0.504024,"vector spaces indeed reported superior performance with linear models as compared to non-linear neural approaches (Mikolov et al., 2013). The relative success of the simple linear approach can be explained in terms of isomorphism across monolingual semantic spaces,1 an idea that receives support from cognitive science (Youn et al., 1999). Word vector spaces are not perfectly isomorphic, however, as shown by Søgaard et al. (2018), who use a Laplacian graph similarity metric to measure this property. In this work, we show that projecting both source and target vector spaces into a third space (Faruqui and Dyer, 2014), using a variant of PA known as Generalized Procrustes Analysis (Gower, 1975), makes it easier to learn the alignment between two word vector spaces, as compared to the single linear transform used in Conneau et al. (2018). Most recent approaches to bilingual dictionary induction find a linear alignment between the word vector spaces of two languages. We show that projecting the two languages onto a third, latent space, rather than directly onto each other, while equivalent in terms of expressivity, makes it easier to learn approximate alignments. Our modified approach also allows for support"
K18-1021,N15-1028,0,0.0274487,"nality, as studied in Mimno and Thompson (2017), increases with corpus size, so we would expect embedding spaces learned from corpora comparable in size, to also be more similar in shape. 6 Related work Bilingual embeddings Many diverse crosslingual word embedding models have been proposed (Ruder et al., 2018). The most popular kind learns a linear transformation from source to target language space (Mikolov et al., 2013). In most recent work, this mapping is constrained to be orthogonal and solved using Procrustes Analysis (Xing et al., 2015; Artetxe et al., 2017, 2018; Conneau et al., 2018; Lu et al., 2015). The approach most similar to ours, Faruqui and Dyer (2014), uses canonical correlation analysis (CCA) to project both source and target language spaces into a third, joint space. In this setup, similarly to GPA, the third space is iteratively updated, such that at timestep t, it is a product of the two language spaces as transformed by the mapping learned at timestep t − 1. The objective that drives the updates of the mapping matrices is to maximize the correlation between the projected embeddings of translational equivalents (where the latter are taken from a gold-standard seed dictionary)."
K18-1021,D17-1308,0,0.0306298,"w-resource languages do not necessarily have lower Procrustes fits than high-resource ones (compare Estonian and Finnish, for example), the gap between the Procrustes fit and GPA precision is on average much higher within low-resource languages than within high-resource ones (52.4610 compared to 25.47, respectively). This finding is in line with the common understanding that the quality of distributional word vectors depends on the amount of data available—we can infer from these results that suboptimal embeddings results in suboptimal cross-lingual alignments. 5.3 rectionality, as studied in Mimno and Thompson (2017), increases with corpus size, so we would expect embedding spaces learned from corpora comparable in size, to also be more similar in shape. 6 Related work Bilingual embeddings Many diverse crosslingual word embedding models have been proposed (Ruder et al., 2018). The most popular kind learns a linear transformation from source to target language space (Mikolov et al., 2013). In most recent work, this mapping is constrained to be orthogonal and solved using Procrustes Analysis (Xing et al., 2015; Artetxe et al., 2017, 2018; Conneau et al., 2018; Lu et al., 2015). The approach most similar to"
K18-1021,P17-1042,0,0.683002,"ment between the word vector spaces of two languages. We show that projecting the two languages onto a third, latent space, rather than directly onto each other, while equivalent in terms of expressivity, makes it easier to learn approximate alignments. Our modified approach also allows for supporting languages to be included in the alignment process, to obtain an even better performance in low resource settings. 1 Introduction Several papers recently demonstrated the potential of very weakly supervised or entirely unsupervised approaches to bilingual dictionary induction (BDI) (Barone, 2016; Artetxe et al., 2017; Zhang et al., 2017; Conneau et al., 2018; Søgaard et al., 2018), the task of identifying translational equivalents across two languages. These approaches cast BDI as a problem of aligning monolingual word embeddings. Pairs of monolingual word vector spaces can be aligned without any explicit crosslingual supervision, solely based on their distributional properties (for an adversarial approach, see Conneau et al. (2018)). Alternatively, weak supervision can be provided in the form of numerals (Artetxe et al., 2017) or identically spelled words (Søgaard et al., 2018). Successful unsupervised o"
K18-1021,P18-1072,1,0.852776,"Missing"
K18-1021,D17-1270,0,0.108811,"Missing"
K18-1021,N15-1104,0,0.607491,"GPA, which aligns both source and target spaces with a third, latent space, constructed by averaging over the two language spaces. (1) thus minimizing the trace between each two corresponding rows of the transformed space T E and F . We build E and F based on a seed dictionary of N entries, such that each pair of corresponding rows in E and F , (en , fn ) for n = 1, . . . , N consists of the embeddings of a translational pair of words. In order to preserve the monolingual quality of the transformed embeddings, it is beneficial to use an orthogonal matrix T for cross-lingual mapping purposes (Xing et al., 2015; Artetxe et al., 2017). Conveniently, the orthogonal Procrustes problem has an analytical solution, based on Singular Value Decomposition (SVD): F > E = U ΣV > T = V U> 3 For an analytical solution to GPA, we compute the average of the embedding matrices E1...k after transformation by T1...k : G=k {T1 ,...,Tk } k X ||Ti Ei − Tj Ej )||2 Ei Ti (4) thus obtaining a latent space, G, which captures properties of each of E1...k , and potentially additional properties emerging from the combination of the spaces. On the very first iteration, prior to having any estimates of T1...k , we set G = Ei for"
K18-1021,P17-1179,0,0.141472,"Missing"
K18-1021,J13-3004,0,\N,Missing
K18-3001,K18-3001,1,0.103672,"Missing"
K18-3001,P16-2090,1,0.838493,"Missing"
K18-3001,K17-2003,1,0.836665,"Missing"
K18-3001,K17-2010,1,0.734971,"Missing"
K18-3001,W18-6011,1,0.913422,"and a target UniMorph sentence is shown in Figure 3. Since the selection of languages in task 2 is small and we do not attempt to correct annotation errors in the UD source materials, conversion between UD and UniMorph morphosyntactic descriptions is generally straightforward.11 However, UD descriptions are more fine-grained than their UniMorph equivalents. For example, UD denotes lexical features such as noun gender which are inherent features of a lexeme possessed by all of its word forms. Such inherent features are missing from UniMorph which exclusively annotates inflectional morphology (McCarthy et al., 2018). Therefore, UD fea$ → sta$ ti$ → dista$ koti$ → kodista$ i$ → ista$ oti$ → odista$ Such rules are then extracted from each example inflection in the training data. At generation time, the longest matching left hand side of a rule is identified and applied to the citation form. For example, if the Finnish noun luoti ‘bullet’ were to be inflected in the elative (N;IN+ABL;SG) using only the extracted rules given above, the transformation oti$ → odista$ would be triggered, producing the output luodista. In case there are multiple candidate rules of equally long left hand sides that all match, tie"
K18-3001,K18-3012,0,0.30177,"Missing"
K18-3001,K18-3015,0,0.276525,"Missing"
K18-3001,P15-2111,1,\N,Missing
K18-3001,K17-2002,1,\N,Missing
K18-3001,K17-3001,0,\N,Missing
K18-3001,W17-4110,1,\N,Missing
K18-3001,N18-2087,1,\N,Missing
K18-3001,P18-1245,1,\N,Missing
K18-3001,L18-1293,1,\N,Missing
K18-3001,K18-3004,0,\N,Missing
K18-3001,K18-3010,0,\N,Missing
K18-3001,K18-3013,0,\N,Missing
K18-3001,K18-3003,0,\N,Missing
K18-3001,K18-3016,0,\N,Missing
K18-3001,K18-3005,0,\N,Missing
K18-3001,W16-2006,0,\N,Missing
K18-3001,K17-2008,1,\N,Missing
K18-3001,K17-2005,0,\N,Missing
K18-3001,K18-3008,0,\N,Missing
K18-3001,K18-3007,0,\N,Missing
K19-1014,P17-1183,0,0.155657,"Missing"
K19-1014,D16-1162,0,0.0150884,"rn three sub-categories of target errors. 3 This label is applied regardless of whether the predicted inﬂected form is correct or not, and therefore is independent of system predictions. Furthermore, it is possible that both the gold data and prediction have the same incorrect inﬂected form, but detecting such cases is challenging. Silly errors This category consists of those “bizarre” errors which defy any purely linguistic characterization. In addition to the aforementioned case of *membled, such errors have also been reported for other language generation tasks such as machine translation (Arthur et al. 2016) and text normalization (Gorman and Sproat 2016, Sproat and Jaitly 2017, Zhang et al. 2019). Allomorphy errors This category consists of those errors which are characterized by misapplication of existing (i.e., independently attested) allomorphic patterns in the target language. Our annotation scheme recognizes four sub-categories of allomorphy error, but we set aside their their description for reasons of space. Spelling errors This category includes inﬂected forms that do not follow language-speciﬁc orthographic conventions but are otherwise correct. 4 Results We performed full error annotat"
K19-1014,J08-4004,0,0.048686,"ined to produce reliable annotations. 142 speakers; the remaining eight were annotated by second-language speakers. In addition to the annotation guidelines, annotators were encouraged to consult authoritative dictionaries and reference grammars—such as the Iso suomen kielioppi (Hakulinen et al. 2008) for Finnish, the Duden for German, the Oxford Latin Dictionary (Lee 1968), or the Diccionario de la lengua española for Spanish—and native speakers. Table 2 reports summary statistics for fully-annotated languages. 4.1 Inter-annotator agreement Table 3 provides raw agreement and Krippendorf’s α (Artstein and Poesio 2008) for those languages known to two annotators. As mentioned above, each annotator is a specialist in computational linguistics, and annotated at least one other language as well. Raw agreement is high, and while chancecorrected agreement statistics like α are notoriously difﬁcult to interpret, α ≥ 0.8, a threshold obtained for all three double-coded languages, is generally considered to indicate substantial reliability (Krippendorff 2004:241f.). 4.2 Errors Table 4 provides the counts of the four major categories of error for all twelve languages and for both systems. We now proceed to describe"
K19-1014,K17-2002,0,0.080358,"Missing"
K19-1014,P19-1376,0,0.0500582,"errors in the Wiktionary data itself. SPELLING Free variation Extraction Wiktionary Figure 1: Overview of our annotation scheme, including subcategories. Annotators are instructed to proceed through the taxonomy from left to right. Pinker and Prince (1988) and Sproat (1992:216f.) dispute this characterization, pointing out bizarre errors like *membled for mailed. More recently, Kirov and Cotterell (2018) claim that modern neural network architectures—such as those used in the CoNLL–SIGMORPHON 2017 Shared Task— generalize reasonably well while largely eliminating these bizarre errors. However, Corkery et al. (2019) argue that the Kirov and Cotterell model predictions align poorly with human productions, and suggest that the reported results may be uncharacteristic due to fortuitous random seeding. We desired a somewhat richer set of errors than this prior work. The ﬁnal taxonomy— incorporating feedback from a ten-language pilot study—consists of four major error categories, with several additional sub-categories. The categories are applied sequentially, as in Figure 1. We now describe these categories. Target errors This category consists of cases where the gold data is incorrect or incomplete.3 We disc"
K19-1014,N13-1138,0,0.119163,"Missing"
K19-1014,W12-3105,0,0.0612779,"Missing"
K19-1014,Q16-1036,1,0.842948,"This label is applied regardless of whether the predicted inﬂected form is correct or not, and therefore is independent of system predictions. Furthermore, it is possible that both the gold data and prediction have the same incorrect inﬂected form, but detecting such cases is challenging. Silly errors This category consists of those “bizarre” errors which defy any purely linguistic characterization. In addition to the aforementioned case of *membled, such errors have also been reported for other language generation tasks such as machine translation (Arthur et al. 2016) and text normalization (Gorman and Sproat 2016, Sproat and Jaitly 2017, Zhang et al. 2019). Allomorphy errors This category consists of those errors which are characterized by misapplication of existing (i.e., independently attested) allomorphic patterns in the target language. Our annotation scheme recognizes four sub-categories of allomorphy error, but we set aside their their description for reasons of space. Spelling errors This category includes inﬂected forms that do not follow language-speciﬁc orthographic conventions but are otherwise correct. 4 Results We performed full error annotation on twelve of the 52 languages. Several othe"
K19-1014,Q13-1035,0,0.0583587,"Missing"
K19-1014,L18-1293,1,0.834532,"Missing"
K19-1014,L16-1498,0,0.140004,"Missing"
K19-1014,W10-2211,0,0.14519,"Missing"
K19-1014,W19-4226,1,0.788689,"undles and asked to produce the appropriate inﬂected forms. In sub-task 2, training data consists of complete inﬂectional paradigms, and at inference time, the system is asked to produce full paradigms for unseen lemmata. We focus on the results from subtask 1, primarily because only two of the twelve teams chose to compete in sub-task 2. However, the proposed error taxonomy could easily be applied to sub-task 2, or to later morphological generation challenges such as sub-task 2 of the CoNLL– SIGMORPHON 2018 shared task (Cotterell et al. 2018) or sub-task 1 of the SIGMORPHON 2019 shared task (McCarthy et al. 2019). 2.1.1 Data The data in both sub-tasks is primarily sampled from UniMorph (Kirov et al. 2016, 2018), a free morphological database. In turn, UniMorph data for our twelve languages is automatically extracted from Wiktionary, a collaborative multilingual online dictionary. UniMorph pairs the cells of Wiktionary morphological paradigms, which bear prose labels like “genitive plural”, to feature bundles in a language-independent morphological schema (Sylak-Glassman et al. 2015; also see Sylak-Glassman 2016). The data consist of the aforementioned triples of lemma, inﬂectional bundle, and inﬂected"
K19-1014,J11-4002,0,0.089003,"Missing"
K19-1014,J19-2004,1,0.83344,"predicted inﬂected form is correct or not, and therefore is independent of system predictions. Furthermore, it is possible that both the gold data and prediction have the same incorrect inﬂected form, but detecting such cases is challenging. Silly errors This category consists of those “bizarre” errors which defy any purely linguistic characterization. In addition to the aforementioned case of *membled, such errors have also been reported for other language generation tasks such as machine translation (Arthur et al. 2016) and text normalization (Gorman and Sproat 2016, Sproat and Jaitly 2017, Zhang et al. 2019). Allomorphy errors This category consists of those errors which are characterized by misapplication of existing (i.e., independently attested) allomorphic patterns in the target language. Our annotation scheme recognizes four sub-categories of allomorphy error, but we set aside their their description for reasons of space. Spelling errors This category includes inﬂected forms that do not follow language-speciﬁc orthographic conventions but are otherwise correct. 4 Results We performed full error annotation on twelve of the 52 languages. Several other languages were initially targeted for anno"
K19-1014,P16-1208,0,0.0460418,"Missing"
K19-1014,P15-2111,0,0.317018,"Missing"
L18-1293,E14-1060,1,0.914644,"Missing"
L18-1293,P16-1156,1,0.911216,"Missing"
L18-1293,N07-1048,0,0.136877,"Missing"
L18-1293,N13-1138,0,0.235978,"Missing"
L18-1293,P08-1115,0,0.0456449,"Missing"
L18-1293,N16-1077,1,0.880807,"Missing"
L18-1293,L16-1498,1,0.945257,"The Universal Morphology (UniMorph) project, centered at the Center for Language and Speech Processing (CLSP) at Johns Hopkins University is a collaborative effort to improve how NLP systems handle complex morphology across the world’s languages. The project releases annotated morphological data using a universal tagset, the UniMorph schema. Each inflected form is associated with a lemma, which typically carries its underlying lexical meaning, and a bundle of morphological features from our schema. Additional supporting data and tools are also released on a per-language basis when available. Kirov et al. (2016) introduced version 1.0 of the UniMorph morphological database, created by extracting and normalizing the inflectional paradigms included in Wiktionary (www.wiktionary.org), a large, broadly multi-lingual crowd-sourced collection of lexical data. This paper describes UniMorph 2.0. It details improvements in Wiktionary extraction and annotation, as well as normalization of non-Wiktionary resources, leading to a much higher quality morphological database. The new dataset spans 52 languages representing a range of language families. As in UniMorph 1.0, we provide paradigms from highlyinflected op"
L18-1293,D14-1095,0,0.116143,"Missing"
L18-1293,N15-1093,0,0.150689,"Missing"
L18-1293,Q15-1026,0,0.0836395,"Missing"
L18-1293,P15-2111,1,0.852807,"rsing and normalization of Wiktionary. Wiktionary is a broadly multilingual resource with many crowd-sourced morphological paradigms in the form of custom HTML tables. Figure 1 illustrates the challenge associated with extracting this data. Wiktionary is designed for human, rather than machine readability, and authors have extensive freedom in formatting data. This leads to wildly differing table layouts across languages which need to be converted to a consistent tabular format. The extraction process developed for UniMorph 1.0 relied heavily on statistical, visual, and positional heuristics (Sylak-Glassman et al., 2015b) to: 1. Determine which entries in an HTML table are inflected forms and which are grammatical descriptors. 2. Link each inflected form with its appropriate descriptors. 3. Convert each set of linked descriptors into a universal feature annotation schema, described in detail in Sylak-Glassman (2016).1 This led to a large dataset of 952,530 unique noun, verb, and adjective lemmas across 350 languages. Unfortunately, 1 pdf unimorph.github.io/doc/unimorph-schema. Figure 1: Paradigm extraction and normalization. the UniMorph 1.0 dataset was very error-prone due to the inability of our heuristics"
L18-1293,zeman-2008-reusable,0,0.0233658,"Each group represents a different type of paradigm (e.g., regular verb). For each group, a sample table was selected, and an annotator replaced each inflected form in the table with the appropriate UniMorph features. All annotation was compliant with the UniMorph Schema, which was designed to represent the full range of semantic distinctions that can be captured by inflectional morphology in any language (SylakGlassman et al., 2015a). The schema is similar in form and spirit to other tagset universalization efforts, such as the Universal Dependencies Project (Choi et al., 2015) and Interset (Zeman, 2008), but is designed specifically for typological completeness for inflectional morphology, including a focus on the morphology of especially low-resource languages. It includes over 200 base features distributed among 23 dimensions of meaning (i.e., morphological categories), including both common dimensions like tense and aspect as well as rarer dimensions like evidentiality and switch-reference. Despite the high coverage of the UniMorph tagset, for UniMorph 2.0, annotators were allowed to employ additional ‘language specific’ LGSPEC(1, 2, 3, etc.) features to mark any missing distinctions, or"
L18-1293,W16-2002,1,\N,Missing
N15-1094,P03-1006,0,0.882486,"tate acceptor: that is, any v ∈ Σ∗ has exactly one accepting path in A. For each arc or final state a in A, we can define a feature function “How 2 Provided that we include special n-grams that match at the boundaries of v. See Appendix B.2 for details. many times is a used when A accepts v?” Thus, f (v) is again a vector of non-negative counts. Section 6 gives algorithms for this general setting. We implement the previous section as a special case, constructing A so that its arcs essentially correspond to the substrings in W. This encodes a variable-order Markov model as an FSA similarly to (Allauzen et al., 2003); see Appendix B.4 for details. In this general setting, E NCODE(θ) just returns a weighted version of A where each arc or final state a has weight exp θa in the (+, ×) semiring. Thus, this WFSA accepts each v with weight exp(θ · f (v)). 3.5 Adaptive featurization How do we choose W (or A)? Expanding W will allow better approximations to p—but at greater computational cost. We would like W to include just the substrings needed to approximate a given p well. For instance, if p is concentrated on a few highprobability strings, then a good W might contain those full strings (with positive weights"
N15-1094,D07-1093,0,0.526368,"nite-state distributions by variable-order n-gram models. On phonological inference problems, we obtain substantial speedup over previous related algorithms with no significant loss in accuracy. 1 Introduction Graphical models are well-suited to reasoning about linguistic structure in the presence of uncertainty. Such models typically use discrete random variables, where each variable ranges over a finite set of values such as words or tags. But a variable can also be allowed to range over an infinite space of discrete structures—in particular, the set of all strings, a case first explored by Bouchard-Côté et al. (2007). This setting arises because human languages make use of many word forms. These strings are systematically related in their spellings due to linguistic processes such as morphology, phonology, abbreviation, copying error and historical change. To analyze or predict novel strings, we can model the joint distribution of many related strings at once. Under a graphical model, the joint probability of an assignment tuple is modeled as a product of potentials on sub-tuples, each of which is usually modeled in turn by a weighted finite-state machine. In general, we wish to infer the values of unknow"
N15-1094,P14-2102,1,0.824667,"Missing"
N15-1094,P06-1039,0,0.0876408,"Missing"
N15-1094,D09-1011,1,0.89159,"ain graphical models are equivalent to cascades of finite-state transducers, which have long been used to model stepwise derivational processes such as speech production (Pereira and Riley, 1997) and transliteration (Knight and Graehl, 1998). Treeshaped graphical models have been used to model the evolution and speciation of word forms, in order to reconstruct ancient languages (Bouchard-Côté et al., 2007; Bouchard-Côté et al., 2008) and discover cognates in related languages (Hall and Klein, 2010; Hall and Klein, 2011). Cyclic graphical models have been used to model morphological paradigms (Dreyer and Eisner, 2009; Dreyer and Eisner, 2011) and to reconstruct phonological underlying forms (Cotterell et al., 2015). All of these graphical models, except Dreyer’s, happen to be directed ones. And all of these papers, except Bouchard-Côté’s, use deterministic inference methods—based on BP. 2.1 Graphical models over strings A directed or undirected graphical model describes a joint probability distribution over a set of random variables. To perform inference given a setting of the model parameters and observations of some variables, it is convenient to construct a factor graph (Kschischang et al., 2001). A fa"
N15-1094,D11-1057,1,0.937489,"equivalent to cascades of finite-state transducers, which have long been used to model stepwise derivational processes such as speech production (Pereira and Riley, 1997) and transliteration (Knight and Graehl, 1998). Treeshaped graphical models have been used to model the evolution and speciation of word forms, in order to reconstruct ancient languages (Bouchard-Côté et al., 2007; Bouchard-Côté et al., 2008) and discover cognates in related languages (Hall and Klein, 2010; Hall and Klein, 2011). Cyclic graphical models have been used to model morphological paradigms (Dreyer and Eisner, 2009; Dreyer and Eisner, 2011) and to reconstruct phonological underlying forms (Cotterell et al., 2015). All of these graphical models, except Dreyer’s, happen to be directed ones. And all of these papers, except Bouchard-Côté’s, use deterministic inference methods—based on BP. 2.1 Graphical models over strings A directed or undirected graphical model describes a joint probability distribution over a set of random variables. To perform inference given a setting of the model parameters and observations of some variables, it is convenient to construct a factor graph (Kschischang et al., 2001). A factor graph is a finite bip"
N15-1094,P02-1001,1,0.690842,"to p) divided by the expected count of ab. Such expected substring counts can be found by the method of Allauzen et al. (2003). For general A, we can use the method sketched by Li et al. (2009, footnote 9): intersect the WFSA for p with the unweighted FSA A, and then run the forwardbackward algorithm to determine the posterior count of each arc in the result. This tells us the expected total number of traversals of each arc in A, if we have kept track of which arcs in the intersection of p with A were derived from which arcs in A. That bookkeeping can be handled with an expectation semiring (Eisner, 2002), or simply with backpointers. Gradient ascent. For any given θ, we can use the WFSAs p and E NCODE(θ) to exactly compute Ev∼p [log qθ (v)] = −H(p, qθ ) (Cortes et al., 2006). We can tune θ to globally maximize this objective. The technique is to intersect p with E NCODE(θ), after lifting their weights into the expectation semiring via the mappings k 7→ hk, 0i and k 7→ h0, log ki respectively. Summing over all paths of this intersection via the forward algorithm yields hZ, ri where Z is the normalizing constant for p. We also sum over paths of E NCODE(θ) to get the normalizing constant Zθ . No"
N15-1094,P10-1105,0,0.674977,"ssociation for Computational Linguistics 2 Background Graphical models over strings are in fairly broad use. Linear-chain graphical models are equivalent to cascades of finite-state transducers, which have long been used to model stepwise derivational processes such as speech production (Pereira and Riley, 1997) and transliteration (Knight and Graehl, 1998). Treeshaped graphical models have been used to model the evolution and speciation of word forms, in order to reconstruct ancient languages (Bouchard-Côté et al., 2007; Bouchard-Côté et al., 2008) and discover cognates in related languages (Hall and Klein, 2010; Hall and Klein, 2011). Cyclic graphical models have been used to model morphological paradigms (Dreyer and Eisner, 2009; Dreyer and Eisner, 2011) and to reconstruct phonological underlying forms (Cotterell et al., 2015). All of these graphical models, except Dreyer’s, happen to be directed ones. And all of these papers, except Bouchard-Côté’s, use deterministic inference methods—based on BP. 2.1 Graphical models over strings A directed or undirected graphical model describes a joint probability distribution over a set of random variables. To perform inference given a setting of the model par"
N15-1094,D11-1032,0,0.533468,"tional Linguistics 2 Background Graphical models over strings are in fairly broad use. Linear-chain graphical models are equivalent to cascades of finite-state transducers, which have long been used to model stepwise derivational processes such as speech production (Pereira and Riley, 1997) and transliteration (Knight and Graehl, 1998). Treeshaped graphical models have been used to model the evolution and speciation of word forms, in order to reconstruct ancient languages (Bouchard-Côté et al., 2007; Bouchard-Côté et al., 2008) and discover cognates in related languages (Hall and Klein, 2010; Hall and Klein, 2011). Cyclic graphical models have been used to model morphological paradigms (Dreyer and Eisner, 2009; Dreyer and Eisner, 2011) and to reconstruct phonological underlying forms (Cotterell et al., 2015). All of these graphical models, except Dreyer’s, happen to be directed ones. And all of these papers, except Bouchard-Côté’s, use deterministic inference methods—based on BP. 2.1 Graphical models over strings A directed or undirected graphical model describes a joint probability distribution over a set of random variables. To perform inference given a setting of the model parameters and observation"
N15-1094,D12-1105,0,0.0201737,"cally, they minimized D(µF →V τ ||qθ τ ), where τ was a simple fixed function (a 0-gram model) included so that they were working with distributions (see section 4.3). This is very similar to our (7). Indeed, Hall and Klein (2010) found their procedure “reminiscent of EP,” hinting that τ was a surrogate for a real µV →F term. Dreyer and Eisner (2009) had also suggested EP as future work. EP has been applied only twice before in the NLP community. Daumé III and Marcu (2006) used EP for query summarization (following Minka and Lafferty (2003)’s application to an LDA model with fixed topics) and Hall and Klein (2012) used EP for rich parsing. However, these papers inferred a single structured variable connected to all factors (as in the traditional presentation of EP—see Appendix A), rather than inferring many structured variables connected in a sparse graphical model. We regard EP as a generalization of loopy BP for just this setting: graphical models with large or unbounded variable domains. Of course, we are not the first to use such a scheme; e.g., Qi (2005, chapter 2) applies EP to linear-chain models with both continuous and discrete hidden states. We believe that EP should also be broadly useful in"
N15-1094,D09-1005,1,0.879236,"es et al., 2006). We can tune θ to globally maximize this objective. The technique is to intersect p with E NCODE(θ), after lifting their weights into the expectation semiring via the mappings k 7→ hk, 0i and k 7→ h0, log ki respectively. Summing over all paths of this intersection via the forward algorithm yields hZ, ri where Z is the normalizing constant for p. We also sum over paths of E NCODE(θ) to get the normalizing constant Zθ . Now the desired objective is r/Z − log Zθ . Its gradient with respect to θ can be found by back-propagation, or equivalently by the forward-backward algorithm (Li and Eisner, 2009). An overlarge gradient step can leave the feasible space (footnote 1) by driving ZθV to ∞ and thus driving (2) to ∞ (Dreyer, 2011, section 2.8.2). In this case, we try again with reduced stepsize. 6 This method always yields a probabilistic FSA, i.e., the arc weights are locally normalized probabilities. This does not sacrifice any expressiveness; see Appendix B.7 for discussion. 6.1 Optimizing θ with a penalty Now consider the penalized objective (3). Ideally, Ω(θ) would count the number of nonzero weights in θ—or better, the number of arcs in E NCODE(θ). But it is not known how to efficient"
N15-1094,P09-1067,1,0.927324,"features To obtain our family Q, we must design f . Our strategy is to choose a set of “interesting” substrings W. For each w ∈ W, define a feature function “How many times does w appear as a substring of v?” Thus, f (v) is simply a vector of counts (nonnegative integers), indexed by the substrings in W. A natural choice of W is the set of all n-grams for fixed n. In this case, Q turns out to be equivalent to the family of n-gram language models.2 Already in previous work (“variational decoding”), we used (2) with this family to approximate WFSAs or weighted hypergraphs that arose at runtime (Li et al., 2009). Yet a fixed n is not ideal. If W is the set of bigrams, one might do well to add the trigram the— perhaps because the is “really” a bigram (counting the digraph th as a single consonant), or because the bigram model fails to capture how common the is under p. Adding the to W ensures that qθ will now match p’s expected count for this trigram. Doing this should not require adding all |Σ|3 trigrams. By including strings of mixed lengths in W we get variable-order Markov models (Ron et al., 1996). 3.4 Arbitrary FSA-based features More generally, let A be any unambiguous and complete finite-state"
N15-1094,D11-1139,0,0.202669,"by distributions from a fixed family—e.g., by trigram models. Each message update is found by minimizing a certain KL-divergence (Minka, 2001a). Second, we generalize to variable-order models. To do this, we augment EP’s minimization problem with a novel penalty term that keeps the number of n-grams finite. In general, we advocate penalizing more “complex” messages (in our setting, large finite-state acceptors). Complex messages are slower to construct, and slower to use in later steps. Our penalty term is formally similar to regularizers that encourage structured sparsity (Bach et al., 2011; Martins et al., 2011). Like a regularizer, it lets us use a more expressive family of distributions, secure in the knowledge that we will use only as many of the parameters as we really need for a “pretty good” fit. But why avoid using more parameters? Regularization seeks better generalization by not overfitting the model to the data. By contrast, we already have a model and are merely doing inference. We seek better runtime by not over-fussing about capturing the model’s marginal distributions. Our “penalized EP” (PEP) inference strategy is applicable to any graphical model with complex messages. In this paper,"
N15-1094,D13-1024,0,0.11146,"owever, `1 would not recognize that θ is simpler with the features {ab, abc, abd} than with the features {ab, pqr, xyz}. The former leads to a smaller WFSA encoding. In other words, it is cheaper to add abd once abc is already present, as a state already exists that represents the context ab. We would thus like the penalty to be the number of distinct prefixes in the set of nonzero features, |{u ∈ Σ∗ : (∃x ∈ Σ∗ ) θux 6= 0}|, (8) as this is the number of ordinary arcs in E NCODE(θ) (see Appendix B.4). Its convex surrogate is def Ω(θ) = X sX u∈Σ∗ x∈Σ∗ 2 θux (9) This tree-structured group lasso (Nelakanti et al., 2013) is an instance of group lasso (Yuan and Lin, 2006) where the string w = abd belongs to four groups, corresponding to its prefixes u = , u = a, u = ab, u = abd. Under group lasso, moving θw away from 0 increases Ω(θ) by λ|θw |(just as in `1 ) for each group in which w is the only nonzero feature. This penalizes for the new WFSA arcs needed for these groups. There are also increases due to w’s other groups, but these are smaller, especially for groups with many strongly weighted features. Our objective (3) is now the sum of a differentiable convex function (2) and a particular nondifferentiabl"
N15-1094,N12-1024,1,0.943634,"Active set method, showing the infinite tree of all features for the alphabet Σ = {a, b}. The green nodes currently have non-zero weights. The yellow nodes are on the frontier and are allowed to become non-zero, but the penalty function is still keeping them at 0. The red nodes are not yet considered, forcing them to remain at 0. a nearby point that improves the non-differentiable term. The proximal operator for tree-structured group lasso (9) can be implemented with an efficient recursive procedure (Jenatton et al., 2011). What if θ is ∞-dimensional because we allow all n-grams as features? Paul and Eisner (2012) used just this feature set in a dual decomposition algorithm. Like them, we rely on an active set method (Schmidt and Murphy, 2010). We fix abcd’s weight at 0 until abc’s weight becomes nonzero (if ever);7 only then does feature abc become “active.” Thus, at a given step, we only have to compute the gradient with respect to the currently nonzero features (green nodes in Figure 2) and their immediate children (yellow nodes). This hierarchical inclusion technique ensures that we only consider a small, finite subset of all n-grams at any given iteration of optimization. Closed-form with greedy g"
N15-1094,P06-1055,0,0.107808,"Missing"
N15-1094,Q15-1031,1,\N,Missing
N15-1094,J98-4003,0,\N,Missing
N15-1140,N03-2002,0,0.0197962,"Missing"
N15-1140,A00-2013,0,0.0289825,"Missing"
N15-1140,2005.mtsummit-papers.11,0,0.0356163,"Missing"
N15-1140,W13-3512,0,0.357292,"is a noun in the nominative case and also both singular and feminine. Each tag is composed of meaningful sub-tag units that are shared across whole tags, e.g., the feature N OM fires on both adjectives and nouns. hoff (2003) introduced factored LMs, which effectively add tiers, allowing easy incorporation of morphological structure as well as part-of-speech (POS) tags. More recently, Müller and Schütze (2011) trained a class-based LM using common suffixes— often indicative of morphology—achieving stateof-the-art results when interpolated with a KneserNey LM. In neural probabilistic modeling, Luong et al. (2013) described a recursive neural network LM, whose topology was derived from the output of M ORFESSOR, an unsupervised morphological segmentation tool (Creutz and Lagus, 2005). Similarly, Qiu et al. (2014) augmented WORD 2 VEC (Mikolov et al., 2013) to embed morphs as well as whole words—also taking advantage of M ORFES SOR . LMs were tackled by dos Santos and Zadrozny (2014) with a convolutional neural network with a k-best max-pooling layer to extract character level n-grams, efficiently inserting orthographic features into the LM—use of the vectors in down-stream POS tagging achieved state-of-"
N15-1140,P11-2092,0,0.017703,"an accompanying English translation. Each word is annotated with a complex morphological tag and its corresponding coarsegrained POS tag. For instance, Stadt is annotated with N.N OM .S G .F EM indicating that it is a noun in the nominative case and also both singular and feminine. Each tag is composed of meaningful sub-tag units that are shared across whole tags, e.g., the feature N OM fires on both adjectives and nouns. hoff (2003) introduced factored LMs, which effectively add tiers, allowing easy incorporation of morphological structure as well as part-of-speech (POS) tags. More recently, Müller and Schütze (2011) trained a class-based LM using common suffixes— often indicative of morphology—achieving stateof-the-art results when interpolated with a KneserNey LM. In neural probabilistic modeling, Luong et al. (2013) described a recursive neural network LM, whose topology was derived from the output of M ORFESSOR, an unsupervised morphological segmentation tool (Creutz and Lagus, 2005). Similarly, Qiu et al. (2014) augmented WORD 2 VEC (Mikolov et al., 2013) to embed morphs as well as whole words—also taking advantage of M ORFES SOR . LMs were tackled by dos Santos and Zadrozny (2014) with a convolution"
N15-1140,C14-1015,0,0.0949489,"nd nouns. hoff (2003) introduced factored LMs, which effectively add tiers, allowing easy incorporation of morphological structure as well as part-of-speech (POS) tags. More recently, Müller and Schütze (2011) trained a class-based LM using common suffixes— often indicative of morphology—achieving stateof-the-art results when interpolated with a KneserNey LM. In neural probabilistic modeling, Luong et al. (2013) described a recursive neural network LM, whose topology was derived from the output of M ORFESSOR, an unsupervised morphological segmentation tool (Creutz and Lagus, 2005). Similarly, Qiu et al. (2014) augmented WORD 2 VEC (Mikolov et al., 2013) to embed morphs as well as whole words—also taking advantage of M ORFES SOR . LMs were tackled by dos Santos and Zadrozny (2014) with a convolutional neural network with a k-best max-pooling layer to extract character level n-grams, efficiently inserting orthographic features into the LM—use of the vectors in down-stream POS tagging achieved state-of-the-art results in Portuguese. Finally, most similar to our model, Botha and Blunsom (2014) introduced the additive logbilinear model (LBL++). Best summarized as a neural factored LM, the LBL++ created"
N15-1140,N01-1024,0,0.0883059,"tributes, e.g., case, gender and number, is also important. An example of an annotated German phrase is found in table 1. This often leads to a large tag set; e.g., in the morphological tag set of Hajiˇc (2000), English had 137 tags whereas morphologically-rich Czech had 970 tags! Clearly, much of the information needed to determine a word’s morphological tag is encoded in the word itself. For example, the suffix ed is generally indicative of the past tense in English. However, distributional similarity has also been shown to be an important cue for morphology (Yarowsky and Wicentowski, 2000; Schone and Jurafsky, 2001). Much as contextual signatures are reliably exploited approximations to the semantics of the lexicon (Harris, 1954)—you shall know the meaning of the word by the company it keeps (Firth, 1957)—they can be similarly exploited for morphological analysis. This is not an unexpected result—in German, e.g., we would expect nouns that follow an adjective in the genitive case to also be in the genitive case themselves. Much of what our model is designed to accomplish is the isolation of the components of the contextual signature that are indeed predictive of morphology. 3 Log-Bilinear Model The LBL i"
N15-1140,P10-1040,0,0.0151709,"out the tags for the subset of the data for which we do not have annotation. 5 In our evaluation, we attempt to intrinsically determine whether it is indeed true that words similar in the embedding space are morphologically related. Qualitative evaluation, shown in figure 1, indicates that this is the case. 5.1 Semi-Supervised Learning In the fully supervised case, the method we proposed above requires a corpus annotated with morphological tags to train. This conflicts with a key use case of word-embeddings—they allow the easy incorporation of large, unannotated corpora into supervised tasks (Turian et al., 2010). To resolve this, we train our model on a partially annotated corpus. The key idea here is that we only need a partial set of labeled data to steer the embeddings to ensure they 1289 Evaluation MorphoDist We introduce a new evaluation metric for morphologically-driven embeddings to quantitatively score models. Roughly, the question we want to evaluate is: are words that are similar in the embedded space also morphologically related? Given a word w and its embedding qw , let Mw be the set of morphological tags associated with w represented by bit vectors. This is a set because words may have s"
N15-1140,P00-1027,0,0.021624,"an, considering other nominal attributes, e.g., case, gender and number, is also important. An example of an annotated German phrase is found in table 1. This often leads to a large tag set; e.g., in the morphological tag set of Hajiˇc (2000), English had 137 tags whereas morphologically-rich Czech had 970 tags! Clearly, much of the information needed to determine a word’s morphological tag is encoded in the word itself. For example, the suffix ed is generally indicative of the past tense in English. However, distributional similarity has also been shown to be an important cue for morphology (Yarowsky and Wicentowski, 2000; Schone and Jurafsky, 2001). Much as contextual signatures are reliably exploited approximations to the semantics of the lexicon (Harris, 1954)—you shall know the meaning of the word by the company it keeps (Firth, 1957)—they can be similarly exploited for morphological analysis. This is not an unexpected result—in German, e.g., we would expect nouns that follow an adjective in the genitive case to also be in the genitive case themselves. Much of what our model is designed to accomplish is the isolation of the components of the contextual signature that are indeed predictive of morphology. 3"
N15-1140,petrov-etal-2012-universal,0,\N,Missing
N16-1076,P16-1231,0,0.0213724,"te must also record a count in [0, 3] of immediately preceding INS edits. No INS edit arc is allowed from a state with counter 3. The counter is not considered by the arc weight function. et al. (2015) recently augmented the sequence-tosequence framework with a continuous analog of stack and queue data structures, to better handle long-range dependencies often found in linguistic data. Some recent papers have used LSTMs or BiLSTMs, as we do, to define probability distributions over action sequences that operate directly on an input sequence. Such actions are aligned to the input. For example, Andor et al. (2016) score edit sequences using a globally normalized model, and Dyer et al. (2015) evaluate the local probability of a parsing action given past actions (and their result) and future words. These architectures are powerful because their LSTMs can examine the output structure; but as a result they do not permit dynamic programming and must fall back on beam search. Our use of dynamic programming for efficient exact inference has long been common in non-neural architectures for sequence transduction, including FST systems that allow “phrasal” replacements s : t where |s|, |t |> 1 (Chen, 2003; Jiamp"
N16-1076,W02-1001,0,0.0326163,"atives of p(y∗ |x) with respect to the arc weights, essentially by using the forward-backward algorithm. We backpropagate further from these partials to find the gradient of (2) with respect to all our model parameters. We describe our gradient-based maximization procedure in section 10.3, along with regularization. Our model does not have to be trained with the conditional log likelihood objective. It could be trained with other objectives such as empirical risk or softmax-margin (Li and Eisner, 2009; Gimpel and Smith, 2010), or with error-driven updates such as in the structured perceptron (Collins, 2002). 7 Inference and Decoding For a new input x at test time, we can now construct a weighted FST, G, that defines a probability distribution over all aligned output strings. This can be manipulated to make various predictions about y∗ and its alignment. In our present experiments, we find the most probable (highest-weighted) path in G (Dijkstra, 1959), ˆ as our prediction. Note and use its output string y that Dijkstra’s algorithm is exact; no beam search is required as in some neural sequence models. ˆ may not be the most probOn the other hand, y able string—extracting that from a weighted FST"
N16-1076,P14-2102,1,0.806242,"output character immediately before the edit s : t, so the state label h0 is the history before the next edit, namely the final character of ht. For edits other than DEL, ht is a bigram of y, which can be evaluated (in context) by the arc weight function w = f (s, t, h, h0 , x, i, j). Naturally, a weighted version of this FST F is far too simple to do well on real NLP tasks (as we show in our experiments). The magic comes from instead weighting G so that we can pay attention to the input context γi:j . The above choice of F corresponds to the “(0, 1, 1) topology” in the more general scheme of Cotterell et al. (2014). For practical reasons, we actually modify it to limit the number of consecutive INS edits to 3.5 This trick bounds |y |to be < 4 · (|x |+ 1), ensuring that the pathsums in section 6 are finite regardless of the model parameters. This simplifies both the pathsum algorithm and the gradient-based training (Dreyer, 2011). Less importantly, since G becomes acyclic, Dijkstra’s algorithm in section 7 simplifies to the Viterbi algorithm. 9 Related Work Our model adds to recent work on linguistic sequence transduction using deep learning. Graves and Schmidhuber (2005) combined BiLSTMs with HMMs. Late"
N16-1076,D08-1113,1,0.935575,"Missing"
N16-1076,P15-1030,0,0.0142159,"ons (and their result) and future words. These architectures are powerful because their LSTMs can examine the output structure; but as a result they do not permit dynamic programming and must fall back on beam search. Our use of dynamic programming for efficient exact inference has long been common in non-neural architectures for sequence transduction, including FST systems that allow “phrasal” replacements s : t where |s|, |t |> 1 (Chen, 2003; Jiampojamarn et al., 2007; Bisani and Ney, 2008). Our work augments these FSTs with neural networks, much as others have augmented CRFs. In this vein, Durrett and Klein (2015) augment a CRF parser (Finkel et al., 2008) to score constituents with a feedforward neural network. Likewise, FitzGerald et al. (2015) employ feedforward nets as a factor in a graphical model for semantic role labeling. Many CRFs have incorporated feedforward neural networks (Bridle, 1990; Peng et al., 2009; Do and Artieres, 2010; Vinel et al., 2011; Fujii et al., 2012; Chen et al., 2015, and others). Some work augments CRFs with BiLSTMs: Huang et al. (2015) report results on part-of-speech tagging and named entity recognition with a linear-chain CRF-BiLSTM, and Kong et al. (2015) on Chinese"
N16-1076,P15-1033,0,0.00564781,"edit arc is allowed from a state with counter 3. The counter is not considered by the arc weight function. et al. (2015) recently augmented the sequence-tosequence framework with a continuous analog of stack and queue data structures, to better handle long-range dependencies often found in linguistic data. Some recent papers have used LSTMs or BiLSTMs, as we do, to define probability distributions over action sequences that operate directly on an input sequence. Such actions are aligned to the input. For example, Andor et al. (2016) score edit sequences using a globally normalized model, and Dyer et al. (2015) evaluate the local probability of a parsing action given past actions (and their result) and future words. These architectures are powerful because their LSTMs can examine the output structure; but as a result they do not permit dynamic programming and must fall back on beam search. Our use of dynamic programming for efficient exact inference has long been common in non-neural architectures for sequence transduction, including FST systems that allow “phrasal” replacements s : t where |s|, |t |> 1 (Chen, 2003; Jiampojamarn et al., 2007; Bisani and Ney, 2008). Our work augments these FSTs with"
N16-1076,P02-1001,1,0.777915,"s exact inference through dynamic programming. We illustrate our method on the tasks of morphological reinflection and lemmatization. 1 2 Introduction Mapping one character sequence to another is a structured prediction problem that arises frequently in NLP and computational linguistics. Common applications include grapheme-to-phoneme (G2P), transliteration, vowelization, normalization, morphology, and phonology. The two sequences may have different lengths. Traditionally, such settings have been modeled with weighted finite-state transducers (WFSTs) with parametric edge weights (Mohri, 1997; Eisner, 2002). This requires manual design of the transducer states and the features extracted from those states. Alternatively, deep learning has recently been tried Notation and Background Let Σx be a discrete input alphabet and Σy be a discrete output alphabet. Our goal is to define a conditional distribution p(y |x) where x ∈ Σ∗x and y ∈ Σ∗y and x and y may be of different lengths. We use italics for characters and boldface for strings. xi denotes the ith character of x, and xi:j denotes the substring xi+1 xi+2 · · · xj of length j − i ≥ 0. Note that xi:i = ε, the empty string. Let n = |x|. Our approac"
N16-1076,N16-1077,0,0.239616,"mportantly, since G becomes acyclic, Dijkstra’s algorithm in section 7 simplifies to the Viterbi algorithm. 9 Related Work Our model adds to recent work on linguistic sequence transduction using deep learning. Graves and Schmidhuber (2005) combined BiLSTMs with HMMs. Later, “sequence-to-sequence” models were applied to machine translation by Sutskever et al. (2014) and to parsing by Vinyals et al. (2015). That framework did not model any alignment between x and y, but adding an “attention” mechanism provides a kind of soft alignment that has improved performance on MT (Bahdanau et al., 2015). Faruqui et al. (2016) apply these methods to morphological reinflection (the only other application to morphology we know of). Grefenstette 5 This multiplies the number of states 4-fold, since each state must also record a count in [0, 3] of immediately preceding INS edits. No INS edit arc is allowed from a state with counter 3. The counter is not considered by the arc weight function. et al. (2015) recently augmented the sequence-tosequence framework with a continuous analog of stack and queue data structures, to better handle long-range dependencies often found in linguistic data. Some recent papers have used LS"
N16-1076,P08-1109,0,0.012501,"Missing"
N16-1076,D15-1112,0,0.00590533,"Missing"
N16-1076,N10-1112,0,0.0280597,"weights Eisner (2002) and Li and Eisner (2009) also explain how to compute the partial derivatives of p(y∗ |x) with respect to the arc weights, essentially by using the forward-backward algorithm. We backpropagate further from these partials to find the gradient of (2) with respect to all our model parameters. We describe our gradient-based maximization procedure in section 10.3, along with regularization. Our model does not have to be trained with the conditional log likelihood objective. It could be trained with other objectives such as empirical risk or softmax-margin (Li and Eisner, 2009; Gimpel and Smith, 2010), or with error-driven updates such as in the structured perceptron (Collins, 2002). 7 Inference and Decoding For a new input x at test time, we can now construct a weighted FST, G, that defines a probability distribution over all aligned output strings. This can be manipulated to make various predictions about y∗ and its alignment. In our present experiments, we find the most probable (highest-weighted) path in G (Dijkstra, 1959), ˆ as our prediction. Note and use its output string y that Dijkstra’s algorithm is exact; no beam search is required as in some neural sequence models. ˆ may not be"
N16-1076,N07-1047,0,0.0152793,"2016) score edit sequences using a globally normalized model, and Dyer et al. (2015) evaluate the local probability of a parsing action given past actions (and their result) and future words. These architectures are powerful because their LSTMs can examine the output structure; but as a result they do not permit dynamic programming and must fall back on beam search. Our use of dynamic programming for efficient exact inference has long been common in non-neural architectures for sequence transduction, including FST systems that allow “phrasal” replacements s : t where |s|, |t |> 1 (Chen, 2003; Jiampojamarn et al., 2007; Bisani and Ney, 2008). Our work augments these FSTs with neural networks, much as others have augmented CRFs. In this vein, Durrett and Klein (2015) augment a CRF parser (Finkel et al., 2008) to score constituents with a feedforward neural network. Likewise, FitzGerald et al. (2015) employ feedforward nets as a factor in a graphical model for semantic role labeling. Many CRFs have incorporated feedforward neural networks (Bridle, 1990; Peng et al., 2009; Do and Artieres, 2010; Vinel et al., 2011; Fujii et al., 2012; Chen et al., 2015, and others). Some work augments CRFs with BiLSTMs: Huang"
N16-1076,P07-2045,0,0.011159,"linear-chain CRF-BiLSTM, and Kong et al. (2015) on Chinese word segmentation and handwriting recognition with a semi-CRF-BiLSTM. 10 Experiments We evaluated our approach on two morphological generation tasks of reinflection (section 10.1) and lemmatization (section 10.2). In the reinflection task, the goal is to transduce verbs from one inflected form into another, whereas the lemmatization task requires the model to reduce an inflected verb to its 628 root form. We compare our WFST-LSTM against two standard baselines, a WFST with hand-engineered features and the Moses phrase-based MT system (Koehn et al., 2007), as well as the more complex latent-variable model of Dreyer et al. (2008). The comparison with Dreyer et al. (2008) is of noted interest since their latent variables are structured particularly for morphological transduction tasks—we are directly testing the ability of the LSTM to structure its hidden layer as effectively as linguistically motivated latent-variables. Additionally, we provide detailed ablation studies and learning curves which show that our neural-WFSA hybrid model can generalize even with very low amounts of training data. 10.1 Morphological Reinflection Following Dreyer (20"
N16-1076,D09-1005,1,0.807921,"s INS edits (for which j = i) a bit differently, using (exi+1 , γi:i+1 , exi , exi+2 ) rather than (eε , γi:i , exi , exi+1 ). This is conveniently the same vector that is used for all other competing edits at this i position (as they all have |s |= 1 in our present implementation); it provides an extra character of lookahead. 4 If an FST has cycles, such as the self-loops in the example of Figure 3, then the forward algorithm’s recurrence equations become cyclic, and must be solved as a linear system rather than sequentially. This is true regardless of how the FST’s weights Eisner (2002) and Li and Eisner (2009) also explain how to compute the partial derivatives of p(y∗ |x) with respect to the arc weights, essentially by using the forward-backward algorithm. We backpropagate further from these partials to find the gradient of (2) with respect to all our model parameters. We describe our gradient-based maximization procedure in section 10.3, along with regularization. Our model does not have to be trained with the conditional log likelihood objective. It could be trained with other objectives such as empirical risk or softmax-margin (Li and Eisner, 2009; Gimpel and Smith, 2010), or with error-driven"
N16-1076,P09-1067,1,0.79775,"hted) path in G (Dijkstra, 1959), ˆ as our prediction. Note and use its output string y that Dijkstra’s algorithm is exact; no beam search is required as in some neural sequence models. ˆ may not be the most probOn the other hand, y able string—extracting that from a weighted FST is NP-hard (Casacuberta and de la Higuera, 1999). The issue is that the total probability of each y is split over many paths. Still, this is a well-studied problem in NLP. Instead of the Viterbi approximation, we could have used a better approximation, such as crunching (May and Knight, 2006) or variational decoding (Li et al., 2009). We actually did try crunching the 10000-best outputs but got no significant improvement, so we do not report those results. 8 Transducer Topology In our experiments, we choose F to be a simple contextual edit FST as illustrated in Figure 2. Just as in Levenshtein distance (Levenshtein, 1966), it allows all edits s : t where |s |≤ 1, |t |≤ 1, |s |+ |t |= 6 0. We consider the edit type to be INS if s = ε, DEL if are defined. (For convenience, our experiments in this paper avoid cycles by limiting consecutive insertions: see section 8.) 627 t = ε, and SUB otherwise. Note that copy is a SUB edit"
N16-1076,N06-1045,0,0.0239768,"iments, we find the most probable (highest-weighted) path in G (Dijkstra, 1959), ˆ as our prediction. Note and use its output string y that Dijkstra’s algorithm is exact; no beam search is required as in some neural sequence models. ˆ may not be the most probOn the other hand, y able string—extracting that from a weighted FST is NP-hard (Casacuberta and de la Higuera, 1999). The issue is that the total probability of each y is split over many paths. Still, this is a well-studied problem in NLP. Instead of the Viterbi approximation, we could have used a better approximation, such as crunching (May and Knight, 2006) or variational decoding (Li et al., 2009). We actually did try crunching the 10000-best outputs but got no significant improvement, so we do not report those results. 8 Transducer Topology In our experiments, we choose F to be a simple contextual edit FST as illustrated in Figure 2. Just as in Levenshtein distance (Levenshtein, 1966), it allows all edits s : t where |s |≤ 1, |t |≤ 1, |s |+ |t |= 6 0. We consider the edit type to be INS if s = ε, DEL if are defined. (For convenience, our experiments in this paper avoid cycles by limiting consecutive insertions: see section 8.) 627 t = ε, and S"
N16-1076,J97-2003,0,0.0655592,"still permits exact inference through dynamic programming. We illustrate our method on the tasks of morphological reinflection and lemmatization. 1 2 Introduction Mapping one character sequence to another is a structured prediction problem that arises frequently in NLP and computational linguistics. Common applications include grapheme-to-phoneme (G2P), transliteration, vowelization, normalization, morphology, and phonology. The two sequences may have different lengths. Traditionally, such settings have been modeled with weighted finite-state transducers (WFSTs) with parametric edge weights (Mohri, 1997; Eisner, 2002). This requires manual design of the transducer states and the features extracted from those states. Alternatively, deep learning has recently been tried Notation and Background Let Σx be a discrete input alphabet and Σy be a discrete output alphabet. Our goal is to define a conditional distribution p(y |x) where x ∈ Σ∗x and y ∈ Σ∗y and x and y may be of different lengths. We use italics for characters and boldface for strings. xi denotes the ith character of x, and xi:j denotes the substring xi+1 xi+2 · · · xj of length j − i ≥ 0. Note that xi:i = ε, the empty string. Let n = |"
N16-1080,P11-1004,0,0.257107,"n-i-est. We derive an importance sampling algorithm for approximate inference in the model and report experimental results on English, German and Indonesian. 1 funniest funyest fun y est memikulmu menpikulmu men pikul mu Zulassung zulassenung zu lassen ung Orthography Underlying Form Segmentation Figure 1: Examples of canonical segmentation for English (top), Indonesian (middle) and German (bottom). Introduction Morphological segmentation is useful for NLP applications, such as, automatic speech recognition (Afify et al., 2006), keyword spotting (Narasimhan et al., 2014), machine translation (Clifton and Sarkar, 2011) and parsing (Seeker and C ¸ etino˘glu, 2015). Prior work cast the problem as surface segmentation: a word form w is segmented into a sequence of substrings whose concatenation is w. In this paper, we introduce the problem of canonical segmentation: w is analyzed as a sequence of canonical morphemes, based on a set of word forms that have been “canonically” annotated for supervised learning. Each canonical morpheme c corresponds to a surface morph s, defined as its orthographic manifestation, i.e., as the substring of w that is generated by applying editing operations like insertion and deleti"
N16-1080,P14-2102,1,0.936568,"ntation factor f (Sarawagi and Cohen, 2004), relating it to previous semi-CRF models of segmentation.2 To fit the model, we maximize the N log-likelihood PN of the training data {(si , ui , wi )}i=1 , L(θ) = i=1 log p(si , ui |wi ), with respect to the model parameters θ. Optimization is done with gradient-based methods—requiring the computation of log Z(w) and ∇ log Z(w), which is intractable.3 Thus, we turn to sampling (Rubinstein and Kroese, 2011) and stochastic gradient methods. Features Our model includes several simple feature templates. The transduction factor of the model is based on (Cotterell et al., 2014): we include features that fire on individual edit actions as well as conjunctions of edit actions and characters on the surrounding context. For the semi-Markov factor, we use the feature set of Cotterell et al. (2015a), which 2 Our transduction factor maps surface forms w to UR strings u of bounded length by imposing an insertion limit k. Thus, |u |≤ |w |+ k. Our experiments use k = 5. 3 Since the semi-CRF features fire on substrings, we would need a dynamic programming state for each substring of each of the exponentially many settings of u. includes indicator features on individual segment"
N16-1080,K15-1017,1,0.872874,"Missing"
N16-1080,Q15-1031,1,0.872986,"i , ui |wi ), with respect to the model parameters θ. Optimization is done with gradient-based methods—requiring the computation of log Z(w) and ∇ log Z(w), which is intractable.3 Thus, we turn to sampling (Rubinstein and Kroese, 2011) and stochastic gradient methods. Features Our model includes several simple feature templates. The transduction factor of the model is based on (Cotterell et al., 2014): we include features that fire on individual edit actions as well as conjunctions of edit actions and characters on the surrounding context. For the semi-Markov factor, we use the feature set of Cotterell et al. (2015a), which 2 Our transduction factor maps surface forms w to UR strings u of bounded length by imposing an insertion limit k. Thus, |u |≤ |w |+ k. Our experiments use k = 5. 3 Since the semi-CRF features fire on substrings, we would need a dynamic programming state for each substring of each of the exponentially many settings of u. includes indicator features on individual segments, conjunctions of segments and segment labels and conjunctions of segments and left and right context on the input string. We also include a feature that checks whether the segment is a word in ASPELL (or a monolingua"
N16-1080,W02-0603,0,0.540523,"instantiation of the indirect estimator leverages an efficient dynamic program to compute the expected features under p(·|u(i) ). This has the effect of decreasing the number of samples required to get a useful estimate of the gradient. Computing p¯(u(i) ) is a side effect of the dynamic program, namely the normalization constant. As a proposal distribution q, we use the following locally normalized distribution, exp(ω &gt;g(u, w)) . &gt; 0 u0 exp(ω g(u , w)) q(u) = P 3 (8) Related Work Most work on morphological segmentation has been unsupervised. The L INGUISTICA (Goldsmith, 2001) and M ORFESSOR (Creutz and Lagus, 2002) models rely on the minimum description length principle (Cover and Thomas, 2012). In short, these methods seek to segment words while at the same time minimizing the number of unique morphs discovered, i.e., the complexity of the model. The M OR FESSOR model has additionally been augmented to handle the semi-supervised scenario (Kohonen et al., 2010). Goldwater et al. (2009) proposed a Bayesian non-parametric approach to word and morphological segmentation. Poon et al. (2009) used contrastive estimation (Smith and Eisner, 2005) to learn a loglinear model for segmentation fully unsupervised. F"
N16-1080,N07-1020,0,0.0319622,"nt independently and decode sequentially. This approach is faster both at train and at test but suffers from cascading errors. Second, we train a joint model, the transduction and the segmentation components are trained to work well together. 5 https://github.com/desmond86/ Indonesian-English-Bilingual-Corpus 6 http://ryancotterell.github.io/ canonical-segmentation/ 667 error distance F1 which has been “corrupted” by the orthographic process. Our approach, however, is capable of restoring the underlying morphemes, e.g., stopping 7→ stop-ing. We note two exceptions to the above statement. Both Dasgupta and Ng (2007) and Naradowsky and Goldwater (2009) incorporate basic, heuristic spelling rules into unsupervised induction algorithms. Relatedly, Cotterell et al. (2015b) induced a phonology in an unsupervised manner. In contrast, our model is fully supervised and supports rich features, which enable accurate prediction on new words. en de id en de id en de id Joint Pipeline SemiCRF WFST 0.27 (.02) 0.33 (.01) 0.33 (.01) 0.63 (.00) 0.41 (.03) 0.53 (.02) 0.65 (.01) 0.74 (.01) 0.10 (.01) 0.22 (.01) 0.27 (.01) 0.71 (.00) 0.98 (.34) 0.63 (.04) 0.68 (.01) 1.35 (.01) 1.01 (.07) 1.10 (.04) 1.32 (.04) 4.24 (.20) 0.1"
N16-1080,D09-1011,0,0.0530571,"We present experiments on three languages: English, German and Indonesian. 2 Model, Inference and Training Our goal is canonical segmentation: identifying both the canonical morphemes and the morphs (their orthographic manifestations) of a word. This task involves segmenting the input as well as accounting for orthographic changes occurring in the word formation processes. Let w be the surface form, u the orthographic underlying representation (UR) of w, and s a labeled segmentation of u. Note: all random 1 Note that funnest is a word of (colloquial) English. 665 variables are string-valued (Dreyer and Eisner, 2009). For example, consider the word unhappiness: unhappiness | {z } transduction 7→ w u segmentation 7→ unhappyness | {z } [ prefix un][stem happy][suffix ness] . | {z } s Note that our notion of an orthographic UR closely resembles the phonological concept of a UR (Kenstowicz, 1994) and, indeed, many orthographic variations are manifestations of phonology. We model this process as a globally normalized log-linear model of the conditional distribution, p(s, u |w) =   1 exp η &gt; f (s, u)+ω &gt; g(u, w) , Z(w) where θ = {η, ω} are the model parameters, f and g are, respectively, feature functions of"
N16-1080,D08-1113,0,0.364328,"closely resembles the phonological concept of a UR (Kenstowicz, 1994) and, indeed, many orthographic variations are manifestations of phonology. We model this process as a globally normalized log-linear model of the conditional distribution, p(s, u |w) =   1 exp η &gt; f (s, u)+ω &gt; g(u, w) , Z(w) where θ = {η, ω} are the model parameters, f and g are, respectively, feature functions of the segmentation-UR and UR-surface-form pairs and P Z(w) = exp η &gt;f (s0 , u0 ) + ω &gt;g(u0 , w) is 0 0 s ,u the partition function. We can view this model as a conjunction of a finite-state transduction factor g (Dreyer et al., 2008) and a semi-Markov segmentation factor f (Sarawagi and Cohen, 2004), relating it to previous semi-CRF models of segmentation.2 To fit the model, we maximize the N log-likelihood PN of the training data {(si , ui , wi )}i=1 , L(θ) = i=1 log p(si , ui |wi ), with respect to the model parameters θ. Optimization is done with gradient-based methods—requiring the computation of log Z(w) and ∇ log Z(w), which is intractable.3 Thus, we turn to sampling (Rubinstein and Kroese, 2011) and stochastic gradient methods. Features Our model includes several simple feature templates. The transduction factor of"
N16-1080,J01-2001,0,0.869368,"Missing"
N16-1080,W10-2210,0,0.804269,"tion q, we use the following locally normalized distribution, exp(ω &gt;g(u, w)) . &gt; 0 u0 exp(ω g(u , w)) q(u) = P 3 (8) Related Work Most work on morphological segmentation has been unsupervised. The L INGUISTICA (Goldsmith, 2001) and M ORFESSOR (Creutz and Lagus, 2002) models rely on the minimum description length principle (Cover and Thomas, 2012). In short, these methods seek to segment words while at the same time minimizing the number of unique morphs discovered, i.e., the complexity of the model. The M OR FESSOR model has additionally been augmented to handle the semi-supervised scenario (Kohonen et al., 2010). Goldwater et al. (2009) proposed a Bayesian non-parametric approach to word and morphological segmentation. Poon et al. (2009) used contrastive estimation (Smith and Eisner, 2005) to learn a loglinear model for segmentation fully unsupervised. Few supervised techniques have been applied to morphological segmentation. Ruokolainen et al. (2013) applied a linear-chain CRF, showing that with a minimal amount of labeled data the performance of standard unsupervised and semi-supervised baselines are surpassed. In follow-up work (Ruokolainen et al., 2014), they found that incorporating distribution"
N16-1080,W10-2211,0,0.0348371,". . u(m) ∼ q. We use m = 1000 in our experiments. Conditioned on each sample value for u, we use exact semi-CRF Viterbi decoding to select s. 4.3 Evaluation Measures Evaluating morphological segmentation is tricky. The standard measure for the supervised task is border F1 , which measures how often the segmentation boundaries posited by the model are correct. However, this measure assumes that the concatenation of the segments is identical to the input string (i.e., surface segmentation) and is thus not applicable to canonical segmentation. On the other hand, the Morpho Challenge competition (Kurimo et al., 2010) uses a measure that samples a large number of word pairs from a linguistic gold standard. A form is considered correct if the gold standard contains at least one overlapping morph and the model posits at least one overlapping morph—this is problematic because for languages with multi-morphemic words (e.g., German), one should consider all morphs. Moreover, we can actually recover the linguistically annotated gold standard in contrast to unsupervised methods. Instead, we report results under three measures: error rate, edit distance and morpheme F1 . Error rate is the proportion of analyses th"
N16-1080,J97-2003,0,0.0956147,"rage edit distance. Bottom: Mean morpheme F1 (higher better). Standard deviation in parentheses. Best result on each line in bold. Baseline: Semi-CRF Segmenter The first baseline is a semi-CRF (Sarawagi and Cohen, 2004) that segments the orthographic form into morphs without canonicalization. Earlier work by Cotterell et al. (2015a) applied this model to supervised morphological segmentation. We use the feature set as Cotterell et al. (2015a), but we do not incorporate their augmented morphotactic state space. Baseline: WFST Segmenter Our second baseline is a weighted finite-state transducer (Mohri, 1997) with a log-linear parameterization (Dreyer et al., 2008). We use the stochastic contextual edit model of Cotterell et al. (2014). We employ context n-gram features (up to 6-grams) on the input string to the left and right of the edit location in addition to 2-gram features on the lower string. The context features are then conjoined with the exact edit action. We refer the reader to Cotterell et al. (2014) for more details. The segmentation boundaries are marked as a distinguished symbol in the target string. This model is not entirely suited for the task as it makes it difficult to include t"
N16-1080,D14-1095,0,0.263957,"a sequence of substrings, e.g., funniest 7→ funn-i-est. We derive an importance sampling algorithm for approximate inference in the model and report experimental results on English, German and Indonesian. 1 funniest funyest fun y est memikulmu menpikulmu men pikul mu Zulassung zulassenung zu lassen ung Orthography Underlying Form Segmentation Figure 1: Examples of canonical segmentation for English (top), Indonesian (middle) and German (bottom). Introduction Morphological segmentation is useful for NLP applications, such as, automatic speech recognition (Afify et al., 2006), keyword spotting (Narasimhan et al., 2014), machine translation (Clifton and Sarkar, 2011) and parsing (Seeker and C ¸ etino˘glu, 2015). Prior work cast the problem as surface segmentation: a word form w is segmented into a sequence of substrings whose concatenation is w. In this paper, we introduce the problem of canonical segmentation: w is analyzed as a sequence of canonical morphemes, based on a set of word forms that have been “canonically” annotated for supervised learning. Each canonical morpheme c corresponds to a surface morph s, defined as its orthographic manifestation, i.e., as the substring of w that is generated by apply"
N16-1080,N09-1024,0,0.430023,"Most work on morphological segmentation has been unsupervised. The L INGUISTICA (Goldsmith, 2001) and M ORFESSOR (Creutz and Lagus, 2002) models rely on the minimum description length principle (Cover and Thomas, 2012). In short, these methods seek to segment words while at the same time minimizing the number of unique morphs discovered, i.e., the complexity of the model. The M OR FESSOR model has additionally been augmented to handle the semi-supervised scenario (Kohonen et al., 2010). Goldwater et al. (2009) proposed a Bayesian non-parametric approach to word and morphological segmentation. Poon et al. (2009) used contrastive estimation (Smith and Eisner, 2005) to learn a loglinear model for segmentation fully unsupervised. Few supervised techniques have been applied to morphological segmentation. Ruokolainen et al. (2013) applied a linear-chain CRF, showing that with a minimal amount of labeled data the performance of standard unsupervised and semi-supervised baselines are surpassed. In follow-up work (Ruokolainen et al., 2014), they found that incorporating distributional character-level features acquired from large unlabeled corpora improved the earlier model. Cotterell et al. (2015a) showed th"
N16-1080,W13-3504,0,0.417711,", 2012). In short, these methods seek to segment words while at the same time minimizing the number of unique morphs discovered, i.e., the complexity of the model. The M OR FESSOR model has additionally been augmented to handle the semi-supervised scenario (Kohonen et al., 2010). Goldwater et al. (2009) proposed a Bayesian non-parametric approach to word and morphological segmentation. Poon et al. (2009) used contrastive estimation (Smith and Eisner, 2005) to learn a loglinear model for segmentation fully unsupervised. Few supervised techniques have been applied to morphological segmentation. Ruokolainen et al. (2013) applied a linear-chain CRF, showing that with a minimal amount of labeled data the performance of standard unsupervised and semi-supervised baselines are surpassed. In follow-up work (Ruokolainen et al., 2014), they found that incorporating distributional character-level features acquired from large unlabeled corpora improved the earlier model. Cotterell et al. (2015a) showed that modeling morphotactics with a semi-CRF improves results further. The previously described approaches only attempt to split words into a sequence of stem and affixes— making it difficult to restore the underlying str"
N16-1080,E14-4017,0,0.16445,"gmented to handle the semi-supervised scenario (Kohonen et al., 2010). Goldwater et al. (2009) proposed a Bayesian non-parametric approach to word and morphological segmentation. Poon et al. (2009) used contrastive estimation (Smith and Eisner, 2005) to learn a loglinear model for segmentation fully unsupervised. Few supervised techniques have been applied to morphological segmentation. Ruokolainen et al. (2013) applied a linear-chain CRF, showing that with a minimal amount of labeled data the performance of standard unsupervised and semi-supervised baselines are surpassed. In follow-up work (Ruokolainen et al., 2014), they found that incorporating distributional character-level features acquired from large unlabeled corpora improved the earlier model. Cotterell et al. (2015a) showed that modeling morphotactics with a semi-CRF improves results further. The previously described approaches only attempt to split words into a sequence of stem and affixes— making it difficult to restore the underlying structure 4 Informally, the indirect importance sampling estimate converges to the true expectation as m → ∞. 4 Experiments We provide canonical segmentation experiments in three languages: English, German and Ind"
N16-1080,Q15-1026,0,0.0243284,"Missing"
N16-1080,P05-1044,0,0.0289335,"unsupervised. The L INGUISTICA (Goldsmith, 2001) and M ORFESSOR (Creutz and Lagus, 2002) models rely on the minimum description length principle (Cover and Thomas, 2012). In short, these methods seek to segment words while at the same time minimizing the number of unique morphs discovered, i.e., the complexity of the model. The M OR FESSOR model has additionally been augmented to handle the semi-supervised scenario (Kohonen et al., 2010). Goldwater et al. (2009) proposed a Bayesian non-parametric approach to word and morphological segmentation. Poon et al. (2009) used contrastive estimation (Smith and Eisner, 2005) to learn a loglinear model for segmentation fully unsupervised. Few supervised techniques have been applied to morphological segmentation. Ruokolainen et al. (2013) applied a linear-chain CRF, showing that with a minimal amount of labeled data the performance of standard unsupervised and semi-supervised baselines are surpassed. In follow-up work (Ruokolainen et al., 2014), they found that incorporating distributional character-level features acquired from large unlabeled corpora improved the earlier model. Cotterell et al. (2015a) showed that modeling morphotactics with a semi-CRF improves re"
N16-1080,P99-1037,0,0.808147,"Missing"
N16-1080,P13-1118,0,0.0853262,"Missing"
N18-1004,P17-1109,1,0.584802,"escribe and quantify the axes along which languages vary. One facet of language that has been the subject of heavy investigation is the nature of vowel inventories, i.e., which vowels a language contains. It is a cross-linguistic universal that all spoken languages have vowels (Gordon, 2016), and the underlying principles guiding vowel selection are understood: vowels must be both easily recognizable and well-dispersed (Schwartz et al., 2005). In this work, we offer a more formal treatment of the subject, deriving a generative probability model of vowel inventory typology. Our work builds on (Cotterell and Eisner, 2017) by investigating not just discrete IPA inventories but the cross-linguistic variation in acoustic formants. The philosophy behind our approach is that linguistic typology should be treated probabilistically 2 Acoustic Phonetics and Formants Much of human communication takes place through speech: one conversant emits a sound wave to be comprehended by a second. In this work, we consider the nature of the portions of such sound waves that correspond to vowels. We briefly review the relevant bits of acoustic phonetics so as to give an overview of the data we are actually modeling and develop our"
N18-2085,K17-1003,0,0.0498703,"n languages are less complex than others: he claims that Creoles are simpler. M¨uller et al. (2012) compare LMs on EuroParl, but do not compare performance across languages. Related Work Recurrent neural language models can effectively learn complex dependencies, even in openvocabulary settings (Hwang and Sung, 2017; Kawakami et al., 2017). Whether the models are able to learn particular syntactic interactions is an intriguing question, and some methodologies have been presented to tease apart under what circumstances variously-trained models encode attested interactions (Linzen et al., 2016; Enguehard et al., 2017). While the sort of detailed, constructionspecific analyses in these papers is surely informative, our evaluation is language-wide. MT researchers have investigated whether an English sentence contains enough information to predict the fine-grained inflections used in its foreignlanguage translations (see Kirov et al., 2017). 7 Conclusion We have presented a clean method for the crosslinguistic comparison of language modeling: We assess whether a language modeling technique can compress a sentence and its translations equally well. We show an interesting correlation between the morphological r"
N18-2085,P82-1020,0,0.743223,"Missing"
N18-2085,N12-1043,0,0.119,"Missing"
N18-2085,P17-1137,0,0.266302,"se they appear in words that were (arbitrarily) designated as OOV in that language. Such models are known as “open-vocabulary” LMs. Notation. Let ∪˙ denote disjoint union, i.e., A ∪˙ B = C iff A ∪ B = C and A ∩ B = ∅. Let Σ be a discrete alphabet of characters, including a distinguished unknown-character symbol ?.2 A charQ|c|+1 acter LM then defines p(c) = i=1 p(ci |c&lt;i ), where we take c|c|+1 to be a distinguished end-ofstring symbol EOS. In this work, we consider two open-vocabulary LMs, as follows. LSTM LM. While neural language models can also take a hybrid approach (Hwang and Sung, 2017; Kawakami et al., 2017), recent advances indicate that full character-level modeling is now competitive with word-level modeling. A large part of this is due to the use of recurrent neural networks (Mikolov et al., 2010), which can generalize about Baseline n-gram LM. We train “flat” hybrid word/character open-vocabulary n-gram models (Bisani and Ney, 2005), defined over strings Σ+ 2 The set of graphemes in these languages can be assumed to be closed, but external graphemes may on rare occasion appear in random text samples. These are rare enough to not materially affect the metrics. 3 The model can be extended to h"
N18-2085,L18-1293,1,0.78621,"Missing"
N18-2085,sproat-etal-2014-database,0,0.0190663,"95 nl 0.90 de et hu lt sk 0.85 ro en sv da 0.80 pt pl cs el lv es sl bg it fi fr 0.9 1.0 BPEC (LSTM over forms) 1.1 Figure 2: Each dot is a language, and its coordinates are the BPEC values for the LSTM LMs over words and lemmata. The top and right margins show kernel density estimates of these two sets of BPEC values. All dots follow the blue regression, but stay below the green line (y = x), and the darker dots—which represent languages with higher counting complexity—tend to fall toward the right but not toward the top, since counting complexity is correlated only with the BPEC over words. Sproat et al. (2014) present a corpus of close translations of sentences in typologically diverse languages along with detailed morphosyntactic and morphosemantic annotations, as the means for assessing linguistic complexity for comparable messages, though they expressly do not take an information-theoretic approach to measuring complexity. In the linguistics literature, McWhorter (2001) argues that certain languages are less complex than others: he claims that Creoles are simpler. M¨uller et al. (2012) compare LMs on EuroParl, but do not compare performance across languages. Related Work Recurrent neural languag"
N18-2085,L16-1680,0,0.050938,"Missing"
N18-2085,E17-2018,1,0.880447,"Missing"
N18-2085,2005.mtsummit-papers.11,0,0.0633624,"anking under BPEC shows that the LSTM has the easiest time modeling English itself. Scandinavian languages Danish and Swedish have BPEC closest to English; these languages are typologically and genetically similar to English. Experiments and Results n-gram versus LSTM. As expected, the LSTM outperforms the baseline n-gram models across the board. In addition, however, n-gram modeling yields relatively poor performance on some languages, such as Dutch, with only modestly more complex inflectional morphology than English. Our experiments are conducted on the 21 languages of the Europarl corpus (Koehn, 2005). The corpus consists of utterances made in the European parliament and are aligned cross-linguistically by a unique utterance id. With the exceptions (noted in Table 1) of Finnish, Hungarian and Estonian, which are Uralic, the languages are Indo-European. 7 4 Characters appearing &lt; 100 times in train are ?. Other phenomena—e.g., perhaps, compounding— may also be poorly modeled by n-grams. The Impact of Inflectional Morphology. Another major take-away is that rich inflectional morphology is a difficulty for both n-gram and LSTM LMs. In this section we give numbers for the LSTMs. Studying Fig."
N18-2087,J96-2001,0,0.528279,"Missing"
N18-2087,D10-1056,0,0.0576364,"Missing"
N18-2087,K17-2001,1,0.914832,"Missing"
N18-2087,Q15-1031,1,0.791664,"digm by its lemma, which is the surface form that fills a certain designated slot such as the infinitive. We instead use lexemes because lemmas may be ambiguous: bank is the lemma for at least two nominal and two verbal paradigms. 2 form ht, ·, s, ·i, and otherwise model   pθ (s |t) ∝ exp u&gt; tanh (W · vt,s ) &gt; 0 (2) frequent ones in the test portion. This is a realistic evaluation: a training lexicon for a new language would tend to contain frequent types, so the system should be tested on its ability to extrapolate to rarer types that could not be looked up in that lexicon, as discussed by Cotterell et al. (2015). To make the split, we sample N word types without replacement, which is equivalent to collecting the first N distinct forms from an annotated corpus generated from the same unigram distribution. The fractional counts that our method estimates may also be useful for corpus linguistics—for example, tracking the frequency of specific lexemes over time, or comparing the rate of participles in the work of two different authors. Finally, the fractional counts can aid the training of NLP methods that operate on a raw corpus, such as distributional embedding of surface form types into a vector space"
N18-2087,petrov-etal-2012-universal,0,0.0719965,"Missing"
N18-2087,N13-1138,0,0.0945789,"(t, `, s |f ), we may disambiguate these counts in expectation, i.e., we attribute a count of c (talked) · pθ (V ERB, t alk, PAST _ PART |talked) to the past participle of the V ERB t alk. Our aim is the construction and unsupervised estimation of the distribution pθ (t, `, s |f ). While the task at hand is novel, what applications does it have? We are especially interested in sampling tuples ht, `, s, f i from an inflected lexicon. Sampling is a necessity for creating train-test splits for evaluating morphological inflectors, which has recently become a standard task in the literature (Durrett and DeNero, 2013; Hulden et al., 2014; Nicolai et al., 2015; Faruqui et al., 2016), and has seen two shared tasks (Cotterell et al., 2016, 2017). Creating train-test splits for training inflectors involves sampling without replacement so that all test types are unseen. Ideally, we would like more frequent word types in the training portion and less What is Syncretism? We say that a surface form f is syncretic if two slots s1 6= s2 exist such that some paradigm π(t, `) maps both s1 and s2 to f . In other words, a single form fills multiple slots in a paradigm: syncretism may be thought of as intra-paradigmatic"
N18-2087,L16-1680,0,0.0421778,"Missing"
N18-2087,N16-1077,0,0.0279486,"we attribute a count of c (talked) · pθ (V ERB, t alk, PAST _ PART |talked) to the past participle of the V ERB t alk. Our aim is the construction and unsupervised estimation of the distribution pθ (t, `, s |f ). While the task at hand is novel, what applications does it have? We are especially interested in sampling tuples ht, `, s, f i from an inflected lexicon. Sampling is a necessity for creating train-test splits for evaluating morphological inflectors, which has recently become a standard task in the literature (Durrett and DeNero, 2013; Hulden et al., 2014; Nicolai et al., 2015; Faruqui et al., 2016), and has seen two shared tasks (Cotterell et al., 2016, 2017). Creating train-test splits for training inflectors involves sampling without replacement so that all test types are unseen. Ideally, we would like more frequent word types in the training portion and less What is Syncretism? We say that a surface form f is syncretic if two slots s1 6= s2 exist such that some paradigm π(t, `) maps both s1 and s2 to f . In other words, a single form fills multiple slots in a paradigm: syncretism may be thought of as intra-paradigmatic ambiguity. This definition does depend on the exact annotation sc"
N18-2087,E17-1048,0,0.0632483,"Missing"
N18-2087,E14-1060,0,0.0509616,"mbiguate these counts in expectation, i.e., we attribute a count of c (talked) · pθ (V ERB, t alk, PAST _ PART |talked) to the past participle of the V ERB t alk. Our aim is the construction and unsupervised estimation of the distribution pθ (t, `, s |f ). While the task at hand is novel, what applications does it have? We are especially interested in sampling tuples ht, `, s, f i from an inflected lexicon. Sampling is a necessity for creating train-test splits for evaluating morphological inflectors, which has recently become a standard task in the literature (Durrett and DeNero, 2013; Hulden et al., 2014; Nicolai et al., 2015; Faruqui et al., 2016), and has seen two shared tasks (Cotterell et al., 2016, 2017). Creating train-test splits for training inflectors involves sampling without replacement so that all test types are unseen. Ideally, we would like more frequent word types in the training portion and less What is Syncretism? We say that a surface form f is syncretic if two slots s1 6= s2 exist such that some paradigm π(t, `) maps both s1 and s2 to f . In other words, a single form fills multiple slots in a paradigm: syncretism may be thought of as intra-paradigmatic ambiguity. This defi"
N18-2087,L18-1293,1,0.786957,"a generative probability distribution over annotated word forms, and fit the model parameters using the token counts of unannotated word forms. The resulting distribution predicts how to partition each form’s token count among its possible annotations. While our method actually deals with all ambiguous forms in the lexicon, it is particularly useful for syncretic forms because syncretism is often systematic and pervasive. Inflected lexicons—lists of morphologically inflected forms—are commonplace in NLP. Such lexicons currently exist for over 100 languages in a standardized annotation scheme (Kirov et al., 2018), making them one of the most multi-lingual annotated resources in existence. These lexicons are typically annotated at the type level, i.e., each word type is listed with its possible morphological analyses, divorced from sentential context. One might imagine that most word types are unambiguous. However, many inflectional systems are replete with a form of ambiguity termed syncretism—a systematic merger of morphological slots. In English, some verbs have five distinct inflected forms, but regular verbs (the vast majority) merge two of these and so distinguish only four. The verb sin g has"
N18-2087,N15-1093,0,0.0378831,"in expectation, i.e., we attribute a count of c (talked) · pθ (V ERB, t alk, PAST _ PART |talked) to the past participle of the V ERB t alk. Our aim is the construction and unsupervised estimation of the distribution pθ (t, `, s |f ). While the task at hand is novel, what applications does it have? We are especially interested in sampling tuples ht, `, s, f i from an inflected lexicon. Sampling is a necessity for creating train-test splits for evaluating morphological inflectors, which has recently become a standard task in the literature (Durrett and DeNero, 2013; Hulden et al., 2014; Nicolai et al., 2015; Faruqui et al., 2016), and has seen two shared tasks (Cotterell et al., 2016, 2017). Creating train-test splits for training inflectors involves sampling without replacement so that all test types are unseen. Ideally, we would like more frequent word types in the training portion and less What is Syncretism? We say that a surface form f is syncretic if two slots s1 6= s2 exist such that some paradigm π(t, `) maps both s1 and s2 to f . In other words, a single form fills multiple slots in a paradigm: syncretism may be thought of as intra-paradigmatic ambiguity. This definition does depend on"
N19-1064,N18-2108,0,0.0397238,"Missing"
N19-1064,W19-3821,0,0.0769635,"Missing"
N19-1064,W19-3621,0,0.255063,"f word representations. In contrast, we analyze bias in contextualized word representations and its effect on a downstream task. To mitigate bias from word embeddings, Bolukbasi et al. (2016) propose a post-processing method to project out the bias subspace from the pre-trained embeddings. Their method is shown to reduce the gender information from the embeddings of gender-neutral words, and, remarkably, maintains the same level of performance on different downstream NLP tasks. Zhao et al. (2018b) further propose a training mechanism to separate gender information from other factors. However, Gonen and Goldberg (2019) argue that entirely removing bias is difficult, if not impossible, and the gender bias information can be often recovered. This paper investigates a natural follow-up question: What are effective bias mitigation techniques for contextualized embeddings? 3 Gender Bias in ELMo In this section we describe three intrinsic analyses highlighting gender bias in trained ELMo contextual word embeddings (Peters et al., 2018). We show that (1) training data for ELMo contains sig#occurrence 5,300,000 1,600,000 #M-biased occs. 170,000 33,000 #F-biased occs. 81,000 36,000 Table 1: Training corpus for ELMo."
N19-1064,W14-3333,0,0.0320149,"Missing"
N19-1064,D17-1018,0,0.0922775,"Missing"
N19-1064,N19-1063,0,0.127097,"s in the training data but also amplifies it (Zhao et al., 2017). For word representations, Bolukbasi et al. (2016) and Caliskan et al. (2017) show that word embeddings encode societal biases about gender roles and occupations, e.g. engineers are stereotypically men, and nurses are stereotypically women. As a consequence, downstream applications that use these pretrained word embeddings also reflect this bias. For example, Zhao et al. (2018a) and Rudinger et al. (2018) show that coreference resolution systems relying on word embeddings encode such occupational stereotypes. In concurrent work, May et al. (2019) measure gender bias in sentence embeddings, but their evaluation is on the aggregation of word representations. In contrast, we analyze bias in contextualized word representations and its effect on a downstream task. To mitigate bias from word embeddings, Bolukbasi et al. (2016) propose a post-processing method to project out the bias subspace from the pre-trained embeddings. Their method is shown to reduce the gender information from the embeddings of gender-neutral words, and, remarkably, maintains the same level of performance on different downstream NLP tasks. Zhao et al. (2018b) further"
N19-1064,D18-1302,0,0.0317387,"ce with entities of the opposite gender. Results show that testtime embedding neutralization is only partially effective, while data augmentation largely mitigates bias demonstrated on WinoBias by the coreference 629 Proceedings of NAACL-HLT 2019, pages 629–634 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics system. 2 M F Related Work Gender bias has been shown to affect several realworld applications relying on automatic language analysis, including online news (Ross and Carter, 2011), advertisements (Sweeney, 2013), abusive language detection (Park et al., 2018), machine translation (Font and Costa-juss`a, 2019; Vanmassenhove et al., 2018), and web search (Kay et al., 2015). In many cases, a model not only replicates bias in the training data but also amplifies it (Zhao et al., 2017). For word representations, Bolukbasi et al. (2016) and Caliskan et al. (2017) show that word embeddings encode societal biases about gender roles and occupations, e.g. engineers are stereotypically men, and nurses are stereotypically women. As a consequence, downstream applications that use these pretrained word embeddings also reflect this bias. For example, Zhao et al."
N19-1064,N15-1082,0,0.0217537,"the information unequally for male and female entities. We use the list collected in (Zhao et al., 2018a) Bias in Coreference Resolution In this section, we establish that coreference systems that depend on ELMo embeddings exhibit significant gender bias. Then we evaluate two simple methods for removing the bias from the systems and show that the bias can largely be reduced. 4.1 Setup We evaluate bias with respect to the WinoBias dataset (Zhao et al., 2018a), a benchmark of paired male and female coreference resolution examples following the Winograd format (Hirst, 1981; Rahman and Ng, 2012; Peng et al., 2015). It contains two different subsets, pro-stereotype, where pronouns are associated with occupations predominately associated with the gender of the pronoun, or anti-stereotype, when the opposite relation is true. 2 We use the ν-SVC formulation and tune the hyperparameter ν (Chang and Lin, 2011) in the range of [0.1, 1] with a step 0.1. 631 Embeddings Data Augmentation Neutralization GloVe ELMo GloVe GloVe GloVe+ELMo GloVe+ELMo GloVe+ELMo GloVe+ELMo OntoNotes Pro. 76.0 63.9 79.1 65.9 72.6 71.7 67.7 65.8 72.7 71.0 71.0 71.1 Semantics Only Anti. Avg. |Diff | 49.4 62.7 26.6* 62.8 63.4 1.1 49.5 64."
N19-1064,D14-1162,0,0.0952619,"Missing"
N19-1064,N18-1202,0,0.489748,"ned ELMo embeddings systematically encode gender information and (3) ELMo unequally encodes gender information about male and female entities. Then, we show that a state-of-the-art coreference system that depends on ELMo inherits its bias and demonstrates significant bias on the WinoBias probing corpus. Finally, we explore two methods to mitigate such gender bias and show that the bias demonstrated on WinoBias can be eliminated. 1 Introduction Distributed representations of words in the form of word embeddings (Mikolov et al., 2013; Pennington et al., 2014) and contextualized word embeddings (Peters et al., 2018; Devlin et al., 2018; Radford et al., 2018; McCann et al., 2017; Radford et al., 2019) have led to huge performance improvement on many NLP tasks. However, several recent studies show that training word embeddings in large corpora could lead to encoding societal biases present in these human-produced data (Bolukbasi et al., 2016; Caliskan et al., 2017). In this work, we extend these analyses to the ELMo contextualized word embeddings. Our work provides a new intrinsic analysis of how ELMo represents gender in biased ways. First, the corpus used for training ELMo has a significant gender skew:"
N19-1064,D12-1071,0,0.0321936,"that ELMo propagates the information unequally for male and female entities. We use the list collected in (Zhao et al., 2018a) Bias in Coreference Resolution In this section, we establish that coreference systems that depend on ELMo embeddings exhibit significant gender bias. Then we evaluate two simple methods for removing the bias from the systems and show that the bias can largely be reduced. 4.1 Setup We evaluate bias with respect to the WinoBias dataset (Zhao et al., 2018a), a benchmark of paired male and female coreference resolution examples following the Winograd format (Hirst, 1981; Rahman and Ng, 2012; Peng et al., 2015). It contains two different subsets, pro-stereotype, where pronouns are associated with occupations predominately associated with the gender of the pronoun, or anti-stereotype, when the opposite relation is true. 2 We use the ν-SVC formulation and tune the hyperparameter ν (Chang and Lin, 2011) in the range of [0.1, 1] with a step 0.1. 631 Embeddings Data Augmentation Neutralization GloVe ELMo GloVe GloVe GloVe+ELMo GloVe+ELMo GloVe+ELMo GloVe+ELMo OntoNotes Pro. 76.0 63.9 79.1 65.9 72.6 71.7 67.7 65.8 72.7 71.0 71.0 71.1 Semantics Only Anti. Avg. |Diff | 49.4 62.7 26.6* 62"
N19-1064,N18-2002,0,0.15905,"Missing"
N19-1064,N18-2003,1,0.935115,"al., 2018), machine translation (Font and Costa-juss`a, 2019; Vanmassenhove et al., 2018), and web search (Kay et al., 2015). In many cases, a model not only replicates bias in the training data but also amplifies it (Zhao et al., 2017). For word representations, Bolukbasi et al. (2016) and Caliskan et al. (2017) show that word embeddings encode societal biases about gender roles and occupations, e.g. engineers are stereotypically men, and nurses are stereotypically women. As a consequence, downstream applications that use these pretrained word embeddings also reflect this bias. For example, Zhao et al. (2018a) and Rudinger et al. (2018) show that coreference resolution systems relying on word embeddings encode such occupational stereotypes. In concurrent work, May et al. (2019) measure gender bias in sentence embeddings, but their evaluation is on the aggregation of word representations. In contrast, we analyze bias in contextualized word representations and its effect on a downstream task. To mitigate bias from word embeddings, Bolukbasi et al. (2016) propose a post-processing method to project out the bias subspace from the pre-trained embeddings. Their method is shown to reduce the gender info"
N19-1064,D18-1521,1,0.888165,"al., 2018), machine translation (Font and Costa-juss`a, 2019; Vanmassenhove et al., 2018), and web search (Kay et al., 2015). In many cases, a model not only replicates bias in the training data but also amplifies it (Zhao et al., 2017). For word representations, Bolukbasi et al. (2016) and Caliskan et al. (2017) show that word embeddings encode societal biases about gender roles and occupations, e.g. engineers are stereotypically men, and nurses are stereotypically women. As a consequence, downstream applications that use these pretrained word embeddings also reflect this bias. For example, Zhao et al. (2018a) and Rudinger et al. (2018) show that coreference resolution systems relying on word embeddings encode such occupational stereotypes. In concurrent work, May et al. (2019) measure gender bias in sentence embeddings, but their evaluation is on the aggregation of word representations. In contrast, we analyze bias in contextualized word representations and its effect on a downstream task. To mitigate bias from word embeddings, Bolukbasi et al. (2016) propose a post-processing method to project out the bias subspace from the pre-trained embeddings. Their method is shown to reduce the gender info"
N19-1065,baccianella-etal-2010-sentiwordnet,0,0.0565213,"ayesian approach for aligning sentiment lexica with different continuous scales. SentiMerge consists of two steps: (i) aligning the lexica via rescaling, and (ii) combining the rescaled lexica using a Gaussian distribution. The authors perform token-level evaluation using a single sentiment analysis dataset where each token is labeled with its contextually dependent sentiment. Because SentiMerge can only combine lexica with continuous scales, we do not include it in our evaluation. 2 Sentiment Lexica and Scales We use the following commonly used Englishlanguage sentiment lexica: SentiWordNet (Baccianella et al., 2010), MPQA (Wilson et al., 2005), SenticNet 5 (Cambria et al., 2014), Hu-Liu (Hu and Liu, 2004), GI (Stone et al., 1962), and VADER (Hutto and Gilbert, 2014). Descriptive statistics for each lexicon are shown in Tab. 1. Each word in SentiWordNet is labeled with two real values, each in the interval [0, 1], corresponding to the strength of positive and negative sentiment (e.g., the label (0 0) is neutral, while the label (1 0) is maximally positive). Each word in VADER is labeled by ten different human evaluators, with each evaluator providing a polarity value on a nine-point scale (where the midpo"
N19-1065,P07-1056,0,0.143257,"ibed in §2. For each word w in the combined vocabulary, we obtain an estimate of z w by taking the mean of Qβw (z w ) = Dir(β w )—i.e., by normalizing β w . We compare this representation to using β w directly, because β w contains information about SentiVAE’s certainty about the word’s latent polarity value. We evaluate our common latent representation via a text classification task involving nine English-language sentiment analysis datasets: IMDB (Maas et al., 2011), Yelp (Zhang et al., 2015), SemEval 2017 Task 4 (SemEval, Rosenthal et al. (2017)), multi-domain sentiment analysis (MultiDom, Blitzer et al. (2007)), and PeerRead (Kang et al., 2018) with splits ACL 2017 and ICLR 2017 (Kang et al., 2018). Each dataset consists of multiple texts (e.g., tweets, articles), each labeled with an overall sentiment (e.g., positive). Descriptive statistics for each dataset are shown in Tab. 2. For the datasets with more than three sentiment labels, we consider two versions—the original and a version with only three (bucketed) sentiment labels. For each dataset, we transform each text into an average polarity value using either our representation, one of the six lexica,4 or a straightforward combination thereof,"
N19-1065,W14-5805,0,0.105563,"Ma et al., 2018). Indeed, given their utility, a veritable cottage industry has emerged focusing on the design of sentiment lexica. In practice, using any single lexicon, unless specifically and carefully designed for the particular domain of interest, has several downsides. For example, any lexicon will typically have low coverage compared to the language’s entire vocabulary, and may have misspecified labels for the domain. In many cases, it may therefore be desirable to combine multiple sentiment lexica into a single representation. Indeed, some research on unifying such lexica has emerged (Emerson and Declerck, 2014; Altrabsheh et al., 2017), borrowing ideas from crowdsourcing (Raykar et al., 2010; Hovy et al., 2013). However, this is a non-trivial task, because lexica can use binary, categorical, or continuous scales to quantify polarity—in addition to different interpretations for each—and thus cannot easily be combined. In Fig. 1, we show an example of the same word labeled using different lexica to illustrate the nature of the challenge. To combine sentiment lexica with disparate scales, we introduce SentiVAE, a novel multiview variant of the variational autoencoder (VAE) (Kingma and Welling, 2014)."
N19-1065,N13-1132,0,0.0300975,"of sentiment lexica. In practice, using any single lexicon, unless specifically and carefully designed for the particular domain of interest, has several downsides. For example, any lexicon will typically have low coverage compared to the language’s entire vocabulary, and may have misspecified labels for the domain. In many cases, it may therefore be desirable to combine multiple sentiment lexica into a single representation. Indeed, some research on unifying such lexica has emerged (Emerson and Declerck, 2014; Altrabsheh et al., 2017), borrowing ideas from crowdsourcing (Raykar et al., 2010; Hovy et al., 2013). However, this is a non-trivial task, because lexica can use binary, categorical, or continuous scales to quantify polarity—in addition to different interpretations for each—and thus cannot easily be combined. In Fig. 1, we show an example of the same word labeled using different lexica to illustrate the nature of the challenge. To combine sentiment lexica with disparate scales, we introduce SentiVAE, a novel multiview variant of the variational autoencoder (VAE) (Kingma and Welling, 2014). SentiVAE, visualized as a graphical model in Fig. 2, differs from the original VAE in two ways: (i) it"
N19-1065,N18-1149,0,0.0425344,"Missing"
N19-1065,P11-1015,0,0.0780531,"to compute the accuracy of the classifier. Experiments and Results To evaluate our approach, we first use SentiVAE to combine the six lexica described in §2. For each word w in the combined vocabulary, we obtain an estimate of z w by taking the mean of Qβw (z w ) = Dir(β w )—i.e., by normalizing β w . We compare this representation to using β w directly, because β w contains information about SentiVAE’s certainty about the word’s latent polarity value. We evaluate our common latent representation via a text classification task involving nine English-language sentiment analysis datasets: IMDB (Maas et al., 2011), Yelp (Zhang et al., 2015), SemEval 2017 Task 4 (SemEval, Rosenthal et al. (2017)), multi-domain sentiment analysis (MultiDom, Blitzer et al. (2007)), and PeerRead (Kang et al., 2018) with splits ACL 2017 and ICLR 2017 (Kang et al., 2018). Each dataset consists of multiple texts (e.g., tweets, articles), each labeled with an overall sentiment (e.g., positive). Descriptive statistics for each dataset are shown in Tab. 2. For the datasets with more than three sentiment labels, we consider two versions—the original and a version with only three (bucketed) sentiment labels. For each dataset, we t"
N19-1065,R15-1064,0,0.0606215,"Missing"
N19-1065,S16-1023,0,0.0456651,"Missing"
N19-1065,P79-1022,0,0.264382,"Missing"
N19-1065,H05-1044,0,0.0391287,"timent lexica with different continuous scales. SentiMerge consists of two steps: (i) aligning the lexica via rescaling, and (ii) combining the rescaled lexica using a Gaussian distribution. The authors perform token-level evaluation using a single sentiment analysis dataset where each token is labeled with its contextually dependent sentiment. Because SentiMerge can only combine lexica with continuous scales, we do not include it in our evaluation. 2 Sentiment Lexica and Scales We use the following commonly used Englishlanguage sentiment lexica: SentiWordNet (Baccianella et al., 2010), MPQA (Wilson et al., 2005), SenticNet 5 (Cambria et al., 2014), Hu-Liu (Hu and Liu, 2004), GI (Stone et al., 1962), and VADER (Hutto and Gilbert, 2014). Descriptive statistics for each lexicon are shown in Tab. 1. Each word in SentiWordNet is labeled with two real values, each in the interval [0, 1], corresponding to the strength of positive and negative sentiment (e.g., the label (0 0) is neutral, while the label (1 0) is maximally positive). Each word in VADER is labeled by ten different human evaluators, with each evaluator providing a polarity value on a nine-point scale (where the midpoint is neutral), yielding a"
N19-1065,S17-2088,0,\N,Missing
N19-1155,E12-1068,0,0.0316466,"lves a string-to-string transduction from an inflected word form to its citation form, known as the lemma. More concretely, consider the English sentence: The bulls are running in Pamplona. A lemmatizer will seek to map each word to a form you may find in a dictionary—for instance, mapping running to run. This linguistic normalization is important in several downstream NLP applications, especially for highly inflected languages. Lemmatization has previously been shown to improve recall for information retrieval (Kanis and Skorkovská, 2010; Monz and De Rijke, 2001), to aid machine translation (Fraser et al., 2012; Chahuneau et al., 2013) and is a core part of modern parsing systems (Björkelund et al., 2010; Zeman et al., 2018). However, the task is quite nuanced as the proper choice of the lemma is context dependent. For * Equal contribution. Listing order is random. instance, in the sentence A running of the bulls took place in Pamplona, the word running is its own lemma, since, here, running is a noun rather than an inflected verb. Several counter-examples exist to this trend, as discussed in depth in Haspelmath and Sims (2013). Thus, a good lemmatizer must make use of some representation of each wo"
N19-1155,E17-1048,0,0.115839,"Missing"
N19-1155,P18-1247,1,0.922785,"ions, separately. We caution the reader that the discussion of these models will be brief: Neither of these particular components is novel with respect to the literature, so the formal details of the two models is best found in the original papers. The point of our paper is to describe a simple manner to combine these existing parts into a state-of-the-art lemmatizer. 3.1 Morphological Tagger: p(m |w) We employ a simple LSTM-based tagger to recover the morphology of a sentence (Heigold et al., 2017; Cotterell and Heigold, 2017). We also experimented with the neural conditional random field of Malaviya et al. (2018), but Heigold et al. (2017) gave slightly better tagging scores on average and is faster to train. Given a sequence of n words w = w1 , . . . , wn , we would like to obtain the morphological tags m = m1 , . . . , mn for each word, where mi ∈ Y. The model first obtains a word representation for each token using a character-level biLSTM (Graves et al., 2013) embedder, which is then input to a word-level biLSTM tagger that predicts tags for each word. Given a function cLSTM that returns the last hidden state of a characterbased LSTM, first we obtain a word representation ui for word wi as, ui = ["
N19-1155,N06-1045,0,0.0431125,"h(enc) ) aj , hj (5) a∈A j=1 p(aj |aj−1 , h(enc) , h(dec) ) j The summation is computed with dynamic programming—specifically, using the forward algorithm for hidden Markov models (Rabiner, 1989). (dec) p(`j |h(enc) ) is a two-layer feed-forward aj , hj network followed by a softmax. The transition p(aj |aj−1 , h(enc) , h(dec) ) is the multiplicative atj tention function with h(enc) and h(dec) as input. j To enforce monotonicity, p(aj |aj−1 ) = 0 if aj < aj−1 . 3.3 Decoding We consider two manners, by which we decode our model. The first is a greedy decoding scheme. The second is a crunching (May and Knight, 2006) scheme. We describe each in turn. Greedy Decoding. In the greedy scheme, we select the best morphological tag sequence m? = argmaxm log p(m |w) (6) and then decode each lemmata `?i = argmax` log p(` | (7) Crunching. In the crunching scheme, we first extract a k-best list of taggings from the morphological tagger. For an input sentence w, call the k-best tags for the ith word K(wi ). Crunching then says we should decode in the following manner (8) log X p(` |mi , wi ) p(mi |w) mi ∈K(wi ) 3.4 Training with Jackknifing In our model, a simple application of maximumlikelihood estimation (MLE) is u"
N19-1155,W18-6011,1,0.611729,"el (i) for languages with fewer training data available and (ii) languages that have richer morphology. Our system and pre-trained models on all languages in the latest version of the UD corpora12 are released at https://sigmorphon.github. io/sharedtasks/2019/task2/. 1 We compare to previously published numbers on nonrecent versions of UD, but the models we release are trained on the current version (2.3). 2 Instead of UD schema for morphological attributes, we use the UniMorph schema (Sylak-Glassman, 2016) instead. Note the mapping from UD schema to UniMorph schema is not one-to-one mapping (McCarthy et al., 2018). 2 Background: Lemmatization Most languages (Dryer and Haspelmath, 2013) in the world exhibit a linguistic phenomenon known as inflectional morphology, which causes word forms to mutate according to the syntactic category of the word. The syntactic context in which the word form occurs determines which form is properly used. One privileged form in the set of inflections is called the lemma. We regard the lemma as a lexicographic convention, often used to better organize dictionaries. Thus, the choice of which inflected form is the lemma is motivated by tradition and convenience, e.g., the lem"
N19-1155,D15-1272,1,0.779367,"point—our current model is not applicable in such a setting. However, we note that a semi-supervised morphological tagger could be trained in such a situation as well, which may benefit lemmatization. performs lemmatization using an averaged perceptron tagger that predicts a (lemma rule, UPOS) pair. Here, a lemma rule generates a lemma by removing parts of the word prefix/suffix and prepending and appending a new prefix/suffix. A guesser first produces correct lemma rules and the tagger is used to disambiguate from them. Lemming. The strongest non-neural baseline we consider is the system of Müller et al. (2015), who, like us, develop a joint model of morphological tagging lemmatization. In contrast to us, however, their model is globally normalized (Lafferty et al., 2001). Due to their global normalization, they directly estimate the parameters of their model with MLE without worrying about exposure bias. However, in order to efficiently normalize the model, they heuristically limit the set of possible lemmata through the use of edit trees (Chrupała, 2008), which makes the computation of the partition function tractable. Morfette. Much like Müller et al. (2015), Morfette relies on the concept of edi"
N19-1155,N16-1076,1,0.811503,"predicting the tags during test-time. Note that greedy decoding is optimal in this tagger as there is no interdependence between the tags mi . 3.2 A Lemmatizer: p(`i |mi , wi ) Neural sequence-to-sequence models (Sutskever et al., 2014; Bahdanau et al., 2015) have yielded state-of-the-art performance on the task of generating morphological variants—including the lemma—as evinced in several recent shared tasks on the subject (Cotterell et al., 2016, 2017, 2018). Our lemmatization factor in eq. (1) is based on such models. Specifically, we make use of a hardattention mechanism (Xu et al., 2015; Rastogi et al., 2016), rather than the original soft-attention mechanism. Our choice of hard attention is motivated by the performance of Makarov and Clematide (2018)’s system at the CoNLL-SIGMORPHON task. We use a nearly identical model, but opt for an exact dynamic-programming-based inference scheme (Wu et al., 2018).4 We briefly describe the model here. Given an inflected word w and a tag m, we would like to obtain the lemma ` ∈ Σ∗ , dropping the subscript for simplicity. Moreover, for the remainder of this section the subscripts will index into the character string `, that is ` = `1 , . . . , `|` |, where each"
N19-1155,K17-3009,0,0.0311965,"ect context-to-lemma approach, avoiding the use of morphological tags. We remark that Bergmanis and Goldwater (2018) assume a setting where lemmata are annotated at the token level, but morphological tags are not available; we contend, however, that such a setting is not entirely realistic as almost all corpora annotated with lemmata at the token level include morphosyntactic annotation, including the vast majority of the UD corpora. Thus, we do not consider it a stretch to assume the annotation of morphological tags to train our joint model.6 UDPipe. Our next baseline is the UDPipe system of Straka and Straková (2017). Their system 6 After correspondence with Toms Bergmanis, we would like to clarify this point. While Bergmanis and Goldwater (2018) explores the model in a token-annotated setting, as do we, the authors argue that such a model is better for a very low-resource scenario where the entire sentence is not annotated for lemmata. We concede this point—our current model is not applicable in such a setting. However, we note that a semi-supervised morphological tagger could be trained in such a situation as well, which may benefit lemmatization. performs lemmatization using an averaged perceptron tagg"
N19-1155,D18-1473,1,0.890687,"Missing"
N19-1155,K18-2001,0,0.0482884,"Missing"
N19-1155,chrupala-etal-2008-learning,0,\N,Missing
N19-1155,C10-3009,0,\N,Missing
N19-1155,P16-1108,0,\N,Missing
N19-1155,P17-2107,0,\N,Missing
N19-1155,N18-1126,0,\N,Missing
N19-1155,D13-1174,0,\N,Missing
N19-1155,W16-2002,1,\N,Missing
N19-1155,K18-3008,0,\N,Missing
N19-1156,E17-2102,0,0.104682,"ded, so can languages, by associating each language with a real-valued vector known as a language embedding. Training such representations as a part of a multilingual model allows us to infer similarities between languages. This is due to the fact that in order for multilingual parameter sharing to be successful in this setting, the neural network needs to use the language embeddings to encode features of the languages. Previous work has explored this type of representation learning in various tasks, such as NMT (Malaviya et al., 2017), ¨ language modelling (Tsvetkov et al., 2016; Ostling and Tiedemann, 2017), and tasks representing morphological, phonological, and syntactic linguistic levels (Bjerva and Augenstein, 2018a). In the context of computational typology, representations obtained through language modelling ¨ have been the most successful (Ostling and Tiedemann, 2017). This approach is particularly interesting since unlabelled data is available for a large portion of the world’s languages, meaning that high quality language embeddings can be obtained for more than 1,000 of the world’s languages. Language Embeddings through LMs In this work, we use a language modelling objective to pre-tra"
N19-1156,P14-1130,0,0.0321674,"ar (Breese et al., 1998) to overcome the cold start problem arising for unseen users or items at test time. The most successful one of these, in turn, is matrix factorisation, as applied in this paper, which represents users and items as (dense) vectors in the same latent feature space and measures their compatibility by taking the dot product between the two representations (Koren et al., 2009; Bokde et al., 2015). Beyond recommender systems, matrix factorisation has shown successes in a wide variety of subareas of NLP (Riedel et al., 2013; Rockt¨aschel et al., 2015; Levy and Goldberg, 2014; Lei et al., 2014; Augenstein et al., 2018). 10 Conclusion We introduce a generative model inspired by the principles-and-parameters framework, drawing on the correlations between typological features of languages to solve the novel task of typological collaborative filtering. We further show that raw text can be utilised to infer similarities between languages, thus allowing for extending the method with semi-supervised language embeddings. Acknowledgements We acknowledge the computational resources provided by CSC in Helsinki through NeIC-NLPL (www.nlpl.eu), and the support of the NVIDIA Corporation with the"
N19-1156,D17-1268,0,0.167387,"rrently often in the form of word embeddings. Similarly to how words can be embedded, so can languages, by associating each language with a real-valued vector known as a language embedding. Training such representations as a part of a multilingual model allows us to infer similarities between languages. This is due to the fact that in order for multilingual parameter sharing to be successful in this setting, the neural network needs to use the language embeddings to encode features of the languages. Previous work has explored this type of representation learning in various tasks, such as NMT (Malaviya et al., 2017), ¨ language modelling (Tsvetkov et al., 2016; Ostling and Tiedemann, 2017), and tasks representing morphological, phonological, and syntactic linguistic levels (Bjerva and Augenstein, 2018a). In the context of computational typology, representations obtained through language modelling ¨ have been the most successful (Ostling and Tiedemann, 2017). This approach is particularly interesting since unlabelled data is available for a large portion of the world’s languages, meaning that high quality language embeddings can be obtained for more than 1,000 of the world’s languages. Language Embeddings"
N19-1156,N13-1095,0,0.0145181,"s. 6 Data WALS. The World Atlas of Language Structures (WALS) is a large knowledge base of typological properties at the lexical, phonological, syntactic and semantic level on which we will run our experiments. The documentation of linguistic structure is spread throughout a wide variety of academic works, ranging from field linguistics to grammars describing the nuances of individual grammatical uses. KB creation is a laborious tasks as it involves distilling knowledge into a single, standardised resource, which, naturally, will be incomplete, prompting the need for methods to complete them (Min et al., 2013). In the case of WALS, few languages have all values annotated for all of the properties. In this section, we offer a formalisation of typological KBs to allow for our development of a probabilistic model over vectors of properties. WALS, for instance, contains n = 202 different parameters (Dryer and Haspelmath, 2013). Binarisation of WALS. Many common typological KBs, including WALS, the one studied here, contain binary as well as non-binary parameters. To deal with this, we binarise the KB as follows: Whenever there is a typological parameter that takes ≥ 3 values, e.g., ‘Feature 81A: Order"
N19-1156,I17-1046,0,0.0669546,"rvey of typology in NLP, see Ponti et al. (2018). Figure 7: Accuracy per feature group (Germanic). stein (2018a), might also be useful in our semisupervised extension of typological collaborative filtering. 9 Related Work Computational Typology The availability of unlabelled datasets for hundreds of languages permits inferring linguistic properties and categories ¨ (Ostling, 2015; Asgari and Sch¨utze, 2017). Individual prediction of typological features has been attempted in conjunction with several NLP tasks (Malaviya et al., 2017; Bjerva and Augenstein, 2018a,b). Our work is most similar to Murawaki (2017), who presents a Bayesian approach to utilising relations between features and languages for feature prediction. However, our work differs on several important counts, as we (i) include language information obtained through unsupervised learning, which allows us to take advantage of raw data and predict features for completely unannotated languages, (ii) analyse the effects of varying amounts of known features, especially in situations with and without in-branch training data, and (iii) view the problem of typological features through the lens of parameters from principles and parameters (Chom"
N19-1156,N13-1008,0,0.312537,"oes the language admit word-final voiced obstruents? 3 A Generative Model of Typology We now seek a probabilistic formalisation of the linguistic theory presented in §2; specifically, for every language `, we seek to explain the observed binary vector of parameters π ` : πi` = 1 indicates that the ith parameter is “on” in language `. The heart of our model will be quite simple: every language ` will have a language embedding λ` ∈ Rd and every parameter will have a parameter embed` ding eπi ∈ Rd . Now, πi` ∼ sigmoid(e&gt; πi λ ). This model also takes inspiration from work in relation extraction (Riedel et al., 2013). Writing the joint distribution over the entire binary vector of parameters, we arrive at p(π ` |λ` ) = |π| Y (1)   ` sigmoid e&gt; πi λ (2) i=1 = |π| Y i=1 = |π| Y 1 ` 1 + exp(−e&gt; πi λ ) i=1 (3) 1 For an overview of differences between these schools, we refer the reader to Haspelmath (2008). `∈L Note that p(λ` ) is, spiritually at least, a universal grammar: it is the prior over what sort of languages can exist, albeit encoded as a real vector. In the parlance of principles and parameters, the prior represents the principles. Then our model parameters are Θ = {eπ1 , . . . , eπ|π |, λ1 , . . ."
N19-1156,N15-1118,0,0.0408962,"Missing"
N19-1203,P17-1183,0,0.0301289,"vectors are passed to a bidirectional LSTM (Graves et al., 2005), where the corresponding hidden states are concatenated at each time step. We simply refer to the hidden state hi ∈ Rd as the result of said concatenation at the i-th step. Using hi , we can define the potential function as ψ (mi , mi−1 ) =  where Ami ,mi−1 is a exp Ami ,mi−1 + o> h , i mi transition weight matrix and omi ∈ Rd is a morphological tag embedding; both are learned. 2.3 The Morphological Inflector The conditional distribution p(wi |`i , mi ) is parameterized by a neural encoder–decoder model with hard attention from Aharoni and Goldberg (2017). The model was one of the top performers in the 2016 SIGMORPHON shared task (Cotterell et al., 2016); it achieved particularly high accuracy in the low-resource setting. Hard attention is motivated by the observation that alignment between the input and output sequences is often monotonic in inflection tasks. In the model, the input lemma is treated as a sequence of characters, and encoded using a bidirectional LSTM (Graves and Schmidhuber, 2005), to produce vectors xj for each character position j. Next the word wi = c = c1 · · · c|wi |is generated in a decoder character-by-character: p(cj |"
N19-1203,W05-0909,0,0.0318394,"trumental), as well as those not associated with specific prepositions, are less well predicted. In addition, we evaluated the model’s performance when all forms are replaced by their corresponding lemmata (as in two cat be sit). For freer word order languages such as Polish or Latin, we observe a substantial drop in performance because most information on inter-word relations and their roles (expressed by means of case system) is lost. 5 Related Work The primary evaluation for most contemporary language and translation modeling research is perplexity, BLEU (Papineni et al., 2002), or METEOR (Banerjee and Lavie, 2005). Undoubtedly, such metrics are necessary for extrinsic evaluation and comparison. However, relatively few studies have focused on intrinsic evaluation of the model’s mastery of grammaticality. Recently, Linzen et al. (2016) investigated the ability of an LSTM language model to capture sentential structure, by evaluating subject–verb agreement with respect to number, and showed that under strong supervision, the LSTM is able to approximate dependencies. Taking it from the other perspective, a truer measure of grammatical competence would be a task of mapping a meaning representation to text, w"
N19-1203,P17-1080,0,0.0256269,"version of language modeling. Language modeling predicts all words of a sentence from scratch, so the usual training and evaluation metric— perplexity—is dominated by the language model’s ability to predict content, which is where most of the uncertainty lies. Our task focuses on just the ability to reconstruct certain missing parts of the sentence—inflectional morphemes and their orthographic realization. This refocuses the modeling effort from semantic coherence to morphosyntactic coherence, an aspect of language that may take a back seat in current language models (see Linzen et al., 2016; Belinkov et al., 2017). Contextual inflection does not perfectly separate grammaticality modeling from content modeling: as illustrated in Tab. 1, mapping two cats _be_ sitting to the fully-inflected two cats were sitting does not require full knowledge of English grammar—the system does not have to predict the required word order nor the required auxiliary verb be, as these are supplied in the input. Conversely, this example does still require predicting some content—the semantic choice of past tense is not given by the input and must be guessed by the system.1 The primary contribution of this paper is a novel str"
N19-1203,W11-2832,0,0.0158478,"presentation specifies all necessary semantic content—content lemmata, dependency relations, and “inherent” closed-class morphemes (semantic features such as noun number, noun definiteness, and verb tense)—and the system is to realize this content according to the morphosyntactic conventions of a language, which means choosing word order, agreement morphemes, function words, and the surface forms of all words. Such tasks have been investigated to some extent—generating text from tectogrammatical trees (Hajic et al., 2002; Ptáˇcek and Žabokrtský, 2006) or from an AMR graph (Song et al., 2017). Belz et al. (2011) organized a related surface realization shared task on mapping unordered and uninflected dependency trees to properly ordered inflected sentences. The generated sentences were afterwards assessed by human annotators, making the task less scalable and more time consuming. Although our task is not perfectly matched to grammaticality modeling, the upside is that it is a “lightweight” task that works directly on text. No meaning representation is required. Thus, training and test data in any language can be prepared simply by lemmatizing a naturally occurring corpus. Finally, as a morphological i"
N19-1203,Q17-1010,0,0.0234037,"or decoding we use a greedy strategy where we first decode the CRF, that is, we solve the problem m? = argmaxm log p(m |`), using the Viterbi (1967) algorithm. We then use this decoded m? to generate forms from the inflector. Note that finding the one-best string under our neural inflector is intractable, and for this reason we use greedy search. 3 Experiments Dataset. We use the Universal Dependencies v1.2 dataset (Nivre et al., 2016) for our experiments. We include all the languages with information on their lemmata and fine-grained grammar tag annotation that also have fasttext embeddings (Bojanowski et al., 2017), which are used for word embedding initialization.5 Evaluation. We evaluate our model’s ability to predict: (i) the correct morphological tags from the lemma context, and (ii) the correct inflected forms. As our evaluation metric, we report 1-best accuracy for both tags and word form prediction. Configuration. We use a word and character embedding dimensionality of 300 and 100, respectively. The hidden state dimensionality is set to 200. All models are trained with Adam (Kingma and Ba, 2014), with a learning rate of 0.001 for 20 epochs. Baselines. We use two baseline systems: (1) the CoNLL–SI"
N19-1203,K18-3001,1,0.790791,"on any morphological annotation. We experiment on several typologically diverse languages from the Universal Dependencies treebanks, showing the utility of incorporating linguisticallymotivated latent variables into NLP models. 1 Introduction NLP systems are often required to generate grammatical text, e.g., in machine translation, summarization, dialogue, and grammar correction. One component of grammaticality is the use of contextually appropriate closed-class morphemes. In this work, we study contextual inflection, which has been recently introduced in the CoNLLSIGMORPHON 2018 shared task (Cotterell et al., 2018) to directly investigate context-dependent morphology in NLP. There, a system must inflect partially lemmatized tokens in sentential context. For example, in English, the system must reconstruct the correct word sequence two cats are sitting from partially lemmatized sequence two _cat_ are sitting. Among other things, this requires: (1) identifying cat as a noun in this context, (2) recognizing that cat should be inflected as plural to agree with the nearby verb and numeral, and (3) realizing this inflection as the suffix s. Most past work in supervised computational morphology—including the p"
N19-1203,K17-2001,1,0.93262,"a system must inflect partially lemmatized tokens in sentential context. For example, in English, the system must reconstruct the correct word sequence two cats are sitting from partially lemmatized sequence two _cat_ are sitting. Among other things, this requires: (1) identifying cat as a noun in this context, (2) recognizing that cat should be inflected as plural to agree with the nearby verb and numeral, and (3) realizing this inflection as the suffix s. Most past work in supervised computational morphology—including the previous CoNLL-SIGMORPHON shared tasks on morphological reinflection (Cotterell et al., 2017)—has focused mainly on step (3) above. As the task has been introduced into the literature only recently, we provide some background. Contextual inflection amounts to a highly constrained version of language modeling. Language modeling predicts all words of a sentence from scratch, so the usual training and evaluation metric— perplexity—is dominated by the language model’s ability to predict content, which is where most of the uncertainty lies. Our task focuses on just the ability to reconstruct certain missing parts of the sentence—inflectional morphemes and their orthographic realization. Th"
N19-1203,N16-1030,0,0.0407715,"tured neural model 1 Y p(m |`) = ψ (mi , mi−1 , `) Z(`) (2) i=1 2 Although wi can sometimes be computed by concatenating `i with mi -specific affixes, it can also be irregular. 3 In case of partially lemmatized sequence we still train the model to predict the tags over the entire sequence, but evaluate it only for lemmatized slots. `1 where ψ(·, ·, ·) ≥ 0 is an arbitrary potential, Z(`) normalizes the distribution, and m0 is a distinguished start-of-sequence symbol. 2019 In this work, we opt for a recurrent neural potential—specifically, we adopt a parameterization similar to the one given by Lample et al. (2016). Our potential ψ is computed as follows. First, the sequence ` is encoded into a sequence of word vectors using the strategy described by Ling et al. (2015): word vectors are passed to a bidirectional LSTM (Graves et al., 2005), where the corresponding hidden states are concatenated at each time step. We simply refer to the hidden state hi ∈ Rd as the result of said concatenation at the i-th step. Using hi , we can define the potential function as ψ (mi , mi−1 ) =  where Ami ,mi−1 is a exp Ami ,mi−1 + o> h , i mi transition weight matrix and omi ∈ Rd is a morphological tag embedding; both ar"
N19-1203,D15-1176,0,0.0216304,"lso be irregular. 3 In case of partially lemmatized sequence we still train the model to predict the tags over the entire sequence, but evaluate it only for lemmatized slots. `1 where ψ(·, ·, ·) ≥ 0 is an arbitrary potential, Z(`) normalizes the distribution, and m0 is a distinguished start-of-sequence symbol. 2019 In this work, we opt for a recurrent neural potential—specifically, we adopt a parameterization similar to the one given by Lample et al. (2016). Our potential ψ is computed as follows. First, the sequence ` is encoded into a sequence of word vectors using the strategy described by Ling et al. (2015): word vectors are passed to a bidirectional LSTM (Graves et al., 2005), where the corresponding hidden states are concatenated at each time step. We simply refer to the hidden state hi ∈ Rd as the result of said concatenation at the i-th step. Using hi , we can define the potential function as ψ (mi , mi−1 ) =  where Ami ,mi−1 is a exp Ami ,mi−1 + o> h , i mi transition weight matrix and omi ∈ Rd is a morphological tag embedding; both are learned. 2.3 The Morphological Inflector The conditional distribution p(wi |`i , mi ) is parameterized by a neural encoder–decoder model with hard attentio"
N19-1203,Q16-1037,0,0.481413,"a highly constrained version of language modeling. Language modeling predicts all words of a sentence from scratch, so the usual training and evaluation metric— perplexity—is dominated by the language model’s ability to predict content, which is where most of the uncertainty lies. Our task focuses on just the ability to reconstruct certain missing parts of the sentence—inflectional morphemes and their orthographic realization. This refocuses the modeling effort from semantic coherence to morphosyntactic coherence, an aspect of language that may take a back seat in current language models (see Linzen et al., 2016; Belinkov et al., 2017). Contextual inflection does not perfectly separate grammaticality modeling from content modeling: as illustrated in Tab. 1, mapping two cats _be_ sitting to the fully-inflected two cats were sitting does not require full knowledge of English grammar—the system does not have to predict the required word order nor the required auxiliary verb be, as these are supplied in the input. Conversely, this example does still require predicting some content—the semantic choice of past tense is not given by the input and must be guessed by the system.1 The primary contribution of t"
N19-1203,L16-1262,0,0.0579627,"Missing"
N19-1203,P02-1040,0,0.103705,"shifting positions (such as the instrumental), as well as those not associated with specific prepositions, are less well predicted. In addition, we evaluated the model’s performance when all forms are replaced by their corresponding lemmata (as in two cat be sit). For freer word order languages such as Polish or Latin, we observe a substantial drop in performance because most information on inter-word relations and their roles (expressed by means of case system) is lost. 5 Related Work The primary evaluation for most contemporary language and translation modeling research is perplexity, BLEU (Papineni et al., 2002), or METEOR (Banerjee and Lavie, 2005). Undoubtedly, such metrics are necessary for extrinsic evaluation and comparison. However, relatively few studies have focused on intrinsic evaluation of the model’s mastery of grammaticality. Recently, Linzen et al. (2016) investigated the ability of an LSTM language model to capture sentential structure, by evaluating subject–verb agreement with respect to number, and showed that under strong supervision, the LSTM is able to approximate dependencies. Taking it from the other perspective, a truer measure of grammatical competence would be a task of mappi"
N19-1203,petrov-etal-2012-universal,0,0.0315183,"Missing"
N19-1203,P17-2002,0,0.014134,"where the meaning representation specifies all necessary semantic content—content lemmata, dependency relations, and “inherent” closed-class morphemes (semantic features such as noun number, noun definiteness, and verb tense)—and the system is to realize this content according to the morphosyntactic conventions of a language, which means choosing word order, agreement morphemes, function words, and the surface forms of all words. Such tasks have been investigated to some extent—generating text from tectogrammatical trees (Hajic et al., 2002; Ptáˇcek and Žabokrtský, 2006) or from an AMR graph (Song et al., 2017). Belz et al. (2011) organized a related surface realization shared task on mapping unordered and uninflected dependency trees to properly ordered inflected sentences. The generated sentences were afterwards assessed by human annotators, making the task less scalable and more time consuming. Although our task is not perfectly matched to grammaticality modeling, the upside is that it is a “lightweight” task that works directly on text. No meaning representation is required. Thus, training and test data in any language can be prepared simply by lemmatizing a naturally occurring corpus. Finally,"
N19-1415,Y96-1001,0,0.215927,"ou zh¯ı zh¯ang ti´ao xi`ang d`ao pˇı d`un objects, general-purpose matters domesticated animals general animals flat objects long, narrow objects items, projects orders, roads, projections horses, cloth meals Table 1: Examples of Mandarin classifiers. Classifiers’ simplified Mandarin Chinese characters (1st column), p¯ıny¯ın pronunciations (2nd column), and commonly modified noun types (3rd column) are listed. Introduction Many of the world’s languages make use of numeral classifiers (Aikhenvald, 2000). While theoretical debate still rages on the function of numeral classifiers (Krifka, 1995; Ahrens and Huang, 1996; Cheng et al., 1998; Chierchia, 1998; Li, 2000; Nisbett, 2004; Bale and Coon, 2014), it is generally accepted that they need to be present for nouns to be modified by numerals, quantifiers, demonstratives, or other qualifiers (Li and Thompson, 1981, 104). In Mandarin Chinese, for instance, the phrase one person translates as 一个人 (y¯ı g`e r´en); the classifier 个 (g`e) has no clear translation in English, yet, nevertheless, it is necessary to place it between the numeral 一 (y¯ı) and the word for person 人 (r´en). There are hundreds of numeral classifiers in the Mandarin lexicon (Po-Ching and Rim"
N19-1415,P17-2096,0,0.0146383,"ordNet (Miller, 1998) synonym sets (synsets), assuming that each synset is independent. For nouns with multiple synsets, we assume that all synsets are equally probable for simplicity. If classifiers are fully semantically determined, then knowing a noun’s synsets should enable one to know the appropriate classifier(s), resulting in high MI. If classifiers are largely idiosyncratic, then noun synsets should have lower MI with classifiers. We do not use WordNet to attempt to capture word polysemy here. 3 Data and Experiments Data Provenance. We apply an existing neural Mandarin word segmenter (Cai et al., 2017) to the Chinese Gigaword corpus (Graff et al., 2005), and then feed the segmented corpus to a neural dependency parser, using Google’s pretrained Parsey Uni4102 H(C) H(C |N ) I(C; N ) H(C |S) I(C; S) H(C |A) I(C; A) 5.61 0.66 4.95 4.14 1.47 3.53 2.08 Table 3: Mutual information between classifiers and nouns I(C; N ), noun senses I(C; S), and adjectives I(C; A), is compared to their entropies. versal model on Mandarin.1 The model is trained on Universal Dependencies datasets v1.3.2 We extract classifier-noun pairs and adjective-classifiernoun triples from sentences, where the adjective and the"
N19-1415,N01-1021,0,0.0127449,"2002; Kirov and Cotterell, 2018) might be affected by predictability, and highly predictable noun-adjacent words, such as gender affixes in German and prenominal adjectives in English, are also shown to confer online processing advantages (Dye et al., 2016, 2017, 2018). Within the Chinese classifier system itself, the very common, general-purpose classifier 个 (g`e) is acquired by children earlier than rarer, more semantically rich ones (Hu, 1993). General classifiers are also found to occur more often in corpora with nouns that are less predictable in context (i.e., nouns with high surprisal; Hale 2001) (Zhan and Levy, 2018), providing initial evidence that predictability likely plays a role in classifiernoun pairing more generally. Furthermore, providing classifiers improves participants’ recall of nouns in laboratory experiments (Zhang and Schmitt, 1998; Gao and Malt, 2009) (but see Huang and Chen 2014)—but, it isn’t known whether classifiers do so by modulating noun predictability. 2 Quantifying Classifier Idiosyncrasy We take an information-theoretic approach to statistically quantify the idiosyncrasy of the Mandarin Chinese classifier system, and measure the uncertainty (entropy) reduct"
N19-1415,H93-1061,0,0.477408,"adjective and the classifier modify the same noun—this is easily determined from the parse. We also record the tuple counts, and use them to compute an empirical distribution over classifiers that modify nouns, and noun-adjective pairs, respectively. Data Preprocessing. Since no annotated supersense list exists for Mandarin, we first use CCCEDICT3 as a Mandarin Chinese-to-English dictionary to translate nouns and adjectives into English. Acknowledging that translating might introduce noise, we subsequently categorize our words into different senses using the SemCor supersense data for nouns (Miller et al., 1993; Tsvetkov et al., 2015), and adjectives (Tsvetkov et al., 2014). After that, we calculate the mutual information under each noun, and adjective supersense. Modeling Assumptions. As this contribution is the first to investigate classifier predictability, we make several simplifying assumptions. Extracting distributions over classifiers from a large corpus, as we do, ignores sentential context, which means we ignore the fact that some nouns (i.e., relational nouns, like m¯am¯a, Mom) are more likely to be found in verb frames or other constructions where classifiers are not needed. We also ignor"
N19-1415,D15-1243,0,0.152319,"information about classifier choice. Notation. Let C be a classifier-valued random variable with range C, the set of Mandarin Chinese 4101 classifiers. Let X be a second random variable, which models a second linguistic quantity, with range X . Mutual information (MI) is defined as I(C; X) ≡ H(C) − H(C |X) X p(c, x) = p(c, x) log p(c)p(x) (1a) (1b) c∈C,x∈X Let N and A denote the sets of nouns and adjectives, respectively, with N and A be noun- and adjective-valued random variables, respectively. Let Ni and Ai denote the sets of nouns and adjectives in ith SemCor supersense category for nouns (Tsvetkov et al., 2015) and adjectives (Tsvetkov et al., 2014), respectively, with their random variables being Ni and Ai , respectively. Let S be the set of all English WordNet (Miller, 1998) senses of nouns, with S be the WordNet sense-valued random variable. Given the formula above and any choice for X ∈ {N, A, Ni , Ai , S}, we can calculate the mutual information between classifiers and other relevant linguistic quantities. 2.1 MI between Classifiers and Nouns Mutual information between classifiers and nouns (I(C; N )) shows how much uncertainty (i.e., entropy) in classifiers can be reduced once we know the noun"
N19-1415,tsvetkov-etal-2014-augmenting-english,0,0.352127,"tation. Let C be a classifier-valued random variable with range C, the set of Mandarin Chinese 4101 classifiers. Let X be a second random variable, which models a second linguistic quantity, with range X . Mutual information (MI) is defined as I(C; X) ≡ H(C) − H(C |X) X p(c, x) = p(c, x) log p(c)p(x) (1a) (1b) c∈C,x∈X Let N and A denote the sets of nouns and adjectives, respectively, with N and A be noun- and adjective-valued random variables, respectively. Let Ni and Ai denote the sets of nouns and adjectives in ith SemCor supersense category for nouns (Tsvetkov et al., 2015) and adjectives (Tsvetkov et al., 2014), respectively, with their random variables being Ni and Ai , respectively. Let S be the set of all English WordNet (Miller, 1998) senses of nouns, with S be the WordNet sense-valued random variable. Given the formula above and any choice for X ∈ {N, A, Ni , Ai , S}, we can calculate the mutual information between classifiers and other relevant linguistic quantities. 2.1 MI between Classifiers and Nouns Mutual information between classifiers and nouns (I(C; N )) shows how much uncertainty (i.e., entropy) in classifiers can be reduced once we know the noun, and vice versa. Because only a few cl"
N19-1415,N18-1181,0,0.162604,"and Cotterell, 2018) might be affected by predictability, and highly predictable noun-adjacent words, such as gender affixes in German and prenominal adjectives in English, are also shown to confer online processing advantages (Dye et al., 2016, 2017, 2018). Within the Chinese classifier system itself, the very common, general-purpose classifier 个 (g`e) is acquired by children earlier than rarer, more semantically rich ones (Hu, 1993). General classifiers are also found to occur more often in corpora with nouns that are less predictable in context (i.e., nouns with high surprisal; Hale 2001) (Zhan and Levy, 2018), providing initial evidence that predictability likely plays a role in classifiernoun pairing more generally. Furthermore, providing classifiers improves participants’ recall of nouns in laboratory experiments (Zhang and Schmitt, 1998; Gao and Malt, 2009) (but see Huang and Chen 2014)—but, it isn’t known whether classifiers do so by modulating noun predictability. 2 Quantifying Classifier Idiosyncrasy We take an information-theoretic approach to statistically quantify the idiosyncrasy of the Mandarin Chinese classifier system, and measure the uncertainty (entropy) reduction—or mutual informat"
P14-2102,P14-1073,1,0.827801,"a contextual edit tend to raise or lower its probability (and the regularizer encourages such generalization). Each contextual edit (C, e) can be characterized as a 5-tuple (s, t, C1 , C20 , C3 ): it replaces s ∈ Σx ∪ {} with t ∈ Σy ∪ {} when s falls between C1 and C20 (so C2 = sC20 ) and t is preceded by C3 . Then each of the 14 features of (C, e) indicates that a particular subset of this 5-tuple has a particular value. The subset always includes s, t, or both. It never includes C1 or C20 without s, and never includes C3 without t. (Mays et al., 1991; Church and Gale, 1991; Kukich, 1992; Andrews et al., 2014). To eliminate experimental confounds, we use no dictionary or language model as one would in practice, but directly evaluate our ability to model p(correct |misspelled). Consider (xk , yk ) = (feeel, feel). Our model defines p(y |xk ) for all y. Our training objective (section 6) tries to make this large for y = yk . A contextual edit model learns here that e 7→  is more likely in the context of ee. We report on test data how much probability mass lands on the true yk . We also report how much mass lands “near” yk , by measuring the expected edit distance of the predicted y to Pthe truth. Ex"
P14-2102,N10-1083,0,0.0420762,"Missing"
P14-2102,P05-1057,0,0.0422806,"1 If N2 = 0, so that we do not condition on xi+1 , we must still condition on whether xi+1 = EOS (a single bit). We gloss over special handling for N2 = 0; but it is in our code. 625 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 625–630, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics This model is reminiscent of conditional models in MT that perform stepwise generation of one string or structure from another—e.g., string alignment models with contextual features (Cherry and Lin, 2003; Liu et al., 2005; Dyer et al., 2013), or tree transducers (Knight and Graehl, 2005). 3 • For each state q, the halt action and the arcs from q with input label  must have total probability of 1. (These are the available choices if there is no next input character.) • Every state q must be co-accessible, i.e., there must be a path of probability > 0 from q to some qF . (Otherwise, the PFST could lose some probability mass to infinite paths. The canonical case of this involves an loop q → q with input label  and probability 1.) Probabilistic FSTs We will construct a probabilistic finite-state transducer (PFST"
P14-2102,P03-1012,0,0.0533545,"t alignment of y to x. 1 If N2 = 0, so that we do not condition on xi+1 , we must still condition on whether xi+1 = EOS (a single bit). We gloss over special handling for N2 = 0; but it is in our code. 625 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 625–630, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics This model is reminiscent of conditional models in MT that perform stepwise generation of one string or structure from another—e.g., string alignment models with contextual features (Cherry and Lin, 2003; Liu et al., 2005; Dyer et al., 2013), or tree transducers (Knight and Graehl, 2005). 3 • For each state q, the halt action and the arcs from q with input label  must have total probability of 1. (These are the available choices if there is no next input character.) • Every state q must be co-accessible, i.e., there must be a path of probability > 0 from q to some qF . (Otherwise, the PFST could lose some probability mass to infinite paths. The canonical case of this involves an loop q → q with input label  and probability 1.) Probabilistic FSTs We will construct a probabilistic finite-stat"
P14-2102,J98-2005,0,0.103253,"ns and functions (Mohri, 1997; Eisner, 2001). A PFST is a two-tape generalization of the wellknown nondeterministic finite-state acceptor. It is a finite directed multigraph where each arc is labeled with an input in Σx ∪ {}, an output in Σy ∪{}, and a probability in [0, 1]. ( is the empty string.) Each state (i.e., vertex) has a halt probability in [0, 1], and there is a single initial state qI . Each path from qI to a final state qF has We take the first two conditions to be part of the definition of a PFST. The final condition requires our PFST to be “tight” in the same sense as a PCFG (Chi and Geman, 1998), although the tightness conditions for a PCFG are more complex. In section 7, we discuss the costs and benefits of PFSTs relative to other options. 4 The Contextual Edit PFST We now define a PFST topology that concisely captures the contextual edit process of section 2. We are given the alphabets Σx , Σy and the context window sizes N1 , N2 , N3 ≥ 0. For each possible context triple C = (C1 , C2 , C3 ) as defined in section 2, we construct an edit state qC whose outgoing arcs correspond to the possible edit operations in that context. One might expect that the SUBST(t) edit operation that rea"
P14-2102,P96-1031,0,0.1924,"r any x ∈ Σ∗x , the total probability of all paths accepting x is 1, so that pθ (y |x) is truly a conditional probability distribution. This is guaranteed by the following sufficient conditions (we omit the proof for space), which do not seem to appear in previous literature: • For each state q and each symbol b ∈ Σx , the arcs from q with input label b or  must have total probability of 1. (These are the available choices if the next input character is x.) 2 Several authors have given recipes for finite-state transducers that perform a single contextual edit operation (Kaplan and Kay, 1994; Mohri and Sproat, 1996; Gerdemann and van Noord, 1999). Such “rewrite rules” can be individually more expressive than our simple edit operations of section 2; but it is unclear how to train a cascade of them to model p(y |x). 626 a b x _ ε:z a bc z _ / ε re a d T(z SER ert ins z /1 c ε:ε )) ,x ,bc a )|( p(IN c: aa bc ba x _ c,x) (a,b (b) | E LET / p(DE te b dele ε:y /p (SU su b PFST, T , has O(|Σx |N1 +N2 |Σy |N3 ) states and O(|Σx |N1 +N2 |Σy |N3 +1 ) arcs. Composing this T with deterministic FSAs takes time linear in the size of the result, using a left-to-right, on-the-fly implementation of the composition opera"
P14-2102,J97-2003,0,0.0482753,"bility > 0 from q to some qF . (Otherwise, the PFST could lose some probability mass to infinite paths. The canonical case of this involves an loop q → q with input label  and probability 1.) Probabilistic FSTs We will construct a probabilistic finite-state transducer (PFST) that compactly models pθ (y | x) for all (x, y) pairs.2 Then various computations with this distribution can be reduced to standard finite-state computations that efficiently employ dynamic programming over the structure of the PFST, and the PFST can be easily combined with other finite-state distributions and functions (Mohri, 1997; Eisner, 2001). A PFST is a two-tape generalization of the wellknown nondeterministic finite-state acceptor. It is a finite directed multigraph where each arc is labeled with an input in Σx ∪ {}, an output in Σy ∪{}, and a probability in [0, 1]. ( is the empty string.) Each state (i.e., vertex) has a halt probability in [0, 1], and there is a single initial state qI . Each path from qI to a final state qF has We take the first two conditions to be part of the definition of a PFST. The final condition requires our PFST to be “tight” in the same sense as a PCFG (Chi and Geman, 1998), althoug"
P14-2102,D08-1113,1,0.806276,"duce xk to yk , relative to competing edits from the same contexts C. This means raising θ · f (C, e) and/or lowering ZC . Thus, log pθ (yk |xk ) depends only on the probabilities of edit arcs in T that appear in xk ◦ T ◦ yk , and the competing edit arcs from the same edit states qC . The gradient ∇θ log pθ (yk |xk ) takes the form &quot; # X X c(C, e) f~(C, e) − pθ (e0 |C)f~(C, e0 ) C,e ping this requirement gives a weighted FST (WFST), whose path weights w(x, y) can be globally normalized (divided by a constant Zx ) to obtain probabilities p(y |x). WFST models of contextual edits were studied by Dreyer et al. (2008). PFSTs and WFSTs are respectively related to MEMMs (McCallum et al., 2000) and CRFs (Lafferty et al., 2001). They gain added power from hidden states and  transitions (although to permit a finite-state encoding, they condition on x in a more restricted way than MEMMs and CRFs). WFSTs are likely to beat PFSTs as linguistic models,4 just as CRFs beat MEMMs (Klein and Manning, 2002). A WFST’s advantage is that the probability of an edit can be indirectly affected by the weights of other edits at a distance. Also, one could construct WFSTs where an edit’s weight directly considers local right ou"
P14-2102,N13-1073,0,0.0294951,"hat we do not condition on xi+1 , we must still condition on whether xi+1 = EOS (a single bit). We gloss over special handling for N2 = 0; but it is in our code. 625 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 625–630, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics This model is reminiscent of conditional models in MT that perform stepwise generation of one string or structure from another—e.g., string alignment models with contextual features (Cherry and Lin, 2003; Liu et al., 2005; Dyer et al., 2013), or tree transducers (Knight and Graehl, 2005). 3 • For each state q, the halt action and the arcs from q with input label  must have total probability of 1. (These are the available choices if there is no next input character.) • Every state q must be co-accessible, i.e., there must be a path of probability > 0 from q to some qF . (Otherwise, the PFST could lose some probability mass to infinite paths. The canonical case of this involves an loop q → q with input label  and probability 1.) Probabilistic FSTs We will construct a probabilistic finite-state transducer (PFST) that compactly mod"
P14-2102,E99-1017,0,0.178288,"Missing"
P14-2102,J94-3001,0,0.128363,"care to ensure that for any x ∈ Σ∗x , the total probability of all paths accepting x is 1, so that pθ (y |x) is truly a conditional probability distribution. This is guaranteed by the following sufficient conditions (we omit the proof for space), which do not seem to appear in previous literature: • For each state q and each symbol b ∈ Σx , the arcs from q with input label b or  must have total probability of 1. (These are the available choices if the next input character is x.) 2 Several authors have given recipes for finite-state transducers that perform a single contextual edit operation (Kaplan and Kay, 1994; Mohri and Sproat, 1996; Gerdemann and van Noord, 1999). Such “rewrite rules” can be individually more expressive than our simple edit operations of section 2; but it is unclear how to train a cascade of them to model p(y |x). 626 a b x _ ε:z a bc z _ / ε re a d T(z SER ert ins z /1 c ε:ε )) ,x ,bc a )|( p(IN c: aa bc ba x _ c,x) (a,b (b) | E LET / p(DE te b dele ε:y /p (SU su b PFST, T , has O(|Σx |N1 +N2 |Σy |N3 ) states and O(|Σx |N1 +N2 |Σy |N3 +1 ) arcs. Composing this T with deterministic FSAs takes time linear in the size of the result, using a left-to-right, on-the-fly implementation"
P14-2102,W02-1002,0,0.0248954,") C,e ping this requirement gives a weighted FST (WFST), whose path weights w(x, y) can be globally normalized (divided by a constant Zx ) to obtain probabilities p(y |x). WFST models of contextual edits were studied by Dreyer et al. (2008). PFSTs and WFSTs are respectively related to MEMMs (McCallum et al., 2000) and CRFs (Lafferty et al., 2001). They gain added power from hidden states and  transitions (although to permit a finite-state encoding, they condition on x in a more restricted way than MEMMs and CRFs). WFSTs are likely to beat PFSTs as linguistic models,4 just as CRFs beat MEMMs (Klein and Manning, 2002). A WFST’s advantage is that the probability of an edit can be indirectly affected by the weights of other edits at a distance. Also, one could construct WFSTs where an edit’s weight directly considers local right output context C4 . So why are we interested in PFSTs? Because they do not require computing a separate normalizing contant Zx for every x. This makes it computationally tractable to use them in settings where x is uncertain because it is unobserved, partially observed (e.g., lacks syllable boundaries), or noisily observed. E.g., at the end of section 5, X represented an uncertain x."
P16-1156,N15-1140,1,0.856025,"Missing"
P16-1156,K15-1017,1,0.876559,"Missing"
P16-1156,W13-3512,0,0.247022,"ative model. More importantly, in addition to modifying vectors of observed words, our model can generate vectors for forms not observed in the training data. Wieting et al. (2015) compute compositional embeddings of phrases, with their simplest method being additive (like ours) over the phrase’s words. Their embeddings are tuned to fit observed phrase similarity scores from PPDB (Ganitkevitch et al., 2013), which allows them to smooth and extend PPDB just as we do to WORD 2 VEC output. Using morphological resources to enhance embeddings at training time has been examined by numerous authors. Luong et al. (2013) used M OR FESSOR (Creutz and Lagus, 2007), an unsupervised morphological induction algorithm, to segment the training corpus. They then trained a recursive neural network (Goller and Kuchler, 1996; Socher, 2014) to generate compositional word embeddings. Our model is much simpler and faster to train. Their evaluation was limited to English and focused on rare English words. dos Santos and Zadrozny (2014) introduced a neural tagging architecture (Collobert et al., 2011) with a characterlevel convolutional layer. Qiu et al. (2014) and Botha and Blunsom (2014) both use M ORFESSOR segmentations t"
P16-1156,Q15-1031,1,0.858459,"tion, we fit a directed Gaussian graphical model (GGM) that simultaneously considers (i) each word’s embedding (obtained from an embedding model like WORD 2 VEC) and (ii) its morphological analysis (obtained from a lexical resource). We then use this model to smooth the provided embeddings, and to generate embeddings for unseen inflections. For a lemma covered by the resource, the GGM can produce embeddings for all its forms (if at least one of these forms has a known embedding); this can be extended to words not covered using a guesser like M ORFESSOR (Creutz and Lagus, 2007) or C HIP M UNK (Cotterell et al., 2015a). A major difference of our approach from related techniques is that our model uses existing morphological resources (e.g., morphological lexicons or finite-state analyzers) rather than semantic resources (e.g., WordNet (Miller et al., 1990) and PPDB (Ganitkevitch et al., 2013)). The former tend to be larger: we often can analyze more words than we have semantic representations for. It would be possible to integrate our GGM into the training procedure for a word embedding system, making that system sensitive to morphological attributes. However, the postprocessing approach in our present pap"
P16-1156,N13-1090,0,0.511556,"in the literature. One of the most interesting discoveries is that these representations capture meaningful morpho-syntactic and semantic properties through very simple linear relations: in a semantic vector space, we observe that vtalked − vtalk ≈ vdrank − vdrink . (1) That this equation approximately holds across many morphologically related 4-tuples indicates that the learned embeddings capture a feature of English morphology—adding the past tense feature roughly corresponds to adding a certain vector. Moreover, manipulating this equation yields what we will call the vector offset method (Mikolov et al., 2013c) for approximating other vectors. For instance, if we only know the vectors for the Spanish words comieron (ate), comemos (eat) and bebieron (drank), we can produce an approximation of the vector for bebemos (drink), as shown in Figure 1. Many languages exhibit much richer morphology than English. While English nouns commonly take two forms – singular and plural— Czech nouns take 12 and Turkish nouns take over 30. This increase in word forms per lemma creates considerable data sparsity. Fortunately, for many languages there exist large morphological lexicons, or better yet, morphological too"
P16-1156,N15-1184,0,0.0886934,"omit i from all Wk . After convergence, set wi ← k∈Mi mk . 4 Note that it is not necessary to define it as λ0 I, introducing a new scale parameter λ0 , since doubling λ0 would have the same effect on the MAP update rules as halving λ and Σi . 1654 Viterbi EM can be regarded as block coordinate descent on the negative log-likelihood function, with E and M steps both improving this common objective along different variables. We update the parameters (M step above) after each 10 passes of updating the latent vectors (section 5’s E step). 7 Related Work Our postprocessing strategy is inspired by Faruqui et al. (2015), who designed a retrofitting procedure to modify pre-trained vectors such that their relations match those found in semantic lexicons. We focus on morphological resources, rather than semantic lexicons, and employ a generative model. More importantly, in addition to modifying vectors of observed words, our model can generate vectors for forms not observed in the training data. Wieting et al. (2015) compute compositional embeddings of phrases, with their simplest method being additive (like ours) over the phrase’s words. Their embeddings are tuned to fit observed phrase similarity scores from"
P16-1156,W15-1518,0,0.0367522,"Missing"
P16-1156,N13-1092,0,0.105894,"Missing"
P16-1156,C14-1015,0,0.0215187,"ddings at training time has been examined by numerous authors. Luong et al. (2013) used M OR FESSOR (Creutz and Lagus, 2007), an unsupervised morphological induction algorithm, to segment the training corpus. They then trained a recursive neural network (Goller and Kuchler, 1996; Socher, 2014) to generate compositional word embeddings. Our model is much simpler and faster to train. Their evaluation was limited to English and focused on rare English words. dos Santos and Zadrozny (2014) introduced a neural tagging architecture (Collobert et al., 2011) with a characterlevel convolutional layer. Qiu et al. (2014) and Botha and Blunsom (2014) both use M ORFESSOR segmentations to augment WORD 2 VEC and a logbilinear (LBL) language model (Mnih and Hinton, 2007), respectively. Similar to us, they have an additive model of the semantics of morphemes, i.e., the embedding of the word form is the sum of the embeddings of its constituents. In contrast to us, however, both include the word form itself in the sum. Finally, Cotterell and Sch¨utze (2015) jointly trained an LBL language model and a morphological tagger (Hajiˇc, 2000) to encourage the embeddings to encode rich morphology. With the exception of (Cott"
P16-1156,A00-2013,0,0.0304152,"Missing"
P16-1156,D09-1124,0,0.00984588,"e confined to rare predicting words.) See Appendix B for more analysis. 1658 Forms / Lemma Skip-Gram GGM English 1.8 58.9 58.9 German 6.3 36.2 37.6 Spanish 8.1 37.8 40.3 Table 4: Word similarity results (correlations) using the WS353 dataset in the three languages, in which it is available. Since all the words in WS-353 are lemmata, we report the average inflected form to lemma ratio for forms appearing in the datasets. 8.3 9 Experiment 3: Word Similarity As a third and final experiment, we consider word similarity using the WS-353 data set (Finkelstein et al., 2001), translated into Spanish (Hassan and Mihalcea, 2009) and German (Leviant, 2016).11 The datasets are composed of 353 pairs of words. Multiple native speakers were then asked to give an integral value between 1 and 10 indicating the similarity of that pair, and those values were then averaged. In each case, we train the GGM on the whole Wikipedia corpus for the language. Since in each language every word in the WS-353 set is in fact a lemma, we use the latent embedding our GGM learns in the experiment. In Spanish, for example, we use the learned latent morpheme embedding for the lemma BEBER (recall this takes information from every element in the"
P16-1156,P15-2111,0,0.0948168,"must instead select from its paradigm the word type, such as beb´eis, that expresses the contextually appropriate properties. Noun tokens in a language may similarly be required to be inflected for properties such as case, gender, and number. A content word is chosen by specifying a lemma (which selects a particular paradigm) together with some inflectional attributes (which select a particular slot within that paradigm). For example, [ Lemma=EAT, Person=3, Number=S INGULAR, Tense=P RESENT ] is a bundle of attribute-value pairs that would be jointly expressed in English by the word form eats (Sylak-Glassman et al., 2015). The regularities observed by Mikolov et al. (2013c) hold between words with similar attributevalue pairs. In Spanish, the word beben “they drink” (Table 1) can be analyzed as expressing the bundle [ Lemma=BEBER, Person=3, Number=P LURAL, Tense=P RESENT ]. Its vector similarity to bebemos “we drink” is due to the fact that both word forms have the same lemma BE BER . Likewise, the vector similarity of beben to comieron “they ate” is due to the conceptual similarity of their lemmas, BEBER “drink” and COMER “eat”. Conversely, that beben is similar to preguntan “they ask” is caused by shared inf"
P16-1156,W15-0105,0,0.0336122,"Missing"
P16-1156,P15-2008,0,0.0459499,"Missing"
P16-1156,Q15-1025,0,0.0118304,"long different variables. We update the parameters (M step above) after each 10 passes of updating the latent vectors (section 5’s E step). 7 Related Work Our postprocessing strategy is inspired by Faruqui et al. (2015), who designed a retrofitting procedure to modify pre-trained vectors such that their relations match those found in semantic lexicons. We focus on morphological resources, rather than semantic lexicons, and employ a generative model. More importantly, in addition to modifying vectors of observed words, our model can generate vectors for forms not observed in the training data. Wieting et al. (2015) compute compositional embeddings of phrases, with their simplest method being additive (like ours) over the phrase’s words. Their embeddings are tuned to fit observed phrase similarity scores from PPDB (Ganitkevitch et al., 2013), which allows them to smooth and extend PPDB just as we do to WORD 2 VEC output. Using morphological resources to enhance embeddings at training time has been examined by numerous authors. Luong et al. (2013) used M OR FESSOR (Creutz and Lagus, 2007), an unsupervised morphological induction algorithm, to segment the training corpus. They then trained a recursive neur"
P17-1109,P15-2085,0,0.0169058,"IPA along the x-axis). in NLP. Determinantal point processes have found a home in the literature in tasks that require diversity. E.g., DPPs have achieved state-of-the-art results on multi-document document summarization (Kulesza and Taskar, 2011), news article selection (Affandi et al., 2012) recommender systems (Gartrell et al., 2017), joint clustering of verbal lexical semantic properties (Reichart and Korhonen, 2013), inter alia. Poisson point processes have also been applied to NLP problems: Yee et al. (2015) model the emerging topic on social media using a homogeneous point process and Lukasik et al. (2015) apply a log-Gaussian point process, a variant of the Poisson point process, to rumor detection in Twitter. We are unaware of previous attempts to probabilistically model vowel inventory typology. Future Work. This work lends itself to several technical extensions. One could expand the function f to more completely characterize each vowel’s acoustic properties, perceptual properties, or distinctive features (footnote 7). One could generalize our point process models to sample finite subsets from the continuous space of vowels (footnote 3). One could consider augmenting the MPP with a new facto"
P17-1109,P13-1085,0,0.0201066,"10 0 0 i u a o e ɔ ɛ ɪ y ʊ ɑ ø æ ə ɨ œ ʏ ɯ ʌ ɤ ɒ ɵ ʉ ɜ ɐ e ö Figure 4: Percentage of the vowel inventories (y-axis) in the Becker-Kristal corpus (Becker-Kristal, 2010) that have a given vowel (shown in IPA along the x-axis). in NLP. Determinantal point processes have found a home in the literature in tasks that require diversity. E.g., DPPs have achieved state-of-the-art results on multi-document document summarization (Kulesza and Taskar, 2011), news article selection (Affandi et al., 2012) recommender systems (Gartrell et al., 2017), joint clustering of verbal lexical semantic properties (Reichart and Korhonen, 2013), inter alia. Poisson point processes have also been applied to NLP problems: Yee et al. (2015) model the emerging topic on social media using a homogeneous point process and Lukasik et al. (2015) apply a log-Gaussian point process, a variant of the Poisson point process, to rumor detection in Twitter. We are unaware of previous attempts to probabilistically model vowel inventory typology. Future Work. This work lends itself to several technical extensions. One could expand the function f to more completely characterize each vowel’s acoustic properties, perceptual properties, or distinctive fe"
P17-1109,W15-0815,0,0.0148679,"es (y-axis) in the Becker-Kristal corpus (Becker-Kristal, 2010) that have a given vowel (shown in IPA along the x-axis). in NLP. Determinantal point processes have found a home in the literature in tasks that require diversity. E.g., DPPs have achieved state-of-the-art results on multi-document document summarization (Kulesza and Taskar, 2011), news article selection (Affandi et al., 2012) recommender systems (Gartrell et al., 2017), joint clustering of verbal lexical semantic properties (Reichart and Korhonen, 2013), inter alia. Poisson point processes have also been applied to NLP problems: Yee et al. (2015) model the emerging topic on social media using a homogeneous point process and Lukasik et al. (2015) apply a log-Gaussian point process, a variant of the Poisson point process, to rumor detection in Twitter. We are unaware of previous attempts to probabilistically model vowel inventory typology. Future Work. This work lends itself to several technical extensions. One could expand the function f to more completely characterize each vowel’s acoustic properties, perceptual properties, or distinctive features (footnote 7). One could generalize our point process models to sample finite subsets fro"
P17-1182,E14-1060,0,0.185562,"Missing"
P17-1182,Q16-1031,0,0.0196567,"we see large performance gains, at least for the 200 sample case: 0.38 (0→ES) vs. 0.54 (PT→ES) and 0.56 (AR→ES). This is evidence that our conjecture is correct that the baseline Arabic mainly acts as a regularizer that prevents the model from memorizing the training set and therefore improves performance. So performance improves even though no true transfer of morphological knowledge takes place. 6 Related Work Cross-lingual transfer learning has been used for many tasks, e.g., automatic speech recognition (Huang et al., 2013), parsing (Cohen et al., 2011; Søgaard, 2011; Naseem et al., 2012; Ammar et al., 2016), language modeling (Tsvetkov et al., 2016), entity recognition (Wang and Manning, 2014b) and machine translation (Johnson et al., 2016; Ha et al., 2016). One straightforward method is to translate datasets and then train a monolingual model (Fortuna and Shawe-Taylor, 2005; Olsson et al., 2005). Also, aligned corpora have been used to project information from annotations in one language to another (Yarowsky et al., 2001; Pad´o and Lapata, 2005). The drawback is that machine translation 2000 errors cause errors in the target. Therefore, alternative methods have been proposed, e.g., to port a mo"
P17-1182,P16-1184,0,0.0440907,"slation (Johnson et al., 2016; Ha et al., 2016). One straightforward method is to translate datasets and then train a monolingual model (Fortuna and Shawe-Taylor, 2005; Olsson et al., 2005). Also, aligned corpora have been used to project information from annotations in one language to another (Yarowsky et al., 2001; Pad´o and Lapata, 2005). The drawback is that machine translation 2000 errors cause errors in the target. Therefore, alternative methods have been proposed, e.g., to port a model trained on the source language to the target language (Shi et al., 2010). In the realm of morphology, Buys and Botha (2016) recently adapted methods for the training of POS taggers to learn weakly supervised morphological taggers with the help of parallel text. Snyder and Barzilay (2008a, 2008b) developed a non-parametric Bayesian model for morphological segmentation. They performed identification of cross-lingual abstract morphemes and segmentation simultaneously and reported, similar to us, best results for related languages. Work on paradigm completion has recently been encouraged by the SIGMORPHON 2016 shared task on morphological reinflection (Cotterell et al., 2016a). Some work first applies an unsupervised"
P17-1182,W14-4012,0,0.19462,"Missing"
P17-1182,P17-2061,0,0.0549325,"Missing"
P17-1182,D11-1005,0,0.0462193,"sfer with transfer from a ciphered language to Spanish, we see large performance gains, at least for the 200 sample case: 0.38 (0→ES) vs. 0.54 (PT→ES) and 0.56 (AR→ES). This is evidence that our conjecture is correct that the baseline Arabic mainly acts as a regularizer that prevents the model from memorizing the training set and therefore improves performance. So performance improves even though no true transfer of morphological knowledge takes place. 6 Related Work Cross-lingual transfer learning has been used for many tasks, e.g., automatic speech recognition (Huang et al., 2013), parsing (Cohen et al., 2011; Søgaard, 2011; Naseem et al., 2012; Ammar et al., 2016), language modeling (Tsvetkov et al., 2016), entity recognition (Wang and Manning, 2014b) and machine translation (Johnson et al., 2016; Ha et al., 2016). One straightforward method is to translate datasets and then train a monolingual model (Fortuna and Shawe-Taylor, 2005; Olsson et al., 2005). Also, aligned corpora have been used to project information from annotations in one language to another (Yarowsky et al., 2001; Pad´o and Lapata, 2005). The drawback is that machine translation 2000 errors cause errors in the target. Therefore, a"
P17-1182,P15-1166,0,0.0464722,"o, define the current state of the art. Encoder-decoder RNNs were developed in parallel by Cho et al. (2014) and Sutskever et al. (2014) for machine translation and extended by Bahdanau et al. (2015) with an attention mechanism, supporting better generalization. They have been applied to NLP tasks like speech recognition (Graves and Schmidhuber, 2005; Graves et al., 2013), parsing (Vinyals et al., 2015) and segmentation (Kann et al., 2016). More recently, a number of papers have used encoder-decoder RNNs in multitask and transfer learning settings; this is mainly work in machine translation: (Dong et al., 2015; Zoph and Knight, 2016; Chu et al., 2017; Johnson et al., 2016; Luong et al., 2016; Firat et al., 2016; Ha et al., 2016), inter alia. Each of these papers has both similarities and differences with our approach. (i) Most train several distinct models whereas we train a single model on input augmented with an explicit encoding of the language (similar to (Johnson et al., 2016)). (ii) Let k and m be the number of different input and output languages. We address the case k ∈ {1, 2, 3} and m = k. Other work has addressed cases with k > 3 or m > 3; this would be an interesting avenue of future res"
P17-1182,N13-1138,0,0.107841,"ical taggers with the help of parallel text. Snyder and Barzilay (2008a, 2008b) developed a non-parametric Bayesian model for morphological segmentation. They performed identification of cross-lingual abstract morphemes and segmentation simultaneously and reported, similar to us, best results for related languages. Work on paradigm completion has recently been encouraged by the SIGMORPHON 2016 shared task on morphological reinflection (Cotterell et al., 2016a). Some work first applies an unsupervised alignment model to source and target string pairs and then learns a string-to-string mapping (Durrett and DeNero, 2013; Nicolai et al., 2015), using, e.g., a semi-Markov conditional random field (Sarawagi and Cohen, 2004). Encoderdecoder RNNs (Aharoni et al., 2016; Faruqui et al., 2016; Kann and Sch¨utze, 2016b), a method which our work further develops for the cross-lingual scenario, define the current state of the art. Encoder-decoder RNNs were developed in parallel by Cho et al. (2014) and Sutskever et al. (2014) for machine translation and extended by Bahdanau et al. (2015) with an attention mechanism, supporting better generalization. They have been applied to NLP tasks like speech recognition (Graves an"
P17-1182,P08-1115,0,0.0686385,"nt of tools for computational morphology with limited annotated data. In many languages, individual lexical entries may be realized as distinct inflections of a single lemma depending on the syntactic context. For example, the 3SgPresInd of the English verbal lemma to bring is brings. In morphologically rich languages, a lemma can have hundreds of individual forms. Thus, both generation and analysis of such morphological inflections are active areas of research in NLP and morphological processing has been shown to be a boon to several other down-stream applications, e.g., machine translation (Dyer et al., 2008), speech recognition (Creutz et al., 2007), parsing (Seeker and C¸etino˘glu, 2015), keyword spotting (Narasimhan et al., 2014) and word embeddings (Cotterell et al., 2016b), inter alia. In this work, we focus on paradigm completion, a form of morphological generation that maps a given lemma to a target inflection, e.g., (bring, Past) 7→ brought (with Past being the target tag). RNN sequence-to-sequence models (Sutskever et al., 2014; Bahdanau et al., 2015) are the state of the art for paradigm completion (Faruqui et al., 2016; Kann and Sch¨utze, 2016a; Cotterell et al., 2016a). However, these"
P17-1182,N16-1077,0,0.272749,"to several other down-stream applications, e.g., machine translation (Dyer et al., 2008), speech recognition (Creutz et al., 2007), parsing (Seeker and C¸etino˘glu, 2015), keyword spotting (Narasimhan et al., 2014) and word embeddings (Cotterell et al., 2016b), inter alia. In this work, we focus on paradigm completion, a form of morphological generation that maps a given lemma to a target inflection, e.g., (bring, Past) 7→ brought (with Past being the target tag). RNN sequence-to-sequence models (Sutskever et al., 2014; Bahdanau et al., 2015) are the state of the art for paradigm completion (Faruqui et al., 2016; Kann and Sch¨utze, 2016a; Cotterell et al., 2016a). However, these models require a large amount of data to achieve competitive performance; this makes them unsuitable for out-of-thebox application to paradigm completion in the low-resource scenario. To mitigate this, we consider transfer learning: we train an end-to-end neural system jointly with limited data from a lowresource language and a larger amount of data from a high-resource language. This technique allows 1993 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1993–2003 c Vancouver, Can"
P17-1182,N16-1101,0,0.0242819,"(2014) and Sutskever et al. (2014) for machine translation and extended by Bahdanau et al. (2015) with an attention mechanism, supporting better generalization. They have been applied to NLP tasks like speech recognition (Graves and Schmidhuber, 2005; Graves et al., 2013), parsing (Vinyals et al., 2015) and segmentation (Kann et al., 2016). More recently, a number of papers have used encoder-decoder RNNs in multitask and transfer learning settings; this is mainly work in machine translation: (Dong et al., 2015; Zoph and Knight, 2016; Chu et al., 2017; Johnson et al., 2016; Luong et al., 2016; Firat et al., 2016; Ha et al., 2016), inter alia. Each of these papers has both similarities and differences with our approach. (i) Most train several distinct models whereas we train a single model on input augmented with an explicit encoding of the language (similar to (Johnson et al., 2016)). (ii) Let k and m be the number of different input and output languages. We address the case k ∈ {1, 2, 3} and m = k. Other work has addressed cases with k > 3 or m > 3; this would be an interesting avenue of future research for paradigm completion. (iii) Whereas training RNNs in machine translation is hard, we only expe"
P17-1182,Q15-1031,1,0.880055,"Missing"
P17-1182,Q17-1024,0,0.0696793,"Missing"
P17-1182,P16-1156,1,0.900318,"Missing"
P17-1182,D16-1097,1,0.909381,"Missing"
P17-1182,P08-1084,0,0.0879979,"r, 2005; Olsson et al., 2005). Also, aligned corpora have been used to project information from annotations in one language to another (Yarowsky et al., 2001; Pad´o and Lapata, 2005). The drawback is that machine translation 2000 errors cause errors in the target. Therefore, alternative methods have been proposed, e.g., to port a model trained on the source language to the target language (Shi et al., 2010). In the realm of morphology, Buys and Botha (2016) recently adapted methods for the training of POS taggers to learn weakly supervised morphological taggers with the help of parallel text. Snyder and Barzilay (2008a, 2008b) developed a non-parametric Bayesian model for morphological segmentation. They performed identification of cross-lingual abstract morphemes and segmentation simultaneously and reported, similar to us, best results for related languages. Work on paradigm completion has recently been encouraged by the SIGMORPHON 2016 shared task on morphological reinflection (Cotterell et al., 2016a). Some work first applies an unsupervised alignment model to source and target string pairs and then learns a string-to-string mapping (Durrett and DeNero, 2013; Nicolai et al., 2015), using, e.g., a semi-M"
P17-1182,P16-2090,1,0.823613,"Missing"
P17-1182,P11-2120,0,0.0812648,"rom a ciphered language to Spanish, we see large performance gains, at least for the 200 sample case: 0.38 (0→ES) vs. 0.54 (PT→ES) and 0.56 (AR→ES). This is evidence that our conjecture is correct that the baseline Arabic mainly acts as a regularizer that prevents the model from memorizing the training set and therefore improves performance. So performance improves even though no true transfer of morphological knowledge takes place. 6 Related Work Cross-lingual transfer learning has been used for many tasks, e.g., automatic speech recognition (Huang et al., 2013), parsing (Cohen et al., 2011; Søgaard, 2011; Naseem et al., 2012; Ammar et al., 2016), language modeling (Tsvetkov et al., 2016), entity recognition (Wang and Manning, 2014b) and machine translation (Johnson et al., 2016; Ha et al., 2016). One straightforward method is to translate datasets and then train a monolingual model (Fortuna and Shawe-Taylor, 2005; Olsson et al., 2005). Also, aligned corpora have been used to project information from annotations in one language to another (Yarowsky et al., 2001; Pad´o and Lapata, 2005). The drawback is that machine translation 2000 errors cause errors in the target. Therefore, alternative meth"
P17-1182,L16-1498,0,0.017455,"e transfer and use it as a baseline for all target languages. 5 Accuracy 0.8 Exp. 1: Transfer Learning for Paradigm Completion In this experiment, we investigate to what extent our model transfers morphological knowledge from a high-resource source language to a low-resource target language. We experimentally answer three questions. (i) Is transfer learning possible for morphology? (ii) How much annotated data do we need in the low-resource target language? (iii) How closely related must the two languages be to achieve good results? Data. Based on complete inflection tables from unimorph.org (Kirov et al., 2016), we create datasets as follows. Each training set consists of 12,000 samples in the high-resource source 0.6 0.4 0.2 Languages Pt Ca It Fr Ar Es 0.0 0 50·2 50·21 50·22 50·23 50·24 50·25 50·26 50·27 Number of Samples Experiments We run four experiments on 21 distinct pairings of languages to show the feasibility of morphological transfer and analyze our method. We first discuss details common to all experiments. We keep hyperparameters during all experiments (and for all languages) fixed to the following values. Encoder and decoder RNNs each have 100 hidden units and the size of all subtag, ch"
P17-1182,D14-1095,0,0.0808608,"be realized as distinct inflections of a single lemma depending on the syntactic context. For example, the 3SgPresInd of the English verbal lemma to bring is brings. In morphologically rich languages, a lemma can have hundreds of individual forms. Thus, both generation and analysis of such morphological inflections are active areas of research in NLP and morphological processing has been shown to be a boon to several other down-stream applications, e.g., machine translation (Dyer et al., 2008), speech recognition (Creutz et al., 2007), parsing (Seeker and C¸etino˘glu, 2015), keyword spotting (Narasimhan et al., 2014) and word embeddings (Cotterell et al., 2016b), inter alia. In this work, we focus on paradigm completion, a form of morphological generation that maps a given lemma to a target inflection, e.g., (bring, Past) 7→ brought (with Past being the target tag). RNN sequence-to-sequence models (Sutskever et al., 2014; Bahdanau et al., 2015) are the state of the art for paradigm completion (Faruqui et al., 2016; Kann and Sch¨utze, 2016a; Cotterell et al., 2016a). However, these models require a large amount of data to achieve competitive performance; this makes them unsuitable for out-of-thebox applica"
P17-1182,P12-1066,0,0.0488593,"language to Spanish, we see large performance gains, at least for the 200 sample case: 0.38 (0→ES) vs. 0.54 (PT→ES) and 0.56 (AR→ES). This is evidence that our conjecture is correct that the baseline Arabic mainly acts as a regularizer that prevents the model from memorizing the training set and therefore improves performance. So performance improves even though no true transfer of morphological knowledge takes place. 6 Related Work Cross-lingual transfer learning has been used for many tasks, e.g., automatic speech recognition (Huang et al., 2013), parsing (Cohen et al., 2011; Søgaard, 2011; Naseem et al., 2012; Ammar et al., 2016), language modeling (Tsvetkov et al., 2016), entity recognition (Wang and Manning, 2014b) and machine translation (Johnson et al., 2016; Ha et al., 2016). One straightforward method is to translate datasets and then train a monolingual model (Fortuna and Shawe-Taylor, 2005; Olsson et al., 2005). Also, aligned corpora have been used to project information from annotations in one language to another (Yarowsky et al., 2001; Pad´o and Lapata, 2005). The drawback is that machine translation 2000 errors cause errors in the target. Therefore, alternative methods have been propose"
P17-1182,N15-1093,0,0.123208,"of parallel text. Snyder and Barzilay (2008a, 2008b) developed a non-parametric Bayesian model for morphological segmentation. They performed identification of cross-lingual abstract morphemes and segmentation simultaneously and reported, similar to us, best results for related languages. Work on paradigm completion has recently been encouraged by the SIGMORPHON 2016 shared task on morphological reinflection (Cotterell et al., 2016a). Some work first applies an unsupervised alignment model to source and target string pairs and then learns a string-to-string mapping (Durrett and DeNero, 2013; Nicolai et al., 2015), using, e.g., a semi-Markov conditional random field (Sarawagi and Cohen, 2004). Encoderdecoder RNNs (Aharoni et al., 2016; Faruqui et al., 2016; Kann and Sch¨utze, 2016b), a method which our work further develops for the cross-lingual scenario, define the current state of the art. Encoder-decoder RNNs were developed in parallel by Cho et al. (2014) and Sutskever et al. (2014) for machine translation and extended by Bahdanau et al. (2015) with an attention mechanism, supporting better generalization. They have been applied to NLP tasks like speech recognition (Graves and Schmidhuber, 2005; Gr"
P17-1182,H05-1108,0,0.117788,"Missing"
P17-1182,petrov-etal-2012-universal,0,0.0908597,"Missing"
P17-1182,Q15-1026,0,0.0607982,"Missing"
P17-1182,D10-1103,0,0.0291944,"tion (Wang and Manning, 2014b) and machine translation (Johnson et al., 2016; Ha et al., 2016). One straightforward method is to translate datasets and then train a monolingual model (Fortuna and Shawe-Taylor, 2005; Olsson et al., 2005). Also, aligned corpora have been used to project information from annotations in one language to another (Yarowsky et al., 2001; Pad´o and Lapata, 2005). The drawback is that machine translation 2000 errors cause errors in the target. Therefore, alternative methods have been proposed, e.g., to port a model trained on the source language to the target language (Shi et al., 2010). In the realm of morphology, Buys and Botha (2016) recently adapted methods for the training of POS taggers to learn weakly supervised morphological taggers with the help of parallel text. Snyder and Barzilay (2008a, 2008b) developed a non-parametric Bayesian model for morphological segmentation. They performed identification of cross-lingual abstract morphemes and segmentation simultaneously and reported, similar to us, best results for related languages. Work on paradigm completion has recently been encouraged by the SIGMORPHON 2016 shared task on morphological reinflection (Cotterell et al"
P17-1182,P15-2111,0,0.0361423,"Past Indicative Indicative Sg Pl Sg Pl sue˜no sue˜nas sue˜na so˜namos so˜na´ is sue˜nan so˜ne´ so˜naste so˜no´ so˜namos so˜nasteis so˜naron Table 1: Partial inflection table for the Spanish verb so˜nar. Introduction Low-resource natural language processing (NLP) remains an open problem for many tasks of interest. Furthermore, for most languages in the world, highcost linguistic annotation and resource creation are unlikely to be undertaken in the near future. In the case of morphology, out of the 7000 currently spoken (Lewis, 2009) languages, only about 200 have computer-readable annotations (Sylak-Glassman et al., 2015) – although morphology is easy to annotate compared to syntax and semantics. Transfer learning is one solution to this problem: it exploits annotations in a high-resource language to train a system for a low-resource language. In this work, we present a method for cross-lingual transfer of inflectional morphology using an encoder-decoder recurrent neural network (RNN). This allows for the development of tools for computational morphology with limited annotated data. In many languages, individual lexical entries may be realized as distinct inflections of a single lemma depending on the syntacti"
P17-1182,N16-1161,0,0.0339616,"for the 200 sample case: 0.38 (0→ES) vs. 0.54 (PT→ES) and 0.56 (AR→ES). This is evidence that our conjecture is correct that the baseline Arabic mainly acts as a regularizer that prevents the model from memorizing the training set and therefore improves performance. So performance improves even though no true transfer of morphological knowledge takes place. 6 Related Work Cross-lingual transfer learning has been used for many tasks, e.g., automatic speech recognition (Huang et al., 2013), parsing (Cohen et al., 2011; Søgaard, 2011; Naseem et al., 2012; Ammar et al., 2016), language modeling (Tsvetkov et al., 2016), entity recognition (Wang and Manning, 2014b) and machine translation (Johnson et al., 2016; Ha et al., 2016). One straightforward method is to translate datasets and then train a monolingual model (Fortuna and Shawe-Taylor, 2005; Olsson et al., 2005). Also, aligned corpora have been used to project information from annotations in one language to another (Yarowsky et al., 2001; Pad´o and Lapata, 2005). The drawback is that machine translation 2000 errors cause errors in the target. Therefore, alternative methods have been proposed, e.g., to port a model trained on the source language to the t"
P17-1182,Q14-1005,0,0.342964,"54 (PT→ES) and 0.56 (AR→ES). This is evidence that our conjecture is correct that the baseline Arabic mainly acts as a regularizer that prevents the model from memorizing the training set and therefore improves performance. So performance improves even though no true transfer of morphological knowledge takes place. 6 Related Work Cross-lingual transfer learning has been used for many tasks, e.g., automatic speech recognition (Huang et al., 2013), parsing (Cohen et al., 2011; Søgaard, 2011; Naseem et al., 2012; Ammar et al., 2016), language modeling (Tsvetkov et al., 2016), entity recognition (Wang and Manning, 2014b) and machine translation (Johnson et al., 2016; Ha et al., 2016). One straightforward method is to translate datasets and then train a monolingual model (Fortuna and Shawe-Taylor, 2005; Olsson et al., 2005). Also, aligned corpora have been used to project information from annotations in one language to another (Yarowsky et al., 2001; Pad´o and Lapata, 2005). The drawback is that machine translation 2000 errors cause errors in the target. Therefore, alternative methods have been proposed, e.g., to port a model trained on the source language to the target language (Shi et al., 2010). In the re"
P17-1182,D15-1083,1,0.848421,"Missing"
P17-1182,H01-1035,0,0.102341,"-lingual transfer learning has been used for many tasks, e.g., automatic speech recognition (Huang et al., 2013), parsing (Cohen et al., 2011; Søgaard, 2011; Naseem et al., 2012; Ammar et al., 2016), language modeling (Tsvetkov et al., 2016), entity recognition (Wang and Manning, 2014b) and machine translation (Johnson et al., 2016; Ha et al., 2016). One straightforward method is to translate datasets and then train a monolingual model (Fortuna and Shawe-Taylor, 2005; Olsson et al., 2005). Also, aligned corpora have been used to project information from annotations in one language to another (Yarowsky et al., 2001; Pad´o and Lapata, 2005). The drawback is that machine translation 2000 errors cause errors in the target. Therefore, alternative methods have been proposed, e.g., to port a model trained on the source language to the target language (Shi et al., 2010). In the realm of morphology, Buys and Botha (2016) recently adapted methods for the training of POS taggers to learn weakly supervised morphological taggers with the help of parallel text. Snyder and Barzilay (2008a, 2008b) developed a non-parametric Bayesian model for morphological segmentation. They performed identification of cross-lingual a"
P17-1182,N16-1004,0,0.0198626,"nt state of the art. Encoder-decoder RNNs were developed in parallel by Cho et al. (2014) and Sutskever et al. (2014) for machine translation and extended by Bahdanau et al. (2015) with an attention mechanism, supporting better generalization. They have been applied to NLP tasks like speech recognition (Graves and Schmidhuber, 2005; Graves et al., 2013), parsing (Vinyals et al., 2015) and segmentation (Kann et al., 2016). More recently, a number of papers have used encoder-decoder RNNs in multitask and transfer learning settings; this is mainly work in machine translation: (Dong et al., 2015; Zoph and Knight, 2016; Chu et al., 2017; Johnson et al., 2016; Luong et al., 2016; Firat et al., 2016; Ha et al., 2016), inter alia. Each of these papers has both similarities and differences with our approach. (i) Most train several distinct models whereas we train a single model on input augmented with an explicit encoding of the language (similar to (Johnson et al., 2016)). (ii) Let k and m be the number of different input and output languages. We address the case k ∈ {1, 2, 3} and m = k. Other work has addressed cases with k > 3 or m > 3; this would be an interesting avenue of future research for paradigm comp"
P17-1182,W16-2007,0,\N,Missing
P18-1245,N15-1107,0,0.0987604,"Missing"
P18-1245,N16-1077,0,0.0709162,"Missing"
P18-1245,D17-1078,1,0.831077,"ut Dunlabeled . unlabeled training data 3: for i = 1 to I do esleep ← ∅ 4: D 5: if i > 1 then 6: for k = 1 to K do ˜ ∼ pθ (·, ·, ·) 7: hf˜ , ˜`, mi esleep ← D esleep ∪ {hf˜ , ˜`, mi} ˜ 8: D esleep 9: maximize log qφ on Dlabeled ∪ D . this corresponds to Eq. (10) + Eq. (8) ewake ← ∅ 10: D 11: for f ∈ Dunlabeled do ˜ ∼ qφ (·, · |f ) 12: h˜`, mi e ewake ∪ {hf , ˜`, mi} ˜ 13: Dwake ← D ewake 14: maximize log pθ on Dlabeled ∪ D . this corresponds to Eq. (11) + Eq. (9) examples and potentially noisy samples produced during the sleep phase, where linear models still outperform non-linear approaches (Cotterell and Heigold, 2017). We note that our variational approximation is compatible with any family. 5.6 We may also view our model as an autoencoder, following Kingma and Welling (2013), who saw that a variational approximation to any generative model naturally has this interpretation. The crucial difference between Kingma and Welling (2013) and this work is that our model is a structured variational autoencoder in the sense that the space of our latent code is structured: the inference network encodes a sentence into a pair of lemmata and morphological tags h`, mi. This bisequence is then decoded back into the seque"
P18-1245,E17-1048,0,0.0395671,"Missing"
P18-1245,K17-2001,1,0.90037,"verted version of UD data, in which the UD morphological tags have been deterministically converted into UniMorph tags. For each of the treebanks in the UD dataset, we divide the training portion into three chunks consisting of the first 500, 1000 and 5000 tokens, respectively. These labeled chunks will constitute three unique sets Dlabeled . The remaining sentences in the training portion will be used as unlabeled data Dunlabeled for each language, i.e., we will discard those labels. The development and test portions will be left untouched. Related Work Closest to our work is Zhou and Neubig (2017), who describe an unstructured variational autoencoder. However, the exact use case of our respective models is distinct. Our method models the syntactic dynamics with an LSTM language model over morphological tags. Thus, in the semisupervised setting, we require token-level annotation. Additionally, our latent variables are interpretable as they correspond to well-understood linguistic quantities. In contrast, Zhou and Neubig (2017) infer latent lemmata as real vectors. To the best of our knowledge, we are only the second attempt, after Zhou and Neubig (2017), to attempt to perform semi-super"
P18-1245,P82-1020,0,0.834478,"Missing"
P18-1245,E14-1060,0,0.0538024,"the syntactic dynamics with an LSTM language model over morphological tags. Thus, in the semisupervised setting, we require token-level annotation. Additionally, our latent variables are interpretable as they correspond to well-understood linguistic quantities. In contrast, Zhou and Neubig (2017) infer latent lemmata as real vectors. To the best of our knowledge, we are only the second attempt, after Zhou and Neubig (2017), to attempt to perform semi-supervised learning for a neural inflection generator. Other non-neural attempts at semi-supervised learning of morphological inflectors include Hulden et al. (2014). Models in this vein are non-neural and often focus on exploiting corpus statistics, e.g., token frequency, rather than explicitly modeling the forms in context. All of these approaches are designed to learn from a typelevel lexicon, rendering direct comparison difficult. 7 Languages. We explore a typologically diverse set of languages of various stocks: Indo-European, Afro-Asiatic, Turkic and Finno-Ugric, as well as the language isolate Basque. We have organized our experimental languages in Tab. 3 by genetic grouping, highlighting sub-families where possible. The Indo-European languages mos"
P18-1245,P16-2090,0,0.163882,"her than the entire morphological tag, as we assume the lemma depends on the part of speech exclusively. (3) j=1 where x<j = x1 , . . . , xj−1 . The prediction at time step j of a single element xj is then parametrized by a neural network: p(xj |x<j ) = softmax (W · hj + b) , Morphological Inflector: pθ (fi |`i , mi ). The final conditional in our model is a morphological inflector, which we parameterize as a neural recurrent sequence-to-sequence model (Sutskever et al., 2014) with Luong dot-style attention (Luong et al., 2015). Our particular model uses a single encoder-decoder architecture (Kann and Schütze, 2016) for all tag pairs within a language and we refer to reader to that paper for further details. Concretely, the encoder runs over a string consisting of the desired slot and all characters of the lemma that is to be inflected (e.g. <w> V PST t a l k </w>), one LSTM running left-to-right, the other right-to-left. Concatenating the hidden states of both RNNs at each time step results in hidden states (enc) hj . The decoder, again, takes the form of an LSTM language model (we take ∆ = Σ), producing the inflected form character by character, but at each time step not only the previous hidden state"
P18-1245,D08-1113,0,0.0541777,"Missing"
P18-1245,N13-1138,0,0.0469128,"Missing"
P18-1245,D15-1166,0,0.0232983,"is a learned embedding function for POS tags. Note that we embed only the POS tag, rather than the entire morphological tag, as we assume the lemma depends on the part of speech exclusively. (3) j=1 where x<j = x1 , . . . , xj−1 . The prediction at time step j of a single element xj is then parametrized by a neural network: p(xj |x<j ) = softmax (W · hj + b) , Morphological Inflector: pθ (fi |`i , mi ). The final conditional in our model is a morphological inflector, which we parameterize as a neural recurrent sequence-to-sequence model (Sutskever et al., 2014) with Luong dot-style attention (Luong et al., 2015). Our particular model uses a single encoder-decoder architecture (Kann and Schütze, 2016) for all tag pairs within a language and we refer to reader to that paper for further details. Concretely, the encoder runs over a string consisting of the desired slot and all characters of the lemma that is to be inflected (e.g. <w> V PST t a l k </w>), one LSTM running left-to-right, the other right-to-left. Concatenating the hidden states of both RNNs at each time step results in hidden states (enc) hj . The decoder, again, takes the form of an LSTM language model (we take ∆ = Σ), producing the inflec"
P18-1245,D15-1272,1,0.855206,"effectively function as inverses, e.g., translation and backtranslation, or language generation and parsing, can be treated with a similar variational autoencoder. While this work only hf ,`,mi∈Dlabeled which is a Monte Carlo approximation of DKL (Dlabeled ||pθ ). As in the sleep phase, we will maximize W = Wsup + γwake · Wunsup , where γwake is, again, a scaling parameter. 5.5 Interpretation as an Autoencoder Our Variational Family How do we choose the variational family qφ ? In terms of NLP nomenclature, qφ represents a joint morphological tagger and lemmatizer. The opensource tool LEMMING (Müller et al., 2015) represents such an object. LEMMING is a higher-order linear-chain conditional random field (CRF; Lafferty et al., 2001), that is an extension of the morphological tagger of Müller et al. (2013). Interestingly, LEMMING is a linear model that makes use of simple character n-gram feature templates. On both the tasks of morphological tagging and lemmatization, neural models have supplanted linear models in terms of performance in the high-resource case (Heigold et al., 2017). However, we are interested in producing an accurate approximation to the posterior in the presence of minimal annotated 6"
P18-1245,D13-1032,0,0.033745,"Dlabeled which is a Monte Carlo approximation of DKL (Dlabeled ||pθ ). As in the sleep phase, we will maximize W = Wsup + γwake · Wunsup , where γwake is, again, a scaling parameter. 5.5 Interpretation as an Autoencoder Our Variational Family How do we choose the variational family qφ ? In terms of NLP nomenclature, qφ represents a joint morphological tagger and lemmatizer. The opensource tool LEMMING (Müller et al., 2015) represents such an object. LEMMING is a higher-order linear-chain conditional random field (CRF; Lafferty et al., 2001), that is an extension of the morphological tagger of Müller et al. (2013). Interestingly, LEMMING is a linear model that makes use of simple character n-gram feature templates. On both the tasks of morphological tagging and lemmatization, neural models have supplanted linear models in terms of performance in the high-resource case (Heigold et al., 2017). However, we are interested in producing an accurate approximation to the posterior in the presence of minimal annotated 6 focuses on the creation of an improved morphological inflector pθ (f |`, m), one could imagine a situation where the encoder was also a task of interest. That is, the goal would be to improve bo"
P18-1245,P15-2111,0,0.262858,"mation to the posterior in the presence of minimal annotated 6 focuses on the creation of an improved morphological inflector pθ (f |`, m), one could imagine a situation where the encoder was also a task of interest. That is, the goal would be to improve both the decoder (the generation model) and the encoder (the variational approximation). 6 7.2 As our model requires token-level morphological annotation, we perform our experiments on the Universal Dependencies (UD) dataset (Nivre et al., 2017). As this stands in contrast to most work on morphological inflection (which has used the UniMorph (Sylak-Glassman et al., 2015)6 datasets), we use a converted version of UD data, in which the UD morphological tags have been deterministically converted into UniMorph tags. For each of the treebanks in the UD dataset, we divide the training portion into three chunks consisting of the first 500, 1000 and 5000 tokens, respectively. These labeled chunks will constitute three unique sets Dlabeled . The remaining sentences in the training portion will be used as unlabeled data Dunlabeled for each language, i.e., we will discard those labels. The development and test portions will be left untouched. Related Work Closest to our"
P18-1245,N15-1093,0,0.0816083,"Missing"
P18-1245,P17-1029,0,0.0162458,"s), we use a converted version of UD data, in which the UD morphological tags have been deterministically converted into UniMorph tags. For each of the treebanks in the UD dataset, we divide the training portion into three chunks consisting of the first 500, 1000 and 5000 tokens, respectively. These labeled chunks will constitute three unique sets Dlabeled . The remaining sentences in the training portion will be used as unlabeled data Dunlabeled for each language, i.e., we will discard those labels. The development and test portions will be left untouched. Related Work Closest to our work is Zhou and Neubig (2017), who describe an unstructured variational autoencoder. However, the exact use case of our respective models is distinct. Our method models the syntactic dynamics with an LSTM language model over morphological tags. Thus, in the semisupervised setting, we require token-level annotation. Additionally, our latent variables are interpretable as they correspond to well-understood linguistic quantities. In contrast, Zhou and Neubig (2017) infer latent lemmata as real vectors. To the best of our knowledge, we are only the second attempt, after Zhou and Neubig (2017), to attempt to perform semi-super"
P19-1148,P17-1183,0,0.168872,"ons of both soft and hard attention are non-monotonic. However, if we look at the data in grapheme-to-phoneme conversion, named-entity transliteration, and morphological inflection—examples are shown in Fig. 1—we see that the tasks require almost exclusively monotonic transduction. Yet, counterintuitively, the state of the art in high resource morphological inflection is held by non-monotonic models (Cotterell et al., 2017)! Indeed, in a recent controlled experiment, Wu et al. (2018) found non-monotonic models (with either soft attention or hard alignment) outperform popular monotonic models (Aharoni and Goldberg, 2017) in the three above mentioned tasks. However, the inductive bias of monotonicity, if correct, should help learn a better model or, at least, learn the same model. In this paper, we hypothesize that the underperformance of monotonic models stems from the lack of joint training of the alignments with the transduction. Generalizing the model of Wu et al. (2018) to enforce monotonic alignments, we show that, for all three tasks considered, monotonicity is a good inductive bias and jointly learning a monotonic alignment improves performance. We provide an exact, cubic-time, dynamic-programming infe"
P19-1148,K17-2002,0,0.0521663,"etails and hyperparameters may be found in App. A. 5.2 Experimental Findings Finding #1: Morphological Inflection. The first empirical finding in our study is that we achieve single-model, state-of-the-art performance on the CoNLL-SIGMORPHON 2017 shared task dataset. The results are shown in Tab. 2. We find that the 1- MONO ties with the 0- MONO system, indicating the additional parameters do not add much. Both of these monotonic systems surpass the non-monotonic system 0- HARD and SOFT. We also report comparison to other top systems at the task in Tab. 1. The previous state-of-the-art model, Bergmanis et al. (2017), is a non-monotonic system that outperformed the monotonic system of Makarov et al. (2017). However, Makarov et al. (2017) is a pipeline system that took alignments from an existing aligner; such a system has no manner, by which it can recover from poor initial 1533 Morphological Inflection Silfverberg et al. (2017) SOFT Makarov et al. (2017) 0- HARD Bergmanis et al. (2017) Makarov and Clematide (2018) 0- MONO 1- MONO ACC Trans 93.0 93.4 93.9 94.5 94.6 94.6 94.8 94.8 ACC Table 1: Average dev performance on morphological inflection of our models against single models from the 2017 shared task."
P19-1148,J93-2003,0,0.151237,"oneme conversion (Yao and Zweig, 2015), named-entity transliteration (Rosca and Breuel, 2016) and morphological inflection generation (Cotterell et al., 2016). While soft attention is very similar to a traditional alignment between the source characters and target characters in some regards, it does not explicitly a distribution over alignments. On the other hand, neural sequence-to-sequence models with hard alignment (Xu et al., 2015; Wu et al., 2018) are analogous to the latent alignment in the classic IBM models for machine translation, which do model the alignment distribution explicitly (Brown et al., 1993). The standard versions of both soft and hard attention are non-monotonic. However, if we look at the data in grapheme-to-phoneme conversion, named-entity transliteration, and morphological inflection—examples are shown in Fig. 1—we see that the tasks require almost exclusively monotonic transduction. Yet, counterintuitively, the state of the art in high resource morphological inflection is held by non-monotonic models (Cotterell et al., 2017)! Indeed, in a recent controlled experiment, Wu et al. (2018) found non-monotonic models (with either soft attention or hard alignment) outperform popula"
P19-1148,K17-2001,1,0.0822279,"hesize that the underperformance of monotonic models stems from the lack of joint training of the alignments with the transduction. Generalizing the model of Wu et al. (2018) to enforce monotonic alignments, we show that, for all three tasks considered, monotonicity is a good inductive bias and jointly learning a monotonic alignment improves performance. We provide an exact, cubic-time, dynamic-programming inference algorithm to compute the log-likelihood and an approximate greedy decoding scheme. Empirically, our results indicate that, rather than the pipeline systems of Aharoni and Goldberg (2017) and Makarov et al. (2017), we should jointly train monotonic alignments with the transduction model, and, indeed, we set the single model state of the art on the task of morphological inflection.1 1 The state of the art for morphological inflection is held by ensemble systems, much like parsing and other structured 1530 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1530–1537 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics Task Grapheme-to-phoneme Transliteration Morphological Inflection Tag N AT+ALL S"
P19-1148,D15-1166,0,0.593489,"earning to transduce. With the help of dynamic programming, we are able to compute the exact marginalization over all monotonic alignments. Our models achieve state-of-the-art performance on morphological inflection. Furthermore, we find strong performance on two other character-level transduction tasks. Code is available at https://github.com/ shijie-wu/neural-transducer. 1 Introduction Many tasks in natural language can be treated as character-level, string-to-string transduction. The current dominant method is the neural sequenceto-sequence model with soft attention (Bahdanau et al., 2015; Luong et al., 2015). This method has achieved state-of-the-art results in a plethora of tasks, for example, grapheme-to-phoneme conversion (Yao and Zweig, 2015), named-entity transliteration (Rosca and Breuel, 2016) and morphological inflection generation (Cotterell et al., 2016). While soft attention is very similar to a traditional alignment between the source characters and target characters in some regards, it does not explicitly a distribution over alignments. On the other hand, neural sequence-to-sequence models with hard alignment (Xu et al., 2015; Wu et al., 2018) are analogous to the latent alignment in"
P19-1148,D18-1314,0,0.0591853,"Missing"
P19-1148,N16-1076,1,0.839931,"on and U ∈ R(w+1)×2dh , a learned parameter. Note that, as before, we also enforce monotonicity as a hard constraint in this parameterization. 4 . Greedy decoding . Forward probability Related Work There have been previous attempts to look at monotonicity in neural transduction. Graves (2012) first introduced the monotonic neural transducer for speech recognition. Building on this, Yu et al. (2016) proposes using a separated shift/emit transition distribution to allow more expressive model. Like us, they also consider morphological inflection and outperform a (weaker) soft attention baseline. Rastogi et al. (2016) offer a neural parameterization of a finite-state transducer, which implicitly encodes monotonic alignments. Instead of learning the alignments directly, Aharoni and Goldberg (2017) take the monotonic alignments from an external model (Sudoh et al., 2013) and train the neural model with these alignments. In followup work, Makarov et al. (2017) show this twostage approach to be effective, winning the CoNLLSIGMORPHON 2017 shared task on morphological inflection (Cotterell et al., 2017). Raffel et al. (2017) propose a stochastic monotonic transition process to allow sample-based online decoding."
P19-1148,K17-2010,0,0.185023,"Missing"
P19-1148,D13-1021,0,0.0622186,"neural transduction. Graves (2012) first introduced the monotonic neural transducer for speech recognition. Building on this, Yu et al. (2016) proposes using a separated shift/emit transition distribution to allow more expressive model. Like us, they also consider morphological inflection and outperform a (weaker) soft attention baseline. Rastogi et al. (2016) offer a neural parameterization of a finite-state transducer, which implicitly encodes monotonic alignments. Instead of learning the alignments directly, Aharoni and Goldberg (2017) take the monotonic alignments from an external model (Sudoh et al., 2013) and train the neural model with these alignments. In followup work, Makarov et al. (2017) show this twostage approach to be effective, winning the CoNLLSIGMORPHON 2017 shared task on morphological inflection (Cotterell et al., 2017). Raffel et al. (2017) propose a stochastic monotonic transition process to allow sample-based online decoding. Empirical Comparison. We compare (i) soft attention without input-feeding (SOFT) (Luong et al., 2015), (ii) 0th -order hard attention (0- HARD) (Wu et al., 2018), (iii) 0th -order monotonic hard attention (0- MONO) and (iv) 1st -order monotonic hard atten"
P19-1148,P18-2060,0,0.0310613,"e model with a more expressive transition function. In this case, we take 1532 Algorithm 1 Greedy decoding. (N is the maximum length of target string.) 1: 2: 3: 4: 5: 6: 7: 8: 9: for i = 1, · · · , N do if i = 1 then P|x| yi∗ = argmaxyi ai =1 p(yi |ai )p(ai |ai−1 ) α(a0 ) α(a1 ) = p(y1∗ |a1 ) p(a1 |a0 ) α(a0 ) else P|x| P|x| yi∗ = argmaxyi ai =1 p(yi |ai ) ai−1 =1 p(ai |ai−1 ) α(ai−1 ) P|x| α(ai ) = p(yi∗ |ai ) ai−1 =1 p(ai |ai−1 ) α(ai−1 ) . Greedy decoding . Forward probability if yi∗ = EOS then return y∗ the 1st -order hard attention to be an offset-based transition distribution similar to Wang et al. (2018): 5 5.1 Experiments Experiments Design Tasks. We consider three character-level transduction tasks: grapheme-to-phoneme conversion (Weide, 1998; Sejnowski and Rosenberg, 1987), named-entity transliteration (Zhang et al., 2015) and morphological inflection in high-esource setting (Cotterell et al., 2017). p(ai |ai−1 , y<i , x) ( softmax(U[hdi ; T heai−1 ])) 0 ≤ ∆ ≤ w = 0 otherwise where ∆ = ai − ai−1 is relative distance to previous attention position and U ∈ R(w+1)×2dh , a learned parameter. Note that, as before, we also enforce monotonicity as a hard constraint in this parameterization. 4 . G"
P19-1148,D18-1473,1,0.719516,"Missing"
P19-1148,D16-1138,0,0.0203676,"logical inflection in high-esource setting (Cotterell et al., 2017). p(ai |ai−1 , y<i , x) ( softmax(U[hdi ; T heai−1 ])) 0 ≤ ∆ ≤ w = 0 otherwise where ∆ = ai − ai−1 is relative distance to previous attention position and U ∈ R(w+1)×2dh , a learned parameter. Note that, as before, we also enforce monotonicity as a hard constraint in this parameterization. 4 . Greedy decoding . Forward probability Related Work There have been previous attempts to look at monotonicity in neural transduction. Graves (2012) first introduced the monotonic neural transducer for speech recognition. Building on this, Yu et al. (2016) proposes using a separated shift/emit transition distribution to allow more expressive model. Like us, they also consider morphological inflection and outperform a (weaker) soft attention baseline. Rastogi et al. (2016) offer a neural parameterization of a finite-state transducer, which implicitly encodes monotonic alignments. Instead of learning the alignments directly, Aharoni and Goldberg (2017) take the monotonic alignments from an external model (Sudoh et al., 2013) and train the neural model with these alignments. In followup work, Makarov et al. (2017) show this twostage approach to be"
P19-1148,W15-3901,0,0.0307534,"= argmaxyi ai =1 p(yi |ai )p(ai |ai−1 ) α(a0 ) α(a1 ) = p(y1∗ |a1 ) p(a1 |a0 ) α(a0 ) else P|x| P|x| yi∗ = argmaxyi ai =1 p(yi |ai ) ai−1 =1 p(ai |ai−1 ) α(ai−1 ) P|x| α(ai ) = p(yi∗ |ai ) ai−1 =1 p(ai |ai−1 ) α(ai−1 ) . Greedy decoding . Forward probability if yi∗ = EOS then return y∗ the 1st -order hard attention to be an offset-based transition distribution similar to Wang et al. (2018): 5 5.1 Experiments Experiments Design Tasks. We consider three character-level transduction tasks: grapheme-to-phoneme conversion (Weide, 1998; Sejnowski and Rosenberg, 1987), named-entity transliteration (Zhang et al., 2015) and morphological inflection in high-esource setting (Cotterell et al., 2017). p(ai |ai−1 , y<i , x) ( softmax(U[hdi ; T heai−1 ])) 0 ≤ ∆ ≤ w = 0 otherwise where ∆ = ai − ai−1 is relative distance to previous attention position and U ∈ R(w+1)×2dh , a learned parameter. Note that, as before, we also enforce monotonicity as a hard constraint in this parameterization. 4 . Greedy decoding . Forward probability Related Work There have been previous attempts to look at monotonicity in neural transduction. Graves (2012) first introduced the monotonic neural transducer for speech recognition. Buildin"
P19-1148,P17-1029,0,0.0254046,"t 2, 2019. 2019 Association for Computational Linguistics Task Grapheme-to-phoneme Transliteration Morphological Inflection Tag N AT+ALL SG Source a c t i o n Target AE K SH AH N A A C H E N l i p u k e l i p u k k e e l l e Figure 1: Example of source and target string for each task. Tag guides transduction in morphological inflection. 2 Hard Attention 2.1 Preliminary We assume the source string x ∈ Σ∗x and the target string y ∈ Σ∗y have finite vocabularies Σx = {x1 , . . . , x|Σx |} and Σy = {y1 , . . . , y|Σy |}, respectively. In tasks where the tag is provided, i.e., labeled transduction (Zhou and Neubig, 2017), we denote the tag as an ordered set t ∈ Σ∗t with a finite tag vocabulary Σt = {t1 , . . . , t|Σt |}. We define the set A = {1, . . . , |x|}|y |to be set of all alignments from x to y where an alignment aligns each target character yi to exactly one source character in x. In other words, it allows zero-to-one2 or many-to-one alignments between x and y. For an a ∈ A, ai = j refers to the event that yi is aligned to xj , the ith character of y and the j th character of x. 2.2 0th -order Hard Attention Hard attention was first introduced to the literature by Xu et al. (2015). We, however, follow"
P19-1148,W16-2002,1,\N,Missing
P19-1161,D15-1272,1,0.897143,"Missing"
P19-1161,W17-1609,0,0.109749,"Missing"
P19-1161,N18-2002,0,0.161701,"Missing"
P19-1161,D18-1473,1,0.749713,"Missing"
P19-1161,N19-1064,1,0.875275,"Missing"
P19-1161,D17-1323,0,0.0449042,"rg et al., 2017; Rudinger et al., 2017; Sutton et al., 2018). Gender stereotypes can manifest in language in overt ways. For example, the sentence he is an engineer is more likely to appear in a corpus than she is an engineer due to the current gender disparity in engineering. Consequently, any NLP system that is trained such a corpus will likely learn to associate engineer with men, but not with women (De-Arteaga et al., 2019). To date, the NLP community has focused primarily on approaches for detecting and mitigating gender stereotypes in English (Bolukbasi et al., 2016; Dixon et al., 2018; Zhao et al., 2017). Yet, gender stereotypes also exist in other languages because they are a function of society, not of grammar. Moreover, because English does not mark grammatical gender, approaches developed for English are not transferable to morphologically rich languages that exhibit gender agreement (Corbett, 1991). In these languages, the words in a sentence are marked with morphological endings that reflect the grammatical gender of the surrounding nouns. This means that if the gender of one word changes, the others have to be updated to match. As a result, simple heuristics, such as augmenting a corpu"
P19-1161,C18-1001,0,\N,Missing
P19-1161,W19-3822,0,\N,Missing
P19-1161,W16-2002,1,\N,Missing
P19-1167,S13-1035,0,0.0607054,"djectives used to describe men? Can we quantify such patterns using existing semantic resources (Tsvetkov et al., 2014)? gender from grammatical gender because the latter does not necessarily convey anything meaningful about the referent. 2 Men are written about more often than women. Indeed, the corpus we use exhibits this trend, as shown in Tab. 1. Female Male other daughter lady wife mother girl woman 2.2 1.4 2.4 3.3 4.2 5.1 11.5 Total 30.2 other husband king son father boy man 6.8 1.8 2.1 2.9 4.2 5.1 39.9 62.7 Table 1: Counts, in millions, of male and female nouns present in the corpus of Goldberg and Orwant (2013). Q3 Does the overall sentiment of the language used to describe men and women differ? To answer these questions, we introduce a generative latent-variable model that jointly represents adjective (or verb) choice, with its sentiment, given the natural gender of a head (or dependent) noun. We use a form of posterior regularization to guide inference of the latent variables (Ganchev et al., 2010). We then use this model to study the syntactic n-gram corpus of (Goldberg and Orwant, 2013). To answer Q1, we conduct an analysis that reveals differences between descriptions of male and female nouns t"
P19-1167,P84-1044,0,0.340361,"Missing"
P19-1167,N19-1065,1,0.870199,"Missing"
P19-1167,1998.amta-tutorials.1,0,0.664349,"female nouns and that these differences align with common gender stereotypes: Positive adjectives used to describe women are more often related to their bodies than adjectives used to describe men. 1 Introduction Word choice is strongly influenced by gender— both that of the speaker and that of the referent (Lakoff, 1973). Even within 24 hours of birth, parents describe their daughters as beautiful, pretty, and cute far more often than their sons (Rubin et al., 1974). To date, much of the research in sociolinguistics on gendered language has focused on laboratory studies and smaller corpora (McKee and Sherriffs, 1957; Williams and Bennett, 1975; Baker, 2005); however, more recent work has begun to focus on larger-scale datasets (Pearce, 2008; CaldasCoulthard and Moon, 2010; Baker, 2014; Norberg, 2016). These studies compare the adjectives (or Female Positive Negative Positive Negative beautiful lovely chaste gorgeous fertile beauteous sexy classy exquisite vivacious vibrant just sound righteous rational peaceable prodigious brave paramount reliable sinless honorable unsuitable unreliable lawless inseparable brutish idle unarmed wounded bigoted unjust brutal Male battered untreated barren shrewish sheltere"
P19-1167,D17-1323,0,0.0916673,"Missing"
P19-1167,N18-2003,0,0.16078,"Missing"
P19-1167,N18-1067,0,0.0283022,"Missing"
P19-1167,W16-0204,0,0.195615,"Missing"
P19-1167,H93-1061,0,\N,Missing
P19-1167,tsvetkov-etal-2014-augmenting-english,0,\N,Missing
P19-1167,W17-1609,0,\N,Missing
P19-1171,J92-1002,0,0.720201,"rds’ prefixes w&lt;k and suffixes w>k .1 3.2 A variational upper bound Entropy, the workhorse of information theory, captures the uncertainty of a probability distribution. In our language modeling case, the quantity is H(W ) ≡ ∑ Pr(w) log w∈Σ∗ 1 . Pr(w) (1) Entropy is the average number of bits required to represent a string in the distribution, under an optimal coding scheme. When computing it, we are faced with two problems: We do not know the distribution over word-forms Pr(W ) and, even if we did, computing Equation 1 requires summing over the infinite set of possible strings Σ∗ . We follow Brown et al. (1992) in tackling these problems together. Approximating Pr(W ) with any known distribution Q(W ), we get a variational upper bound on H(W ) from their cross-entropy, i.e. H(W ) ≤ HQ (W ) = ∑ w∈Σ∗ 1 Pr(w) log (2a) 1 . Q(w) (2b) In line with, e.g., Cucerzan and Yarowsky (2003), we treat affixes as word-initial or word-final sequences, regardless of their status as attested morphological entities. 1753 Equation 2b still requires knowledge of Pr(W ) and involves an infinite sum, though. Nonetheless, we ˜ of samples from Pr(W ) to can use a finite set W get an empirical estimate of this value. HQ (W )"
P19-1171,N03-1006,0,0.110592,"y is the average number of bits required to represent a string in the distribution, under an optimal coding scheme. When computing it, we are faced with two problems: We do not know the distribution over word-forms Pr(W ) and, even if we did, computing Equation 1 requires summing over the infinite set of possible strings Σ∗ . We follow Brown et al. (1992) in tackling these problems together. Approximating Pr(W ) with any known distribution Q(W ), we get a variational upper bound on H(W ) from their cross-entropy, i.e. H(W ) ≤ HQ (W ) = ∑ w∈Σ∗ 1 Pr(w) log (2a) 1 . Q(w) (2b) In line with, e.g., Cucerzan and Yarowsky (2003), we treat affixes as word-initial or word-final sequences, regardless of their status as attested morphological entities. 1753 Equation 2b still requires knowledge of Pr(W ) and involves an infinite sum, though. Nonetheless, we ˜ of samples from Pr(W ) to can use a finite set W get an empirical estimate of this value. HQ (W ) ≈ 1 N 1 , log ∑ N i=1 ˜ (i) Q w ˜ ∼ Pr(W ) ˜ (i) ∈ W w (3) with equality if we let N → We now use Equation 3 as an estimate for the entropy of a lexicon. tighter. There is nothing principled that we can say about the result, except that it is consistent. The procedure f"
P19-1171,P16-1225,0,0.313803,"meaning, and the signifier, which has no meaning but manifests the form of the sign. Saussure himself, however, also documented instances of sound symbolism in language (Saussure, 1912). In this paper, we present computational evidence of relevance to both aspects of Saussure’s work. While dominant among linguists, arbitrariness has been subject to both long theoretical debate (Wilkins, 1668; Eco, 1995; Johnson, 2004; Pullum and Scholz, 2007) and numerous empirical and experimental studies (Hutchins, 1998; Bergen, 2004; Monaghan et al., 2011; Abramova and Fern´andez, 2016; Blasi et al., 2016; Gutierrez et al., 2016; Dautriche et al., 2017). Taken as a whole, these studies suggest non-trivial interactions in the form– meaning interface between the signified and the signifier (Dingemanse et al., 2015). Although the new wave of studies on form– meaning associations range across multiple languages, methods and working hypotheses, they all converge on two important dimensions: 1. The description of meaning is parameterized with pre-defined labels—e.g., by using existing ontologies like List et al. (2016). 2. The description of forms is restricted to the presence, absence or sheer number of occurrence of part"
P19-1171,P18-1027,0,0.0299014,"s shown in Equation 5b. We use upper bounds on both the entropy and conditional entropy measures, so our calculated mutual information is an estimate. This estimate is as good as our bounds are tight, being perfect when Pr(W ) = Q(W ) and Pr(W |V ) = Q(W |V ). Still, as we subtract two upper bounds, we cannot guarantee that our MI estimate approaches the real MI from above or below because we do not know which of the entropies’ bounds are 2 ∏ Pr (wi |w&lt;i ) . (7) i=1 Recurrent neural networks are great representation extractors, being able to model long dependencies—up to a few hundred tokens (Khandelwal et al., 2018)—and complex distributions Pr(wi |w&lt;i ) (Mikolov et al., 2010; Sundermeyer et al., 2012). We choose LSTM language models in particular, the state-of-the-art for character-level language modeling (Merity et al., 2018).3 Our architecture embeds a word—a sequence of tokens wi ∈ Σ—using an embedding lookup table, resulting in vectors zi ∈ Rd . These are fed into an LSTM, which produces high-dimensional representations of the sequence (hidden states): Systematicity will thus be framed as (statistically significant) nonzero mutual information I(V ;W ). 3.4 Recurrent neural LM Pr(w) = Systematicity a"
P19-1171,L16-1379,0,0.0168626,"tchins, 1998; Bergen, 2004; Monaghan et al., 2011; Abramova and Fern´andez, 2016; Blasi et al., 2016; Gutierrez et al., 2016; Dautriche et al., 2017). Taken as a whole, these studies suggest non-trivial interactions in the form– meaning interface between the signified and the signifier (Dingemanse et al., 2015). Although the new wave of studies on form– meaning associations range across multiple languages, methods and working hypotheses, they all converge on two important dimensions: 1. The description of meaning is parameterized with pre-defined labels—e.g., by using existing ontologies like List et al. (2016). 2. The description of forms is restricted to the presence, absence or sheer number of occurrence of particular units (such as phones, syllables or handshapes). We take an information-theoretic approach to quan1751 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1751–1764 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics tifying the relationship between form and meaning using flexible representations in both domains, rephrasing the question of systematicity: How much does certainty of one reduce uncertain"
P19-1171,W18-1206,0,0.166979,". In addition to this lexicon-level characterization of systematicity, we also show that this paradigm can be leveraged for studying more narrowlydefined form-meaning associations such as phonesthemes—submorphemic, meaning-bearing units— in the style of Gutierrez et al. (2016). These short sound sequences typically suggest some aspect of meaning in the words that contain them, like -ump for rounded things in English. Previous computational studies, whether focusing on characterizing the degree of systematicity (Monaghan et al., 2014b,a, 2011; Shillcock et al., 2001), discovering phonesthemes (Liu et al., 2018), or both (Gutierrez et al., 2016), have invariably framed systematicity in terms of distances and/or similarities–the relation between word-form distance/similarity on the one hand (e.g., based on string edit distance) and semantic distance/similarity on the other (e.g., as defined within a semantic vector space). Our methods have the virtue of not relying on some predefined notion of similarity or distance in either domain for our measurement of systematicity. Empirically, we focus on two experimental regimes. First, we focus on a large corpus (CELEX) of phone transcriptions in Dutch, Englis"
P19-1382,D17-1011,0,0.030622,"Missing"
P19-1382,N18-1083,1,0.898096,"d of knowledge base population. Path Ranking Algorithm (PRA) is an algorithm which finds relation paths by traversing the knowledge graph, which can then be used to predict implicatures and feature values (Lao and Cohen, 2010; Lao et al., 2011).1 We train PRA using the standard hyperparameters of the existing implementation, which includes regularising with `1 = 0.001 and `2 = 0.001, as well as using negative sampling. Baseline #4: Language embeddings Although we aim to predict implications, and not only feature values, we compare with previous work on predicting typological features in WALS (Bjerva and Augenstein, 2018a). As their setup is different, we use their highest reported score as a baseline. Feature Prediction Results. Table 1 contains the results from feature prediction across the chapters outlined in WALS. Our implementation is able to predict features across categories above baseline levels. At increasing numbers of implicants, prediction power tends to increase. This is not the case for all feature categories, however. One such case is Nominal Syntax, in which performance peaks at 3 implicants. This is expected, as correlations only exist between some features, thus at a certain point access to"
P19-1382,W18-0207,1,0.510671,"d of knowledge base population. Path Ranking Algorithm (PRA) is an algorithm which finds relation paths by traversing the knowledge graph, which can then be used to predict implicatures and feature values (Lao and Cohen, 2010; Lao et al., 2011).1 We train PRA using the standard hyperparameters of the existing implementation, which includes regularising with `1 = 0.001 and `2 = 0.001, as well as using negative sampling. Baseline #4: Language embeddings Although we aim to predict implications, and not only feature values, we compare with previous work on predicting typological features in WALS (Bjerva and Augenstein, 2018a). As their setup is different, we use their highest reported score as a baseline. Feature Prediction Results. Table 1 contains the results from feature prediction across the chapters outlined in WALS. Our implementation is able to predict features across categories above baseline levels. At increasing numbers of implicants, prediction power tends to increase. This is not the case for all feature categories, however. One such case is Nominal Syntax, in which performance peaks at 3 implicants. This is expected, as correlations only exist between some features, thus at a certain point access to"
P19-1382,N19-1156,1,0.624091,"rtain conditional probability distribution. We do this by first placing a joint distribution over the vector of typological features, and then marginalising out all features other than the two under consideration. This computation is made tractable by learning a tree-structured graphical model (Figure 1) with the PC algorithm of Neapolitan (2004) and then applying the belief propagation (BP) algorithm (Pearl, 1982). We draw inspiration from manual linguistic efforts to this problem (Greenberg, 1963; Lehmann, 1978), as well as from previous computational methods (Daum´e III and Campbell, 2007; Bjerva et al., 2019a). Additionally, we provide a qualitative analysis of predicted implications, as well as performing an empirical evaluation on typological feature prediction, com3924 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3924–3930 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics paring to strong baselines. 2 From A Generative Model to Probabilistic Implications Notation. We now seek a probabilistic formalisation of typological implications. First, we will introduce the relevant notation. Let ` be a language. W"
P19-1382,P07-1009,0,0.36283,"Missing"
P19-1382,C10-1044,0,0.0875114,"Missing"
P19-1382,J19-2006,1,0.853583,"rtain conditional probability distribution. We do this by first placing a joint distribution over the vector of typological features, and then marginalising out all features other than the two under consideration. This computation is made tractable by learning a tree-structured graphical model (Figure 1) with the PC algorithm of Neapolitan (2004) and then applying the belief propagation (BP) algorithm (Pearl, 1982). We draw inspiration from manual linguistic efforts to this problem (Greenberg, 1963; Lehmann, 1978), as well as from previous computational methods (Daum´e III and Campbell, 2007; Bjerva et al., 2019a). Additionally, we provide a qualitative analysis of predicted implications, as well as performing an empirical evaluation on typological feature prediction, com3924 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3924–3930 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics paring to strong baselines. 2 From A Generative Model to Probabilistic Implications Notation. We now seek a probabilistic formalisation of typological implications. First, we will introduce the relevant notation. Let ` be a language. W"
P19-1382,D11-1049,0,0.0918847,"Missing"
P19-1382,P17-1109,1,0.846546,"typological implications. In addition to work on finding implications based on known features, there is an increasing amount of work on computational methods to discovering typological features (Ponti et al., 2018). Work in this area includes unsupervised discovery of word ¨ order (Ostling, 2015) or other linguistic features (Asgari and Sch¨utze, 2017), typological probing of language representations (Bjerva et al., 2019b; Beinborn and Choenni, 2019), and several papers attempt to predict typological features in WALS (Georgi et al., 2010; Malaviya et al., 2017; Bjerva and Augenstein, 2018a,b; Cotterell and Eisner, 2017, 2018; Bjerva et al., 2019a). 6 Conclusions We defined the notion of probabilistic implications, and presented a computational model which successfully identifies known universals, including Greenberg universals, but also uncovers new ones, worthy of further investigation by typologists. Additionally, our approach outperforms strong baselines for prediction of typological features. Acknowledgments We acknowledge the computational resources provided by CSC in Helsinki through NeIC-NLPL (www.nlpl.eu), and the support of the NVIDIA Corporation with the donation of the Titan Xp GPU used for this"
P19-1382,D18-1543,1,0.696833,"Missing"
P19-1382,N18-1004,1,0.902651,"Missing"
P19-1382,D17-1268,0,0.257826,"7), we are the first to introduce a probabilisation of typological implications. In addition to work on finding implications based on known features, there is an increasing amount of work on computational methods to discovering typological features (Ponti et al., 2018). Work in this area includes unsupervised discovery of word ¨ order (Ostling, 2015) or other linguistic features (Asgari and Sch¨utze, 2017), typological probing of language representations (Bjerva et al., 2019b; Beinborn and Choenni, 2019), and several papers attempt to predict typological features in WALS (Georgi et al., 2010; Malaviya et al., 2017; Bjerva and Augenstein, 2018a,b; Cotterell and Eisner, 2017, 2018; Bjerva et al., 2019a). 6 Conclusions We defined the notion of probabilistic implications, and presented a computational model which successfully identifies known universals, including Greenberg universals, but also uncovers new ones, worthy of further investigation by typologists. Additionally, our approach outperforms strong baselines for prediction of typological features. Acknowledgments We acknowledge the computational resources provided by CSC in Helsinki through NeIC-NLPL (www.nlpl.eu), and the support of the NVIDIA Corp"
P19-1382,P15-2034,0,0.0142401,"this direction has been manual, typological knowledge bases do exist now (Dryer and Haspelmath, 2013; Partick Littel and Levin, 2016), which allows for automated discovery of implications. Although previous computational work exists (Daum´e III and Campbell, 2007), we are the first to introduce a probabilisation of typological implications. In addition to work on finding implications based on known features, there is an increasing amount of work on computational methods to discovering typological features (Ponti et al., 2018). Work in this area includes unsupervised discovery of word ¨ order (Ostling, 2015) or other linguistic features (Asgari and Sch¨utze, 2017), typological probing of language representations (Bjerva et al., 2019b; Beinborn and Choenni, 2019), and several papers attempt to predict typological features in WALS (Georgi et al., 2010; Malaviya et al., 2017; Bjerva and Augenstein, 2018a,b; Cotterell and Eisner, 2017, 2018; Bjerva et al., 2019a). 6 Conclusions We defined the notion of probabilistic implications, and presented a computational model which successfully identifies known universals, including Greenberg universals, but also uncovers new ones, worthy of further investigati"
P19-1382,E17-2102,0,0.153608,"er workings of language and define the space of plausible languages. Universals can aid cognitive scientists examining the underlying processes of language, as there arguably is a cognitive reason for why, e.g., languages with OV ordering are postpositional (Greenberg, 1963). In the context of natural language processing (NLP), when creating synthetic data for multilingual NLP, one should consider universals to maintain the plausibility of the data (Wang and Eisner, 2016). Computational typology can furthermore be used to induce language representations, useful in, e.g., lan¨ guage modelling (Ostling and Tiedemann, 2017) and syntactic parsing (de Lhoneux et al., 2018). In this paper, we argue that the deterministic Greenbergian view of implications (Greenberg, 1963) is outdated. Instead, we suggest that a probabilistic view of implications is more suitable, and define the notion of a probabilistic typological implication as a certain conditional probability distribution. We do this by first placing a joint distribution over the vector of typological features, and then marginalising out all features other than the two under consideration. This computation is made tractable by learning a tree-structured graphic"
P19-1382,Q16-1035,0,0.0272503,"e the presence of one feature strictly implies the presence of another. Universals are important to investigate as they offer insight into the inner workings of language and define the space of plausible languages. Universals can aid cognitive scientists examining the underlying processes of language, as there arguably is a cognitive reason for why, e.g., languages with OV ordering are postpositional (Greenberg, 1963). In the context of natural language processing (NLP), when creating synthetic data for multilingual NLP, one should consider universals to maintain the plausibility of the data (Wang and Eisner, 2016). Computational typology can furthermore be used to induce language representations, useful in, e.g., lan¨ guage modelling (Ostling and Tiedemann, 2017) and syntactic parsing (de Lhoneux et al., 2018). In this paper, we argue that the deterministic Greenbergian view of implications (Greenberg, 1963) is outdated. Instead, we suggest that a probabilistic view of implications is more suitable, and define the notion of a probabilistic typological implication as a certain conditional probability distribution. We do this by first placing a joint distribution over the vector of typological features,"
P19-1384,W18-6404,0,0.0147066,"entence length is accounted for? The answer to these questions promises insights into the nature of constraints on the human parser, opening new research avenues on the computational complexity of human language. 2 Language Arabic Bulgarian Catalan Danish Dutch French German Greek Hebrew Italian Latvian Portuguese Romanian Russian Slovenian Spanish Swedish Data Corpora We focus on 17 languages, selected based on data and tool availability. We annotated 2 types of large, publicly available corpora: Wikipedia dumps from March 2017 and, where available, the WMT News Crawl corpora from 2007-2017 (Bei et al., 2018). Table 1 provides basic statistics of the annotated corpora. Parsing Each corpus was tokenized using UDPipe’s (Straka et al., 2016) pre-trained UDv2.2 models (Straka, 2018) and then parsed as follows: We trained Dozat et al. (2017)’s parsing model, a state-of-the-art graph-based neural dependency parser, on the Universal Dependencies 2.2 dataset (Nivre et al., 2018). We used the hyperparameter configuration described in Dozat et al. (2017), and pre-trained FastText word embeddings for frequent words (Bojanowski et al., 2016). We are aiming to make the parsed corpora available as soon as possi"
P19-1384,L16-1680,0,0.0273389,"Missing"
P19-1491,P18-1007,0,0.0157771,"We restrict the set of characters to those that we see at least 25 times in the training set, replacing all others with a new symbol ^, as is common and easily defensible in openvocabulary language modeling (Mielke and Eisner, 2018). We make an exception for Chinese, where we only require each character to appear at least twice. These thresholds result in negligible “out-of-alphabet” rates for all languages. 7 In practice, in both training and testing, we only evaluate the probability of the canonical segmentation of the held-out string, rather than the total probability of all segmentations (Kudo, 2018; Mielke and Eisner, 2018, Appendix D.2). 8 Figure 2 shows the 21 languages of the Europarl dataset. Optimal values: 0.2 (et); 0.3 (fi, lt); 0.4 (de, es, hu, lv, sk, sl); 0.5 (da, fr, pl, sv); 0.6 (bg, ru); 0.7 (el); 0.8 (en); 0.9 (it, pt). 3 6 5.8 5.6 5.4 ·106 hu nl pl el ro cs pt es it en et fr sk bg sl fi lv lt sv da 3.2 de hu pl el cs nl pt ro it es et en sk fr sl lt bg fi lv da sv hu de hu pl cs el pt nl et it ro es fr en sl sk bg lt fi lv sv da 0.2 hu de pl cs ro nl el et pt es it fi lt fr sk en sl bg lv sv da hu de hu de de pl cs ro nl et el it pt es lt fi sk sl fr lv en bg pl cs et nl"
P19-1491,E12-1026,0,0.0141882,") the raw character sequence length—are statistically significant indicators of modeling difficulty within our large set of languages. In contrast, we fail to reproduce our earlier results from Cotterell et al. (2018),1 which suggested morphological complexity as an indicator of modeling complexity. In fact, we find no tenable correlation to a wide variety of typological features, taken from the WALS dataset and other sources. Additionally, exploiting our model’s ability to handle missing data, we directly test the hypothesis that translationese leads to easier language-modeling (Baker, 1993; Lembersky et al., 2012). We ultimatelycast doubt on this claim, showing that, under the strictest controls, translationese is different, but not any easier to model according to our notion of difficulty. We conclude with a recommendation: The world being small, typology is in practice a small-data problem. there is a real danger that cross-linguistic studies will under-sample and thus over-extrapolate. We outline directions for future, more robust, investigations, and further caution that future work of this sort should focus on datasets with far more languages, something our new methods now allow. 2 The Surprisal o"
P19-1491,P16-1162,0,0.00976653,".5 If we were to assume that our language models were perfect in the sense that they captured the true probability distribution of a language, we could make the former claim; but we suspect that much of the difference can be explained by our imperfect LMs rather than inherent differences in the expressed information (see the discussion in footnote 3). 2.3 BPE-RNNLM BPE-based open-vocabulary language models make use of sub-word units instead of either words or characters and are a strong baseline on multiple languages (Mielke and Eisner, 2018). Before training the RNN, byte pair encoding (BPE; Sennrich et al., 2016) is applied globally to the training corpus, splitting each word (i.e., each space-separated substring) into one or more units. The RNN is then trained over the sequence of units, which looks like this: “The |ex|os|kel|eton |is |gener|ally |blue”. The set of subword units is finite and determined from training data only, but it is a superset of the alphabet, making it possible to explain any novel word in held-out data via some segmentation.7 One important thing to note is that the size of this set can be tuned by specifying the number of BPE merges, allowing us to smoothly vary between a word"
P19-1491,L16-1680,0,0.0569772,"Missing"
P19-1491,K17-3009,0,0.0601694,"Missing"
P19-1491,H01-1035,0,0.0770762,"led by the data that it did not make much sense to spend time on them. Specifically, for full inference, we implemented all models in STAN (Carpenter et al., 2017), a 4.2 14 One could also use a Cauchy distribution instead of the Laplace distribution to get even heavier tails, but we saw little difference between the two in practice. 15 Further enhancements are possible: we discuss our “Model 3” in Appendix B, but it did not seem to fit better. The Bible: 62 Languages The Bible is a religious text that has been used for decades as a dataset for massively multilingual NLP (Resnik et al., 1999; Yarowsky et al., 2001; Agi´c et al., 2016). Concretely, we use the 5 de pl hu fr lt da el it bg fi cs ro nl pt es et sl en sk sv lv hardly affected when tuning the number of BPE merges per-language instead of globally, validating our approach of using the BPE model for our experiments. A bigger difference seems to be the choice of char-RNNLM vs. BPE-RNNLM, which changes the ranking of languages both on Europarl data and on Bibles. We still see German as the hardest language, but almost all other languages switch places. Specifically, we can see that the variance of the char-RNNLM is much higher. chars BPE (0.4|V |"
P19-1491,N18-1202,0,0.0467576,"al properties that make certain languages harder to language-model than others. One of the oldest tasks in NLP (Shannon, 1951) is language modeling, which attempts to estimate a distribution ?(x) over strings x of a language. Recent years have seen impressive improvements with recurrent neural language models (e.g., Merity et al., 2018). Language modeling is an important component of tasks such as speech recognition, machine translation, and text normalization. It has also enabled the construction of contextual word embeddings that provide impressive performance gains in many other NLP tasks (Peters et al., 2018)—though those downstream evaluations, too, have focused on a small number of (mostly English) datasets. In prior work (Cotterell et al., 2018), we compared languages in terms of the difficulty of language modeling, controlling for differences in content by using a multi-lingual, fully parallel text corpus. Few such corpora exist: in that paper, we made Introduction Do current NLP tools serve all languages? Technically, yes, as there are rarely hard constraints that prohibit application to specific languages, as long as there is data annotated for the task. However, in practice, the answer is m"
P19-1491,W09-0106,0,\N,Missing
P19-1491,2005.mtsummit-papers.11,0,\N,Missing
P19-1491,mayer-cysouw-2014-creating,0,\N,Missing
P19-1491,N18-2085,1,\N,Missing
P19-1491,L18-1293,1,\N,Missing
P19-1491,D18-1312,0,\N,Missing
P19-1491,Q15-1030,0,\N,Missing
P19-1505,N18-2087,1,0.901344,"examine these questions, focusing in particular on the relationship between irregularity and frequency. One of the fundamental challenges in studying irregularity is defining the phenomenon in a way that is applicable across languages. We begin the paper by addressing this question. First, we formalize the problem of inflectional morphology and present a novel, information-theoretic measure of the degree of irregularity of an inflected form. This definition builds on recent work that defines (ir)regularity in terms of the probabilistic predictability of a form given the rest of the language (Cotterell et al., 2018a; Ackerman and Malouf, 2013). Making use of a state-of-the-art model of morphological inflection, we estimate our measure of irregularity across a large number of word forms from 28 languages drawn from the UniMorph database (Kirov et al., 2018). Based on these estimates we perform three studies. First, we validate our estimates by examining the predictions on English past tense forms— showing that the model’s predicts accord with human judgements of irregularity. We also examine the overall rate of accuracy of our model. Second, we examine the degree of irregularity across languages, showing"
P19-1505,K17-2001,1,0.911073,"Missing"
P19-1505,E17-2120,1,0.928386,"ngan ponga Figure 1: Lemma paradigm tree 4 Modeling Morphological Inflection Our goal is to estimate P(w |`, σ, L−` ) from data. We do this by using a structured probabilistic model of string transduction which we call pθ . In the following sections, we describe this model, how we handle syncretism in the model, our training (holdout and test) scheme, and our estimates of the degree of irregularity ι. 4.1 A Lemma-Based Model In linguistic morphology, a major division is between item-and-arrangement or morpheme-based models and word-and-paradigm or word-based models (Hockett, 1954). Following (Cotterell et al., 2017b), we adopt a word-based approach. To do this, we designate a unique surface form for each paradigm ` known as the lemma. The lemma is associated with a slot which we notate σ ˇ : `.ˇ σ ∈ Σ∗ . The lemma can be thought of as a dictionary or citation form of a word and is traditionally chosen by lexicographers of a language. For example, in many Western European languages the lemma of verb forms is the infinitive. Figure 1 shows several of the forms of the Spanish verb poner (“to put”) organized around the lemma form. In what follows, we use the lemma to identify lexemes, and wherever a probabi"
P19-1505,L18-1293,1,0.891403,"Missing"
P19-1505,P19-1148,1,0.833438,"can be thought of as a dictionary or citation form of a word and is traditionally chosen by lexicographers of a language. For example, in many Western European languages the lemma of verb forms is the infinitive. Figure 1 shows several of the forms of the Spanish verb poner (“to put”) organized around the lemma form. In what follows, we use the lemma to identify lexemes, and wherever a probability distribution would condition on the abstract lexeme ` we instead condition on the lemma `.ˇ σ. Our probabilistic model of string transduction pθ is a monotonic model with hard attention described in Wu and Cotterell (2019) and can be viewed as a graphical model over strings like the one shown in 5119 NOM GEN ACC DAT SG PL SG PL Wort Wortes Wort Worte W¨orter W¨orter W¨orter W¨ortern Herr Herrn Herrn Herrn Herren Herren Herren Herren Table 1: Full paradigms for the German nouns Wort (“word”) and Herr (“mister”) with abbreviated and tabularized UniMorph annotation. The syncretic forms are bolded and colored by ambiguity class. Note that, while in the plural, the nominative and accusative are always syncretic across all paradigms, the same is not true in the singular. Figure 1. It is expressed as follows. pθ (w |`"
P19-1505,Q19-1021,1,\N,Missing
Q15-1031,N15-1094,1,0.837701,"ynamically restrict to a finite support set of plausible values for m. We take this to be the union of the 20-best lists of all messages sent to m.9 We then prune those messages so that they give weight 0 to all strings outside m’s support set. As a result, m’s outgoing messages and belief are also confined to its support set. Note that the support set is not handspecified, but determined automatically by taking the top hypotheses under the probability model. Improved approaches with no pruning are possible. After submitting this paper, we developed a penalized expectation propagation method (Cotterell and Eisner, 2015). It dynamically approximates messages using log-linear functions (based on variable-order n-gram features) whose support is the entire space Σ∗ . We also developed a dual decomposition method (Peng et al., 2015), which if it converges, exactly recovers the single most 8 This is standard—although the uniform distribution over the space of strings is actually an improper distribution. It is expressed by a single-state WFSM whose arcs have weight 1. It can be shown that the beliefs are proper distributions after one iteration, though the upward messages may not be. 9 In general, we should update"
Q15-1031,P14-2102,1,0.677762,"(after erasing #) to the surface [wER@r]. At the point shown, it is applying the “intervocalic alveolar flapping” rule, replacing /t/ in this context by applying SUBST(R). cape such low-likelihood solutions, much as backtracking escapes zero-likelihood solutions. 3.2 Mapping URs to SRs: The phonology Sθ We currently model Sθ (s |u) as the probability that a left-to-right stochastic contextual edit process (Figure 2) would edit u into s. This probability is a sum over all edit sequences that produce s from u—that is, all s-to-u alignments. Stochastic contextual edit processes were described by Cotterell et al. (2014). Such a process writes surface string s ∈ Σ∗s while reading the underlying string u ∈ Σ∗u . If the process has so far consumed some prefix of the input and produced some prefix of the output, it will next make a stochastic choice among 2|Σs |+ 1 possible edits. Edits of the form SUBST(c) or INSERT(c) (for c ∈ Σs ) append c to the output string. Edits of the form SUBST(c) or DELETE will (also) consume the next input phoneme; if no input phonemes remain, the only possible edits are INSERT(c) or HALT. The stochastic choice of edit, given context, is governed by a conditional log-linear distribut"
Q15-1031,D09-1011,1,0.359415,"vely. Each variable’s distribution is conditioned on the values of its parents, if any. Layer 1 represents the unknown M (a) for various a. Notice that each M (a) is softly constrained by the prior Mφ , and also by its need to help produce various observed surface words via Sθ . Each underlying word u at level 2 is a concatenation of its underlying morphs M (ai ) at level 1. Thus, the topology at levels 1–2 is given by supervision. We would have to learn this topology if the word’s morphemes ai were not known. Our approach captures the unbounded generative capacity of language. In contrast to Dreyer and Eisner (2009) (see section 8), we have defined a directed graphical model. Hence new unobserved descendants can be added without changing the posterior distribution over the existing variables. So our finite network can be viewed as a subgraph of an infinite graph. That is, we make no closed-vocabulary assumption, but implicitly include (and predict the surface forms of) any unobserved words that could result from combining morphemes, even morphemes not in our dataset. While the present paper focuses on word types, we could extend the model to consider tokens as well. In Figure 1, each phonological surface"
Q15-1031,D11-1057,1,0.865251,"Missing"
Q15-1031,D10-1056,0,0.0117644,"ctor may be the increased error rate of the phonological learner, visible even with oracle data. Thus, we suspect that a Sθ model with better generalization would improve our results at all training sizes. Note that weakening Sθ —allowing only “noisy concatenation”—clearly harms the method, proving the need for true phonological modeling. 8 Related Work We must impute the inputs to the phonological noisy channel Sθ (URs) because we observe only the outputs (SRs). Other NLP problems of this form include unsupervised text normalization (Yang and Eisenstein, 2013), unsupervised training of HMMs (Christodoulopoulos et al., 2010), and particularly unsupervised lexicon acquisition from phonological data (Elsner et al., 2012). However, unlike these studies, we currently use some indirect supervision—we know each SR’s morpheme sequence, though not the actual morphs. Jarosz (2013, §2) and Tesar (2014, chapters 5– 6) review work on learning the phonology Sθ . Phonologists pioneered stochastic-gradient and passive-aggressive training methods—the Gradual Learning Algorithm (Boersma, 1998) and ErrorDriven Constraint Demotion (Tesar and Smolensky, 1998)—for structured prediction of the surface word s from the underlying word u"
Q15-1031,D08-1113,1,0.747087,"morphs, either identifying a small set of plausible morphs or prohibiting segmental insertion/deletion. We use finite-state methods so that it is possible to consider the space Σ∗u of all strings. 13 She still assumes that word SRs are annotated with morpheme boundaries, and that a small set of possible morphs is given. These assumptions are relaxed by Eisenstat (2009). On the other hand, we are divided from previous work by our inability to use an OT grammar (Prince and Smolensky, 2004), a stochastic OT grammar (Boersma, 1997), or even a maximum entropy grammar (Goldwater and Johnson, 2003; Dreyer et al., 2008; Eisenstat, 2009). The reason is that our BP method inverts the phonological mapping Sθ to find possible word URs. Given a word SR s, we construct a WFSM (message) that scores every possible UR u ∈ Σ∗u —the score of u is Sθ (s |u). For this to be possible without approximation, Sθ itself must be represented as a WFSM (section 3.2). Unfortunately, the WFSM for a maximum entropy grammar does not compute Sθ but only an unnormalized version, with a different normalizing constant Zu needed for each u. We plan to confront this issue in future work. In the NLP community, Elsner et al. (2013) resembl"
Q15-1031,P02-1008,1,0.80485,"y motivation for probabilistic models of phonology (Pierrehumbert, 2003) has been to explain “soft” phenomena: synchronic variation (Sankoff, 1978; Boersma and Hayes, 2001) or graded acceptability judgments on novel surface forms (Hayes and Wilson, 2008). These applications are orthogonal to our motivation, as we do not observe any variation or gradience in our present experiments. Fundamentally, we use probabilities to measure irregularity—which simply means unpredictability and is a matter of degree. Our objective function will quantitatively favor explanations that show greater regularity (Eisner, 2002b). A probabilistic treatment also allows relatively simple learning methods (e.g., Boersma and Hayes (2001)) since inference never has to backtrack from a contradiction. Our method searches a continuous space of phonologies Sθ , all of which are consistent with every mapping S. That is, we always have Sθ (s |u) > 0 for all u, s, so our current guess of Sθ is always capable of explaining the observed words, albeit perhaps with low probability. Our EM learner tunes Sθ (and Mφ ) so as to raise the probability of the observed surface forms, marginalizing over the reconstructed lexicon M of underl"
Q15-1031,P12-1020,0,0.139946,"spect that a Sθ model with better generalization would improve our results at all training sizes. Note that weakening Sθ —allowing only “noisy concatenation”—clearly harms the method, proving the need for true phonological modeling. 8 Related Work We must impute the inputs to the phonological noisy channel Sθ (URs) because we observe only the outputs (SRs). Other NLP problems of this form include unsupervised text normalization (Yang and Eisenstein, 2013), unsupervised training of HMMs (Christodoulopoulos et al., 2010), and particularly unsupervised lexicon acquisition from phonological data (Elsner et al., 2012). However, unlike these studies, we currently use some indirect supervision—we know each SR’s morpheme sequence, though not the actual morphs. Jarosz (2013, §2) and Tesar (2014, chapters 5– 6) review work on learning the phonology Sθ . Phonologists pioneered stochastic-gradient and passive-aggressive training methods—the Gradual Learning Algorithm (Boersma, 1998) and ErrorDriven Constraint Demotion (Tesar and Smolensky, 1998)—for structured prediction of the surface word s from the underlying word u. If s is not fully observed during training (layer 4 of Figure 1 is observed, not layer 3), the"
Q15-1031,D13-1005,0,0.197829,"n, 2003; Dreyer et al., 2008; Eisenstat, 2009). The reason is that our BP method inverts the phonological mapping Sθ to find possible word URs. Given a word SR s, we construct a WFSM (message) that scores every possible UR u ∈ Σ∗u —the score of u is Sθ (s |u). For this to be possible without approximation, Sθ itself must be represented as a WFSM (section 3.2). Unfortunately, the WFSM for a maximum entropy grammar does not compute Sθ but only an unnormalized version, with a different normalizing constant Zu needed for each u. We plan to confront this issue in future work. In the NLP community, Elsner et al. (2013) resembles our work in many respects. Like us, they recover a latent underlying lexicon (using the same simple prior Mφ ) and use EM to learn a phonology (rather similar to our Sθ , though less powerful).14 Unlike us, they do not assume annotation of the (abstract) morpheme sequence, but jointly learn a nonparametric bigram model to discover the morphemes. Their evaluation is quite different, as their aim is actually to recover underlying words from phonemically transcribed child-directed English utterances. However, nothing in their model distinguishes words from morphemes—indeed, sometimes t"
Q15-1031,E12-1068,0,0.0103876,"obabilistic finite-state transducer). Layer 4 derives observable phonetic forms from layer 3. This deletes unpronounced symbols such as syllable boundaries, and translates the phonemes into an observed phonetic, articulatory, or acoustic representation. However, our present paper simply merges layers 3 and 4: our layer 3 does not currently make use of any unpronounced symbols (e.g., syllable boundaries) and we observe it directly. to model surface spellings (as needed for MT on text). Good morphological analysis has been used to improve NLP tasks such as machine translation, parsing, and NER (Fraser et al., 2012; Hohensee and Bender, 2012; Yeniterzi, 2011). Using loopy belief propagation, this paper attacks larger-scale learning problems than prior work on this task (section 8). We also develop a new evaluation paradigm that examines how well an inferred grammar predicts held-out SRs. Unlike previous algorithms, we do not pre-restrict the possible URs for each morpheme to a small or structured finite set, but use weighted finite-state machines to reason about the infinite space of all strings. Our graphical model captures the standard assumption that each morpheme has a single UR, unlike some probabi"
Q15-1031,P10-1105,0,0.0723142,"the partition function and requires retraining. By contrast, our trained directed model is a productive phonological system that can generate unboundedly many new words (see section 4.1). By analogy, n samples from a Gaussian would be described with a directed model, and inferring the Gaussian parameters predicts any number of future samples n + 1, n + 2, . . .. Bouchard-Cˆot´e et al., in several papers from 2007 through 2013, have used directed graphical models over strings, like ours though without loops, to model diachronic sound change. Sometimes they use belief propagation for inference (Hall and Klein, 2010). Their goal is to recover latent historical forms (conceptually, surface forms) rather than latent underlying forms. The results are evaluated against manual reconstructions. None of this work has segmented words into morphs, although Dreyer et al. (2008) did segment surface words into latent “regions.” Creutz and Lagus (2005) and Goldsmith (2006) segment an unannotated collection of words into reusable morphs, but without modeling contextual sound change, i.e., phonology. 9 Conclusions and Future Work We have laid out a probabilistic model for generative phonology. This lets us infer likely"
Q15-1031,N12-1032,0,0.0283479,"ate transducer). Layer 4 derives observable phonetic forms from layer 3. This deletes unpronounced symbols such as syllable boundaries, and translates the phonemes into an observed phonetic, articulatory, or acoustic representation. However, our present paper simply merges layers 3 and 4: our layer 3 does not currently make use of any unpronounced symbols (e.g., syllable boundaries) and we observe it directly. to model surface spellings (as needed for MT on text). Good morphological analysis has been used to improve NLP tasks such as machine translation, parsing, and NER (Fraser et al., 2012; Hohensee and Bender, 2012; Yeniterzi, 2011). Using loopy belief propagation, this paper attacks larger-scale learning problems than prior work on this task (section 8). We also develop a new evaluation paradigm that examines how well an inferred grammar predicts held-out SRs. Unlike previous algorithms, we do not pre-restrict the possible URs for each morpheme to a small or structured finite set, but use weighted finite-state machines to reason about the infinite space of all strings. Our graphical model captures the standard assumption that each morpheme has a single UR, unlike some probabilistic learners. However, w"
Q15-1031,W09-0803,0,0.0294032,"Any other WFSMs could be substituted. We are on rather firm ground in restricting to finite-state (regular) models of Sθ . The apparent regularity of natural-language phonology was first observed by Johnson (1972), so computational phonology has generally preferred grammar formalisms that compile into (unweighted) finite-state machines, whether the formalism is based on rewrite rules (Kaplan and Kay, 1994) or constraints (Eisner, 2002a; Riggle, 2004). Similarly, U could be any finite-state relation,7 not just concatenation as in section 2. Thus our framework could handle templatic morphology (Hulden, 2009), infixation, or circumfixation. Although only regular factors are allowed in our graphical model, a loopy graphical model with multiple such factors can actually capture nonregular phenomena, for example by using auxiliary variables (Dreyer and Eisner, 2009, §3.4). Approximate inference then proceeds by loopy BP on this model. In particular, reduplication is not regular if unbounded, but we can adopt morphological doubling theory (Inkelas and Zoll, 2005) and model it by having U concatenate two copies of the same morph. During inference of URs, this morph exchanges messages with two substring"
Q15-1031,W06-3207,0,0.0268686,"the SRs. (Tesar and Merchant instead used binary variables, one for each segmental feature in each UR—requiring the simplifying assumption that the URs are known except for their segmental features. They assume that SRs are annotated with morph boundaries and that the phonology only changes segmental features, never inserting or deleting segments.) On the other hand, Tesar and Merchant reason globally about the constraint ranking, whereas in this paper, we only locally improve the phonology—we use EM, rather than the full Bayesian approach that treats the parameters θ~ as variables within BP. Jarosz (2006) is closest to our work in that she uses EM, just as we do, to maximize the probability of observed surface forms whose constituent morphemes (but not morphs) are known.13 Her model is a probabilistic analogue of Apoussidou (2006), who uses a latent-variable structured perceptron. A non-standard aspect of this model (defended by Pater et al. (2012)) is that a morpheme a can stochastically choose different morphs M (a) when it appears in different words. To obtain a single shared morph, one could penalize this distribution’s entropy, driving it toward 0 as learning proceeds. Such an approach—wh"
Q15-1031,J94-3001,0,0.484864,"t layers 1, 2, and 3 given the values at their parents. As section 3 models these, for any φ and θ, we can represent Mφ as a 1-tape WFSM (acceptor), U as a multi-tape WFSM, and Sθ as a 2-tape WFSM (transducer).6 Any other WFSMs could be substituted. We are on rather firm ground in restricting to finite-state (regular) models of Sθ . The apparent regularity of natural-language phonology was first observed by Johnson (1972), so computational phonology has generally preferred grammar formalisms that compile into (unweighted) finite-state machines, whether the formalism is based on rewrite rules (Kaplan and Kay, 1994) or constraints (Eisner, 2002a; Riggle, 2004). Similarly, U could be any finite-state relation,7 not just concatenation as in section 2. Thus our framework could handle templatic morphology (Hulden, 2009), infixation, or circumfixation. Although only regular factors are allowed in our graphical model, a loopy graphical model with multiple such factors can actually capture nonregular phenomena, for example by using auxiliary variables (Dreyer and Eisner, 2009, §3.4). Approximate inference then proceeds by loopy BP on this model. In particular, reduplication is not regular if unbounded, but we c"
Q15-1031,D09-1005,1,0.27958,"al edits (section 3.2), averaged over edit contexts in proportion to how many times those contexts were likely encountered. The latent alignment makes the objective non-concave. In our EM setting, uk is not known. So our Mstep P replaces log Sθ (sk |uk ) with its expectation, uk bk (uk ) log Sθ (sk |uk ), where bk is the normalized belief about uk computed by the previous E-step. Since bk and Sθ are both represented by WFSMs (with 1 and 2 tapes respectively), it is possible to compute this quantity and its gradient exactly, using finite-state composition in a secondorder expectation semiring (Li and Eisner, 2009). For speed, however, we currently prune bk back to the 5-best values of uk . This lets us use a simpler and faster approach: a weighted average over 5 runs of the Cotterell et al. (2014) algorithm. Our asymptotic runtime benefits from the fact that our graphical model is directed (so our objective does not have to contrast with all other values of uk ) and the fact that Sθ is locally normalized (so our objective does not have to contrast with all other values of sk for each uk ). In practice we are far faster than Dreyer and Eisner (2009). We initialized the parameter vector θ to ~0, except f"
Q15-1031,W12-2308,0,0.0252249,"ents.) On the other hand, Tesar and Merchant reason globally about the constraint ranking, whereas in this paper, we only locally improve the phonology—we use EM, rather than the full Bayesian approach that treats the parameters θ~ as variables within BP. Jarosz (2006) is closest to our work in that she uses EM, just as we do, to maximize the probability of observed surface forms whose constituent morphemes (but not morphs) are known.13 Her model is a probabilistic analogue of Apoussidou (2006), who uses a latent-variable structured perceptron. A non-standard aspect of this model (defended by Pater et al. (2012)) is that a morpheme a can stochastically choose different morphs M (a) when it appears in different words. To obtain a single shared morph, one could penalize this distribution’s entropy, driving it toward 0 as learning proceeds. Such an approach—which builds on a suggestion by Eisenstat (2009, §5.4)—would loosely resemble dual decomposition (Peng et al., 2015). Unlike our BP approach, it would maximize rather than marginalize over possible underlying morphs. Our work has focused on scaling up inference. For the phonology S, the above papers learn the weights or rankings of just a few plausib"
Q15-1031,D15-1108,1,0.912515,"utside m’s support set. As a result, m’s outgoing messages and belief are also confined to its support set. Note that the support set is not handspecified, but determined automatically by taking the top hypotheses under the probability model. Improved approaches with no pruning are possible. After submitting this paper, we developed a penalized expectation propagation method (Cotterell and Eisner, 2015). It dynamically approximates messages using log-linear functions (based on variable-order n-gram features) whose support is the entire space Σ∗ . We also developed a dual decomposition method (Peng et al., 2015), which if it converges, exactly recovers the single most 8 This is standard—although the uniform distribution over the space of strings is actually an improper distribution. It is expressed by a single-state WFSM whose arcs have weight 1. It can be shown that the beliefs are proper distributions after one iteration, though the upward messages may not be. 9 In general, we should update this support set dynamically as inference and learning improve the messages. But in our present experiments, that appears unnecessary, since the initial support set always appears to contain the “correct” UR. B"
Q15-1031,D13-1007,0,0.015784,"(Maori) in all the training SRs. However, a contributing factor may be the increased error rate of the phonological learner, visible even with oracle data. Thus, we suspect that a Sθ model with better generalization would improve our results at all training sizes. Note that weakening Sθ —allowing only “noisy concatenation”—clearly harms the method, proving the need for true phonological modeling. 8 Related Work We must impute the inputs to the phonological noisy channel Sθ (URs) because we observe only the outputs (SRs). Other NLP problems of this form include unsupervised text normalization (Yang and Eisenstein, 2013), unsupervised training of HMMs (Christodoulopoulos et al., 2010), and particularly unsupervised lexicon acquisition from phonological data (Elsner et al., 2012). However, unlike these studies, we currently use some indirect supervision—we know each SR’s morpheme sequence, though not the actual morphs. Jarosz (2013, §2) and Tesar (2014, chapters 5– 6) review work on learning the phonology Sθ . Phonologists pioneered stochastic-gradient and passive-aggressive training methods—the Gradual Learning Algorithm (Boersma, 1998) and ErrorDriven Constraint Demotion (Tesar and Smolensky, 1998)—for struc"
Q15-1031,P11-3019,0,0.0257895,"rives observable phonetic forms from layer 3. This deletes unpronounced symbols such as syllable boundaries, and translates the phonemes into an observed phonetic, articulatory, or acoustic representation. However, our present paper simply merges layers 3 and 4: our layer 3 does not currently make use of any unpronounced symbols (e.g., syllable boundaries) and we observe it directly. to model surface spellings (as needed for MT on text). Good morphological analysis has been used to improve NLP tasks such as machine translation, parsing, and NER (Fraser et al., 2012; Hohensee and Bender, 2012; Yeniterzi, 2011). Using loopy belief propagation, this paper attacks larger-scale learning problems than prior work on this task (section 8). We also develop a new evaluation paradigm that examines how well an inferred grammar predicts held-out SRs. Unlike previous algorithms, we do not pre-restrict the possible URs for each morpheme to a small or structured finite set, but use weighted finite-state machines to reason about the infinite space of all strings. Our graphical model captures the standard assumption that each morpheme has a single UR, unlike some probabilistic learners. However, we do not try to le"
Q18-1003,D15-1041,0,0.0188175,"latedness, which is a proxy for semantic coherence. HR (highrelatedness) words were judged to be more compositional than LR (low-relatedness) words. Character-Level Neural Retrofitting. As a further strong baseline, we consider a retrofitting (Faruqui et al., 2015) approach based on characterlevel recurrent neural networks. Recently, running a recurrent net over the character stream has become a popular way of incorporating subword information into a model—empirical gains have been observed in a diverse set of NLP tasks: POS tagging (dos Santos and Zadrozny, 2014; Ling et al., 2015), parsing (Ballesteros et al., 2015) and language modeling (Kim et al., 2016). To the best of our knowledge, character-level retrofitting is a novel approach. Given a vector v for a word form w, we seek a function to minimize the following objective 1 ||v − hN ||22 , 2 (10) where hN is the final hidden state of a recurrent neural architecture, i.e., hi = σ(Ahi−1 + Bwi ), (11) where σ is a non-linearity and wi is the ith character in w, hi−1 is the previous hidden state and A and B are matrices. While we have defined the architecture for a vanilla RNN, we experiment with two more advanced recurrent architectures: GRUs (Cho et al."
Q18-1003,D14-1179,0,0.013185,"Missing"
Q18-1003,P11-1004,0,0.0806345,"Missing"
Q18-1003,P14-2102,1,0.89922,"ix as σ 2 I, which is positive definite. representation (SR) w, the character string observed in raw text, and an underlying representation (UR), a character string with orthographic processes reversed. The aim of this factor is to place high weight on good pairs, e.g., the pair (w=questionably,u=questionablely), so we can accurately restore character-level changes. We encode this portion of the model as a weighted finite-state machine for ease of computation. This factor generalizes probabilistic edit distance (Ristad and Yianilos, 1998) by looking at additional input and output context; see Cotterell et al. (2014) for details. As mentioned above and in contrast to Cotterell et al. (2014), we bound the insertion limit in the edit distance model.8 Computing the score between two strings u and w requires a dynamic program that runs in O(|u|·|w|). This is a generalization of the forward algorithm for Hidden Markov Models (HMMs) (Rabiner, 1989). We employ standard feature templates for the task that look at features of edit operations, e.g., substitute i for y, in varying context granularities. See Cotterell et al. (2016b) for details. Recent work has also explored weighting of WFST arcs with scores compute"
Q18-1003,K15-1017,1,0.927378,"Missing"
Q18-1003,D16-1256,1,0.883949,"Missing"
Q18-1003,N16-1080,1,0.916571,"Missing"
Q18-1003,D11-1057,0,0.0361717,"Missing"
Q18-1003,N15-1184,0,0.0858587,"Missing"
Q18-1003,Q16-1001,0,0.0133792,"it is un∇θ log p(v, s, l, u |w) = f (s, l, u)&gt; + g(u, w)&gt; supervised and uses vectors as features, rather than 1 explicitly treating vector composition. All of the − 2 (v − Cβ (s, l))∇θ Cβ (s, l) σ above work focuses on surface segmentation and not − ∇θ log Zθ (w), (9) canonical segmentation, as we do. A related line of work that has different goals conwhere we use the importance sampling algorithm cerns morphological generation. Two recent papers described in §4.1 to approximate the gradient of that address this problem using deep learning are the log-partition function, following Bengio and Faruqui et al. (2016a) and Faruqui et al. (2016b). Senecal (2003). Note that ∇θ Cβ (s, l) depends on In an older line of work, Yarowsky and Wicenthe composition function used. In the most com- towski (2000) and Wicentowski (2002) exploit log plicated case when Cβ is a RNN, we can com- frequency ratios of inflectionally related forms to pute ∇β Cβ (s, l) efficiently with backpropagation tease apart that, e.g., the past tense of sing is not through time (Werbos, 1990). We take M = 10 im- singed, but instead sang. Related work by Dreyer portance samples; using so few samples can lead to a and Eisner (2011) uses a Di"
Q18-1003,N16-1077,0,0.0137907,"it is un∇θ log p(v, s, l, u |w) = f (s, l, u)&gt; + g(u, w)&gt; supervised and uses vectors as features, rather than 1 explicitly treating vector composition. All of the − 2 (v − Cβ (s, l))∇θ Cβ (s, l) σ above work focuses on surface segmentation and not − ∇θ log Zθ (w), (9) canonical segmentation, as we do. A related line of work that has different goals conwhere we use the importance sampling algorithm cerns morphological generation. Two recent papers described in §4.1 to approximate the gradient of that address this problem using deep learning are the log-partition function, following Bengio and Faruqui et al. (2016a) and Faruqui et al. (2016b). Senecal (2003). Note that ∇θ Cβ (s, l) depends on In an older line of work, Yarowsky and Wicenthe composition function used. In the most com- towski (2000) and Wicentowski (2002) exploit log plicated case when Cβ is a RNN, we can com- frequency ratios of inflectionally related forms to pute ∇β Cβ (s, l) efficiently with backpropagation tease apart that, e.g., the past tense of sing is not through time (Werbos, 1990). We take M = 10 im- singed, but instead sang. Related work by Dreyer portance samples; using so few samples can lead to a and Eisner (2011) uses a Di"
Q18-1003,N16-1101,0,0.0235884,"Missing"
Q18-1003,N16-1155,0,0.0140959,"a novel approach. Given a vector v for a word form w, we seek a function to minimize the following objective 1 ||v − hN ||22 , 2 (10) where hN is the final hidden state of a recurrent neural architecture, i.e., hi = σ(Ahi−1 + Bwi ), (11) where σ is a non-linearity and wi is the ith character in w, hi−1 is the previous hidden state and A and B are matrices. While we have defined the architecture for a vanilla RNN, we experiment with two more advanced recurrent architectures: GRUs (Cho et al., 2014b) and LSTMs (Hochreiter and Schmidhuber, 1997) as well as deep variants (Sutskever et al., 2014; Gillick et al., 2016; Firat et al., 2016). Importantly, this model has no knowledge of morphology—it can only rely on representations it extracts from the characters. This gives us a clear ablation on the benefit of adding structured morphological knowledge. We optimize the depth and the size of the hidden units on development data using a coarse-grained grid search. We found a depth of 2 and hidden units of size 100 (in both LSTM and GRU) performed best. We trained all models for 100 iterations of Adam (Kingma and Ba, 2015) with L2 regularization with regularization coefficient 0.01. Table 4 shows that the two c"
Q18-1003,P16-1020,0,0.0509962,"Missing"
Q18-1003,D16-1097,1,0.868472,"Missing"
Q18-1003,J94-3001,0,0.135854,"deviation is given in parentheses. We compare against two baselines that do not make use of semantic vectors: (i) “Semi-CRF (baseline)”, a semi-CRF that cannot account for orthographic changes and (ii) “Joint (Baseline)”, a version of our joint model without vectors. We also compare against an oracle version with access to gold URs (“Joint + UR (Oracle)”, “Joint + UR + Vec (Oracle)”), revealing that the toughest part of the canonical segmentation task is reversing the orthographic changes. calization as an orthographic analogue to phonology. On this interpretation, the finite-state systems of Kaplan and Kay (1994), which computationally apply SPE-style phonological rules (Chomsky and Halle, 1968), may be run backwards to get canonical underlying forms. 6 Experiments and Results We conduct experiments on English and German derivational morphology. We analyze our joint model’s ability to segment words into their canonical morphemes as well as its ability to compositionally derive vectors for new words. Finally, we explore the relationship between distributional semantics and morphological productivity. For English, we use the pretrained vectors of Levy and Goldberg (2014a) for all experiments. For German"
Q18-1003,W15-0108,0,0.171782,"Missing"
Q18-1003,W10-2211,0,0.0323643,"del, i.e., how much could we benefit from a richer model. Our hyperparameters are (i) the regularization coefficient λ and (ii) σ 2 , the variance of the Gaussian factor. We use grid search to tune them: λ ∈ {0.0, 101 , 102 , 103 , 104 , 105 }, σ 2 ∈ {0.25, 0.5, 0.75, 1.0}. Metrics. We use three metrics to evaluate segmentation accuracy. Note that the evaluation of canonical segmentation is hard since a system may return a sequence of morphemes whose concatenation is not the same length as the concatenation of the gold morphemes. This rules out metrics for surface segmentation like border F1 (Kurimo et al., 2010), which require the strings to be of the same length. We now define the metrics. (i) Segmentation accuracy measures whether every single canonical morpheme in the returned sequence is correct. It is inflexible: closer answers are penalized the same as 13 i.e., a model without the Gaussian factor that scores vectors. 41 more distant answers. (ii) Morpheme F1 (van den Bosch and Daelemans, 1999) takes the predicted sequence of canonical morphemes, turns it into a set, computes precision and recall in the standard way and based on that then computes F1 . This metric gives credit if some of the can"
Q18-1003,P13-1149,0,0.317863,"erived from the adjective discontented. It is true that both words share a close semantic relationship, but the transformation is clearly more than a simple inflectional marking of syntax. Indeed, we can go one step further and define a chain of words content 7→ contented 7→ discontented 7→ discontentedness. In the computational literature, derivational morphology has received less attention than inflectional. There are, however, two bodies of work on derivation in computational linguistics. First, there is a series of papers that explore the relation between lexical semantics and derivation (Lazaridou et al., 2013; Zeller et al., 2014; Pad´o et al., 2015; Kisselew et al., 2015). All of these assume a gold morphological analysis and primarily focus on the effect of derivation on distributional semantics. The second body of work, e.g., the unsupervised morphological segmenter M ORFESSOR (Creutz and Lagus, 2007), does not deal with semantics and makes no distinction between inflectional and derivational morphology.3 Even though the boundary between inflectional and derivational morphology is a continuum rather than a rigid divide (Haspelmath and Sims, 2013), there is still the clear distinction that deriv"
Q18-1003,P14-2050,0,0.530231,"antics of morphemes and words allows us to improve morphological analysis. On the English portion of CELEX (Baayen et al., 1993), we achieve a 5 point improvement in segmentation accuracy and a 3 point improvement in morpheme F1 . On the German DErivBase dataset we achieve a 3 point improvement in segmentation accuracy and a 3 point improvement in morpheme F1 . • Second, we explore improved models of vector composition for synthesizing word meaning. We find a recurrent neural network improves over previously proposed additive models. Moreover, we find that more syntactically oriented vectors (Levy and Goldberg, 2014a) are better suited for morphology than bag-ofword (BOW) models. • Finally, we explore the productivity of English derivational affixes in the context of distributional semantics. 2 Derivational Morphology Two important goals of morphology, the linguistic study of the internal structure of words, are to describe the relation between different words in the lexicon and to decompose them into morphemes, the smallest linguistic unit bearing meaning. Morphology can be divided into two types: inflectional and derivational. Inflectional morphology is the set of processes through which the word form"
Q18-1003,P96-1004,0,0.455302,". Inflectional morphology is the set of processes through which the word form outwardly 34 displays syntactic information, e.g., verb tense. It follows that an inflectional affix typically neither changes the part-of-speech (POS) nor the semantics of the word. For example, the English verb to run takes various forms: run, runs, ran and running, all of which convey “moving by foot quickly”, but appear in complementary syntactic contexts. Derivation deals with the formation of new words that have semantic shifts in meaning (often including POS) and is tightly intertwined with lexical semantics (Light, 1996). Consider the example of the English noun discontentedness, which is derived from the adjective discontented. It is true that both words share a close semantic relationship, but the transformation is clearly more than a simple inflectional marking of syntax. Indeed, we can go one step further and define a chain of words content 7→ contented 7→ discontented 7→ discontentedness. In the computational literature, derivational morphology has received less attention than inflectional. There are, however, two bodies of work on derivation in computational linguistics. First, there is a series of pape"
Q18-1003,D15-1176,0,0.0357222,"vectors are annotated for relatedness, which is a proxy for semantic coherence. HR (highrelatedness) words were judged to be more compositional than LR (low-relatedness) words. Character-Level Neural Retrofitting. As a further strong baseline, we consider a retrofitting (Faruqui et al., 2015) approach based on characterlevel recurrent neural networks. Recently, running a recurrent net over the character stream has become a popular way of incorporating subword information into a model—empirical gains have been observed in a diverse set of NLP tasks: POS tagging (dos Santos and Zadrozny, 2014; Ling et al., 2015), parsing (Ballesteros et al., 2015) and language modeling (Kim et al., 2016). To the best of our knowledge, character-level retrofitting is a novel approach. Given a vector v for a word form w, we seek a function to minimize the following objective 1 ||v − hN ||22 , 2 (10) where hN is the final hidden state of a recurrent neural architecture, i.e., hi = σ(Ahi−1 + Bwi ), (11) where σ is a non-linearity and wi is the ith character in w, hi−1 is the previous hidden state and A and B are matrices. While we have defined the architecture for a vanilla RNN, we experiment with two more advanced recur"
Q18-1003,W13-3512,0,0.100424,"zation. for the semi-supervised incorporation of distributional semantics into a structured model of inflec4.3 Decoding tional paradigm completion. Decoding the model is also intractable. To approxiOur work is also related to recent attempts to inmate the solution, we again employ importance sam- tegrate morphological knowledge into general empling. We take M =10,000 importance samples and bedding models. For example, Botha and Blunselect the highest weighted sample. som (2014) train a log-bilinear language model that models the composition of morphological structure. 5 Related Work Likewise, Luong et al. (2013) train a recursive neural The idea that vector semantics is useful for mor- network (Goller and K¨uchler, 1996) over a heuristiphological segmentation is not new. Count vectors cally derived tree structure to learn morphological (Salton, 1971; Turney and Pantel, 2010) have been composition over continuous vectors. Our work is shown to be beneficial in the unsupervised induction different in that we learn a joint model of segmenof morphology (Schone and Jurafsky, 2000; Schone tation and composition. Moreover, supervised morand Jurafsky, 2001). Embeddings were shown to phological analysis can dr"
Q18-1003,P08-1028,0,0.0631841,".79 .78 .77 .57 .80 .81 .80 .74 .72 .67 .80 .81 .81 .75 .73 BOW2 Lazaridou all HR LR Table 4: Vector approximation (measured by mean cosine similarity) with gold morphology on the train/test split of Lazaridou et al. (2013). HR/LR = high/low-relatedness words. See Lazaridou et al. (2013) for details. rent neural networks are currently the most widely used nonlinear sequence model and simple RNNs are the simplest such models. Part of the motivation for considering a richer class of models lies in our removal of the twomorpheme assumption. Indeed, it is unclear that the wadd and fulladd models (Mitchell and Lapata, 2008) are useful models in the general case of multimorphemic words—the weights are tied by position, i.e., the first morpheme’s vector (be it a prefix or stem) is always multiplied by the same matrix. Comparison with Lazaridou et al. To compare with Lazaridou et al. (2013), we use their exact train/test split. Those results are reported in Table 4. This dataset enforces that all words are composed of 42 exactly two morphemes. Thus, a word like unquestionably is segmented as un+questionably, without further decomposition. The vectors employed by Lazaridou et al. (2013) are high-dimensional count ve"
Q18-1003,D14-1095,0,0.0566642,"Missing"
Q18-1003,Q15-1012,0,0.0837571,"makes no distinction between inflectional and derivational morphology.3 Even though the boundary between inflectional and derivational morphology is a continuum rather than a rigid divide (Haspelmath and Sims, 2013), there is still the clear distinction that derivation changes meaning whereas inflection does not. Our goal in this paper is to develop an account of how the meaning of a word form can be computed jointly, combining these two lines of work. Productivity and Semantic Coherence. We highlight two related issues in derivation that motivated the development of our model: productivity 3 Narasimhan et al. (2015) also make no distinction between inflectional and derivational morphology, but their model is an exception in that it includes vector similarity as a semantic feature. See §5 for discussion. and semantic coherence. Roughly, a productive affix is one that can still actively be employed to form new words in a language. For example, the English nominalizing affix ness (red7→red+ness) can be attached to just about any adjective, including novel forms. In contrast, the archaic English nominalizing affix th (dear7→dear+th, heal7→heal+th, steal7→steal+th) does not allow us to form new words such as"
Q18-1003,N16-1076,1,0.81264,"the edit distance model.8 Computing the score between two strings u and w requires a dynamic program that runs in O(|u|·|w|). This is a generalization of the forward algorithm for Hidden Markov Models (HMMs) (Rabiner, 1989). We employ standard feature templates for the task that look at features of edit operations, e.g., substitute i for y, in varying context granularities. See Cotterell et al. (2016b) for details. Recent work has also explored weighting of WFST arcs with scores computed by LSTMs (Hochreiter and Schmidhuber, 1997), obviating the need for human selection of feature templates (Rastogi et al., 2016). 3.2 Segmentation Factor The second factor  is the segmentation factor: &gt; exp f (s, l, u) η . The goal of this factor is to score a segmentation s of a UR u. In our example, it scores the input-output pair (u=questionablely, s=question+able+ly). It additionally scores a labeling of the segmentation. Our label set in this work is L = {stem, prefix, suffix}. The proper labeling of the segmentation above is l=question:stem+able:suffix+ly:suffix. The labeling is critical for our composition functions Cβ (Cotterell et al., 2015): which vectors are used depends on the label given to the segment; e"
Q18-1003,W13-3504,0,0.0197535,"d K¨uchler, 1996) over a heuristiphological segmentation is not new. Count vectors cally derived tree structure to learn morphological (Salton, 1971; Turney and Pantel, 2010) have been composition over continuous vectors. Our work is shown to be beneficial in the unsupervised induction different in that we learn a joint model of segmenof morphology (Schone and Jurafsky, 2000; Schone tation and composition. Moreover, supervised morand Jurafsky, 2001). Embeddings were shown to phological analysis can drastically outperform unsuact similarly (Soricut and Och, 2015). Our method pervised analysis (Ruokolainen et al., 2013). Early work by Kay (1977) can be interpreted as differs from this line of research in two key ways. (i) We present a probabilistic model of the pro- finite-state canonical segmentation, but it neither adcess of synthesizing the word’s meaning from the dresses nor experimentally evaluates the question of meaning of its morphemes. Prior work was ei- joint modeling of morphological analysis and sether not probabilistic or did not explicitly model mantic synthesis. Moreover, we may view canonito the joint model. Each of these distributions is tractable—we can compute the marginals with dynamic pr"
Q18-1003,W00-0712,0,0.571451,"aining locally normalized distributions for the individual components. Concretely, we train two proposal distributions q1 (u |w) and q2 (l, s |u) that take the form of a WFST and a semi-CRF, respectively, using features identical 11 The subvector β is responsible for computing only the mean of the Gaussian factor and thus has no impact on its normalization coefficient (Murphy, 2012). 12 Informally, the indirect importance sampling estimate converges to the true expectation as M → ∞ (the definition of statistical consistency). morphemes. (ii) Our method is supervised and focuses on derivation. Schone and Jurafsky (2000) and Soricut and Och (2015), being fully unsupervised, do not distinguish between inflection and derivation and Schone and Jurafsky (2001) focus on inflection. More recently, Narasimhan et al. (2015) 4.2 Learning look at the unsupervised induction of “morphologWe optimize the log-likelihood of the model using ical chains” with semantic vectors as a crucial feaA DAG RAD (Duchi et al., 2011), which is SGD with ture. Their goal is to jointly figure out an ordering a special per-parameter learning rate. The full gra- of word formation and a morphological segmentadient of the objective for one trai"
Q18-1003,N01-1024,0,0.0833015,", s |u) that take the form of a WFST and a semi-CRF, respectively, using features identical 11 The subvector β is responsible for computing only the mean of the Gaussian factor and thus has no impact on its normalization coefficient (Murphy, 2012). 12 Informally, the indirect importance sampling estimate converges to the true expectation as M → ∞ (the definition of statistical consistency). morphemes. (ii) Our method is supervised and focuses on derivation. Schone and Jurafsky (2000) and Soricut and Och (2015), being fully unsupervised, do not distinguish between inflection and derivation and Schone and Jurafsky (2001) focus on inflection. More recently, Narasimhan et al. (2015) 4.2 Learning look at the unsupervised induction of “morphologWe optimize the log-likelihood of the model using ical chains” with semantic vectors as a crucial feaA DAG RAD (Duchi et al., 2011), which is SGD with ture. Their goal is to jointly figure out an ordering a special per-parameter learning rate. The full gra- of word formation and a morphological segmentadient of the objective for one training example is: tion, e.g., play7→playful7→playfulness. While it is a rich model like ours, theirs differs in that it is un∇θ log p(v, s,"
Q18-1003,Q15-1026,0,0.0702613,"Missing"
Q18-1003,N15-1186,0,0.214251,"ibutions for the individual components. Concretely, we train two proposal distributions q1 (u |w) and q2 (l, s |u) that take the form of a WFST and a semi-CRF, respectively, using features identical 11 The subvector β is responsible for computing only the mean of the Gaussian factor and thus has no impact on its normalization coefficient (Murphy, 2012). 12 Informally, the indirect importance sampling estimate converges to the true expectation as M → ∞ (the definition of statistical consistency). morphemes. (ii) Our method is supervised and focuses on derivation. Schone and Jurafsky (2000) and Soricut and Och (2015), being fully unsupervised, do not distinguish between inflection and derivation and Schone and Jurafsky (2001) focus on inflection. More recently, Narasimhan et al. (2015) 4.2 Learning look at the unsupervised induction of “morphologWe optimize the log-likelihood of the model using ical chains” with semantic vectors as a crucial feaA DAG RAD (Duchi et al., 2011), which is SGD with ture. Their goal is to jointly figure out an ordering a special per-parameter learning rate. The full gra- of word formation and a morphological segmentadient of the objective for one training example is: tion, e.g."
Q18-1003,P99-1037,0,0.123687,"Missing"
Q18-1003,D14-1167,0,0.0335737,"is semantically compositional. Implicit in such a treatment is the desire to only segment a word if the segmentation is derived from a productive process. While most prior work on morphological segmentation has not explicitly modeled productivity,5 we believe, from a computational modeling perspective, segmenting only productive affixes is preferable. This is analogous to the modeling of phrase compositionality in embedding models, where it can be better to not further decompose noncompositional multiword units like named entities and idiomatic expressions; see, e.g., Mikolov et al. (2013b), Wang et al. (2014), Yin and Sch¨utze (2015), Yaghoobzadeh and Sch¨utze (2015), and Hashimoto and Tsuruoka (2016).6 In this paper, we refer to the semantic aspect of the model either as semantic synthesis or as coherence. These are two ways of looking at semantics that are related as follows. If the synthesis (i.e., composition) of the meaning of the derived form from the meaning of its parts is a regular application of the linguistic rules of derivation, then the meaning so constructed is coherent. These are the cases where a joint model is expected to be beneficial for both segmentation and interpretation. 3 A"
Q18-1003,D15-1083,1,0.784005,"Missing"
Q18-1003,P00-1027,0,0.370285,"Missing"
Q18-1003,N15-1091,1,0.843313,"Missing"
Q18-1003,P13-1118,0,0.0946051,"Missing"
Q18-1003,C14-1163,0,0.316639,"Missing"
Q18-1045,N15-1107,0,0.0383463,"representations imposes the strong claim that they have nothing in common phonologically, despite sharing all phonemes. P&P suggest this is unlikely to be the case. As one point of evidence, the metathesis of the kind that differentiates [slIt] and [sIlt] is a common diachronic change. In English, for example, [horse] evolved from [hross], and [bird] from [brid] (Jesperson, 1942). 654 3.2 Encoder-Decoder Architectures The NLP community has recently developed an analogue to the past-tense generation task originally considered by R&M: morphological paradigm completion (Durrett and DeNero, 2013; Ahlberg et al., 2015; Cotterell et al., 2015; Nicolai et al., 2015; Faruqui et al., 2016). The goal is to train a model capable of mapping the lemma (stem in the case of English) to each form in the paradigm. In the case of English, the goal would be to map a lemma, for example, walk, to its past-tense word walked as well as its gerund and third person present singular, walking and walks, respectively. This task generalizes the R&M setting in that it requires learning more mappings than simply lemma to past tense. By definition, any system that solves the more general morphological paradigm completion task must a"
Q18-1045,W02-0607,0,0.113403,"l networks at morphological inflection, often with a &gt;10 percentage point differential in accuracy on held-out data (Cotterell et al., 2016). In the linguistics literature, the most straightforward, direct, machine-implemented instantiation of the P&P proposal is, arguably, the Minimal Generalization Learner (MGL) of Albright and Hayes (2003) (c.f., Allen and Becker, 2015; Taatgen and Anderson, 2002). This model takes a mapping of phonemes to phonological features and makes feature-level generalizations like the post-voice [-d] rule described earlier. For a detailed technical description, see Albright and Hayes (2002). We treat the MGL as a baseline in §5. Unlike Taatgen and Anderson (2002), who explicitly account for dual route processing by including both memory retrieval and rule application submodules, Albright and Hayes (2003) and Allen and Becker (2015) rely on discovering and correctly weighting (using weights learned by log-linear regression) highly stem-specific rules to account for irregular transformations. Within the context of rule-based systems, several proposals focus on the question of rule generalization, rather than rule synthesis. That is, given a set of predefined rules, the systems imp"
Q18-1045,N16-1077,0,0.0150292,"ommon phonologically, despite sharing all phonemes. P&P suggest this is unlikely to be the case. As one point of evidence, the metathesis of the kind that differentiates [slIt] and [sIlt] is a common diachronic change. In English, for example, [horse] evolved from [hross], and [bird] from [brid] (Jesperson, 1942). 654 3.2 Encoder-Decoder Architectures The NLP community has recently developed an analogue to the past-tense generation task originally considered by R&M: morphological paradigm completion (Durrett and DeNero, 2013; Ahlberg et al., 2015; Cotterell et al., 2015; Nicolai et al., 2015; Faruqui et al., 2016). The goal is to train a model capable of mapping the lemma (stem in the case of English) to each form in the paradigm. In the case of English, the goal would be to map a lemma, for example, walk, to its past-tense word walked as well as its gerund and third person present singular, walking and walks, respectively. This task generalizes the R&M setting in that it requires learning more mappings than simply lemma to past tense. By definition, any system that solves the more general morphological paradigm completion task must also be able to solve the original R&M task. As we wish to highlight t"
Q18-1045,Q15-1031,1,0.827968,"s the strong claim that they have nothing in common phonologically, despite sharing all phonemes. P&P suggest this is unlikely to be the case. As one point of evidence, the metathesis of the kind that differentiates [slIt] and [sIlt] is a common diachronic change. In English, for example, [horse] evolved from [hross], and [bird] from [brid] (Jesperson, 1942). 654 3.2 Encoder-Decoder Architectures The NLP community has recently developed an analogue to the past-tense generation task originally considered by R&M: morphological paradigm completion (Durrett and DeNero, 2013; Ahlberg et al., 2015; Cotterell et al., 2015; Nicolai et al., 2015; Faruqui et al., 2016). The goal is to train a model capable of mapping the lemma (stem in the case of English) to each form in the paradigm. In the case of English, the goal would be to map a lemma, for example, walk, to its past-tense word walked as well as its gerund and third person present singular, walking and walks, respectively. This task generalizes the R&M setting in that it requires learning more mappings than simply lemma to past tense. By definition, any system that solves the more general morphological paradigm completion task must also be able to solve the"
Q18-1045,N13-1138,0,0.0127944,"s. The use of Wickelphone representations imposes the strong claim that they have nothing in common phonologically, despite sharing all phonemes. P&P suggest this is unlikely to be the case. As one point of evidence, the metathesis of the kind that differentiates [slIt] and [sIlt] is a common diachronic change. In English, for example, [horse] evolved from [hross], and [bird] from [brid] (Jesperson, 1942). 654 3.2 Encoder-Decoder Architectures The NLP community has recently developed an analogue to the past-tense generation task originally considered by R&M: morphological paradigm completion (Durrett and DeNero, 2013; Ahlberg et al., 2015; Cotterell et al., 2015; Nicolai et al., 2015; Faruqui et al., 2016). The goal is to train a model capable of mapping the lemma (stem in the case of English) to each form in the paradigm. In the case of English, the goal would be to map a lemma, for example, walk, to its past-tense word walked as well as its gerund and third person present singular, walking and walks, respectively. This task generalizes the R&M setting in that it requires learning more mappings than simply lemma to past tense. By definition, any system that solves the more general morphological paradigm"
Q18-1045,J96-4003,0,0.556026,"Missing"
Q18-1045,W16-2010,0,0.203836,"t et al., 2011), where each task is a single transduction (e.g., stem 7→ past). Note that R&M would need a separate network for each transduction (e.g., stem 7→ gerund and stem 7→ past participle). In fact, the current state of the art in NLP is to learn all morphological transductions in a paradigm jointly. The key insight is to construct a single network p(y |x, t) to predict all inflections, marking the transformation in the input string—that is, we feed the network the string “w a l k &lt;gerund&gt;” as input, augmenting the alphabet Σ to include morphological descriptors. We refer to reader to Kann and Schütze (2016) for the encoding details. Thus, one network predicts all forms; for example, p(y |x=walk, t=past) yields a distribution over past tense forms for walk and p(y |x=walk, t=gerund) yields a distribution over gerunds. 655 4 For the experiments in this paper, we use the variant in Bahdanau et al. (2014), which has explicitly been shown to be state of the art in morphological transduction (Cotterell et al., 2016). Type of Model Reference Input Output Feedforward Network Feedforward Network Feedforward Network Attractor Network Feedforward Network Recurrent Neural Network Feedforward Network Feedfor"
Q18-1045,J94-3001,0,0.472186,"rted to the past tense by the addition of the suffix [-d], the learner may create a collapsed rule that adds the suffix to all stems that end in a [+voice] sound. Different rules may be assigned weights (e.g., probabilities) derived from how many stem–past pairs exemplify the rules. These weights decide which rules to apply to produce the past tense. 657 Several systems that follow this rule-based template have been developed in NLP. Although the SPE itself does not impose detailed restrictions on rule structure, these systems generate rules that can be compiled into finite-state transducers (Kaplan and Kay, 1994; Ahlberg et al., 2015). These systems generalize well, but even the most successful variants have been shown to perform significantly worse than state-of-the-art neural networks at morphological inflection, often with a &gt;10 percentage point differential in accuracy on held-out data (Cotterell et al., 2016). In the linguistics literature, the most straightforward, direct, machine-implemented instantiation of the P&P proposal is, arguably, the Minimal Generalization Learner (MGL) of Albright and Hayes (2003) (c.f., Allen and Becker, 2015; Taatgen and Anderson, 2002). This model takes a mapping"
Q18-1045,P17-4012,0,0.0232387,"of 300 units. The encoder consists of a bidirectional LSTM (Hochreiter and Schmidhuber, 1997) with two layers. There is a dropout value of 0.3 between the layers. The decoder is a unidirectional LSTM with two layers. Both the encoder and decoder have 100 hidden units. Training was done using the Adadelta procedure (Zeiler, 2012) with a learning rate of 1.0 and a minibatch size of 20. We train for 100 epochs to ensure that all verb forms in the training data are adequately learned. We decode the model with beam search (k = 12). The code for our experiments is derived from the OpenNMT package (Klein et al., 2017). We use accuracy as our metric of performance. We train the MGL as a non-neural baseline, using the code distributed with Albright and Hayes (2003) with default settings. Experiment 1: Learning the Past Tense In the first experiment, we seek to show: (i) the ED model successfully learns to conjugate both regular and irregular verbs in the training data, and generalizes to held-out data at convergence and (ii) the pattern of errors the model exhibits is compatible with attested speech errors. CELEX Data Set. Our base data set consists of 4,039 verb types in the CELEX database (Baayen et al., 1"
Q18-1045,N15-1093,0,0.0132607,"they have nothing in common phonologically, despite sharing all phonemes. P&P suggest this is unlikely to be the case. As one point of evidence, the metathesis of the kind that differentiates [slIt] and [sIlt] is a common diachronic change. In English, for example, [horse] evolved from [hross], and [bird] from [brid] (Jesperson, 1942). 654 3.2 Encoder-Decoder Architectures The NLP community has recently developed an analogue to the past-tense generation task originally considered by R&M: morphological paradigm completion (Durrett and DeNero, 2013; Ahlberg et al., 2015; Cotterell et al., 2015; Nicolai et al., 2015; Faruqui et al., 2016). The goal is to train a model capable of mapping the lemma (stem in the case of English) to each form in the paradigm. In the case of English, the goal would be to map a lemma, for example, walk, to its past-tense word walked as well as its gerund and third person present singular, walking and walks, respectively. This task generalizes the R&M setting in that it requires learning more mappings than simply lemma to past tense. By definition, any system that solves the more general morphological paradigm completion task must also be able to solve the original R&M task. As"
Q18-1045,Q16-1037,0,0.0236324,"nsive computational resources, neural networks now constitute the state of the art in many NLP tasks. However, NLP as a discipline has a distinct practical bent and more often concerns itself with the large-scale engineering applications of language technologies. As such, the field’s findings are not always considered relevant to the scientific study of language (i.e., the field of linguistics). Recent work, however, has indicated that this perception is changing, with researchers, for example, probing the ability of neural networks to learn syntactic dependencies like subject–verb agreement (Linzen et al., 2016). Moreover, in the domains of morphology and phonology, both NLP practitioners and linguists have considered virtually identical problems, seemingly unbeknownst to each other. For example, both computational and theoretical morphologists are concerned with how different inflected forms in the lexicon are related and how one can learn to generate such inflections from data. Indeed, the original R&M network focuses on such a generation task, namely, generating English past tense forms from their stems. R&M’s network, however, was severely limited and did not generalize correctly to held-out data"
Q18-1045,D15-1166,0,0.0206257,"Missing"
Q18-1045,P15-2111,1,0.799902,"Missing"
Q19-1021,J92-1002,0,0.61358,"Missing"
Q19-1021,K18-3001,1,0.871881,"ge word type is observed too rarely for a learner to memorize an irregular surface form for it. Yet even in such a language, some word types are frequent, because some lexemes and some slots are especially useful. Thus, if learnability of the lexicon is indeed the driving force,13 then we should make the finer-grained prediction that irregularity may survive in the more frequently observed word types, regardless of paradigm size. Rarer forms are more likely to be predictable—meaning that they are either regular, or else irregular in a way that is predictable from a related frequent irregular (Cotterell et al., 2018a). perform a nonparametric permutation test that destroys the claimed correlation between the e-complexity and i-complexity values. From our observed points {(x1 , y1 ), . . . , (xm , ym )}, we can stochastically construct a new set of points {(x1 , yσ(1) ), . . . , (xm , yσ(m) )} where σ is a permutation of 1, 2, . . . , m selected uniformly at random. The resulting scatterplot is what we would expect under the null hypothesis of no correlation. Our p-value is the probability that the new scatterplot has an even emptier upper righthand corner—that is, the probability that the area under the"
Q19-1021,K17-2001,1,0.929128,"Missing"
Q19-1021,D07-1093,0,0.124261,"Missing"
Q19-1021,P14-2102,1,0.839978,"mitigate sample bias caused by variable-sized dictionaries in our database. In many languages, irregular words are also very frequent and may be more likely to be included in a dictionary first. If that’s the case, smaller dictionaries might have lexical statistics skewed toward irregulars more so than larger dictionaries. In general, larger dictionaries should be more representative samples of a language’s broader lexicon. 10 In the computer science literature, it is far more common to construct distributions with support over Σ∗ (Paz, 2003; Bouchard-Cˆot´e et al., 2007; Dreyer et al., 2008; Cotterell et al., 2014), which do not have this problem. 336 tween paradigm slots. We call this the ‘‘green scheme.’’ it is difficult to use them in a lower-resource scenario. To estimate a language’s e-complexity (§2.2.1), we average over all paradigms in the UniMorph inflected lexicon. To estimate i-complexity, we first partition those paradigms into training, development and test sets. We identify the paradigm shapes from the training set (§4.1). We also use the training set to train the parameters θ of our conditional distribution (§4.3), then estimate conditional entropies on the development set and use Edmonds"
Q19-1021,J86-2003,0,0.451974,"for Archi, for example). Our hypothesis is subtly different in that we postulate that morphological systems face a trade-off between e-complexity and i-complexity: a system may be complex under either metric, but not under both. The amount of e-complexity permitted is higher when i-complexity is low. This line of thinking harks back to the equal complexity conjecture of Hockett, who stated: ‘‘objective measurement is difficult, but impressionistically it would seem that the total grammatical complexity of any language, counting both the morphology and syntax, is about the same as any other’’ (Hockett, 1958, pp. 180–181). Similar trade-offs have been found in other branches of linguistics (see Oh [2015] for a review). For example, there is a trade-off between rate of speech and syllable complexity (Pellegrino et al., 2011): This means that even though Spanish speakers utter many more syllables per second than Chinese, the overall information rate is quite similar as Chinese syllables carry more information (they contain tone information). Hockett’s equal complexity conjecture is controversial: some languages (such as Riau Indonesian) do seem low in complexity across morphology and syntax (Gil, 1"
Q19-1021,Q15-1031,1,0.850215,"nese, the overall information rate is quite similar as Chinese syllables carry more information (they contain tone information). Hockett’s equal complexity conjecture is controversial: some languages (such as Riau Indonesian) do seem low in complexity across morphology and syntax (Gil, 1994). This is why Ackerman and Malouf instead posit that a linguistic system has bounded integrative complexity—it must not be too high, though it can be low, as indeed it is in isolating languages like Chinese and Thai. 3 3.1 Paradigm Entropy Morphology as a Distribution Following Dreyer and Eisner (2009) and Cotterell et al. (2015), we identify a language’s inflectional system with a probability distribution p(M = m) 329 over possible paradigms.4 Our measure of i-complexity will be related to the entropy of this distribution. For instance, knowing the behavior of the English verb system essentially means knowing a joint distribution over 5-tuples of surface forms such as (run, runs, ran, run, running). More precisely, one knows probabilities such as p(M .pres = run, M .3s = runs, M .past = ran, M .pastp = run, M .presp = running). We do not observe p directly, but each observed paradigm (5-tuple) can help us estimate it"
Q19-1021,P16-2090,0,0.0965679,"Missing"
Q19-1021,E17-2120,1,0.94135,"r setting, both the training and the test examples are paradigms from a given inflected lexicon. 4 A Generative Model of the Paradigm To fit q given the training set, we need a tractable family Q of joint distributions over paradigms, with parameters θ . The structure of the model and the number of parameters θ will be determined automatically from the training set: A language with more slots overall or more paradigm shapes will require more parameters. This means that Q is technically a semi-parametric family. 4.1 4.2 A Tree-Structured Distribution Next, conditioned on the shape s, we follow Cotterell et al. (2017b) and generate all the forms of the paradigm using a tree-structured Bayesian network—a directed graphical model in which the form at each slot is generated conditionally on the form at a single parent slot. Figure 1 illustrates two possible tree structures for Spanish verbs. Each paradigm shape s has its own tree structure. If slot σ exists in shape s, we denote its Paradigm Shapes We say that two paradigms m, m0 have the same shape if they define the same slots (that is, domain(m) = domain(m0 )) and the same pairs of slots are syncretic in both paradigms (that is, 331 parent in our shape s"
Q19-1021,D17-1074,1,0.932816,"r setting, both the training and the test examples are paradigms from a given inflected lexicon. 4 A Generative Model of the Paradigm To fit q given the training set, we need a tractable family Q of joint distributions over paradigms, with parameters θ . The structure of the model and the number of parameters θ will be determined automatically from the training set: A language with more slots overall or more paradigm shapes will require more parameters. This means that Q is technically a semi-parametric family. 4.1 4.2 A Tree-Structured Distribution Next, conditioned on the shape s, we follow Cotterell et al. (2017b) and generate all the forms of the paradigm using a tree-structured Bayesian network—a directed graphical model in which the form at each slot is generated conditionally on the form at a single parent slot. Figure 1 illustrates two possible tree structures for Spanish verbs. Each paradigm shape s has its own tree structure. If slot σ exists in shape s, we denote its Paradigm Shapes We say that two paradigms m, m0 have the same shape if they define the same slots (that is, domain(m) = domain(m0 )) and the same pairs of slots are syncretic in both paradigms (that is, 331 parent in our shape s"
Q19-1021,L18-1293,1,0.883669,"Missing"
Q19-1021,D09-1011,1,0.762439,"syllables per second than Chinese, the overall information rate is quite similar as Chinese syllables carry more information (they contain tone information). Hockett’s equal complexity conjecture is controversial: some languages (such as Riau Indonesian) do seem low in complexity across morphology and syntax (Gil, 1994). This is why Ackerman and Malouf instead posit that a linguistic system has bounded integrative complexity—it must not be too high, though it can be low, as indeed it is in isolating languages like Chinese and Thai. 3 3.1 Paradigm Entropy Morphology as a Distribution Following Dreyer and Eisner (2009) and Cotterell et al. (2015), we identify a language’s inflectional system with a probability distribution p(M = m) 329 over possible paradigms.4 Our measure of i-complexity will be related to the entropy of this distribution. For instance, knowing the behavior of the English verb system essentially means knowing a joint distribution over 5-tuples of surface forms such as (run, runs, ran, run, running). More precisely, one knows probabilities such as p(M .pres = run, M .3s = runs, M .past = ran, M .pastp = run, M .presp = running). We do not observe p directly, but each observed paradigm (5-tu"
Q19-1021,P17-4012,0,0.0417736,"Missing"
Q19-1021,D11-1057,1,0.856952,") from p. Any novel verb paradigm in the future would be drawn from p as well. The distribution p represents the inflectional system because it describes what regular paradigms and plausible irregular paradigms tend to look like. The fact that some paradigms are used more frequently than others (more tokens in a corpus) does not mean that they have higher probability under the morphological system p(m). Rather, their higher usage reflects the higher probability of their lexemes. That is due to unrelated factors—the probability of a lexeme may be modeled separately by a stick-breaking process (Dreyer and Eisner, 2011), or may reflect the semantic meaning associated to that lexeme. The role of p(m) in the model is only to serve as the base distribution from which a lexeme type ` selects the tuple of strings m = M (`) that will be used thereafter to express `. We expect the system to place low probability on implausible paradigms: For example, p(run, , , run, running) is close to zero. Moreover, we expect it to assign high conditional probability to the result of applying highly regular processes: For example, for p(M .presp |M .3s) in English, we have p(wugging |wugs) ≈ p(running |runs) ≈ 1, where wug is a"
Q19-1021,D08-1113,1,0.711343,"ges should also help mitigate sample bias caused by variable-sized dictionaries in our database. In many languages, irregular words are also very frequent and may be more likely to be included in a dictionary first. If that’s the case, smaller dictionaries might have lexical statistics skewed toward irregulars more so than larger dictionaries. In general, larger dictionaries should be more representative samples of a language’s broader lexicon. 10 In the computer science literature, it is far more common to construct distributions with support over Σ∗ (Paz, 2003; Bouchard-Cˆot´e et al., 2007; Dreyer et al., 2008; Cotterell et al., 2014), which do not have this problem. 336 tween paradigm slots. We call this the ‘‘green scheme.’’ it is difficult to use them in a lower-resource scenario. To estimate a language’s e-complexity (§2.2.1), we average over all paradigms in the UniMorph inflected lexicon. To estimate i-complexity, we first partition those paradigms into training, development and test sets. We identify the paradigm shapes from the training set (§4.1). We also use the training set to train the parameters θ of our conditional distribution (§4.3), then estimate conditional entropies on the develo"
Q19-1021,P15-2111,1,0.923479,"Missing"
Q19-1021,W16-2002,1,\N,Missing
S17-1011,W13-5500,0,0.0785013,"Missing"
S17-1011,N06-2015,0,0.041987,"j and are trained with negative sampling (Goldberg and Levy, 2014). Frame semantics currently used in NLP have a rich history in linguistic literature. Fillmore (1976)’s frames are based on a word’s context and prototypical concepts that an individual word evokes; they intend to represent the meaning of lexical items by mapping words to real world concepts and shared experiences. Frame-based semantics have inspired many semantic annotation schemata and datasets, such as FrameNet (Baker et al., 1998), PropBank (Palmer et al., 2005), and Verbnet (Schuler, 2005), as well as composite resources (Hovy et al., 2006; Palmer, 2009; Banarescu et al., 2012).1 Thematic Roles and Proto Roles These resources map words to their meanings through discrete/categorically labeled frames and roles; sometimes, as in FrameNet, the roles can be very descriptive (e.g., the D EGREE role for the A F FIRM OR DENY frame), while in other cases, as in PropBank, the roles can be quite general (e.g., A RG 0). Regardless of the actual schema, the roles are based on thematic roles, which map a predicate’s arguments to a semantic representation that makes various semantic distinctions among the arguments (Dowty, 1989).2 Dowty (1991"
S17-1011,E17-2028,1,0.89107,"Missing"
S17-1011,D14-1181,0,0.00257494,"oduction Consider “Bill” in Fig. 1: what is his involvement with the words “would try,” and what does this involvement mean? Word embeddings represent such meaning as points in a real-valued vector space (Deerwester et al., 1990; Mikolov et al., 2013). These representations are often learned by exploiting the frequency that the word cooccurs with contexts, often within a user-defined window (Harris, 1954; Turney and Pantel, 2010). When built from large-scale sources, like Wikipedia or web crawls, embeddings capture general characteristics of words and allow for robust downstream applications (Kim, 2014; Das et al., 2015). Frame semantics generalize word meanings to that of analyzing structured and interconnected labeled “concepts” and abstractions (Minsky, 1974; Fillmore, 1976, 1982). These concepts, or roles, implicitly encode expected properties of that word. In a frame semantic analysis of Fig. 1, the segment “would try” triggers the ATTEMPT frame, filling the expected roles AGENT and G OAL with “Bill” and “the same tactic,” respectively. While frame semantics provide a structured form for analyzing words with crisp, categorically-labeled concepts, the encoded properties and expectations"
S17-1011,P15-1077,0,0.0143566,"nsider “Bill” in Fig. 1: what is his involvement with the words “would try,” and what does this involvement mean? Word embeddings represent such meaning as points in a real-valued vector space (Deerwester et al., 1990; Mikolov et al., 2013). These representations are often learned by exploiting the frequency that the word cooccurs with contexts, often within a user-defined window (Harris, 1954; Turney and Pantel, 2010). When built from large-scale sources, like Wikipedia or web crawls, embeddings capture general characteristics of words and allow for robust downstream applications (Kim, 2014; Das et al., 2015). Frame semantics generalize word meanings to that of analyzing structured and interconnected labeled “concepts” and abstractions (Minsky, 1974; Fillmore, 1976, 1982). These concepts, or roles, implicitly encode expected properties of that word. In a frame semantic analysis of Fig. 1, the segment “would try” triggers the ATTEMPT frame, filling the expected roles AGENT and G OAL with “Bill” and “the same tactic,” respectively. While frame semantics provide a structured form for analyzing words with crisp, categorically-labeled concepts, the encoded properties and expectations are implicit. What"
S17-1011,P14-2050,0,0.208239,"91) argues for proto-thematic roles, e.g. P ROTO -AGENT rather than AGENT, where distinctions in proto-roles are based on clusterings of logical entailments. That is, P ROTO -AGENTs often have certain properties in common, e.g., manipulating other objects or willingly participating in an action; P ROTO -PATIENTs are often changed or affected by some action. By decomposing the meaning of roles into properties or expectations that can be reasoned about, proto-roles can be seen as including a form of vector representation within structured frame semantics. 3 3.1 Skip-Gram as Matrix Factorization Levy and Goldberg (2014b), and subsequently Keerthi et al. (2015), showed how vectors learned under SG with the negative sampling are, under certain conditions, the factorization of (shifted) positive pointwise mutual information. Cotterell et al. (2017) showed that SG is a form of exponential family PCA that factorizes the matrix of word/context cooccurrence counts (rather than shifted positive PMI values). With this interpretation, they generalize SG from matrix to tensor factorization, and provide a theoretical basis for modeling higher-order SG (or additional context, such as morphological features of words) wit"
S17-1011,J05-1004,0,0.0291525,"roduct. Traditionally, the context words i are those words within a small window of j and are trained with negative sampling (Goldberg and Levy, 2014). Frame semantics currently used in NLP have a rich history in linguistic literature. Fillmore (1976)’s frames are based on a word’s context and prototypical concepts that an individual word evokes; they intend to represent the meaning of lexical items by mapping words to real world concepts and shared experiences. Frame-based semantics have inspired many semantic annotation schemata and datasets, such as FrameNet (Baker et al., 1998), PropBank (Palmer et al., 2005), and Verbnet (Schuler, 2005), as well as composite resources (Hovy et al., 2006; Palmer, 2009; Banarescu et al., 2012).1 Thematic Roles and Proto Roles These resources map words to their meanings through discrete/categorically labeled frames and roles; sometimes, as in FrameNet, the roles can be very descriptive (e.g., the D EGREE role for the A F FIRM OR DENY frame), while in other cases, as in PropBank, the roles can be quite general (e.g., A RG 0). Regardless of the actual schema, the roles are based on thematic roles, which map a predicate’s arguments to a semantic representation that mak"
S17-1011,D16-1177,1,0.900121,"Missing"
S17-1011,N15-1058,1,0.896754,"Missing"
S17-1011,W16-5905,1,0.893286,"Missing"
S17-1011,P15-1173,0,0.0341317,"Missing"
S17-1011,D17-3004,1,0.878664,"Missing"
S17-1011,D15-1243,0,0.0282225,"ta with both the standard sliding context window approach (§3) and the frame-based approach (§4). Upper numbers (Roman) are for newswire; lower numbers (italics) are Wikipedia. For both corpora, 800 total FrameNet frame types and 5100 PropBank frame types are extracted. to enable any arbitrary dimensional tensor factorization, as described in §3.2. We learn 100dimensional embeddings for words that appear at least 100 times from 15 negative samples.4 The implementation is available at https://github. com/fmof/tensor-factorization. Metric We evaluate our learned (trigger) embeddings w via QVEC (Tsvetkov et al., 2015). QVEC uses canonical correlation analysis to measure the Pearson correlation between w and a collection of oracle lexical vectors o. These oracle vectors are derived from a human-annotated resource. For QVEC , higher is better: a higher score indicates w more closely correlates (positively) with o. Evaluating Semantic Content with SPR Motivated by Dowty (1991)’s proto-role theory, Reisinger et al. (2015), with a subsequent expansion by White et al. (2016), annotated thousands of predicate-argument pairs (v, a) with (boolean) applicability and (ordinal) likelihoods of wellmotivated semantic pr"
S17-1011,D15-1306,0,0.0607359,"Missing"
S17-1011,Q15-1034,1,\N,Missing
S17-1011,W13-5503,0,\N,Missing
W16-2002,P14-2102,1,0.883884,"Missing"
W16-2002,W16-2007,0,0.0605324,"eral tack as the previous two systems—they used a pipelined approach that first discovered an alignment between the string pairs and then discriminatively trained a transduction. The alignment algorithm employed is the same as that of the baseline system, which relies on a rich-get-richer scheme based on the Chinese restaurant process (Sudoh et al., 2013), as discussed in §5. After obtaining the alignments, they extracted edit operations based on the alignments and used a semi-Markov CRF to apply the edits in a manner very similar to the work of Durrett and DeNero (2013). BIU-MIT The BIU-MIT (Aharoni et al., 2016) team submitted two systems. Their first model, like LMU, built upon the sequence-to-sequence architecture (Sutskever et al., 2014; Bahdanau et al., 2014; Faruqui et al., 2016), but with several im16 System LMU-1 LMU-2 BIU/MIT-1 BIU/MIT-2 HEL MSU CU EHU COL/NYU OSU UA O RACLE.E Task 1 1.0 (95.56) 2.0 (95.56) — — — 3.8 (84.06) 4.6 (81.02) 5.5 (79.24) 6.5 (67.86) — 4.6 (81.83) 97.49 Standard Task 2 1.0 (96.35) 2.0 (96.23) — — — 3.6 (86.06) 5.0 (72.98) — 4.7 (75.59) — 4.7 (74.06) 98.15 Task 3 1.0 (95.83) 2.0 (95.83) — — — 3.8 (84.87) 5.0 (71.75) — 4.8 (67.61) — 4.4 (71.23) 97.97 Task 1 1.0 (95.56"
W16-2002,Q15-1031,1,0.850026,"nd then train a classifier, Faruqui et al. (2016) applied a neural sequence-to-sequence architecture (Sutskever et al., 2014) to the problem. In contrast to paradigm completion, the task of reinflection is harder as it may require both morphologically analyzing the source form and transducing it to the target form. In addition, the training set may include only partial paradigms. However, many of the approaches taken by the shared task participants drew inspiration from work on paradigm completion. Some work, however, has considered full reinflection. For example, Dreyer and Eisner (2009) and Cotterell et al. (2015) apply graphical models with string-valued variables to model the paradigm jointly. In such models it is possible to predict values for cells in the paradigm conditioned on sets of other cells, which are not required to include the lemma. 2 a a 3 t t 4 t 5 o o 6 ssa - In general, each edit has the form copy, insert(string), delete(number), or subst(string), where subst(w) has the same effect as delete(|w|) followed by insert(w). The system treats edit sequence prediction as a sequential decision-making problem, greedily choosing each edit action given the previously chosen actions. This choice"
W16-2002,E14-1060,1,0.267684,"nd suffixes. Prefixes and suffixes are directly associated with morphological features. Stems within paradigms are further processed, using either linguistic intuitions or an empirical approach based on string alignments, to extract the stem letters that undergo changes across inflections. The extracted patterns are intended to capture stem-internal changes, such as vowel changes in Arabic. Reinflection is performed by selecting a set of changes to apply to a stem, and attaching appropriate affixes to the result. Moscow State The Moscow State system (Sorokin, 2016) is derived from the work of Ahlberg et al. (2014) and Ahlberg et al. (2015). The general idea is to use finite-state techniques to compactly model all paradigms in an abstract form called an ‘abstract paradigm’. Roughly speaking, an abstract paradigm is a set of rule transformations that derive all slots from the shared string subsequences present in each slot. Their method relies on the computation of longest common subsequence (Gusfield, 1997) to derive the abstract paradigms, which is similar to its use in the related task of lemmatization (Chrupała et al., 2008; ¨ Helsinki The Helsinki system (Ostling, 2016), like LMU and BIU-MIT, built"
W16-2002,D09-1011,1,0.861261,"tions at the paradigm level and then train a classifier, Faruqui et al. (2016) applied a neural sequence-to-sequence architecture (Sutskever et al., 2014) to the problem. In contrast to paradigm completion, the task of reinflection is harder as it may require both morphologically analyzing the source form and transducing it to the target form. In addition, the training set may include only partial paradigms. However, many of the approaches taken by the shared task participants drew inspiration from work on paradigm completion. Some work, however, has considered full reinflection. For example, Dreyer and Eisner (2009) and Cotterell et al. (2015) apply graphical models with string-valued variables to model the paradigm jointly. In such models it is possible to predict values for cells in the paradigm conditioned on sets of other cells, which are not required to include the lemma. 2 a a 3 t t 4 t 5 o o 6 ssa - In general, each edit has the form copy, insert(string), delete(number), or subst(string), where subst(w) has the same effect as delete(|w|) followed by insert(w). The system treats edit sequence prediction as a sequential decision-making problem, greedily choosing each edit action given the previously"
W16-2002,N15-1107,1,0.936397,"work on computational approaches to inflectional morphology has focused on a special case of reinflection, where the input form is always the lemma (i.e. the citation form). Thus, the task is to generate all inflections in a paradigm from the lemma and often goes by the name of paradigm completion in the literature. There has been a flurry of recent work in this vein: Durrett and DeNero (2013) heuristically extracted transformational rules and learned a statistical model to apply the rules, Nicolai et al. (2015) tackled the problem using standard tools from discriminative string transduction, Ahlberg et al. (2015) used a finite-state construction to extract complete candidate inflections at the paradigm level and then train a classifier, Faruqui et al. (2016) applied a neural sequence-to-sequence architecture (Sutskever et al., 2014) to the problem. In contrast to paradigm completion, the task of reinflection is harder as it may require both morphologically analyzing the source form and transducing it to the target form. In addition, the training set may include only partial paradigms. However, many of the approaches taken by the shared task participants drew inspiration from work on paradigm completio"
W16-2002,D11-1057,1,0.266047,"for the addition of bound morphemes, but also incorporation, which involves combining lexical stems that are often used to form independent words (Mithun, 1984). Such languages combine the need to decompound, generate derivational alternatives, and accurately inflect any resulting words. implemented as finite state transducers (Beesley and Karttunen, 2003), often return all morphologically plausible analyses if there is ambiguity. Learning to mimic the behavior of a hand-written analyzer in this respect could offer a more challenging task, and one that is useful within unsupervised learning (Dreyer and Eisner, 2011) as well as parsing. Existing wide-coverage morphological analyzers could be leveraged in the design of a more interactive shared task, where handcoded models or approximate surface rules could serve as informants for grammatical inference algorithms. The current task design did not explore all potential inflectional complexities in the languages included. For example, cliticization processes were generally not present in the language data. Adding such inflectional elements to the task can potentially make it more realistic in terms of real-world data sparsity in L1 learning scenarios. For exa"
W16-2002,W16-2004,0,0.0554931,"em. Like the Colorado system, they followed Durrett and DeNero (2013) and used a semi-Markov CRF to apply the edit operations. In contrast to Durrett and DeNero (2013), who employed a 0th-order model, the OSU system used a 1st-order model. A major drawback of the system was the cost of inference. The unpruned set of edit operations had over 500 elements. As the cost of inference in the model is quadratic in the size of the state space (the number of edit operations), this created a significant slowdown with over 15 days required to train in some cases. CRF (Sarawagi and Cohen, 2004). EHU EHU (Alegria and Etxeberria, 2016) took an approach based on standard grapheme-tophoneme machinery. They extend the Phonetisaurus (Novak et al., 2012) toolkit, based on the OpenFST WFST library (Allauzen et al., 2007), to the task of morphological reinflection. Their system is organized as a pipeline. Given pairs of input and output strings, the first step involves an unsupervised algorithm to extract an alignment (many-to-one or one-to-many). Then, they train the weights of the WFSTs using the imputed alignments, introducing morphological tags as symbols on the input side of the transduction. Alberta The Alberta system (Nicol"
W16-2002,D08-1113,1,0.900508,"Missing"
W16-2002,C04-1022,0,0.0394056,"inct word forms for any given 1 This latter figure rises to 52 if the entire imperfectiveperfective pair (e.g. govorit’/skazat’ ‘speak, tell’) is considered to be a single lemma. 10 Proceedings of the 14th Annual SIGMORPHON Workshop on Computational Research in Phonetics, Phonology, and Morphology, pages 10–22, c Berlin, Germany, August 11, 2016. 2016 Association for Computational Linguistics forms, the probability of encountering any single word form decreases, reducing the effectiveness of frequency-based techniques in performing tasks like word alignment and language modeling (Koehn, 2010; Duh and Kirchhoff, 2004). Techniques like lemmatization and stemming can ameliorate data sparsity (Goldwater and McClosky, 2005), but these rely on morphological knowledge, particularly the mapping from inflected forms to lemmas and the list of morphs together with their ordering. Developing systems that can accurately learn and capture these mappings, overt affixes, and the principles that govern how those affixes combine is crucial to maximizing the crosslinguistic capabilities of most human language technology. The goal of the 2016 SIGMORPHON Shared Task2 was to spur the development of systems that could accuratel"
W16-2002,N13-1138,0,0.0757217,"06 1.05 Table 3: Descriptive statistics on data released to shared task participants. Figures represent averages across tasks. Abbreviations in the headers: ‘Lem’ = lemmas, ‘Full’ = number of full tags, T2T = average occurrences of tag-to-tag pairs, I-Tag & O-Tag = average occurrences of each input or output tag, resp., and ‘Sync’ = average forms per tag (syncretism). • Standard Release: Arabic, Finnish, Georgian, German, Navajo, Russian, Spanish, and Turkish • Surprise: Hungarian and Maltese Finnish, German, and Spanish have been the subject of much recent work, due to data made available by Durrett and DeNero (2013), while the other datasets used in the shared task are released here for the first time. For all languages, the word forms in the data are orthographic (not phonological) strings in the native script, except in the case of Arabic, where we used the romanized forms available from Wiktionary. An accented letter is treated as a single character. Descriptive statistics of the data are provided in Table 3. The typological character of these languages varies widely. German and Spanish inflection generation has been studied extensively, and the morphological character of the languages is similar: Bot"
W16-2002,P15-1033,0,0.00992926,"the context and they represent individual morphosyntactic attributes as well. In addition, they include template-inspired components to better cope with the templatic morphology of Arabic and Maltese. The second architecture, while also neural, more radically departs from previously proposed sequence-to-sequence models. The aligner from the baseline system is used to create a series of edit actions, similar to the systems in Camp 1. Rather than use a CRF, the BIU-MIT team predicted the sequence of edit actions using a neural model, much in the same way as a transition-based LSTM parser does (Dyer et al., 2015; Kiperwasser and Goldberg, 2016). The architectural consequence of this is that it replaces the soft alignment mechanism of (Bahdanau et al., 2014) with a hard attention mechanism, similar to Rastogi et al. (2016). Camp 3: Time for Some Linguistics The third camp relied on linguistics-inspired heuristics to reduce the problem to multi-way classification. This camp is less unified than the other two, as both teams used very different heuristics. Columbia – New York University Abu Dhabi The system developed jointly by Columbia and NYUAD (Taji et al., 2016) is based on the work of Eskander et al"
W16-2002,D13-1105,0,0.0212324,"r et al., 2015; Kiperwasser and Goldberg, 2016). The architectural consequence of this is that it replaces the soft alignment mechanism of (Bahdanau et al., 2014) with a hard attention mechanism, similar to Rastogi et al. (2016). Camp 3: Time for Some Linguistics The third camp relied on linguistics-inspired heuristics to reduce the problem to multi-way classification. This camp is less unified than the other two, as both teams used very different heuristics. Columbia – New York University Abu Dhabi The system developed jointly by Columbia and NYUAD (Taji et al., 2016) is based on the work of Eskander et al. (2013). It is unique among the submitted systems in that the first step in the pipeline is segmentation of the input words into prefixes, stems, and suffixes. Prefixes and suffixes are directly associated with morphological features. Stems within paradigms are further processed, using either linguistic intuitions or an empirical approach based on string alignments, to extract the stem letters that undergo changes across inflections. The extracted patterns are intended to capture stem-internal changes, such as vowel changes in Arabic. Reinflection is performed by selecting a set of changes to apply t"
W16-2002,N16-1077,0,0.544713,"(i.e. the citation form). Thus, the task is to generate all inflections in a paradigm from the lemma and often goes by the name of paradigm completion in the literature. There has been a flurry of recent work in this vein: Durrett and DeNero (2013) heuristically extracted transformational rules and learned a statistical model to apply the rules, Nicolai et al. (2015) tackled the problem using standard tools from discriminative string transduction, Ahlberg et al. (2015) used a finite-state construction to extract complete candidate inflections at the paradigm level and then train a classifier, Faruqui et al. (2016) applied a neural sequence-to-sequence architecture (Sutskever et al., 2014) to the problem. In contrast to paradigm completion, the task of reinflection is harder as it may require both morphologically analyzing the source form and transducing it to the target form. In addition, the training set may include only partial paradigms. However, many of the approaches taken by the shared task participants drew inspiration from work on paradigm completion. Some work, however, has considered full reinflection. For example, Dreyer and Eisner (2009) and Cotterell et al. (2015) apply graphical models wi"
W16-2002,W14-4012,0,0.0716839,"Missing"
W16-2002,chrupala-etal-2008-learning,0,0.161107,"Missing"
W16-2002,L16-1498,1,0.655386,"Missing"
W16-2002,H05-1085,0,0.0534859,"pair (e.g. govorit’/skazat’ ‘speak, tell’) is considered to be a single lemma. 10 Proceedings of the 14th Annual SIGMORPHON Workshop on Computational Research in Phonetics, Phonology, and Morphology, pages 10–22, c Berlin, Germany, August 11, 2016. 2016 Association for Computational Linguistics forms, the probability of encountering any single word form decreases, reducing the effectiveness of frequency-based techniques in performing tasks like word alignment and language modeling (Koehn, 2010; Duh and Kirchhoff, 2004). Techniques like lemmatization and stemming can ameliorate data sparsity (Goldwater and McClosky, 2005), but these rely on morphological knowledge, particularly the mapping from inflected forms to lemmas and the list of morphs together with their ordering. Developing systems that can accurately learn and capture these mappings, overt affixes, and the principles that govern how those affixes combine is crucial to maximizing the crosslinguistic capabilities of most human language technology. The goal of the 2016 SIGMORPHON Shared Task2 was to spur the development of systems that could accurately generate morphologically inflected words for a set of 10 languages based on a range of training parame"
W16-2002,J10-4005,0,0.0133976,"al of 10 distinct word forms for any given 1 This latter figure rises to 52 if the entire imperfectiveperfective pair (e.g. govorit’/skazat’ ‘speak, tell’) is considered to be a single lemma. 10 Proceedings of the 14th Annual SIGMORPHON Workshop on Computational Research in Phonetics, Phonology, and Morphology, pages 10–22, c Berlin, Germany, August 11, 2016. 2016 Association for Computational Linguistics forms, the probability of encountering any single word form decreases, reducing the effectiveness of frequency-based techniques in performing tasks like word alignment and language modeling (Koehn, 2010; Duh and Kirchhoff, 2004). Techniques like lemmatization and stemming can ameliorate data sparsity (Goldwater and McClosky, 2005), but these rely on morphological knowledge, particularly the mapping from inflected forms to lemmas and the list of morphs together with their ordering. Developing systems that can accurately learn and capture these mappings, overt affixes, and the principles that govern how those affixes combine is crucial to maximizing the crosslinguistic capabilities of most human language technology. The goal of the 2016 SIGMORPHON Shared Task2 was to spur the development of sy"
W16-2002,W16-2006,0,0.0709506,"Missing"
W16-2002,N07-1047,0,0.0429849,"e. Given pairs of input and output strings, the first step involves an unsupervised algorithm to extract an alignment (many-to-one or one-to-many). Then, they train the weights of the WFSTs using the imputed alignments, introducing morphological tags as symbols on the input side of the transduction. Alberta The Alberta system (Nicolai et al., 2016) is derived from the earlier work by Nicolai et al. (2015) and is methodologically quite similar to that of EHU—an unsupervised alignment model is first applied to the training pairs to impute an alignment. In this case, they employ the M2M-aligner (Jiampojamarn et al., 2007). In contrast to EHU, Nicolai et al. (2016) do allow many-to-many alignments. After computing the alignments, they discriminatively learn a stringto-string mapping using the DirectTL+ model (Jiampojamarn et al., 2008). This model is stateof-the-art for the grapheme-to-phoneme task and is very similar to the EHU system in that it assumes a monotonic alignment and could therefore be encoded as a WFST. Despite the similarity to the EHU system, the model performs much better overall. This increase in performance may be attributable to the extensive use of language-specific heuristics, detailed in"
W16-2002,D15-1272,1,0.145283,"Missing"
W16-2002,N15-1093,0,0.142748,"te(3). This results in the output katto, via the following alignment: 1 k k Previous Work Much previous work on computational approaches to inflectional morphology has focused on a special case of reinflection, where the input form is always the lemma (i.e. the citation form). Thus, the task is to generate all inflections in a paradigm from the lemma and often goes by the name of paradigm completion in the literature. There has been a flurry of recent work in this vein: Durrett and DeNero (2013) heuristically extracted transformational rules and learned a statistical model to apply the rules, Nicolai et al. (2015) tackled the problem using standard tools from discriminative string transduction, Ahlberg et al. (2015) used a finite-state construction to extract complete candidate inflections at the paradigm level and then train a classifier, Faruqui et al. (2016) applied a neural sequence-to-sequence architecture (Sutskever et al., 2014) to the problem. In contrast to paradigm completion, the task of reinflection is harder as it may require both morphologically analyzing the source form and transducing it to the target form. In addition, the training set may include only partial paradigms. However, many"
W16-2002,P08-1103,0,0.0216551,"s, introducing morphological tags as symbols on the input side of the transduction. Alberta The Alberta system (Nicolai et al., 2016) is derived from the earlier work by Nicolai et al. (2015) and is methodologically quite similar to that of EHU—an unsupervised alignment model is first applied to the training pairs to impute an alignment. In this case, they employ the M2M-aligner (Jiampojamarn et al., 2007). In contrast to EHU, Nicolai et al. (2016) do allow many-to-many alignments. After computing the alignments, they discriminatively learn a stringto-string mapping using the DirectTL+ model (Jiampojamarn et al., 2008). This model is stateof-the-art for the grapheme-to-phoneme task and is very similar to the EHU system in that it assumes a monotonic alignment and could therefore be encoded as a WFST. Despite the similarity to the EHU system, the model performs much better overall. This increase in performance may be attributable to the extensive use of language-specific heuristics, detailed in the paper, or the application of a discriminative reranker. 6.2 Camp 2: Revenge of the RNN A surprising result of the shared task is the large performance gap between the top performing neural models and the rest of t"
W16-2002,W16-2005,0,0.0679067,"Missing"
W16-2002,W16-2010,0,0.168052,"Missing"
W16-2002,W12-6208,0,0.0181364,"In contrast to Durrett and DeNero (2013), who employed a 0th-order model, the OSU system used a 1st-order model. A major drawback of the system was the cost of inference. The unpruned set of edit operations had over 500 elements. As the cost of inference in the model is quadratic in the size of the state space (the number of edit operations), this created a significant slowdown with over 15 days required to train in some cases. CRF (Sarawagi and Cohen, 2004). EHU EHU (Alegria and Etxeberria, 2016) took an approach based on standard grapheme-tophoneme machinery. They extend the Phonetisaurus (Novak et al., 2012) toolkit, based on the OpenFST WFST library (Allauzen et al., 2007), to the task of morphological reinflection. Their system is organized as a pipeline. Given pairs of input and output strings, the first step involves an unsupervised algorithm to extract an alignment (many-to-one or one-to-many). Then, they train the weights of the WFSTs using the imputed alignments, introducing morphological tags as symbols on the input side of the transduction. Alberta The Alberta system (Nicolai et al., 2016) is derived from the earlier work by Nicolai et al. (2015) and is methodologically quite similar to"
W16-2002,W16-2003,0,0.0522292,"rived from the work of Ahlberg et al. (2014) and Ahlberg et al. (2015). The general idea is to use finite-state techniques to compactly model all paradigms in an abstract form called an ‘abstract paradigm’. Roughly speaking, an abstract paradigm is a set of rule transformations that derive all slots from the shared string subsequences present in each slot. Their method relies on the computation of longest common subsequence (Gusfield, 1997) to derive the abstract paradigms, which is similar to its use in the related task of lemmatization (Chrupała et al., 2008; ¨ Helsinki The Helsinki system (Ostling, 2016), like LMU and BIU-MIT, built off of the sequenceto-sequence architecture, augmenting the system with several innovations. First, a single decoder was used, rather than a unique one for all possible morphological tags, which allows for additional parameter sharing, similar to LMU. More LSTM layers were also added to the decoder, creating a deeper network. Finally, a convolutional layer over the character inputs was used, which was found to significantly increase performance over models without the convolutional layers. 17 trained a neural model to predict edit operations, consistently ranked b"
W16-2002,W16-2008,0,0.0168371,"eline system are given in Table 5. Most participants in the shared task were able to outperform the baseline, often by a significant margin. 6.1 Camp 1: Align and Transduce Most of the systems in this camp drew inspiration from the work of Durrett and DeNero (2013), who extracted a set of edit operations and applied the transformations with a semi-Markov 8 Note that at training time, we know the correct lemma for S thanks to the task 1 data, which is permitted for use by task 2 in the standard track. This is also why task 2 is permitted to use the trained task 1 system. 15 OSU The OSU system (King, 2016) also used a pipelined approach. They first extracted sequences of edit operations using Hirschberg’s algorithm (Hirschberg, 1975). This reduces the string-to-string mapping problem to a sequence tagging problem. Like the Colorado system, they followed Durrett and DeNero (2013) and used a semi-Markov CRF to apply the edit operations. In contrast to Durrett and DeNero (2013), who employed a 0th-order model, the OSU system used a 1st-order model. A major drawback of the system was the cost of inference. The unpruned set of edit operations had over 500 elements. As the cost of inference in the mo"
W16-2002,Q16-1023,0,0.0186037,"ey represent individual morphosyntactic attributes as well. In addition, they include template-inspired components to better cope with the templatic morphology of Arabic and Maltese. The second architecture, while also neural, more radically departs from previously proposed sequence-to-sequence models. The aligner from the baseline system is used to create a series of edit actions, similar to the systems in Camp 1. Rather than use a CRF, the BIU-MIT team predicted the sequence of edit actions using a neural model, much in the same way as a transition-based LSTM parser does (Dyer et al., 2015; Kiperwasser and Goldberg, 2016). The architectural consequence of this is that it replaces the soft alignment mechanism of (Bahdanau et al., 2014) with a hard attention mechanism, similar to Rastogi et al. (2016). Camp 3: Time for Some Linguistics The third camp relied on linguistics-inspired heuristics to reduce the problem to multi-way classification. This camp is less unified than the other two, as both teams used very different heuristics. Columbia – New York University Abu Dhabi The system developed jointly by Columbia and NYUAD (Taji et al., 2016) is based on the work of Eskander et al. (2013). It is unique among the"
W16-2002,N16-1076,1,0.132588,"nd architecture, while also neural, more radically departs from previously proposed sequence-to-sequence models. The aligner from the baseline system is used to create a series of edit actions, similar to the systems in Camp 1. Rather than use a CRF, the BIU-MIT team predicted the sequence of edit actions using a neural model, much in the same way as a transition-based LSTM parser does (Dyer et al., 2015; Kiperwasser and Goldberg, 2016). The architectural consequence of this is that it replaces the soft alignment mechanism of (Bahdanau et al., 2014) with a hard attention mechanism, similar to Rastogi et al. (2016). Camp 3: Time for Some Linguistics The third camp relied on linguistics-inspired heuristics to reduce the problem to multi-way classification. This camp is less unified than the other two, as both teams used very different heuristics. Columbia – New York University Abu Dhabi The system developed jointly by Columbia and NYUAD (Taji et al., 2016) is based on the work of Eskander et al. (2013). It is unique among the submitted systems in that the first step in the pipeline is segmentation of the input words into prefixes, stems, and suffixes. Prefixes and suffixes are directly associated with mo"
W16-2002,W16-2009,0,0.0638163,"of the input words into prefixes, stems, and suffixes. Prefixes and suffixes are directly associated with morphological features. Stems within paradigms are further processed, using either linguistic intuitions or an empirical approach based on string alignments, to extract the stem letters that undergo changes across inflections. The extracted patterns are intended to capture stem-internal changes, such as vowel changes in Arabic. Reinflection is performed by selecting a set of changes to apply to a stem, and attaching appropriate affixes to the result. Moscow State The Moscow State system (Sorokin, 2016) is derived from the work of Ahlberg et al. (2014) and Ahlberg et al. (2015). The general idea is to use finite-state techniques to compactly model all paradigms in an abstract form called an ‘abstract paradigm’. Roughly speaking, an abstract paradigm is a set of rule transformations that derive all slots from the shared string subsequences present in each slot. Their method relies on the computation of longest common subsequence (Gusfield, 1997) to derive the abstract paradigms, which is similar to its use in the related task of lemmatization (Chrupała et al., 2008; ¨ Helsinki The Helsinki sy"
W16-2002,D13-1021,0,0.118112,"Missing"
W16-2002,P15-2111,1,0.178102,"Missing"
W18-6011,P17-1080,0,0.0204505,"ph represents both categories as a single templatic value. 2. We discard any values that UniMorph doesn’t annotate for a particular part of speech, like gender and number in French verb participles, or German noun genders. Our approach to converting UD MSDs to UniMorph MSDs begins with the attribute-value lookup, then amends it on a language-specific basis. Alterations informed by the MSD and the word form, like insertion, substitution, and deletion, increase the number of agreeing annotations. They are critical for work that examines the MSD monolithically instead of feature-by-feature (e.g. Belinkov et al., 2017; Cotterell and Heigold, 2017): Without exact matches, converting the individual tags becomes hollow. 3. We make MSD additions when they are unambiguously implied by the resources, like PFV to accompany PST in Spanish “pasado simple”, but PST to accompany IPFV in Spanish “pasado continuo”. 4. We also incorporate fixes using information outside of the MSD like the L G S PEC 1 tag for Spanish’s “-ra” forms, as described in §4, and other language-specific corrections, like mapping the various dative cases to the crosslingually comparable case annotations used in UniMorph. Beginning our process, w"
W18-6011,J93-2003,0,0.0985893,"ords to be a match if their form and lemma are present in both resources. Syncretism allows a single surface form to realize multiple MSDs (Spanish “mandaba” can be first- or third-person), so we define success as the computed MSD matching any of the word’s UniMorph MSDs. This gives rise to an equation for recall: of the word–lemma pairs found in both resources, how many of their UniMorph-converted MSDs are present in the UniMorph tables? Why not a learned mapping? One can imagine learning the UniMorph MSD corresponding to a UD dataset’s MSD by a set-to-set translation model like IBM Model 1 (Brown et al., 1993). Unfortunately, statistical (and especially neural) machine translation generalizes in unreliable ways. Our goal is a straightforward, easily manipulable and extensible conversion that prioritizes correctness over coverage. 6 Intrinsic evaluation Experiments Why no held-out test set? Our problem here is not a learning problem, so the question is ill-posed. There is no training set, and the two resources for a given language make up a test set. The quality of our model—the conversion tool—comes from how well we encode prior knowledge about the relationship between the UD and UniMorph corpora."
W18-6011,D17-1078,1,0.850043,"gories as a single templatic value. 2. We discard any values that UniMorph doesn’t annotate for a particular part of speech, like gender and number in French verb participles, or German noun genders. Our approach to converting UD MSDs to UniMorph MSDs begins with the attribute-value lookup, then amends it on a language-specific basis. Alterations informed by the MSD and the word form, like insertion, substitution, and deletion, increase the number of agreeing annotations. They are critical for work that examines the MSD monolithically instead of feature-by-feature (e.g. Belinkov et al., 2017; Cotterell and Heigold, 2017): Without exact matches, converting the individual tags becomes hollow. 3. We make MSD additions when they are unambiguously implied by the resources, like PFV to accompany PST in Spanish “pasado simple”, but PST to accompany IPFV in Spanish “pasado continuo”. 4. We also incorporate fixes using information outside of the MSD like the L G S PEC 1 tag for Spanish’s “-ra” forms, as described in §4, and other language-specific corrections, like mapping the various dative cases to the crosslingually comparable case annotations used in UniMorph. Beginning our process, we relied on documentation of t"
W18-6011,K18-3001,1,0.882683,"Missing"
W18-6011,K17-2001,1,0.892721,"Missing"
W18-6011,W17-0405,0,0.0486603,"Missing"
W18-6011,W05-0807,1,0.760167,"Missing"
W18-6011,L18-1293,1,0.652734,"Missing"
W18-6011,E17-2018,1,0.849189,"Morph’s schema does not indicate the type of pronoun (demonstrative, interrogative, etc.), and when lexical information is not recorded in UniMorph, we delete it from the MSD during transformation. On the other hand, UniMorph’s atomic tags have more parts to guess, but they are often related. (E.g. I PFV always entails P ST in Spanish.) Altogether, these forces seem to have little impact on tagging performance. 8 In addition to using the number of extra rules as a proxy for harmony between resources, one could perform cross-lingual projection of morphological tags (Dr´abek and Yarowsky, 2005; Kirov et al., 2017). Our approach succeeds even without parallel corpora. 9 Conclusion and Future Work We created a tool for annotating Universal Dependencies CoNLL-U files with UniMorph annotations. Our tool is ready to use off-the-shelf today, requires no training, and is deterministic. While under-specification necessitates a lossy and imperfect conversion, ours is interpretable. Patterns of mistakes can be identified and ameliorated. The tool allows a bridge between resources annotated in the Universal Dependencies and Universal Morphology (UniMorph) schemata. As the Universal Dependencies project provides a"
W18-6011,L16-1498,1,0.848261,"directly comparable across languages. Its features are informed by a distinction between universal categories, which are widespread and psychologically “real” to speakers; and comparative concepts, only used by linguistic typologists to compare languages (Haspelmath, 2010). Additionally, it strives for identity of meaning across languages, not simply similarity of terminology. As a prime example, it does not regularly label a dative case for nouns, for reasons explained in depth by Haspelmath (2010).4 The UniMorph resources for a language contain complete paradigms extracted from Wiktionary (Kirov et al., 2016, 2018). Word types are annotated to form a database, mapping a lemma–tag pair to a surface form. The schema is explained in detail in Sylak-Glassman (2016). It has been used in the SIGMORPHON shared task (Cotterell et al., 2016) and the CoNLL–SIGMORPHON shared tasks (Cotterell et al., 2017, 2018). Several components of the UniMorph schema have been adopted by UD.5 Universal Dependencies The Universal Dependencies morphological schema comprises part of speech and 23 additional attributes (also called features in UD) annotating meaning or syntax, as well as language-specific attributes. In orde"
W18-6011,P18-1247,0,0.200895,"tated datasets. UD’s v2.1 release (Nivre et al., 2017) has 102 treebanks in 60 languages. The large resource, constructed by independent parties, evinces problems in the goal of a universal inventory of annotations. Annotators may choose to omit certain values (like the coerced gender of refrescante in Figure 1), and they may disagree on how a linguistic concept is encoded. (See, e.g., Haspelmath’s (2010) description of the dative case.) Additionally, many of the treebanks “were created by fully- or semi-automatic conversion from treebanks with less comprehensive annotation schemata than UD” (Malaviya et al., 2018). For instance, the Spanish word “vas” “you go” is incorrectly labeled G ENDER : F EM|N UMBER : P L because it ends in a character sequence which is common among feminine plural nouns. (Nevertheless, the part of speech field for “vas” is correct.) UniMorph’s development is more centralized and pipelined.7 Inflectional paradigms are scraped from Wiktionary, annotators map positions in the scraped data to MSDs, and the mapping is automatically applied to all of the scraped paradigms. Because annotators handle languages they are familiar with (or related ones), realization of the schema is also d"
W18-6011,J93-2004,0,0.0604809,"a given lemma and part of speech gives a paradigm: a mapping from slots to surface forms. Regular English verbs have five slots in their paradigm (Long, 1957), which we illustrate for the verb prove, using simple labels for the forms in Table 1. A morphosyntactic schema prescribes how language can be annotated—giving stricter categories than our simple labels for prove—and can vary in the level of detail provided. Part of speech tags are an example of a very coarse schema, ignoring details of person, gender, and number. A slightly finer-grained schema for English is the Penn Treebank tagset (Marcus et al., 1993), which includes signals for English morphology. For instance, its VBZ tag pertains to the specially inflected 3rd-person singular, present-tense verb form (e.g. “proves” in Table 1). If the tag in a schema is detailed enough that it exactly specifies a slot in a paradigm, it is • We detail a deterministic mapping from UD morphological annotations to UniMorph. Language-specific edits of the tags in 31 languages increase harmony between converted UD and existing UniMorph data (§5). • We provide an implementation of this mapping and post-editing, which replaces the UD features in a CoNLL-U file"
W18-6011,W17-0419,0,0.0599591,"Missing"
W18-6011,petrov-etal-2012-universal,0,0.0495453,"nguage. Our approach, by contrast, is a direct flight from the source to the target.) Because UniMorph corpora are noisy, the encoding from the interlingua would have to be rewritten for each target. Further, decoding the UD MSD into the interlingua cannot leverage external information like the lemma and form. The creators of HamleDT sought to harmonize dependency annotations among treebanks, similar to our goal of harmonizing across resources (Zeman et al., 2014). The treebanks they sought to harmonize used multiple diverse annotation schemes, which the authors unified under a single scheme. Petrov et al. (2012) present mappings into a coarse, “universal” part of speech for 22 languages. Working with POS tags rather than morphological tags (which have far more dimensions), their space of options to harmonize is much smaller than ours. Our extrinsic evaluation is most in line with the paradigm of Wisniewski and Lacroix (2017) (and similar work therein), who compare syntactic parser performance on UD treebanks annotated with two styles of dependency representation. Our problem differs, though, in that the dependency representations express different relationships, while our two schemata vastly overlap."
W18-6011,W17-0412,0,0.0275171,"ject releases type-level annotated tables, the newfound compatibility opens up new experiments. A prime example of exploiting tokenand type-level data is T¨ackstr¨om et al. (2013). That work presents a part-of-speech (POS) dictionary built from Wiktionary, where the POS tagger is also constrained to options available in their typelevel POS dictionary, improving performance. Our transformation means that datasets are prepared for similar experiments with morphological tagging. It would also be reasonable to incorporate this tool as a subroutine to UDPipe (Straka and Strakov´a, 2017) and Udapi (Popel et al., 2017). We leave open the task of converting in the opposite direction, turning UniMorph MSDs into Universal Dependencies MSDs. Because our conversion rules are interpretable, we identify shortcomings in both resources, using each as validation for the other. We were able to find specific instances of incorrectly applied UniMorph annotation, as well as specific instances of cross-lingual inconsistency in both resources. These findings will harden both resources and better align them with their goal of universal, crosslingual annotation. Related Work The goal of a tagset-to-tagset mapping of morpholo"
W18-6011,K17-3009,0,0.0457031,"Missing"
W18-6011,P15-2111,1,0.851167,"Missing"
W18-6011,K17-3001,0,0.0560233,"alled a morphosyntactic description (MSD).2 These descriptions require varying amounts of detail: While the English verbal paradigm is small enough to fit on a page, the verbal paradigm of the Northeast Caucasian language Archi can have over 1,500,000 slots (Kibrik, 1998). 3 is an annotated treebank, making it a resource of token-level annotations. The schema is guided by these treebanks, with feature names chosen for relevance to native speakers. (In §3.2, we will contrast this with UniMorph’s treatment of morphosyntactic categories.) The UD datasets have been used in the CoNLL shared tasks (Zeman et al., 2017, 2018 to appear). Two Schemata, Two Philosophies Unlike the Penn Treebank tags, the UD and UniMorph schemata are cross-lingual and include a fuller lexicon of attribute-value pairs, such as P ER SON : 1. Each was built according to a different set of principles. UD’s schema is constructed bottomup, adapting to include new features when they’re identified in languages. UniMorph, conversely, is top-down: A cross-lingual survey of the literature of morphological phenomena guided its design. UniMorph aims to be linguistically complete, containing all known morphosyntactic attributes. Both schemat"
W19-4226,P17-2107,0,0.0357656,"Missing"
W19-4226,N18-1126,0,0.0330462,"gs (BERT) provided by Google (Devlin et al., 2019). CBNU1 used a mix of pre-trained embeddings from the CoNLL 2017 shared task and fastText. Further, some teams trained their own embeddings to aid performance. CUNI–Malta performs lemmatization as operations over edit actions with LSTM and ReLU. Tagging is a bidirectional LSTM augmented by the edit actions (i.e., two-stage decoding), predicting features separately. The Edinburgh system is a character-based LSTM encoder-decoder with attention, implemented in OpenNMT. It can be seen as an extension of the contextual lemmatization system Lematus (Bergmanis and Goldwater, 2018) to include morphological tagging, or alternatively as an adaptation of the morphological re-inflection system MED (Kann and Sch¨utze, 2016) to incorporate context and perform analysis rather than re-inflection. Like these systems it uses a completely generic encoderdecoder architecture with no specific adaptation to the morphological processing task other than the form of the input. In the submitted version of the system, the input is split into short chunks corresponding to the target word plus one word of context on either side, and the system is trained to output the corresponding lemmas a"
W19-4226,K18-3001,1,0.746475,"previous shared task (Cotterell et al., 2018), training a neural network on unambiguous forms to estimate the distribution over all, even ambiguous, forms. We then sampled 12,000 triples without replacement from this distribution. The first 100 were taken as training data for low-resource settings. The first 10,000 were used as high-resource training sets. As these sets are nested, the highest-count triples tend to appear in the smaller training sets.3 Data conversion The morphological annotations for the 2019 shared task were converted to the UniMorph schema (Kirov et al., 2018) according to McCarthy et al. (2018), who provide a deterministic mapping that increases agreement across languages. This also moves the part of speech into the bundle of morphological features. We do not attempt to individually correct any errors in the UD source material. Further, some languages received additional pre-processing. In the Finnish data, we removed morpheme boundaries that were present in the lemmata (e.g., puhe#kieli 7→ puhekieli ‘spoken+language’). Russian lemmata in the GSD treebank were presented in all uppercase; to match Swahili. Likewise, the low-resource language Telugu had fewer than 100 forms. 4 When su"
W19-4226,K17-2001,1,0.692718,"Missing"
W19-4226,N19-1155,1,0.843499,"ive teams participated in the first Task, with a variety of methods aimed at leveraging the crosslingual data to improve system performance. The University of Alberta (UAlberta) performed a focused investigation on four language pairs, training cognate-projection systems from external cognate lists. Two methods were considered: one which trained a high-resource neural encoderdecoder, and projected the test data into the HRL, and one that projected the HRL data into the LRL, and trained a combined system. Results demonstrated that certain language pairs may be amenable to such methods. Neural (Malaviya et al., 2019): This is a stateof-the-art neural model that also performs joint morphological tagging and lemmatization, but also accounts for the exposure bias with the application of maximum likelihood (MLE). The model stitches the tagger and lemmatizer together with the use of jackknifing (Agi´c and Schluter, 2017) to expose the lemmatizer to the errors made by the tagger model during training. The morphological tagger is based on a character-level biLSTM embedder that produces the embedding for a word, 4 HRL–LRL adyghe–kabardian albanian–breton arabic–classical-syriac arabic–maltese arabic–turkmen armen"
W19-4226,N19-1423,0,0.0153778,"only applied to a subset of languages, making scores incomparable. † indicates that additional external resources were used for training, and ‡ indicates that training data were shared across languages or treebanks. ging). Although they predict complete tags, they use the individual features to regularize the decoder. Small gains are also obtained from joining multilingual corpora and ensembling. improve their lemmatization and feature analysis. Several teams made use of pre-trained embeddings. CHARLES-SAARLAND-2 and UFALPRAGUE1 used pretrained contextual embeddings (BERT) provided by Google (Devlin et al., 2019). CBNU1 used a mix of pre-trained embeddings from the CoNLL 2017 shared task and fastText. Further, some teams trained their own embeddings to aid performance. CUNI–Malta performs lemmatization as operations over edit actions with LSTM and ReLU. Tagging is a bidirectional LSTM augmented by the edit actions (i.e., two-stage decoding), predicting features separately. The Edinburgh system is a character-based LSTM encoder-decoder with attention, implemented in OpenNMT. It can be seen as an extension of the contextual lemmatization system Lematus (Bergmanis and Goldwater, 2018) to include morpholo"
W19-4226,Q18-1032,0,0.0456659,"Missing"
W19-4226,W17-4110,0,0.124781,"Missing"
W19-4226,D15-1272,1,0.898864,"Missing"
W19-4226,W16-2010,0,0.0549216,"Missing"
W19-4226,D18-1103,0,0.0261867,"he system, the input is split into short chunks corresponding to the target word plus one word of context on either side, and the system is trained to output the corresponding lemmas and tags for each three-word chunk. 6 Future Directions In general, the application of typology to natural language processing (e.g., Gerz et al., 2018; Ponti et al., 2018) provides an interesting avenue for multilinguality. Further, our shared task was designed to only leverage a single helper language, though many may exist with lexical or morphological overlap with the target language. Techniques like those of Neubig and Hu (2018) may aid in designing universal inflection architectures. Neither task this year included unannotated monolingual corpora. Using such data is well-motivated from an L1-learning point of view, and may affect the performance of low-resource data settings. In the case of inflection an interesting future topic could involve departing from orthographic representation and using more IPA-like representations, i.e. transductions over pronunciations. DifferSeveral teams relied on external resources to 8 Table 6: Task 2 Lemma Accuracy scores 9 UD UD UD UD UD UD UD UD UD UD UD UD UD UD UD UD UD UD UD UD"
W19-4226,L18-1293,1,0.829575,"previous shared task (Cotterell et al., 2018), training a neural network on unambiguous forms to estimate the distribution over all, even ambiguous, forms. We then sampled 12,000 triples without replacement from this distribution. The first 100 were taken as training data for low-resource settings. The first 10,000 were used as high-resource training sets. As these sets are nested, the highest-count triples tend to appear in the smaller training sets.3 Data conversion The morphological annotations for the 2019 shared task were converted to the UniMorph schema (Kirov et al., 2018) according to McCarthy et al. (2018), who provide a deterministic mapping that increases agreement across languages. This also moves the part of speech into the bundle of morphological features. We do not attempt to individually correct any errors in the UD source material. Further, some languages received additional pre-processing. In the Finnish data, we removed morpheme boundaries that were present in the lemmata (e.g., puhe#kieli 7→ puhekieli ‘spoken+language’). Russian lemmata in the GSD treebank were presented in all uppercase; to match Swahili. Likewise, the low-resource language Telugu had fewer than 100 forms. 4 When su"
W19-4226,D15-1166,0,0.0843899,"ani Kurdish were created as part of the Alexina project (Walther et al., 2010; Walther and Sagot, 2010). 2 These datasets can be obtained from https:// sigmorphon.github.io/sharedtasks/2019/ 3 Several high-resource languages had necessarily fewer, but on a similar order of magnitude. Bengali, Uzbek, Kannada, 3 the 2018 shared task, we lowercased these. In development and test data, all fields except for form and index within the sentence were struck. 4 4.1 Team Baselines Task 1 Baseline We include four neural sequence-to-sequence models mapping lemma into inflected word forms: soft attention (Luong et al., 2015), non-monotonic hard attention (Wu et al., 2018), monotonic hard attention and a variant with offset-based transition distribution (Wu and Cotterell, 2019). Neural sequenceto-sequence models with soft attention (Luong et al., 2015) have dominated previous SIGMORPHON shared tasks (Cotterell et al., 2017). Wu et al. (2018) instead models the alignment between characters in the lemma and the inflected word form explicitly with hard attention and learns this alignment and transduction jointly. Wu and Cotterell (2019) shows that enforcing strict monotonicity with hard attention is beneficial in tas"
W19-4226,P18-1247,1,0.817396,"ciently prescribed by the lemma, as with the Romanian verbal inflection classes or nominal gender in German. As we move toward multilingual models for morphology, it becomes important to understand which representations are critical or irrelevant for adapting to new languages; this may be probed in the style of (Thompson et al., 2018), and it can be used as a first step toward designing systems that avoid “catastrophic forgetting” as they learn to inflect new languages (Thompson et al., 2019). Future directions for Task 2 include exploring cross-lingual analysis—in stride with both Task 1 and Malaviya et al. (2018)—and leveraging these analyses in downstream tasks. 7 In the second task, several methods were implemented by multiple groups, with the most successful systems implementing variations of multiheaded attention, multi-level encoding, multiple decoders, and ELMo and BERT contextual embeddings. We have released the training, development, and test sets, and expect these datasets to provide a useful benchmark for future research into learning of inflectional morphology and string-to-string transduction. Acknowledgments MS has received funding from the European Research Council (ERC) under the Europe"
W19-4226,P15-2111,1,0.881319,"Missing"
W19-4226,N19-1209,0,0.0249795,"tasks. One pertinent facet of this is information about inflectional categories—often the inflectional information is insufficiently prescribed by the lemma, as with the Romanian verbal inflection classes or nominal gender in German. As we move toward multilingual models for morphology, it becomes important to understand which representations are critical or irrelevant for adapting to new languages; this may be probed in the style of (Thompson et al., 2018), and it can be used as a first step toward designing systems that avoid “catastrophic forgetting” as they learn to inflect new languages (Thompson et al., 2019). Future directions for Task 2 include exploring cross-lingual analysis—in stride with both Task 1 and Malaviya et al. (2018)—and leveraging these analyses in downstream tasks. 7 In the second task, several methods were implemented by multiple groups, with the most successful systems implementing variations of multiheaded attention, multi-level encoding, multiple decoders, and ELMo and BERT contextual embeddings. We have released the training, development, and test sets, and expect these datasets to provide a useful benchmark for future research into learning of inflectional morphology and str"
W19-4226,W18-6313,1,0.850382,"sentangle.8 Creating new data sets that accurately reflect learner exposure (whether L1 or L2) is also an important consideration in the design of future shared tasks. One pertinent facet of this is information about inflectional categories—often the inflectional information is insufficiently prescribed by the lemma, as with the Romanian verbal inflection classes or nominal gender in German. As we move toward multilingual models for morphology, it becomes important to understand which representations are critical or irrelevant for adapting to new languages; this may be probed in the style of (Thompson et al., 2018), and it can be used as a first step toward designing systems that avoid “catastrophic forgetting” as they learn to inflect new languages (Thompson et al., 2019). Future directions for Task 2 include exploring cross-lingual analysis—in stride with both Task 1 and Malaviya et al. (2018)—and leveraging these analyses in downstream tasks. 7 In the second task, several methods were implemented by multiple groups, with the most successful systems implementing variations of multiheaded attention, multi-level encoding, multiple decoders, and ELMo and BERT contextual embeddings. We have released the t"
W19-4226,W18-5818,1,0.888961,"Missing"
W19-4226,P19-1148,1,0.888951,"sigmorphon.github.io/sharedtasks/2019/ 3 Several high-resource languages had necessarily fewer, but on a similar order of magnitude. Bengali, Uzbek, Kannada, 3 the 2018 shared task, we lowercased these. In development and test data, all fields except for form and index within the sentence were struck. 4 4.1 Team Baselines Task 1 Baseline We include four neural sequence-to-sequence models mapping lemma into inflected word forms: soft attention (Luong et al., 2015), non-monotonic hard attention (Wu et al., 2018), monotonic hard attention and a variant with offset-based transition distribution (Wu and Cotterell, 2019). Neural sequenceto-sequence models with soft attention (Luong et al., 2015) have dominated previous SIGMORPHON shared tasks (Cotterell et al., 2017). Wu et al. (2018) instead models the alignment between characters in the lemma and the inflected word form explicitly with hard attention and learns this alignment and transduction jointly. Wu and Cotterell (2019) shows that enforcing strict monotonicity with hard attention is beneficial in tasks such as morphological inflection where the transduction is mostly monotonic. The encoder is a biLSTM while the decoder is a left-to-right LSTM. All mode"
W19-4226,D18-1473,1,0.906631,"Missing"
W19-4226,D16-1163,0,0.0280678,"larger number of examples in either a related or unrelated language. Each test example asked participants to produce some other inflected form when given a lemma and a bundle of morphosyntactic features as input. The goal, thus, is to perform morphological inflection in the low-resource language, having hopefully exploited some similarity to the high-resource language. Models which perform well here can aid downstream tasks like machine translation in lowresource settings. All datasets were resampled from UniMorph, which makes them distinct from past years. The mode of the task is inspired by Zoph et al. (2016), who fine-tune a model pre-trained on a high-resource language to perform well on a lowresource language. We do not, though, require that models be trained by fine-tuning. Joint modeling or any number of methods may be explored instead. Task 2: Morphological analysis in context Although inflection of words in a context-agnostic manner is a useful evaluation of the morphological quality of a system, people do not learn morphology in isolation. In 2018, the second task of the CoNLL– SIGMORPHON Shared Task (Cotterell et al., 2018) required submitting systems to complete an inflectional cloze tas"
W19-4226,W18-6011,1,\N,Missing
