P19-1510,{NNE}: A Dataset for Nested Named Entity Recognition in {E}nglish Newswire,2019,25,0,6,1,25855,nicky ringland,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,1,"Named entity recognition (NER) is widely used in natural language processing applications and downstream tasks. However, most NER tools target flat annotation from popular datasets, eschewing the semantic information available in nested entity mentions. We describe NNE{---}a fine-grained, nested named entity dataset over the full Wall Street Journal portion of the Penn Treebank (PTB). Our annotation comprises 279,795 mentions of 114 entity types with up to 6 layers of nesting. We hope the public release of this large dataset for English newswire will encourage development of new techniques for nested NER."
P15-1111,Identifying Cascading Errors using Constraints in Dependency Parsing,2015,23,4,2,1,37524,dominick ng,Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),1,"Dependency parsers are usually evaluated on attachment accuracy. Whilst easily interpreted, the metric does not illustrate the cascading impact of errors, where the parser chooses an incorrect arc, and is subsequently forced to choose further incorrect arcs elsewhere in the parse. We apply arc-level constraints to MSTparser and ZPar, enforcing the correct analysis of specific error classes, whilst otherwise continuing with decoding. We investigate the direct and indirect impact of applying constraints to the parser. Erroneous NP and punctuation attachments cause the most cascading errors, while incorrect PP and coordination attachments are frequent but less influential. Punctuation is especially challenging, as it has long been ignored in parsing, and serves a variety of disparate syntactic roles."
W14-5207,Command-line utilities for managing and exploring annotated corpora,2014,7,1,3,1,28420,joel nothman,Proceedings of the Workshop on Open Infrastructures and Analysis Frameworks for {HLT},0,"Users of annotated corpora frequently perform basic operations such as inspecting the available annotations, filtering documents, formatting data, and aggregating basic statistics over a corpus. While these may be easily performed over flat text files with stream-processing UNIX tools, similar tools for structured annotation require custom design. Dawborn and Curran (2014) have developed a declarative description and storage for structured annotation, on top of which we have built generic command-line utilities. We describe the most useful utilities xe2x80x90 some for quick data exploration, others for high-level corpus management xe2x80x90 with reference to comparable UNIX utilities. We suggest that such tools are universally valuable for working with structured corpora; in turn, their utility promotes common storage and distribution formats for annotated text."
U14-1005,Trading accuracy for faster named entity linking,2014,10,1,3,0,38920,kristy hughes,Proceedings of the Australasian Language Technology Association Workshop 2014,0,None
U14-1006,Unsupervised Biographical Event Extraction Using {W}ikipedia Traffic,2014,18,0,3,0,38921,alexander hogue,Proceedings of the Australasian Language Technology Association Workshop 2014,0,"xe2x80x9cWhat is Julian Assange known for?xe2x80x9d Can we define his importance in terms of key events? xe2x80xa2 Some people are famous for what they did, rather than what they are xe2x80xa2 Julian Assange founded WikiLeaks in 2006 and In 2012 he was granted political asylum by Ecuador are sentences which quickly convey Assangexe2x80x99s notability xe2x80xa2 Current way of extracting these eventsxe2x86x92 need to create a huge data set to train on! xe2x86x92 $$$ xe2x80xa2Many ways someone can be famous xe2x86x92 lots of data xe2x80xa2 Spikes in Wikipedia page traffic xe2x86x92 something interesting happened xe2x80xa2 Find spikesxe2x86x92 find edit to page at the same time xe2x86x92 match sentence in current-day article to edit xe2x80xa2 No annotation needed! xe2x80x9cObama was re-elected president in November 2012..xe2x80x9d xe2x80x9cOn November 4, Obama won the presidency..xe2x80x9d"
D14-1089,Analysing recall loss in named entity slot filling,2014,29,7,3,0,29015,glen pink,Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP}),0,"State-of-the-art fact extraction is heavily constrained by recall, as demonstrated by recent performance in TAC Slot Filling. We isolate this recall loss for NE slots by systematically analysing each stage of the slot filling pipeline as a filter over correct answers. Recall is critical as candidates never generated can never be recovered, whereas precision can always be increased in downstream processing. We provide precise, empirical confirmation of previously hypothesised sources of recall loss in slot filling. While NE type constraints substantially reduce the search space with only a minor recall penalty, we find that 10% to 39% of slot fills will be entirely ignored by most systems. One in six correct answers are lost if coreference is not used, but this can be mostly retained by simple name matching rules."
C14-1072,docrep: A lightweight and efficient document representation framework,2014,20,4,2,1,38280,tim dawborn,"Proceedings of {COLING} 2014, the 25th International Conference on Computational Linguistics: Technical Papers",0,"Modelling linguistic phenomena requires highly structured and complex data representations. Document representation frameworks (DRFs) provide an interface to store and retrieve multiple annotation layers over a document. Researchers face a difficult choice: using a heavy-weight DRF or implement a custom DRF. The cost is substantial, either learning a new complex system, or continually adding features to a home-grown system that risks overrunning its original scope. We introduce DOCREP, a lightweight and efficient DRF, and compare it against existing DRFs. We discuss our design goals and implementations in C, Python, and Java. We transform the OntoNotes 5 corpus using DOCREP and UIMA, providing a quantitative comparison, as well as discussing modelling trade-offs. We conclude with qualitative feedback from researchers who have used DOCREP for their own projects. Ultimately, we hope DOCREP is useful for the busy researcher who wants the benefits of a DRF, but has better things to do than to write one."
C14-1201,Limited memory incremental coreference resolution,2014,25,4,2,1,9606,kellie webster,"Proceedings of {COLING} 2014, the 25th International Conference on Computational Linguistics: Technical Papers",0,"We propose an algorithm for coreference resolution based on analogy with shift-reduce parsing. By reconceptualising the task in this way, we unite ranking- and cluster-based approaches to coreference resolution, which have until now been largely orthogonal. Additionally, our framework naturally lends itself to rich discourse modelling, which we use to define a series of psycholinguistically motivated features. We achieve CoNLL scores of 63.33 and 62.91 on the CoNLL-2012 DEV and TEST splits of the OntoNotes 5 corpus, beating the publicly available state of the art systems. These results are also competitive with the best reported research systems despite our system having low memory requirements and a simpler model."
U13-1007,Examining the Impact of Coreference Resolution on Quote Attribution,2013,21,0,3,0,41176,tim okeefe,Proceedings of the Australasian Language Technology Association Workshop 2013 ({ALTA} 2013),0,"Quote attribution is the task of identifying the speaker of each quote within a document. While recent research has established large-scale corpora for this task, these corpora are not yet consistent in the way they handle candidate speakers, and many of the reported results rely on gold standard annotations of both entities and coreference chains. In this work we evaluate three quote attribution systems with automatically produced candidate speakers and coreference chains. We perform these experiments over four separate corpora, which allows us to determine how coreference resolution effects quote attribution, and to use the task as an extrinsic evaluation of three coreference systems."
P13-2018,An Empirical Examination of Challenges in {C}hinese Parsing,2013,19,11,3,1,2960,jonathan kummerfeld,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"Aspects of Chinese syntax result in a distinctive mix of parsing challenges. However, the contribution of individual sources of error to overall difficulty is not well understood. We conduct a comprehensive automatic analysis of error types made by Chinese parsers, covering a broad range of error types for large sets of sentences, enabling the first empirical ranking of Chinese error types by their performance impact. We also investigate which error types are resolved by using gold part-of-speech tags, showing that improving Chinese tagging only addresses certain error types, leaving substantial outstanding challenges."
P13-2092,An annotated corpus of quoted opinions in news articles,2013,17,8,2,0,41176,tim okeefe,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"Quotes are used in news articles as evidence of a personxe2x80x99s opinion, and thus are a useful target for opinion mining. However, labelling each quote with a polarity score directed at a textually-anchored target can ignore the broader issue that the speaker is commenting on. We address this by instead labelling quotes as supporting or opposing a clear expression of a point of view on a topic, called a position statement. Using this we construct a corpus covering 7 topics with 2,228 quotes."
P13-2118,Joint Apposition Extraction with Syntactic and Semantic Constraints,2013,29,3,2,1,24328,will radford,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"Appositions are adjacent NPs used to add information to a discourse. We propose systems exploiting syntactic and semantic constraints to extract appositions from OntoNotes. Our joint log-linear model outperforms the state-of-the-art Favre and Hakkani-Txc2xa8 ur (2009) model by 10% on Broadcast News, and achieves 54.3% Fscore on multiple genres."
D13-1101,Automatically Detecting and Attributing Indirect Quotations,2013,21,28,4,1,30025,silvia pareti,Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,0,"Direct quotations are used for opinion min- ing and information extraction as they have an easy to extract span and they can be attributed to a speaker with high accuracy. However, simply focusing on direct quotations ignores around half of all reported speech, which is in the form of indirect or mixed speech. This work presents the first large-scale experiments in indirect and mixed quotation extraction and attribution. We propose two methods of ex- tracting all quote types from news articles and evaluate them on two large annotated corpora, one of which is a contribution of this work. We further show that direct quotation attribu- tion methods can be successfully applied to in- direct and mixed quotation attribution."
P12-2021,Robust Conversion of {CCG} Derivations to Phrase Structure Trees,2012,23,2,3,1,2960,jonathan kummerfeld,Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"We propose an improved, bottom-up method for converting CCG derivations into PTB-style phrase structure trees. In contrast with past work (Clark and Curran, 2009), which used simple transductions on category pairs, our approach uses richer transductions attached to single categories. Our conversion preserves more sentences under round-trip conversion (51.1% vs. 39.6%) and is more robust. In particular, unlike past methods, ours does not require ad-hoc rules over non-local features, and so can be easily integrated into a parser."
P12-2045,Event Linking: Grounding Event Reference in a News Archive,2012,16,11,4,1,28420,joel nothman,Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"Interpreting news requires identifying its constituent events. Events are complex linguistically and ontologically, so disambiguating their reference is challenging. We introduce event linking, which canonically labels an event reference with the article where it was first reported. This implicitly relaxes coreference to co-reporting, and will practically enable augmenting news archives with semantic hyperlinks. We annotate and analyse a corpus of 150 documents, extracting 501 links to a news archive with reasonable inter-annotator agreement."
P12-1052,Dependency Hashing for n-best {CCG} Parsing,2012,20,6,2,1,37524,dominick ng,Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"Optimising for one grammatical representation, but evaluating over a different one is a particular challenge for parsers and n-best CCG parsing. We find that this mismatch causes many n-best CCG parses to be semantically equivalent, and describe a hashing technique that eliminates this problem, improving oracle n-best F-score by 0.7% and reranking accuracy by 0.4%. We also present a comprehensive analysis of errors made by the C&C CCG parser, providing the first breakdown of the impact of implementation decisions, such as supertagging, on parsing accuracy."
N12-1030,The Challenges of Parsing {C}hinese with {C}ombinatory {C}ategorial {G}rammar,2012,32,7,2,1,41398,daniel tse,Proceedings of the 2012 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"We apply Combinatory Categorial Grammar to wide-coverage parsing in Chinese with the new Chinese CCGbank, bringing a formalism capable of transparently recovering non-local dependencies to a language in which they are particularly frequent.n n We train two state-of-the-art English CCG parsers: the parser of Petrov and Klein (P&K), and the Clark and Curran (C&C) parser, uncovering a surprising performance gap between them not observed in English --- 72.73 (P&K) and 67.09 (C&C) F-score on PCTB 6.n n We explore the challenges of Chinese CCG parsing through three novel ideas: developing corpus variants rather than treating the corpus as fixed; controlling noun/verb and other POS ambiguities; and quantifying the impact of constructions like pro-drop."
D12-1072,A Sequence Labelling Approach to Quote Attribution,2012,17,31,3,0,43632,timothy okeefe,Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,0,"Quote extraction and attribution is the task of automatically extracting quotes from text and attributing each quote to its correct speaker. The present state-of-the-art system uses gold standard information from previous decisions in its features, which, when removed, results in a large drop in performance. We treat the problem as a sequence labelling task, which allows us to incorporate sequence features without using gold standard information. We present results on two new corpora and an augmented version of a third, achieving a new state-of-the-art for systems using only realistic features."
D12-1096,Parser Showdown at the {W}all {S}treet Corral: An Empirical Investigation of Error Types in Parser Output,2012,42,47,3,1,2960,jonathan kummerfeld,Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,0,"Constituency parser performance is primarily interpreted through a single metric, F-score on WSJ section 23, that conveys no linguistic information regarding the remaining errors. We classify errors within a set of linguistically meaningful types using tree transformations that repair groups of errors together. We use this analysis to answer a range of questions about parser behaviour, including what linguistic constructions are difficult for state-of-the-art parsers, what types of errors are being resolved by rerankers, and what types are introduced when parsing out-of-domain text."
C12-1018,Improvements to Training an {RNN} parser,2012,23,4,2,0,37389,richard billingsley,Proceedings of {COLING} 2012,0,None
C12-1088,Improving {C}ombinatory {C}ategorial {G}rammar Parse Reranking with Dependency Grammar Features,2012,30,5,4,0,32577,sunghwan kim,Proceedings of {COLING} 2012,0,"This paper presents a novel method of improving Combinatory Categorial Grammar (CCG) parsing using features generated from Dependency Grammar (DG) parses and combined using reranking. Different grammar formalisms have different strengths and different parsing models have consequently divergent views of the data. More specifically, dependency parsers are sensitive to linguistic generalisations that differ from the generalisations that the CCG parser is sensitive to, and which the reranker exploits to identify the parse most likely to be correct. We propose DG-derived reranking features, which are obtained by comparing dependencies from the CCG parser with DG dependencies, and demonstrate how they improve the performance of a CCG parser and reranker in a variety of settings. We record a final labeled F-score of 87.93% on section 23 of CCGbank, 0.5% and 0.35% improvements over the base parser (87.43%) and reranker (87.58%), respectively."
U11-1010,Frontier Pruning for Shift-Reduce {CCG} Parsing,2011,26,2,2,0,44479,stephen merity,Proceedings of the Australasian Language Technology Association Workshop 2011,0,None
P11-2046,Relation Guided Bootstrapping of Semantic Lexicons,2011,14,8,3,1,44608,tara mcintosh,Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,0,"State-of-the-art bootstrapping systems rely on expert-crafted semantic constraints such as negative categories to reduce semantic drift. Unfortunately, their use introduces a substantial amount of supervised knowledge. We present the Relation Guided Bootstrapping (RGB) algorithm, which simultaneously extracts lexicons and open relationships to guide lexicon growth and reduce semantic drift. This removes the necessity for manually crafting category and relationship constraints, and manually generating negative categories."
J11-4006,Parsing Noun Phrases in the {P}enn {T}reebank,2011,58,7,2,1,44739,david vadas,Computational Linguistics,0,"Noun phrases (nps) are a crucial part of natural language, and can have a very complex structure. However, this np structure is largely ignored by the statistical parsing field, as the most widely used corpus is not annotated with it. This lack of gold-standard data has restricted previous efforts to parse nps, making it impossible to perform the supervised experiments that have achieved high performance in so many Natural Language Processing (nlp) tasks.n n We comprehensively solve this problem by manually annotating np structure for the entire Wall Street Journal section of the Penn Treebank. The inter-annotator agreement scores that we attain dispel the belief that the task is too difficult, and demonstrate that consistent np annotation is possible. Our gold-standard np data is now available for use in all parsers.n n We experiment with this new data, applying the Collins (2003) parsing model, and find that its recovery of np structure is significantly worse than its overall performance. The parser's F-score is up to 5.69% lower than a baseline that uses deterministic rules. Through much experimentation, we determine that this result is primarily caused by a lack of lexical information.n n To solve this problem we construct a wide-coverage, large-scale np Bracketing system. With our Penn Treebank data set, which is orders of magnitude larger than those used previously, we build a supervised model that achieves excellent results. Our model performs at 93.8% F-score on the simple task that most previous work has undertaken, and extends to bracket longer, more complex nps that are rarely dealt with in the literature. We attain 89.14% F-score on this much more difficult task. Finally, we implement a post-processing module that brackets nps identified by the Bikel (2004) parser. Our np Bracketing model includes a wide variety of features that provide the lexical information that was missing during the parser experiments, and as a result, we outperform the parser's F-score by 9.04%.n n These experiments demonstrate the utility of the corpus, and show that many nlp applications can now make use of np structure."
W10-0515,Tracking Information Flow between Primary and Secondary News Sources,2010,7,0,3,1,24328,will radford,Proceedings of the {NAACL} {HLT} 2010 Workshop on Computational Linguistics in a World of Social Media,0,"Tracking information flow (IFLOW) is crucial to understanding the evolution of news stories. We present analysis and experiments for IFLOW between company announcements and newswire. Error analysis shows that many FPs are annotation errors and many FNs are due to coarse-grained document-level modelling. Experiments show that document meta-data features (e.g., category, length, timing) improve f-scores relative to upper bound by 23%."
U10-1014,Reranking a wide-coverage ccg parser,2010,24,7,3,1,37524,dominick ng,Proceedings of the Australasian Language Technology Association Workshop 2010,0,None
S10-1069,{SCHWA}: {PETE} Using {CCG} Dependencies with the {C{\\&}C} Parser,2010,12,3,4,1,37524,dominick ng,Proceedings of the 5th International Workshop on Semantic Evaluation,0,"This paper describes the SCHWA system entered by the University of Sydney in SemEval 2010 Task 12 -- Parser Evaluation using Textual Entailments (Yuret et al., 2010). Our system achieved an overall accuracy of 70% in the task evaluation.n n We used the C&C parser to build CCG dependency parses of the truth and hypothesis sentences. We then used partial match heuristics to determine whether the system should predict entailment. Heuristics were used because the dependencies generated by the parser are construction specific, making full compatibility unlikely. We also manually annotated the development set with CCG analyses, establishing an upper bound for our entailment system of 87%."
P10-1022,Rebanking {CCG}bank for Improved {NP} Interpretation,2010,26,17,2,1,37818,matthew honnibal,Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,1,"Once released, treebanks tend to remain unchanged despite any shortcomings in their depth of linguistic analysis or coverage of specific phenomena. Instead, separate resources are created to address such problems. In this paper we show how to improve the quality of a treebank, by integrating resources and implementing improved analyses for specific constructions.n n We demonstrate this rebanking process by creating an updated version of CCG-bank that includes the predicate-argument structure of both verbs and nouns, base-NP brackets, verb-particle constructions, and restrictive and non-restrictive nominal modifiers; and evaluate the impact of these changes on a statistical parser."
P10-1036,Faster Parsing by Supertagger Adaptation,2010,37,16,5,1,2960,jonathan kummerfeld,Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,1,"We propose a novel self-training method for a parser which uses a lexicalised grammar and supertagger, focusing on increasing the speed of the parser rather than its accuracy. The idea is to train the supertagger on large amounts of parser output, so that the supertagger can learn to supply the supertags that the parser will eventually choose as part of the highest-scoring derivation. Since the supertagger supplies fewer supertags overall, the parsing speed is increased. We demonstrate the effectiveness of the method using a CCG supertagger and parser, obtaining significant speed increases on newspaper text with no loss in accuracy. We also show that the method can be used to adapt the CCG parser to new domains, obtaining accuracy and speed improvements for Wikipedia and biomedical text."
C10-2051,Morphological Analysis Can Improve a {CCG} Parser for {E}nglish,2010,14,3,3,1,37818,matthew honnibal,Coling 2010: Posters,0,"Because English is a low morphology language, current statistical parsers tend to ignore morphology and accept some level of redundancy. This paper investigates how costly such redundancy is for a lexicalised grammar such as CCG.n n We use morphological analysis to split verb inflectional suffixes into separate tokens, so that they can receive their own lexical categories. We find that this improves accuracy when the splits are based on correct POS tags, but that errors in gold standard or automatically assigned POS tags are costly for the system. This shows that the parser can benefit from morphological analysis, so long as the analysis is correct."
C10-2168,Chart Pruning for Fast Lexicalised-Grammar Parsing,2010,24,17,5,0,884,yue zhang,Coling 2010: Posters,0,"Given the increasing need to process massive amounts of textual data, efficiency of NLP tools is becoming a pressing concern. Parsers based on lexicalised grammar formalisms, such as TAG and CCG, can be made more efficient using supertagging, which for CCG is so effective that every derivation consistent with the supertagger output can be stored in a packed chart. However, wide-coverage CCG parsers still produce a very large number of derivations for typical newspaper or Wikipedia sentences. In this paper we investigate two forms of chart pruning, and develop a novel method for pruning complete cells in a parse chart. The result is a wide-coverage CCG parser that can process almost 100 sentences per second, with little or no loss in accuracy over the baseline with no pruning."
C10-1122,{C}hinese {CCG}bank: extracting {CCG} derivations from the {P}enn {C}hinese Treebank,2010,32,23,2,1,41398,daniel tse,Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010),0,"Automated conversion has allowed the development of wide-coverage corpora for a variety of grammar formalisms without the expense of manual annotation. Analysing new languages also tests formalisms, exposing their strengths and weaknesses.n n We present Chinese CCGbank, a 760,000 word corpus annotated with Combinatory Categorial Grammar (ccg) derivations, induced automatically from the Penn Chinese Treebank (pctb). We design parsimonious ccg analyses for a range of Chinese syntactic constructions, and transform the pctb trees to produce them. Our process yields a corpus of 27,759 derivations, covering 98.1% of the pctb."
W09-3603,Accurate Argumentative Zoning with Maximum Entropy models,2009,20,28,3,0,44479,stephen merity,Proceedings of the 2009 Workshop on Text and Citation Analysis for Scholarly Digital Libraries ({NLPIR}4{DL}),0,"We present a maximum entropy classifier that significantly improves the accuracy of Argumentative Zoning in scientific literature. We examine the features used to achieve this result and experiment with Argumentative Zoning as a sequence tagging task, decoded with Viterbi using up to four previous classification decisions. The result is a 23% F-score increase on the Computational Linguistics conference papers marked up by Teufel (1999).n n Finally, we demonstrate the performance of our system in different scientific domains by applying it to a corpus of Astronomy journal articles annotated using a modified Argumentative Zoning scheme."
W09-3302,Named Entity Recognition in {W}ikipedia,2009,22,40,5,0,46903,dominic balasuriya,Proceedings of the 2009 Workshop on The People{'}s Web Meets {NLP}: Collaboratively Constructed Semantic Resources (People{'}s Web),0,"Named entity recognition (NER) is used in many domains beyond the newswire text that comprises current gold-standard corpora. Recent work has used Wikipedia's link structure to automatically generate near gold-standard annotations. Until now, these resources have only been evaluated on newswire corpora or themselves.n n We present the first NER evaluation on a Wikipedia gold standard (WG) corpus. Our analysis of cross-corpus performance on WG shows that Wikipedia text may be a harder NER domain than newswire. We find that an automatic annotation of Wikipedia has high agreement with WG and, when used as training data, outperforms newswire models by up to 7.7%."
W09-3306,Evaluating a Statistical {CCG} Parser on {W}ikipedia,2009,12,11,3,1,37818,matthew honnibal,Proceedings of the 2009 Workshop on The People{'}s Web Meets {NLP}: Collaboratively Constructed Semantic Resources (People{'}s Web),0,"The vast majority of parser evaluation is conducted on the 1984 Wall Street Journal (WSJ). In-domain evaluation of this kind is important for system development, but gives little indication about how the parser will perform on many practical problems. Wikipedia is an interesting domain for parsing that has so far been under-explored. We present statistical parsing results that for the first time provide information about what sort of performance a user parsing Wikipedia text can expect.n n We find that the C&C parser's standard model is 4.3% less accurate on Wikipedia text, but that a simple self-training exercise reduces the gap to 3.8%. The self-training also speeds up the parser on newswire text by 20%."
U09-1003,Tracking Information Flow in Financial Text,2009,-1,-1,3,1,24328,will radford,Proceedings of the Australasian Language Technology Association Workshop 2009,0,None
U09-1004,Classifying articles in {E}nglish and {G}erman {W}ikipedia,2009,18,6,4,1,25855,nicky ringland,Proceedings of the Australasian Language Technology Association Workshop 2009,0,"Named Entity (NE) information is critical for Information Extraction (IE) tasks. However, the cost of manually annotating sufficient data for training purposes, especially for multiple languages, is prohibitive, meaning automated methods for developing resources are crucial. We investigate the automatic generation of NE annotated data in German from Wikipedia. By incorporating structural features of Wikipedia, we can develop a German corpus which accurately classifies Wikipedia articles into NE categories to within 1% F -score of the state-of-the-art process in English."
U09-1009,Faster parsing and supertagging model estimation,2009,31,2,3,1,2960,jonathan kummerfeld,Proceedings of the Australasian Language Technology Association Workshop 2009,0,"Parsers are often the bottleneck for data acquisition, processing text too slowly to be widely applied. One way to improve the efficiency of parsers is to construct more confident statistical models. More training data would enable the use of more sophisticated features and also provide more evidence for current features, but gold standard annotated data is limited and expensive to produce. We demonstrate faster methods for training a supertagger using hundreds of millions of automatically annotated words, constructing statistical models that further constrain the number of derivations the parser must consider. By introducing new features and using an automatically annotated corpus we are able to double parsing speed on Wikipedia and the Wall Street Journal, and gain accuracy slightly when parsing Section 00 of the Wall Street Journal."
U09-1010,{CCG} parsing with one syntactic structure per n-gram,2009,11,2,2,1,38280,tim dawborn,Proceedings of the Australasian Language Technology Association Workshop 2009,0,None
U09-1012,A Sentiment Detection Engine for {I}nternet Stock Message Boards,2009,-1,-1,3,0,47131,christopher chua,Proceedings of the Australasian Language Technology Association Workshop 2009,0,None
U09-1015,Improved Text Categorisation for {W}ikipedia Named Entities,2009,15,19,2,0,47132,sam tardif,Proceedings of the Australasian Language Technology Association Workshop 2009,0,"The accuracy of named entity recognition systems relies heavily upon the volume and quality of available training data. Improving the process of automatically producing such training data is an important task, as manual acquisition is both time consuming and expensive. We explore the use of a variety of machine learning algorithms for categorising Wikipedia articles, an initial step in producing the named entity training data. We were able to achieve a categorisation accuracy of 95% F -score over six coarse categories, an improvement of up to 5% F -score over previous methods."
U09-1017,Integrating Verb-Particle Constructions into {CCG} Parsing,2009,9,5,2,0,45611,james constable,Proceedings of the Australasian Language Technology Association Workshop 2009,0,"Despite their prevalence in the English language, multiword expressions like verb-particle constructions (VPCs) are often poorly handled by NLP systems. This problem is partly due to inadequacies in existing corpora; the primary corpus for CCG-oriented work, CCGbank, does not account for VPCs at all, and is inconsistent in its handling of them. In this paper, we apply some corrective transformations to CCGbank, and then use it to retrain an augmented version of the Clark and Curran CCG parser. Using our technique, we observe no significant change in F-score, while the resulting parse is semantically more sound."
P09-2014,Comparing the Accuracy of {CCG} and {P}enn {T}reebank Parsers,2009,8,7,2,0.787739,20968,stephen clark,Proceedings of the {ACL}-{IJCNLP} 2009 Conference Short Papers,0,"We compare the CCG parser of Clark and Curran (2007) with a state-of-the-art Penn Treebank (PTB) parser. An accuracy comparison is performed by converting the CCG derivations into PTB trees. We show that the conversion is extremely difficult to perform, but are able to fairly compare the parsers on a representative subset of the PTB test section, obtaining results for the CCG parser that are statistically no different to those for the Berkeley parser."
P09-1045,Reducing Semantic Drift with Bagging and Distributional Similarity,2009,22,50,2,1,44608,tara mcintosh,Proceedings of the Joint Conference of the 47th Annual Meeting of the {ACL} and the 4th International Joint Conference on Natural Language Processing of the {AFNLP},1,"Iterative bootstrapping algorithms are typically compared using a single set of hand-picked seeds. However, we demonstrate that performance varies greatly depending on these seeds, and favourable seeds for one algorithm can perform very poorly with others, making comparisons unreliable. We exploit this wide variation with bagging, sampling from automatically extracted seeds to reduce semantic drift.n n However, semantic drift still occurs in later iterations. We propose an integrated distributional similarity filter to identify and censor potential semantic drifts, ensuring over 10% higher precision when extracting large semantic lexicons."
E09-1070,Analysing {W}ikipedia and Gold-Standard Corpora for {NER} Training,2009,17,48,3,1,28420,joel nothman,Proceedings of the 12th Conference of the {E}uropean Chapter of the {ACL} ({EACL} 2009),0,"Named entity recognition (ner) for English typically involves one of three gold standards: muc, conll, or bbn, all created by costly manual annotation. Recent work has used Wikipedia to automatically create a massive corpus of named entity annotated text.n n We present the first comprehensive cross-corpus evaluation of ner. We identify the causes of poor cross-corpus performance and demonstrate ways of making them more compatible. Using our process, we develop a Wikipedia corpus which outperforms gold standard corpora on cross-corpus evaluation by up to 11%."
D09-1126,Fully Lexicalising {CCG}bank with Hat Categories,2009,16,6,2,1,37818,matthew honnibal,Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,0,"We introduce an extension to CCG that allows form and function to be represented simultaneously, reducing the proliferation of modifier categories seen in standard CCG analyses.n n We can then remove the non-combinatory rules CCGbank uses to address this problem, producing a grammar that is fully lexicalised and far less ambiguous.n n There are intrinsic benefits to full lexicalisation, such as semantic transparency and simpler domain adaptation. The clearest advantage is a 52--88% improvement in parse speeds, which comes with only a small reduction in accuracy."
U08-1002,Answer Attenuation in Question Answering,2008,9,0,2,0,47843,katie bell,Proceedings of the Australasian Language Technology Association Workshop 2008,0,None
U08-1006,Automatic Acquisition of Training Data for Statistical Parsers,2008,25,1,2,0,44619,susan howlett,Proceedings of the Australasian Language Technology Association Workshop 2008,0,"The limitations of existing data sets for training parsers has led to a need for additional data. However, the cost of manually annotating the amount and range of data required is prohibitive. For a number of simple facts like those sought in Question Answering, we compile a corpus of sentences extracted from the Web that contain the fact keywords. We use a state-of-the-art parser to parse these sentences, constraining the analysis of the more complex sentences using information from the simpler sentences. This allows us to automatically create additional annotated sentences which we then use to augment our existing training data."
U08-1008,Classification of Verb Particle Constructions with the {G}oogle {W}eb1{T} Corpus,2008,21,4,2,1,2960,jonathan kummerfeld,Proceedings of the Australasian Language Technology Association Workshop 2008,0,"Manually maintaining comprehensive databases of multi-word expressions, for example Verb-Particle Constructions (VPCs), is infeasible. We describe a new type level classifier for potential VPCs, which uses information in the Google Web1T corpus to perform a simple linguistic constituency test. Specifically, we consider the fronting test, comparing the frequencies of the two possible orderings of the given verb and particle. Using only a small set of queries for each verb-particle pair, the system was able to achieve an F-score of 75.7% in our evaluation while processing thousands of queries a second."
U08-1013,Weighted Mutual Exclusion Bootstrapping for Domain Independent Lexicon and Template Acquisition,2008,18,26,2,1,44608,tara mcintosh,Proceedings of the Australasian Language Technology Association Workshop 2008,0,"We present the Weighted Mutual Exclusion Bootstrapping (WMEB) algorithm for simultaneously extracting precise semantic lexicons and templates for multiple categories. WMEB is capable of extracting larger lexicons with higher precision than previous techniques, successfully reducing semantic drift by incorporating new weighting functions and a cumulative template pool while still enforcing mutual exclusion between the categories. We compare WMEB and two state-of-theart approaches on the Web 1T corpus and two large biomedical literature collections. WMEB is more efficient and scalable, and we demonstrate that it significantly outperforms the other approaches on the noisy web corpus and biomedical text."
U08-1014,Investigating Features for Classifying Noun Relations,2008,17,0,4,1,37524,dominick ng,Proceedings of the Australasian Language Technology Association Workshop 2008,0,Automated recognition of the semantic relationship between two nouns in a sentence is useful for a wide variety of tasks in NLP. Previous approaches have used kernel methods with semantic and lexical evidence for classification. We present a system based on a maximum entropy classifier which also considers both the grammatical dependencies in a sentence and significance information based on the Google Web 1T dataset. We report results comparable with state of the art performance using limited data based on the SemEval 2007 shared task on nominal classification.
U08-1016,Transforming {W}ikipedia into Named Entity Training Data,2008,18,76,2,1,28420,joel nothman,Proceedings of the Australasian Language Technology Association Workshop 2008,0,"Statistical named entity recognisers require costly hand-labelled training data and, as a result, most existing corpora are small. We exploit Wikipedia to create a massive corpus of named entity annotated text. We transform Wikipediaxe2x80x99s links into named entity annotations by classifying the target articles into common entity types (e.g. person, organisation and location). Comparing to MUC, CONLL and BBN corpora, Wikipedia generally performs better than other cross-corpus train/test pairs."
U08-1019,Punctuation Normalisation for Cleaner Treebanks and Parsers,2008,11,2,2,1,41398,daniel tse,Proceedings of the Australasian Language Technology Association Workshop 2008,0,"Although punctuation is pervasive in written text, their treatment in parsers and corpora is often second-class. We examine the treatment of commas in CCGbank, a wide-coverage corpus for Combinatory Categorial Grammar (CCG), reanalysing its comma structures in order to eliminate a class of redundant rules, obtaining a more consistent treebank. We then eliminate these rules from C&C, a wide-coverage statistical CCG parser, obtaining a 37% increase in parsing speed on the standard CCGbank test set and a considerable reduction in memory consumed, without affecting parser accuracy."
P08-1039,Parsing Noun Phrase Structure with {CCG},2008,13,23,2,1,44739,david vadas,Proceedings of ACL-08: HLT,1,"Statistical parsing of noun phrase ( NP) structure has been hampered by a lack of goldstandard data. This is a significant problem for CCGbank, where binary branching NP derivations are often incorrect, a result of the automatic conversion from the Penn Treebank. We correct these errors in CCGbank using a gold-standard corpus of NP structure, resulting in a much more accurate corpus. We also implement novel NER features that generalise the lexical information needed to parse NPs and provide important semantic information. Finally, evaluating against DepBank demonstrates the effectiveness of our modified corpus and novel features, with an increase in parser performance of 1.51%."
W07-2206,Improving the Efficiency of a Wide-Coverage {CCG} Parser,2007,17,15,2,0,48925,bojan djordjevic,Proceedings of the Tenth International Conference on Parsing Technologies,0,"The C&C CCG parser is a highly efficient linguistically motivated parser. The efficiency is achieved using a tightly-integrated supertagger, which assigns CCG lexical categories to words in a sentence. The integration allows the parser to request more categories if it cannot find a spanning analysis. We present several enhancements to the CKY chart parsing algorithm used by the parser. The first proposal is chart repair, which allows the chart to be efficiently updated by adding lexical categories individually, and we evaluate several strategies for adding these categories. The second proposal is to add constraints to the chart which require certain spans to be constituents. Finally, we propose partial beam search to further reduce the search space. Overall, the parsing speed is improved by over 35% with negligible loss of accuracy or coverage."
W07-1202,Perceptron Training for a Wide-Coverage Lexicalized-Grammar Parser,2007,25,13,2,1,20968,stephen clark,{ACL} 2007 Workshop on Deep Linguistic Processing,0,"This paper investigates perceptron training for a wide-coverage CCG parser and compares the perceptron with a log-linear model. The CCG parser uses a phrase-structure parsing model and dynamic programming in the form of the Viterbi algorithm to find the highest scoring derivation. The difficulty in using the perceptron for a phrase-structure parsing model is the need for an efficient decoder. We exploit the lexicalized nature of CCG by using a finite-state supertagger to do much of the parsing work, resulting in a highly efficient decoder. The perceptron performs as well as the log-linear model; it trains in a few hours on a single machine; and it requires only a few hundred MB of RAM for practical training compared to 20 GB for the log-linear model. We also investigate the order in which the training examples are presented to the online perceptron learner, and find that order does not significantly affect the results."
W07-1212,Creating a Systemic Functional Grammar Corpus from the {P}enn {T}reebank,2007,9,9,2,1,37818,matthew honnibal,{ACL} 2007 Workshop on Deep Linguistic Processing,0,"The lack of a large annotated systemic functional grammar (SFG) corpus has posed a significant challenge for the development of the theory. Automating SFG annotation is challenging because the theory uses a minimal constituency model, allocating as much of the work as possible to a set of hierarchically organised features.n n In this paper we show that despite the unorthodox organisation of SFG, adapting existing resources remains the most practical way to create an annotated corpus. We present and analyse SFGBank, an automated conversion of the Penn Treebank into systemic functional grammar. The corpus is comparable to those available for other linguistic theories, offering many opportunities for new research."
W07-1023,Challenges for extracting biomedical knowledge from full text,2007,16,9,2,1,44608,tara mcintosh,"Biological, translational, and clinical language processing",0,"At present, most biomedical Information Retrieval and Extraction tools process abstracts rather than full-text articles. The increasing availability of full text will allow more knowledge to be extracted with greater reliability. To investigate the challenges of full-text processing, we manually annotated a corpus of cited articles from a Molecular Interaction Map (Kohn, 1999).n n Our analysis demonstrates the necessity of full-text processing; identifies the article sections where interactions are most commonly stated; and quantifies both the amount of external knowledge required and the proportion of interactions requiring multiple or deeper inference steps. Further, it identifies a range of NLP tools required, including: identifying synonyms, and resolving coreference and negated expressions. This is important guidance for researchers engineering biomedical text processing systems."
W07-0610,The Topology of Synonymy and Homonymy Networks,2007,26,3,2,1,49054,james gorman,Proceedings of the Workshop on Cognitive Aspects of Computational Language Acquisition,0,"Semantic networks have been used successfully to explain access to the mental lexicon. Topological analyses of these networks have focused on acquisition and generation. We extend this work to look at models that distinguish semantic relations. We find the scale-free properties of association networks are not found in synonymy-homonymy networks, and that this is consistent with studies of childhood acquisition of these relationships. We further find that distributional models of language acquisition display similar topological properties to these networks."
U07-1011,Experiments in Mutual Exclusion Bootstrapping,2007,10,6,2,1,46859,tara murphy,Proceedings of the Australasian Language Technology Workshop 2007,0,"Mutual Exclusion Bootstrapping (MEB) was designed to overcome the problem of semantic drift suffered by iterative bootstrapping, where the meaning of extracted terms quickly drifts from the original seed terms (Curran et al., 2007). MEB works by extracting mutually exclusive classes in parallel which constrain each other. In this paper we explore the strengths and limitations of MEB by applying it to two novel lexical-semantic extraction tasks: extracting bigram named entities and WordNet lexical file classes (Fellbaum, 1998) from the Google Web 1T 5-grams."
U07-1016,Parsing Internal Noun Phrase Structure with Collins{'} Models,2007,12,4,2,1,44739,david vadas,Proceedings of the Australasian Language Technology Workshop 2007,0,"Collinsxe2x80x99 widely-used parsing models treat noun phrases (NPs) in a different manner to other constituents. We investigate these differences, using the recently released internal NP bracketing data (Vadas and Curran, 2007a). Altering the structure of the Treebank, as this data does, has a number of consequences, as parsers built using Collinsxe2x80x99 models assume that their training and test data will have structure similar to the Penn Treebankxe2x80x99s. Our results demonstrate that it is difficult for Collinsxe2x80x99 models to adapt to this new NP structure, and that parsers using these models make mistakes as a result. This emphasises how important treebank structure itself is, and the large amount of influence it can have."
U07-1021,Distributional Similarity of Multi-Word Expressions,2007,10,1,2,0,49094,laura ingram,Proceedings of the Australasian Language Technology Workshop 2007,0,"Most existing systems for automatically extracting lexical-semantic resources neglect multi-word expressions (MWEs), even though approximately 30% of gold-standard thesauri entries are MWEs. We present a distributional similarity system that identifies synonyms for MWEs. We extend Grefenstettexe2x80x99s SEXTANT shallow parser to first identify bigram MWEs using collocation statistics from the Google WEB1T corpus. We extract contexts from WEB1T to increase coverage on the sparser bigrams."
U07-1022,Extending {CCG}bank with Quotes and Multi-modal {CCG},2007,5,1,2,1,41398,daniel tse,Proceedings of the Australasian Language Technology Workshop 2007,0,"CCGbank is an automatic conversion of the Penn Treebank to Combinatory Categorial Grammar (CCG). We present two extensions to CCGbank which involve manipulating its derivation and category structure. We discuss approaches for the automatic re-insertion of removed quote symbols and evaluate their impact on the performance of the C&C CCG parser. We also analyse CCGbank to extract a multi-modal CCG lexicon, which will allow the removal of hardcoded language-specific constraints from the C&C parser, granting benefits to parsing speed and accuracy."
P07-2009,Linguistically Motivated Large-Scale {NLP} with {C}{\\&}{C} and Boxer,2007,18,201,1,1,25856,james curran,Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions,0,"The statistical modelling of language, together with advances in wide-coverage grammar development, have led to high levels of robustness and efficiency in NLP systems and made linguistically motivated large-scale language processing a possibility (Matsuzaki et al., 2007; Kaplan et al., 2004). This paper describes an NLP system which is based on syntactic and semantic formalisms from theoretical linguistics, and which we have used to analyse the entire Gigaword corpus (1 billion words) in less than 5 days using only 18 processors. This combination of detail and speed of analysis represents a break-through in NLP technology."
P07-1031,Adding Noun Phrase Structure to the {P}enn {T}reebank,2007,15,95,2,1,44739,david vadas,Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,1,"The Penn Treebank does not annotate within base noun phrases (NPs), committing only to flat structures that ignore the complexity of English NPs. This means that tools trained on Treebank data cannot learn the correct internal structure of NPs. This paper details the process of adding gold-standard bracketing within each noun phrase in the Penn Treebank. We then examine the consistency and reliability of our annotations. Finally, we use this resource to determine NP structure using several statistical approaches, thus demonstrating the utility of the corpus. This adds detail to the Penn Treebank that is necessary for many NLP applications."
P07-1032,Formalism-Independent Parser Evaluation with {CCG} and {D}ep{B}ank,2007,21,53,2,1,20968,stephen clark,Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,1,"A key question facing the parsing community is how to compare parsers which use different grammar formalisms and produce different output. Evaluating a parser on the same resource used to create it can lead to non-comparable accuracy scores and an over-optimistic view of parser performance. In this paper we evaluate a CCG parser on DepBank, and demonstrate the difficulties in converting the parser output into DepBank grammatical relations. In addition we present a method for measuring the effectiveness of the conversion, which provides an upper bound on parsing accuracy. The CCG parser obtains an F-score of 81.9% on labelled dependencies, against an upper bound of 84.8%. We compare the CCG parser against the RASP parser, outperforming RASP by over 5% overall and on the majority of dependency types."
J07-4004,Wide-Coverage Efficient Statistical Parsing with {CCG} and Log-Linear Models,2007,95,324,2,1,20968,stephen clark,Computational Linguistics,0,"This article describes a number of log-linear parsing models for an automatically extracted lexicalized grammar. The models are xe2x80x9cfullxe2x80x9d parsing models in the sense that probabilities are defined for complete parses, rather than for independent events derived by decomposing the parse tree. Discriminative training is used to estimate the models, which requires incorrect parses for each sentence in the training data as well as the correct parse. The lexicalized grammar formalism used is Combinatory Categorial Grammar (CCG), and the grammar is automatically extracted from CCGbank, a CCG version of the Penn Treebank. The combination of discriminative training and an automatically extracted grammar leads to a significant memory requirement (up to 25 GB), which is satisfied using a parallel implementation of the BFGS optimization algorithm running on a Beowulf cluster. Dynamic programming over a packed chart, in combination with the parallel implementation, allows us to solve one of the largest-scale estimation problems in the statistical parsing literature in under three hours.n n A key component of the parsing system, for both training and testing, is a Maximum Entropy supertagger which assigns CCG lexical categories to words in a sentence. The supertagger makes the discriminative training feasible, and also leads to a highly efficient parser. Surprisingly, given CCG's xe2x80x9cspurious ambiguity,xe2x80x9d the parsing speeds are significantly higher than those reported for comparable parsers in the literature. We also extend the existing parsing techniques for CCG by developing a new model and efficient parsing algorithm which exploits all derivations, including CCG's nonstandard derivations. This model and parsing algorithm, when combined with normal-form constraints, give state-of-the-art accuracy for the recovery of predicate-argument dependencies from CCGbank. The parser is also evaluated on DepBank and compared against the RASP parser, outperforming RASP overall and on the majority of relation types. The evaluation on DepBank raises a number of issues regarding parser evaluation.n n This article provides a comprehensive blueprint for building a wide-coverage CCG parser. We demonstrate that both accurate and highly efficient parsing is possible with CCG."
W06-1654,Random Indexing using Statistical Weight Functions,2006,17,14,2,1,49054,james gorman,Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing,0,"Random Indexing is a vector space technique that provides an efficient and scalable approximation to distributional similarity problems. We present experiments showing Random Indexing to be poor at handling large volumes of data and evaluate the use of weighting functions for improving the performance of Random Indexing. We find that Random Index is robust for small data sets, but performance degrades because of the influence high frequency attributes in large data sets. The use of appropriate weight functions improves this significantly."
U06-1003,Efficient {C}ombinatory {C}ategorial {G}rammar Parsing,2006,10,6,2,0,48925,bojan djordjevic,Proceedings of the Australasian Language Technology Workshop 2006,0,"Efficient wide-coverage parsing is integral to large-scale NLP applications. Unfortunately, parsers for linguistically motivated formalisms, e.g. HPSG and TAG, are often too inefficient for these applications. This paper describes two modifications to the standard CKY chart parsing algorithm used in the Clark and Curran (2006) Combinatory Categorial Grammar (CCG) parser. The first modification extends the tight integration of the supertagger and parser, so that individual supertags can be added to the chart, which is then repaired rather than rebuilt. The second modification adds constraints to the chart that restrict which constituents can combine. Parsing speed is improved by 30xe2x80x9335% without a significant accuracy penalty and a small increase in coverage when both of these modifications are used."
U06-1010,Named Entity Recognition for Astronomy Literature,2006,16,13,3,1,46859,tara murphy,Proceedings of the Australasian Language Technology Workshop 2006,0,"We present a system for named entity recognition (ner) in astronomy journal articles. We have developed this system on a ne corpus comprising approximately 200,000 words of text from astronomy articles. These have been manually annotated with xe2x88xbc40 entity types of interest to astronomers. We report on the challenges involved in extracting the corpus, defining entity classes and annotating scientific text. We investigate which features of an existing state-of-the-art Maximum Entropy approach perform well on astronomy text. Our system achieves an F-score of 87.8%."
P06-1046,Scaling Distributional Similarity to Large Corpora,2006,21,36,2,1,49054,james gorman,Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,1,"Accurately representing synonymy using distributional similarity requires large volumes of data to reliably represent infrequent words. However, the naive nearest-neighbour approach to comparing context vectors extracted from large corpora scales poorly (O(n2) in the vocabulary size).In this paper, we compare several existing approaches to approximating the nearest-neighbour search for distributional similarity. We investigate the trade-off between efficiency and accuracy, and find that SASH (Houle and Sakuma, 2005) provides the best balance."
P06-1088,Multi-Tagging for Lexicalized-Grammar Parsing,2006,22,38,1,1,25856,james curran,Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,1,"With performance above 97% accuracy for newspaper text, part of speech (POS) tagging might be considered a solved problem. Previous studies have shown that allowing the parser to resolve POS tag ambiguity does not improve performance. However, for grammar formalisms which use more fine-grained grammatical categories, for example TAG and CCG, tagging accuracy is much lower. In fact, for these formalisms, premature ambiguity resolution makes parsing infeasible.We describe a multi-tagging approach which maintains a suitable level of lexical category ambiguity for accurate and efficient CCG parsing. We extend this multi-tagging approach to the POS level to overcome errors introduced by automatically assigned POS tags. Although POS tagging accuracy seems high, maintaining some POS tag ambiguity in the language processing pipeline results in more accurate CCG supertagging."
N06-1019,Partial Training for a Lexicalized-Grammar Parser,2006,20,19,2,1,20968,stephen clark,"Proceedings of the Human Language Technology Conference of the {NAACL}, Main Conference",0,"We propose a solution to the annotation bottleneck for statistical parsing, by exploiting the lexicalized nature of Combinatory Categorial Grammar (CCG). The parsing model uses predicate-argument dependencies for training, which are derived from sequences of CCG lexical categories rather than full derivations. A simple method is used for extracting dependencies from lexical category sequences, resulting in high precision, yet incomplete and noisy data. The dependency parsing model of Clark and Curran (2004b) is extended to exploit this partial training data. Remarkably, the accuracy of the parser trained on data derived from category sequences alone is only 1.3% worse in terms of F-score than the parser trained on complete dependency structures."
E06-1030,Web Text Corpus for Natural Language Processing,2006,17,41,2,0,50621,vinci liu,11th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"Web text has been successfully used as training data for many NLP applications. While most previous work accesses web text through search engine hit counts, we created a Web Corpus by downloading web pages to create a topic-diverse collection of 10 billion words of English. We show that for context-sensitive spelling correction the Web Corpus results are better than using a search engine. For thesaurus extraction, it achieved similar overall results to a corpus of newspaper text. With many more words available on the web, better results can be obtained by collecting much larger web corpora."
W05-1011,Approximate Searching for Distributional Similarity,2005,14,4,2,1,49054,james gorman,Proceedings of the {ACL}-{SIGLEX} Workshop on Deep Lexical Acquisition,0,"Distributional similarity requires large volumes of data to accurately represent infrequent words. However, the nearest-neighbour approach to finding synonyms suffers from poor scalability. The Spatial Approximation Sample Hierarchy (SASH), proposed by Houle (2003b), is a data structure for approximate nearest-neighbour queries that balances the efficiency/approximation trade-off. We have intergrated this into an existing distributional similarity system, tripling efficiency with a minor accuracy penalty."
U05-1007,Tagging Unknown Words with Raw Text Features,2005,6,4,2,1,44739,david vadas,Proceedings of the Australasian Language Technology Workshop 2005,0,"Processing unknown words is disproportionately important because of their high information content. It is crucial in domains with specialist vocabularies where relevant training material is scarce, for example: biological text. Unknown word processing often begins with Part of Speech (POS) tagging, where accuracy is typically 10% worse than on known words. We demonstrate that features extracted from large raw text corpora can significantly increase accuracy on unknown words. These features supply a large part of what we are missing with unknown words: context information about how the word is used. We describe a Maximum Entropy modelling approach which uses real-valued features to represent unannotated contextual information. Our initial experiments with real-valued features have resulted in an increased accuracy from 87.39% to 88.85% on unknown words."
U05-1009,Augmenting Approximate Similarity Searching with Lexical Information,2005,-1,-1,2,1,49054,james gorman,Proceedings of the Australasian Language Technology Workshop 2005,0,None
U05-1024,Words and Word Usage: Newspaper Text versus the Web,2005,10,0,2,0,50621,vinci liu,Proceedings of the Australasian Language Technology Workshop 2005,0,"This paper explores the differences in words and word usage in two corpora xe2x80x93 one derived from newspaper text and the other from the web. A corpus of web pages is compiled from a controlled traversal of the web, producing a topicdiverse collection of 2 billion words of web text1. We compare this Web Corpus with the Gigaword Corpus, a 2 billion word collection of news articles. The Web Corpus is applied to the task of automatic thesaurus extraction, obtaining similar overall results to using the Gigaword. The quality of synonyms extracted for each target word is dependent on the wordxe2x80x99s usage in the corpus. With many more words available on the web, a much larger Web Corpus can be created to obtain better results in different nlp tasks."
U05-1027,Programming With Unrestricted Natural Language,2005,17,14,2,1,44739,david vadas,Proceedings of the Australasian Language Technology Workshop 2005,0,"We argue it is better to program in a natural language such as English, instead of a programming language like Java. A natural language interface for programming should result in greater readability, as well as making possible a more intuitive way of writing code. In contrast to previous controlled language systems, we allow unrestricted syntax, using wide-coverage syntactic and semantic methods to extract information from the userxe2x80x99s instructions. We also look at how people actually give programming instructions in English, collecting and annotating a corpus of such statements. We identify differences between sentences in this corpus and in typical newspaper text, and the effect they have on how we process the natural language input. Finally, we demonstrate a prototype system, that is capable of translating some English instructions into executable code."
U05-1029,A Distributed Architecture for Interactive Parse Annotation,2005,22,4,5,0,49832,baden hughes,Proceedings of the Australasian Language Technology Workshop 2005,0,"In this paper we describe a modular system architecture for distributed parse annotation using interactive correction. This involves interactively adding constraints to an existing parse until the returned parse is correct. Using a mixed initiative approach, human annotators interact live with distributed ccg parser servers through an annotation gui. The examples presented to each annotator are selected by an active learning framework to maximise the value of the annotated corpus for machine learners. We report on an initial implementation based on a distributed workflow architecture."
P05-1004,Supersense Tagging of Unknown Nouns Using Semantic Similarity,2005,25,46,1,1,25856,james curran,Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics ({ACL}{'}05),1,"The limited coverage of lexical-semantic resources is a significant problem for NLP systems which can be alleviated by automatically classifying the unknown words. Supersense tagging assigns unknown nouns one of 26 broad semantic categories used by lexicographers to organise their manual insertion into WORDNET. Ciaramita and Johnson (2003) present a tagger which uses synonym set glosses as annotated training examples. We describe an unsupervised approach, based on vector-space similarity, which does not require annotated examples but significantly outperforms their tagger. We also demonstrate the use of an extremely large shallow-parsed corpus for calculating vector-space semantic similarity."
W04-3215,Object-Extraction and Question-Parsing using {CCG},2004,17,33,3,1,20968,stephen clark,Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing,0,"Accurate dependency recovery has recently been reported for a number of wide-coverage statistical parsers using Combinatory Categorial Grammar (CCG). However, overall figures give no indication of a parserxe2x80x99s performance on specific constructions, nor how suitable a parser is for specific applications. In this paper we give a detailed evaluation of a CCG parser on object extraction dependencies found in WSJ text. We also show how the parser can be used to parse questions for Question Answering. The accuracy of the original parser on questions is very poor, and we propose a novel technique for porting the parser to a new domain, by creating new labelled data at the lexical category level only. Using a supertagger to assign categories to words, trained on the new data, leads to a dramatic increase in question parsing accuracy."
P04-1014,Parsing the {WSJ} Using {CCG} and Log-Linear Models,2004,19,265,2,1,20968,stephen clark,Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics ({ACL}-04),1,"This paper describes and evaluates log-linear parsing models for Combinatory Categorial Grammar (CCG). A parallel implementation of the L-BFGS optimisation algorithm is described, which runs on a Beowulf cluster allowing the complete Penn Treebank to be used for estimation. We also develop a new efficient parsing algorithm for CCG which maximises expected recall of dependencies. We compare models which use all CCG derivations, including non-standard derivations, with normal-form models. The performances of the two models are comparable and the results are competitive with existing wide-coverage CCG parsers."
C04-1041,The Importance of Supertagging for Wide-Coverage {CCG} Parsing,2004,17,126,2,1,20968,stephen clark,{COLING} 2004: Proceedings of the 20th International Conference on Computational Linguistics,0,"This paper describes the role of supertagging in a wide-coverage CCG parser which uses a log-linear model to select an analysis. The supertagger reduces the derivation space over which model estimation is performed, reducing the space required for discriminative training. It also dramatically increases the speed of the parser. We show that large increases in speed can be obtained by tightly integrating the supertagger with the CCG grammar and parser. This is the first work we are aware of to successfully integrate a supertagger with a full parser which uses an automatically extracted grammar. We also further reduce the derivation space using constraints on category combination. The result is an accurate wide-coverage CCG parser which is an order of magnitude faster than comparable systems for other linguistically motivated formalisms."
C04-1180,Wide-Coverage Semantic Representations from a {CCG} Parser,2004,21,172,4,0,6245,johan bos,{COLING} 2004: Proceedings of the 20th International Conference on Computational Linguistics,0,"This paper shows how to construct semantic representations from the derivations produced by a wide-coverage CCG parser. Unlike the dependency structures returned by the parser itself, these can be used directly for semantic interpretation. We demonstrate that well-formed semantic representations can be produced for over 97% of the sentences in unseen WSJ text. We believe this is a major step towards widecoverage semantic interpretation, one of the key objectives of the field of NLP."
W03-1013,Log-Linear Models for Wide-Coverage {CCG} Parsing,2003,21,65,2,1,20968,stephen clark,Proceedings of the 2003 Conference on Empirical Methods in Natural Language Processing,0,"This paper describes log-linear parsing models for Combinatory Categorial Grammar (CCG). Log-linear models can easily encode the long-range dependencies inherent in coordination and extraction phenomena, which CCG was designed to handle. Log-linear models have previously been applied to statistical parsing, under the assumption that all possible parses for a sentence can be enumerated. Enumerating all parses is infeasible for large grammars; however, dynamic programming over a packed chart can be used to efficiently estimate the model parameters. We describe a parellelised implementation which runs on a Beowulf cluster and allows the complete WSJ Penn Treebank to be used for estimation."
W03-0806,Blueprint for a High Performance {NLP} Infrastructure,2003,31,11,1,1,25856,james curran,Proceedings of the {HLT}-{NAACL} 2003 Workshop on Software Engineering and Architecture of Language Technology Systems ({SEALTS}),0,"Natural Language Processing (NLP) system developers face a number of new challenges. Interest is increasing for real-world systems that use NLP tools and techniques. The quantity of text now available for training and processing is increasing dramatically. Also, the range of languages and tasks being researched continues to grow rapidly. Thus it is an ideal time to consider the development of new experimental frameworks. We describe the requirements, initial design and exploratory implementation of a high performance NLP infrastructure."
W03-0407,Bootstrapping {POS}-taggers using unlabelled data,2003,17,105,2,1,20968,stephen clark,Proceedings of the Seventh Conference on Natural Language Learning at {HLT}-{NAACL} 2003,0,"This paper investigates booststrapping part-of-speech taggers using co-training, in which two taggers are iteratively re-trained on each other's output. Since the output of the taggers is noisy, there is a question of which newly labelled examples to add to the training set. We investigate selecting examples by directly maximising tagger agreement on unlabelled data, a method which has been theoretically and empirically motivated in the co-training literature. Our results show that agreement-based co-training can significantly improve tagging performance for small seed datasets. Further results show that this form of co-training considerably outperforms self-training. However, we find that simply re-training on all the newly labelled data can, in some cases, yield comparable results to agreement-based co-training, with only a fraction of the computational cost."
W03-0424,Language Independent {NER} using a Maximum Entropy Tagger,2003,12,199,1,1,25856,james curran,Proceedings of the Seventh Conference on Natural Language Learning at {HLT}-{NAACL} 2003,0,"Named Entity Recognition (NER) systems need to integrate a wide variety of information for optimal performance. This paper demonstrates that a maximum entropy tagger can effectively encode such information and identify named entities with very high accuracy. The tagger uses features which can be obtained for a variety of languages and works effectively not only for English, but also for other languages such as German and Dutch."
E03-1071,Investigating {GIS} and Smoothing for Maximum Entropy Taggers,2003,20,91,1,1,25856,james curran,10th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"This paper investigates two elements of Maximum Entropy tagging: the use of a correction feature in the Generalised Iterative Scaling (GIS) estimation algorithm, and techniques for model smoothing. We show analytically and empirically that the correction feature, assumed to be required for the correctness of GIS, is unnecessary. We also explore the use of a Gaussian prior and a simple cutoff for smoothing. The experiments are performed with two tagsets: the standard Penn Treebank POS tagset and the larger set of lexical types from Combinatory Categorial Grammar."
W02-2008,A Very Very Large Corpus Doesn{'}t Always Yield Reliable Estimates,2002,10,22,1,1,25856,james curran,{COLING}-02: The 6th Conference on Natural Language Learning 2002 ({C}o{NLL}-2002),0,"Banko and Brill (2001) suggested that the development of very large training corpora may be more effective for progress in empirical Natural Language Processing than improving methods that use existing smaller training corpora.This work tests their claim by exploring whether a very large corpus can eliminate the sparseness problems associated with estimating unigram probabilities. We do this by empirically investigating the convergence behaviour of unigram probability estimates on a one billion word corpus. When using one billion words, as expected, we do find that many of our estimates do converge to their eventual value. However, we also find that for some words, no such convergence occurs. This leads us to conclude that simply relying upon large corpora is not in itself sufficient: we must pay attention to the statistical modelling as well."
W02-1029,Ensemble Methods for Automatic Thesaurus Extraction,2002,22,65,1,1,25856,james curran,Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing ({EMNLP} 2002),0,"Ensemble methods are state of the art for many NLP tasks. Recent work by Banko and Brill (2001) suggests that this would not necessarily be true if very large training corpora were available. However, their results are limited by the simplicity of their evaluation task and individual classifiers.Our work explores ensemble efficacy for the more complex task of automatic thesaurus extraction on up to 300 million words. We examine our conflicting results in terms of the constraints on, and complexity of, different contextual representations, which contribute to the sparseness-and noise-induced bias behaviour of NLP systems on very large corpora."
W02-0908,Improvements in Automatic Thesaurus Extraction,2002,20,174,1,1,25856,james curran,Proceedings of the {ACL}-02 Workshop on Unsupervised Lexical Acquisition,0,"The use of semantic resources is common in modern NLP systems, but methods to extract lexical semantics have only recently begun to perform well enough for practical use. We evaluate existing and new similarity metrics for thesaurus extraction, and experiment with the trade-off between extraction performance and efficiency. We propose an approximation algorithm, based on canonical attributes and coarse- and fine-grained matching, that reduces the time complexity and execution time of thesaurus extraction with only a marginal performance penalty."
P02-1030,Scaling Context Space,2002,19,62,1,1,25856,james curran,Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics,1,"Context is used in many NLP systems as an indicator of a term's syntactic and semantic function. The accuracy of the system is dependent on the quality and quantity of contextual information available to describe each term. However, the quantity variable is no longer fixed by limited corpus resources. Given fixed training time and computational resources, it makes sense for systems to invest time in extracting high quality contextual information from a fixed corpus. However, with an effectively limitless quantity of text available, extraction rate and representation size need to be considered. We use thesaurus extraction with a range of context extracting tools to demonstrate the interaction between context quantity, time and size on a corpus of 300 million words."
