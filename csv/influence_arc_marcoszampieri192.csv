2015.eamt-1.17,2012.eamt-1.33,1,0.857982,"Missing"
2015.eamt-1.17,C04-1046,0,0.11191,"h levels. We highlight crucial limitations of such metrics for this task, mainly the fact that they disregard the discourse structure of the texts. To better understand these limitations, we designed experiments with human annotators and proposed a way of quantifying differences in translation quality that can only be observed when sentences are judged in the context of entire documents or paragraphs. Our results indicate that the use of context can lead to more informative labels for quality annotation beyond sentence level. 1 Introduction Quality estimation (QE) of machine translation (MT) (Blatz et al., 2004; Specia et al., 2009) is an area that focuses on predicting the quality of new, unseen machine translation data without relying on human references. This is done by training models using features extracted from source and target texts and, when available, from the MT system, along with a quality label for each instance. Most current work on QE is done at the sentence level. A popular application of sentence-level QE is to support post-editing of MT (He et al., 2010). As quality labels, Likert scores have been used for post-editing effort, as well as post-editing time and edit distance between"
2015.eamt-1.17,W12-3156,0,0.102075,"sing automatic metrics such as BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) as quality labels. However, their results highlighted issues with these metrics for the task at hand: the evaluation of the scores predicted in terms of mean error was inconclusive. In most cases, the prediction model only slightly improves over a simple baseline where the average BLEU or TER score of the training documents is assigned to all test documents. Other studies have considered document-level information in order to improve, analyse or au121 tomatically evaluate MT output (not for QE purposes). Carpuat and Simard (2012) report that MT output is overall consistent in its lexical choices, nearly as consistent as manually translated texts. Meyer and Webber (2013) and Li et al. (2014) show that the translation of connectives differs from humans to MT, and that the presence of explicit connectives correlates with higher HTER values. Guzm´an et al. (2014) explore rhetorical structure (RST) trees (Mann and Thompson, 1987) for automatic evaluation of MT into English, outperforming traditional metrics at system-level evaluation. Thus far, no previous work has investigated ways to provide a global quality score for an"
2015.eamt-1.17,P14-1065,0,0.0530227,"Missing"
2015.eamt-1.17,P10-1064,1,0.914336,"Missing"
2015.eamt-1.17,P14-2047,0,0.0164777,"the task at hand: the evaluation of the scores predicted in terms of mean error was inconclusive. In most cases, the prediction model only slightly improves over a simple baseline where the average BLEU or TER score of the training documents is assigned to all test documents. Other studies have considered document-level information in order to improve, analyse or au121 tomatically evaluate MT output (not for QE purposes). Carpuat and Simard (2012) report that MT output is overall consistent in its lexical choices, nearly as consistent as manually translated texts. Meyer and Webber (2013) and Li et al. (2014) show that the translation of connectives differs from humans to MT, and that the presence of explicit connectives correlates with higher HTER values. Guzm´an et al. (2014) explore rhetorical structure (RST) trees (Mann and Thompson, 1987) for automatic evaluation of MT into English, outperforming traditional metrics at system-level evaluation. Thus far, no previous work has investigated ways to provide a global quality score for an entire document that takes into account document structure, without access to reference translations. Previous work on document-level QE use automatic evaluation m"
2015.eamt-1.17,W13-3303,0,0.0260842,"ssues with these metrics for the task at hand: the evaluation of the scores predicted in terms of mean error was inconclusive. In most cases, the prediction model only slightly improves over a simple baseline where the average BLEU or TER score of the training documents is assigned to all test documents. Other studies have considered document-level information in order to improve, analyse or au121 tomatically evaluate MT output (not for QE purposes). Carpuat and Simard (2012) report that MT output is overall consistent in its lexical choices, nearly as consistent as manually translated texts. Meyer and Webber (2013) and Li et al. (2014) show that the translation of connectives differs from humans to MT, and that the presence of explicit connectives correlates with higher HTER values. Guzm´an et al. (2014) explore rhetorical structure (RST) trees (Mann and Thompson, 1987) for automatic evaluation of MT into English, outperforming traditional metrics at system-level evaluation. Thus far, no previous work has investigated ways to provide a global quality score for an entire document that takes into account document structure, without access to reference translations. Previous work on document-level QE use a"
2015.eamt-1.17,P02-1040,0,0.0918502,"and structure of the document and the relationship between its sentences. While certain sentences are considered perfect in isolation, their combination in context may lead to incoherent text. Conversely, while a sentence can be considered poor in isolation, when put in context, it may benefit from information in surrounding sentences, leading to a document that is fit for purpose. Document-level quality prediction is a rather understudied problem. Recent work has looked into document-level prediction (Scarton and Specia, 2014; Soricut and Echihabi, 2010) using automatic metrics such as BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) as quality labels. However, their results highlighted issues with these metrics for the task at hand: the evaluation of the scores predicted in terms of mean error was inconclusive. In most cases, the prediction model only slightly improves over a simple baseline where the average BLEU or TER score of the training documents is assigned to all test documents. Other studies have considered document-level information in order to improve, analyse or au121 tomatically evaluate MT output (not for QE purposes). Carpuat and Simard (2012) report that MT output is overall"
2015.eamt-1.17,P09-2004,0,0.0370577,"omly selected from the full corpus. For PE1 and PE2, only source (English) paragraphs with 3-8 sentences were selected (filter SNUMBER) to ensure that there is enough information beyond sentence-level to be evaluated and make the task feasible for the annotators. These paragraphs were further filtered to select those with cohesive devices. Cohesive devices are linguistic units that play a role in establishing cohesion between clauses, sentences or paragraphs (Halliday and Hasan, 1976). Pronouns and discourse connectives are examples of such devices. A list of pronouns and the connectives from Pitler and Nenkova (2009) was considered for that. Finally, paragraphs were ranked according to the number of cohesive devices they contain and the top 200 paragraphs were selected (filter C-DEV). Table 3 shows the statistics of the initial corpus and the resulting selection after each filter. FULL CORPUS S-NUMBER C-DEV Number of Paragraphs 1, 215 394 200 Number of Cohesive devices 6, 488 3, 329 2, 338 Table 3: WMT13 English source corpus. For the PE1 experiment, the paragraphs in CDEV were randomised. Then, sets containing seven paragraphs each were created. For each set, the sentences of its paragraphs were also ran"
2015.eamt-1.17,potet-etal-2012-collection,0,0.122026,"Missing"
2015.eamt-1.17,2014.eamt-1.21,1,0.877315,"onsider more information than sentence-level quality. This includes, for example, the topic and structure of the document and the relationship between its sentences. While certain sentences are considered perfect in isolation, their combination in context may lead to incoherent text. Conversely, while a sentence can be considered poor in isolation, when put in context, it may benefit from information in surrounding sentences, leading to a document that is fit for purpose. Document-level quality prediction is a rather understudied problem. Recent work has looked into document-level prediction (Scarton and Specia, 2014; Soricut and Echihabi, 2010) using automatic metrics such as BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) as quality labels. However, their results highlighted issues with these metrics for the task at hand: the evaluation of the scores predicted in terms of mean error was inconclusive. In most cases, the prediction model only slightly improves over a simple baseline where the average BLEU or TER score of the training documents is assigned to all test documents. Other studies have considered document-level information in order to improve, analyse or au121 tomatically evaluate MT"
2015.eamt-1.17,2006.amta-papers.25,0,0.41877,"s on predicting the quality of new, unseen machine translation data without relying on human references. This is done by training models using features extracted from source and target texts and, when available, from the MT system, along with a quality label for each instance. Most current work on QE is done at the sentence level. A popular application of sentence-level QE is to support post-editing of MT (He et al., 2010). As quality labels, Likert scores have been used for post-editing effort, as well as post-editing time and edit distance between the MT output and the final version – HTER (Snover et al., 2006). c 2015 The authors. This article is licensed under a Creative Commons 3.0 licence, no derivative works, attribution, CCBY-ND. There are, however, scenarios where quality prediction beyond sentence level is needed, most notably in cases when automatic translations without post-editing are required. This is the case, for example, of quality prediction for an entire product review translation in order to decide whether or not it can be published as is, so that customers speaking other languages can understand it. The quality of a document is often seen as some form of aggregation of the quality"
2015.eamt-1.17,P10-1063,0,0.608392,"han sentence-level quality. This includes, for example, the topic and structure of the document and the relationship between its sentences. While certain sentences are considered perfect in isolation, their combination in context may lead to incoherent text. Conversely, while a sentence can be considered poor in isolation, when put in context, it may benefit from information in surrounding sentences, leading to a document that is fit for purpose. Document-level quality prediction is a rather understudied problem. Recent work has looked into document-level prediction (Scarton and Specia, 2014; Soricut and Echihabi, 2010) using automatic metrics such as BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) as quality labels. However, their results highlighted issues with these metrics for the task at hand: the evaluation of the scores predicted in terms of mean error was inconclusive. In most cases, the prediction model only slightly improves over a simple baseline where the average BLEU or TER score of the training documents is assigned to all test documents. Other studies have considered document-level information in order to improve, analyse or au121 tomatically evaluate MT output (not for QE purposes)"
2015.eamt-1.17,2009.eamt-1.5,1,0.876811,"ht crucial limitations of such metrics for this task, mainly the fact that they disregard the discourse structure of the texts. To better understand these limitations, we designed experiments with human annotators and proposed a way of quantifying differences in translation quality that can only be observed when sentences are judged in the context of entire documents or paragraphs. Our results indicate that the use of context can lead to more informative labels for quality annotation beyond sentence level. 1 Introduction Quality estimation (QE) of machine translation (MT) (Blatz et al., 2004; Specia et al., 2009) is an area that focuses on predicting the quality of new, unseen machine translation data without relying on human references. This is done by training models using features extracted from source and target texts and, when available, from the MT system, along with a quality label for each instance. Most current work on QE is done at the sentence level. A popular application of sentence-level QE is to support post-editing of MT (He et al., 2010). As quality labels, Likert scores have been used for post-editing effort, as well as post-editing time and edit distance between the MT output and the"
2015.eamt-1.17,D14-1025,0,0.053694,"Missing"
2015.eamt-1.17,C14-2028,0,\N,Missing
2015.eamt-1.17,W13-2201,1,\N,Missing
2015.eamt-1.6,P02-1040,0,0.106419,"ch use paraphrasing in TM matching and retrieval. Utiyama et al. (2011) proposed an approach using a finite state transducer. They evaluate the approach with one translator and find that paraphrasing is useful for TM both in terms of precision and recall of the retrieval process. However, their approach limits TM matching to exact matches only. Gupta and Or˘asan (2014) also use paraphrasing at the fuzzy match level and they report an improvement in retrieval and quality of retrieved segments. The quality of retrieved segments was evaluated using the machine translation evaluation metric BLEU (Papineni et al., 2002). Simard and Fujita (2012) used different MT evaluation metrics for similarity calculation as well as for testing the quality of retrieval. For most of the metrics, the authors find that, the metric which is used in evaluation gives better score to itself (e.g. BLEU gives highest score to matches retrieved using BLEU as similarity measure). Keystroke and post-editing time analysis are not new for TM and MT. Keystroke analysis has been used to judge translators’ productivity (Langlais and Lapalme, 2002; Whyman and Somers, 1999). 36 Koponen et al. (2012) suggested that post-editing time reflects"
2015.eamt-1.6,P06-1055,0,0.00717483,"t we provided one more option to translators. Translators can choose among three options: A is better; B is better; or both are equal. 7 translators participated in this experiment. 4 Corpus, Tool and Translators expertise As a TM and test data, we have used EnglishGerman pairs of the Europarl V7.0 (Koehn, 2005) corpus with English as the source language and German as the target language. From this corpus we have filtered out segments of fewer than seven words and greater than 40 words, to create the TM and test datasets. Tokenization of the English data was done using the Berkeley Tokenizer (Petrov et al., 2006). We have used the lexical and phrasal paraphrases from the PPDB corpus (Ganitkevitch et al., 2013) of L size. In these experiments, we have not paraphrased any capitalised words (but we lowercase them for both baseline and paraphrasing similarities calculation). This is to avoid paraphrasing any named entities. Table 1 shows our corpus statistics. The translators involved in Segments Source words Target words TM 1565194 37824634 36267909 Test Set 9981 240916 230620 Table 1: Corpus Statistics our experiments were third year bachelor or masters translation students who were native speakers of G"
2015.eamt-1.6,1999.mtsummit-1.48,0,0.178112,"om scratch when an exact match is not available. However, this retrieval process is still limited to editdistance based measures operating on surface form c 2015 The authors. This article is licensed under a Creative  Commons 3.0 licence, no derivative works, attribution, CCBY-ND. 35 Several researchers have used semantic or syntactic information in TMs, but their evaluations were shallow and most of the time limited to subjective evaluation carried out by the authors. This makes it hard to judge how much a semantically informed TM matching system can benefit a translator. Existing research (Planas and Furuse, 1999; Hod´asz and Pohl, 2005; Pekar and Mitkov, 2007; Mitkov, 2008) pointed out the need for similarity 1 http://www.omegat.org calculations in TMs beyond surface form comparisons. Both Planas and Furuse (1999) and Hodasz and Pohl (2005) proposed to use lemma and parts of speech along with surface form comparison. Hodasz and Pohl (2005) also extend the matching process to a sentence skeleton where noun phrases are either tagged by a translator or by a heuristic NP aligner developed for English-Hungarian translation. Planas and Furuse (1999) tested a prototype model on 50 sentences from the softwar"
2015.eamt-1.6,aziz-etal-2012-pet,0,0.0163664,"ificantly improves the retrieval results. We have also observed that there are different paraphrases used to bring about this improvement. In the interval [70, 85), 169 different paraphrases are used to retrieve 98 additional segments. To check the quality of the retrieved segments human evaluations are carried out. The sets’ distribution for human evaluation is given in the Table 3. The sets contain randomly selected segments from the additionally retrieved segments using paraphrasing which changed their top ranking.2 TH Set1 Set2 Total 4.1 Familiarisation with the Tool We used the PET tool (Aziz et al., 2012) for all our human experiments. However, settings were changed depending on the experiment. To familiarise translators with the PET tool we carried out a pilot experiment before the actual experiment with the Europarl corpus. This experiment was 100 117 16 13.67 9 24 14 100 2 5 7 [85, 100) 6 4 10 [70, 85) 6 7 13 Total 14 16 30 Table 3: Test Sets for Human Experiments 2 The sets are constructed so that a translator can post-edit a file in one sitting. There is no differentiation between the evaluations based on sets and all evaluations are carried out in both sets in a similar fashion with diff"
2015.eamt-1.6,2012.amta-papers.26,0,0.0678513,"M matching and retrieval. Utiyama et al. (2011) proposed an approach using a finite state transducer. They evaluate the approach with one translator and find that paraphrasing is useful for TM both in terms of precision and recall of the retrieval process. However, their approach limits TM matching to exact matches only. Gupta and Or˘asan (2014) also use paraphrasing at the fuzzy match level and they report an improvement in retrieval and quality of retrieved segments. The quality of retrieved segments was evaluated using the machine translation evaluation metric BLEU (Papineni et al., 2002). Simard and Fujita (2012) used different MT evaluation metrics for similarity calculation as well as for testing the quality of retrieval. For most of the metrics, the authors find that, the metric which is used in evaluation gives better score to itself (e.g. BLEU gives highest score to matches retrieved using BLEU as similarity measure). Keystroke and post-editing time analysis are not new for TM and MT. Keystroke analysis has been used to judge translators’ productivity (Langlais and Lapalme, 2002; Whyman and Somers, 1999). 36 Koponen et al. (2012) suggested that post-editing time reflects the cognitive effort in p"
2015.eamt-1.6,W14-3348,0,0.0285348,"Missing"
2015.eamt-1.6,2006.amta-papers.25,0,0.165174,"Missing"
2015.eamt-1.6,N13-1092,0,0.0373631,"s better; B is better; or both are equal. 7 translators participated in this experiment. 4 Corpus, Tool and Translators expertise As a TM and test data, we have used EnglishGerman pairs of the Europarl V7.0 (Koehn, 2005) corpus with English as the source language and German as the target language. From this corpus we have filtered out segments of fewer than seven words and greater than 40 words, to create the TM and test datasets. Tokenization of the English data was done using the Berkeley Tokenizer (Petrov et al., 2006). We have used the lexical and phrasal paraphrases from the PPDB corpus (Ganitkevitch et al., 2013) of L size. In these experiments, we have not paraphrased any capitalised words (but we lowercase them for both baseline and paraphrasing similarities calculation). This is to avoid paraphrasing any named entities. Table 1 shows our corpus statistics. The translators involved in Segments Source words Target words TM 1565194 37824634 36267909 Test Set 9981 240916 230620 Table 1: Corpus Statistics our experiments were third year bachelor or masters translation students who were native speakers of German with English language level C1, in the age group of 21 to 40 years with a majority of female"
2015.eamt-1.6,R11-1014,0,0.0138269,"on metrics for similarity calculation as well as for testing the quality of retrieval. For most of the metrics, the authors find that, the metric which is used in evaluation gives better score to itself (e.g. BLEU gives highest score to matches retrieved using BLEU as similarity measure). Keystroke and post-editing time analysis are not new for TM and MT. Keystroke analysis has been used to judge translators’ productivity (Langlais and Lapalme, 2002; Whyman and Somers, 1999). 36 Koponen et al. (2012) suggested that post-editing time reflects the cognitive effort in post-editing the MT output. Sousa et al. (2011) evaluated different MT system performances against translating from scratch. Their study also concluded that subjective evaluations of MT system output correlate with the post-editing time needed. Zampieri and Vela (2014) used post-editing time to compare TM and MT translations. 3 Our Approach and Experiments We have used the approach presented in Gupta and Or˘asan (2014) to include paraphrasing in the TM matching and retrieval process. The approach classifies paraphrases into different types for efficient implementation based on the matching of the words between the source and corresponding"
2015.eamt-1.6,2014.eamt-1.2,1,0.60298,"Missing"
2015.eamt-1.6,2011.mtsummit-papers.37,0,0.166221,"ed was considered usable if less than half of the words required editing to match the input sentence. The authors concluded that the approach gives more usable results compared to Trados Workbench used as a baseline. Hodasz and Pohl (2005) claimed that their approach stores simplified patterns and hence makes it more probable to find a match in the TM. Pekar and Mitkov (2007) presented an approach based on syntactic transformation rules. On evaluation of the prototype model using a query sentence, the authors found that the syntactic rules help in retrieving better segments. Recently, work by Utiyama et al. (2011) and Gupta and Or˘asan (2014) presented approaches which use paraphrasing in TM matching and retrieval. Utiyama et al. (2011) proposed an approach using a finite state transducer. They evaluate the approach with one translator and find that paraphrasing is useful for TM both in terms of precision and recall of the retrieval process. However, their approach limits TM matching to exact matches only. Gupta and Or˘asan (2014) also use paraphrasing at the fuzzy match level and they report an improvement in retrieval and quality of retrieved segments. The quality of retrieved segments was evaluated"
2015.eamt-1.6,2005.mtsummit-papers.11,0,0.039766,"etter. 17 translators participated in this experiment. Finally, the decision of whether ‘ED is better’ or ‘PP is better’ is made on the basis of how many translators choose one over the other. 3.3 Subjective Evaluation with Three Options (SE3) This evaluation is similar to Evaluation SE2 except that we provided one more option to translators. Translators can choose among three options: A is better; B is better; or both are equal. 7 translators participated in this experiment. 4 Corpus, Tool and Translators expertise As a TM and test data, we have used EnglishGerman pairs of the Europarl V7.0 (Koehn, 2005) corpus with English as the source language and German as the target language. From this corpus we have filtered out segments of fewer than seven words and greater than 40 words, to create the TM and test datasets. Tokenization of the English data was done using the Berkeley Tokenizer (Petrov et al., 2006). We have used the lexical and phrasal paraphrases from the PPDB corpus (Ganitkevitch et al., 2013) of L size. In these experiments, we have not paraphrased any capitalised words (but we lowercase them for both baseline and paraphrasing similarities calculation). This is to avoid paraphrasing"
2015.eamt-1.6,2012.amta-wptp.2,0,0.0130456,"hine translation evaluation metric BLEU (Papineni et al., 2002). Simard and Fujita (2012) used different MT evaluation metrics for similarity calculation as well as for testing the quality of retrieval. For most of the metrics, the authors find that, the metric which is used in evaluation gives better score to itself (e.g. BLEU gives highest score to matches retrieved using BLEU as similarity measure). Keystroke and post-editing time analysis are not new for TM and MT. Keystroke analysis has been used to judge translators’ productivity (Langlais and Lapalme, 2002; Whyman and Somers, 1999). 36 Koponen et al. (2012) suggested that post-editing time reflects the cognitive effort in post-editing the MT output. Sousa et al. (2011) evaluated different MT system performances against translating from scratch. Their study also concluded that subjective evaluations of MT system output correlate with the post-editing time needed. Zampieri and Vela (2014) used post-editing time to compare TM and MT translations. 3 Our Approach and Experiments We have used the approach presented in Gupta and Or˘asan (2014) to include paraphrasing in the TM matching and retrieval process. The approach classifies paraphrases into dif"
2015.eamt-1.6,W14-0314,1,0.730688,"es highest score to matches retrieved using BLEU as similarity measure). Keystroke and post-editing time analysis are not new for TM and MT. Keystroke analysis has been used to judge translators’ productivity (Langlais and Lapalme, 2002; Whyman and Somers, 1999). 36 Koponen et al. (2012) suggested that post-editing time reflects the cognitive effort in post-editing the MT output. Sousa et al. (2011) evaluated different MT system performances against translating from scratch. Their study also concluded that subjective evaluations of MT system output correlate with the post-editing time needed. Zampieri and Vela (2014) used post-editing time to compare TM and MT translations. 3 Our Approach and Experiments We have used the approach presented in Gupta and Or˘asan (2014) to include paraphrasing in the TM matching and retrieval process. The approach classifies paraphrases into different types for efficient implementation based on the matching of the words between the source and corresponding paraphrase. Using this approach, the fuzzy match score between segments can be calculated in polynomial time despite the inclusion of paraphrases. The method uses dynamic programming along with greedy approximation. The me"
2015.eamt-1.6,2012.eamt-1.31,0,\N,Missing
2015.eamt-1.6,2012.tc-1.5,0,\N,Missing
2020.aacl-demo.5,2020.coling-demos.2,1,0.339677,"Missing"
2020.aacl-demo.5,R13-1023,1,0.858375,"Missing"
2020.aacl-demo.5,P14-5010,0,0.00252469,"tenance record datasets generally contain free text fields describing issues and actions, as in the instances presented in Table 1. Most standard NLP pipelines for pre-processing and annotation are trained on standard contemporary corpora (e.g. 3. The development and evaluation of a number of Python (pre-)processing tools available at MaintNet including stop word removal, stemmers, lemmatizers, POS tagging, and clustering. We carry out an evaluation of MaintNet’s spell checkers and POS taggers comparing them to off-the-shelf NLP packages such as NLTK (Bird et al., 2009) and Stanford Core NLP (Manning et al., 2014), as well as clustering methods. 1 Available at: fa3019/MaintNet/ https://people.rit.edu/ 26 Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing: System Demonstrations, pages 26–32 c December 4 - 7, 2020. 2020 Association for Computational Linguistics ID 111552 111563 111574 111585 Issue/Problem R/H FWD UPPER BAFL SEAL NEEDS TO BE RESECURED CAP SCREWE MISSING, L/H ENG #4 BAFLE CYL #1 BAFFLE CRACKED AT SCREW SUPPORT & FWD BAFL BELOWE #1 #3 FWD PUSH ROD TUBE GSK L"
2020.aacl-demo.5,H92-1116,0,0.275192,"ance or a categorization of the issue type. To address this issue, we implemented several pre-processing steps to clean and extract as much information from logbooks as possible. The pipeline is shown in Figure 2. The Python scripts for all components in this pipeline are made available through Maintnet. The process starts with text normalization, including lowercasing, stop word and punctuation removal, and treating special characters with NLTK’s (Bird et al., 2009) regular expression library, followed by tokenization (NLTK tokenizer), stemming (Snowball Stemmer), and lemmatization (WordNet (Miller, 1992)). With use of the collected morphosyntactic information, POS annotation is carried out with the NLTK POS tagger. Term frequency-inverse document frequency (TF-IDF) is obtained using the gensim tfidf model (Rehurek and Sojka, 2010). Our analysis of the logbooks found that many of the misspellings and abbreviations lead to incorrect or non-existent dictionary look ups. To overcome this issue, we explored various state-of-the-art spellcheckers including Enchant2 , Pyspellchecker3 , Symspellpy4 , and Autocorrect5 . Given the inaccuracy of existing techniques, we developed methods of correcting sy"
2020.coling-demos.2,H92-1116,0,0.682603,"Missing"
2020.emnlp-main.470,W18-4411,0,0.0118368,"ing, pages 5838–5844, c November 16–20, 2020. 2020 Association for Computational Linguistics 2 Related Work 3 There is a growing interest in the development of computational models to identify offensive content online. Early approaches relied heavily on feature engineering combined with traditional machine learning classifiers such as naive bayes and support vector machines (Xu et al., 2012; Dadvar et al., 2013). More recently, neural networks such as LSTMs, bidirectional LSTMs, and GRUs combined with word embeddings have proved to outperform traditional machine learning methods in this task (Aroyehun and Gelbukh, 2018; Majumder et al., 2018). In the last couple of years, transformer models like ELMO (Peters et al., 2018) and BERT (Devlin et al., 2019) have been applied to offensive language identification achieving competitive scores and topping the leaderboards in recent shared tasks (Liu et al., 2019; Ranasinghe et al., 2019). Most of these approaches use existing pretrained transformer models which can also be used as text classification models. The clear majority of studies on this topic deal with English (Malmasi and Zampieri, 2017; Yao et al., 2019; Ridenhour et al., 2020) partially motivated by the"
2020.emnlp-main.470,S19-2007,0,0.437721,"ap OLID level A (offensive vs. non-offensive) to labels in the other three datasets. OLID’s annotation model is intended to serve as a general-purpose model for multiple abusive language detection subtasks (Waseem et al., 2017). The transfer learning strategy used in this paper provides us with an interesting opportunity to evaluate how closely the OLID labels relate to the classes in datasets annotated using different guidelines and sub-task definitions (e.g. aggression and hate speech). The Hindi dataset (Mandl et al., 2019) was used in the HASOC 2019 shared task, while the Spanish dataset (Basile et al., 2019) was used in SemEval2019 Task 5 (HatEval). They both contain Twitter data and two labels. The Bengali dataset (Bhattacharya et al., 2020) was used in the TRAC-2 shared task (Kumar et al., 2020) on aggression identification. It is different than the other three datasets in terms of domain (Facebook instead of Twitter) and set of labels (three classes instead of binary), allowing us to compare the performance of cross-lingual embeddings on off-domain data and off-task data. 5839 Lang. Bengali Inst. 4,000 S Labels F overtly aggressive, covertly aggressive, non aggressive English 14,100 T offensiv"
2020.emnlp-main.470,2020.trac-1.25,0,0.0230536,"Missing"
2020.emnlp-main.470,2020.lrec-1.758,0,0.209019,"Missing"
2020.emnlp-main.470,2019.jeptalnrecital-court.21,0,0.126573,"et al., 2019; Ranasinghe et al., 2019). Most of these approaches use existing pretrained transformer models which can also be used as text classification models. The clear majority of studies on this topic deal with English (Malmasi and Zampieri, 2017; Yao et al., 2019; Ridenhour et al., 2020) partially motivated by the availability English resources (e.g. corpora, lexicon, and pre-trained models). In recent years, a number of studies have been published on other languages such as Arabic (Mubarak et al., 2020), Danish (Sigurbergsson and Derczynski, 2020), Dutch (Tulkens et al., 2016), French (Chiril et al., 2019), Greek (Pitenis et al., 2020), Italian (Poletto et al., 2017), Portuguese (Fortuna et al., 2019), Slovene (Fiˇser et al., 2017), and Turkish (C¸o¨ ltekin, 2020) creating new datasets and resources for these languages. Recent competitions organized in 2020 such as TRAC (Kumar et al., 2020) and OffensEval (Zampieri et al., 2020) have included datasets in multiple languages providing participants with the opportunity to explore cross-lingual learning models opening exciting new avenues for research on languages other than English and, in particular, on low-resource languages. The aforementioned"
2020.emnlp-main.470,P19-4007,0,0.0711394,"Missing"
2020.emnlp-main.470,N19-1423,0,0.213122,"he development of computational models to identify offensive content online. Early approaches relied heavily on feature engineering combined with traditional machine learning classifiers such as naive bayes and support vector machines (Xu et al., 2012; Dadvar et al., 2013). More recently, neural networks such as LSTMs, bidirectional LSTMs, and GRUs combined with word embeddings have proved to outperform traditional machine learning methods in this task (Aroyehun and Gelbukh, 2018; Majumder et al., 2018). In the last couple of years, transformer models like ELMO (Peters et al., 2018) and BERT (Devlin et al., 2019) have been applied to offensive language identification achieving competitive scores and topping the leaderboards in recent shared tasks (Liu et al., 2019; Ranasinghe et al., 2019). Most of these approaches use existing pretrained transformer models which can also be used as text classification models. The clear majority of studies on this topic deal with English (Malmasi and Zampieri, 2017; Yao et al., 2019; Ridenhour et al., 2020) partially motivated by the availability English resources (e.g. corpora, lexicon, and pre-trained models). In recent years, a number of studies have been published"
2020.emnlp-main.470,W17-3007,0,0.0302786,"Missing"
2020.emnlp-main.470,W19-3510,0,0.0366163,"rmer models which can also be used as text classification models. The clear majority of studies on this topic deal with English (Malmasi and Zampieri, 2017; Yao et al., 2019; Ridenhour et al., 2020) partially motivated by the availability English resources (e.g. corpora, lexicon, and pre-trained models). In recent years, a number of studies have been published on other languages such as Arabic (Mubarak et al., 2020), Danish (Sigurbergsson and Derczynski, 2020), Dutch (Tulkens et al., 2016), French (Chiril et al., 2019), Greek (Pitenis et al., 2020), Italian (Poletto et al., 2017), Portuguese (Fortuna et al., 2019), Slovene (Fiˇser et al., 2017), and Turkish (C¸o¨ ltekin, 2020) creating new datasets and resources for these languages. Recent competitions organized in 2020 such as TRAC (Kumar et al., 2020) and OffensEval (Zampieri et al., 2020) have included datasets in multiple languages providing participants with the opportunity to explore cross-lingual learning models opening exciting new avenues for research on languages other than English and, in particular, on low-resource languages. The aforementioned deep learning methods require large annotated datasets to perform well which is not always availa"
2020.emnlp-main.470,2020.semeval-1.274,0,0.0987396,"Missing"
2020.emnlp-main.470,W18-4401,1,0.923547,"Missing"
2020.emnlp-main.470,2020.trac-1.1,1,0.896096,"Missing"
2020.emnlp-main.470,S19-2011,0,0.0570263,"machine learning classifiers such as naive bayes and support vector machines (Xu et al., 2012; Dadvar et al., 2013). More recently, neural networks such as LSTMs, bidirectional LSTMs, and GRUs combined with word embeddings have proved to outperform traditional machine learning methods in this task (Aroyehun and Gelbukh, 2018; Majumder et al., 2018). In the last couple of years, transformer models like ELMO (Peters et al., 2018) and BERT (Devlin et al., 2019) have been applied to offensive language identification achieving competitive scores and topping the leaderboards in recent shared tasks (Liu et al., 2019; Ranasinghe et al., 2019). Most of these approaches use existing pretrained transformer models which can also be used as text classification models. The clear majority of studies on this topic deal with English (Malmasi and Zampieri, 2017; Yao et al., 2019; Ridenhour et al., 2020) partially motivated by the availability English resources (e.g. corpora, lexicon, and pre-trained models). In recent years, a number of studies have been published on other languages such as Arabic (Mubarak et al., 2020), Danish (Sigurbergsson and Derczynski, 2020), Dutch (Tulkens et al., 2016), French (Chiril et al"
2020.emnlp-main.470,W18-4423,0,0.133402,"Missing"
2020.emnlp-main.470,malmasi-zampieri-2017-detecting,1,0.872028,"roved to outperform traditional machine learning methods in this task (Aroyehun and Gelbukh, 2018; Majumder et al., 2018). In the last couple of years, transformer models like ELMO (Peters et al., 2018) and BERT (Devlin et al., 2019) have been applied to offensive language identification achieving competitive scores and topping the leaderboards in recent shared tasks (Liu et al., 2019; Ranasinghe et al., 2019). Most of these approaches use existing pretrained transformer models which can also be used as text classification models. The clear majority of studies on this topic deal with English (Malmasi and Zampieri, 2017; Yao et al., 2019; Ridenhour et al., 2020) partially motivated by the availability English resources (e.g. corpora, lexicon, and pre-trained models). In recent years, a number of studies have been published on other languages such as Arabic (Mubarak et al., 2020), Danish (Sigurbergsson and Derczynski, 2020), Dutch (Tulkens et al., 2016), French (Chiril et al., 2019), Greek (Pitenis et al., 2020), Italian (Poletto et al., 2017), Portuguese (Fortuna et al., 2019), Slovene (Fiˇser et al., 2017), and Turkish (C¸o¨ ltekin, 2020) creating new datasets and resources for these languages. Recent compe"
2020.emnlp-main.470,W17-3008,0,0.0699086,"Missing"
2020.emnlp-main.470,P19-2051,0,0.0310726,"Missing"
2020.emnlp-main.470,S19-2008,0,0.0504713,"Missing"
2020.emnlp-main.470,N18-1202,0,0.0499858,"here is a growing interest in the development of computational models to identify offensive content online. Early approaches relied heavily on feature engineering combined with traditional machine learning classifiers such as naive bayes and support vector machines (Xu et al., 2012; Dadvar et al., 2013). More recently, neural networks such as LSTMs, bidirectional LSTMs, and GRUs combined with word embeddings have proved to outperform traditional machine learning methods in this task (Aroyehun and Gelbukh, 2018; Majumder et al., 2018). In the last couple of years, transformer models like ELMO (Peters et al., 2018) and BERT (Devlin et al., 2019) have been applied to offensive language identification achieving competitive scores and topping the leaderboards in recent shared tasks (Liu et al., 2019; Ranasinghe et al., 2019). Most of these approaches use existing pretrained transformer models which can also be used as text classification models. The clear majority of studies on this topic deal with English (Malmasi and Zampieri, 2017; Yao et al., 2019; Ridenhour et al., 2020) partially motivated by the availability English resources (e.g. corpora, lexicon, and pre-trained models). In recent years, a number"
2020.emnlp-main.470,W17-3012,0,0.014046,"rgeted profanity; C: Offensive language target identification - individual vs. group vs. other. We chose OLID due to the flexibility provided by its hierarchical annotation model that considers multiple types of offensive content in a single taxonomy (e.g. targeted insults to a group are often hate speech whereas targeted insults to an individual are often cyberbulling). This allows us to map OLID level A (offensive vs. non-offensive) to labels in the other three datasets. OLID’s annotation model is intended to serve as a general-purpose model for multiple abusive language detection subtasks (Waseem et al., 2017). The transfer learning strategy used in this paper provides us with an interesting opportunity to evaluate how closely the OLID labels relate to the classes in datasets annotated using different guidelines and sub-task definitions (e.g. aggression and hate speech). The Hindi dataset (Mandl et al., 2019) was used in the HASOC 2019 shared task, while the Spanish dataset (Basile et al., 2019) was used in SemEval2019 Task 5 (HatEval). They both contain Twitter data and two labels. The Bengali dataset (Bhattacharya et al., 2020) was used in the TRAC-2 shared task (Kumar et al., 2020) on aggression"
2020.emnlp-main.470,P19-1493,0,0.0210707,",000 T hate offensive, non hate-offensive Spanish 6,600 T hateful, non-hateful Table 1: Instances (Inst.), source (S) and labels in all datasets. F stands for Facebook and T for Twitter. 4 Methodology Transformer models have been used successfully for various NLP tasks (Devlin et al., 2019). Most of the tasks were focused on English language due to the fact the most of the pre-trained transformer models were trained on English data. Even though, there were several multilingual models like BERTm (Devlin et al., 2019) there was much speculations about its ability to represent all the languages (Pires et al., 2019) and although BERT-m model showed some cross-lingual characteristics it has not been trained on crosslingual data (Karthikeyan et al., 2020). The motivation behind this methodology was the recently released cross-lingual transformer models - XLM-R (Conneau et al., 2019) which has been trained on 104 languages. The interesting fact about XLM-R is that it is very compatible in monolingual benchmarks while achieving the best results in cross-lingual benchmarks at the same time (Conneau et al., 2019). The main idea of the methodology is that we train a classification model on a resource rich, typi"
2020.emnlp-main.470,N12-1084,0,0.096539,"nd the English model will be freely available to everyone interested in working on low-resource languages using the same methodology. 5838 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 5838–5844, c November 16–20, 2020. 2020 Association for Computational Linguistics 2 Related Work 3 There is a growing interest in the development of computational models to identify offensive content online. Early approaches relied heavily on feature engineering combined with traditional machine learning classifiers such as naive bayes and support vector machines (Xu et al., 2012; Dadvar et al., 2013). More recently, neural networks such as LSTMs, bidirectional LSTMs, and GRUs combined with word embeddings have proved to outperform traditional machine learning methods in this task (Aroyehun and Gelbukh, 2018; Majumder et al., 2018). In the last couple of years, transformer models like ELMO (Peters et al., 2018) and BERT (Devlin et al., 2019) have been applied to offensive language identification achieving competitive scores and topping the leaderboards in recent shared tasks (Liu et al., 2019; Ranasinghe et al., 2019). Most of these approaches use existing pretrained"
2020.emnlp-main.470,2020.lrec-1.629,1,0.800586,"l., 2019). Most of these approaches use existing pretrained transformer models which can also be used as text classification models. The clear majority of studies on this topic deal with English (Malmasi and Zampieri, 2017; Yao et al., 2019; Ridenhour et al., 2020) partially motivated by the availability English resources (e.g. corpora, lexicon, and pre-trained models). In recent years, a number of studies have been published on other languages such as Arabic (Mubarak et al., 2020), Danish (Sigurbergsson and Derczynski, 2020), Dutch (Tulkens et al., 2016), French (Chiril et al., 2019), Greek (Pitenis et al., 2020), Italian (Poletto et al., 2017), Portuguese (Fortuna et al., 2019), Slovene (Fiˇser et al., 2017), and Turkish (C¸o¨ ltekin, 2020) creating new datasets and resources for these languages. Recent competitions organized in 2020 such as TRAC (Kumar et al., 2020) and OffensEval (Zampieri et al., 2020) have included datasets in multiple languages providing participants with the opportunity to explore cross-lingual learning models opening exciting new avenues for research on languages other than English and, in particular, on low-resource languages. The aforementioned deep learning methods require"
2020.emnlp-main.470,2020.trac-1.9,0,0.221387,"t systems in TRAC-2 for Bengali, HASOC for Hindi, HatEval for Spanish in terms of weighted and macro F1 score according to the metrics reported by the task organizers - TRAC-2 reported only macro F1, HatEval reported only weighted F1, and HASOC reported both. Finally, we evaluate the improvement of the transfer learning strategy in the performance of both BERT and XLM-R. We present the results along with the majority class baseline for each language in Table 2. TL indicates that the model used the inter language transfer learning strategy described in Subsection 4.2. Language Model XLM-R (TL) Risch and Krestel (2020) Bengali BERT-m (TL) XLM-R BERT-m Baseline XLM-R (TL) BERT-m (TL) Hindi Bashar and Nayak (2019) XLM-R BERT-m Baseline XLM-R (TL) BERT-m (TL) Vega et al. (2019) Spanish P´erez and Luque (2019) XLM-R BERT-m Baseline M F1 0.8415 0.8219 0.8197 0.8142 0.8132 0.2498 0.8568 0.8211 0.8149 0.8061 0.8025 0.3510 0.7513 0.7319 W F1 0.8423 0.8231 0.8188 0.8157 0.4491 0.8580 0.8220 0.8202 0.8072 0.8030 0.3798 0.7591 0.7385 0.7300 0.7300 0.7224 0.7265 0.7215 0.7234 0.3700 0.4348 Table 2: Results ordered by macro (M) F1 for Bengali and weighted (W) F1 for Hindi and Spanish. For Hindi, transfer learning with X"
2020.emnlp-main.470,N19-1144,1,0.940249,"languages. In this paper, we address the problem of data scarcity in offensive language identification by using transfer learning and crosslingual transformers from a resource rich language like English to three other languages: Bengali, Hindi, and Spanish. Data We acquired datasets in English and three other languages: Bengali, Hindi, and Spanish (listed in Table 1). The four datasets have been used in shared tasks in 2019 and 2020 allowing us to compare the performance of our methods to other approaches. As our English dataset, we chose the Offensive Language Identification Dataset (OLID) (Zampieri et al., 2019a), used in the SemEval-2019 Task 6 (OffensEval) (Zampieri et al., 2019b). OLID is arguably one of the most popular offensive language datasets. It contains manually annotated tweets with the following three-level taxonomy and labels: A: Offensive language identification - offensive vs. non-offensive; B: Categorization of offensive language - targeted insult or thread vs. untargeted profanity; C: Offensive language target identification - individual vs. group vs. other. We chose OLID due to the flexibility provided by its hierarchical annotation model that considers multiple types of offensive"
2020.emnlp-main.470,S19-2010,1,0.939536,"languages. In this paper, we address the problem of data scarcity in offensive language identification by using transfer learning and crosslingual transformers from a resource rich language like English to three other languages: Bengali, Hindi, and Spanish. Data We acquired datasets in English and three other languages: Bengali, Hindi, and Spanish (listed in Table 1). The four datasets have been used in shared tasks in 2019 and 2020 allowing us to compare the performance of our methods to other approaches. As our English dataset, we chose the Offensive Language Identification Dataset (OLID) (Zampieri et al., 2019a), used in the SemEval-2019 Task 6 (OffensEval) (Zampieri et al., 2019b). OLID is arguably one of the most popular offensive language datasets. It contains manually annotated tweets with the following three-level taxonomy and labels: A: Offensive language identification - offensive vs. non-offensive; B: Categorization of offensive language - targeted insult or thread vs. untargeted profanity; C: Offensive language target identification - individual vs. group vs. other. We chose OLID due to the flexibility provided by its hierarchical annotation model that considers multiple types of offensive"
2020.emnlp-main.470,2020.lrec-1.430,0,0.101349,"ompetitive scores and topping the leaderboards in recent shared tasks (Liu et al., 2019; Ranasinghe et al., 2019). Most of these approaches use existing pretrained transformer models which can also be used as text classification models. The clear majority of studies on this topic deal with English (Malmasi and Zampieri, 2017; Yao et al., 2019; Ridenhour et al., 2020) partially motivated by the availability English resources (e.g. corpora, lexicon, and pre-trained models). In recent years, a number of studies have been published on other languages such as Arabic (Mubarak et al., 2020), Danish (Sigurbergsson and Derczynski, 2020), Dutch (Tulkens et al., 2016), French (Chiril et al., 2019), Greek (Pitenis et al., 2020), Italian (Poletto et al., 2017), Portuguese (Fortuna et al., 2019), Slovene (Fiˇser et al., 2017), and Turkish (C¸o¨ ltekin, 2020) creating new datasets and resources for these languages. Recent competitions organized in 2020 such as TRAC (Kumar et al., 2020) and OffensEval (Zampieri et al., 2020) have included datasets in multiple languages providing participants with the opportunity to explore cross-lingual learning models opening exciting new avenues for research on languages other than English and, i"
2020.emnlp-main.470,S19-2079,0,0.0508149,"Missing"
2020.loresmt-1.3,W17-7618,0,0.0442044,"with a comparable socio-cultural and linguistic embedding, for example Wolof (non-Mande), which is comparable in terms of number of speakers, borrowings from Arabic and French influence, and oral traditions. The next section will provide more details on digital resources and describe the process of exploring and collecting data and choosing parallel corpora for the training of the NMT model. 3 3.1 Data Collection Bambara Corpora We discovered that there has been no prior development of automatic translation of Bambara, despite a relatively large volume of research on the language (Culy, 1985; Aplonova and Tyers, 2017; Aplonova, 2018). As a pilot study for assessing the potential for automatic translation of Bambara, Leventhal et al. (2020) crowdsourced a small set of written or oral translations from French to Bambara. Additional work was carried out exploring novel crowdsourcing strategies for data collection in Mali Luger et al. (2020). The Corpus Bambara de Référence (Vydrin et al., 2011) is the largest collection of electronic texts in Bambara. It includes scanned and textbased electronic formats. A number of parallel texts based on this data exist. For example, Vydrin (2018) analyzed Bambara’s separa"
2020.loresmt-1.3,2020.acl-srw.22,0,0.0330747,"h), of which a large percentage are only stubs. Of the small number of full articles, most do not consistently employ the standard orthography of Bambara. A selection of those, however, was prepared to be used as monolingual data for MT data augmentation. Most African NMT studies have been based on the JW300 corpus (Agi´c and Vuli´c, 2019), e.g. most of the Masakhane benchmarks (∀ et al., 2020). JW300 only contains less than 200 sentences of Dyula, a closely related language to Bambara that it is mutually intelligible with. It might be useful for future cross-lingual studies (Wu et al., 2019; Goyal et al., 2020), but in order to avoid interference between languages, we focus on Bambara data exclusively in this first study. The most promising for our NMT approach was a dictionary data set from SIL Mali5 with examples of sentences used to demonstrate word usage in Spanish, French, English, and Bambara; and a tri-lingual health guide titled “Where there is no doctor.6 ” Detailed corpus statistics are listed in Table 1. chapters files paragraphs unigrams bigrams trigrams stopwords Bambara French English 3,548 2,023 2,158 4,847 2,021 2,146 4,855 2,021 2,158 9,336 8,209 26,430 5,816 147 27 336 9,367 9,893"
2020.loresmt-1.3,2020.acl-main.149,0,0.0188846,"e word-based model cannot resolve out-of-vocabulary words, and the characterlevel model struggled with word composition. With BPE, smaller subwords seem to perform slightly better than larger ones. BPE dropout (Provilkov et al., 2019), which was previously reported to be helpful for low-resource MT (Richburg et al., 2020), did not increase the quality of the results. We observe a trend towards higher scores for translations into Bambara than in the reverse direction, but this cross-lingual comparison has to be taken with a grain of salt, since it is influenced by source and target complexity (Bugliarello et al., 2020). Ambiguities on the Bambara side, such as the gender of pronouns illustrated in the example in Section 3.2, might make translation into English and French particularly difficult. Hyperparameters Our NMT is a transformer (Vaswani et al., 2017) of appropriate size for a relatively smaller training dataset (van Biljon et al., 2020). It has six layers with four attention heads for encoder and decoder, the transformer layer has a size of 1024, and the hidden layer size 256, the embeddings have 256 units. Embeddings and vocabularies are not shared across languages, but the softmax layer weights are"
2020.loresmt-1.3,D19-3019,1,0.838895,"in Section 3.2, might make translation into English and French particularly difficult. Hyperparameters Our NMT is a transformer (Vaswani et al., 2017) of appropriate size for a relatively smaller training dataset (van Biljon et al., 2020). It has six layers with four attention heads for encoder and decoder, the transformer layer has a size of 1024, and the hidden layer size 256, the embeddings have 256 units. Embeddings and vocabularies are not shared across languages, but the softmax layer weights are tied to the output embedding weights. The model is implemented with the Joey NMT framework (Kreutzer et al., 2019) based on PyTorch (Paszke et al., 2019). Training runs for 120 epochs in batches of 1024 tokens each. The ADAM optimizer (Kingma and Ba, 2014) is used with a constant learning rate of 0.0004 to update model weights. This setting was found to be best to tune for highest BLEU, compared to decaying or warmup-cooldown learning rate scheduling. For regularization, we experimented with dropout and label smoothing (Szegedy et al., 2016). The best values were 0.1 for dropout and 0.2 for label smoothing across the board. For inference, beam search with width of 5 is used. The remaining hyperparameters"
2020.loresmt-1.3,2020.winlp-1.40,0,0.0354146,"povi´c, 2015) computed with SacreBLEU (Post, 2018).10 Tables 2 and 3 show the results for French and English translations respectively. We find that word- and character-level modeling performs sub par compared to subword-level segmentation, which is in line with previous work on low-resource MT. The word-based model cannot resolve out-of-vocabulary words, and the characterlevel model struggled with word composition. With BPE, smaller subwords seem to perform slightly better than larger ones. BPE dropout (Provilkov et al., 2019), which was previously reported to be helpful for low-resource MT (Richburg et al., 2020), did not increase the quality of the results. We observe a trend towards higher scores for translations into Bambara than in the reverse direction, but this cross-lingual comparison has to be taken with a grain of salt, since it is influenced by source and target complexity (Bugliarello et al., 2020). Ambiguities on the Bambara side, such as the gender of pronouns illustrated in the example in Section 3.2, might make translation into English and French particularly difficult. Hyperparameters Our NMT is a transformer (Vaswani et al., 2017) of appropriate size for a relatively smaller training"
2020.loresmt-1.3,P16-1162,0,0.0776547,"tences, the validation set of 270 sentences, the test set of 268 sentences for Bambara-French. 3. Additional explanations in the other languages while those are absent in Bambara: Before: fr: “Un doigt ne peut pas prendre un caillou (C’est important d’aider les uns les autres).” bam: “Bolokªni kelen t¢ se ka b¢l¢ ta.” 7 One can imagine that translating these into French or English is difficult since there is no indicator of the correct choice. (Johnson, 2018) 26 3.5 Monolingual Data both language pairs into subword units (byte pair encodings, BPE) (500 or 1000, separately) using subword-nmt9 (Sennrich et al., 2016), and apply BPE dropout to the training sets of both languages (Provilkov et al., 2019). We also experiment with character-level translation for French. In addition to the translations, we obtained a dataset of 488 monolingual Bambara sentences, sampled from all articles in the Bambara Wikipedia and covering a range of topics, but with preponderance of articles related to Mali. We used this monolingual dataset for experiments in data augmentation through back-translation, described in Section 5.1. 5 5.1 4 4.1 NMT Development Automatic Evaluation Segmentation. We evaluate the models’ translatio"
2020.loresmt-1.3,P02-1040,0,0.110364,"v et al., 2019). We also experiment with character-level translation for French. In addition to the translations, we obtained a dataset of 488 monolingual Bambara sentences, sampled from all articles in the Bambara Wikipedia and covering a range of topics, but with preponderance of articles related to Mali. We used this monolingual dataset for experiments in data augmentation through back-translation, described in Section 5.1. 5 5.1 4 4.1 NMT Development Automatic Evaluation Segmentation. We evaluate the models’ translations against reference translations on our heldout sets with corpus BLEU (Papineni et al., 2002) and ChrF (Popovi´c, 2015) computed with SacreBLEU (Post, 2018).10 Tables 2 and 3 show the results for French and English translations respectively. We find that word- and character-level modeling performs sub par compared to subword-level segmentation, which is in line with previous work on low-resource MT. The word-based model cannot resolve out-of-vocabulary words, and the characterlevel model struggled with word composition. With BPE, smaller subwords seem to perform slightly better than larger ones. BPE dropout (Provilkov et al., 2019), which was previously reported to be helpful for low-"
2020.loresmt-1.3,W19-1406,0,0.0237071,"to 6M for English), of which a large percentage are only stubs. Of the small number of full articles, most do not consistently employ the standard orthography of Bambara. A selection of those, however, was prepared to be used as monolingual data for MT data augmentation. Most African NMT studies have been based on the JW300 corpus (Agi´c and Vuli´c, 2019), e.g. most of the Masakhane benchmarks (∀ et al., 2020). JW300 only contains less than 200 sentences of Dyula, a closely related language to Bambara that it is mutually intelligible with. It might be useful for future cross-lingual studies (Wu et al., 2019; Goyal et al., 2020), but in order to avoid interference between languages, we focus on Bambara data exclusively in this first study. The most promising for our NMT approach was a dictionary data set from SIL Mali5 with examples of sentences used to demonstrate word usage in Spanish, French, English, and Bambara; and a tri-lingual health guide titled “Where there is no doctor.6 ” Detailed corpus statistics are listed in Table 1. chapters files paragraphs unigrams bigrams trigrams stopwords Bambara French English 3,548 2,023 2,158 4,847 2,021 2,146 4,855 2,021 2,158 9,336 8,209 26,430 5,816 14"
2020.loresmt-1.3,W15-3049,0,0.0296695,"Missing"
2020.loresmt-1.3,W18-6319,0,0.0165654,"rench. In addition to the translations, we obtained a dataset of 488 monolingual Bambara sentences, sampled from all articles in the Bambara Wikipedia and covering a range of topics, but with preponderance of articles related to Mali. We used this monolingual dataset for experiments in data augmentation through back-translation, described in Section 5.1. 5 5.1 4 4.1 NMT Development Automatic Evaluation Segmentation. We evaluate the models’ translations against reference translations on our heldout sets with corpus BLEU (Papineni et al., 2002) and ChrF (Popovi´c, 2015) computed with SacreBLEU (Post, 2018).10 Tables 2 and 3 show the results for French and English translations respectively. We find that word- and character-level modeling performs sub par compared to subword-level segmentation, which is in line with previous work on low-resource MT. The word-based model cannot resolve out-of-vocabulary words, and the characterlevel model struggled with word composition. With BPE, smaller subwords seem to perform slightly better than larger ones. BPE dropout (Provilkov et al., 2019), which was previously reported to be helpful for low-resource MT (Richburg et al., 2020), did not increase the quali"
2020.lrec-1.629,S19-2007,0,0.0633936,"al., 2017), a corpus of offensive comments from Facebook and Reddit in Danish (Sigurbergsson and Derczynski, 2020), another Twitter corpus in German (Wiegand et al., 2018) for GermEval2018, a second Italian corpus from Facebook and Twitter (Bosco et al., 2018), an aggressive post corpus from Mexican Twitter in Spanish (Aragón et al., 2018) and finally an aggressive comments corpus from Facebook in Hindi (Kumar et al., 2018). SemEval 2019 presented a novel task: Multilingual detection of hate speech specifically against immigrants and women with a dataset from Twitter, in English and Spanish (Basile et al., 2019). 3. The OGTD Dataset The posts in OGTD v1.0 were collected between May and June, 2019. We used the Twitter API initially collecting tweets from popular and trending hashtags in Greece, including television programs such as series, reality and entertainment shows. Due to the municipal, regional as well as the European Parliament election taking place at the time, many hashtags included tweets discussing the elections. The intuition behind this approach is that Twitter as a microblogging service often gathers complaints and profane comments on widely viewed television and politics, and as such,"
2020.lrec-1.629,2020.lrec-1.758,0,0.31459,"Missing"
2020.lrec-1.629,N19-1423,0,0.260784,"nd testing, is showing in Table 1. 2 https://github.com/thunlp/paragraph2vec 5114 https://www.lighttag.io/ Labels Offensive Not Offensive All Training Set 955 2,390 3,345 Test Set 446 988 1,434 Total 1,401 3,378 4,779 Table 1: Distribution of labels in the OGTD v1.0. Figure 1: Cohen’s Kappa for each pair of annotators et al., 2018). Each Greek word can be represented with a 300 dimensional vector using the trained model. The vector then can be used to feed in to the deep learning models which will be described in section 4.1.2.. For the last deep learning architecture we wanted to use a BERT (Devlin et al., 2019) model trained on Greek. However there was no BERT model available for Greek language. The model that came closest our requirement was multilingual BERT model 4 trained on 108 languages (Devlin et al., 2019) including Greek. Since training BERT is a very computationaly expensive task we used the available multilingual BERT cased model for the sixth deep learning architecture. 4.1. 4. Methods Before experimenting with OGTD, an unique aspect of Greek which is the accentuation of characters for correct pronunciation needed to be normalized. When posting a tweet, many users omit accents due to the"
2020.lrec-1.629,R19-1056,1,0.70351,"using the default values in scikit-learn. The final classifier was two models based on the Bayes theorem: Multinomial Naïve Bayes, which works with occurrence counts, and Bernoulli Naïve Bayes, which is designed for binary features. 4.1.2. Deep Learning Models Six different deep learning models were considered. All of these models have been used in an aggression detection task. The models are Pooled GRU (Plum et al., 2019), Stacked LSTM with Attention (Plum et al., 2019), LSTM and GRU with Attention (Plum et al., 2019), 2D Convolution with Pooling (Ranasinghe et al., 2019), GRU with Capsule (Hettiarachchi and Ranasinghe, 2019), LSTM with Capsule and Attention (Ranasinghe et al., 2019) and BERT (Devlin et al., 2019). These models has been used in HASOC 2019 and achieved a third place finish in English task and a eighth place finish in German and Hindi subtasks (Ranasinghe et al., 2019). Parameters described in (Ranasinghe et al., 2019) were used as the default parameters in order to ease the training process. The code for the deep learning has been made available on Github 5 . 4.2. Results The performance of individual classifiers for offensive language identification with TF/IDF unigram features is demonstrated in"
2020.lrec-1.629,W18-4401,1,0.893335,"nguage identification applying machine learning and deep learning systems on annotated data to identify such content. Researchers in the field have worked with different definitions of offensive language with hate speech being the most studied among these types (Davidson et al., 2017). (Waseem et al., 2017) investigate the similarity between these subtasks. With a few noteworthy exceptions, most research so far has dealt with English, due to the availability of language resources. This gap in the literature recently started to be addressed with studies on Spanish (Aragón et al., 2018), Hindi (Kumar et al., 2018), and German (Wiegand et al., 2018), to name a few. In this paper we contribute in this direction presenting the first Greek annotated dataset for offensive language identification: the Offensive Greek Tweet Dataset (OGTD). OGTD uses a working definition of offensive language inspired by the OLID dataset for English (Zampieri et al., 2019a) used in the recent OffensEval (SemEval-2019 Task 6) (Zampieri et al., 2019b). In its version, 1.0 OGTD contains nearly 4,800 posts collected from Twitter and manually annotated by a team of volunteers, resulting in a highquality annotated dataset. We traine"
2020.lrec-1.629,malmasi-zampieri-2017-detecting,1,0.830029,"for English (Zampieri et al., 2019a) used in the recent OffensEval (SemEval-2019 Task 6) (Zampieri et al., 2019b). In its version, 1.0 OGTD contains nearly 4,800 posts collected from Twitter and manually annotated by a team of volunteers, resulting in a highquality annotated dataset. We trained a number of systems on this dataset and our best results have been obtained from a system using LSTMs and GRU with attention which achieved 0.89 F1 score. 2. Related Work The bulk of work on detecting abusive posts online addressed particular types of such language like textual attacks and hate speech (Malmasi and Zampieri, 2017), aggression (Kumar et al., 2018), and others. OGTD considers a more general definition of offensiveness inspired by the first layer of the hierarchical annotation model described in (Zampieri et al., 2019a). (Zampieri et al., 2019a) model distinguishes targeted from general profanity, and considers the target of offensive posts as indicators of potential hate speech posts (insults targeted at groups) and cyberbulling posts (insults targeted at individuals). Offensive Language: Previous work presented a dataset with sentences labelled as flame (i.e. attacking or containing abusive words) or ok"
2020.lrec-1.629,W16-3638,0,0.0390705,"al., 2010) with a Naïve Bayes hybrid classifier and a user offensiveness estimation using an offensive lexicon and sentence syntactic structures (Chen et al., 2012). A dataset of 3.3M comments from the Yahoo Finance and News website, labelled as abusive or clean, was utilized in several experiments using ngrams, linguistic and syntactic features, combined with different types of word and comment embeddings as distributional semantics features (Nobata et al., 2016). The usefulness of character n-grams for abusive language detection was explored on the same dataset with three different methods (Mehdad and Tetreault, 2016). The most recent project expanded on existing ideas for defining offensive language and presented the OLID (Offensive Language Identification Dataset), a corpus of Twitter posts hierarchically annotated on three levels, whether they contain offensive language or not, whether the offense is targeted and finally, the target of the offense (Zampieri et al., 2019a). A CNN (Convolutional neural network) deep learning approach outperformed every model trained, with pre-trained FastText embeddings and updateable embeddings learned by the model as features. In OffensEval (SemEval-2019 Task 6), partic"
2020.lrec-1.629,W17-3008,0,0.0557353,"ied to distinguish offensive language and hate speech, with the hate class being the hardest to classify. 2.1. Non-English Datasets Research on other languages includes datasets such as: A Dutch corpus of posts from the social networking site Ask.fm for the detection of cyberbullying (Van Hee et al., 2015), a German Twitter corpus exploring the issue of hate speech targeted to refugees (Ross et al., 2016), another Dutch corpus using data from two anti-Islamic groups in Facebook (Tulkens et al., 2016), a hate speech corpus in Italian (Pelosi et al., 2017), an abusive language corpus in Arabic (Mubarak et al., 2017), a corpus of offensive comments from Facebook and Reddit in Danish (Sigurbergsson and Derczynski, 2020), another Twitter corpus in German (Wiegand et al., 2018) for GermEval2018, a second Italian corpus from Facebook and Twitter (Bosco et al., 2018), an aggressive post corpus from Mexican Twitter in Spanish (Aragón et al., 2018) and finally an aggressive comments corpus from Facebook in Hindi (Kumar et al., 2018). SemEval 2019 presented a novel task: Multilingual detection of hate speech specifically against immigrants and women with a dataset from Twitter, in English and Spanish (Basile et a"
2020.lrec-1.629,2020.lrec-1.430,0,0.38941,"st to classify. 2.1. Non-English Datasets Research on other languages includes datasets such as: A Dutch corpus of posts from the social networking site Ask.fm for the detection of cyberbullying (Van Hee et al., 2015), a German Twitter corpus exploring the issue of hate speech targeted to refugees (Ross et al., 2016), another Dutch corpus using data from two anti-Islamic groups in Facebook (Tulkens et al., 2016), a hate speech corpus in Italian (Pelosi et al., 2017), an abusive language corpus in Arabic (Mubarak et al., 2017), a corpus of offensive comments from Facebook and Reddit in Danish (Sigurbergsson and Derczynski, 2020), another Twitter corpus in German (Wiegand et al., 2018) for GermEval2018, a second Italian corpus from Facebook and Twitter (Bosco et al., 2018), an aggressive post corpus from Mexican Twitter in Spanish (Aragón et al., 2018) and finally an aggressive comments corpus from Facebook in Hindi (Kumar et al., 2018). SemEval 2019 presented a novel task: Multilingual detection of hate speech specifically against immigrants and women with a dataset from Twitter, in English and Spanish (Basile et al., 2019). 3. The OGTD Dataset The posts in OGTD v1.0 were collected between May and June, 2019. We used"
2020.lrec-1.629,W17-3012,0,0.120009,"at individuals or groups. As such content increasingly occurs online, it has become a growing issue for online communities. This has come to the attention of social media platforms and authorities underlining the urgency to moderate and deal with such content. Several studies in NLP have approached offensive language identification applying machine learning and deep learning systems on annotated data to identify such content. Researchers in the field have worked with different definitions of offensive language with hate speech being the most studied among these types (Davidson et al., 2017). (Waseem et al., 2017) investigate the similarity between these subtasks. With a few noteworthy exceptions, most research so far has dealt with English, due to the availability of language resources. This gap in the literature recently started to be addressed with studies on Spanish (Aragón et al., 2018), Hindi (Kumar et al., 2018), and German (Wiegand et al., 2018), to name a few. In this paper we contribute in this direction presenting the first Greek annotated dataset for offensive language identification: the Offensive Greek Tweet Dataset (OGTD). OGTD uses a working definition of offensive language inspired by"
2020.lrec-1.629,N19-1144,1,0.891688,"tween these subtasks. With a few noteworthy exceptions, most research so far has dealt with English, due to the availability of language resources. This gap in the literature recently started to be addressed with studies on Spanish (Aragón et al., 2018), Hindi (Kumar et al., 2018), and German (Wiegand et al., 2018), to name a few. In this paper we contribute in this direction presenting the first Greek annotated dataset for offensive language identification: the Offensive Greek Tweet Dataset (OGTD). OGTD uses a working definition of offensive language inspired by the OLID dataset for English (Zampieri et al., 2019a) used in the recent OffensEval (SemEval-2019 Task 6) (Zampieri et al., 2019b). In its version, 1.0 OGTD contains nearly 4,800 posts collected from Twitter and manually annotated by a team of volunteers, resulting in a highquality annotated dataset. We trained a number of systems on this dataset and our best results have been obtained from a system using LSTMs and GRU with attention which achieved 0.89 F1 score. 2. Related Work The bulk of work on detecting abusive posts online addressed particular types of such language like textual attacks and hate speech (Malmasi and Zampieri, 2017), aggre"
2020.lrec-1.629,S19-2010,1,0.665715,"tween these subtasks. With a few noteworthy exceptions, most research so far has dealt with English, due to the availability of language resources. This gap in the literature recently started to be addressed with studies on Spanish (Aragón et al., 2018), Hindi (Kumar et al., 2018), and German (Wiegand et al., 2018), to name a few. In this paper we contribute in this direction presenting the first Greek annotated dataset for offensive language identification: the Offensive Greek Tweet Dataset (OGTD). OGTD uses a working definition of offensive language inspired by the OLID dataset for English (Zampieri et al., 2019a) used in the recent OffensEval (SemEval-2019 Task 6) (Zampieri et al., 2019b). In its version, 1.0 OGTD contains nearly 4,800 posts collected from Twitter and manually annotated by a team of volunteers, resulting in a highquality annotated dataset. We trained a number of systems on this dataset and our best results have been obtained from a system using LSTMs and GRU with attention which achieved 0.89 F1 score. 2. Related Work The bulk of work on detecting abusive posts online addressed particular types of such language like textual attacks and hate speech (Malmasi and Zampieri, 2017), aggre"
2020.lrec-1.629,2020.semeval-1.188,1,0.74761,"s, encompassing posts related to an array of topics popular among Greek people (e.g. political elections, TV shows, etc.). Tweets were manually annotated by a team volunteers through an annotation platform. We used the same guidelines used in the annotation of the English OLID dataset (Zampieri et al., 2019a). Finally, we run several machine learning and deep learning classifiers and the best results were achieved by a LSTM and GRU with Attention model. 5.1. Ongoing - OGTD v2.0 and OffensEval 2020 We have recently released OGTD v2.0 as training data for OffensEval 2020 (SemEval-2020 Task 12) (Zampieri et al., 2020).6 The reasoning behind the expansion of the dataset was to have a larger Greek dataset for the competition. New posts were collected in November 2019 following the same approach we used to compile v1.0 described in this paper. This second batch of tweets included tweets with trending hashtags, shows and topics from Greece at the time. Additionally, keywords that proved to retrieve interesting tweets in the first version were once again used in the search, along with new keywords like pejorative terms. When the collection was finished, 5,508 tweets were randomly sampled to be then annotated by"
2020.readi-1.9,D19-3009,0,0.0152351,"rves meaning, or is grammatical. To overcome some of these shortcomings, Xu et al. (2016) introduced the SARI method of evaluating text simplification systems. SARI comprises parallel simplifiedunsimplified sentences and measures additions, deletions and those words that are kept by a system. IT does this by comparing input sentences to reference sentences to determine the appropriateness of a simplification. However, SARI is still an automated measure and optimising systems to get a good SARI score may lead to systems that do well on the metric, but not in human evaluations. Recently, EASSE (Alva-Manchego et al., 2019) has been released to attempt to standardise simplification evaluation by providing a common reference implementation of several text simplification benchmarks. Our work does not attempt to simplify a whole sentence through paraphrasing or machine translation, but instead looks at the possibility of identifying which words in a sentence are complex and specifically, how complex those words are. This is a task intrinsically linked to the evaluation of text simplification as the ultimate goal of the task is to reduce the overall complexity of a text. Therefore, by properly understanding and pred"
2020.readi-1.9,D17-1070,0,0.0448279,"Missing"
2020.readi-1.9,2005.mtsummit-papers.11,0,0.163097,"also allowed multiple instances of each word (up to 5) to allow for cases in our corpus where one word is annotated with different complexity values given different contexts. To add further variation to our data, three corpora were selected as follows: Bible: We selected the World English Bible translation from Christodouloupoulos and Steedman (2015). This is a modern translation, so does not contain archaic words (thee, thou, etc.), but still contains religious language that may be complex. Europarl: We used the English portion of the European Pariliament proceedings selected from europarl (Koehn, 2005). This is a very varied corpus talking about all manner of matters related to european policy. As this is speech transcription, it is often dialogical in nature. Biomedical: We also selected articles from the CRAFT corpus (Bada et al., 2012), which are all in the biomedical domain. These present a very specialised type of language that will be unfamiliar to non-domain experts. Each corpus has its own unique language features and styles. Predicting the lexical complexity of diverse sources further distinguishes our work from previous attempts, which have traditionally focused on Wikipedia and N"
2020.readi-1.9,P02-1040,0,0.106643,"value for each word was calculated as the proportion of annotators that found a word complex (i.e., if 5 out of 10 annotators marked a word as complex then the word was given a score of 0.5), a measure which is difficult to interpret as it relies on an aggregation of an arbitrary number of absolute binary judgements of complexity to give a continuous value. 2.2. Text Simplification Text simplification evaluation is an active area of research, with recent efforts focussing on evaluating the whole process of text simplification in the style of machine translation evaluation. Whilst BLEU score (Papineni et al., 2002) has been used for text simplification evaluation, this is not necessarily an informative measure, as it inly measures similarity to the target. It does not help a researcher to understand whether the resultant text preserves meaning, or is grammatical. To overcome some of these shortcomings, Xu et al. (2016) introduced the SARI method of evaluating text simplification systems. SARI comprises parallel simplifiedunsimplified sentences and measures additions, deletions and those words that are kept by a system. IT does this by comparing input sentences to reference sentences to determine the app"
2020.readi-1.9,D14-1162,0,0.0985442,"tiwords have a higher score of 0.444. This is reflected across each genre, with the largest difference being in biomedical (0.395 / 0.470) and the smallest change being in the Bible (0.380 / 0.428). 4. Baseline System We developed a baseline for predicting the complexity of a word using our data. We used a linear regression with embedding features for the word and context as well as three hand crafted features, which are known to be strong predictors of lexical complexity. Specifically, the feature sets we used are as follows: Glove Embeddings: We captured the 300-dimensional Glove embedding (Pennington et al., 2014) for each token in our corpus. This was encoded as 300 separate features (one for each dimension of the embedding). InferSent Embeddings: We captured the 4,096dimensional embeddings produced by the InferSent library (Conneau et al., 2017) for each context. These were encoded as 4,096 separate features, one for each dimension of the embedding. Hand Crafted Features: We recorded features which are typically known to be strong predictors of lexical complexity. Specifically, we looked at (1) word frequency according to the GoogleWeb1T resource (Brants and Franz, 2006), (2) Word length (as number o"
2020.readi-1.9,P13-3015,1,0.855961,"9,476 sentences each annotated by around 7 annotators. Keywords: Complex Word Identification, Text Simplification, Lexical Complexity Prediction 1. Introduction In many readability applications, it is useful to know the complexity of a given word. In early approaches to the readability task, simple metrics such as whether a word had more than 3 syllables (Mc Laughlin, 1969) or was on a given list or not (Dale and Chall, 1948) were used to identify complex words. More recently, automated methods for detecting complex words have also been used such as using a threshold on the word’s frequency (Shardlow, 2013), or attempting to use a machine learning classifier to determine whether a word is complex or not (Paetzold and Specia, 2016; Yimam et al., 2018). These approaches make the fundamental assumption that lexical complexity is binary. That words fall into one of two categories: difficult, or not. Previous approaches to Complex Word Identification (CWI), such as the one used in the CWI shared task (SemEval-2016 Task 11) (Paetzold and Specia, 2016), therefore typically refer to binary identification of complex words. A word close to the decision boundary is assumed to be just as complex as one furt"
2020.readi-1.9,Q16-1029,0,0.0260273,"ts of complexity to give a continuous value. 2.2. Text Simplification Text simplification evaluation is an active area of research, with recent efforts focussing on evaluating the whole process of text simplification in the style of machine translation evaluation. Whilst BLEU score (Papineni et al., 2002) has been used for text simplification evaluation, this is not necessarily an informative measure, as it inly measures similarity to the target. It does not help a researcher to understand whether the resultant text preserves meaning, or is grammatical. To overcome some of these shortcomings, Xu et al. (2016) introduced the SARI method of evaluating text simplification systems. SARI comprises parallel simplifiedunsimplified sentences and measures additions, deletions and those words that are kept by a system. IT does this by comparing input sentences to reference sentences to determine the appropriateness of a simplification. However, SARI is still an automated measure and optimising systems to get a good SARI score may lead to systems that do well on the metric, but not in human evaluations. Recently, EASSE (Alva-Manchego et al., 2019) has been released to attempt to standardise simplification ev"
2020.readi-1.9,W18-0507,1,0.588321,"Missing"
2020.readi-1.9,W17-5910,1,0.70241,"taset if the word has been assigned by at least one of the annotators as complex. All words that have not been assigned by at least one annotator as complex have been labeled as non-complex. The task was to use this dataset to train classifiers to predict lexical complexity assigning a label 0 to non-complex words and 1 to complex ones. The dataset made available by the CWI 2016 organizers comprised a training set of 2,237 instances and a much larger test set of 88,221 instances, an unusual setting in most NLP shared tasks where most often the training set is much larger than the test set. In Zampieri et al. (2017) oracle and ensemble methods have been used to investigate the performance of the participating systems. The study showed that most systems performed poorly due to the way the data was annotated and also due to the fact that lexical complexity was modelled as a binary task, a shortcoming addressed by CompLex. Finally, a second iteration of the CWI shared task was organized at the BEA workshop 2018 (Yimam et al., 2018). In CWI 2018, a multilingual dataset was made available containing English, German, and Spanish training and testing data for monolingual tracks, and a French test set for multil"
2020.readi-1.9,S16-1085,0,\N,Missing
2020.semeval-1.188,2020.semeval-1.206,0,0.0947199,"Missing"
2020.semeval-1.188,C18-1139,0,0.018965,"ømberg-Derczynski et al., 2020), etc. 4 1428 Many teams also used context-independent embeddings from word2vec (Mikolov et al., 2013) or GloVe (Pennington et al., 2014), including language-specific embeddings such as Mazajak (Farha and Magdy, 2019) for Arabic. Some teams used other techniques: word n-grams, character n-grams, lexicons for sentiment analysis, and lexicon of offensive words. Other representations included emoji priors extracted from the weakly supervised SOLID dataset for English, and sentiment analysis using NLTK (Bird et al., 2009), Vader (Hutto and Gilbert, 2014), and FLAIR (Akbik et al., 2018). Machine learning models In terms of machine learning models, most teams used some kind of pretrained Transformers: typically BERT, but RoBERTa, XLM-RoBERTa (Conneau et al., 2020), ALBERT (Lan et al., 2019), and GPT-2 (Radford et al., 2019) were also popular. Other popular models included CNNs (Fukushima, 1980), RNNs (Rumelhart et al., 1986), and GRUs (Cho et al., 2014). Older models such as SVMs (Cortes and Vapnik, 1995) were also used, typically as part of ensembles. 5 English Track A total of 87 teams made submissions for the English track (23 of them participated in the 2019 edition of th"
2020.trac-1.1,2020.trac-1.14,0,0.231789,"Missing"
2020.trac-1.1,2020.trac-1.15,0,0.0507202,"Missing"
2020.trac-1.1,2020.trac-1.12,0,0.0357812,"Missing"
2020.trac-1.1,S19-2007,0,0.414215,"l., 2018), cyberbullying (Xu et al., 2012; Dadvar et al., 2013), hate speech (Davidson et al., 2017), and offensive content (Zampieri et al., 2019a) to name a few. Prior studies have tackled abusive language identification in content from different platforms such as Twitter (Xu et al., 2012; Burnap and Williams, 2015; Davidson et al., 2017; Wiegand et al., 2018), Wikipedia comments1 , and Facebook (Kumar et al., 2018). A number of shared tasks been organized focusing on the automatic detection of offensive language (Struß et al., 2019; Zampieri et al., 2019b; Mandl et al., 2019), hate speech (Basile et al., 2019) and aggression (Kumar et al., 2018). These have motivated the creation of for various languages such as English, German, Hindi, Italian, Spanish, and others. In this paper, we discuss the results of the second iteration of the TRAC shared task, organized as part of the Workshop on Trolling, Aggression and Cyberbullying at LREC 2020. The task consisted of two sub-tasks - aggression identification and gendered aggression identification on YouTube comments in three languages: Bengali, Hindi and English. To the best of our knowledge, TRAC-2 is the first shared task to include YouTube comments as"
2020.trac-1.1,2020.trac-1.17,0,0.0557343,"Missing"
2020.trac-1.1,W18-4401,1,0.88136,"provided in each language for each sub-task. A total of 70 teams registered to participate in the task and 19 teams submitted their test runs. The best system obtained a weighted F-score of approximately 0.80 in sub-task A for all the three languages. While approximately 0.87 in sub-task B for all the three languages. Keywords: Aggression, Gendered Aggression, English, Hindi, Bengali, TRAC 1. Introduction 2. In recent years, there have been several studies exploring the computational modelling and automatic detection of abusive content in social media focusing on toxic comments1 , aggression (Kumar et al., 2018), cyberbullying (Xu et al., 2012; Dadvar et al., 2013), hate speech (Davidson et al., 2017), and offensive content (Zampieri et al., 2019a) to name a few. Prior studies have tackled abusive language identification in content from different platforms such as Twitter (Xu et al., 2012; Burnap and Williams, 2015; Davidson et al., 2017; Wiegand et al., 2018), Wikipedia comments1 , and Facebook (Kumar et al., 2018). A number of shared tasks been organized focusing on the automatic detection of offensive language (Struß et al., 2019; Zampieri et al., 2019b; Mandl et al., 2019), hate speech (Basile et"
2020.trac-1.1,2020.trac-1.18,0,0.295322,"Missing"
2020.trac-1.1,2020.trac-1.10,0,0.0770099,"Missing"
2020.trac-1.1,malmasi-zampieri-2017-detecting,1,0.860794,"(Cambria et al., 2010; Kumar et al., 2014; Mojica, 2016; Mihaylov et al., 2015), flaming / insults (Sax, 2016; Nitin et al., 2012), radicalization (Agarwal and Sureka, 2015; Agarwal and Sureka, 2017), racism (Greevy and Smeaton, 2004; Greevy, 2004), misogyny ((Menczer et al., 2015; Frenda et al., 2019; Hewitt et al., 2016; Fersini et al., 2018; Anzovino et al., 2018; Sharifirad and Matwin, 2019)), online aggression (Kumar et al., 2018), cyberbullying (Xu et al., 2012; Dadvar et al., 2013), hate speech (Kwok and Wang, 2013; Djuric et al., 2015; Burnap and Williams, 2015; Davidson et al., 2017; Malmasi and Zampieri, 2017; Malmasi and Zampieri, 2018), and offensive language (Wiegand et al., 2018; Zampieri et al., 2019b). The terms used in the literature have overlapping properties as discussed in Waseem et al. (2017) and Zampieri et al. (2019a). The most important differences concern their target (e.g. hate speech is typically targeted at groups whereas cyberbulling targets individuals), which is represented in TRAC-2 Task B, and types (e.g. veiled or direct abuse), represented in TRAC-2 Task A. Most related studies focus on English, but significant amount of work has been carried out for other languages too."
2020.trac-1.1,K15-1032,0,0.029561,"s. Section 2. discusses related studies and shared tasks to TRAC2. Section 3. presents the setup and schedule of TRAC-2 and Section 4. presents the dataset used in the competition. Section 5. presents the approaches used by participants of the competition and Section 6. presents and analyzes the results they obtained. Finally, 7. concludes this paper and presents avenues for future work. 1 Related Work Automatically identifying the various forms of abusive language online has been studied from different angles. Examples include trolling (Cambria et al., 2010; Kumar et al., 2014; Mojica, 2016; Mihaylov et al., 2015), flaming / insults (Sax, 2016; Nitin et al., 2012), radicalization (Agarwal and Sureka, 2015; Agarwal and Sureka, 2017), racism (Greevy and Smeaton, 2004; Greevy, 2004), misogyny ((Menczer et al., 2015; Frenda et al., 2019; Hewitt et al., 2016; Fersini et al., 2018; Anzovino et al., 2018; Sharifirad and Matwin, 2019)), online aggression (Kumar et al., 2018), cyberbullying (Xu et al., 2012; Dadvar et al., 2013), hate speech (Kwok and Wang, 2013; Djuric et al., 2015; Burnap and Williams, 2015; Davidson et al., 2017; Malmasi and Zampieri, 2017; Malmasi and Zampieri, 2018), and offensive language"
2020.trac-1.1,2020.trac-1.19,0,0.0543347,"Missing"
2020.trac-1.1,2020.trac-1.11,0,0.0381996,"Missing"
2020.trac-1.1,2020.lrec-1.629,1,0.848565,"). The terms used in the literature have overlapping properties as discussed in Waseem et al. (2017) and Zampieri et al. (2019a). The most important differences concern their target (e.g. hate speech is typically targeted at groups whereas cyberbulling targets individuals), which is represented in TRAC-2 Task B, and types (e.g. veiled or direct abuse), represented in TRAC-2 Task A. Most related studies focus on English, but significant amount of work has been carried out for other languages too. This includes languages such as Arabic (Mubarak et al., 2020), German (Struß et al., 2019), Greek (Pitenis et al., 2020), Hindi (Mandl et al., 2019), and Spanish (Basile et al., 2019). TRAC - 2 is the second iteration of the TRAC shared task on Aggression Identification (Kumar et al., 2018) hosted at the TRAC workshop at COLING 2018. The first edition of TRAC included English and Hindi data from Facebook and Twitter. It consisted of a three-way classification task with posts labelled as overtly aggressive, covertly aggressive, and non-aggressive. TRAC received 30 submissions and the results obtained by participants suggested that neural network-based systems and machine learning classifiers http://bit.ly/2FhLMV"
2020.trac-1.1,2020.trac-1.9,0,0.502008,"Missing"
2020.trac-1.1,2020.trac-1.20,0,0.330248,"Missing"
2020.trac-1.1,2020.trac-1.16,0,0.0574059,"Missing"
2020.trac-1.1,W17-3012,0,0.0377661,"sm (Greevy and Smeaton, 2004; Greevy, 2004), misogyny ((Menczer et al., 2015; Frenda et al., 2019; Hewitt et al., 2016; Fersini et al., 2018; Anzovino et al., 2018; Sharifirad and Matwin, 2019)), online aggression (Kumar et al., 2018), cyberbullying (Xu et al., 2012; Dadvar et al., 2013), hate speech (Kwok and Wang, 2013; Djuric et al., 2015; Burnap and Williams, 2015; Davidson et al., 2017; Malmasi and Zampieri, 2017; Malmasi and Zampieri, 2018), and offensive language (Wiegand et al., 2018; Zampieri et al., 2019b). The terms used in the literature have overlapping properties as discussed in Waseem et al. (2017) and Zampieri et al. (2019a). The most important differences concern their target (e.g. hate speech is typically targeted at groups whereas cyberbulling targets individuals), which is represented in TRAC-2 Task B, and types (e.g. veiled or direct abuse), represented in TRAC-2 Task A. Most related studies focus on English, but significant amount of work has been carried out for other languages too. This includes languages such as Arabic (Mubarak et al., 2020), German (Struß et al., 2019), Greek (Pitenis et al., 2020), Hindi (Mandl et al., 2019), and Spanish (Basile et al., 2019). TRAC - 2 is th"
2020.trac-1.1,N12-1084,0,0.623973,"ub-task. A total of 70 teams registered to participate in the task and 19 teams submitted their test runs. The best system obtained a weighted F-score of approximately 0.80 in sub-task A for all the three languages. While approximately 0.87 in sub-task B for all the three languages. Keywords: Aggression, Gendered Aggression, English, Hindi, Bengali, TRAC 1. Introduction 2. In recent years, there have been several studies exploring the computational modelling and automatic detection of abusive content in social media focusing on toxic comments1 , aggression (Kumar et al., 2018), cyberbullying (Xu et al., 2012; Dadvar et al., 2013), hate speech (Davidson et al., 2017), and offensive content (Zampieri et al., 2019a) to name a few. Prior studies have tackled abusive language identification in content from different platforms such as Twitter (Xu et al., 2012; Burnap and Williams, 2015; Davidson et al., 2017; Wiegand et al., 2018), Wikipedia comments1 , and Facebook (Kumar et al., 2018). A number of shared tasks been organized focusing on the automatic detection of offensive language (Struß et al., 2019; Zampieri et al., 2019b; Mandl et al., 2019), hate speech (Basile et al., 2019) and aggression (Kuma"
2020.trac-1.1,N19-1144,1,0.919061,"t runs. The best system obtained a weighted F-score of approximately 0.80 in sub-task A for all the three languages. While approximately 0.87 in sub-task B for all the three languages. Keywords: Aggression, Gendered Aggression, English, Hindi, Bengali, TRAC 1. Introduction 2. In recent years, there have been several studies exploring the computational modelling and automatic detection of abusive content in social media focusing on toxic comments1 , aggression (Kumar et al., 2018), cyberbullying (Xu et al., 2012; Dadvar et al., 2013), hate speech (Davidson et al., 2017), and offensive content (Zampieri et al., 2019a) to name a few. Prior studies have tackled abusive language identification in content from different platforms such as Twitter (Xu et al., 2012; Burnap and Williams, 2015; Davidson et al., 2017; Wiegand et al., 2018), Wikipedia comments1 , and Facebook (Kumar et al., 2018). A number of shared tasks been organized focusing on the automatic detection of offensive language (Struß et al., 2019; Zampieri et al., 2019b; Mandl et al., 2019), hate speech (Basile et al., 2019) and aggression (Kumar et al., 2018). These have motivated the creation of for various languages such as English, German, Hind"
2020.trac-1.1,S19-2010,1,0.879571,"t runs. The best system obtained a weighted F-score of approximately 0.80 in sub-task A for all the three languages. While approximately 0.87 in sub-task B for all the three languages. Keywords: Aggression, Gendered Aggression, English, Hindi, Bengali, TRAC 1. Introduction 2. In recent years, there have been several studies exploring the computational modelling and automatic detection of abusive content in social media focusing on toxic comments1 , aggression (Kumar et al., 2018), cyberbullying (Xu et al., 2012; Dadvar et al., 2013), hate speech (Davidson et al., 2017), and offensive content (Zampieri et al., 2019a) to name a few. Prior studies have tackled abusive language identification in content from different platforms such as Twitter (Xu et al., 2012; Burnap and Williams, 2015; Davidson et al., 2017; Wiegand et al., 2018), Wikipedia comments1 , and Facebook (Kumar et al., 2018). A number of shared tasks been organized focusing on the automatic detection of offensive language (Struß et al., 2019; Zampieri et al., 2019b; Mandl et al., 2019), hate speech (Basile et al., 2019) and aggression (Kumar et al., 2018). These have motivated the creation of for various languages such as English, German, Hind"
2020.vardial-1.1,2020.vardial-1.24,0,0.254959,"Missing"
2020.vardial-1.1,2020.vardial-1.26,0,0.535455,"y stages of the COVID-19 pandemic. Lock downs and restrictive measures in many countries during this period have impacted universities and research centers worldwide causing significant disruption. We believe that this situation is very likely to have discouraged more teams to participate in this year’s evaluation campaign. 2 Team RDI Akanksha Anumit¸i CUBoulder-UBC Phlyers HeLju NRC Piyush Mishra SUKI The Linguistadors T¨ubingen UAIC UnibucKernel UPB ZHAW-InIT Total SMG ULI System Description Paper X X (Popa and S, tef˘anescu, 2020) * (Ceolin and Zhang, 2020) (Scherrer and Ljubeˇsi´c, 2020) (Bernier-Colborne and Goutte, 2020) (Mishra, 2020) (Jauhiainen et al., 2020a) X X X X X X X X X X X (C¸o¨ ltekin, 2020) (Rebeja and Cristea, 2020) (G˘aman and Ionescu, 2020a) (Zaharia et al., 2020) (Benites et al., 2020) X X X 8 7 1 11 Table 1: The teams that participated in the VarDial Evaluation Campaign 2020 along with their system description papers. *The system description paper by team CUBoulder-UBC does not appear in the VarDial workshop proceedings. CUBoulder-UBC reused a system described in Hulden et al. (2015). 4 Romanian Dialect Identification RDI 4.1 Dataset The training data is composed of news articles from the Mo"
2020.vardial-1.1,W19-1402,0,0.3829,"Missing"
2020.vardial-1.1,W18-3909,1,0.890114,"Missing"
2020.vardial-1.1,P19-1068,1,0.744799,"Commons Attribution 4.0 International License. //creativecommons.org/licenses/by/4.0/. 1 https://sites.google.com/view/vardial2020/evaluation-campaign 2 For recent surveys on these topics see Zampieri et al. (2020) and Jauhiainen et al. (2019c). License details: http: 1 Proceedings of the 7th VarDial Workshop on NLP for Similar Languages, Varieties and Dialects, pages 1–14 Barcelona, Spain (Online), December 13, 2020 2 Shared Tasks at VarDial 2020 Romanian Dialect Identification (RDI): In the Romanian Dialect Identification (RDI) shared task, we provided participants with the MOROCO data set (Butnaru and Ionescu, 2019) for training, which contains Moldavian (MD) and Romanian (RO) samples of text collected from the news domain. The task was a binary classification by dialect, in which a classification model is required to discriminate between the Moldavian (MD) and the Romanian (RO) dialects. The task was closed, therefore, participants are not allowed to use external data to train their models. The test set contained newly collected text samples from a different domain, not previously included in MOROCO, resulting in a cross-domain dialect identification task. Social Media Variety Geolocation (SMG): In cont"
2020.vardial-1.1,2020.vardial-1.25,0,0.269727,"Missing"
2020.vardial-1.1,2020.vardial-1.17,0,0.072284,"Missing"
2020.vardial-1.1,N19-1423,0,0.024395,"ainen et al. (2020b). 6.2 Participants and Approaches Unfortunately, the ULI shared task had only one team submitting results to the tracks. The NRC team submitted three runs for each of the shared task tracks. All the runs used BERT-related deep neural networks taking sequences of characters as input similar to what the NRC team used when they won the CLI shared task (Jauhiainen et al., 2019b) in the previous VarDial Evaluation Campaign (BernierColborne et al., 2019). The encoders of the networks were pre-trained on masked language modeling (MLM) and sentence pair classification (SPC) tasks (Devlin et al., 2019). The third run on each track was using only the information on the training set as opposed to the second run, in which the MLM was also done on the unlabeled test set in order to adapt the model. The first run on each track was a plurality voting ensemble of the six models used in the second and third runs of all the tracks. 6.3 Results For the baseline, we used an implementation of the HeLI method equal to the one we used when evaluating language identification methods for 285 languages (Jauhiainen et al., 2017). The baseline and the NRC teams results are listed in Tables 8, 9, and 10. Rank"
2020.vardial-1.1,2020.findings-emnlp.387,0,0.274164,"Missing"
2020.vardial-1.1,goldhahn-etal-2012-building,0,0.217587,"same data format and evaluation methodology. Both constrained and unconstrained submissions were allowed, but only one participating team made use of the latter. Uralic Language Identification (ULI): This shared task focused on discriminating between endangered languages of the Uralic group. In addition to 29 Uralic minority languages, the shared task also featured 149 non-relevant languages. For training, we provided texts from the Wanca 2016 corpora (Jauhiainen et al., 2019a) for the relevant languages while the texts for the non-relevant languages came from the Leipzig corpora collection (Goldhahn et al., 2012). The test set for the relevant languages included sentences from the forthcoming Wanca 2017 corpora (Jauhiainen et al., 2020b) that were not present in the Wanca 2016 corpora. The sentences for the non-relevant languages were from the Leipzig corpora collection. The ULI shared task was divided into three separate tracks using the same training and test data. The difference between the tracks was based on how the submissions were scored: track 1 focused on macro-averaged F-score for the 29 relevant languages, track 2 on micro-averaged F-score for the relevant languages, and track 3 on macro-av"
2020.vardial-1.1,L16-1284,1,0.891978,"Missing"
2020.vardial-1.1,2020.vardial-1.23,1,0.814174,"Missing"
2020.vardial-1.1,D18-1469,1,0.402749,"e SMG task is split into three subtasks covering different language areas: the BCMS subtask is focused on geolocated tweets published in the area of Croatia, Bosnia and Herzegovina, Montenegro and Serbia in the HBS macro-language (Ljubeˇsi´c et al., 2016); the DE-AT subtask focuses on conversations from the microblogging platform Jodel initiated in Germany and Austria, which are written in standard German but commonly contain regional and dialectal forms; the CH subtask is based on Jodel conversations initiated in Switzerland, which were found to be held majoritarily in Swiss German dialects (Hovy and Purschke, 2018). All three subtasks used the same data format and evaluation methodology. Both constrained and unconstrained submissions were allowed, but only one participating team made use of the latter. Uralic Language Identification (ULI): This shared task focused on discriminating between endangered languages of the Uralic group. In addition to 29 Uralic minority languages, the shared task also featured 149 non-relevant languages. For training, we provided texts from the Wanca 2016 corpora (Jauhiainen et al., 2019a) for the relevant languages while the texts for the non-relevant languages came from the"
2020.vardial-1.1,W17-1225,1,0.926999,"Missing"
2020.vardial-1.1,J16-3005,1,0.860747,"h quantile loss. SUKI. This approach divides each geographic area into a fixed grid with 81 areas and uses a n-gram language model to predict the most likely area (Jauhiainen et al., 2020a). The Linguistadors. These submissions are based on classic regression methods (linear regression, lasso regression, and ridge regression) and rely on TF-IDF weighted input features. UnibucKernel. The UnibucKernel team (G˘aman and Ionescu, 2020a) submitted two single systems, a character-level CNN (Zhang et al., 2015) with double regression output, and a Nu-SVR model trained on top of n-gram string kernels (Ionescu et al., 2016). The third system is an ensemble approach based on XGBoost, trained on the predictions provided by the two previously mentioned systems and an LSTMbased one. The LSTM is trained on top of fine-tuned German BERT embeddings. ZHAW-InIT. The ZHAW-InIT team (Benites et al., 2020) uses unsupervised k-means clustering to infer a set of dialect classes which are then used in a classification architecture. Their systems are based 7 either on SVMs with TF-IDF weighted word and character n-gram features, or on the HELI language modeling architecture (ZHAW-InIT (HELI)). The SVM submission to the CH subta"
2020.vardial-1.1,W17-0221,1,0.877702,"Missing"
2020.vardial-1.1,W19-1409,1,0.856366,"Missing"
2020.vardial-1.1,2020.vardial-1.21,1,0.758565,"Missing"
2020.vardial-1.1,W16-4801,1,0.731395,"Missing"
2020.vardial-1.1,2020.vardial-1.27,0,0.363938,"ock downs and restrictive measures in many countries during this period have impacted universities and research centers worldwide causing significant disruption. We believe that this situation is very likely to have discouraged more teams to participate in this year’s evaluation campaign. 2 Team RDI Akanksha Anumit¸i CUBoulder-UBC Phlyers HeLju NRC Piyush Mishra SUKI The Linguistadors T¨ubingen UAIC UnibucKernel UPB ZHAW-InIT Total SMG ULI System Description Paper X X (Popa and S, tef˘anescu, 2020) * (Ceolin and Zhang, 2020) (Scherrer and Ljubeˇsi´c, 2020) (Bernier-Colborne and Goutte, 2020) (Mishra, 2020) (Jauhiainen et al., 2020a) X X X X X X X X X X X (C¸o¨ ltekin, 2020) (Rebeja and Cristea, 2020) (G˘aman and Ionescu, 2020a) (Zaharia et al., 2020) (Benites et al., 2020) X X X 8 7 1 11 Table 1: The teams that participated in the VarDial Evaluation Campaign 2020 along with their system description papers. *The system description paper by team CUBoulder-UBC does not appear in the VarDial workshop proceedings. CUBoulder-UBC reused a system described in Hulden et al. (2015). 4 Romanian Dialect Identification RDI 4.1 Dataset The training data is composed of news articles from the Moldavian and Rom"
2020.vardial-1.1,2020.vardial-1.18,0,0.531349,"Missing"
2020.vardial-1.1,2020.vardial-1.20,0,0.189489,"Missing"
2020.vardial-1.1,L16-1641,1,0.889566,"Missing"
2020.vardial-1.1,2020.vardial-1.19,1,0.750864,"Missing"
2020.vardial-1.1,2020.vardial-1.22,0,0.515168,"Missing"
2020.vardial-1.1,W17-1201,1,0.773425,"Missing"
2020.wmt-1.1,2020.nlpcovid19-2.5,1,0.796341,"tails of the evaluation. 4.1.1 Covid Test Suite TICO-19 The TICO-19 test suite was developed to evaluate how well can MT systems handle the newlyemerged topic of COVID-19. Accurate automatic translation can play an important role in facilitating communication in order to protect at-risk populations and combat the infodemic of misinformation, as described by the World Health Organization. The test suite has no corresponding paper so its authors provided an analysis of the outcomes directly here. The submitted systems were evaluated using the test set from the recently-released TICO-19 dataset (Anastasopoulos et al., 2020). The dataset provides manually created translations of COVID19 related data. The test set consists of PubMed articles (678 sentences from 5 scientific articles), patient-medical professional conversations (104 sentences), as well as related Wikipedia articles (411 sentences), announcements (98 sentences from Wikisource), and news items (67 sentences from Wikinews), for a total of 2100 sentences. Table 15 outlines the BLEU scores by each submitted system in the English-to-X directions, also breaking down the results per domain. The analysis shows that some systems are significantly more prepar"
2020.wmt-1.1,2020.wmt-1.6,0,0.0647231,"AIP-NTT U BIQUS UEDIN UEDIN-CUNI UQAM_TAN L E VOLC T RANS W E C HAT WMTB IOMED BASELINE YOLO ZLABS - NLP Institution Air Force Research Laboratory (Gwinnup and Anderson, 2020) Independent submission (Xv, 2020) Charles University (Popel, 2020, 2018; Kocmi, 2020) Dublin City University (Parthasarathy et al., 2020) DeepMind (Yu et al., 2020) DiDi AI Labs (Chen et al., 2020b) (no associated paper) Indepdendent Submission (Kim et al., 2020) eTranslation (Oravecz et al., 2020) Facebook AI (Chen et al., 2020a) University of Groningen (Roest et al., 2020; Dhar et al., 2020) Global Tone Communication (Bei et al., 2020) University of Helsinki and Aalto University (Scherrer et al., 2020a) Huawei TSC (Wei et al., 2020a) Institute of Information Engineering, Chinese Academy of Sciences (Wei et al., 2020b) Microsoft STC India (Goyal et al., 2020) NICT-Kyoto (Marie et al., 2020) NICT-Rui (Li et al., 2020) NiuTrans (Zhang et al., 2020) National Research Council Canada (Knowles et al., 2020) OPPO (Shi et al., 2020) PROMT (Molchanov, 2020) SJTU-NICT (Li et al., 2020) Samsung Research Poland (Krubi´nski et al., 2020) TALP UPC (Escolano et al., 2020) Tencent Translation (Wu et al., 2020b) NLP Lab at Tsinghua Universit"
2020.wmt-1.1,2020.wmt-1.38,0,0.0746945,"Missing"
2020.wmt-1.1,2020.wmt-1.54,1,0.802974,"Missing"
2020.wmt-1.1,W07-0718,1,0.671054,"Missing"
2020.wmt-1.1,W08-0309,1,0.762341,"Missing"
2020.wmt-1.1,W12-3102,1,0.500805,"Missing"
2020.wmt-1.1,2020.lrec-1.461,0,0.0795779,"Missing"
2020.wmt-1.1,2012.eamt-1.60,0,0.124643,"tted systems, for those where the authors provided such details. The training corpus for Inuktitut↔English is the recently released Nunavut Hansard Inuktitut– English Parallel Corpus 3.0 (Joanis et al., 2020). For the Japanese↔English tasks, we added several freely available parallel corpora to the training data. It includes JParaCrawl v2.0 (Morishita et al., 2020), a large web-based parallel corpus, Japanese-English Subtitle Corpus (JESC) (Pryzant et al., 2017), the Kyoto Free Translation Task (KFTT) corpus (Neubig, 2011), constructed from the Kyoto-related Wikipedia articles, and TED Talks (Cettolo et al., 2012). The monolingual data we provided was similar to last year’s, with a 2019 news crawl added to all the news corpora. In addition, we provided versions of the news corpora for Czech, English and German, with both the document and paragraph structure retained. In other words, we did not apply sentence splitting to these corpora, and we retained the document boundaries and text ordering of the originals. Training, development, and test data for Pashto↔English and Khmer↔English are shared with the Parallel Corpus Filtering Shared Task (Koehn et al., 2020). The training data mostly comes from OPUS"
2020.wmt-1.1,2020.wmt-1.3,0,0.0731913,"on Machine Translation (WMT20)1 was held online with EMNLP 2020 and hosted a number of shared tasks on various aspects of machine translation. This conference built on 14 previous editions of WMT as workshops and conferences (Koehn and Monz, 2006; CallisonBurch et al., 2007, 2008, 2009, 2010, 2011, 2012; Bojar et al., 2013, 2014, 2015, 2016, 2017, 2018; Barrault et al., 2019). This year we conducted several official tasks. We report in this paper on the news and similar translation tasks. Additional shared tasks are described in separate papers in these proceedings: • automatic post-editing (Chatterjee et al., 2020) • biomedical translation (Bawden et al., 2020b) • chat translation (Farajian et al., 2020) • lifelong learning (Barrault et al., 2020) 1 Makoto Morishita NTT Santanu Pal WIPRO AI Abstract 1 Philipp Koehn JHU http://www.statmt.org/wmt20/ 1 Proceedings of the 5th Conference on Machine Translation (WMT), pages 1–55 c Online, November 19–20, 2020. 2020 Association for Computational Linguistics as “direct assessment”) that we explored in the previous years with convincing results in terms of the trade-off between annotation effort and reliable distinctions between systems. The primary objectives o"
2020.wmt-1.1,2020.wmt-1.8,0,0.0898111,"D D I D I -NLP DONG - NMT ENMT E T RANSLATION FACEBOOK AI G RONINGEN GTCOM H ELSINKI NLP H UAWEI TSC IIE M ICROSOFT STC I NDIA NICT-K YOTO NICT-RUI N IU T RANS NRC OPPO PROMT SJTU-NICT SRPOL TALP UPC T ENCENT T RANSLATION THUNLP T ILDE T OHOKU -AIP-NTT U BIQUS UEDIN UEDIN-CUNI UQAM_TAN L E VOLC T RANS W E C HAT WMTB IOMED BASELINE YOLO ZLABS - NLP Institution Air Force Research Laboratory (Gwinnup and Anderson, 2020) Independent submission (Xv, 2020) Charles University (Popel, 2020, 2018; Kocmi, 2020) Dublin City University (Parthasarathy et al., 2020) DeepMind (Yu et al., 2020) DiDi AI Labs (Chen et al., 2020b) (no associated paper) Indepdendent Submission (Kim et al., 2020) eTranslation (Oravecz et al., 2020) Facebook AI (Chen et al., 2020a) University of Groningen (Roest et al., 2020; Dhar et al., 2020) Global Tone Communication (Bei et al., 2020) University of Helsinki and Aalto University (Scherrer et al., 2020a) Huawei TSC (Wei et al., 2020a) Institute of Information Engineering, Chinese Academy of Sciences (Wei et al., 2020b) Microsoft STC India (Goyal et al., 2020) NICT-Kyoto (Marie et al., 2020) NICT-Rui (Li et al., 2020) NiuTrans (Zhang et al., 2020) National Research Council Canada (Know"
2020.wmt-1.1,2009.freeopmt-1.3,0,0.088081,"ve (CONTRASTIVE) or primary (PRIMARY), and the BLEU, RIBES and TER results. The scores are sorted by BLEU. In general, primary systems tend to be better than contrastive systems, as expected, but there are some exceptions. This year we recived major number of participants for the case of Indo-Aryan language group NUST-FJWU NUST-FJWU system is an extension of state-of-the-art Transformer model with hierarchical attention networks to incorporate contextual information. During training the model used back-translation. Prompsit This team is participating with a rulebased system based on Apertium (Forcada et al., 2009-11). Apertium is a free/open-source platform for developing rule-based machine translation systems and language technology that was first released in 2005. Apertium is hosted in Github where both language data and code are licensed under the GNU GPL. It is a research and business platform with a very active community that loves small languages. Language pairs are at a very different level of development and output quality in the platform, depending on two main variables: how much funded or in-kind effort has 32 5.4 i.e. Hindi–Marathi (in both directions). We received 22 submissions from 14 te"
2020.wmt-1.1,2020.wmt-1.80,0,0.0933589,"airs, to be evaluated on test sets consisting mainly of news stories. The task was also opened up to additional test suites to probe specific aspects of translation. In the similar language translation task, participants built machine translation systems for translating between closely related pairs of languages. Chi-kiu Lo NRC Masaaki Nagata NTT Marcos Zampieri Rochester Institute of Technology metrics (Mathur et al., 2020) parallel corpus filtering (Koehn et al., 2020) quality estimation (Specia et al., 2020a) robustness (Specia et al., 2020b) unsupervised and very low-resource translation (Fraser, 2020) In the news translation task (Section 2), participants were asked to translate a shared test set, optionally restricting themselves to the provided training data (“constrained” condition). We included 22 translation directions this year, with translation between English and each of Chinese, Czech, German and Russian, as well as French to and from German being repeated from last year, and English to and from Inuktitut, Japanese, Polish and Tamil being new for this year. Furthermore, English to and from Khmer and Pashto were included, using the same test sets as in the corpus filtering task. Th"
2020.wmt-1.1,W19-5204,0,0.0543621,"Missing"
2020.wmt-1.1,2020.emnlp-main.5,0,0.0410594,"luation of out-ofEnglish translations, HITs were generated using the same method as described for the SR+DC evaluation of into-English translations in Section 3.2.1 with minor modifications. Source-based DA allows to include human references in the evaluation as another system to provide an estimate of human performance. Human references were added to the pull of system outputs prior to sampling documents for tasks generation. If multiple references are available, which is the case for English→German (3 alternative reference translations, including 1 generated using the paraphrasing method of Freitag et al. (2020)) and English→Chinese (2 translations), each reference is assessed individually. Since the annotations are made by researchers and professional translators who ensure a betTable 11: Amount of data collected in the WMT20 manual document- and segment-level evaluation campaigns for bilingual/source-based evaluation out of English and nonEnglish pairs. et al., 2020; Laubli et al., 2020). It differs from SR+DC DA introduced in WMT19 (Bojar et al., 2019), and still used in into-English human evaluation this year, where a single segment from a document is provided on a screen at a time, followed by s"
2020.wmt-1.1,W18-3931,1,0.874637,"ese improvements to increased capacity (which allows increased sensitivity to long-range relations) of the models. 5 Similar Language Translation Most shared tasks at WMT (e.g. News, Biomedical) have historically dealt with translating texts from and to English. In recent years, we observed a growing interest in training systems to translate between languages other than English. This includes a number of papers applying MT to translate between pairs of closely-related languages, national language varieties, and dialects 27 of the same language (Zhang, 1998; Marujo et al., 2011; Hassani, 2017; Costa-jussà et al., 2018; Popovi´c et al., 2020). To address this topic, the first Similar Language Translation (SLT) shared task at WMT 2019 has been organized. It featured data from three pairs of closely-related languages from different language families: Spanish - Portuguese (Romance languages), Czech - Polish (Slavic languages), and Hindi - Nepali (IndoAryan languages). Following the success of the first SLT shared task at WMT 2019 and the interest of the community in this topic, we organize, for the second time at WMT, this shared task to evaluate the performance of state-of-the-art translation systems on trans"
2020.wmt-1.1,2020.wmt-1.18,0,0.0913945,"2020b) Microsoft STC India (Goyal et al., 2020) NICT-Kyoto (Marie et al., 2020) NICT-Rui (Li et al., 2020) NiuTrans (Zhang et al., 2020) National Research Council Canada (Knowles et al., 2020) OPPO (Shi et al., 2020) PROMT (Molchanov, 2020) SJTU-NICT (Li et al., 2020) Samsung Research Poland (Krubi´nski et al., 2020) TALP UPC (Escolano et al., 2020) Tencent Translation (Wu et al., 2020b) NLP Lab at Tsinghua University (no associated paper) Tilde (Krišlauks and Pinnis, 2020) Tohoku-AIP-NTT (Kiyono et al., 2020) Ubiqus (Hernandez and Nguyen, 2020) University of Edinburgh (Bawden et al., 2020a; Germann, 2020) University of Edinburgh and Charles University (Germann et al., 2020) Université du Québec à Montréal (no associated paper) ByteDance AI Lab (Wu et al., 2020a) WeChat (Meng et al., 2020) Baseline System from Biomedical Task (Bawden et al., 2020b) American University of Beirut (no associated paper) Zoho Corporation (no associated paper) Table 6: Participants in the shared translation task. Not all teams participated in all language pairs. The translations from the online systems were not submitted by their respective companies but were obtained by us, and are therefore anonymized in a fashion"
2020.wmt-1.1,2020.wmt-1.43,0,0.0835067,"Missing"
2020.wmt-1.1,2020.wmt-1.9,0,0.0939415,"Missing"
2020.wmt-1.1,2020.wmt-1.19,0,0.0674131,"Missing"
2020.wmt-1.1,2009.mtsummit-btm.6,0,0.103443,"Missing"
2020.wmt-1.1,W13-2305,1,0.929934,"work which can be well applied to different translation. directions. Techniques used in the submitted systems include optional multilingual pre-training (mRASP) for low resource languages, very deep Transformer or dynamic convolution models up to 50 encoder layers, iterative backtranslation, knowledge distillation, model ensemble and development set fine-tuning. The key ingredient of the process seems the strong focus on diversification of the (synthetic) training data, using multiple scalings of the Transformer model 3.1 Direct Assessment Since running a comparison of direct assessments (DA, Graham et al., 2013, 2014, 2016) and relative ranking in 2016 (Bojar et al., 2016) and verifying a high correlation of system rankings for the two methods, as well as the advantages of DA, such as quality controlled crowd-sourcing and linear growth relative to numbers of submissions, we have employed DA as the primary mechanism for evaluating systems. With DA human evaluation, 15 human assessors are asked to rate a given translation by how adequately it expresses the meaning of the corresponding reference translation or source language input on an analogue scale, which corresponds to an underlying absolute 0–100"
2020.wmt-1.1,E14-1047,1,0.888167,"Missing"
2020.wmt-1.1,2020.lrec-1.312,1,0.804196,"A screenshot of OCELoT is shown in Figure 5. For presentation of the results, systems are treated as either constrained or unconstrained, depending on whether their models were trained only on the provided data. Since we do not know how they were built, the online systems are treated as unconstrained during the automatic and human evaluations. In the rest of this section, we provide brief details of the submitted systems, for those where the authors provided such details. The training corpus for Inuktitut↔English is the recently released Nunavut Hansard Inuktitut– English Parallel Corpus 3.0 (Joanis et al., 2020). For the Japanese↔English tasks, we added several freely available parallel corpora to the training data. It includes JParaCrawl v2.0 (Morishita et al., 2020), a large web-based parallel corpus, Japanese-English Subtitle Corpus (JESC) (Pryzant et al., 2017), the Kyoto Free Translation Task (KFTT) corpus (Neubig, 2011), constructed from the Kyoto-related Wikipedia articles, and TED Talks (Cettolo et al., 2012). The monolingual data we provided was similar to last year’s, with a 2019 news crawl added to all the news corpora. In addition, we provided versions of the news corpora for Czech, Engli"
2020.wmt-1.1,2020.emnlp-main.6,1,0.839606,"shown in Table 3, where the first and second are simple merges or splits, whereas the third is a rare case of more complex reordering. We leave a detailed analysis of the translators’ treatment of paragraph-split data for future work. development set is provided, it is a mixture of both “source-original” and “target-original” texts, in order to maximise its size, although the original language is always marked in the sgm file, except for Inuktitut↔English. The consequences of directionality in test sets has been discussed recently in the literature (Freitag et al., 2019; Laubli et al., 2020; Graham et al., 2020), and the conclusion is that it can have an effect on detrimental effect on the accuracy of system evaluation. We use “source-original” parallel sentences wherever possible, on the basis that it is the more realistic scenario for practical MT usage. Exception: the test sets for the two Inuktitut↔English translation directions contain the same data, without regard to original direction. For most news text in the test and development sets, English was the original language and Inuktitut the translation, while the parliamentary data mixes the two directions. The origins of the news test documents"
2020.wmt-1.1,2020.wmt-1.11,0,0.0940191,"N GTCOM H ELSINKI NLP H UAWEI TSC IIE M ICROSOFT STC I NDIA NICT-K YOTO NICT-RUI N IU T RANS NRC OPPO PROMT SJTU-NICT SRPOL TALP UPC T ENCENT T RANSLATION THUNLP T ILDE T OHOKU -AIP-NTT U BIQUS UEDIN UEDIN-CUNI UQAM_TAN L E VOLC T RANS W E C HAT WMTB IOMED BASELINE YOLO ZLABS - NLP Institution Air Force Research Laboratory (Gwinnup and Anderson, 2020) Independent submission (Xv, 2020) Charles University (Popel, 2020, 2018; Kocmi, 2020) Dublin City University (Parthasarathy et al., 2020) DeepMind (Yu et al., 2020) DiDi AI Labs (Chen et al., 2020b) (no associated paper) Indepdendent Submission (Kim et al., 2020) eTranslation (Oravecz et al., 2020) Facebook AI (Chen et al., 2020a) University of Groningen (Roest et al., 2020; Dhar et al., 2020) Global Tone Communication (Bei et al., 2020) University of Helsinki and Aalto University (Scherrer et al., 2020a) Huawei TSC (Wei et al., 2020a) Institute of Information Engineering, Chinese Academy of Sciences (Wei et al., 2020b) Microsoft STC India (Goyal et al., 2020) NICT-Kyoto (Marie et al., 2020) NICT-Rui (Li et al., 2020) NiuTrans (Zhang et al., 2020) National Research Council Canada (Knowles et al., 2020) OPPO (Shi et al., 2020) PROMT (Molchanov, 2020) S"
2020.wmt-1.1,D19-1632,1,0.881933,"ent and paragraph structure retained. In other words, we did not apply sentence splitting to these corpora, and we retained the document boundaries and text ordering of the originals. Training, development, and test data for Pashto↔English and Khmer↔English are shared with the Parallel Corpus Filtering Shared Task (Koehn et al., 2020). The training data mostly comes from OPUS (software localization, Tatoeba, Global Voices), the Bible, and specialprepared corpora from TED Talks and the Jehova Witness web site (JW300). The development and test sets were created as part of the Flores initiative (Guzmán et al., 2019) by professional translation of Wikipedia content with careful vetting of the translations. Please refer the to the Parallel Corpus Filtering Shared Task overview paper for details on these corpora. 2.3.1 AFRL (Gwinnup and Anderson, 2020) AFRL - SYSCOMB 20 is a system combination consisting of two Marian transformer ensembles, one OpenNMT transformer system and a Moses phrase-based system. AFRL - FINETUNE is an OpenNMT transformer system fine-tuned on newstest2014-2017. 2.3.2 (Xv, 2020) ARIEL XV is a Transformer base model trained with the Sockeye sequence modeling toolkit usSome statistics ab"
2020.wmt-1.1,2020.wmt-1.12,1,0.754946,"Missing"
2020.wmt-1.1,2020.wmt-1.13,0,0.0737827,"2020b) (no associated paper) Indepdendent Submission (Kim et al., 2020) eTranslation (Oravecz et al., 2020) Facebook AI (Chen et al., 2020a) University of Groningen (Roest et al., 2020; Dhar et al., 2020) Global Tone Communication (Bei et al., 2020) University of Helsinki and Aalto University (Scherrer et al., 2020a) Huawei TSC (Wei et al., 2020a) Institute of Information Engineering, Chinese Academy of Sciences (Wei et al., 2020b) Microsoft STC India (Goyal et al., 2020) NICT-Kyoto (Marie et al., 2020) NICT-Rui (Li et al., 2020) NiuTrans (Zhang et al., 2020) National Research Council Canada (Knowles et al., 2020) OPPO (Shi et al., 2020) PROMT (Molchanov, 2020) SJTU-NICT (Li et al., 2020) Samsung Research Poland (Krubi´nski et al., 2020) TALP UPC (Escolano et al., 2020) Tencent Translation (Wu et al., 2020b) NLP Lab at Tsinghua University (no associated paper) Tilde (Krišlauks and Pinnis, 2020) Tohoku-AIP-NTT (Kiyono et al., 2020) Ubiqus (Hernandez and Nguyen, 2020) University of Edinburgh (Bawden et al., 2020a; Germann, 2020) University of Edinburgh and Charles University (Germann et al., 2020) Université du Québec à Montréal (no associated paper) ByteDance AI Lab (Wu et al., 2020a) WeChat (Meng et al"
2020.wmt-1.1,2020.wmt-1.20,0,0.057602,"Missing"
2020.wmt-1.1,2020.wmt-1.14,1,0.820019,"set, and the origlang tag indicates the original source language. 9 Team AFRL ARIEL XV CUNI DCU D EEP M IND D I D I -NLP DONG - NMT ENMT E T RANSLATION FACEBOOK AI G RONINGEN GTCOM H ELSINKI NLP H UAWEI TSC IIE M ICROSOFT STC I NDIA NICT-K YOTO NICT-RUI N IU T RANS NRC OPPO PROMT SJTU-NICT SRPOL TALP UPC T ENCENT T RANSLATION THUNLP T ILDE T OHOKU -AIP-NTT U BIQUS UEDIN UEDIN-CUNI UQAM_TAN L E VOLC T RANS W E C HAT WMTB IOMED BASELINE YOLO ZLABS - NLP Institution Air Force Research Laboratory (Gwinnup and Anderson, 2020) Independent submission (Xv, 2020) Charles University (Popel, 2020, 2018; Kocmi, 2020) Dublin City University (Parthasarathy et al., 2020) DeepMind (Yu et al., 2020) DiDi AI Labs (Chen et al., 2020b) (no associated paper) Indepdendent Submission (Kim et al., 2020) eTranslation (Oravecz et al., 2020) Facebook AI (Chen et al., 2020a) University of Groningen (Roest et al., 2020; Dhar et al., 2020) Global Tone Communication (Bei et al., 2020) University of Helsinki and Aalto University (Scherrer et al., 2020a) Huawei TSC (Wei et al., 2020a) Institute of Information Engineering, Chinese Academy of Sciences (Wei et al., 2020b) Microsoft STC India (Goyal et al., 2020) NICT-Kyoto (Mari"
2020.wmt-1.1,2020.wmt-1.39,1,0.812433,"kables was collected in the first phase of the annotation, which amounted to 4k assessments across the systems. The second annotation phase with 6.5k assessments compared markable translations, always checking outputs of all the 13 competing MT systems but still considering the document-level context of each of them. Among other things, the observations indicate that the better the system, the lower the variance in manual scores. Markables annotation then confirms that frequent errors like bad translation of a term need not be the most severe and conversely, 4.1.3 Gender Coreference and Bias (Kocmi et al., 2020) The test suite by Kocmi et al. (2020) focuses on the gender bias in professions (e.g. physician, teacher, secretary) for the translation from English into Czech, German, Polish and Russian. These nouns are ambiguous with respect to gender in English but exhibit gender in the examined target languages. The test suite is based on the fact that a pronoun referring to the ambiguous noun can reveal the gender of the noun in the English source sentence. Once disambiguated, the gender needs to be preserved in translation. To correctly translate the given noun, the translation system thus has to corr"
2020.wmt-1.1,2020.wmt-1.53,0,0.089538,"Missing"
2020.wmt-1.1,2020.wmt-1.78,1,0.815194,"rence on Machine Translation (WMT) 2020. In the news task, participants were asked to build machine translation systems for any of 11 language pairs, to be evaluated on test sets consisting mainly of news stories. The task was also opened up to additional test suites to probe specific aspects of translation. In the similar language translation task, participants built machine translation systems for translating between closely related pairs of languages. Chi-kiu Lo NRC Masaaki Nagata NTT Marcos Zampieri Rochester Institute of Technology metrics (Mathur et al., 2020) parallel corpus filtering (Koehn et al., 2020) quality estimation (Specia et al., 2020a) robustness (Specia et al., 2020b) unsupervised and very low-resource translation (Fraser, 2020) In the news translation task (Section 2), participants were asked to translate a shared test set, optionally restricting themselves to the provided training data (“constrained” condition). We included 22 translation directions this year, with translation between English and each of Chinese, Czech, German and Russian, as well as French to and from German being repeated from last year, and English to and from Inuktitut, Japanese, Polish and Tamil being new fo"
2020.wmt-1.1,W17-1208,0,0.0524248,"ttribute all these improvements to increased capacity (which allows increased sensitivity to long-range relations) of the models. 5 Similar Language Translation Most shared tasks at WMT (e.g. News, Biomedical) have historically dealt with translating texts from and to English. In recent years, we observed a growing interest in training systems to translate between languages other than English. This includes a number of papers applying MT to translate between pairs of closely-related languages, national language varieties, and dialects 27 of the same language (Zhang, 1998; Marujo et al., 2011; Hassani, 2017; Costa-jussà et al., 2018; Popovi´c et al., 2020). To address this topic, the first Similar Language Translation (SLT) shared task at WMT 2019 has been organized. It featured data from three pairs of closely-related languages from different language families: Spanish - Portuguese (Romance languages), Czech - Polish (Slavic languages), and Hindi - Nepali (IndoAryan languages). Following the success of the first SLT shared task at WMT 2019 and the interest of the community in this topic, we organize, for the second time at WMT, this shared task to evaluate the performance of state-of-the-art tr"
2020.wmt-1.1,2020.wmt-1.21,0,0.0791885,"Missing"
2020.wmt-1.1,2020.wmt-1.23,0,0.0607352,"020) Dublin City University (Parthasarathy et al., 2020) DeepMind (Yu et al., 2020) DiDi AI Labs (Chen et al., 2020b) (no associated paper) Indepdendent Submission (Kim et al., 2020) eTranslation (Oravecz et al., 2020) Facebook AI (Chen et al., 2020a) University of Groningen (Roest et al., 2020; Dhar et al., 2020) Global Tone Communication (Bei et al., 2020) University of Helsinki and Aalto University (Scherrer et al., 2020a) Huawei TSC (Wei et al., 2020a) Institute of Information Engineering, Chinese Academy of Sciences (Wei et al., 2020b) Microsoft STC India (Goyal et al., 2020) NICT-Kyoto (Marie et al., 2020) NICT-Rui (Li et al., 2020) NiuTrans (Zhang et al., 2020) National Research Council Canada (Knowles et al., 2020) OPPO (Shi et al., 2020) PROMT (Molchanov, 2020) SJTU-NICT (Li et al., 2020) Samsung Research Poland (Krubi´nski et al., 2020) TALP UPC (Escolano et al., 2020) Tencent Translation (Wu et al., 2020b) NLP Lab at Tsinghua University (no associated paper) Tilde (Krišlauks and Pinnis, 2020) Tohoku-AIP-NTT (Kiyono et al., 2020) Ubiqus (Hernandez and Nguyen, 2020) University of Edinburgh (Bawden et al., 2020a; Germann, 2020) University of Edinburgh and Charles University (Germann et al., 2"
2020.wmt-1.1,2020.wmt-1.77,1,0.84299,"slation task, both organised alongside the Conference on Machine Translation (WMT) 2020. In the news task, participants were asked to build machine translation systems for any of 11 language pairs, to be evaluated on test sets consisting mainly of news stories. The task was also opened up to additional test suites to probe specific aspects of translation. In the similar language translation task, participants built machine translation systems for translating between closely related pairs of languages. Chi-kiu Lo NRC Masaaki Nagata NTT Marcos Zampieri Rochester Institute of Technology metrics (Mathur et al., 2020) parallel corpus filtering (Koehn et al., 2020) quality estimation (Specia et al., 2020a) robustness (Specia et al., 2020b) unsupervised and very low-resource translation (Fraser, 2020) In the news translation task (Section 2), participants were asked to translate a shared test set, optionally restricting themselves to the provided training data (“constrained” condition). We included 22 translation directions this year, with translation between English and each of Chinese, Czech, German and Russian, as well as French to and from German being repeated from last year, and English to and from Inu"
2020.wmt-1.1,2020.wmt-1.24,0,0.0435945,"Missing"
2020.wmt-1.1,D18-1512,0,0.05415,"Missing"
2020.wmt-1.1,2020.wmt-1.47,1,0.740067,"Missing"
2020.wmt-1.1,W18-3601,1,0.891679,", we have employed DA as the primary mechanism for evaluating systems. With DA human evaluation, 15 human assessors are asked to rate a given translation by how adequately it expresses the meaning of the corresponding reference translation or source language input on an analogue scale, which corresponds to an underlying absolute 0–100 rating scale.5 No sentence or document length restriction is applied during manual evaluation. Direct Assessment is also employed for evaluation of video captioning systems at TRECvid (Graham et al., 2018; Awad et al., 2019) and multilingual surface realisation (Mille et al., 2018, 2019). 3.1.1 tion 2, most of our test sets do not include reversecreated sentence pairs, except when there were resource constraints on the creation of the test sets. 3.1.3 Prior to WMT19, the issue of including document context was raised within the community (Läubli et al., 2018; Toral et al., 2018) and at WMT19 a range of DA styles were subsequently tested that included document context. In WMT19, two options were run, firstly, an evaluation that included the document context “+DC” (with document context), and secondly, a variation that omitted document context “−DC” (without document con"
2020.wmt-1.1,2020.wmt-1.27,0,0.247738,"he original source language. 9 Team AFRL ARIEL XV CUNI DCU D EEP M IND D I D I -NLP DONG - NMT ENMT E T RANSLATION FACEBOOK AI G RONINGEN GTCOM H ELSINKI NLP H UAWEI TSC IIE M ICROSOFT STC I NDIA NICT-K YOTO NICT-RUI N IU T RANS NRC OPPO PROMT SJTU-NICT SRPOL TALP UPC T ENCENT T RANSLATION THUNLP T ILDE T OHOKU -AIP-NTT U BIQUS UEDIN UEDIN-CUNI UQAM_TAN L E VOLC T RANS W E C HAT WMTB IOMED BASELINE YOLO ZLABS - NLP Institution Air Force Research Laboratory (Gwinnup and Anderson, 2020) Independent submission (Xv, 2020) Charles University (Popel, 2020, 2018; Kocmi, 2020) Dublin City University (Parthasarathy et al., 2020) DeepMind (Yu et al., 2020) DiDi AI Labs (Chen et al., 2020b) (no associated paper) Indepdendent Submission (Kim et al., 2020) eTranslation (Oravecz et al., 2020) Facebook AI (Chen et al., 2020a) University of Groningen (Roest et al., 2020; Dhar et al., 2020) Global Tone Communication (Bei et al., 2020) University of Helsinki and Aalto University (Scherrer et al., 2020a) Huawei TSC (Wei et al., 2020a) Institute of Information Engineering, Chinese Academy of Sciences (Wei et al., 2020b) Microsoft STC India (Goyal et al., 2020) NICT-Kyoto (Marie et al., 2020) NICT-Rui (Li et al., 2020) NiuTrans"
2020.wmt-1.1,D19-6301,1,0.888512,"Missing"
2020.wmt-1.1,W18-6424,0,0.0431841,"(Kocmi, 2020) combines transfer learning from a high-resource language pair Czech–English into the low-resource Inuktitut-English with an additional backtranslation step. Surprising behaviour is noticed when using synthetic data, which can be possibly attributed to a narrow domain of training and test data. The system is the Transformer model in a constrained submission. 2.3.3 Charles University (CUNI) CUNI-D OC T RANSFORMER (Popel, 2020) is similar to the sentence-level version (CUNI-T2T2018, CUBBITT), but trained on sequences with multiple sentences of up to 3000 characters. CUNI-T2T-2018 (Popel, 2018), also called CUBBITT, is exactly the same system as in WMT2018. It is the Transformer model trained according to Popel and Bojar (2018) plus a novel concat-regime backtranslation with checkpoint averaging (Popel et al., 2020), tuned separately for CZ-domain and non CZ-domain articles, possibly handling also translation-direction (“translationese”) issues. For cs→en also a coreference preprocessing was used adding the female-gender CUNI-T RANSFORMER (Popel, 2020) is similar to the WMT2018 version of CUBBITT, but with 12 encoder layers instead of 6 and trained on CzEng 2.0 instead of CzEng 1.7."
2020.wmt-1.1,2020.wmt-1.25,0,0.094349,"Missing"
2020.wmt-1.1,2020.wmt-1.28,0,0.0792624,"cument in the test set, and the origlang tag indicates the original source language. 9 Team AFRL ARIEL XV CUNI DCU D EEP M IND D I D I -NLP DONG - NMT ENMT E T RANSLATION FACEBOOK AI G RONINGEN GTCOM H ELSINKI NLP H UAWEI TSC IIE M ICROSOFT STC I NDIA NICT-K YOTO NICT-RUI N IU T RANS NRC OPPO PROMT SJTU-NICT SRPOL TALP UPC T ENCENT T RANSLATION THUNLP T ILDE T OHOKU -AIP-NTT U BIQUS UEDIN UEDIN-CUNI UQAM_TAN L E VOLC T RANS W E C HAT WMTB IOMED BASELINE YOLO ZLABS - NLP Institution Air Force Research Laboratory (Gwinnup and Anderson, 2020) Independent submission (Xv, 2020) Charles University (Popel, 2020, 2018; Kocmi, 2020) Dublin City University (Parthasarathy et al., 2020) DeepMind (Yu et al., 2020) DiDi AI Labs (Chen et al., 2020b) (no associated paper) Indepdendent Submission (Kim et al., 2020) eTranslation (Oravecz et al., 2020) Facebook AI (Chen et al., 2020a) University of Groningen (Roest et al., 2020; Dhar et al., 2020) Global Tone Communication (Bei et al., 2020) University of Helsinki and Aalto University (Scherrer et al., 2020a) Huawei TSC (Wei et al., 2020a) Institute of Information Engineering, Chinese Academy of Sciences (Wei et al., 2020b) Microsoft STC India (Goyal et al., 20"
2020.wmt-1.1,2020.lrec-1.443,1,0.79707,"er their models were trained only on the provided data. Since we do not know how they were built, the online systems are treated as unconstrained during the automatic and human evaluations. In the rest of this section, we provide brief details of the submitted systems, for those where the authors provided such details. The training corpus for Inuktitut↔English is the recently released Nunavut Hansard Inuktitut– English Parallel Corpus 3.0 (Joanis et al., 2020). For the Japanese↔English tasks, we added several freely available parallel corpora to the training data. It includes JParaCrawl v2.0 (Morishita et al., 2020), a large web-based parallel corpus, Japanese-English Subtitle Corpus (JESC) (Pryzant et al., 2017), the Kyoto Free Translation Task (KFTT) corpus (Neubig, 2011), constructed from the Kyoto-related Wikipedia articles, and TED Talks (Cettolo et al., 2012). The monolingual data we provided was similar to last year’s, with a 2019 news crawl added to all the news corpora. In addition, we provided versions of the news corpora for Czech, English and German, with both the document and paragraph structure retained. In other words, we did not apply sentence splitting to these corpora, and we retained t"
2020.wmt-1.1,2020.wmt-1.48,0,0.090424,"Missing"
2020.wmt-1.1,2020.wmt-1.49,0,0.0519886,"Missing"
2020.wmt-1.1,W19-6712,0,0.0573476,"tly crawled multilingual parallel corpora from Indian government websites (Haddow and Kirefu, 2020; Siripragada et al., 2020), the Tanzil corpus (Tiedemann, 2009), the Pavlick dicParagraph-split Test Sets For the language pairs English↔Czech, English↔German and English→Chinese, we provided the translators with paragraph-split texts, instead of sentence-split texts. We did this in order to provide the translators with greater freedom and, hopefully, to improve the quality of the translation. Allowing translators to merge and split sentences removes one of the “translation shifts” identified by Popovic (2019), which can make translations create solely for MT evaluation different from translations produced for other purposes. We first show some descriptive statistics of the source texts, for Czech, English and German, in 3 Europarl Parallel Corpus Czech ↔ English German ↔ English Polish↔ English German ↔ French Sentences 645,241 1,825,745 632,435 1,801,076 Words 14,948,900 17,380,340 48,125,573 50,506,059 14,691,199 16,995,232 47,517,102 55,366,136 Distinct words 172,452 63,289 371,748 113,960 170,271 62,694 368,585 134,762 News Commentary Parallel Corpus Czech ↔ English 248,927 5,570,734 6,156,063"
2020.wmt-1.1,2020.wmt-1.26,0,0.0845151,"Missing"
2020.wmt-1.1,2020.vardial-1.10,0,0.0933804,"Missing"
2020.wmt-1.1,W18-6301,0,0.038239,"Missing"
2020.wmt-1.1,2020.wmt-1.51,0,0.0917045,"Missing"
2020.wmt-1.1,2020.wmt-1.50,1,0.78567,"Missing"
2020.wmt-1.1,2020.wmt-1.52,0,0.0485419,"Missing"
2020.wmt-1.1,P19-1164,0,0.0581517,"26 26.37 25.51 24.82 28.33 23.33 21.13 21.96 20.43 22.90 22.58 21.90 22.17 22.17 20.53 19.40 20.01 40.44 32.39 30.39 37.04 32.27 27.54 25.97 26.09 46.38 37.30 36.05 35.96 33.76 33.07 27.20 27.07 Table 15: TICO-19 test suite results on the English-to-X WMT20 translation directions. 26 4.1.5 antecedent (a less common direction of information flow), and then correctly express the noun in the target language. The success of the MT system in this test can be established automatically, whenever the gender of the target word can be automatically identified. Kocmi et al. (2020) build upon the WinoMT (Stanovsky et al., 2019) test set, which provides exactly the necessary type of sentences containing an ambiguous profession noun and a personal pronoun which unambiguously (for the human eye) refers to it based the situation described. When extending WinMT with Czech and Polish, Stanovsky et al. have to disregard some test patterns but the principle remains. The results indicate that all MT systems fail in this test, following gender bias (stereotypical patterns attributing the masculine gender to some professions and feminine gender to others) rather than the coreference link. Word Sense Disambiguation (Scherrer et"
2020.wmt-1.1,2020.wmt-1.31,0,0.0881792,"morphological segmentation of the polysynthetic Inuktitut, testing rule-based, supervised, semi-supervised as well as unsupervised word segmentation methods, (2) whether or not adding data from a related language (Greenlandic) helps, and (3) whether contextual word embeddings (XLM) improve translation. G RONINGEN - ENIU use Transformer implemented in Marian with the default setting, improving the performance also with tagged backtranslation, domain-specific data, ensembling and finetuning. 2.3.7 DONG - NMT (no associated paper) No description provided. 2.3.8 ENMT (Kim et al., 2020) Kim et al. (2020) base their approach on transferring knowledge of domain and linguistic characteristics by pre-training the encoder-decoder model with large amount of in-domain monolingual data through unsupervised and supervised prediction task. The model is then fine-tuned with parallel data and in-domain synthetic data, generated with iterative back-translation. For additional gain, final results are generated with an ensemble model and re-ranked with averaged models and language models. G RONINGEN - ENTAM (Dhar et al., 2020) study the effects of various techniques such as linguistically motivated segmenta"
2020.wmt-1.1,2020.wmt-1.32,0,0.0839317,"- NLP Institution Air Force Research Laboratory (Gwinnup and Anderson, 2020) Independent submission (Xv, 2020) Charles University (Popel, 2020, 2018; Kocmi, 2020) Dublin City University (Parthasarathy et al., 2020) DeepMind (Yu et al., 2020) DiDi AI Labs (Chen et al., 2020b) (no associated paper) Indepdendent Submission (Kim et al., 2020) eTranslation (Oravecz et al., 2020) Facebook AI (Chen et al., 2020a) University of Groningen (Roest et al., 2020; Dhar et al., 2020) Global Tone Communication (Bei et al., 2020) University of Helsinki and Aalto University (Scherrer et al., 2020a) Huawei TSC (Wei et al., 2020a) Institute of Information Engineering, Chinese Academy of Sciences (Wei et al., 2020b) Microsoft STC India (Goyal et al., 2020) NICT-Kyoto (Marie et al., 2020) NICT-Rui (Li et al., 2020) NiuTrans (Zhang et al., 2020) National Research Council Canada (Knowles et al., 2020) OPPO (Shi et al., 2020) PROMT (Molchanov, 2020) SJTU-NICT (Li et al., 2020) Samsung Research Poland (Krubi´nski et al., 2020) TALP UPC (Escolano et al., 2020) Tencent Translation (Wu et al., 2020b) NLP Lab at Tsinghua University (no associated paper) Tilde (Krišlauks and Pinnis, 2020) Tohoku-AIP-NTT (Kiyono et al., 2020) Ub"
2020.wmt-1.1,2020.wmt-1.33,0,0.080166,"Missing"
2020.wmt-1.1,2020.wmt-1.34,0,0.0803867,"Missing"
2020.wmt-1.1,2020.wmt-1.35,0,0.0951745,"ss web site (JW300). The development and test sets were created as part of the Flores initiative (Guzmán et al., 2019) by professional translation of Wikipedia content with careful vetting of the translations. Please refer the to the Parallel Corpus Filtering Shared Task overview paper for details on these corpora. 2.3.1 AFRL (Gwinnup and Anderson, 2020) AFRL - SYSCOMB 20 is a system combination consisting of two Marian transformer ensembles, one OpenNMT transformer system and a Moses phrase-based system. AFRL - FINETUNE is an OpenNMT transformer system fine-tuned on newstest2014-2017. 2.3.2 (Xv, 2020) ARIEL XV is a Transformer base model trained with the Sockeye sequence modeling toolkit usSome statistics about the training and test materials are given in Figures 1, 2, 3 and 4. 4 8 ARIEL XV https://github.com/AppraiseDev/OCELoT English I English II English III Chinese Czech German Inuktitut Japanese Polish Russian Tamil ABC News (2), All Africa (5), Brisbane Times (1), CBS LA (1), CBS News (1), CNBC (3), CNN (2), Daily Express (1), Daily Mail (2), Fox News (1), Gateway (1), Guardian (3), Huffington Post (2), London Evening Standard (2), Metro (2), NDTV (7), RTE (7), Reuters (4), STV (2), S"
2020.wmt-1.1,2020.wmt-1.55,0,0.0877526,"Missing"
2020.wmt-1.1,P17-4012,0,0.0273892,"o SJTU-NICT using large XLM model to improve NMT but the exact relation is unclear. 2.3.14 H UAWEI TSC (Wei et al., 2020a) H UAWEI TSC use Transformer-big with a further increased model size, focussing on standard techniques of careful pre-processing and filtering, back-translation and forward translation, including self-training, i.e. translating one of the sides of the original parallel data. Ensembling of individual training runs is used in the forward as well as backward translation, and single models are created from the ensembles using knowledge distillation. The submission uses THUNMT (Zhang et al., 2017) open-source engine. 2.3.19 N IU T RANS (Zhang et al., 2020) N IU T RANS gain their performance from focussed attention to six areas: (1) careful data preprocessing and filtering, (2) iterative back-translation to generate additional training data, (3) using different model architectures, such as wider and/or deeper models, relative position representation and relative length, to enhance the diversity of translations, (4) iterative knowledge distillation by in-domain monolingual data, (5) iterative finetuning for domain adaptation using small training batches, (6) rule-based post-processing of"
2020.wmt-1.1,P98-2238,0,0.590812,"and punctuation, and we tend to attribute all these improvements to increased capacity (which allows increased sensitivity to long-range relations) of the models. 5 Similar Language Translation Most shared tasks at WMT (e.g. News, Biomedical) have historically dealt with translating texts from and to English. In recent years, we observed a growing interest in training systems to translate between languages other than English. This includes a number of papers applying MT to translate between pairs of closely-related languages, national language varieties, and dialects 27 of the same language (Zhang, 1998; Marujo et al., 2011; Hassani, 2017; Costa-jussà et al., 2018; Popovi´c et al., 2020). To address this topic, the first Similar Language Translation (SLT) shared task at WMT 2019 has been organized. It featured data from three pairs of closely-related languages from different language families: Spanish - Portuguese (Romance languages), Czech - Polish (Slavic languages), and Hindi - Nepali (IndoAryan languages). Following the success of the first SLT shared task at WMT 2019 and the interest of the community in this topic, we organize, for the second time at WMT, this shared task to evaluate th"
2020.wmt-1.1,2020.wmt-1.41,1,0.814291,"Missing"
2020.wmt-1.50,W14-3302,0,0.219263,"In this paper we describe the WIPRO-RIT submission to the SLT 2020 Indo-Aryan track. Our WIPRO-RIT system is based on the model described in Johnson et al. (2017). WIPRORIT achieved competitive performance ranking 1st in Marathi to Hindi and 2nd in Hindi to Marathi translation among 22 systems. 424 Proceedings of the 5th Conference on Machine Translation (WMT), pages 424–429 c Online, November 19–20, 2020. 2020 Association for Computational Linguistics 2 Related Work using a Hindi–English NMT system. The Hindi–English NMT system was trained on English–Hindi parallel data released in WMT 2014 (Bojar et al., 2014), IITB parallel corpus (Kunchukuttan et al., 2018), the parallel dataset was collected from news (Siripragada et al., 2020) and the PMIndia (Haddow and Kirefu, 2020) parallel corpus (see Table 1). With the substantial performance improvements brought to MT by neural approaches, a growing interest in translating between pairs of similar languages, language varieties, and dialects has been observed. Recent studies have addressed MT between Arabic dialects (Harrat et al., 2019; Shapiro and Duh, 2019) Catalan and Spanish, Croatian and Serbian (Popović et al., 2020), (Costa-jussà, 2017), Brazilian"
2020.wmt-1.50,W17-1207,0,0.0176905,"WMT 2014 (Bojar et al., 2014), IITB parallel corpus (Kunchukuttan et al., 2018), the parallel dataset was collected from news (Siripragada et al., 2020) and the PMIndia (Haddow and Kirefu, 2020) parallel corpus (see Table 1). With the substantial performance improvements brought to MT by neural approaches, a growing interest in translating between pairs of similar languages, language varieties, and dialects has been observed. Recent studies have addressed MT between Arabic dialects (Harrat et al., 2019; Shapiro and Duh, 2019) Catalan and Spanish, Croatian and Serbian (Popović et al., 2020), (Costa-jussà, 2017), Brazilian and European Portuguese (Costajussà et al., 2018), and several pairs of languages and language varieties such as Brazilian and European Portuguese, Canadian and European French, and similar languages such as Croatian and Serbian, and Indonesian and Malay (Lakew et al., 2018). The interest on diatopic language variation is evidenced by the recent iterations of the VarDial workshop in which papers on MT applied to similar languages varieties, and dialects (Shapiro and Duh, 2019; Myint Oo et al., 2019; Popović et al., 2020) have been presented along with evaluation campaigns featuring"
2020.wmt-1.50,W18-3931,1,0.817983,"Missing"
2020.wmt-1.50,2020.vardial-1.1,1,0.712774,"nguages such as Croatian and Serbian, and Indonesian and Malay (Lakew et al., 2018). The interest on diatopic language variation is evidenced by the recent iterations of the VarDial workshop in which papers on MT applied to similar languages varieties, and dialects (Shapiro and Duh, 2019; Myint Oo et al., 2019; Popović et al., 2020) have been presented along with evaluation campaigns featuring multiple shared tasks on a number of related topics such as cross-lingual morphological analysis, cross-lingual parsing, dialect identification, and morphosyntactic tagging (Zampieri et al., 2018, 2019; Găman et al., 2020). 3 Data Sources WMT News IITB PM India Total Remove duplicates Cleaning∗ #sentences 273,885 156,344 1,561,840 56,831 2,048,900 1,464,419 961,036 Table 1: English–Hindi parallel data statistics. ∗ Removing noisy mixed language sentences. We also back-translated 5 million Marathi monolingual sehments using our WIPRO-RIT CONTRASTIVE 1 system described in more detail Section 6. For Marathi–Hindi we did not use any back translation data in our CONTRASTIVE 2 and PRIMARY submissions. In the both cases 5 million English–Hindi backtranslation data provide significant (p < 0.01) improvements over CONTR"
2020.wmt-1.50,D10-1092,0,0.0410104,"der subword units (Sennrich et al., 2016) by using byte-pair encoding (BPE). In the preprocessing step, instead of learning an explicit mapping between BPEs in the English (EN), Hindi (HI) and Marathi (MR), we define BPE tokens by jointly processing all parallel data. Thus, all derive a single BPE vocabulary. Since HI and MR belong to the similar languages, they naturally share a good fraction of BPE tokens, which reduces the vocabulary size. We report evaluation results (evaluated by the shared task organizers) of our approach with the released Test data. BLEU (Papineni et al., 2002), RIBES (Isozaki et al., 2010) and TER (Snover et al., 2006) are used to evaluate the performance of all participating systems in the shared task. 1 https://github.com/anoopkunchukuttan/ indic_nlp_library/ 426 Parallel Data Filtered SLT Filtered EN–HI BT EN–HI BT HI–MR #sentences 33,923 961,036 5 million 5 million C1 ✓ ✓ ✓ C2 ✓ ✓ ✓ ✓ P ✓ ✓ ✓ ✓ Table 4: The training criteria data statistics of our submitted systems (C1 = Contrastive 1, C2 = Contrastive 2, P = Primary, and BT = Back-translated data). 5.2 Hyper-parameter Setup vocabulary size of 32K. After each epoch, the training data is shuffled. During decoding, we perform"
2020.wmt-1.50,P02-1040,0,0.114981,"anguages Santanu Pal1 , Marcos Zampieri2 1 Wipro AI Lab, India 2 Rochester Institute of Technology, USA santanu.pal2@wipro.com Abstract MT systems on translating between pairs of similar languages without English as a pivot language (Barrault et al., 2019). The organizers provided participants with training, development, and testing parallel data from three pairs of languages from three different language families: Spanish - Portuguese (Romance languages), Czech - Polish (Slavic languages), and Hindi - Nepali (Indo-Aryan languages). Systems were evaluated using automatic metrics, namely BLEU (Papineni et al., 2002) and TER (Snover et al., 2006). In this paper we present the WIPRO-RIT systems submitted to the Similar Language Translation shared task at WMT 2020. The second edition of this shared task featured parallel data from pairs/groups of similar languages from three different language families: Indo-Aryan languages (Hindi and Marathi), Romance languages (Catalan, Portuguese, and Spanish), and South Slavic Languages (Croatian, Serbian, and Slovene). We report the results obtained by our systems in translating from Hindi to Marathi and from Marathi to Hindi. WIPRO-RIT achieved competitive performance"
2020.wmt-1.50,2020.vardial-1.10,0,0.146479,"arallel data released in WMT 2014 (Bojar et al., 2014), IITB parallel corpus (Kunchukuttan et al., 2018), the parallel dataset was collected from news (Siripragada et al., 2020) and the PMIndia (Haddow and Kirefu, 2020) parallel corpus (see Table 1). With the substantial performance improvements brought to MT by neural approaches, a growing interest in translating between pairs of similar languages, language varieties, and dialects has been observed. Recent studies have addressed MT between Arabic dialects (Harrat et al., 2019; Shapiro and Duh, 2019) Catalan and Spanish, Croatian and Serbian (Popović et al., 2020), (Costa-jussà, 2017), Brazilian and European Portuguese (Costajussà et al., 2018), and several pairs of languages and language varieties such as Brazilian and European Portuguese, Canadian and European French, and similar languages such as Croatian and Serbian, and Indonesian and Malay (Lakew et al., 2018). The interest on diatopic language variation is evidenced by the recent iterations of the VarDial workshop in which papers on MT applied to similar languages varieties, and dialects (Shapiro and Duh, 2019; Myint Oo et al., 2019; Popović et al., 2020) have been presented along with evaluatio"
2020.wmt-1.50,P16-1162,0,0.0392221,"e transformer architecture that can translate between multiple languages. To make use of multilingual data within a single NMT model, we perform one simple modification to the source side of the multilingual data, we use an additional token at the beginning of the each source sentence to indicate the target language by the NMT model would be translated as shown in Table 3. We train the model with all the processed multilingual data consisting of sen5.1 Experiment Setup To handle out-of-vocabulary words and to reduce the vocabulary size, instead of considering words, we consider subword units (Sennrich et al., 2016) by using byte-pair encoding (BPE). In the preprocessing step, instead of learning an explicit mapping between BPEs in the English (EN), Hindi (HI) and Marathi (MR), we define BPE tokens by jointly processing all parallel data. Thus, all derive a single BPE vocabulary. Since HI and MR belong to the similar languages, they naturally share a good fraction of BPE tokens, which reduces the vocabulary size. We report evaluation results (evaluated by the shared task organizers) of our approach with the released Test data. BLEU (Papineni et al., 2002), RIBES (Isozaki et al., 2010) and TER (Snover et"
2020.wmt-1.50,W19-1424,0,0.032189,"ystem. The Hindi–English NMT system was trained on English–Hindi parallel data released in WMT 2014 (Bojar et al., 2014), IITB parallel corpus (Kunchukuttan et al., 2018), the parallel dataset was collected from news (Siripragada et al., 2020) and the PMIndia (Haddow and Kirefu, 2020) parallel corpus (see Table 1). With the substantial performance improvements brought to MT by neural approaches, a growing interest in translating between pairs of similar languages, language varieties, and dialects has been observed. Recent studies have addressed MT between Arabic dialects (Harrat et al., 2019; Shapiro and Duh, 2019) Catalan and Spanish, Croatian and Serbian (Popović et al., 2020), (Costa-jussà, 2017), Brazilian and European Portuguese (Costajussà et al., 2018), and several pairs of languages and language varieties such as Brazilian and European Portuguese, Canadian and European French, and similar languages such as Croatian and Serbian, and Indonesian and Malay (Lakew et al., 2018). The interest on diatopic language variation is evidenced by the recent iterations of the VarDial workshop in which papers on MT applied to similar languages varieties, and dialects (Shapiro and Duh, 2019; Myint Oo et al., 201"
2020.wmt-1.50,P07-2045,0,0.0185268,"a subset 5 million Marathi monolingual data. We performed similar cleaning and pre-processing methods as we described in case of parallel data. The five million Hindi monolingual sentences were first back-translated to English Parallel News PM India Indic WordNet Total Filtered∗ #sentences 12,349 25,897 11,188 49,434 33923 Table 2: Data statistics of released SLT Data; ∗ Filtration methods: (i) remove duplicates and (ii) filtering noisy mixed language sentences. We performed the following two steps: (i) we use the cleaning process described in Pal et al. (2015), and (ii) we execute the Moses (Koehn et al., 2007) corpus cleaning scripts with minimum and maximum number of tokens set to 1 and 100, respectively. After cleaning and re425 Parallel Sentences L1 → L2 HI→MR MR→HI EN→HI Source Raw data Processed data Raw data Processed data Raw data Processed data Target देश एकल परयास से आगे बढ़ चुके ह। देश आता सामाईक परयतन करत आहेत. TO_MR देश एकल परयास से आगे बढ़ चुके ह। देश आता सामाईक परयतन करत आहेत. देश आता सामाईक परयतन करत आहेत. देश एकल परयास से आगे बढ़ चुके ह। TO_HI देश आता सामाईक परयतन करत आहेत. देश एकल परयास से आगे बढ़ चुके ह। The MoU was signed in February, 2016. इस एमओयू पर फरवरी, 2016 म हसता"
2020.wmt-1.50,2020.lrec-1.462,0,0.0599331,"the model described in Johnson et al. (2017). WIPRORIT achieved competitive performance ranking 1st in Marathi to Hindi and 2nd in Hindi to Marathi translation among 22 systems. 424 Proceedings of the 5th Conference on Machine Translation (WMT), pages 424–429 c Online, November 19–20, 2020. 2020 Association for Computational Linguistics 2 Related Work using a Hindi–English NMT system. The Hindi–English NMT system was trained on English–Hindi parallel data released in WMT 2014 (Bojar et al., 2014), IITB parallel corpus (Kunchukuttan et al., 2018), the parallel dataset was collected from news (Siripragada et al., 2020) and the PMIndia (Haddow and Kirefu, 2020) parallel corpus (see Table 1). With the substantial performance improvements brought to MT by neural approaches, a growing interest in translating between pairs of similar languages, language varieties, and dialects has been observed. Recent studies have addressed MT between Arabic dialects (Harrat et al., 2019; Shapiro and Duh, 2019) Catalan and Spanish, Croatian and Serbian (Popović et al., 2020), (Costa-jussà, 2017), Brazilian and European Portuguese (Costajussà et al., 2018), and several pairs of languages and language varieties such as Brazilian"
2020.wmt-1.50,2006.amta-papers.25,0,0.338951,"ampieri2 1 Wipro AI Lab, India 2 Rochester Institute of Technology, USA santanu.pal2@wipro.com Abstract MT systems on translating between pairs of similar languages without English as a pivot language (Barrault et al., 2019). The organizers provided participants with training, development, and testing parallel data from three pairs of languages from three different language families: Spanish - Portuguese (Romance languages), Czech - Polish (Slavic languages), and Hindi - Nepali (Indo-Aryan languages). Systems were evaluated using automatic metrics, namely BLEU (Papineni et al., 2002) and TER (Snover et al., 2006). In this paper we present the WIPRO-RIT systems submitted to the Similar Language Translation shared task at WMT 2020. The second edition of this shared task featured parallel data from pairs/groups of similar languages from three different language families: Indo-Aryan languages (Hindi and Marathi), Romance languages (Catalan, Portuguese, and Spanish), and South Slavic Languages (Croatian, Serbian, and Slovene). We report the results obtained by our systems in translating from Hindi to Marathi and from Marathi to Hindi. WIPRO-RIT achieved competitive performance ranking 1st in Marathi to Hin"
2020.wmt-1.50,L18-1548,0,0.0312343,"mission to the SLT 2020 Indo-Aryan track. Our WIPRO-RIT system is based on the model described in Johnson et al. (2017). WIPRORIT achieved competitive performance ranking 1st in Marathi to Hindi and 2nd in Hindi to Marathi translation among 22 systems. 424 Proceedings of the 5th Conference on Machine Translation (WMT), pages 424–429 c Online, November 19–20, 2020. 2020 Association for Computational Linguistics 2 Related Work using a Hindi–English NMT system. The Hindi–English NMT system was trained on English–Hindi parallel data released in WMT 2014 (Bojar et al., 2014), IITB parallel corpus (Kunchukuttan et al., 2018), the parallel dataset was collected from news (Siripragada et al., 2020) and the PMIndia (Haddow and Kirefu, 2020) parallel corpus (see Table 1). With the substantial performance improvements brought to MT by neural approaches, a growing interest in translating between pairs of similar languages, language varieties, and dialects has been observed. Recent studies have addressed MT between Arabic dialects (Harrat et al., 2019; Shapiro and Duh, 2019) Catalan and Spanish, Croatian and Serbian (Popović et al., 2020), (Costa-jussà, 2017), Brazilian and European Portuguese (Costajussà et al., 2018),"
2020.wmt-1.50,W18-6316,0,0.0184966,"by neural approaches, a growing interest in translating between pairs of similar languages, language varieties, and dialects has been observed. Recent studies have addressed MT between Arabic dialects (Harrat et al., 2019; Shapiro and Duh, 2019) Catalan and Spanish, Croatian and Serbian (Popović et al., 2020), (Costa-jussà, 2017), Brazilian and European Portuguese (Costajussà et al., 2018), and several pairs of languages and language varieties such as Brazilian and European Portuguese, Canadian and European French, and similar languages such as Croatian and Serbian, and Indonesian and Malay (Lakew et al., 2018). The interest on diatopic language variation is evidenced by the recent iterations of the VarDial workshop in which papers on MT applied to similar languages varieties, and dialects (Shapiro and Duh, 2019; Myint Oo et al., 2019; Popović et al., 2020) have been presented along with evaluation campaigns featuring multiple shared tasks on a number of related topics such as cross-lingual morphological analysis, cross-lingual parsing, dialect identification, and morphosyntactic tagging (Zampieri et al., 2018, 2019; Găman et al., 2020). 3 Data Sources WMT News IITB PM India Total Remove duplicates"
2020.wmt-1.50,W19-1408,0,0.0313334,"and Duh, 2019) Catalan and Spanish, Croatian and Serbian (Popović et al., 2020), (Costa-jussà, 2017), Brazilian and European Portuguese (Costajussà et al., 2018), and several pairs of languages and language varieties such as Brazilian and European Portuguese, Canadian and European French, and similar languages such as Croatian and Serbian, and Indonesian and Malay (Lakew et al., 2018). The interest on diatopic language variation is evidenced by the recent iterations of the VarDial workshop in which papers on MT applied to similar languages varieties, and dialects (Shapiro and Duh, 2019; Myint Oo et al., 2019; Popović et al., 2020) have been presented along with evaluation campaigns featuring multiple shared tasks on a number of related topics such as cross-lingual morphological analysis, cross-lingual parsing, dialect identification, and morphosyntactic tagging (Zampieri et al., 2018, 2019; Găman et al., 2020). 3 Data Sources WMT News IITB PM India Total Remove duplicates Cleaning∗ #sentences 273,885 156,344 1,561,840 56,831 2,048,900 1,464,419 961,036 Table 1: English–Hindi parallel data statistics. ∗ Removing noisy mixed language sentences. We also back-translated 5 million Marathi monolingual"
2020.wmt-1.50,W15-5206,1,0.867989,"Missing"
2020.wmt-1.50,W15-3017,1,0.848858,"Missing"
2020.wmt-1.50,L16-1095,1,0.895951,"Missing"
2021.acl-long.312,2020.coling-demos.2,1,0.887493,"eric and could be applied to any learning problem with substantial class imbalances. 1 Introduction Predictive maintenance techniques are applied to engineering systems to estimate when maintenance should be performed to reduce costs and improve operational efficiency (Carvalho et al., 2019), as well as mitigate risk and increase safety. Maintenance records are an important source of information for predictive maintenance (McArthur et al., 2018). These records are often stored in the form of technical logbooks in which each entry contains fields that identify and describe a maintenance issue (Akhbardeh et al., 2020a). Being able to classify these technical events is an important step in the development of predictive maintenance systems. In most technical logbooks, issues are manually Original Entry Pre-processed Entry fwd eng baff seeal needs resecured. r/h eng #3 intake gsk leaking. bird struck on p/w at twy. bird rmvd. location rptd as nm from rwy aprch end. forward engine baffle seal needs resecured. right engine number 3 intake gasket leaking. bird struck on pilot window at taxiway. bird removed location reported as new mexico from runway approach end. Table 1: Original and text-normalized example d"
2021.acl-long.312,2020.aacl-demo.5,1,0.8964,"eric and could be applied to any learning problem with substantial class imbalances. 1 Introduction Predictive maintenance techniques are applied to engineering systems to estimate when maintenance should be performed to reduce costs and improve operational efficiency (Carvalho et al., 2019), as well as mitigate risk and increase safety. Maintenance records are an important source of information for predictive maintenance (McArthur et al., 2018). These records are often stored in the form of technical logbooks in which each entry contains fields that identify and describe a maintenance issue (Akhbardeh et al., 2020a). Being able to classify these technical events is an important step in the development of predictive maintenance systems. In most technical logbooks, issues are manually Original Entry Pre-processed Entry fwd eng baff seeal needs resecured. r/h eng #3 intake gsk leaking. bird struck on p/w at twy. bird rmvd. location rptd as nm from rwy aprch end. forward engine baffle seal needs resecured. right engine number 3 intake gasket leaking. bird struck on pilot window at taxiway. bird removed location reported as new mexico from runway approach end. Table 1: Original and text-normalized example d"
2021.acl-long.312,D19-1371,0,0.0192359,"nce on average was FaciMain, which is the dataset which the LSTM model had the closest performance to the CNN and BERT models, and was also the only one which the LSTM model outperformed the DNN model. The pre-trained BERT model provided a reasonable classification performance compared to the other deep learning models, however as BERT is pre-trained on standard language, the performance when applying to logbook data was not optimal. Training or fine-tunning BERT to technical logbook data is likely to improve performance as observed in the legal and scientific domains (Chalkidis et al., 2020; Beltagy et al., 2019). As training or finetuning BERT requires large amounts of data, a limitation for fine-tuning a domain-specific BERT is the amount of logbook data available. 7 Conclusion and Future Work This work focused on predictive maintenance and technical event/issue classification, with a special focus on addressing class imbalance. We acquired seven logbook datasets from three technical domains containing short instances with non-standard grammar and spelling, and many abbreviations. To address RQ1, we evaluated multiple strategies to address the extreme class imbalance in these datasets and we showed"
2021.acl-long.312,N15-1075,0,0.0121103,"otive maintenance (3), automotive safety (4), and facility maintenance (5). Each instance shows how domain-specific terminology, abbreviations (Abbr.), and misspelled words (in bold font) are used by the domain expert, and also illustrates some of the event types covered. More details are provided in Section 3. notable performance by using the n-gram features with the Maximum Entropy (MaxEnt) classifier. There is also relevant research on event classification in social media. For example, Ritter et al. (2012) proposed an open-source event extraction and supervised tagger for noisy microblogs. Cherry and Guo (2015) applied word embedding-based modeling for information extraction on news-wire and tweets, comparing named entity taggers to improve their method. Hammar et al. (2018) performed experimental work on Instagram text using weakly supervised text classification to extracted clothing brand based on user descriptions in posts. The problem of class imbalance has been studied in recent years for numerous natural language processing tasks. Tayyar Madabushi et al. (2019) studied automatic propaganda event detection from a news dataset using a pre-trained BERT model. They recognized that the BERT model h"
2021.acl-long.312,W19-6207,0,0.0879888,"Missing"
2021.acl-long.312,E17-2110,0,0.0159587,"ecall, and F1 score by utilizing a macro-average over all classes, as this gives every class equal weight, and hence reveals how well the models and training data selection strategies perform. 4.2 Model Architecture and Training Different machine learning methods were considered for technical event/issue classification (e.g. engine failure, turbine failure). Each instance is an individual short logbook entry and contains approximately 2 to 20 tokens (12 words on average per instance including function words), as shown in Table 3.The methods used in this study were a Deep Neural Network (DNN) (Dernoncourt et al., 2017), a Long Short-Term Memory (LSTM) (Suzgun et al., 2019), recurrent neural network (RNN) (Pascanu et al., 2013), a Convolutional Neural Network (CNN) (Lin et al., 2018), and BERT (Devlin et al., 2019). Deep Neural Network A deep artificial neural network (DNN), as described by Dernoncourt et al. (2017), can learn abstract representation and features of the input instances that would help to achieve better performance on predicting the issue type in the logbook dataset. The DNN used was a 3 layer, fully connected feed forward neural network with an input embedding layer of dimension 300 and equa"
2021.acl-long.312,N19-1423,0,0.069764,"rchitecture and Training Different machine learning methods were considered for technical event/issue classification (e.g. engine failure, turbine failure). Each instance is an individual short logbook entry and contains approximately 2 to 20 tokens (12 words on average per instance including function words), as shown in Table 3.The methods used in this study were a Deep Neural Network (DNN) (Dernoncourt et al., 2017), a Long Short-Term Memory (LSTM) (Suzgun et al., 2019), recurrent neural network (RNN) (Pascanu et al., 2013), a Convolutional Neural Network (CNN) (Lin et al., 2018), and BERT (Devlin et al., 2019). Deep Neural Network A deep artificial neural network (DNN), as described by Dernoncourt et al. (2017), can learn abstract representation and features of the input instances that would help to achieve better performance on predicting the issue type in the logbook dataset. The DNN used was a 3 layer, fully connected feed forward neural network with an input embedding layer of dimension 300 and equal size number of words followed by 2 dense layers with 512 hidden units with ReLU activation functions followed by a dropout layer. Finally, we added a fully connected dense layer with size equal to"
2021.acl-long.312,N18-2108,0,0.0563211,"Missing"
2021.acl-long.312,W14-4320,0,0.0314244,"y supervised text classification to extracted clothing brand based on user descriptions in posts. The problem of class imbalance has been studied in recent years for numerous natural language processing tasks. Tayyar Madabushi et al. (2019) studied automatic propaganda event detection from a news dataset using a pre-trained BERT model. They recognized that the BERT model had issues in generalizing. To overcome this issue, they proposed a cost-weighting method. Al-Azani and ElAlfy (2017) analyzed polarity measurement in imbalanced tweet datasets utilizing features learned with word embeddings. Li and Nenkova (2014) studied the class imbalance problem in the task of discourse relation identification by comparing the accuracy of multiple classifiers. They showed that utilizing a unified method and further downsampling the negative instances can significantly enhance the performance of the prediction model on unbalanced binary and multi-classes. Dealing with unbalance classes is also studied well in the sentiment classification task. Li et al. (2012) introduced an active learning method that overcomes the problem of data class unbalance by choosing the significant sample of minority class for manual annota"
2021.acl-long.312,D12-1013,0,0.0385237,"-weighting method. Al-Azani and ElAlfy (2017) analyzed polarity measurement in imbalanced tweet datasets utilizing features learned with word embeddings. Li and Nenkova (2014) studied the class imbalance problem in the task of discourse relation identification by comparing the accuracy of multiple classifiers. They showed that utilizing a unified method and further downsampling the negative instances can significantly enhance the performance of the prediction model on unbalanced binary and multi-classes. Dealing with unbalance classes is also studied well in the sentiment classification task. Li et al. (2012) introduced an active learning method that overcomes the problem of data class unbalance by choosing the significant sample of minority class for manual annotation and majority class for automatic annotation to lower the amount of human annotation required. Furthermore, Damaschk et al. (2019) examined techniques to overcome the problem of dealing with high-class imbalance in classifying a collection of song lyrics. They employed neural network models including a multi-layer perceptron and a Doc2Vec model in their experiments where the finding was that undersampling the majority class can be a"
2021.acl-long.312,2020.acl-main.45,0,0.029857,"the significant sample of minority class for manual annotation and majority class for automatic annotation to lower the amount of human annotation required. Furthermore, Damaschk et al. (2019) examined techniques to overcome the problem of dealing with high-class imbalance in classifying a collection of song lyrics. They employed neural network models including a multi-layer perceptron and a Doc2Vec model in their experiments where the finding was that undersampling the majority class can be a reasonable approach to remove the data sparsity and further improve the classification performance. Li et al. (2020) also explored the problem of high data imbalance using cross-entropy criteria as well as standard performance metrics. They proposed a loss function called Dice loss that assigns equal importance to the false negatives and the false positives. In computer vision, Bowley et al. (2019) developed an automated feedback loop method to identify and classify wildlife species from Unmanned Aerial Systems imagery, for training CNNs to overcome the unbalanced class issue. On their expert imagery dataset, the error rate decreased substantially from 0.88 to 0.05. This work adapts this feedback loop strat"
2021.acl-long.312,D18-1485,0,0.157845,"rategies perform. 4.2 Model Architecture and Training Different machine learning methods were considered for technical event/issue classification (e.g. engine failure, turbine failure). Each instance is an individual short logbook entry and contains approximately 2 to 20 tokens (12 words on average per instance including function words), as shown in Table 3.The methods used in this study were a Deep Neural Network (DNN) (Dernoncourt et al., 2017), a Long Short-Term Memory (LSTM) (Suzgun et al., 2019), recurrent neural network (RNN) (Pascanu et al., 2013), a Convolutional Neural Network (CNN) (Lin et al., 2018), and BERT (Devlin et al., 2019). Deep Neural Network A deep artificial neural network (DNN), as described by Dernoncourt et al. (2017), can learn abstract representation and features of the input instances that would help to achieve better performance on predicting the issue type in the logbook dataset. The DNN used was a 3 layer, fully connected feed forward neural network with an input embedding layer of dimension 300 and equal size number of words followed by 2 dense layers with 512 hidden units with ReLU activation functions followed by a dropout layer. Finally, we added a fully connected"
2021.acl-long.312,D19-6109,0,0.0574907,"Missing"
2021.acl-long.312,maragoudakis-etal-2006-dealing,0,0.0237727,"., 2019) to apply a large class data from one technical domain to another. For example, instances that describe an engine failure in the aviation domain are distinct from engine failure instances reported in the automotive domain. In this paper we apply five different methods for selecting training data for the models to analyze their effects on classification performance: (1) under(down)and (2) over-sampling, (3) random down-sampling, (4) a feedback loop strategy, and (5) a baseline strategy which simply uses all available data. Re-sampling Under- and over-sampling are resampling techniques (Maragoudakis et al., 2006) that were used to create balanced class sizes for model training. For over-sampling, instances of the minority classes are randomly copied so that all classes would have the same number of instances as the largest class. For under-sampling, observations are randomly removed from the majority classes, so that all classes have the same number of instances as the smallest class. For both approaches, we first divided our datasets into test and Feedback Loop To address class imbalances in text classification, this work adapts the approach in Bowley et al. (2019) from the computer vision domain. Th"
2021.acl-long.312,W11-0143,0,0.0224652,"r. 7) Faci-Main contains six years of logbook reports collected for building maintenance. These technical logbooks include short, compact, and descriptive domain-specific English texts single instances usually contain between 2 and 20 tokens on average including abbreviations and domain-specific words. An example instance from Table 2, r/h fwd upper baff seal needs to be resecured, shows how the instances for a specific issue class are comprised from specific vocabulary (less ambiguity), and therefore contain a high level of granularity (level of description for an event from multiple words) (Mulkar-Mehta et al., 2011). Table 3 presents statistics for each dataset, in terms of the number of instances, average instance length, number of classes, and the minimum, average, median and maximum class size to represent how imbalanced the datasets are. An instance in the logbook can be formed as a complete description of the technical event (such as a safety or maintenance inspection) like: #2 & #4 cyl rocker cover gsk are leaking, or it might contain an incomplete description by solely referring to the damaged part/section of machinery (hyd cap chck eng light on) using few domain words. In either form of the probl"
2021.acl-long.312,U09-1014,0,0.0116095,"unstructured electronic health records provided by the U.K. National Health Service. They evaluated a deep artificial neural network model on the expertannotated textual dataset of a safety incident to identify similar events that occurred. Del´eger et al. (2010) proposed a method to deal with unstructured clinical records, using rule-based techniques to extract names of medicines and related information such as prescribed dosage. Savova et al. (2010) considered free-text electronic medical records for information extraction purposes and developed a system to obtain clinical domain knowledge. Patrick and Li (2009) proposed the cascade methods of extracting the medication records such as treatment duration or reason, obtained from patient’s historical records. Their approach for event extraction includes text normalization, tokenization, and context identification. A system using multiple features outperformed a baseline method using a bag of words model. Yetisgen-Yildiz et al. (2013) proposed the lung disease phenotypes identification method to prevent the use of a handoperated identification strategy. They employed NLP pipelines including text pre-processing and further text classification on the text"
2021.acl-long.312,D14-1162,0,0.0878609,"Missing"
2021.acl-long.312,D18-1210,0,0.0118425,"ing the number of hidden units equal to the embedding dimension, followed by a dropout layer. Finally, we added a fully connected layer with size equal to the number of classes, with a SoftMax activation function. Convolutional Neural Network Convolutional neural networks (CNNs) have demonstrated exceptional success in NLP tasks such as document classification, language modeling, or machine translation (Lin et al., 2018). As Xu et al. (2020) described, CNN models can produce consistent performance when applied to the various text types such as short sequences. We evaluated a CNN architecture (Shen et al., 2018) with a convolutional layer, followed by batch normalization, ReLU, and a dropout layer, which was followed by a maxpooling layer. The model contained 300 convolutional filters with the size of 1 by n-gram length pooling with the size of 1 by the length of the input sequence, followed by concatenation layer, then finally connected to a fully connected dense layer, and an output layer equal to the size of the dataset class using a SoftMax activation function. Bidirectional Encoder Representations We also evaluated using the pre-trained uncased Bidirectional Encoder Representations (BERT) for En"
2021.acl-long.312,W19-0128,0,0.0291754,"Missing"
2021.acl-long.312,D19-5018,0,0.0228967,"in social media. For example, Ritter et al. (2012) proposed an open-source event extraction and supervised tagger for noisy microblogs. Cherry and Guo (2015) applied word embedding-based modeling for information extraction on news-wire and tweets, comparing named entity taggers to improve their method. Hammar et al. (2018) performed experimental work on Instagram text using weakly supervised text classification to extracted clothing brand based on user descriptions in posts. The problem of class imbalance has been studied in recent years for numerous natural language processing tasks. Tayyar Madabushi et al. (2019) studied automatic propaganda event detection from a news dataset using a pre-trained BERT model. They recognized that the BERT model had issues in generalizing. To overcome this issue, they proposed a cost-weighting method. Al-Azani and ElAlfy (2017) analyzed polarity measurement in imbalanced tweet datasets utilizing features learned with word embeddings. Li and Nenkova (2014) studied the class imbalance problem in the task of discourse relation identification by comparing the accuracy of multiple classifiers. They showed that utilizing a unified method and further downsampling the negative"
2021.acl-long.312,W13-1902,0,0.0235844,"s of medicines and related information such as prescribed dosage. Savova et al. (2010) considered free-text electronic medical records for information extraction purposes and developed a system to obtain clinical domain knowledge. Patrick and Li (2009) proposed the cascade methods of extracting the medication records such as treatment duration or reason, obtained from patient’s historical records. Their approach for event extraction includes text normalization, tokenization, and context identification. A system using multiple features outperformed a baseline method using a bag of words model. Yetisgen-Yildiz et al. (2013) proposed the lung disease phenotypes identification method to prevent the use of a handoperated identification strategy. They employed NLP pipelines including text pre-processing and further text classification on the textual reports to identify the patients with a positive diagnosis for the disease. Based on the outcome, they achieve 4035 Tech. Event or Issue Label Example Instance of Technical Logbook Entry SUBSTANTIAL DAMAGE BAFFLE DAMAGE MINOR DAMAGE UNKNOWN PM SERVICE DRIVING ISSUE STOP SIGN RUNNING BUILDING PM ENG NEED REPAIR PREVENTIVE MAINT Abbr., Misspelling, Terminology (1) AFT ON T"
2021.findings-acl.315,N19-1423,0,0.0081961,"ues in research in politeness/offensiveness and mental health. 1 Introduction The use of offensive language is pervasive in social media and it has been studied from different perspectives. A popular line of research is the study of computational models to identify offensive content online relying on traditional machine learning classifiers (e.g. naive bayes and SVMs) (Xu et al., 2012; Dadvar et al., 2013), neural networks (e.g. LSTMs, GRUs) with word embeddings (Aroyehun and Gelbukh, 2018; Majumder et al., 2018), and more recently, transformer models like ELMO (Peters et al., 2018) and BERT (Devlin et al., 2019) which have shown to obtain competitive scores topping the leaderboards in recent shared tasks on offensive language and hate speech detection (Liu et al., 2019). Offensive language is related to the notion of impoliteness (Culpeper, 2011) and it can take various forms from general and often harmless profanity WARNING: This paper contains offensive words. to abusive language intended to cause harm, such as cyberbullying and hate speech (Waseem et al., 2017). Computational models have been applied not only to identify the various types of offensive content (Basile et al., 2019) but also to, for"
2021.findings-acl.315,J93-1003,0,0.111648,"written by individuals with depression. We compute the keyness score (Kilgarriff, 2009; Gabrielatos, 2018) of content words (removing stop words) from posts labeled as offensive written by users with self-reported diagnosis. The keyness is computed in order to show which words occur more often in the texts from depressed individuals showing signs of depression (target corpus) in comparison to the texts from users diagnosed with depression that do not show signs of depression (reference corpus). We calculate the frequencies of words from the two corpora and then the loglikelihood Ratio (G2 ) (Dunning, 1993) for each word. In Figure 2 we present the top 20 words, ordered by G2 from each corpus, in the two datasets. We show that, while users without signs of depression refer more to sexual and profane terms, posts by users showing signs of depression include more negative words such as bad, hate, sick, death. This result corroborates the findings described in the literature on cognitive errors or biases in depression (Beck and Haigh, 2014). It is well known that depressed individuals tend to view life events more negatively than their non-depressed peers (Everaert et al., 2017). Furthermore, depre"
2021.findings-acl.315,D18-1471,0,0.0309956,"Missing"
2021.findings-acl.315,2020.lrec-1.758,0,0.287431,"Missing"
2021.findings-acl.315,2020.trac-1.1,1,0.766797,"Missing"
2021.findings-acl.315,S19-2011,0,0.178135,"m different perspectives. A popular line of research is the study of computational models to identify offensive content online relying on traditional machine learning classifiers (e.g. naive bayes and SVMs) (Xu et al., 2012; Dadvar et al., 2013), neural networks (e.g. LSTMs, GRUs) with word embeddings (Aroyehun and Gelbukh, 2018; Majumder et al., 2018), and more recently, transformer models like ELMO (Peters et al., 2018) and BERT (Devlin et al., 2019) which have shown to obtain competitive scores topping the leaderboards in recent shared tasks on offensive language and hate speech detection (Liu et al., 2019). Offensive language is related to the notion of impoliteness (Culpeper, 2011) and it can take various forms from general and often harmless profanity WARNING: This paper contains offensive words. to abusive language intended to cause harm, such as cyberbullying and hate speech (Waseem et al., 2017). Computational models have been applied not only to identify the various types of offensive content (Basile et al., 2019) but also to, for example, study the relation between profanity and hate speech (Malmasi and Zampieri, 2018) and the different functions and intentions of vulgarity in social med"
2021.findings-acl.315,W15-1204,0,0.0602495,"Missing"
2021.findings-acl.315,W18-4423,0,0.0305093,"Missing"
2021.findings-acl.315,W16-0314,1,0.766796,"anguage in other languages like Greek (Pitenis et al., 2020) and Turkish (C¸o¨ ltekin, 2020) while a few others have applied cross-lingual models to take advantage of existing English datasets when making predictions in languages with fewer resources (Ranasinghe and Zampieri, 2020). Several studies have applied machine learning and NLP methods to address research questions related to mental health in social media such as identifying users with a particular mental health condition and predicting the risk of self-harm or suicide ideation (De Choudhury et al., 2013; Preot¸iucPietro et al., 2015; Malmasi et al., 2016; De Choudhury et al., 2016; Chancellor and De Choudhury, 2020). The CLPsych workshop co-located with international NLP conferences has hosted multiple competitions on these topics providing participants with important benchmark datasets and attracting a large number of teams (Coppersmith et al., 2015; Milne et al., 2016; Zirikly et al., 2019). There have been multiple studies on the impact of offensive and hateful speech on the individual’s psychological mental health and well-being (Bannink et al., 2014; Saha et al., 2019). The use of offensive language by individuals with mental health cond"
2021.findings-acl.315,W17-3012,0,0.0182153,"dings (Aroyehun and Gelbukh, 2018; Majumder et al., 2018), and more recently, transformer models like ELMO (Peters et al., 2018) and BERT (Devlin et al., 2019) which have shown to obtain competitive scores topping the leaderboards in recent shared tasks on offensive language and hate speech detection (Liu et al., 2019). Offensive language is related to the notion of impoliteness (Culpeper, 2011) and it can take various forms from general and often harmless profanity WARNING: This paper contains offensive words. to abusive language intended to cause harm, such as cyberbullying and hate speech (Waseem et al., 2017). Computational models have been applied not only to identify the various types of offensive content (Basile et al., 2019) but also to, for example, study the relation between profanity and hate speech (Malmasi and Zampieri, 2018) and the different functions and intentions of vulgarity in social media (Holgate et al., 2018). Most of the datasets used in the aforementioned studies contain data sampled from the general population and therefore very little light has been shed on the use of offensive language in online communication by specific groups such as individuals with mental health conditi"
2021.findings-acl.315,W16-0312,0,0.0194963,"rning and NLP methods to address research questions related to mental health in social media such as identifying users with a particular mental health condition and predicting the risk of self-harm or suicide ideation (De Choudhury et al., 2013; Preot¸iucPietro et al., 2015; Malmasi et al., 2016; De Choudhury et al., 2016; Chancellor and De Choudhury, 2020). The CLPsych workshop co-located with international NLP conferences has hosted multiple competitions on these topics providing participants with important benchmark datasets and attracting a large number of teams (Coppersmith et al., 2015; Milne et al., 2016; Zirikly et al., 2019). There have been multiple studies on the impact of offensive and hateful speech on the individual’s psychological mental health and well-being (Bannink et al., 2014; Saha et al., 2019). The use of offensive language by individuals with mental health conditions, however, has not been substantially studies with the exception of Birnbaum et al. (2020) that analyzed the use of offensive language in Facebook messages from individuals with mood disorders. Our work fills this important gap by providing further empirical evidence of the use of offensive language by individuals"
2021.findings-acl.315,N12-1084,0,0.0121008,"analysis indicates that offensive language is more frequently used in the samples written by individuals with self-reported depression as well as individuals showing signs of depression. The results discussed here open new avenues in research in politeness/offensiveness and mental health. 1 Introduction The use of offensive language is pervasive in social media and it has been studied from different perspectives. A popular line of research is the study of computational models to identify offensive content online relying on traditional machine learning classifiers (e.g. naive bayes and SVMs) (Xu et al., 2012; Dadvar et al., 2013), neural networks (e.g. LSTMs, GRUs) with word embeddings (Aroyehun and Gelbukh, 2018; Majumder et al., 2018), and more recently, transformer models like ELMO (Peters et al., 2018) and BERT (Devlin et al., 2019) which have shown to obtain competitive scores topping the leaderboards in recent shared tasks on offensive language and hate speech detection (Liu et al., 2019). Offensive language is related to the notion of impoliteness (Culpeper, 2011) and it can take various forms from general and often harmless profanity WARNING: This paper contains offensive words. to abusiv"
2021.findings-acl.315,D17-1322,0,0.0119463,"language target identification: individual (IND) vs. group (GRP) vs. other (OTH). This hierarchical taxonomy provides us with a flexibility as it represents multiple types of offensive content in a single annotation scheme (e.g. posts targeted at an individual are often cyberbullying and posts targeted at a group are often hate speech) making it a great fit for this kind of analysis. In our experiments, we consider level A (offensive vs. non-offensive) and level B (target vs. untargeted). Mental Health We run all our experiments on the Reddit Self-reported Depression Diagnosis (RSDD) dataset (Yates et al., 2017) and on the Early Risk Prediction on the Internet (eRisk) 2018 dataset (Losada and Crestani, 2016), two publicly available datasets containing posts from Reddit. The RSDD dataset consists of users annotated as having depression by their mention of diagnosis and control users, which are users who do not suffer from depression (there is not any mention of diagnosis in their posts). To prevent users labeled with depression to be easily identified by specific keywords, the authors removed posts containing depression terms (e.g. depression, depressive) or belonging to mental health related subreddi"
2021.findings-acl.315,N18-1202,0,0.0081378,"ts discussed here open new avenues in research in politeness/offensiveness and mental health. 1 Introduction The use of offensive language is pervasive in social media and it has been studied from different perspectives. A popular line of research is the study of computational models to identify offensive content online relying on traditional machine learning classifiers (e.g. naive bayes and SVMs) (Xu et al., 2012; Dadvar et al., 2013), neural networks (e.g. LSTMs, GRUs) with word embeddings (Aroyehun and Gelbukh, 2018; Majumder et al., 2018), and more recently, transformer models like ELMO (Peters et al., 2018) and BERT (Devlin et al., 2019) which have shown to obtain competitive scores topping the leaderboards in recent shared tasks on offensive language and hate speech detection (Liu et al., 2019). Offensive language is related to the notion of impoliteness (Culpeper, 2011) and it can take various forms from general and often harmless profanity WARNING: This paper contains offensive words. to abusive language intended to cause harm, such as cyberbullying and hate speech (Waseem et al., 2017). Computational models have been applied not only to identify the various types of offensive content (Basile"
2021.findings-acl.315,2020.lrec-1.629,1,0.86189,"a (Basile et al., 2019; Kumar et al., 2020). More recently, with the goal of improving explainability, offensive language identification at the token-level has received more attention (Mathew et al., 2021; Ranasinghe and Zampieri, 2021). A number of computational models have been applied to this task ranging from traditional machine learning classifiers, most notably SVMs (MacAvaney et al., 2019), to various deep learning models (Liu et al., 2019). While the clear majority of studies on this topic deal with English, some studies have addressed offensive language in other languages like Greek (Pitenis et al., 2020) and Turkish (C¸o¨ ltekin, 2020) while a few others have applied cross-lingual models to take advantage of existing English datasets when making predictions in languages with fewer resources (Ranasinghe and Zampieri, 2020). Several studies have applied machine learning and NLP methods to address research questions related to mental health in social media such as identifying users with a particular mental health condition and predicting the risk of self-harm or suicide ideation (De Choudhury et al., 2013; Preot¸iucPietro et al., 2015; Malmasi et al., 2016; De Choudhury et al., 2016; Chancellor"
2021.findings-acl.315,W15-1203,0,0.0388819,"Missing"
2021.findings-acl.315,2020.emnlp-main.470,1,0.864501,"ghe and Zampieri, 2021). A number of computational models have been applied to this task ranging from traditional machine learning classifiers, most notably SVMs (MacAvaney et al., 2019), to various deep learning models (Liu et al., 2019). While the clear majority of studies on this topic deal with English, some studies have addressed offensive language in other languages like Greek (Pitenis et al., 2020) and Turkish (C¸o¨ ltekin, 2020) while a few others have applied cross-lingual models to take advantage of existing English datasets when making predictions in languages with fewer resources (Ranasinghe and Zampieri, 2020). Several studies have applied machine learning and NLP methods to address research questions related to mental health in social media such as identifying users with a particular mental health condition and predicting the risk of self-harm or suicide ideation (De Choudhury et al., 2013; Preot¸iucPietro et al., 2015; Malmasi et al., 2016; De Choudhury et al., 2016; Chancellor and De Choudhury, 2020). The CLPsych workshop co-located with international NLP conferences has hosted multiple competitions on these topics providing participants with important benchmark datasets and attracting a large n"
2021.findings-acl.315,N19-1144,1,0.948269,"020) that analyzed the use of offensive language in Facebook messages from individuals with mood disorders. Our work fills this important gap by providing further empirical evidence of the use of offensive language by individuals with diagnosed depression or showing signs of depression. Data In our experiments, we use three publicly available English datasets with data collected from social media: one with offensive language annotation, and two datasets with posts from users with selfreported depression diagnosis. Offensive Language We use the Offensive Language Identification Dataset (OLID) (Zampieri et al., 2019a) to train offensive language identification models. OLID contains a total of 14,100 manually annotated posts from Twitter and it was released as the official dataset of SemEval-2019 Task 6 (OffensEval) (Zampieri et al., 2019b). We chose OLID due to its general hierarchical annotation taxonomy with the following levels: Level A: Offensive language identification: offensive (OFF) vs. non-offensive (NOT) Level B: Categorization of offensive language: targeted insult or threats (TIN) vs. untargeted profanity (UNT). Level C: Offensive language target identification: individual (IND) vs. group (GR"
2021.findings-acl.315,S19-2010,1,0.943698,"020) that analyzed the use of offensive language in Facebook messages from individuals with mood disorders. Our work fills this important gap by providing further empirical evidence of the use of offensive language by individuals with diagnosed depression or showing signs of depression. Data In our experiments, we use three publicly available English datasets with data collected from social media: one with offensive language annotation, and two datasets with posts from users with selfreported depression diagnosis. Offensive Language We use the Offensive Language Identification Dataset (OLID) (Zampieri et al., 2019a) to train offensive language identification models. OLID contains a total of 14,100 manually annotated posts from Twitter and it was released as the official dataset of SemEval-2019 Task 6 (OffensEval) (Zampieri et al., 2019b). We chose OLID due to its general hierarchical annotation taxonomy with the following levels: Level A: Offensive language identification: offensive (OFF) vs. non-offensive (NOT) Level B: Categorization of offensive language: targeted insult or threats (TIN) vs. untargeted profanity (UNT). Level C: Offensive language target identification: individual (IND) vs. group (GR"
2021.findings-acl.315,P18-1125,0,0.0122791,"individuals suffering from depression more likely to contain offensive language in existing datasets? RQ2: Are there differences in the nature of offensive language used by individuals with depression compared to control groups? 3600 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 3600–3606 August 1–6, 2021. ©2021 Association for Computational Linguistics 2 Related Work 3 Offensive language identification is a popular topic in NLP. Researchers have been working to improve the performance of systems trained to identify conversations that are likely to go awry (Zhang et al., 2018) and to detect the various types of offensive posts in social media (Basile et al., 2019; Kumar et al., 2020). More recently, with the goal of improving explainability, offensive language identification at the token-level has received more attention (Mathew et al., 2021; Ranasinghe and Zampieri, 2021). A number of computational models have been applied to this task ranging from traditional machine learning classifiers, most notably SVMs (MacAvaney et al., 2019), to various deep learning models (Liu et al., 2019). While the clear majority of studies on this topic deal with English, some studies"
2021.findings-acl.315,W19-3003,0,0.0105253,"s to address research questions related to mental health in social media such as identifying users with a particular mental health condition and predicting the risk of self-harm or suicide ideation (De Choudhury et al., 2013; Preot¸iucPietro et al., 2015; Malmasi et al., 2016; De Choudhury et al., 2016; Chancellor and De Choudhury, 2020). The CLPsych workshop co-located with international NLP conferences has hosted multiple competitions on these topics providing participants with important benchmark datasets and attracting a large number of teams (Coppersmith et al., 2015; Milne et al., 2016; Zirikly et al., 2019). There have been multiple studies on the impact of offensive and hateful speech on the individual’s psychological mental health and well-being (Bannink et al., 2014; Saha et al., 2019). The use of offensive language by individuals with mental health conditions, however, has not been substantially studies with the exception of Birnbaum et al. (2020) that analyzed the use of offensive language in Facebook messages from individuals with mood disorders. Our work fills this important gap by providing further empirical evidence of the use of offensive language by individuals with diagnosed depressi"
2021.findings-acl.315,2021.naacl-demos.17,1,0.686327,": ACL-IJCNLP 2021, pages 3600–3606 August 1–6, 2021. ©2021 Association for Computational Linguistics 2 Related Work 3 Offensive language identification is a popular topic in NLP. Researchers have been working to improve the performance of systems trained to identify conversations that are likely to go awry (Zhang et al., 2018) and to detect the various types of offensive posts in social media (Basile et al., 2019; Kumar et al., 2020). More recently, with the goal of improving explainability, offensive language identification at the token-level has received more attention (Mathew et al., 2021; Ranasinghe and Zampieri, 2021). A number of computational models have been applied to this task ranging from traditional machine learning classifiers, most notably SVMs (MacAvaney et al., 2019), to various deep learning models (Liu et al., 2019). While the clear majority of studies on this topic deal with English, some studies have addressed offensive language in other languages like Greek (Pitenis et al., 2020) and Turkish (C¸o¨ ltekin, 2020) while a few others have applied cross-lingual models to take advantage of existing English datasets when making predictions in languages with fewer resources (Ranasinghe and Zampieri"
2021.findings-acl.80,S19-2007,0,0.510452,"mmission, 2020) in the EU. Even in the United States, content moderation or the lack thereof can have significant impact on business (e.g., Parler was denied server space), government (U.S. Capitol Riots), and individuals (hate speech is linked to self-harm). Explainability is needed to indicate in detail why content has WARNING: This paper contains tweet examples and words that are offensive in nature. been deleted or flagged as inappropriate. Moreover, users can be educated by such feedback to avoid future biases. There have been several areas of work in the detection of offensive language (Basile et al., 2019; Fortuna and Nunes, 2018; Ranasinghe and Zampieri, 2020), covering overlapping characteristics such as toxicity, hate speech, cyberbullying, and cyber-aggression. Further, using a hierarchical approach to analyze different aspects of the offensive content, such as the type and the target of the offense, helps provide explainability. The Offensive Language Identification Dataset, or OLID, (Zampieri et al., 2019a) is one such example, and it has been widely used in research. OLID contains 14,100 English tweets, which were manually annotated using a three-level taxonomy: A: Offensive Language De"
2021.findings-acl.80,P19-1271,0,0.0187585,"annotation taxonomy. Section 4 introduces the computational models used in this study. Section 5 presents the SOLID dataset. Section 6 discusses the experimental results and Section 6.3 offers additional discussion and analysis. Finally, Section 7 concludes and discusses possible directions for future work. 2 Related Work There have been several recent studies on offensive language detection and related tasks such as hate speech, cyberbulling, aggression, and toxic comment detection. Hate speech identification is by far the most studied abusive language detection task (Ousidhoum et al., 2019; Chung et al., 2019; Mathew et al., 2021). One of the most widely used datasets is the one by Davidson et al. (2017), which contains over 24,000 English tweets labeled as non-offensive, hate speech, and profanity. A recent shared task on the topic is HatEval (Basile et al., 2019). In cyberbullying detection, Xu et al. (2012) used sentiment analysis and topic models to identify relevant topics. Dadvar et al. (2013) and Safi Samghabadi et al. (2020) studied the use of the conversational context for detecting cyberbullying. In particular, Dadvar et al. (2013) used userrelated features such as the frequency of profa"
2021.findings-acl.80,2020.lrec-1.758,0,0.105274,"ntext of n neighboring tokens. We compute the PMI score (Turney and Littman, 2003) of each n-gram in the training set w.r.t. each class:  P M I(wi , cj ) = log2 4 Models In this section, we describe the models used for semi-supervised annotation and for evaluating the C Table 1: Examples from the OLID dataset. Level A: Offensive Language Detection Is the text offensive? OFF Inappropriate language, insults, or threats. NOT Neither offensive, nor profane. The taxonomy was successfully adopted for several languages (Mubarak et al., 2021; Pitenis et al., 2020; Sigurbergsson and Derczynski, 2020; Çöltekin, 2020), and it was used in a series of shared tasks (Zampieri et al., 2019b; Mandl et al., 2019). Tweets from the OLID dataset labeled with the taxonomy are shown in Table 1. The OLID dataset consists of 13,241 training and 860 test tweets. Table 2 presents detailed statistics about the distribution of the labels. There is a substantial class imbalance on each level of annotation, especially at Level B. Furthermore, there is a sizable difference in the total number of annotations between the levels due to the schema (e.g., Level C is 1/3 smaller than Level A), and the data sizes for B and C are rath"
2021.findings-acl.80,N19-1423,0,0.277554,"w-toxic-comm ent-classification-challenge 916 sented in OLID’s taxonomy. We create a largescale semi-supervised dataset using the same annotation taxonomy as in OLID. 3 Tweet The OLID (Zampieri et al., 2019a) dataset tackles the challenge of detecting offensive language using a labeling schema that classifies each example using the following three-level hierarchy: contribution of SOLID for offensive language identification. We use a suite of heterogeneous machine learning models: PMI (Turney and Littman, 2003), FastText (Joulin et al., 2017), LSTM (Hochreiter and Schmidhuber, 1997), and BERT (Devlin et al., 2019). They have diverse inductive biases, which is an essential prerequisite for our semi-supervised setup (see Section 4.5). We assume that an ensemble of models with different inductive biases decreases each individual model’s bias. Level B: Categorization of Offensive Language Is the offensive text targeted? TIN Targeted insult or threat towards a group or individual. UNT Untargeted profanity or swearing. 4.1 PMI We use a PMI-based model that computes the ngram-based similarity of a tweet to the tweets of a particular class c in the training dataset. The model is considered naïve as it accounts"
2021.findings-acl.80,W18-4416,0,0.0227143,"ification Challenge2 at Kaggle provided participants with comments from Wikipedia annotated using six labels: toxic, severe toxic, obscene, threat, insult, and identity hate. The recent SemEval-2021 Toxic Spans Detection shared task addressed the identification of the token spans that made a post toxic (Pavlopoulos et al., 2021). There were several shared tasks that have focused specifically on offensive language identification. For example, GermEval 2018 (Wiegand et al., 2018) which focused on offensive language identification in German tweets, HASOC 2019 (Mandl et al., 2019), and TRAC 2018 (Fortuna et al., 2018). In this paper, we extend the prior work of the OLID dataset (Zampieri et al., 2019a). OLID is annotated using a hierarchical annotation schema as in (Basile et al., 2019; Mandl et al., 2019). In contrast to prior approaches, it takes both the target and the type of offensive content into account. This allows multiple types of offensive content (e.g., hate speech and cyberbullying) to be repre1 Available at: https://sites.google.com/sit e/offensevalsharedtask/solid 2 http://kaggle.com/c/jigsaw-toxic-comm ent-classification-challenge 916 sented in OLID’s taxonomy. We create a largescale semi-s"
2021.findings-acl.80,L18-1550,0,0.0186439,"use learning rates of 0.00002 for Levels A and B, and 0.00004 for Level C. We apply per-class weights to cope with the data imbalance in Level C as follows: IND=1, GRP=2, OTH=10. We use the Adam optimizer and a linear warm-up schedule with a 0.05 warm-up ratio. LSTM In contrast to the prior models, the LSTM model (Hochreiter and Schmidhuber, 1997; Vaswani et al., 2017) can account for long-distance relations between words. First is an embedding layer initialized with a concatenation of the GloVe 300-dimensional (Pennington et al., 2014) and FastText’s Common Crawl 300-dimensional embeddings (Grave et al., 2018). It is followed by a dropout and a bi-directional LSTM layer with an attention mechanism on top of it. We concatenate the attention mechanism’s output with averaged and maximum global poolings on the outputs of 918 4.5 Democratic Co-training Democratic co-training (Zhou and Goldman, 2004) is a semi-supervised technique used to create large datasets with noisy labels when provided with a set of diverse models trained in a supervised way. This approach has been successfully applied in tasks like time series prediction with missing data (Mohamed et al., 2007), early prognosis of academic perform"
2021.findings-acl.80,E17-2068,0,0.327106,"s.google.com/sit e/offensevalsharedtask/solid 2 http://kaggle.com/c/jigsaw-toxic-comm ent-classification-challenge 916 sented in OLID’s taxonomy. We create a largescale semi-supervised dataset using the same annotation taxonomy as in OLID. 3 Tweet The OLID (Zampieri et al., 2019a) dataset tackles the challenge of detecting offensive language using a labeling schema that classifies each example using the following three-level hierarchy: contribution of SOLID for offensive language identification. We use a suite of heterogeneous machine learning models: PMI (Turney and Littman, 2003), FastText (Joulin et al., 2017), LSTM (Hochreiter and Schmidhuber, 1997), and BERT (Devlin et al., 2019). They have diverse inductive biases, which is an essential prerequisite for our semi-supervised setup (see Section 4.5). We assume that an ensemble of models with different inductive biases decreases each individual model’s bias. Level B: Categorization of Offensive Language Is the offensive text targeted? TIN Targeted insult or threat towards a group or individual. UNT Untargeted profanity or swearing. 4.1 PMI We use a PMI-based model that computes the ngram-based similarity of a tweet to the tweets of a particular clas"
2021.findings-acl.80,W18-4401,1,0.910821,"Missing"
2021.findings-acl.80,2020.trac-1.1,1,0.909142,"Missing"
2021.findings-acl.80,2021.wanlp-1.13,0,0.170384,"ts only for the ngram frequencies in the discrete token space and only in the context of n neighboring tokens. We compute the PMI score (Turney and Littman, 2003) of each n-gram in the training set w.r.t. each class:  P M I(wi , cj ) = log2 4 Models In this section, we describe the models used for semi-supervised annotation and for evaluating the C Table 1: Examples from the OLID dataset. Level A: Offensive Language Detection Is the text offensive? OFF Inappropriate language, insults, or threats. NOT Neither offensive, nor profane. The taxonomy was successfully adopted for several languages (Mubarak et al., 2021; Pitenis et al., 2020; Sigurbergsson and Derczynski, 2020; Çöltekin, 2020), and it was used in a series of shared tasks (Zampieri et al., 2019b; Mandl et al., 2019). Tweets from the OLID dataset labeled with the taxonomy are shown in Table 1. The OLID dataset consists of 13,241 training and 860 test tweets. Table 2 presents detailed statistics about the distribution of the labels. There is a substantial class imbalance on each level of annotation, especially at Level B. Furthermore, there is a sizable difference in the total number of annotations between the levels due to the schema (e.g., Le"
2021.findings-acl.80,2020.lrec-1.430,0,0.0212663,"crete token space and only in the context of n neighboring tokens. We compute the PMI score (Turney and Littman, 2003) of each n-gram in the training set w.r.t. each class:  P M I(wi , cj ) = log2 4 Models In this section, we describe the models used for semi-supervised annotation and for evaluating the C Table 1: Examples from the OLID dataset. Level A: Offensive Language Detection Is the text offensive? OFF Inappropriate language, insults, or threats. NOT Neither offensive, nor profane. The taxonomy was successfully adopted for several languages (Mubarak et al., 2021; Pitenis et al., 2020; Sigurbergsson and Derczynski, 2020; Çöltekin, 2020), and it was used in a series of shared tasks (Zampieri et al., 2019b; Mandl et al., 2019). Tweets from the OLID dataset labeled with the taxonomy are shown in Table 1. The OLID dataset consists of 13,241 training and 860 test tweets. Table 2 presents detailed statistics about the distribution of the labels. There is a substantial class imbalance on each level of annotation, especially at Level B. Furthermore, there is a sizable difference in the total number of annotations between the levels due to the schema (e.g., Level C is 1/3 smaller than Level A), and the data sizes for"
2021.findings-acl.80,D19-1474,0,0.0248631,"es the OLID dataset and annotation taxonomy. Section 4 introduces the computational models used in this study. Section 5 presents the SOLID dataset. Section 6 discusses the experimental results and Section 6.3 offers additional discussion and analysis. Finally, Section 7 concludes and discusses possible directions for future work. 2 Related Work There have been several recent studies on offensive language detection and related tasks such as hate speech, cyberbulling, aggression, and toxic comment detection. Hate speech identification is by far the most studied abusive language detection task (Ousidhoum et al., 2019; Chung et al., 2019; Mathew et al., 2021). One of the most widely used datasets is the one by Davidson et al. (2017), which contains over 24,000 English tweets labeled as non-offensive, hate speech, and profanity. A recent shared task on the topic is HatEval (Basile et al., 2019). In cyberbullying detection, Xu et al. (2012) used sentiment analysis and topic models to identify relevant topics. Dadvar et al. (2013) and Safi Samghabadi et al. (2020) studied the use of the conversational context for detecting cyberbullying. In particular, Dadvar et al. (2013) used userrelated features such as th"
2021.findings-acl.80,2021.semeval-1.6,0,0.0283613,"omments in English and Hindi for training and validation. Facebook and Twitter datasets were used for testing. The goal was to discriminate between three classes: non-aggressive, covertly aggressive, and overly aggressive. Two other shared tasks addressed toxic language. The Toxic Comment Classification Challenge2 at Kaggle provided participants with comments from Wikipedia annotated using six labels: toxic, severe toxic, obscene, threat, insult, and identity hate. The recent SemEval-2021 Toxic Spans Detection shared task addressed the identification of the token spans that made a post toxic (Pavlopoulos et al., 2021). There were several shared tasks that have focused specifically on offensive language identification. For example, GermEval 2018 (Wiegand et al., 2018) which focused on offensive language identification in German tweets, HASOC 2019 (Mandl et al., 2019), and TRAC 2018 (Fortuna et al., 2018). In this paper, we extend the prior work of the OLID dataset (Zampieri et al., 2019a). OLID is annotated using a hierarchical annotation schema as in (Basile et al., 2019; Mandl et al., 2019). In contrast to prior approaches, it takes both the target and the type of offensive content into account. This allo"
2021.findings-acl.80,D14-1162,0,0.0926922,"We fine-tune BERT for 2, 3, and 3 epochs for Level A, B, and C, respectively. We use learning rates of 0.00002 for Levels A and B, and 0.00004 for Level C. We apply per-class weights to cope with the data imbalance in Level C as follows: IND=1, GRP=2, OTH=10. We use the Adam optimizer and a linear warm-up schedule with a 0.05 warm-up ratio. LSTM In contrast to the prior models, the LSTM model (Hochreiter and Schmidhuber, 1997; Vaswani et al., 2017) can account for long-distance relations between words. First is an embedding layer initialized with a concatenation of the GloVe 300-dimensional (Pennington et al., 2014) and FastText’s Common Crawl 300-dimensional embeddings (Grave et al., 2018). It is followed by a dropout and a bi-directional LSTM layer with an attention mechanism on top of it. We concatenate the attention mechanism’s output with averaged and maximum global poolings on the outputs of 918 4.5 Democratic Co-training Democratic co-training (Zhou and Goldman, 2004) is a semi-supervised technique used to create large datasets with noisy labels when provided with a set of diverse models trained in a supervised way. This approach has been successfully applied in tasks like time series prediction w"
2021.findings-acl.80,2020.lrec-1.629,1,0.862054,"frequencies in the discrete token space and only in the context of n neighboring tokens. We compute the PMI score (Turney and Littman, 2003) of each n-gram in the training set w.r.t. each class:  P M I(wi , cj ) = log2 4 Models In this section, we describe the models used for semi-supervised annotation and for evaluating the C Table 1: Examples from the OLID dataset. Level A: Offensive Language Detection Is the text offensive? OFF Inappropriate language, insults, or threats. NOT Neither offensive, nor profane. The taxonomy was successfully adopted for several languages (Mubarak et al., 2021; Pitenis et al., 2020; Sigurbergsson and Derczynski, 2020; Çöltekin, 2020), and it was used in a series of shared tasks (Zampieri et al., 2019b; Mandl et al., 2019). Tweets from the OLID dataset labeled with the taxonomy are shown in Table 1. The OLID dataset consists of 13,241 training and 860 test tweets. Table 2 presents detailed statistics about the distribution of the labels. There is a substantial class imbalance on each level of annotation, especially at Level B. Furthermore, there is a sizable difference in the total number of annotations between the levels due to the schema (e.g., Level C is 1/3 smaller t"
2021.findings-acl.80,2020.emnlp-main.470,1,0.91373,"Missing"
2021.findings-acl.80,N12-1084,0,0.0385142,"2 Related Work There have been several recent studies on offensive language detection and related tasks such as hate speech, cyberbulling, aggression, and toxic comment detection. Hate speech identification is by far the most studied abusive language detection task (Ousidhoum et al., 2019; Chung et al., 2019; Mathew et al., 2021). One of the most widely used datasets is the one by Davidson et al. (2017), which contains over 24,000 English tweets labeled as non-offensive, hate speech, and profanity. A recent shared task on the topic is HatEval (Basile et al., 2019). In cyberbullying detection, Xu et al. (2012) used sentiment analysis and topic models to identify relevant topics. Dadvar et al. (2013) and Safi Samghabadi et al. (2020) studied the use of the conversational context for detecting cyberbullying. In particular, Dadvar et al. (2013) used userrelated features such as the frequency of profanity in previous messages. More recent work has addressed the issues of scalable and timely detection of cyberbullying in online social networks. To this end, Rafiq et al. (2018) employed a dynamic priority scheduler, and Yao et al. (2019) proposed a sequential hypothesis testing. Safi Samghabadi et al. (2"
2021.findings-acl.80,N19-1144,1,0.929775,"deleted or flagged as inappropriate. Moreover, users can be educated by such feedback to avoid future biases. There have been several areas of work in the detection of offensive language (Basile et al., 2019; Fortuna and Nunes, 2018; Ranasinghe and Zampieri, 2020), covering overlapping characteristics such as toxicity, hate speech, cyberbullying, and cyber-aggression. Further, using a hierarchical approach to analyze different aspects of the offensive content, such as the type and the target of the offense, helps provide explainability. The Offensive Language Identification Dataset, or OLID, (Zampieri et al., 2019a) is one such example, and it has been widely used in research. OLID contains 14,100 English tweets, which were manually annotated using a three-level taxonomy: A: Offensive Language Detection B: Categorization of Offensive Language C: Offensive Language Target Identification The taxonomy proposed in OLID makes it possible to represent different kinds of offensive content as a function of the type and the target of a post. For example, offensive messages targeting a group are likely hate speech, whereas offensive messages targeting an individual are likely cyberbullying. OLID has been used to"
2021.findings-acl.80,S19-2010,1,0.931649,"deleted or flagged as inappropriate. Moreover, users can be educated by such feedback to avoid future biases. There have been several areas of work in the detection of offensive language (Basile et al., 2019; Fortuna and Nunes, 2018; Ranasinghe and Zampieri, 2020), covering overlapping characteristics such as toxicity, hate speech, cyberbullying, and cyber-aggression. Further, using a hierarchical approach to analyze different aspects of the offensive content, such as the type and the target of the offense, helps provide explainability. The Offensive Language Identification Dataset, or OLID, (Zampieri et al., 2019a) is one such example, and it has been widely used in research. OLID contains 14,100 English tweets, which were manually annotated using a three-level taxonomy: A: Offensive Language Detection B: Categorization of Offensive Language C: Offensive Language Target Identification The taxonomy proposed in OLID makes it possible to represent different kinds of offensive content as a function of the type and the target of a post. For example, offensive messages targeting a group are likely hate speech, whereas offensive messages targeting an individual are likely cyberbullying. OLID has been used to"
2021.findings-emnlp.154,2020.osact-1.14,0,0.0269419,"tic methods to detect such posts au- specific knowledge. To cope with this limitation, more recently, domain-specific models have been tomatically. Early efforts included methods that trained and/or fine tuned to different domains such used various linguistic features in tandem with linear classifiers (Malmasi and Zampieri, 2017) as finance (FinBERT) (Araci, 2019), law (LEGALwhile, more recently, deep neural networks (DNNs) BERT) (Chalkidis et al., 2020), scientific texts (Ranasinghe et al., 2019), transfer learning (Wiede- (SciBERT) (Beltagy et al., 2019), and microblogmann et al., 2020; Abu Farha and Magdy, 2020), ging (BerTweet) (Nguyen et al., 2020). Caselli et al. (2021) recently released Hateand pre-trained language models (Liu et al., 2019a; BERT, a BERT transformer model for abusive lanRanasinghe and Zampieri, 2020, 2021) have led to even further advances. As evidenced in recent com- guage detection trained on the Reddit Abusive Language English dataset (RAL-E). HateBERT petitions, the performance of these models varies achieves competitive performance on a few benchwith the sub-task that they are designed to address as well as the datasets used to train them. For exam- mark datasets but relies"
2021.findings-emnlp.154,S19-2011,0,0.261053,"ion. The base model is prefication corpus available with over 1.4 million trained on a large English corpus, e.g., Wikipedia, offensive instances. We evaluate fBERT’s perBookCorpus (Zhu et al., 2015), using unsuperformance on identifying offensive content on vised masked language modeling and next sentence multiple English datasets and we test several prediction objectives to adjust the model weights. thresholds for selecting instances from SOLID. Various other transformer-based models have also The fBERT model will be made freely available to the community. been introduced including RoBERTa (Liu et al., 2019b), XLNet (Yang et al., 2019), XLM-R (Con1 Introduction neau et al., 2019). All of these models, however, are trained on general-purpose corpora for better To cope with the spread of offensive content and language understanding, generally lacking domainhate speech online, researchers have worked to develop automatic methods to detect such posts au- specific knowledge. To cope with this limitation, more recently, domain-specific models have been tomatically. Early efforts included methods that trained and/or fine tuned to different domains such used various linguistic features in tandem with li"
2021.findings-emnlp.154,S19-2007,0,0.0473106,"Missing"
2021.findings-emnlp.154,2021.ccl-1.108,0,0.0709115,"Missing"
2021.findings-emnlp.154,D19-1371,0,0.0230816,"te speech online, researchers have worked to develop automatic methods to detect such posts au- specific knowledge. To cope with this limitation, more recently, domain-specific models have been tomatically. Early efforts included methods that trained and/or fine tuned to different domains such used various linguistic features in tandem with linear classifiers (Malmasi and Zampieri, 2017) as finance (FinBERT) (Araci, 2019), law (LEGALwhile, more recently, deep neural networks (DNNs) BERT) (Chalkidis et al., 2020), scientific texts (Ranasinghe et al., 2019), transfer learning (Wiede- (SciBERT) (Beltagy et al., 2019), and microblogmann et al., 2020; Abu Farha and Magdy, 2020), ging (BerTweet) (Nguyen et al., 2020). Caselli et al. (2021) recently released Hateand pre-trained language models (Liu et al., 2019a; BERT, a BERT transformer model for abusive lanRanasinghe and Zampieri, 2020, 2021) have led to even further advances. As evidenced in recent com- guage detection trained on the Reddit Abusive Language English dataset (RAL-E). HateBERT petitions, the performance of these models varies achieves competitive performance on a few benchwith the sub-task that they are designed to address as well as the data"
2021.findings-emnlp.154,malmasi-zampieri-2017-detecting,1,0.829006,"al., 2019), XLM-R (Con1 Introduction neau et al., 2019). All of these models, however, are trained on general-purpose corpora for better To cope with the spread of offensive content and language understanding, generally lacking domainhate speech online, researchers have worked to develop automatic methods to detect such posts au- specific knowledge. To cope with this limitation, more recently, domain-specific models have been tomatically. Early efforts included methods that trained and/or fine tuned to different domains such used various linguistic features in tandem with linear classifiers (Malmasi and Zampieri, 2017) as finance (FinBERT) (Araci, 2019), law (LEGALwhile, more recently, deep neural networks (DNNs) BERT) (Chalkidis et al., 2020), scientific texts (Ranasinghe et al., 2019), transfer learning (Wiede- (SciBERT) (Beltagy et al., 2019), and microblogmann et al., 2020; Abu Farha and Magdy, 2020), ging (BerTweet) (Nguyen et al., 2020). Caselli et al. (2021) recently released Hateand pre-trained language models (Liu et al., 2019a; BERT, a BERT transformer model for abusive lanRanasinghe and Zampieri, 2020, 2021) have led to even further advances. As evidenced in recent com- guage detection trained on"
2021.findings-emnlp.154,2021.woah-1.3,0,0.0618667,"Missing"
2021.findings-emnlp.154,P19-4007,0,0.0225275,"Missing"
2021.findings-emnlp.154,N19-1423,0,0.511203,"Offensive Content Diptanu Sarkar1 , Marcos Zampieri1 , Tharindu Ranasinghe2 , Alexander Ororbia1 1 Rochester Institute of Technology, USA 2 University of Wolverhampton, UK ds9297@rit.edu Abstract models such as BERT (Devlin et al., 2019) outperformed other neural architectures and statistical Transformer-based models such as BERT, XLlearning methods. NET, and XLM-R have achieved state-of-theThe introduction of representations learned art performance across various NLP tasks through the bidirectional encoding inherent to neuincluding the identification of offensive lanral transformers (BERT) (Devlin et al., 2019) has guage and hate speech, an important problem in social media. In this paper, we present been driven much progress in areas in NLP such fBERT, a BERT model retrained on SOLID, as language understanding, named entity recognithe largest English offensive language identition, and text classification. The base model is prefication corpus available with over 1.4 million trained on a large English corpus, e.g., Wikipedia, offensive instances. We evaluate fBERT’s perBookCorpus (Zhu et al., 2015), using unsuperformance on identifying offensive content on vised masked language modeling and next sent"
2021.findings-emnlp.154,S19-2009,0,0.0280274,"Missing"
2021.findings-emnlp.154,W18-4401,1,0.817969,"e on a few benchwith the sub-task that they are designed to address as well as the datasets used to train them. For exam- mark datasets but relies heavily on manually annotated labels. Moreover, HateBERT was trained on a ple, classical statistical learning models such as the task-specific dataset (aggression) instead of a more support vector machine (SVM) have outperformed neural transformers in hate speech detection at Hat- general dataset that encompasses multiple types of offensive language (e.g. hate speech, cyberbulEval 2019 (Basile et al., 2019) and in aggression detection at TRAC 2018 (Kumar et al., 2018). How- lying, profanity) like the popular OLID (Zampieri et al., 2019a) used in OfffensEval 2019 at SemEval. ever, for both of these tasks in OffensEval 2019 and 2020 (Zampieri et al., 2019b, 2020), which focused To address this gap, in this study, we present on the identification of more general offensive lan- fBERT, a pre-trained BERT model trained on guage identification, pre-trained transformer-based SOLID (Rosenthal et al., 2021), a recently released 1792 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 1792–1798 November 7–11, 2021. ©2021 Association for Compu"
2021.findings-emnlp.154,2020.trac-1.1,1,0.797553,"Missing"
2021.findings-emnlp.154,2020.emnlp-demos.2,0,0.0192271,"ific knowledge. To cope with this limitation, more recently, domain-specific models have been tomatically. Early efforts included methods that trained and/or fine tuned to different domains such used various linguistic features in tandem with linear classifiers (Malmasi and Zampieri, 2017) as finance (FinBERT) (Araci, 2019), law (LEGALwhile, more recently, deep neural networks (DNNs) BERT) (Chalkidis et al., 2020), scientific texts (Ranasinghe et al., 2019), transfer learning (Wiede- (SciBERT) (Beltagy et al., 2019), and microblogmann et al., 2020; Abu Farha and Magdy, 2020), ging (BerTweet) (Nguyen et al., 2020). Caselli et al. (2021) recently released Hateand pre-trained language models (Liu et al., 2019a; BERT, a BERT transformer model for abusive lanRanasinghe and Zampieri, 2020, 2021) have led to even further advances. As evidenced in recent com- guage detection trained on the Reddit Abusive Language English dataset (RAL-E). HateBERT petitions, the performance of these models varies achieves competitive performance on a few benchwith the sub-task that they are designed to address as well as the datasets used to train them. For exam- mark datasets but relies heavily on manually annotated labels. M"
2021.findings-emnlp.154,2020.emnlp-main.470,1,0.723801,"uned to different domains such used various linguistic features in tandem with linear classifiers (Malmasi and Zampieri, 2017) as finance (FinBERT) (Araci, 2019), law (LEGALwhile, more recently, deep neural networks (DNNs) BERT) (Chalkidis et al., 2020), scientific texts (Ranasinghe et al., 2019), transfer learning (Wiede- (SciBERT) (Beltagy et al., 2019), and microblogmann et al., 2020; Abu Farha and Magdy, 2020), ging (BerTweet) (Nguyen et al., 2020). Caselli et al. (2021) recently released Hateand pre-trained language models (Liu et al., 2019a; BERT, a BERT transformer model for abusive lanRanasinghe and Zampieri, 2020, 2021) have led to even further advances. As evidenced in recent com- guage detection trained on the Reddit Abusive Language English dataset (RAL-E). HateBERT petitions, the performance of these models varies achieves competitive performance on a few benchwith the sub-task that they are designed to address as well as the datasets used to train them. For exam- mark datasets but relies heavily on manually annotated labels. Moreover, HateBERT was trained on a ple, classical statistical learning models such as the task-specific dataset (aggression) instead of a more support vector machine (SVM) h"
2021.findings-emnlp.154,2021.naacl-demos.17,1,0.80993,"Missing"
2021.findings-emnlp.154,2021.findings-acl.80,1,0.816602,"al dataset that encompasses multiple types of offensive language (e.g. hate speech, cyberbulEval 2019 (Basile et al., 2019) and in aggression detection at TRAC 2018 (Kumar et al., 2018). How- lying, profanity) like the popular OLID (Zampieri et al., 2019a) used in OfffensEval 2019 at SemEval. ever, for both of these tasks in OffensEval 2019 and 2020 (Zampieri et al., 2019b, 2020), which focused To address this gap, in this study, we present on the identification of more general offensive lan- fBERT, a pre-trained BERT model trained on guage identification, pre-trained transformer-based SOLID (Rosenthal et al., 2021), a recently released 1792 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 1792–1798 November 7–11, 2021. ©2021 Association for Computational Linguistics large dataset crated using OLID’s general annotation model but using semi-supervised learning instead of manually annotated labels. SOLID contains over 1.4 million English tweets with offensive scores greater than 0.5. We show that the proposed fBERT outperforms both a plain BERT implementation and HateBERT on various offensive and hate speech detection tasks. The contributions of this paper are as follows: 1. An"
2021.findings-emnlp.154,2020.semeval-1.213,0,0.0156418,"architectures such as BERT (Devlin et al., 2019). These systems have achieved top performance in popular competitions such as HASOC 2019 (Mandl et al., 2019), HatEval 2019 (Basile et al., 2019), OffensEval 2019 and 2020 (Zampieri et al., 2019b, 2020), and TRAC 2020 (Kumar et al., 2020). The great performance obtained by these systems provides further evidence that pre-trained transformer models are a good fit for the kind of semantic understanding required when identifying offensive content online. Most of the top systems submitted to the aforementioned competitions (Ranasinghe et al., 2019; Wiedemann et al., 2020; Liu et al., 2019a), howThreshold Instances ever, use models pre-trained on standard contem0.5 - 1.0 1,446,580 porary texts. User generated content and offensive 0.6 - 1.0 1,040,525 language online, however, contain its own set of 0.7 - 1.0 700,719 distinctive features that models trained on standard 0.8 - 1.0 348,038 texts may fail to represent. Therefore, fine-tuning 0.9 - 1.0 2,771 pre-trained models to this challenging domain is a promising but under explored research direction. Table 1: Offensive instances from the SOLID dataset, organized according to threshold. To the best of our knowl"
2021.findings-emnlp.154,N19-1144,1,0.934962,"as well as the datasets used to train them. For exam- mark datasets but relies heavily on manually annotated labels. Moreover, HateBERT was trained on a ple, classical statistical learning models such as the task-specific dataset (aggression) instead of a more support vector machine (SVM) have outperformed neural transformers in hate speech detection at Hat- general dataset that encompasses multiple types of offensive language (e.g. hate speech, cyberbulEval 2019 (Basile et al., 2019) and in aggression detection at TRAC 2018 (Kumar et al., 2018). How- lying, profanity) like the popular OLID (Zampieri et al., 2019a) used in OfffensEval 2019 at SemEval. ever, for both of these tasks in OffensEval 2019 and 2020 (Zampieri et al., 2019b, 2020), which focused To address this gap, in this study, we present on the identification of more general offensive lan- fBERT, a pre-trained BERT model trained on guage identification, pre-trained transformer-based SOLID (Rosenthal et al., 2021), a recently released 1792 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 1792–1798 November 7–11, 2021. ©2021 Association for Computational Linguistics large dataset crated using OLID’s general annota"
2021.findings-emnlp.154,S19-2010,1,0.938986,"as well as the datasets used to train them. For exam- mark datasets but relies heavily on manually annotated labels. Moreover, HateBERT was trained on a ple, classical statistical learning models such as the task-specific dataset (aggression) instead of a more support vector machine (SVM) have outperformed neural transformers in hate speech detection at Hat- general dataset that encompasses multiple types of offensive language (e.g. hate speech, cyberbulEval 2019 (Basile et al., 2019) and in aggression detection at TRAC 2018 (Kumar et al., 2018). How- lying, profanity) like the popular OLID (Zampieri et al., 2019a) used in OfffensEval 2019 at SemEval. ever, for both of these tasks in OffensEval 2019 and 2020 (Zampieri et al., 2019b, 2020), which focused To address this gap, in this study, we present on the identification of more general offensive lan- fBERT, a pre-trained BERT model trained on guage identification, pre-trained transformer-based SOLID (Rosenthal et al., 2021), a recently released 1792 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 1792–1798 November 7–11, 2021. ©2021 Association for Computational Linguistics large dataset crated using OLID’s general annota"
2021.findings-emnlp.296,S19-2007,0,0.062471,"Missing"
2021.findings-emnlp.296,2021.findings-acl.315,1,0.716869,"to account (Wiegand et al., 2018; Mendelsohn et al., 2020; Palmer et al., 2017; Eder et al., 2019; Castroviejo et al., 2020). A few related works With the increase of social media usage, the issue of toxic language has become an important problem in our society. Automatic methods are needed to help mitigate this problem, and for this reason the study of toxic speech in NLP has become very popularity in recent years. Different categories and definitions have been proposed, including hate speech (Schmidt and Wiegand, 2017; Vashistha and Zubiaga, 2021), offensive language (Zampieri et al., 2019; Bucur et al., 2021), aggression (Kumar et al., 2018, 2020), as well as further sub-categories depending on the targets, such as women, migrants, etc. (Basile et al., 2019). From a computational perspective, the problem is usually approached as a classification task at the post level, where a clas1 sifier is trained to predict whether a social media https://www.merriam-webster.com/ post contains offensive/toxic language. dictionary/pejorative 3493 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 3493–3498 November 7–11, 2021. ©2021 Association for Computational Linguistics to ours incl"
2021.findings-emnlp.296,N19-1423,0,0.00666428,"ee classes. tweet, we want to be able to say if the word was The selected tweet-word pairs extracted for both used pejoratively or not in that tweet. of the data sets were then annotated with binary In order to prepare our data, for each tweet-word valued labels, denoting whether the word in the pair, the tweet was tokenized and the position of pair is used pejoratively (label 1) or not (label 0) in the occurrence of the word was found among the the tweet. We used the Wiktionary definitions in tokens. Then, we generated a contextual embedorder to label words as pejorative only when used ding (Devlin et al., 2019) for that occurrence, by with senses marked as ”derogatory” in Wiktionary. employing various BERT models, pre-trained on 3495 English texts, provided by the huggingface Python library (Wolf et al., 2019). The embedding obtained for the specified position is computed by summing the 768-dimensional hidden states generated for that position by each of the 12 layers of the BERT architecture. We note that, for out-of-vocabulary words, the BERT tokenizer provided by the huggingface library splits them into sub-words. In this case we chose to generate the embeddings for each of the sub-words of our w"
2021.findings-emnlp.296,W19-3513,0,0.0223092,"hrough semantic amelioration over the years - it used to be a slur and is losing its negative connotation (Brontsema, 2004)). Recognizing the complexity of the phenomenon, with its linguistic subtleties as well as the variability related to culture and context, are important to successfully recognize pejorative words and by extension offensive posts and hate speech. Pejorative language is still largely underexplored in computational linguistics. There are very few studies addressing or taking pejorative language into account (Wiegand et al., 2018; Mendelsohn et al., 2020; Palmer et al., 2017; Eder et al., 2019; Castroviejo et al., 2020). A few related works With the increase of social media usage, the issue of toxic language has become an important problem in our society. Automatic methods are needed to help mitigate this problem, and for this reason the study of toxic speech in NLP has become very popularity in recent years. Different categories and definitions have been proposed, including hate speech (Schmidt and Wiegand, 2017; Vashistha and Zubiaga, 2021), offensive language (Zampieri et al., 2019; Bucur et al., 2021), aggression (Kumar et al., 2018, 2020), as well as further sub-categories dep"
2021.findings-emnlp.296,W18-4401,1,0.836499,"Mendelsohn et al., 2020; Palmer et al., 2017; Eder et al., 2019; Castroviejo et al., 2020). A few related works With the increase of social media usage, the issue of toxic language has become an important problem in our society. Automatic methods are needed to help mitigate this problem, and for this reason the study of toxic speech in NLP has become very popularity in recent years. Different categories and definitions have been proposed, including hate speech (Schmidt and Wiegand, 2017; Vashistha and Zubiaga, 2021), offensive language (Zampieri et al., 2019; Bucur et al., 2021), aggression (Kumar et al., 2018, 2020), as well as further sub-categories depending on the targets, such as women, migrants, etc. (Basile et al., 2019). From a computational perspective, the problem is usually approached as a classification task at the post level, where a clas1 sifier is trained to predict whether a social media https://www.merriam-webster.com/ post contains offensive/toxic language. dictionary/pejorative 3493 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 3493–3498 November 7–11, 2021. ©2021 Association for Computational Linguistics to ours include Palmer et al. (2017) who foc"
2021.findings-emnlp.296,N16-2013,0,0.0328905,"ian, we used another onlineavailable dictionary, dexonline3 , and selected all of the words that had a pejorative definition and where the definition was intended for the word not for an expression built around the word. 2 3 https://www.wiktionary.org/ https://dexonline.ro/ Figure 1: Distribution of parts of speech for the collected words for each language in WordNet. 3 Pejorative Tweet Dataset For building a data set of English texts containing words that are used pejoratively, we started by looking at three datasets of hate speech on Twitter: (Davidson et al., 2017), (Basile et al., 2019). (Waseem and Hovy, 2016), and selected the tweets that contain words from our pejorative lexicon (after normalizing words to their stems). For each data set, we extracted pairs of words and tweets where they occur. The dataset published by Davidson et al. (2017) contains tweets annotated with one of three classes (hateful, offensive and neither). For each label, the 3494 number of pejorative words found in the tweets is the following: 1, 114 out of 1, 430 hateful tweets, 8, 358 out of 19, 190 offensive tweets, and 2, 221 among the remaining 4, 163 tweets were found to contain pejorative words. The hate speech dataset"
2021.findings-emnlp.296,N18-1095,0,0.0203922,"rative meaning through semantic change (e.g. the word “queer” went through semantic amelioration over the years - it used to be a slur and is losing its negative connotation (Brontsema, 2004)). Recognizing the complexity of the phenomenon, with its linguistic subtleties as well as the variability related to culture and context, are important to successfully recognize pejorative words and by extension offensive posts and hate speech. Pejorative language is still largely underexplored in computational linguistics. There are very few studies addressing or taking pejorative language into account (Wiegand et al., 2018; Mendelsohn et al., 2020; Palmer et al., 2017; Eder et al., 2019; Castroviejo et al., 2020). A few related works With the increase of social media usage, the issue of toxic language has become an important problem in our society. Automatic methods are needed to help mitigate this problem, and for this reason the study of toxic speech in NLP has become very popularity in recent years. Different categories and definitions have been proposed, including hate speech (Schmidt and Wiegand, 2017; Vashistha and Zubiaga, 2021), offensive language (Zampieri et al., 2019; Bucur et al., 2021), aggression"
2021.findings-emnlp.296,2020.trac-1.1,1,0.813703,"Missing"
2021.findings-emnlp.296,2020.figlang-1.34,0,0.0852125,"Missing"
2021.findings-emnlp.296,N19-1144,1,0.931957,"d ‘slur’ refer to symbolic vehicles designed by convention to derogate targeted individuals or groups” (Anderson and Lepore, 2013). While pejorative language is often used in offensive speech (Castroviejo et al., 2020), they are not identical categories. There are offensive posts that do not use pejorative words (e.g. “Women belong in the kitchen”), and pejorative uses of words that are not harmful (“What a shitty chair”) because the offensive content is not targeted at a person or a group as described in the popular annotation taxonomy of the Offensive Language Identification Dataset (OLID) (Zampieri et al., 2019). Words can have a negative meaning in one context and not in others (such as the figurative meanings of “trash” or “pussy”); or be pejorative in one language or culture, and not in others (such as the Romanian “cioara” (literally, “crow”) - a slur for people of color). Slurs can also lose their pejorative meaning through semantic change (e.g. the word “queer” went through semantic amelioration over the years - it used to be a slur and is losing its negative connotation (Brontsema, 2004)). Recognizing the complexity of the phenomenon, with its linguistic subtleties as well as the variability r"
2021.findings-emnlp.296,2021.ccl-1.108,0,0.025455,"Missing"
2021.findings-emnlp.296,2020.emnlp-demos.2,0,0.0390588,"Missing"
2021.findings-emnlp.296,W17-3014,0,0.0397028,"Missing"
2021.findings-emnlp.296,2020.emnlp-main.470,1,0.761635,"-of-the-art contextual embeddings in order to automatically distinguish pejorative from non-pejorative uses of words, obtaining promising results. In the future, we would like to explore modelling the problem of pejorativity detection as a sequence labelling task. At the application level, integrating pejorativity detection into hate speech detection systems, for example, would be a promising area for future research. From a linguistic perspective, it would be interesting to analyze occurrence and pejorative value cross-lingually taking advantage of large pretrained cross-lingual models as in Ranasinghe and Zampieri (2020, 2021) for offensive language identification. We expect pejorative connotations to be difficult to translate and not transfer well across languages, which could also have practical implications. We would also like to extend our dataset of social media posts to cover more pejorative terms, as well as other languages. 4 The lexicon and the corpus are available at: https: //nlp.unibuc.ro/resources Luvell Anderson and Ernie Lepore. 2013. What did you call me? slurs as prohibited words setting things up. Analytic Philosophy, 54(3):350–63. Valerio Basile, Cristina Bosco, Elisabetta Fersini, Nozza D"
2021.findings-emnlp.296,2021.naacl-demos.17,1,0.783374,"Missing"
2021.findings-emnlp.296,W17-1101,0,0.0248501,"in computational linguistics. There are very few studies addressing or taking pejorative language into account (Wiegand et al., 2018; Mendelsohn et al., 2020; Palmer et al., 2017; Eder et al., 2019; Castroviejo et al., 2020). A few related works With the increase of social media usage, the issue of toxic language has become an important problem in our society. Automatic methods are needed to help mitigate this problem, and for this reason the study of toxic speech in NLP has become very popularity in recent years. Different categories and definitions have been proposed, including hate speech (Schmidt and Wiegand, 2017; Vashistha and Zubiaga, 2021), offensive language (Zampieri et al., 2019; Bucur et al., 2021), aggression (Kumar et al., 2018, 2020), as well as further sub-categories depending on the targets, such as women, migrants, etc. (Basile et al., 2019). From a computational perspective, the problem is usually approached as a classification task at the post level, where a clas1 sifier is trained to predict whether a social media https://www.merriam-webster.com/ post contains offensive/toxic language. dictionary/pejorative 3493 Findings of the Association for Computational Linguistics: EMNLP 2021, pag"
2021.germeval-1.5,W19-3510,0,0.023791,"an extensive topic that has become popular over the past several years. The majority of the research related to this topic is centered on English data due to the availability of annotated datasets (Zampieri et al., 2019a; Rosenthal et al., 2021). Notwithstanding this, offensive language datasets are being annotated in other languages. Researchers have examined offensive content across multiple social media platforms and have both annotated and utilized data from different languages such as Greek (Pitenis et al., 2020), Marathi (Gaikwad et al., 2021), Italian (Chiril et al., 2019), Portuguese (Fortuna et al., 2019; Vargas et al., 2021), Arabic (Mubarak et al., 2021), Turkish (C¸o¨ ltekin, 2020), and multiple languages of India (Ranasinghe and Zampieri, 2021a). Past approaches to tackling the problem of offensive content on social media have relied on using a variety of computational models ranging from traditional machine learning classifiers such as Logistic Regression and SVMs (Malmasi and Zampieri, 2018), to various deep learning models (de Gibert et al., 2018). SemEval-2019 Task 5 (HatEval) (Basile et al., 2019) presented the challenge of detecting the presence of hate speech and identifying furthe"
2021.germeval-1.5,2020.osact-1.2,0,0.0216081,"019), the detection of toxic content remains an integral 32 Proceedings of the GermEval 2021 Shared Task on the Identification of Toxic, Engaging, and Fact-Claiming Comments co-located with KONVENS 2 Related Work tification. The use of pre-trained BERT models, as well as BERT-based models, was shown to be able to achieve competitive performance in popular competitions such as OffensEval (Zampieri et al., 2019b, 2020). Language-specific and multilingual models have also been introduced to assist NLP research in various languages such as GBERT for German (Chan et al., 2020), AraBERT for Arabic (Antoun et al., 2020), and the multilingual XLM-R (Conneau et al., 2019) that has been been applied to offensive language identification (Ranasinghe and Zampieri, 2020, 2021c). The identification of offensive language in online discussions is an extensive topic that has become popular over the past several years. The majority of the research related to this topic is centered on English data due to the availability of annotated datasets (Zampieri et al., 2019a; Rosenthal et al., 2021). Notwithstanding this, offensive language datasets are being annotated in other languages. Researchers have examined offensive conte"
2021.germeval-1.5,S19-2007,0,0.0601678,"Missing"
2021.germeval-1.5,2021.acl-long.210,0,0.0364309,"book comments in German. Considering the relatedness of the three tasks, we approached the problem using large pre-trained transformer models and multitask learning. Our results indicate that multitask learning achieves performance superior to the more common single task learning approach in all three tasks. We submit our best systems to GermEval-2021 under the team name WLVRIT. 1 Introduction The popularity and accessibility associated with social media have greatly promoted user-generated content. At the same time, social media sites have increasingly become more prone to offensive content (Hada et al., 2021; Zhu and Bhat, 2021; Bucur et al., 2021). As such, identifying the toxic language in social media is a topic that has gained, and continues to gain traction. Research surrounding the problem of offensive content has centered around the application of computational models that can identify various forms of negative content such as hate speech (Malmasi and Zampieri, 2018; Nozza, 2021), abuse (Corazza et al., 2020), aggression (Kumar et al., 2018, 2020), and cyber-bullying (Rosa et al., 2019; Cheng et al., 2021; Salawu et al., 2021). GermEval-2021 (Risch et al., 2021) focuses on identifying mult"
2021.germeval-1.5,2021.findings-acl.315,1,0.704161,"e relatedness of the three tasks, we approached the problem using large pre-trained transformer models and multitask learning. Our results indicate that multitask learning achieves performance superior to the more common single task learning approach in all three tasks. We submit our best systems to GermEval-2021 under the team name WLVRIT. 1 Introduction The popularity and accessibility associated with social media have greatly promoted user-generated content. At the same time, social media sites have increasingly become more prone to offensive content (Hada et al., 2021; Zhu and Bhat, 2021; Bucur et al., 2021). As such, identifying the toxic language in social media is a topic that has gained, and continues to gain traction. Research surrounding the problem of offensive content has centered around the application of computational models that can identify various forms of negative content such as hate speech (Malmasi and Zampieri, 2018; Nozza, 2021), abuse (Corazza et al., 2020), aggression (Kumar et al., 2018, 2020), and cyber-bullying (Rosa et al., 2019; Cheng et al., 2021; Salawu et al., 2021). GermEval-2021 (Risch et al., 2021) focuses on identifying multiple types of comments in social media. T"
2021.germeval-1.5,R19-1056,1,0.840564,"d Zampieri, 2018), to various deep learning models (de Gibert et al., 2018). SemEval-2019 Task 5 (HatEval) (Basile et al., 2019) presented the challenge of detecting the presence of hate speech and identifying further features in hateful contents, which included two sub-tasks. For subtask A, which was the hate speech (HS) category, the best performance was achieved by training a support vector machine (SVM) model with a radial basis function (RBF) kernel. Several other high scoring teams used a convolutional neural network (CNN) which was traditionally the most popular approach to this topic (Hettiarachchi and Ranasinghe, 2019). For TRAC1 (Kumar et al., 2018), the challenge was to develop a classifier that could discriminate between three levels of aggression in social media. The results showed that with careful consideration, classifiers like SVM and even random forest could perform at par with deep neural networks. However, in the end, more than half of the top 15 systems were trained on neural networks which demonstrates the approach’s effectiveness. The introduction of BERT (Devlin et al., 2019) spurred the use of pre-trained transformer models for classifying offensive speech (Ranasinghe and Zampieri, 2021b). A"
2021.germeval-1.5,2020.lrec-1.758,0,0.0222544,"he research related to this topic is centered on English data due to the availability of annotated datasets (Zampieri et al., 2019a; Rosenthal et al., 2021). Notwithstanding this, offensive language datasets are being annotated in other languages. Researchers have examined offensive content across multiple social media platforms and have both annotated and utilized data from different languages such as Greek (Pitenis et al., 2020), Marathi (Gaikwad et al., 2021), Italian (Chiril et al., 2019), Portuguese (Fortuna et al., 2019; Vargas et al., 2021), Arabic (Mubarak et al., 2021), Turkish (C¸o¨ ltekin, 2020), and multiple languages of India (Ranasinghe and Zampieri, 2021a). Past approaches to tackling the problem of offensive content on social media have relied on using a variety of computational models ranging from traditional machine learning classifiers such as Logistic Regression and SVMs (Malmasi and Zampieri, 2018), to various deep learning models (de Gibert et al., 2018). SemEval-2019 Task 5 (HatEval) (Basile et al., 2019) presented the challenge of detecting the presence of hate speech and identifying further features in hateful contents, which included two sub-tasks. For subtask A, which"
2021.germeval-1.5,2020.semeval-1.16,1,0.799709,"Missing"
2021.germeval-1.5,2020.coling-main.598,0,0.0795337,"Missing"
2021.germeval-1.5,2020.wnut-1.49,1,0.856175,"Missing"
2021.germeval-1.5,2021.acl-long.168,0,0.0251014,"ame time, social media sites have increasingly become more prone to offensive content (Hada et al., 2021; Zhu and Bhat, 2021; Bucur et al., 2021). As such, identifying the toxic language in social media is a topic that has gained, and continues to gain traction. Research surrounding the problem of offensive content has centered around the application of computational models that can identify various forms of negative content such as hate speech (Malmasi and Zampieri, 2018; Nozza, 2021), abuse (Corazza et al., 2020), aggression (Kumar et al., 2018, 2020), and cyber-bullying (Rosa et al., 2019; Cheng et al., 2021; Salawu et al., 2021). GermEval-2021 (Risch et al., 2021) focuses on identifying multiple types of comments in social media. This year’s shared task is divided into three distinct classifications of comments: i) Toxic, ii) Engaging, and iii) Fact-Claiming. Like previous GermEval shared tasks (Struß et al., 2019), the detection of toxic content remains an integral 32 Proceedings of the GermEval 2021 Shared Task on the Identification of Toxic, Engaging, and Fact-Claiming Comments co-located with KONVENS 2 Related Work tification. The use of pre-trained BERT models, as well as BERT-based models,"
2021.germeval-1.5,2021.vardial-1.14,1,0.787792,"German television broadcaster. The training dataset has a total of 3,244 instances and comprises 1,074 instances without any toxic, engaging or fact claiming content. In Table 1, we present four different Facebook user comments along with their annotation. Toxic Engaging Fact-Claiming Training 0 1 0 1 0 1 0 1 All 0 0 1 1 1 0 0 1 0 0 0 0 1 1 1 1 1074 739 239 89 403 160 406 134 3244 Table 2: GermEval 2021 - Training Set User Comment Distribution 4 Methods Considering the success that neural transformers have demonstrated across various natural language processing tasks (Uyangodage et al., 2021; Jauhiainen et al., 2021; Hettiarachchi and Ranasinghe, 2020a) including offensive language identification (Ranasinghe and Zampieri, 2020, 2021b; Dai et al., 2020) we used transformers to tackle this task too. 33 Proceedings of the GermEval 2021 Shared Task on the Identification of Toxic, Engaging, and Fact-Claiming Comments co-located with KONVENS Comment Sub1 Sub2 Sub3 ”Die AfD sind genau so neoliberal und kapitalistische Zerst¨orer unserer Heimat, wie die CDU, CSU, FDP, SPD und Gr¨une auch.” ”Sarazin ist ein rechtsradikaler Mensch. Ein Menschenhasser. Sie kennen nur Zerst¨orung. Die Geschichte hat es gezeigt.” ”@U"
2021.germeval-1.5,2019.jeptalnrecital-court.21,0,0.0376168,"language in online discussions is an extensive topic that has become popular over the past several years. The majority of the research related to this topic is centered on English data due to the availability of annotated datasets (Zampieri et al., 2019a; Rosenthal et al., 2021). Notwithstanding this, offensive language datasets are being annotated in other languages. Researchers have examined offensive content across multiple social media platforms and have both annotated and utilized data from different languages such as Greek (Pitenis et al., 2020), Marathi (Gaikwad et al., 2021), Italian (Chiril et al., 2019), Portuguese (Fortuna et al., 2019; Vargas et al., 2021), Arabic (Mubarak et al., 2021), Turkish (C¸o¨ ltekin, 2020), and multiple languages of India (Ranasinghe and Zampieri, 2021a). Past approaches to tackling the problem of offensive content on social media have relied on using a variety of computational models ranging from traditional machine learning classifiers such as Logistic Regression and SVMs (Malmasi and Zampieri, 2018), to various deep learning models (de Gibert et al., 2018). SemEval-2019 Task 5 (HatEval) (Basile et al., 2019) presented the challenge of detecting the presence of"
2021.germeval-1.5,W17-4218,0,0.0608693,"Missing"
2021.germeval-1.5,P19-4007,0,0.0570258,"Missing"
2021.germeval-1.5,W18-4401,1,0.925992,"ocial media have greatly promoted user-generated content. At the same time, social media sites have increasingly become more prone to offensive content (Hada et al., 2021; Zhu and Bhat, 2021; Bucur et al., 2021). As such, identifying the toxic language in social media is a topic that has gained, and continues to gain traction. Research surrounding the problem of offensive content has centered around the application of computational models that can identify various forms of negative content such as hate speech (Malmasi and Zampieri, 2018; Nozza, 2021), abuse (Corazza et al., 2020), aggression (Kumar et al., 2018, 2020), and cyber-bullying (Rosa et al., 2019; Cheng et al., 2021; Salawu et al., 2021). GermEval-2021 (Risch et al., 2021) focuses on identifying multiple types of comments in social media. This year’s shared task is divided into three distinct classifications of comments: i) Toxic, ii) Engaging, and iii) Fact-Claiming. Like previous GermEval shared tasks (Struß et al., 2019), the detection of toxic content remains an integral 32 Proceedings of the GermEval 2021 Shared Task on the Identification of Toxic, Engaging, and Fact-Claiming Comments co-located with KONVENS 2 Related Work tification."
2021.germeval-1.5,2020.findings-emnlp.84,0,0.0279381,"and accessibility associated with social media have greatly promoted user-generated content. At the same time, social media sites have increasingly become more prone to offensive content (Hada et al., 2021; Zhu and Bhat, 2021; Bucur et al., 2021). As such, identifying the toxic language in social media is a topic that has gained, and continues to gain traction. Research surrounding the problem of offensive content has centered around the application of computational models that can identify various forms of negative content such as hate speech (Malmasi and Zampieri, 2018; Nozza, 2021), abuse (Corazza et al., 2020), aggression (Kumar et al., 2018, 2020), and cyber-bullying (Rosa et al., 2019; Cheng et al., 2021; Salawu et al., 2021). GermEval-2021 (Risch et al., 2021) focuses on identifying multiple types of comments in social media. This year’s shared task is divided into three distinct classifications of comments: i) Toxic, ii) Engaging, and iii) Fact-Claiming. Like previous GermEval shared tasks (Struß et al., 2019), the detection of toxic content remains an integral 32 Proceedings of the GermEval 2021 Shared Task on the Identification of Toxic, Engaging, and Fact-Claiming Comments co-located with KO"
2021.germeval-1.5,2020.trac-1.1,1,0.837923,"Missing"
2021.germeval-1.5,2020.semeval-1.272,0,0.0213725,"act claiming content. In Table 1, we present four different Facebook user comments along with their annotation. Toxic Engaging Fact-Claiming Training 0 1 0 1 0 1 0 1 All 0 0 1 1 1 0 0 1 0 0 0 0 1 1 1 1 1074 739 239 89 403 160 406 134 3244 Table 2: GermEval 2021 - Training Set User Comment Distribution 4 Methods Considering the success that neural transformers have demonstrated across various natural language processing tasks (Uyangodage et al., 2021; Jauhiainen et al., 2021; Hettiarachchi and Ranasinghe, 2020a) including offensive language identification (Ranasinghe and Zampieri, 2020, 2021b; Dai et al., 2020) we used transformers to tackle this task too. 33 Proceedings of the GermEval 2021 Shared Task on the Identification of Toxic, Engaging, and Fact-Claiming Comments co-located with KONVENS Comment Sub1 Sub2 Sub3 ”Die AfD sind genau so neoliberal und kapitalistische Zerst¨orer unserer Heimat, wie die CDU, CSU, FDP, SPD und Gr¨une auch.” ”Sarazin ist ein rechtsradikaler Mensch. Ein Menschenhasser. Sie kennen nur Zerst¨orung. Die Geschichte hat es gezeigt.” ”@USER, du hast das Thema im Kern nicht verstanden” ”Ich frage dich, verlassen Menschen gerne ihre Heimat?” 1 0 0 1 0 1 0 0 0 0 1 0 Table 1: A"
2021.germeval-1.5,S19-2011,0,0.0168993,"aders to engage in a discussion. In a similar light, identifying fact-claiming comments is equally important as platforms need to consistently review and verify user-generated content to uphold their responsibility as information distributors (Mihaylova et al., 2018; Shaar et al., 2020). We pose that multitask learning (MTL) is a suitable approach for this year’s GermEval as it enables what is learned from each task to aid in the learning of other tasks. The current state-of-the-art approach for offensive language identification is neural transformers modeled using single task learning (SLT) (Liu et al., 2019; Ranasinghe and Zampieri, 2020). It is well-known that training large neural transformer models often result in long processing times. As GermEval-2021 features three related tasks, from a performance standpoint, we pose that training a model jointly on three tasks is likely to be computationally more efficient than training three models in isolation. Moreover, as GermEval-2021 provides a single dataset for the three tasks, MTL can also be used to help improving performance across tasks. As such, we introduce multitask learning whereby one model can predict all three tasks as an alternative a"
2021.germeval-1.5,N19-1423,0,0.182251,"d a convolutional neural network (CNN) which was traditionally the most popular approach to this topic (Hettiarachchi and Ranasinghe, 2019). For TRAC1 (Kumar et al., 2018), the challenge was to develop a classifier that could discriminate between three levels of aggression in social media. The results showed that with careful consideration, classifiers like SVM and even random forest could perform at par with deep neural networks. However, in the end, more than half of the top 15 systems were trained on neural networks which demonstrates the approach’s effectiveness. The introduction of BERT (Devlin et al., 2019) spurred the use of pre-trained transformer models for classifying offensive speech (Ranasinghe and Zampieri, 2021b). As a result, neural transformer based language models have increasingly become more popular in offensive language iden3 Data In the GermEval-2021 dataset, the focus has been extended beyond the identification of offensive comments to include two additional classes: engaging comments that can motivate readers to participate in conversations, and fact-claiming comments. The dataset for this iteration of GermEval comprises over 3,000 Facebook user comments that have been extracted"
2021.germeval-1.5,2021.woah-1.16,0,0.0341799,"ia sites have increasingly become more prone to offensive content (Hada et al., 2021; Zhu and Bhat, 2021; Bucur et al., 2021). As such, identifying the toxic language in social media is a topic that has gained, and continues to gain traction. Research surrounding the problem of offensive content has centered around the application of computational models that can identify various forms of negative content such as hate speech (Malmasi and Zampieri, 2018; Nozza, 2021), abuse (Corazza et al., 2020), aggression (Kumar et al., 2018, 2020), and cyber-bullying (Rosa et al., 2019; Cheng et al., 2021; Salawu et al., 2021). GermEval-2021 (Risch et al., 2021) focuses on identifying multiple types of comments in social media. This year’s shared task is divided into three distinct classifications of comments: i) Toxic, ii) Engaging, and iii) Fact-Claiming. Like previous GermEval shared tasks (Struß et al., 2019), the detection of toxic content remains an integral 32 Proceedings of the GermEval 2021 Shared Task on the Identification of Toxic, Engaging, and Fact-Claiming Comments co-located with KONVENS 2 Related Work tification. The use of pre-trained BERT models, as well as BERT-based models, was shown to be able"
2021.germeval-1.5,2021.wanlp-1.13,0,0.025516,"past several years. The majority of the research related to this topic is centered on English data due to the availability of annotated datasets (Zampieri et al., 2019a; Rosenthal et al., 2021). Notwithstanding this, offensive language datasets are being annotated in other languages. Researchers have examined offensive content across multiple social media platforms and have both annotated and utilized data from different languages such as Greek (Pitenis et al., 2020), Marathi (Gaikwad et al., 2021), Italian (Chiril et al., 2019), Portuguese (Fortuna et al., 2019; Vargas et al., 2021), Arabic (Mubarak et al., 2021), Turkish (C¸o¨ ltekin, 2020), and multiple languages of India (Ranasinghe and Zampieri, 2021a). Past approaches to tackling the problem of offensive content on social media have relied on using a variety of computational models ranging from traditional machine learning classifiers such as Logistic Regression and SVMs (Malmasi and Zampieri, 2018), to various deep learning models (de Gibert et al., 2018). SemEval-2019 Task 5 (HatEval) (Basile et al., 2019) presented the challenge of detecting the presence of hate speech and identifying further features in hateful contents, which included two su"
2021.germeval-1.5,W17-0802,0,0.041729,"Missing"
2021.germeval-1.5,2021.acl-short.114,0,0.0373743,"ction The popularity and accessibility associated with social media have greatly promoted user-generated content. At the same time, social media sites have increasingly become more prone to offensive content (Hada et al., 2021; Zhu and Bhat, 2021; Bucur et al., 2021). As such, identifying the toxic language in social media is a topic that has gained, and continues to gain traction. Research surrounding the problem of offensive content has centered around the application of computational models that can identify various forms of negative content such as hate speech (Malmasi and Zampieri, 2018; Nozza, 2021), abuse (Corazza et al., 2020), aggression (Kumar et al., 2018, 2020), and cyber-bullying (Rosa et al., 2019; Cheng et al., 2021; Salawu et al., 2021). GermEval-2021 (Risch et al., 2021) focuses on identifying multiple types of comments in social media. This year’s shared task is divided into three distinct classifications of comments: i) Toxic, ii) Engaging, and iii) Fact-Claiming. Like previous GermEval shared tasks (Struß et al., 2019), the detection of toxic content remains an integral 32 Proceedings of the GermEval 2021 Shared Task on the Identification of Toxic, Engaging, and Fact-Claimi"
2021.germeval-1.5,2020.lrec-1.629,1,0.825985,"ghe and Zampieri, 2020, 2021c). The identification of offensive language in online discussions is an extensive topic that has become popular over the past several years. The majority of the research related to this topic is centered on English data due to the availability of annotated datasets (Zampieri et al., 2019a; Rosenthal et al., 2021). Notwithstanding this, offensive language datasets are being annotated in other languages. Researchers have examined offensive content across multiple social media platforms and have both annotated and utilized data from different languages such as Greek (Pitenis et al., 2020), Marathi (Gaikwad et al., 2021), Italian (Chiril et al., 2019), Portuguese (Fortuna et al., 2019; Vargas et al., 2021), Arabic (Mubarak et al., 2021), Turkish (C¸o¨ ltekin, 2020), and multiple languages of India (Ranasinghe and Zampieri, 2021a). Past approaches to tackling the problem of offensive content on social media have relied on using a variety of computational models ranging from traditional machine learning classifiers such as Logistic Regression and SVMs (Malmasi and Zampieri, 2018), to various deep learning models (de Gibert et al., 2018). SemEval-2019 Task 5 (HatEval) (Basile et a"
2021.germeval-1.5,2020.emnlp-main.470,1,0.908489,"a discussion. In a similar light, identifying fact-claiming comments is equally important as platforms need to consistently review and verify user-generated content to uphold their responsibility as information distributors (Mihaylova et al., 2018; Shaar et al., 2020). We pose that multitask learning (MTL) is a suitable approach for this year’s GermEval as it enables what is learned from each task to aid in the learning of other tasks. The current state-of-the-art approach for offensive language identification is neural transformers modeled using single task learning (SLT) (Liu et al., 2019; Ranasinghe and Zampieri, 2020). It is well-known that training large neural transformer models often result in long processing times. As GermEval-2021 features three related tasks, from a performance standpoint, we pose that training a model jointly on three tasks is likely to be computationally more efficient than training three models in isolation. Moreover, as GermEval-2021 provides a single dataset for the three tasks, MTL can also be used to help improving performance across tasks. As such, we introduce multitask learning whereby one model can predict all three tasks as an alternative approach. In this paper, we prese"
2021.germeval-1.5,2021.naacl-demos.17,1,0.891207,"English data due to the availability of annotated datasets (Zampieri et al., 2019a; Rosenthal et al., 2021). Notwithstanding this, offensive language datasets are being annotated in other languages. Researchers have examined offensive content across multiple social media platforms and have both annotated and utilized data from different languages such as Greek (Pitenis et al., 2020), Marathi (Gaikwad et al., 2021), Italian (Chiril et al., 2019), Portuguese (Fortuna et al., 2019; Vargas et al., 2021), Arabic (Mubarak et al., 2021), Turkish (C¸o¨ ltekin, 2020), and multiple languages of India (Ranasinghe and Zampieri, 2021a). Past approaches to tackling the problem of offensive content on social media have relied on using a variety of computational models ranging from traditional machine learning classifiers such as Logistic Regression and SVMs (Malmasi and Zampieri, 2018), to various deep learning models (de Gibert et al., 2018). SemEval-2019 Task 5 (HatEval) (Basile et al., 2019) presented the challenge of detecting the presence of hate speech and identifying further features in hateful contents, which included two sub-tasks. For subtask A, which was the hate speech (HS) category, the best performance was ach"
2021.germeval-1.5,N19-1144,1,0.791519,"omments in social media. This year’s shared task is divided into three distinct classifications of comments: i) Toxic, ii) Engaging, and iii) Fact-Claiming. Like previous GermEval shared tasks (Struß et al., 2019), the detection of toxic content remains an integral 32 Proceedings of the GermEval 2021 Shared Task on the Identification of Toxic, Engaging, and Fact-Claiming Comments co-located with KONVENS 2 Related Work tification. The use of pre-trained BERT models, as well as BERT-based models, was shown to be able to achieve competitive performance in popular competitions such as OffensEval (Zampieri et al., 2019b, 2020). Language-specific and multilingual models have also been introduced to assist NLP research in various languages such as GBERT for German (Chan et al., 2020), AraBERT for Arabic (Antoun et al., 2020), and the multilingual XLM-R (Conneau et al., 2019) that has been been applied to offensive language identification (Ranasinghe and Zampieri, 2020, 2021c). The identification of offensive language in online discussions is an extensive topic that has become popular over the past several years. The majority of the research related to this topic is centered on English data due to the availabi"
2021.germeval-1.5,S19-2010,1,0.780025,"omments in social media. This year’s shared task is divided into three distinct classifications of comments: i) Toxic, ii) Engaging, and iii) Fact-Claiming. Like previous GermEval shared tasks (Struß et al., 2019), the detection of toxic content remains an integral 32 Proceedings of the GermEval 2021 Shared Task on the Identification of Toxic, Engaging, and Fact-Claiming Comments co-located with KONVENS 2 Related Work tification. The use of pre-trained BERT models, as well as BERT-based models, was shown to be able to achieve competitive performance in popular competitions such as OffensEval (Zampieri et al., 2019b, 2020). Language-specific and multilingual models have also been introduced to assist NLP research in various languages such as GBERT for German (Chan et al., 2020), AraBERT for Arabic (Antoun et al., 2020), and the multilingual XLM-R (Conneau et al., 2019) that has been been applied to offensive language identification (Ranasinghe and Zampieri, 2020, 2021c). The identification of offensive language in online discussions is an extensive topic that has become popular over the past several years. The majority of the research related to this topic is centered on English data due to the availabi"
2021.germeval-1.5,2021.germeval-1.0,0,0.113251,"e prone to offensive content (Hada et al., 2021; Zhu and Bhat, 2021; Bucur et al., 2021). As such, identifying the toxic language in social media is a topic that has gained, and continues to gain traction. Research surrounding the problem of offensive content has centered around the application of computational models that can identify various forms of negative content such as hate speech (Malmasi and Zampieri, 2018; Nozza, 2021), abuse (Corazza et al., 2020), aggression (Kumar et al., 2018, 2020), and cyber-bullying (Rosa et al., 2019; Cheng et al., 2021; Salawu et al., 2021). GermEval-2021 (Risch et al., 2021) focuses on identifying multiple types of comments in social media. This year’s shared task is divided into three distinct classifications of comments: i) Toxic, ii) Engaging, and iii) Fact-Claiming. Like previous GermEval shared tasks (Struß et al., 2019), the detection of toxic content remains an integral 32 Proceedings of the GermEval 2021 Shared Task on the Identification of Toxic, Engaging, and Fact-Claiming Comments co-located with KONVENS 2 Related Work tification. The use of pre-trained BERT models, as well as BERT-based models, was shown to be able to achieve competitive performance i"
2021.germeval-1.5,2021.findings-acl.12,0,0.0309671,"rman. Considering the relatedness of the three tasks, we approached the problem using large pre-trained transformer models and multitask learning. Our results indicate that multitask learning achieves performance superior to the more common single task learning approach in all three tasks. We submit our best systems to GermEval-2021 under the team name WLVRIT. 1 Introduction The popularity and accessibility associated with social media have greatly promoted user-generated content. At the same time, social media sites have increasingly become more prone to offensive content (Hada et al., 2021; Zhu and Bhat, 2021; Bucur et al., 2021). As such, identifying the toxic language in social media is a topic that has gained, and continues to gain traction. Research surrounding the problem of offensive content has centered around the application of computational models that can identify various forms of negative content such as hate speech (Malmasi and Zampieri, 2018; Nozza, 2021), abuse (Corazza et al., 2020), aggression (Kumar et al., 2018, 2020), and cyber-bullying (Rosa et al., 2019; Cheng et al., 2021; Salawu et al., 2021). GermEval-2021 (Risch et al., 2021) focuses on identifying multiple types of commen"
2021.germeval-1.5,2021.findings-acl.80,1,0.818319,"have also been introduced to assist NLP research in various languages such as GBERT for German (Chan et al., 2020), AraBERT for Arabic (Antoun et al., 2020), and the multilingual XLM-R (Conneau et al., 2019) that has been been applied to offensive language identification (Ranasinghe and Zampieri, 2020, 2021c). The identification of offensive language in online discussions is an extensive topic that has become popular over the past several years. The majority of the research related to this topic is centered on English data due to the availability of annotated datasets (Zampieri et al., 2019a; Rosenthal et al., 2021). Notwithstanding this, offensive language datasets are being annotated in other languages. Researchers have examined offensive content across multiple social media platforms and have both annotated and utilized data from different languages such as Greek (Pitenis et al., 2020), Marathi (Gaikwad et al., 2021), Italian (Chiril et al., 2019), Portuguese (Fortuna et al., 2019; Vargas et al., 2021), Arabic (Mubarak et al., 2021), Turkish (C¸o¨ ltekin, 2020), and multiple languages of India (Ranasinghe and Zampieri, 2021a). Past approaches to tackling the problem of offensive content on social medi"
2021.naacl-demos.17,2021.ccl-1.108,0,0.0963384,"Missing"
2021.naacl-demos.17,W18-4423,0,0.107314,"Missing"
2021.naacl-demos.17,malmasi-zampieri-2017-detecting,1,0.906623,"Missing"
2021.naacl-demos.17,2020.semeval-1.205,0,0.0441034,"Missing"
2021.naacl-demos.17,2021.semeval-1.6,0,0.0464415,"ge are widespread in social media posts motivating a number of studies on automatically detecting the various types of offensive content (e.g. aggression (Kumar et al., 2018, 2020), cyber-bullying (Rosa et al., 2019), hate speech (Malmasi and Zampieri, 2018), etc.). Most previous work has focused on classifying full instances (e.g. posts, comments, documents) (e.g. offensive vs. not offensive) while the identification of the particular spans that make a text offensive has been mostly neglected. Identifying offensive spans in texts is the goal of the SemEval-2021 Task 5: Toxic Spans Detection (Pavlopoulos et al., 2021). The organisers of this task argue that highlighting toxic spans in texts helps assisting human moderators (e.g. news portals moderators) and that this can be a first step in semi-automated content moderation. Finally, as we demonstrate in this paper, addressing offensive spans in texts will make the output of offensive language detection systems more interpretable thus allowing a more detailed linguistics analysis of predictions and improving the quality of such systems. With these important points in mind, we developed MUDES: Multilingual Detection of Offen3. We release a Python Application"
2021.naacl-demos.17,N18-1202,0,0.0433779,"models and performing inference in the code level. 4. For general users and non-programmers, we release a user-friendly web-based User Interface (UI), which provides the functionality to input a text in multiple languages and to identify the offensive span in that text. 2 Related Work Early approaches to offensive language identification relied on traditional machine learning classifiers (Dadvar et al., 2013) and later on neural networks combined with word embeddings (Majumder et al., 2018; Hettiarachchi and Ranasinghe, 2019). Transformer-based models like BERT (Devlin et al., 2019) and ELMO (Peters et al., 2018) have been recently applied to offensive language detection achieving competitive scores (Wang et al., 2020; Ranasinghe and Hettiarachchi, 2020) in recent SemEval competitions such as HatEval (Basile et al., 2019) OffensEval (Zampieri et al., 2020). In terms of languages, the majority of studies on WARNING: This paper contains text excerpts and words that are offensive in nature. this topic deal with English (Malmasi and Zampieri, 144 Proceedings of NAACL-HLT 2021: Demonstrations, pages 144–152 June 6–11, 2021. ©2021 Association for Computational Linguistics Post Offensive Spans Stupid hatcher"
2021.naacl-demos.17,2020.lrec-1.629,1,0.927732,"30, 31, 32, 33, 34] [] [12, 13, 14, 15, 16] Table 1: Four comments from the dataset, with their annotations. The offensive words are displayed in red and the spans are indicated by the character position in the instance. 2017; Yao et al., 2019; Ridenhour et al., 2020; Rosenthal et al., 2020) due to the the wide availability of language resources such as corpora and pre-trained models. In recent years, several studies have been published on identifying offensive content in other languages such as Arabic (Mubarak et al., 2020), Dutch (Tulkens et al., 2016), French (Chiril et al., 2019), Greek (Pitenis et al., 2020), Italian (Poletto et al., 2017), Portuguese (Fortuna et al., 2019), and Turkish (Çöltekin, 2020). Most of these studies have created new datasets and resources for these languages opening avenues for multilingual models as those presented in Ranasinghe and Zampieri (2020). However, all studies presented in this section focused on classifying full texts, as discussed in the Introduction. MUDES’ objective is to fill this gap and perform span level offensive language identification. 3 Data The general idea is to learn a robust model from this dataset and generalize to other English datasets whic"
2021.naacl-demos.17,2020.semeval-1.251,1,0.791934,"Missing"
2021.naacl-demos.17,2021.semeval-1.111,1,0.820509,"tes set cardinality. F1t (Ai , G) = 2 · P t (Ai , G) · Rt (Ai , G) P t (Ai , G) + Rt (Ai , G) 5.2 (1) S t ∩S t S t ∩S t Ai G Ai G P t (Ai , G) = Rt (Ai , G) = |SGt | t SA i We present the results along with the baseline provided by the organisers in Table 2. The baseline is implemented using a spaCy NER pipeline. The spaCy NER system contains a word embedding strategy using sub word features and Bloom embedding (Serrà and Karatzoglou, 2017), and a deep convolution neural network with residual connections. Additionally, we compare our results to a lexicon-based word match approach mentioned in Ranasinghe et al. (2021) where the lexicon is based on profanity words from online resources1,2 . Model Name en-large en-base multilingual-large multilingual-base spaCy baseline Lexicon word match (Ranasinghe et al., 2021) Base Model roberta-large xlnet-base-cased XLM-R-large XLM-R-base NA F1 score 0.6886 0.6734 0.6338 0.6160 0.5976 NA 0.3378 For the English off-domain and multilingual datasets we followed a different evaluation process. We used a pre-trained MUDES’ model trained on TSDTrain to predict the offensive spans for all texts in the test sets of two non-English datasets (Danish, and Greek) and English off-d"
2021.naacl-demos.17,2020.emnlp-main.470,1,0.822496,"2020; Rosenthal et al., 2020) due to the the wide availability of language resources such as corpora and pre-trained models. In recent years, several studies have been published on identifying offensive content in other languages such as Arabic (Mubarak et al., 2020), Dutch (Tulkens et al., 2016), French (Chiril et al., 2019), Greek (Pitenis et al., 2020), Italian (Poletto et al., 2017), Portuguese (Fortuna et al., 2019), and Turkish (Çöltekin, 2020). Most of these studies have created new datasets and resources for these languages opening avenues for multilingual models as those presented in Ranasinghe and Zampieri (2020). However, all studies presented in this section focused on classifying full texts, as discussed in the Introduction. MUDES’ objective is to fill this gap and perform span level offensive language identification. 3 Data The general idea is to learn a robust model from this dataset and generalize to other English datasets which do not contain span annotation. Another goal is to investigate the feasibility of annotation projection to other languages. Other Datasets In order to evaluate our framework in different domains and languages we used three publicly available offensive language identifica"
2021.naacl-demos.17,2020.semeval-1.189,0,0.0218905,"-friendly web-based User Interface (UI), which provides the functionality to input a text in multiple languages and to identify the offensive span in that text. 2 Related Work Early approaches to offensive language identification relied on traditional machine learning classifiers (Dadvar et al., 2013) and later on neural networks combined with word embeddings (Majumder et al., 2018; Hettiarachchi and Ranasinghe, 2019). Transformer-based models like BERT (Devlin et al., 2019) and ELMO (Peters et al., 2018) have been recently applied to offensive language detection achieving competitive scores (Wang et al., 2020; Ranasinghe and Hettiarachchi, 2020) in recent SemEval competitions such as HatEval (Basile et al., 2019) OffensEval (Zampieri et al., 2020). In terms of languages, the majority of studies on WARNING: This paper contains text excerpts and words that are offensive in nature. this topic deal with English (Malmasi and Zampieri, 144 Proceedings of NAACL-HLT 2021: Demonstrations, pages 144–152 June 6–11, 2021. ©2021 Association for Computational Linguistics Post Offensive Spans Stupid hatcheries have completely fucked everything Victimitis: You are such an asshole. So is his mother. They are silve"
2021.naacl-demos.17,2020.semeval-1.213,0,0.0752671,"Missing"
2021.naacl-demos.17,N19-1144,1,0.953979,"ion. MUDES’ objective is to fill this gap and perform span level offensive language identification. 3 Data The general idea is to learn a robust model from this dataset and generalize to other English datasets which do not contain span annotation. Another goal is to investigate the feasibility of annotation projection to other languages. Other Datasets In order to evaluate our framework in different domains and languages we used three publicly available offensive language identification datasets. As an off-domain English dataset, we choose the Offensive Language Identification Dataset (OLID) (Zampieri et al., 2019a), used in OffensEval 2019 (SemEval-2019 Task 6) (Zampieri et al., 2019b), containing over 14,000 posts from Twitter. To evaluate our framework in different languages, we selected a Danish (Sigurbergsson and Derczynski, 2020) and a Greek (Pitenis et al., 2020) dataset. These two datasets have been provided by the organisers of OffensEval 2020 (SemEval-2020 Task 12) (Zampieri et al., 2020) and were annotated using OLID’s annotation guidelines. The Danish dataset contains over 3,000 posts from Facebook and Reddit while the Greek dataset contains over 10,000 Twitter posts, allowing us to evaluat"
2021.naacl-demos.17,S19-2010,1,0.945444,"ion. MUDES’ objective is to fill this gap and perform span level offensive language identification. 3 Data The general idea is to learn a robust model from this dataset and generalize to other English datasets which do not contain span annotation. Another goal is to investigate the feasibility of annotation projection to other languages. Other Datasets In order to evaluate our framework in different domains and languages we used three publicly available offensive language identification datasets. As an off-domain English dataset, we choose the Offensive Language Identification Dataset (OLID) (Zampieri et al., 2019a), used in OffensEval 2019 (SemEval-2019 Task 6) (Zampieri et al., 2019b), containing over 14,000 posts from Twitter. To evaluate our framework in different languages, we selected a Danish (Sigurbergsson and Derczynski, 2020) and a Greek (Pitenis et al., 2020) dataset. These two datasets have been provided by the organisers of OffensEval 2020 (SemEval-2020 Task 12) (Zampieri et al., 2020) and were annotated using OLID’s annotation guidelines. The Danish dataset contains over 3,000 posts from Facebook and Reddit while the Greek dataset contains over 10,000 Twitter posts, allowing us to evaluat"
2021.naacl-demos.17,2020.lrec-1.430,0,0.295042,"ich do not contain span annotation. Another goal is to investigate the feasibility of annotation projection to other languages. Other Datasets In order to evaluate our framework in different domains and languages we used three publicly available offensive language identification datasets. As an off-domain English dataset, we choose the Offensive Language Identification Dataset (OLID) (Zampieri et al., 2019a), used in OffensEval 2019 (SemEval-2019 Task 6) (Zampieri et al., 2019b), containing over 14,000 posts from Twitter. To evaluate our framework in different languages, we selected a Danish (Sigurbergsson and Derczynski, 2020) and a Greek (Pitenis et al., 2020) dataset. These two datasets have been provided by the organisers of OffensEval 2020 (SemEval-2020 Task 12) (Zampieri et al., 2020) and were annotated using OLID’s annotation guidelines. The Danish dataset contains over 3,000 posts from Facebook and Reddit while the Greek dataset contains over 10,000 Twitter posts, allowing us to evaluate our dataset in an off-domain, multilingual setting. As these three datasets have been annotated at the instance level, we followed an evaluation process explained in Section 5. The main dataset used to train the machine lear"
2021.semeval-1.1,P19-1267,0,0.017165,"rank systems based on their score on Pearson’s correlation, giving a final ranking over all systems, it should be noted that there is very little variation in score between the top systems and all other systems. For Task 1 there are 0.0182 points of Pearson’s Correlation separating the systems at ranks 1 and 10. For Task 2 a similar difference of 0.021 points of Pearson’s Correlation separates the systems at ranks 1 and 10. These are small differences and it may be the case that had we selected a different random split in our dataset this would have led to a different ordering in our results (Gorman and Bedrick, 2019; Søgaard et al., 2020). This is not unique to our task and is something for the SemEval community to ruminate on as the focus of NLP tasks continues to move towards better evaluation rather than better systems. References Ahmed AbuRa’ed and Horacio Saggion. 2018. LaSTUS/TALN at Complex Word Identification (CWI) 2018 Shared Task. In Proceedings of the 13th Workshop on Innovative Use of NLP for Building Educational Applications, New Orleans, United States. Association for Computational Linguistics. David Alfter and Ildik´o Pil´an. 2018. SB@GU at the Complex Word Identification 2018 Shared Task."
2021.semeval-1.1,W18-0540,0,0.136791,"Missing"
2021.semeval-1.111,2021.vardial-1.14,1,0.713814,"Missing"
2021.semeval-1.111,2020.tacl-1.5,0,0.0558069,"Missing"
2021.semeval-1.111,W18-4401,1,0.91199,"r model achieves an 0.68 F1-Score. Furthermore, we develop an open-source framework for multilingual detection of offensive spans, i.e., MUDES, based on neural transformers that detect toxic spans in texts. 1 Introduction The widespread adoption and use of social media has led to a drastic increase in the generation of abusive and profane content on the web. To counter this deluge of negative content, social media companies and government institutions have turned to developing and applying computational models that can identify the various forms of offensive content online such as aggression (Kumar et al., 2018, 2020), cyber-bullying (Rosa et al., 2019), and hate speech (Ridenhour et al., 2020). Prior work has either designed methods for identifying conversations that are likely to go awry (Zhang WARNING: This paper contains text excerpts and words that are offensive in nature. With respect to identifying offensive language in conversations, comments, and posts, noticeable progress has been made with a variety of large, annotated datasets made available in recent years (Pitenis et al., 2020; Rosenthal et al., 2020). The identification of the particular text spans that make a post offensive, however,"
2021.semeval-1.111,2020.trac-1.1,1,0.763086,"Missing"
2021.semeval-1.111,W17-3013,0,0.0516069,"Missing"
2021.semeval-1.111,R19-1056,1,0.723429,"archers have noted that n-gram based features are very useful when building reliable, automated hatespeech detection models. Statistical learning models aided with natural language processing (NLP) techniques are frequently used for post-level offensive and hateful language detection (Davidson et al., 2017; Indurthi et al., 2019). Given the increased use of deep learning in NLP tasks, offensive language identification has seen the introduction of methods based on convolutional neural networks (CNNs) and Long Short-term Memory (LSTM) networks (Badjatiya et al., 2017; Gamb¨ack and Sikdar, 2017; Hettiarachchi and Ranasinghe, 2019). The most common approach has been to use a word/character embedding model such as Word2vec (Mikolov et al., 2013), GloVe (Pennington et al., 2014), or fastText (Mikolov et al., 2018) to embed words/tokens and then feed them to an artificial neural network (ANN) (Zampieri et al., 2019b). With the introduction of BERT (Devlin et al., 2019), neural transformer models have become popular in offensive language identification. In hate speech and offensive content identification in Indo-European languages, the BERT model has been shown to outperform GRU (Gated Recurrent Unit) and LSTM-based models"
2021.semeval-1.111,S19-2011,0,0.0190089,"identification. In hate speech and offensive content identification in Indo-European languages, the BERT model has been shown to outperform GRU (Gated Recurrent Unit) and LSTM-based models (Ranasinghe et al., 2019). In Mandl et al. (2019b), the best performing teams on the task employed BERT-based pretrained models that identified the type of hate and target of a (text) post. The SemEval-2019 Task 6 (Zampieri et al., 2019b) presented the challenge of identifying and categorizing offensive posts on social media, which included three sub-tasks. In sub-task A: offensive language identification, Liu et al. (2019a) applied a pre-trained BERT model to achieve the highest F1 score. In Sub-task B: automatic categorization of offense types, BERT-based models also achieved competitive rankings. We noticed similar trends in SemEval-2020 Task 12 (Zampieri et al., 2020) as well. Not limited to English, transformer models have yielded strong results in resource-scarce languages like Bengali (Ranasinghe and Zampieri, 2020) and Malayalam (Ranasinghe et al., 2020) along with cross-lingual transfer learning from resource-rich languages (Ranasinghe and Zampieri, 2020, 2021b). Nonetheless, despite the recent success"
2021.semeval-1.111,2021.ccl-1.108,0,0.112383,"Missing"
2021.semeval-1.111,2021.semeval-1.6,0,0.121973,"ets made available in recent years (Pitenis et al., 2020; Rosenthal et al., 2020). The identification of the particular text spans that make a post offensive, however, has been mostly neglected (Mathew et al., 2021) as current state-ofthe-art offensive language identification models flag the entire post or comment but do not actually highlight the offensive parts. The pressing need for toxic span detection models to assist human content moderation, processing and flagging content in a more interpretable fashion, has motivated the organization of the SemEval-2021 Task 5: Toxic Spans Detection (Pavlopoulos et al., 2021). In this paper, we present the WLV-RIT submission to the SemEval-2021 Task 5. We explore several statistical learning models and report the performance of the best model, which is based on a neural transformer. Next, we generalise our approach to an open-source framework called MUDES: Multilingual Detection of Offensive Spans (Ranasinghe and Zampieri, 2021a). Alongside the framework, we also release the pretrained models as well as a user-friendly web-based User Interface (UI) based on Docker, which provides the functionality of automatically identifying the offensive spans in a given input t"
2021.semeval-1.111,2020.coling-main.78,0,0.0210348,"architecture we designed for this study, which has 4.2 million trainable parameters. We trained the model on mini-batches of 16 samples with a 0.005 learning rate for 5 epochs with a maximum sequence length of 200. 4.3 Neural Transformers Recently, pre-trained language models have been shown to be quite useful across a variety of NLP tasks, particularly those based on bidirectional neural transformers such as BERT (Devlin et al., 2019; Li et al., 2019). Transformer-based models have also been shown to be highly effective in sequence classification tasks such as named entity recognition (NER) (Luoma and Pyysalo, 2020). In our work, we extend the BERT model by integrating a token level classifier. The token-level classifier is a linear transformation that takes the last hidden state of the sequence as the input and produces a label for each token as its output. In this case, each token will be predicted to have one of two possible labels – toxic or not toxic. We fine-tuned the uncased BERT transformer model with a maximum 835 Figure 2: The two-part model architecture. Part A depicts the language model and Part B is the token classifier. (Ranasinghe and Zampieri, 2021a) sequence length of 400 with batches of"
2021.semeval-1.111,malmasi-zampieri-2017-detecting,1,0.858979,"HateXplain (Mathew et al., 2021) is, to the best of our knowledge, the only dataset that we could find that has been annotated at the word level. The dataset consists of 20, 000 posts from Gab and Twitter. Each data sample is annotated with one of the hate/offensive/normal labels, communities being targeted, and words of the text are marked by the annotators who support the label. Models In the past, trolling, aggression, and cyberbullying identification tasks on social media data have been approached using machine and deep learning-focused models (Kumar et al., 2018). Across several studies (Malmasi and Zampieri, 2017, 2018; Waseem and Hovy, 2016) researchers have noted that n-gram based features are very useful when building reliable, automated hatespeech detection models. Statistical learning models aided with natural language processing (NLP) techniques are frequently used for post-level offensive and hateful language detection (Davidson et al., 2017; Indurthi et al., 2019). Given the increased use of deep learning in NLP tasks, offensive language identification has seen the introduction of methods based on convolutional neural networks (CNNs) and Long Short-term Memory (LSTM) networks (Badjatiya et al."
2021.semeval-1.111,L18-1008,0,0.0270321,"techniques are frequently used for post-level offensive and hateful language detection (Davidson et al., 2017; Indurthi et al., 2019). Given the increased use of deep learning in NLP tasks, offensive language identification has seen the introduction of methods based on convolutional neural networks (CNNs) and Long Short-term Memory (LSTM) networks (Badjatiya et al., 2017; Gamb¨ack and Sikdar, 2017; Hettiarachchi and Ranasinghe, 2019). The most common approach has been to use a word/character embedding model such as Word2vec (Mikolov et al., 2013), GloVe (Pennington et al., 2014), or fastText (Mikolov et al., 2018) to embed words/tokens and then feed them to an artificial neural network (ANN) (Zampieri et al., 2019b). With the introduction of BERT (Devlin et al., 2019), neural transformer models have become popular in offensive language identification. In hate speech and offensive content identification in Indo-European languages, the BERT model has been shown to outperform GRU (Gated Recurrent Unit) and LSTM-based models (Ranasinghe et al., 2019). In Mandl et al. (2019b), the best performing teams on the task employed BERT-based pretrained models that identified the type of hate and target of a (text)"
2021.semeval-1.111,W17-3008,0,0.022986,", category, and target identification. Rosenthal et al. (2020) further extended the dataset using a semi-supervised model that was trained with over nine million annotated English tweets. Recently, Mathew et al. (2021) released the first benchmark dataset which covered the three primary areas of online hate-speech detection. The dataset contained a 3-class classification problem (hate-speech, offensive, or neither), a targeted community, as well as the spans that make the text hateful or offensive. Furthermore, offensive language datasets have been annotated in other languages such as Arabic (Mubarak et al., 2017), Danish (Sigurbergsson and Derczynski, 2020), Dutch (Tulkens et al., 2016), French (Chiril et al., 2019), Greek (Pitenis et al., 2020), Portuguese (Fortuna et al., 2019), Spanish (Basile et al., 2019b), and Turkish (C¸o¨ ltekin, 2020). Apart from the dataset released for SemEval2021 Task 5, HateXplain (Mathew et al., 2021) is, to the best of our knowledge, the only dataset that we could find that has been annotated at the word level. The dataset consists of 20, 000 posts from Gab and Twitter. Each data sample is annotated with one of the hate/offensive/normal labels, communities being targete"
2021.semeval-1.111,D14-1162,0,0.0840659,"Missing"
2021.semeval-1.111,2020.lrec-1.629,1,0.936288,"pplying computational models that can identify the various forms of offensive content online such as aggression (Kumar et al., 2018, 2020), cyber-bullying (Rosa et al., 2019), and hate speech (Ridenhour et al., 2020). Prior work has either designed methods for identifying conversations that are likely to go awry (Zhang WARNING: This paper contains text excerpts and words that are offensive in nature. With respect to identifying offensive language in conversations, comments, and posts, noticeable progress has been made with a variety of large, annotated datasets made available in recent years (Pitenis et al., 2020; Rosenthal et al., 2020). The identification of the particular text spans that make a post offensive, however, has been mostly neglected (Mathew et al., 2021) as current state-ofthe-art offensive language identification models flag the entire post or comment but do not actually highlight the offensive parts. The pressing need for toxic span detection models to assist human content moderation, processing and flagging content in a more interpretable fashion, has motivated the organization of the SemEval-2021 Task 5: Toxic Spans Detection (Pavlopoulos et al., 2021). In this paper, we present the"
2021.semeval-1.111,2020.semeval-1.251,1,0.743694,"Missing"
2021.semeval-1.111,2020.emnlp-main.470,1,0.868997,"19 Task 6 (Zampieri et al., 2019b) presented the challenge of identifying and categorizing offensive posts on social media, which included three sub-tasks. In sub-task A: offensive language identification, Liu et al. (2019a) applied a pre-trained BERT model to achieve the highest F1 score. In Sub-task B: automatic categorization of offense types, BERT-based models also achieved competitive rankings. We noticed similar trends in SemEval-2020 Task 12 (Zampieri et al., 2020) as well. Not limited to English, transformer models have yielded strong results in resource-scarce languages like Bengali (Ranasinghe and Zampieri, 2020) and Malayalam (Ranasinghe et al., 2020) along with cross-lingual transfer learning from resource-rich languages (Ranasinghe and Zampieri, 2020, 2021b). Nonetheless, despite the recent success of statistical learning in offensive language detection problems, due to the lack of finer-grained, detailed datasets, models are limited in their ability to predict word-level labels. 3 Task and Dataset In the SemEval-2021 Task 5 dataset, the sequence of words that makes a particular post or comment toxic is defined as a toxic span. The dataset for this task is extracted from posts in the Civil Comments"
2021.semeval-1.111,2021.naacl-demos.17,1,0.952198,"sive parts. The pressing need for toxic span detection models to assist human content moderation, processing and flagging content in a more interpretable fashion, has motivated the organization of the SemEval-2021 Task 5: Toxic Spans Detection (Pavlopoulos et al., 2021). In this paper, we present the WLV-RIT submission to the SemEval-2021 Task 5. We explore several statistical learning models and report the performance of the best model, which is based on a neural transformer. Next, we generalise our approach to an open-source framework called MUDES: Multilingual Detection of Offensive Spans (Ranasinghe and Zampieri, 2021a). Alongside the framework, we also release the pretrained models as well as a user-friendly web-based User Interface (UI) based on Docker, which provides the functionality of automatically identifying the offensive spans in a given input text. 833 Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021), pages 833–840 Bangkok, Thailand (online), August 5–6, 2021. ©2021 Association for Computational Linguistics 2 Related Work Datasets Over the past several years, multiple post-level, offensive language benchmark datasets have been released. In Zampieri et al. (2019"
2021.semeval-1.111,2020.lrec-1.430,0,0.0697737,"cation. Rosenthal et al. (2020) further extended the dataset using a semi-supervised model that was trained with over nine million annotated English tweets. Recently, Mathew et al. (2021) released the first benchmark dataset which covered the three primary areas of online hate-speech detection. The dataset contained a 3-class classification problem (hate-speech, offensive, or neither), a targeted community, as well as the spans that make the text hateful or offensive. Furthermore, offensive language datasets have been annotated in other languages such as Arabic (Mubarak et al., 2017), Danish (Sigurbergsson and Derczynski, 2020), Dutch (Tulkens et al., 2016), French (Chiril et al., 2019), Greek (Pitenis et al., 2020), Portuguese (Fortuna et al., 2019), Spanish (Basile et al., 2019b), and Turkish (C¸o¨ ltekin, 2020). Apart from the dataset released for SemEval2021 Task 5, HateXplain (Mathew et al., 2021) is, to the best of our knowledge, the only dataset that we could find that has been annotated at the word level. The dataset consists of 20, 000 posts from Gab and Twitter. Each data sample is annotated with one of the hate/offensive/normal labels, communities being targeted, and words of the text are marked by the an"
2021.semeval-1.111,N16-2013,0,0.0163828,"s, to the best of our knowledge, the only dataset that we could find that has been annotated at the word level. The dataset consists of 20, 000 posts from Gab and Twitter. Each data sample is annotated with one of the hate/offensive/normal labels, communities being targeted, and words of the text are marked by the annotators who support the label. Models In the past, trolling, aggression, and cyberbullying identification tasks on social media data have been approached using machine and deep learning-focused models (Kumar et al., 2018). Across several studies (Malmasi and Zampieri, 2017, 2018; Waseem and Hovy, 2016) researchers have noted that n-gram based features are very useful when building reliable, automated hatespeech detection models. Statistical learning models aided with natural language processing (NLP) techniques are frequently used for post-level offensive and hateful language detection (Davidson et al., 2017; Indurthi et al., 2019). Given the increased use of deep learning in NLP tasks, offensive language identification has seen the introduction of methods based on convolutional neural networks (CNNs) and Long Short-term Memory (LSTM) networks (Badjatiya et al., 2017; Gamb¨ack and Sikdar, 2"
2021.semeval-1.111,N19-1144,1,0.944795,"ecting Toxic Spans Tharindu Ranasinghe1 , Diptanu Sarkar2 , Marcos Zampieri2 , Alexander Ororbia2 1 University of Wolverhampton,UK 2 Rochester Institute of Technology, USA T.D.RanasingheHettiarachchige@wlv.ac.uk Abstract et al., 2018; Chang et al., 2020) or detecting offensive content and labelling posts at the instances level – this has been the focus in the recent shared tasks like HASOC at FIRE 2019 (Mandl et al., 2019a) and FIRE 2020 (Mandl et al., 2020), GermEval 2019 Task 2 (Struß et al., 2019), TRAC (Kumar et al., 2018, 2020), HatEval (Basile et al., 2019a), OffensEval at SemEval-2019 (Zampieri et al., 2019b) and SemEval-2020 (Zampieri et al., 2020). In recent years, the widespread use of social media has led to an increase in the generation of toxic and offensive content on online platforms. In response, social media platforms have worked on developing automatic detection methods and employing human moderators to cope with this deluge of offensive content. While various state-of-the-art statistical models have been applied to detect toxic posts, there are only a few studies that focus on detecting the words or expressions that make a post offensive. This motivates the organization of the SemEva"
2021.semeval-1.111,S19-2010,1,0.953347,"ecting Toxic Spans Tharindu Ranasinghe1 , Diptanu Sarkar2 , Marcos Zampieri2 , Alexander Ororbia2 1 University of Wolverhampton,UK 2 Rochester Institute of Technology, USA T.D.RanasingheHettiarachchige@wlv.ac.uk Abstract et al., 2018; Chang et al., 2020) or detecting offensive content and labelling posts at the instances level – this has been the focus in the recent shared tasks like HASOC at FIRE 2019 (Mandl et al., 2019a) and FIRE 2020 (Mandl et al., 2020), GermEval 2019 Task 2 (Struß et al., 2019), TRAC (Kumar et al., 2018, 2020), HatEval (Basile et al., 2019a), OffensEval at SemEval-2019 (Zampieri et al., 2019b) and SemEval-2020 (Zampieri et al., 2020). In recent years, the widespread use of social media has led to an increase in the generation of toxic and offensive content on online platforms. In response, social media platforms have worked on developing automatic detection methods and employing human moderators to cope with this deluge of offensive content. While various state-of-the-art statistical models have been applied to detect toxic posts, there are only a few studies that focus on detecting the words or expressions that make a post offensive. This motivates the organization of the SemEva"
2021.semeval-1.111,P18-1125,0,0.056722,"Missing"
2021.semeval-1.67,W18-0538,0,0.0213435,"(Malmasi et al., 2016; Zampieri et al., 2016; Paetzold and Specia, 2016; Yimam et al., 2018). Among the best performing systems in CWI 2018, Gooding and Kochmar (2018) used an ensemble of classifiers. They found that during their system’s development, the boosting classifier AdaBoost, a random forest classifier, or a combination of both classifiers achieved the highest performance. These systems used multiple features such as the word’s grammatical category, Google character n-gram frequency as well as a range of psycholinguistic features (Gooding and Kochmar, 2018). Of the remaining systems, Aroyehun et al. (2018) and Hartmann and Borges dos Santos (2018) utilized statistical features, such as word length and number of syllables, psycholinguistic features such as familiarity, age of acquisition, concreteness, and imagery scores, and word n-grams. Hartmann and Borges dos Santos (2018) compared the performance of tree ensembles to a convolutional neural network (CNN). They found that their tree ensembles performed better than their CNN, especially when the target expression contained more than three words (Aroyehun et al., 2018). 3 Task and Dataset The LCP shared task organizers provided participants wit"
2021.semeval-1.67,W18-0520,0,0.0144896,"vious task (Paetzold and Specia, 2016), probably due to the properties of the two datasets (Zampieri et al., 2017). State-of-the-art neural net models and word embedding models performed worse than conventional models such as decision trees (DTs) and random forests (RFs) (Yimam et al., 2018). Among the conventional models, the use of statistical, character n-gram, and psycholinguistic features was found to be highly effective in improving CWI performance (Malmasi et al., 2016; Zampieri et al., 2016; Paetzold and Specia, 2016; Yimam et al., 2018). Among the best performing systems in CWI 2018, Gooding and Kochmar (2018) used an ensemble of classifiers. They found that during their system’s development, the boosting classifier AdaBoost, a random forest classifier, or a combination of both classifiers achieved the highest performance. These systems used multiple features such as the word’s grammatical category, Google character n-gram frequency as well as a range of psycholinguistic features (Gooding and Kochmar, 2018). Of the remaining systems, Aroyehun et al. (2018) and Hartmann and Borges dos Santos (2018) utilized statistical features, such as word length and number of syllables, psycholinguistic features"
2021.semeval-1.67,W18-0540,0,0.0532199,"Missing"
2021.semeval-1.67,O13-1007,0,0.0161013,"Missing"
2021.semeval-1.67,D18-1410,0,0.0530862,"Missing"
2021.semeval-1.67,S16-1154,1,0.83687,"results in terms of mean absolute error, mean squared error, Pearson correlation, and Spearman correlation. 1 Introduction Lexical complexity prediction (LCP) is the task of predicting the complexity value of a target word within a given text (Shardlow et al., 2020). Complexity within LCP is used as a “synonym for difficulty” (Malmasi and Zampieri, 2016)1 . A complex word is therefore a word that a target population may find difficult to understand. Various LCP systems have been designed to identify words that may be found to be complex for children (Kajiwara et al., 2013), language learners (Malmasi et al., 2016), or people suffering from a reading disability, such as dyslexia (Rello et al., 2013). These systems have been utilized within assistive language technologies, lexical simplification systems, and in a variety of other applications. LCP is related to complex word identification (CWI) (Paetzold and Specia, 2016). CWI is modeled as a binary classification task by assigning each target word with a complex or non-complex label. The shortcomings of modeling lexical complexity using binary labels have been discussed in previous work (Zampieri et al., 2017; Maddela and Xu, 2018), motivating the organ"
2021.semeval-1.67,S16-1153,1,0.57084,"wide range of linguistic features (e.g. psycholinguistic features, n-grams, word frequency, POS tags) to predict the complexity of single words in this dataset. We analyze the impact of different linguistic features on the classification performance and we evaluate the results in terms of mean absolute error, mean squared error, Pearson correlation, and Spearman correlation. 1 Introduction Lexical complexity prediction (LCP) is the task of predicting the complexity value of a target word within a given text (Shardlow et al., 2020). Complexity within LCP is used as a “synonym for difficulty” (Malmasi and Zampieri, 2016)1 . A complex word is therefore a word that a target population may find difficult to understand. Various LCP systems have been designed to identify words that may be found to be complex for children (Kajiwara et al., 2013), language learners (Malmasi et al., 2016), or people suffering from a reading disability, such as dyslexia (Rello et al., 2013). These systems have been utilized within assistive language technologies, lexical simplification systems, and in a variety of other applications. LCP is related to complex word identification (CWI) (Paetzold and Specia, 2016). CWI is modeled as a b"
2021.semeval-1.67,P18-1017,0,0.0164172,"first learned. Concreteness refers to “the degree to which the concept denoted by a word refers to a perceptible entity” (Brysbaert et al., 2013). Familiarity and prevalence are somewhat similar. Familiarity is how well known the word is to an individual and was obtained through self-report (Gilhooly and Logie, 1980). Prevalence was calculated in accordance to the percentage of people who knew the word (Brysbaert et al., 2019). Lastly, arousal is a measure of how active or passive a word’s meaning is interpreted as being3 . For instance, the word “nervous” indicates more arousal than “lazy” (Mohammad, 2018). As such, grammatical categories such as adjectives, verbs, and adverbs may incite higher levels of arousal than nouns. Average AoA was calculated by averaging the AoAs provided in the Living Word Vocabulary Dataset (Dale and O’Rourke, 1981) with an updated version of this dataset (Brysbaert and Biemiller, 2017). Both datasets consisted of AoA values for 44,000 English word meanings. Concreteness, familiarity and arousal values were taken from the MRC Psycholinguistic Database (Wilson, 1988) as well as three newer datasets each containing 37,058, 61,858 and 20,000 English words (Brysbaert et"
2021.semeval-1.67,S16-1152,0,0.0591002,"Missing"
2021.semeval-1.67,S16-1161,0,0.0627925,"ifficult (0.75-1) (Shardlow et al., 2020). In this paper, we describe (in detail in Section 4) the LCP-RIT entry to SemEval-2021 Task 1. We approached LCP from a feature engineering perspective with a particular focus on the adoption of psycholinguistic features, such as average ageof-acquisition (AoA), familiarity, prevalence, concreteness, and arousal, alongside the use of prior complexity labels. Our submitted system utilized a combination of these linguistic features, which we compared to a baseline model that only used statistical features: word length, word frequency and syllable count (Quijada and Medero, 2016; Mukherjee et al., 2016). On our training dataset, our submitted system achieved a mean absolute error (MAE) of 0.067, mean squared error (MSE) of 0.007, Person Correlation (R) score of 0.779, and a Spearman Correlation (ρ) score of 0.724. This surpassed our baseline model’s performance by a MAE of 0.008, MSE of 0.003, as well as R and ρ scores of 0.075 and 0.062 respectively. 1 The term “complex” within LCP is not necessarily related to the terms simplex and complex used in morphology. 2 Related Work Before SemEval-2021 Task 1: LCP, two CWI shared tasks were organized at one SemEval-2016 and"
2021.semeval-1.67,2020.readi-1.9,1,0.80588,"complexity using a five point Likert scale. Our system uses logistic regression and a wide range of linguistic features (e.g. psycholinguistic features, n-grams, word frequency, POS tags) to predict the complexity of single words in this dataset. We analyze the impact of different linguistic features on the classification performance and we evaluate the results in terms of mean absolute error, mean squared error, Pearson correlation, and Spearman correlation. 1 Introduction Lexical complexity prediction (LCP) is the task of predicting the complexity value of a target word within a given text (Shardlow et al., 2020). Complexity within LCP is used as a “synonym for difficulty” (Malmasi and Zampieri, 2016)1 . A complex word is therefore a word that a target population may find difficult to understand. Various LCP systems have been designed to identify words that may be found to be complex for children (Kajiwara et al., 2013), language learners (Malmasi et al., 2016), or people suffering from a reading disability, such as dyslexia (Rello et al., 2013). These systems have been utilized within assistive language technologies, lexical simplification systems, and in a variety of other applications. LCP is relat"
2021.semeval-1.67,W18-0507,1,0.883121,"Missing"
2021.semeval-1.67,W17-5910,1,0.893271,"(Kajiwara et al., 2013), language learners (Malmasi et al., 2016), or people suffering from a reading disability, such as dyslexia (Rello et al., 2013). These systems have been utilized within assistive language technologies, lexical simplification systems, and in a variety of other applications. LCP is related to complex word identification (CWI) (Paetzold and Specia, 2016). CWI is modeled as a binary classification task by assigning each target word with a complex or non-complex label. The shortcomings of modeling lexical complexity using binary labels have been discussed in previous work (Zampieri et al., 2017; Maddela and Xu, 2018), motivating the organization of SemEval2021 Task 1: Lexical Complexity Prediction.2 LCP models complexity in a continuum and the goal is to predict a target word’s degree of complexity by assigning it a value between 0 and 1. This value may then correspond to one of the following labels: very easy (0), easy (0-0.25), neutral (0.250.5), difficult (0.5-0.75), or very difficult (0.75-1) (Shardlow et al., 2020). In this paper, we describe (in detail in Section 4) the LCP-RIT entry to SemEval-2021 Task 1. We approached LCP from a feature engineering perspective with a partic"
2021.semeval-1.67,S16-1155,1,0.88728,"Missing"
2021.vardial-1.1,2020.vardial-1.26,0,0.0398438,"Missing"
2021.vardial-1.1,W19-1402,0,0.103765,"Missing"
2021.vardial-1.1,2021.vardial-1.15,0,0.0589664,"Missing"
2021.vardial-1.1,P19-1068,1,0.833848,"ntification (RDI): The 2021 Romanian Dialect Identification shared task is at the third iteration, following the 2019 Moldavian vs. Romanian Cross-Dialect Topic identification (MRC) (Zampieri et al., 2019) and the 2020 Romanian Dialect Identification (RDI) (G˘aman et al., 2020) shared tasks. The 2021 RDI shared task is formulated as a cross-domain binary classification by dialect problem, in which a classification model is required to discriminate between the Moldavian (MD) and the Romanian (RO) subdialects. This year, we provided participants with an augmented version of the MOROCO data set (Butnaru and Ionescu, 2019) for training, which contains Moldavian and Romanian samples of text collected from the news domain. Last year’s test set of tweets (G˘aman and Ionescu, 2020b) is used for validation. A new set of tweets has been collected for the 2021 shared task. The task has two formats, open and closed. In the closed format, participants are not allowed to use external data to train their models. In the open format, participants are allowed to use external resources such as unlabeled corpora, lexicons and pre-trained embeddings (e.g. BERT), but the use of additional labeled data is still not allowed. Urali"
2021.vardial-1.1,2021.vardial-1.12,0,0.478027,"nnada) grammar with English lexicon or English grammar with south Dravidian lexicons (Jose et al., 2020; Priyadharshini et al., 2020). The comments were written in the Latin Script with different types of code-mixing. The language tag of the comment were given. The challenge of the task was to identify the language of the given comment. It was 2 http://urn.fi/urn:nbn:fi: lb-2020102201 2 Team DLI RDI SMG ULI HeLju HWR LAST NAYEL NRC Phlyers SUKI UnibucKernel UPB System Description Papers (Scherrer and Ljubeˇsi´c, 2021) (Jauhiainen et al., 2021b) (Bestgen, 2021) (Bernier-Colborne et al., 2021) (Ceolin, 2021) (Jauhiainen et al., 2021a) (G˘aman et al., 2021) (Zaharia et al., 2021) Table 1: The teams that participated in the VarDial Evaluation Campaign 2021. a challenging task, since Tamil, Malayalam and Kannada are closely related languages, some of the words being common in all these languages. The participants had to train a system to identify the language of each comment. Our dataset size is 16,672 comments for training and 4,588 for testing. There were three language tags such as Tamil, Malayalam and Kannada. A new category Not in intended language was added to include comments written in a lan"
2021.vardial-1.1,2020.vardial-1.25,0,0.247563,"opment data for training and (ii) the idea of adapting the language model to the test set. The team that was ranked in the second place is UPB. Their best submission is an ensemble that comprises several deep models, including a Romanian BERT. Different from their last year’s participation (Zaharia et al., 2020), they carefully split the training set into sentences. This idea was borrowed from top-ranked teams of the 2020 RDI shared task. Phlyers ranked on the third place in the 2021 ranking, without significant differences in terms of performance with respect to their previous participation (Ceolin and Zhang, 2020). Despite having access to significantly more in-domain data compared with the previous RDI shared task, the participants were not able to report significant performance gains. Indeed, the top scoring team (C ¸ o¨ ltekin, 2020) in 2020 reached a macro F1 score of 0.7876, while the top scoring team in 2021 achieved a macro F1 score of 0.7772. Although the test sets are not identical, we 6 Social Media Variety Geolocation (SMG) 6.1 Dataset The SMG task is based on three datasets from two Social Media platforms, Jodel and Twitter. Since its first edition in 2020, the datasets have been expanded."
2021.vardial-1.1,2020.sltu-1.25,1,0.720696,"format and evaluation methodology. 4 3 Participating Teams A total of nine teams submitted runs to one or more shared tasks in this year’s VarDial evaluation campaign. In Table 1, we list the teams that participated in the shared tasks, including references to the 8 system description papers which will be published as parts of the VarDial workshop proceedings. Detailed information about the submissions in each respective task is included in the following sections of this report. 4.1 Dravidian Language Identification (DLI) Dataset The DLI task is based on three datasets from YouTube comments (Chakravarthi et al., 2020b,a; Hande et al., 2020). In the 2021 (DLI) shared task, participants have to train a model on comments written in Roman script. Our corpora contains all the three types of code-mixed sentences: InterSentential switch, Intra-Sentential switch and Tag switching. All comments were written in Roman script (Non-native script) with either one of the south Dravidian (Tamil, Malayalam, and Kannada) grammar with English lexicon or English grammar with south Dravidian lexicons (Jose et al., 2020; Priyadharshini et al., 2020). The comments were written in the Latin Script with different types of code-mi"
2021.vardial-1.1,2020.sltu-1.28,1,0.713401,"format and evaluation methodology. 4 3 Participating Teams A total of nine teams submitted runs to one or more shared tasks in this year’s VarDial evaluation campaign. In Table 1, we list the teams that participated in the shared tasks, including references to the 8 system description papers which will be published as parts of the VarDial workshop proceedings. Detailed information about the submissions in each respective task is included in the following sections of this report. 4.1 Dravidian Language Identification (DLI) Dataset The DLI task is based on three datasets from YouTube comments (Chakravarthi et al., 2020b,a; Hande et al., 2020). In the 2021 (DLI) shared task, participants have to train a model on comments written in Roman script. Our corpora contains all the three types of code-mixed sentences: InterSentential switch, Intra-Sentential switch and Tag switching. All comments were written in Roman script (Non-native script) with either one of the south Dravidian (Tamil, Malayalam, and Kannada) grammar with English lexicon or English grammar with south Dravidian lexicons (Jose et al., 2020; Priyadharshini et al., 2020). The comments were written in the Latin Script with different types of code-mi"
2021.vardial-1.1,W19-1409,1,0.900062,"Missing"
2021.vardial-1.1,W18-3929,1,0.901892,"Missing"
2021.vardial-1.1,W14-5316,0,0.049676,"Missing"
2021.vardial-1.1,2021.vardial-1.10,1,0.840728,"Missing"
2021.vardial-1.1,2020.vardial-1.21,1,0.873523,"Missing"
2021.vardial-1.1,2020.vardial-1.1,1,0.844419,"Missing"
2021.vardial-1.1,2021.vardial-1.9,1,0.849095,"Missing"
2021.vardial-1.1,2020.vardial-1.23,1,0.84553,"Missing"
2021.vardial-1.1,W17-0221,1,0.89467,"Missing"
2021.vardial-1.1,2020.peoples-1.6,1,0.765867,"logy. 4 3 Participating Teams A total of nine teams submitted runs to one or more shared tasks in this year’s VarDial evaluation campaign. In Table 1, we list the teams that participated in the shared tasks, including references to the 8 system description papers which will be published as parts of the VarDial workshop proceedings. Detailed information about the submissions in each respective task is included in the following sections of this report. 4.1 Dravidian Language Identification (DLI) Dataset The DLI task is based on three datasets from YouTube comments (Chakravarthi et al., 2020b,a; Hande et al., 2020). In the 2021 (DLI) shared task, participants have to train a model on comments written in Roman script. Our corpora contains all the three types of code-mixed sentences: InterSentential switch, Intra-Sentential switch and Tag switching. All comments were written in Roman script (Non-native script) with either one of the south Dravidian (Tamil, Malayalam, and Kannada) grammar with English lexicon or English grammar with south Dravidian lexicons (Jose et al., 2020; Priyadharshini et al., 2020). The comments were written in the Latin Script with different types of code-mixing. The language tag o"
2021.vardial-1.1,W19-1419,1,0.865245,"Missing"
2021.vardial-1.1,2021.vardial-1.14,1,0.79889,"poro movie la Enna irukunu baki ellam. 4.2 Results 4 Participants and Approaches Due to the short time between the announcement of the shared task and the submission deadline, the participation was lower than we expected. Four teams submitted results to the shared task. Bestgen (2021) proposed a logistic regression model based on n-grams of characters with maximum length as features to classify the comments. The authors achieved a high score with simple techniques. The authors also analyzed the results in detail. For more information, the reader should look at the working notes of the author. Jauhiainen et al. (2021b) submitted results using two models, a Na¨ıve Bayes (NB) classifier with adaptive language models, which was shown to obtain competitive performance in many language and dialect identification tasks, and a transformerbased model, which is widely regarded as the stateof-the-art in a number of NLP tasks. Their first Table 2: The results of all entries by the four team participating in the DLI shared task in terms of Macro-F1. Given the difficulty of the DLI 2021 task, the level of performance achieved by the systems is appreciable. Identifying the Other-language category was particularly diffi"
2021.vardial-1.1,2021.vardial-1.13,0,0.185158,"h Dravidian lexicons (Jose et al., 2020; Priyadharshini et al., 2020). The comments were written in the Latin Script with different types of code-mixing. The language tag of the comment were given. The challenge of the task was to identify the language of the given comment. It was 2 http://urn.fi/urn:nbn:fi: lb-2020102201 2 Team DLI RDI SMG ULI HeLju HWR LAST NAYEL NRC Phlyers SUKI UnibucKernel UPB System Description Papers (Scherrer and Ljubeˇsi´c, 2021) (Jauhiainen et al., 2021b) (Bestgen, 2021) (Bernier-Colborne et al., 2021) (Ceolin, 2021) (Jauhiainen et al., 2021a) (G˘aman et al., 2021) (Zaharia et al., 2021) Table 1: The teams that participated in the VarDial Evaluation Campaign 2021. a challenging task, since Tamil, Malayalam and Kannada are closely related languages, some of the words being common in all these languages. The participants had to train a system to identify the language of each comment. Our dataset size is 16,672 comments for training and 4,588 for testing. There were three language tags such as Tamil, Malayalam and Kannada. A new category Not in intended language was added to include comments written in a language other than the Dravidian languages. A sample comment from our data"
2021.vardial-1.1,W17-1201,1,0.533591,"Missing"
2021.vardial-1.1,C16-1322,1,0.889269,"Missing"
2021.vardial-1.1,W18-3901,1,0.749269,"Missing"
2021.vardial-1.1,2021.vardial-1.16,1,0.843333,"Missing"
2021.vardial-1.1,W14-5307,1,0.728758,"Missing"
2021.vardial-1.14,2020.sltu-1.25,0,0.0562298,"oire of the language identifiers presented by, for example, Majliˇs (2011) and Kocmi and Bojar (2017). Hanumathappa and Reddy (2012) conducted identification experiments between Kannada and Telugu, a Central Dravidian language. 3 Shared Task Setup and Data The evaluation measure in the DLI shared task was the macro F1 score, which gives equal value for each language independent of their actual distribution in the test set. The data set provided by the DLI organizers contains a total of 22,164 YouTube comments written in a mix of English and one of the aforementioned South Dravidian languages (Chakravarthi et al., 2020a,b; Hande et al., 2020). In addition to the target languages, the data included comments in other languages as well. It was divided into 16,674 instances for training and 4,590 instances for testing. The number of instances for each language is show in Figure 1. 121 Set kan mal tam other Total Training 493 4,204 10,969 1,008 16,674 Test 4,590 Total 22,164 Table 1: Number of instances in the DLI dataset for Kannada (kan), Malayalam (mal), and Tamil (tam). In order to evaluate and compare our methods using the training data, we divided the training data into training and development portions. F"
2021.vardial-1.14,2020.sltu-1.28,0,0.0610318,"oire of the language identifiers presented by, for example, Majliˇs (2011) and Kocmi and Bojar (2017). Hanumathappa and Reddy (2012) conducted identification experiments between Kannada and Telugu, a Central Dravidian language. 3 Shared Task Setup and Data The evaluation measure in the DLI shared task was the macro F1 score, which gives equal value for each language independent of their actual distribution in the test set. The data set provided by the DLI organizers contains a total of 22,164 YouTube comments written in a mix of English and one of the aforementioned South Dravidian languages (Chakravarthi et al., 2020a,b; Hande et al., 2020). In addition to the target languages, the data included comments in other languages as well. It was divided into 16,674 instances for training and 4,590 instances for testing. The number of instances for each language is show in Figure 1. 121 Set kan mal tam other Total Training 493 4,204 10,969 1,008 16,674 Test 4,590 Total 22,164 Table 1: Number of instances in the DLI dataset for Kannada (kan), Malayalam (mal), and Tamil (tam). In order to evaluate and compare our methods using the training data, we divided the training data into training and development portions. F"
2021.vardial-1.14,P19-4007,0,0.0615872,"Missing"
2021.vardial-1.14,N19-1423,0,0.183688,"in which using the language set identification did not improve the results. Unfortunately, we did not have time to finalize our experiments as we were left contemplating about the nature of the multilinguality inherent in the DLI data set and were not encouraged by our initial results. 4.5 Transformers The system for our second submission (described in Section 5.2) was based on pretrained transformer The publication of the HeLI 2.0 implementation is still currently in our queue. 5 https://github.com/tosaja/HeLI 6 https://github.com/tosaja/ TunnistinPalveluMulti 123 models: multilingual BERT (Devlin et al., 2019) and XLM-RoBERTa (XLM-R) (Conneau et al., 2019). The system used pretrained language models available from the Hugging Face Team 7 . Transformer are effective than RNN based deep learning architectures (Hettiarachchi and Ranasinghe, 2019) in text classification tasks (Ranasinghe et al., 2019; Ranasinghe and Hettiarachchi, 2020; Pitenis et al., 2020). It was also evaluated using the same development set which was used in the previously described experiments. It the initial experiments it achieved a macro F1 score of 0.785, which was considerably lower than the 0.861 gained by the simple naive B"
2021.vardial-1.14,2020.coling-main.141,0,0.053543,"Missing"
2021.vardial-1.14,L16-1284,1,0.897138,"Missing"
2021.vardial-1.14,2020.vardial-1.1,1,0.860565,"Missing"
2021.vardial-1.14,2020.wac-1.4,1,0.826433,"Missing"
2021.vardial-1.14,2021.vardial-1.9,1,0.833959,"Missing"
2021.vardial-1.14,W18-3929,1,0.873949,"Missing"
2021.vardial-1.14,W19-1419,1,0.903313,"Missing"
2021.vardial-1.14,2020.peoples-1.6,0,0.067659,"rs presented by, for example, Majliˇs (2011) and Kocmi and Bojar (2017). Hanumathappa and Reddy (2012) conducted identification experiments between Kannada and Telugu, a Central Dravidian language. 3 Shared Task Setup and Data The evaluation measure in the DLI shared task was the macro F1 score, which gives equal value for each language independent of their actual distribution in the test set. The data set provided by the DLI organizers contains a total of 22,164 YouTube comments written in a mix of English and one of the aforementioned South Dravidian languages (Chakravarthi et al., 2020a,b; Hande et al., 2020). In addition to the target languages, the data included comments in other languages as well. It was divided into 16,674 instances for training and 4,590 instances for testing. The number of instances for each language is show in Figure 1. 121 Set kan mal tam other Total Training 493 4,204 10,969 1,008 16,674 Test 4,590 Total 22,164 Table 1: Number of instances in the DLI dataset for Kannada (kan), Malayalam (mal), and Tamil (tam). In order to evaluate and compare our methods using the training data, we divided the training data into training and development portions. For training, we used the"
2021.vardial-1.14,R19-1056,1,0.3728,"the DLI data set and were not encouraged by our initial results. 4.5 Transformers The system for our second submission (described in Section 5.2) was based on pretrained transformer The publication of the HeLI 2.0 implementation is still currently in our queue. 5 https://github.com/tosaja/HeLI 6 https://github.com/tosaja/ TunnistinPalveluMulti 123 models: multilingual BERT (Devlin et al., 2019) and XLM-RoBERTa (XLM-R) (Conneau et al., 2019). The system used pretrained language models available from the Hugging Face Team 7 . Transformer are effective than RNN based deep learning architectures (Hettiarachchi and Ranasinghe, 2019) in text classification tasks (Ranasinghe et al., 2019; Ranasinghe and Hettiarachchi, 2020; Pitenis et al., 2020). It was also evaluated using the same development set which was used in the previously described experiments. It the initial experiments it achieved a macro F1 score of 0.785, which was considerably lower than the 0.861 gained by the simple naive Bayes model even though it used pretrained models as opposed to the naive Bayes which was using only the data provided for the DLI task. We considered generating additional training material from the SUKI data and other available Dravidian"
2021.vardial-1.14,2020.semeval-1.16,1,0.677469,"Missing"
2021.vardial-1.14,2020.wnut-1.49,1,0.780837,"Missing"
2021.vardial-1.14,W16-4820,1,0.886701,"Missing"
2021.vardial-1.14,E17-1087,0,0.0664723,"Missing"
2021.vardial-1.14,W17-1222,1,0.798387,"d Dialects, pages 120–127 April 20, 2021 ©2021 Association for Computational Linguistics 2 Related Work The task of automatic language identification of texts has been under continuous research since 1960s as is witnessed, for example, by the works of Mustonen (1965), House and Neuburg (1977), Henrich (1989), Grefenstette (1995), and Martins and Silva (2005). The problem of discriminating between similar languages, language varieties, and dialects is a particularly challenging one and it has also been addressed by a number of studies such as Tiedemann and Ljubeˇsi´c (2012); Tan et al. (2014); Malmasi and Zampieri (2017b,a), and the aforementioned shared tasks at the VarDial workshop. In addition to addressing the issue of discriminating between similar languages, Jauhiainen et al. (2019c) provide an extensive overview of the history and methods used in language identification of texts in general. 2.1 Language Identification of South Dravidian Languages Even though the DLI 2021 shared task was the first time a shared task solely focused on discriminating between Dravidian languages, Dravidian languages have been part of language repertoire of LI research before. Most of the research so far has focused on tex"
2021.vardial-1.14,W17-1220,1,0.652675,"d Dialects, pages 120–127 April 20, 2021 ©2021 Association for Computational Linguistics 2 Related Work The task of automatic language identification of texts has been under continuous research since 1960s as is witnessed, for example, by the works of Mustonen (1965), House and Neuburg (1977), Henrich (1989), Grefenstette (1995), and Martins and Silva (2005). The problem of discriminating between similar languages, language varieties, and dialects is a particularly challenging one and it has also been addressed by a number of studies such as Tiedemann and Ljubeˇsi´c (2012); Tan et al. (2014); Malmasi and Zampieri (2017b,a), and the aforementioned shared tasks at the VarDial workshop. In addition to addressing the issue of discriminating between similar languages, Jauhiainen et al. (2019c) provide an extensive overview of the history and methods used in language identification of texts in general. 2.1 Language Identification of South Dravidian Languages Even though the DLI 2021 shared task was the first time a shared task solely focused on discriminating between Dravidian languages, Dravidian languages have been part of language repertoire of LI research before. Most of the research so far has focused on tex"
2021.vardial-1.14,W17-1219,0,0.0194668,"We present the submissions by team HWR to the Dravidian Language Identification (DLI) shared task at VarDial 2021. The DLI shared task featured a challenging dataset including YouTube comments written in Roman script containing code-mixed text with English and one of the three South Dravidian languages: Kannada, Malayalam, and Tamil. Our two systems, obtained competitive performance with the NB system achieving 2nd position in the competition. Our results are in line with the general trend of deep learning methods not being overtly competitive in language identification tasks as discussed in Medvedeva et al. (2017). Table 4 shows the results of the shared task. Our first run was clearly better than the second and not far behind the results of the LAST-team. Rank 1 2 2 4 Team LAST LAST LAST HWR NYAEL NAYEL Phlyers Phlyers HWR NAYEL Run 1 2 3 1 1 2 1 2 2 3 Macro F1 0.93 0.92 0.92 0.92 0.92 0.91 0.89 0.89 0.89 0.84 Acknowledgments Table 4: The results of each team participating on the DLI shared task in terms of Macro F1. Our results are displayed in bold. 6 Conclusion We would like to thank the DLI organizers for making this interesting dataset available. This research has been partly funded by The Finnis"
2021.vardial-1.14,W16-5805,0,0.0461755,"al., 2016) and many shared tasks organized in past VarDial workshops (Zampieri et al., 2018, 2019; G˘aman et al., 2020). The DLI debuted as the first competition on similar language identification using code-mixed data associated with the VarDial workshops. Very similar multilingual language identification shared tasks were organized as part of Forum for Information Retrieval Evaluation (FIRE) meetings in 2013 and 2014 (Roy et al., 2013; Choudhury et al., 2014).2 A similar competition to the DLI are also the shared tasks on Language Identification in Code-Switched Data (Solorio et al., 2014; Molina et al., 2016). These shared tasks addressed intrasentential language identification with word level labeling whereas the DLI shared task is language identification shared task with predictions at the document level. We take this opportunity to evaluate the performance of two models for this task, a Naive Bayes classifier using adaptive language models and a transformers-based system described in detail in Section 4. 1 https://sites.google.com/view/ vardial2021/evaluation-campaign 2 http://fire.irsi.res.in/fire/2021/ home 120 Proceedings of the 8th VarDial Workshop on NLP for Similar Languages, Varieties an"
2021.vardial-1.14,C12-1160,0,0.0732644,"Missing"
2021.vardial-1.14,N19-4013,0,0.131736,"Missing"
2021.vardial-1.14,2020.lrec-1.629,1,0.788873,"ed in Section 5.2) was based on pretrained transformer The publication of the HeLI 2.0 implementation is still currently in our queue. 5 https://github.com/tosaja/HeLI 6 https://github.com/tosaja/ TunnistinPalveluMulti 123 models: multilingual BERT (Devlin et al., 2019) and XLM-RoBERTa (XLM-R) (Conneau et al., 2019). The system used pretrained language models available from the Hugging Face Team 7 . Transformer are effective than RNN based deep learning architectures (Hettiarachchi and Ranasinghe, 2019) in text classification tasks (Ranasinghe et al., 2019; Ranasinghe and Hettiarachchi, 2020; Pitenis et al., 2020). It was also evaluated using the same development set which was used in the previously described experiments. It the initial experiments it achieved a macro F1 score of 0.785, which was considerably lower than the 0.861 gained by the simple naive Bayes model even though it used pretrained models as opposed to the naive Bayes which was using only the data provided for the DLI task. We considered generating additional training material from the SUKI data and other available Dravidian corpora written with the native script using automatic transliteration.8 However, we were doubtful about the per"
2021.vardial-1.14,2020.semeval-1.251,1,0.612848,"Missing"
2021.vardial-1.14,2020.emnlp-main.470,1,0.780927,"0), span classification (Ranasinghe and Zampieri, 2021), word similarity (Hettiarachchi and Ranasinghe, 2020a), question answering (Yang et al., 2019) etc. We pass the sentence through the transformer model and add a softmax layer on top of the [CLS] token as a normal classification architecture with transformers (Ranasinghe and Hettiarachchi, 2020). We fine-tune all the parameters from transformer model as well as the softmax layer jointly by maximising the log-probability of the correct label. This architecture has been used widely in many text classification tasks (Ranasinghe et al., 2019; Ranasinghe and Zampieri, 2020) that includes Malayalam code-mix texts too (Ranasinghe et al., 2020). We did not perform any preprocessing to this architecture. Considering the pretrained transformer models that supports Kannada, Malayalam and Tamil we used multilingual bert (Devlin et al., 2019) and XLM-R large (Conneau et al., 2019) models. We divided the dataset into a training set and a validation set using 0.8:0.2 split on the data set. We predominantly fine tuned the learning rate and number of epochs of the classification model manually to obtain the best results for the validation set. We obtained 1e− 5 as the best"
2021.vardial-1.14,2021.naacl-demos.17,1,0.485495,"Missing"
C16-2021,C14-2028,0,0.0270396,"t post-editing MT output increases translators’ productivity and improves translation consistency (Guerberof, 2009; Plitt and Masselot, 2010; Zampieri and Vela, 2014). Alongside classical TM matches, computer-aided translation (CAT) Tools that integrate MT and TM output are a trend in the translation and localization industries providing translators more useful suggestions. Another important trend is the development of web-based CAT tools which require no local software installation and allow teams of translators to work on the same project simultaneously (e.g., WordFast Anywhere1 , MateCat2 (Federico et al., 2014), and Wordbee3 , Lilt4 etc.). This paper presents CATaLog Online, a web-based CAT tool that provides translators MT, TM and APE output and ensures data capture for APE development and translation process research. The MT and APE systems integrated in CATaLog Online are based on Pal et al. (2015) and Pal et al. (2016b), respectively. In this paper, we present the key features implemented in CATaLog Online and their importance to translation project managers, translators, and MT and APE developers. Compared to state-ofthe-art CAT tools (e.g., MateCat, Lilt) CATaLog Online offers the following ad"
C16-2021,W15-5206,1,0.763618,"Missing"
C16-2021,W15-3017,1,0.660709,"Missing"
C16-2021,L16-1095,1,0.88264,"Missing"
C16-2021,W16-2379,1,0.862937,"Missing"
C16-2021,W14-0314,1,0.843535,"th current state-of-the-art CAT tools, CATaLog Online provides an enhanced interface, an option to integrate APE and more informative logs to help translation process research. 1 Introduction Machine translation (MT) technology has improved substantially over the past few decades. MT output is no longer used just for gisting but also for post-editing by professional translators as an important part of the translation workflow. Several studies confirm that post-editing MT output increases translators’ productivity and improves translation consistency (Guerberof, 2009; Plitt and Masselot, 2010; Zampieri and Vela, 2014). Alongside classical TM matches, computer-aided translation (CAT) Tools that integrate MT and TM output are a trend in the translation and localization industries providing translators more useful suggestions. Another important trend is the development of web-based CAT tools which require no local software installation and allow teams of translators to work on the same project simultaneously (e.g., WordFast Anywhere1 , MateCat2 (Federico et al., 2014), and Wordbee3 , Lilt4 etc.). This paper presents CATaLog Online, a web-based CAT tool that provides translators MT, TM and APE output and ensur"
E14-4004,R13-1018,1,0.743466,"; Abe and Tsumoto, 2010; Kumar et al., 2011) and a few use external linguistic knowledge (Kanhabua and Nørv˚ag, 2009). A couple of approaches try to classify texts not only regarding the time span in which the texts were written, but also their geographical location such as (Mokhov, 2010) for French and, more recently, (Trieschnigg et al., 2012) for Dutch. At the word level, two studies aim to model and understand how word usage and meaning change over time (Wijaya and Yeniterzi, 2011), (Mihalcea and Nastase, 2012). The most recent studies in temporal text classification to our knowledge are (Ciobanu et al., 2013) ˇ for Romanian using lexical features and (Stajner and Zampieri, 2013) for Portuguese using stylistic and readability features. 17 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 17–21, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics 3 Methods 3.1 linear model. The method is to convert a dataset of the form D = {(x, y) : x ∈ Rd , y ∈ Y} into a pairwise dataset: Corpora To evaluate the method proposed here we used three historical corpora. An English historical corpus entitled Corpus of La"
E14-4004,W06-0903,0,0.171854,"and NLP in general; historical linguists and philologists who investigate language change; and finally scholars in the digital humanities who often deal with historical One of the first studies to model temporal information for the automatic dating of documents is the work of de Jong et al. (2005). In these experiments, authors used unigram language models to classify Dutch texts spanning from January 1999 to February 2005 using normalised log-likelihood ratio (NLLR) (Kraaij, 2004). As to the features used, a number of approaches proposed to automatic date take into account lexical features (Dalli and Wilks, 2006; Abe and Tsumoto, 2010; Kumar et al., 2011) and a few use external linguistic knowledge (Kanhabua and Nørv˚ag, 2009). A couple of approaches try to classify texts not only regarding the time span in which the texts were written, but also their geographical location such as (Mokhov, 2010) for French and, more recently, (Trieschnigg et al., 2012) for Dutch. At the word level, two studies aim to model and understand how word usage and meaning change over time (Wijaya and Yeniterzi, 2011), (Mihalcea and Nastase, 2012). The most recent studies in temporal text classification to our knowledge are ("
E14-4004,P12-2051,0,0.108696,"a number of approaches proposed to automatic date take into account lexical features (Dalli and Wilks, 2006; Abe and Tsumoto, 2010; Kumar et al., 2011) and a few use external linguistic knowledge (Kanhabua and Nørv˚ag, 2009). A couple of approaches try to classify texts not only regarding the time span in which the texts were written, but also their geographical location such as (Mokhov, 2010) for French and, more recently, (Trieschnigg et al., 2012) for Dutch. At the word level, two studies aim to model and understand how word usage and meaning change over time (Wijaya and Yeniterzi, 2011), (Mihalcea and Nastase, 2012). The most recent studies in temporal text classification to our knowledge are (Ciobanu et al., 2013) ˇ for Romanian using lexical features and (Stajner and Zampieri, 2013) for Portuguese using stylistic and readability features. 17 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 17–21, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics 3 Methods 3.1 linear model. The method is to convert a dataset of the form D = {(x, y) : x ∈ Rd , y ∈ Y} into a pairwise dataset: Corpora To evaluate the meth"
F13-2010,J93-1003,0,0.14999,"Missing"
F13-2010,Y08-1042,0,0.0359643,"Missing"
F13-2010,P12-3005,0,0.0632582,"Missing"
L16-1095,2013.mtsummit-wptp.13,0,0.0502028,"of the translation workflow. In recent years, commercial computer-aided translation (CAT) tools have started to provide not only the popular translation memory (TM) matches but also MT segments to be post-edited by translators. The use of MT output for post-editing is regarded to increase translator’s productivity and also to improve consistency in translation (Federico et al., 2012; Zampieri and Vela, 2014). In light of this, a recent trend in the field is to develop tools that integrate both MT and TM output providing translators a larger number of more useful and more accurate suggestions (Cettolo et al., 2013). Contributing in this direction, this paper presents a new web-based CAT tool called CATaLog online1 developed based on CATaLog, a recently-released desktop CAT tool (Nayek et al., 2015). The tool can be used to post-edit MT output as well as TM segments. CATaLog online records a wide range of logs that are not available in any commercial CAT tool making it a useful tool for project management and translation process research. We have observed a substantial increase in the number of online CAT tools available, both for commercial and non-commercial purposes. This includes tools such as WordFa"
L16-1095,2012.amta-papers.22,0,0.0455595,"lation process and translator’s productivity. Keywords: post-editing, machine translation, translation memories 1. Introduction With the improvement of machine translation (MT) software, post-editing tools have become an important part of the translation workflow. In recent years, commercial computer-aided translation (CAT) tools have started to provide not only the popular translation memory (TM) matches but also MT segments to be post-edited by translators. The use of MT output for post-editing is regarded to increase translator’s productivity and also to improve consistency in translation (Federico et al., 2012; Zampieri and Vela, 2014). In light of this, a recent trend in the field is to develop tools that integrate both MT and TM output providing translators a larger number of more useful and more accurate suggestions (Cettolo et al., 2013). Contributing in this direction, this paper presents a new web-based CAT tool called CATaLog online1 developed based on CATaLog, a recently-released desktop CAT tool (Nayek et al., 2015). The tool can be used to post-edit MT output as well as TM segments. CATaLog online records a wide range of logs that are not available in any commercial CAT tool making it a u"
L16-1095,C14-2028,0,0.467888,"rection, this paper presents a new web-based CAT tool called CATaLog online1 developed based on CATaLog, a recently-released desktop CAT tool (Nayek et al., 2015). The tool can be used to post-edit MT output as well as TM segments. CATaLog online records a wide range of logs that are not available in any commercial CAT tool making it a useful tool for project management and translation process research. We have observed a substantial increase in the number of online CAT tools available, both for commercial and non-commercial purposes. This includes tools such as WordFast Anywhere2 , MateCat3 (Federico et al., 2014), Wordbee4 , and many others. In our opinion, this is a trend in the translation industry and it motivated us to release CATaLog online. Online CAT tools have a number of advantages over desktop tools, most notably: they do not require local installation; they can be used from any computer; projects can be easily shared with multiple translators; project managers can track the progress of projects on the fly. This paper presents CATaLog online and summarizes the 1 The tool is available online. For more information, consult the following URL: http://ttg.uni-saarland.de/software/catalog 2 https:"
L16-1095,2014.eamt-1.2,0,0.038703,"Missing"
L16-1095,W15-4905,1,0.897711,"Missing"
L16-1095,P10-1064,1,0.894586,"Missing"
L16-1095,2010.jec-1.3,0,0.0197391,"f translated segments contained in the TM; 2) the quality of the TM matching and retrieval engine. To improve the latter, developers have been working on incorporating semantic knowledge to TMs by providing paraphrasing (Utiyama et al., 2011; Gupta and Or˘asan, 2014; Gupta et al., 2015), as well as incorporating syntactic information (Clark, 2002; Gotti et al., 2005; Vanallemeersch and Vandeghinste, 2014). To increase the number of suggestions presented to translators, a recent trend in state-of-the-art CAT tools is the aforementioned integration of TM segments and MT output (He et al., 2010; Kanavos and Kartsaklis, 2010). With the improvement of state-of-the-art MT systems, MT output is no 599 longer considered to be suitable just for gisting purposes and it has been used in real-world translation projects as well. CAT tools such as MateCat present MT output along segments retrieved from TMs in the list of suitable suggestions (Cettolo et al., 2013; Federico et al., 2014). Substantial work has been carried out on improving translation recommendation systems which recommends posteditors either to use TM output or MT output (He et al., 2010). To optimize performance these systems use classifier trained to predi"
L16-1095,2010.jec-1.4,0,0.0200997,"t al., 2010). To optimize performance these systems use classifier trained to predict which output (TM or MT) requires less effort to be used for post-editing. Work on integrating MT with TM has also been done to make TM output more suitable for post-editing aiming to diminishing translators’ effort (Kanavos and Kartsaklis, 2010). Simard and Isabelle (2009) present the integration of Phrase-based Statistical MT (PB-SMT) with translation memories in a computer-aided translation environment in which the PB-SMT system exploits the most similar matches by making use of TM-based feature functions. Koehn and Senellart (2010) present another MT-TM integration strategy. In this study an Statistical MT (SMT) system is used to fill in the gaps in retrieved TM segments. 3. The Tool CATaLog online is a language independent tool that enables users to upload their own translation memories on the platform of the tool. It provides three major functionalities: • It provides a novel and user-friendly online CAT environment to post-editors and translators to reduce postediting time and effort, as displayed in Figure 1. • It collects post-editing logs which are a fundamental source of information for the translation process re"
L16-1095,P07-2045,0,0.00699121,"Missing"
L16-1095,J10-4005,0,0.0159378,"to upload the translations produced by third-party MT systems. A new feature in both CATaLog and CATaLog online is the ranking of matched TM segments based on their similarity given by Translation Error Rate (TER) (Snover et al., 2006). The system finds the matched and unmatched parts between the input segment and the five most similar TM segments from the TER alignment. It also finds out the correspondences between the source and target tokens in the matched TM segments and their corresponding translations using GIZA++ (Och and Ney, 2003) word alignments with grow-diag-final-and heuristics (Koehn, 2010). Matched parts and unmatched parts, both in the source and the target text, are colourcoded for better visualisation and displayed in green and red respectively. CATaLog online provides facilities to translate either single sentences or in batch mode i.e., by uploading a file. As shown in Figure 1, for a given input sentence (English in this case), the current version of CATaLog online provides two alternative translation suggestions in the target language (German in this case): MT and TM. The TM suggestion is colour-coded. When the translator selects the colour-coded TM alternative (c.f., Fi"
L16-1095,2008.amta-srw.4,0,0.0371658,"ion 2 presents related studies focusing on the integration between TM matches and MT output to improve CAT tools; Section 3 describes in detail the main functions of CATaLog online; Section 4 presents the language pairs and data that are currently included in CATaLog online; Section 5 discusses the main functions of CATaLog online and their importance for translators, researchers and project managements; finally, Section 6 concludes this paper and presents avenues for future research. 2. Related Work CAT tools are regarded to increase translator’s productivity and improve translation quality (Lagoudaki, 2008). The core component of most commercial CAT tools are translation memories. TMs work under the assumption that previously translated segments are likely to be good examples for new translations. This is particularly true when translating documents from the same domain which share a similar structure and/or vocabulary. Two important aspects should be considered when working with TMs: 1) the quality and number of translated segments contained in the TM; 2) the quality of the TM matching and retrieval engine. To improve the latter, developers have been working on incorporating semantic knowledge"
L16-1095,W15-5206,1,0.787357,"Missing"
L16-1095,J03-1002,0,0.0059723,"e background MT system (Pal et al., 2015a) integrated in the CAT tool or to upload the translations produced by third-party MT systems. A new feature in both CATaLog and CATaLog online is the ranking of matched TM segments based on their similarity given by Translation Error Rate (TER) (Snover et al., 2006). The system finds the matched and unmatched parts between the input segment and the five most similar TM segments from the TER alignment. It also finds out the correspondences between the source and target tokens in the matched TM segments and their corresponding translations using GIZA++ (Och and Ney, 2003) word alignments with grow-diag-final-and heuristics (Koehn, 2010). Matched parts and unmatched parts, both in the source and the target text, are colourcoded for better visualisation and displayed in green and red respectively. CATaLog online provides facilities to translate either single sentences or in batch mode i.e., by uploading a file. As shown in Figure 1, for a given input sentence (English in this case), the current version of CATaLog online provides two alternative translation suggestions in the target language (German in this case): MT and TM. The TM suggestion is colour-coded. Whe"
L16-1095,W15-3017,1,0.667715,"Missing"
L16-1095,W15-3026,1,0.840277,"Missing"
L16-1095,W15-4916,1,0.889319,"Missing"
L16-1095,2009.mtsummit-papers.14,0,0.0528677,"egments retrieved from TMs in the list of suitable suggestions (Cettolo et al., 2013; Federico et al., 2014). Substantial work has been carried out on improving translation recommendation systems which recommends posteditors either to use TM output or MT output (He et al., 2010). To optimize performance these systems use classifier trained to predict which output (TM or MT) requires less effort to be used for post-editing. Work on integrating MT with TM has also been done to make TM output more suitable for post-editing aiming to diminishing translators’ effort (Kanavos and Kartsaklis, 2010). Simard and Isabelle (2009) present the integration of Phrase-based Statistical MT (PB-SMT) with translation memories in a computer-aided translation environment in which the PB-SMT system exploits the most similar matches by making use of TM-based feature functions. Koehn and Senellart (2010) present another MT-TM integration strategy. In this study an Statistical MT (SMT) system is used to fill in the gaps in retrieved TM segments. 3. The Tool CATaLog online is a language independent tool that enables users to upload their own translation memories on the platform of the tool. It provides three major functionalities: •"
L16-1095,2006.amta-papers.25,0,0.0723154,"y to compare various translation engines taking human evaluation into account. A more detailed description of these functionalities is given in the following sections. 3.1. A Novel CAT Environment In CATaLog online, users can choose between MT output and TM segments. The tool allows the user to choose either the background MT system (Pal et al., 2015a) integrated in the CAT tool or to upload the translations produced by third-party MT systems. A new feature in both CATaLog and CATaLog online is the ranking of matched TM segments based on their similarity given by Translation Error Rate (TER) (Snover et al., 2006). The system finds the matched and unmatched parts between the input segment and the five most similar TM segments from the TER alignment. It also finds out the correspondences between the source and target tokens in the matched TM segments and their corresponding translations using GIZA++ (Och and Ney, 2003) word alignments with grow-diag-final-and heuristics (Koehn, 2010). Matched parts and unmatched parts, both in the source and the target text, are colourcoded for better visualisation and displayed in green and red respectively. CATaLog online provides facilities to translate either single"
L16-1095,W14-3323,1,0.897115,"Missing"
L16-1095,2011.mtsummit-papers.37,0,0.027697,"rcial CAT tools are translation memories. TMs work under the assumption that previously translated segments are likely to be good examples for new translations. This is particularly true when translating documents from the same domain which share a similar structure and/or vocabulary. Two important aspects should be considered when working with TMs: 1) the quality and number of translated segments contained in the TM; 2) the quality of the TM matching and retrieval engine. To improve the latter, developers have been working on incorporating semantic knowledge to TMs by providing paraphrasing (Utiyama et al., 2011; Gupta and Or˘asan, 2014; Gupta et al., 2015), as well as incorporating syntactic information (Clark, 2002; Gotti et al., 2005; Vanallemeersch and Vandeghinste, 2014). To increase the number of suggestions presented to translators, a recent trend in state-of-the-art CAT tools is the aforementioned integration of TM segments and MT output (He et al., 2010; Kanavos and Kartsaklis, 2010). With the improvement of state-of-the-art MT systems, MT output is no 599 longer considered to be suitable just for gisting purposes and it has been used in real-world translation projects as well. CAT tools suc"
L16-1095,2014.tc-1.11,0,0.0129636,"anslations. This is particularly true when translating documents from the same domain which share a similar structure and/or vocabulary. Two important aspects should be considered when working with TMs: 1) the quality and number of translated segments contained in the TM; 2) the quality of the TM matching and retrieval engine. To improve the latter, developers have been working on incorporating semantic knowledge to TMs by providing paraphrasing (Utiyama et al., 2011; Gupta and Or˘asan, 2014; Gupta et al., 2015), as well as incorporating syntactic information (Clark, 2002; Gotti et al., 2005; Vanallemeersch and Vandeghinste, 2014). To increase the number of suggestions presented to translators, a recent trend in state-of-the-art CAT tools is the aforementioned integration of TM segments and MT output (He et al., 2010; Kanavos and Kartsaklis, 2010). With the improvement of state-of-the-art MT systems, MT output is no 599 longer considered to be suitable just for gisting purposes and it has been used in real-world translation projects as well. CAT tools such as MateCat present MT output along segments retrieved from TMs in the list of suitable suggestions (Cettolo et al., 2013; Federico et al., 2014). Substantial work ha"
L16-1095,W14-0314,1,0.846882,"slator’s productivity. Keywords: post-editing, machine translation, translation memories 1. Introduction With the improvement of machine translation (MT) software, post-editing tools have become an important part of the translation workflow. In recent years, commercial computer-aided translation (CAT) tools have started to provide not only the popular translation memory (TM) matches but also MT segments to be post-edited by translators. The use of MT output for post-editing is regarded to increase translator’s productivity and also to improve consistency in translation (Federico et al., 2012; Zampieri and Vela, 2014). In light of this, a recent trend in the field is to develop tools that integrate both MT and TM output providing translators a larger number of more useful and more accurate suggestions (Cettolo et al., 2013). Contributing in this direction, this paper presents a new web-based CAT tool called CATaLog online1 developed based on CATaLog, a recently-released desktop CAT tool (Nayek et al., 2015). The tool can be used to post-edit MT output as well as TM segments. CATaLog online records a wide range of logs that are not available in any commercial CAT tool making it a useful tool for project man"
L16-1095,2015.eamt-1.17,1,\N,Missing
L16-1095,2015.eamt-1.6,1,\N,Missing
L16-1284,W15-5412,0,0.0342684,"Missing"
L16-1284,W15-5410,0,0.477289,"e were many cases of republication (e.g. British texts republished by an American newspaper and tagged as American by the original sources) that made the task for this language group unfeasible.(Zampieri et al., 2014) Closed Open MAC MMS NRC SUKI BOBICEV BRUNIBP PRHLT INRIA NLEL OSEVAL 95.54 95.24 95.24 94.67 94.14 93.66 92.74 83.91 64.04 - 95.65 91.84 76.17 MAC SUKI NRC MMS BOBICEV PRHLT NLEL OSEVAL 94.01 93.02 93.01 92.78 92.22 90.80 62.78 - 93.41 89.56 75.30 System Description Test Set A (Malmasi and Dras, 2015b) (Zampieri et al., 2015a) (Goutte and L´eger, 2015) (Jauhiainen et al., 2015) (Bobicev, 2015) ´ et al., 2015) (Acs (Franco-Salvador et al., 2015) (Fabra-Boluda et al., 2015) Test Set B (Malmasi and Dras, 2015b) (Jauhiainen et al., 2015) (Goutte and L´eger, 2015) (Zampieri et al., 2015a) (Bobicev, 2015) (Franco-Salvador et al., 2015) (Fabra-Boluda et al., 2015) - Table 2: DSL Shared Task 2015 - Accuracy results for open and closed submissions using test sets A and B. Teams ranked by their results in the closed submission. We observe that for all systems, performance dropped from test set A (complete texts) to test set B (name entities removed). This was the expected outcome. However, p"
L16-1284,D14-1069,0,0.0549297,"Zampieri et al., 2015b). The analysis and results of this paper complement the information presented in the shared task reports and provide novel and important information for researchers and developers interested in language identification and particularly in the problem of discriminating between similar languages. 2. Related Work Language identification in written texts is a wellestablished research topic in computational linguistics. Interest in the task is evidenced by early n-gram-based approaches (Dunning, 1994; Grefenstette, 1995) to more recent studies (Brown, 2013; Lui et al., 2014a; Brown, 2014; Sim˜oes et al., 2014). The interest in the discrimination of similar languages, language varieties, and dialects is more recent but it has been growing in the past few years. Examples of studies include the cases of Malay and Indonesian (Ranaivo-Malanc¸on, 2006), Chinese varieties (Huang and Lee, 2008), South Slavic languages (Ljubeˇsi´c et al., 2007; Ljubeˇsi´c and Kranjˇci´c, 2015), Portuguese varieties (Zampieri and Gebre, 2012), Spanish varieties (Zampieri et al., 2013; Maier and G´omez-Rodrıguez, 2014), English varieties (Lui and Cook, 2013), Persian and Dari (Malmasi and Dras, 2015a),"
L16-1284,L16-1522,0,0.0379652,"014). The interest in the discrimination of similar languages, language varieties, and dialects is more recent but it has been growing in the past few years. Examples of studies include the cases of Malay and Indonesian (Ranaivo-Malanc¸on, 2006), Chinese varieties (Huang and Lee, 2008), South Slavic languages (Ljubeˇsi´c et al., 2007; Ljubeˇsi´c and Kranjˇci´c, 2015), Portuguese varieties (Zampieri and Gebre, 2012), Spanish varieties (Zampieri et al., 2013; Maier and G´omez-Rodrıguez, 2014), English varieties (Lui and Cook, 2013), Persian and Dari (Malmasi and Dras, 2015a), Romanian dialects (Ciobanu and Dinu, 2016), and a number of studies on Arabic dialects (Elfardy and Diab, 2014; Zaidan and Callison-Burch, 2014; Tillmann et al., 2014; Malmasi et al., 2015a) A number of shared tasks on language identification have been organized in the recent years ranging from generalpurpose language identification (Baldwin and Lui, 2010) to more specific challenges such as the TweetLID shared task which focused on Twitter data (Zubiaga et al., 2014; Zubiaga et al., 2015), the shared task on Language Identification in Code-Switched Data (Solorio et al., 2014), and the two editions of the discriminating between simila"
L16-1284,W15-5409,0,0.249914,"y an American newspaper and tagged as American by the original sources) that made the task for this language group unfeasible.(Zampieri et al., 2014) Closed Open MAC MMS NRC SUKI BOBICEV BRUNIBP PRHLT INRIA NLEL OSEVAL 95.54 95.24 95.24 94.67 94.14 93.66 92.74 83.91 64.04 - 95.65 91.84 76.17 MAC SUKI NRC MMS BOBICEV PRHLT NLEL OSEVAL 94.01 93.02 93.01 92.78 92.22 90.80 62.78 - 93.41 89.56 75.30 System Description Test Set A (Malmasi and Dras, 2015b) (Zampieri et al., 2015a) (Goutte and L´eger, 2015) (Jauhiainen et al., 2015) (Bobicev, 2015) ´ et al., 2015) (Acs (Franco-Salvador et al., 2015) (Fabra-Boluda et al., 2015) Test Set B (Malmasi and Dras, 2015b) (Jauhiainen et al., 2015) (Goutte and L´eger, 2015) (Zampieri et al., 2015a) (Bobicev, 2015) (Franco-Salvador et al., 2015) (Fabra-Boluda et al., 2015) - Table 2: DSL Shared Task 2015 - Accuracy results for open and closed submissions using test sets A and B. Teams ranked by their results in the closed submission. We observe that for all systems, performance dropped from test set A (complete texts) to test set B (name entities removed). This was the expected outcome. However, performance is, in most cases, only 1 or 2 percentage points lower which is not a"
L16-1284,W15-5403,0,0.727991,".g. British texts republished by an American newspaper and tagged as American by the original sources) that made the task for this language group unfeasible.(Zampieri et al., 2014) Closed Open MAC MMS NRC SUKI BOBICEV BRUNIBP PRHLT INRIA NLEL OSEVAL 95.54 95.24 95.24 94.67 94.14 93.66 92.74 83.91 64.04 - 95.65 91.84 76.17 MAC SUKI NRC MMS BOBICEV PRHLT NLEL OSEVAL 94.01 93.02 93.01 92.78 92.22 90.80 62.78 - 93.41 89.56 75.30 System Description Test Set A (Malmasi and Dras, 2015b) (Zampieri et al., 2015a) (Goutte and L´eger, 2015) (Jauhiainen et al., 2015) (Bobicev, 2015) ´ et al., 2015) (Acs (Franco-Salvador et al., 2015) (Fabra-Boluda et al., 2015) Test Set B (Malmasi and Dras, 2015b) (Jauhiainen et al., 2015) (Goutte and L´eger, 2015) (Zampieri et al., 2015a) (Bobicev, 2015) (Franco-Salvador et al., 2015) (Fabra-Boluda et al., 2015) - Table 2: DSL Shared Task 2015 - Accuracy results for open and closed submissions using test sets A and B. Teams ranked by their results in the closed submission. We observe that for all systems, performance dropped from test set A (complete texts) to test set B (name entities removed). This was the expected outcome. However, performance is, in most cases, only 1 or 2 percentage"
L16-1284,W13-1728,1,0.85986,"A and 4 In the 2015 edition, organizers did not use language group names as in the 2014 edition. We use them for both editions in this paper for the sake of clarity and consistency. 1801 B was MAC (Malmasi and Dras, 2015b) which proposed an ensemble of SVM classifiers for this task. Two other SVMbased approaches were tied in 2nd for test set A, one by the NRC team (Goutte and L´eger, 2015) and MMS (Zampieri et al., 2015a), which experimented with three different approaches and obtained the best results combining TF-IDF and an SVM classifier previously used for native language identification (Gebre et al., 2013). The NRC team included members of NRC-CNRC, winners of the DSL closed submission track in 2014. Both in 2014 and in 2015 they used a two-stage classification approach to predict first the language group and then the language within the predicted group. Two other teams used two-stage classification approaches: NLEL (Fabra-Boluda et al., 2015) and BRUniBP ´ et al., 2015). (Acs A number of computational techniques have been explored in the DSL 2015 including token-based backoff by SUKI team (Jauhiainen et al., 2015), prediction by partial matching (PPM) by BOBICEV (Bobicev, 2015), and word and s"
L16-1284,W15-5413,1,0.877349,"Missing"
L16-1284,W14-5316,1,0.633192,"Missing"
L16-1284,Y08-1042,0,0.049537,"en similar languages. 2. Related Work Language identification in written texts is a wellestablished research topic in computational linguistics. Interest in the task is evidenced by early n-gram-based approaches (Dunning, 1994; Grefenstette, 1995) to more recent studies (Brown, 2013; Lui et al., 2014a; Brown, 2014; Sim˜oes et al., 2014). The interest in the discrimination of similar languages, language varieties, and dialects is more recent but it has been growing in the past few years. Examples of studies include the cases of Malay and Indonesian (Ranaivo-Malanc¸on, 2006), Chinese varieties (Huang and Lee, 2008), South Slavic languages (Ljubeˇsi´c et al., 2007; Ljubeˇsi´c and Kranjˇci´c, 2015), Portuguese varieties (Zampieri and Gebre, 2012), Spanish varieties (Zampieri et al., 2013; Maier and G´omez-Rodrıguez, 2014), English varieties (Lui and Cook, 2013), Persian and Dari (Malmasi and Dras, 2015a), Romanian dialects (Ciobanu and Dinu, 2016), and a number of studies on Arabic dialects (Elfardy and Diab, 2014; Zaidan and Callison-Burch, 2014; Tillmann et al., 2014; Malmasi et al., 2015a) A number of shared tasks on language identification have been organized in the recent years ranging from generalpu"
L16-1284,W15-5408,0,0.33486,"Missing"
L16-1284,W14-5317,0,0.112771,"Missing"
L16-1284,U13-1003,0,0.421039,"more recent studies (Brown, 2013; Lui et al., 2014a; Brown, 2014; Sim˜oes et al., 2014). The interest in the discrimination of similar languages, language varieties, and dialects is more recent but it has been growing in the past few years. Examples of studies include the cases of Malay and Indonesian (Ranaivo-Malanc¸on, 2006), Chinese varieties (Huang and Lee, 2008), South Slavic languages (Ljubeˇsi´c et al., 2007; Ljubeˇsi´c and Kranjˇci´c, 2015), Portuguese varieties (Zampieri and Gebre, 2012), Spanish varieties (Zampieri et al., 2013; Maier and G´omez-Rodrıguez, 2014), English varieties (Lui and Cook, 2013), Persian and Dari (Malmasi and Dras, 2015a), Romanian dialects (Ciobanu and Dinu, 2016), and a number of studies on Arabic dialects (Elfardy and Diab, 2014; Zaidan and Callison-Burch, 2014; Tillmann et al., 2014; Malmasi et al., 2015a) A number of shared tasks on language identification have been organized in the recent years ranging from generalpurpose language identification (Baldwin and Lui, 2010) to more specific challenges such as the TweetLID shared task which focused on Twitter data (Zubiaga et al., 2014; Zubiaga et al., 2015), the shared task on Language Identification in Code-Switche"
L16-1284,Q14-1003,0,0.14435,"ieri et al., 2014; Zampieri et al., 2015b). The analysis and results of this paper complement the information presented in the shared task reports and provide novel and important information for researchers and developers interested in language identification and particularly in the problem of discriminating between similar languages. 2. Related Work Language identification in written texts is a wellestablished research topic in computational linguistics. Interest in the task is evidenced by early n-gram-based approaches (Dunning, 1994; Grefenstette, 1995) to more recent studies (Brown, 2013; Lui et al., 2014a; Brown, 2014; Sim˜oes et al., 2014). The interest in the discrimination of similar languages, language varieties, and dialects is more recent but it has been growing in the past few years. Examples of studies include the cases of Malay and Indonesian (Ranaivo-Malanc¸on, 2006), Chinese varieties (Huang and Lee, 2008), South Slavic languages (Ljubeˇsi´c et al., 2007; Ljubeˇsi´c and Kranjˇci´c, 2015), Portuguese varieties (Zampieri and Gebre, 2012), Spanish varieties (Zampieri et al., 2013; Maier and G´omez-Rodrıguez, 2014), English varieties (Lui and Cook, 2013), Persian and Dari (Malmasi and"
L16-1284,W14-5315,0,0.117061,"ieri et al., 2014; Zampieri et al., 2015b). The analysis and results of this paper complement the information presented in the shared task reports and provide novel and important information for researchers and developers interested in language identification and particularly in the problem of discriminating between similar languages. 2. Related Work Language identification in written texts is a wellestablished research topic in computational linguistics. Interest in the task is evidenced by early n-gram-based approaches (Dunning, 1994; Grefenstette, 1995) to more recent studies (Brown, 2013; Lui et al., 2014a; Brown, 2014; Sim˜oes et al., 2014). The interest in the discrimination of similar languages, language varieties, and dialects is more recent but it has been growing in the past few years. Examples of studies include the cases of Malay and Indonesian (Ranaivo-Malanc¸on, 2006), Chinese varieties (Huang and Lee, 2008), South Slavic languages (Ljubeˇsi´c et al., 2007; Ljubeˇsi´c and Kranjˇci´c, 2015), Portuguese varieties (Zampieri and Gebre, 2012), Spanish varieties (Zampieri et al., 2013; Maier and G´omez-Rodrıguez, 2014), English varieties (Lui and Cook, 2013), Persian and Dari (Malmasi and"
L16-1284,W14-4204,0,0.229199,"Missing"
L16-1284,D14-1144,1,0.843128,"al., 2014). The dataset is entitled DSL Corpus Collection, or DSLCC, and it includes short excerpts from journalistic texts from previously released corpora and repository.2 Texts in the DSLCC v. 1.0 were written in thirteen languages or language varieties and divided into the following six groups: Group A (Bosnian, Croatian, Serbian), Group B (Indonesian, Malay), Group C (Czech, Slovak), Group D (Brazilian Portuguese, European Portuguese), Group E (Peninsu1 This task focuses on identifying the mother tongue of a learner writer based on stylistic cues; all the texts are in the same language (Malmasi and Dras, 2014; Malmasi and Dras, 2015c). 2 See Tan et al. (2014) for a complete list of sources. 1800 lar Spanish, Argentine Spanish), and Group F3 (American English, British English). In the 2014 edition, eight teams participated and submitted results to the DSL shared task (eight teams in the closed and two teams in the open submission). Five of these teams wrote system description papers. The complete shared task report is available in Zampieri et al. (2014). We summarize the results in Table 1 in terms of accuracy (best performing entries displayed in bold). Team NRC-CNRC RAE UMich UniMelb-NLP QMUL LIR"
L16-1284,W15-5407,1,0.445859,"anguage identification systems (Tiedemann and Ljubeˇsi´c, 2012). Closely-related languages such as Indonesian and Malay or Croatian and Serbian are very similar both at their spoken and at their written forms making it difficult for systems to discriminate between them. Varieties of the same language, e.g. Spanish from South America or Spain, are even more difficult to detect than similar languages. Nevertheless, in both cases, recent work has shown that it is possible to train algorithms to discriminate between similar languages and language varieties with high accuracy (Goutte et al., 2014; Malmasi and Dras, 2015b). This study looks in more detail into the features that help algorithms discriminating between similar languages, taking into account the results of two recent editions of the Discriminating between Similar Languages (DSL) shared task (Zampieri et al., 2014; Zampieri et al., 2015b). The analysis and results of this paper complement the information presented in the shared task reports and provide novel and important information for researchers and developers interested in language identification and particularly in the problem of discriminating between similar languages. 2. Related Work Lang"
L16-1284,W15-0620,1,0.852128,"w years. Examples of studies include the cases of Malay and Indonesian (Ranaivo-Malanc¸on, 2006), Chinese varieties (Huang and Lee, 2008), South Slavic languages (Ljubeˇsi´c et al., 2007; Ljubeˇsi´c and Kranjˇci´c, 2015), Portuguese varieties (Zampieri and Gebre, 2012), Spanish varieties (Zampieri et al., 2013; Maier and G´omez-Rodrıguez, 2014), English varieties (Lui and Cook, 2013), Persian and Dari (Malmasi and Dras, 2015a), Romanian dialects (Ciobanu and Dinu, 2016), and a number of studies on Arabic dialects (Elfardy and Diab, 2014; Zaidan and Callison-Burch, 2014; Tillmann et al., 2014; Malmasi et al., 2015a) A number of shared tasks on language identification have been organized in the recent years ranging from generalpurpose language identification (Baldwin and Lui, 2010) to more specific challenges such as the TweetLID shared task which focused on Twitter data (Zubiaga et al., 2014; Zubiaga et al., 2015), the shared task on Language Identification in Code-Switched Data (Solorio et al., 2014), and the two editions of the discriminating between similar languages (DSL) shared task. To our knowledge, however, no comprehensive analysis of the kind we are proposing in this paper has been carried ou"
L16-1284,W14-5314,0,0.212335,"Missing"
L16-1284,W14-3907,0,0.0354693,"n and Dari (Malmasi and Dras, 2015a), Romanian dialects (Ciobanu and Dinu, 2016), and a number of studies on Arabic dialects (Elfardy and Diab, 2014; Zaidan and Callison-Burch, 2014; Tillmann et al., 2014; Malmasi et al., 2015a) A number of shared tasks on language identification have been organized in the recent years ranging from generalpurpose language identification (Baldwin and Lui, 2010) to more specific challenges such as the TweetLID shared task which focused on Twitter data (Zubiaga et al., 2014; Zubiaga et al., 2015), the shared task on Language Identification in Code-Switched Data (Solorio et al., 2014), and the two editions of the discriminating between similar languages (DSL) shared task. To our knowledge, however, no comprehensive analysis of the kind we are proposing in this paper has been carried out on the results obtained in a language identification shared task and our work fills this gap. The most similar analysis was applied to Native Language Identification (NLI)1 using the 2013 NLI shared task dataset (Malmasi et al., 2015b). In the next sections we present the systems that participated in the two editions of the DSL shared task. 2.1. DSL Shared Task 2014 The first edition of the"
L16-1284,C12-1160,0,0.332039,"Missing"
L16-1284,W14-5313,0,0.149574,"growing in the past few years. Examples of studies include the cases of Malay and Indonesian (Ranaivo-Malanc¸on, 2006), Chinese varieties (Huang and Lee, 2008), South Slavic languages (Ljubeˇsi´c et al., 2007; Ljubeˇsi´c and Kranjˇci´c, 2015), Portuguese varieties (Zampieri and Gebre, 2012), Spanish varieties (Zampieri et al., 2013; Maier and G´omez-Rodrıguez, 2014), English varieties (Lui and Cook, 2013), Persian and Dari (Malmasi and Dras, 2015a), Romanian dialects (Ciobanu and Dinu, 2016), and a number of studies on Arabic dialects (Elfardy and Diab, 2014; Zaidan and Callison-Burch, 2014; Tillmann et al., 2014; Malmasi et al., 2015a) A number of shared tasks on language identification have been organized in the recent years ranging from generalpurpose language identification (Baldwin and Lui, 2010) to more specific challenges such as the TweetLID shared task which focused on Twitter data (Zubiaga et al., 2014; Zubiaga et al., 2015), the shared task on Language Identification in Code-Switched Data (Solorio et al., 2014), and the two editions of the discriminating between similar languages (DSL) shared task. To our knowledge, however, no comprehensive analysis of the kind we are proposing in this pap"
L16-1284,W14-5307,1,0.851072,"Missing"
L16-1284,W15-5411,1,0.885723,"Missing"
L16-1284,W15-5401,1,0.771913,"Missing"
L16-1647,P12-1011,0,0.0220187,"estion of representing time intervals for temporal text classification. Related Work Modeling temporal information in text is a relevant task to a number of NLP applications. Information Retrieval (IR) methods, for example, often have to process temporal information in both queries and documents to deal with dynamicity of the content found in data repositories and the Web (Dakka et al., 2012; Preotiuc-Pietro, 2014; Kanhabua et al., 2015; Zhao and Hauff, 2015b; Zhao and Hauff, 2015a). Time expressions (e.g. after 2010), can help algorithms to identify the approximate publication date of texts (Chambers, 2012), but there are a number of cases in which they are not present in text and one alternative is to use features related to language change as we propose in this paper. As will be evidenced in this section, even though there were a number of attempts to approach temporal text classification, to our knowledge this task was not substantially explored as other text classification tasks. The work by de Jong et al. (2005) uses unigram language models combined with smoothing techniques and log-likelihood ratio measure (NLLR) (Kraaij, 2004) to classify documents within different time spans. The method"
L16-1647,R13-1018,0,0.0129348,"investigate the most informative features for this task and how they are linked to language change. Keywords: Language Change, Temporal Text Classification, Support Vector Machines, Text Categorization 1. Introduction 2. It is well-known that language changes over time both in spoken and in written forms. Changes in written language can be manifested in many ways such as the use of the lexicon, grammatical structures, and textual stylistics. Recent studies have shown that it is possible to use language change to predict the approximate publication date of texts in diachronic text collections (Ciobanu et al., 2013a; Popescu and Strapparava, 2015). This task is called temporal text classification and to our knowledge it has not been substantially explored in the literature as other text classification tasks. This paper contributes in this direction. In this paper we investigate the use of supervised machine learning classifiers to predict when a text was written using lexical and morphosyntactic information. The classifiers were trained and tested on a sample of a historical Portuguese corpus called Colonia (Zampieri and Becker, 2013) which contains texts spanning from the 16th to the early 20th century"
L16-1647,W13-2714,0,0.0164411,"investigate the most informative features for this task and how they are linked to language change. Keywords: Language Change, Temporal Text Classification, Support Vector Machines, Text Categorization 1. Introduction 2. It is well-known that language changes over time both in spoken and in written forms. Changes in written language can be manifested in many ways such as the use of the lexicon, grammatical structures, and textual stylistics. Recent studies have shown that it is possible to use language change to predict the approximate publication date of texts in diachronic text collections (Ciobanu et al., 2013a; Popescu and Strapparava, 2015). This task is called temporal text classification and to our knowledge it has not been substantially explored in the literature as other text classification tasks. This paper contributes in this direction. In this paper we investigate the use of supervised machine learning classifiers to predict when a text was written using lexical and morphosyntactic information. The classifiers were trained and tested on a sample of a historical Portuguese corpus called Colonia (Zampieri and Becker, 2013) which contains texts spanning from the 16th to the early 20th century"
L16-1647,W06-0903,0,0.343169,"e a number of attempts to approach temporal text classification, to our knowledge this task was not substantially explored as other text classification tasks. The work by de Jong et al. (2005) uses unigram language models combined with smoothing techniques and log-likelihood ratio measure (NLLR) (Kraaij, 2004) to classify documents within different time spans. The method was tested on a collection of Dutch journalistic texts published from January 1999 to February 2005. Other methods, such as Kumar et al. (2011), make use of information gain to estimate the best features in classification. In Dalli and Wilks (2006) researchers train a classifier to predict the publication date of texts within a time span of nine years. The method uses words as features and it is aided by words which increase their frequency at some point of time, particularly named entities. Another study that works under a similar assumption is the one published by Abe and Tsumoto (2010). The authors proposed the use of similarity metrics to categorize texts based on keywords calculated using tf-idf (term frequency - inverse document frequency). Garcia-Fernandez et al. (2011) presents a method to predict the publication dates of excerp"
L16-1647,Q16-1003,0,0.0689138,"Missing"
L16-1647,E14-4019,1,0.882439,"tion of historical corpora. Although Colonia is to our knowledge the biggest Portuguese corpus of its kind, it does not contain many texts from each period. The number of documents varies between 13 from the 16th century and 38 from the 19th century. For text classification, however, the number of documents available (especially at the training stage) is very important to provide enough information to achieve high classification performance. To circumvent this limitation, in this paper we propose the use of composite documents made of sentences from various texts. Following the methodology of Malmasi and Dras (2014a), we randomly select and combine the sentences from the same class (time period) to generate artificial texts of approximately 330 tokens on average, creating a set of documents for training and testing. This methodology ensures that the texts for each class are a mix of different authorship styles and topics. It also means that all documents are similar and comparable in length making the task more challenging. Previous work in other text classification tasks has shown that longer texts can be easier to classify (Malmasi et al., 2015). In our experiments we model two dimensions of language"
L16-1647,D14-1144,1,0.86872,"tion of historical corpora. Although Colonia is to our knowledge the biggest Portuguese corpus of its kind, it does not contain many texts from each period. The number of documents varies between 13 from the 16th century and 38 from the 19th century. For text classification, however, the number of documents available (especially at the training stage) is very important to provide enough information to achieve high classification performance. To circumvent this limitation, in this paper we propose the use of composite documents made of sentences from various texts. Following the methodology of Malmasi and Dras (2014a), we randomly select and combine the sentences from the same class (time period) to generate artificial texts of approximately 330 tokens on average, creating a set of documents for training and testing. This methodology ensures that the texts for each class are a mix of different authorship styles and topics. It also means that all documents are similar and comparable in length making the task more challenging. Previous work in other text classification tasks has shown that longer texts can be easier to classify (Malmasi et al., 2015). In our experiments we model two dimensions of language"
L16-1647,P12-2051,0,0.075944,"Missing"
L16-1647,E14-4004,1,0.951879,"The authors concluded that the use of lexical features is the best source of information for this task. An important issue to take into account when working on temporal text classification is how to represent time. Most studies, including our own, model the task as supervised classification in which algorithms are trained to assign texts to an n number of classes. Each of these n classes represent an arbitrarily defined time interval, for example: a month, a year, or a decade. However, there have been a few attempts to approach this task without relying on predefined time spans. The study by Niculae et al. (2014) approached the task using ranking and pairwise comparisons to predict for each pair of documents which one is older and finally to produce a rank of all documents in a collection from older to newer. Another recent study to tackle the issue of time intervals is Efremova et al. (2015). In this study authors apply clustering methods to automatically obtain optimal time partitions in a dataset of historical Dutch notary acts. We return to this question in Section 6.1. of this paper. The style of texts also changes over time and it can be a good indicator to predict the publication date of a docˇ"
L16-1647,I13-1040,0,0.041697,"Missing"
L16-1647,S15-2147,0,0.336739,"formative features for this task and how they are linked to language change. Keywords: Language Change, Temporal Text Classification, Support Vector Machines, Text Categorization 1. Introduction 2. It is well-known that language changes over time both in spoken and in written forms. Changes in written language can be manifested in many ways such as the use of the lexicon, grammatical structures, and textual stylistics. Recent studies have shown that it is possible to use language change to predict the approximate publication date of texts in diachronic text collections (Ciobanu et al., 2013a; Popescu and Strapparava, 2015). This task is called temporal text classification and to our knowledge it has not been substantially explored in the literature as other text classification tasks. This paper contributes in this direction. In this paper we investigate the use of supervised machine learning classifiers to predict when a text was written using lexical and morphosyntactic information. The classifiers were trained and tested on a sample of a historical Portuguese corpus called Colonia (Zampieri and Becker, 2013) which contains texts spanning from the 16th to the early 20th century. The approach we propose here is"
L16-1647,S15-2142,0,0.0219181,"Missing"
L16-1647,S15-2148,0,0.327017,"Missing"
L16-1647,S15-2144,1,0.861705,"Missing"
L18-1392,ehrmann-etal-2014-representing,0,0.0183788,"ting multilingual data including (Gracia et al., 2014), which extends some of the lemon properties describing relationships among translations. Recently, multilingual data sets have been created such as DBnary (S´erasset, 2012), which was released with the 2468 main purpose of describing translations among lexical entries. Another resource that describes multilingual content is BabelNet (Navigli and Ponzetto, 2010), which integrates knowledge from various lexical resources, such as WordNet (Miller, 1995). Additionally, BabelNet has adopted the lemon structure for representing lexical entries (Ehrmann et al., 2014). Although these resources are linked lexical multilingual data sets, they contain a limited number of idioms described correctly along with their respective translations across languages. This lack of information about MWE and idioms is due to the missing appropriate ontologies and vocabularies for handling this phenomena properly. Despite Lexinfo ontology (Cimiano et al., 2011) contains a certain property just for representing idioms, there are no appropriate classes to reuse this information. Fortunately, the W3C Ontology Lexica Community Group2 has created an extension of lemon called Onto"
L18-1392,gracia-etal-2014-enabling,0,0.0548766,"Missing"
L18-1392,L16-1386,0,0.0409527,"Missing"
L18-1392,P10-1023,0,0.0500123,"multilingual way. Subsequently, a significant amount of effort has been invested in order to improve the support of multilingual contents. To this end, other modules have been extended from lemon for representing multilingual data including (Gracia et al., 2014), which extends some of the lemon properties describing relationships among translations. Recently, multilingual data sets have been created such as DBnary (S´erasset, 2012), which was released with the 2468 main purpose of describing translations among lexical entries. Another resource that describes multilingual content is BabelNet (Navigli and Ponzetto, 2010), which integrates knowledge from various lexical resources, such as WordNet (Miller, 1995). Additionally, BabelNet has adopted the lemon structure for representing lexical entries (Ehrmann et al., 2014). Although these resources are linked lexical multilingual data sets, they contain a limited number of idioms described correctly along with their respective translations across languages. This lack of information about MWE and idioms is due to the missing appropriate ontologies and vocabularies for handling this phenomena properly. Despite Lexinfo ontology (Cimiano et al., 2011) contains a cer"
L18-1392,zhang-etal-2014-xlid,0,0.0675554,"Missing"
L18-1481,D15-1230,0,0.0265517,"cially from RDF). The version of RDF2PT used in this paper, all experimental results and the texts generated for the experiments are publicly available.1 2. Related Work According to Staykova (2014) and Bouayad-Agha et al. (2014), there has been a plenty of works which investigated the generation of Natural Language (NL) texts from Semantic Web Technologies (SWT) as an input data. However, the subject of research has only recently gained significant momentum. This attention comes from the great number of published works such as (Cimiano et al., 2013; Duma and Klein, 2013; Ell and Harth, 2014; Biran and McKeown, 2015) which used RDF as an input data and achieved promising results. Also, the works published in the WebNLG (Colin et al., 2016) challenge, which used deep learning techniques such as (Sleimi and Gardent, 2016; Mrabet et al., 2016), also contributed to this interest. RDF has also been showing promising benefits to the generation of benchmarks for evaluating NLG systems (Gardent et al., 2017; Perez-Beltrachini et al., 2016; Mohammed et al., 2016; Schwitter et al., 2004; Hewlett et al., 2005; Sun and Mellish, 2006). Despite the plethora of works written on handling SWT data, only a few have exploit"
L18-1481,W11-2817,0,0.396248,"zilian Portuguese texts from RDF using a rule-based approach. We intend to exploit the application of ML models along with other Brazilian Portuguese resources15 to produce more fluent results and also to investigate the usage of ML classification algorithms to improve the choice of grammatical gender of words. Moreover, we aim to create a multilingual version of RDF2PT which will consist of French, German, Italian and Spanish. To this end, we will exploit the similarity among their syntaxes in the micro-planning task and we will reuse their respective SimpleNLG versions (Mazzei et al., 2016; Bollmann, 2011; Vaudry and Lapalme, 2013; Ramos-Soto et al., 2017) for the realization task. Acknowledgments This work has been supported by the H2020 project HOBBIT (GA no. 688227) and supported by the Brazilian National Council for Scientific and Technological Development (CNPq) (no. 206971/2014-1 and no. 203065/20140.). This research has also been supported by the German Federal Ministry of Transport and Digital Infrastructure (BMVI) in the projects LIMBO (no. 19F2029I), OPAL 14 http://gerbil.aksw.org/gerbil/ experiment?id=201801050040 and http://gerbil. aksw.org/gerbil/experiment?id=201801110012 15 Reso"
L18-1481,W13-2102,0,0.0383036,"Missing"
L18-1481,W16-6626,0,0.0769275,"r less consensus on what the input should be (Gatt and Krahmer, 2017). A large number of inputs have been taken for NLG systems, including images (Xu et al., 2015), numeric data (Gkatzia et al., 2014), semantic representations (Theune et al., 2001) and Semantic Web (SW) data (Ngonga Ngomo et al., 2013; Bouayad-Agha et al., 2014). Presently, the generation of natural language from SW, more precisely from RDF data, has gained substantial attention (Bouayad-Agha et al., 2014; Staykova, 2014). Some challenges have been proposed to investigate the quality of automatically generated texts from RDF (Colin et al., 2016). Moreover, RDF has demonstrated a promising ability to support the creation of NLG benchmarks (Gardent et al., 2017). However, English is the only language which has been widely targeted. Even though there are studies which explore the generation of content in languages other than English, to the best of our knowledge, no work has been proposed to generate texts in Brazilian Portuguese from RDF data. In this paper, we propose RDF2PT, a rule-based approach to verbalize RDF data to Brazilian Portuguese. While the exciting avenue of using deep learning techniques in NLG approaches (Gatt and Krah"
L18-1481,W10-1617,0,0.506771,"Missing"
L18-1481,novais-etal-2012-portuguese,0,0.0742304,"Missing"
L18-1481,W14-4412,0,0.456339,"Missing"
L18-1481,W13-0108,0,0.0303569,"ic generation of Brazilian Portuguese (especially from RDF). The version of RDF2PT used in this paper, all experimental results and the texts generated for the experiments are publicly available.1 2. Related Work According to Staykova (2014) and Bouayad-Agha et al. (2014), there has been a plenty of works which investigated the generation of Natural Language (NL) texts from Semantic Web Technologies (SWT) as an input data. However, the subject of research has only recently gained significant momentum. This attention comes from the great number of published works such as (Cimiano et al., 2013; Duma and Klein, 2013; Ell and Harth, 2014; Biran and McKeown, 2015) which used RDF as an input data and achieved promising results. Also, the works published in the WebNLG (Colin et al., 2016) challenge, which used deep learning techniques such as (Sleimi and Gardent, 2016; Mrabet et al., 2016), also contributed to this interest. RDF has also been showing promising benefits to the generation of benchmarks for evaluating NLG systems (Gardent et al., 2017; Perez-Beltrachini et al., 2016; Mohammed et al., 2016; Schwitter et al., 2004; Hewlett et al., 2005; Sun and Mellish, 2006). Despite the plethora of works writte"
L18-1481,W14-4405,0,0.0229913,"lian Portuguese (especially from RDF). The version of RDF2PT used in this paper, all experimental results and the texts generated for the experiments are publicly available.1 2. Related Work According to Staykova (2014) and Bouayad-Agha et al. (2014), there has been a plenty of works which investigated the generation of Natural Language (NL) texts from Semantic Web Technologies (SWT) as an input data. However, the subject of research has only recently gained significant momentum. This attention comes from the great number of published works such as (Cimiano et al., 2013; Duma and Klein, 2013; Ell and Harth, 2014; Biran and McKeown, 2015) which used RDF as an input data and achieved promising results. Also, the works published in the WebNLG (Colin et al., 2016) challenge, which used deep learning techniques such as (Sleimi and Gardent, 2016; Mrabet et al., 2016), also contributed to this interest. RDF has also been showing promising benefits to the generation of benchmarks for evaluating NLG systems (Gardent et al., 2017; Perez-Beltrachini et al., 2016; Mohammed et al., 2016; Schwitter et al., 2004; Hewlett et al., 2005; Sun and Mellish, 2006). Despite the plethora of works written on handling SWT dat"
L18-1481,P16-1054,1,0.936362,"step. Micro Planning This step is concerned with the planning of a sentence. It comprises three sub-tasks. Firstly, Sentence aggregation decides whether information will be presented individually or separately. Second, Lexicalization chooses the right words and phrases in natural language for expressing the semantics about the data. Third, Coreference generation (also known as Referring expression) is the task responsible for generating syntagms (references) to discourse entities, for example, whether the text should refer to an entity using a definite description, a pronoun or a proper noun (Ferreira et al., 2016). In the following, we describe the challenges behind the tasks entailed. Sentence aggregation This task is based on Ngonga Ngomo et al. (2013). It is divided into two phases, subject grouping and object grouping. Subject grouping collapses the predicates and objects of two triples if their subjects are the same. Object grouping collapses the subjects of two triples if the predicates and objects of the triples are the same. The common elements are usually subject noun phrases and verb phrases (verbs together with object noun phrases). In order to maximize the grouping effects, we additionally"
L18-1481,P17-1017,0,0.0757369,"NLG systems, including images (Xu et al., 2015), numeric data (Gkatzia et al., 2014), semantic representations (Theune et al., 2001) and Semantic Web (SW) data (Ngonga Ngomo et al., 2013; Bouayad-Agha et al., 2014). Presently, the generation of natural language from SW, more precisely from RDF data, has gained substantial attention (Bouayad-Agha et al., 2014; Staykova, 2014). Some challenges have been proposed to investigate the quality of automatically generated texts from RDF (Colin et al., 2016). Moreover, RDF has demonstrated a promising ability to support the creation of NLG benchmarks (Gardent et al., 2017). However, English is the only language which has been widely targeted. Even though there are studies which explore the generation of content in languages other than English, to the best of our knowledge, no work has been proposed to generate texts in Brazilian Portuguese from RDF data. In this paper, we propose RDF2PT, a rule-based approach to verbalize RDF data to Brazilian Portuguese. While the exciting avenue of using deep learning techniques in NLG approaches (Gatt and Krahmer, 2017) is open to this task and deep learning has already shown promising results for RDF data (Sleimi and Garden"
L18-1481,P14-1116,0,0.028301,"e to generate text which is similar to that generated by humans and can hence be easily understood. Keywords: natural language generation, verbalization, semantic web 1. Introduction Natural Language Generation (NLG) is the process of generating coherent natural language text from non-linguistic data (Reiter and Dale, 2000). Despite community agreement on the actual text and speech output of these systems, there is far less consensus on what the input should be (Gatt and Krahmer, 2017). A large number of inputs have been taken for NLG systems, including images (Xu et al., 2015), numeric data (Gkatzia et al., 2014), semantic representations (Theune et al., 2001) and Semantic Web (SW) data (Ngonga Ngomo et al., 2013; Bouayad-Agha et al., 2014). Presently, the generation of natural language from SW, more precisely from RDF data, has gained substantial attention (Bouayad-Agha et al., 2014; Staykova, 2014). Some challenges have been proposed to investigate the quality of automatically generated texts from RDF (Colin et al., 2016). Moreover, RDF has demonstrated a promising ability to support the creation of NLG benchmarks (Gardent et al., 2017). However, English is the only language which has been widely ta"
L18-1481,W16-6630,0,0.428786,"es for generating Brazilian Portuguese texts from RDF using a rule-based approach. We intend to exploit the application of ML models along with other Brazilian Portuguese resources15 to produce more fluent results and also to investigate the usage of ML classification algorithms to improve the choice of grammatical gender of words. Moreover, we aim to create a multilingual version of RDF2PT which will consist of French, German, Italian and Spanish. To this end, we will exploit the similarity among their syntaxes in the micro-planning task and we will reuse their respective SimpleNLG versions (Mazzei et al., 2016; Bollmann, 2011; Vaudry and Lapalme, 2013; Ramos-Soto et al., 2017) for the realization task. Acknowledgments This work has been supported by the H2020 project HOBBIT (GA no. 688227) and supported by the Brazilian National Council for Scientific and Technological Development (CNPq) (no. 206971/2014-1 and no. 203065/20140.). This research has also been supported by the German Federal Ministry of Transport and Digital Infrastructure (BMVI) in the projects LIMBO (no. 19F2029I), OPAL 14 http://gerbil.aksw.org/gerbil/ experiment?id=201801050040 and http://gerbil. aksw.org/gerbil/experiment?id=2018"
L18-1481,W16-6616,0,0.0146611,"icant momentum. This attention comes from the great number of published works such as (Cimiano et al., 2013; Duma and Klein, 2013; Ell and Harth, 2014; Biran and McKeown, 2015) which used RDF as an input data and achieved promising results. Also, the works published in the WebNLG (Colin et al., 2016) challenge, which used deep learning techniques such as (Sleimi and Gardent, 2016; Mrabet et al., 2016), also contributed to this interest. RDF has also been showing promising benefits to the generation of benchmarks for evaluating NLG systems (Gardent et al., 2017; Perez-Beltrachini et al., 2016; Mohammed et al., 2016; Schwitter et al., 2004; Hewlett et al., 2005; Sun and Mellish, 2006). Despite the plethora of works written on handling SWT data, only a few have exploited the generation of languages other than English, for instance, Keet and Khumalo (2017) to Zulu language. Additionally, a considerable number of NLG approaches can be found to European or Brazilian 3043 1 https://github.com/dice-group/RDF2PT Portuguese languages (Pereira and Paraboni, 2008; Cuevas and Paraboni, 2008; de Novais et al., 2009; de Novais et al., 2010; de Novais et al., 2012; de Novais and Paraboni, 2013; De Oliveira and Sripada"
L18-1481,W16-3506,0,0.161927,"dobj(BE2 , ρ(o2 )) (3) Note that pronoun(s) returns the correct pronoun for a resource based on its type and gender (see subsection 3.4.). Therewith, we can generate O local de nascimento de Albert Einstein Ulm e seu local de falecimento ´ e Princeton. (eng: Albert Einstein’s birthplace is Ulm and his death place is Princeton). In addition, in case the KB provides the ending date of a given resource through some predicate, for example dbo:deathDate, RDF2PT is able to lexicalize all the verbs in the past tense. 4. We based our evaluation methodology on Gardent et al. (2017) and Ferreira et al. (2016). Our main goal was to evaluate how well RDF2PT represents the information obtained from the data. We hence divided our evaluation set into expert and non-expert users. Both sets were made up of native speakers of Brazilian Portuguese. We selected six DBpedia categories like Gardent et al. (2017) for selecting the topic of texts. The categories were Astronaut, Scientist, Building, WrittenWork, City, and University. We detail below how we carried out both evaluation sets. Experts - We aimed to evaluate the adequacy and fluency of the generated texts from the perspective of experts. All experts"
L18-1481,C16-1141,0,0.0299554,"Missing"
L18-1481,W17-3521,0,0.155472,"Missing"
L18-1481,W16-3511,0,0.165347,"ent et al., 2017). However, English is the only language which has been widely targeted. Even though there are studies which explore the generation of content in languages other than English, to the best of our knowledge, no work has been proposed to generate texts in Brazilian Portuguese from RDF data. In this paper, we propose RDF2PT, a rule-based approach to verbalize RDF data to Brazilian Portuguese. While the exciting avenue of using deep learning techniques in NLG approaches (Gatt and Krahmer, 2017) is open to this task and deep learning has already shown promising results for RDF data (Sleimi and Gardent, 2016), the morphological richness of Portuguese led us to develop a rule-based approach. This was to ensure that we could identify the challenges imposed by this language from the SW perspective before applying Machine Learning (ML) algorithms. RDF2PT is able to generate either a single sentence or a summary of a given resource. In order to validate our approach, we evaluated RDF2PT using experts in Natural Language Processing (NLP) and SW as well as non-experts who are lay users or non-users of SW technologies. Both groups are native speakers of Brazilian Portuguese. The results suggest that RDF2P"
L18-1481,W13-2125,0,0.138277,"e texts from RDF using a rule-based approach. We intend to exploit the application of ML models along with other Brazilian Portuguese resources15 to produce more fluent results and also to investigate the usage of ML classification algorithms to improve the choice of grammatical gender of words. Moreover, we aim to create a multilingual version of RDF2PT which will consist of French, German, Italian and Spanish. To this end, we will exploit the similarity among their syntaxes in the micro-planning task and we will reuse their respective SimpleNLG versions (Mazzei et al., 2016; Bollmann, 2011; Vaudry and Lapalme, 2013; Ramos-Soto et al., 2017) for the realization task. Acknowledgments This work has been supported by the H2020 project HOBBIT (GA no. 688227) and supported by the Brazilian National Council for Scientific and Technological Development (CNPq) (no. 206971/2014-1 and no. 203065/20140.). This research has also been supported by the German Federal Ministry of Transport and Digital Infrastructure (BMVI) in the projects LIMBO (no. 19F2029I), OPAL 14 http://gerbil.aksw.org/gerbil/ experiment?id=201801050040 and http://gerbil. aksw.org/gerbil/experiment?id=201801110012 15 Resources: http://www.nilc.icm"
malmasi-zampieri-2017-detecting,N12-1084,0,\N,Missing
malmasi-zampieri-2017-detecting,N15-1160,1,\N,Missing
malmasi-zampieri-2017-detecting,W16-0314,1,\N,Missing
malmasi-zampieri-2017-detecting,W17-3003,0,\N,Missing
malmasi-zampieri-2017-detecting,W17-1101,0,\N,Missing
malmasi-zampieri-2017-detecting,W17-3008,0,\N,Missing
N19-1144,N12-1084,0,0.430083,"are the performance of different machine learning models on OLID. 1 Introduction Offensive content has become pervasive in social media and thus a serious concern for government organizations, online communities, and social media platforms. One of the most common strategies to tackle the problem is to train systems capable of recognizing offensive content, which can then be deleted or set aside for human moderation. In the last few years, there have been several studies on the application of computational methods to deal with this problem. Prior work has studied offensive language in Twitter (Xu et al., 2012; Burnap and Williams, 2015; Davidson et al., 2017; Wiegand et al., 2018), Wikipedia comments,1 and Facebook posts (Kumar et al., 2018). 1 Previous studies have looked into different aspects of offensive language such as the use of abusive language (Nobata et al., 2016; Mubarak et al., 2017), (cyber-)aggression (Kumar et al., 2018), (cyber-)bullying (Xu et al., 2012; Dadvar et al., 2013), toxic comments1 , hate speech (Kwok and Wang, 2013; Djuric et al., 2015; Burnap and Williams, 2015; Davidson et al., 2017; Malmasi and Zampieri, 2017, 2018), and offensive language (Wiegand et al., 2018). Rec"
N19-1144,W18-3901,1,0.892109,"Missing"
N19-1144,S19-2010,1,0.442727,"Missing"
N19-1144,D14-1181,0,0.011836,"i) an input embedding layer, (ii) a bidirectional LSTM layer, and (iii) an average pooling layer of input features. The concatenation of the LSTM layer and the average pooling layer is further passed through a dense layer, whose output is ultimately passed through a softmax to produce the final prediction. We set two input channels for the input embedding layers: pre-trained FastText embeddings (Bojanowski et al., 2017), as well as updatable embeddings learned by the model during training. CNN Finally, we experiment with a Convolutional Neural Network (CNN) model based on the architecture of (Kim, 2014), and using the same multi-channel inputs as the above BiLSTM. 4.1 Offensive Language Detection The performance on discriminating between offensive (OFF) and non-offensive (NOT) posts is reported in Table 4. We can see that all models perform significantly better than chance, with the neural models performing substantially better than the SVM. The CNN outperforms the RNN model, achieving a macro-F1 score of 0.80. 4.2 Categorization of Offensive Language In this set of experiments, the models were trained to discriminate between targeted insults and threats (TIN) and untargeted (UNT) offenses,"
N19-1144,W18-4401,1,0.535969,"edia and thus a serious concern for government organizations, online communities, and social media platforms. One of the most common strategies to tackle the problem is to train systems capable of recognizing offensive content, which can then be deleted or set aside for human moderation. In the last few years, there have been several studies on the application of computational methods to deal with this problem. Prior work has studied offensive language in Twitter (Xu et al., 2012; Burnap and Williams, 2015; Davidson et al., 2017; Wiegand et al., 2018), Wikipedia comments,1 and Facebook posts (Kumar et al., 2018). 1 Previous studies have looked into different aspects of offensive language such as the use of abusive language (Nobata et al., 2016; Mubarak et al., 2017), (cyber-)aggression (Kumar et al., 2018), (cyber-)bullying (Xu et al., 2012; Dadvar et al., 2013), toxic comments1 , hate speech (Kwok and Wang, 2013; Djuric et al., 2015; Burnap and Williams, 2015; Davidson et al., 2017; Malmasi and Zampieri, 2017, 2018), and offensive language (Wiegand et al., 2018). Recently, Waseem et al. (2017) analyzed the similarities between different approaches proposed in previous work and argued that there was"
N19-1144,malmasi-zampieri-2017-detecting,1,0.80182,"h this problem. Prior work has studied offensive language in Twitter (Xu et al., 2012; Burnap and Williams, 2015; Davidson et al., 2017; Wiegand et al., 2018), Wikipedia comments,1 and Facebook posts (Kumar et al., 2018). 1 Previous studies have looked into different aspects of offensive language such as the use of abusive language (Nobata et al., 2016; Mubarak et al., 2017), (cyber-)aggression (Kumar et al., 2018), (cyber-)bullying (Xu et al., 2012; Dadvar et al., 2013), toxic comments1 , hate speech (Kwok and Wang, 2013; Djuric et al., 2015; Burnap and Williams, 2015; Davidson et al., 2017; Malmasi and Zampieri, 2017, 2018), and offensive language (Wiegand et al., 2018). Recently, Waseem et al. (2017) analyzed the similarities between different approaches proposed in previous work and argued that there was a need for a typology that differentiates between whether the (abusive) language is directed towards a specific individual or entity, or towards a generalized group, and whether the abusive content is explicit or implicit. Wiegand et al. (2018) further applied this idea to German tweets. They experimented with a task on detecting offensive vs. nonoffensive tweets, and also with a second task on further"
N19-1144,W17-3008,0,0.10368,"he problem is to train systems capable of recognizing offensive content, which can then be deleted or set aside for human moderation. In the last few years, there have been several studies on the application of computational methods to deal with this problem. Prior work has studied offensive language in Twitter (Xu et al., 2012; Burnap and Williams, 2015; Davidson et al., 2017; Wiegand et al., 2018), Wikipedia comments,1 and Facebook posts (Kumar et al., 2018). 1 Previous studies have looked into different aspects of offensive language such as the use of abusive language (Nobata et al., 2016; Mubarak et al., 2017), (cyber-)aggression (Kumar et al., 2018), (cyber-)bullying (Xu et al., 2012; Dadvar et al., 2013), toxic comments1 , hate speech (Kwok and Wang, 2013; Djuric et al., 2015; Burnap and Williams, 2015; Davidson et al., 2017; Malmasi and Zampieri, 2017, 2018), and offensive language (Wiegand et al., 2018). Recently, Waseem et al. (2017) analyzed the similarities between different approaches proposed in previous work and argued that there was a need for a typology that differentiates between whether the (abusive) language is directed towards a specific individual or entity, or towards a generalize"
N19-1144,W17-3012,0,0.142505,"p and Williams, 2015; Davidson et al., 2017; Wiegand et al., 2018), Wikipedia comments,1 and Facebook posts (Kumar et al., 2018). 1 Previous studies have looked into different aspects of offensive language such as the use of abusive language (Nobata et al., 2016; Mubarak et al., 2017), (cyber-)aggression (Kumar et al., 2018), (cyber-)bullying (Xu et al., 2012; Dadvar et al., 2013), toxic comments1 , hate speech (Kwok and Wang, 2013; Djuric et al., 2015; Burnap and Williams, 2015; Davidson et al., 2017; Malmasi and Zampieri, 2017, 2018), and offensive language (Wiegand et al., 2018). Recently, Waseem et al. (2017) analyzed the similarities between different approaches proposed in previous work and argued that there was a need for a typology that differentiates between whether the (abusive) language is directed towards a specific individual or entity, or towards a generalized group, and whether the abusive content is explicit or implicit. Wiegand et al. (2018) further applied this idea to German tweets. They experimented with a task on detecting offensive vs. nonoffensive tweets, and also with a second task on further sub-classifying the offensive tweets as profanity, insult, or abuse. However, to the b"
N19-1144,Q17-1010,0,\N,Missing
R13-1023,D11-1119,0,0.0422077,"Missing"
R13-1023,pedler-mitton-2010-large,0,0.0646182,"Missing"
R13-1023,D09-1093,0,0.165278,"t notably to the work of Damerau (1964). Nowadays, spell checkers are an important component of a number of computer software such as web browsers, text processors and others. In recent years, spell checking has become a very important application to search engines (Martins and Silva, 2004). Companies like Google or Yahoo! use log files of all users’ queries to map the relation between misspellings and the intended spelling reaching very high accuracy. The language of queries, however, is typically shorter than naturally occurring text, making this application of spell checking very specific (Whitelaw et al., 2009). Spell checking methods have two main functions. The first one is to identify possible misspellings that a user may commit. As described by Mitton (1996), misspellings can be related to the writer’s (poor) writing and spelling competence, to learning disabilities such as dyslexia, and also to simple performance errors, known as typos. The written production of non-native speakers also plays an important role in spell checking as they are, on average, more prone to errors 2 Related Work Spell checking techniques have been substantially studied over the years. Mitton (2010) points out that the"
R13-1023,D09-1129,0,\N,Missing
S15-2144,W13-2714,1,0.825227,"LP techniques on a digitized collection of French texts published between 1801 and 1944. Stylerelated markers and features, including readability features, have been shown to reveal temporal information in English as well as Portuguese (Stamou, ˇ 2005; Stajner and Zampieri, 2013). An intersecting research direction combines diatopic (regional) and diachronic variation for French journalistic texts (Grouin et al., 2010) and for the Dutch Folktale Database, which includes texts from different dialects and varieties of Dutch, as well as historical texts (Trieschnigg et al., 2012). More recently, Ciobanu et al. (2013) propose supervised classification with unigram features with χ2 feature selection on a collection of historical Romanian texts, noting that the informative features are words having changed form over time. Niculae et al. (2014) circumvent the limitations of supervised classification by posing the problem as ordinal regression with a learning-to-rank approach. They evaluate their method on datasets in English, Portuguese and Romanian. The superior flexibility of the ranking approach makes it a better fit for the problem for852 mulation of the ‘Diachronic Text Evaluation’ task, motivating us to"
S15-2144,W06-0903,0,0.362218,"anguage models and distances in distribution space to classify documents to the time period with the most similar language (de Jong et al., 2005; Kumar et al., 2011). Kanhabua and Nørv˚ag (2009) use temporal language models to assign timestamps to unlabeled documents. An extension of such models for continuous time is proposed by Wang et al. (2008), who use Brownian motion as a model for topic change over time. This approach is simpler and faster than the discrete time version, but it cannot be directly applied to documents with different degrees of label uncertainty, such as interval labels. Dalli and Wilks (2006) train a classifier to date texts within a time span of nine years. The method uses lexical features and it is aided by words whose frequencies increase at some point in time, most notably named entities. Abe and Tsumoto (2010) propose similarity metrics to categorise texts based on keywords calculated by indexes such as tf-idf. Garcia-Fernandez et al. (2011) explore different NLP techniques on a digitized collection of French texts published between 1801 and 1944. Stylerelated markers and features, including readability features, have been shown to reveal temporal information in English as we"
S15-2144,P12-2051,0,0.183283,"ised classification by posing the problem as ordinal regression with a learning-to-rank approach. They evaluate their method on datasets in English, Portuguese and Romanian. The superior flexibility of the ranking approach makes it a better fit for the problem for852 mulation of the ‘Diachronic Text Evaluation’ task, motivating us to base our implementation on it. A different, but related, problem is to model and understand how words usage and meaning change over time. Wijaya and Yeniterzi (2011) use the Google NGram corpus aiming to identify clusters of topics surrounding the word over time. Mihalcea and Nastase (2012) split the Google Books corpus into three wide epochs and introduce the task of word epoch disambiguation. Turning this problem around, Popescu and Strapparava (2013) use a similar approach to statistically characterize epochs by lexical and emotion features. 3 Methods The ‘Diachronic Text Evaluation’ shared task consists of three subtasks (Popescu and Strapparava, 2015): classification of documents containing explicit references to time-specific persons or events (T1), classification of documents with time-specific language use (T2), and recognition of time-specific expressions (T3). The AMBR"
S15-2144,E14-4004,1,0.941706,"tuguese (Stamou, ˇ 2005; Stajner and Zampieri, 2013). An intersecting research direction combines diatopic (regional) and diachronic variation for French journalistic texts (Grouin et al., 2010) and for the Dutch Folktale Database, which includes texts from different dialects and varieties of Dutch, as well as historical texts (Trieschnigg et al., 2012). More recently, Ciobanu et al. (2013) propose supervised classification with unigram features with χ2 feature selection on a collection of historical Romanian texts, noting that the informative features are words having changed form over time. Niculae et al. (2014) circumvent the limitations of supervised classification by posing the problem as ordinal regression with a learning-to-rank approach. They evaluate their method on datasets in English, Portuguese and Romanian. The superior flexibility of the ranking approach makes it a better fit for the problem for852 mulation of the ‘Diachronic Text Evaluation’ task, motivating us to base our implementation on it. A different, but related, problem is to model and understand how words usage and meaning change over time. Wijaya and Yeniterzi (2011) use the Google NGram corpus aiming to identify clusters of to"
S15-2144,I13-1040,0,0.380693,"Romanian. The superior flexibility of the ranking approach makes it a better fit for the problem for852 mulation of the ‘Diachronic Text Evaluation’ task, motivating us to base our implementation on it. A different, but related, problem is to model and understand how words usage and meaning change over time. Wijaya and Yeniterzi (2011) use the Google NGram corpus aiming to identify clusters of topics surrounding the word over time. Mihalcea and Nastase (2012) split the Google Books corpus into three wide epochs and introduce the task of word epoch disambiguation. Turning this problem around, Popescu and Strapparava (2013) use a similar approach to statistically characterize epochs by lexical and emotion features. 3 Methods The ‘Diachronic Text Evaluation’ shared task consists of three subtasks (Popescu and Strapparava, 2015): classification of documents containing explicit references to time-specific persons or events (T1), classification of documents with time-specific language use (T2), and recognition of time-specific expressions (T3). The AMBRA system participated in T1 and T2. 3.1 Corpus The training data released for the shared task consists of 323 documents for T1 and 4,202 documents for T2. Each docume"
S15-2144,S15-2147,0,0.67111,"Missing"
S16-1153,E14-4019,1,0.903466,"Missing"
S16-1153,W15-5407,1,0.733845,"re. We describe this process below. 4.1 Ensemble Construction Our ensemble was created using a set of decision stump classifiers. A decision stump is a decision tree trained using only a single feature (Iba and Langley, 1992); it is usually considered a weak learner. We used the features listed in Section 3 to create an ensemble of 9 classifiers. Each classifier predicts every input and assigns a probability output to each of the two possible labels. Classifiers ensembles have proved to an efficient 993 and robust alternative in other text classification tasks such as language identification (Malmasi and Dras, 2015a) and grammatical error detection (Xiang et al., 2015). This motivated us to try this approach in the CWI SemEval task. 4.2 Meta-classifier For our meta-classifier, we adopted a decision tree with bootstrap aggregating (bagging). The inputs to each decision tree are the two probability outputs from each decision stump in our ensemble, along with the original gold label. 200 bagged decision trees were created using this input. The final label was selected through a plurality voting process over the entire set of bagged decision trees. 4.3 Systems Using the methods described so far, we created"
S16-1153,P13-3015,0,0.178415,"short phrases for simpler ones to improve text readability and comprehension aimed at a given target population (e.g. children, language learners, people with reading impairment, etc.). Lexical simplification is considered to be the sub-task of text simplification that deals with the lexicon while other sub-tasks address, for example, complex syntactic structures (Siddharthan, 2014). To perform lexical simplification efficiently, computational methods should be first applied to identify which words in a text pose more difficulty to readers and they therefore good candidates for substitution (Shardlow, 2013). This task is called complex word In the example presented above, the CWI systems should label extraordinary, bipedal and locomotion as complex words.1 To accomplish this task, the MAZA team applied a decision stump meta-classifier and a wide set of features that we will describe here. 2 Data Organizers of the SemEval CWI task provided a training and test set comprising English sentences with each word annotated with a complex or simple label. According to the CWI task website2 : ‘400 annotators were presented with several sentences and asked to select which words they did not understand 1 No"
S16-1153,S12-1046,0,0.124254,"Missing"
S16-1153,W13-1706,0,0.0291774,"f 200 sentences. A word is considered to be complex if at least one of the 20 annotators assigned them as complex. Subsequently a test set with the same format was released containing 88,221 sentences. According to the organizers, the test set contains by judgments made over 9,000 sentences by a single annotator. The proportion of training vs. test instances of 1:40 should also be noted as it represents and additional challenge to participants. This data split is different from other similar text classification shared tasks which provide much more training than test instances (at least 10:1) (Tetreault et al., 2013; Zampieri et al., 2015). Given the amount of training data, participating teams should employ efficient algorithms able to perform generalizations on a much larger test set.3 3 Features We experimented with two types of features in our submissions. Each of these two classes, as described below, contains several features which we combine using a meta-classifier. 3.1 Frequency and Length Features These are features based on the occurrence of the target word in a given reference corpus and its length. The idea is inspired by the Zipfian frequency distribution of words that indicate that the most"
S16-1153,W15-4415,0,0.0299871,"ion Our ensemble was created using a set of decision stump classifiers. A decision stump is a decision tree trained using only a single feature (Iba and Langley, 1992); it is usually considered a weak learner. We used the features listed in Section 3 to create an ensemble of 9 classifiers. Each classifier predicts every input and assigns a probability output to each of the two possible labels. Classifiers ensembles have proved to an efficient 993 and robust alternative in other text classification tasks such as language identification (Malmasi and Dras, 2015a) and grammatical error detection (Xiang et al., 2015). This motivated us to try this approach in the CWI SemEval task. 4.2 Meta-classifier For our meta-classifier, we adopted a decision tree with bootstrap aggregating (bagging). The inputs to each decision tree are the two probability outputs from each decision stump in our ensemble, along with the original gold label. 200 bagged decision trees were created using this input. The final label was selected through a plurality voting process over the entire set of bagged decision trees. 4.3 Systems Using the methods described so far, we created two different systems for the shared task. They are sum"
S16-1153,W15-5401,1,0.898649,"Missing"
S16-1154,E14-4019,1,0.88592,"Missing"
S16-1154,P13-3015,0,0.106496,"ormed by human annotators required to label the data.1 We present the description of the LTG entry in the SemEval-2016 Complex Word Identification (CWI) task, which aimed to develop systems for identifying complex words in English sentences. Our entry focused on the use of contextual language model features and the application of ensemble classification methods. Both of our systems achieved good performance, ranking in 2nd and 3rd place overall in terms of F-Score. 1 2 Introduction Complex Word Identification (CWI) is the task of identifying complex words in texts using computational methods (Shardlow, 2013). The task is usually carried out as part of lexical and text simplification systems. Shardlow (2014) considers CWI as the first processing step in lexical simplification pipelines. Complex or difficult words should first be identified so they can be later substituted by simpler ones to improve text readability. CWI has gained more importance in the last decade as lexical and text simplification systems have been developed or tailored for a number of purposes. They have been applied to make texts more accessible to language learners (Petersen and Ostendorf, 2007); other researchers have explor"
S16-1155,C12-1023,0,0.0225042,"reas text simplification comprises also the modification of syntactic structures to improve readability. Most text simplification systems also contain a lexical simplification module or component which often relies on the accurate identification of complex words for subsequent substitution. The three tasks are therefore inseparable. Both lexical and text simplification approaches have been widely investigated. They have been 1002 applied to different languages, examples include: Basque (Aranzabe et al., 2012), Italian (Barlacchi and Tonelli, 2013), Portuguese (Alu´ısio et al., 2008), Spanish (Bott et al., 2012), and the SemEval lexical simplification task for English (Specia et al., 2012). To the best of our knowledge, very few methods have focused solely on complex word identification prior to the CWI shared task. An exception is the work by Shardlow (2013) which compared different techniques to identify complex words. 3 Methods 3.1 Task and Data The SemEval 2016 Task 11, Complex Word Identification (CWI) is a binary text classification task at the word level. Systems are trained to attribute a label of either 1 (for complex words) or 0 (for simple words) to each word in a given sentence. There are"
S16-1155,W13-1703,0,0.0202682,"rom suitable text corpora (Zipf, 1949). Another aspect to consider is the length of the words. Words that are more frequent tend to be shorter as noted by Zipf: ‘the magnitude of words tends, on the whole, to stand in an inverse (not necessarily proportionate) relationship to the number of occurrences’ (Zipf, 1935). That said, our approach takes both frequency and word length into account to determine whether a word is complex or not. Finally, another aspect that we take into account is the difficulty in vocabulary acquisition that is related to the spelling of complex words (Xu et al., 2011; Dahlmeier et al., 2013). Educational applications that are tailored towards non-native speakers use character-level n-grams to identify possible spelling errors that language learner make. Thus making character combinations another interesting aspect to be consider in this task. 2 Related Work CWI is a sub-task included in many lexical and text simplification systems. Lexical simplification, as the name suggests, focuses only on the substitution of complex words for simpler words in texts whereas text simplification comprises also the modification of syntactic structures to improve readability. Most text simplificat"
S16-1155,W13-1728,1,0.861926,"ation of the n-grams since we can assume that longer words are more complex. But we have the word length feature to account for the length of words, so the normalization of the n-grams probabilities would account for density of the n-gram probabilities independent of the length of the word. Additionally, we computed (iii) sentence length and (iv) sum probability of the character trigrams of the sentence to account for contextual orthographic complexity with respect to the word-level spelling complexity. These sentence-level features are similar to those used in Native Language Identification (Gebre et al., 2013; Malmasi and Dras, 2015; Malmasi et al., 2015b). As a meta-feature that captures both word and sentential level spelling complexity, we use the proportion of word to sentence orthographic difficulty by taking the ratio of the aforementioned features (ii) and (iv). 3.2.3 Classifiers We trained 3 different classifiers using the features described in Table 1: a (i) Random Forest Classifier (RFC), (ii) Nearest Neighbor Classifier6 (NNC) and (iii) Support Vector Machine7 (SVM). Nearest neighbor classifiers usually work well when the distribution between the training set data points are dense and s"
S16-1155,D14-1144,0,0.0326451,"Missing"
S16-1155,W15-0620,0,0.032725,"t longer words are more complex. But we have the word length feature to account for the length of words, so the normalization of the n-grams probabilities would account for density of the n-gram probabilities independent of the length of the word. Additionally, we computed (iii) sentence length and (iv) sum probability of the character trigrams of the sentence to account for contextual orthographic complexity with respect to the word-level spelling complexity. These sentence-level features are similar to those used in Native Language Identification (Gebre et al., 2013; Malmasi and Dras, 2015; Malmasi et al., 2015b). As a meta-feature that captures both word and sentential level spelling complexity, we use the proportion of word to sentence orthographic difficulty by taking the ratio of the aforementioned features (ii) and (iv). 3.2.3 Classifiers We trained 3 different classifiers using the features described in Table 1: a (i) Random Forest Classifier (RFC), (ii) Nearest Neighbor Classifier6 (NNC) and (iii) Support Vector Machine7 (SVM). Nearest neighbor classifiers usually work well when the distribution between the training set data points are dense and similar to (or representative) of test set. Sin"
S16-1155,P15-4015,0,0.0387075,"Missing"
S16-1155,P13-3015,0,0.529415,"x words for subsequent substitution. The three tasks are therefore inseparable. Both lexical and text simplification approaches have been widely investigated. They have been 1002 applied to different languages, examples include: Basque (Aranzabe et al., 2012), Italian (Barlacchi and Tonelli, 2013), Portuguese (Alu´ısio et al., 2008), Spanish (Bott et al., 2012), and the SemEval lexical simplification task for English (Specia et al., 2012). To the best of our knowledge, very few methods have focused solely on complex word identification prior to the CWI shared task. An exception is the work by Shardlow (2013) which compared different techniques to identify complex words. 3 Methods 3.1 Task and Data The SemEval 2016 Task 11, Complex Word Identification (CWI) is a binary text classification task at the word level. Systems are trained to attribute a label of either 1 (for complex words) or 0 (for simple words) to each word in a given sentence. There are no borderline cases or gradation, all words are either complex of simple. A tokenized data set containing English sentences annotated with the complex or simple label for each word was provided. The training set contained 2,237 sentences, and the test"
S16-1155,S12-1046,0,0.0171217,"res to improve readability. Most text simplification systems also contain a lexical simplification module or component which often relies on the accurate identification of complex words for subsequent substitution. The three tasks are therefore inseparable. Both lexical and text simplification approaches have been widely investigated. They have been 1002 applied to different languages, examples include: Basque (Aranzabe et al., 2012), Italian (Barlacchi and Tonelli, 2013), Portuguese (Alu´ısio et al., 2008), Spanish (Bott et al., 2012), and the SemEval lexical simplification task for English (Specia et al., 2012). To the best of our knowledge, very few methods have focused solely on complex word identification prior to the CWI shared task. An exception is the work by Shardlow (2013) which compared different techniques to identify complex words. 3 Methods 3.1 Task and Data The SemEval 2016 Task 11, Complex Word Identification (CWI) is a binary text classification task at the word level. Systems are trained to attribute a label of either 1 (for complex words) or 0 (for simple words) to each word in a given sentence. There are no borderline cases or gradation, all words are either complex of simple. A to"
S16-1155,W13-1706,0,0.0283488,"the complex or simple label for each word was provided. The training set contained 2,237 sentences, and the test set contained 88,221 sentences. The shared task website2 states that: ‘the data was collected through a survey, in which 400 annotators were presented with several sentences and asked to select which words they did not understand their meaning’. There was no information of whether annotators were English native speakers. The proportion of training vs. test instances makes the task more challenging than other similar shared tasks which provide much more training than test instances (Tetreault et al., 2013; Zampieri et al., 2015), a common practice in text classification tasks.3 3.2 Approach Given the motivation described in Section 1.1, we approach the CWI task using word frequency and character-level n-gram features.4 To emulate a language learner exposure to English, we use newspa2 http://alt.qcri.org/semeval2016/task11/ The Chinese grammatical error diagnosis (CGED) shared task (Yu et al., 2014) is an exception. See the discussion in Zampieri and Tan (2014). 4 Our implementation is open source and it can be found on: https://github.com/alvations/MacSaar-CWI 3 pers text from the English subs"
S16-1155,D11-1119,0,0.0211827,"Missing"
S16-1155,W15-5401,1,0.888572,"Missing"
S19-2010,S19-2121,0,0.0487207,"Missing"
S19-2010,S19-2100,0,0.0543071,"Missing"
S19-2010,S19-2120,0,0.040833,"Missing"
S19-2010,S19-2110,0,0.0445018,"Missing"
S19-2010,S19-2129,0,0.043829,"Missing"
S19-2010,S19-2144,0,0.044782,"Missing"
S19-2010,W18-4411,0,0.0794205,"Aggression identification: The TRAC shared task on Aggression Identification (Kumar et al., 2018) provided participants with a dataset containing 15,000 annotated Facebook posts and comments in English and Hindi for training and validation. For testing, two different sets, one from Facebook and one from Twitter, were used. The goal was to discriminate between three classes: non-aggressive, covertly aggressive, and overtly aggressive. The best-performing systems in this competition used deep learning approaches based on convolutional neural networks (CNN), recurrent neural networks, and LSTM (Aroyehun and Gelbukh, 2018; Majumder et al., 2018). While each of the above tasks tackles a particular type of abuse or offense, there are many commonalities. For example, an insult targeted at an individual is commonly known as cyberbulling and insults targeted at a group are known as hate speech. The hierarchical annotation model proposed in OLID (Zampieri et al., 2019) and used in OffensEval aims to capture this. We hope that the OLID’s dataset would become a useful resource for various offensive language identification tasks. 3 The training and testing material for OffensEval is the aforementioned Offensive Languag"
S19-2010,W18-4416,0,0.0526243,"ings of the 13th International Workshop on Semantic Evaluation (SemEval-2019), pages 75–86 Minneapolis, Minnesota, USA, June 6–7, 2019. ©2019 Association for Computational Linguistics 2 Related Work Toxic comments: The Toxic Comment Classification Challenge5 was an open competition at Kaggle, which provided participants with comments from Wikipedia organized in six classes: toxic, severe toxic, obscene, threat, insult, identity hate. The dataset was also used outside of the competition (Georgakopoulos et al., 2018), including as additional training material for the aforementioned TRAC shared (Fortuna et al., 2018). Different abusive and offense language identification problems have been explored in the literature ranging from aggression to cyber bullying, hate speech, toxic comments, and offensive language. Below we discuss each of them briefly. Aggression identification: The TRAC shared task on Aggression Identification (Kumar et al., 2018) provided participants with a dataset containing 15,000 annotated Facebook posts and comments in English and Hindi for training and validation. For testing, two different sets, one from Facebook and one from Twitter, were used. The goal was to discriminate between t"
S19-2010,S19-2111,0,0.0516978,"Missing"
S19-2010,S19-2131,0,0.076679,"Missing"
S19-2010,S19-2107,0,0.0613142,"Missing"
S19-2010,P11-2008,0,0.060614,"Missing"
S19-2010,S19-2098,1,0.882958,"Missing"
S19-2010,W15-4322,0,0.0652014,"Missing"
S19-2010,W18-4401,1,0.628306,"Facebook and Twitter. As manual filtering is very time consuming, and as it can cause post-traumatic stress disorder-like symptoms to human annotators, there have been many research efforts aiming at automating the process. The task is usually modeled as a supervised classification problem, where systems are trained on posts annotated with respect to the presence of some form of abusive or offensive content. Examples of offensive content studied in previous work include hate speech (Davidson et al., 2017; Malmasi and Zampieri, 2017, 2018), cyberbulling (Dinakar et al., 2011), and aggression (Kumar et al., 2018). Moreover, given the multitude of terms and definitions used in the literature, some recent studies have investigated the common aspects of different abusive language detection sub-tasks (Waseem et al., 2017; Wiegand et al., 2018). Sub-task C: Offense target identification (66 participating teams) The remainder of this paper is organized as follows: Section 2 discusses prior work, including shared tasks related to OffensEval. Section 3 presents the shared task description and the subtasks included in OffensEval. Section 4 includes a brief description of OLID based on (Zampieri et al., 2019)."
S19-2010,S19-2139,0,0.0394818,"Missing"
S19-2010,S19-2011,0,0.137461,"Missing"
S19-2010,S19-2115,0,0.0421739,"Missing"
S19-2010,W18-4423,0,0.178724,"Missing"
S19-2010,S19-2116,0,0.0624146,"Missing"
S19-2010,malmasi-zampieri-2017-detecting,1,0.577817,"ears have seen the proliferation of offensive language in social media platforms such as Facebook and Twitter. As manual filtering is very time consuming, and as it can cause post-traumatic stress disorder-like symptoms to human annotators, there have been many research efforts aiming at automating the process. The task is usually modeled as a supervised classification problem, where systems are trained on posts annotated with respect to the presence of some form of abusive or offensive content. Examples of offensive content studied in previous work include hate speech (Davidson et al., 2017; Malmasi and Zampieri, 2017, 2018), cyberbulling (Dinakar et al., 2011), and aggression (Kumar et al., 2018). Moreover, given the multitude of terms and definitions used in the literature, some recent studies have investigated the common aspects of different abusive language detection sub-tasks (Waseem et al., 2017; Wiegand et al., 2018). Sub-task C: Offense target identification (66 participating teams) The remainder of this paper is organized as follows: Section 2 discusses prior work, including shared tasks related to OffensEval. Section 3 presents the shared task description and the subtasks included in OffensEval."
S19-2010,S19-2109,0,0.0646835,"Missing"
S19-2010,S19-2134,0,0.0461997,"Missing"
S19-2010,P14-5010,0,0.00482749,"Missing"
S19-2010,S19-2105,0,0.0325559,"Missing"
S19-2010,D14-1162,0,0.0847332,"Missing"
S19-2010,S19-2127,0,0.0671146,"Missing"
S19-2010,S19-2103,0,0.0392423,"Missing"
S19-2010,N18-1202,0,0.0384382,"tators on the platform and by using test questions to discard annotators who did not achieve a certain threshold. All the tweets were annotated by two people. In case of disagreement, a third annotation was requested, and ultimately we used a majority vote. Examples of tweets from the dataset with their annotation labels are shown in Table 1. 5 Results The models used in the task submissions ranged from traditional machine learning, e.g., SVM and logistic regression, to deep learning, e.g., CNN, RNN, BiLSTM, including attention mechanism, to state-of-the-art deep learning models such as ELMo (Peters et al., 2018) and BERT (Devlin et al.). Figure 2 shows a pie chart indicating the breakdown by model type for all participating systems in sub-task A. Deep learning was clearly the most popular approach, as were also ensemble models. Similar trends were observed for subtasks B and C. Table 2: The teams that participated in OffensEval and submitted system description papers. 7 78 https://www.figure-eight.com/ The results for each of the sub-tasks are shown in Table 4. Due to the large number of submissions, we only show the F1-score for the top-10 teams, followed by result ranges for the rest of the teams."
S19-2010,S19-2118,0,0.0364634,"Missing"
S19-2010,S19-2104,0,0.0457917,"Missing"
S19-2010,S19-2123,0,0.1488,"Missing"
S19-2010,S19-2136,0,0.0650507,"Missing"
S19-2010,S19-2112,0,0.0343921,"Missing"
S19-2010,S19-2141,0,0.0614093,"Missing"
S19-2010,S19-2140,0,0.0351825,"Missing"
S19-2010,S19-2119,0,0.0413788,"Missing"
S19-2010,S19-2113,0,0.0437079,"Missing"
S19-2010,S19-2066,0,0.0526442,"Missing"
S19-2010,S19-2102,0,0.0607101,"Missing"
S19-2010,S19-2125,0,0.0321167,"Missing"
S19-2010,S19-2106,0,0.0611208,"Missing"
S19-2010,S19-2132,0,0.0476906,"Missing"
S19-2010,S19-2108,0,0.131809,"Missing"
S19-2010,W17-3012,0,0.152463,"ing the process. The task is usually modeled as a supervised classification problem, where systems are trained on posts annotated with respect to the presence of some form of abusive or offensive content. Examples of offensive content studied in previous work include hate speech (Davidson et al., 2017; Malmasi and Zampieri, 2017, 2018), cyberbulling (Dinakar et al., 2011), and aggression (Kumar et al., 2018). Moreover, given the multitude of terms and definitions used in the literature, some recent studies have investigated the common aspects of different abusive language detection sub-tasks (Waseem et al., 2017; Wiegand et al., 2018). Sub-task C: Offense target identification (66 participating teams) The remainder of this paper is organized as follows: Section 2 discusses prior work, including shared tasks related to OffensEval. Section 3 presents the shared task description and the subtasks included in OffensEval. Section 4 includes a brief description of OLID based on (Zampieri et al., 2019). Section 5 discusses the participating systems and their results in the shared task. Finally, Section 6 concludes and suggests directions for future work. 1 http://competitions.codalab.org/ competitions/20011"
S19-2010,S19-2126,0,0.0759606,"Missing"
S19-2010,S19-2137,0,0.0528763,"Missing"
S19-2010,S19-2135,0,0.0463278,"Missing"
S19-2010,S19-2128,0,0.0505463,"Missing"
S19-2010,S19-2097,0,0.0448575,"Missing"
S19-2010,N12-1084,0,0.417325,"nsive language identification tasks. 3 The training and testing material for OffensEval is the aforementioned Offensive Language Identification Dataset (OLID) dataset, which was built specifically for this task. OLID was annotated using a hierarchical three-level annotation model introduced in Zampieri et al. (2019). Four examples of annotated instances from the dataset are presented in Table 1. We use the annotation of each of the three layers in OLID for a sub-task in OffensEval as described below. Bullying detection: There have been several studies on cyber bullying detection. For example, Xu et al. (2012) used sentiment analysis and topic models to identify relevant topics, and Dadvar et al. (2013) used user-related features such as the frequency of profanity in previous messages. Hate speech identification: This is the most studied abusive language detection task (Kwok and Wang, 2013; Burnap and Williams, 2015; Djuric et al., 2015). More recently, Davidson et al. (2017) presented the hate speech detection dataset with over 24,000 English tweets labeled as non offensive, hate speech, and profanity. 3.1 Sub-task A: Offensive language identification In this sub-task, the goal is to discriminate"
S19-2010,S19-2124,0,0.0512483,"Missing"
S19-2010,S19-2101,0,0.0376161,"Missing"
S19-2010,N19-1144,1,0.39554,"orizing Offensive Language in Social Media (OffensEval) Marcos Zampieri,1 Shervin Malmasi,2 Preslav Nakov,3 Sara Rosenthal,4 Noura Farra,5 Ritesh Kumar6 1 University of Wolverhampton, UK, 2 Amazon Research, USA 3 Qatar Computing Research Institute, HBKU, Qatar 4 IBM Research, USA, 5 Columbia University, USA, 6 Bhim Rao Ambedkar University, India m.zampieri@wlv.ac.uk Abstract Interestingly, none of this previous work has studied both the type and the target of the offensive language, which is our approach here. Our task, OffensEval1 , uses the Offensive Language Identification Dataset (OLID)2 (Zampieri et al., 2019), which we created specifically for this task. OLID is annotated following a hierarchical three-level annotation schema that takes both the target and the type of offensive content into account. Thus, it can relate to phenomena captured by previous datasets such as the one by Davidson et al. (2017). Hate speech, for example, is commonly understood as an insult targeted at a group, whereas cyberbulling is typically targeted at an individual. We defined three sub-tasks, corresponding to the three levels in our annotation schema:3 We present the results and the main findings of SemEval-2019 Task"
S19-2010,S19-2130,0,0.0564906,"Missing"
S19-2010,S19-2142,0,0.0349466,"Missing"
S19-2010,S19-2117,0,0.0415284,"Missing"
S19-2010,S19-2138,0,0.094085,"Missing"
S19-2010,S19-2143,0,0.05351,"Missing"
S19-2093,S19-2007,0,0.0280948,"ns, then a second set of layers produces a final representation of the sentence. Finally, this representation is passed through a softmax dense layer that produces a final classification label. For each language, we created two variants of UTFPR: one trained exclusively over the training data provided by the organizers (UTFPR/O), and another that uses a pre-trained set of character-toword RNN layers extracted from the models introduced by Paetzold (2018) (UTFPR/W). The pretrained model was trained for the English multiclass classification Emotion Analysis shared task Task Description HatEval (Basile et al., 2019) provides participants with annotated datasets to create systems capable of properly identifying hate speech in tweets writ1 https://competitions.codalab.org/ competitions/20011 520 Figure 1: Architecture of the UTFPR models. missions for English, and 31st out of 35 valid submissions for Spanish. of WASSA 2018, which featured a training set of 153, 383 instances composed of a tweet and an emotion label. This pre-trained model for English was used for the UTFPR/W variant of both languages, since we wanted to test the hypothesis that pre-training a character-to-word RNN on a large dataset for En"
S19-2093,W18-6224,1,0.837176,"ectly predicting all three of the aforementioned labels. In this paper, we focus on Task A exclusively, for both English and Spanish. We participated in the competition using the team name UTFPR. 4 The UTFPR Models The UTFPR models are minimalistic Recurrent Neural Networks (RNNs) that learn compositional numerical representations of words based on the sequence of characters that compose them, then use them to learn a final representation for the sentence being analyzed. These models, of which the architecture is illustrated in Figure 1, are somewhat similar to those of Ling et al. (2015) and Paetzold (2018), who use RNNs to create compositional neural models for different tasks. As illustrated, the UTFPR models take as input a sentence, split it into words, then split the words into a sequence of characters in order to pass them through a character embedding layer. The character embeddings are passed onto a set of bidirectional RNN layers that produces word representations, then a second set of layers produces a final representation of the sentence. Finally, this representation is passed through a softmax dense layer that produces a final classification label. For each language, we created two v"
S19-2093,W17-1101,0,0.0327898,"vesting in ways to cope with such content using human moderation of posts, triage of content, deletion of offensive posts, and banning abusive users. One of the most common and effective strategies to tackle the problem of offensive language online is to train systems capable of recognizing such content. Several studies have been published in the last few years on identifying abusive language (Nobata et al., 2016), cyber aggression (Kumar et al., 2018), cyber bullying (Dadvar et al., 2013), and hate speech (Burnap and Williams, 2015; Davidson et al., 2017). As evidenced in two recent surveys (Schmidt and Wiegand, 2017; Fortuna and Nunes, 2018) and in a number of other 2 Related Work As evidenced in the introduction of this paper, there have been a number of studies on automatic hate speech identification published in the last few years. One of the most influential recent papers on hate speech identification is the one by Davidson et al. (2017). In this paper, the authors presented the Hate Speech Detection dataset which contains posts retrieved from social media labeled with three categories: OK (posts not containing profanity or hate speech), Offensive (posts containing swear words and general profanity),"
S19-2093,W17-3012,0,0.0138337,"ation for Computational Linguistics gued that annotating texts with respect to the presence of hate speech has an intrinsic degree of subjectivity (Malmasi and Zampieri, 2018). Along with the recent studies published, there have been a few related shared tasks organized on the topic. These include GermEval (Wiegand et al., 2018) for German, TRAC (Kumar et al., 2018) for English and Hindi, and OffensEval1 (Zampieri et al., 2019b) for English. The latter is also organized within the scope of SemEval-2019. OffensEval considers offensive language in general whereas HatEval focuses on hate speech. Waseem et al. (2017) proposes a typology of abusive language detection sub-tasks taking two factors into account: the target of the message and whether the content is explicit or implicit. Considering that hate speech is commonly understood as speech attacking a group based on ethnicity, religion, etc, and that cyber bulling, for example, is understood as an attack towards an individual, the target factor plays an important role in the identification and the definition of hate speech when compared to other forms of abusive content. The two SemEval-2019 shared tasks, HatEval and OffensEval, both include a sub-task"
S19-2093,N19-1144,1,0.884481,"in swear words. It has been ar519 Proceedings of the 13th International Workshop on Semantic Evaluation (SemEval-2019), pages 519–523 Minneapolis, Minnesota, USA, June 6–7, 2019. ©2019 Association for Computational Linguistics gued that annotating texts with respect to the presence of hate speech has an intrinsic degree of subjectivity (Malmasi and Zampieri, 2018). Along with the recent studies published, there have been a few related shared tasks organized on the topic. These include GermEval (Wiegand et al., 2018) for German, TRAC (Kumar et al., 2018) for English and Hindi, and OffensEval1 (Zampieri et al., 2019b) for English. The latter is also organized within the scope of SemEval-2019. OffensEval considers offensive language in general whereas HatEval focuses on hate speech. Waseem et al. (2017) proposes a typology of abusive language detection sub-tasks taking two factors into account: the target of the message and whether the content is explicit or implicit. Considering that hate speech is commonly understood as speech attacking a group based on ethnicity, religion, etc, and that cyber bulling, for example, is understood as an attack towards an individual, the target factor plays an important ro"
S19-2093,W17-3013,0,0.0670864,"Missing"
S19-2093,S19-2010,1,0.83211,"in swear words. It has been ar519 Proceedings of the 13th International Workshop on Semantic Evaluation (SemEval-2019), pages 519–523 Minneapolis, Minnesota, USA, June 6–7, 2019. ©2019 Association for Computational Linguistics gued that annotating texts with respect to the presence of hate speech has an intrinsic degree of subjectivity (Malmasi and Zampieri, 2018). Along with the recent studies published, there have been a few related shared tasks organized on the topic. These include GermEval (Wiegand et al., 2018) for German, TRAC (Kumar et al., 2018) for English and Hindi, and OffensEval1 (Zampieri et al., 2019b) for English. The latter is also organized within the scope of SemEval-2019. OffensEval considers offensive language in general whereas HatEval focuses on hate speech. Waseem et al. (2017) proposes a typology of abusive language detection sub-tasks taking two factors into account: the target of the message and whether the content is explicit or implicit. Considering that hate speech is commonly understood as speech attacking a group based on ethnicity, religion, etc, and that cyber bulling, for example, is understood as an attack towards an individual, the target factor plays an important ro"
S19-2093,W18-4401,1,0.891356,"Missing"
S19-2093,D15-1176,0,0.0398909,"ful. • Sub-task B: Correctly predicting all three of the aforementioned labels. In this paper, we focus on Task A exclusively, for both English and Spanish. We participated in the competition using the team name UTFPR. 4 The UTFPR Models The UTFPR models are minimalistic Recurrent Neural Networks (RNNs) that learn compositional numerical representations of words based on the sequence of characters that compose them, then use them to learn a final representation for the sentence being analyzed. These models, of which the architecture is illustrated in Figure 1, are somewhat similar to those of Ling et al. (2015) and Paetzold (2018), who use RNNs to create compositional neural models for different tasks. As illustrated, the UTFPR models take as input a sentence, split it into words, then split the words into a sequence of characters in order to pass them through a character embedding layer. The character embeddings are passed onto a set of bidirectional RNN layers that produces word representations, then a second set of layers produces a final representation of the sentence. Finally, this representation is passed through a softmax dense layer that produces a final classification label. For each langua"
S19-2093,malmasi-zampieri-2017-detecting,1,\N,Missing
sulea-etal-2017-predicting,J13-4005,0,\N,Missing
sulea-etal-2017-predicting,E14-4004,1,\N,Missing
sulea-etal-2017-predicting,W12-0411,0,\N,Missing
sulea-etal-2017-predicting,W16-0314,1,\N,Missing
sulea-etal-2017-predicting,W04-1006,0,\N,Missing
sulea-etal-2017-predicting,W12-0515,0,\N,Missing
U18-1012,W16-0314,1,0.929087,"thods to categorize patent applications according to standardized taxonomies such as the International Patent Classification (IPC)2 as discussed in the studies by Benzineb and Guyot (2011); Fall et al. (2003). In this paper, we present a system to automatically categorize patent applications from Australia according to the top sections of the IPC taxonomy using a dataset provided by the organizers of the ALTA 2018 shared task on Classifying Patent Applications (Molla and Seneviratne, 2018).3 The dataset and the taxonomy are presented in more detail in Section 3. Building on our previous work (Malmasi et al., 2016a; Malmasi and Zampieri, 2017), our system is based on SVM ensembles and it achieved the highest performance of the competition. 2 Related Work There have been a number of studies applying NLP and Information Retrieval (IR) methods to patent applications specifically, and to legal texts in general, published in the last few years. Applications of NLP and IR to legal texts include the use of text summarization methods (Farzindar and Lapalme, 2004) to summarize legal documents and most recently, court ruling prediction. A few papers have been published on this topic, such as the one by Katz et a"
U18-1012,U18-1011,0,0.0202833,"NLP methods in patent application processing systems as evidenced in Section 2. One such example is the use of text classification methods to categorize patent applications according to standardized taxonomies such as the International Patent Classification (IPC)2 as discussed in the studies by Benzineb and Guyot (2011); Fall et al. (2003). In this paper, we present a system to automatically categorize patent applications from Australia according to the top sections of the IPC taxonomy using a dataset provided by the organizers of the ALTA 2018 shared task on Classifying Patent Applications (Molla and Seneviratne, 2018).3 The dataset and the taxonomy are presented in more detail in Section 3. Building on our previous work (Malmasi et al., 2016a; Malmasi and Zampieri, 2017), our system is based on SVM ensembles and it achieved the highest performance of the competition. 2 Related Work There have been a number of studies applying NLP and Information Retrieval (IR) methods to patent applications specifically, and to legal texts in general, published in the last few years. Applications of NLP and IR to legal texts include the use of text summarization methods (Farzindar and Lapalme, 2004) to summarize legal docu"
U18-1012,sulea-etal-2017-predicting,1,0.874672,"Missing"
U18-1012,W04-1006,0,0.0432954,"Missing"
U18-1012,W13-1728,1,0.82671,"zers of the ALTA 2018 shared task consists of a collection of Australian patent applications. The dataset contains 5,000 documents released for training and 1,000 documents for testing. The classes relevant for the task consisted of eight different main branches of the WIPO class ontology as follows: 4.2 Features For feature extraction we used and extended the methods reported in Malmasi and Zampieri (2017). Term Frequency (TF) of n-grams with n ranging from 3 to 6 for characters and 1-2 for words have been used. Along with term frequency we calculated the inverse document frequency (TF-IDF) (Gebre et al., 2013) which resulted in the best single feature set for prediction. • A: Human necessities; • B: Performing operations, transporting; • C: Chemistry, metallurgy; 4.3 • D: Textiles, paper; Classifier We used an ensemble-based classifier for this task. Our base classifiers are linear Support Vector Machines (SVM). SVMs have proven to deliver very good performance in a number of text classification problems. It was previously used for complex word identification (Malmasi et al., 2016a), triage of forum posts (Malmasi et al., 2016b), dialect identification (Malmasi and Zampieri, 2017), hate speech dete"
U18-1012,W18-3901,1,0.894099,"Missing"
U18-1012,W18-3929,0,0.127619,"Missing"
U18-1012,S16-1154,1,0.901714,"thods to categorize patent applications according to standardized taxonomies such as the International Patent Classification (IPC)2 as discussed in the studies by Benzineb and Guyot (2011); Fall et al. (2003). In this paper, we present a system to automatically categorize patent applications from Australia according to the top sections of the IPC taxonomy using a dataset provided by the organizers of the ALTA 2018 shared task on Classifying Patent Applications (Molla and Seneviratne, 2018).3 The dataset and the taxonomy are presented in more detail in Section 3. Building on our previous work (Malmasi et al., 2016a; Malmasi and Zampieri, 2017), our system is based on SVM ensembles and it achieved the highest performance of the competition. 2 Related Work There have been a number of studies applying NLP and Information Retrieval (IR) methods to patent applications specifically, and to legal texts in general, published in the last few years. Applications of NLP and IR to legal texts include the use of text summarization methods (Farzindar and Lapalme, 2004) to summarize legal documents and most recently, court ruling prediction. A few papers have been published on this topic, such as the one by Katz et a"
U18-1012,W17-1220,1,0.848826,"This was done be replacing all consecutive non-alphanumeric characters with a single space. Next, we converted the text to lowercase and removed any tokens representing numbers. Data The dataset released by the organizers of the ALTA 2018 shared task consists of a collection of Australian patent applications. The dataset contains 5,000 documents released for training and 1,000 documents for testing. The classes relevant for the task consisted of eight different main branches of the WIPO class ontology as follows: 4.2 Features For feature extraction we used and extended the methods reported in Malmasi and Zampieri (2017). Term Frequency (TF) of n-grams with n ranging from 3 to 6 for characters and 1-2 for words have been used. Along with term frequency we calculated the inverse document frequency (TF-IDF) (Gebre et al., 2013) which resulted in the best single feature set for prediction. • A: Human necessities; • B: Performing operations, transporting; • C: Chemistry, metallurgy; 4.3 • D: Textiles, paper; Classifier We used an ensemble-based classifier for this task. Our base classifiers are linear Support Vector Machines (SVM). SVMs have proven to deliver very good performance in a number of text classificati"
W13-1728,P06-4018,0,0.154469,"or training 100 for validating 100 for testing POS n-grams Character n-grams Spelling errors 4.2 Features We explored different kinds and combinations of features that we assumed to be different for different L1 speakers and that are also commonly used in the NLI literature (Koppel et al., 2005; Tetreault et al., 2012). Table 2 shows the sources of the features we considered. Unigrams and bigrams of words are explored separately and in combination. One through four grams of part of speech tags have also been explored. For POS tagging of the essays, we applied the default POS tagger from NLTK (Bird, 2006). Spelling errors have also been treated as features. We used the collection of words in Peter Norvig’s website1 as a reference dictionary. The collection consists of about a million words. It is a concatenation of several public domain books from Project Gutenberg and lists of most frequent words from Wiktionary and the British National Corpus. Character n-grams have also been explored for both the words in the essays and for words with 1 4.2.1 Term Frequency (TF) Term Frequency refers to the number of times a particular term appears in an essay. In our experiments, terms are n-grams of chara"
W13-1728,C12-1158,0,0.201373,"ed (all characters are in lower case). These cut-off values are experimentally selected. Table 2: A summary of features used in our experiments Table 1: TOEFL11 L1 languages # of essays per L1 Word n-grams Arabic, Chinese, French, German, Hindi, Italian, Japanese, Korean, Spanish, Telugu, Turkish 900 for training 100 for validating 100 for testing POS n-grams Character n-grams Spelling errors 4.2 Features We explored different kinds and combinations of features that we assumed to be different for different L1 speakers and that are also commonly used in the NLI literature (Koppel et al., 2005; Tetreault et al., 2012). Table 2 shows the sources of the features we considered. Unigrams and bigrams of words are explored separately and in combination. One through four grams of part of speech tags have also been explored. For POS tagging of the essays, we applied the default POS tagger from NLTK (Bird, 2006). Spelling errors have also been treated as features. We used the collection of words in Peter Norvig’s website1 as a reference dictionary. The collection consists of about a million words. It is a concatenation of several public domain books from Project Gutenberg and lists of most frequent words from Wikti"
W13-1728,W13-1706,0,0.0843139,"vidual influences speech production, it should be possible to identify these traits in written language as well. NLI methods are particularly relevant for languages with a significant number of foreign speakers, most notably, English. It is estimated that the number of non-native speakers of English outnumbers the number of native speakers by two to one (Lewis et al., 2013). The written production of non-native speakers is abundant on the Internet, academia, and other contexts where English is used as lingua franca. This study presents the system that participated in the 2013 NLI Shared Task (Tetreault et al., 2013) under the name Cologne-Nijmegen. The novel aspect of the system is the use of TF-IDF weighting schemes. For this study, we experimented with a number of algorithms and features. Linear SVM and logistic regression achieved the best accuracies on the combined features of unigrams and bigrams of words. The rest of the paper will explain in detail the features, methods and results achieved. 2 Motivation There are two main reasons to study NLI. On one hand, there is a strong linguistic motivation, particularly in the field of SLA and on the other hand, there is the practical relevance of the task"
W13-1728,N01-1031,0,0.0252864,"Missing"
W13-1728,W07-0602,0,0.0169529,"Missing"
W13-1728,U09-1008,0,0.0498213,"study NLI. On one hand, there is a strong linguistic motivation, particularly in the field of SLA and on the other hand, there is the practical relevance of the task and its integration to a number of computational applications. The linguistic motivation of NLI is the possibility of using classification methods to study the inter216 Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 216–223, c Atlanta, Georgia, June 13 2013. 2013 Association for Computational Linguistics play between native and foreign language acquisition and performance (Wong and Dras, 2009). One of the SLA theories that investigate these phenomena is contrastive analysis, which is used to explain why some structures of L2 are more difficult to acquire than others (Lado, 1957). Contrastive analysis postulates that the difficulty in mastering L2 depends on the differences between L1 and L2. In the process of acquiring L2, language transfer (also known as L1 interference) occurs and speakers apply knowledge from their native language to a second language, taking advantage of their similarities. Computational methods applied to L2 written production can function as a corpus-driven m"
W14-0314,2013.mtsummit-posters.7,0,0.0171617,"ed productivity and quality of human translations using MT and TM output. The experiment was conducted starting with the hypothesis that the time invested in post-editing one string of machine translated text will correspond to the same time invested in editing a fuzzy matched string located in the 80-90 percent range. This study quantified the performance of 8 translators using a post-editing tool. According to the author, the results indicate that using a TM with 80 to 90 fuzzy matches produces more errors than using MT segments or human translation. The aforementioned recent work by Morado Vazquez et al. (2013) investigates the performance of twelve human translators (students) using the ACCEPT post-editing tool. Researchers provided MT and TM output and compared time, quality and keystroke effort. Findings of this study indicate that the use of a specific MT has a great impact in the translation activity in all three aspects. In the context of software localization, productivity was also tested by Plitt and Masselot (2010) combining MT output and a post-editing tool. Another study compared the performance of human translators in a scenario using TMs and a commercial CAT tool (Across) with a second"
W14-0314,J96-2004,0,0.117329,"Missing"
W14-0314,2013.mtsummit-wptp.13,0,0.126108,"d Work CAT tools have become very popular in the last 20 years. They are used by freelance translators as well as by companies and language service providers to increase translation’s quality and speed (Somers and Diaz, 2004; Lagoudaki, 2008). The use of CAT tools is part of the core curriculum of most translation studies degrees and a reasonable level of proficiency in the use of these tools is expected from all graduates. With the improvement of state-of-the-art MT software, a recent trend in CAT research is its integration with machine translation tools as for example the MateCat1 project (Cettolo et al., 2013). There is considerable amount of studies on MT post-editing published in the last years (Specia, 2011; Green et al., 2013). Due to the scope of our 1 www.matecat.com 93 Workshop on Humans and Computer-assisted Translation, pages 93–98, c Gothenburg, Sweden, 26 April 2014. 2014 Association for Computational Linguistics editing tool, a commercial CAT tool, the SDL Trados Studio 2014 version. A similar setting to ours was explored by Federico et al. (2012) using SDL Trados Studio integrating a commercial MT software. We took the decision of working a commercial CAT tool for two reasons: first, b"
W14-0314,P02-1040,0,0.0906101,"Missing"
W14-0314,W11-2107,0,0.0607621,"Missing"
W14-0314,2006.amta-papers.25,0,0.124997,"is possible to apply state-of-the-art metrics such as BLEU (Papineni et al., 2002) or METEOR (Denkowski and Lavie, 2011) to estimate the quality of these translations regardless of how they are produced. For machine translation output, quality nowadays is measured by automatic evaluation metrics such as the aforementioned IBM BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Denkowski and Lavie, 2011), the Levensthein (1966) distance based WER (word errorrate) metric, the position-independent error rate metric PER (Tillmann et al., 1997) and the translation error rate metric TER (Snover et al., 2006) with its newer version TERp (Snover et al., 2009). The most frequently used one is IBM BLEU (Papineni et al., 2002). It is easy to use, language-independent, fast and requires only the candidate and reference translation. IBM BLEU is based on the n-gram precision by matching the machine translation output against one or more reference translations. It accounts for adequacy and fluency through word precision, respectively the n-gram precision, by calculating the geometric mean. Instead of recall, in IBM BLEU the brevity penalty (BP) was introduced. Different from IBM BLEU, METEOR evaluates a c"
W14-0314,2012.amta-papers.22,0,0.124528,"-of-the-art MT software, a recent trend in CAT research is its integration with machine translation tools as for example the MateCat1 project (Cettolo et al., 2013). There is considerable amount of studies on MT post-editing published in the last years (Specia, 2011; Green et al., 2013). Due to the scope of our 1 www.matecat.com 93 Workshop on Humans and Computer-assisted Translation, pages 93–98, c Gothenburg, Sweden, 26 April 2014. 2014 Association for Computational Linguistics editing tool, a commercial CAT tool, the SDL Trados Studio 2014 version. A similar setting to ours was explored by Federico et al. (2012) using SDL Trados Studio integrating a commercial MT software. We took the decision of working a commercial CAT tool for two reasons: first, because this is the real-world scenario faced by translators in most companies and language service providers2 and second, because it allows us to explore a different variable that the aforementioned studies did not substantially explore, namely: MT output as TM segments. paper (and space limitation) we will deliberately not discuss the findings of these experiments and instead focus on those that involve the use of translation memories. Post-editing tool"
W14-0314,W09-0441,0,0.0488656,"as BLEU (Papineni et al., 2002) or METEOR (Denkowski and Lavie, 2011) to estimate the quality of these translations regardless of how they are produced. For machine translation output, quality nowadays is measured by automatic evaluation metrics such as the aforementioned IBM BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Denkowski and Lavie, 2011), the Levensthein (1966) distance based WER (word errorrate) metric, the position-independent error rate metric PER (Tillmann et al., 1997) and the translation error rate metric TER (Snover et al., 2006) with its newer version TERp (Snover et al., 2009). The most frequently used one is IBM BLEU (Papineni et al., 2002). It is easy to use, language-independent, fast and requires only the candidate and reference translation. IBM BLEU is based on the n-gram precision by matching the machine translation output against one or more reference translations. It accounts for adequacy and fluency through word precision, respectively the n-gram precision, by calculating the geometric mean. Instead of recall, in IBM BLEU the brevity penalty (BP) was introduced. Different from IBM BLEU, METEOR evaluates a candidate translation by calculating the precision"
W14-0314,2011.eamt-1.12,0,0.021842,"as by companies and language service providers to increase translation’s quality and speed (Somers and Diaz, 2004; Lagoudaki, 2008). The use of CAT tools is part of the core curriculum of most translation studies degrees and a reasonable level of proficiency in the use of these tools is expected from all graduates. With the improvement of state-of-the-art MT software, a recent trend in CAT research is its integration with machine translation tools as for example the MateCat1 project (Cettolo et al., 2013). There is considerable amount of studies on MT post-editing published in the last years (Specia, 2011; Green et al., 2013). Due to the scope of our 1 www.matecat.com 93 Workshop on Humans and Computer-assisted Translation, pages 93–98, c Gothenburg, Sweden, 26 April 2014. 2014 Association for Computational Linguistics editing tool, a commercial CAT tool, the SDL Trados Studio 2014 version. A similar setting to ours was explored by Federico et al. (2012) using SDL Trados Studio integrating a commercial MT software. We took the decision of working a commercial CAT tool for two reasons: first, because this is the real-world scenario faced by translators in most companies and language service pro"
W14-0314,2008.amta-srw.4,0,0.353168,"lf. This can be time consuming and the memory should ideally contain a good amount of translated segments to be considered useful and accurate. For this reason, many novice translators do not see the benefits of the use of TM right at the beginning, although it is consensual that on the long run the use of TMs increase the quality and speed of their work. To cope 2 Related Work CAT tools have become very popular in the last 20 years. They are used by freelance translators as well as by companies and language service providers to increase translation’s quality and speed (Somers and Diaz, 2004; Lagoudaki, 2008). The use of CAT tools is part of the core curriculum of most translation studies degrees and a reasonable level of proficiency in the use of these tools is expected from all graduates. With the improvement of state-of-the-art MT software, a recent trend in CAT research is its integration with machine translation tools as for example the MateCat1 project (Cettolo et al., 2013). There is considerable amount of studies on MT post-editing published in the last years (Specia, 2011; Green et al., 2013). Due to the scope of our 1 www.matecat.com 93 Workshop on Humans and Computer-assisted Translatio"
W14-0314,tiedemann-2012-parallel,0,0.0297037,"Missing"
W14-0314,2013.mtsummit-wptp.10,0,0.12638,"Missing"
W14-5307,W14-5316,0,0.368134,"Missing"
W14-5307,Y08-1042,0,0.178551,"Groups - DSL 2014 Shared Task For this collection, randomly sampled sentences from journalistic corpora (and corpora collections) were selected for each of the 13 classes. Journalistic corpora were preferred because they represent standard language, which is an important factor to be considered when working with language varieties. Other data sources (e.g. Wikipedia) do not make any distinction between language varieties and they are therefore not suitable for the purpose of the shared task. A number of studies mentioned in the related work section use journalistic texts for similar reasons (Huang and Lee, 2008; Grouin et al., 2010; Zampieri and Gebre, 2012) Given what has been said in this section, we consider the collection to be a suitable comparable corpora from this task, which was compiled to avoid bias in classification towards source, register and topics. The 5 6 We considered a token as orthographic units delimited by white spaces. http://www.loc.gov/standards/iso639-2/php/English_list.php 60 DSL corpus collection was distributed in tab delimited format; the first column contains a sentence in the language/variety, the second column states its group and the last column refers to its languag"
W14-5307,W14-5317,0,0.274993,"within each group: binary for groups B-F and one versus all for group A, which contains three classes (Bosnian, Croatian and Serbian). An interesting contribution proposed by the RAE team (Porta and Sancho, 2014) are the so-called ‘white lists’ inspired by the ‘blacklist’ classifier (Tiedemann and Ljubeˇsi´c, 2012). These lists are word lists exclusive to a language or variety, similar to one of the features that Ranaivo-Malanc¸on (2006) proposed to discriminate between Malay and Indonesian. 64 Two groups used Information Gain (IG) to select the best features for classification, namely UMich (King et al., 2014) and UniMelb-NLP (Lui et al., 2014). These teams were also the only ones to submit open submissions. The UniMelb-NLP team tried different classification methods and features (including delexicalized models) in each run. The best results were obtained by their own method, the off-the-shelf general-purpose language identification software langid.py (Lui and Baldwin, 2012). This method has been widely used for general-purpose language identification and its performance is regarded superior to similar general-purpose methods such as TextCat. In the shared task, the system was modelled hierarchical"
W14-5307,P12-3005,0,0.0284087,"or variety, similar to one of the features that Ranaivo-Malanc¸on (2006) proposed to discriminate between Malay and Indonesian. 64 Two groups used Information Gain (IG) to select the best features for classification, namely UMich (King et al., 2014) and UniMelb-NLP (Lui et al., 2014). These teams were also the only ones to submit open submissions. The UniMelb-NLP team tried different classification methods and features (including delexicalized models) in each run. The best results were obtained by their own method, the off-the-shelf general-purpose language identification software langid.py (Lui and Baldwin, 2012). This method has been widely used for general-purpose language identification and its performance is regarded superior to similar general-purpose methods such as TextCat. In the shared task, the system was modelled hierarchically firstly identifying the language group that a sentence belongs to and subsequently the specific language, achieving performance comparable to the state-of-the-art, but still slightly below the other three systems. The QMUL team (Purver, 2014) proposed a linear SVM classifier using words and characters as features. The author investigated the influence of the cost par"
W14-5307,U13-1003,0,0.384905,"between more languages1 , they still struggle to discriminate between similar languages such as Croatian and Serbian or Malay and Indonesian. From an NLP point of view, the difficulty systems face when discriminating between closely related languages is similar to the problem of discriminating between standard national language varieties (e.g. American English and British English or Brazilian Portuguese and European Portuguese), henceforth varieties. Recent studies show that language varieties can be discriminated automatically using words or characters as features (Zampieri and Gebre, 2012; Lui and Cook, 2013) . However, due to performance limitations, state-of-the-art general-purpose language identification systems do not distinguish texts from different national varieties, modelling pluricentric languages as unique classes. To evaluate how state-of-the-art systems perform in identifying similar languages and varieties, we decided to organize the Discriminating between Similar Languages (DSL)2 shared task. This shared task was organized within the scope of the workshop on Applying NLP Tools to Similar Languages, Varieties and Dialects (VarDial) in the 2014 edition of COLING. The motivation behind"
W14-5307,W14-5315,0,0.274321,"B-F and one versus all for group A, which contains three classes (Bosnian, Croatian and Serbian). An interesting contribution proposed by the RAE team (Porta and Sancho, 2014) are the so-called ‘white lists’ inspired by the ‘blacklist’ classifier (Tiedemann and Ljubeˇsi´c, 2012). These lists are word lists exclusive to a language or variety, similar to one of the features that Ranaivo-Malanc¸on (2006) proposed to discriminate between Malay and Indonesian. 64 Two groups used Information Gain (IG) to select the best features for classification, namely UMich (King et al., 2014) and UniMelb-NLP (Lui et al., 2014). These teams were also the only ones to submit open submissions. The UniMelb-NLP team tried different classification methods and features (including delexicalized models) in each run. The best results were obtained by their own method, the off-the-shelf general-purpose language identification software langid.py (Lui and Baldwin, 2012). This method has been widely used for general-purpose language identification and its performance is regarded superior to similar general-purpose methods such as TextCat. In the shared task, the system was modelled hierarchically firstly identifying the language"
W14-5307,W14-5314,0,0.685536,"ly: NRCCNRC, RAE, UMich, UniMelb-NLP and QMUL. The best scores were obtained by the NRC-CNRC (Goutte et al., 2014) team which proposed a twostep approach to predict first the language group than the language of each instance. The language group was predicted in a 6-way classification using a probabilistic model similar to a Naive Bayes classifier, and later the method applied SVM classifiers to discriminate within each group: binary for groups B-F and one versus all for group A, which contains three classes (Bosnian, Croatian and Serbian). An interesting contribution proposed by the RAE team (Porta and Sancho, 2014) are the so-called ‘white lists’ inspired by the ‘blacklist’ classifier (Tiedemann and Ljubeˇsi´c, 2012). These lists are word lists exclusive to a language or variety, similar to one of the features that Ranaivo-Malanc¸on (2006) proposed to discriminate between Malay and Indonesian. 64 Two groups used Information Gain (IG) to select the best features for classification, namely UMich (King et al., 2014) and UniMelb-NLP (Lui et al., 2014). These teams were also the only ones to submit open submissions. The UniMelb-NLP team tried different classification methods and features (including delexical"
W14-5307,W13-1706,0,0.0421907,"own (2013) reports results on a system trained to recognize more than 1,100 languages 2 http://corporavm.uni-koeln.de/vardial/sharedtask.html 58 Proceedings of the First Workshop on Applying NLP Tools to Similar Languages, Varieties and Dialects, pages 58–67, Dublin, Ireland, August 23 2014. decade in which they were published. Other related shared tasks include the ALTW 2010 multilingual language identification shared task, a general-purpose language identification task containing data from 74 languages (Baldwin and Lui, 2010) and finally the Native Language Identification (NLI) shared task (Tetreault et al., 2013) where participants were provided English essays written by foreign students of 11 different mother tongues (Blanchard et al., 2013). Participants had to train their systems to identify the native language of the writer of each text. 2 Related Work Among the first studies to investigate the question of discriminating between similar languages is the study published by Ranaivo-Malanc¸on (2006). The author presents a semi-supervised model to distinguish between Indonesian and Malay, two closely related languages from the Austronesian family represented in the DSL shared task. The study uses the"
W14-5307,C12-1160,1,0.601337,"Missing"
W14-5307,U10-1003,0,\N,Missing
W14-5307,J14-1006,0,\N,Missing
W14-5307,W14-5318,0,\N,Missing
W15-4905,P02-1040,0,0.108875,"hich use paraphrasing in TM matching and retrieval. Utiyama et al. (2011) proposed an approach using a ﬁnite state transducer. They evaluate the approach with one translator and ﬁnd that paraphrasing is useful for TM both in terms of precision and recall of the retrieval process. However, their approach limits TM matching to exact matches only. Gupta and Or˘asan (2014) also use paraphrasing at the fuzzy match level and they report an improvement in retrieval and quality of retrieved segments. The quality of retrieved segments was evaluated using the machine translation evaluation metric BLEU (Papineni et al., 2002). Simard and Fujita (2012) used different MT evaluation metrics for similarity calculation as well as for testing the quality of retrieval. For most of the metrics, the authors ﬁnd that, the metric which is used in evaluation gives better score to itself (e.g. BLEU gives highest score to matches retrieved using BLEU as similarity measure). Keystroke and post-editing time analysis are not new for TM and MT. Keystroke analysis has been used to judge translators’ productivity (Langlais and Lapalme, 2002; Whyman and Somers, 1999). 36 Koponen et al. (2012) suggested that post-editing time reﬂects t"
W15-4905,P06-1055,0,0.00708211,"at we provided one more option to translators. Translators can choose among three options: A is better; B is better; or both are equal. 7 translators participated in this experiment. 4 Corpus, Tool and Translators expertise As a TM and test data, we have used EnglishGerman pairs of the Europarl V7.0 (Koehn, 2005) corpus with English as the source language and German as the target language. From this corpus we have ﬁltered out segments of fewer than seven words and greater than 40 words, to create the TM and test datasets. Tokenization of the English data was done using the Berkeley Tokenizer (Petrov et al., 2006). We have used the lexical and phrasal paraphrases from the PPDB corpus (Ganitkevitch et al., 2013) of L size. In these experiments, we have not paraphrased any capitalised words (but we lowercase them for both baseline and paraphrasing similarities calculation). This is to avoid paraphrasing any named entities. Table 1 shows our corpus statistics. The translators involved in Segments Source words Target words TM 1565194 37824634 36267909 Test Set 9981 240916 230620 Table 1: Corpus Statistics our experiments were third year bachelor or masters translation students who were native speakers of G"
W15-4905,1999.mtsummit-1.48,0,0.202963,"rom scratch when an exact match is not available. However, this retrieval process is still limited to editdistance based measures operating on surface form c 2015 The authors. This article is licensed under a Creative  Commons 3.0 licence, no derivative works, attribution, CCBY-ND. 35 Several researchers have used semantic or syntactic information in TMs, but their evaluations were shallow and most of the time limited to subjective evaluation carried out by the authors. This makes it hard to judge how much a semantically informed TM matching system can beneﬁt a translator. Existing research (Planas and Furuse, 1999; Hod´asz and Pohl, 2005; Pekar and Mitkov, 2007; Mitkov, 2008) pointed out the need for similarity 1 http://www.omegat.org calculations in TMs beyond surface form comparisons. Both Planas and Furuse (1999) and Hodasz and Pohl (2005) proposed to use lemma and parts of speech along with surface form comparison. Hodasz and Pohl (2005) also extend the matching process to a sentence skeleton where noun phrases are either tagged by a translator or by a heuristic NP aligner developed for English-Hungarian translation. Planas and Furuse (1999) tested a prototype model on 50 sentences from the softwar"
W15-4905,aziz-etal-2012-pet,0,0.0168428,"niﬁcantly improves the retrieval results. We have also observed that there are different paraphrases used to bring about this improvement. In the interval [70, 85), 169 different paraphrases are used to retrieve 98 additional segments. To check the quality of the retrieved segments human evaluations are carried out. The sets’ distribution for human evaluation is given in the Table 3. The sets contain randomly selected segments from the additionally retrieved segments using paraphrasing which changed their top ranking.2 TH Set1 Set2 Total 4.1 Familiarisation with the Tool We used the PET tool (Aziz et al., 2012) for all our human experiments. However, settings were changed depending on the experiment. To familiarise translators with the PET tool we carried out a pilot experiment before the actual experiment with the Europarl corpus. This experiment was 100 117 16 13.67 9 24 14 100 2 5 7 [85, 100) 6 4 10 [70, 85) 6 7 13 Total 14 16 30 Table 3: Test Sets for Human Experiments 2 The sets are constructed so that a translator can post-edit a ﬁle in one sitting. There is no differentiation between the evaluations based on sets and all evaluations are carried out in both sets in a similar fashion with diffe"
W15-4905,2012.amta-papers.26,0,0.142654,"TM matching and retrieval. Utiyama et al. (2011) proposed an approach using a ﬁnite state transducer. They evaluate the approach with one translator and ﬁnd that paraphrasing is useful for TM both in terms of precision and recall of the retrieval process. However, their approach limits TM matching to exact matches only. Gupta and Or˘asan (2014) also use paraphrasing at the fuzzy match level and they report an improvement in retrieval and quality of retrieved segments. The quality of retrieved segments was evaluated using the machine translation evaluation metric BLEU (Papineni et al., 2002). Simard and Fujita (2012) used different MT evaluation metrics for similarity calculation as well as for testing the quality of retrieval. For most of the metrics, the authors ﬁnd that, the metric which is used in evaluation gives better score to itself (e.g. BLEU gives highest score to matches retrieved using BLEU as similarity measure). Keystroke and post-editing time analysis are not new for TM and MT. Keystroke analysis has been used to judge translators’ productivity (Langlais and Lapalme, 2002; Whyman and Somers, 1999). 36 Koponen et al. (2012) suggested that post-editing time reﬂects the cognitive effort in pos"
W15-4905,W14-3348,0,0.0312779,"Missing"
W15-4905,2006.amta-papers.25,0,0.154022,"Missing"
W15-4905,N13-1092,0,0.0373292,"is better; B is better; or both are equal. 7 translators participated in this experiment. 4 Corpus, Tool and Translators expertise As a TM and test data, we have used EnglishGerman pairs of the Europarl V7.0 (Koehn, 2005) corpus with English as the source language and German as the target language. From this corpus we have ﬁltered out segments of fewer than seven words and greater than 40 words, to create the TM and test datasets. Tokenization of the English data was done using the Berkeley Tokenizer (Petrov et al., 2006). We have used the lexical and phrasal paraphrases from the PPDB corpus (Ganitkevitch et al., 2013) of L size. In these experiments, we have not paraphrased any capitalised words (but we lowercase them for both baseline and paraphrasing similarities calculation). This is to avoid paraphrasing any named entities. Table 1 shows our corpus statistics. The translators involved in Segments Source words Target words TM 1565194 37824634 36267909 Test Set 9981 240916 230620 Table 1: Corpus Statistics our experiments were third year bachelor or masters translation students who were native speakers of German with English language level C1, in the age group of 21 to 40 years with a majority of female"
W15-4905,R11-1014,0,0.014323,"tion metrics for similarity calculation as well as for testing the quality of retrieval. For most of the metrics, the authors ﬁnd that, the metric which is used in evaluation gives better score to itself (e.g. BLEU gives highest score to matches retrieved using BLEU as similarity measure). Keystroke and post-editing time analysis are not new for TM and MT. Keystroke analysis has been used to judge translators’ productivity (Langlais and Lapalme, 2002; Whyman and Somers, 1999). 36 Koponen et al. (2012) suggested that post-editing time reﬂects the cognitive effort in post-editing the MT output. Sousa et al. (2011) evaluated different MT system performances against translating from scratch. Their study also concluded that subjective evaluations of MT system output correlate with the post-editing time needed. Zampieri and Vela (2014) used post-editing time to compare TM and MT translations. 3 Our Approach and Experiments We have used the approach presented in Gupta and Or˘asan (2014) to include paraphrasing in the TM matching and retrieval process. The approach classiﬁes paraphrases into different types for efﬁcient implementation based on the matching of the words between the source and corresponding pa"
W15-4905,2014.eamt-1.2,1,0.494135,"Missing"
W15-4905,2011.mtsummit-papers.37,0,0.157075,"eved was considered usable if less than half of the words required editing to match the input sentence. The authors concluded that the approach gives more usable results compared to Trados Workbench used as a baseline. Hodasz and Pohl (2005) claimed that their approach stores simpliﬁed patterns and hence makes it more probable to ﬁnd a match in the TM. Pekar and Mitkov (2007) presented an approach based on syntactic transformation rules. On evaluation of the prototype model using a query sentence, the authors found that the syntactic rules help in retrieving better segments. Recently, work by Utiyama et al. (2011) and Gupta and Or˘asan (2014) presented approaches which use paraphrasing in TM matching and retrieval. Utiyama et al. (2011) proposed an approach using a ﬁnite state transducer. They evaluate the approach with one translator and ﬁnd that paraphrasing is useful for TM both in terms of precision and recall of the retrieval process. However, their approach limits TM matching to exact matches only. Gupta and Or˘asan (2014) also use paraphrasing at the fuzzy match level and they report an improvement in retrieval and quality of retrieved segments. The quality of retrieved segments was evaluated us"
W15-4905,2005.mtsummit-papers.11,0,0.0396766,"etter. 17 translators participated in this experiment. Finally, the decision of whether ‘ED is better’ or ‘PP is better’ is made on the basis of how many translators choose one over the other. 3.3 Subjective Evaluation with Three Options (SE3) This evaluation is similar to Evaluation SE2 except that we provided one more option to translators. Translators can choose among three options: A is better; B is better; or both are equal. 7 translators participated in this experiment. 4 Corpus, Tool and Translators expertise As a TM and test data, we have used EnglishGerman pairs of the Europarl V7.0 (Koehn, 2005) corpus with English as the source language and German as the target language. From this corpus we have ﬁltered out segments of fewer than seven words and greater than 40 words, to create the TM and test datasets. Tokenization of the English data was done using the Berkeley Tokenizer (Petrov et al., 2006). We have used the lexical and phrasal paraphrases from the PPDB corpus (Ganitkevitch et al., 2013) of L size. In these experiments, we have not paraphrased any capitalised words (but we lowercase them for both baseline and paraphrasing similarities calculation). This is to avoid paraphrasing"
W15-4905,2012.amta-wptp.2,0,0.0132314,"chine translation evaluation metric BLEU (Papineni et al., 2002). Simard and Fujita (2012) used different MT evaluation metrics for similarity calculation as well as for testing the quality of retrieval. For most of the metrics, the authors ﬁnd that, the metric which is used in evaluation gives better score to itself (e.g. BLEU gives highest score to matches retrieved using BLEU as similarity measure). Keystroke and post-editing time analysis are not new for TM and MT. Keystroke analysis has been used to judge translators’ productivity (Langlais and Lapalme, 2002; Whyman and Somers, 1999). 36 Koponen et al. (2012) suggested that post-editing time reﬂects the cognitive effort in post-editing the MT output. Sousa et al. (2011) evaluated different MT system performances against translating from scratch. Their study also concluded that subjective evaluations of MT system output correlate with the post-editing time needed. Zampieri and Vela (2014) used post-editing time to compare TM and MT translations. 3 Our Approach and Experiments We have used the approach presented in Gupta and Or˘asan (2014) to include paraphrasing in the TM matching and retrieval process. The approach classiﬁes paraphrases into diffe"
W15-4905,W14-0314,1,0.749112,"ves highest score to matches retrieved using BLEU as similarity measure). Keystroke and post-editing time analysis are not new for TM and MT. Keystroke analysis has been used to judge translators’ productivity (Langlais and Lapalme, 2002; Whyman and Somers, 1999). 36 Koponen et al. (2012) suggested that post-editing time reﬂects the cognitive effort in post-editing the MT output. Sousa et al. (2011) evaluated different MT system performances against translating from scratch. Their study also concluded that subjective evaluations of MT system output correlate with the post-editing time needed. Zampieri and Vela (2014) used post-editing time to compare TM and MT translations. 3 Our Approach and Experiments We have used the approach presented in Gupta and Or˘asan (2014) to include paraphrasing in the TM matching and retrieval process. The approach classiﬁes paraphrases into different types for efﬁcient implementation based on the matching of the words between the source and corresponding paraphrase. Using this approach, the fuzzy match score between segments can be calculated in polynomial time despite the inclusion of paraphrases. The method uses dynamic programming along with greedy approximation. The meth"
W15-4905,2012.eamt-1.31,0,\N,Missing
W15-4905,2012.tc-1.5,0,\N,Missing
W15-4916,2012.eamt-1.33,1,0.851202,"Missing"
W15-4916,C04-1046,0,0.147834,"h levels. We highlight crucial limitations of such metrics for this task, mainly the fact that they disregard the discourse structure of the texts. To better understand these limitations, we designed experiments with human annotators and proposed a way of quantifying differences in translation quality that can only be observed when sentences are judged in the context of entire documents or paragraphs. Our results indicate that the use of context can lead to more informative labels for quality annotation beyond sentence level. 1 Introduction Quality estimation (QE) of machine translation (MT) (Blatz et al., 2004; Specia et al., 2009) is an area that focuses on predicting the quality of new, unseen machine translation data without relying on human references. This is done by training models using features extracted from source and target texts and, when available, from the MT system, along with a quality label for each instance. Most current work on QE is done at the sentence level. A popular application of sentence-level QE is to support post-editing of MT (He et al., 2010). As quality labels, Likert scores have been used for post-editing effort, as well as post-editing time and edit distance between"
W15-4916,W12-3156,0,0.103809,"sing automatic metrics such as BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) as quality labels. However, their results highlighted issues with these metrics for the task at hand: the evaluation of the scores predicted in terms of mean error was inconclusive. In most cases, the prediction model only slightly improves over a simple baseline where the average BLEU or TER score of the training documents is assigned to all test documents. Other studies have considered document-level information in order to improve, analyse or au121 tomatically evaluate MT output (not for QE purposes). Carpuat and Simard (2012) report that MT output is overall consistent in its lexical choices, nearly as consistent as manually translated texts. Meyer and Webber (2013) and Li et al. (2014) show that the translation of connectives differs from humans to MT, and that the presence of explicit connectives correlates with higher HTER values. Guzm´an et al. (2014) explore rhetorical structure (RST) trees (Mann and Thompson, 1987) for automatic evaluation of MT into English, outperforming traditional metrics at system-level evaluation. Thus far, no previous work has investigated ways to provide a global quality score for an"
W15-4916,P14-1065,0,0.0528997,"Missing"
W15-4916,P10-1064,1,0.914135,"Missing"
W15-4916,P14-2047,0,0.0154408,"the task at hand: the evaluation of the scores predicted in terms of mean error was inconclusive. In most cases, the prediction model only slightly improves over a simple baseline where the average BLEU or TER score of the training documents is assigned to all test documents. Other studies have considered document-level information in order to improve, analyse or au121 tomatically evaluate MT output (not for QE purposes). Carpuat and Simard (2012) report that MT output is overall consistent in its lexical choices, nearly as consistent as manually translated texts. Meyer and Webber (2013) and Li et al. (2014) show that the translation of connectives differs from humans to MT, and that the presence of explicit connectives correlates with higher HTER values. Guzm´an et al. (2014) explore rhetorical structure (RST) trees (Mann and Thompson, 1987) for automatic evaluation of MT into English, outperforming traditional metrics at system-level evaluation. Thus far, no previous work has investigated ways to provide a global quality score for an entire document that takes into account document structure, without access to reference translations. Previous work on document-level QE use automatic evaluation m"
W15-4916,W13-3303,0,0.0260337,"ssues with these metrics for the task at hand: the evaluation of the scores predicted in terms of mean error was inconclusive. In most cases, the prediction model only slightly improves over a simple baseline where the average BLEU or TER score of the training documents is assigned to all test documents. Other studies have considered document-level information in order to improve, analyse or au121 tomatically evaluate MT output (not for QE purposes). Carpuat and Simard (2012) report that MT output is overall consistent in its lexical choices, nearly as consistent as manually translated texts. Meyer and Webber (2013) and Li et al. (2014) show that the translation of connectives differs from humans to MT, and that the presence of explicit connectives correlates with higher HTER values. Guzm´an et al. (2014) explore rhetorical structure (RST) trees (Mann and Thompson, 1987) for automatic evaluation of MT into English, outperforming traditional metrics at system-level evaluation. Thus far, no previous work has investigated ways to provide a global quality score for an entire document that takes into account document structure, without access to reference translations. Previous work on document-level QE use a"
W15-4916,P02-1040,0,0.0919032,"c and structure of the document and the relationship between its sentences. While certain sentences are considered perfect in isolation, their combination in context may lead to incoherent text. Conversely, while a sentence can be considered poor in isolation, when put in context, it may beneﬁt from information in surrounding sentences, leading to a document that is ﬁt for purpose. Document-level quality prediction is a rather understudied problem. Recent work has looked into document-level prediction (Scarton and Specia, 2014; Soricut and Echihabi, 2010) using automatic metrics such as BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) as quality labels. However, their results highlighted issues with these metrics for the task at hand: the evaluation of the scores predicted in terms of mean error was inconclusive. In most cases, the prediction model only slightly improves over a simple baseline where the average BLEU or TER score of the training documents is assigned to all test documents. Other studies have considered document-level information in order to improve, analyse or au121 tomatically evaluate MT output (not for QE purposes). Carpuat and Simard (2012) report that MT output is overall"
W15-4916,P09-2004,0,0.0711715,"ndomly selected from the full corpus. For PE1 and PE2, only source (English) paragraphs with 3-8 sentences were selected (ﬁlter SNUMBER) to ensure that there is enough information beyond sentence-level to be evaluated and make the task feasible for the annotators. These paragraphs were further ﬁltered to select those with cohesive devices. Cohesive devices are linguistic units that play a role in establishing cohesion between clauses, sentences or paragraphs (Halliday and Hasan, 1976). Pronouns and discourse connectives are examples of such devices. A list of pronouns and the connectives from Pitler and Nenkova (2009) was considered for that. Finally, paragraphs were ranked according to the number of cohesive devices they contain and the top 200 paragraphs were selected (ﬁlter C-DEV). Table 3 shows the statistics of the initial corpus and the resulting selection after each ﬁlter. FULL CORPUS S-NUMBER C-DEV Number of Paragraphs 1, 215 394 200 Number of Cohesive devices 6, 488 3, 329 2, 338 Table 3: WMT13 English source corpus. For the PE1 experiment, the paragraphs in CDEV were randomised. Then, sets containing seven paragraphs each were created. For each set, the sentences of its paragraphs were also rando"
W15-4916,potet-etal-2012-collection,0,0.104329,"Missing"
W15-4916,2014.eamt-1.21,1,0.850194,"consider more information than sentence-level quality. This includes, for example, the topic and structure of the document and the relationship between its sentences. While certain sentences are considered perfect in isolation, their combination in context may lead to incoherent text. Conversely, while a sentence can be considered poor in isolation, when put in context, it may beneﬁt from information in surrounding sentences, leading to a document that is ﬁt for purpose. Document-level quality prediction is a rather understudied problem. Recent work has looked into document-level prediction (Scarton and Specia, 2014; Soricut and Echihabi, 2010) using automatic metrics such as BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) as quality labels. However, their results highlighted issues with these metrics for the task at hand: the evaluation of the scores predicted in terms of mean error was inconclusive. In most cases, the prediction model only slightly improves over a simple baseline where the average BLEU or TER score of the training documents is assigned to all test documents. Other studies have considered document-level information in order to improve, analyse or au121 tomatically evaluate MT"
W15-4916,2006.amta-papers.25,0,0.549835,"es on predicting the quality of new, unseen machine translation data without relying on human references. This is done by training models using features extracted from source and target texts and, when available, from the MT system, along with a quality label for each instance. Most current work on QE is done at the sentence level. A popular application of sentence-level QE is to support post-editing of MT (He et al., 2010). As quality labels, Likert scores have been used for post-editing effort, as well as post-editing time and edit distance between the MT output and the ﬁnal version – HTER (Snover et al., 2006). c 2015 The authors. This article is licensed under a Creative � Commons 3.0 licence, no derivative works, attribution, CCBY-ND. There are, however, scenarios where quality prediction beyond sentence level is needed, most notably in cases when automatic translations without post-editing are required. This is the case, for example, of quality prediction for an entire product review translation in order to decide whether or not it can be published as is, so that customers speaking other languages can understand it. The quality of a document is often seen as some form of aggregation of the quali"
W15-4916,P10-1063,0,0.677539,"than sentence-level quality. This includes, for example, the topic and structure of the document and the relationship between its sentences. While certain sentences are considered perfect in isolation, their combination in context may lead to incoherent text. Conversely, while a sentence can be considered poor in isolation, when put in context, it may beneﬁt from information in surrounding sentences, leading to a document that is ﬁt for purpose. Document-level quality prediction is a rather understudied problem. Recent work has looked into document-level prediction (Scarton and Specia, 2014; Soricut and Echihabi, 2010) using automatic metrics such as BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) as quality labels. However, their results highlighted issues with these metrics for the task at hand: the evaluation of the scores predicted in terms of mean error was inconclusive. In most cases, the prediction model only slightly improves over a simple baseline where the average BLEU or TER score of the training documents is assigned to all test documents. Other studies have considered document-level information in order to improve, analyse or au121 tomatically evaluate MT output (not for QE purposes)"
W15-4916,2009.eamt-1.5,1,0.889526,"ht crucial limitations of such metrics for this task, mainly the fact that they disregard the discourse structure of the texts. To better understand these limitations, we designed experiments with human annotators and proposed a way of quantifying differences in translation quality that can only be observed when sentences are judged in the context of entire documents or paragraphs. Our results indicate that the use of context can lead to more informative labels for quality annotation beyond sentence level. 1 Introduction Quality estimation (QE) of machine translation (MT) (Blatz et al., 2004; Specia et al., 2009) is an area that focuses on predicting the quality of new, unseen machine translation data without relying on human references. This is done by training models using features extracted from source and target texts and, when available, from the MT system, along with a quality label for each instance. Most current work on QE is done at the sentence level. A popular application of sentence-level QE is to support post-editing of MT (He et al., 2010). As quality labels, Likert scores have been used for post-editing effort, as well as post-editing time and edit distance between the MT output and the"
W15-4916,D14-1025,0,0.053746,"Missing"
W15-4916,C14-2028,0,\N,Missing
W15-4916,W13-2201,1,\N,Missing
W15-5206,2013.mtsummit-wptp.13,0,0.122457,"2011; Gupta and Orăsan, 2014; Gupta et al., 2015), as well as syntax (Clark, 2002; Gotti et al., 2005) in this process. Another recent direction that research in CAT tools is taking is the integration of both TM and machine translation (MT) output (He et al., 2010; Kanavos and Kartsaklis, 2010). With the improvement of state-ofthe-art MT systems, MT output is no longer used just for gisting, it is now being used in real-world translation projects. Taking advantage of these improvements, CAT tools such as MateCat1 , have been integrating MT output along TMs in the list of suitable suggestions (Cettolo et al., 2013). In this paper we are concerned both with retrieval and with the post-editing interface of TMs. We present a new CAT tool called CATaLog2 , which is language pair independent and allows users to upload their own memories in This paper explores a new TM-based CAT tool entitled CATaLog. New features have been integrated into the tool which aim to improve post-editing both in terms of performance and productivity. One of the new features of CATaLog is a color coding scheme that is based on the similarity between a particular input sentence and the segments retrieved from the TM. This color codin"
W15-5206,2012.amta-papers.22,0,0.124202,"presented using English - Bengali data. 2 ments and mismatched portions are translated by an SMT system to ﬁll in the gaps. Even though this paper describes work in progress, our aim is to develop a tool that is as intuitive as possible for end users and this should have direct impact on translators’ performance and productivity. In the recent years, several productive studies were also carried out measuring diﬀerent aspects of the translation process such as cognitive load, eﬀort, time, quality as well as other criteria (Bowker, 2005; O’Brien, 2006; Guerberof, 2009; Plitt and Masselot, 2010; Federico et al., 2012; Guerberof, 2012; Zampieri and Vela, 2014). User studies were taken into account when developing CATaLog as our main motivation is to improve the translation workﬂow. In this paper, however, we do not yet explore the impact of our tool in the translation process, because the functionalities required for this kind of study are currently under development in CATaLog. Future work aims to investigate the impact of the new features we are proposing on the translator’s work. Related Work CAT tools have become very popular in the translation and localization industries in the last two decades. They"
W15-5206,2014.eamt-1.2,0,0.105775,"retrieved segments suggested by the CAT tool or translating new segments from scratch. This process is done iteratively and every new translation increases the size of the translation memory making it both more useful and more helpful to future translations. Although in the ﬁrst place it might sound very simplistic, the process of matching source and target segments, and retrieving translated segments from the TM is far from trivial. To improve the retrieval engines, researchers have been working on diﬀerent ways of incorporating semantic knowledge, such as paraphrasing (Utiyama et al., 2011; Gupta and Orăsan, 2014; Gupta et al., 2015), as well as syntax (Clark, 2002; Gotti et al., 2005) in this process. Another recent direction that research in CAT tools is taking is the integration of both TM and machine translation (MT) output (He et al., 2010; Kanavos and Kartsaklis, 2010). With the improvement of state-ofthe-art MT systems, MT output is no longer used just for gisting, it is now being used in real-world translation projects. Taking advantage of these improvements, CAT tools such as MateCat1 , have been integrating MT output along TMs in the list of suitable suggestions (Cettolo et al., 2013). In th"
W15-5206,2009.mtsummit-papers.14,0,0.0410983,"achines) to decide which output (TM or MT) is most suitable to use for post-editing. Work on integrating MT with TM has also been done to make TM output more suitable for post-editing diminishing translators’ eﬀort (Kanavos and Kartsaklis, 2010). Another study presented a Dynamic Translation Memory which identiﬁes the longest common subsequence in the the closest matching source segment, identiﬁes the corresponding subsequence in its translation, and dynamically adds this source-target phrase pair to the phrase table of a phrasebased ststistical MT (PB-SMT) system (Biçici and Dymetman, 2008). Simard and Isabelle (2009) reported a work on integration of PB-SMT with TM technology in a CAT environment in which the PBSMT system exploits the most similar matches by making use of TM-based feature functions. Koehn and Senellart (2010) reported another MT-TM integration strategy where TM is used to retrieve matching source seg3 System Description We demonstrate the functionalities and features of CATaLog in an English - Bengali translation task. The TM database consists of English sentences taken from BTEC3 (Basic Travel Expression Corpus) corpus and their Bengali translations4 . Unseen input or test segments are p"
W15-5206,W15-4905,1,0.742271,"Missing"
W15-5206,W09-0441,0,0.0370839,"is used to retrieve matching source seg3 System Description We demonstrate the functionalities and features of CATaLog in an English - Bengali translation task. The TM database consists of English sentences taken from BTEC3 (Basic Travel Expression Corpus) corpus and their Bengali translations4 . Unseen input or test segments are provided to the post-editing tool and the tool matches each of the input segments to the most similar segments contained in the TM. TM segments are then ranked according their the similarity to the test sentence using the popular Translation Error Rate (TER) metric (Snover et al., 2009). The top 5 most similar segments are chosen and presented to the translator ordered by their similarity. One very important aspect of computing similarity is alignment. Each test (input) segment in the source language (SL) is aligned with the reference SL sentences in the TM and each SL sentence in the TM is aligned to its respective translation. From these two sets 3 BTEC corpus contains tourism-related sentences similar to those that are usually found in phrase books for tourists going abroad 4 Work in progress. 37 of alignments we apply a method to ﬁnd out which parts of the translation ar"
W15-5206,P10-1064,1,0.924209,"Missing"
W15-5206,2011.eamt-1.12,0,0.0128172,"udy are currently under development in CATaLog. Future work aims to investigate the impact of the new features we are proposing on the translator’s work. Related Work CAT tools have become very popular in the translation and localization industries in the last two decades. They are used by many language service providers, freelance translators to improve translation quality and to increase translator’s productivity (Lagoudaki, 2008). Although the work presented in this paper focuses on TM, it should also be noted that there were many studies on MT post-editing published in the last few years (Specia, 2011; Green et al., 2013; Green, 2014) and as mentioned in the last section, one of the recent trends is the development of hybrid systems that are able to combine MT with TM output. Therefore work on MT post-editing presents signiﬁcant overlap with state-of-the-art CAT tools and to what we propose in this paper. Substantial work have also been carried out on improving translation recommendation systems which recommends post-editors either to use TM output or MT output (He et al., 2010). To achieve good performance with this kind of systems, researchers typically train a binary classiﬁer (e.g., Su"
W15-5206,2010.jec-1.3,0,0.508262,"lthough in the ﬁrst place it might sound very simplistic, the process of matching source and target segments, and retrieving translated segments from the TM is far from trivial. To improve the retrieval engines, researchers have been working on diﬀerent ways of incorporating semantic knowledge, such as paraphrasing (Utiyama et al., 2011; Gupta and Orăsan, 2014; Gupta et al., 2015), as well as syntax (Clark, 2002; Gotti et al., 2005) in this process. Another recent direction that research in CAT tools is taking is the integration of both TM and machine translation (MT) output (He et al., 2010; Kanavos and Kartsaklis, 2010). With the improvement of state-ofthe-art MT systems, MT output is no longer used just for gisting, it is now being used in real-world translation projects. Taking advantage of these improvements, CAT tools such as MateCat1 , have been integrating MT output along TMs in the list of suitable suggestions (Cettolo et al., 2013). In this paper we are concerned both with retrieval and with the post-editing interface of TMs. We present a new CAT tool called CATaLog2 , which is language pair independent and allows users to upload their own memories in This paper explores a new TM-based CAT tool entit"
W15-5206,2011.mtsummit-papers.37,0,0.259019,"editors by correcting retrieved segments suggested by the CAT tool or translating new segments from scratch. This process is done iteratively and every new translation increases the size of the translation memory making it both more useful and more helpful to future translations. Although in the ﬁrst place it might sound very simplistic, the process of matching source and target segments, and retrieving translated segments from the TM is far from trivial. To improve the retrieval engines, researchers have been working on diﬀerent ways of incorporating semantic knowledge, such as paraphrasing (Utiyama et al., 2011; Gupta and Orăsan, 2014; Gupta et al., 2015), as well as syntax (Clark, 2002; Gotti et al., 2005) in this process. Another recent direction that research in CAT tools is taking is the integration of both TM and machine translation (MT) output (He et al., 2010; Kanavos and Kartsaklis, 2010). With the improvement of state-ofthe-art MT systems, MT output is no longer used just for gisting, it is now being used in real-world translation projects. Taking advantage of these improvements, CAT tools such as MateCat1 , have been integrating MT output along TMs in the list of suitable suggestions (Cett"
W15-5206,W14-0314,1,0.778874,"2 ments and mismatched portions are translated by an SMT system to ﬁll in the gaps. Even though this paper describes work in progress, our aim is to develop a tool that is as intuitive as possible for end users and this should have direct impact on translators’ performance and productivity. In the recent years, several productive studies were also carried out measuring diﬀerent aspects of the translation process such as cognitive load, eﬀort, time, quality as well as other criteria (Bowker, 2005; O’Brien, 2006; Guerberof, 2009; Plitt and Masselot, 2010; Federico et al., 2012; Guerberof, 2012; Zampieri and Vela, 2014). User studies were taken into account when developing CATaLog as our main motivation is to improve the translation workﬂow. In this paper, however, we do not yet explore the impact of our tool in the translation process, because the functionalities required for this kind of study are currently under development in CATaLog. Future work aims to investigate the impact of the new features we are proposing on the translator’s work. Related Work CAT tools have become very popular in the translation and localization industries in the last two decades. They are used by many language service providers"
W15-5206,2010.jec-1.4,0,0.0590105,"t (Kanavos and Kartsaklis, 2010). Another study presented a Dynamic Translation Memory which identiﬁes the longest common subsequence in the the closest matching source segment, identiﬁes the corresponding subsequence in its translation, and dynamically adds this source-target phrase pair to the phrase table of a phrasebased ststistical MT (PB-SMT) system (Biçici and Dymetman, 2008). Simard and Isabelle (2009) reported a work on integration of PB-SMT with TM technology in a CAT environment in which the PBSMT system exploits the most similar matches by making use of TM-based feature functions. Koehn and Senellart (2010) reported another MT-TM integration strategy where TM is used to retrieve matching source seg3 System Description We demonstrate the functionalities and features of CATaLog in an English - Bengali translation task. The TM database consists of English sentences taken from BTEC3 (Basic Travel Expression Corpus) corpus and their Bengali translations4 . Unseen input or test segments are provided to the post-editing tool and the tool matches each of the input segments to the most similar segments contained in the TM. TM segments are then ranked according their the similarity to the test sentence us"
W15-5206,2008.amta-srw.4,0,0.397037,"e the translation workﬂow. In this paper, however, we do not yet explore the impact of our tool in the translation process, because the functionalities required for this kind of study are currently under development in CATaLog. Future work aims to investigate the impact of the new features we are proposing on the translator’s work. Related Work CAT tools have become very popular in the translation and localization industries in the last two decades. They are used by many language service providers, freelance translators to improve translation quality and to increase translator’s productivity (Lagoudaki, 2008). Although the work presented in this paper focuses on TM, it should also be noted that there were many studies on MT post-editing published in the last few years (Specia, 2011; Green et al., 2013; Green, 2014) and as mentioned in the last section, one of the recent trends is the development of hybrid systems that are able to combine MT with TM output. Therefore work on MT post-editing presents signiﬁcant overlap with state-of-the-art CAT tools and to what we propose in this paper. Substantial work have also been carried out on improving translation recommendation systems which recommends post"
W15-5206,N06-1014,0,0.0673427,"he top candidates by the TM. Figure 1 presents a snapshot of CATaLog. Color Coding Among the top 5 choices, post-editor selects one reference translation to do the post-editing task. To make that decision process easy, we color code the matched parts and unmatched parts in each reference translation. Green portion implies that they are matched fragments and red portion implies a mismatch. The alignments between the TM source sentences and their corresponding translations are generated using GIZA++ (Och and Ney, 2003) in the present work. However, any other word aligner, e.g., Berkley Aligner (Liang et al., 2006), could be used to produce this alignment. The alignment between the matched source segment and the corresponding translation, together with the TER alignment between the input sentence and the matched source segment, are used to generate the aforementioned color coding between selected source and target sentences. The GIZA++ alignment ﬁle is directly fed into the present TM tool. Given below is an example TM sentence pair along with the corresponding word alignment input to the TM. Input: you gave me wrong number . • English: we want to have a table near the window . Source Matches: 1. you ga"
W15-5206,J03-1002,0,0.00872292,"ssign a higher cost for insertion than deletion, and hence such sentences will not be shown as the top candidates by the TM. Figure 1 presents a snapshot of CATaLog. Color Coding Among the top 5 choices, post-editor selects one reference translation to do the post-editing task. To make that decision process easy, we color code the matched parts and unmatched parts in each reference translation. Green portion implies that they are matched fragments and red portion implies a mismatch. The alignments between the TM source sentences and their corresponding translations are generated using GIZA++ (Och and Ney, 2003) in the present work. However, any other word aligner, e.g., Berkley Aligner (Liang et al., 2006), could be used to produce this alignment. The alignment between the matched source segment and the corresponding translation, together with the TER alignment between the input sentence and the matched source segment, are used to generate the aforementioned color coding between selected source and target sentences. The GIZA++ alignment ﬁle is directly fed into the present TM tool. Given below is an example TM sentence pair along with the corresponding word alignment input to the TM. Input: you gave"
W15-5206,2012.amta-papers.26,0,0.0145925,". | D S S || S | | | we - would like a table by the window . For ﬁnding out the similar and dissimilar parts between the test segment and a matching TM segment, we use TER alignments. TER is an error metric and it gives an edit ratio (often referred to as edit rate or error rate) in terms of how much editing is required to convert a sentence into another with respect to the length of the ﬁrst sentence. Allowable edit operations include insert, delete, substitute and shift. We use the TER metric (using tercom-7.2515 ) to ﬁnd the edit rate between a test sentence and the TM reference sentences. Simard and Fujita (2012) ﬁrst proposed the use of MT evaluation metrics as similarity functions in implementing TM functionality. They experimented with several MT evaluation metrics, viz. BLEU, NIST, Meteor and TER, and studied their behaviors on TM performance. In the TM tool presented here we use TER as the similarity metric as it is very fast and lightweight and it directly mimics the human post-editing eﬀort. Moreover, the tercom-7.251 package also produces the alignments between the sentence pair from which it is very easy to identify which portions in the matching segment match with the input sentence and whic"
W15-5206,2015.eamt-1.6,1,\N,Missing
W15-5401,Y08-1042,0,0.153038,"iscusses related work, Section 3 describes the general setup of the task, Section 4 presents the results of the competition, Section 5 summarizes the approaches used by the participants, and Section 6 offers conclusions. 2 Related Work 2.2 Language identification has attracted a lot of research attention in recent years, covering a number of similar languages and language varieties such as Malay and Indonesian (RanaivoMalanc¸on, 2006), Persian and Dari (Malmasi and Dras, 2015a), Brazilian and European Portuguese (Zampieri and Gebre, 2012), varieties of Mandarin in China, Taiwan and Singapore (Huang and Lee, 2008), and English varieties (Lui and Cook, 2013), among others. This interest has eventually given rise to special shared tasks, which allowed researchers to compare and benchmark various approaches on common standard datasets. Below we will describe some of these shared tasks, including the first edition of the DSL task. 2.1 The First Edition of the DSL Task For the first edition of the task, we compiled the DSL Corpus Collection (Tan et al., 2014), or DSLCC v.1.0, which included excerpts from journalistic texts from sources such as the SETimes Corpus1 (Tyers and Alperen, 2010), HC Corpora2 and t"
W15-5401,W15-5408,0,0.229488,"Missing"
W15-5401,W15-5410,0,0.0986295,"sk track, we further made available DSLCC v2.1, which extended DSLCC v2.0 with Mexican Spanish and Macanese Portuguese data. 6 The script we used to substitute named entities with placeholders is available here: https://github.com/ Simdiva/DSL-Task/blob/master/blindNE.py 4 Date May 20, 2015 June 22, 2015 June 24, 2015 June 26, 2015 July 20, 2015 Table 3: The DSL 2015 Shared Task schedule. 4 Team BOICEV BRUNIBP INRIA MAC MMS* NLEL NRC OSEVAL PRHLT SUKI Total Closed (Normal) X X X X X X X X X 9 Closed (No NEs) X X X X X X X 7 Open (Normal) X X X 3 Open (No NEs) X X X 3 System Description Paper (Bobicev, 2015) ´ et al., 2015) (Acs (Malmasi and Dras, 2015b) (Zampieri et al., 2015) (Fabra-Boluda et al., 2015) (Goutte and L´eger, 2015) (Franco-Salvador et al., 2015) (Jauhiainen et al., 2015a) 8 Table 4: The participating teams in the DSL 2015 Shared Task. 4 Rank 1 2-3 2-3 4 5 6 7 8 9 Results In this section, we present the results of the 2nd edition of the DSL shared task.7 Most of the participating teams used DSLCC v2.0 only, and thus took part in the closed submission track. Yet, three of the teams collected additional data or used DSLCC v1.0, and thereby participated in the open submission. 4.1 Sub"
W15-5401,D14-1069,0,0.183053,"Missing"
W15-5401,W14-5317,0,0.204271,"Missing"
W15-5401,W15-5409,0,0.310823,"Spanish and Macanese Portuguese data. 6 The script we used to substitute named entities with placeholders is available here: https://github.com/ Simdiva/DSL-Task/blob/master/blindNE.py 4 Date May 20, 2015 June 22, 2015 June 24, 2015 June 26, 2015 July 20, 2015 Table 3: The DSL 2015 Shared Task schedule. 4 Team BOICEV BRUNIBP INRIA MAC MMS* NLEL NRC OSEVAL PRHLT SUKI Total Closed (Normal) X X X X X X X X X 9 Closed (No NEs) X X X X X X X 7 Open (Normal) X X X 3 Open (No NEs) X X X 3 System Description Paper (Bobicev, 2015) ´ et al., 2015) (Acs (Malmasi and Dras, 2015b) (Zampieri et al., 2015) (Fabra-Boluda et al., 2015) (Goutte and L´eger, 2015) (Franco-Salvador et al., 2015) (Jauhiainen et al., 2015a) 8 Table 4: The participating teams in the DSL 2015 Shared Task. 4 Rank 1 2-3 2-3 4 5 6 7 8 9 Results In this section, we present the results of the 2nd edition of the DSL shared task.7 Most of the participating teams used DSLCC v2.0 only, and thus took part in the closed submission track. Yet, three of the teams collected additional data or used DSLCC v1.0, and thereby participated in the open submission. 4.1 Submitted Runs Accuracy 95.54 95.24 95.24 94.67 94.14 93.66 92.74 83.91 64.04 Table 5: Closed submissi"
W15-5401,W15-5403,0,0.148019,"used to substitute named entities with placeholders is available here: https://github.com/ Simdiva/DSL-Task/blob/master/blindNE.py 4 Date May 20, 2015 June 22, 2015 June 24, 2015 June 26, 2015 July 20, 2015 Table 3: The DSL 2015 Shared Task schedule. 4 Team BOICEV BRUNIBP INRIA MAC MMS* NLEL NRC OSEVAL PRHLT SUKI Total Closed (Normal) X X X X X X X X X 9 Closed (No NEs) X X X X X X X 7 Open (Normal) X X X 3 Open (No NEs) X X X 3 System Description Paper (Bobicev, 2015) ´ et al., 2015) (Acs (Malmasi and Dras, 2015b) (Zampieri et al., 2015) (Fabra-Boluda et al., 2015) (Goutte and L´eger, 2015) (Franco-Salvador et al., 2015) (Jauhiainen et al., 2015a) 8 Table 4: The participating teams in the DSL 2015 Shared Task. 4 Rank 1 2-3 2-3 4 5 6 7 8 9 Results In this section, we present the results of the 2nd edition of the DSL shared task.7 Most of the participating teams used DSLCC v2.0 only, and thus took part in the closed submission track. Yet, three of the teams collected additional data or used DSLCC v1.0, and thereby participated in the open submission. 4.1 Submitted Runs Accuracy 95.54 95.24 95.24 94.67 94.14 93.66 92.74 83.91 64.04 Table 5: Closed submission results for test set A. A total of 24 teams subscribed"
W15-5401,I11-1062,0,0.0379892,"Missing"
W15-5401,P12-3005,0,0.0283147,"Missing"
W15-5401,W13-1728,1,0.611887,"e group, and then chooses between languages or varieties within this group. The team achieved very strong results this year, ranking second in the closed submission on test set A, third on test set B, and first in the open submission on both test sets A and B. Two other participants used two-stage classification: NLEL (Fabra-Boluda et al., 2015) and ´ et al., 2015). BRUniBP (Acs The MMS team experimented with three approaches (Zampieri et al., 2015), and their best run combined TF.IDF weighting and an SVM classifier, which was previously successfully applied to native language identification (Gebre et al., 2013). The SUKI team (Jauhiainen et al., 2015a) used token-based backoff, which was previously applied to general-purpose language identification (Jauhiainen et al., 2015b). The BOBICEV team applied prediction by partial matching, which had not been used for this task before (Bobicev, 2015). Finally, the PRHLT team (Franco-Salvador et al., 2015) used word and sentence vectors, which is to our knowledge the first attempt to apply them to discriminating between similar languages. Table 7: Open submission results for test set A. This could be related to the availability of DSLCC v1.0 as an obvious add"
W15-5401,U13-1003,0,0.339119,"e general setup of the task, Section 4 presents the results of the competition, Section 5 summarizes the approaches used by the participants, and Section 6 offers conclusions. 2 Related Work 2.2 Language identification has attracted a lot of research attention in recent years, covering a number of similar languages and language varieties such as Malay and Indonesian (RanaivoMalanc¸on, 2006), Persian and Dari (Malmasi and Dras, 2015a), Brazilian and European Portuguese (Zampieri and Gebre, 2012), varieties of Mandarin in China, Taiwan and Singapore (Huang and Lee, 2008), and English varieties (Lui and Cook, 2013), among others. This interest has eventually given rise to special shared tasks, which allowed researchers to compare and benchmark various approaches on common standard datasets. Below we will describe some of these shared tasks, including the first edition of the DSL task. 2.1 The First Edition of the DSL Task For the first edition of the task, we compiled the DSL Corpus Collection (Tan et al., 2014), or DSLCC v.1.0, which included excerpts from journalistic texts from sources such as the SETimes Corpus1 (Tyers and Alperen, 2010), HC Corpora2 and the Leipzig Corpora Collection (Biemann et al"
W15-5401,W15-5413,0,0.538185,"Missing"
W15-5401,W14-5315,0,0.370837,"Missing"
W15-5401,W14-5316,0,0.374689,"Missing"
W15-5401,W15-5407,0,0.1816,"ke the task more challenging and less dependent on the text topic and domain. The remainder of this paper is organized as follows: Section 2 discusses related work, Section 3 describes the general setup of the task, Section 4 presents the results of the competition, Section 5 summarizes the approaches used by the participants, and Section 6 offers conclusions. 2 Related Work 2.2 Language identification has attracted a lot of research attention in recent years, covering a number of similar languages and language varieties such as Malay and Indonesian (RanaivoMalanc¸on, 2006), Persian and Dari (Malmasi and Dras, 2015a), Brazilian and European Portuguese (Zampieri and Gebre, 2012), varieties of Mandarin in China, Taiwan and Singapore (Huang and Lee, 2008), and English varieties (Lui and Cook, 2013), among others. This interest has eventually given rise to special shared tasks, which allowed researchers to compare and benchmark various approaches on common standard datasets. Below we will describe some of these shared tasks, including the first edition of the DSL task. 2.1 The First Edition of the DSL Task For the first edition of the task, we compiled the DSL Corpus Collection (Tan et al., 2014), or DSLCC"
W15-5401,C12-1160,1,0.783877,"Missing"
W15-5401,tiedemann-2012-parallel,1,0.851577,"Missing"
W15-5401,W14-5314,0,0.282427,"Missing"
W15-5401,xia-etal-2010-problems,0,0.0651996,"Missing"
W15-5401,W14-5904,0,0.0750336,"Missing"
W15-5401,W14-5307,1,0.737281,"Missing"
W15-5401,W15-5411,1,0.87912,"Missing"
W15-5401,W14-2505,0,0.0277162,"raining and development subsets, and we further prepared two test sets, as described in Section 3.3 below. As in 2014, teams could make two types of submissions (for each team, we allowed up to three runs per submission type; in the official ranking, we included the run with the highest score only): • Closed submission: Using only the DSLCC v2.0 for training. • Open submission: Using any dataset other than DSLCC v2.0 for training.3 3.2 The Unshared Task Track Along with the Shared Task, this year we proposed an Unshared Task track inspired by the unshared task in PoliInformatics held in 2014 (Smith et al., 2014). For this track, teams were allowed to use any version of DSLCC to investigate differences between similar languages and language varieties using NLP methods. We were interested in studying questions like these: • Are there fundamental grammatical differences in a language group? • What are the most distinctive lexical choices for each language? • Which text representation is most suitable to investigate language variation? • What is the impact of lexical and grammatical variation on NLP applications? Although eleven teams subscribed for the Unshared Task track, none of them ended up submitin"
W15-5401,W14-3907,0,0.0457835,"2015. 2015 Association for Computational Linguistics Another popular research direction has been on language identification on Twitter, which was driven by interest in geolocation prediction for end-user applications (Ljubeˇsi´c and Kranjˇci´c, 2015). This interest has given rise to the TweetLID shared task (Zubiaga et al., 2014), which asked participants to recognize the language of tweet messages, focusing on English and on languages spoken on the Iberian peninsula such as Basque, Catalan, Spanish, and Portuguese. The Shared Task on Language Identification in CodeSwitched Data held in 2014 (Solorio et al., 2014) is another related competition, where the focus was on tweets in which users were mixing two or more languages in the same tweet. First, in order to simulate a real-world language identification scenario, we included in the testing dataset some languages that were not present in the training dataset. Moreover, we included a second test set, where we substituted the named entities with placeholders to make the task more challenging and less dependent on the text topic and domain. The remainder of this paper is organized as follows: Section 2 discusses related work, Section 3 describes the gene"
W15-5401,U10-1003,0,\N,Missing
W15-5401,W14-5318,0,\N,Missing
W15-5411,W13-1728,1,0.737778,"ma a` s indulgˆencias vendidas pelo #NE# na #NE# #NE# quando os fi´eis compravam a redenc¸a˜ o das suas almas dando dinheiro aos padres. Code bs hr sr id my cz sk pt-BR pt-PT es-AR es-ES bg mk xx 4 Approach Given that each team was allowed to submit a maximum of three runs to each track (closed and open), we decided to take this opportunity to test and compare different approaches. To do that, we developed three systems based on team MMS-member’s previous work in language identification and related tasks. The first two systems were previously used for the Native Language Identification (NLI) (Gebre et al., 2013) and the third one has been applied to language variety identification. The following is a list of the three systems and the their corresponding submission runs: Table 1: DSL corpus by language and variety. In detail, the corpus collection contains 308,000 short text excerpts sampled from journalistic texts • Run 1 - Logistic Regression with TF-IDF Weighting 2 For the sake of simplicity, we refer to both languages and language varieties as languages. 67 • Run 2 - SVM with TF-IDF Weighting texts is not a good discriminator, and should be given less weight than one which occurs in fewer texts. T"
W15-5411,W14-5316,0,0.187078,"Missing"
W15-5411,Y08-1042,0,0.329428,"ithms and words and characters as features to solve the task. Unlike general-purpose language identification, most of the systems trained to discriminate between similar languages perform best using high order character n-grams and word n-gram representations. Different groups or pairs of similar languages and language varieties have been studied using data from different sources such as standard contemporary newspapers and social media. Recent studies include: Indian languages (Murthy and Kumar, 2006), Malay and Indonesian (Ranaivo-Malanc¸on, 2006), Mainland, Singapore and Taiwanese Chinese (Huang and Lee, 2008), Brazilian and European Portuguese (Zampieri and Gebre, 2012), South Slavic languages (Tiedemann Introduction Automatic language identification is an important task in Natural Language Processing (NLP), which consists of applying computational methods to identify the language a document is written in. Language identification is often modelled as a classification task and it is often the first processing stage of many NLP applications and pipelines. Although language identification is largely considered to be a solved task, recent studies have shown that language identification systems often f"
W15-5411,W14-5317,0,0.0889212,"tion of Arabic dialects (Elfardy and Diab, 2014; Zaidan and CallisonBurch, 2014; Tillmann et al., 2014; Sadat et al., 2014; Salloum et al., 2014; Malmasi et al., 2015). From a purely engineering perspective, discriminating between dialects poses the same challenges as the discrimination between similar languages and language varieties. 3 (1) The DSL Task Regarding the choice of only participating in the closed submission track, we first analysed the results of the 2014 edition where we realised that only two teams decided to participate in both open and closed submission tracks, namely UMich (King et al., 2014) and UniMelb-NLP (Lui et al., 2014). Both of them had better performance in the closed submission track and reported that more training data does not necessarily lead to higher performance and that the features learned by the classifiers are, to a certain extent, dataset specific. Therefore, we decided to use only the dataset provided by the organisers and only participate in the closed submission track. The shared task organisers provided all participants with an updated version of the DSL corpus collection v.2.0 (DSLCC) (Tan et al., 2014). This corpus is composed of 14 classes, 13 languages2"
W15-5411,I11-1062,0,0.0209894,"nguages (Tiedemann Introduction Automatic language identification is an important task in Natural Language Processing (NLP), which consists of applying computational methods to identify the language a document is written in. Language identification is often modelled as a classification task and it is often the first processing stage of many NLP applications and pipelines. Although language identification is largely considered to be a solved task, recent studies have shown that language identification systems often fail to achieve satisfactory performance across different datasets and domains (Lui and Baldwin, 2011), particularly with: datasets containing short pieces of texts such as tweets (Zubiaga et al., 2014); code-switching data (Solorio et al., 2014); or when discriminating between very similar languages (Zampieri et al., 2014). Given these challenges, the Discriminating between Similar Languages (DSL) shared task provides an excellent opportunity for researchers interested in evaluating and comparing their 1 MMS is an acronym for our affiliations/locations (Malaga, Munich and Saarland). In the shared task report (Zampieri et al., 2015) the team is displayed as MMS*. The * indicates that a shared"
W15-5411,U13-1003,0,0.432873,"lass are divided into 3 partitions, i.e. 18,000, 2,000 and 2,000 instances for training, development and testing, respectively. The test set is further subdivided into two test sets (A and B), each one containing 1,000 instances. While the test set A contains original texts, the organisers replaced named entities for place holders in the set B in order to decrease thematic bias in the classification process. Below we present an example of a Portuguese instance containing place holders #NE# instead of the named entities. and Ljubeˇsi´c, 2012; Ljubeˇsi´c and Kranjˇci´c, 2015) English varieties (Lui and Cook, 2013), Spanish varieties (Zampieri et al., 2013; Maier and G´omezRodrıguez, 2014), and Persian and Dari (Malmasi and Dras, 2015). Over the last few years there has been a significant increase of interest in the computational processing of Arabic. This is evidenced by a number of research papers on different NLP tasks and applications including the identification/discrimination of Arabic dialects (Elfardy and Diab, 2014; Zaidan and CallisonBurch, 2014; Tillmann et al., 2014; Sadat et al., 2014; Salloum et al., 2014; Malmasi et al., 2015). From a purely engineering perspective, discriminating between"
W15-5411,W14-5315,0,0.142253,"Diab, 2014; Zaidan and CallisonBurch, 2014; Tillmann et al., 2014; Sadat et al., 2014; Salloum et al., 2014; Malmasi et al., 2015). From a purely engineering perspective, discriminating between dialects poses the same challenges as the discrimination between similar languages and language varieties. 3 (1) The DSL Task Regarding the choice of only participating in the closed submission track, we first analysed the results of the 2014 edition where we realised that only two teams decided to participate in both open and closed submission tracks, namely UMich (King et al., 2014) and UniMelb-NLP (Lui et al., 2014). Both of them had better performance in the closed submission track and reported that more training data does not necessarily lead to higher performance and that the features learned by the classifiers are, to a certain extent, dataset specific. Therefore, we decided to use only the dataset provided by the organisers and only participate in the closed submission track. The shared task organisers provided all participants with an updated version of the DSL corpus collection v.2.0 (DSLCC) (Tan et al., 2014). This corpus is composed of 14 classes, 13 languages2 and one class containing documents"
W15-5411,W14-4204,0,0.156387,"Missing"
W15-5411,C12-1160,0,0.0994912,"Missing"
W15-5411,W14-5313,0,0.123685,"ining place holders #NE# instead of the named entities. and Ljubeˇsi´c, 2012; Ljubeˇsi´c and Kranjˇci´c, 2015) English varieties (Lui and Cook, 2013), Spanish varieties (Zampieri et al., 2013; Maier and G´omezRodrıguez, 2014), and Persian and Dari (Malmasi and Dras, 2015). Over the last few years there has been a significant increase of interest in the computational processing of Arabic. This is evidenced by a number of research papers on different NLP tasks and applications including the identification/discrimination of Arabic dialects (Elfardy and Diab, 2014; Zaidan and CallisonBurch, 2014; Tillmann et al., 2014; Sadat et al., 2014; Salloum et al., 2014; Malmasi et al., 2015). From a purely engineering perspective, discriminating between dialects poses the same challenges as the discrimination between similar languages and language varieties. 3 (1) The DSL Task Regarding the choice of only participating in the closed submission track, we first analysed the results of the 2014 edition where we realised that only two teams decided to participate in both open and closed submission tracks, namely UMich (King et al., 2014) and UniMelb-NLP (Lui et al., 2014). Both of them had better performance in the clos"
W15-5411,E14-4004,1,0.879058,"Missing"
W15-5411,zampieri-gebre-2014-varclass,1,0.743895,"Missing"
W15-5411,W14-5904,0,0.0694914,"# instead of the named entities. and Ljubeˇsi´c, 2012; Ljubeˇsi´c and Kranjˇci´c, 2015) English varieties (Lui and Cook, 2013), Spanish varieties (Zampieri et al., 2013; Maier and G´omezRodrıguez, 2014), and Persian and Dari (Malmasi and Dras, 2015). Over the last few years there has been a significant increase of interest in the computational processing of Arabic. This is evidenced by a number of research papers on different NLP tasks and applications including the identification/discrimination of Arabic dialects (Elfardy and Diab, 2014; Zaidan and CallisonBurch, 2014; Tillmann et al., 2014; Sadat et al., 2014; Salloum et al., 2014; Malmasi et al., 2015). From a purely engineering perspective, discriminating between dialects poses the same challenges as the discrimination between similar languages and language varieties. 3 (1) The DSL Task Regarding the choice of only participating in the closed submission track, we first analysed the results of the 2014 edition where we realised that only two teams decided to participate in both open and closed submission tracks, namely UMich (King et al., 2014) and UniMelb-NLP (Lui et al., 2014). Both of them had better performance in the closed submission track"
W15-5411,W14-5307,1,0.894268,"Missing"
W15-5411,P14-2125,0,0.0407051,"ed entities. and Ljubeˇsi´c, 2012; Ljubeˇsi´c and Kranjˇci´c, 2015) English varieties (Lui and Cook, 2013), Spanish varieties (Zampieri et al., 2013; Maier and G´omezRodrıguez, 2014), and Persian and Dari (Malmasi and Dras, 2015). Over the last few years there has been a significant increase of interest in the computational processing of Arabic. This is evidenced by a number of research papers on different NLP tasks and applications including the identification/discrimination of Arabic dialects (Elfardy and Diab, 2014; Zaidan and CallisonBurch, 2014; Tillmann et al., 2014; Sadat et al., 2014; Salloum et al., 2014; Malmasi et al., 2015). From a purely engineering perspective, discriminating between dialects poses the same challenges as the discrimination between similar languages and language varieties. 3 (1) The DSL Task Regarding the choice of only participating in the closed submission track, we first analysed the results of the 2014 edition where we realised that only two teams decided to participate in both open and closed submission tracks, namely UMich (King et al., 2014) and UniMelb-NLP (Lui et al., 2014). Both of them had better performance in the closed submission track and reported that more"
W15-5411,W15-5401,1,0.864728,"Missing"
W15-5411,W14-3907,0,0.0381874,"plying computational methods to identify the language a document is written in. Language identification is often modelled as a classification task and it is often the first processing stage of many NLP applications and pipelines. Although language identification is largely considered to be a solved task, recent studies have shown that language identification systems often fail to achieve satisfactory performance across different datasets and domains (Lui and Baldwin, 2011), particularly with: datasets containing short pieces of texts such as tweets (Zubiaga et al., 2014); code-switching data (Solorio et al., 2014); or when discriminating between very similar languages (Zampieri et al., 2014). Given these challenges, the Discriminating between Similar Languages (DSL) shared task provides an excellent opportunity for researchers interested in evaluating and comparing their 1 MMS is an acronym for our affiliations/locations (Malaga, Munich and Saarland). In the shared task report (Zampieri et al., 2015) the team is displayed as MMS*. The * indicates that a shared task organiser is a team member. 66 Proceedings of the Joint Workshop on Language Technology for Closely Related Languages, Varieties and Dialec"
W15-5411,W13-1706,0,0.0469809,"mostly because they are frequent in both varieties. On the other hand, words like London or rubbish might not be as frequent as the, of, and, yet, they are better discriminative words for British English. Therefore, the actual importance of a term for this task depends on how infrequent the term is in other texts. This can be modelled using Inverse Document Frequency (IDF). IDF is based on the assumption that a term which occurs in many 4.2 Classifiers Systems developed for Run 1 and Run 2 were previously used in the Native Language Identification (NLI) (Gebre et al., 2013) shared task 2013 (Tetreault et al., 2013) by the CologneNijmegen team with good results. They both rely on the TF-IDF weighting scheme combined with two different classifiers. For Run 1, we opt for Logistic Regression using the LIBLINEAR open source library (Fan et al., 2008) from scikit-learn (Pedregosa et al., 2011) and fix the regularisation parameter to 100.0. This regression algorithm has been used in different classification problems including for example temporal text classification (Niculae et al., 2014). 3 The TF-IDF description presented in this section is based on our previous work (Gebre et al., 2013) 4 In our experiments"
W16-0314,W14-3207,0,0.0144592,"native language (Gebre et al., 2013; Malmasi and Dras, 2015a), age and gender (Nguyen et al., 2013), and even economic conditions such as income (Preot¸iuc-Pietro et al., 2015). These tasks are often considered to be a part of a broader natural language processing task known as authorship profiling (Rangel et al., 2013). More recently, such approaches have been applied to investigating psychological factors associated with the author of a text. For practical purposes most of the applications that deal with clinical psychology use social media data such as Twitter, Facebook, and online forums (Coppersmith et al., 2014). Examples of health and psychological conditions studied using texts and social media are: suicide risk (Thompson et al., 2014), depression (Schwartz et al., 2014), autism (Tanaka et al., 2014; Rouhizadeh et al., 2015), and schizophrenia (Mitchell et al., 2015). In this paper we propose an approach to predict the severity of posts in a mental health online forum. Posts were classified into for levels of severity (or urgency) represented by the labels green, amber, red, and crisis according to indication of risky or harmful behavior by users (e.g. self-harm, suicide, etc.). This kind of classi"
W16-0314,W13-1728,1,0.766386,"individual classifiers as well as an ensemble classifier. This meta-classifier was then extended to a Random Forest of meta-classifiers, yielding further improvements in classification accuracy. We achieved competitive results, ranking first among a total of 60 submitted entries in the competition. 1 Introduction Computational methods have been widely used to extract and/or predict a number of phenomena in text documents. It has been shown that algorithms are able to learn a wide range of information about the authors of texts as well. This includes, for example, the author’s native language (Gebre et al., 2013; Malmasi and Dras, 2015a), age and gender (Nguyen et al., 2013), and even economic conditions such as income (Preot¸iuc-Pietro et al., 2015). These tasks are often considered to be a part of a broader natural language processing task known as authorship profiling (Rangel et al., 2013). More recently, such approaches have been applied to investigating psychological factors associated with the author of a text. For practical purposes most of the applications that deal with clinical psychology use social media data such as Twitter, Facebook, and online forums (Coppersmith et al., 2014). Examples"
W16-0314,guthrie-etal-2006-closer,0,0.0183918,"prioritize addressing this post. • Amber: a moderator needs to look at this and assess if there are enough responses and support from others or if they should reply. • Red: a moderator needs to look at this as soon as possible and take action. • Crisis: the author (or someone they know) might hurt themselves or others (a red instance that is of urgent importance). Participating systems should be trained to predict these labels, with evaluation on the test set. • Word skip-grams: To capture the longer distance dependencies not covered by word ngrams we also used word skip-grams as described in Guthrie et al. (2006). We extract 1, 2 and 3-skip word bigrams. • Lemma n-grams: we used a lemmatized version of the texts and extract lemma n-grams of order 1–3. • Word Representations: To increase the generalizability of our models we used word representation features based on Brown clustering as a form of semi-supervised learning. This was done using the method described by Malmasi et al. (2015a). We used the clusters generated by Owoputi et al. (2013). They collected From 56 million English tweets (837 million tokens) and used it to generate 1,000 hierarchical clusters over 217 thousand words. 3.3 Feature Extr"
W16-0314,E14-4019,1,0.894839,"Missing"
W16-0314,W15-5407,1,0.746304,"rs as well as an ensemble classifier. This meta-classifier was then extended to a Random Forest of meta-classifiers, yielding further improvements in classification accuracy. We achieved competitive results, ranking first among a total of 60 submitted entries in the competition. 1 Introduction Computational methods have been widely used to extract and/or predict a number of phenomena in text documents. It has been shown that algorithms are able to learn a wide range of information about the authors of texts as well. This includes, for example, the author’s native language (Gebre et al., 2013; Malmasi and Dras, 2015a), age and gender (Nguyen et al., 2013), and even economic conditions such as income (Preot¸iuc-Pietro et al., 2015). These tasks are often considered to be a part of a broader natural language processing task known as authorship profiling (Rangel et al., 2013). More recently, such approaches have been applied to investigating psychological factors associated with the author of a text. For practical purposes most of the applications that deal with clinical psychology use social media data such as Twitter, Facebook, and online forums (Coppersmith et al., 2014). Examples of health and psycholog"
W16-0314,U15-1008,1,0.837443,"systems should be trained to predict these labels, with evaluation on the test set. • Word skip-grams: To capture the longer distance dependencies not covered by word ngrams we also used word skip-grams as described in Guthrie et al. (2006). We extract 1, 2 and 3-skip word bigrams. • Lemma n-grams: we used a lemmatized version of the texts and extract lemma n-grams of order 1–3. • Word Representations: To increase the generalizability of our models we used word representation features based on Brown clustering as a form of semi-supervised learning. This was done using the method described by Malmasi et al. (2015a). We used the clusters generated by Owoputi et al. (2013). They collected From 56 million English tweets (837 million tokens) and used it to generate 1,000 hierarchical clusters over 217 thousand words. 3.3 Feature Extraction We used three categories of features: lexical, syntactic, and metadata features. These features and our preprocessing method are outlined here. 3.1 Lexical Features • Character n-grams: we extracted n-grams of order 2–8. Table 1: CLPsych Corpus Divided by Data Set 3 3.2 Preprocessing The following preprocessing was performed on the texts: HTML removal was performed, wit"
W16-0314,S16-1154,1,0.836465,"asi and Dras, 2015b). Run Run 1 Run 2 Run 3 Run 4 Run 5 Official Accuracy F-score Accuracy Rank Score (NG vs. G) (NG vs. G) 0.37 0.80 0.83 0.89 11th 0.38 0.80 0.83 0.89 9th 0.42 0.83 0.87 0.91 1st 0.42 0.84 0.87 0.91 1st 0.40 0.82 0.85 0.90 6th Table 2: Official CLPsych scores. Best results in bold. Rankings are out of the 60 systems submitted. Classifiers ensembles have proven to be an efficient and robust alternative in other text classification tasks such as language identification (Malmasi and Dras, 2015a), grammatical error detection (Xiang et al., 2015), and complex word identification (Malmasi et al., 2016). 4.2 Meta-classifier For our meta-classifier, We experimented with three algorithms: Random Forests of decision trees, a linear SVM just like our base classifiers and a Radial Basis Function (RBF) kernel SVM. The inputs to the meta-classifier are the continuous outputs from each base SVM classifier in our ensemble, along with the original gold label. For the Random Forest classifiers, the final label is selected through a plurality voting process across all decision trees in the forest. All were found to perform well, but the linear SVM was was outperformed by its RBF-kernel counterpart. This"
W16-0314,W15-1202,0,0.0122244,"task known as authorship profiling (Rangel et al., 2013). More recently, such approaches have been applied to investigating psychological factors associated with the author of a text. For practical purposes most of the applications that deal with clinical psychology use social media data such as Twitter, Facebook, and online forums (Coppersmith et al., 2014). Examples of health and psychological conditions studied using texts and social media are: suicide risk (Thompson et al., 2014), depression (Schwartz et al., 2014), autism (Tanaka et al., 2014; Rouhizadeh et al., 2015), and schizophrenia (Mitchell et al., 2015). In this paper we propose an approach to predict the severity of posts in a mental health online forum. Posts were classified into for levels of severity (or urgency) represented by the labels green, amber, red, and crisis according to indication of risky or harmful behavior by users (e.g. self-harm, suicide, etc.). This kind of classification task serves to provide automatic triage of user posts in order to help moderators of forums and related online communities to respond to urgent posts. Our approach competed in the CLPsych 2016 shared task and achieved the highest accuracy among submitte"
W16-0314,N13-1039,0,0.0716559,"Missing"
W16-0314,W15-1214,0,0.0518374,"Missing"
W16-0314,W14-3214,0,0.158613,"Missing"
W16-0314,W14-3211,0,0.0226416,"considered to be a part of a broader natural language processing task known as authorship profiling (Rangel et al., 2013). More recently, such approaches have been applied to investigating psychological factors associated with the author of a text. For practical purposes most of the applications that deal with clinical psychology use social media data such as Twitter, Facebook, and online forums (Coppersmith et al., 2014). Examples of health and psychological conditions studied using texts and social media are: suicide risk (Thompson et al., 2014), depression (Schwartz et al., 2014), autism (Tanaka et al., 2014; Rouhizadeh et al., 2015), and schizophrenia (Mitchell et al., 2015). In this paper we propose an approach to predict the severity of posts in a mental health online forum. Posts were classified into for levels of severity (or urgency) represented by the labels green, amber, red, and crisis according to indication of risky or harmful behavior by users (e.g. self-harm, suicide, etc.). This kind of classification task serves to provide automatic triage of user posts in order to help moderators of forums and related online communities to respond to urgent posts. Our approach competed in the CLPs"
W16-0314,W14-3201,0,0.146759,"uch as income (Preot¸iuc-Pietro et al., 2015). These tasks are often considered to be a part of a broader natural language processing task known as authorship profiling (Rangel et al., 2013). More recently, such approaches have been applied to investigating psychological factors associated with the author of a text. For practical purposes most of the applications that deal with clinical psychology use social media data such as Twitter, Facebook, and online forums (Coppersmith et al., 2014). Examples of health and psychological conditions studied using texts and social media are: suicide risk (Thompson et al., 2014), depression (Schwartz et al., 2014), autism (Tanaka et al., 2014; Rouhizadeh et al., 2015), and schizophrenia (Mitchell et al., 2015). In this paper we propose an approach to predict the severity of posts in a mental health online forum. Posts were classified into for levels of severity (or urgency) represented by the labels green, amber, red, and crisis according to indication of risky or harmful behavior by users (e.g. self-harm, suicide, etc.). This kind of classification task serves to provide automatic triage of user posts in order to help moderators of forums and related online communit"
W16-0314,W15-4415,0,0.0452785,"s (Malmasi and Dras, 2014; Malmasi et al., 2015b; Malmasi and Dras, 2015b). Run Run 1 Run 2 Run 3 Run 4 Run 5 Official Accuracy F-score Accuracy Rank Score (NG vs. G) (NG vs. G) 0.37 0.80 0.83 0.89 11th 0.38 0.80 0.83 0.89 9th 0.42 0.83 0.87 0.91 1st 0.42 0.84 0.87 0.91 1st 0.40 0.82 0.85 0.90 6th Table 2: Official CLPsych scores. Best results in bold. Rankings are out of the 60 systems submitted. Classifiers ensembles have proven to be an efficient and robust alternative in other text classification tasks such as language identification (Malmasi and Dras, 2015a), grammatical error detection (Xiang et al., 2015), and complex word identification (Malmasi et al., 2016). 4.2 Meta-classifier For our meta-classifier, We experimented with three algorithms: Random Forests of decision trees, a linear SVM just like our base classifiers and a Radial Basis Function (RBF) kernel SVM. The inputs to the meta-classifier are the continuous outputs from each base SVM classifier in our ensemble, along with the original gold label. For the Random Forest classifiers, the final label is selected through a plurality voting process across all decision trees in the forest. All were found to perform well, but the linear SVM"
W16-2301,W13-3520,0,0.0148252,"Missing"
W16-2301,2011.mtsummit-papers.35,0,0.440323,"Missing"
W16-2301,W15-3001,1,0.419439,"Missing"
W16-2301,W16-2305,0,0.0273073,"Missing"
W16-2301,L16-1662,0,0.00889007,"introduce a new set of features for the sentence-Level QE task. The features extracted include three alignment-based features, three bilingual embedding-based features, two embeddingbased features constrained on alignment links, as well as a set of 74 bigrams used as boolean features. The set of bigrams represents the most frequent bigrams in translations that have changed after the post-edition, and they are compiled by aligning translations to their post-editions provided in the WMT QE datasets. To produce these features, GIZA++ (Och and Ney, 2003) was used for word alignment and Multivec (Berard et al., 2016) was used for the bilingual model, which jointly learns distributed representations for source and target languages using a parallel corpus. To build the bilingual model, domain-specific data compiled from the resources made available for the WMT 16 IT-Domain shared task was used. As prediction model, a Linear Regression model using scikit-learn was built using a combination of QuEst++ baseline features and the new features proposed. tional features for both tasks. UNBABEL (Task 2): Two systems were submitted for the word-level task. UNBABEL 2 linear is a feature-based linear sequential model."
W16-2301,W11-2101,1,0.851508,"Missing"
W16-2301,W16-2306,0,0.0263259,"Missing"
W16-2301,W16-2382,0,0.0451767,"Missing"
W16-2301,W16-2302,1,0.346609,"Missing"
W16-2301,W16-2307,1,0.756182,"Missing"
W16-2301,W16-2308,0,0.0373021,"Missing"
W16-2301,buck-etal-2014-n,0,0.0150538,"Missing"
W16-2301,W13-2201,1,0.287085,"Missing"
W16-2301,W14-3302,1,0.399147,"Missing"
W16-2301,W14-3340,1,0.419045,"Missing"
W16-2301,W07-0718,1,0.759293,"oth automatically and manually. The human evaluation (§3) involves asking human judges to rank sentences output by anonymized systems. We obtained large numbers of rankings from researchers who contributed The quality estimation task had three subtasks, with a total of 14 teams, submitting 39 entries. The automatic post-editing task had a total of 6 teams, submitting 11 entries. 1 Introduction We present the results of the shared tasks of the First Conference on Statistical Machine Translation (WMT) held at ACL 2016. This conference builds on nine previous WMT workshops (Koehn and Monz, 2006; Callison-Burch et al., 2007, 2008, 2009, 2010, 2011, 2012; Bojar et al., 2013, 2014, 2015). 131 Proceedings of the First Conference on Machine Translation, Volume 2: Shared Task Papers, pages 131–198, c Berlin, Germany, August 11-12, 2016. 2016 Association for Computational Linguistics to refine evaluation and estimation methodologies for machine translation. As before, all of the data, translations, and collected human judgments are publicly available.1 We hope these datasets serve as a valuable resource for research into statistical machine translation and automatic evaluation or prediction of translation quality. New"
W16-2301,W15-3025,1,0.888663,"used for the Task 2p. The submissions to the word-level task are modified in order to comply with the phrase-level task. sequences. Nevertheless, it is worth noticing the phraselevel QE systems introduced a number of interesting strategies that allowed them to outperform a strong baseline phrase-level model. Finally, we recall that the evaluation metric – word-level F1 mult – has difficulties to distinguish phrase-level systems. This suggests that we may need to find a different metric for evaluation of the phrase-level task, with phrase-level F1 -mult one of the candidates. 7 pointed out by Chatterjee et al. (2015b), from the application point of view the task is motivated by its possible uses to: • Improve MT output by exploiting information unavailable to the decoder, or by performing deeper text analysis that is too expensive at the decoding stage; • Cope with systematic errors of an MT system whose decoding process is not accessible; • Provide professional translators with improved MT output quality to reduce (human) post-editing effort; Automatic Post-editing Task This year WMT hosted the second round of the shared task on MT automatic post-editing (APE), which consists in automatically correcting"
W16-2301,P15-2026,1,0.895785,"used for the Task 2p. The submissions to the word-level task are modified in order to comply with the phrase-level task. sequences. Nevertheless, it is worth noticing the phraselevel QE systems introduced a number of interesting strategies that allowed them to outperform a strong baseline phrase-level model. Finally, we recall that the evaluation metric – word-level F1 mult – has difficulties to distinguish phrase-level systems. This suggests that we may need to find a different metric for evaluation of the phrase-level task, with phrase-level F1 -mult one of the candidates. 7 pointed out by Chatterjee et al. (2015b), from the application point of view the task is motivated by its possible uses to: • Improve MT output by exploiting information unavailable to the decoder, or by performing deeper text analysis that is too expensive at the decoding stage; • Cope with systematic errors of an MT system whose decoding process is not accessible; • Provide professional translators with improved MT output quality to reduce (human) post-editing effort; Automatic Post-editing Task This year WMT hosted the second round of the shared task on MT automatic post-editing (APE), which consists in automatically correcting"
W16-2301,W08-0309,1,0.809107,"Missing"
W16-2301,W16-2309,0,0.0305938,"Missing"
W16-2301,W10-1703,1,0.312318,"Missing"
W16-2301,W16-2336,0,0.0356983,"Missing"
W16-2301,W12-3102,1,0.235434,"ir native language. Consequently, health professionals may use the translated information to make clinical decisions impacting patients care. It is vital that translation systems do not contribute to the dissemination of incorrect clinical information. Therefore, the evaluation of biomedical translation systems should include an assessment at the document level indicating whether a translation conveyed erroneous clinical information. 6 Quality Estimation The fifth edition of the WMT shared task on quality estimation (QE) of machine translation (MT) builds on the previous editions of the task (Callison-Burch et al., 2012; Bojar et al., 2013, 2014, 2015), with “traditional” tasks at sentence and word levels, a new task for entire documents quality prediction, and a variant of the word-level task: phrase-level estimation. The goals of this year’s shared task were: • To advance work on sentence and wordlevel quality estimation by providing domainspecific, larger and professionally annotated datasets. • To analyse the effectiveness of different types of quality labels provided by humans for longer texts in document-level prediction. • To investigate quality estimation at a new level of granularity: phrases. Plan"
W16-2301,W16-2330,0,0.0328047,"Missing"
W16-2301,W16-2310,1,0.835279,"Missing"
W16-2301,W11-2103,1,0.65163,"Missing"
W16-2301,W16-2331,0,0.0203904,"cance test results for pairs of systems competing in the news domain translation task (en-ru), where a green cell denotes a significantly higher DA adequacy score for the system in a given row over the system in a given column. cs-en fi-en tr-en de-en ru-en ro-en 0.997 0.996 0.988 0.964 0.961 0.920 en-ru 0.975 DA Correlation with RR Table 11: Correlation between overall DA standardized mean adequacy scores and RR Trueskill scores. 150 4 IT Translation Task from the QTLeap project: HF&FCUL for Portuguese, UPV/EHU for Spanish and Basque, IICT-BAS for Bulgarian, CUNI for Czech and UG for Dutch). Duma and Menzel (2016) describe UHDS- DOC 2 VEC and UHBS- LMI (University of Hamburg). Pahari et al. (2016) describe JU-USAAR (Jadavpur University & Saarland University). Cuong et al. (2016) describe ILLC-U VA-S CORPIO (University of Amsterdam). IILC-U VA-DS is based on Hoang and Sima’an (2014). PROMT-RULE - BASED and PROMT-H YBRID systems were submitted by the PROMT LLC company and they are not described in any paper. QTL -M OSES is the standard Moses setup (MERT-tuned on the in-domain training data, but otherwise without any domain-adaptation) and serves as a baseline. The IT-domain translation task introduced th"
W16-2301,W15-3009,1,0.788321,"Missing"
W16-2301,P15-1174,1,0.28154,"endently inserted in another part of this sentence, i.e. to correct an unrelated error. The statistics of the datasets are outlined in Table 20. Evaluation Evaluation was performed against the true HTER label and/or ranking, using the following metrics: • Scoring: Pearson’s r correlation score (primary metric, official score for ranking submissions), Mean Average Error (MAE) and Root Mean Squared Error (RMSE). • Ranking: Spearman’s ρ rank correlation and DeltaAvg. Statistical significance on Pearson r and Spearman rho was computed using the William’s test, following the approach suggested in (Graham, 2015). Results Table 19 summarises the results for Task 1, ranking participating systems best to worst using Pearson’s r correlation as primary key. Spearman’s ρ correlation scores should be used to rank systems according to the ranking variant. We note that three systems have not submitted results ranking evaluation variant. 6.4 Task 2: Predicting word-level quality The goal of this task is to evaluate the extent to which we can detect word-level errors in MT output. Various classes of errors can be found in translations, but for this task we consider all error types together, aiming at making a b"
W16-2301,W16-2311,0,0.0122367,"016) PROMT Automated Translation Solutions (Molchanov and Bykov, 2016) QT21 System Combination (Peter et al., 2016b) RWTH Aachen (Peter et al., 2016a) ¨ TUBITAK (Bektas¸ et al., 2016) University of Edinburgh (Sennrich et al., 2016) University of Edinburgh (Williams et al., 2016) UEDIN - SYNTAX UEDIN - LMU UH -* USFD - RESCORING UUT YSDA ONLINE -[ A , B , F, G ] University of Edinburgh / University of Munich (Huck et al., 2016) University of Helsinki (Tiedemann et al., 2016) University of Sheffield (Blain et al., 2016) Uppsala University (Tiedemann et al., 2016) Yandex School of Data Analysis (Dvorkovich et al., 2016) Four online statistical machine translation systems Table 2: Participants in the shared translation task. Not all teams participated in all language pairs. The translations from the commercial and online systems were not submitted by their respective companies but were obtained by us, and are therefore anonymized in a fashion consistent with previous years of the workshop. 136 Figure 2: Screenshot of the Appraise interface used in the human evaluation campaign. The annotator is presented with a source segment, a reference translation, and up to five outputs from competing systems (anonymized"
W16-2301,W13-2305,1,0.509145,"Missing"
W16-2301,W15-3036,0,0.0603245,"Missing"
W16-2301,E14-1047,1,0.831614,"Missing"
W16-2301,W16-2378,0,0.168663,"ts for pairs of systems competing in the news domain translation task (en-ru), where a green cell denotes a significantly higher DA adequacy score for the system in a given row over the system in a given column. cs-en fi-en tr-en de-en ru-en ro-en 0.997 0.996 0.988 0.964 0.961 0.920 en-ru 0.975 DA Correlation with RR Table 11: Correlation between overall DA standardized mean adequacy scores and RR Trueskill scores. 150 4 IT Translation Task from the QTLeap project: HF&FCUL for Portuguese, UPV/EHU for Spanish and Basque, IICT-BAS for Bulgarian, CUNI for Czech and UG for Dutch). Duma and Menzel (2016) describe UHDS- DOC 2 VEC and UHBS- LMI (University of Hamburg). Pahari et al. (2016) describe JU-USAAR (Jadavpur University & Saarland University). Cuong et al. (2016) describe ILLC-U VA-S CORPIO (University of Amsterdam). IILC-U VA-DS is based on Hoang and Sima’an (2014). PROMT-RULE - BASED and PROMT-H YBRID systems were submitted by the PROMT LLC company and they are not described in any paper. QTL -M OSES is the standard Moses setup (MERT-tuned on the in-domain training data, but otherwise without any domain-adaptation) and serves as a baseline. The IT-domain translation task introduced th"
W16-2301,W11-2123,0,0.0116622,"s were also evaluated against a reimplementation of the approach firstly proposed by Simard et al. (2007).36 Last year, in fact, this statistical post-editing approach represented the common backbone of all submissions (this is also reflected by the close results achieved by participants in the pilot task). For this purpose, a phrasebased SMT system based on Moses (Koehn et al., 2007) was used. Translation and reordering models were estimated following the Moses protocol with default setup using MGIZA++ (Gao and Vogel, 2008) for word alignment. For language modeling we used the KenLM toolkit (Heafield, 2011) for standard n-gram modeling with an n-gram length of 5. Finally, the APE system was tuned on Adam Mickiewicz University. This system is among the very first ones exploring the application of neural translation models to the APE task. In particular, it investigates the following aspects: i) the use of artificially-created postedited data to train the neural models, ii) the loglinear combination of monolingual and bilingual models in an ensemble-like manner, iii) the addition of task-specific features in the log-linear model to control the final output quality. Concerning the data, in addition"
W16-2301,W16-2384,0,0.0416841,"Missing"
W16-2301,W13-0805,0,0.0110515,"of words in the reference. Lower TER values indicate lower distance from the reference as a proxy for higher MT quality. 33 http://www.cs.umd.edu/˜snover/tercom/ 34 https://github.com/moses-smt/mosesdecoder/ blob/master/scripts/generic/multi-bleu.perl 27 The source sentences (together with their reference translations which were not used for the task) were provided by TAUS (https://www.taus.net/) and originally come from a unique IT vendor. 28 It consists of a phrase-based machine translation system leveraging generic and in-domain parallel training data and using a pre-reordering technique (Herrmann et al., 2013). It takes also advantages of POS and word class-based language models. 29 German native speakers working at Text&Form https: //www.textform.com/. 176 Train (12,000) Dev (1,000) Test (2,000) SRC 201,505 17,827 31,477 Tokens TGT 210,573 19,355 34,332 PE 214,720 19,763 35,276 Table 31: Types SRC TGT 9,328 14,185 2,931 3,333 3,908 4,695 Data statistics. APE@WMT15 APE@WMT16 (EN-ES, news, crowd) (EN-DE, IT, prof.) SRC 2.905 6.616 TGT 3.312 8.845 PE 3.085 8.245 Table 32: Repetition Rate (RR) of the WMT15 (EnglishSpanish, news domain, crowdsourced post-edits) and WMT16 (English-German, IT domain, pro"
W16-2301,W16-2315,1,0.824148,"Missing"
W16-2301,P07-2045,1,0.017773,"gets unmodified.35 Baseline results are reported in Table 34. 7.2 Participants Monolingual translation as another term of comparison. To get some insights about the progress with respect to the first pilot task, participating systems were also evaluated against a reimplementation of the approach firstly proposed by Simard et al. (2007).36 Last year, in fact, this statistical post-editing approach represented the common backbone of all submissions (this is also reflected by the close results achieved by participants in the pilot task). For this purpose, a phrasebased SMT system based on Moses (Koehn et al., 2007) was used. Translation and reordering models were estimated following the Moses protocol with default setup using MGIZA++ (Gao and Vogel, 2008) for word alignment. For language modeling we used the KenLM toolkit (Heafield, 2011) for standard n-gram modeling with an n-gram length of 5. Finally, the APE system was tuned on Adam Mickiewicz University. This system is among the very first ones exploring the application of neural translation models to the APE task. In particular, it investigates the following aspects: i) the use of artificially-created postedited data to train the neural models, ii)"
W16-2301,W16-2337,0,0.0376798,"Missing"
W16-2301,W16-2303,1,0.821661,"Missing"
W16-2301,L16-1582,1,0.790825,"Missing"
W16-2301,W16-2385,0,0.0304494,"Missing"
W16-2301,P16-2095,1,0.885979,"e package to extract these and other quality estimation features and perform model learning, with suggested methods for all levels of prediction. Participants, described in Section 6.2, could submit up to two systems for each task. Data used to build MT systems or internal system information (such as model scores or n-best lists) were made available on request for Tasks 1 and 2. 6.1 The γ,  and C parameters were optimised via grid search with 5-fold cross validation on the training set. Word-level baseline system: For Tasks 2 and 2p, the baseline features were extracted with the Marmot tool (Logacheva et al., 2016b). For the baseline system we used a number of features that have been found the most informative in previous research on word-level QE. Our baseline set of features is loosely based on the one described in (Luong et al., 2014). It contains the following 22 features: • Word count in the source and target sentences, source and target token count ratio. Although these features are sentence-level (i.e. their values will be the same for all words in a sentence), the length of a sentence might influence the probability of a word being wrong. Baseline systems Sentence-level baseline system: For Tas"
W16-2301,W15-3037,0,0.00849978,"stems were submitted for the word-level task. UNBABEL 2 linear is a feature-based linear sequential model. It uses the baseline features provided by the shared task organisers (with slight changes) conjoined with individual labels and pairs of consecutive labels. It also uses various syntactic dependency-based features (dependency relations, heads, and second-order structures like siblings and grandparents). The syntactic dependencies are predicted with TurboParser trained on the TIGER German treebank. UNBABEL 2 ensemble uses a stacked architecture, inspired by the last year’s QUETCH+ system (Kreutzer et al., 2015), which combines three neural systems: one feedforward and two recurrent ones. The predictions of these systems are added as additional features in the linear system above. The following external resources were used: part-of-speech tags and extra syntactic dependency information obtained with TurboTagger and TurboParser (Martins et al., 2013), trained on the Penn Treebank (for English) and on the version of the German TIGER corpus used in the SPMRL shared task (Seddah et al., 2014) for German. For the neural models, pretrained word embeddings from Polyglot (AlRfou et al., 2013) and those produ"
W16-2301,W14-3342,0,0.0237077,"Data used to build MT systems or internal system information (such as model scores or n-best lists) were made available on request for Tasks 1 and 2. 6.1 The γ,  and C parameters were optimised via grid search with 5-fold cross validation on the training set. Word-level baseline system: For Tasks 2 and 2p, the baseline features were extracted with the Marmot tool (Logacheva et al., 2016b). For the baseline system we used a number of features that have been found the most informative in previous research on word-level QE. Our baseline set of features is loosely based on the one described in (Luong et al., 2014). It contains the following 22 features: • Word count in the source and target sentences, source and target token count ratio. Although these features are sentence-level (i.e. their values will be the same for all words in a sentence), the length of a sentence might influence the probability of a word being wrong. Baseline systems Sentence-level baseline system: For Task 1, QuEst++13 (Specia et al., 2015) was used to extract 17 features from the SMT source/target language training corpus: • Number of tokens in source & target sentences. • Target token, its left and right contexts of one word."
W16-2301,W16-2318,0,0.0145349,"Missing"
W16-2301,N06-1014,0,0.0215966,"d on n previous translation and reordering decisions. This technique is able to model both local and long-range reorderings that are quite useful when dealing with the German language. To improve the capability of choosing the correct edit to process, eight new features are added to the loglinear model. These features capture the cost of deleting a phrase and different information on possible gaps in reordering operations. The monolingual alignments between the MT outputs and their post-edits are computed using different methods based on TER, METEOR (Snover et al., 2006) and Berkeley Aligner (Liang et al., 2006). Only the task data is used for these submissions. 7.3 179 TER/BLEU results ID AMU Primary AMU Contrastive FBK Contrastive FBK Primary USAAR Primary USAAR Constrastive CUNI Primary (Simard et al., 2007) Baseline DCU Contrastive JUSAAR Primary JUSAAR Contrastive DCU Primary Avg. TER 21.52 23.06 23.92 23.94 24.14 24.14 24.31 24.64 24.76 26.79 26.92 26.97 28.97 BLEU 67.65 66.09 64.75 64.75 64.10 64.00 63.32 63.47 62.11 58.60 59.44 59.18 55.19 tained this year by the top runs can only be reached by moving from the basic statistical MT backbone shared by all last year’s participants to new and mor"
W16-2301,P13-2109,0,0.0179517,"eads, and second-order structures like siblings and grandparents). The syntactic dependencies are predicted with TurboParser trained on the TIGER German treebank. UNBABEL 2 ensemble uses a stacked architecture, inspired by the last year’s QUETCH+ system (Kreutzer et al., 2015), which combines three neural systems: one feedforward and two recurrent ones. The predictions of these systems are added as additional features in the linear system above. The following external resources were used: part-of-speech tags and extra syntactic dependency information obtained with TurboTagger and TurboParser (Martins et al., 2013), trained on the Penn Treebank (for English) and on the version of the German TIGER corpus used in the SPMRL shared task (Seddah et al., 2014) for German. For the neural models, pretrained word embeddings from Polyglot (AlRfou et al., 2013) and those produced with a neural MT system (Bahdanau et al., 2014) were used. UGENT-LT3 (Task 1, Task 2): The submissions for the word-level task use 41 features in combination with the baseline feature set to train binary classifiers. The 41 additional features attempt to capture accuracy errors (concerned with the meaning transfer from the source to targe"
W16-2301,W16-2387,0,0.0253511,"Missing"
W16-2301,W16-2317,0,0.0414065,"Missing"
W16-2301,N13-1090,0,0.00833788,"Missing"
W16-2301,2015.iwslt-papers.4,1,0.737239,"Missing"
W16-2301,W16-2319,0,0.0339318,"Missing"
W16-2301,W16-2386,1,0.814188,"e package to extract these and other quality estimation features and perform model learning, with suggested methods for all levels of prediction. Participants, described in Section 6.2, could submit up to two systems for each task. Data used to build MT systems or internal system information (such as model scores or n-best lists) were made available on request for Tasks 1 and 2. 6.1 The γ,  and C parameters were optimised via grid search with 5-fold cross validation on the training set. Word-level baseline system: For Tasks 2 and 2p, the baseline features were extracted with the Marmot tool (Logacheva et al., 2016b). For the baseline system we used a number of features that have been found the most informative in previous research on word-level QE. Our baseline set of features is loosely based on the one described in (Luong et al., 2014). It contains the following 22 features: • Word count in the source and target sentences, source and target token count ratio. Although these features are sentence-level (i.e. their values will be the same for all words in a sentence), the length of a sentence might influence the probability of a word being wrong. Baseline systems Sentence-level baseline system: For Tas"
W16-2301,L16-1470,1,0.826657,"Missing"
W16-2301,P03-1021,0,0.0340784,"7 Tokens TGT 210,573 19,355 34,332 PE 214,720 19,763 35,276 Table 31: Types SRC TGT 9,328 14,185 2,931 3,333 3,908 4,695 Data statistics. APE@WMT15 APE@WMT16 (EN-ES, news, crowd) (EN-DE, IT, prof.) SRC 2.905 6.616 TGT 3.312 8.845 PE 3.085 8.245 Table 32: Repetition Rate (RR) of the WMT15 (EnglishSpanish, news domain, crowdsourced post-edits) and WMT16 (English-German, IT domain, professional posteditors) APE Task data. 7.1.3 PE 16,388 3,506 5,047 SRC 5,628 1,922 2,479 Lemmas TGT PE 11,418 13,244 2,686 2,806 3,753 4,050 the development set, optimizing TER/BLEU with Minimum Error Rate Training (Och, 2003). The results of this additional term of comparison are also reported in Table 34. For each submitted run, the statistical significance of performance differences with respect to the baselines and the re-implementation of Simard et al. (2007) was calculated with the bootstrap test (Koehn, 2004). Baseline The official baseline results are the TER and BLEU scores calculated by comparing the raw MT output with the human post-edits. In practice, the baseline APE system is a system that leaves all the test targets unmodified.35 Baseline results are reported in Table 34. 7.2 Participants Monolingual"
W16-2301,W16-2338,0,0.0374669,"Missing"
W16-2301,J03-1002,0,0.0268639,"word alignments and bilingual distributed representations to introduce a new set of features for the sentence-Level QE task. The features extracted include three alignment-based features, three bilingual embedding-based features, two embeddingbased features constrained on alignment links, as well as a set of 74 bigrams used as boolean features. The set of bigrams represents the most frequent bigrams in translations that have changed after the post-edition, and they are compiled by aligning translations to their post-editions provided in the WMT QE datasets. To produce these features, GIZA++ (Och and Ney, 2003) was used for word alignment and Multivec (Berard et al., 2016) was used for the bilingual model, which jointly learns distributed representations for source and target languages using a parallel corpus. To build the bilingual model, domain-specific data compiled from the resources made available for the WMT 16 IT-Domain shared task was used. As prediction model, a Linear Regression model using scikit-learn was built using a combination of QuEst++ baseline features and the new features proposed. tional features for both tasks. UNBABEL (Task 2): Two systems were submitted for the word-level tas"
W16-2301,W16-2321,0,0.0372138,"Missing"
W16-2301,W16-2388,1,0.748184,"Missing"
W16-2301,W16-2333,0,0.0259256,"Missing"
W16-2301,W15-3026,0,0.0339859,"Missing"
W16-2301,W16-2379,1,0.819672,"Missing"
W16-2301,W16-2334,1,0.811656,"Missing"
W16-2301,P02-1040,0,0.105557,", eventually, make the APE task more feasible by automatic systems. Other changes concern the language combination and the evaluation mode. As regards the languages, we moved from English-Spanish to English-German, which is one of the language pairs covered by the QT21 Project26 that supported data collection and post-editing. Concerning the evaluation, we changed from TER scores computed both in case-sensitive and caseinsensitive mode to a single ranking based on case sensitive measurements. Besides these changes the new round of the APE task included some extensions in the evaluation. BLEU (Papineni et al., 2002) has been introduced as a secondary evaluation metric to measure the improvements over the rough MT output. In addition, to gain further insights on final output quality, a subset of the outputs of the submitted systems has also been manually evaluated. Based on these changes and extensions, the goals of this year’s shared task were to: i) improve and stabilize the evaluation framework in view of future rounds, ii) analyze the effect on task 26 feasibility of data coming from a narrow domain, iii) analyze the effect of post-edits collected from professional translators, iv) analyze how humans"
W16-2301,W16-2389,0,0.0210658,"Missing"
W16-2301,W16-2390,0,0.0283246,"Missing"
W16-2301,W16-2322,0,0.0139204,"ng and evaluating the manual translations has settled into the following pattern. We ask human annotators to rank the outputs of five systems. From these rankings, we produce pairwise translation comparisons, and then evaluate them with a version of the TrueSkill algorithm adapted to our task. We refer to this approach (described in Section 3.4) as the relative ranking approach (RR), so named because the pairwise comparisons denote only relative ability between a pair of systems, and cannot be used to infer their absolute quality. These results are used to produce the official ranking for the WMT 2016 tasks. However, work in evaluation over the past few years has provided fresh insight into ways to collect direct assessments (DA) of machine translation quality. In this setting, annotators are asked to provide an assessment of the direct quality of the output of a system relative to a reference translation. In order to evaluate the potential of this approach for future WMT evaluations, we conducted a direct assessment evaluation in parallel. This evaluation, together with a comparison of the official results, is described in Section 3.5. 2.3 3.1 2.2 Training data As in past years we provide"
W16-2301,W16-2392,1,0.772695,"Missing"
W16-2301,L16-1649,1,0.704564,"): one for the 17 baseline features and another for the 500 features from the embeddings. Since each kernel has its own set of hyperparameters, the full model can leverage the contributions from the two different sets. The second system (GRAPH-DISC) combines the baseline features with discourse-aware features. The discourse aware features are the same as the ones used by Scarton et al. (2015a) plus Latent Semantic Analysis (LSA) cohesion features (Scarton and Specia, 2014), number of subtrees and height of the Rhetorical Structure Theory (RST) tree and entity graph-based coherence scores (Sim Smith et al., 2016). Discourseaware and RST tree features were extracted only for English (tools are only available for this language), LSA features were extracted for both languages, and entity graph-based coherence scores were extracted for the target language only (Spanish), as the source documents are expected to be coherent. This QE model was trained with an SVR algorithm. YSDA (Task 1): The YSDA submission is based on a simple idea that the more complex the sentence is the more difficult it is to translate. For this purpose, it uses information provided by syntactic parsing (information from parsing trees,"
W16-2301,W16-2391,1,0.796337,"Missing"
W16-2301,2014.eamt-1.21,1,0.643048,"e extracted by averaging word embeddings in the document. The GP model was trained with two Rational Quadratic kernels (Rasmussen and Williams, 2006): one for the 17 baseline features and another for the 500 features from the embeddings. Since each kernel has its own set of hyperparameters, the full model can leverage the contributions from the two different sets. The second system (GRAPH-DISC) combines the baseline features with discourse-aware features. The discourse aware features are the same as the ones used by Scarton et al. (2015a) plus Latent Semantic Analysis (LSA) cohesion features (Scarton and Specia, 2014), number of subtrees and height of the Rhetorical Structure Theory (RST) tree and entity graph-based coherence scores (Sim Smith et al., 2016). Discourseaware and RST tree features were extracted only for English (tools are only available for this language), LSA features were extracted for both languages, and entity graph-based coherence scores were extracted for the target language only (Spanish), as the source documents are expected to be coherent. This QE model was trained with an SVR algorithm. YSDA (Task 1): The YSDA submission is based on a simple idea that the more complex the sentence"
W16-2301,W15-3040,1,0.909092,"on-word corpus,18 with a vocabulary size of 527K words. Document embeddings are extracted by averaging word embeddings in the document. The GP model was trained with two Rational Quadratic kernels (Rasmussen and Williams, 2006): one for the 17 baseline features and another for the 500 features from the embeddings. Since each kernel has its own set of hyperparameters, the full model can leverage the contributions from the two different sets. The second system (GRAPH-DISC) combines the baseline features with discourse-aware features. The discourse aware features are the same as the ones used by Scarton et al. (2015a) plus Latent Semantic Analysis (LSA) cohesion features (Scarton and Specia, 2014), number of subtrees and height of the Rhetorical Structure Theory (RST) tree and entity graph-based coherence scores (Sim Smith et al., 2016). Discourseaware and RST tree features were extracted only for English (tools are only available for this language), LSA features were extracted for both languages, and entity graph-based coherence scores were extracted for the target language only (Spanish), as the source documents are expected to be coherent. This QE model was trained with an SVR algorithm. YSDA (Task 1)"
W16-2301,2006.amta-papers.25,0,0.850121,"ombinations of several features. A regression model was training to predict BLEU as target metric instead HTER. The machine learning pipeline uses an SVR with RBF kernel to predict BLEU scores, followed by a linear SVR to predict HTER scores from BLEU scores. As external resources, the system uses a syntactic parser, pseudo-references and back-translation from web-scale MT system, and a web-scale language model. 6.3 Task 1: Predicting sentence-level quality This task consists in scoring (and ranking) translation sentences according to the percentage of their words that need to be fixed. HTER (Snover et al., 2006) is used as quality score, i.e. the minimum edit distance between the machine translation and its manually post-edited version in [0,1]. As in previous years, two variants of the results could be submitted: • Scoring: An absolute HTER score for each sentence translation, to be interpreted as an error metric: lower scores mean better translations. • Ranking: A ranking of sentence translations for all source sentences from best to worst. For this variant, it does not matter how the ranking is produced (from HTER predictions or by other means). The reference ranking is defined based on the true H"
W16-2301,W15-4916,1,0.824025,"Missing"
W16-2301,W16-2346,1,0.0602876,"Missing"
W16-2301,W16-2325,1,0.816227,"Missing"
W16-2301,W16-2393,0,0.033041,"Missing"
W16-2301,W16-2326,0,0.0221449,"Missing"
W16-2301,W16-2327,1,0.737603,"Missing"
W16-2301,W16-2328,0,0.0403946,"Missing"
W16-2301,C00-2137,0,0.0384283,"Missing"
W16-2301,D07-1091,1,\N,Missing
W16-2301,W09-0401,1,\N,Missing
W16-2301,P11-1105,0,\N,Missing
W16-2301,N07-1064,0,\N,Missing
W16-2301,W04-3250,1,\N,Missing
W16-2301,P15-4020,1,\N,Missing
W16-2301,W16-2377,1,\N,Missing
W16-2301,aziz-etal-2012-pet,1,\N,Missing
W16-2301,2012.eamt-1.31,0,\N,Missing
W16-2301,C14-1182,0,\N,Missing
W16-2301,W16-2316,0,\N,Missing
W16-2301,W16-2332,1,\N,Missing
W16-2301,W16-2320,1,\N,Missing
W16-2301,W16-2335,0,\N,Missing
W16-2301,W16-2329,0,\N,Missing
W16-2301,W16-2383,0,\N,Missing
W16-2301,W16-2381,1,\N,Missing
W16-2301,W16-2312,0,\N,Missing
W16-2301,P16-1162,1,\N,Missing
W16-2301,W13-2243,1,\N,Missing
W16-2301,W08-0509,0,\N,Missing
W16-2301,2015.eamt-1.17,1,\N,Missing
W16-2301,W14-6111,0,\N,Missing
W16-2301,2012.tc-1.5,1,\N,Missing
W16-2301,W16-2347,1,\N,Missing
W16-2301,W16-2314,0,\N,Missing
W16-2379,N12-1047,0,0.0301838,"archical, monotone, swap, left to right bidirectional (hiermslr-bidirectional) method (Galley and Manning, 2008) and conditioned on both source and target language. The 5-gram target language model was trained using KenLM (Heafield, 2011). Phrase pairs that occur only once in the training data are assigned an unduly high probability mass (i.e. 1). To compensate this shortcoming, we performed smoothing of the phrase table using the GoodTuring smoothing technique (Foster et al., 2006). System tuning was carried out using Minimum Error Rate Training (MERT) (Och, 2003) optimized with k-best MIRA (Cherry and Foster, 2012) on a held out development set of size 500 sentences randomly extracted from training data. Therefore, all model has been build on 11,500 parallel T LM T –T LP E sentences. After the parameters were tuned, decoding was carried out on the held out development test set (‘Dev’ in Table 1) as well as test set. Table 1 presents the statistics of the training, development and test sets released for the English–German APE Task organized in WMT2016. These data sets did not require any preprocessing in terms of encoding or alignment. Train Dev Test 12,000 1,000 2,000 EN 201,505 17,827 31,477 Tokens DE-"
W16-2379,P03-1021,0,0.187832,"rdering model was trained with the hierarchical, monotone, swap, left to right bidirectional (hiermslr-bidirectional) method (Galley and Manning, 2008) and conditioned on both source and target language. The 5-gram target language model was trained using KenLM (Heafield, 2011). Phrase pairs that occur only once in the training data are assigned an unduly high probability mass (i.e. 1). To compensate this shortcoming, we performed smoothing of the phrase table using the GoodTuring smoothing technique (Foster et al., 2006). System tuning was carried out using Minimum Error Rate Training (MERT) (Och, 2003) optimized with k-best MIRA (Cherry and Foster, 2012) on a held out development set of size 500 sentences randomly extracted from training data. Therefore, all model has been build on 11,500 parallel T LM T –T LP E sentences. After the parameters were tuned, decoding was carried out on the held out development test set (‘Dev’ in Table 1) as well as test set. Table 1 presents the statistics of the training, development and test sets released for the English–German APE Task organized in WMT2016. These data sets did not require any preprocessing in terms of encoding or alignment. Train Dev Test 1"
W16-2379,P11-1105,0,0.11743,"Missing"
W16-2379,W15-3026,1,0.834799,"Missing"
W16-2379,J15-2001,0,0.138111,"Missing"
W16-2379,L16-1095,1,0.842747,"Missing"
W16-2379,W06-1607,0,0.0408401,"our PB-SAPE and hierarchical phrase-based statistical (HPB-SAPE) system respectively. The reordering model was trained with the hierarchical, monotone, swap, left to right bidirectional (hiermslr-bidirectional) method (Galley and Manning, 2008) and conditioned on both source and target language. The 5-gram target language model was trained using KenLM (Heafield, 2011). Phrase pairs that occur only once in the training data are assigned an unduly high probability mass (i.e. 1). To compensate this shortcoming, we performed smoothing of the phrase table using the GoodTuring smoothing technique (Foster et al., 2006). System tuning was carried out using Minimum Error Rate Training (MERT) (Och, 2003) optimized with k-best MIRA (Cherry and Foster, 2012) on a held out development set of size 500 sentences randomly extracted from training data. Therefore, all model has been build on 11,500 parallel T LM T –T LP E sentences. After the parameters were tuned, decoding was carried out on the held out development test set (‘Dev’ in Table 1) as well as test set. Table 1 presents the statistics of the training, development and test sets released for the English–German APE Task organized in WMT2016. These data sets d"
W16-2379,P02-1040,0,0.109971,"LM-Toolkit (Stolcke, 2002) 760 3 Experiment SEN The effectiveness of the present work is demonstrated by using the standard log-linear PB-SMT model for our phrase based SAPE (PB-SAPE) model. The MT outputs are provided by WMT2016 APE task (c.f Table 1) are considered as baseline system translation. For building our SAPE system, we experimented with various maximum phrase lengths for the translation model and n–gram settings for the language model. We found that using a maximum phrase length of 10 for the translation model and a 6-gram language model produces the best results in terms of BLEU (Papineni et al., 2002) scores for our SAPE model. The other experimental settings were concerned with word alignment model between T LM T and T LP E are trained on three different aligners: Berkeley Aligner (Liang et al., 2006), METEOR aligner (Lavie and Agarwal, 2007) and TER (Snover et al., 2006). The phraseextraction (Koehn et al., 2003) and hierarchical phrase-extraction (Chiang, 2005) are used to build our PB-SAPE and hierarchical phrase-based statistical (HPB-SAPE) system respectively. The reordering model was trained with the hierarchical, monotone, swap, left to right bidirectional (hiermslr-bidirectional)"
W16-2379,D08-1089,0,0.0406055,"for our SAPE model. The other experimental settings were concerned with word alignment model between T LM T and T LP E are trained on three different aligners: Berkeley Aligner (Liang et al., 2006), METEOR aligner (Lavie and Agarwal, 2007) and TER (Snover et al., 2006). The phraseextraction (Koehn et al., 2003) and hierarchical phrase-extraction (Chiang, 2005) are used to build our PB-SAPE and hierarchical phrase-based statistical (HPB-SAPE) system respectively. The reordering model was trained with the hierarchical, monotone, swap, left to right bidirectional (hiermslr-bidirectional) method (Galley and Manning, 2008) and conditioned on both source and target language. The 5-gram target language model was trained using KenLM (Heafield, 2011). Phrase pairs that occur only once in the training data are assigned an unduly high probability mass (i.e. 1). To compensate this shortcoming, we performed smoothing of the phrase table using the GoodTuring smoothing technique (Foster et al., 2006). System tuning was carried out using Minimum Error Rate Training (MERT) (Och, 2003) optimized with k-best MIRA (Cherry and Foster, 2012) on a held out development set of size 500 sentences randomly extracted from training da"
W16-2379,N07-1064,0,0.859858,"diting Santanu Pal1 , Marcos Zampieri1,2 , Josef van Genabith1,2 1 Saarland University, Saarbr¨ucken, Germany 2 German Research Center for Artificial Intelligence (DFKI), Germany {santanu.pal, marcos.zampieri, josef.vangenabith}@uni-saarland.de Abstract tency (Guerberof, 2009; Plitt and Masselot, 2010; Zampieri and Vela, 2014). With this respect the ultimate goal of MT systems is to provide output that can be post-edited with the least effort as possible by human translators. One of the strategies to improve MT output is to apply automatic post-editing (APE) methods (Knight and Chander, 1994; Simard et al., 2007a; Simard et al., 2007b). APE methods work under the assumption that some errors in MT systems are recurrent and they can be corrected automatically in a post-processing stage thus providing output that is more adequate to be post-edited. APE methods are applied before human post-editing increasing translators’ productivity. This paper presents a new approach to APE which was submitted by the USAAR team to the Automatic Post-editing (APE) shared task at WMT-2016. Our system combines two models: monolingual phrase-based and operation sequential model with an edit distance based word alignment b"
W16-2379,W07-0728,0,0.494383,"diting Santanu Pal1 , Marcos Zampieri1,2 , Josef van Genabith1,2 1 Saarland University, Saarbr¨ucken, Germany 2 German Research Center for Artificial Intelligence (DFKI), Germany {santanu.pal, marcos.zampieri, josef.vangenabith}@uni-saarland.de Abstract tency (Guerberof, 2009; Plitt and Masselot, 2010; Zampieri and Vela, 2014). With this respect the ultimate goal of MT systems is to provide output that can be post-edited with the least effort as possible by human translators. One of the strategies to improve MT output is to apply automatic post-editing (APE) methods (Knight and Chander, 1994; Simard et al., 2007a; Simard et al., 2007b). APE methods work under the assumption that some errors in MT systems are recurrent and they can be corrected automatically in a post-processing stage thus providing output that is more adequate to be post-edited. APE methods are applied before human post-editing increasing translators’ productivity. This paper presents a new approach to APE which was submitted by the USAAR team to the Automatic Post-editing (APE) shared task at WMT-2016. Our system combines two models: monolingual phrase-based and operation sequential model with an edit distance based word alignment b"
W16-2379,W11-2123,0,0.0478937,"ree different aligners: Berkeley Aligner (Liang et al., 2006), METEOR aligner (Lavie and Agarwal, 2007) and TER (Snover et al., 2006). The phraseextraction (Koehn et al., 2003) and hierarchical phrase-extraction (Chiang, 2005) are used to build our PB-SAPE and hierarchical phrase-based statistical (HPB-SAPE) system respectively. The reordering model was trained with the hierarchical, monotone, swap, left to right bidirectional (hiermslr-bidirectional) method (Galley and Manning, 2008) and conditioned on both source and target language. The 5-gram target language model was trained using KenLM (Heafield, 2011). Phrase pairs that occur only once in the training data are assigned an unduly high probability mass (i.e. 1). To compensate this shortcoming, we performed smoothing of the phrase table using the GoodTuring smoothing technique (Foster et al., 2006). System tuning was carried out using Minimum Error Rate Training (MERT) (Och, 2003) optimized with k-best MIRA (Cherry and Foster, 2012) on a held out development set of size 500 sentences randomly extracted from training data. Therefore, all model has been build on 11,500 parallel T LM T –T LP E sentences. After the parameters were tuned, decoding"
W16-2379,2006.amta-papers.25,0,0.644646,"system translation. For building our SAPE system, we experimented with various maximum phrase lengths for the translation model and n–gram settings for the language model. We found that using a maximum phrase length of 10 for the translation model and a 6-gram language model produces the best results in terms of BLEU (Papineni et al., 2002) scores for our SAPE model. The other experimental settings were concerned with word alignment model between T LM T and T LP E are trained on three different aligners: Berkeley Aligner (Liang et al., 2006), METEOR aligner (Lavie and Agarwal, 2007) and TER (Snover et al., 2006). The phraseextraction (Koehn et al., 2003) and hierarchical phrase-extraction (Chiang, 2005) are used to build our PB-SAPE and hierarchical phrase-based statistical (HPB-SAPE) system respectively. The reordering model was trained with the hierarchical, monotone, swap, left to right bidirectional (hiermslr-bidirectional) method (Galley and Manning, 2008) and conditioned on both source and target language. The 5-gram target language model was trained using KenLM (Heafield, 2011). Phrase pairs that occur only once in the training data are assigned an unduly high probability mass (i.e. 1). To com"
W16-2379,N03-1017,0,0.046298,"es our proposed system, in particular PB-SMT coupled OSM model. In Section 3, we outline the data used for experiments and complete experimental setup. Section 4 presents the results of the automatic evaluation, followed by conclusion and future work in Section 5. 2 p(mt, pe) ≈ I Y i=1 p(oi |oi−m+1 ...oi−1 ) (1) The decoder searches best translation in Equation 2 from the model using language model plm (pe) USAAR APE System Our APE system is based on operational N-gram sequential model which integrates translation and reordering operations into the phrase-based APE system. Traditional PB-SMT (Koehn et al., 2003) provides a powerful translation mechanism which can directly be modelled to a phrase-based SAPE (PB-SAPE) system (Simard et al., 2007a; Simard et al., 2007b; Pal et al., 2015) using target language MT output (T LM T ) and their corresponding post-edited version (T LP E ) as a parallel training corpus. Unlike PB-SMT, PB-SAPE also follows similar kind of drawbacks such as dependency across phrases, handling discontinuous phrases etc. Our OSM-APE system is based on phrase based N-gram APE model, however reordering approach is essentially different, it considers all possible orderings of phrases"
W16-2379,P07-2045,0,0.0304214,"Missing"
W16-2379,W14-0314,1,0.877599,"Missing"
W16-2379,W07-0734,0,0.289413,"able 1) are considered as baseline system translation. For building our SAPE system, we experimented with various maximum phrase lengths for the translation model and n–gram settings for the language model. We found that using a maximum phrase length of 10 for the translation model and a 6-gram language model produces the best results in terms of BLEU (Papineni et al., 2002) scores for our SAPE model. The other experimental settings were concerned with word alignment model between T LM T and T LP E are trained on three different aligners: Berkeley Aligner (Liang et al., 2006), METEOR aligner (Lavie and Agarwal, 2007) and TER (Snover et al., 2006). The phraseextraction (Koehn et al., 2003) and hierarchical phrase-extraction (Chiang, 2005) are used to build our PB-SAPE and hierarchical phrase-based statistical (HPB-SAPE) system respectively. The reordering model was trained with the hierarchical, monotone, swap, left to right bidirectional (hiermslr-bidirectional) method (Galley and Manning, 2008) and conditioned on both source and target language. The 5-gram target language model was trained using KenLM (Heafield, 2011). Phrase pairs that occur only once in the training data are assigned an unduly high pro"
W16-2379,N06-1014,0,0.596721,"e provided by WMT2016 APE task (c.f Table 1) are considered as baseline system translation. For building our SAPE system, we experimented with various maximum phrase lengths for the translation model and n–gram settings for the language model. We found that using a maximum phrase length of 10 for the translation model and a 6-gram language model produces the best results in terms of BLEU (Papineni et al., 2002) scores for our SAPE model. The other experimental settings were concerned with word alignment model between T LM T and T LP E are trained on three different aligners: Berkeley Aligner (Liang et al., 2006), METEOR aligner (Lavie and Agarwal, 2007) and TER (Snover et al., 2006). The phraseextraction (Koehn et al., 2003) and hierarchical phrase-extraction (Chiang, 2005) are used to build our PB-SAPE and hierarchical phrase-based statistical (HPB-SAPE) system respectively. The reordering model was trained with the hierarchical, monotone, swap, left to right bidirectional (hiermslr-bidirectional) method (Galley and Manning, 2008) and conditioned on both source and target language. The 5-gram target language model was trained using KenLM (Heafield, 2011). Phrase pairs that occur only once in the tra"
W16-2379,W15-5206,1,0.834735,"Missing"
W16-2379,J03-1002,0,0.0182983,"a linear sequence of operations such as lexical generation of post-edited translation and their orderings. The translation and reordering decisions are conditioned on n previous translation and reordering decisions. The model also can able to consistently modelled both local and long-range reorderings. Traditional OSM based MT model consists of three sequence of operations: pe∗ = argmaxpe p(mt, pe) × plm (pe) ppr (pe) (2) QI ppr (pe) ≈ i=1 p(wi |wi−m+1 ...wi−1 ), is the prior probability that marginalize the joint probability p(mt, pe). The model is then represented in a log-linear approach (Och and Ney, 2003) (in Equation 3) that makes it useful to incorporate standard features along with several novel features that improve the accuracy. pe∗ = argmaxpe I X λi hi (mt, pe) (3) i=1 where λi is the weight associated with the feature hi (mt, pe): p(mt, pe), ppr (pe) and plm (pe). Apart from this 8 additional features has been included in the log-linear model: 1. Length penalty: Length of the pe in words 2. Deletion penalty 3. Gap bonus: Total number of gap inserted to produce PE sentence 4. Open gap penalty : Number of open gaps, this penalty controls how quickly gap was closed. • Generates a sequence"
W16-2379,P05-1033,0,\N,Missing
W16-4801,W16-4821,0,0.0339271,"Missing"
W16-4801,W16-4826,0,0.0502832,"Missing"
W16-4801,W16-4827,0,0.11257,"Missing"
W16-4801,W16-4819,0,0.0808398,"Missing"
W16-4801,W16-4816,0,0.0637155,"Missing"
W16-4801,W15-5410,0,0.238809,"Missing"
W16-4801,W16-4802,0,0.114498,"Missing"
W16-4801,W16-4831,0,0.0913939,"Missing"
W16-4801,W16-4830,0,0.0438028,"Missing"
W16-4801,D14-1154,0,0.0630846,"otivated the organization of shared tasks such as the DSL challenge, which allowed researchers to compare various approaches using the same dataset. Along with the interest in similar languages and language variety identification, we observed substantial interest in applying natural language processing (NLP) methods for the processing of dialectal Arabic with special interest in methods to discriminate between Arabic dialects. Shoufan and Al-Ameri (2015) presented a comprehensive survey on these methods including recent studies on Arabic dialect identification such as (Elfardy and Diab, 2014; Darwish et al., 2014; Zaidan and Callison-Burch, 2014; This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http://creativecommons.org/licenses/by/4.0/ 1 Proceedings of the Third Workshop on NLP for Similar Languages, Varieties and Dialects, pages 1–14, Osaka, Japan, December 12 2016. Tillmann et al., 2014; Malmasi and Dras, 2015a). Methods for Arabic dialect detection present significant overlap with methods proposed for similar language identification. For this reason, in the 2016 edition of the DSL challenge we offered a subtask on Arabic dialect identification"
W16-4801,W16-4828,0,0.0387334,"Missing"
W16-4801,W15-5409,0,0.149587,"Missing"
W16-4801,W16-4829,0,0.0511602,"Missing"
W16-4801,W15-5403,0,0.27462,"Missing"
W16-4801,W16-4822,0,0.0759257,"Missing"
W16-4801,W15-5413,0,0.562658,"Missing"
W16-4801,W16-4823,0,0.0418563,"Missing"
W16-4801,W14-5316,0,0.385305,"Missing"
W16-4801,L16-1284,1,0.849869,"Missing"
W16-4801,W16-4824,0,0.0430542,"Missing"
W16-4801,W16-4817,0,0.0342647,"Missing"
W16-4801,W16-4815,0,0.0384942,"Missing"
W16-4801,Y08-1042,0,0.0574334,"ticipants. Below we present the task setup, the evaluation results, and a brief discussion about the features and learning methods that worked best. More detail about each particular system can be found in the corresponding system description paper, as cited in this report. 2 Related Work Language and dialect identification have attracted a lot of research attention in recent years, covering a number of similar languages and language varieties such as South-Slavic languages (Ljubeˇsi´c et al., 2007), English varieties (Lui and Cook, 2013), varieties of Mandarin in China, Taiwan and Singapore (Huang and Lee, 2008), Malay vs. Indonesian (Ranaivo-Malanc¸on, 2006), Brazilian vs. European Portuguese (Zampieri and Gebre, 2012), and Persian vs. Dari (Malmasi and Dras, 2015a), to mention just a few. The interest in this aspect of language identification has motivated the organization of shared tasks such as the DSL challenge, which allowed researchers to compare various approaches using the same dataset. Along with the interest in similar languages and language variety identification, we observed substantial interest in applying natural language processing (NLP) methods for the processing of dialectal Arabic"
W16-4801,W16-4818,0,0.128184,"Missing"
W16-4801,J16-3005,0,0.112984,"Missing"
W16-4801,W15-5408,0,0.217783,"Missing"
W16-4801,W16-4820,0,0.38437,"Missing"
W16-4801,W14-5317,0,0.0630949,"Missing"
W16-4801,U13-1003,0,0.256503,"size and scope featuring two subtasks and attracting a record number of participants. Below we present the task setup, the evaluation results, and a brief discussion about the features and learning methods that worked best. More detail about each particular system can be found in the corresponding system description paper, as cited in this report. 2 Related Work Language and dialect identification have attracted a lot of research attention in recent years, covering a number of similar languages and language varieties such as South-Slavic languages (Ljubeˇsi´c et al., 2007), English varieties (Lui and Cook, 2013), varieties of Mandarin in China, Taiwan and Singapore (Huang and Lee, 2008), Malay vs. Indonesian (Ranaivo-Malanc¸on, 2006), Brazilian vs. European Portuguese (Zampieri and Gebre, 2012), and Persian vs. Dari (Malmasi and Dras, 2015a), to mention just a few. The interest in this aspect of language identification has motivated the organization of shared tasks such as the DSL challenge, which allowed researchers to compare various approaches using the same dataset. Along with the interest in similar languages and language variety identification, we observed substantial interest in applying natur"
W16-4801,W14-5315,0,0.0614151,"Missing"
W16-4801,W15-5407,1,0.493189,"detail about each particular system can be found in the corresponding system description paper, as cited in this report. 2 Related Work Language and dialect identification have attracted a lot of research attention in recent years, covering a number of similar languages and language varieties such as South-Slavic languages (Ljubeˇsi´c et al., 2007), English varieties (Lui and Cook, 2013), varieties of Mandarin in China, Taiwan and Singapore (Huang and Lee, 2008), Malay vs. Indonesian (Ranaivo-Malanc¸on, 2006), Brazilian vs. European Portuguese (Zampieri and Gebre, 2012), and Persian vs. Dari (Malmasi and Dras, 2015a), to mention just a few. The interest in this aspect of language identification has motivated the organization of shared tasks such as the DSL challenge, which allowed researchers to compare various approaches using the same dataset. Along with the interest in similar languages and language variety identification, we observed substantial interest in applying natural language processing (NLP) methods for the processing of dialectal Arabic with special interest in methods to discriminate between Arabic dialects. Shoufan and Al-Ameri (2015) presented a comprehensive survey on these methods incl"
W16-4801,W16-4814,1,0.85912,"Missing"
W16-4801,W16-4825,0,0.0362109,"Missing"
W16-4801,W14-5314,0,0.0924571,"Missing"
W16-4801,W15-3205,0,0.0361398,"ortuguese (Zampieri and Gebre, 2012), and Persian vs. Dari (Malmasi and Dras, 2015a), to mention just a few. The interest in this aspect of language identification has motivated the organization of shared tasks such as the DSL challenge, which allowed researchers to compare various approaches using the same dataset. Along with the interest in similar languages and language variety identification, we observed substantial interest in applying natural language processing (NLP) methods for the processing of dialectal Arabic with special interest in methods to discriminate between Arabic dialects. Shoufan and Al-Ameri (2015) presented a comprehensive survey on these methods including recent studies on Arabic dialect identification such as (Elfardy and Diab, 2014; Darwish et al., 2014; Zaidan and Callison-Burch, 2014; This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http://creativecommons.org/licenses/by/4.0/ 1 Proceedings of the Third Workshop on NLP for Similar Languages, Varieties and Dialects, pages 1–14, Osaka, Japan, December 12 2016. Tillmann et al., 2014; Malmasi and Dras, 2015a). Methods for Arabic dialect detection present significant overlap with met"
W16-4801,W14-5313,0,0.115089,"of dialectal Arabic with special interest in methods to discriminate between Arabic dialects. Shoufan and Al-Ameri (2015) presented a comprehensive survey on these methods including recent studies on Arabic dialect identification such as (Elfardy and Diab, 2014; Darwish et al., 2014; Zaidan and Callison-Burch, 2014; This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http://creativecommons.org/licenses/by/4.0/ 1 Proceedings of the Third Workshop on NLP for Similar Languages, Varieties and Dialects, pages 1–14, Osaka, Japan, December 12 2016. Tillmann et al., 2014; Malmasi and Dras, 2015a). Methods for Arabic dialect detection present significant overlap with methods proposed for similar language identification. For this reason, in the 2016 edition of the DSL challenge we offered a subtask on Arabic dialect identification. Below, we discuss some related shared tasks including the first two editions of the DSL challenge. 2.1 Related Shared Tasks Several shared tasks related to the DSL task have been organized in recent years. Two examples are the ALTW language identification shared task (Baldwin and Lui, 2010) on general-purpose language identification,"
W16-4801,P11-1122,0,0.0270219,"Missing"
W16-4801,W14-5307,1,0.744553,"Missing"
W16-4801,W15-5411,1,0.882098,"Missing"
W16-4814,al-sabbagh-girju-2012-yadac,0,0.193839,"We experimented with three different ensemble fusion strategies, with the mean probability approach providing the best performance. 1 Introduction The interest in processing Arabic texts and speech data has grown substantially in the last decade.1 Due to its intrinsic variation, research has been carried out not only on Modern Standard Arabic (MSA), but also on the various Arabic dialects spoken in North Africa and in the Middle East. Research in NLP and Arabic dialects includes, most notably, machine translation of Arabic dialects (Zbib et al., 2012), corpus compilation for Arabic dialects (Al-Sabbagh and Girju, 2012; Cotterell and CallisonBurch, 2014), parsing (Chiang et al., 2006), and Arabic dialect identification (Zaidan and Callison-Burch, 2014). The latter has become a vibrant research topic with several papers published in the last few years (Sadat et al., 2014; Malmasi et al., 2015). In this paper we revisit the task of Arabic dialect identification proposing an ensemble method applied to a corpus of broadcast speeches transcribed from MSA and four Arabic dialects: Egyptian, Gulf, Levantine, and North African (Ali et al., 2016). The system competed in the Arabic dialect identification sub-task of"
W16-4814,bouamor-etal-2014-multidialectal,0,0.0955956,"Missing"
W16-4814,E06-1047,0,0.0283987,"e mean probability approach providing the best performance. 1 Introduction The interest in processing Arabic texts and speech data has grown substantially in the last decade.1 Due to its intrinsic variation, research has been carried out not only on Modern Standard Arabic (MSA), but also on the various Arabic dialects spoken in North Africa and in the Middle East. Research in NLP and Arabic dialects includes, most notably, machine translation of Arabic dialects (Zbib et al., 2012), corpus compilation for Arabic dialects (Al-Sabbagh and Girju, 2012; Cotterell and CallisonBurch, 2014), parsing (Chiang et al., 2006), and Arabic dialect identification (Zaidan and Callison-Burch, 2014). The latter has become a vibrant research topic with several papers published in the last few years (Sadat et al., 2014; Malmasi et al., 2015). In this paper we revisit the task of Arabic dialect identification proposing an ensemble method applied to a corpus of broadcast speeches transcribed from MSA and four Arabic dialects: Egyptian, Gulf, Levantine, and North African (Ali et al., 2016). The system competed in the Arabic dialect identification sub-task of the 2016 edition of the DSL shared task (Malmasi et al., 2016b)2 un"
W16-4814,L16-1522,0,0.0911986,"Missing"
W16-4814,cotterell-callison-burch-2014-multi,0,0.297758,"Missing"
W16-4814,D14-1154,0,0.433541,"Missing"
W16-4814,P13-2081,0,0.265769,"Missing"
W16-4814,W14-5316,0,0.222489,"Missing"
W16-4814,L16-1284,1,0.858644,"Missing"
W16-4814,U13-1003,0,0.342387,"Missing"
W16-4814,W15-5407,1,0.70075,"released one month later for the official evaluation. A breakdown of the number of training sentences for each of these classes is listed in Table 1. Dialect Egyptian Gulf Levantine Modern Standard North African Total Class EGY GLF LAV MSA NOR Sentences 1,578 1,672 1,758 999 1,612 7,619 Table 1: The breakdown of the dialectal training data provided (Ali et al., 2016). 3.2 Approach There have been various methods proposed for dialect identification in recent years. Given its success in previous work, we decided to use an ensemble classifier for our entry. We follow the methodology described by Malmasi and Dras (2015b): we extract a number of different feature types and train a single linear model using each feature type. We extract the following feature types, each of them used to train a single classification model: • Character n-grams (n = 1–6): these substrings, depending on the order, can implicitly capture various sub-lexical features including single letters, phonemes, syllables, morphemes and suffixes. They could capture interesting inter-dialectal differences that generalize better than word n-grams. • Word unigrams: entire words can capture lexical differences between dialects. 107 We did not pe"
W16-4814,W16-4801,1,0.864908,"Missing"
W16-4814,W14-5904,0,0.151233,"insic variation, research has been carried out not only on Modern Standard Arabic (MSA), but also on the various Arabic dialects spoken in North Africa and in the Middle East. Research in NLP and Arabic dialects includes, most notably, machine translation of Arabic dialects (Zbib et al., 2012), corpus compilation for Arabic dialects (Al-Sabbagh and Girju, 2012; Cotterell and CallisonBurch, 2014), parsing (Chiang et al., 2006), and Arabic dialect identification (Zaidan and Callison-Burch, 2014). The latter has become a vibrant research topic with several papers published in the last few years (Sadat et al., 2014; Malmasi et al., 2015). In this paper we revisit the task of Arabic dialect identification proposing an ensemble method applied to a corpus of broadcast speeches transcribed from MSA and four Arabic dialects: Egyptian, Gulf, Levantine, and North African (Ali et al., 2016). The system competed in the Arabic dialect identification sub-task of the 2016 edition of the DSL shared task (Malmasi et al., 2016b)2 under the team name MAZA. The system achieved very good performance and was ranked first among the 18 teams that participated in the closed submission track. 2 Related Work There have been se"
W16-4814,W15-3205,0,0.138174,"Missing"
W16-4814,W14-5313,0,0.134582,"Missing"
W16-4814,P11-2007,0,0.232637,"Missing"
W16-4814,W14-5307,1,0.890445,"Missing"
W16-4814,N12-1006,0,0.0257736,"er ensemble with a set of linear models as base classifiers. We experimented with three different ensemble fusion strategies, with the mean probability approach providing the best performance. 1 Introduction The interest in processing Arabic texts and speech data has grown substantially in the last decade.1 Due to its intrinsic variation, research has been carried out not only on Modern Standard Arabic (MSA), but also on the various Arabic dialects spoken in North Africa and in the Middle East. Research in NLP and Arabic dialects includes, most notably, machine translation of Arabic dialects (Zbib et al., 2012), corpus compilation for Arabic dialects (Al-Sabbagh and Girju, 2012; Cotterell and CallisonBurch, 2014), parsing (Chiang et al., 2006), and Arabic dialect identification (Zaidan and Callison-Burch, 2014). The latter has become a vibrant research topic with several papers published in the last few years (Sadat et al., 2014; Malmasi et al., 2015). In this paper we revisit the task of Arabic dialect identification proposing an ensemble method applied to a corpus of broadcast speeches transcribed from MSA and four Arabic dialects: Egyptian, Gulf, Levantine, and North African (Ali et al., 2016). T"
W17-1201,W16-4802,0,0.127115,"character ngrams and a Na”ive Bayes classifier. The system followed the work of the system submitted to the DSL 2016 by Barbaresi (2016). • CECL: The system uses a two-step approach as in (Goutte et al., 2014). The first step identifies the language group using an SVM classifier with a linear kernel trained on character n-grams (1-4) that occur at least 100 times in the dataset weighted by Okapi BM25 (Robertson et al., 1995). The second step discriminates between each language within the group using a set of SVM classifiers trained • tubasfs: Following the success of tubasfs at DSL 2016 (C¸o¨ ltekin and Rama, 2016), which was ranked first in the closed training track, this year’s tubasfs submission used a linear SVM classifier. The system used both characters and words as features, and carefully optimized hyperparameters: n-gram size and margin/regularization parameter for SVM. 5 In 2016 ADI and DSL were organized under the name DSL shared task, and ADI was run as a sub-task. 4 • gauge: This team submitted a total of three runs. Run 1 used an SVM classifier with character n-grams (2–6), run 2 (their best run) used logistic regression trained using character n-grams (1–6), and run 3 used hard voting of t"
W17-1201,W17-1221,0,0.532877,"of related languages. 1 Proceedings of the Fourth Workshop on NLP for Similar Languages, Varieties and Dialects, pages 1–15, c Valencia, Spain, April 3, 2017. 2017 Association for Computational Linguistics Team ahaqst bayesline CECL cic ualg Citius Ixa Imaxin CLUZH CUNI deepCybErNet gauge Helsinki-CLP MAZA (ADI) MAZA (GDI) mm lct qcri mit SUKI timeflow tubasfs unibuckernel XAC Bayesline Total DSL X X X X X X ADI X X X X X 11 CLP X X X X X X GDI X X X X X X X X X X 10 6 X X X 3 System Description Paper (Hanani et al., 2017) – (Bestgen, 2017) (G´omez-Adorno et al., 2017) (Gamallo et al., 2017) (Clematide and Makarov, 2017) (Rosa et al., 2017) – – (Tiedemann, 2017) (Malmasi and Zampieri, 2017a) (Malmasi and Zampieri, 2017b) (Medvedeva et al., 2017) – (Jauhiainen et al., 2017) (Criscuolo and Aluisio, 2017) (C ¸ o¨ ltekin and Rama, 2017) (Ionescu and Butnaru, 2017) (Barbaresi, 2017) 15 Table 1: The teams that participated in the VarDial’2017 Evaluation Campaign. 1.2 Participating Teams The fourth edition of the DSL shared task was motivated by the success of the previous editions and by the growing interest of the research community in the identification of dialects and similar languages, as evidenced by recent pu"
W17-1201,W17-1215,0,0.0474398,"Missing"
W17-1201,W17-1213,0,0.0702486,"pairs include a triple of related languages. 1 Proceedings of the Fourth Workshop on NLP for Similar Languages, Varieties and Dialects, pages 1–15, c Valencia, Spain, April 3, 2017. 2017 Association for Computational Linguistics Team ahaqst bayesline CECL cic ualg Citius Ixa Imaxin CLUZH CUNI deepCybErNet gauge Helsinki-CLP MAZA (ADI) MAZA (GDI) mm lct qcri mit SUKI timeflow tubasfs unibuckernel XAC Bayesline Total DSL X X X X X X ADI X X X X X 11 CLP X X X X X X GDI X X X X X X X X X X 10 6 X X X 3 System Description Paper (Hanani et al., 2017) – (Bestgen, 2017) (G´omez-Adorno et al., 2017) (Gamallo et al., 2017) (Clematide and Makarov, 2017) (Rosa et al., 2017) – – (Tiedemann, 2017) (Malmasi and Zampieri, 2017a) (Malmasi and Zampieri, 2017b) (Medvedeva et al., 2017) – (Jauhiainen et al., 2017) (Criscuolo and Aluisio, 2017) (C ¸ o¨ ltekin and Rama, 2017) (Ionescu and Butnaru, 2017) (Barbaresi, 2017) 15 Table 1: The teams that participated in the VarDial’2017 Evaluation Campaign. 1.2 Participating Teams The fourth edition of the DSL shared task was motivated by the success of the previous editions and by the growing interest of the research community in the identification of dialects and similar langua"
W17-1201,W13-1728,1,0.0339111,"rovided lexical features. This year, we added a multi-model aspect to the task by further providing acoustic features. The system description paper of CECL (Bestgen, 2017) provides some interesting insights about the DSL task. First, they found out that BM25 weighting, which was previously applied to native language identification (NLI) (Wang et al., 2016), worked better than using TF.IDF. They further highlighted the similarity between similar language identification and NLI as evidenced by a number of entries in the DSL task that are adaptations of systems used for NLI (Goutte et al., 2013; Gebre et al., 2013; Jarvis et al., 2013). We observe that the variation in performance among the top ten teams is less than four percentage points. The team ranked last (eleventh) approached the task using LSTM and achieved an F1 score of 0.202. Unfortunately, they did not submit a system description paper, and thus we do not have much detail about their system. However, in the DSL 2016 task (Malmasi et al., 2016), neural network-based approaches already proved not to be very competitive for the task. See (Medvedeva et al., 2017) for a comparison between the performance of an SVM and an RNN approach for the DSL"
W17-1201,W17-1217,0,0.0300428,"Missing"
W17-1201,W15-5413,0,0.101469,"Missing"
W17-1201,W13-1712,0,0.199423,"Missing"
W17-1201,W14-5316,0,0.160565,"Missing"
W17-1201,U13-1003,0,0.160089,"est set A: in the closed training setting, where the systems were trained only using the training data provided by the DSL organizers, their accuracy dropped from 95.54 to 94.01, from 95.24 to 92.78, from 95.24 to 93.01, and from 94.67 to 93.02, respectively.3 Finally, inspired by recent work on language identification of user-generated content (Ljubeˇsi´c and Kranjˇci´c, 2015; Abainia et al., 2016), in the DSL 2016 task (Malmasi et al., 2016), we looked at how systems perform on discriminating between similar languages and language varieties across different domains, an aspect highlighted by Lui and Cook (2013) and Lui (2014). For this purpose, we provided an out-of-domain test set containing manually annotated microblog posts written in Bosnian, Croatian, Serbian, Brazilian and European Portuguese. 2.1 2.2 Dataset The DSLCC v4.04 contains 22,000 short excerpts of news texts for each language or language variety divided into 20,000 texts for training (18,000 texts) and development (2,000 texts), and 2,000 texts for testing. It contains a total of 8.6 million tokens for training and over half a million tokens for testing. The fourteen languages included in the v4.0 grouped by similarity are Bosnian,"
W17-1201,L16-1284,1,0.900242,"Missing"
W17-1201,W16-3928,0,0.0176574,"ranging from 3 for CLP to 11 for DSL. Below we describe the individual tasks. 2 Discriminating between Similar Languages (DSL) Discriminating between similar languages is one of the main challenges faced by language identification systems. Since 2014 the DSL shared task has been organized every year providing scholars and developers with an opportunity to evaluate language identification methods using a standard dataset and evaluation methodology. Albeit related to other shared tasks such as the 2014 TweetLID challenge (Zubiaga et al., 2014) and the 2016 shared task on Geolocation Prediction (Han et al., 2016), the DSL shared task continues to be the only shared task focusing on the discrimination between similar languages and language varieties. 1 The MAZA team submitted two separate papers: one for each task they participated in. 2 This number does not include the submissions to the Arabic Dialect Identification subtask of DSL in 2016. 2 At DSL 2015, the four best systems, MAC (Malmasi and Dras, 2015b), MMS (Zampieri et al., 2015a), NRC (Goutte and L´eger, 2015), and SUKI (Jauhiainen et al., 2015) performed similarly on test set B compared to test set A: in the closed training setting, where the"
W17-1201,W15-5407,1,0.88226,"ods using a standard dataset and evaluation methodology. Albeit related to other shared tasks such as the 2014 TweetLID challenge (Zubiaga et al., 2014) and the 2016 shared task on Geolocation Prediction (Han et al., 2016), the DSL shared task continues to be the only shared task focusing on the discrimination between similar languages and language varieties. 1 The MAZA team submitted two separate papers: one for each task they participated in. 2 This number does not include the submissions to the Arabic Dialect Identification subtask of DSL in 2016. 2 At DSL 2015, the four best systems, MAC (Malmasi and Dras, 2015b), MMS (Zampieri et al., 2015a), NRC (Goutte and L´eger, 2015), and SUKI (Jauhiainen et al., 2015) performed similarly on test set B compared to test set A: in the closed training setting, where the systems were trained only using the training data provided by the DSL organizers, their accuracy dropped from 95.54 to 94.01, from 95.24 to 92.78, from 95.24 to 93.01, and from 94.67 to 93.02, respectively.3 Finally, inspired by recent work on language identification of user-generated content (Ljubeˇsi´c and Kranjˇci´c, 2015; Abainia et al., 2016), in the DSL 2016 task (Malmasi et al., 2016), we l"
W17-1201,W17-1222,1,0.800029,"milar Languages, Varieties and Dialects, pages 1–15, c Valencia, Spain, April 3, 2017. 2017 Association for Computational Linguistics Team ahaqst bayesline CECL cic ualg Citius Ixa Imaxin CLUZH CUNI deepCybErNet gauge Helsinki-CLP MAZA (ADI) MAZA (GDI) mm lct qcri mit SUKI timeflow tubasfs unibuckernel XAC Bayesline Total DSL X X X X X X ADI X X X X X 11 CLP X X X X X X GDI X X X X X X X X X X 10 6 X X X 3 System Description Paper (Hanani et al., 2017) – (Bestgen, 2017) (G´omez-Adorno et al., 2017) (Gamallo et al., 2017) (Clematide and Makarov, 2017) (Rosa et al., 2017) – – (Tiedemann, 2017) (Malmasi and Zampieri, 2017a) (Malmasi and Zampieri, 2017b) (Medvedeva et al., 2017) – (Jauhiainen et al., 2017) (Criscuolo and Aluisio, 2017) (C ¸ o¨ ltekin and Rama, 2017) (Ionescu and Butnaru, 2017) (Barbaresi, 2017) 15 Table 1: The teams that participated in the VarDial’2017 Evaluation Campaign. 1.2 Participating Teams The fourth edition of the DSL shared task was motivated by the success of the previous editions and by the growing interest of the research community in the identification of dialects and similar languages, as evidenced by recent publications (Xu et al., 2016; Radford and Gall´e, 2016; Castro et al.,"
W17-1201,W17-1211,0,0.333885,"Danish, and Norwegian (TL) – Swedish (SL). Note that the latter two pairs include a triple of related languages. 1 Proceedings of the Fourth Workshop on NLP for Similar Languages, Varieties and Dialects, pages 1–15, c Valencia, Spain, April 3, 2017. 2017 Association for Computational Linguistics Team ahaqst bayesline CECL cic ualg Citius Ixa Imaxin CLUZH CUNI deepCybErNet gauge Helsinki-CLP MAZA (ADI) MAZA (GDI) mm lct qcri mit SUKI timeflow tubasfs unibuckernel XAC Bayesline Total DSL X X X X X X ADI X X X X X 11 CLP X X X X X X GDI X X X X X X X X X X 10 6 X X X 3 System Description Paper (Hanani et al., 2017) – (Bestgen, 2017) (G´omez-Adorno et al., 2017) (Gamallo et al., 2017) (Clematide and Makarov, 2017) (Rosa et al., 2017) – – (Tiedemann, 2017) (Malmasi and Zampieri, 2017a) (Malmasi and Zampieri, 2017b) (Medvedeva et al., 2017) – (Jauhiainen et al., 2017) (Criscuolo and Aluisio, 2017) (C ¸ o¨ ltekin and Rama, 2017) (Ionescu and Butnaru, 2017) (Barbaresi, 2017) 15 Table 1: The teams that participated in the VarDial’2017 Evaluation Campaign. 1.2 Participating Teams The fourth edition of the DSL shared task was motivated by the success of the previous editions and by the growing interest of the r"
W17-1201,W17-1220,1,0.878577,"milar Languages, Varieties and Dialects, pages 1–15, c Valencia, Spain, April 3, 2017. 2017 Association for Computational Linguistics Team ahaqst bayesline CECL cic ualg Citius Ixa Imaxin CLUZH CUNI deepCybErNet gauge Helsinki-CLP MAZA (ADI) MAZA (GDI) mm lct qcri mit SUKI timeflow tubasfs unibuckernel XAC Bayesline Total DSL X X X X X X ADI X X X X X 11 CLP X X X X X X GDI X X X X X X X X X X 10 6 X X X 3 System Description Paper (Hanani et al., 2017) – (Bestgen, 2017) (G´omez-Adorno et al., 2017) (Gamallo et al., 2017) (Clematide and Makarov, 2017) (Rosa et al., 2017) – – (Tiedemann, 2017) (Malmasi and Zampieri, 2017a) (Malmasi and Zampieri, 2017b) (Medvedeva et al., 2017) – (Jauhiainen et al., 2017) (Criscuolo and Aluisio, 2017) (C ¸ o¨ ltekin and Rama, 2017) (Ionescu and Butnaru, 2017) (Barbaresi, 2017) 15 Table 1: The teams that participated in the VarDial’2017 Evaluation Campaign. 1.2 Participating Teams The fourth edition of the DSL shared task was motivated by the success of the previous editions and by the growing interest of the research community in the identification of dialects and similar languages, as evidenced by recent publications (Xu et al., 2016; Radford and Gall´e, 2016; Castro et al.,"
W17-1201,W16-4801,1,0.679876,"Missing"
W17-1201,W17-1225,0,0.428199,"us Ixa Imaxin CLUZH CUNI deepCybErNet gauge Helsinki-CLP MAZA (ADI) MAZA (GDI) mm lct qcri mit SUKI timeflow tubasfs unibuckernel XAC Bayesline Total DSL X X X X X X ADI X X X X X 11 CLP X X X X X X GDI X X X X X X X X X X 10 6 X X X 3 System Description Paper (Hanani et al., 2017) – (Bestgen, 2017) (G´omez-Adorno et al., 2017) (Gamallo et al., 2017) (Clematide and Makarov, 2017) (Rosa et al., 2017) – – (Tiedemann, 2017) (Malmasi and Zampieri, 2017a) (Malmasi and Zampieri, 2017b) (Medvedeva et al., 2017) – (Jauhiainen et al., 2017) (Criscuolo and Aluisio, 2017) (C ¸ o¨ ltekin and Rama, 2017) (Ionescu and Butnaru, 2017) (Barbaresi, 2017) 15 Table 1: The teams that participated in the VarDial’2017 Evaluation Campaign. 1.2 Participating Teams The fourth edition of the DSL shared task was motivated by the success of the previous editions and by the growing interest of the research community in the identification of dialects and similar languages, as evidenced by recent publications (Xu et al., 2016; Radford and Gall´e, 2016; Castro et al., 2016). We also saw the number of system submissions to the DSL challenge grow from 8 in 2014 to 10 in 2015 and then to 17 in 2016.2 The 2015 and the 2016 editions of the DSL"
W17-1201,W13-1714,0,0.148384,"ures. This year, we added a multi-model aspect to the task by further providing acoustic features. The system description paper of CECL (Bestgen, 2017) provides some interesting insights about the DSL task. First, they found out that BM25 weighting, which was previously applied to native language identification (NLI) (Wang et al., 2016), worked better than using TF.IDF. They further highlighted the similarity between similar language identification and NLI as evidenced by a number of entries in the DSL task that are adaptations of systems used for NLI (Goutte et al., 2013; Gebre et al., 2013; Jarvis et al., 2013). We observe that the variation in performance among the top ten teams is less than four percentage points. The team ranked last (eleventh) approached the task using LSTM and achieved an F1 score of 0.202. Unfortunately, they did not submit a system description paper, and thus we do not have much detail about their system. However, in the DSL 2016 task (Malmasi et al., 2016), neural network-based approaches already proved not to be very competitive for the task. See (Medvedeva et al., 2017) for a comparison between the performance of an SVM and an RNN approach for the DSL task. 2.5 Arabic Dial"
W17-1201,W17-1219,0,0.132166,"cia, Spain, April 3, 2017. 2017 Association for Computational Linguistics Team ahaqst bayesline CECL cic ualg Citius Ixa Imaxin CLUZH CUNI deepCybErNet gauge Helsinki-CLP MAZA (ADI) MAZA (GDI) mm lct qcri mit SUKI timeflow tubasfs unibuckernel XAC Bayesline Total DSL X X X X X X ADI X X X X X 11 CLP X X X X X X GDI X X X X X X X X X X 10 6 X X X 3 System Description Paper (Hanani et al., 2017) – (Bestgen, 2017) (G´omez-Adorno et al., 2017) (Gamallo et al., 2017) (Clematide and Makarov, 2017) (Rosa et al., 2017) – – (Tiedemann, 2017) (Malmasi and Zampieri, 2017a) (Malmasi and Zampieri, 2017b) (Medvedeva et al., 2017) – (Jauhiainen et al., 2017) (Criscuolo and Aluisio, 2017) (C ¸ o¨ ltekin and Rama, 2017) (Ionescu and Butnaru, 2017) (Barbaresi, 2017) 15 Table 1: The teams that participated in the VarDial’2017 Evaluation Campaign. 1.2 Participating Teams The fourth edition of the DSL shared task was motivated by the success of the previous editions and by the growing interest of the research community in the identification of dialects and similar languages, as evidenced by recent publications (Xu et al., 2016; Radford and Gall´e, 2016; Castro et al., 2016). We also saw the number of system submissions to th"
W17-1201,W15-5408,0,0.243101,"Missing"
W17-1201,W16-4820,0,0.425079,"Missing"
W17-1201,W17-1212,0,0.183999,"Missing"
W17-1201,W17-1226,0,0.0868204,"Missing"
W17-1201,L16-1641,1,0.367123,"Missing"
W17-1201,N15-1010,0,0.0146702,"Missing"
W17-1201,W15-3040,0,0.0163982,"ams (1–6), and run 3 used hard voting of three systems: SVM, Logistic Regression, and Na”ive Bayes and character ngrams (2–6) as features. • bayesline: This team participated with a Multinomial Na¨ıve Bayes (MNB) classifier similar to that of Tan et al. (2014), with no special parameter tuning, as this system was initially intended to serve as an intelligent baseline for the task (but now it has matured into a competitive system). In their bestperforming run 1, they relied primarily on character 4-grams as features. The feature sets they used were selected by a search strategy as proposed in (Scarton et al., 2015). • cic ualg: This team submitted three runs. Runs 1 and 2 first predict the language group, and then discriminate between the languages within that group. The first step uses an SVM classifier with a combination of character 3– 5-grams, typed character 3-grams, applying the character n-gram categories introduced by Sapkota et al. (2015), and word unigrams using TF-weighting. The second step uses the same features and different classifiers: SVMs + Multinominal Na¨ıve Bayes (MNB) in run 1, and MNB in run 2 (which works best). Run 3 uses a single MNB classifier to discriminate between all fourte"
W17-1201,D10-1112,1,0.910362,"CN i-vector (as in Run 2) with (ii) an SVM model trained on count bag of characters 2–4-grams, which yielded an F1 of 0.612. This year, we introduced a new dialectal area, which focused on German dialects of Switzerland. Indeed, the German-speaking part of Switzerland is characterized by the widespread use of dialects in everyday communication, and by a large number of different dialects and dialectal areas. There have been two major approaches to Swiss German dialect identification in the literature. The corpus-based approach predicts the dialect of any text fragment extracted from a corpus (Scherrer and Rambow, 2010; Hollenstein and Aepli, 2015). The dialectological approach tries to identify a small set of distinguishing dialectal features, which are then elicited interactively from the user in order to identify his or her dialect (Leemann et al., 2016). In this task, we adopt a corpus-based approach, and we develop a new dataset for this. • deepCybErNet: This team submitted two runs. Run 1 adopted a Bi-LSTM architecture using the lexical features, and achieved an F1 score of 0.208, while run 2 used the i-vector features and achieved an F1 of 0.574. 3.3 Results Table 5 shows the evaluation results for t"
W17-1201,W14-5307,1,0.307051,"Missing"
W17-1201,W15-5411,1,0.900323,"Missing"
W17-1201,L16-1680,0,0.0188911,"Missing"
W17-1201,N12-1052,0,0.0102303,"Missing"
W17-1201,W14-1614,1,0.904663,"Missing"
W17-1201,tiedemann-2012-parallel,1,0.0255189,"LP task: parallel training data. Participants were asked not to use the development data with their gold standard annotation of dependency relations for any training purposes. The purpose of the development datasets is entirely for testing model performance during system development. All the knowledge used for parsing should origin in the provided source language data. Other sources (except for target language sources) could also be used in unconstrained submissions, but none of the participants chose that option. For the constrained setup, we also provided parallel datasets coming from OPUS (Tiedemann, 2012) that could be used for training cross-lingual parsers in any way. The datasets included translated movie subtitles and contained quite a bit of noise in terms of alignment, encoding, and translation quality. They were also from a very different domain, which made the setup quite realistic considering that one would used whatever could be found for the task. The sizes of the parallel datasets are given in Table 8. In the setup of the shared task, we also provided simple baselines and an “upper bound” of a model trained on annotated target language data. The cross-lingual baselines included del"
W17-1201,C14-1175,1,0.927054,"nd without any optimization of the hyper parameters. The size of the source language data is given in Table 5. We can see that for Czech we have by far the largest corpus, which will also be reflected in the results we obtain. VarDial 2017 featured for the first time a crosslingual parsing task for closely related languages.7 Transfer learning and annotation projection are popular approaches in this field and various techniques and models have been proposed in the literature in particular in connection with dependency parsing (Hwa et al., 2005; McDonald et al., 2013; T¨ackstr¨om et al., 2012; Tiedemann, 2014). The motivation for cross-lingual models is the attempt to bootstrap tools for languages that do not have annotated resources, which are typically necessary for supervised data-driven techniques, using data and resources from other languages. This is especially successful for closely related languages with similar syntactic structures and strong lexical overlap (Agi´c et al., 2012). With this background, it is a natural extension for our shared task to consider cross-lingual parsing as well. We do so by simulating the resource-poor situation by selecting language pairs from the Universal Depe"
W17-1201,W15-2137,1,0.514813,"ad of around 0.7. 4.5 Summary This first edition of the GDI task was a success, given the short time between the 2016 and 2017 editions. In the future, we would like to better control transcriber effects, either by a more thorough selection of training and test data, or by adding transcriber-independent features such as acoustic features, as has been done in the ADI task this year. Further dialectal areas could also be added. 10 5 Cross-lingual Dependency Parsing (CLP) Avoiding gold labels is important here in order to avoid exaggerated results that blur the picture of a more realistic setup (Tiedemann, 2015). The tagger models are trained on the original target language treebanks using UDpipe (Straka et al., 2016) with standard settings and without any optimization of the hyper parameters. The size of the source language data is given in Table 5. We can see that for Czech we have by far the largest corpus, which will also be reflected in the results we obtain. VarDial 2017 featured for the first time a crosslingual parsing task for closely related languages.7 Transfer learning and annotation projection are popular approaches in this field and various techniques and models have been proposed in th"
W17-1201,W17-1216,1,0.921592,"shop on NLP for Similar Languages, Varieties and Dialects, pages 1–15, c Valencia, Spain, April 3, 2017. 2017 Association for Computational Linguistics Team ahaqst bayesline CECL cic ualg Citius Ixa Imaxin CLUZH CUNI deepCybErNet gauge Helsinki-CLP MAZA (ADI) MAZA (GDI) mm lct qcri mit SUKI timeflow tubasfs unibuckernel XAC Bayesline Total DSL X X X X X X ADI X X X X X 11 CLP X X X X X X GDI X X X X X X X X X X 10 6 X X X 3 System Description Paper (Hanani et al., 2017) – (Bestgen, 2017) (G´omez-Adorno et al., 2017) (Gamallo et al., 2017) (Clematide and Makarov, 2017) (Rosa et al., 2017) – – (Tiedemann, 2017) (Malmasi and Zampieri, 2017a) (Malmasi and Zampieri, 2017b) (Medvedeva et al., 2017) – (Jauhiainen et al., 2017) (Criscuolo and Aluisio, 2017) (C ¸ o¨ ltekin and Rama, 2017) (Ionescu and Butnaru, 2017) (Barbaresi, 2017) 15 Table 1: The teams that participated in the VarDial’2017 Evaluation Campaign. 1.2 Participating Teams The fourth edition of the DSL shared task was motivated by the success of the previous editions and by the growing interest of the research community in the identification of dialects and similar languages, as evidenced by recent publications (Xu et al., 2016; Radford and G"
W17-1220,L16-1522,0,0.0672129,"been the focus of a few recent studies. Methods for German dialect identification have proved to be particularly important for the validation of methods applied to the compilation of German dialect corpora (Scherrer and Rambow, 2010a; Scherrer and Rambow, 2010b; Hollenstein and Aepli, 2015). The work presented here also relates to studies on the discrimination between groups of similar languages, language varieties, and dialects such as South Slavic languages (Ljubeˇsi´c et al., 2007), Portuguese varieties (Zampieri and Gebre, 2012), English varieties (Lui and Cook, 2013), Romanian dialects (Ciobanu and Dinu, 2016), Chinese varieties (Xu et al., 2016), and past editions of the DSL shared task (Zampieri et al., 2014; Zampieri et al., 2015; Malmasi et al., 2016c). Introduction German is well-known for its intrinsic dialectal variation. Standard national varieties spoken in Germany, Austria, and Switzerland co-exist with a number of dialects spoken in everyday communication. The case of Switzerland is particular representative of this situation because of the multitude and importance of dialects which are widely spoken throughout the country. The German Dialect Identification (GDI) task, part of the VarDia"
W17-1220,L16-1641,0,0.266934,"Missing"
W17-1220,W14-5310,0,0.131388,"I shared task. 1 2 Processing dialectal data is a challenge for NLP applications. When dealing with nonstandard language, systems are trained to recognize spelling and syntactic variation for further processing in applications such as Machine Translation. In the case of German, a number of studies have been published on developing NLP tools and resources for processing non-standard language (Dipper et al., 2013), dealing with spelling variation on dialectal data and carrying out spelling normalization (Samardˇzi´c et al., 2015), and improving the performance of POS taggers for dialectal data (Hollenstein and Aepli, 2014). The identification of Swiss German dialects, the topic of the GDI shared task, has been the focus of a few recent studies. Methods for German dialect identification have proved to be particularly important for the validation of methods applied to the compilation of German dialect corpora (Scherrer and Rambow, 2010a; Scherrer and Rambow, 2010b; Hollenstein and Aepli, 2015). The work presented here also relates to studies on the discrimination between groups of similar languages, language varieties, and dialects such as South Slavic languages (Ljubeˇsi´c et al., 2007), Portuguese varieties (Za"
W17-1220,D10-1112,0,0.109538,"n developing NLP tools and resources for processing non-standard language (Dipper et al., 2013), dealing with spelling variation on dialectal data and carrying out spelling normalization (Samardˇzi´c et al., 2015), and improving the performance of POS taggers for dialectal data (Hollenstein and Aepli, 2014). The identification of Swiss German dialects, the topic of the GDI shared task, has been the focus of a few recent studies. Methods for German dialect identification have proved to be particularly important for the validation of methods applied to the compilation of German dialect corpora (Scherrer and Rambow, 2010a; Scherrer and Rambow, 2010b; Hollenstein and Aepli, 2015). The work presented here also relates to studies on the discrimination between groups of similar languages, language varieties, and dialects such as South Slavic languages (Ljubeˇsi´c et al., 2007), Portuguese varieties (Zampieri and Gebre, 2012), English varieties (Lui and Cook, 2013), Romanian dialects (Ciobanu and Dinu, 2016), Chinese varieties (Xu et al., 2016), and past editions of the DSL shared task (Zampieri et al., 2014; Zampieri et al., 2015; Malmasi et al., 2016c). Introduction German is well-known for its intrinsic dialect"
W17-1220,W15-4415,0,0.0480206,"achieved first place in both the 2015 (Malmasi and Dras, 2015a) and 2014 (Goutte et al., 2014) editions of the DSL shared task.4 3.4 Ensemble Classifiers The best performing system in the 2015 edition of the DSL challenge (Malmasi and Dras, 2015a) used SVM ensembles evidencing the adequacy of this approach for the task of discriminating between similar languages and language varieties. In light of this, we decided to test two ensemble methods. Classifier ensembles have also proven to be an efficient and robust alternative in other text classification tasks such as grammatical error detection (Xiang et al., 2015), and complex word identification (Malmasi et al., 2016a). We follow the methodology described by Malmasi and Dras (2015a): we extract a number of different feature types and train a single linear model using each feature type. Our ensemble was created using linear Support Vector Machine classifiers. We used the seven feature types listed in Section 3.2 to create our ensemble of classifiers. Each classifier predicts every input and also assigns a continuous output to each of the possible labels. Using this information, we created the following two ensembles. Features We employ two lexical surf"
W17-1220,U13-1003,0,0.157345,"s, the topic of the GDI shared task, has been the focus of a few recent studies. Methods for German dialect identification have proved to be particularly important for the validation of methods applied to the compilation of German dialect corpora (Scherrer and Rambow, 2010a; Scherrer and Rambow, 2010b; Hollenstein and Aepli, 2015). The work presented here also relates to studies on the discrimination between groups of similar languages, language varieties, and dialects such as South Slavic languages (Ljubeˇsi´c et al., 2007), Portuguese varieties (Zampieri and Gebre, 2012), English varieties (Lui and Cook, 2013), Romanian dialects (Ciobanu and Dinu, 2016), Chinese varieties (Xu et al., 2016), and past editions of the DSL shared task (Zampieri et al., 2014; Zampieri et al., 2015; Malmasi et al., 2016c). Introduction German is well-known for its intrinsic dialectal variation. Standard national varieties spoken in Germany, Austria, and Switzerland co-exist with a number of dialects spoken in everyday communication. The case of Switzerland is particular representative of this situation because of the multitude and importance of dialects which are widely spoken throughout the country. The German Dialect I"
W17-1220,W14-5307,1,0.899768,"Missing"
W17-1220,W15-5407,1,0.768111,"sified by the systems. The training set contains a total of around 14,000 instances (114,000 tokens) and the test set contains a total of 3,638 instances (29,500 tokens). We approach the text using ensemble classifiers and a meta-classifier. In the next sections we describe the features and algorithms used in the MAZA submissions in detail. 3.2 For our base classifier we use a linear Support Vector Machine (SVM). SVMs have proven to deliver very good performance in discriminating between language varieties and in other text classification problems,3 SVMs achieved first place in both the 2015 (Malmasi and Dras, 2015a) and 2014 (Goutte et al., 2014) editions of the DSL shared task.4 3.4 Ensemble Classifiers The best performing system in the 2015 edition of the DSL challenge (Malmasi and Dras, 2015a) used SVM ensembles evidencing the adequacy of this approach for the task of discriminating between similar languages and language varieties. In light of this, we decided to test two ensemble methods. Classifier ensembles have also proven to be an efficient and robust alternative in other text classification tasks such as grammatical error detection (Xiang et al., 2015), and complex word identification (Malmasi"
W17-1220,W16-4814,1,0.849057,"e used a Random Forest as our meta-classification algorithm. We submitted this system as run 3. 4 Results In this section we present results in two steps. First we comment on the performance obtained using each feature type and the results obtained by cross-validation on the training set. Secondly, we present the official results obtained by our system on the test set and we discuss the performance of our best method in identifying each dialect. Meta-classifier System In addition to classifier ensembles, meta-classifier systems have proven to be very competitive for text classification tasks (Malmasi and Zampieri, 2016) and we decided to include a meta-classifier in our entry. Also referred to as classifier stacking. A meta-classifier architecture is generally composed of an ensemble of base classifiers that each make predictions for all of the input data. Their individual predictions, along with the gold labels are used to train a second-level meta-classifier that learns to predict the label for an input, given the decisions of the individual classifiers. This setup is illustrated in Figure 1. This meta-classifier attempts to learn from the collective knowledge rep4.1 Cross-validation Results We first repor"
W17-1220,W16-0314,1,0.945278,"ods applied to the compilation of German dialect corpora (Scherrer and Rambow, 2010a; Scherrer and Rambow, 2010b; Hollenstein and Aepli, 2015). The work presented here also relates to studies on the discrimination between groups of similar languages, language varieties, and dialects such as South Slavic languages (Ljubeˇsi´c et al., 2007), Portuguese varieties (Zampieri and Gebre, 2012), English varieties (Lui and Cook, 2013), Romanian dialects (Ciobanu and Dinu, 2016), Chinese varieties (Xu et al., 2016), and past editions of the DSL shared task (Zampieri et al., 2014; Zampieri et al., 2015; Malmasi et al., 2016c). Introduction German is well-known for its intrinsic dialectal variation. Standard national varieties spoken in Germany, Austria, and Switzerland co-exist with a number of dialects spoken in everyday communication. The case of Switzerland is particular representative of this situation because of the multitude and importance of dialects which are widely spoken throughout the country. The German Dialect Identification (GDI) task, part of the VarDial Evaluation Campaign 2017 (Zampieri et al., 2017), addressed the problem of German dialectal variation by providing a dataset of transcripts from"
W17-1220,W16-4801,1,0.834739,"Missing"
W17-1222,al-sabbagh-girju-2012-yadac,0,0.0987336,"Missing"
W17-1222,W09-0807,0,0.161607,"Missing"
W17-1222,E06-1047,0,0.0769645,"Missing"
W17-1222,cotterell-callison-burch-2014-multi,0,0.0718728,"Missing"
W17-1222,W15-3205,0,0.0141688,"s grown substantially in the last decades. This is evidenced by several publications on the topic and the dedicated series of workshops (WANLP) co-located with major international computational linguistics conferences.1 Several Arabic dialects are spoken in North Africa and in the Middle East co-existing with Modern Standard Arabic (MSA) in a diglossic situation. Arabic dialects are used in both spoken and written forms (e.g. user-generated content) and pose a number of challenges for NLP applications. Several studies on dialectal variation of Arabic have been published including corpus 2 See Shoufan and Al-Ameri (2015) for a survey on NLP methods for processing Arabic dialects including a section on Arabic dialect identification. 1 https://sites.google.com/a/nyu.edu/ wanlp2017/ 178 Proceedings of the Fourth Workshop on NLP for Similar Languages, Varieties and Dialects, pages 178–183, c Valencia, Spain, April 3, 2017. 2017 Association for Computational Linguistics 3 Methods and Data • Word n-grams: The surface forms of words can be used as a feature for classification. Each unique word may be used as a feature (i.e. unigrams), but the use of bigram distributions is also common. In this scenario, the n-grams"
W17-1222,W14-5316,0,0.0484881,"Missing"
W17-1222,L16-1284,1,0.901992,"Missing"
W17-1222,W14-5313,0,0.115926,"Missing"
W17-1222,W15-4415,0,0.112675,"Missing"
W17-1222,W14-5307,1,0.910842,"Missing"
W17-1222,E14-4019,1,0.882308,"Missing"
W17-1222,W15-5407,1,0.899557,"used as a feature for classification. Each unique word may be used as a feature (i.e. unigrams), but the use of bigram distributions is also common. In this scenario, the n-grams are extracted along with their frequency distributions. For this study we evaluate unigram features. We approach this task as a multi-class classification problem. For our base classifier we utilize a linear Support Vector Machine (SVM). SVMs have proven to deliver very good performance in discriminating between language varieties and in other text classification problems, SVMs achieved first place in both the 2015 (Malmasi and Dras, 2015a) and 2014 (Goutte et al., 2014) editions of the DSL shared task.3 3.1 • iVector Audio Features: Identity vectors or iVectors are a probabilistic compression process for dimensionality reduction. They have been used in speech processing for dialect and accent identification (Bahari et al., 2014), as well as for language identification systems (Dehak et al., 2011). Data The data comes from the aforementioned Arabic dialect dataset by Ali et al. (2016) used in the 2016 edition of the ADI shared task. It contains audio and ASR transcripts of broadcast, debate, and discussion programs from videos"
W17-1222,W16-4814,1,0.537212,"is taken in to account, even when it is not the predicted label (e.g. it could have the second highest probability). This method has been shown to work well on a wide range of problems and, in general, it is considered to be simple, intuitive, stable (Kuncheva, 2014, p. 155) and resilient to estimation errors (Kittler et al., 1998) making it one of the most robust combiners discussed in the literature. We submitted this system as run 2. 3.6 Meta-classifier (System 3) In addition to classifier ensembles, meta-classifier systems have proven to be very competitive for text classification tasks (Malmasi and Zampieri, 2016) and we decided to include a meta-classifier in our entry. Also referred to as classifier stacking, a meta-classifier architecture is generally composed of an ensemble of base classifiers that each make predictions for all of the input data. Their individual predictions, along with the gold labels are used to train a second-level meta-classifier that learns to predict the label for an input, given the decisions of the individual classifiers. This setup is illustrated in Figure 2. This meta-classifier attempts to learn from the collective knowledge represented by the ensemble of local classifie"
W17-1222,N12-1006,0,0.0388755,"Missing"
W17-1222,W16-0314,1,0.87207,"iadsy and Hirschberg (2009), and Bahari et al. (2014). Identifying Arabic dialects in text also became a popular research topic in recent years with several studies published about it (Zaidan and Callison-Burch, 2014; Sadat et al., 2014; Tillmann et al., 2014; Malmasi et al., 2015). To our knowledge, however, the 2017 ADI is the first shared task to provide participants with the opportunity to carry out Arabic dialect identification using a dataset containing both audio and text (transcriptions). The first edition of the ADI shared task, organized in 2016 as a sub-task of the DSL shared task (Malmasi et al., 2016c), used a similar dataset to the ADI 2017 dataset, but included only transcriptions. Introduction The interest in Arabic natural language processing (NLP) has grown substantially in the last decades. This is evidenced by several publications on the topic and the dedicated series of workshops (WANLP) co-located with major international computational linguistics conferences.1 Several Arabic dialects are spoken in North Africa and in the Middle East co-existing with Modern Standard Arabic (MSA) in a diglossic situation. Arabic dialects are used in both spoken and written forms (e.g. user-generat"
W17-1222,W16-4801,1,0.877372,"Missing"
W17-1222,W14-5904,0,0.0644087,"Missing"
W17-5045,W13-1712,0,0.0527967,"Missing"
W17-5045,W17-1214,0,0.014955,"alities into account. The approach used in our submission is described next. • A variation of NRC’s SVM approach (Goutte et al., 2013) competed in the DSL 2014 (Goutte et al., 2014) achieving the best results. • Bobicev applied Prediction for Partial Matching (PPM) in the NLI shared task (Bobicev, 2013) with results that did not reach top ten performance. A similar improved approached competed in the DSL 2015 (Bobicev, 2015) ranking in the top half of the table. • A similar approach to the one by Jarvis (Jarvis et al., 2013) that ranked 1st place in the NLI task 2013 competed in the DSL 2017 (Bestgen, 2017), achieving the best performance in the competition. 3.2 • Variations of MQ’s SVM ensemble approach (Malmasi et al., 2013) have competed in the DSL 2015 (Malmasi and Dras, 2015) and the ADI 2016 (Malmasi and Zampieri, 2016), achieving the best performance in both shared tasks. Approach We built a classification system based on SVM ensembles, following the methodology proposed by Malmasi and Dras (2015). The idea behind classification ensembles is to improve the overall performance by combining the results of multiple classifiers. Such systems have proved successful not only in NLI and dialect"
W17-5045,W14-5316,0,0.0410166,"Missing"
W17-5045,L16-1284,1,0.91673,"Missing"
W17-5045,W13-1724,0,0.172835,"including iVectors) could be used. The test dataset, containing 1,100 instances with essays, speech transcriptions and iVectors, was released at a later date. The use of a dataset containing text and speech is the main new aspect of the 2017 NLI task so we decide to compete in the fusion track taking both modalities into account. The approach used in our submission is described next. • A variation of NRC’s SVM approach (Goutte et al., 2013) competed in the DSL 2014 (Goutte et al., 2014) achieving the best results. • Bobicev applied Prediction for Partial Matching (PPM) in the NLI shared task (Bobicev, 2013) with results that did not reach top ten performance. A similar improved approached competed in the DSL 2015 (Bobicev, 2015) ranking in the top half of the table. • A similar approach to the one by Jarvis (Jarvis et al., 2013) that ranked 1st place in the NLI task 2013 competed in the DSL 2017 (Bestgen, 2017), achieving the best performance in the competition. 3.2 • Variations of MQ’s SVM ensemble approach (Malmasi et al., 2013) have competed in the DSL 2015 (Malmasi and Dras, 2015) and the ADI 2016 (Malmasi and Zampieri, 2016), achieving the best performance in both shared tasks. Approach We"
W17-5045,W13-1713,0,0.0547241,"Missing"
W17-5045,W15-5410,0,0.162088,"ors, was released at a later date. The use of a dataset containing text and speech is the main new aspect of the 2017 NLI task so we decide to compete in the fusion track taking both modalities into account. The approach used in our submission is described next. • A variation of NRC’s SVM approach (Goutte et al., 2013) competed in the DSL 2014 (Goutte et al., 2014) achieving the best results. • Bobicev applied Prediction for Partial Matching (PPM) in the NLI shared task (Bobicev, 2013) with results that did not reach top ten performance. A similar improved approached competed in the DSL 2015 (Bobicev, 2015) ranking in the top half of the table. • A similar approach to the one by Jarvis (Jarvis et al., 2013) that ranked 1st place in the NLI task 2013 competed in the DSL 2017 (Bestgen, 2017), achieving the best performance in the competition. 3.2 • Variations of MQ’s SVM ensemble approach (Malmasi et al., 2013) have competed in the DSL 2015 (Malmasi and Dras, 2015) and the ADI 2016 (Malmasi and Zampieri, 2016), achieving the best performance in both shared tasks. Approach We built a classification system based on SVM ensembles, following the methodology proposed by Malmasi and Dras (2015). The ide"
W17-5045,brooke-hirst-2012-measuring,0,0.0175122,"terances) that are likely to be written or spoken by speakers of the same language. There are two important reasons to study NLI. Firstly, there is SLA. NLI methods can be applied to learner corpora to investigate the influence of native language in second language acquisition and production complementing corpus-based and corpus-driven studies. The second reason is a 2 Related Work There have been several NLI studies published in the past few years. Due to the availability of suitable language resources for English (e.g. learner corpora), the vast majority of these studies dealt with English (Brooke and Hirst, 2012; Bykh and Meurers, 2014), however, a few NLI studies have been published on other languages. Examples of NLI applied to languages other than English include Arabic (Ionescu, 2015), Chinese (Wang et al., 2016), and Finnish (Malmasi and Dras, 2014). To the best of our knowledge, the NLI Shared Task 2013 (Tetreault et al., 2013) was the first 398 Proceedings of the 12th Workshop on Innovative Use of NLP for Building Educational Applications, pages 398–404 c Copenhagen, Denmark, September 8, 2017. 2017 Association for Computational Linguistics Team Jarvis Oslo Unibuc MITRE Tuebingen NRC CMU-Haifa"
W17-5045,W17-1225,0,0.0196201,"stablished the first benchmark for NLI on written texts. Organizers of the first NLI task provided participants with the TOEFL 11 (Blanchard et al., 2013) dataset which contained essays written by students native speakers of the same eleven languages included in the NLI Shared Task 2017. Twenty-nine teams participated in the competition, testing a wide range of computational methods for NLI. In Table 1 we list the top ten best 399 3 • Variations of the string kernels method by the Unibuc team (Popescu and Ionescu, 2013) competed in the ADI task in 2016 (Ionescu and Popescu, 2016) and in 2017 (Ionescu and Butnaru, 2017) achieving the best results. Methods In the next sections we describe the data provided by the shared task organizers and the ensemble SVM approach applied by the ZCD team. 3.1 • Cologne-Nijmegen’s TF-IDF-based approach (Gebre et al., 2013) competed in the DSL shared task 2015 (Zampieri et al., 2015a) as team MMS ranking among the top 3 systems. Data The organizers of the NLI Shared Task 2017 provided participants with data corresponding to eleven native languages: Arabic, Chinese, French, German, Hindi, Italian, Japanese, Korean, Spanish, Telugu and Turkish. The training dataset consists of 1"
W17-5045,C14-1185,0,0.0862542,"y to be written or spoken by speakers of the same language. There are two important reasons to study NLI. Firstly, there is SLA. NLI methods can be applied to learner corpora to investigate the influence of native language in second language acquisition and production complementing corpus-based and corpus-driven studies. The second reason is a 2 Related Work There have been several NLI studies published in the past few years. Due to the availability of suitable language resources for English (e.g. learner corpora), the vast majority of these studies dealt with English (Brooke and Hirst, 2012; Bykh and Meurers, 2014), however, a few NLI studies have been published on other languages. Examples of NLI applied to languages other than English include Arabic (Ionescu, 2015), Chinese (Wang et al., 2016), and Finnish (Malmasi and Dras, 2014). To the best of our knowledge, the NLI Shared Task 2013 (Tetreault et al., 2013) was the first 398 Proceedings of the 12th Workshop on Innovative Use of NLP for Building Educational Applications, pages 398–404 c Copenhagen, Denmark, September 8, 2017. 2017 Association for Computational Linguistics Team Jarvis Oslo Unibuc MITRE Tuebingen NRC CMU-Haifa Cologne-Nijmegen NAIST U"
W17-5045,W16-4818,0,0.0234789,"red Task 2013 (Tetreault et al., 2013) established the first benchmark for NLI on written texts. Organizers of the first NLI task provided participants with the TOEFL 11 (Blanchard et al., 2013) dataset which contained essays written by students native speakers of the same eleven languages included in the NLI Shared Task 2017. Twenty-nine teams participated in the competition, testing a wide range of computational methods for NLI. In Table 1 we list the top ten best 399 3 • Variations of the string kernels method by the Unibuc team (Popescu and Ionescu, 2013) competed in the ADI task in 2016 (Ionescu and Popescu, 2016) and in 2017 (Ionescu and Butnaru, 2017) achieving the best results. Methods In the next sections we describe the data provided by the shared task organizers and the ensemble SVM approach applied by the ZCD team. 3.1 • Cologne-Nijmegen’s TF-IDF-based approach (Gebre et al., 2013) competed in the DSL shared task 2015 (Zampieri et al., 2015a) as team MMS ranking among the top 3 systems. Data The organizers of the NLI Shared Task 2017 provided participants with data corresponding to eleven native languages: Arabic, Chinese, French, German, Hindi, Italian, Japanese, Korean, Spanish, Telugu and Tur"
W17-5045,W13-1714,0,0.281178,"on word n-grams (1-2), and POS n-grams (1-5), and syntactic features (dependencies) Ensemble of SVM classifiers trained on character trigrams, word n-grams (1-2), POS n-grams (2-4), and syntactic features (dependencies) Maximum Entropy trained on word n-grams (1-4), POS n-grams (1-4), and spelling features SVM classifier with TF-IDF weighting trained on character ngrams (1-6), word n-grams (1-2), and POS n-grams (1-4) SVM trained on character n-grams (2-3), word n-grams (1-2), and POS n-grams (2-3), and syntactic features (dependencies and TSG) SVM trained on word n-grams (1-2) System Paper (Jarvis et al., 2013) (Lynum, 2013) (Popescu and Ionescu, 2013) (Henderson et al., 2013) (Bykh et al., 2013) (Goutte et al., 2013) (Tsvetkov et al., 2013) (Gebre et al., 2013) (Mizumoto et al., 2013) (Wu et al., 2013) Table 1: Top ten NLI Shared Task 2013 entries ordered by performance. entries ranked by performance along with their respective system description papers. The best system by Jarvis et al. (2013) applied a linear SVM classifier trained on character, word, and POS n-grams. Seven out of the ten best entries in the shared task used SVM classifiers. This indicates that SMVs are a very good fit for NLI and"
W17-5045,W13-1726,0,0.0408826,"Missing"
W17-5045,W13-1734,0,0.030856,"Missing"
W17-5045,W13-1728,1,0.873312,"s (1-2), POS n-grams (2-4), and syntactic features (dependencies) Maximum Entropy trained on word n-grams (1-4), POS n-grams (1-4), and spelling features SVM classifier with TF-IDF weighting trained on character ngrams (1-6), word n-grams (1-2), and POS n-grams (1-4) SVM trained on character n-grams (2-3), word n-grams (1-2), and POS n-grams (2-3), and syntactic features (dependencies and TSG) SVM trained on word n-grams (1-2) System Paper (Jarvis et al., 2013) (Lynum, 2013) (Popescu and Ionescu, 2013) (Henderson et al., 2013) (Bykh et al., 2013) (Goutte et al., 2013) (Tsvetkov et al., 2013) (Gebre et al., 2013) (Mizumoto et al., 2013) (Wu et al., 2013) Table 1: Top ten NLI Shared Task 2013 entries ordered by performance. entries ranked by performance along with their respective system description papers. The best system by Jarvis et al. (2013) applied a linear SVM classifier trained on character, word, and POS n-grams. Seven out of the ten best entries in the shared task used SVM classifiers. This indicates that SMVs are a very good fit for NLI and motivates us to test SVM classifiers in our ensemble-based system described in this paper. shared task to provide a benchmark for NLI focusing on written"
W17-5045,U14-1020,0,0.0220629,"in second language acquisition and production complementing corpus-based and corpus-driven studies. The second reason is a 2 Related Work There have been several NLI studies published in the past few years. Due to the availability of suitable language resources for English (e.g. learner corpora), the vast majority of these studies dealt with English (Brooke and Hirst, 2012; Bykh and Meurers, 2014), however, a few NLI studies have been published on other languages. Examples of NLI applied to languages other than English include Arabic (Ionescu, 2015), Chinese (Wang et al., 2016), and Finnish (Malmasi and Dras, 2014). To the best of our knowledge, the NLI Shared Task 2013 (Tetreault et al., 2013) was the first 398 Proceedings of the 12th Workshop on Innovative Use of NLP for Building Educational Applications, pages 398–404 c Copenhagen, Denmark, September 8, 2017. 2017 Association for Computational Linguistics Team Jarvis Oslo Unibuc MITRE Tuebingen NRC CMU-Haifa Cologne-Nijmegen NAIST UTD Approach SVM trained on character n-grams (1-9), word n-grams (1-4), and POS n-grams (1-4) SVM trained on character n-grams (1-7) String Kernels and Local Rank Distance (LRD) Bayes ensemble of multiple classifiers SVM t"
W17-5045,W13-1706,0,0.294568,"pus-driven studies. The second reason is a 2 Related Work There have been several NLI studies published in the past few years. Due to the availability of suitable language resources for English (e.g. learner corpora), the vast majority of these studies dealt with English (Brooke and Hirst, 2012; Bykh and Meurers, 2014), however, a few NLI studies have been published on other languages. Examples of NLI applied to languages other than English include Arabic (Ionescu, 2015), Chinese (Wang et al., 2016), and Finnish (Malmasi and Dras, 2014). To the best of our knowledge, the NLI Shared Task 2013 (Tetreault et al., 2013) was the first 398 Proceedings of the 12th Workshop on Innovative Use of NLP for Building Educational Applications, pages 398–404 c Copenhagen, Denmark, September 8, 2017. 2017 Association for Computational Linguistics Team Jarvis Oslo Unibuc MITRE Tuebingen NRC CMU-Haifa Cologne-Nijmegen NAIST UTD Approach SVM trained on character n-grams (1-9), word n-grams (1-4), and POS n-grams (1-4) SVM trained on character n-grams (1-7) String Kernels and Local Rank Distance (LRD) Bayes ensemble of multiple classifiers SVM trained on word n-grams (1-2), and POS n-grams (1-5), and syntactic features (depe"
W17-5045,W15-5407,0,0.0358415,"et al., 2014) achieving the best results. • Bobicev applied Prediction for Partial Matching (PPM) in the NLI shared task (Bobicev, 2013) with results that did not reach top ten performance. A similar improved approached competed in the DSL 2015 (Bobicev, 2015) ranking in the top half of the table. • A similar approach to the one by Jarvis (Jarvis et al., 2013) that ranked 1st place in the NLI task 2013 competed in the DSL 2017 (Bestgen, 2017), achieving the best performance in the competition. 3.2 • Variations of MQ’s SVM ensemble approach (Malmasi et al., 2013) have competed in the DSL 2015 (Malmasi and Dras, 2015) and the ADI 2016 (Malmasi and Zampieri, 2016), achieving the best performance in both shared tasks. Approach We built a classification system based on SVM ensembles, following the methodology proposed by Malmasi and Dras (2015). The idea behind classification ensembles is to improve the overall performance by combining the results of multiple classifiers. Such systems have proved successful not only in NLI and dialect identification, as evidenced in the previous sections, but also in numerous text classification tasks, among which are complex word identification (Malmasi et al., 2016a) and gr"
W17-5045,W13-1736,0,0.0637207,"Missing"
W17-5045,W17-5007,0,0.117872,"Missing"
W17-5045,W13-1716,0,0.0284878,"outte et al., 2013) competed in the DSL 2014 (Goutte et al., 2014) achieving the best results. • Bobicev applied Prediction for Partial Matching (PPM) in the NLI shared task (Bobicev, 2013) with results that did not reach top ten performance. A similar improved approached competed in the DSL 2015 (Bobicev, 2015) ranking in the top half of the table. • A similar approach to the one by Jarvis (Jarvis et al., 2013) that ranked 1st place in the NLI task 2013 competed in the DSL 2017 (Bestgen, 2017), achieving the best performance in the competition. 3.2 • Variations of MQ’s SVM ensemble approach (Malmasi et al., 2013) have competed in the DSL 2015 (Malmasi and Dras, 2015) and the ADI 2016 (Malmasi and Zampieri, 2016), achieving the best performance in both shared tasks. Approach We built a classification system based on SVM ensembles, following the methodology proposed by Malmasi and Dras (2015). The idea behind classification ensembles is to improve the overall performance by combining the results of multiple classifiers. Such systems have proved successful not only in NLI and dialect identification, as evidenced in the previous sections, but also in numerous text classification tasks, among which are com"
W17-5045,W13-1720,0,0.072442,"Missing"
W17-5045,W16-4814,1,0.792098,"• Bobicev applied Prediction for Partial Matching (PPM) in the NLI shared task (Bobicev, 2013) with results that did not reach top ten performance. A similar improved approached competed in the DSL 2015 (Bobicev, 2015) ranking in the top half of the table. • A similar approach to the one by Jarvis (Jarvis et al., 2013) that ranked 1st place in the NLI task 2013 competed in the DSL 2017 (Bestgen, 2017), achieving the best performance in the competition. 3.2 • Variations of MQ’s SVM ensemble approach (Malmasi et al., 2013) have competed in the DSL 2015 (Malmasi and Dras, 2015) and the ADI 2016 (Malmasi and Zampieri, 2016), achieving the best performance in both shared tasks. Approach We built a classification system based on SVM ensembles, following the methodology proposed by Malmasi and Dras (2015). The idea behind classification ensembles is to improve the overall performance by combining the results of multiple classifiers. Such systems have proved successful not only in NLI and dialect identification, as evidenced in the previous sections, but also in numerous text classification tasks, among which are complex word identification (Malmasi et al., 2016a) and grammatical error diagnosis (Xiang et al., 2015)"
W17-5045,W15-4415,0,0.0799458,"and Zampieri, 2016), achieving the best performance in both shared tasks. Approach We built a classification system based on SVM ensembles, following the methodology proposed by Malmasi and Dras (2015). The idea behind classification ensembles is to improve the overall performance by combining the results of multiple classifiers. Such systems have proved successful not only in NLI and dialect identification, as evidenced in the previous sections, but also in numerous text classification tasks, among which are complex word identification (Malmasi et al., 2016a) and grammatical error diagnosis (Xiang et al., 2015). The classifiers can differ in a wide range of aspects, such as algorithms, training data, features or parameters. In our system, the classifiers used different features. We experimented with the following features: character n-grams (with n in {1, ..., 10}) from essays and speech transcripts, word n-grams (with n in {1, 2}) from essays and speech transcripts, and iVectors. For the n-gram features we This section evidenced an important overlap between NLI methods and dialect identification methods both in terms of participation overlap in the shared tasks and in terms of successful approaches"
W17-5045,W16-4801,1,0.910549,"Missing"
W17-5045,W15-5411,1,0.872338,"Missing"
W17-5045,W13-1717,0,0.0610131,"Missing"
W17-5045,W17-1201,1,0.897772,"Missing"
W17-5045,W14-5307,1,0.921728,"Missing"
W17-5045,W13-1735,0,0.176378,"ams (1-5), and syntactic features (dependencies) Ensemble of SVM classifiers trained on character trigrams, word n-grams (1-2), POS n-grams (2-4), and syntactic features (dependencies) Maximum Entropy trained on word n-grams (1-4), POS n-grams (1-4), and spelling features SVM classifier with TF-IDF weighting trained on character ngrams (1-6), word n-grams (1-2), and POS n-grams (1-4) SVM trained on character n-grams (2-3), word n-grams (1-2), and POS n-grams (2-3), and syntactic features (dependencies and TSG) SVM trained on word n-grams (1-2) System Paper (Jarvis et al., 2013) (Lynum, 2013) (Popescu and Ionescu, 2013) (Henderson et al., 2013) (Bykh et al., 2013) (Goutte et al., 2013) (Tsvetkov et al., 2013) (Gebre et al., 2013) (Mizumoto et al., 2013) (Wu et al., 2013) Table 1: Top ten NLI Shared Task 2013 entries ordered by performance. entries ranked by performance along with their respective system description papers. The best system by Jarvis et al. (2013) applied a linear SVM classifier trained on character, word, and POS n-grams. Seven out of the ten best entries in the shared task used SVM classifiers. This indicates that SMVs are a very good fit for NLI and motivates us to test SVM classifiers in o"
W17-5045,W15-5401,1,0.918132,"Missing"
W17-5910,S16-1150,0,0.136082,"Missing"
W17-5910,S16-1161,0,0.157498,"Missing"
W17-5910,L16-1284,1,0.850305,"Missing"
W17-5910,S16-1164,0,0.117013,"Missing"
W17-5910,S16-1157,0,0.169796,"Missing"
W17-5910,P13-3015,0,0.19796,"Goutte et al. (2016) for language variety identification. Introduction Lexical complexity plays a crucial role in reading comprehension. Several NLP systems have been developed to simplify texts to second language learners (Petersen and Ostendorf, 2007) and to native speakers with low literacy levels (Specia, 2010) and reading disabilities (Rello et al., 2013). Identifying which words are likely to be considered complex by a given target population is an important task in many text simplification pipelines called complex word identification (CWI). CWI has been addressed as a stand-alone task (Shardlow, 2013) and as part of studies in lexical and text simplification (Paetzold, 2016). The recent SemEval 2016 Task 11 on Complex Word Identification – henceforth SemEval CWI – addressed this challenge by providing participants with a manually annotated dataset for this purpose (Paetzold and Specia, 2016a). In the SemEval CWI dataset, words in context were tagged as complex or non-complex, that is, difficult to be understood by non-native English speakers, or not. Participating teams used this dataset to train classi2 Methods and Experiments In this section we present the data, the methods, and an overv"
W17-5910,S16-1162,0,0.0588997,"Missing"
W17-5910,S16-1146,0,0.0839136,"Missing"
W17-5910,yimam-etal-2017-multilingual,0,0.0467335,"otators over a set of 200 sentences, while the test set is composed by the judgments made over 9,000 sentences by only one annotator. The 9,200 sentences were evenly distributed across the 400 annotators. In the training set, a word is considered to be complex if at least one of the 20 annotators judged them so, thus reproducing a scenario that captures one of the biggest challenges in lexical simplification: predicting the vocabulary limitations of individuals based on the overall limitations of a group. This dataset is one of the few datasets available for CWI, another example is the one by Yimam et al. (2017). We build ensemble classifiers taking the output of systems that participated in the SemEval CWI task as input. This approach is equivalent to training multiple classifiers and combining them using ensembles. Our first goal is to build highperformance classifiers using plurality voting. Our second goal is to estimate the theoretical upper bound performance given the output of the systems that participated in the SemEval CWI competition using the oracle classifier. Following Malmasi et al. (2015) and Goutte et al. (2016) we use two approaches: 2.2 3.1 Plurality Voting: This approach selects th"
W17-5910,W15-0620,1,0.877257,"nvestigate whether human annotation correlates to the systems’ performance by carefully analyzing the samples of multiple annotators. Although in the shared task complexity was modeled as a binary classification task, we pose that lexical complexity should actually be seen in a continuum spectrum. Intuitively, words that are labeled as complex more often should be easier to be predicted by CWI systems. This hypothesis is investigated in Section 3.3. To the best of our knowledge, no evaluation of this kind has been carried out for CWI. The most similar analyses to ours have been carried out by Malmasi et al. (2015) for native language identification and by Goutte et al. (2016) for language variety identification. Introduction Lexical complexity plays a crucial role in reading comprehension. Several NLP systems have been developed to simplify texts to second language learners (Petersen and Ostendorf, 2007) and to native speakers with low literacy levels (Specia, 2010) and reading disabilities (Rello et al., 2013). Identifying which words are likely to be considered complex by a given target population is an important task in many text simplification pipelines called complex word identification (CWI). CWI"
W17-5910,S16-1155,1,0.693934,"Missing"
W17-5910,S16-1152,0,0.123637,"Missing"
W17-5910,C16-1069,1,0.841348,"cal complexity plays a crucial role in reading comprehension. Several NLP systems have been developed to simplify texts to second language learners (Petersen and Ostendorf, 2007) and to native speakers with low literacy levels (Specia, 2010) and reading disabilities (Rello et al., 2013). Identifying which words are likely to be considered complex by a given target population is an important task in many text simplification pipelines called complex word identification (CWI). CWI has been addressed as a stand-alone task (Shardlow, 2013) and as part of studies in lexical and text simplification (Paetzold, 2016). The recent SemEval 2016 Task 11 on Complex Word Identification – henceforth SemEval CWI – addressed this challenge by providing participants with a manually annotated dataset for this purpose (Paetzold and Specia, 2016a). In the SemEval CWI dataset, words in context were tagged as complex or non-complex, that is, difficult to be understood by non-native English speakers, or not. Participating teams used this dataset to train classi2 Methods and Experiments In this section we present the data, the methods, and an overview of the experiments we propose in this paper. The goal of the experiment"
W18-0507,W14-1206,0,0.0720755,"Missing"
W18-0507,W18-0519,0,0.153197,"Missing"
W18-0507,S16-1156,0,0.0850516,"Missing"
W18-0507,S16-1151,0,0.0753176,"Missing"
W18-0507,W18-0539,1,0.893727,"Missing"
W18-0507,W18-0538,0,0.100445,"lower log-probability for complex words. The systems submitted performed the best out of all systems for the cross-lingual task (the French dataset) both for the binary and probabilistic classification tasks, showing a promising direction in the creation of CWI dataset for new languages. 1 74 https://code.google.com/archive/p/word2vec/ tant features. Their best system shows an average performance compared to the other systems in the shared task for the monolingual English binary classification track. NLP-CIC present systems for the English and Spanish multilingual binary classification tasks (Aroyehun et al., 2018). The feature sets include morphological features such as frequency counts of target word on large corpora such as Wikipedia and Simple Wikipedia, syntactic and lexical features, psycholinguistic features from the MRC psycholinguistic database and entity features using the OpenNLP and CoreNLP tools, and word embedding distance as a feature which is computed between the target word and the sentence. Tree learners such as Random Forest, Gradient Boosted, and Tree Ensembles are used to train different classifiers. Furthermore, a deep learning approach based on 2D convolutional (CNN) and word embe"
W18-0507,S16-1148,0,0.0982594,"Missing"
W18-0507,W18-0518,0,0.065579,"on the ensemble techniques where AdaBoost classifier with 5000 estimators achieves the highest results, followed by the bootstrap aggregation classifier of Random Forest. All the features are used for the N EWS and W IKI N EWS datasets, but for the W IKIPEDIA dataset, MCR psycholinguistic features are excluded. For the probabilistic classification task, the same feature setups are used and the Linear Regression algorithm is used to estimate values of targets. CoastalCPH describe systems developed for multilingual and cross-lingual domains for the binary and probabilistic classification tasks (Bingel and Bjerva, 2018). Unlike most systems, they have focused mainly on German, Spanish, and French datasets in order to investigate if multitask learning can be applied to the cross-lingual CWI task. They have devised two models, using languageagnostic approach with an ensemble that comprises of Random Forests (random forest classifiers for the binary classification task and random forest regressors for the probabilistic classification tasks, with 100 trees) and feed-forward neural networks. Camb describes different systems (Gooding and Kochmar, 2018) they have developed for the monolingual English datasets both"
W18-0507,W18-0520,0,0.193647,"l domains for the binary and probabilistic classification tasks (Bingel and Bjerva, 2018). Unlike most systems, they have focused mainly on German, Spanish, and French datasets in order to investigate if multitask learning can be applied to the cross-lingual CWI task. They have devised two models, using languageagnostic approach with an ensemble that comprises of Random Forests (random forest classifiers for the binary classification task and random forest regressors for the probabilistic classification tasks, with 100 trees) and feed-forward neural networks. Camb describes different systems (Gooding and Kochmar, 2018) they have developed for the monolingual English datasets both for the binary and probabilistic classification tasks. They have used features that are based on the insights of the CWI shared task 2016 (Paetzold and Specia, 2016a) such as lexical features (word length, number of syllables, WordNet features such as the number of synsets), word n-gram and POS tags, and dependency parse relations. In addition, they have used features such as the number of words grammatically related to the target word, psycholinguistic features from the MRC database, CEFR (Common European Framework of Reference fo"
W18-0507,S16-1160,0,0.102164,"Missing"
W18-0507,W18-0540,0,0.0627627,"Missing"
W18-0507,W18-0521,0,0.24579,"layers followed by a sigmoid layer, which predicted the probability of the target word being complex. Their experiments show that the feature engineering approach achieved the best results using the XGBoost classifier for the binary classification task. They submitted four systems using XGBoost, average embeddings, LSTMs with transfer learning, and a voting system that combines the other three. For the probabilistic classification task, their LSTMs achieve the best results. TMU submitted multilingual and cross-lingual CWI systems for both of the binary and probabilistic classification tasks (Kajiwara and Komachi, 2018). The systems use two variants of frequency features from the learner corpus (Lang-8 corpus) from Mizumoto et al. (2011) and from the general domain corpus (Wikipedia and WikiNews). The list of features used in building the model include the number of characters in the target word, number of words in the target phrase, and frequency of the target word in learner corpus (Lang-8 corpus) and general domain corpus (Wikipedia and WikiNews). Random forest classifiers are used for the binary classification task while random forest regressors are used for the probabilistic classification task using th"
W18-0507,S16-1164,0,0.0425763,"Missing"
W18-0507,S16-1149,1,0.538636,"elines is identifying which words are considered complex by a given target population (Shardlow, 2013). This task is known as complex word identification (CWI) and it has been attracting attention from the research community in the past few years. In this paper we present the findings of the second Complex Word Identification (CWI) shared task organized as part of the thirteenth Workshop on Innovative Use of NLP for Building Educational Applications (BEA) co-located with NAACL-HLT’2018. The second CWI shared task follows a successful first edition featuring 21 teams organized at SemEval’2016 (Paetzold and Specia, 2016a). While the first CWI shared task targeted an English dataset, the second edition focused on multilingualism providing datasets containing four languages: English, German, French, and Spanish. • English monolingual CWI; • German monolingual CWI; • Spanish monolingual CWI; and • Multilingual CWI with a French test set. For the first three tracks, participants were provided with training and testing data for the same language. For French, participants were provided only with a French test set and no French training data. In the CWI 2016, the task was cast as binary classification. To be able t"
W18-0507,S16-1162,0,0.138871,"Missing"
W18-0507,S16-1158,0,0.0719992,"Missing"
W18-0507,S16-1163,0,0.176167,"Missing"
W18-0507,D14-1162,0,0.0887949,"earning methods, 2) using the average embedding of target words as an input to a neural network, and 3) modeling the context of the target words using an LSTM. For the feature engineering-based systems, features such as linguistic, psycholinguistic, and language model features were used to train different binary and probabilistic classifiers. Lexical features include word length, number of syllables, and number of senses, hypernyms, and hyponyms in WordNet. For N-gram features, probabilities of the n-gram containing the target words were For embedding-based systems, a pre-trained GloVe model (Pennington et al., 2014) was used to get the vector representations of target words. For MWE, the average of the vectors is used. In the first approach, the resulting vector is passed on to a neural network with two ReLu layers followed by a sigmoid layer, which predicted the probability of the target word being complex. Their experiments show that the feature engineering approach achieved the best results using the XGBoost classifier for the binary classification task. They submitted four systems using XGBoost, average embeddings, LSTMs with transfer learning, and a voting system that combines the other three. For t"
W18-0507,S16-1154,1,0.847385,"Missing"
W18-0507,W18-0541,0,0.102296,"Missing"
W18-0507,S16-1153,1,0.879975,"Missing"
W18-0507,S16-1161,0,0.200613,"Missing"
W18-0507,S16-1147,0,0.032877,"Missing"
W18-0507,S16-1157,0,0.212408,"Missing"
W18-0507,I11-1017,0,0.0188214,"that the feature engineering approach achieved the best results using the XGBoost classifier for the binary classification task. They submitted four systems using XGBoost, average embeddings, LSTMs with transfer learning, and a voting system that combines the other three. For the probabilistic classification task, their LSTMs achieve the best results. TMU submitted multilingual and cross-lingual CWI systems for both of the binary and probabilistic classification tasks (Kajiwara and Komachi, 2018). The systems use two variants of frequency features from the learner corpus (Lang-8 corpus) from Mizumoto et al. (2011) and from the general domain corpus (Wikipedia and WikiNews). The list of features used in building the model include the number of characters in the target word, number of words in the target phrase, and frequency of the target word in learner corpus (Lang-8 corpus) and general domain corpus (Wikipedia and WikiNews). Random forest classifiers are used for the binary classification task while random forest regressors are used for the probabilistic classification task using the scikit-learn library. Feature ablation shows that both the length, frequency, and probability features (based on corpu"
W18-0507,P13-3015,0,0.247331,"ative speakers. To train their systems, participants received a labeled training set where words in context were annotated regarding their complexity. One month later, an unlabeled test set was provided and participating teams were required to upload their predictions for evaluation. More information about the data collection is presented in Section 3. Given the multilingual dataset provided, the CWI challenge was divided into four tracks: Introduction The most common first step in lexical simplification pipelines is identifying which words are considered complex by a given target population (Shardlow, 2013). This task is known as complex word identification (CWI) and it has been attracting attention from the research community in the past few years. In this paper we present the findings of the second Complex Word Identification (CWI) shared task organized as part of the thirteenth Workshop on Innovative Use of NLP for Building Educational Applications (BEA) co-located with NAACL-HLT’2018. The second CWI shared task follows a successful first edition featuring 21 teams organized at SemEval’2016 (Paetzold and Specia, 2016a). While the first CWI shared task targeted an English dataset, the second e"
W18-0507,S16-1152,0,0.262443,"Missing"
W18-0507,S16-1159,0,0.0576998,"Missing"
W18-0507,L16-1035,1,0.906663,"Missing"
W18-0507,W18-0522,0,0.230666,"n dependency relation, frequency features based on the BNC, Wikipedia, and Dale and Chall list corpora, number of synsets and senses in WordNet, and so on. The experiment is conducted using the Weka machine learning framework using the Support vector machine (with linear and radial basis function kernels), Na¨ıve Bayes, Logistic Regression, Random Tree, and Random Forest classification algorithms. The final experiments employ Support Vector Machines and Random Forest classifiers. CFILT IITB Developed ensemble-based classification systems for the English monolingual binary classification task (Wani et al., 2018). Lexical features based on WordNet for the target word are extracted as follows: 1) Degree of Polysemy: number of senses of the target word in WordNet, 2) Hyponym and Hypernym Tree Depth: the position of the word in WordNet’s hierarchical tree, and 3) Holonym and Meronym Counts: based on the relationship of the target word to its components (meronyms) or to the things it is contained in (Holonym’s). Additional feature classes include size-based features such as word count, word length, vowel counts, and syllable counts. They also use vocabulary-based features such as Ogden Basic (from Ogden’s"
W18-0507,S16-1146,0,0.112937,"Missing"
W18-0507,I17-2068,1,0.54282,"least 10 native English speakers and 10 non-native English speakers so that it is possible to investigate if native and non-native speakers have different CWI needs. 5) Complex words are not pre-highlighted, as in previous contributions, so that annotators are not biased to the pre-selection of the complex phrases. 6) In addition to single words, we allowed the annotation of multi-word expressions (MWE), up to a size of 50 characters. Table 2 shows the total, native, and non-native number of annotators that participated in the annotation task. Datasets We have used the CWIG3G2 datasets from (Yimam et al., 2017b,a) for the complex word identification (CWI) shared task 2018. The datasets are collected for multiple languages (English, German, Spanish). The English datasets cover different text genres, namely News (professionally written news), WikiNews (news written by amateurs), and Wikipedia articles. Below, we will briefly describe the annotation process and the statistics of collected datasets. For detail explanation of the datasets, please refer to the works of Yimam et al. (2017b,a) Furthermore, to bolster the cross-lingual CWI experiment, we have collected a CWI dataset for French. The French d"
W18-0507,yimam-etal-2017-multilingual,1,0.591517,"least 10 native English speakers and 10 non-native English speakers so that it is possible to investigate if native and non-native speakers have different CWI needs. 5) Complex words are not pre-highlighted, as in previous contributions, so that annotators are not biased to the pre-selection of the complex phrases. 6) In addition to single words, we allowed the annotation of multi-word expressions (MWE), up to a size of 50 characters. Table 2 shows the total, native, and non-native number of annotators that participated in the annotation task. Datasets We have used the CWIG3G2 datasets from (Yimam et al., 2017b,a) for the complex word identification (CWI) shared task 2018. The datasets are collected for multiple languages (English, German, Spanish). The English datasets cover different text genres, namely News (professionally written news), WikiNews (news written by amateurs), and Wikipedia articles. Below, we will briefly describe the annotation process and the statistics of collected datasets. For detail explanation of the datasets, please refer to the works of Yimam et al. (2017b,a) Furthermore, to bolster the cross-lingual CWI experiment, we have collected a CWI dataset for French. The French d"
W18-0507,W17-5910,1,0.857961,"Missing"
W18-0507,S16-1155,1,0.729801,"Missing"
W18-0507,S16-1150,0,\N,Missing
W18-0534,W17-5007,1,0.740618,"as presented in Table 3. COPLE2 LEIRIA PEAPL2 TOTAL Texts 1,058 Tokens 201,921 Types 9,373 TTR 0.05 330 57,358 4,504 0.08 480 1,868 121,138 380,417 6,808 20,685 0.06 0.05 Table 1: Distribution of the dataset: Number of texts, tokens, types, and type/token ratio (TTER) per source corpus. Related Work NLI has attracted a lot of attention in recent years. Due to the availability of suitable data, as discussed earlier, this attention has been particularly focused on English. The most notable examples are the two editions of the NLI shared task organized in 2013 (Tetreault et al., 2013) and 2017 (Malmasi et al., 2017). Even though most NLI research has been carried out on English data, an important research trend in recent years has been the application of NLI methods to other languages, as discussed in Malmasi and Dras (2015). Recent NLI studies on languages other than English include Arabic (Malmasi and Dras, 2014a) and Chinese (Malmasi and Dras, 2014b; Wang et al., 2015). To the best of our knowledge, no study has been published on Portuguese and the NLI-PT dataset opens new possibilities of research for Portuguese. In Section 4.1 we present the first simple baseline results for this task. Finally, as N"
W18-0534,boyd-etal-2014-merlin,0,0.0457443,"Missing"
W18-0534,padro-stanilovsky-2012-freeling,0,0.245629,"Missing"
W18-0534,E14-3013,0,0.0214445,"d: • Computer-aided Language Learning (CALL): CALL software has been developed for Portuguese (Marujo et al., 2009). Further improvements in these tools can take advantage of the training material available in NLI-PT for a number of purposes such as L1-tailored exercise design. • Grammatical error detection and correction: as discussed in Zampieri and Tan (2014), a known challenge in this task is acquiring suitable training data to account for the variation of errors present in non-native texts. One of the strategies developed to cope with this problem is to generate artificial training data (Felice and Yuan, 2014). Augmenting training data using a suitable annotated dataset such as NLI-PT can improve the quality of existing grammatical error correction systems for Portuguese. 5 Conclusion and Future Work This paper presented NLI-PT, the first Portuguese dataset compiled for NLI. NLI-PT contains 1,868 texts written by speakers of 15 L1s amounting to over 380,000 tokens. As discussed in Section 4, NLI-PT opens several avenues for future research. It can be used for different research purposes beyond NLI such as grammatical error correction and CALL. An experiment with the texts written by the speakers of"
W18-0534,W16-6502,0,0.0608381,"Missing"
W18-0534,W14-3625,1,0.839027,"LI has attracted a lot of attention in recent years. Due to the availability of suitable data, as discussed earlier, this attention has been particularly focused on English. The most notable examples are the two editions of the NLI shared task organized in 2013 (Tetreault et al., 2013) and 2017 (Malmasi et al., 2017). Even though most NLI research has been carried out on English data, an important research trend in recent years has been the application of NLI methods to other languages, as discussed in Malmasi and Dras (2015). Recent NLI studies on languages other than English include Arabic (Malmasi and Dras, 2014a) and Chinese (Malmasi and Dras, 2014b; Wang et al., 2015). To the best of our knowledge, no study has been published on Portuguese and the NLI-PT dataset opens new possibilities of research for Portuguese. In Section 4.1 we present the first simple baseline results for this task. Finally, as NLI-PT can be used in other applications besides NLI, it is important to point out that a number of studies have been published on educational NLP applications for Portuguese and on the 6 NLI-PT is available http://www.clul.ulisboa.pt/en/resources-en/11resources/894-nli-pt-a-portuguese-native-languageide"
W18-0534,W13-1706,0,0.0208356,") Leiria corpus, and (iii) PEAPL27 as presented in Table 3. COPLE2 LEIRIA PEAPL2 TOTAL Texts 1,058 Tokens 201,921 Types 9,373 TTR 0.05 330 57,358 4,504 0.08 480 1,868 121,138 380,417 6,808 20,685 0.06 0.05 Table 1: Distribution of the dataset: Number of texts, tokens, types, and type/token ratio (TTER) per source corpus. Related Work NLI has attracted a lot of attention in recent years. Due to the availability of suitable data, as discussed earlier, this attention has been particularly focused on English. The most notable examples are the two editions of the NLI shared task organized in 2013 (Tetreault et al., 2013) and 2017 (Malmasi et al., 2017). Even though most NLI research has been carried out on English data, an important research trend in recent years has been the application of NLI methods to other languages, as discussed in Malmasi and Dras (2015). Recent NLI studies on languages other than English include Arabic (Malmasi and Dras, 2014a) and Chinese (Malmasi and Dras, 2014b; Wang et al., 2015). To the best of our knowledge, no study has been published on Portuguese and the NLI-PT dataset opens new possibilities of research for Portuguese. In Section 4.1 we present the first simple baseline resu"
W18-0534,E14-4019,1,0.846479,"LI has attracted a lot of attention in recent years. Due to the availability of suitable data, as discussed earlier, this attention has been particularly focused on English. The most notable examples are the two editions of the NLI shared task organized in 2013 (Tetreault et al., 2013) and 2017 (Malmasi et al., 2017). Even though most NLI research has been carried out on English data, an important research trend in recent years has been the application of NLI methods to other languages, as discussed in Malmasi and Dras (2015). Recent NLI studies on languages other than English include Arabic (Malmasi and Dras, 2014a) and Chinese (Malmasi and Dras, 2014b; Wang et al., 2015). To the best of our knowledge, no study has been published on Portuguese and the NLI-PT dataset opens new possibilities of research for Portuguese. In Section 4.1 we present the first simple baseline results for this task. Finally, as NLI-PT can be used in other applications besides NLI, it is important to point out that a number of studies have been published on educational NLP applications for Portuguese and on the 6 NLI-PT is available http://www.clul.ulisboa.pt/en/resources-en/11resources/894-nli-pt-a-portuguese-native-languageide"
W18-0534,W15-0614,1,0.832559,"availability of suitable data, as discussed earlier, this attention has been particularly focused on English. The most notable examples are the two editions of the NLI shared task organized in 2013 (Tetreault et al., 2013) and 2017 (Malmasi et al., 2017). Even though most NLI research has been carried out on English data, an important research trend in recent years has been the application of NLI methods to other languages, as discussed in Malmasi and Dras (2015). Recent NLI studies on languages other than English include Arabic (Malmasi and Dras, 2014a) and Chinese (Malmasi and Dras, 2014b; Wang et al., 2015). To the best of our knowledge, no study has been published on Portuguese and the NLI-PT dataset opens new possibilities of research for Portuguese. In Section 4.1 we present the first simple baseline results for this task. Finally, as NLI-PT can be used in other applications besides NLI, it is important to point out that a number of studies have been published on educational NLP applications for Portuguese and on the 6 NLI-PT is available http://www.clul.ulisboa.pt/en/resources-en/11resources/894-nli-pt-a-portuguese-native-languageidentification-dataset Corpus Description The three corpora co"
W18-3901,W18-3913,0,0.217545,"per. Team ADI Arabic Identification benf BZU CLiPS CEA List DeepLIMA CoAStaL DFSlangid dkosmajac GDI classification ILIdentification JANES JSI LaMa LTL-UDE mmb lct safina STEVENDU2018 SUKI X SYSTRAN Taurus T¨ubingen-Oslo Twist Bytes Meta UH&CU UnibucKernel we are indian XAC Total X DFS GDI ILI MTT System Description Papers X X X X X X X X X X X X X X X (Naser and Hanani, 2018) (Kreutz and Daelemans, 2018) (Meftah and Semmar, 2018) (Ciobanu et al., 2018a) (Ciobanu et al., 2018b) (Ljubeˇsi´c, 2018) (Ljubeˇsi´c, 2018) X X X X X X X X X X X X X X X X X X X 6 X 12 X 8 X X 8 6 (Kroon et al., 2018) (Ali, 2018a; Ali, 2018b; Ali, 2018c) (Du and Wang, 2018) (Jauhiainen et al., 2018a; Jauhiainen et al., 2018b; Jauhiainen et al., 2018c) (Michon et al., 2018) (van Halteren and Oostdijk, 2018) (C ¸ o¨ ltekin et al., 2018) (Benites et al., 2018) (Silfverberg and Drobac, 2018) (Butnaru and Ionescu, 2018) (Gupta et al., 2018) (Barbaresi, 2018) 22 Table 1: The teams that participated in the VarDial’2018 evaluation campaign. 4 Previous Shared Tasks Since the first DSL challenge, the shared tasks organized within the scope of the VarDial workshop have enjoyed substantial increase in the number of participants"
W18-3901,W18-3919,0,0.254393,"per. Team ADI Arabic Identification benf BZU CLiPS CEA List DeepLIMA CoAStaL DFSlangid dkosmajac GDI classification ILIdentification JANES JSI LaMa LTL-UDE mmb lct safina STEVENDU2018 SUKI X SYSTRAN Taurus T¨ubingen-Oslo Twist Bytes Meta UH&CU UnibucKernel we are indian XAC Total X DFS GDI ILI MTT System Description Papers X X X X X X X X X X X X X X X (Naser and Hanani, 2018) (Kreutz and Daelemans, 2018) (Meftah and Semmar, 2018) (Ciobanu et al., 2018a) (Ciobanu et al., 2018b) (Ljubeˇsi´c, 2018) (Ljubeˇsi´c, 2018) X X X X X X X X X X X X X X X X X X X 6 X 12 X 8 X X 8 6 (Kroon et al., 2018) (Ali, 2018a; Ali, 2018b; Ali, 2018c) (Du and Wang, 2018) (Jauhiainen et al., 2018a; Jauhiainen et al., 2018b; Jauhiainen et al., 2018c) (Michon et al., 2018) (van Halteren and Oostdijk, 2018) (C ¸ o¨ ltekin et al., 2018) (Benites et al., 2018) (Silfverberg and Drobac, 2018) (Butnaru and Ionescu, 2018) (Gupta et al., 2018) (Barbaresi, 2018) 22 Table 1: The teams that participated in the VarDial’2018 evaluation campaign. 4 Previous Shared Tasks Since the first DSL challenge, the shared tasks organized within the scope of the VarDial workshop have enjoyed substantial increase in the number of participants"
W18-3901,W18-3932,0,0.129693,"per. Team ADI Arabic Identification benf BZU CLiPS CEA List DeepLIMA CoAStaL DFSlangid dkosmajac GDI classification ILIdentification JANES JSI LaMa LTL-UDE mmb lct safina STEVENDU2018 SUKI X SYSTRAN Taurus T¨ubingen-Oslo Twist Bytes Meta UH&CU UnibucKernel we are indian XAC Total X DFS GDI ILI MTT System Description Papers X X X X X X X X X X X X X X X (Naser and Hanani, 2018) (Kreutz and Daelemans, 2018) (Meftah and Semmar, 2018) (Ciobanu et al., 2018a) (Ciobanu et al., 2018b) (Ljubeˇsi´c, 2018) (Ljubeˇsi´c, 2018) X X X X X X X X X X X X X X X X X X X 6 X 12 X 8 X X 8 6 (Kroon et al., 2018) (Ali, 2018a; Ali, 2018b; Ali, 2018c) (Du and Wang, 2018) (Jauhiainen et al., 2018a; Jauhiainen et al., 2018b; Jauhiainen et al., 2018c) (Michon et al., 2018) (van Halteren and Oostdijk, 2018) (C ¸ o¨ ltekin et al., 2018) (Benites et al., 2018) (Silfverberg and Drobac, 2018) (Butnaru and Ionescu, 2018) (Gupta et al., 2018) (Barbaresi, 2018) 22 Table 1: The teams that participated in the VarDial’2018 evaluation campaign. 4 Previous Shared Tasks Since the first DSL challenge, the shared tasks organized within the scope of the VarDial workshop have enjoyed substantial increase in the number of participants"
W18-3901,W17-1223,0,0.131933,"data to reduce the transcriber effects seen last year. 7 • The LaMa system is a blend (weighted vote) of eight classifiers being stochastic gradient descent (hinge and modified Huber), multinomial Na¨ıve Bayes, both counts and tf-idf, FastText, and modified Kneser-Ney smoothing. The classifiers were trained using word n-grams (1-6) and character n-grams (1-8). The hyperparameters were determined with cross-validation and searching on the development set. • XAC system is a refined version of the n-gram-based Bayesline system described in last year’s XAC submission to the VarDial shared tasks (Barbaresi, 2017), and previously used as a baseline for the DSL shared task (Tan et al., 2014). The XAC team achieved their best results using a Na¨ıve Bayes classifier. • The GDI classification system is based on an ensemble of multiple SVM classifiers. The system was trained on various word- and character-level features. • The dkosmajac system is based on a normalized Euclidean distance measure. The distances are calculated between a sample and each class profile. The class profiles are generated by selecting the most frequent features for each class, which results in profiles that are of the same length fo"
W18-3901,W18-3918,0,0.0590619,"Missing"
W18-3901,W18-3925,0,0.142653,"Missing"
W18-3901,W17-1214,0,0.138486,"was part of the first VarDial evaluation campaign (Zampieri et al., 2017). It provided manual transcriptions of recorded interviews from four dialect areas of the German-speaking Switzerland, namely Bern, Basel, Lucerne, and Zurich. The training and the test data was extracted from the ArchiMob corpus (Samardˇzi´c et al., 2016). The training data consisted in 3,000–4,000 utterances from 3–5 different speakers per dialect; the test data consisted of about 900 utterances by a single speaker per dialect. A total of ten teams participated in the 2017 GDI task and the two best-performing systems (Bestgen, 2017; Malmasi and Zampieri, 2017b) achieved weighted F1-measure of up to 0.66. Transcribers were shown to affect the performance of the systems, e.g., for the Lucerne dialect, whose test set was transcribed by a different person than the training set, recall figures were only around 0.3. 5 Third Arabic Dialect Identification (ADI) This year’s third edition of the ADI task addressed the multi-dialectal challenge in spoken Arabic in the broadcast domain. Last year, in the second edition of the ADI task (Zampieri et al., 2017), we offered the input represented as (i) automatic text transcriptions gen"
W18-3901,W18-3909,0,0.15091,"Total X DFS GDI ILI MTT System Description Papers X X X X X X X X X X X X X X X (Naser and Hanani, 2018) (Kreutz and Daelemans, 2018) (Meftah and Semmar, 2018) (Ciobanu et al., 2018a) (Ciobanu et al., 2018b) (Ljubeˇsi´c, 2018) (Ljubeˇsi´c, 2018) X X X X X X X X X X X X X X X X X X X 6 X 12 X 8 X X 8 6 (Kroon et al., 2018) (Ali, 2018a; Ali, 2018b; Ali, 2018c) (Du and Wang, 2018) (Jauhiainen et al., 2018a; Jauhiainen et al., 2018b; Jauhiainen et al., 2018c) (Michon et al., 2018) (van Halteren and Oostdijk, 2018) (C ¸ o¨ ltekin et al., 2018) (Benites et al., 2018) (Silfverberg and Drobac, 2018) (Butnaru and Ionescu, 2018) (Gupta et al., 2018) (Barbaresi, 2018) 22 Table 1: The teams that participated in the VarDial’2018 evaluation campaign. 4 Previous Shared Tasks Since the first DSL challenge, the shared tasks organized within the scope of the VarDial workshop have enjoyed substantial increase in the number of participants and in the overall interest from the NLP community. This motivated the organizers to turn the shared tasks at VarDial into a more comprehensive evaluation exercise with four shared tasks in 2017. This year, the VarDial workshop featured the second edition of the VarDial evaluation campaign w"
W18-3901,W18-3933,1,0.889149,"Missing"
W18-3901,W18-3920,1,0.880795,"Missing"
W18-3901,W18-3926,0,0.0553879,"Missing"
W18-3901,W18-3921,0,0.054375,"Missing"
W18-3901,W17-1225,0,0.134424,"r of hours. 5.2 Participants and Approaches In this section, we present a short description of the systems that competed in the ADI shared task: • UnibucKernel system (Butnaru and Ionescu, 2018) combines three kernel matrices: one calculated using just the lexical features, another one computed on embeddings, and a combined kernel computed on the phonetic features. The final matrix is the mean of these three matrices. As a classifier, they used Kernel Ridge Regression. The approach is similar to the systems that ranked second and first in the previous two ADI tasks (Ionescu and Popescu, 2016; Ionescu and Butnaru, 2017). • Safina system (Ali, 2018a) accepts a sequence of 256 characters as input in addition to the acoustic embedding vectors. First, the sequence of characters is one-hot encoded, then it is passed to a GRU layer, which is followed by a convolutional layer with different filter sizes ranging from 2 to 7. The convolutional layer is followed by batch normalizations, max-pooling, and dropout layers, and finally a softmax layer. In contrast, the acoustic embedding vectors go directly to another softmax layer. The final output is the average between these two softmax layers, which represents the prob"
W18-3901,W16-4818,0,0.048944,", duration (Dur.), in number of hours. 5.2 Participants and Approaches In this section, we present a short description of the systems that competed in the ADI shared task: • UnibucKernel system (Butnaru and Ionescu, 2018) combines three kernel matrices: one calculated using just the lexical features, another one computed on embeddings, and a combined kernel computed on the phonetic features. The final matrix is the mean of these three matrices. As a classifier, they used Kernel Ridge Regression. The approach is similar to the systems that ranked second and first in the previous two ADI tasks (Ionescu and Popescu, 2016; Ionescu and Butnaru, 2017). • Safina system (Ali, 2018a) accepts a sequence of 256 characters as input in addition to the acoustic embedding vectors. First, the sequence of characters is one-hot encoded, then it is passed to a GRU layer, which is followed by a convolutional layer with different filter sizes ranging from 2 to 7. The convolutional layer is followed by batch normalizations, max-pooling, and dropout layers, and finally a softmax layer. In contrast, the acoustic embedding vectors go directly to another softmax layer. The final output is the average between these two softmax layer"
W18-3901,W18-3915,0,0.0733204,"Missing"
W18-3901,W18-3929,0,0.411409,"Missing"
W18-3901,W18-3907,0,0.178349,"Missing"
W18-3901,W18-3922,0,0.0589219,"Missing"
W18-3901,W18-3928,0,0.0556119,"Missing"
W18-3901,kumar-2012-challenges,1,0.824307,"ed printed stories, novels and essays in books, magazines, and newspapers. We scanned the printed materials, then we performed OCR, and finally we asked native speakers of the respective languages to correct the OCR output. Since there are no specific OCR models available for these languages, we used the Google OCR for Hindi, part of the Drive API. Since all the languages used the Devanagari script, we expected the OCR to work reasonably well, and overall it did. We further managed to get some blogs in Magahi and Bhojpuri. There are several corpora already available for Modern Standard Hindi (Kumar, 2012; Kumar, 2014a; Kumar, 2014b; Choudhary and Jha, 2011). However, in order to keep the domain the same as for the other languages, we collected data from blogs that mainly contain stories and novels. Thus, the Modern Standard Hindi data collected for this study is also from the literature domain.5 9.2 Participants and Approaches • The SUKI team used HeLI with adaptive language models based on character n-grams from 1 to 6, as previously described in Section 5. The also used an iterative version of the language model adaptation technique, with three additional adaptation epochs. ¨ • Tubingen-Osl"
W18-3901,kumar-2014-developing,1,0.832345,"ories, novels and essays in books, magazines, and newspapers. We scanned the printed materials, then we performed OCR, and finally we asked native speakers of the respective languages to correct the OCR output. Since there are no specific OCR models available for these languages, we used the Google OCR for Hindi, part of the Drive API. Since all the languages used the Devanagari script, we expected the OCR to work reasonably well, and overall it did. We further managed to get some blogs in Magahi and Bhojpuri. There are several corpora already available for Modern Standard Hindi (Kumar, 2012; Kumar, 2014a; Kumar, 2014b; Choudhary and Jha, 2011). However, in order to keep the domain the same as for the other languages, we collected data from blogs that mainly contain stories and novels. Thus, the Modern Standard Hindi data collected for this study is also from the literature domain.5 9.2 Participants and Approaches • The SUKI team used HeLI with adaptive language models based on character n-grams from 1 to 6, as previously described in Section 5. The also used an iterative version of the language model adaptation technique, with three additional adaptation epochs. ¨ • Tubingen-Oslo team submit"
W18-3901,W14-0405,1,0.912975,"Missing"
W18-3901,W18-3917,1,0.897664,"Missing"
W18-3901,L16-1242,1,0.902855,"Missing"
W18-3901,L16-1676,1,0.914666,"Missing"
W18-3901,W16-4814,1,0.86288,"-Oslo system (C ¸ o¨ ltekin et al., 2018) is trained on word and character n-grams using a single SVM classifier, which is fine-tuned using cross-validation. It is similar to the submissions by the same authors to previous VarDial shared tasks (C¸o¨ ltekin and Rama, 2017; C¸o¨ ltekin and Rama, 2016). They also tried an approach based on RNN, which worked worse. • Arabic Identification system is based on an ensemble of SVM classifiers trained on character and word n-grams. The approach is similar to the systems ranked second and first in the previous two ADI tasks (Malmasi and Zampieri, 2017a; Malmasi and Zampieri, 2016). 5 5.3 Results Six teams submitted runs for the ADI shared task and the results are shown in Table 3. The best result, an F1 score of 0.589, was achieved by UnibucKernel,1 followed by safina, with an F1 score of 0.575. The following three teams are tied for the third place as they are not statistically different. Rank 1 2 3 3 3 4 Team F1 (Macro) UnibucKernel safina BZU SYSTRAN T¨ubingen-Oslo Arabic Identification 0.589 0.576 0.534 0.529 0.514 0.500 Table 3: ADI results: ranked taking statistical significance into account. 5.4 Summary We introduced multi-phoneme representation for the dialecta"
W18-3901,W17-1222,1,0.833986,"e first VarDial evaluation campaign (Zampieri et al., 2017). It provided manual transcriptions of recorded interviews from four dialect areas of the German-speaking Switzerland, namely Bern, Basel, Lucerne, and Zurich. The training and the test data was extracted from the ArchiMob corpus (Samardˇzi´c et al., 2016). The training data consisted in 3,000–4,000 utterances from 3–5 different speakers per dialect; the test data consisted of about 900 utterances by a single speaker per dialect. A total of ten teams participated in the 2017 GDI task and the two best-performing systems (Bestgen, 2017; Malmasi and Zampieri, 2017b) achieved weighted F1-measure of up to 0.66. Transcribers were shown to affect the performance of the systems, e.g., for the Lucerne dialect, whose test set was transcribed by a different person than the training set, recall figures were only around 0.3. 5 Third Arabic Dialect Identification (ADI) This year’s third edition of the ADI task addressed the multi-dialectal challenge in spoken Arabic in the broadcast domain. Last year, in the second edition of the ADI task (Zampieri et al., 2017), we offered the input represented as (i) automatic text transcriptions generated using large-vocabular"
W18-3901,W17-1220,1,0.80558,"e first VarDial evaluation campaign (Zampieri et al., 2017). It provided manual transcriptions of recorded interviews from four dialect areas of the German-speaking Switzerland, namely Bern, Basel, Lucerne, and Zurich. The training and the test data was extracted from the ArchiMob corpus (Samardˇzi´c et al., 2016). The training data consisted in 3,000–4,000 utterances from 3–5 different speakers per dialect; the test data consisted of about 900 utterances by a single speaker per dialect. A total of ten teams participated in the 2017 GDI task and the two best-performing systems (Bestgen, 2017; Malmasi and Zampieri, 2017b) achieved weighted F1-measure of up to 0.66. Transcribers were shown to affect the performance of the systems, e.g., for the Lucerne dialect, whose test set was transcribed by a different person than the training set, recall figures were only around 0.3. 5 Third Arabic Dialect Identification (ADI) This year’s third edition of the ADI task addressed the multi-dialectal challenge in spoken Arabic in the broadcast domain. Last year, in the second edition of the ADI task (Zampieri et al., 2017), we offered the input represented as (i) automatic text transcriptions generated using large-vocabular"
W18-3901,W16-4801,1,0.733081,"Missing"
W18-3901,W18-3927,0,0.0554913,"Missing"
W18-3901,W18-3914,0,0.175754,"ES JSI LaMa LTL-UDE mmb lct safina STEVENDU2018 SUKI X SYSTRAN Taurus T¨ubingen-Oslo Twist Bytes Meta UH&CU UnibucKernel we are indian XAC Total X DFS GDI ILI MTT System Description Papers X X X X X X X X X X X X X X X (Naser and Hanani, 2018) (Kreutz and Daelemans, 2018) (Meftah and Semmar, 2018) (Ciobanu et al., 2018a) (Ciobanu et al., 2018b) (Ljubeˇsi´c, 2018) (Ljubeˇsi´c, 2018) X X X X X X X X X X X X X X X X X X X 6 X 12 X 8 X X 8 6 (Kroon et al., 2018) (Ali, 2018a; Ali, 2018b; Ali, 2018c) (Du and Wang, 2018) (Jauhiainen et al., 2018a; Jauhiainen et al., 2018b; Jauhiainen et al., 2018c) (Michon et al., 2018) (van Halteren and Oostdijk, 2018) (C ¸ o¨ ltekin et al., 2018) (Benites et al., 2018) (Silfverberg and Drobac, 2018) (Butnaru and Ionescu, 2018) (Gupta et al., 2018) (Barbaresi, 2018) 22 Table 1: The teams that participated in the VarDial’2018 evaluation campaign. 4 Previous Shared Tasks Since the first DSL challenge, the shared tasks organized within the scope of the VarDial workshop have enjoyed substantial increase in the number of participants and in the overall interest from the NLP community. This motivated the organizers to turn the shared tasks at VarDial into a more comprehensive eva"
W18-3901,W18-3924,0,0.0833746,"and the number of submissions varied widely across the tasks, ranging from 6 entries for ADI and MTT to 12 entries for DFS. Table 1 lists the participating teams, the shared tasks they took part in, and a reference to the system description paper. Team ADI Arabic Identification benf BZU CLiPS CEA List DeepLIMA CoAStaL DFSlangid dkosmajac GDI classification ILIdentification JANES JSI LaMa LTL-UDE mmb lct safina STEVENDU2018 SUKI X SYSTRAN Taurus T¨ubingen-Oslo Twist Bytes Meta UH&CU UnibucKernel we are indian XAC Total X DFS GDI ILI MTT System Description Papers X X X X X X X X X X X X X X X (Naser and Hanani, 2018) (Kreutz and Daelemans, 2018) (Meftah and Semmar, 2018) (Ciobanu et al., 2018a) (Ciobanu et al., 2018b) (Ljubeˇsi´c, 2018) (Ljubeˇsi´c, 2018) X X X X X X X X X X X X X X X X X X X 6 X 12 X 8 X X 8 6 (Kroon et al., 2018) (Ali, 2018a; Ali, 2018b; Ali, 2018c) (Du and Wang, 2018) (Jauhiainen et al., 2018a; Jauhiainen et al., 2018b; Jauhiainen et al., 2018c) (Michon et al., 2018) (van Halteren and Oostdijk, 2018) (C ¸ o¨ ltekin et al., 2018) (Benites et al., 2018) (Silfverberg and Drobac, 2018) (Butnaru and Ionescu, 2018) (Gupta et al., 2018) (Barbaresi, 2018) 22 Table 1: The teams that participate"
W18-3901,L16-1641,1,0.509642,"Missing"
W18-3901,W17-1224,1,0.744067,"Missing"
W18-3901,W18-3923,1,0.879634,"Missing"
W18-3901,W14-5307,1,0.857122,"Missing"
W18-3901,W15-5401,1,0.855743,"Missing"
W18-3901,W17-1201,1,0.607978,"Missing"
W18-3920,W16-4819,0,0.0223457,"om 2014 (Zampieri et al., 2014) to 2017 (Zampieri et al., 2017b) within the scope of the VarDial workshop series. Four versions of the DSLCC dataset (Tan et al., 2014) have been released containing short excerpts of journalistic texts written in similar languages and language varieties. In the four editions of the DSL shared task a variety of computation methods have been tested. This includes Maximum Entropy (Porta and Sancho, 2014), Prediction by Partial Matching (PPM) (Bobicev, 2015), language model perplexity (Gamallo et al., 2017), SVMs (Purver, 2014), Convolution Neural Networks (CNNs) (Belinkov and Glass, 2016), word-based back-off models (Jauhiainen et al., 2015; Jauhiainen et al., 2016), and classifier ensembles (Malmasi and Dras, 2015), the approach we apply in this paper. Classifier ensembles showed very good performance not only in language identification but also in similar tasks. Therefore, we build on the experience of our previous work and improve the system that we have previously applied to similar tasks, namely author profiling (Ciobanu et al., 2017) and native language identification (Zampieri et al., 2017a). A detailed description of our system is presented in Section 4. 3 Data The dat"
W18-3920,W15-5410,0,0.0278754,"gs are the aforementioned Discriminating between Similar Languages (DSL) shared tasks. The DSL tasks have been organized from 2014 (Zampieri et al., 2014) to 2017 (Zampieri et al., 2017b) within the scope of the VarDial workshop series. Four versions of the DSLCC dataset (Tan et al., 2014) have been released containing short excerpts of journalistic texts written in similar languages and language varieties. In the four editions of the DSL shared task a variety of computation methods have been tested. This includes Maximum Entropy (Porta and Sancho, 2014), Prediction by Partial Matching (PPM) (Bobicev, 2015), language model perplexity (Gamallo et al., 2017), SVMs (Purver, 2014), Convolution Neural Networks (CNNs) (Belinkov and Glass, 2016), word-based back-off models (Jauhiainen et al., 2015; Jauhiainen et al., 2016), and classifier ensembles (Malmasi and Dras, 2015), the approach we apply in this paper. Classifier ensembles showed very good performance not only in language identification but also in similar tasks. Therefore, we build on the experience of our previous work and improve the system that we have previously applied to similar tasks, namely author profiling (Ciobanu et al., 2017) and n"
W18-3920,D14-1069,0,0.0726343,"Missing"
W18-3920,W17-1213,0,0.0135081,"tween Similar Languages (DSL) shared tasks. The DSL tasks have been organized from 2014 (Zampieri et al., 2014) to 2017 (Zampieri et al., 2017b) within the scope of the VarDial workshop series. Four versions of the DSLCC dataset (Tan et al., 2014) have been released containing short excerpts of journalistic texts written in similar languages and language varieties. In the four editions of the DSL shared task a variety of computation methods have been tested. This includes Maximum Entropy (Porta and Sancho, 2014), Prediction by Partial Matching (PPM) (Bobicev, 2015), language model perplexity (Gamallo et al., 2017), SVMs (Purver, 2014), Convolution Neural Networks (CNNs) (Belinkov and Glass, 2016), word-based back-off models (Jauhiainen et al., 2015; Jauhiainen et al., 2016), and classifier ensembles (Malmasi and Dras, 2015), the approach we apply in this paper. Classifier ensembles showed very good performance not only in language identification but also in similar tasks. Therefore, we build on the experience of our previous work and improve the system that we have previously applied to similar tasks, namely author profiling (Ciobanu et al., 2017) and native language identification (Zampieri et al., 20"
W18-3920,L16-1284,1,0.827322,"ombination. With the aid of Hindi speakers, in Section 5.1 we presented a concise error analysis of the misclassified instances of the development set. We observed a few interesting patterns in the misclassified instances, most notably that many of the misclassified sentences were too short, containing only one, two or three words, and that several of them contained only named entities. making it very challenging for classifiers to identify the language of these instances. Another issue discussed in Section 5.1, is that some instances could not be discriminated by native speakers, as noted by Goutte et al. (2016). To cope with these instances one possible direction for future work is to allow a multi-label classification setup in which sentences could be assign to more than one category if annotators labeled them as such. In future work we would like to explore and compare our methods to other high performance methods for this task. In particular, we would like to try an implementation of the token-based back-off method proposed by the SUKI team. As evidenced in Section 5, SUKI’s system achieved substantially higher performance than the other methods in this competition. Acknowledgements We would like"
W18-3920,W15-5408,0,0.088526,"., 2017b) within the scope of the VarDial workshop series. Four versions of the DSLCC dataset (Tan et al., 2014) have been released containing short excerpts of journalistic texts written in similar languages and language varieties. In the four editions of the DSL shared task a variety of computation methods have been tested. This includes Maximum Entropy (Porta and Sancho, 2014), Prediction by Partial Matching (PPM) (Bobicev, 2015), language model perplexity (Gamallo et al., 2017), SVMs (Purver, 2014), Convolution Neural Networks (CNNs) (Belinkov and Glass, 2016), word-based back-off models (Jauhiainen et al., 2015; Jauhiainen et al., 2016), and classifier ensembles (Malmasi and Dras, 2015), the approach we apply in this paper. Classifier ensembles showed very good performance not only in language identification but also in similar tasks. Therefore, we build on the experience of our previous work and improve the system that we have previously applied to similar tasks, namely author profiling (Ciobanu et al., 2017) and native language identification (Zampieri et al., 2017a). A detailed description of our system is presented in Section 4. 3 Data The dataset made available by the organizers of the Indo-Ary"
W18-3920,W16-4820,0,0.0514295,"e of the VarDial workshop series. Four versions of the DSLCC dataset (Tan et al., 2014) have been released containing short excerpts of journalistic texts written in similar languages and language varieties. In the four editions of the DSL shared task a variety of computation methods have been tested. This includes Maximum Entropy (Porta and Sancho, 2014), Prediction by Partial Matching (PPM) (Bobicev, 2015), language model perplexity (Gamallo et al., 2017), SVMs (Purver, 2014), Convolution Neural Networks (CNNs) (Belinkov and Glass, 2016), word-based back-off models (Jauhiainen et al., 2015; Jauhiainen et al., 2016), and classifier ensembles (Malmasi and Dras, 2015), the approach we apply in this paper. Classifier ensembles showed very good performance not only in language identification but also in similar tasks. Therefore, we build on the experience of our previous work and improve the system that we have previously applied to similar tasks, namely author profiling (Ciobanu et al., 2017) and native language identification (Zampieri et al., 2017a). A detailed description of our system is presented in Section 4. 3 Data The dataset made available by the organizers of the Indo-Aryan Language Identification"
W18-3920,W15-5407,1,0.925336,"he DSLCC dataset (Tan et al., 2014) have been released containing short excerpts of journalistic texts written in similar languages and language varieties. In the four editions of the DSL shared task a variety of computation methods have been tested. This includes Maximum Entropy (Porta and Sancho, 2014), Prediction by Partial Matching (PPM) (Bobicev, 2015), language model perplexity (Gamallo et al., 2017), SVMs (Purver, 2014), Convolution Neural Networks (CNNs) (Belinkov and Glass, 2016), word-based back-off models (Jauhiainen et al., 2015; Jauhiainen et al., 2016), and classifier ensembles (Malmasi and Dras, 2015), the approach we apply in this paper. Classifier ensembles showed very good performance not only in language identification but also in similar tasks. Therefore, we build on the experience of our previous work and improve the system that we have previously applied to similar tasks, namely author profiling (Ciobanu et al., 2017) and native language identification (Zampieri et al., 2017a). A detailed description of our system is presented in Section 4. 3 Data The dataset made available by the organizers of the Indo-Aryan Language Identification (ILI) task comprises five similar languages spoken"
W18-3920,W16-4801,1,0.936687,", 2018) and in previous work (Tiedemann and Ljubešić, 2012; Goutte et al., 2016), discriminating between similar languages is one of the main challenges in automatic language identification. State-of-the-art n-gram-based language identification systems are able to discriminate between unrelated languages with very high performance but very often struggle to discriminate between similar languages. This challenge motivated the organization of recent evaluation campaigns such as the TweetLID (Zubiaga et al., 2016) which included languages spoken in the Iberian peninsula and the DSL shared tasks (Malmasi et al., 2016b; Zampieri et al., 2015) which included groups of similar languages such as Malay and Indonesian, Bulgarian and Macedonian, and Bosnian, Croatian, and Serbian as well as groups of language varieties such as Brazilian and European Portuguese. In this paper we revisit the problem of discriminating between similar languages presenting a system to discriminate between five languages of the Indo-Aryan family: Hindi, Braj Bhasha, Awadhi, Bhojpuri, and Magahi. Inspired by systems that performed well in past editions of the DSL shared task such as the one by Malmasi and Dras (2015), we developed a sy"
W18-3920,W14-5314,0,0.0166142,"identification of very similar languages in multilingual settings are the aforementioned Discriminating between Similar Languages (DSL) shared tasks. The DSL tasks have been organized from 2014 (Zampieri et al., 2014) to 2017 (Zampieri et al., 2017b) within the scope of the VarDial workshop series. Four versions of the DSLCC dataset (Tan et al., 2014) have been released containing short excerpts of journalistic texts written in similar languages and language varieties. In the four editions of the DSL shared task a variety of computation methods have been tested. This includes Maximum Entropy (Porta and Sancho, 2014), Prediction by Partial Matching (PPM) (Bobicev, 2015), language model perplexity (Gamallo et al., 2017), SVMs (Purver, 2014), Convolution Neural Networks (CNNs) (Belinkov and Glass, 2016), word-based back-off models (Jauhiainen et al., 2015; Jauhiainen et al., 2016), and classifier ensembles (Malmasi and Dras, 2015), the approach we apply in this paper. Classifier ensembles showed very good performance not only in language identification but also in similar tasks. Therefore, we build on the experience of our previous work and improve the system that we have previously applied to similar tasks"
W18-3920,W14-5318,0,0.017647,"shared tasks. The DSL tasks have been organized from 2014 (Zampieri et al., 2014) to 2017 (Zampieri et al., 2017b) within the scope of the VarDial workshop series. Four versions of the DSLCC dataset (Tan et al., 2014) have been released containing short excerpts of journalistic texts written in similar languages and language varieties. In the four editions of the DSL shared task a variety of computation methods have been tested. This includes Maximum Entropy (Porta and Sancho, 2014), Prediction by Partial Matching (PPM) (Bobicev, 2015), language model perplexity (Gamallo et al., 2017), SVMs (Purver, 2014), Convolution Neural Networks (CNNs) (Belinkov and Glass, 2016), word-based back-off models (Jauhiainen et al., 2015; Jauhiainen et al., 2016), and classifier ensembles (Malmasi and Dras, 2015), the approach we apply in this paper. Classifier ensembles showed very good performance not only in language identification but also in similar tasks. Therefore, we build on the experience of our previous work and improve the system that we have previously applied to similar tasks, namely author profiling (Ciobanu et al., 2017) and native language identification (Zampieri et al., 2017a). A detailed desc"
W18-3920,C12-1160,0,0.0763191,"Missing"
W18-3920,W15-4415,0,0.0311268,"aterial or external resource. 4 Methodology Following our aforementioned previous work (Ciobanu et al., 2017), we built a classification system based on SVM ensembles using the same methodology proposed by Malmasi and Dras (2015). The purpose of using classification ensembles is to improve the overall performance and robustness by combining the results of multiple classifiers. Such systems have proved successful not only in NLI and dialect identification, but also in various text classification tasks, such as complex word identification (Malmasi et al., 2016a) and grammatical error diagnosis (Xiang et al., 2015). The classifiers can differ in a wide range of aspects; for example, algorithms, training data, features or parameters. We implemented our system using the Scikit-learn (Pedregosa et al., 2011) machine learning library, with each classifier in the ensemble using a different type of features. For the individual classifiers, we employed the SVM implementation based on the Liblinear library (Fan et al., 2008), LinearSVC1 , with a linear kernel. This implementation has the advantage of scaling well to large number of samples. For the ensemble, we employed the majority rule VotingClassifier2 , whi"
W18-3920,W14-5307,1,0.895397,", Croatian, Montenegrin, and Serbian (Ljubesic and Kranjcic, 2015). This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http: //creativecommons.org/licenses/by/4.0/ 178 Proceedings of the Fifth Workshop on NLP for Similar Languages, Varieties and Dialects, pages 178–184 Santa Fe, New Mexico, USA, August 20, 2018. A first attempting of benchmarking the identification of very similar languages in multilingual settings are the aforementioned Discriminating between Similar Languages (DSL) shared tasks. The DSL tasks have been organized from 2014 (Zampieri et al., 2014) to 2017 (Zampieri et al., 2017b) within the scope of the VarDial workshop series. Four versions of the DSLCC dataset (Tan et al., 2014) have been released containing short excerpts of journalistic texts written in similar languages and language varieties. In the four editions of the DSL shared task a variety of computation methods have been tested. This includes Maximum Entropy (Porta and Sancho, 2014), Prediction by Partial Matching (PPM) (Bobicev, 2015), language model perplexity (Gamallo et al., 2017), SVMs (Purver, 2014), Convolution Neural Networks (CNNs) (Belinkov and Glass, 2016), word"
W18-3920,W15-5401,1,0.921353,"Missing"
W18-3920,W17-5045,1,0.84077,"bian (Ljubesic and Kranjcic, 2015). This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http: //creativecommons.org/licenses/by/4.0/ 178 Proceedings of the Fifth Workshop on NLP for Similar Languages, Varieties and Dialects, pages 178–184 Santa Fe, New Mexico, USA, August 20, 2018. A first attempting of benchmarking the identification of very similar languages in multilingual settings are the aforementioned Discriminating between Similar Languages (DSL) shared tasks. The DSL tasks have been organized from 2014 (Zampieri et al., 2014) to 2017 (Zampieri et al., 2017b) within the scope of the VarDial workshop series. Four versions of the DSLCC dataset (Tan et al., 2014) have been released containing short excerpts of journalistic texts written in similar languages and language varieties. In the four editions of the DSL shared task a variety of computation methods have been tested. This includes Maximum Entropy (Porta and Sancho, 2014), Prediction by Partial Matching (PPM) (Bobicev, 2015), language model perplexity (Gamallo et al., 2017), SVMs (Purver, 2014), Convolution Neural Networks (CNNs) (Belinkov and Glass, 2016), word-based back-off models (Jauhiai"
W18-3920,W18-3901,1,0.883574,"Missing"
W18-3931,D18-1549,0,0.0246348,"et al., 2016)). Far from being settled, the architecture of NMT systems is constantly evolving. Given the youth of the paradigm and while the main structure of encoder-decoder is still maintained, the implementation of such is done either using recurrent neural networks (RNN) with attention mechanisms (Bahdanau et al., 2015), to convolutional neural networks (CNN) (Gehring et al., 2017) and to only attention mechanisms (Vaswani et al., 2017). For the same reason, research in NMT goes in many directions, including minimal units (Sennrich et al., 2016), unsupervised training and low resources (Artetxe et al., 2018) or transfer learning (Zoph et al., 2016), to name and cite just a few. In this paper we tackle an under-explored problem and apply NMT techniques to translate between language varieties. In previous work (Costa-juss`a, 2017), NMT has been used to translate between Spanish and Catalan, two closely-related Romance languages from the Iberian peninsula, outperforming phrase-based SMT approaches. In this paper we test whether this is also true for national varieties of the same language taking Brazilian and European Portuguese as a case study. To the best of our knowledge the use of NMT to transla"
W18-3931,W14-4012,0,0.102819,"Missing"
W18-3931,P16-2058,1,0.900353,"Missing"
W18-3931,W17-4123,1,0.872714,"Missing"
W18-3931,W17-1207,1,0.793656,"Missing"
W18-3931,2014.eamt-1.34,0,0.696539,"Missing"
W18-3931,L16-1284,1,0.907441,"Missing"
W18-3931,W17-1208,0,0.191108,"ttish Gaelic, (Goyal and Lehal, 2010) on Hindi and Punjabi, a few studies 1 In this paper, when talking about language varieties, we use the verbs adapt, edit, and translate interchangeably. In previous work Marujo et al. (2011) used the word adaptation whereas Fancellu et al. (2014) used the word conversion. We consider it, however, as a full-fledged translation task and approach the task as such. 2 The 1990 orthographic agreement has been recently introduced in both countries diminishing these differences. 276 on Afrikaans and Dutch (Van Huyssteen and Pilon, 2009; Otte and Tyers, 2011), and Hassani (2017) on Kurdish dialects. To the best of our knowledge, however, the use of NMT is under-explored in these tasks and no language variety translation system has been developed using NMT. The most similar study is the one by (Costa-juss`a et al., 2017) who developed a neural-based MT system to translate between Catalan and Spanish. The use of NMT to translate between language varieties is the main contribution of our work. 3 Methods To be able to compare MT approaches, we trained SMT and NMT systems using the same dataset described in Section 3.1. The two systems are described in detail in Section 3"
W18-3931,W17-3204,0,0.0248067,"ystem in comparison to the statistical system. 1 Introduction In the last five years Neural Machine Translation (NMT) has evolved from a new and promising paradigm in Machine Translation (MT) to an established state-of-the-art technology. A few studies pose that performance difference between Statistical Machine Translation (SMT) and NMT is not as a great as one could imagine (Castilho et al., 2017) while others show interesting challenges for NMT (compared to SMT) such as learning with limited amount of data, out-of-domain, long sentences, low frequency words or lack of word alignment model (Koehn and Knowles, 2017). Even so, NMT systems have constantly ranked in the top positions in the competitions held in MT conferences and workshops such as WMT (Bojar et al., 2016) and WAT (Nakazawa et al., 2016). They have also been achieving commercial success (e.g. Google’s GNMT (Wu et al., 2016)). Far from being settled, the architecture of NMT systems is constantly evolving. Given the youth of the paradigm and while the main structure of encoder-decoder is still maintained, the implementation of such is done either using recurrent neural networks (RNN) with attention mechanisms (Bahdanau et al., 2015), to convol"
W18-3931,N03-1017,0,0.0589814,"The use of NMT to translate between language varieties is the main contribution of our work. 3 Methods To be able to compare MT approaches, we trained SMT and NMT systems using the same dataset described in Section 3.1. The two systems are described in detail in Section 3.2. Systems within the SMT category use statistical techniques to compose the final translation. There are a variety of alternatives that are state-of-the art, including: n-gram (Mari˜no et al., 2006), syntax (Yamada and Knight, 2001) or hierarchical to name a few. In this paper, we are using the popular phrase-based system (Koehn et al., 2003). Systems within the NMT category use a machine learning architecture based on neural networks to compose the final translation. As mentioned, there are several architectures which have been proven state-of-the-art, all of them based on an encoder-decoder schema but using either recurrent neural networks (Cho et al., 2014), convolutional neural networks (Gehring et al., 2017) or the transformer architecture based only on attention-based mechanisms (Vaswani et al., 2017). These architectures can be adapted to deal with different input representations either words, subwords (Sennrich et al., 201"
W18-3931,P07-2045,0,0.0103602,"each language for training, with over 33 million tokens for BP and over 34 million tokens for EP. Finally, 2,000 parallel sentences were kept for development and another 2,000 sentences for testing. 3.2 Systems Statistical-based. In a phrase-based system, the main model, which is the translation model, is extracted by statistical co-occurrences from a parallel corpus at the level of sentences. This translation model is combined in the decoder with other models to compose the most probable translation given a source input. We built a standard phrase-based system with Moses open source toolkit (Koehn et al., 2007). The main parameters of our implementation include: grow-diagonal-final-and word alignment symmetrization, lexicalized reordering, relative frequencies (conditional and posterior probabilities) with phrase discounting, lexical weights, phrase bonus, accepting phrases up to length 10, 5-gram language model with Kneser-Ney smoothing, word bonus and MERT (Minimum Error Rate Training) optimisation. These parameters are taken from previous work (Costa-juss`a et al., 2017). 3 4 http://opus.lingfil.uu.se/ The clean version of the corpus is available upon request. 277 Neural-based. Specifically, neur"
W18-3931,Q17-1026,0,0.0341541,"arning architecture based on neural networks to compose the final translation. As mentioned, there are several architectures which have been proven state-of-the-art, all of them based on an encoder-decoder schema but using either recurrent neural networks (Cho et al., 2014), convolutional neural networks (Gehring et al., 2017) or the transformer architecture based only on attention-based mechanisms (Vaswani et al., 2017). These architectures can be adapted to deal with different input representations either words, subwords (Sennrich et al., 2016), characters (Costa-juss`a and Fonollosa, 2016; Lee et al., 2017) or bytes (Costa-juss`a et al., 2017). In this paper, we are using the first option of recurrent neural networks with an added attention-based mechanism (Bahdanau et al., 2015) and bytes as input representations (Costa-juss`a et al., 2017). 3.1 Data Compiling suitable parallel language variety corpora for NLP tasks is not trivial. Popular and freely available data sources (e.g. Wikipedia) used in NLP do not account for regional variation. One possible data source that includes national varieties of the same language are technical user manuals which are often localized between countries. Howeve"
W18-3931,2011.eamt-1.22,0,0.0155857,"ell (2006) on Irish and Scottish Gaelic, (Goyal and Lehal, 2010) on Hindi and Punjabi, a few studies 1 In this paper, when talking about language varieties, we use the verbs adapt, edit, and translate interchangeably. In previous work Marujo et al. (2011) used the word adaptation whereas Fancellu et al. (2014) used the word conversion. We consider it, however, as a full-fledged translation task and approach the task as such. 2 The 1990 orthographic agreement has been recently introduced in both countries diminishing these differences. 276 on Afrikaans and Dutch (Van Huyssteen and Pilon, 2009; Otte and Tyers, 2011), and Hassani (2017) on Kurdish dialects. To the best of our knowledge, however, the use of NMT is under-explored in these tasks and no language variety translation system has been developed using NMT. The most similar study is the one by (Costa-juss`a et al., 2017) who developed a neural-based MT system to translate between Catalan and Spanish. The use of NMT to translate between language varieties is the main contribution of our work. 3 Methods To be able to compare MT approaches, we trained SMT and NMT systems using the same dataset described in Section 3.1. The two systems are described in"
W18-3931,L16-1095,1,0.85961,"Missing"
W18-3931,P02-1040,0,0.100921,"CAT tool developed for translation process research. We ask native speakers of Brazilian Portuguese first to compare segments translated by NMT and SMT, choosing the best output, and subsequently to rate translations taking both fluency and adequacy into account using a 1 to 7 Likert scale. More information and the results of these experiments are presented in Section 4.2. 4 Results 4.1 Automatic Metrics In this section we present the results obtained by the statistical-based system based of phrases and the neural-based system based on seq2seq with attention and bytes in terms of BLEU score (Papineni et al., 2002). Table 1 presents the results obtained by the three systems when translating from EP to BP and Table 2 presents results obtained from BP to EP. The best results for each setting are presented in bold. System BLEU Score Phrase-based SMT Neural MT 47.68 48.58 Table 1: European to Brazilian Portuguese translation results in terms of BLEU score. System BLEU Score Phrase-based SMT Neural MT 47.34 47.54 Table 2: Brazilian to European Portuguese translation results in terms of BLEU score. We observed that in both directions the NMT system outperformed the SMT approach. The neural system obtained the"
W18-3931,P16-1162,0,0.471326,"ve also been achieving commercial success (e.g. Google’s GNMT (Wu et al., 2016)). Far from being settled, the architecture of NMT systems is constantly evolving. Given the youth of the paradigm and while the main structure of encoder-decoder is still maintained, the implementation of such is done either using recurrent neural networks (RNN) with attention mechanisms (Bahdanau et al., 2015), to convolutional neural networks (CNN) (Gehring et al., 2017) and to only attention mechanisms (Vaswani et al., 2017). For the same reason, research in NMT goes in many directions, including minimal units (Sennrich et al., 2016), unsupervised training and low resources (Artetxe et al., 2018) or transfer learning (Zoph et al., 2016), to name and cite just a few. In this paper we tackle an under-explored problem and apply NMT techniques to translate between language varieties. In previous work (Costa-juss`a, 2017), NMT has been used to translate between Spanish and Catalan, two closely-related Romance languages from the Iberian peninsula, outperforming phrase-based SMT approaches. In this paper we test whether this is also true for national varieties of the same language taking Brazilian and European Portuguese as a ca"
W18-3931,tiedemann-2012-parallel,0,0.0209058,"y corpora for NLP tasks is not trivial. Popular and freely available data sources (e.g. Wikipedia) used in NLP do not account for regional variation. One possible data source that includes national varieties of the same language are technical user manuals which are often localized between countries. However, user manuals contain a very specific technical language with short and idiomatic sentences representing commands. We searched for suitable datasets and we acquired an aligned Brazilian - European Portuguese parallel corpus of film subtitle dialogues from Open Subtitles available at Opus3 (Tiedemann, 2012). We removed all XML tags available in the data. The cleaned corpus, which we will be making available for the community as another contribution of our work4 , comprises 4.3 million sentences in each language for training, with over 33 million tokens for BP and over 34 million tokens for EP. Finally, 2,000 parallel sentences were kept for development and another 2,000 sentences for testing. 3.2 Systems Statistical-based. In a phrase-based system, the main model, which is the translation model, is extracted by statistical co-occurrences from a parallel corpus at the level of sentences. This tra"
W18-3931,P01-1067,0,0.307045,"e one by (Costa-juss`a et al., 2017) who developed a neural-based MT system to translate between Catalan and Spanish. The use of NMT to translate between language varieties is the main contribution of our work. 3 Methods To be able to compare MT approaches, we trained SMT and NMT systems using the same dataset described in Section 3.1. The two systems are described in detail in Section 3.2. Systems within the SMT category use statistical techniques to compose the final translation. There are a variety of alternatives that are state-of-the art, including: n-gram (Mari˜no et al., 2006), syntax (Yamada and Knight, 2001) or hierarchical to name a few. In this paper, we are using the popular phrase-based system (Koehn et al., 2003). Systems within the NMT category use a machine learning architecture based on neural networks to compose the final translation. As mentioned, there are several architectures which have been proven state-of-the-art, all of them based on an encoder-decoder schema but using either recurrent neural networks (Cho et al., 2014), convolutional neural networks (Gehring et al., 2017) or the transformer architecture based only on attention-based mechanisms (Vaswani et al., 2017). These archit"
W18-3931,P98-2238,0,0.0882976,"er Zero Hora, and Ted Talks to evaluate their method. Another example of a system developed to translate between Brazilian and European Portuguese is the one by Fancellu et al. (2014) who presented and SMT system trained on a parallel collection from Intel translation memories. The authors report 0.589 BLEU score using a Moses baseline system. Apart from the two aforementioned studies on translating between Portuguese varieties there have been a few studies published on translating between similar languages, language varieties, and dialects of other languages. Examples of such studies include Zhang (1998) on Mandarin and Cantonese Chinese, Scannell (2006) on Irish and Scottish Gaelic, (Goyal and Lehal, 2010) on Hindi and Punjabi, a few studies 1 In this paper, when talking about language varieties, we use the verbs adapt, edit, and translate interchangeably. In previous work Marujo et al. (2011) used the word adaptation whereas Fancellu et al. (2014) used the word conversion. We consider it, however, as a full-fledged translation task and approach the task as such. 2 The 1990 orthographic agreement has been recently introduced in both countries diminishing these differences. 276 on Afrikaans a"
W18-3931,D16-1163,0,0.0335215,"architecture of NMT systems is constantly evolving. Given the youth of the paradigm and while the main structure of encoder-decoder is still maintained, the implementation of such is done either using recurrent neural networks (RNN) with attention mechanisms (Bahdanau et al., 2015), to convolutional neural networks (CNN) (Gehring et al., 2017) and to only attention mechanisms (Vaswani et al., 2017). For the same reason, research in NMT goes in many directions, including minimal units (Sennrich et al., 2016), unsupervised training and low resources (Artetxe et al., 2018) or transfer learning (Zoph et al., 2016), to name and cite just a few. In this paper we tackle an under-explored problem and apply NMT techniques to translate between language varieties. In previous work (Costa-juss`a, 2017), NMT has been used to translate between Spanish and Catalan, two closely-related Romance languages from the Iberian peninsula, outperforming phrase-based SMT approaches. In this paper we test whether this is also true for national varieties of the same language taking Brazilian and European Portuguese as a case study. To the best of our knowledge the use of NMT to translate between national language varieties ha"
W18-3931,C98-2233,0,\N,Missing
W18-4401,W18-4411,0,0.269919,"Missing"
W18-4401,W18-4417,0,0.0963664,"Missing"
W18-4401,W18-4416,0,0.18561,"Missing"
W18-4401,W18-4409,0,0.0701163,"Missing"
W18-4401,R15-1086,0,0.178547,"Missing"
W18-4401,W18-4407,1,0.878228,"Missing"
W18-4401,L18-1226,1,0.762344,"Missing"
W18-4401,W18-4415,0,0.0689419,"Missing"
W18-4401,W18-4410,0,0.0536961,"Missing"
W18-4401,malmasi-zampieri-2017-detecting,1,0.349626,"eople. It is therefore important that preventive measures can be taken to cope with abusive behaviour aggression online. One of the strategies to cope with aggressive behaviour online is to manually monitor and moderate user-generated content, however, the amount and pace at which new data is being created on the web has rendered manual methods of moderation and intervention almost completely impractical. As such the use (semi-) automatic methods to identify such behaviour has become important and has attracted more attention from the research community in recent years (Davidson et al., 2017; Malmasi and Zampieri, 2017). This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ License details: http:// 1 Proceedings of the First Workshop on Trolling, Aggression and Cyberbullying, pages 1–11 Santa Fe, USA, August 25, 2018. This paper reports the results of the first Shared Task on Aggression Identification which was organised jointly with the First Workshop on Trolling, Aggression and Cyberbullying (TRAC - 1) at COLING 2018. 2 Related Work Verbal aggression per se has been rarely explored within the field of Natural Language Processing. However,"
W18-4401,K15-1032,0,0.0438624,"kshop on Trolling, Aggression and Cyberbullying, pages 1–11 Santa Fe, USA, August 25, 2018. This paper reports the results of the first Shared Task on Aggression Identification which was organised jointly with the First Workshop on Trolling, Aggression and Cyberbullying (TRAC - 1) at COLING 2018. 2 Related Work Verbal aggression per se has been rarely explored within the field of Natural Language Processing. However, previous research in the field has been carried out to automatically recognise several related behaviour such as trolling (Cambria et al., 2010; Kumar et al., 2014; Mojica, 2016; Mihaylov et al., 2015) , cyberbullying (Dinakar et al., 2012; Nitta et al., 2013; Dadvar et al., 2013; Dadvar et al., 2014; Hee et al., 2015), flaming / insults (Sax, 2016; Nitin et al., 2012), abusive / offensive language (Chen et al., 2012; Nobata et al., 2016; Waseem et al., 2017), hate speech (Pinkesh Badjatiya and Varma, 2017; Burnap and Williams, 2014; Davidson et al., 2017; Vigna et al., 2017; Djuric et al., 2015; Fortana, 2017; Gitari et al., 2015; Malmasi and Zampieri, 2018; Waseem and Hovy, 2016; Schmidt and Wiegand, 2017), radicalization (Agarwal and Sureka, 2015; Agarwal and Sureka, 2017), racism (Greev"
W18-4401,W18-4423,0,0.110195,"Missing"
W18-4401,W18-4406,0,0.0471652,"Missing"
W18-4401,I13-1066,0,0.2398,"nta Fe, USA, August 25, 2018. This paper reports the results of the first Shared Task on Aggression Identification which was organised jointly with the First Workshop on Trolling, Aggression and Cyberbullying (TRAC - 1) at COLING 2018. 2 Related Work Verbal aggression per se has been rarely explored within the field of Natural Language Processing. However, previous research in the field has been carried out to automatically recognise several related behaviour such as trolling (Cambria et al., 2010; Kumar et al., 2014; Mojica, 2016; Mihaylov et al., 2015) , cyberbullying (Dinakar et al., 2012; Nitta et al., 2013; Dadvar et al., 2013; Dadvar et al., 2014; Hee et al., 2015), flaming / insults (Sax, 2016; Nitin et al., 2012), abusive / offensive language (Chen et al., 2012; Nobata et al., 2016; Waseem et al., 2017), hate speech (Pinkesh Badjatiya and Varma, 2017; Burnap and Williams, 2014; Davidson et al., 2017; Vigna et al., 2017; Djuric et al., 2015; Fortana, 2017; Gitari et al., 2015; Malmasi and Zampieri, 2018; Waseem and Hovy, 2016; Schmidt and Wiegand, 2017), radicalization (Agarwal and Sureka, 2015; Agarwal and Sureka, 2017), racism (Greevy and Smeaton, 2004; Greevy, 2004) and others. In addition"
W18-4401,W18-4419,0,0.0576791,"Missing"
W18-4401,W18-4414,0,0.0438083,"Missing"
W18-4401,W18-4404,0,0.205006,"Missing"
W18-4401,W18-4403,0,0.0269422,"Missing"
W18-4401,W18-4418,0,0.101033,"Missing"
W18-4401,W18-4408,0,0.071919,"Missing"
W18-4401,W18-4402,0,0.0564524,"Missing"
W18-4401,W17-1101,0,0.284481,"ted behaviour such as trolling (Cambria et al., 2010; Kumar et al., 2014; Mojica, 2016; Mihaylov et al., 2015) , cyberbullying (Dinakar et al., 2012; Nitta et al., 2013; Dadvar et al., 2013; Dadvar et al., 2014; Hee et al., 2015), flaming / insults (Sax, 2016; Nitin et al., 2012), abusive / offensive language (Chen et al., 2012; Nobata et al., 2016; Waseem et al., 2017), hate speech (Pinkesh Badjatiya and Varma, 2017; Burnap and Williams, 2014; Davidson et al., 2017; Vigna et al., 2017; Djuric et al., 2015; Fortana, 2017; Gitari et al., 2015; Malmasi and Zampieri, 2018; Waseem and Hovy, 2016; Schmidt and Wiegand, 2017), radicalization (Agarwal and Sureka, 2015; Agarwal and Sureka, 2017), racism (Greevy and Smeaton, 2004; Greevy, 2004) and others. In addition to these, there have been some pragmatic studies on behaviour like trolling (Hardaker, 2010; Hardaker, 2013). This huge interest in the field from different perspectives has created a conglomeration of terminologies as well as understandings of the phenomenon. On the one hand, this provides us with a very rich and extensive insight into the phenomena yet, on the other hand, it has also created a theoretical gap in the understanding of interrelationship"
W18-4401,W18-4421,0,0.0392755,"Missing"
W18-4401,N16-2013,0,0.160048,"recognise several related behaviour such as trolling (Cambria et al., 2010; Kumar et al., 2014; Mojica, 2016; Mihaylov et al., 2015) , cyberbullying (Dinakar et al., 2012; Nitta et al., 2013; Dadvar et al., 2013; Dadvar et al., 2014; Hee et al., 2015), flaming / insults (Sax, 2016; Nitin et al., 2012), abusive / offensive language (Chen et al., 2012; Nobata et al., 2016; Waseem et al., 2017), hate speech (Pinkesh Badjatiya and Varma, 2017; Burnap and Williams, 2014; Davidson et al., 2017; Vigna et al., 2017; Djuric et al., 2015; Fortana, 2017; Gitari et al., 2015; Malmasi and Zampieri, 2018; Waseem and Hovy, 2016; Schmidt and Wiegand, 2017), radicalization (Agarwal and Sureka, 2015; Agarwal and Sureka, 2017), racism (Greevy and Smeaton, 2004; Greevy, 2004) and others. In addition to these, there have been some pragmatic studies on behaviour like trolling (Hardaker, 2010; Hardaker, 2013). This huge interest in the field from different perspectives has created a conglomeration of terminologies as well as understandings of the phenomenon. On the one hand, this provides us with a very rich and extensive insight into the phenomena yet, on the other hand, it has also created a theoretical gap in the underst"
W18-4401,W17-3012,0,0.433423,"llying (TRAC - 1) at COLING 2018. 2 Related Work Verbal aggression per se has been rarely explored within the field of Natural Language Processing. However, previous research in the field has been carried out to automatically recognise several related behaviour such as trolling (Cambria et al., 2010; Kumar et al., 2014; Mojica, 2016; Mihaylov et al., 2015) , cyberbullying (Dinakar et al., 2012; Nitta et al., 2013; Dadvar et al., 2013; Dadvar et al., 2014; Hee et al., 2015), flaming / insults (Sax, 2016; Nitin et al., 2012), abusive / offensive language (Chen et al., 2012; Nobata et al., 2016; Waseem et al., 2017), hate speech (Pinkesh Badjatiya and Varma, 2017; Burnap and Williams, 2014; Davidson et al., 2017; Vigna et al., 2017; Djuric et al., 2015; Fortana, 2017; Gitari et al., 2015; Malmasi and Zampieri, 2018; Waseem and Hovy, 2016; Schmidt and Wiegand, 2017), radicalization (Agarwal and Sureka, 2015; Agarwal and Sureka, 2017), racism (Greevy and Smeaton, 2004; Greevy, 2004) and others. In addition to these, there have been some pragmatic studies on behaviour like trolling (Hardaker, 2010; Hardaker, 2013). This huge interest in the field from different perspectives has created a conglomeration of t"
W19-1401,W19-1402,0,0.268136,"Missing"
W19-1401,P19-1068,1,0.913629,"guages and asked to predict the valid morphological analyses for a seventh, unseen language. In the “Semi-Closed” track, the process was the same, only participants were provided with additional raw data by the organisers. This was in the form of raw text Wikipedia dumps, bilingual dictionaries from the Apertium project and any treebanks available in the known languages from the Universal Dependencies project. Moldavian vs. Romanian Cross-dialect Topic identification (MRC): In the Moldavian vs. Romanian Cross-topic Identification shared task, we provided participants with the MOROCO data set (Butnaru and Ionescu, 2019) which contains Moldavian and Romanian samples of text collected from the news domain. The samples belong to one of the following six topics: culture, finance, politics, science, sports, and tech. The samples are pre-processed in order to eliminate named entities. For each sample, the data set provides corresponding dialectal and category labels. To this end, we proposed three subtasks for the 2019 VarDial Evaluation Campaign. The first sub-task was a binary classification by dialect task, in which a classification model is required to discriminate between the Moldavian and the Romanian dialec"
W19-1401,W18-3929,1,0.894916,"Missing"
W19-1401,W19-1413,1,0.836296,"Missing"
W19-1401,W19-1416,0,0.056458,"Missing"
W19-1401,W18-3907,1,0.895659,"Missing"
W19-1401,Y96-1018,1,0.197287,"k. 5.5 Summary Three teams participated in this first iteration of the cross-lingual analysis task. Two of the teams employed variations of neural encoderdecoder systems. Apart from lemmatization performance, it proved to be difficult to attain consistent improvements over the neural baseline systems. However, the suffix stripping approach used by the HSE team did deliver clear improvements in lemmatization for both Turkic and Romance languages. 6 6.1 Dataset Texts to distinguish between the two variations were compiled from the two existing corpora of news: Sinica Corpus for Taiwan Mandarin (Chen et al., 1996) and LCMC (The Lancaster Corpus of Mandarin Chinese, (McEnery and Xiao, 2003)) for Mainland Mandarin. Both corpora are segmented and tokenized. We remove the punctuation and unify the orthography used to eliminate orthographic cues. Since both corpora are balanced corpora, our initial thought was to provide genre-aware classification. However, inspection of both corpora suggested the genres were not defined in the same way and are not distributed homogeneously. In the next edition this idea may be exploited by using some additional resources as genre vs. regional variations which is an importa"
W19-1401,W19-1419,1,0.847943,"Missing"
W19-1401,W19-1414,0,0.0601978,"Missing"
W19-1401,W16-4801,1,0.6692,"Missing"
W19-1401,W19-1420,0,0.0913091,"ces from newspapers for each Mandarin variety. The main task is to determine if a sentence is written in the Mandarin Cuneiform Language Identification (CLI): This shared task focused on discriminating between languages and dialects originally written using the cuneiform script. The task included 2 dif2 Team Adaptcenter BAM dkosmajac DTeam SharifCL ghpaetzold gretelliz92 ekh IUCL HSE itsalexyang lonewolf MineriaUNAM NRC-CNRC R2I LIS PZ SC-UPB situx SUKI tearsofjoy T¨ubingenOslo Twist Bytes Total GDI CMA DMT X MRC CLI X X System Description Papers (Butnaru, 2019) X X X X X X (Tudoreanu, 2019) (Doostmohammadi and Nassajian, 2019) X X (Hu et al., 2019) (Mikhailov et al., 2019) (Yang and Xiang, 2019) X X X X X X X X (Bernier-Colborne et al., 2019) (Chifu, 2019) (Paetzold and Zampieri, 2019) (Onose and Cercel, 2019) X X X X X X X 7 5 X 8 X X 6 3 (Jauhiainen et al., 2019b) (Wu et al., 2019) (C¸o¨ ltekin and Barnes, 2019) (Benites et al., 2019) 14 Table 1: The teams that participated in the Third VarDial Evaluation Campaign. took part in, and a reference to each of the 14 system description papers published in the VarDial workshop proceedings. ferent languages: Sumerian and Akkadian. Furthermore, the Akkadian language was"
W19-1401,W19-1415,0,0.0347692,"Missing"
W19-1401,L18-1550,0,0.0287077,"sed on a majority voting scheme applied on five classification models: kNearest Neighbors, Logistic Regression, Support Vector Machines, Neural Networks and Random Forests. For the first and the third runs, the models are trained on both training and development sets. For the second run, the model is trained only on the training set. SC-UPB. The SC-UPB team first cleaned the dataset by removing stopwords as well as special characters. The first run submitted to each of the three subtasks is based on a model that represents text as the mean of word vectors given by a pretrained FastText model (Grave et al., 2018). The representation is provided as input to a Recurrent Neural Network with gated recurrent units, which is trained using the Adam optimizer with a batch size of 64 for 20 epochs and early stopping. The second run submitted to each of the three subtasks is based on a hierarchical attention network introduced by Yang et al. (2016). The model is trained using the Adam optimizer with a batch size of 64 for 20 epochs and early stopping. tearsofjoy. The tearsofjoy team used a linear SVM classifier with a combination of character and word n-gram features, which are weighted with the BM25 weighting"
W19-1401,W18-4802,0,0.0115539,"zers is a substantial task. It entails creation of extensive word lists and grammatical descriptions. This requires both linguistic expertise and technical expertise in the rule formalism which is used. Hence, there exists a demand for less labor intensive approaches especially for lowresource languages. Classically, rule-based analyzers have been augmented with statistical guessers which provide analyses for out-of-lexicon word forms (Lind´en, 2009). Recently, purely data-driven morphological analysis has received increasing attention (Nicolai and Kondrak, 2017; Silfverberg and Hulden, 2018; Moeller et al., 2018; Silfverberg and Tyers, 2019). Purely data-driven systems learn an analysis model from a data set of morphologically analyzed word forms and can then be applied 5.1 Dataset The dataset was compiled specifically for the shared task. We used the Wikipedias in all the languages to create a frequency list of surface tokens for each language. We then analysed these lists using the morphological analysers from the Apertium (Forcada et al., 2011) project. The lists of analyses were trimmed to include only openclass parts of speech (nouns, adjectives, adverbs and verbs). We then removed any form whic"
W19-1401,W19-1417,0,0.0619352,"Missing"
W19-1401,E17-2034,0,0.0280356,"e-art for this task, however, developing rule-based analyzers is a substantial task. It entails creation of extensive word lists and grammatical descriptions. This requires both linguistic expertise and technical expertise in the rule formalism which is used. Hence, there exists a demand for less labor intensive approaches especially for lowresource languages. Classically, rule-based analyzers have been augmented with statistical guessers which provide analyses for out-of-lexicon word forms (Lind´en, 2009). Recently, purely data-driven morphological analysis has received increasing attention (Nicolai and Kondrak, 2017; Silfverberg and Hulden, 2018; Moeller et al., 2018; Silfverberg and Tyers, 2019). Purely data-driven systems learn an analysis model from a data set of morphologically analyzed word forms and can then be applied 5.1 Dataset The dataset was compiled specifically for the shared task. We used the Wikipedias in all the languages to create a frequency list of surface tokens for each language. We then analysed these lists using the morphological analysers from the Apertium (Forcada et al., 2011) project. The lists of analyses were trimmed to include only openclass parts of speech (nouns, adjective"
W19-1401,D18-1135,1,0.824726,"016). The model is trained using the Adam optimizer with a batch size of 64 for 20 epochs and early stopping. tearsofjoy. The tearsofjoy team used a linear SVM classifier with a combination of character and word n-gram features, which are weighted with the BM25 weighting scheme. Their model’s parameters are tuned independently for each subtask, using random search and 5-fold crossvalidation. The tearsofjoy team also tried a transductive learning approach which is based on retraining the model by adding confident predictions from the test set to the training set, an idea previously studied in (Ionescu and Butnaru, 2018). • Binary classification by dialect (subtask 1) – the task is to discriminate between the Moldavian and the Romanian dialects. • MD→RO cross-dialect multi-class categorization by topic (subtask 2) – the task is to classify the samples written in the Romanian dialect into six topics, using a model trained on samples written in the Moldavian dialect. • RO→MD cross-dialect multi-class categorization by topic (subtask 3) – the task is to classify the samples written in the Moldavian dialect into six topics, using a model trained on samples written in the Romanian dialect. 7.2 Participants and App"
W19-1401,W19-1409,1,0.877823,"Missing"
W19-1401,W19-1418,0,0.0615736,"Missing"
W19-1401,N16-1174,0,0.0228023,"t. SC-UPB. The SC-UPB team first cleaned the dataset by removing stopwords as well as special characters. The first run submitted to each of the three subtasks is based on a model that represents text as the mean of word vectors given by a pretrained FastText model (Grave et al., 2018). The representation is provided as input to a Recurrent Neural Network with gated recurrent units, which is trained using the Adam optimizer with a batch size of 64 for 20 epochs and early stopping. The second run submitted to each of the three subtasks is based on a hierarchical attention network introduced by Yang et al. (2016). The model is trained using the Adam optimizer with a batch size of 64 for 20 epochs and early stopping. tearsofjoy. The tearsofjoy team used a linear SVM classifier with a combination of character and word n-gram features, which are weighted with the BM25 weighting scheme. Their model’s parameters are tuned independently for each subtask, using random search and 5-fold crossvalidation. The tearsofjoy team also tried a transductive learning approach which is based on retraining the model by adding confident predictions from the test set to the training set, an idea previously studied in (Ione"
W19-1401,W19-1423,1,0.825557,"e ranking for subtask 3, as shown in Table 8. 7.4 Dataset Summary We proposed three MRC subtasks for VarDial 2019. Three participants submitted runs for all three subtasks, and another two participants submitted runs only for subtask 1. Two teams (DTeam 5 12 http://oracc.museum.upenn.edu Language or Dialect Sumerian Old Babylonian Middle Babylonian peripheral Standard Babylonian Neo-Babylonian Late Babylonian Neo-Assyrian two systems in more detail. The PZ team used a SVM metaclassifier ensemble of several linear SVM classifiers trained using character n-gram and character skip-gram features. Paetzold and Zampieri (2019) give further details. The SharifCL team submitted three runs and their best performing system was an ensemble of a SVM and a NB classifier (Doostmohammadi and Nassajian, 2019). The ghpaetzold team submitted only one run using 2-layer compositional recurrent neural network that learns numerical representations of sentences based on their words, and of words based on their characters. Their system is described in more detail by Paetzold and Zampieri (2019). The ekh team used a sum of relative frequencies of character bigrams together with a penalty value for those bigrams or unigrams that were"
W19-1401,W17-1201,1,0.803354,"Missing"
W19-1401,L16-1641,1,0.880892,"Missing"
W19-1401,W14-5307,1,0.821127,"Missing"
W19-1401,W18-0209,1,0.707638,"r, developing rule-based analyzers is a substantial task. It entails creation of extensive word lists and grammatical descriptions. This requires both linguistic expertise and technical expertise in the rule formalism which is used. Hence, there exists a demand for less labor intensive approaches especially for lowresource languages. Classically, rule-based analyzers have been augmented with statistical guessers which provide analyses for out-of-lexicon word forms (Lind´en, 2009). Recently, purely data-driven morphological analysis has received increasing attention (Nicolai and Kondrak, 2017; Silfverberg and Hulden, 2018; Moeller et al., 2018; Silfverberg and Tyers, 2019). Purely data-driven systems learn an analysis model from a data set of morphologically analyzed word forms and can then be applied 5.1 Dataset The dataset was compiled specifically for the shared task. We used the Wikipedias in all the languages to create a frequency list of surface tokens for each language. We then analysed these lists using the morphological analysers from the Apertium (Forcada et al., 2011) project. The lists of analyses were trimmed to include only openclass parts of speech (nouns, adjectives, adverbs and verbs). We then"
W19-1401,W19-0301,1,0.916428,"task. It entails creation of extensive word lists and grammatical descriptions. This requires both linguistic expertise and technical expertise in the rule formalism which is used. Hence, there exists a demand for less labor intensive approaches especially for lowresource languages. Classically, rule-based analyzers have been augmented with statistical guessers which provide analyses for out-of-lexicon word forms (Lind´en, 2009). Recently, purely data-driven morphological analysis has received increasing attention (Nicolai and Kondrak, 2017; Silfverberg and Hulden, 2018; Moeller et al., 2018; Silfverberg and Tyers, 2019). Purely data-driven systems learn an analysis model from a data set of morphologically analyzed word forms and can then be applied 5.1 Dataset The dataset was compiled specifically for the shared task. We used the Wikipedias in all the languages to create a frequency list of surface tokens for each language. We then analysed these lists using the morphological analysers from the Apertium (Forcada et al., 2011) project. The lists of analyses were trimmed to include only openclass parts of speech (nouns, adjectives, adverbs and verbs). We then removed any form which did not include at least one"
W19-1401,E14-2006,0,0.0437089,"Missing"
W19-1401,W19-1422,0,0.0730531,"Missing"
W19-1401,W19-1412,0,0.0741523,"Missing"
W19-1423,P19-1068,0,0.0730096,"ip character bigrams and trigrams; • 2-skip character bigrams and trigrams; • 3-skip character bigrams and trigrams. Each feature class is used to train a single linear SVM classifier using LIBLINEAR (Fan et al., 1 210 http://oracc.museum.upenn.edu/ 2008). The outputs of these SVM classifiers on the training data are then used to train the metaclassifier. 4 As demonstrated by Ling et al. (2015), compositional recurrent neural networks can offer very reliable performance on a variety of NLP tasks. Previous language identification and dialect studies (Medvedeva et al., 2017; Kroon et al., 2018; Butnaru and Ionescu, 2019) and the results of the previous shared tasks organized at VarDial (Zampieri et al., 2017, 2018), however, showed that deep learning approaches do not outperform more linear n-gram-based methods so we were interested in comparing the performance of a neural model to the meta-classifier for this dataset. Results Table 2 showcases the results obtained by our team (PZ in bold) and the best submission by each of the eight teams which participating in the CLI shared task. Even though the competition allowed the use of other datasets (open submission), we have used only the dataset provided by the s"
W19-1423,L18-1387,0,0.0651532,"Missing"
W19-1423,L16-1522,0,0.0275791,"ialects and it was ranked fourth in the competition among eight teams. 1 Introduction As discussed in a recent survey (Jauhiainen et al., 2018), discriminating between similar languages, national language varieties, and dialects is an important challenge faced by state-of-the-art language identification systems. The topic has attracted more and more attention from the CL/NLP community in recent years with publications on similar languages of the Iberian peninsula (Zubiaga et al., 2016), and varieties and dialects of several languages such as Greek (Sababa and Stassopoulou, 2018) and Romanian (Ciobanu and Dinu, 2016) to name a few. As evidenced in Section 2, the focus of most of these studies is the identification of languages and dialects using contemporary data. A few exceptions include the work by Trieschnigg et al. (2012) who applied language identification methods to historical varieties of Dutch and the work by Jauhiainen et al. (2019) on languages written in cuneiform script: Sumerian and Akkadian. 2 Related Work Since its first edition in 2014, shared tasks on similar language and dialect identification have been organized together with the VarDial workshop co-located with international conference"
W19-1423,W19-1409,0,0.312218,"Missing"
W19-1423,W18-3928,0,0.045412,"of the Cuneiform Language Identification (CLI) shared task organized at VarDial 2019 (Zampieri et al., 2019). Our submission, under the team name PZ, is an adaptation of an n-gram-based meta-classifier system which showed very good performance in previous language identification shared tasks (Malmasi and Zampieri, 2017b,a). Furthermore, we compare the performance of the meta-classifier to the submissions to the CLI shared task and, in particular, to a deep learning approach submitted by the team ghpaetzold. It has been shown in previous language identification studies (Medvedeva et al., 2017; Kroon et al., 2018) that deep learning approaches do not outperform n-gram-based methods and we were interested in investigating whether this is also true for the languages and dialects included in CLI. This paper presents methods to discriminate between languages and dialects written in Cuneiform script, one of the first writing systems in the world. We report the results obtained by the PZ team in the Cuneiform Language Identification (CLI) shared task organized within the scope of the VarDial Evaluation Campaign 2019. The task included two languages, Sumerian and Akkadian. The latter is divided into six diale"
W19-1423,W17-1201,1,0.915379,"Missing"
W19-1423,W18-3901,1,0.890509,"Missing"
W19-1423,W17-1222,1,0.931706,"uneiform is an ancient writing system invented by the Sumerians for more than three millennia. In this paper we describe computational approaches to language identification on texts written in cuneiform script. For this purpose we use the dataset made available by Jauhiainen et al. (2019) to participants of the Cuneiform Language Identification (CLI) shared task organized at VarDial 2019 (Zampieri et al., 2019). Our submission, under the team name PZ, is an adaptation of an n-gram-based meta-classifier system which showed very good performance in previous language identification shared tasks (Malmasi and Zampieri, 2017b,a). Furthermore, we compare the performance of the meta-classifier to the submissions to the CLI shared task and, in particular, to a deep learning approach submitted by the team ghpaetzold. It has been shown in previous language identification studies (Medvedeva et al., 2017; Kroon et al., 2018) that deep learning approaches do not outperform n-gram-based methods and we were interested in investigating whether this is also true for the languages and dialects included in CLI. This paper presents methods to discriminate between languages and dialects written in Cuneiform script, one of the fi"
W19-1423,W17-1220,1,0.928976,"uneiform is an ancient writing system invented by the Sumerians for more than three millennia. In this paper we describe computational approaches to language identification on texts written in cuneiform script. For this purpose we use the dataset made available by Jauhiainen et al. (2019) to participants of the Cuneiform Language Identification (CLI) shared task organized at VarDial 2019 (Zampieri et al., 2019). Our submission, under the team name PZ, is an adaptation of an n-gram-based meta-classifier system which showed very good performance in previous language identification shared tasks (Malmasi and Zampieri, 2017b,a). Furthermore, we compare the performance of the meta-classifier to the submissions to the CLI shared task and, in particular, to a deep learning approach submitted by the team ghpaetzold. It has been shown in previous language identification studies (Medvedeva et al., 2017; Kroon et al., 2018) that deep learning approaches do not outperform n-gram-based methods and we were interested in investigating whether this is also true for the languages and dialects included in CLI. This paper presents methods to discriminate between languages and dialects written in Cuneiform script, one of the fi"
W19-1423,W16-0314,1,0.927798,"l. (2012) who applied language identification methods to historical varieties of Dutch and the work by Jauhiainen et al. (2019) on languages written in cuneiform script: Sumerian and Akkadian. 2 Related Work Since its first edition in 2014, shared tasks on similar language and dialect identification have been organized together with the VarDial workshop co-located with international conferences such as COLING, EACL, and NAACL. The first and most well-attended of these competitions was the Discrminating between Similar Languages (DSL) shared task which has been organized between 2014 and 2017 (Malmasi et al., 2016b; Zampieri et al., 2014, 2015, 2017). The DSL provided the first benchmark for evaluation of language identification systems developed for similar languages and language varieties using the DSL Corpus Col209 Proceedings of VarDial, pages 209–213 c Minneapolis, MN, June 7, 2019 2019 Association for Computational Linguistics Language or Dialect Late Babylonian Middle Babylonian peripheral Neo-Assyrian Neo-Babylonian Old Babylonian Standard Babylonian Sumerian Total Code LTB MPB NE NEB OLB STB SUX Texts 671 365 3,570 1,212 527 1,661 5,000 13,006 Lines 31,893 11,015 65,932 19,414 7,605 35,633 107"
W19-1423,W14-5307,1,0.902016,"Missing"
W19-1423,W16-4801,1,0.88199,"Missing"
W19-1423,W15-5401,1,0.887227,"Missing"
W19-1423,W17-1219,0,0.0772777,"(2019) to participants of the Cuneiform Language Identification (CLI) shared task organized at VarDial 2019 (Zampieri et al., 2019). Our submission, under the team name PZ, is an adaptation of an n-gram-based meta-classifier system which showed very good performance in previous language identification shared tasks (Malmasi and Zampieri, 2017b,a). Furthermore, we compare the performance of the meta-classifier to the submissions to the CLI shared task and, in particular, to a deep learning approach submitted by the team ghpaetzold. It has been shown in previous language identification studies (Medvedeva et al., 2017; Kroon et al., 2018) that deep learning approaches do not outperform n-gram-based methods and we were interested in investigating whether this is also true for the languages and dialects included in CLI. This paper presents methods to discriminate between languages and dialects written in Cuneiform script, one of the first writing systems in the world. We report the results obtained by the PZ team in the Cuneiform Language Identification (CLI) shared task organized within the scope of the VarDial Evaluation Campaign 2019. The task included two languages, Sumerian and Akkadian. The latter is d"
W19-1423,W18-6224,1,0.893538,"Missing"
W19-1423,S19-2140,1,0.86902,"Missing"
W19-5301,W19-5424,1,0.858444,"Missing"
W19-5301,W19-5306,0,0.248769,"al., 2019) University of Kyoto (Cromieres and Kurohashi, 2019) Lingua Custodia (Burlot, 2019) LIUM (Bougares et al., 2019) LMU Munich (Stojanovski and Fraser, 2019; Stojanovski et al., 2019) MLLP, Technical University of Valencia (Iranzo-Sánchez et al., 2019) Microsoft Translator (Junczys-Dowmunt, 2019) Microsoft Research Asia (Xia et al., 2019) Northeastern University / NiuTrans Co., Ltd. (Li et al., 2019a) National Institute of Information and Communications Technology (Dabre et al., 2019; Marie et al., 2019b) National Research Council of Canada (Littell et al., 2019) Bo˘gaziçi University (Biçici, 2019) PROMT LLC (Molchanov, 2019) University of Groningen (Toral et al., 2019) RWTH Aachen (Rosendahl et al., 2019) TALP Research Center, Universitat Politècnica de Catalunya (Casas et al., 2019) University of Tartu (Tättar et al., 2019) Tilde (Pinnis et al., 2019) Universitat d’Alacant (Sánchez-Cartagena et al., 2019) University of Cambridge (Stahlberg et al., 2019) Saarland University, DFKI (España-Bonet and Ruiter, 2019) University of Edinburgh (Bawden et al., 2019a) University of Maryland (Briakou and Carpuat, 2019) (no associated paper) University of Sydney (Ding and Tao, 2019) (no associated"
W19-5301,D18-1549,0,0.116951,"s are available for this system. 2.5.7 BASELINE - RE - RERANK (no associated CUNI-T RANSFORMER -T2T2018 (Popel, 2018) is the exact same system as used last year. paper) BASELINE - RE - RERANK is a standard Transformer, with corpus filtering, pre-processing, postprocessing, averaging and ensembling as well as n-best list reranking. 2.5.8 CUNI-T RANSFORMER -M ARIAN (Popel et al., 2019) is a “reimplementation” of the last year’s system (Popel, 2018) in Marian (JunczysDowmunt et al., 2018). CA I RE (Liu et al., 2019) CUNI-U NSUPERVISED -NER- POST (Kvapilíková et al., 2019) follows the strategy of Artetxe et al. (2018), creating a seed phrase-based system where the phrase table is initialized from cross-lingual embedding mappings trained on monolingual data, followed by a neural machine translation system trained on synthetic parallel corpus. The synthetic corpus is produced by the seed phrase-based MT system or by a such a model refined through iterative back-translation. CUNI-U NSUPERVISED -NER- POST further focuses on the handling of named entities, i.e. the part of vocabulary where the cross-lingual embedding mapping suffer most. CA I RE is a hybrid system that took part only in the unsupervised track."
W19-5301,D18-1332,0,0.0215805,"et al., 2018). For English↔Gujarati, synthetic parallel data from two sources, backtranslation and pivoting through Hindi, is produced using unsupervised and semi-supervised NMT models, pre-trained using a cross-lingual language objective (Lample and Conneau, 2019) For German→English, the impact of vast amounts of back-translated training data on translation quality is studied, and some additional insights are gained over (Edunov et al., 2018). Towards the end of training, for German→English and Chinese↔English, the mini-batch size was increased up to fifty-fold by delaying gradient updates (Bogoychev et al., 2018) as an alternative to learning rate cooldown (Smith, 2018). For Chinese↔English, a comparison of different segmentation strategies showed that character-based decoding was superior to the translation of subwords when translating into Chinese. Pre-processing strategies were also investigated for English→Czech, showing that preprocessing can be simplified without loss to MT quality. UEDIN’s main findings on the Chinese↔English translation task are that character-level model on the Chinese side can be used when translating into Chinese to improve the BLEU score. The same does not hold when transl"
W19-5301,W19-5351,0,0.0505622,"Missing"
W19-5301,W19-5423,0,0.0419015,"Missing"
W19-5301,W18-6412,1,0.856623,"Missing"
W19-5301,W19-5305,0,0.0496912,"Missing"
W19-5301,W12-3102,1,0.474924,"Missing"
W19-5301,W19-5310,0,0.0432396,"Missing"
W19-5301,W19-5425,0,0.0236032,"ys-Dowmunt et al., 2018) and Phrase-based machine translation system (implemented with Moses) and for the Spanish-Portuguese task. The system combination included features formerly presented in (Marie and Fujita, 2018), including scores left-to-right and right-to-left, sentence level translation probabilities and language model scores. Also authors provide contrastive results with an unsupervised phrase-based MT system which achieves quite close results to their primary system. Authors associate high performance of the unsupervised system to the language similarity. Incomslav: Team INCOMSLAV (Chen and Avgustinova, 2019) by Saarlad University participated in the Czech to Polish translation task only. The team’s primary submission builds on a transformer-based NMT baseline with back translation which has been submitted one of their contrastive submission. Incomslav’s primary system is a phoneme-based system re-scored using their NMT baseline. A second contrastive submission builds our phrase-based SMT system combined with a joint BPE model. NITS-CNLP: The NITS-CNLP team (Laskar et al., 2019) by the National Institute of Technology Silchar in India submitted results to the HI-NE translation task in both directi"
W19-5301,W07-0718,1,0.530103,"ojar Charles University Yvette Graham Barry Haddow Dublin City University University of Edinburgh Philipp Koehn JHU / University of Edinburgh Mathias Müller University of Zurich Marta R. Costa-jussà Christian Federmann UPC Microsoft Cloud + AI Shervin Malmasi Harvard Medical School Santanu Pal Saarland University Matt Post JHU Abstract Introduction The Fourth Conference on Machine Translation (WMT) held at ACL 20191 hosts a number of shared tasks on various aspects of machine translation. This conference builds on 13 previous editions of WMT as workshops and conferences (Koehn and Monz, 2006; Callison-Burch et al., 2007, 2008, 2009, 2010, 2011, 2012; Bojar et al., 2013, 2014, 2015, 2016, 2017, 2018). This year we conducted several official tasks. We report in this paper on the news and similar translation tasks. Additional shared tasks are described in separate papers in these proceedings: • biomedical translation (Bawden et al., 2019b) • automatic post-editing (Chatterjee et al., 2019) • metrics (Ma et al., 2019) • quality estimation (Fonseca et al., 2019) • parallel corpus filtering (Koehn et al., 2019) • robustness (Li et al., 2019b) In the news translation task (Section 2), participants were asked to tra"
W19-5301,P16-2058,1,0.819865,"ipated with the Transformer (Vaswani et al., 2017) implemented in the OpenNMT toolkit. They focused on word segmentation methods and compared a cognate-aware segmentation method, Cognate Morfessor (Grönroos et al., 2018), with character segmentation and unsupervised segmentation methods. As primary submission they submitted this Cognate Morfessor that optimizes subword segmentations consistently for cognates. They participated for all translation directions in Spanish-Portuguese and Czech-Polish, and this Cognate Morfessor performed better for Czech-Polish, while characterbased segmentations (Costa-jussà and Fonollosa, 2016), while much more inefficient, were superior for Spanish-Portuguese. UPC-TALP: The UPC-TALP team (Biesialska et al., 2019) by the Universitat Politècnica de Catalunya submitted a Transformer (implemented with Fairseq (Ott et al., 2019)) for the Czechto-Polish task and a Phrase-based system (implemented with Moses (Koehn et al., 2007)) for Spanish-to-Portuguese. They tested adding monolingual data to the NMT system by copying the same data on the source and target sides, with negative results. Also, their system combination based on sentence-level BLEU in back-translation 5.4 Conclusion of Simi"
W19-5301,W08-0309,1,0.659809,"Missing"
W19-5301,W18-3931,1,0.820211,"or they use English as a pivot language to translate between resource-poorer languages. The interest in English is reflected, for example, in the WMT translation tasks (e.g. News, Biomedical) which have always included language pairs in which texts are translated to and/or from English. With the widespread use of MT technology, there is more and more interest in training systems to translate between languages other than English. One evidence of this is the need of directly translating between pairs of similar languages, varieties, and dialects (Zhang, 1998; Marujo et al., 2011; Hassani, 2017; Costa-jussà et al., 2018). The main challenge is to take advantage of the similarity between languages to overcome the limitation given the low amount of available parallel data to produce an accurate output. Given the interest of the community in this topic we organize, for the first time at WMT, a shared task on ""Similar Language Translation"" to evaluate the performance of state-of-the-art translation systems on translating between pairs of languages from the same language family. We provide participants with training and testing data from three language pairs: Spanish - Portuguese (Romance languages), Czech - Polis"
W19-5301,W19-5312,0,0.0773587,"Missing"
W19-5301,W19-5313,0,0.0931699,"University (Marchisio et al., 2019) (no associated paper) University of Saarland (Mondal et al., 2019) Kingsoft AI (Guo et al., 2019) University of Kyoto (Cromieres and Kurohashi, 2019) Lingua Custodia (Burlot, 2019) LIUM (Bougares et al., 2019) LMU Munich (Stojanovski and Fraser, 2019; Stojanovski et al., 2019) MLLP, Technical University of Valencia (Iranzo-Sánchez et al., 2019) Microsoft Translator (Junczys-Dowmunt, 2019) Microsoft Research Asia (Xia et al., 2019) Northeastern University / NiuTrans Co., Ltd. (Li et al., 2019a) National Institute of Information and Communications Technology (Dabre et al., 2019; Marie et al., 2019b) National Research Council of Canada (Littell et al., 2019) Bo˘gaziçi University (Biçici, 2019) PROMT LLC (Molchanov, 2019) University of Groningen (Toral et al., 2019) RWTH Aachen (Rosendahl et al., 2019) TALP Research Center, Universitat Politècnica de Catalunya (Casas et al., 2019) University of Tartu (Tättar et al., 2019) Tilde (Pinnis et al., 2019) Universitat d’Alacant (Sánchez-Cartagena et al., 2019) University of Cambridge (Stahlberg et al., 2019) Saarland University, DFKI (España-Bonet and Ruiter, 2019) University of Edinburgh (Bawden et al., 2019a) University of"
W19-5301,W19-5314,0,0.0200465,"Bo˘gaziçi University (Biçici, 2019) PROMT LLC (Molchanov, 2019) University of Groningen (Toral et al., 2019) RWTH Aachen (Rosendahl et al., 2019) TALP Research Center, Universitat Politècnica de Catalunya (Casas et al., 2019) University of Tartu (Tättar et al., 2019) Tilde (Pinnis et al., 2019) Universitat d’Alacant (Sánchez-Cartagena et al., 2019) University of Cambridge (Stahlberg et al., 2019) Saarland University, DFKI (España-Bonet and Ruiter, 2019) University of Edinburgh (Bawden et al., 2019a) University of Maryland (Briakou and Carpuat, 2019) (no associated paper) University of Sydney (Ding and Tao, 2019) (no associated paper) 8 participated in all language pairs. The translations from the Table 5: Participants in the shared translation task. Not all teams online systems were not submitted by their respective companies but were obtained by us, and are therefore anonymized in a fashion consistent with previous years of the workshop. 2.5.6 BTRANS only the middle sentence was considered for the final translation hypothesis, otherwise shorter context of two sentences or just a single sentence was used. Unfortunately, no details are available for this system. 2.5.7 BASELINE - RE - RERANK (no associ"
W19-5301,D18-1045,0,0.0285693,"xt was morphologically segmented with Apertium. The UEDIN systems are supervised NMT systems based on the transformer architecture and trained using Marian (Junczys-Dowmunt et al., 2018). For English↔Gujarati, synthetic parallel data from two sources, backtranslation and pivoting through Hindi, is produced using unsupervised and semi-supervised NMT models, pre-trained using a cross-lingual language objective (Lample and Conneau, 2019) For German→English, the impact of vast amounts of back-translated training data on translation quality is studied, and some additional insights are gained over (Edunov et al., 2018). Towards the end of training, for German→English and Chinese↔English, the mini-batch size was increased up to fifty-fold by delaying gradient updates (Bogoychev et al., 2018) as an alternative to learning rate cooldown (Smith, 2018). For Chinese↔English, a comparison of different segmentation strategies showed that character-based decoding was superior to the translation of subwords when translating into Chinese. Pre-processing strategies were also investigated for English→Czech, showing that preprocessing can be simplified without loss to MT quality. UEDIN’s main findings on the Chinese↔Engl"
W19-5301,W18-6410,0,0.0193718,"ormance can be found in Hindi-Nepali (both directions), where the best performing system is around 50 BLEU (53 for Hindi-to-Nepali and 49.1 for Nepali-toHindi), and the lowest entry is 1.4 for Hindi-toNepali and 0 for Nepali-to-Hindi. The lowest variance is for Polish-to-Czech and it may be because only two teams participated. UHelsinki: The University of Helsinki team (Scherrer et al., 2019) participated with the Transformer (Vaswani et al., 2017) implemented in the OpenNMT toolkit. They focused on word segmentation methods and compared a cognate-aware segmentation method, Cognate Morfessor (Grönroos et al., 2018), with character segmentation and unsupervised segmentation methods. As primary submission they submitted this Cognate Morfessor that optimizes subword segmentations consistently for cognates. They participated for all translation directions in Spanish-Portuguese and Czech-Polish, and this Cognate Morfessor performed better for Czech-Polish, while characterbased segmentations (Costa-jussà and Fonollosa, 2016), while much more inefficient, were superior for Spanish-Portuguese. UPC-TALP: The UPC-TALP team (Biesialska et al., 2019) by the Universitat Politècnica de Catalunya submitted a Transform"
W19-5301,W19-5317,0,0.114089,"ed paper) (Liu et al., 2019) Charles University (Popel et al., 2019; Kocmi and Bojar, 2019) and (Kvapilíková et al., 2019) Kumamoto University, Telkom University, Indonesian Institute of Sciences (Budiwati et al., 2019) DFKI (Zhang and van Genabith, 2019) eTranslation (Oravecz et al., 2019) Facebook AI Research (Ng et al., 2019) GTCOM (Bei et al., 2019) University of Helsinki (Talman et al., 2019) IIIT Hyderabad (Goyal and Sharma, 2019) IIT Patna (Sen et al., 2019) Johns Hopkins University (Marchisio et al., 2019) (no associated paper) University of Saarland (Mondal et al., 2019) Kingsoft AI (Guo et al., 2019) University of Kyoto (Cromieres and Kurohashi, 2019) Lingua Custodia (Burlot, 2019) LIUM (Bougares et al., 2019) LMU Munich (Stojanovski and Fraser, 2019; Stojanovski et al., 2019) MLLP, Technical University of Valencia (Iranzo-Sánchez et al., 2019) Microsoft Translator (Junczys-Dowmunt, 2019) Microsoft Research Asia (Xia et al., 2019) Northeastern University / NiuTrans Co., Ltd. (Li et al., 2019a) National Institute of Information and Communications Technology (Dabre et al., 2019; Marie et al., 2019b) National Research Council of Canada (Littell et al., 2019) Bo˘gaziçi University (Biçici, 201"
W19-5301,W19-5315,0,0.0266353,"Missing"
W19-5301,W19-5318,0,0.198338,"ance of the systems when translating from French to German seems to heavily depend on the 7 http://data.statmt.org/wmt19/ translation-task/dev.tgz 6 Systems MSRA.MADL eTranslation LIUM MLLP-UPV onlineA TartuNLP onlineB onlineY onlineG onlineX FULL 47.3 45.4 43.7 41.5 40.8 39.2 39.1 39.0 38.5 38.1 source FR 38.3 37.4 37.5 36.4 35.4 34.8 35.3 34.7 34.6 35.6 source DE 50.0 47.8 45.5 43.0 42.3 40.5 40.2 40.2 39.7 38.8 evaluations. In the rest of this sub-section, we provide brief details of the submitted systems, for those in cases where the authors provided such details. 2.5.1 AFRL - SYSCOMB 19 (Gwinnup et al., 2019) is a system combination of a Marian ensemble system, two distinct OpenNMT systems, a Sockeyebased Elastic Weight Consolidation system, and one Moses phrase-based system. Table 3: French→German Meteor scores. Systems MSRA.MADL LinguaCustodia MLLP_UPV Kyoto_University_T2T LIUM onlineY onlineB TartuNLP onlineA onlineX onlineG FULL 52.0 51.3 49.5 48.8 48.3 47.5 46.4 46.3 45.3 42.7 41.7 source FR 51.9 52.5 49.9 49.7 46.5 43.7 43.7 45.0 43.7 41.6 40.9 source DE 52.0 51.0 49.4 48.6 48.7 48.4 47.0 46.7 45.8 42.9 41.9 AFRL- EWC (Gwinnup et al., 2019) is a Sockeye Transformer system trained with the de"
W19-5301,W19-5316,0,0.109691,"boratory (Gwinnup et al., 2019) Apertium (Pirinen, 2019) Apprentice (Li and Specia, 2019) Aylien Ltd. (Hokamp et al., 2019) Baidu (Sun et al., 2019) (no associated paper) (no associated paper) (Liu et al., 2019) Charles University (Popel et al., 2019; Kocmi and Bojar, 2019) and (Kvapilíková et al., 2019) Kumamoto University, Telkom University, Indonesian Institute of Sciences (Budiwati et al., 2019) DFKI (Zhang and van Genabith, 2019) eTranslation (Oravecz et al., 2019) Facebook AI Research (Ng et al., 2019) GTCOM (Bei et al., 2019) University of Helsinki (Talman et al., 2019) IIIT Hyderabad (Goyal and Sharma, 2019) IIT Patna (Sen et al., 2019) Johns Hopkins University (Marchisio et al., 2019) (no associated paper) University of Saarland (Mondal et al., 2019) Kingsoft AI (Guo et al., 2019) University of Kyoto (Cromieres and Kurohashi, 2019) Lingua Custodia (Burlot, 2019) LIUM (Bougares et al., 2019) LMU Munich (Stojanovski and Fraser, 2019; Stojanovski et al., 2019) MLLP, Technical University of Valencia (Iranzo-Sánchez et al., 2019) Microsoft Translator (Junczys-Dowmunt, 2019) Microsoft Research Asia (Xia et al., 2019) Northeastern University / NiuTrans Co., Ltd. (Li et al., 2019a) National Institute of"
W19-5301,E14-1047,1,0.904335,"Missing"
W19-5301,W19-5427,0,0.0467733,"Missing"
W19-5301,W19-5322,1,0.807321,"Missing"
W19-5301,W19-5302,1,0.715203,"20191 hosts a number of shared tasks on various aspects of machine translation. This conference builds on 13 previous editions of WMT as workshops and conferences (Koehn and Monz, 2006; Callison-Burch et al., 2007, 2008, 2009, 2010, 2011, 2012; Bojar et al., 2013, 2014, 2015, 2016, 2017, 2018). This year we conducted several official tasks. We report in this paper on the news and similar translation tasks. Additional shared tasks are described in separate papers in these proceedings: • biomedical translation (Bawden et al., 2019b) • automatic post-editing (Chatterjee et al., 2019) • metrics (Ma et al., 2019) • quality estimation (Fonseca et al., 2019) • parallel corpus filtering (Koehn et al., 2019) • robustness (Li et al., 2019b) In the news translation task (Section 2), participants were asked to translate a shared test set, optionally restricting themselves to the provided training data (“constrained” condition). We 1 Christof Monz University of Amsterdam Marcos Zampieri University of Wolverhampton held 18 translation tasks this year, between English and each of Chinese, Czech (into Czech only), German, Finnish, Lithuanian, and Russian. New this year were Gujarati↔English and Kazakh↔English. B"
W19-5301,W19-5333,0,0.0923169,"Missing"
W19-5301,W19-5353,0,0.0658382,"Missing"
W19-5301,W19-5430,1,0.873847,"Missing"
W19-5301,W19-5431,0,0.0199622,"Universitat Politècnica de València (UPV) participated with a Transformer (implemented with FairSeq (Ott et al., 2019)) and a finetuning strategy for domain adaptaion in the task of Spanish-Portuguese. Fine-tunning on the development data provide improvements of almost 12 BLEU points, which may explain their clear best performance in the task for this language pair. As a contrastive system authors provided only for the Portuguese-to-Spanish a novel 2D alternating RNN model which did not respond so well when fine-tunning. UBC-NLP: Team UBC-NLP from the University of British Columbia in Canada (Przystupa and Abdul-Mageed, 2019) compared the performance of the LSTM plus attention (Bahdanau et al., 2015) and Transformer (Vaswani et al., 2017) (implemented in OpenNMT toolkit22 ) perform for the three tasks at hand. Authors use backtranslation to introduce monolingual data in their systems. LSTM plus attention outperformed Transformer for Hindi-Nepali, and viceversa for the other two tasks. As reported by the authors, Hindi-Nepali task provides much more shorter sentences than KYOTOUNIVERSITY: Kyoto University’s submission, listed simply as KYOTO in Table 25 for PT → ES task is based on transformer NMT system. They used"
W19-5301,P02-1040,0,0.11337,"ation of the source (CS), and a second encoder to encode sub-word (byte-pair-encoding) information of the source (CS). The results obtained by their system in translating from Czech→Polish and comment on the impact of out-of-domain test data in the performance of their system. UDSDFKI ranked second among ten teams in Czech– Polish translation. 5.3 Results We present results for the three language pairs, each of them in the two possible directions. For this first edition of the Similar Translation Task and differently from News task, evaluation was only performed on automatic basis using BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) measures. Each language direction is reported in one different table which contain information of the team; type of system, either contrastive (C) or primary (P), and the BLEU and TER results. In general, primary systems tend to be better than contrastive systems, as expected, but there are some exceptions. Even if we are presenting 3 pairs of languages each pair belonging to the same family, translation quality in terms of BLEU varies signficantly. While the best systems for Spanish-Portuguese are above 64 BLEU and below 21 TER (see Tables 26 and 27), best syste"
W19-5301,W19-5354,0,0.0611791,"Missing"
W19-5301,W18-6486,0,0.0189853,"the agglutinative nature of Kazakh, (ii) data from an additional language (Russian), given the scarcity of English–Kazakh data and (iii) synthetic data for the source language filtered using language-independent sentence similarity. RUG _ KKEN _ MORFESSOR Tilde developed both constrained and unconstrained NMT systems for English-Lithuanian and Lithuanian-English using the Marian toolkit. All systems feature ensembles of four to five transformer models that were trained using the quasi-hyperbolic Adam optimiser (Ma and Yarats, 2018). Data for the systems were prepared using TildeMT filtering (Pinnis, 2018) and preprocessing (Pinnis et al., 2018) methods. For unconstrained systems, data were additionally filtered using dual conditional cross-entropy filtering (Junczys-Dowmunt, 2018a). All systems were trained using iterative back-translation (Rikters, 2018) and feature synthetic data that allows training NMT systems to support handling of unknown phenomena (Pinnis et al., 2017). During translation, automatic named entity and nontranslatable phrase post-editing were performed. For constrained systems, named entities and nontranslatable phrase lists were extracted from the parallel training data."
W19-5301,W19-5335,0,0.0408704,"Missing"
W19-5301,W19-5344,1,0.904781,"n Institute of Sciences (Budiwati et al., 2019) DFKI (Zhang and van Genabith, 2019) eTranslation (Oravecz et al., 2019) Facebook AI Research (Ng et al., 2019) GTCOM (Bei et al., 2019) University of Helsinki (Talman et al., 2019) IIIT Hyderabad (Goyal and Sharma, 2019) IIT Patna (Sen et al., 2019) Johns Hopkins University (Marchisio et al., 2019) (no associated paper) University of Saarland (Mondal et al., 2019) Kingsoft AI (Guo et al., 2019) University of Kyoto (Cromieres and Kurohashi, 2019) Lingua Custodia (Burlot, 2019) LIUM (Bougares et al., 2019) LMU Munich (Stojanovski and Fraser, 2019; Stojanovski et al., 2019) MLLP, Technical University of Valencia (Iranzo-Sánchez et al., 2019) Microsoft Translator (Junczys-Dowmunt, 2019) Microsoft Research Asia (Xia et al., 2019) Northeastern University / NiuTrans Co., Ltd. (Li et al., 2019a) National Institute of Information and Communications Technology (Dabre et al., 2019; Marie et al., 2019b) National Research Council of Canada (Littell et al., 2019) Bo˘gaziçi University (Biçici, 2019) PROMT LLC (Molchanov, 2019) University of Groningen (Toral et al., 2019) RWTH Aachen (Rosendahl et al., 2019) TALP Research Center, Universitat Politècnica de Catalunya (Casas e"
W19-5301,W19-5346,0,0.197908,"rtium (Pirinen, 2019) Apprentice (Li and Specia, 2019) Aylien Ltd. (Hokamp et al., 2019) Baidu (Sun et al., 2019) (no associated paper) (no associated paper) (Liu et al., 2019) Charles University (Popel et al., 2019; Kocmi and Bojar, 2019) and (Kvapilíková et al., 2019) Kumamoto University, Telkom University, Indonesian Institute of Sciences (Budiwati et al., 2019) DFKI (Zhang and van Genabith, 2019) eTranslation (Oravecz et al., 2019) Facebook AI Research (Ng et al., 2019) GTCOM (Bei et al., 2019) University of Helsinki (Talman et al., 2019) IIIT Hyderabad (Goyal and Sharma, 2019) IIT Patna (Sen et al., 2019) Johns Hopkins University (Marchisio et al., 2019) (no associated paper) University of Saarland (Mondal et al., 2019) Kingsoft AI (Guo et al., 2019) University of Kyoto (Cromieres and Kurohashi, 2019) Lingua Custodia (Burlot, 2019) LIUM (Bougares et al., 2019) LMU Munich (Stojanovski and Fraser, 2019; Stojanovski et al., 2019) MLLP, Technical University of Valencia (Iranzo-Sánchez et al., 2019) Microsoft Translator (Junczys-Dowmunt, 2019) Microsoft Research Asia (Xia et al., 2019) Northeastern University / NiuTrans Co., Ltd. (Li et al., 2019a) National Institute of Information and Communicatio"
W19-5301,W19-5341,0,0.0172601,"A,B,G,X,Y. For presentation of the results, systems are treated as either constrained or unconstrained, depending on whether their models were trained only on the provided data. Since we do not know how they were built, the online systems are treated as unconstrained during the automatic and human AYLIEN _ MULTILINGUAL (Hokamp et al., 2019) The Aylien research team built a Multilingual NMT system which is trained on all WMT2019 language pairs in all directions, then fine-tuned for a small number of iterations on Gujarati-English data only, including some self-backtranslated data. 2.5.5 BAIDU (Sun et al., 2019) Baidu systems are based on the Transformer architecture with several improvements. Data selection, back translation, data augmentation, knowledge distillation, domain adaptation, model ensemble and re-ranking are employed and proven effective in our experiments. 7 Team AFRL A PERTIUM - FIN - ENG A PPRENTICE - C AYLIEN _ MULTILINGUAL BAIDU BTRANS BASELINE - RE - RERANK CA I RE CUNI DBMS-KU DFKI - NMT E T RANSLATION FACEBOOK FAIR GTCOM H ELSINKI NLP IIITH-MT IITP JHU JUMT JU_S AARLAND KSAI K YOTO U NIVERSITY L INGUA C USTODIA LIUM LMU-NMT MLLP-UPV MS T RANSLATOR MSRA N IU T RANS NICT NRC PARFDA"
W19-5301,P16-1162,1,0.310296,"ssible, 2.5.13 E T RANSLATION (Oravecz et al., 2019) E T RANSLATION En-De E T RANSLATION ’s EnDe system is an ensemble of 3 base Transformers and a Transformer-type language model, trained from all available parallel data (cleaned up and filtered with dual conditional cross-entropy filtering) and with additional back-translated data generated 9 2.5.17 from monolingual news. Each Transformer model is fine tuned on previous years’ test sets. H ELSINKI NLP is a Transformer (Vaswani et al., 2017) style model implemented in OpenNMTpy using a variety of corpus filtering techniques, truecasing, BPE (Sennrich et al., 2016), backtranslation, ensembling and fine-tuning for domain adaptation. E T RANSLATION Fr-De The Fr-De system is an ensemble of 2 big Transformers (with size 8192 FFN layers). Back-translation data was selected using topic modelling techniques to tune the model towards the domain defined in the task. 2.5.18 En-Lt The En-Lt system is an ensemble of 2 big Transformers (as for Fr-De) and a Transformer type language model. The training data contains the Rapid corpus and the news domain back-translated data sets 2 times oversampled. E T RANSLATION 2.5.19 FACEBOOK FAIR (Ng et al., 2019) 2.5.20 JHU (Mar"
W19-5301,W19-5339,0,0.0767002,"Missing"
W19-5301,W19-5347,0,0.0328257,"Missing"
W19-5301,W19-5342,1,0.887858,"encia (Iranzo-Sánchez et al., 2019) Microsoft Translator (Junczys-Dowmunt, 2019) Microsoft Research Asia (Xia et al., 2019) Northeastern University / NiuTrans Co., Ltd. (Li et al., 2019a) National Institute of Information and Communications Technology (Dabre et al., 2019; Marie et al., 2019b) National Research Council of Canada (Littell et al., 2019) Bo˘gaziçi University (Biçici, 2019) PROMT LLC (Molchanov, 2019) University of Groningen (Toral et al., 2019) RWTH Aachen (Rosendahl et al., 2019) TALP Research Center, Universitat Politècnica de Catalunya (Casas et al., 2019) University of Tartu (Tättar et al., 2019) Tilde (Pinnis et al., 2019) Universitat d’Alacant (Sánchez-Cartagena et al., 2019) University of Cambridge (Stahlberg et al., 2019) Saarland University, DFKI (España-Bonet and Ruiter, 2019) University of Edinburgh (Bawden et al., 2019a) University of Maryland (Briakou and Carpuat, 2019) (no associated paper) University of Sydney (Ding and Tao, 2019) (no associated paper) 8 participated in all language pairs. The translations from the Table 5: Participants in the shared translation task. Not all teams online systems were not submitted by their respective companies but were obtained by us, and"
W19-5301,W19-5355,1,0.869964,"Missing"
W19-5301,W19-5350,0,0.0441915,"Missing"
W19-5301,P98-2238,0,0.38957,"n trained to translate texts from and to English or they use English as a pivot language to translate between resource-poorer languages. The interest in English is reflected, for example, in the WMT translation tasks (e.g. News, Biomedical) which have always included language pairs in which texts are translated to and/or from English. With the widespread use of MT technology, there is more and more interest in training systems to translate between languages other than English. One evidence of this is the need of directly translating between pairs of similar languages, varieties, and dialects (Zhang, 1998; Marujo et al., 2011; Hassani, 2017; Costa-jussà et al., 2018). The main challenge is to take advantage of the similarity between languages to overcome the limitation given the low amount of available parallel data to produce an accurate output. Given the interest of the community in this topic we organize, for the first time at WMT, a shared task on ""Similar Language Translation"" to evaluate the performance of state-of-the-art translation systems on translating between pairs of languages from the same language family. We provide participants with training and testing data from three language"
W19-5430,P07-2045,0,0.0110993,"(or generaldomain) data only and it differs substantially from the released development set which is part of a TED corpus. The parallel data includes Europarl v9, Wiki-titles v1, and JRC-Acquis. We combine all the released data and prepare a large outdomain dataset. 3.1 score = |Hsrc (srcj , lmi ) − Hsrc (srcj , lmo )| + |Htrg (trgj , lmi ) − Htrg (trgj , lmo ) |(1) 4 Pre-processing The out-domain data is noisy for our purposes, so we apply methods for cleaning. We performed the following two steps: (i) we use the cleaning process described in Pal et al. (2015), and (ii) we execute the Moses (Koehn et al., 2007) corpus cleaning scripts with minimum and maximum number of tokens set to 1 and 100, respectively. After cleaning, we perform punctuation normalization, and then we use the Moses tokenizer to tokenize the out-domain corpus with ‘no-escape’ option. Finally, we apply true-casing. The cleaned version of the released data, i.e., the General corpus containing 1,394,319 sentences, is sorted based on the score in Equation 1. Thereafter, We split the entire data (1,394,319) into two sets; we use the first 1,000 for validation and the remaining as training data. The released development set (Dev) is us"
W19-5430,P02-1040,0,0.107429,"verhampton, UK 3 German Research Center for Artificial Intelligence (DFKI), Saarland Informatics Campus, Germany santanu.pal@uni-saarland.de 1 Abstract ing, development, and testing data from the following language pairs: Spanish - Portuguese (Romance languages), Czech - Polish (Slavic languages), and Hindi - Nepali (Indo-Aryan languages). Participant could submit system outputs to any of the three language pairs in any direction. The shared task attracted a good number of participants and the performance of all entries was evaluated using popular MT automatic evaluation metrics, namely BLEU (Papineni et al., 2002) and TER (Snover et al., 2006). In this paper we describe the UDS-DFKI system to the WMT 2019 Similar Language Translation task. The system achieved competitive performance and ranked second among ten entries in Czech to Polish translation in terms of BLEU score. In this paper we present the UDS-DFKI system submitted to the Similar Language Translation shared task at WMT 2019. The first edition of this shared task featured data from three pairs of similar languages: Czech and Polish, Hindi and Nepali, and Portuguese and Spanish. Participants could choose to participate in any of these three tr"
W19-5430,W15-3017,1,0.860655,"Missing"
W19-5430,D11-1033,0,0.0388467,"h, and two pairs of similar languages Croatian–Serbian 219 Proceedings of the Fourth Conference on Machine Translation (WMT), Volume 3: Shared Task Papers (Day 2) pages 219–223 c Florence, Italy, August 1-2, 2019. 2019 Association for Computational Linguistics et al., 2011), which are very similar to the indomain data (for our case the development set), and (ii) transferenceALL, utilizing all the released out-domain data sorted by Equation 1. The transference500Ktraining set is prepared using in-domain (development set) bilingual cross-entropy difference for data selection as was described in Axelrod et al. (2011). The difference in cross-entropy is computed based on two language models (LM): a domain-specific LM is estimated from the in-domain (containing 2050 sentences) corpus (lmi ) and the out-domain LM (lmo ) is estimated from the eScape corpus. We rank the eScape corpus by assigning a score to each of the individual sentences which is the sum of the three cross-entropy (H) differences. For a j th sentence pair srcj –trg j , the score is calculated based on Equation 1. and Indonesian–Malay. Processing similar languages and language varieties has attracted attention not only in the MT community but"
W19-5430,P16-1162,0,0.0274215,"initially train on the complete out-of-domain dataset (General). The General data is sorted based on their in-domain similarities as described in Equation 1. transferenceALLmodels are then fine-tuned towards the 500K (in-domain-like) data. Finally, we perform checkpoint averaging using the 8 best checkpoints. We report the results on the provided development set, which we use as a test set before the submission. Additionally we also report the official test set result. To handle out-of-vocabulary words and to reduce the vocabulary size, instead of considering words, we consider subword units (Sennrich et al., 2016) by using byte-pair encoding (BPE). In the preprocessing step, instead of learning an explicit mapping between BPEs in the Czech (CS) and Polish (PL), we define BPE tokens by jointly processing all parallel data. Thus, CS and PL derive a single BPE vocabulary. Since CS and PL belong to the similar language, they naturally share a good fraction of BPE tokens, which reduces the vocabulary size. We pass word level information on the first encoder and the BPE information to the second one. On the decoder side of the transference model we pass only BPE text. We evaluate our approach with developmen"
W19-5430,2006.amta-papers.25,0,0.499063,"Center for Artificial Intelligence (DFKI), Saarland Informatics Campus, Germany santanu.pal@uni-saarland.de 1 Abstract ing, development, and testing data from the following language pairs: Spanish - Portuguese (Romance languages), Czech - Polish (Slavic languages), and Hindi - Nepali (Indo-Aryan languages). Participant could submit system outputs to any of the three language pairs in any direction. The shared task attracted a good number of participants and the performance of all entries was evaluated using popular MT automatic evaluation metrics, namely BLEU (Papineni et al., 2002) and TER (Snover et al., 2006). In this paper we describe the UDS-DFKI system to the WMT 2019 Similar Language Translation task. The system achieved competitive performance and ranked second among ten entries in Czech to Polish translation in terms of BLEU score. In this paper we present the UDS-DFKI system submitted to the Similar Language Translation shared task at WMT 2019. The first edition of this shared task featured data from three pairs of similar languages: Czech and Polish, Hindi and Nepali, and Portuguese and Spanish. Participants could choose to participate in any of these three tracks and submit system outputs"
W19-5430,W17-1207,0,0.031028,"Missing"
W19-5430,W18-3931,1,0.847571,"Missing"
W19-5430,2014.eamt-1.34,0,0.0234245,"nterest in training systems to translate between languages other than English (Costa-juss`a, 2017). One reason for this is the growing need of direct translation between pairs of similar languages, and to a lesser extent language varieties, without the use of English as a pivot language. The main challenge is to overcome the limitation of available parallel data taking advantage of the similarity between languages. Studies have been published on translating between similar languages (e.g. Catalan - Spanish (Costa-juss`a, 2017)) and language varieties such as European and Brazilian Portuguese (Fancellu et al., 2014; Costa-juss`a et al., 2018). The study by Lakew et al. (2018) tackles both training MT systems to translate between European–Brazilian Portuguese and European–Canadian French, and two pairs of similar languages Croatian–Serbian 219 Proceedings of the Fourth Conference on Machine Translation (WMT), Volume 3: Shared Task Papers (Day 2) pages 219–223 c Florence, Italy, August 1-2, 2019. 2019 Association for Computational Linguistics et al., 2011), which are very similar to the indomain data (for our case the development set), and (ii) transferenceALL, utilizing all the released out-domain data s"
W19-5430,W18-6316,0,\N,Missing
W19-6702,C14-2028,0,0.137444,", CATaLog Online uses the Nutch2 information retrieval (IR) system. Nutch follows the standard IR model of Lucene3 with document parsing, document Indexing, TF-IDF calculation, query parsing and finally searching/document retrieval and document ranking. In our implementation, each document contains (a) a TM source segment, (b) its corresponding translation and (c) the word alignments. Machine Translation and Automatic Post Editing Along with TM matches, CATaLog Online provides MT output (Pal et al., 2015a) to the translator, an option provided by many state-of-the-art CAT tools (e.g. MateCat (Federico et al., 2014)). Besides the retrieved TM segment and the MT output CATaLog Online provides also a third option to the translator: the output of an automatic post-editing system meant to be post-edited as the MT output. The APE system is based in an OSM model (Pal et al., 2016b) and proved to deliver competitive performance in previous editions of the Automatic Post Editing (APE) shared task at WMT Bojar et al. (2016). 2 3 http://nutch.apache.org/ http://lucene.apache.org/ Proceedings of MT Summit XVII, volume 2 Editing Logs For a given input segment, CATaLog Online provides four different options: TM, MT,"
W19-6702,W15-4905,1,0.909403,"Missing"
W19-6702,W12-3123,0,0.598965,"Missing"
W19-6702,W15-5206,1,0.888157,"Missing"
W19-6702,W15-3017,1,0.90096,"Missing"
W19-6702,W15-3026,1,0.902711,"Missing"
W19-6702,L16-1095,1,0.855593,"Missing"
W19-6702,W16-2379,1,0.892057,"Missing"
W19-6702,2015.tc-1.15,0,0.150837,"Missing"
W19-6702,W14-0314,1,0.712543,"the users preferred using CATaLog Online over existing CAT tools in some respects, especially by selecting the output of the MT system and taking advantage of the color scheme for TM suggestions. 1 Introduction The use of computer software is an important part of the modern translation workflow (Zaretskaya et al., 2015; Schneider et al., 2019). A number of tools are widely used by professional translators, most notably CAT tools and terminology management software. These tools increase translators’ productivity, improve consistency in translation and, in turn, reduce the cost of translation (Zampieri and Vela, 2014). The most important compo© 2019 The authors. This article is licensed under a Creative Commons 4.0 licence, no derivative works, attribution, CC-BY-ND. Proceedings of MT Summit XVII, volume 2 nent in state-of-the-art CAT tools are translation memories (TM). The translators can either accept, reject or modify the suggestions received from the TM engine. As the process is done iteratively, every new translation increases the size of the translation memory making it more useful for future translations. The idea behind TMs is relatively simple, however, the process of matching and retrieval of so"
W19-6702,2015.eamt-1.6,1,\N,Missing
zampieri-gebre-2014-varclass,grothe-etal-2008-comparative,0,\N,Missing
zampieri-gebre-2014-varclass,P12-3005,0,\N,Missing
zampieri-gebre-2014-varclass,tiedemann-2012-parallel,0,\N,Missing
