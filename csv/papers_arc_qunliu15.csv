2021.iwslt-1.17,Multilingual Speech Translation with Unified Transformer: Huawei {N}oah{'}s Ark Lab at {IWSLT} 2021,2021,-1,-1,3,0,5773,xingshan zeng,Proceedings of the 18th International Conference on Spoken Language Translation (IWSLT 2021),0,"This paper describes the system submitted to the IWSLT 2021 Multilingual Speech Translation (MultiST) task from Huawei Noah{'}s Ark Lab. We use a unified transformer architecture for our MultiST model, so that the data from different modalities (i.e., speech and text) and different tasks (i.e., Speech Recognition, Machine Translation, and Speech Translation) can be exploited to enhance the model{'}s ability. Specifically, speech and text inputs are firstly fed to different feature extractors to extract acoustic and textual features, respectively. Then, these features are processed by a shared encoder{--}decoder architecture. We apply several training techniques to improve the performance, including multi-task learning, task-level curriculum learning, data augmentation, etc. Our final system achieves significantly better results than bilingual baselines on supervised language pairs and yields reasonable results on zero-shot language pairs."
2021.findings-emnlp.195,Generate {\\&} Rank: A Multi-task Framework for Math Word Problems,2021,-1,-1,7,0,6911,jianhao shen,Findings of the Association for Computational Linguistics: EMNLP 2021,0,"Math word problem (MWP) is a challenging and critical task in natural language processing. Many recent studies formalize MWP as a generation task and have adopted sequence-to-sequence models to transform problem descriptions to mathematical expressions. However, mathematical expressions are prone to minor mistakes while the generation objective does not explicitly handle such mistakes. To address this limitation, we devise a new ranking task for MWP and propose Generate {\&} Rank, a multi-task framework based on a generative pre-trained language model. By joint training with generation and ranking, the model learns from its own mistakes and is able to distinguish between correct and incorrect expressions. Meanwhile, we perform tree-based disturbance specially designed for MWP and an online update to boost the ranker. We demonstrate the effectiveness of our proposed method on the benchmark and the results show that our method consistently outperforms baselines in all datasets. Particularly, in the classical Math23k, our method is 7{\%} (78.4{\%} to 85.4{\%}) higher than the state-of-the-art. Code could be found at https://github.com/huawei-noah/noah-research."
2021.findings-emnlp.323,Revisiting Robust Neural Machine Translation: A Transformer Case Study,2021,-1,-1,3,1,7213,peyman passban,Findings of the Association for Computational Linguistics: EMNLP 2021,0,"Transformers have brought a remarkable improvement in the performance of neural machine translation (NMT) systems but they could be surprisingly vulnerable to noise. In this work, we try to investigate how noise breaks Transformers and if there exist solutions to deal with such issues. There is a large body of work in the NMT literature on analyzing the behavior of conventional models for the problem of noise but Transformers are relatively understudied in this context. Motivated by this, we introduce a novel data-driven technique called Target Augmented Fine-tuning (TAFT) to incorporate noise during training. This idea is comparable to the well-known fine-tuning strategy. Moreover, we propose two other novel extensions to the original Transformer: Controlled Denoising (CD) and Dual-Channel Decoding (DCD), that modify the neural architecture as well as the training process to handle noise. One important characteristic of our techniques is that they only impact the training phase and do not impose any overhead at inference time. We evaluated our techniques to translate the English{--}German pair in both directions and observed that our models have a higher tolerance to noise. More specifically, they perform with no deterioration where up to 10{\%} of entire test words are infected by noise."
2021.findings-acl.137,Better Robustness by More Coverage: Adversarial and Mixup Data Augmentation for Robust Finetuning,2021,-1,-1,6,0,1016,chenglei si,Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021,0,None
2021.findings-acl.139,{H}y{K}now: End-to-End Task-Oriented Dialog Modeling with Hybrid Knowledge Management,2021,-1,-1,4,0,7840,silin gao,Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021,0,None
2021.findings-acl.218,{R}eal{T}ran{S}: End-to-End Simultaneous Speech Translation with Convolutional Weighted-Shrinking Transformer,2021,-1,-1,3,0,5773,xingshan zeng,Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021,0,None
2021.findings-acl.241,"Two Parents, One Child: {D}ual Transfer for Low-Resource Neural Machine Translation",2021,-1,-1,3,0,8091,meng zhang,Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021,0,None
2021.emnlp-main.211,{D}y{L}ex: Incorporating Dynamic Lexicons into {BERT} for Sequence Labeling,2021,-1,-1,10,0,9080,baojun wang,Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,0,"Incorporating lexical knowledge into deep learning models has been proved to be very effective for sequence labeling tasks. However, previous works commonly have difficulty dealing with large-scale dynamic lexicons which often cause excessive matching noise and problems of frequent updates. In this paper, we propose DyLex, a plug-in lexicon incorporation approach for BERT based sequence labeling tasks. Instead of leveraging embeddings of words in the lexicon as in conventional methods, we adopt word-agnostic tag embeddings to avoid re-training the representation while updating the lexicon. Moreover, we employ an effective supervised lexical knowledge denoising method to smooth out matching noise. Finally, we introduce a col-wise attention based knowledge fusion mechanism to guarantee the pluggability of the proposed framework. Experiments on ten datasets of three tasks show that the proposed framework achieves new SOTA, even with very large scale lexicons."
2021.emnlp-main.256,Neural Machine Translation with Heterogeneous Topic Knowledge Embeddings,2021,-1,-1,4,0,9171,weixuan wang,Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,0,"Neural Machine Translation (NMT) has shown a strong ability to utilize local context to disambiguate the meaning of words. However, it remains a challenge for NMT to leverage broader context information like topics. In this paper, we propose heterogeneous ways of embedding topic information at the sentence level into an NMT model to improve translation performance. Specifically, the topic information can be incorporated as pre-encoder topic embedding, post-encoder topic embedding, and decoder topic embedding to increase the likelihood of selecting target words from the same topic of the source sentence. Experimental results show that NMT models with the proposed topic knowledge embedding outperform the baselines on the English -{\textgreater} German and English -{\textgreater} French translation tasks."
2021.emnlp-main.267,Self-Supervised Quality Estimation for Machine Translation,2021,-1,-1,7,0,8955,yuanhang zheng,Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,0,"Quality estimation (QE) of machine translation (MT) aims to evaluate the quality of machine-translated sentences without references and is important in practical applications of MT. Training QE models require massive parallel data with hand-crafted quality annotations, which are time-consuming and labor-intensive to obtain. To address the issue of the absence of annotated training data, previous studies attempt to develop unsupervised QE methods. However, very few of them can be applied to both sentence- and word-level QE tasks, and they may suffer from noises in the synthetic data. To reduce the negative impact of noises, we propose a self-supervised method for both sentence- and word-level QE, which performs quality estimation by recovering the masked target words. Experimental results show that our method outperforms previous unsupervised methods on several QE tasks in different language pairs and domains."
2021.emnlp-main.306,{C}hinese {WPLC}: A {C}hinese Dataset for Evaluating Pretrained Language Models on Word Prediction Given Long-Range Context,2021,-1,-1,4,0,9335,huibin ge,Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,0,"This paper presents a Chinese dataset for evaluating pretrained language models on Word Prediction given Long-term Context (Chinese WPLC). We propose both automatic and manual selection strategies tailored to Chinese to guarantee that target words in passages collected from over 69K novels can only be predicted with long-term context beyond the scope of sentences containing the target words. Dataset analysis reveals that the types of target words range from common nouns to Chinese 4-character idioms. We also observe that linguistic relations between target words and long-range context exhibit diversity, including lexical match, synonym, summary and reasoning. Experiment results show that the Chinese pretrained language model PanGu-$\alpha$ is 45 points behind human in terms of top-1 word prediction accuracy, indicating that Chinese WPLC is a challenging dataset. The dataset is publicly available at https://git.openi.org.cn/PCL-Platform.Intelligence/Chinese{\_}WPLC."
2021.emnlp-main.340,Improving Unsupervised Question Answering via Summarization-Informed Question Generation,2021,-1,-1,6,0,9402,chenyang lyu,Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,0,"Question Generation (QG) is the task of generating a plausible question for a given {\textless}passage, answer{\textgreater} pair. Template-based QG uses linguistically-informed heuristics to transform declarative sentences into interrogatives, whereas supervised QG uses existing Question Answering (QA) datasets to train a system to generate a question given a passage and an answer. A disadvantage of the heuristic approach is that the generated questions are heavily tied to their declarative counterparts. A disadvantage of the supervised approach is that they are heavily tied to the domain/language of the QA dataset used as training data. In order to overcome these shortcomings, we propose a distantly-supervised QG method which uses questions generated heuristically from summaries as a source of training data for a QG system. We make use of freely available news summary data, transforming declarative summary sentences into appropriate questions using heuristics informed by dependency parsing, named entity recognition and semantic role labeling. The resulting questions are then combined with the original news articles to train an end-to-end neural QG model. We extrinsically evaluate our approach using unsupervised QA: our QG model is used to generate synthetic QA pairs for training a QA model. Experimental results show that, trained with only 20k English Wikipedia-based synthetic QA pairs, the QA model substantially outperforms previous unsupervised models on three in-domain datasets (SQuAD1.1, Natural Questions, TriviaQA) and three out-of-domain datasets (NewsQA, BioASQ, DuoRC), demonstrating the transferability of the approach."
2021.emnlp-main.580,Uncertainty-Aware Balancing for Multilingual and Multi-Domain Neural Machine Translation Training,2021,-1,-1,6,0,9816,minghao wu,Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,0,"Learning multilingual and multi-domain translation model is challenging as the heterogeneous and imbalanced data make the model converge inconsistently over different corpora in real world. One common practice is to adjust the share of each corpus in the training, so that the learning process is balanced and low-resource cases can benefit from the high resource ones. However, automatic balancing methods usually depend on the intra- and inter-dataset characteristics, which is usually agnostic or requires human priors. In this work, we propose an approach, MultiUAT, that dynamically adjusts the training data usage based on the model{'}s uncertainty on a small set of trusted clean data for multi-corpus machine translation. We experiments with two classes of uncertainty measures on multilingual (16 languages with 4 settings) and multi-domain settings (4 for in-domain and 2 for out-of-domain on English-German translation) and demonstrate our approach MultiUAT substantially outperforms its baselines, including both static and dynamic strategies. We analyze the cross-domain transfer and show the deficiency of static and similarity based methods."
2021.emnlp-main.663,Document Graph for Neural Machine Translation,2021,-1,-1,4,0,9976,mingzhou xu,Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,0,"Previous works have shown that contextual information can improve the performance of neural machine translation (NMT). However, most existing document-level NMT methods failed to leverage contexts beyond a few set of previous sentences. How to make use of the whole document as global contexts is still a challenge. To address this issue, we hypothesize that a document can be represented as a graph that connects relevant contexts regardless of their distances. We employ several types of relations, including adjacency, syntactic dependency, lexical consistency, and coreference, to construct the document graph. Then, we incorporate both source and target graphs into the conventional Transformer architecture with graph convolutional networks. Experiments on various NMT benchmarks, including IWSLT English{--}French, Chinese-English, WMT English{--}German and Opensubtitle English{--}Russian, demonstrate that using document graphs can significantly improve the translation quality. Extensive analysis verifies that the document graph is beneficial for capturing discourse phenomena."
2021.acl-long.318,A Mutual Information Maximization Approach for the Spurious Solution Problem in Weakly Supervised Question Answering,2021,-1,-1,3,0,13156,zhihong shao,Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),1,"Weakly supervised question answering usually has only the final answers as supervision signals while the correct solutions to derive the answers are not provided. This setting gives rise to the spurious solution problem: there may exist many spurious solutions that coincidentally derive the correct answer, but training on such solutions can hurt model performance (e.g., producing wrong solutions or answers). For example, for discrete reasoning tasks as on DROP, there may exist many equations to derive a numeric answer, and typically only one of them is correct. Previous learning methods mostly filter out spurious solutions with heuristics or using model confidence, but do not explicitly exploit the semantic correlations between a question and its solution. In this paper, to alleviate the spurious solution problem, we propose to explicitly exploit such semantic correlations by maximizing the mutual information between question-answer pairs and predicted solutions. Extensive experiments on four question answering datasets show that our method significantly outperforms previous learning methods in terms of task performance and is more effective in training models to produce correct solutions."
2021.acl-long.334,{B}inary{BERT}: Pushing the Limit of {BERT} Quantization,2021,-1,-1,7,0,13199,haoli bai,Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),1,"The rapid development of large pre-trained language models has greatly increased the demand for model compression techniques, among which quantization is a popular solution. In this paper, we propose BinaryBERT, which pushes BERT quantization to the limit by weight binarization. We find that a binary BERT is hard to be trained directly than a ternary counterpart due to its complex and irregular loss landscape. Therefore, we propose ternary weight splitting, which initializes BinaryBERT by equivalently splitting from a half-sized ternary network. The binary model thus inherits the good performance of the ternary one, and can be further enhanced by fine-tuning the new architecture after splitting. Empirical results show that our BinaryBERT has only a slight performance drop compared with the full-precision model while being 24x smaller, achieving the state-of-the-art compression results on the GLUE and SQuAD benchmarks. Code will be released."
2021.acl-long.400,{A}uto{T}iny{BERT}: Automatic Hyper-parameter Optimization for Efficient Pre-trained Language Models,2021,-1,-1,6,1,6912,yichun yin,Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),1,"Pre-trained language models (PLMs) have achieved great success in natural language processing. Most of PLMs follow the default setting of architecture hyper-parameters (e.g., the hidden dimension is a quarter of the intermediate dimension in feed-forward sub-networks) in BERT. Few studies have been conducted to explore the design of architecture hyper-parameters in BERT, especially for the more efficient PLMs with tiny sizes, which are essential for practical deployment on resource-constrained devices. In this paper, we adopt the one-shot Neural Architecture Search (NAS) to automatically search architecture hyper-parameters. Specifically, we carefully design the techniques of one-shot learning and the search space to provide an adaptive and efficient development way of tiny PLMs for various latency constraints. We name our method AutoTinyBERT and evaluate its effectiveness on the GLUE and SQuAD benchmarks. The extensive experiments show that our method outperforms both the SOTA search-based baseline (NAS-BERT) and the SOTA distillation-based methods (such as DistilBERT, TinyBERT, MiniLM, and MobileBERT). In addition, based on the obtained architectures, we propose a more efficient development method that is even faster than the development of a single PLM. The source code and models will be publicly available upon publication."
2021.acl-long.469,{TGEA}: An Error-Annotated Dataset and Benchmark Tasks for {T}ext{G}eneration from Pretrained Language Models,2021,-1,-1,4,0,13370,jie he,Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),1,"In order to deeply understand the capability of pretrained language models in text generation and conduct a diagnostic evaluation, we propose TGEA, an error-annotated dataset with multiple benchmark tasks for text generation from pretrained language models (PLMs). We use carefully selected prompt words to guide GPT-2 to generate candidate sentences, from which we select 47K for error annotation. Crowdsourced workers manually check each of these sentences and detect 12k erroneous sentences. We create an error taxonomy to cover 24 types of errors occurring in these erroneous sentences according to the nature of errors with respect to linguistics and knowledge (e.g., common sense). For each erroneous span in PLM-generated sentences, we also detect another span that is closely associated with it. Each error is hence manually labeled with comprehensive annotations, including the span of the error, the associated span, minimal correction to the error, the type of the error, and rationale behind the error. Apart from the fully annotated dataset, we also present a detailed description of the data collection procedure, statistics and analysis of the dataset. This is the first dataset with comprehensive annotations for PLM-generated texts, which facilitates the diagnostic evaluation of PLM-based text generation. Furthermore, we use TGEA as a benchmark dataset and propose a series of automatic diagnosis tasks, including error detection, error type classification, associated span detection, error rationale generation, to further promote future study on the automatic error detection and correction on texts generated by pretrained language models."
2021.acl-long.509,{G}host{BERT}: Generate More Features with Cheap Operations for {BERT},2021,-1,-1,6,0,13433,zhiqi huang,Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),1,"Transformer-based pre-trained language models like BERT, though powerful in many tasks, are expensive in both memory and computation, due to their large number of parameters. Previous works show that some parameters in these models can be pruned away without severe accuracy drop. However, these redundant features contribute to a comprehensive understanding of the training data and removing them weakens the model{'}s representation ability. In this paper, we propose GhostBERT, which generates more features with very cheap operations from the remaining features. In this way, GhostBERT has similar memory and computational cost as the pruned model, but enjoys much larger representation power. The proposed ghost module can also be applied to unpruned BERT models to enhance their performance with negligible additional parameters and computation. Empirical results on the GLUE benchmark on three backbone models (i.e., BERT, RoBERTa and ELECTRA) verify the efficacy of our proposed method."
2020.wmt-1.93,Huawei{'}s Submissions to the {WMT}20 Biomedical Translation Task,2020,-1,-1,7,1,1682,wei peng,Proceedings of the Fifth Conference on Machine Translation,0,"This paper describes Huawei{'}s submissions to the WMT20 biomedical translation shared task. Apart from experimenting with finetuning on domain-specific bitexts, we explore effects of in-domain dictionaries on enhancing cross-domain neural machine translation performance. We utilize a transfer learning strategy through pre-trained machine translation models and extensive scope of engineering endeavors. Four of our ten submissions achieve state-of-the-art performance according to the official automatic evaluation results, namely translation directions on English{\textless}-{\textgreater}French, English-{\textgreater}German and English-{\textgreater}Italian."
2020.findings-emnlp.104,{H}yper{T}ext: Endowing {F}ast{T}ext with Hyperbolic Geometry,2020,-1,-1,6,0,19518,yudong zhu,Findings of the Association for Computational Linguistics: EMNLP 2020,0,"Natural language data exhibit tree-like hierarchical structures such as the hypernym-hyponym hierarchy in WordNet. FastText, as the state-of-the-art text classifier based on shallow neural network in Euclidean space, may not represent such hierarchies precisely with limited representation capacity. Considering that hyperbolic space is naturally suitable for modelling tree-like hierarchical data, we propose a new model named HyperText for efficient text classification by endowing FastText with hyperbolic geometry. Empirically, we show that HyperText outperforms FastText on a range of text classification tasks with much reduced parameters."
2020.findings-emnlp.207,{BERT}-{MK}: Integrating Graph Contextualized Knowledge into Pre-trained Language Models,2020,-1,-1,5,0,3750,bin he,Findings of the Association for Computational Linguistics: EMNLP 2020,0,"Complex node interactions are common in knowledge graphs (KGs), and these interactions can be considered as contextualized knowledge exists in the topological structure of KGs. Traditional knowledge representation learning (KRL) methods usually treat a single triple as a training unit, neglecting the usage of graph contextualized knowledge. To utilize these unexploited graph-level knowledge, we propose an approach to model subgraphs in a medical KG. Then, the learned knowledge is integrated with a pre-trained language model to do the knowledge generalization. Experimental results demonstrate that our model achieves the state-of-the-art performance on several medical NLP tasks, and the improvement above MedERNIE indicates that graph contextualized knowledge is beneficial."
2020.findings-emnlp.327,The Box is in the Pen: Evaluating Commonsense Reasoning in Neural Machine Translation,2020,-1,-1,4,0,13370,jie he,Findings of the Association for Computational Linguistics: EMNLP 2020,0,"Does neural machine translation yield translations that are congenial with common sense? In this paper, we present a test suite to evaluate the commonsense reasoning capability of neural machine translation. The test suite consists of three test sets, covering lexical and contextless/contextual syntactic ambiguity that requires commonsense knowledge to resolve. We manually create 1,200 triples, each of which contain a source sentence and two contrastive translations, involving 7 different common sense types. Language models pretrained on large-scale corpora, such as BERT, GPT-2, achieve a commonsense reasoning accuracy of lower than 72{\%} on target translations of this test suite. We conduct extensive experiments on the test suite to evaluate commonsense reasoning in neural machine translation and investigate factors that have impact on this capability. Our experiments and analyses demonstrate that neural machine translation performs poorly on commonsense reasoning of the three ambiguity types in terms of both reasoning accuracy ( 6 60.1{\%}) and reasoning consistency (6 31{\%}). We will release our test suite as a machine translation commonsense reasoning testbed to promote future work in this direction."
2020.findings-emnlp.372,{T}iny{BERT}: Distilling {BERT} for Natural Language Understanding,2020,-1,-1,8,0,19890,xiaoqi jiao,Findings of the Association for Computational Linguistics: EMNLP 2020,0,"Language model pre-training, such as BERT, has significantly improved the performances of many natural language processing tasks. However, pre-trained language models are usually computationally expensive, so it is difficult to efficiently execute them on resource-restricted devices. To accelerate inference and reduce model size while maintaining accuracy, we first propose a novel Transformer distillation method that is specially designed for knowledge distillation (KD) of the Transformer-based models. By leveraging this new KD method, the plenty of knowledge encoded in a large {``}teacher{''} BERT can be effectively transferred to a small {``}student{''} TinyBERT. Then, we introduce a new two-stage learning framework for TinyBERT, which performs Transformer distillation at both the pre-training and task-specific learning stages. This framework ensures that TinyBERT can capture the general-domain as well as the task-specific knowledge in BERT. TinyBERT4 with 4 layers is empirically effective and achieves more than 96.8{\%} the performance of its teacher BERT-Base on GLUE benchmark, while being 7.5x smaller and 9.4x faster on inference. TinyBERT4 is also significantly better than 4-layer state-of-the-art baselines on BERT distillation, with only {\textasciitilde}28{\%} parameters and {\textasciitilde}31{\%} inference time of them. Moreover, TinyBERT6 with 6 layers performs on-par with its teacher BERT-Base."
2020.emnlp-main.37,{T}ernary{BERT}: Distillation-aware Ultra-low Bit {BERT},2020,-1,-1,7,0.241018,8103,wei zhang,Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),0,"Transformer-based pre-training models like BERT have achieved remarkable performance in many natural language processing tasks. However, these models are both computation and memory expensive, hindering their deployment to resource-constrained devices. In this work, we propose TernaryBERT, which ternarizes the weights in a fine-tuned BERT model. Specifically, we use both approximation-based and loss-aware ternarization methods and empirically investigate the ternarization granularity of different parts of BERT. Moreover, to reduce the accuracy degradation caused by lower capacity of low bits, we leverage the knowledge distillation technique in the training process. Experiments on the GLUE benchmark and SQuAD show that our proposed TernaryBERT outperforms the other BERT quantization methods, and even achieves comparable performance as the full-precision model while being 14.9x smaller."
2020.emnlp-main.42,Accurate Word Alignment Induction from Neural Machine Translation,2020,29,0,5,0,8634,yun chen,Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),0,"Despite its original goal to jointly learn to align and translate, prior researches suggest that Transformer captures poor word alignments through its attention mechanism. In this paper, we show that attention weights do capture accurate word alignments and propose two novel word alignment induction methods Shift-Att and Shift-AET. The main idea is to induce alignments at the step when the to-be-aligned target token is the decoder input rather than the decoder output as in previous work. Shift-Att is an interpretation method that induces alignments from the attention weights of Transformer and does not require parameter update or architecture change. Shift-AET extracts alignments from an additional alignment module which is tightly integrated into Transformer and trained in isolation with supervision from symmetrized Shift-Att alignments. Experiments on three publicly available datasets demonstrate that both methods perform better than their corresponding neural baselines and Shift-AET significantly outperforms GIZA++ by 1.4-4.8 AER points."
2020.emnlp-main.74,Why Skip If You Can Combine: A Simple Knowledge Distillation Technique for Intermediate Layers,2020,-1,-1,4,0,9852,yimeng wu,Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),0,"With the growth of computing power neural machine translation (NMT) models also grow accordingly and become better. However, they also become harder to deploy on edge devices due to memory constraints. To cope with this problem, a common practice is to distill knowledge from a large and accurately-trained teacher network (T) into a compact student network (S). Although knowledge distillation (KD) is useful in most cases, our study shows that existing KD techniques might not be suitable enough for deep NMT engines, so we propose a novel alternative. In our model, besides matching T and S predictions we have a combinatorial mechanism to inject layer-level supervision from T to S. In this paper, we target low-resource settings and evaluate our translation engines for PortugueseâEnglish, TurkishâEnglish, and EnglishâGerman directions. Students trained using our technique have 50{\%} fewer parameters and can still deliver comparable results to those of 12-layer teachers."
2020.acl-main.24,Probabilistically Masked Language Model Capable of Autoregressive Generation in Arbitrary Word Order,2020,24,0,3,0.952381,13371,yi liao,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,1,"Masked language model and autoregressive language model are two types of language models. While pretrained masked language models such as BERT overwhelm the line of natural language understanding (NLU) tasks, autoregressive language models such as GPT are especially capable in natural language generation (NLG). In this paper, we propose a probabilistic masking scheme for the masked language model, which we call probabilistically masked language model (PMLM). We implement a specific PMLM with a uniform prior distribution on the masking ratio named u-PMLM. We prove that u-PMLM is equivalent to an autoregressive permutated language model. One main advantage of the model is that it supports text generation in arbitrary order with surprisingly good quality, which could potentially enable new applications over traditional unidirectional generation. Besides, the pretrained u-PMLM also outperforms BERT on a bunch of downstream NLU tasks."
2020.acl-main.383,Perturbed Masking: Parameter-free Probing for Analyzing and Interpreting {BERT},2020,52,0,4,0,9177,zhiyong wu,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,1,"By introducing a small set of additional parameters, a \textit{probe} learns to solve specific linguistic tasks (e.g., dependency parsing) in a supervised manner using feature representations (e.g., contextualized embeddings). The effectiveness of such \textit{probing} tasks is taken as evidence that the pre-trained model encodes linguistic knowledge. However, this approach of evaluating a language model is undermined by the uncertainty of the amount of knowledge that is learned by the probe itself. Complementary to those works, we propose a parameter-free probing technique for analyzing pre-trained language models (e.g., BERT). Our method does not require direct supervision from the probing tasks, nor do we introduce additional parameters to the probing process. Our experiments on BERT show that syntactic trees recovered from BERT using our method are significantly better than linguistically-uninformed baselines. We further feed the empirically induced dependency structures into a downstream sentiment classification task and find its improvement compatible with or even superior to a human-designed dependency schema."
2020.acl-main.540,Word-level Textual Adversarial Attacking as Combinatorial Optimization,2020,-1,-1,6,0,13631,yuan zang,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,1,"Adversarial attacks are carried out to reveal the vulnerability of deep neural networks. Textual adversarial attacking is challenging because text is discrete and a small perturbation can bring significant change to the original input. Word-level attacking, which can be regarded as a combinatorial optimization problem, is a well-studied class of textual attack methods. However, existing word-level attack models are far from perfect, largely because unsuitable search space reduction methods and inefficient optimization algorithms are employed. In this paper, we propose a novel attack model, which incorporates the sememe-based word substitution method and particle swarm optimization-based search algorithm to solve the two problems separately. We conduct exhaustive experiments to evaluate our attack model by attacking BiLSTM and BERT on three benchmark datasets. Experimental results demonstrate that our model consistently achieves much higher attack success rates and crafts more high-quality adversarial examples as compared to baseline methods. Also, further experiments show our model has higher transferability and can bring more robustness enhancement to victim models by adversarial training. All the code and data of this paper can be obtained on https://github.com/thunlp/SememePSO-Attack."
2020.aacl-main.23,A General Framework for Adaptation of Neural Machine Translation to Simultaneous Translation,2020,-1,-1,5,0,8634,yun chen,Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing,0,"Despite the success of neural machine translation (NMT), simultaneous neural machine translation (SNMT), the task of translating in real time before a full sentence has been observed, remains challenging due to the syntactic structure difference and simultaneity requirements. In this paper, we propose a general framework for adapting neural machine translation to translate simultaneously. Our framework contains two parts: prefix translation that utilizes a consecutive NMT model to translate source prefixes and a stopping criterion that determines when to stop the prefix translation. Experiments on three translation corpora and two language pairs show the efficacy of the proposed framework on balancing the quality and latency in adapting NMT to perform simultaneous translation."
W19-5420,Huawei{'}s {NMT} Systems for the {WMT} 2019 Biomedical Translation Task,2019,0,0,4,1,1682,wei peng,"Proceedings of the Fourth Conference on Machine Translation (Volume 3: Shared Task Papers, Day 2)",0,"This paper describes Huawei{'}s neural machine translation systems for the WMT 2019 biomedical translation shared task. We trained and fine-tuned our systems on a combination of out-of-domain and in-domain parallel corpora for six translation directions covering English{--}Chinese, English{--}French and English{--}German language pairs. Our submitted systems achieve the best BLEU scores on English{--}French and English{--}German language pairs according to the official evaluation results. In the English{--}Chinese translation task, our systems are in the second place. The enhanced performance is attributed to more in-domain training and more sophisticated models developed. Development of translation models and transfer learning (or domain adaptation) methods has significantly contributed to the progress of the task."
W19-2307,Bilingual-{GAN}: A Step Towards Parallel Text Generation,2019,32,3,4,0,208,ahmad rashid,Proceedings of the Workshop on Methods for Optimizing and Evaluating Neural Language Generation,0,"Latent space based GAN methods and attention based sequence to sequence models have achieved impressive results in text generation and unsupervised machine translation respectively. Leveraging the two domains, we propose an adversarial latent space based model capable of generating parallel sentences in two languages concurrently and translating bidirectionally. The bilingual generation goal is achieved by sampling from the latent space that is shared between both languages. First two denoising autoencoders are trained, with shared encoders and back-translation to enforce a shared latent state between the two languages. The decoder is shared for the two translation directions. Next, a GAN is trained to generate synthetic {`}code{'} mimicking the languages{'} shared latent space. This code is then fed into the decoder to generate text in either language. We perform our experiments on Europarl and Multi30k datasets, on the English-French language pair, and document our performance using both supervised and unsupervised machine translation."
P19-1139,{ERNIE}: Enhanced Language Representation with Informative Entities,2019,54,21,6,0,7834,zhengyan zhang,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,1,"Neural language representation models such as BERT pre-trained on large-scale corpora can well capture rich semantic patterns from plain text, and be fine-tuned to consistently improve the performance of various NLP tasks. However, the existing pre-trained language models rarely consider incorporating knowledge graphs (KGs), which can provide rich structured knowledge facts for better language understanding. We argue that informative entities in KGs can enhance language representation with external knowledge. In this paper, we utilize both large-scale textual corpora and KGs to train an enhanced language representation model (ERNIE), which can take full advantage of lexical, syntactic, and knowledge information simultaneously. The experimental results have demonstrated that ERNIE achieves significant improvements on various knowledge-driven tasks, and meanwhile is comparable with the state-of-the-art model BERT on other common NLP tasks. The code and datasets will be available in the future."
P19-1332,Decomposable Neural Paraphrase Generation,2019,0,5,4,0,6487,zichao li,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,1,"Paraphrasing exists at different granularity levels, such as lexical level, phrasal level and sentential level. This paper presents Decomposable Neural Paraphrase Generator (DNPG), a Transformer-based model that can learn and generate paraphrases of a sentence at different levels of granularity in a disentangled way. Specifically, the model is composed of multiple encoders and decoders with different structures, each of which corresponds to a specific granularity. The empirical study shows that the decomposition mechanism of DNPG makes paraphrase generation more interpretable and controllable. Based on DNPG, we further develop an unsupervised domain adaptation method for paraphrase generation. Experimental results show that the proposed model achieves competitive in-domain performance compared to state-of-the-art neural models, and significantly better performance when adapting to a new domain."
P19-1426,Bridging the Gap between Training and Inference for Neural Machine Translation,2019,19,8,5,1,21362,wen zhang,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,1,"Neural Machine Translation (NMT) generates target words sequentially in the way of predicting the next word conditioned on the context words. At training time, it predicts with the ground truth words as context while at inference it has to generate the entire sequence from scratch. This discrepancy of the fed context leads to error accumulation among the way. Furthermore, word-level training requires strict matching between the generated sequence and the ground truth sequence which leads to overcorrection over different but reasonable translations. In this paper, we address these issues by sampling context words not only from the ground truth sequence but also from the predicted sequence by the model during training, where the predicted sequence is selected with a sentence-level optimum. Experiment results on Chinese-{\textgreater}English and WMT{'}14 English-{\textgreater}German translation tasks demonstrate that our approach can achieve significant improvements on multiple datasets."
P19-1571,Modeling Semantic Compositionality with Sememe Knowledge,2019,40,1,6,0.833333,7835,fanchao qi,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,1,"Semantic compositionality (SC) refers to the phenomenon that the meaning of a complex linguistic unit can be composed of the meanings of its constituents. Most related works focus on using complicated compositionality functions to model SC while few works consider external knowledge in models. In this paper, we verify the effectiveness of sememes, the minimum semantic units of human languages, in modeling SC by a confirmatory experiment. Furthermore, we make the first attempt to incorporate sememe knowledge into SC models, and employ the sememe-incorporated models in learning representations of multiword expressions, a typical task of SC. In experiments, we implement our models by incorporating knowledge from a famous sememe knowledge base HowNet and perform both intrinsic and extrinsic evaluations. Experimental results show that our models achieve significant performance boost as compared to the baseline methods without considering sememe knowledge. We further conduct quantitative analysis and case studies to demonstrate the effectiveness of applying sememe knowledge in modeling SC.All the code and data of this paper can be obtained on https://github.com/thunlp/Sememe-SC."
N19-1312,Improving Domain Adaptation Translation with Domain Invariant and Specific Information,2019,0,6,3,0,4168,shuhao gu,"Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",0,"In domain adaptation for neural machine translation, translation performance can benefit from separating features into domain-specific features and common features. In this paper, we propose a method to explicitly model the two kinds of information in the encoder-decoder framework so as to exploit out-of-domain data in in-domain training. In our method, we maintain a private encoder and a private decoder for each domain which are used to model domain-specific information. In the meantime, we introduce a common encoder and a common decoder shared by all the domains which can only have domain-independent information flow through. Besides, we add a discriminator to the shared encoder and employ adversarial training for the whole model to reinforce the performance of information separation and machine translation simultaneously. Experiment results show that our method can outperform competitive baselines greatly on multiple data sets."
W18-6556,{E}2{E} {NLG} Challenge Submission: Towards Controllable Generation of Diverse Natural Language,2018,0,6,4,0,16529,henry elder,Proceedings of the 11th International Conference on Natural Language Generation,0,"In natural language generation (NLG), the task is to generate utterances from a more abstract input, such as structured data. An added challenge is to generate utterances that contain an accurate representation of the input, while reflecting the fluency and variety of human-generated text. In this paper, we report experiments with NLG models that can be used in task oriented dialogue systems. We explore the use of additional input to the model to encourage diversity and control of outputs. While our submission does not rank highly using automated metrics, qualitative investigation of generated utterances suggests the use of additional information in neural network NLG systems to be a promising research direction."
W18-3405,Multimodal Neural Machine Translation for Low-resource Language Pairs using Synthetic Data,2018,0,1,3,1,9996,koel chowdhury,Proceedings of the Workshop on Deep Learning Approaches for Low-Resource {NLP},0,"In this paper, we investigate the effectiveness of training a multimodal neural machine translation (MNMT) system with image features for a low-resource language pair, Hindi and English, using synthetic data. A three-way parallel corpus which contains bilingual texts and corresponding images is required to train a MNMT system with image features. However, such a corpus is not available for low resource language pairs. To address this, we developed both a synthetic training dataset and a manually curated development/test dataset for Hindi based on an existing English-image parallel corpus. We used these datasets to build our image description translation system by adopting state-of-the-art MNMT models. Our results show that it is possible to train a MNMT system for low-resource language pairs through the use of synthetic data and that such a system can benefit from image features."
P18-1138,Knowledge Diffusion for Neural Dialogue Generation,2018,0,41,5,0,29157,shuman liu,Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"End-to-end neural dialogue generation has shown promising results recently, but it does not employ knowledge to guide the generation and hence tends to generate short, general, and meaningless responses. In this paper, we propose a neural knowledge diffusion (NKD) model to introduce knowledge into dialogue generation. This method can not only match the relevant facts for the input utterance but diffuse them to similar entities. With the help of facts matching and entity diffusion, the neural dialogue generation is augmented with the ability of convergent and divergent thinking over the knowledge base. Our empirical study on a real-world dataset prove that our model is capable of generating meaningful, diverse and natural responses for both factoid-questions and knowledge grounded chi-chats. The experiment results also show that our model outperforms competitive baseline models significantly."
N18-1006,Improving Character-Based Decoding Using Target-Side Morphological Information for Neural Machine Translation,2018,19,2,2,1,7213,peyman passban,"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",0,"Recently, neural machine translation (NMT) has emerged as a powerful alternative to conventional statistical approaches. However, its performance drops considerably in the presence of morphologically rich languages (MRLs). Neural engines usually fail to tackle the large vocabulary and high out-of-vocabulary (OOV) word rate of MRLs. Therefore, it is not suitable to exploit existing word-based models to translate this set of languages. In this paper, we propose an extension to the state-of-the-art model of Chung et al. (2016), which works at the character level and boosts the decoder with target-side morphological information. In our architecture, an additional morphology table is plugged into the model. Each time the decoder samples from a target vocabulary, the table sends auxiliary signals from the most relevant affixes in order to enrich the decoder{'}s current state and constrain it to provide better predictions. We evaluated our model to translate English into German, Russian, and Turkish as three MRLs and observed significant improvements."
D18-1333,Learning to Jointly Translate and Predict Dropped Pronouns with a Shared Reconstruction Mechanism,2018,12,2,4,1,7026,longyue wang,Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,0,"Pronouns are frequently omitted in pro-drop languages, such as Chinese, generally leading to significant challenges with respect to the production of complete translations. Recently, Wang et al. (2018) proposed a novel reconstruction-based approach to alleviating dropped pronoun (DP) translation problems for neural machine translation models. In this work, we improve the original model from two perspectives. First, we employ a shared reconstructor to better exploit encoder and decoder representations. Second, we jointly learn to translate and predict DPs in an end-to-end manner, to avoid the errors propagated from an external DP prediction model. Experimental results show that our approach significantly improves both translation performance and DP prediction accuracy."
D18-1460,Speeding Up Neural Machine Translation Decoding by Cube Pruning,2018,19,0,5,1,21362,wen zhang,Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,0,"Although neural machine translation has achieved promising results, it suffers from slow translation speed. The direct consequence is that a trade-off has to be made between translation quality and speed, thus its performance can not come into full play. We apply cube pruning, a popular technique to speed up dynamic programming, into neural machine translation to speed up the translation. To construct the equivalence class, similar target hidden states are combined, leading to less RNN expansion operations on the target side and less softmax operations over the large target vocabulary. The experiments show that, at the same or even better translation quality, our method can translate faster compared with naive beam search by 3.3x on GPUs and 3.5x on CPUs."
C18-1110,Refining Source Representations with Relation Networks for Neural Machine Translation,2018,0,2,4,1,21362,wen zhang,Proceedings of the 27th International Conference on Computational Linguistics,0,"Although neural machine translation with the encoder-decoder framework has achieved great success recently, it still suffers drawbacks of forgetting distant information, which is an inherent disadvantage of recurrent neural network structure, and disregarding relationship between source words during encoding step. Whereas in practice, the former information and relationship are often useful in current step. We target on solving these problems and thus introduce relation networks to learn better representations of the source. The relation networks are able to facilitate memorization capability of recurrent neural network via associating source words with each other, this would also help retain their relationships. Then the source representations and all the relations are fed into the attention component together while decoding, with the main encoder-decoder framework unchanged. Experiments on several datasets show that our method can improve the translation performance significantly over the conventional encoder-decoder model and even outperform the approach involving supervised syntactic knowledge."
C18-1265,Tailoring Neural Architectures for Translating from Morphologically Rich Languages,2018,0,2,3,1,7213,peyman passban,Proceedings of the 27th International Conference on Computational Linguistics,0,"A morphologically complex word (MCW) is a hierarchical constituent with meaning-preserving subunits, so word-based models which rely on surface forms might not be powerful enough to translate such structures. When translating from morphologically rich languages (MRLs), a source word could be mapped to several words or even a full sentence on the target side, which means an MCW should not be treated as an atomic unit. In order to provide better translations for MRLs, we boost the existing neural machine translation (NMT) architecture with a double- channel encoder and a double-attentive decoder. The main goal targeted in this research is to provide richer information on the encoder side and redesign the decoder accordingly to benefit from such information. Our experimental results demonstrate that we could achieve our goal as the proposed model outperforms existing subword- and character-based architectures and showed significant improvements on translating from German, Russian, and Turkish into English."
W17-4717,Findings of the 2017 Conference on Machine Translation ({WMT}17),2017,0,109,9,0,292,ondvrej bojar,Proceedings of the Second Conference on Machine Translation,0,"This paper presents the results of the WMT17 shared tasks, which includedn three machine translation (MT) tasks (news, biomedical, and multimodal), two evaluation tasks (metrics and run-time estimation of MT quality), an automatic post-editing task, a neural MT training task, and a bandit learning task."
W17-4745,{CASICT}-{DCU} Neural Machine Translation Systems for {WMT}17,2017,0,8,5,0,6813,jinchao zhang,Proceedings of the Second Conference on Machine Translation,0,None
W17-4747,{DCU} System Report on the {WMT} 2017 Multi-modal Machine Translation Task,2017,0,1,3,0,4092,iacer calixto,Proceedings of the Second Conference on Machine Translation,0,None
W17-4768,{B}lend: a Novel Combined {MT} Metric Based on Direct Assessment {---} {CASICT}-{DCU} submission to {WMT}17 Metrics Task,2017,19,11,4,1,13912,qingsong ma,Proceedings of the Second Conference on Machine Translation,0,None
W17-1715,Detection of Verbal Multi-Word Expressions via Conditional Random Fields with Syntactic Dependency Features and Semantic Re-Ranking,2017,23,3,7,0,17866,alfredo maldonado,Proceedings of the 13th Workshop on Multiword Expressions ({MWE} 2017),0,"A description of a system for identifying Verbal Multi-Word Expressions (VMWEs) in running text is presented. The system mainly exploits universal syntactic dependency features through a Conditional Random Fields (CRF) sequence model. The system competed in the Closed Track at the PARSEME VMWE Shared Task 2017, ranking 2nd place in most languages on full VMWE-based evaluation and 1st in three languages on token-based evaluation. In addition, this paper presents an option to re-rank the 10 best CRF-predicted sequences via semantic vectors, boosting its scores above other systems in the competition. We also show that all systems in the competition would struggle to beat a simple lookup baseline system and argue for a more purpose-specific evaluation scheme."
calixto-liu-2017-sentence,Sentence-Level Multilingual Multi-modal Embedding for Natural Language Processing,2017,15,3,2,0,4092,iacer calixto,"Proceedings of the International Conference Recent Advances in Natural Language Processing, {RANLP} 2017",0,"We propose a novel discriminative ranking model that learns embeddings from multilingual and multi-modal data, meaning that our model can take advantage of images and descriptions in multiple languages to improve embedding quality. To that end, we introduce an objective function that uses pairwise ranking adapted to the case of three or more input sources. We compare our model against different baselines, and evaluate the robustness of our embeddings on image{--}sentence ranking (ISR), semantic textual similarity (STS), and neural machine translation (NMT). We find that the additional multilingual signals lead to improvements on all three tasks, and we highlight that our model can be used to consistently improve the adequacy of translations generated with NMT models when re-ranking n-best lists."
P17-1013,Deep Neural Machine Translation with Linear Associative Unit,2017,24,9,4,1,4605,mingxuan wang,Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"Deep Neural Networks (DNNs) have provably enhanced the state-of-the-art Neural Machine Translation (NMT) with its capability in modeling complex functions and capturing complex linguistic structures. However NMT with deep architecture in its encoder or decoder RNNs often suffer from severe gradient diffusion due to the non-linear recurrent activations, which often makes the optimization much more difficult. To address this problem we propose a novel linear associative units (LAU) to reduce the gradient propagation path inside the recurrent unit. Different from conventional approaches (LSTM unit and GRU), LAUs uses linear associative connections between input and output of the recurrent unit, which allows unimpeded information flow through both space and time The model is quite simple, but it is surprisingly effective. Our empirical study on Chinese-English translation shows that our model with proper configuration can improve by 11.7 BLEU upon Groundhog and the best reported on results in the same setting. On WMT14 English-German task and a larger WMT14 English-French task, our model achieves comparable results with the state-of-the-art."
P17-1140,Incorporating Word Reordering Knowledge into Attention-based Neural Machine Translation,2017,6,13,3,0,6813,jinchao zhang,Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"This paper proposes three distortion models to explicitly incorporate the word reordering knowledge into attention-based Neural Machine Translation (NMT) for further improving translation performance. Our proposed models enable attention mechanism to attend to source words regarding both the semantic requirement and the word reordering penalty. Experiments on Chinese-English translation show that the approaches can improve word alignment quality and achieve significant translation improvements over a basic attention-based NMT by large margins. Compared with previous works on identical corpora, our system achieves the state-of-the-art performance on translation quality."
P17-1141,Lexically Constrained Decoding for Sequence Generation Using Grid Beam Search,2017,19,19,2,0.638298,22646,chris hokamp,Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"We present Grid Beam Search (GBS), an algorithm which extends beam search to allow the inclusion of pre-specified lexical constraints. The algorithm can be used with any model which generates sequences token by token. Lexical constraints take the form of phrases or words that must be present in the output sequence. This is a very general way to incorporate auxillary knowledge into a model{'}s output without requiring any modification of the parameters or training data. We demonstrate the feasibility and flexibility of Lexically Constrained Decoding by conducting experiments on Neural Interactive-Predictive Translation, as well as Domain Adaptation for Neural Machine Translation. Experiments show that GBS can provide large improvements in translation quality in interactive scenarios, and that, even without any user input, GBS can be used to achieve significant gains in performance in domain adaptation scenarios."
P17-1175,Doubly-Attentive Decoder for Multi-modal Neural Machine Translation,2017,6,36,2,0,4092,iacer calixto,Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"We introduce a Multi-modal Neural Machine Translation model in which a doubly-attentive decoder naturally incorporates spatial visual features obtained using pre-trained convolutional neural networks, bridging the gap between image description and translation. Our decoder learns to attend to source-language words and parts of an image independently by means of two separate attention mechanisms as it generates words in the target language. We find that our model can efficiently exploit not just back-translated in-domain multi-modal data but also large general-domain text-only MT corpora. We also report state-of-the-art results on the Multi30k data set."
I17-4010,{ADAPT} Centre Cone Team at {IJCNLP}-2017 Task 5: A Similarity-Based Logistic Regression Approach to Multi-choice Question Answering in an Examinations Shared Task,2017,0,0,4,0,10031,daria dzendzik,"Proceedings of the {IJCNLP} 2017, Shared Tasks",0,"We describe the work of a team from the ADAPT Centre in Ireland in addressing automatic answer selection for the Multi-choice Question Answering in Examinations shared task. The system is based on a logistic regression over the string similarities between question, answer, and additional text. We obtain the highest grade out of six systems: 48.7{\%} accuracy on a validation set (vs. a baseline of 29.45{\%}) and 45.6{\%} on a test set."
I17-3009,Semantics-Enhanced Task-Oriented Dialogue Translation: A Case Study on Hotel Booking,2017,0,2,6,1,7026,longyue wang,"Proceedings of the {IJCNLP} 2017, System Demonstrations",0,"We showcase TODAY, a semantics-enhanced task-oriented dialogue translation system, whose novelties are: (i) task-oriented named entity (NE) definition and a hybrid strategy for NE recognition and translation; and (ii) a novel grounded semantic method for dialogue understanding and task-order management. TODAY is a case-study demo which can efficiently and accurately assist customers and agents in different languages to reach an agreement in a dialogue for the hotel booking."
E17-2056,Neural Automatic Post-Editing Using Prior Alignment and Reranking,2017,0,9,4,0,13776,santanu pal,"Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 2, Short Papers",0,"We present a second-stage machine translation (MT) system based on a neural machine translation (NMT) approach to automatic post-editing (APE) that improves the translation quality provided by a first-stage MT system. Our APE system (APE{\_}Sym) is an extended version of an attention based NMT model with bilingual symmetry employing bidirectional models, mt{--}pe and pe{--}mt. APE translations produced by our system show statistically significant improvements over the first-stage MT, phrase-based APE and the best reported score on the WMT 2016 APE dataset by a previous neural APE system. Re-ranking (APE{\_}Rerank) of the n-best translations from the phrase-based APE and APE{\_}Sym systems provides further substantial improvements over the symmetric neural APE model. Human evaluation confirms that the APE{\_}Rerank generated PE translations improve on the previous best neural APE system at WMT 2016."
E17-2057,Improving Evaluation of Document-level Machine Translation Quality Estimation,2017,11,3,4,0.336541,9403,yvette graham,"Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 2, Short Papers",0,"Meaningful conclusions about the relative performance of NLP systems are only possible if the gold standard employed in a given evaluation is both valid and reliable. In this paper, we explore the validity of human annotations currently employed in the evaluation of document-level quality estimation for machine translation (MT). We demonstrate the degree to which MT system rankings are dependent on weights employed in the construction of the gold standard, before proposing direct human assessment as a valid alternative. Experiments show direct assessment (DA) scores for documents to be highly reliable, achieving a correlation of above 0.9 in a self-replication experiment, in addition to a substantial estimated cost reduction through quality controlled crowd-sourcing. The original gold standard based on post-edits incurs a 10{--}20 times greater cost than DA."
E17-2095,Context-Aware Graph Segmentation for Graph-Based Translation,2017,10,0,3,1,5774,liangyou li,"Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 2, Short Papers",0,"In this paper, we present an improved graph-based translation model which segments an input graph into node-induced subgraphs by taking source context into consideration. Translations are generated by combining subgraph translations left-to-right using beam search. Experiments on Chinese{--}English and German{--}English demonstrate that the context-aware segmentation significantly improves the baseline graph-based model."
E17-1012,If You Can{'}t Beat Them Join Them: Handcrafted Features Complement Neural Nets for Non-Factoid Answer Reranking,2017,18,6,4,0,33018,dasha bogdanova,"Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 1, Long Papers",0,"We show that a neural approach to the task of non-factoid answer reranking can benefit from the inclusion of tried-and-tested handcrafted features. We present a neural network architecture based on a combination of recurrent neural networks that are used to encode questions and answers, and a multilayer perceptron. We show how this approach can be combined with additional features, in particular, the discourse features used by previous research. Our neural approach achieves state-of-the-art performance on a public dataset from Yahoo! Answers and its performance is further improved by incorporating the discourse features. Additionally, we present a new dataset of Ask Ubuntu questions where the hybrid approach also achieves good results."
D17-1105,Incorporating Global Visual Features into Attention-based Neural Machine Translation.,2017,0,17,2,0,4092,iacer calixto,Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing,0,"We introduce multi-modal, attention-based neural machine translation (NMT) models which incorporate visual features into different parts of both the encoder and the decoder. Global image features are extracted using a pre-trained convolutional neural network and are incorporated (i) as words in the source sentence, (ii) to initialise the encoder hidden state, and (iii) as additional data to initialise the decoder hidden state. In our experiments, we evaluate translations into English and German, how different strategies to incorporate global image features compare and which ones perform best. We also study the impact that adding synthetic multi-modal, multilingual data brings and find that the additional data have a positive impact on multi-modal NMT models. We report new state-of-the-art results and our best models also significantly improve on a comparable phrase-based Statistical MT (PBSMT) model trained on the Multi30k data set according to all metrics evaluated. To the best of our knowledge, it is the first time a purely neural model significantly improves over a PBSMT model on all metrics evaluated on this data set."
D17-1262,Further Investigation into Reference Bias in Monolingual Evaluation of Machine Translation,2017,6,3,4,1,13912,qingsong ma,Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing,0,"Monolingual evaluation of Machine Translation (MT) aims to simplify human assessment by requiring assessors to compare the meaning of the MT output with a reference translation, opening up the task to a much larger pool of genuinely qualified evaluators. Monolingual evaluation runs the risk, however, of bias in favour of MT systems that happen to produce translations superficially similar to the reference and, consistent with this intuition, previous investigations have concluded monolingual assessment to be strongly biased in this respect. On re-examination of past analyses, we identify a series of potential analytical errors that force some important questions to be raised about the reliability of past conclusions, however. We subsequently carry out further investigation into reference bias via direct human assessment of MT adequacy via quality controlled crowd-sourcing. Contrary to both intuition and past conclusions, results for show no significant evidence of reference bias in monolingual evaluation of MT."
D17-1301,Exploiting Cross-Sentence Context for Neural Machine Translation,2017,18,24,4,1,7026,longyue wang,Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing,0,"In translation, considering the document as a whole can help to resolve ambiguities and inconsistencies. In this paper, we propose a cross-sentence context-aware approach and investigate the influence of historical contextual information on the performance of neural machine translation (NMT). First, this history is summarized in a hierarchical way. We then integrate the historical representation into NMT in two strategies: 1) a warm-start of encoder and decoder states, and 2) an auxiliary context source for updating decoder states. Experimental results on a large Chinese-English translation task show that our approach significantly improves upon a strong attention-based NMT system by up to +2.1 BLEU points."
W16-3403,Improving Phrase-Based {SMT} Using Cross-Granularity Embedding Similarity,2016,19,2,4,1,7213,peyman passban,Proceedings of the 19th Annual Conference of the {E}uropean Association for Machine Translation,0,None
W16-3406,Combining Translation Memories and Syntax-Based {SMT}: Experiments with Real Industrial Data,2016,23,0,3,1,5774,liangyou li,Proceedings of the 19th Annual Conference of the {E}uropean Association for Machine Translation,0,"One major drawback of using Translation Memories (TMs) in phrase-based Machinen Translation (MT) is that only continuous phrases are considered. In contrast, syntax-based MTn allows phrasal discontinuity by learning translation rules containing non-terminals. In this paper,n we combine a TM with syntax-based MT via sparse features. These features are extracted duringn decoding based on translation rules and their corresponding patterns in the TM. We have testedn this approach by carrying out experiments on real Englishxe2x80x93Spanish industrial data. Our resultsn show that these TM features significantly improve syntax-based MT. Our final system yieldsn improvements of up to 3.1 BLEU, 1.6 METEOR, and -2.6 TER when compared with a stateof-the-art phrase-based MT system."
W16-0602,Extending Phrase-Based Translation with Dependencies by Using Graphs,2016,13,0,3,1,5774,liangyou li,Proceedings of the 2nd Workshop on Semantics-Driven Machine Translation ({S}ed{MT} 2016),0,"In this paper, we propose a graph-based translation model which takes advantage of discontinuous phrases. The model segments a graph which combines bigram and dependency relations into subgraphs and produces translations by combining translations of these subgraphs. Experiments on Chinesexe2x80x90English and Germanxe2x80x90English tasks show that our system is significantly better than the phrase-based model. By explicitly modeling the graph segmentation, our system gains further improvement."
P16-2045,Phrase-Level Combination of {SMT} and {TM} Using Constrained Word Lattice,2016,17,1,3,1,5774,liangyou li,Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"Constrained translation has improved statistical machine translation (SMT) by combining it with translation memory (TM) at sentence-level. In this paper, we propose using a constrained word lattice, which encodes input phrases and TM constraints together, to combine SMT and TM at phrase-level. Experiments on Englishxe2x80x90 Chinese and Englishxe2x80x90French show that our approach is significantly better than previous combination methods, including sentence-level constrained translation and a recent phrase-level combination."
P16-1010,Graph-Based Translation Via Graph Segmentation,2016,27,0,3,1,5774,liangyou li,Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,None
N16-1001,Achieving Accurate Conclusions in Evaluation of Automatic Machine Translation Metrics,2016,4,9,2,0.336541,9403,yvette graham,Proceedings of the 2016 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"Automatic Machine Translation metrics, suchn as BLEU, are widely used in empirical evaluation as a substitute for human assessment.n Subsequently, the performance of a given metric is measured by its strength of correlationn with human judgment. When a newly proposed metric achieves a stronger correlationn over that of a baseline, it is important to taken into account the uncertainty inherent in correlation point estimates prior to concludingn improvements in metric performance. Confidence intervals for correlations with humann judgment are rarely reported in metric evaluations, however, and when they have beenn reported, the most suitable methods have unfortunately not been applied. For example,n incorrect assumptions about correlation sampling distributions made in past evaluationsn risk over-estimation of significant differencesn in metric performance. In this paper, we provide analysis of each of the issues that mayn lead to inaccuracies before providing detail ofn a method that overcomes previous challenges.n Additionally, we propose a new method ofn translation sampling that in contrast achievesn genuine high conclusivity in evaluation of then relative performance of metrics."
N16-1113,A Novel Approach to Dropped Pronoun Translation,2016,32,6,6,1,7026,longyue wang,Proceedings of the 2016 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"Dropped Pronouns (DP) in which pronounsn are frequently dropped in the source languagen but should be retained in the target languagen are challenge in machine translation. In response to this problem, we propose a semisupervised approach to recall possibly missingn pronouns in the translation. Firstly, we buildn training data for DP generation in which then DPs are automatically labelled according ton the alignment information from a parallel corpus. Secondly, we build a deep learning-basedn DP generator for input sentences in decodingn when no corresponding references exist. Moren specifically, the generation is two-phase: (1)n DP position detection, which is modeled as an sequential labelling task with recurrent neuraln networks; and (2) DP prediction, which employs a multilayer perceptron with rich features. Finally, we integrate the above outputsn into our translation system to recall missingn pronouns by both extracting rules from then DP-labelled training data and translating then DP-generated input sentences. Experimentaln results show that our approach achieves a significant improvement of 1.58 BLEU points inn translation performance with 66% F-score forn DP generation accuracy."
L16-1352,{P}rophet{MT}: A Tree-based {SMT}-driven Controlled Language Authoring/Post-Editing Tool,2016,0,0,3,1,35090,xiaofeng wu,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"This paper presents ProphetMT, a tree-based SMT-driven Controlled Language (CL) authoring and post-editing tool. ProphetMT employs the source-side rules in a translation model and provides them as auto-suggestions to users. Accordingly, one might say that users are writing in a Controlled Language that is understood by the computer. ProphetMT also allows users to easily attach structural information as they compose content. When a specific rule is selected, a partial translation is promptly generated on-the-fly with the help of the structural information. Our experiments conducted on English-to-Chinese show that our proposed ProphetMT system can not only better regularise an author{'}s writing behaviour, but also significantly improve translation fluency which is vital to reduce the post-editing time. Additionally, when the writing and translation process is over, ProphetMT can provide an effective colour scheme to further improve the productivity of post-editors by explicitly featuring the relations between the source and target rules."
L16-1436,Automatic Construction of Discourse Corpora for Dialogue Translation,2016,0,4,5,1,7026,longyue wang,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"In this paper, a novel approach is proposed to automatically construct parallel discourse corpus for dialogue machine translation. Firstly, the parallel subtitle data and its corresponding monolingual movie script data are crawled and collected from Internet. Then tags such as speaker and discourse boundary from the script data are projected to its subtitle data via an information retrieval approach in order to map monolingual discourse to bilingual texts. We not only evaluate the mapping results, but also integrate speaker information into the translation. Experiments show our proposed method can achieve 81.79{\%} and 98.64{\%} accuracy on speaker and dialogue boundary annotation, and speaker-based language model adaptation can obtain around 0.5 BLEU points improvement in translation qualities. Finally, we publicly release around 100K parallel discourse data with manual speaker and dialogue boundary annotation."
D16-1027,Memory-enhanced Decoder for Neural Machine Translation,2016,15,30,4,1,4605,mingxuan wang,Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing,0,"We propose to enhance the RNN decoder in a neural machine translator (NMT) with external memory, as a natural but powerful extension to the state in the decoding RNN. This memory-enhanced RNN decoder is called textsc{MemDec}. At each time during decoding, textsc{MemDec} will read from this memory and write to this memory once, both with content-based addressing. Unlike the unbounded memory in previous workcite{RNNsearch} to store the representation of source sentence, the memory in textsc{MemDec} is a matrix with pre-determined size designed to better capture the information important for the decoding process at each time step. Our empirical study on Chinese-English translation shows that it can improve by $4.8$ BLEU upon Groundhog and $5.3$ BLEU upon on Moses, yielding the best performance achieved with the same training set."
D16-1037,Variational Neural Discourse Relation Recognizer,2016,23,5,4,0,5777,biao zhang,Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing,0,"Implicit discourse relation recognition is a crucial component for automatic discourselevel analysis and nature language understanding. Previous studies exploit discriminative models that are built on either powerful manual features or deep discourse representations. In this paper, instead, we explore generative models and propose a variational neural discourse relation recognizer. We refer to this model as VarNDRR. VarNDRR establishes a directed probabilistic model with a latent continuous variable that generates both a discourse and the relation between the two arguments of the discourse. In order to perform efficient inference and learning, we introduce neural discourse relation models to approximate the prior and posterior distributions of the latent variable, and employ these approximated distributions to optimize a reparameterized variational lower bound. This allows VarNDRR to be trained with standard stochastic gradient methods. Experiments on the benchmark data set show that VarNDRR can achieve comparable results against stateof- the-art baselines without using any manual features."
D16-1070,Neural Network for Heterogeneous Annotations,2016,35,15,3,1,4565,hongshen chen,Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing,0,None
C16-1057,A subtree-based factorization of dependency parsing,2016,21,1,2,0,31642,qiuye zhao,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers",0,"We propose a dependency parsing pipeline, in which the parsing of long-distance projections and localized dependencies are explicitly decomposed at the input level. A chosen baseline dependency parsing model performs only on {`}carved{'} sequences at the second stage, which are transformed from coarse constituent parsing outputs at the first stage. When k-best constituent parsing outputs are kept, a third-stage is required to search for an optimal combination of the overlapped dependency subtrees. In this sense, our dependency model is subtree-factored. We explore alternative approaches for scoring subtrees, including feature-based models as well as continuous representations. The search for optimal subset to combine is formulated as an ILP problem. This framework especially benefits the models poor on long sentences, generally improving baselines by 0.75-1.28 (UAS) on English, achieving comparable performance with high-order models but faster. For Chinese, the most notable increase is as high as 3.63 (UAS) when the proposed framework is applied to first-order parsing models."
C16-1131,Fast Gated Neural Domain Adaptation: Language Model as a Case Study,2016,18,3,4,0.603689,22733,jian zhang,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers",0,"Neural network training has been shown to be advantageous in many natural language processing applications, such as language modelling or machine translation. In this paper, we describe in detail a novel domain adaptation mechanism in neural network training. Instead of learning and adapting the neural network on millions of training sentences {--} which can be very time-consuming or even infeasible in some cases {--} we design a domain adaptation gating mechanism which can be used in recurrent neural networks and quickly learn the out-of-domain knowledge directly from the word vector representations with little speed overhead. In our experiments, we use the recurrent neural network language model (LM) as a case study. We show that the neural LM perplexity can be reduced by 7.395 and 12.011 using the proposed domain adaptation mechanism on the Penn Treebank and News data, respectively. Furthermore, we show that using the domain-adapted neural LM to re-rank the statistical machine translation n-best list on the French-to-English language pair can significantly improve translation quality."
C16-1170,Topic-Informed Neural Machine Translation,2016,28,9,4,0.603689,22733,jian zhang,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers",0,"In recent years, neural machine translation (NMT) has demonstrated state-of-the-art machine translation (MT) performance. It is a new approach to MT, which tries to learn a set of parameters to maximize the conditional probability of target sentences given source sentences. In this paper, we present a novel approach to improve the translation performance in NMT by conveying topic knowledge during translation. The proposed topic-informed NMT can increase the likelihood of selecting words from the same topic and domain for translation. Experimentally, we demonstrate that topic-informed NMT can achieve a 1.15 (3.3{\%} relative) and 1.67 (5.4{\%} relative) absolute improvement in BLEU score on the Chinese-to-English language pair using NIST 2004 and 2005 test sets, respectively, compared to NMT without topic information."
C16-1205,Interactive Attention for Neural Machine Translation,2016,18,2,4,1,3627,fandong meng,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers",0,"Conventional attention-based Neural Machine Translation (NMT) conducts dynamic alignment in generating the target sentence. By repeatedly reading the representation of source sentence, which keeps fixed after generated by the encoder (Bahdanau et al., 2015), the attention mechanism has greatly enhanced state-of-the-art NMT. In this paper, we propose a new attention mechanism, called INTERACTIVE ATTENTION, which models the interaction between the decoder and the representation of source sentence during translation by both reading and writing operations. INTERACTIVE ATTENTION can keep track of the interaction history and therefore improve the translation performance. Experiments on NIST Chinese-English translation task show that INTERACTIVE ATTENTION can achieve significant improvements over both the previous attention-based NMT baseline and some state-of-the-art variants of attention-based NMT (i.e., coverage models (Tu et al., 2016)). And neural machine translator with our INTERACTIVE ATTENTION can outperform the open source attention-based NMT system Groundhog by 4.22 BLEU points and the open source phrase-based system Moses by 3.94 BLEU points averagely on multiple test sets."
C16-1243,Enriching Phrase Tables for Statistical Machine Translation Using Mixed Embeddings,2016,18,3,2,1,7213,peyman passban,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers",0,"The phrase table is considered to be the main bilingual resource for the phrase-based statistical machine translation (PBSMT) model. During translation, a source sentence is decomposed into several phrases. The best match of each source phrase is selected among several target-side counterparts within the phrase table, and processed by the decoder to generate a sentence-level translation. The best match is chosen according to several factors, including a set of bilingual features. PBSMT engines by default provide four probability scores in phrase tables which are considered as the main set of bilingual features. Our goal is to enrich that set of features, as a better feature set should yield better translations. We propose new scores generated by a Convolutional Neural Network (CNN) which indicate the semantic relatedness of phrase pairs. We evaluate our model in different experimental settings with different language pairs. We observe significant improvements when the proposed features are incorporated into the PBSMT pipeline."
2016.tc-1.17,Calculating the percentage reduction in translator effort when using machine translation,2016,-1,-1,2,0,34736,andrzej zydron,Proceedings of Translating and the Computer 38,0,None
W15-4911,Benchmarking {SMT} Performance for {F}arsi Using the {TEP}++ Corpus,2015,14,4,3,1,7213,peyman passban,Proceedings of the 18th Annual Conference of the {E}uropean Association for Machine Translation,0,"Statistical machine translation (SMT) suffers from various problems which are exacerbated where training data is in shortn supply. In this paper we address the datan sparsity problem in the Farsi (Persian) language and introduce a new parallel corpus, TEP. Compared to previous results the new dataset is more efficient forn Farsi SMT engines and yields better output. In our experiments using TEP asn bilingual training data and BLEU as a metric, we achieved improvements of 11.17n (60%) and 7.76 (63.92%) in the Farsixe2x80x93n English and Englishxe2x80x93Farsi directions, respectively. Furthermore we describe ann engine (SF2FF) to translate between formal and informal Farsi which in terms ofn syntax and terminology can be seen asn different languages. The SF2FF enginen also works as an intelligent normalizer forn Farsi texts. To demonstrate its use, SF2FFn was used to clean the IWSLTxe2x80x932013 datasetn to produce normalized data, which gaven improvements in translation quality overn FBKxe2x80x99s Farsi engine when used as trainingn data"
W15-4934,{H}andy{CAT} - An Open-Source Platform for {CAT} Tool Research,2015,0,0,2,0,36566,christopher hokamp,Proceedings of the 18th Annual Conference of the {E}uropean Association for Machine Translation,0,None
W15-3005,"{P}ar{FDA} for Fast Deployment of Accurate Statistical Machine Translation Systems, Benchmarks, and Statistics",2015,9,11,2,0.69881,13953,ergun biccici,Proceedings of the Tenth Workshop on Statistical Machine Translation,0,"We build parallel FDA5 (ParFDA) Moses statistical machine translation (SMT) systems for all language pairs in the workshop on statistical machine translation (Bojar et al., 2015) (WMT15) translation task and obtain results close to the top with an average of 3.176 BLEU points difference using significantly less resources for building SMT systems. ParFDA is a parallel implementation of feature decay algorithms (FDA) developed for fast deploy"
W15-3035,Referential Translation Machines for Predicting Translation Quality and Related Statistics,2015,15,8,2,0.69881,13953,ergun biccici,Proceedings of the Tenth Workshop on Statistical Machine Translation,0,We use referential translation machines (RTMs) for predicting translation performance. RTMs pioneer a language independent approach to all similarity tasks and remove the need to access any task or domain specific information or resource. We improve our RTM models with the
W15-3053,{CASICT}-{DCU} Participation in {WMT}2015 Metrics Task,2015,15,10,4,1,36867,hui yu,Proceedings of the Tenth Workshop on Statistical Machine Translation,0,"Human-designed sub-structures are required by most of the syntax-based machine translation evaluation metrics. In this paper, we propose a novel evaluation metric based on dependency parsing model, which does not need this human involvement. Experimental results show that the new single metric gets better correlation than METEOR on system level and is comparable with it on sentence level. To introduce more information, we combine the new metric with many other metrics. The combined metric obtains state-of-theart performance on both system level evaluation and sentence level evaluation on WMT 2014."
W15-3055,{MT} Tuning on {RED}: A Dependency-Based Evaluation Metric,2015,18,3,3,1,5774,liangyou li,Proceedings of the Tenth Workshop on Statistical Machine Translation,0,"In this paper, we describe our submission to WMT 2015 Tuning Task. We integrate a dependency-based MT evaluation metric, RED, to Moses and compare it with BLEU and METEOR in conjunction with two tuning methods: MERT and MIRA. Experiments are conducted using hierarchical phrase-based models on Czechxe2x80x90English and Englishxe2x80x90Czech tasks. Our results show that MIRA performs better than MERT in most cases. Using RED performs similarly to METEOR when tuning is performed using MIRA. We submit our system tuned by MIRA towards RED to WMT 2015. In human evaluations, we achieve the 1st rank in all 7 systems on the Englishxe2x80x90Czech task and 6/9 on the Czechxe2x80x90 English task."
P15-1003,Encoding Source Language with Convolutional Neural Network for Machine Translation,2015,21,48,6,1,3627,fandong meng,Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),1,"The recently proposed neural network joint model (NNJM) (Devlin et al., 2014) augments the n-gram target language model with a heuristically chosen source context window, achieving state-of-the-art performance in SMT. In this paper, we give a more systematic treatment by summarizing the relevant source information through a convolutional architecture guided by the target information. With different guiding signals during decoding, our specifically designed convolutiongating architectures can pinpoint the parts of a source sentence that are relevant to predicting a target word, and fuse them with the context of entire source sentence to form a unified representation. This representation, together with target language words, are fed to a deep neural network (DNN) to form a stronger NNJM. Experiments on two NIST Chinese-English translation tasks show that the proposed model can achieve significant improvements over the previous NNJM by up to 1.08 BLEU points on average"
P15-1151,gen{CNN}: A Convolutional Architecture for Word Sequence Prediction,2015,26,15,5,1,4605,mingxuan wang,Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),1,None
K15-2011,The {DCU} Discourse Parser: A Sense Classification Task,2015,22,1,3,1,37719,tsuyoshi okita,Proceedings of the Nineteenth Conference on Computational Natural Language Learning - Shared Task,0,This paper describes the discourse parsing system developed at Dublin City University for participation in the CoNLL 2015 shared task. We participated in two tasks: a connective and argument identification task and a sense classification task. This paper focuses on the latter task and especially the sense classification for implicit connectives.
K15-2014,"The {DCU} Discourse Parser for Connective, Argument Identification and Explicit Sense Classification",2015,8,5,5,1,7026,longyue wang,Proceedings of the Nineteenth Conference on Computational Natural Language Learning - Shared Task,0,"This paper describes our submission to the CoNLL-2015 shared task on discourse parsing. We factor the pipeline into subcomponents which are then used to form the final sequential architecture. Focusing on achieving good performance when inferring explicit discourse relations, we apply maximum entropy and recurrent neural networks to different sub-tasks such as connective identification, argument extraction, and sense classification. The our final system achieves 16.51%, 12.73% and 11.15% overall F1 scores on the dev, WSJ and blind test sets, respectively."
J15-1005,Automatic Adaptation of Annotations,2015,51,3,4,1,23255,wenbin jiang,Computational Linguistics,0,"Manually annotated corpora are indispensable resources, yet for many annotation tasks, such as the creation of treebanks, there exist multiple corpora with different and incompatible annotation guidelines. This leads to an inefficient use of human expertise, but it could be remedied by integrating knowledge across corpora with different annotation guidelines. In this article we describe the problem of annotation adaptation and the intrinsic principles of the solutions, and present a series of successively enhanced models that can automatically adapt the divergence between different annotation formats.n n We evaluate our algorithms on the tasks of Chinese word segmentation and dependency parsing. For word segmentation, where there are no universal segmentation guidelines because of the lack of morphology in Chinese, we perform annotation adaptation from the much larger People's Daily corpus to the smaller but more popular Penn Chinese Treebank. For dependency parsing, we perform annotation adaptation from the Penn Chinese Treebank to a semantics-oriented Dependency Treebank, which is annotated using significantly different annotation guidelines. In both experiments, automatic annotation adaptation brings significant improvement, achieving state-of-the-art performance despite the use of purely local features in training."
D15-1004,Dependency Graph-to-String Translation,2015,33,2,3,1,5774,liangyou li,Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,0,"Compared to tree grammars, graph grammars have stronger generative capacity over structures. Based on an edge replacement grammar, in this paper we propose to use a synchronous graph-to-string grammar for statistical machine translation. The graph we use is directly converted from a dependency tree by labelling edges. We build our translation model in the log-linear framework with standard features. Large-scale experiments on Chinesexe2x80x90English and Germanxe2x80x90English tasks show that our model is significantly better than the state-of-the-art hierarchical phrase-based (HPB) model and a recently improved dependency tree-to-string model on BLEU, METEOR and TER scores. Experiments also suggest that our model has better capability to perform long-distance reordering and is more suitable for translating long sentences."
2015.tc-1.3,The {EXPERT} project: Advancing the state of the art in hybrid translation technologies,2015,8,0,7,0,12551,constantin orasan,Proceedings of Translating and the Computer 37,0,None
2015.eamt-1.12,Benchmarking {SMT} Performance for {F}arsi Using the {TEP}++ Corpus,2015,14,4,3,1,7213,peyman passban,Proceedings of the 18th Annual Conference of the European Association for Machine Translation,0,"Statistical machine translation (SMT) suffers from various problems which are exacerbated where training data is in shortn supply. In this paper we address the datan sparsity problem in the Farsi (Persian) language and introduce a new parallel corpus, TEP. Compared to previous results the new dataset is more efficient forn Farsi SMT engines and yields better output. In our experiments using TEP asn bilingual training data and BLEU as a metric, we achieved improvements of 11.17n (60%) and 7.76 (63.92%) in the Farsixe2x80x93n English and Englishxe2x80x93Farsi directions, respectively. Furthermore we describe ann engine (SF2FF) to translate between formal and informal Farsi which in terms ofn syntax and terminology can be seen asn different languages. The SF2FF enginen also works as an intelligent normalizer forn Farsi texts. To demonstrate its use, SF2FFn was used to clean the IWSLTxe2x80x932013 datasetn to produce normalized data, which gaven improvements in translation quality overn FBKxe2x80x99s Farsi engine when used as trainingn data"
2015.eamt-1.35,{H}andy{CAT} - An Open-Source Platform for {CAT} Tool Research,2015,0,0,2,0.638298,22646,chris hokamp,Proceedings of the 18th Annual Conference of the European Association for Machine Translation,0,None
W14-4014,Transformation and Decomposition for Efficiently Implementing and Improving Dependency-to-String Model In {M}oses,2014,29,5,4,1,5774,liangyou li,"Proceedings of {SSST}-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation",0,"Dependency structure provides grammatical relations between words, which have shown to be effective in Statistical Machine Translation (SMT). In this paper, we present an open source module in Moses which implements a dependency-to-string model. We propose a method to transform the input dependency tree into a corresponding constituent tree for reusing the tree-based decoder in Moses. In our experiments, this method achieves comparable results with the standard model. Furthermore, we enrich this model via the decomposition of dependency structure, including extracting rules from the substructures of the dependency tree during training and creating a pseudo-forest instead of the tree per se as the input during decoding. Large-scale experiments on Chinesexe2x80x90English and Germanxe2x80x90English tasks show that the decomposition approach improves the baseline dependencyto-string model significantly. Our system achieves comparable results with the state-of-the-art hierarchical phrase-based model (HPB). Finally, when resorting to phrasal rules, the dependency-to-string model performs significantly better than Moses HPB."
W14-3303,Parallel {FDA}5 for Fast Deployment of Accurate Statistical Machine Translation Systems,2014,12,13,2,0.69881,13953,ergun biccici,Proceedings of the Ninth Workshop on Statistical Machine Translation,0,"We use parallel FDA5, an efficiently parameterized and optimized parallel implementation of feature decay algorithms for fast deployment of accurate statistical machine translation systems, taking only about half a day for each translation direction. We build Parallel FDA5 Moses SMT systems for all language pairs in the WMT14 translation task and obtain SMT performance close to the top Moses systems with an average of 3.49 BLEU points difference using significantly less resources for training and development."
W14-3314,The {DCU}-{ICTCAS} {MT} system at {WMT} 2014 on {G}erman-{E}nglish Translation Task,2014,19,8,6,1,5774,liangyou li,Proceedings of the Ninth Workshop on Statistical Machine Translation,0,"This paper describes the DCU submission to WMT 2014 on German-English translation task. Our system uses phrasebased translation model with several popular techniques, including Lexicalized Reordering Model, Operation Sequence Model and Language Model interpolation. Our final submission is the result of system combination on several systems which have different pre-processing and alignments."
W14-3325,{DCU}-Lingo24 Participation in {WMT} 2014 {H}indi-{E}nglish Translation task,2014,19,2,6,1,35090,xiaofeng wu,Proceedings of the Ninth Workshop on Statistical Machine Translation,0,"This paper describes the DCU-Lingo24 submission to WMT 2014 for the HindiEnglish translation task. We exploit miscellaneous methods in our system, including: Context-Informed PB-SMT, OOV Word Conversion (OWC), MultiAlignment Combination (MAC), Operation Sequence Model (OSM), Stemming Align and Normal Phrase Extraction (SANPE), and Language Model Interpolation (LMI). We also describe various preprocessing steps we tried for Hindi in this task."
W14-3329,{DCU} Terminology Translation System for Medical Query Subtask at {WMT}14,2014,20,4,4,1,37719,tsuyoshi okita,Proceedings of the Ninth Workshop on Statistical Machine Translation,0,None
W14-3355,"{RED}, The {DCU}-{CASICT} Submission of Metrics Tasks",2014,12,3,3,1,35090,xiaofeng wu,Proceedings of the Ninth Workshop on Statistical Machine Translation,0,None
E14-4036,Active Learning for Post-Editing Based Incrementally Retrained {MT},2014,18,3,3,0,33902,aswarth dara,"Proceedings of the 14th Conference of the {E}uropean Chapter of the Association for Computational Linguistics, volume 2: Short Papers",0,"Machine translation, in particular statistical machine translation (SMT), is making big inroads into the localisation and translation industry. In typical workflows (S)MT output is checked and (where required) manually post-edited by human translators. Recently, a significant amount of research has concentrated on capturing human post-editing outputs as early as possible to incrementally update/modify SMT models to avoid repeat mistakes. Typically in these approaches, MT and post-edits happen sequentially and chronologically, following the way unseen data (the translation job) is presented. In this paper, we add to the existing literature addressing the question whether and if so, to what extent, this process can be improved upon by Active Learning, where input is not presented chronologically but dynamically selected according to criteria that maximise performance with respect to (whatever is) the remaining data. We explore novel (source side-only) selection criteria and show performance increases of 0.67-2.65 points TER absolute on average on typical industry data sets compared to sequential PEbased incrementally retrained SMT."
D14-1021,Syntactic {SMT} Using a Discriminative Text Generation Model,2014,36,5,5,0.0360992,884,yue zhang,Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP}),0,"We study a novel architecture for syntactic SMT. In contrast to the dominant approach in the literature, the system does not rely on translation rules, but treat translation as an unconstrained target sentence generation task, using soft features to capture lexical and syntactic correspondences between the source and target languages. Target syntax features and bilingual translation features are trained consistently in a discriminative model. Experiments using the IWSLT 2010 dataset show that the system achieves BLEU comparable to the state-of-the-art syntactic SMT systems."
D14-1060,Modeling Term Translation for Document-informed Machine Translation,2014,35,6,4,1,3627,fandong meng,Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP}),0,"Term translation is of great importance for statistical machine translation (SMT), especially document-informed SMT. In this paper, we investigate three issues of term translation in the context of documentinformed SMT and propose three corresponding models: (a) a term translation disambiguation model which selects desirable translations for terms in the source language with domain information, (b) a term translation consistency model that encourages consistent translations for terms with a high strength of translation consistency throughout a document, and (c) a term bracketing model that rewards translation hypotheses where bracketable source terms are translated as a whole unit. We integrate the three models into hierarchical phrase-based SMT and evaluate their effectiveness on NIST ChineseEnglish translation tasks with large-scale training data. Experiment results show that all three models can achieve significant improvements over the baseline. Additionally, we can obtain a further improvement when combining the three models."
C14-1104,A Dependency Edge-based Transfer Model for Statistical Machine Translation,2014,29,3,5,1,4565,hongshen chen,"Proceedings of {COLING} 2014, the 25th International Conference on Computational Linguistics: Technical Papers",0,"Previous models in syntax-based statistical machine translation usually resort to some kinds of synchronous procedures, few of these works are based on the analysis-transfer-generation methodology. In this paper, we present a statistical implementation of the analysis-transfergeneration methodology in rule-based translation. The procedures of syntax analysis, syntax transfer and language generation are modeled independently in order to break the synchronous constraint, resorting to dependency structures with dependency edges as atomic manipulating units. Large-scale experiments on Chinese to English translation show that our model exhibits state-of-the-art performance by significantly outperforming the phrase-based model. The statistical transfer-generation method results in significantly better performance with much smaller models."
C14-1107,A Structured Language Model for Incremental Tree-to-String Translation,2014,27,0,4,1,19902,heng yu,"Proceedings of {COLING} 2014, the 25th International Conference on Computational Linguistics: Technical Papers",0,"Tree-to-string systems have gained significant popularity thanks to their simplicity and e ciency by exploring the source syntax information, but they lack in the target syntax to guarantee the grammaticality of the output. Instead of using complex tree-to-tree models, we integrate a structured language model, a left-to-right shift-reduce parser in specific, into an incremental tree-to-string model, and introduce an e cient grouping and pruning mechanism for this integration. Large-scale experiments on various Chinese-English test sets show that with a reasonable speed our method gains an average improvement of 0.7 points in terms of (Ter-Bleu)/2 than a state-of-the-art tree-to-string system."
C14-1146,Annotation Adaptation and Language Adaptation in {NLP},2014,0,0,1,1,5775,qun liu,"Proceedings of {COLING} 2014, the 25th International Conference on Computational Linguistics: Technical Papers",0,"Adaptation technologies are always useful in NLP when there is discrepancy between the training scenario and use scenario. They are also effective in alleviating the data scarcity problem. Domain adaptation is the most popular kind of adaptation technologies and is intensively researched. In this talk we will introduce two other kinds of adaptation technologies: annotation adaptation and language adaptation. Annotation adaptation is used to improve the performance of an automatic annotation task by leveraging corpora with different annotation schemas, while language adaptation is used to solve an NLP problem in one language by utilizing the linguistic knowledge which is learnt from solving the same problem in another language. We investigate these technologies mainly for the tasks of word segmentation and parsing, however similar technologies may be developed for other NLP tasks also."
C14-1193,{RED}: A Reference Dependency Based {MT} Evaluation Metric,2014,17,14,5,1,36867,hui yu,"Proceedings of {COLING} 2014, the 25th International Conference on Computational Linguistics: Technical Papers",0,"Most of the widely-used automatic evaluation metrics consider only the local fragments of the references and translations, and they ignore the evaluation on the syntax level. Current syntaxbased evaluation metrics try to introduce syntax information but suffer from the poor parsing results of the noisy machine translations. To alleviate this problem, we propose a novel dependency-based evaluation metric which only employs the dependency information of the references. We use two kinds of reference dependency structures: headword chain to capture the long distance dependency information, and fixed and floating structures to capture the local continuous ngram. Experiment results show that our metric achieves higher correlations with human judgments than BLEU, TER and HWCM on WMT 2012 and WMT 2013. By introducing extra linguistic resources and tuning parameters, the new metric gets the state-of-the-art performance which is better than METEOR and SEMPOS on system level, and is comparable with METEOR on sentence level on WMT 2012 and WMT 2013."
C14-1209,Augment Dependency-to-String Translation with Fixed and Floating Structures,2014,21,4,3,1,4085,jun xie,"Proceedings of {COLING} 2014, the 25th International Conference on Computational Linguistics: Technical Papers",0,"In this paper, we propose an augmented dependency-to-string model to combine the merits of both the head-dependents relations at handling long distance reordering and the fixed and floating structures at handling local reordering. For this purpose, we first compactly represent both the head-dependent relation and the fixed and floating structures into translation rules; second, in decoding we build xe2x80x9con-the-flyxe2x80x9d new translation rules from the compact translation rules that can incorporate non-syntactic phrases into translations, thus alleviate the non-syntactic phrase coverage problem of dependency-to-string translation (Xie et al., 2011). Large-scale experiments on Chinese-to-English translation show that our augmented dependency-to-string model gains significant improvement of averaged 0.85 BLEU scores on three test sets over the dependencyto-string model."
2014.amta-researchers.8,A probabilistic feature-based fill-up for {SMT},2014,-1,-1,4,0.603689,22733,jian zhang,Proceedings of the 11th Conference of the Association for Machine Translation in the Americas: MT Researchers Track,0,"In this paper, we describe an effective translation model combination approach based on the estimation of a probabilistic Support Vector Machine (SVM). We collect domain knowledge from both in-domain and general-domain corpora inspired by a commonly used data selection algorithm, which we then use as features for the SVM training. Drawing on previous work on binary-featured phrase table fill-up (Nakov, 2008; Bisazza et al., 2011), we substitute the binary feature in the original work with our probabilistic domain-likeness feature. Later, we design two experiments to evaluate the proposed probabilistic feature-based approach on the French-to-English language pair using data provided at WMT07, WMT13 and IWLST11 translation tasks. Our experiments demonstrate that translation performance can gain significant improvements of up to +0.36 and +0.82 BLEU scores by using our probabilistic feature-based translation model fill-up approach compared with the binary featured fill-up approach in both experiments."
2014.amta-researchers.16,Review and analysis of {C}hina workshop on machine translation 2013 evaluation,2014,12,0,4,0,40426,sitong yang,Proceedings of the 11th Conference of the Association for Machine Translation in the Americas: MT Researchers Track,0,"This paper gives a general review and detailed analysis of China Workshop on Machine Translation (CWMT) Evaluation. Compared with the past CWMT evaluation campaigns, CWMT2013 evaluation is characterized as follows: first, adopting gray-box evaluation which makes the results more replicable and controllable; second, adding one rule-based system as a counterpart; third, carrying out manual evaluations on some specific tasks to give a more comprehensive analysis of the translation errors. Boosted by those new features, our analysis and case study on the evaluation results shows the pros and cons of both rule-based and statistical systems, and reveals some interesting correlations bewteen automatic and manual evaluation metrics on different translation systems."
2014.amta-researchers.19,A discriminative framework of integrating translation memory features into {SMT},2014,-1,-1,3,1,5774,liangyou li,Proceedings of the 11th Conference of the Association for Machine Translation in the Americas: MT Researchers Track,0,"Combining Translation Memory (TM) with Statistical Machine Translation (SMT) together has been demonstrated to be beneficial. In this paper, we present a discriminative framework which can integrate TM into SMT by incorporating TM-related feature functions. Experiments on English{--}Chinese and English{--}French tasks show that our system using TM feature functions only from the best fuzzy match performs significantly better than the baseline phrase- based system on both tasks, and our discriminative model achieves comparable results to those of an effective generative model which uses similar features. Furthermore, with the capacity of handling a large amount of features in the discriminative framework, we propose a method to efficiently use multiple fuzzy matches which brings more feature functions and further significantly improves our system."
W13-2222,Shallow Semantically-Informed {PBSMT} and {HPBSMT},2013,30,3,2,1,37719,tsuyoshi okita,Proceedings of the Eighth Workshop on Statistical Machine Translation,0,"This paper describes shallow semantically-informed Hierarchical Phrase-based SMT (HPBSMT) and Phrase-Based SMT (PBSMT) systems developed at Dublin City University for participation in the translation task between EN-ES and ES-EN at the Workshop on Statistical Machine Translation (WMT 13). The system uses PBSMT and HPBSMT decoders with multiple LMs, but will run only one decoding path decided before starting translation. Therefore the paper does not present a multi-engine system combination. We investigate three types of shallow semantics: (i) Quality Estimation (QE) score, (ii) genre ID, and (iii) context ID derived from context-dependent language models. Our results show that the improvement is 0.8 points absolute (BLEU) for EN-ES and 0.7 points for ES-EN compared to the standard PBSMT system (single best system). It is important to note that we developed this method when the standard (confusion network-based) system combination is ineffective such as in the case when the input is only two."
W13-2227,The {CNGL}-{DCU}-{P}rompsit Translation Systems for {WMT}13,2013,14,10,7,0,8609,raphael rubino,Proceedings of the Eighth Workshop on Statistical Machine Translation,0,"This paper presents the experiments conducted by the Machine Translation group at DCU and Prompsit Language Engineering for the WMT13 translation task. Three language pairs are considered: SpanishEnglish and French-English in both directions and German-English in that direction. For the Spanish-English pair, the use of linguistic information to select parallel data is investigated. For the FrenchEnglish pair, the usefulness of the small indomain parallel corpus is evaluated, compared to an out-of-domain parallel data sub-sampling method. Finally, for the German-English system, we describe our work in addressing the long distance reordering problem and a system combination strategy."
W13-2256,{DCU} Participation in {WMT}2013 Metrics Task,2013,11,7,3,1,35090,xiaofeng wu,Proceedings of the Eighth Workshop on Statistical Machine Translation,0,"In this paper, we propose a novel syntactic based MT evaluation metric which only employs the dependency information in the source side. Experimental results show that our method achieves higher correlation with human judgments than BLEU, TER, HWCM and METEOR at both sentence and system level for all of the four language pairs in WMT 2010."
P13-2064,A Novel Graph-based Compact Representation of Word Alignment,2013,23,6,1,1,5775,qun liu,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"In this paper, we propose a novel compact representation called weighted bipartite hypergraph to exploit the fertility model, which plays a critical role in word alignment. However, estimating the probabilities of rules extracted from hypergraphs is an NP-complete problem, which is computationally infeasible. Therefore, we propose a divide-and-conquer strategy by decomposing a hypergraph into a set of independent subhypergraphs. The experiments show that our approach outperforms both 1-best and n-best alignments."
P13-2065,Stem Translation with Affix-Based Rule Selection for Agglutinative Languages,2013,31,1,4,1,41427,zhiyang wang,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"Current translation models are mainly designed for languages with limited morphology, which are not readily applicable to agglutinative languages as the difference in the way lexical forms are generated. In this paper, we propose a novel approach for translating agglutinative languages by treating stems and affixes differently. We employ stem as the atomic translation unit to alleviate data spareness. In addition, we associate each stemgranularity translation rule with a distribution of related affixes, and select desirable rules according to the similarity of their affix distributions with given spans to be translated. Experimental results show that our approach significantly improves the translation performance on tasks of translating from three Turkic languages to Chinese."
P13-2068,Bilingual Lexical Cohesion Trigger Model for Document-Level Machine Translation,2013,13,10,5,0,41428,guosheng ben,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"In this paper, we propose a bilingual lexical cohesion trigger model to capture lexical cohesion for document-level machine translation. We integrate the model into hierarchical phrase-based machine translation and achieve an absolute improvement of 0.85 BLEU points on average over the baseline on NIST Chinese-English test sets."
P13-2105,Iterative Transformation of Annotation Guidelines for Constituency Parsing,2013,28,1,4,0,1813,xiang li,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"This paper presents an effective algorithm of annotation adaptation for constituency treebanks, which transforms a treebank from one annotation guideline to another with an iterative optimization procedure, thus to build a much larger treebank to train an enhanced parser without increasing model complexity. Experiments show that the transformed Tsinghua Chinese Treebank as additional training data brings significant improvement over the baseline trained on Penn Chinese Treebank only."
P13-1075,Discriminative Learning with Natural Annotations: Word Segmentation as a Case Study,2013,41,20,5,1,23255,wenbin jiang,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"Structural information in web text provides natural annotations for NLP problems such as word segmentation and parsing. In this paper we propose a discriminative learning algorithm to take advantage of the linguistic knowledge in large amounts of natural annotations on the Internet. It utilizes the Internet as an external corpus with massive (although slight and sparse) natural annotations, and enables a classifier to evolve on the large-scaled and real-time updated web text. With Chinese word segmentation as a case study, experiments show that the segmenter enhanced with the Chinese wikipedia achieves significant improvement on a series of testing sets from different domains, even with a single classifier and local features."
P13-1105,Bilingually-Guided Monolingual Dependency Grammar Induction,2013,37,7,4,0,4636,kai liu,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"This paper describes a novel strategy for automatic induction of a monolingual dependency grammar under the guidance of bilingually-projected dependency. By moderately leveraging the dependency information projected from the parsed counterpart language, and simultaneously mining the underlying syntactic structure of the language considered, it effectively integrates the advantages of bilingual projection and unsupervised induction, so as to induce a monolingual grammar much better than previous models only using bilingual projection or unsupervised induction. We induced dependency grammar for five different languages under the guidance of dependency information projected from the parsed English translation, experiments show that the bilinguallyguided method achieves a significant improvement of 28: 5% over the unsupervised baseline and 3: 0% over the best projection baseline on average."
I13-1051,A Topic-Triggered Language Model for Statistical Machine Translation,2013,36,1,4,1,19902,heng yu,Proceedings of the Sixth International Joint Conference on Natural Language Processing,0,"Language model is an essential part in statistical machine translation, but traditional n-gram language models can only capture a limited local context in the translated sentence, thus lacking the global information for prediction. This paper describes a novel topic-triggered language model, which takes into account the topical context by estimating the n-gram probability under the given topics and online adjusts language model score according to di erent topic distributions. Experimental results show that our method provides a average improvement of 0.76 Bleu on NIST Chinese-to-English translation task and a reduction in word perplexity of the test-document."
D13-1051,Improving Alignment of System Combination by Using Multi-objective Optimization,2013,30,2,5,1,1231,tian xia,Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,0,"This paper proposes a multi-objective optimization framework which supports heterogeneous information sources to improve alignment in machine translation system combination techniques. In this area, most of techniques usually utilize confusion networks (CN) as their central data structure to compact an exponential number of an potential hypotheses, and because better hypothesis alignment may benefit constructing better quality confusion networks, it is natural to add more useful information to improve alignment results. However, these information may be heterogeneous, so the widely-used Viterbi algorithm for searching the best alignment may not apply here. In the multi-objective optimization framework, each information source is viewed as an independent objective, and a new goal of improving all objectives can be searched by mature algorithms. The solutions from this framework, termed Pareto optimal solutions, are then combined to construct confusion networks. Experiments on two Chinese-to-English translation datasets show significant improvements, 0.97 and 1.06 BLEU points over a strong Indirected Hidden Markov Model-based (IHMM) system, and 4.75 and 3.53 points over the best single machine translation systems."
D13-1108,Translation with Source Constituency and Dependency Trees,2013,32,14,5,1,3627,fandong meng,Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,0,"We present a novel translation model, which simultaneously exploits the constituency and dependency trees on the source side, to combine the advantages of two types of trees. We take head-dependents relations of dependency trees as backbone and incorporate phrasal nodes of constituency trees as the source side of our translation rules, and the target side as strings. Our rules hold the property of long distance reorderings and the compatibility with phrases. Large-scale experimental results show that our model achieves significantly improvements over the constituency-to-string (2.45 BLEU on average) and dependencyto-string (0.91 BLEU on average) models, which only employ single type of trees, and significantly outperforms the state-of-theart hierarchical phrase-based model (1.12 BLEU on average), on three Chinese-English NIST test sets."
2013.mtsummit-european.4,Machine Translation in {CNGL} {II},2013,-1,-1,1,1,5775,qun liu,Proceedings of Machine Translation Summit XIV: European projects,0,None
W12-5704,System Combination with Extra Alignment Information,2012,-1,-1,4,1,35090,xiaofeng wu,Proceedings of the Second Workshop on Applying Machine Learning Techniques to Optimise the Division of Labour in Hybrid {MT},0,None
W12-4506,{ICT}: System Description for {C}o{NLL}-2012,2012,6,4,2,1,23879,hao xiong,Joint Conference on {EMNLP} and {C}o{NLL} - Shared Task,0,"In this paper, we present our system description for the CoNLL-2012 coreference resolution task on English, Chinese and Arabic. We investigate a projection-based model in which we first translate Chinese and Arabic into English, run a publicly available coreference system, and then use a new projection algorithm to map the coreferring entities back from English into mention candidates detected in the Chinese and Arabic source. We compare to a baseline that just runs the English coreference system on the supplied parses for Chinese and Arabic. Because our method does not beat the baseline system on the development set, we submit outputs generated by the baseline system as our final submission."
S12-1073,{ICT}:A System Combination for {C}hinese Semantic Dependency Parsing,2012,8,1,2,1,23879,hao xiong,"*{SEM} 2012: The First Joint Conference on Lexical and Computational Semantics {--} Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation ({S}em{E}val 2012)",0,"The goal of semantic dependency parsing is to build dependency structure and label semantic relation between a head and its modifier. To attain this goal, we concentrate on obtaining better dependency structure to predict better semantic relations, and propose a method to combine the results of three state-of-the-art dependency parsers. Unfortunately, we made a mistake when we generate the final output that results in a lower score of 56.31% in term of Labeled Attachment Score (LAS), reported by organizers. After giving golden testing set, we fix the bug and rerun the evaluation script, this time we obtain the score of 62.8% which is consistent with the results on developing set. We will report detailed experimental results with correct program as a comparison standard for further research."
S12-1108,{ICT}: A Translation based Method for Cross-lingual Textual Entailment,2012,25,4,3,1,3627,fandong meng,"*{SEM} 2012: The First Joint Conference on Lexical and Computational Semantics {--} Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation ({S}em{E}val 2012)",0,"In this paper, we present our system description in task of Cross-lingual Textual Entailment. The goal of this task is to detect entailment relations between two sentences written in different languages. To accomplish this goal, we first translate sentences written in foreign languages into English. Then, we use EDITS, an open source package, to recognize entailment relations. Since EDITS only draws monodirectional relations while the task requires bidirectional prediction, thus we exchange the hypothesis and test to detect entailment in another direction. Experimental results show that our method achieves promising results but not perfect results compared to other participants."
P12-2066,Identifying High-Impact Sub-Structures for Convolution Kernels in Document-level Sentiment Classification,2012,24,15,5,1,4187,zhaopeng tu,Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"Convolution kernels support the modeling of complex syntactic information in machine-learning tasks. However, such models are highly sensitive to the type and size of syntactic structure used. It is therefore an important challenge to automatically identify high impact sub-structures relevant to a given task. In this paper we present a systematic study investigating (combinations of) sequence and convolution kernels using different types of substructures in document-level sentiment classification. We show that minimal sub-structures extracted from constituency and dependency trees guided by a polarity lexicon show 1.45 point absolute improvement in accuracy over a bag-of-words classifier on a widely used sentiment corpus."
P12-1048,Translation Model Adaptation for Statistical Machine Translation with Monolingual Topic Information,2012,45,34,7,1,8403,jinsong su,Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"To adapt a translation model trained from the data in one domain to another, previous works paid more attention to the studies of parallel corpus while ignoring the in-domain monolingual corpora which can be obtained more easily. In this paper, we propose a novel approach for translation model adaptation by utilizing in-domain monolingual topic information instead of the in-domain bilingual corpora, which incorporates the topic information into translation probability estimation. Our method establishes the relationship between the out-of-domain bilingual corpus and the in-domain monolingual corpora via topic mapping and phrase-topic distribution probability estimation from in-domain monolingual corpora. Experimental result on the NIST Chinese-English translation task shows that our approach significantly outperforms the baseline system."
P12-1079,A Topic Similarity Model for Hierarchical Phrase-based Translation,2012,24,34,4,1,9396,xinyan xiao,Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"Previous work using topic model for statistical machine translation (SMT) explore topic information at the word level. However, SMT has been advanced from word-based paradigm to phrase/rule-based paradigm. We therefore propose a topic similarity model to exploit topic information at the synchronous rule level for hierarchical phrase-based translation. We associate each synchronous rule with a topic distribution, and select desirable rules according to the similarity of their topic distributions with given documents. We show that our model significantly improves the translation performance over the baseline on NIST Chinese-to-English translation experiments. Our model also achieves a better performance and a faster speed than previous approaches that work at the word level."
P12-1100,Hierarchical Chunk-to-String Translation,2012,31,5,4,1,4169,yang feng,Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"We present a hierarchical chunk-to-string translation model, which can be seen as a compromise between the hierarchical phrase-based model and the tree-to-string model, to combine the merits of the two models. With the help of shallow parsing, our model learns rules consisting of words and chunks and meanwhile introduce syntax cohesion. Under the weighed synchronous context-free grammar defined by these rules, our model searches for the best translation derivation and yields target translation simultaneously. Our experiments show that our model significantly outperforms the hierarchical phrase-based model and the tree-to-string model on English-Chinese Translation tasks."
D12-1038,Iterative Annotation Transformation with Predict-Self Reestimation for {C}hinese Word Segmentation,2012,27,7,3,1,23255,wenbin jiang,Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,0,"In this paper we first describe the technology of automatic annotation transformation, which is based on the annotation adaptation algorithm (Jiang et al., 2009). It can automatically transform a human-annotated corpus from one annotation guideline to another. We then propose two optimization strategies, iterative training and predict-self reestimation, to further improve the accuracy of annotation guideline transformation. Experiments on Chinese word segmentation show that, the iterative training strategy together with predict-self reestimation brings significant improvement over the simple annotation transformation baseline, and leads to classifiers with significantly higher accuracy and several times faster processing than annotation adaptation does. On the Penn Chinese Treebank 5.0, it achieves an F-measure of 98.43%, significantly outperforms previous works although using a single classifier with only local features."
D12-1109,Left-to-Right Tree-to-String Decoding with Prediction,2012,23,6,3,1,4169,yang feng,Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,0,"Decoding algorithms for syntax based machine translation suffer from high computational complexity, a consequence of intersecting a language model with a context free grammar. Left-to-right decoding, which generates the target string in order, can improve decoding efficiency by simplifying the language model evaluation. This paper presents a novel left to right decoding algorithm for tree-to-string translation, using a bottom-up parsing strategy and dynamic future cost estimation for each partial translation. Our method outperforms previously published tree-to-string decoders, including a competing left-to-right method."
C12-2080,Discriminative Boosting from Dictionary and Raw Text {--} A Novel Approach to Build A {C}hinese Word Segmenter,2012,22,0,4,1,3627,fandong meng,Proceedings of {COLING} 2012: Posters,0,"Chinese word segmentation (CWS) is a basic and important task for Chinese information processing. Standard approaches to CWS treat it as a sequence labelling task. Without manually annotated corpora, these approaches are ineffective. When a dictionary is available, dictionary maximum matching (DMM) is a good alternative. However, its performance is far from perfect due to the poor ability on out-of-vocabulary (OOV) words recognition. In this paper, we propose a novel approach that integrates the advantages of discriminative training and DMM, to build a high quality word segmenter with only a dictionary and a raw text. Experiments in CWS on different domains show that, compared with DMM, our approach brings significant improvements in both the news domain and the Chinese medicine patent domain, with error reductions of 21.50% and 13.66%, respectively. Furthermore, our approach achieves recall rate increments of OOV words by 42.54% and 23.72%, respectively in both domains."
C12-2122,Combining Multiple Alignments to Improve Machine Translation,2012,43,14,5,1,4187,zhaopeng tu,Proceedings of {COLING} 2012: Posters,0,"Word alignment is a critical component of machine translation systems. Various methods for word alignment have been proposed, and different models can produce significantly different outputs. To exploit the advantages of different models, we propose three ways to combine multiple alignments for machine translation: (1) alignment selection, a novel method to select an alignment with the least expected loss from multiple alignments within the minimum Bayes risk framework; (2) alignment refinement, an improved algorithm to refine multiple alignments into a new alignment that favors the consensus of various models; (3) alignment compaction, a compact representation that encodes all alignments generated by different methods (including (1) and (2) above) using a novel calculation of link probabilities. Experiments show that our approach not only improves the alignment quality, but also significantly improves translation performance by up to 1.96 BLEU points over single best alignments, and 1.28 points over merging rules extracted from multiple alignments individually."
C12-1176,Unsupervised Discriminative Induction of Synchronous Grammar for Machine Translation,2012,40,7,4,1,9396,xinyan xiao,Proceedings of {COLING} 2012,0,"We present a global log-linear model for synchronous grammar induction, which is capable of incorporating arbitrary features. The parameters in the model are trained in an unsuper- vised fashion from parallel sentences without word alignments. To make parameter training tractable, we also propose a novel and efficient cube pruning based synchronous parsing algo- rithm. Using learned synchronous grammar rules with millions of features that contain rule level, word level and translation boundary information, we significantly outperform a compet- itive hierarchical phrased-based baseline system by 1.4 BLEU on average on three NIST test"
W11-1911,{ETS}: An Error Tolerable System for Coreference Resolution,2011,11,2,5,1,23879,hao xiong,Proceedings of the Fifteenth Conference on Computational Natural Language Learning: Shared Task,0,"This paper presents our error tolerable system for coreference resolution in CoNLL-2011 (Pradhan et al., 2011) shared task (closed track). Different from most previous reported work, we detect mention candidates based on packed forest instead of single parse tree, and we use beam search algorithm based on the Bell Tree to create entities. Experimental results show that our methods achieve promising results on the development set."
P11-1128,Adjoining Tree-to-String Translation,2011,34,9,2,0.429373,1457,yang liu,Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,1,"We introduce synchronous tree adjoining grammars (TAG) into tree-to-string translation, which converts a source tree to a target string. Without reconstructing TAG derivations explicitly, our rule extraction algorithm directly learns tree-to-string rules from aligned Treebank-style trees. As tree-to-string translation casts decoding as a tree parsing problem rather than parsing, the decoder still runs fast when adjoining is included. Less than 2 times slower, the adjoining tree-to-string system improves translation quality by 0.7 BLEU over the baseline system only allowing for tree substitution on NIST Chinese-English test sets."
I11-1145,Extracting Hierarchical Rules from a Weighted Alignment Matrix,2011,23,8,3,1,4187,zhaopeng tu,Proceedings of 5th International Joint Conference on Natural Language Processing,0,"Word alignment is a fundamental step in machine translation. Current statistical machine translation systems suffer from a major drawback: they only extract rules from 1-best alignments, which adversely affects the rule sets quality due to alignment mistakes. To alleviate this problem, we extract hierarchical rules from weighted alignment matrix (Liu et al., 2009). Since the sub-phrase pairs would change the inside and outside areas in the weighted alignment matrix of the hierarchical rules, we propose a new algorithm to calculate the relative frequencies and lexical weights of hierarchical rules. To achieve a balance between rule table size and performance, we construct a scoring measure that incorporates both frequency and lexical weight to select the best target phrase for each source phrase. Experiments show that our approach improves BLEU score by ranging from 1.4 to 1.9 points over baseline for hierarchical phrase-based, and 1.4 to 1.5 points for tree-to-string model."
D11-1020,A novel dependency-to-string model for statistical machine translation,2011,22,52,3,1,4085,jun xie,Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,0,"Dependency structure, as a first step towards semantics, is believed to be helpful to improve translation quality. However, previous works on dependency structure based models typically resort to insertion operations to complete translations, which make it difficult to specify ordering information in translation rules. In our model of this paper, we handle this problem by directly specifying the ordering information in head-dependents rules which represent the source side as head-dependents relations and the target side as strings. The head-dependents rules require only substitution operation, thus our model requires no heuristics or separate ordering models of the previous works to control the word order of translations. Large-scale experiments show that our model performs well on long distance reordering, and outperforms the state-of-the-art constituency-to-string model (1.47 BLEU on average) and hierarchical phrase-based model (0.46 BLEU on average) on two Chinese-English NIST test sets without resort to phrases or parse forest. For the first time, a source dependency structure based model catches up with and surpasses the state-of-the-art translation models."
D11-1081,Fast Generation of Translation Forest for Large-Scale {SMT} Discriminative Training,2011,31,9,3,1,9396,xinyan xiao,Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,0,"Although discriminative training guarantees to improve statistical machine translation by incorporating a large amount of overlapping features, it is hard to scale up to large data due to decoding complexity. We propose a new algorithm to generate translation forest of training data in linear time with the help of word alignment. Our algorithm also alleviates the oracle selection problem by ensuring that a forest always contains derivations that exactly yield the reference translation. With millions of features trained on 519K sentences in 0.03 second per sentence, our system achieves significant improvement by 0.84 Bleu over the baseline system on the NIST Chinese-English test sets."
D11-1110,Relaxed Cross-lingual Projection of Constituent Syntax,2011,31,13,2,1,23255,wenbin jiang,Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,0,"We propose a relaxed correspondence assumption for cross-lingual projection of constituent syntax, which allows a supposed constituent of the target sentence to correspond to an unrestricted treelet in the source parse. Such a relaxed assumption fundamentally tolerates the syntactic non-isomorphism between languages, and enables us to learn the target-language-specific syntactic idiosyncrasy rather than a strained grammar directly projected from the source language syntax. Based on this assumption, a novel constituency projection method is also proposed in order to induce a projected constituent tree-bank from the source-parsed bilingual corpus. Experiments show that, the parser trained on the projected treebank dramatically outperforms previous projected and unsupervised parsers."
2011.mtsummit-wpt.4,Feedback Selecting of Manually Acquired Rules Using Automatic Evaluation,2011,-1,-1,4,0,41957,xianhua li,Proceedings of the 4th Workshop on Patent Translation,0,None
2011.mtsummit-papers.3,Maximum Rank Correlation Training for Statistical Machine Translation,2011,-1,-1,4,0,25863,daqi zheng,Proceedings of Machine Translation Summit XIII: Papers,0,None
2011.mtsummit-papers.33,Bagging-based System Combination for Domain Adaption,2011,-1,-1,4,1,3601,linfeng song,Proceedings of Machine Translation Summit XIII: Papers,0,None
2011.mtsummit-papers.42,Multi-granularity Word Alignment and Decoding for Agglutinative Language Translation,2011,-1,-1,3,1,41427,zhiyang wang,Proceedings of Machine Translation Summit XIII: Papers,0,None
Y10-1009,Statistical Translation Model Based On Source Syntax Structure,2010,8,0,1,1,5775,qun liu,"Proceedings of the 24th Pacific Asia Conference on Language, Information and Computation",0,"Syntax-based statistical translation model is proved to be better than phrase- based model, especially for language pairs with very different syntax structures, such as Chinese and English. In this talk I will introduce a serial of statistical translation models based on source syntax structure. The tree-based model uses the one best syntax tree for translation. The forest-based model uses a compact forest which encodes exponential number of syntax trees in a polynomial spaces and lead to better performance. The joint parsing and translation model produces source parse trees, using the source side of the translation rules instead of separate parsing rules, and generate translations on the target side simultaneously, which outperforms the forest-based model. Some extensions of these models are introduced also."
P10-2003,Learning Lexicalized Reordering Models from Reordering Graphs,2010,14,4,5,1,8403,jinsong su,Proceedings of the {ACL} 2010 Conference Short Papers,0,"Lexicalized reordering models play a crucial role in phrase-based translation systems. They are usually learned from the word-aligned bilingual corpus by examining the reordering relations of adjacent phrases. Instead of just checking whether there is one phrase adjacent to a given phrase, we argue that it is important to take the number of adjacent phrases into account for better estimations of reordering models. We propose to use a structure named reordering graph, which represents all phrase segmentations of a sentence pair, to learn lexicalized reordering models efficiently. Experimental results on the NIST Chinese-English test sets show that our approach significantly outperforms the baseline method."
P10-2026,Better Filtration and Augmentation for Hierarchical Phrase-Based Translation Rules,2010,17,3,3,1,41427,zhiyang wang,Proceedings of the {ACL} 2010 Conference Short Papers,0,"This paper presents a novel filtration criterion to restrict the rule extraction for the hierarchical phrase-based translation model, where a bilingual but relaxed well-formed dependency restriction is used to filter out bad rules. Furthermore, a new feature which describes the regularity that the source/target dependency edge triggers the target/source word is also proposed. Experimental results show that, the new criteria weeds out about 40% rules while with translation performance improvement, and the new feature brings another improvement to the baseline system, especially on larger corpus."
P10-1002,Dependency Parsing and Projection Based on Word-Pair Classification,2010,34,19,2,1,23255,wenbin jiang,Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,1,"In this paper we describe an intuitionistic method for dependency parsing, where a classifier is used to determine whether a pair of words forms a dependency edge. And we also propose an effective strategy for dependency projection, where the dependency relationships of the word pairs in the source language are projected to the word pairs of the target language, leading to a set of classification instances rather than a complete tree. Experiments show that, the classifier trained on the projected classification instances significantly outperforms previous projected dependency parsers. More importantly, when this classifier is integrated into a maximum spanning tree (MST) dependency parser, obvious improvement is obtained over the MST baseline."
P10-1145,Constituency to Dependency Translation with Forests,2010,34,14,2,1,7643,haitao mi,Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,1,"Tree-to-string systems (and their forest-based extensions) have gained steady popularity thanks to their simplicity and efficiency, but there is a major limitation: they are unable to guarantee the grammaticality of the output, which is explicitly modeled in string-to-tree systems via target-side syntax. We thus propose to combine the advantages of both, and present a novel constituency-to-dependency translation model, which uses constituency forests on the source side to direct the translation, and dependency trees on the target side (as a language model) to ensure grammaticality. Medium-scale experiments show an absolute and statistically significant improvement of 0.7 BLEU points over a state-of-the-art forest-based tree-to-string system even with fewer rules. This is also the first time that a tree-to-tree model can surpass tree-to-string counterparts."
J10-3002,Discriminative Word Alignment by Linear Modeling,2010,42,41,2,0.542645,1457,yang liu,Computational Linguistics,0,"Word alignment plays an important role in many NLP tasks as it indicates the correspondence between words in a parallel text. Although widely used to align large bilingual corpora, generative models are hard to extend to incorporate arbitrary useful linguistic information. This article presents a discriminative framework for word alignment based on a linear model. Within this framework, all knowledge sources are treated as feature functions, which depend on a source language sentence, a target language sentence, and the alignment between them. We describe a number of features that could produce symmetric alignments. Our model is easy to extend and can be optimized with respect to evaluation metrics directly. The model achieves state-of-the-art alignment quality on three word alignment shared tasks for five language pairs with varying divergence and richness of resources. We further show that our approach improves translation performance for various statistical machine translation systems."
C10-2033,An Efficient Shift-Reduce Decoding Algorithm for Phrased-Based Machine Translation,2010,14,14,4,1,4169,yang feng,Coling 2010: Posters,0,"In statistical machine translation, decoding without any reordering constraint is an NP-hard problem. Inversion Transduction Grammars (ITGs) exploit linguistic structure and can well balance the needed flexibility against complexity constraints. Currently, translation models with ITG constraints usually employs the cube-time CYK algorithm. In this paper, we present a shift-reduce decoding algorithm that can generate ITG-legal translation from left to right in linear time. This algorithm runs in a reduce-eager style and is suited to phrase-based models. Using the state-of-the-art decoder Moses as the baseline, experiment results show that the shift-reduce algorithm can significantly improve both the accuracy and the speed on different test sets."
C10-2059,Effective Constituent Projection across Languages,2010,31,3,4,1,23255,wenbin jiang,Coling 2010: Posters,0,"We describe an effective constituent projection strategy, where constituent projection is performed on the basis of dependency projection. Especially, a novel measurement is proposed to evaluate the candidate projected constituents for a target language sentence, and a PCFG-style parsing procedure is then used to search for the most probable projected constituent tree. Experiments show that, the parser trained on the projected treebank can significantly boost a state-of-the-art supervised parser. When integrated into a tree-based machine translation system, the projected parser leads to translation performance comparable with using a supervised parser trained on thousands of annotated trees."
C10-2096,Machine Translation with Lattices and Forests,2010,28,2,3,1,7643,haitao mi,Coling 2010: Posters,0,"Traditional 1-best translation pipelines suffer a major drawback: the errors of 1-best outputs, inevitably introduced by each module, will propagate and accumulate along the pipeline. In order to alleviate this problem, we use compact structures, lattice and forest, in each module instead of 1-best results. We integrate both lattice and forest into a single tree-to-string system, and explore the algorithms of lattice parsing, lattice-forest-based rule extraction and decoding. More importantly, our model takes into account all the probabilities of different steps, such as segmentation, parsing, and translation. The main advantage of our model is that we can make global decision to search for the best segmentation, parse-tree and translation in one step. Medium-scale experiments show an improvement of 0.9 BLEU points over a state-of-the-art forest-based baseline."
C10-2136,Dependency-Based Bracketing Transduction Grammar for Statistical Machine Translation,2010,25,2,6,1,8403,jinsong su,Coling 2010: Posters,0,"In this paper, we propose a novel dependency-based bracketing transduction grammar for statistical machine translation, which converts a source sentence into a target dependency tree. Different from conventional bracketing transduction grammar models, we encode target dependency information into our lexical rules directly, and then we employ two different maximum entropy models to determine the reordering and combination of partial dependency structures, when we merge two neighboring blocks. By incorporating dependency language model further, large-scale experiments on Chinese-English task show that our system achieves significant improvements over the baseline system on various test sets even with fewer phrases."
C10-1080,Joint Parsing and Translation,2010,32,18,2,0.542645,1457,yang liu,Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010),0,"Tree-based translation models, which exploit the linguistic syntax of source language, usually separate decoding into two steps: parsing and translation. Although this separation makes tree-based decoding simple and efficient, its translation performance is usually limited by the number of parse trees offered by parser. Alternatively, we propose to parse and translate jointly by casting tree-based translation as parsing. Given a source-language sentence, our joint decoder produces a parse tree on the source side and a translation on the target side simultaneously. By combining translation and parsing models in a discriminative framework, our approach significantly outperforms a forest-based tree-to-string system by 1.1 absolute BLEU points on the NIST 2005 Chinese-English test set. As a parser, our joint decoder achieves an F1 score of 80.6% on the Penn Chinese Treebank."
C10-1123,Dependency Forest for Statistical Machine Translation,2010,26,26,4,1,4187,zhaopeng tu,Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010),0,We propose a structure called dependency forest for statistical machine translation. A dependency forest compactly represents multiple dependency trees. We develop new algorithms for extracting string-to-dependency rules and training dependency language models. Our forest-based string-to-dependency system obtains significant improvements ranging from 1.36 to 1.46 BLEU points over the tree-based baseline on the NIST 2004/2005/2006 Chinese-English test sets.
C10-1135,Joint Tokenization and Translation,2010,30,12,4,1,9396,xinyan xiao,Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010),0,"As tokenization is usually ambiguous for many natural languages such as Chinese and Korean, tokenization errors might potentially introduce translation mistakes for translation systems that rely on 1-best to-kenizations. While using lattices to offer more alternatives to translation systems have elegantly alleviated this problem, we take a further step to tokenize and translate jointly. Taking a sequence of atomic units that can be combined to form words in different ways as input, our joint decoder produces a tokenization on the source side and a translation on the target side simultaneously. By integrating tokenization and translation features in a discriminative framework, our joint decoder outperforms the baseline translation systems using 1-best tokenizations and lattices significantly on both Chinese-English and Korean-Chinese tasks. Interestingly, as a tokenizer, our joint decoder achieves significant improvements over monolingual Chinese tokenizers."
2010.iwslt-evaluation.8,The {ICT} statistical machine translation system for {IWSLT} 2010,2010,20,1,9,1,23879,hao xiong,Proceedings of the 7th International Workshop on Spoken Language Translation: Evaluation Campaign,0,"This paper illustrates the ICT Statistical Machine Transla tion system used in the evaluation campaign of the International Workshop on Spoken Language Translation 2010. We participate in the DIALOG tasks for Chinese-to-English and English-to-Chinese translation respectively. For both ta sks, our system has achieved significant improvement with several effective methods as follows: 1) refining the data preprocessing, including Chinese word segmentation, named entity recognition, etc. 2) reducing the number of Out-ofVocabulary(OOV) on the final test set by applying a fuzzy matching strategy. 3) considering generating a better input for the decoder from the N-best lists of ASR output as a special kind of translation task for the ASR task. 4) improving the performance of every single decoder, and reranking the n-best list for the final results submitted."
W09-3803,Automatic Adaptation of Annotation Standards for Dependency Parsing ? Using Projected Treebank as Source Corpus,2009,18,12,2,1,23255,wenbin jiang,Proceedings of the 11th International Conference on Parsing Technologies ({IWPT}{'}09),0,"We describe for dependency parsing an annotation adaptation strategy, which can automatically transfer the knowledge from a source corpus with a different annotation standard to the desired target parser, with the supervision by a target corpus annotated in the desired standard. Furthermore, instead of a hand-annotated one, a projected treebank derived from a bilingual corpus is used as the source corpus. This benefits the resource-scarce languages which haven't different handannotated treebanks. Experiments show that the target parser gains significant improvement over the baseline parser trained on the target corpus only, when the target corpus is smaller."
W09-2907,Improving Statistical Machine Translation Using Domain Bilingual Multiword Expressions,2009,26,72,4,0,46938,zhixiang ren,"Proceedings of the Workshop on Multiword Expressions: Identification, Interpretation, Disambiguation and Applications ({MWE} 2009)",0,"Multiword expressions (MWEs) have been proved useful for many natural language processing tasks. However, how to use them to improve performance of statistical machine translation (SMT) is not well studied. This paper presents a simple yet effective strategy to extract domain bilingual multiword expressions. In addition, we implement three methods to integrate bilingual MWEs to Moses, the state-of-the-art phrase-based machine translation system. Experiments show that bilingual MWEs could improve translation performance significantly."
P09-2031,Reducing {SMT} Rule Table with Monolingual Key Phrase,2009,6,5,5,1,6457,zhongjun he,Proceedings of the {ACL}-{IJCNLP} 2009 Conference Short Papers,0,"This paper presents an effective approach to discard most entries of the rule table for statistical machine translation. The rule table is filtered by monolingual key phrases, which are extracted from source text using a technique based on term extraction. Experiments show that 78% of the rule table is reduced without worsening translation performance. In most cases, our approach results in measurable improvements in BLEU score."
P09-2035,Sub-Sentence Division for Tree-Based Machine Translation,2009,19,10,5,1,23879,hao xiong,Proceedings of the {ACL}-{IJCNLP} 2009 Conference Short Papers,0,"Tree-based statistical machine translation models have made significant progress in recent years, especially when replacing 1-best trees with packed forests. However, as the parsing accuracy usually goes down dramatically with the increase of sentence length, translating long sentences often takes long time and only produces degenerate translations. We propose a new method named sub-sentence division that reduces the decoding time and improves the translation quality for tree-based translation. Our approach divides long sentences into several sub-sentences by exploiting tree structures. Large-scale experiments on the NIST 2008 Chinese-to-English test set show that our approach achieves an absolute improvement of 1.1 BLEU points over the baseline system in 50% less time."
P09-1059,Automatic Adaptation of Annotation Standards: {C}hinese Word Segmentation and {POS} Tagging {--} A Case Study,2009,26,65,3,1,23255,wenbin jiang,Proceedings of the Joint Conference of the 47th Annual Meeting of the {ACL} and the 4th International Joint Conference on Natural Language Processing of the {AFNLP},1,"Manually annotated corpora are valuable but scarce resources, yet for many annotation tasks such as treebanking and sequence labeling there exist multiple corpora with different and incompatible annotation guidelines or standards. This seems to be a great waste of human efforts, and it would be nice to automatically adapt one annotation standard to another. We present a simple yet effective strategy that transfers knowledge from a differently annotated corpus to the corpus with desired annotation. We test the efficacy of this method in the context of Chinese word segmentation and part-of-speech tagging, where no segmentation and POS tagging standards are widely accepted due to the lack of morphology in Chinese. Experiments show that adaptation from the much larger People's Daily corpus to the smaller but more popular Penn Chinese Treebank results in significant improvements in both segmentation and tagging accuracies (with error reductions of 30.2% and 14%, respectively), which in turn helps improve Chinese parsing accuracy."
P09-1063,Improving Tree-to-Tree Translation with Packed Forests,2009,26,72,3,0.653004,1457,yang liu,Proceedings of the Joint Conference of the 47th Annual Meeting of the {ACL} and the 4th International Joint Conference on Natural Language Processing of the {AFNLP},1,"Current tree-to-tree models suffer from parsing errors as they usually use only 1-best parses for rule extraction and decoding. We instead propose a forest-based tree-to-tree model that uses packed forests. The model is based on a probabilistic synchronous tree substitution grammar (STSG), which can be learned from aligned forest pairs automatically. The decoder finds ways of decomposing trees in the source forest into elementary trees using the source projection of STSG while building target forest in parallel. Comparable to the state-of-the-art phrase-based system Moses, using packed forests in tree-to-tree translation results in a significant absolute improvement of 3.6 BLEU points over using 1-best trees."
P09-1065,Joint Decoding with Multiple Translation Models,2009,30,36,4,0.653004,1457,yang liu,Proceedings of the Joint Conference of the 47th Annual Meeting of the {ACL} and the 4th International Joint Conference on Natural Language Processing of the {AFNLP},1,"Current SMT systems usually decode with single translation models and cannot benefit from the strengths of other models in decoding phase. We instead propose joint decoding, a method that combines multiple translation models in one decoder. Our joint decoder draws connections among multiple models by integrating the translation hypergraphs they produce individually. Therefore, one model can share translations and even derivations with other models. Comparable to the state-of-the-art system combination technique, joint decoding achieves an absolute improvement of 1.5 BLEU points over individual decoding."
D09-1106,Weighted Alignment Matrices for Statistical Machine Translation,2009,25,35,4,0.653004,1457,yang liu,Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,0,Current statistical machine translation systems usually extract rules from bilingual corpora annotated with 1-best alignments. They are prone to learn noisy rules due to alignment mistakes. We propose a new structure called weighted alignment matrix to encode all possible alignments for a parallel text compactly. The key idea is to assign a probability to each word pair to indicate how well they are aligned. We design new algorithms for extracting phrase pairs from weighted alignment matrices and estimating their probabilities. Our experiments on multiple language pairs show that using weighted matrices achieves consistent improvements over using n-best lists in significant less extraction time.
D09-1115,Lattice-based System Combination for Statistical Machine Translation,2009,14,19,4,1,4169,yang feng,Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,0,"Current system combination methods usually use confusion networks to find consensus translations among different systems. Requiring one-to-one mappings between the words in candidate translations, confusion networks have difficulty in handling more general situations in which several words are connected to another several words. Instead, we propose a lattice-based system combination model that allows for such phrase alignments and uses lattices to encode all candidate translations. Experiments show that our approach achieves significant improvements over the state-of-the-art baseline system on Chinese-to-English translation test sets."
D09-1127,Bilingually-Constrained (Monolingual) Shift-Reduce Parsing,2009,25,61,3,0.473722,8438,liang huang,Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,0,"Jointly parsing two languages has been shown to improve accuracies on either or both sides. However, its search space is much bigger than the monolingual case, forcing existing approaches to employ complicated modeling and crude approximations. Here we propose a much simpler alternative, bilingually-constrained monolingual parsing, where a source-language parser learns to exploit reorderings as additional observation, but not bothering to build the target-side tree as well. We show specifically how to enhance a shift-reduce dependency parser with alignment features to resolve shift-reduce conflicts. Experiments on the bilingual portion of Chinese Treebank show that, with just 3 bilingual features, we can improve parsing accuracies by 0.6% (absolute) for both English and Chinese over a state-of-the-art baseline, with negligible (~6%) efficiency overhead, thus much faster than biparsing."
2009.mtsummit-papers.20,Introduction to {C}hina{'}s {CWMT}2008 Machine Translation Evaluation,2009,8,3,3,0.952381,40427,hongmei zhao,Proceedings of Machine Translation Summit XII: Papers,0,"This paper presents an overall introduction to the CWMT2008 evaluation and focuses on its two new metrics: BLEU-SBP (Chiang et al., 2008) and linguistic check-point method (Zhou et al., 2008). BLEU-SBP is a revised BLEU with strict brevity penalty. Our experiments validated BLEU-SBPxe2x80x99s effectivity in resolving the nondecomposability problem of both NIST-BLEU and IBMBLEU at sentence level. Linguistic checkpoint method (LCM) is a linguistic diagnostic evaluation method based on automatically constructed linguistic check-points, and our evaluation indicates that this method can be used to evaluate the capability of an MT system in translating various linguistic phenomena, and revealed good correlations between BLEU score and LCM scores in most tasks. With the aid of these metrics, we disclosed some detailed performance differences between statistical MT systems and rulebased MT systems. In addition, through the study on some practical cases we suggest that the high BLEU score doesnxe2x80x99t necessarily mean high translation adequacy."
2009.iwslt-evaluation.8,The {ICT} statistical machine translation system for the {IWSLT} 2009,2009,17,2,11,1,7643,haitao mi,Proceedings of the 6th International Workshop on Spoken Language Translation: Evaluation Campaign,0,"This paper describes the ICT Statistical Machine Translation systems that used in the evaluation campaign of the International Workshop on Spoken Language Translation (IWSLT) 2009. For this year{'}s evaluation, we participated in the Challenge Task (Chinese-English and English-Chinese) and BTEC Task (Chinese-English). And we mainly focus on one new method to improve single system{'}s translation quality. Specifically, we developed a sentence-similarity based development set selection technique. For each task, we finally submitted the single system who got the maximum BLEU scores on the selected development set. The four single translation systems are based on different techniques: a linguistically syntax-based system, two formally syntax-based systems and a phrase-based system. Typically, we didn{'}t use any rescoring or system combination techniques in this year{'}s evaluation."
P08-2041,Partial Matching Strategy for Phrase-based Statistical Machine Translation,2008,10,2,2,1,6457,zhongjun he,"Proceedings of ACL-08: HLT, Short Papers",0,This paper presents a partial matching strategy for phrase-based statistical machine translation (PBSMT). Source phrases which do not appear in the training corpus can be translated by word substitution according to partially matched phrases. The advantage of this method is that it can alleviate the data sparseness problem if the amount of bilingual corpus is limited. We incorporate our approach into the state-of-the-art PBSMT system Moses and achieve statistically significant improvements on both small and large corpora.
P08-1023,Forest-Based Translation,2008,24,133,3,1,7643,haitao mi,Proceedings of ACL-08: HLT,1,"Among syntax-based translation models, the tree-based approach, which takes as input a parse tree of the source sentence, is a promising direction being faster and simpler than its string-based counterpart. However, current tree-based systems suffer from a major drawback: they only use the 1-best parse to direct the translation, which potentially introduces translation mistakes due to parsing errors. We propose a forest-based approach that translates a packed forest of exponentially many parses, which encodes many more alternatives than standard n-best lists. Large-scale experiments show an absolute improvement of 1.7 BLEU points over the 1-best baseline. This result is also 0.8 points higher than decoding with 30-best parses, and takes even less time."
P08-1102,A Cascaded Linear Model for Joint {C}hinese Word Segmentation and Part-of-Speech Tagging,2008,11,95,3,1,23255,wenbin jiang,Proceedings of ACL-08: HLT,1,"We propose a cascaded linear model for joint Chinese word segmentation and partof-speech tagging. With a character-based perceptron as the core, combined with realvalued features such as language models, the cascaded model is able to efficiently utilize knowledge sources that are inconvenient to incorporate into the perceptron directly. Experiments show that the cascaded model achieves improved accuracies on both segmentation only and joint segmentation and part-of-speech tagging. On the Penn Chinese Treebank 5.0, we obtain an error reduction of 18.5% on segmentation and 12% on joint segmentation and part-of-speech tagging over the perceptron-only baseline."
I08-1066,Refinements in {BTG}-based Statistical Machine Translation,2008,19,10,5,1,3236,deyi xiong,Proceedings of the Third International Joint Conference on Natural Language Processing: Volume-{I},0,"Bracketing Transduction Grammar (BTG) has been well studied and used in statistical machine translation (SMT) with promising results. However, there are two major issues for BTG-based SMT. First, there is no effective mechanism available for predicting orders between neighboring blocks in the original BTG. Second, the computational cost is high. In this paper, we introduce two refinements for BTG-based SMT to achieve better reordering and higher-speed decoding, which include (1) reordering heuristics to prevent incorrect swapping and reduce search space, and (2) special phrases with tags to indicate sentence beginning and ending. The two refinements are integrated into a well-established BTG-based Chinese-toEnglish SMT system that is trained on largescale parallel data. Experimental results on the NIST MT-05 task show that the proposed refinements contribute significant improvement of 2% in BLEU score over the baseline system."
D08-1010,Maximum Entropy based Rule Selection Model for Syntax-based Statistical Machine Translation,2008,19,26,1,1,5775,qun liu,Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,0,"This paper proposes a novel maximum entropy based rule selection (MERS) model for syntax-based statistical machine translation (SMT). The MERS model combines local contextual information around rules and information of sub-trees covered by variables in rules. Therefore, our model allows the decoder to perform context-dependent rule selection during decoding. We incorporate the MERS model into a state-of-the-art linguistically syntax-based SMT model, the tree-to-string alignment template model. Experiments show that our approach achieves significant improvements over the baseline system."
C08-1041,Improving Statistical Machine Translation using Lexicalized Rule Selection,2008,22,46,2,1,6457,zhongjun he,Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008),0,This paper proposes a novel lexicalized approach for rule selection for syntax-based statistical machine translation (SMT). We build maximum entropy (MaxEnt) models which combine rich context information for selecting translation rules during decoding. We successfully integrate the MaxEnt-based rule selection models into the state-of-the-art syntax-based SMT model. Experiments show that our lexicalized approach for rule selection achieves statistically significant improvements over the state-of-the-art SMT system.
C08-1049,Word Lattice Reranking for {C}hinese Word Segmentation and Part-of-Speech Tagging,2008,13,46,3,1,23255,wenbin jiang,Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008),0,"In this paper, we describe a new reranking strategy named word lattice reranking, for the task of joint Chinese word segmentation and part-of-speech (POS) tagging. As a derivation of the forest reranking for parsing (Huang, 2008), this strategy reranks on the pruned word lattice, which potentially contains much more candidates while using less storage, compared with the traditional n-best list reranking. With a perceptron classifier trained with local features as the baseline, word lattice reranking performs reranking with non-local features that can't be easily incorporated into the perceptron baseline. Experimental results show that, this strategy achieves improvement on both segmentation and POS tagging, above the perceptron baseline and the n-best list reranking."
2008.iwslt-evaluation.7,The {ICT} system description for {IWSLT} 2008.,2008,18,1,8,0.655085,1457,yang liu,Proceedings of the 5th International Workshop on Spoken Language Translation: Evaluation Campaign,0,"This paper presents a description for the ICT systems involved in the IWSLT 2008 evaluation campaign. This year, we participated in Chinese-English and English-Chinese translation directions. Four statistical machine translation systems were used: one linguistically syntax-based, two formally syntax-based, and one phrase-based. The outputs of the four SMT systems were fed to a sentence-level system combiner, which was expected to produce better translations than single systems. We will report the results of the four single systems and the combiner on both the development and test sets."
W07-0706,A Dependency Treelet String Correspondence Model for Statistical Machine Translation,2007,17,21,2,1,3236,deyi xiong,Proceedings of the Second Workshop on Statistical Machine Translation,0,"This paper describes a novel model using dependency structures on the source side for syntax-based statistical machine translation: Dependency Treelet String Correspondence Model (DTSC). The DTSC model maps source dependency structures to target strings. In this model translation pairs of source treelets and target strings with their word alignments are learned automatically from the parsed and aligned corpus. The DTSC model allows source treelets and target strings with variables so that the model can generalize to handle dependency structures with the same head word but with different modifiers and arguments. Additionally, target strings can be also discontinuous by using gaps which are corresponding to the uncovered nodes which are not included in the source treelets. A chart-style decoding algorithm with two basic operations--substituting and attaching--is designed for the DTSC model. We argue that the DTSC model proposed here is capable of lexicalization, generalization, and handling discontinuous phrases which are very desirable for machine translation. We finally evaluate our current implementation of a simplified version of DTSC for statistical machine translation."
P07-1089,Forest-to-String Statistical Translation Rules,2007,16,54,3,0.833333,1457,yang liu,Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,1,"In this paper, we propose forest-to-string rules to enhance the expressive power of tree-to-string translation models. A forestto-string rule is capable of capturing nonsyntactic phrase pairs by describing the correspondence between multiple parse trees and one string. To integrate these rules into tree-to-string translation models, auxiliary rules are introduced to provide a generalization level. Experimental results show that, on the NIST 2005 Chinese-English test set, the tree-to-string model augmented with forest-to-string rules achieves a relative improvement of 4.3% in terms of BLEU score over the original model which allows treeto-string rules only."
D07-1036,Improving Statistical Machine Translation Performance by Training Data Selection and Optimization,2007,17,114,3,1,37737,yajuan lu,Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning ({EMNLP}-{C}o{NLL}),0,"Parallel corpus is an indispensable resource for translation model training in statistical machine translation (SMT). Instead of collecting more and more parallel training corpora, this paper aims to improve SMT performance by exploiting full potential of the existing parallel corpora. Two kinds of methods are proposed: offline data optimization and online model optimization. The offline method adapts the training data by redistributing the weight of each training sentence pairs. The online method adapts the translation model by redistributing the weight of each predefined submodels. Information retrieval model is used for the weighting scheme in both methods. Experimental results show that without using any additional resource, both methods can improve SMT performance significantly."
2007.iwslt-1.17,The {ICT} statistical machine translation systems for {IWSLT} 2007,2007,10,2,9,1,6457,zhongjun he,Proceedings of the Fourth International Workshop on Spoken Language Translation,0,"In this paper, we give an overview of the ICT statistical machine translation systems for the evaluation campaign of the International Workshop on Spoken Language Translation (IWSLT) 2007. In this year{'}s evaluation, we participated in the Chinese-English transcript translation task, and developed three systems based on different techniques: a formally syntax-based system Bruin, an extended phrase-based system Confucius and a linguistically syntax-based system Lynx. We will describe the models of these three systems, and compare their performance in detail. We set Bruin as our primary system, which ranks 2 among the 15 primary results according to the official evaluation results."
P06-1066,Maximum Entropy Based Phrase Reordering Model for Statistical Machine Translation,2006,20,228,2,1,3236,deyi xiong,Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,1,"We propose a novel reordering model for phrase-based statistical machine translation (SMT) that uses a maximum entropy (MaxEnt) model to predicate reorderings of neighbor blocks (phrase pairs). The model provides content-dependent, hierarchical phrasal reordering with generalization based on features automatically learned from a real-world bitext. We present an algorithm to extract all reordering events of neighbor blocks from bilingual data. In our experiments on Chinese-to-English translation, this MaxEnt-based reordering model obtains significant improvements in BLEU score on the NIST MT-05 and IWSLT-04 tasks."
P06-1077,Tree-to-String Alignment Template for Statistical Machine Translation,2006,24,289,2,0.833333,1457,yang liu,Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,1,"We present a novel translation model based on tree-to-string alignment template (TAT) which describes the alignment between a source parse tree and a target string. A TAT is capable of generating both terminals and non-terminals and performing reordering at both low and high levels. The model is linguistically syntax-based because TATs are extracted automatically from word-aligned, source side parsed parallel texts. To translate a source sentence, we first employ a parser to produce a source parse tree and then apply TATs to transform the tree into a target string. Our experiments show that the TAT-based model significantly outperforms Pharaoh, a state-of-the-art decoder for phrase-based models."
P05-1057,Log-Linear Models for Word Alignment,2005,19,82,2,0.833333,1457,yang liu,Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics ({ACL}{'}05),1,"We present a framework for word alignment based on log-linear models. All knowledge sources are treated as feature functions, which depend on the source language sentence, the target language sentence and possible additional variables. Log-linear models allow statistical alignment models to be easily extended by incorporating syntactic information. In this paper, we use IBM Model 3 alignment probabilities, POS correspondence, and bilingual dictionary coverage as features. Our experiments show that log-linear models significantly outperform IBM translation models."
I05-1007,Parsing the {P}enn {C}hinese Treebank with Semantic Knowledge,2005,16,64,3,1,3236,deyi xiong,Second International Joint Conference on Natural Language Processing: Full Papers,0,"We build a class-based selection preference sub-model to incorporate external semantic knowledge from two Chinese electronic semantic dictionaries. This sub-model is combined with modifier-head generation sub-model. After being optimized on the held out data by the EM algorithm, our improved parser achieves 79.4% (F1 measure), as well as a 4.4% relative decrease in error rate on the Penn Chinese Treebank (CTB). Further analysis of performance improvement indicates that semantic knowledge is helpful for nominal compounds, coordination, and Nxe2x8bx84V tagging disambiguation, as well as alleviating the sparseness of information available in treebank."
2005.mtsummit-papers.18,A Multi-aligner for {J}apanese-{C}hinese Parallel Corpora,2005,-1,-1,2,0,9062,yujie zhang,Proceedings of Machine Translation Summit X: Papers,0,"Automatic word alignment is an important technology for extracting translation knowledge from parallel corpora. However, automatic techniques cannot resolve this problem completely because of variances in translations. We therefore need to investigate the performance potential of automatic word alignment and then decide how to suitably apply it. In this paper we first propose a lexical knowledge-based approach to word alignment on a Japanese-Chinese corpus. Then we evaluate the performance of the proposed approach on the corpus. At the same time we also apply a statistics-based approach, the well-known toolkit GIZA++, to the same test data. Through comparison of the performances of the two approaches, we propose a multi-aligner, exploiting the lexical knowledge-based aligner and the statistics-based aligner at the same time. Quantitative results confirmed the effectiveness of the multi-aligner."
2005.mtsummit-invited.7,Introduction to {C}hina{'}s {HTRDP} Machine Translation Evaluation,2005,-1,-1,1,1,5775,qun liu,Proceedings of Machine Translation Summit X: Invited papers,0,"Since 1994, China{'}s HTRDP machine translation evaluation has been conducted for five times. Systems of various translation directions between Chinese, English, Japanese and French have been tested. Both human evaluation and automatic evaluation are conducted in HTRDP evaluation. In recent years, the evaluation was organized jointly with NICT of Japan. This paper introduces some details of this evaluation."
W03-1709,{C}hinese Lexical Analysis Using Hierarchical Hidden {M}arkov Model,2003,14,104,2,0,45169,huaping zhang,Proceedings of the Second {SIGHAN} Workshop on {C}hinese Language Processing,0,"This paper presents a unified approach for Chinese lexical analysis using hierarchical hidden Markov model (HHMM), which aims to incorporate Chinese word segmentation, Part-Of-Speech tagging, disambiguation and unknown words recognition into a whole theoretical frame. A class-based HMM is applied in word segmentation, and in this level unknown words are treated in the same way as common words listed in the lexicon. Unknown words are recognized with reliability in role-based HMM. As for disambiguation, the authors bring forth an n-shortest-path strategy that, in the early stage, reserves top N segmentation results as candidates and covers more ambiguity. Various experiments show that each level in HHMM contributes to lexical analysis. An HHMM-based system ICTCLAS was accomplished. The recent official evaluation indicates that ICTCLAS is one of the best Chinese lexical analyzers. In a word, HHMM is effective to Chinese lexical analysis."
W03-1730,{HHMM}-based {C}hinese Lexical Analyzer {ICTCLAS},2003,4,373,4,0,45169,huaping zhang,Proceedings of the Second {SIGHAN} Workshop on {C}hinese Language Processing,0,"This document presents the results from Inst. of Computing Tech., CAS in the ACL SIGHAN-sponsored First International Chinese Word Segmentation Bake-off. The authors introduce the unified HHMM-based frame of our Chinese lexical analyzer ICTCLAS and explain the operation of the six tracks. Then provide the evaluation results and give more analysis. Evaluation on ICTCLAS shows that its performance is competitive. Compared with other system, ICTCLAS has ranked top both in CTB and PK closed track. In PK open track, it ranks second position. ICTCLAS BIG5 version was transformed from GB version only in two days; however, it achieved well in two BIG5 closed tracks. Through the first bakeoff, we could learn more about the development in Chinese word segmentation and become more confident on our HHMM-based approach. At the same time, we really find our problems during the evaluation. The bakeoff is interesting and helpful."
O03-5002,{C}hinese Named Entity Recognition Using Role Model,2003,10,40,2,0,45169,huaping zhang,"International Journal of Computational Linguistics {\\&} {C}hinese Language Processing, Volume 8, Number 2, August 2003",0,"This paper presents a stochastic model to tackle the problem of Chinese named entity recognition. In this research, we unify component tokens of named entity and their contexts into a generalized role set, which is like part-of-speech (POS). The probabilities of role emission and transition are acquired after machine learning on a role-labeled data set, which is transformed from a hand-corrected corpus after word segmentation and POS tagging are performed. Given an original string, role Viterbi tagging is employed on tokens segmented in the initial process. Then named entities are identified and classified through maximum matching on the best role sequence. In addition, named entity recognition using role model is incorporated along with the unified class-based bigram model for word segmentation. Thus, named entity candidates can be further selected in the final process of Chinese lexical analysis. Various evaluations conducted using one"
W02-1817,Automatic Recognition of {C}hinese Unknown Words Based on Roles Tagging,2002,3,52,2,0,53160,kevin zhang,{COLING}-02: The First {SIGHAN} Workshop on {C}hinese Language Processing,0,"This paper presents a unified solution, which is based on the idea of roles tagging, to the complicated problems of Chinese unknown words recognition. In our approach, an unknown word is identified according to its component tokens and context tokens. In order to capture the functions of tokens, we use the concept of roles. Roles are tagged through applying the Viterbi algorithm in the fashion of a POS tagger. In the resulted most probable roles sequence, all the eligible unknown words are recognized through a maximum patterns matching. We have got excellent precision and recalling rates, especially for person names and transliterations. The result and experiments in our system ICTCLAS shows that our approach based on roles tagging is simple yet effective."
W02-1117,A Character-net Based {C}hinese Text Segmentation Method,2002,3,5,2,0,53210,lixin zhou,{COLING}-02: {SEMANET}: Building and Using Semantic Networks,0,"The segmentation of Chinese texts is a key process in Chinese information processing. The difficulties in segmentation are the process of ambiguous character string and unknown Chinese words. In order to obtain the correct result, the first is identification of all possible candidates of Chinese words in a text. In this paper, a data structure Chinese-character-net is put forward, then, based on this character-net, a new algorithm is presented to obtain all possible candidate of Chinese words in a text. This paper gives the experiment result. Finally the characteristics of the algorithm are analysed."
O02-2003,åºæ¼ãç¥ç¶²ãçè¾­å½èªç¾©ç¸ä¼¼åº¦è¨ç® (Word Similarity Computing Based on How-net) [In {C}hinese],2002,5,163,1,1,5775,qun liu,"International Journal of Computational Linguistics {\\&} {C}hinese Language Processing, Volume 7, Number 2, August 2002: Special Issue on Computational {C}hinese Lexical Semantics",0,None
liu-yu-1998-transeasy,{T}rans{E}asy: A {C}hinese-{E}nglish machine translation system based on hybrid approach,1998,4,3,1,1,5775,qun liu,Proceedings of the Third Conference of the Association for Machine Translation in the Americas: System Descriptions,0,"This paper describes the progress of a machine translation system from Chinese to English. The system is based on a reusable platform of MT software components. It{'}s a rule-based system, and some statistical algorithms are used as heuristic functions in parsing as well. There are about 50,000 Chinese words and 400 global parsing rules in the system. The system got a good result in a public test of MT system in China in Mar. 1998. It is a research vehicle up to now."
