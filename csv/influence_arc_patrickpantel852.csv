A00-2011,W99-0905,0,0.0281851,"Word-for-word glossing is the process of directly translating each word or term in a document without considering the word order. Automating this process would benefit many NLP applications. For example, in crosslanguage information retrieval, glossing a document often provides a sufficient translation for humans to comprehend the key concepts. Furthermore, a glossing algorithm can be used for lexical selection in a full-fledged machine translation (MT) system. Many corpus-based MT systems require parallel corpora (Brown et al., 1990; Brown et al., 1991; Gale and Church, 1991; Resnik, 1999). Kikui (1999) used a word sense disambiguation algorithm and a non-paralM bilingual corpus to resolve translation ambiguity. In this paper, we present a word-for-word glossing algorithm that requires only a source language corpus. The intuitive idea behind our algorithm is the following. Suppose w is a word to be translated. We first identify a set of words similar to w that occurred in the same context as w in a large corpus. We then use this set (called the contextually similar words of w) to select a translation for w. For example, the contextually similar words of duty in fiduciary duty include respons"
A00-2011,P98-2127,1,0.537109,"ibility, obligation, role, ... This list is then used to select a translation for duty. In the next section, we describe the resources required by our algorithm. In Section 3, we present an algorithm for constructing the contextually similar words of a word in a context. Section 4 presents the word-for-word glossing algorithm and Section 5 describes the group similarity metric used in our algorithm. In Section 6, we present some experimental results and finally, in Section 7, we conclude with a discussion of future work. 2. Resources The input to our algorithm includes a collocation database (Lin, 1998b) and a corpus-based thesaurus (Lin, 1998a), which are both available on the Interne0. In addition, we require a bilingual thesaurus. Below, we briefly describe these resources. 2.1. Collocation database Given a word w in a dependency relationship (such as subject or object), the collocation database can be used to retrieve the words that occurred in that relationship with w, in a large corpus, along with their frequencies2. Figure 1 shows excerpts of the entries in the collocation database for the words corporate, duty, and fiduciary. The database contains a total of 11 million unique depend"
A00-2011,P99-1068,0,0.0119051,"1. Introduction Word-for-word glossing is the process of directly translating each word or term in a document without considering the word order. Automating this process would benefit many NLP applications. For example, in crosslanguage information retrieval, glossing a document often provides a sufficient translation for humans to comprehend the key concepts. Furthermore, a glossing algorithm can be used for lexical selection in a full-fledged machine translation (MT) system. Many corpus-based MT systems require parallel corpora (Brown et al., 1990; Brown et al., 1991; Gale and Church, 1991; Resnik, 1999). Kikui (1999) used a word sense disambiguation algorithm and a non-paralM bilingual corpus to resolve translation ambiguity. In this paper, we present a word-for-word glossing algorithm that requires only a source language corpus. The intuitive idea behind our algorithm is the following. Suppose w is a word to be translated. We first identify a set of words similar to w that occurred in the same context as w in a large corpus. We then use this set (called the contextually similar words of w) to select a translation for w. For example, the contextually similar words of duty in fiduciary duty i"
A00-2011,J90-2002,0,0.167527,"Missing"
A00-2011,P91-1022,0,0.110094,"Missing"
A00-2011,P91-1023,0,0.031315,"a bilingual thesaurus. 1. Introduction Word-for-word glossing is the process of directly translating each word or term in a document without considering the word order. Automating this process would benefit many NLP applications. For example, in crosslanguage information retrieval, glossing a document often provides a sufficient translation for humans to comprehend the key concepts. Furthermore, a glossing algorithm can be used for lexical selection in a full-fledged machine translation (MT) system. Many corpus-based MT systems require parallel corpora (Brown et al., 1990; Brown et al., 1991; Gale and Church, 1991; Resnik, 1999). Kikui (1999) used a word sense disambiguation algorithm and a non-paralM bilingual corpus to resolve translation ambiguity. In this paper, we present a word-for-word glossing algorithm that requires only a source language corpus. The intuitive idea behind our algorithm is the following. Suppose w is a word to be translated. We first identify a set of words similar to w that occurred in the same context as w in a large corpus. We then use this set (called the contextually similar words of w) to select a translation for w. For example, the contextually similar words of duty in f"
A00-2011,J93-1004,0,\N,Missing
A00-2011,C98-2122,1,\N,Missing
C02-1144,J98-1006,0,0.026788,"Missing"
C02-1144,P99-1005,0,0.0242603,"hemes have been proposed. They generally fall under two categories: • • comparing cluster outputs with manually generated answer keys (hereon referred to as classes); or embedding the clusters in an application and using its evaluation measure. An example of the first approach considers the average entropy of the clusters, which measures the purity of the clusters (Steinbach, Karypis, and Kumar 2000). However, maximum purity is trivially achieved when each element forms its own cluster. An example of the second approach evaluates the clusters by using them to smooth probability distributions (Lee and Pereira 1999). Like the entropy scheme, we assume that there is an answer key that defines how the elements are supposed to be clustered. Let C be a set of clusters and A be the answer key. We define the editing distance, dist(C, A), as the number of operations required to make C consistent with A. We say that C is consistent with A if there is a one to one mapping between clusters in C and the classes in A such that for each cluster c in C, all elements of c belong to the same class in A. We allow two editing operations: • • merge two clusters; and move an element from one cluster to another. A) C) a b e"
C02-1144,C94-1079,1,0.341631,"we first estimate the probability of a random word belonging to a subhierarchy (a synset and its hyponyms). We use the frequency counts of synsets in the SemCor corpus (Landes, Leacock, Tengi 1998) to estimate the probability of a subhierarchy. Since SemCor is a fairly small corpus, the frequency counts of the synsets in the lower part of the WordNet hierarchy are very sparse. We smooth the probabilities by assuming that all siblings are equally likely given the parent. A class is then defined as the maximal subhierarchy with probability less than a threshold (we used e-2). We used Minipar 1 (Lin 1994), a broadcoverage English parser, to parse about 1GB (144M words) of newspaper text from the TREC collection (1988 AP Newswire, 1989-90 LA Times, and 1991 San Jose Mercury) at a speed of about 500 words/second on a PIII-750 with 512MB memory. We collected the frequency counts of the grammatical relationships (contexts) output by Minipar and used them to compute the pointwise mutual information values from Section 3. The test set is constructed by intersecting the words in WordNet with the nouns in the corpus whose total mutual information with all of its contexts exceeds a threshold m. Since W"
C02-1144,P98-2127,1,0.0689826,"y of partitional algorithms. Buckshot (Cutting, Karger, Pedersen, Tukey 1992) addresses the problem of randomly selecting initial centroids in K-means by combining it with average-link clustering. Buckshot first applies average-link to a random sample of n elements to generate K clusters. It then uses the centroids of the clusters as the initial K centroids of K-means clustering. The sample size counterbalances the quadratic running time of average-link to make Buckshot efficient: O(K×T×n + nlogn). The parameters K and T are usually considered to be small numbers. 3 Word Similarity Following (Lin 1998), we represent each word by a feature vector. Each feature corresponds to a context in which the word occurs. For example, “threaten with __” is a context. If the word handgun occurred in this context, the context is a feature of handgun. The value of the feature is the pointwise mutual information (Manning and Schütze 1999 p.178) between the feature and the word. Let c be a context and Fc(w) be the frequency count of a word w occurring in context c. The pointwise mutual information between c and w is defined as: miw,c = ∑ Fc ( w ) N Fi ( w ) i N × ∑ Fc ( j ) j N where N = ∑∑ F ( j ) is i i th"
C02-1144,C98-2122,1,\N,Missing
C04-1111,P99-1008,0,0.188158,"Missing"
C04-1111,J95-4004,0,0.0678,"7.80 joke 5 7.37 V:subj:N joke 39 7.11 tape 10 7.09 poke 15 6.87 host 40 6.47 co-host 4 6.14 banter 3 6.00 interview 20 5.89 N:appo:N host 127 12.46 comedian 12 11.02 King 13 9.49 star 6 7.47 4 We propose an algorithm for learning highly scalable lexico-POS patterns. Given two sentences with their surface form and part of speech tags, the algorithm finds the optimal lexico-POS alignment. For example, consider the following 2 sentences: Figure 1. Excerpt of the grammatical signature for the television host class. 1) Platinum is a precious metal. 2) Molybdenum is a metal. Applying a POS tagger (Brill 1995) gives the following output: where n is the number of elements to be clustered, cef is the frequency count of word e in grammatical context f, and N is the total frequency count of all features of all words. 3.2 Surface POS Surface POS Phase II Platinum NNP is VBZ a DT Molybdenum NNP precious JJ is VBZ a DT metal NN . . metal NN . . A very good pattern to generalize from the alignment of these two strings would be Following (Pantel and Lin 2002), a committee for each semantic class is constructed. A committee is a set of representative elements that unambiguously describe the members of a poss"
C04-1111,P99-1016,0,0.103552,". These systems mostly employ clustering algorithms to group words according to their meanings in text. Assuming the distributional hypothesis (Harris 1985), words that occur in similar grammatical contexts are similar in meaning. Curran and Moens (2002) experimented with corpus size and complexity of proximity features in building automatic thesauri. CBC (Clustering by Committee) proposed by Pantel and Lin (2002) achieves high recall and precision in generating similarity lists of words discriminated by their meaning and senses. However, such clustering algorithms fail to name their classes. Caraballo (1999) was the first to use clustering for labeling is-a relations using conjunction and apposition features to build noun clusters. Recently, Pantel and Ravichandran (2004) extended this approach by making use of all syntactic dependency features for each noun. 3 Hypernym azalea bipolar disorder Bordeaux Flintstones salmon Riloff and Shepherd (1997) used a semiautomatic method for discovering similar words using a few seed examples by using pattern-based techniques and human supervision. Berland and Charniak (1999) used similar pattern-based techniques and other heuristics to extract meronymy (part"
C04-1111,P02-1030,0,0.0138495,", tangerine, ... (B) Phil Donahue, Pat Sajak, Arsenio Hall, Geraldo Rivera, Don Imus, Larry King, David Letterman, Conan O&apos;Brien, Rosie O&apos;Donnell, Jenny Jones, Sally Jessy Raphael, Oprah Winfrey, Jerry Springer, Howard Stern, Jay Leno, Johnny Carson, ... Co-occurrence-based approaches The second class of algorithms uses cooccurrence statistics (Hindle 1990, Lin 1998). These systems mostly employ clustering algorithms to group words according to their meanings in text. Assuming the distributional hypothesis (Harris 1985), words that occur in similar grammatical contexts are similar in meaning. Curran and Moens (2002) experimented with corpus size and complexity of proximity features in building automatic thesauri. CBC (Clustering by Committee) proposed by Pantel and Lin (2002) achieves high recall and precision in generating similarity lists of words discriminated by their meaning and senses. However, such clustering algorithms fail to name their classes. Caraballo (1999) was the first to use clustering for labeling is-a relations using conjunction and apposition features to build noun clusters. Recently, Pantel and Ravichandran (2004) extended this approach by making use of all syntactic dependency featu"
C04-1111,J93-1003,0,0.0269929,"X and Y and a reasonable way to delimit them would be to use POS information. All the patterns produced by the multi-level pattern learning algorithm were generated from positive examples. From amongst these patterns, we need to find the most important ones. This is a critical step because frequently occurring patterns have low precision whereas rarely occurring patterns have high precision. From the Information Extraction point of view neither of these patterns is very useful. We need to find patterns with relatively high occurrence and high precision. We apply the log likelihood principle (Dunning 1993) to compute this score. The top 15 patterns according to this metric are listed in Table 3 (we omit the POS variations for visibility). Some of these patterns are similar to the ones discovered by Hearst (1992) while other patterns are similar to the ones used by Fleischman et al. (2003). Y like X and _NN, X and other Y Y, including X, Y, such as X Y, especially X keeping track of the edit operations (which is the second part of the algorithm). Algorithm for calculating the minimal edit distance between two strings D[0,0]=0 for i = 1 to n do D[i,0] = D[i-1,0] + cost(insertion) for j = 1 to m d"
C04-1111,P03-1001,1,0.884551,"l step because frequently occurring patterns have low precision whereas rarely occurring patterns have high precision. From the Information Extraction point of view neither of these patterns is very useful. We need to find patterns with relatively high occurrence and high precision. We apply the log likelihood principle (Dunning 1993) to compute this score. The top 15 patterns according to this metric are listed in Table 3 (we omit the POS variations for visibility). Some of these patterns are similar to the ones discovered by Hearst (1992) while other patterns are similar to the ones used by Fleischman et al. (2003). Y like X and _NN, X and other Y Y, including X, Y, such as X Y, especially X keeping track of the edit operations (which is the second part of the algorithm). Algorithm for calculating the minimal edit distance between two strings D[0,0]=0 for i = 1 to n do D[i,0] = D[i-1,0] + cost(insertion) for j = 1 to m do D[0,j] = D[0,j-1] + cost(deletion) for i = 1 to n do for j = 1 to m do D[i,j] = min( D[i-1,j-1] + cost(substitution), D[i-1,j] + cost(insertion), D[i,j-1] + cost(deletion)) Print (D[n,m]) Algorithm for optimal pattern retrieval 4.3 i = n, j = m; while i ≠ 0 and j ≠ 0 if D[i,j] = D[i-1,"
C04-1111,N03-1011,0,0.0528261,"Missing"
C04-1111,C92-2082,0,0.322965,"formance to a linguisticallyrich approach. The current state of the art cooccurrence model requires an estimated 10 years just to parse a 1TB corpus (see Table 1). Instead of using a syntactically motivated co-occurrence approach as above, our system uses lexico-syntactic rules. In particular, it finds lexico-POS patterns by making modifications to the basic edit distance algorithm. Once these patterns have been learnt, 2 Relevant Work Previous approaches to extracting is-a relations fall under two categories: pattern-based and cooccurrence-based approaches. 2.1 Pattern-based approaches Marti Hearst (1992) was the first to use a pattern-based approach to extract hyponym relations from a raw corpus. She used an iterative process to semi-automatically learn patterns. However, a corpus of 20MB words yielded only 400 examples. Our pattern-based algorithm is very similar to the one used by Hearst. She uses seed examples to manually discover her patterns whearas we use a minimal edit distance algorithm to automatically discover the patterns. 771 Table 1. Approximate processing time on a single Pentium-4 2.5 GHz machine. TOOL 15 GB ORPUS 1 TB CORPUS POS Tagger NP Chunker Dependency Parser Syntactic Pa"
C04-1111,P90-1034,0,0.14707,"two semantic classes discovered by CBC: (A) peach, pear, pineapple, apricot, mango, raspberry, lemon, cherry, strawberry, melon, blueberry, fig, apple, plum, nectarine, avocado, grapefruit, papaya, banana, cantaloupe, cranberry, blackberry, lime, orange, tangerine, ... (B) Phil Donahue, Pat Sajak, Arsenio Hall, Geraldo Rivera, Don Imus, Larry King, David Letterman, Conan O&apos;Brien, Rosie O&apos;Donnell, Jenny Jones, Sally Jessy Raphael, Oprah Winfrey, Jerry Springer, Howard Stern, Jay Leno, Johnny Carson, ... Co-occurrence-based approaches The second class of algorithms uses cooccurrence statistics (Hindle 1990, Lin 1998). These systems mostly employ clustering algorithms to group words according to their meanings in text. Assuming the distributional hypothesis (Harris 1985), words that occur in similar grammatical contexts are similar in meaning. Curran and Moens (2002) experimented with corpus size and complexity of proximity features in building automatic thesauri. CBC (Clustering by Committee) proposed by Pantel and Lin (2002) achieves high recall and precision in generating similarity lists of words discriminated by their meaning and senses. However, such clustering algorithms fail to name thei"
C04-1111,C94-1079,0,0.0470748,"ly set (by trial and error): cost(insertion) = 3 cost(deletion) = 3 cost(substitution) = 0 if a1i=b1j = 1 if a1i≠b1j, a2i=b2j = 2 if a1i≠b1j, a2i≠b2j 4.2 Time complexity 5.1 Implementation and filtering Experimental Setup We use a 15GB newspaper corpus consisting of TREC9, TREC 2002, Yahoo! News ~0.5GB, AP newswire ~2GB, New York Times ~2GB, Reuters ~0.8GB, Wall Street Journal ~1.2GB, and various online news website ~1.5GB. For our experiments, we extract from this corpus six data sets of different sizes: 1.5MB, 15 MB, 150 MB, 1.5GB, 6GB and 15GB. For the co-occurrence model, we used Minipar (Lin 1994), a broad coverage parser, to parse each data set. We collected the frequency counts of the grammatical relationships (contexts) output by Minipar and used them to compute the pointwise mutual information vectors described in Section 3.1. For the pattern-based approach, we use Brill’s POS tagger (1995) to tag each data set. The above algorithm takes O(y2) time for every pair of strings of length at most y. Hence, if there are x strings in the collection, each string having at most length y, the algorithm has time complexity O(x2y2) to extract all the patterns in the collection. Applying the ab"
C04-1111,P98-2127,0,0.0377977,"classes discovered by CBC: (A) peach, pear, pineapple, apricot, mango, raspberry, lemon, cherry, strawberry, melon, blueberry, fig, apple, plum, nectarine, avocado, grapefruit, papaya, banana, cantaloupe, cranberry, blackberry, lime, orange, tangerine, ... (B) Phil Donahue, Pat Sajak, Arsenio Hall, Geraldo Rivera, Don Imus, Larry King, David Letterman, Conan O&apos;Brien, Rosie O&apos;Donnell, Jenny Jones, Sally Jessy Raphael, Oprah Winfrey, Jerry Springer, Howard Stern, Jay Leno, Johnny Carson, ... Co-occurrence-based approaches The second class of algorithms uses cooccurrence statistics (Hindle 1990, Lin 1998). These systems mostly employ clustering algorithms to group words according to their meanings in text. Assuming the distributional hypothesis (Harris 1985), words that occur in similar grammatical contexts are similar in meaning. Curran and Moens (2002) experimented with corpus size and complexity of proximity features in building automatic thesauri. CBC (Clustering by Committee) proposed by Pantel and Lin (2002) achieves high recall and precision in generating similarity lists of words discriminated by their meaning and senses. However, such clustering algorithms fail to name their classes."
C04-1111,P02-1038,0,0.0292064,"this paper, we study the challenges of working at the terascale. We present an algorithm, designed for the terascale, for mining is-a relations that achieves similar performance to a state-of-the-art linguistically-rich method. We focus on the accuracy of these two systems as a function of processing time and corpus size. 1 Introduction The Natural Language Processing (NLP) community has recently seen a growth in corpus-based methods. Algorithms light in linguistic theories but rich in available training data have been successfully applied to several applications such as machine translation (Och and Ney 2002), information extraction (Etzioni et al. 2004), and question answering (Brill et al. 2001). In the last decade, we have seen an explosion in the amount of available digital text resources. It is estimated that the Internet contains hundreds of terabytes of text data, most of which is in an unstructured format. Yet, many NLP algorithms tap into only megabytes or gigabytes of this information. In this paper, we make a step towards acquiring semantic knowledge from terabytes of data. We present an algorithm for extracting is-a relations, designed for the terascale, and compare it to a state of th"
C04-1111,N04-1041,1,0.8846,"nd question answering (Brill et al. 2001). In the last decade, we have seen an explosion in the amount of available digital text resources. It is estimated that the Internet contains hundreds of terabytes of text data, most of which is in an unstructured format. Yet, many NLP algorithms tap into only megabytes or gigabytes of this information. In this paper, we make a step towards acquiring semantic knowledge from terabytes of data. We present an algorithm for extracting is-a relations, designed for the terascale, and compare it to a state of the art method that employs deep analysis of text (Pantel and Ravichandran 2004). We show that by simply utilizing more data on this task, we can achieve similar performance to a linguisticallyrich approach. The current state of the art cooccurrence model requires an estimated 10 years just to parse a 1TB corpus (see Table 1). Instead of using a syntactically motivated co-occurrence approach as above, our system uses lexico-syntactic rules. In particular, it finds lexico-POS patterns by making modifications to the basic edit distance algorithm. Once these patterns have been learnt, 2 Relevant Work Previous approaches to extracting is-a relations fall under two categories:"
C04-1111,W97-0313,0,0.018009,"Missing"
C08-1086,P99-1071,0,0.0467473,"the total number of documents that contain the keyword (by normalizing term frequencies with inverse document frequencies). Various methods including tf-idf have been comparatively evaluated by Salton and Buckley (1987). Creating nbest lists using the above algorithms produce result sets where each result is considered independently. In this paper, we investigate the utility of considering the result sets jointly and compare our joint method to a pointwise mutual information model. Within the NLP community, n-best list ranking has been looked at carefully in parsing, extractive summarization (Barzilay et al. 1999; Hovy and Lin 1998), and machine translation (Zhang et al. 2006), to name a few. The problem of learning to rank a set of objects by combining a given collection of ranking functions using boosting techniques is investigated in (Freund et al. 2003). This rank boosting technique has been used in re-ranking parsers (Collins and Koo 2000; Charniak and Johnson 2005). Such reranking approaches usually improve the likelihood of candidate results using extraneous features and, for example in parsing, the properties of the trees. In this paper, we focus on a difference task: the lexical semantics tas"
C08-1086,X98-1026,0,0.0194031,"cuments that contain the keyword (by normalizing term frequencies with inverse document frequencies). Various methods including tf-idf have been comparatively evaluated by Salton and Buckley (1987). Creating nbest lists using the above algorithms produce result sets where each result is considered independently. In this paper, we investigate the utility of considering the result sets jointly and compare our joint method to a pointwise mutual information model. Within the NLP community, n-best list ranking has been looked at carefully in parsing, extractive summarization (Barzilay et al. 1999; Hovy and Lin 1998), and machine translation (Zhang et al. 2006), to name a few. The problem of learning to rank a set of objects by combining a given collection of ranking functions using boosting techniques is investigated in (Freund et al. 2003). This rank boosting technique has been used in re-ranking parsers (Collins and Koo 2000; Charniak and Johnson 2005). Such reranking approaches usually improve the likelihood of candidate results using extraneous features and, for example in parsing, the properties of the trees. In this paper, we focus on a difference task: the lexical semantics task of selecting the b"
C08-1086,P98-2127,0,0.0626713,"hat contain the keyword (by normalizing term frequencies with inverse document frequencies). Various methods including tf-idf have been comparatively evaluated by Salton and Buckley (1987). Creating nbest lists using the above algorithms produce result sets where each result is considered independently. In this paper, we investigate the utility of considering the result sets jointly and compare our joint method to a pointwise mutual information model. Within the NLP community, n-best list ranking has been looked at carefully in parsing, extractive summarization (Barzilay et al. 1999; Hovy and Lin 1998), and machine translation (Zhang et al. 2006), to name a few. The problem of learning to rank a set of objects by combining a given collection of ranking functions using boosting techniques is investigated in (Freund et al. 2003). This rank boosting technique has been used in re-ranking parsers (Collins and Koo 2000; Charniak and Johnson 2005). Such reranking approaches usually improve the likelihood of candidate results using extraneous features and, for example in parsing, the properties of the trees. In this paper, we focus on a difference task: the lexical semantics task of selecting the b"
C08-1086,P93-1016,0,0.161013,"t, consume, slice, ... EIIR: Expected Independent Information Ranking Model (Baseline Model) Recall the task definition from Section 3. Finding a property r that most reduces the uncertainty in a query set Q can be modeled by measuring the strength of association between r and Q. Following Pantel and Lin (2002), we use pointwise mutual information (pmi) to measure the association strength between two events q and r, where q is a term in Q and r is syntactic dependency, as follows (Church and Hanks 1989): We obtain candidate properties by parsing a large textual corpus with the Minipar parser (Lin 1993)5. For each word in the corpus, we extract all of its dependency links, forming a feature vector of syntactic dependencies. For example, below is a sample of the feature vector for the word apple: adj-mod:gala, adj-mod:shredded, object-of:caramelize, object-of:eat, object-of:import, ... pmi (q, r ) = log c (q ,r ) N ∑ c ( w, r ) w∈W Intersecting apple’s feature vector with beef’s, we are left with the following candidate properties: N In this paper, we omit the relation name of the syntactic dependencies, and instead write: This list of syntactic dependencies forms the candidate properties for"
C08-1086,C08-2033,1,0.916748,"the similarity of these terms: {they are products, they can be imported, they can be exported} Even though can be imported and can be exported are highly ranked explanations, taken jointly, once we know one the other does not offer much more information since most things that can be imported can also be exported. In other words, there is a large overlap in information between the two properties. A more informative set of explanations could be obtained by replacing one of these two properties with a property that scored lower but had less information overlap with the others, for example: 2 In (Vyas and Pantel 2008), we explore the task of explaining the similarity between terms in detail. In this paper, we focus on the task of choosing the best set of explanations given a set of candidates. 681 Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 681–688 Manchester, August 2008 {they are products, they can be imported, they can be eaten} Even though, taken alone, the property can be eaten may not be as informative as can be exported, it does indeed add more information to the explanation set when considered jointly with the other explanations. In this paper,"
C08-1086,P05-1022,0,0.0167593,"tigate the utility of considering the result sets jointly and compare our joint method to a pointwise mutual information model. Within the NLP community, n-best list ranking has been looked at carefully in parsing, extractive summarization (Barzilay et al. 1999; Hovy and Lin 1998), and machine translation (Zhang et al. 2006), to name a few. The problem of learning to rank a set of objects by combining a given collection of ranking functions using boosting techniques is investigated in (Freund et al. 2003). This rank boosting technique has been used in re-ranking parsers (Collins and Koo 2000; Charniak and Johnson 2005). Such reranking approaches usually improve the likelihood of candidate results using extraneous features and, for example in parsing, the properties of the trees. In this paper, we focus on a difference task: the lexical semantics task of selecting the best semantic properties that help explain why a set of query terms are similar. Unlike in parsing and machine translation, we are not ulti682 mately looking for the best single result, but instead the n-best. Looking at commercial applications, there are many examples showcasing the importance of ranking, for example Internet search engines li"
C08-1086,N07-1013,0,0.0619074,"Missing"
C08-1086,P89-1010,0,0.0346872,"ximizes the joint information content of a result set. Instead of ranking documents in an IR setting, we focus in this paper on a new task of selecting the best semantic properties that describe the similarity of a set of query terms. By no means an exhaustive list, the most commonly cited ranking and scoring algorithms are HITS (Kleinberg 1998) and PageRank (Page et al. 1998), which rank hyperlinked documents using the concepts of hubs and authorities. The most well-known keyword scoring methods within the IR community are the tf-idf (Salton and McGill 1983) and pointwise mutual information (Church and Hanks 1989) measures, which put more importance on matching keywords that occur frequently in a document relative to the total number of documents that contain the keyword (by normalizing term frequencies with inverse document frequencies). Various methods including tf-idf have been comparatively evaluated by Salton and Buckley (1987). Creating nbest lists using the above algorithms produce result sets where each result is considered independently. In this paper, we investigate the utility of considering the result sets jointly and compare our joint method to a pointwise mutual information model. Within"
C08-1086,W06-1626,0,0.0692845,"Missing"
C08-1086,J90-1003,0,\N,Missing
C08-1086,W97-0704,0,\N,Missing
C08-1086,J05-1003,0,\N,Missing
C08-1086,C98-2122,0,\N,Missing
C08-2033,P99-1004,0,0.0517888,"s. Our contributions include: 1) an information-theoretic objective function for quantifying the utility of an explanation set; 2) a survey of psycholinguistics and philosophy for evidence of different sources of explanations such as descriptive properties and prototypes; 3) computational baseline models for automatically generating various types of explanations; and 4) a qualitative evaluation of our explanation generation engine. 1 2 Introduction Computing similarity is at the core of many computer science tasks. Many have developed algorithms for computing the semantic similarity of words (Lee, 1999), of expressions to generate paraphrases (Lin and Pantel, 2001) and of documents (Salton and McGill, 1983). However, little investigation has been spent on automatically explaining why a particular set of elements are similar to one another. Explaining similarity is an important part of various natural language applications such as question answering and building lexical ontologies such as WordNet (Fellbaum, 1998). Several questions must be addressed before one can begin to explore this topic. First, what constitutes a good c 2008. Licensed under the Creative Commons Attribution-Noncommercial-"
C08-2033,C94-1079,0,0.0379715,"of nouns. For each of these concepts, we randomly chose a set of cluster instances (nouns), where the size of each set was randomly chosen to consist of two to five nouns. Each of these samples forms a query. For each explanation source described in Section 4, we generated explanation sets for the random samples and in the next section we show a random selection of these system outputs. 5.2 Examples of Explanations using Descriptive Properties For the algorithm discussed in Section 4.1, we derived our descriptive properties using the output of the dependency analysis generated by the Minipar (Lin, 1994) dependency parser. We use syntactic dependencies between words to model their semantic properties. The assumption here is that some grammatical relations, such as subject and object can yield semantic properties of terms. For example, from a phrase like ”students eat many apples”, we can infer the properties can-beeaten for apples and can-eat for students. In this paper, we use a combination of corpus statistics and manual filters for grammatical relations to uncover candidate semantic properties. Table 1: Explanations generated using descriptive properties. Query Sets Palestinian-Israeli, In"
C08-2033,C08-1086,1,0.822194,"BJ), financial(ADJ-MOD), political(ADJ-MOD) Intuitively, one would prefer adjectival modifiers and verbal propositions as good descriptive properties for explanations, and from the examples, we can see our algorithm generates such descriptive properties because of the high information contribution of such properties to the query set. However, our algorithm does not try to reduce the redundancy within the sets of explanations. We can see redundant explanations for examples in Table 1. The reason is that each explanation added to the set is independent of the ones already present in the set. In Pantel and Vyas (2008) we propose a joint information model to overcome this problem. 5.3 Explanations using Prototypes The algorithm discussed in Section 4.2 uses words that share the semantic category with words within the query set as the set of candidate explanations. 133 We can approximate the notion of semantic categories using clusters of semantically similar words. For this we used the CBC collection (Pantel and Lin 2002) of nouns. Using these clusters as semantic categories, the candidate set of all explanations is the set of all the words that belong to the same cluster. Table 2 shows some system outputs."
C08-2033,J97-1004,0,\N,Missing
C10-1057,P08-1004,0,0.0532682,"Missing"
C10-1057,P09-1113,0,0.0586453,"Missing"
C10-1057,P08-1079,0,0.0319153,"Missing"
C10-1057,P06-1015,1,0.931955,"Missing"
C10-1057,C92-2082,0,0.141849,"Missing"
C10-1057,P02-1006,0,0.233293,"Missing"
C10-1057,D08-1061,0,0.268124,"Missing"
C10-1057,D09-1098,1,\N,Missing
C10-1057,W06-3809,0,\N,Missing
C14-1140,N09-1054,0,0.0706006,", which use user-specific click through rate (CTR). Although these applications and our task share the use of CTR as a supervision signal, there is a key difference: Whereas in web search CTR is used as a predictor/feature at runtime, our task specifically aims at predicting interestingness in the absence of web usage features: Our input is completely unstructured and there is no assumption of prior user interaction data. Use of probabilistic models: Our semantic model is built over LDA (Blei et al., 2003) and has resemblances to Link-LDA models (Erosheva et al., 2004) and Comment-LDA models (Yano et al., 2009). However, these are tailored for blogs and associated comment discussions which is very different from our source to destination browsing transition logs. Guo et al., (2009b) used probabilistic models for discovering entity classes from query logs and in (Lin et al., 2012), latent intents in entity centric search were explored. Gao et al. (2011) employ statistical machine translation to connect two types of content, learning semantic translation of queries to document titles. None of the above models, however, are directly applicable to the joint topic mappings that are involved in source to"
D07-1017,P01-1008,0,0.203747,"Missing"
D07-1017,P99-1071,0,0.0691133,"Missing"
D07-1017,W04-3205,1,0.29143,"approaches produce collections of inference rules that have good recall, they suffer from the complementary problem of low precision. They also make no attempt to distinguish between symmetric and asymmetric inference rules. Given the potential positive impact shown in Section 2.1 of learning the directionality of inference rules, there is a need for methods, such as the one we present, to improve existing automatically created resources. 2.3 Learning Directionality There have been a few approaches at learning the directionality of restricted sets of semantic relations, mostly between verbs. Chklovski and Pantel (2004) used lexico-syntactic patterns over the Web to detect certain types of symmetric and asymmetric relations between verbs. They manually examined and obtained lexico-syntactic patterns that help identify the types of relations they considered and used these lexico-syntactic patterns over the Web to detect these relations among a set of candidate verb pairs. Their approach however is limited only to verbs and to specific types of verb-verb relations. Zanzotto et al. (2006) explored a selectional preference-based approach to learn asymmetric inference rules between verbs. They used the selectiona"
D07-1017,P03-1003,0,0.0173803,"Missing"
D07-1017,P06-1114,0,0.170016,"s distributional hypothesis, we use selectional preferences to gather evidence of inference directionality and plausibility. Experiments show empirical evidence that our approach can classify inference rules significantly better than several baselines. 1 Introduction Paraphrases are textual expressions that convey the same meaning using different surface forms. Textual entailment is a similar phenomenon, in which the presence of one expression licenses the validity of another. Paraphrases and inference rules are known to improve performance in various NLP applications like Question Answering (Harabagiu and Hickl 2006), summarization (Barzilay et al. 1999) and Information Retrieval (Anick and Tipirneni 1999). Paraphrase and entailment involve inference rules that license a conclusion when a premise is given. Deciding whether a proposed inference rule is fully valid is difficult, however, and most NL systems instead focus on plausible inference. In this case, one statement has some likelihood of All rules in DIRT are considered symmetric. Though here, one is most likely to infer that “X eats Y” ⇒ “X likes Y”, because if someone eats something, he most probably likes it 1, but if he likes something he might n"
D07-1017,P93-1016,0,0.0370205,"atural, here, we simply mean that a manual inspection by the authors showed that, at depth four, the resulting clusters had struck a better granularity balance than other cutoff points. We acknowledge that this is a very coarse way of extracting concepts from WordNet. on the relational selectional preferences, something we do not address this in this paper. 4.3 Implementation We implemented LEDIR with both the JRM and IRM models using inference rules from DIRT and semantic classes from both CBC and WordNet. We parsed the 1999 AP newswire collection consisting of 31 million words with Minipar (Lin 1993) and used this to obtain the probability statistics for the models (as described in section 3.2). We performed both system-wide evaluations and intrinsic evaluations with different values of α and β parameters. Section 5 presents these results and our error analysis. 4.4 Gold Standard Construction In order to evaluate the performance of the different systems, we compare their outputs against a manually annotated gold standard. To create this gold standard, we randomly sampled 160 inference rules of the form pi ⇔ pj from DIRT. We discarded three rules since they contained nominalizations5. For"
D07-1017,N03-1022,0,0.0107569,"erences as the basis for our algorithm. We provide empirical evidence to validate the following main contribution: Claim: Relational selectional preferences can be used to automatically determine the plausibility and directionality of an inference rule. 2 Related Work In this section, we describe applications that can benefit by using inference rules and their directionality. We then talk about some previous work in this area. 2.1 Applications Open domain question answering approaches often cast QA as the problem of finding some kind of semantic inference between a question and its answer(s) (Moldovan et al. 2003; Echiabi and Marcu 2003). Harabagiu and Hickl (2006) recently demonstrated that textual entailment inference information, which in this system is a set of directional inference relations, improves the performance of a QA system significantly even without using any other form of semantic inference. This evidence supports the idea that learning the directionality of other sets of inference rules may improve QA performance. In Multi-Document Summarization (MDS), paraphrasing is useful for determining sentences that have similar meanings (Barzilay et al. 1999). Knowing the directionality between"
D07-1017,N07-1071,1,0.620214,"erence rules and to identify the directionality of the correct ones. Let pi ⇔ pj be an inference rule where each p is a binary semantic relation between two entities x and y. Let <x, p, y> be an instance of relation p. Formal problem definition: Given the inference rule pi ⇔ pj, we want to conclude which one of the following is more appropriate: 1. pi ⇔ pj 2. pi ⇒ pj 3. pi ⇐ pj 4. No plausible inference Consider the example (1) from section 1. There, it is most plausible to conclude “X eats Y” ⇒ “X likes Y”. Our algorithm LEDIR uses selectional preferences along the lines of Resnik (1996) and Pantel et al. (2007) to determine the plausibility and directionality of inference rules. 3.1 Underlying Assumption Many approaches to modeling lexical semantics have relied on the distributional hypothesis (Harris 1954), which states that words that appear in the same contexts tend to have similar meanings. The idea is that context is a good indicator of a word meaning. Lin and Pantel (2001) proposed an extension to the distributional hypothesis and applied it to paths in dependency trees, where if two paths tend to occur in similar contexts it is hypothesized that the meanings of the paths tend to be similar. I"
D07-1017,W04-3206,0,0.660215,"versity of Southern California Marina del Rey, CA {rahul,pantel,hovy}@isi.edu being identical in meaning to, or derivable from, the other. In the rest of this paper we discuss plausible inference only. Given the importance of inference, several researchers have developed inference rule collections. While manually built resources like WordNet (Fellbaum 1998) and Cyc (Lenat 1995) have been around for years, for coverage and domain adaptability reasons many recent approaches have focused on automatic acquisition of paraphrases (Barzilay and McKeown 2001) and inference rules (Lin and Pantel 2001; Szpektor et al. 2004). The downside of these approaches is that they often result in incorrect inference rules or in inference rules that are underspecified in directionality (i.e. asymmetric but are wrongly considered symmetric). For example, consider an inference rule from DIRT (Lin and Pantel 2001): X eats Y ⇔ X likes Y (1) Abstract Semantic inference is a core component of many natural language applications. In response, several researchers have developed algorithms for automatically learning inference rules from textual corpora. However, these rules are often either imprecise or underspecified in directionali"
D07-1017,N06-1008,0,0.0279927,"s over the Web to detect these relations among a set of candidate verb pairs. Their approach however is limited only to verbs and to specific types of verb-verb relations. Zanzotto et al. (2006) explored a selectional preference-based approach to learn asymmetric inference rules between verbs. They used the selectional preferences of a single verb, i.e. the semantic types of a verb’s arguments, to infer an asymmetric inference between the verb and the verb form of its argument type. Their approach however applies also only to verbs and is limited to some specific types of verb-argument pairs. Torisawa (2006) presented a method to acquire inference rules with temporal constraints, between verbs. They used co-occurrences between verbs in Japanese coordinated sentences and co-occurrences between verbs and nouns to learn the verb-verb inference rules. Like the previous two methods, their approach too deals only with verbs and is limited to learning inference rules that are temporal in nature. Geffet and Dagan (2005) proposed an extension to the distributional hypothesis to discover entailment relation between words. They model the context of a word using its syntactic features and compare the context"
D07-1017,P06-1107,0,0.0360578,"ve been a few approaches at learning the directionality of restricted sets of semantic relations, mostly between verbs. Chklovski and Pantel (2004) used lexico-syntactic patterns over the Web to detect certain types of symmetric and asymmetric relations between verbs. They manually examined and obtained lexico-syntactic patterns that help identify the types of relations they considered and used these lexico-syntactic patterns over the Web to detect these relations among a set of candidate verb pairs. Their approach however is limited only to verbs and to specific types of verb-verb relations. Zanzotto et al. (2006) explored a selectional preference-based approach to learn asymmetric inference rules between verbs. They used the selectional preferences of a single verb, i.e. the semantic types of a verb’s arguments, to infer an asymmetric inference between the verb and the verb form of its argument type. Their approach however applies also only to verbs and is limited to some specific types of verb-argument pairs. Torisawa (2006) presented a method to acquire inference rules with temporal constraints, between verbs. They used co-occurrences between verbs in Japanese coordinated sentences and co-occurrence"
D07-1017,P05-1014,0,\N,Missing
D09-1025,P98-2127,0,0.00508499,"ts of manually provided seed patterns or seed instances. This approach is very common in both the NLP and Semantic Web communities (Cimiano and Staab, 2004; Cafarella et al., 2005; Pantel and Pennacchiotti, 2006; Pas¸ca et al., 2006). The distributional approach uses contextual evidence to model the instances of a given class, following the distributional hypothesis (Harris, 1964). Weakly supervised, these methods take a small set of seed instances (or the class label) and extract new instances from noun phrases that are most similar to the seeds (i.e., that share similar contexts). Following Lin (1998), example systems include Fleischman and Hovy (2002), Cimiano and Volker (2005), Tanev and Magnini (2006), and Pantel et al. (2009). crawled from the Web at Yahoo! in 2008; • Query logs (q): one year of web search queries issued to the Yahoo! search engine; • Web tables: all HTML inner tables extracted from the above Web source; and • Wikipedia: an official Wikipedia dump from February 2008, consisting of about 2 million articles. Feature families are further subclassified into five types: frequency (F) (frequency-based features); co-occurrence (C) (features capturing first order co-occurrence"
D09-1025,J95-4004,0,0.0100641,"features generated by the four generators presented in Section 3.1.2. The Decoder then ranks each entity according to the trained model. 3.2 Experimental Evaluation Experimental Setup Evaluated classes. We evaluate our system over three classes: Actors (movie, tv and stage actors); Athletes (professional and amateur); Musicians (singers, musicians, composers, bands, and orchestras) System setup. We instantiated our knowledge extractors, KEpat and KEdis from Section 3.1.1, over our Web crawl of 600 million documents (see Section 3.1.2). The documents were preprocessed using Brill’s POS-tagger (Brill, 1995) and the Abney’s chunker (Abney, 1991). For KEdis , context vectors are extracted for noun phrases recognized as NP-chunks with removed modifiers. The vector space includes the 250M most frequent noun chunks in the corpus. KEdis returns as instances all noun phrases having a similarity with the seeds’ centroid above τ = 0.0051 . The sets of seeds S for KEdis include 10, 24 and 10 manually chosen instances for respectively the Actors, Athletes and Musicians classes2 . The sets of seeds P for KEpat Related Work Entity extraction systems follow two main approaches: pattern-based and distributiona"
D09-1025,P06-2075,0,0.0520006,"be used to better understand the commonalities and differences between existing information extraction systems. After presenting the framework in the next section, Section 2.2 shows how previous information extraction algorithms can be cast into ES. In Section 3 we describe our novel entity extraction algorithm based on ES. Ranker (R): a module for ranking the knowledge instances returned from KEs using the features generated by FGs. Ranking algorithms may be rule-based (e.g., the one using a threshold on distributional similarity in (Pas¸ca et al., 2006)) or ML-based (e.g., the SVM model in (Mirkin et al., 2006) for combining pattern-based and distributional features). 239 fers from previous work. Talukdar et al. (2008) present a weaklysupervised system for extracting large sets of class-instance pairs using two knowledge extractors: a pattern-based extractor supported by distributional evidence, which harvests candidate pairs from a Web corpus; and a table extractor that harvests candidates from Web tables. The Ranker uses graph random walks to combine the information of the two extractors and output the final list. The authors show large improvements in coverage with little precision loss. Mirkin e"
D09-1025,H05-1071,0,0.0614733,"s. Our experimental methodology and analysis is described in Section 4 and shows empirical evidence that our extractor significantly outperforms prior art. Finally, Section 5 concludes with a discussion and future work. 2 Sources (S): textual repositories of information, either structured (e.g., a database such as DBpedia), semi-structured (e.g., Wikipedia Infoboxes or HTML tables) or unstructured (e.g., news articles or a webcrawl). Knowledge Extractors (KE): algorithms responsible for extracting candidate instances such as entities or facts. Examples include fact extraction systems such as (Cafarella et al., 2005) and entity extraction systems such as (Pas¸ca, 2007). Ensemble Semantics Feature Generators (FG): methods that extract evidence (features) of knowledge in order to decide which candidate instances extracted from KEs are correct. Examples include capitalization features for named entity extractors, and the distributional similarity matrix used in (Pas¸ca et al., 2006) for filtering facts. Ensemble Semantics (ES) is a general framework for modeling information extraction algorithms that combine multiple sources of information and multiple extractors. The ES framework allows to: • Represent mult"
D09-1025,P08-1003,0,0.0657113,"Missing"
D09-1025,W06-3909,1,0.838151,"Missing"
D09-1025,D08-1061,0,0.236002,"Missing"
D09-1025,E06-1003,0,0.0293888,"th the NLP and Semantic Web communities (Cimiano and Staab, 2004; Cafarella et al., 2005; Pantel and Pennacchiotti, 2006; Pas¸ca et al., 2006). The distributional approach uses contextual evidence to model the instances of a given class, following the distributional hypothesis (Harris, 1964). Weakly supervised, these methods take a small set of seed instances (or the class label) and extract new instances from noun phrases that are most similar to the seeds (i.e., that share similar contexts). Following Lin (1998), example systems include Fleischman and Hovy (2002), Cimiano and Volker (2005), Tanev and Magnini (2006), and Pantel et al. (2009). crawled from the Web at Yahoo! in 2008; • Query logs (q): one year of web search queries issued to the Yahoo! search engine; • Web tables: all HTML inner tables extracted from the above Web source; and • Wikipedia: an official Wikipedia dump from February 2008, consisting of about 2 million articles. Feature families are further subclassified into five types: frequency (F) (frequency-based features); co-occurrence (C) (features capturing first order co-occurrences between an instance and class seeds); distributional (D) (features based on the distributional similari"
D09-1025,C92-2082,0,0.197708,"training instances are manually labeled by human experts, through a long and costly editorial process. Information sources (S) serve as inputs to the system. Some sources will serve as sources for knowledge extractors to generate candidate instances, some will serve as sources for feature generators to generate features or evidence of knowledge, and some will serve as both. 2.2 Related Work To date, most information extraction systems rely on a model composed of a single source S, a single extractor KE and a single feature generator F G. For example, many classic relation extraction systems (Hearst, 1992; Riloff and Jones, 1999; Pantel and Pennacchiotti, 2006; Pas¸ca et al., 2006) are based on a single pattern-based extractor KE, which is seeded with a set of patterns or instances for a given relation (e.g. the pattern ‘X starred in Y’ for the act-in relation). The extractor then iteratively extracts new instances until a stop condition is met. The resulting extractor scores are proposed by F G as a feature. The Ranker simply consists of a sorting function on the feature from F G. Systems such as the above that do not consist of multiple sources, knowledge extractors or feature generators are"
D09-1025,P06-1015,1,\N,Missing
D09-1025,C02-1130,0,\N,Missing
D09-1025,C98-2122,0,\N,Missing
D09-1025,D09-1098,1,\N,Missing
D09-1098,P04-1056,0,0.0207671,"a given threshold. Rychlý and Kilgarriff (2007), Elsayed et al. (2008) and Agirre et al. (2009) use reverse indexing and the MapReduce framework to distribute the similarity computations across several machines. Our proposed approach combines these two strategies and efficiently computes the exact similarity (cosine, Jaccard, Dice, and Overlap) between all pairs. 2.2 Entity extraction and classification Building entity lexicons is a task of great interest for which structured, semi-structured and unstructured data have all been explored (GoogleSets; Sarmento et al. 2007; Wang and Cohen 2007; Bunescu and Mooney 2004; Etzioni et al. 2005; Paşca et al. 2006). Our own work focuses on set expansion from unstructured Web text. Apart from the choice of a data source, state-of-the-art entity extraction methods differ in their use of numerous, few or no labeled examples, the open or targeted nature of the extraction as well as the types of features employed. Supervised approaches (McCallum and Li 2003, Bunescu and Mooney 2004) rely on large sets of labeled examples, perform targeted extraction and employ a variety of sentence- and corpus-level features. While very precise, these methods are typically used for co"
D09-1098,P89-1010,0,0.0843583,"ency, tf-idf, pointwise mutual information), or ultimately in measuring the similarity between two context vectors (e.g., using Euclidean distance, Cosine, Dice). In this paper, we adopt the following methodology for computing term similarity. Our various web crawls, described in Section 6.1, are POStagged using Brill’s tagger (1995) and chunked using a variant of the Abney chunker (Abney 1991). Terms are NP chunks with some modifiers removed; their contexts (i.e., features) are defined as their rightmost and leftmost stemmed chunks. We weigh each context f using pointwise mutual information (Church and Hanks 1989). Let PMI(w) denote a pointwise mutual information vector, constructed for each term as follows: PMI(w) = (pmiw1, pmiw2, …, pmiwm), where pmiwf is the pointwise mutual information between term w and feature f: pmi wf = log c wf × N n m i =1 j =1 where cwf is the frequency of feature f occurring for term w, n is the number of unique terms and N is the total number of features for all terms. Term similarities are computed by comparing these pmi context vectors using measures such as cosine, Jaccard, and Dice. ∑ cif × ∑ c wj 940 Large-Scale Implementation Computing the similarity between terms on"
D09-1098,N09-1003,0,0.315315,"NLP including word classification (Turney and Littman 2003), word sense disambiguation (Yuret and Yatbaz 2009), contextspelling correction (Jones and Martin 1997), fact extraction (Paşca et al. 2006), semantic role labeling (Erk 2007), and applications in IR such as query expansion (Cao et al. 2008) and textual advertising (Chang et al. 2009). 2.1 Computing Term Similarities The distributional hypothesis (Harris 1954), which links the meaning of words to their contexts, has inspired many algorithms for computing term similarities (Lund and Burgess 1996; Lin 1998; Lee 1999; Erk and Padó 2008; Agirre et al. 2009). Brute force similarity computation compares all the contexts for each pair of terms, with complexity O(n2m) where n is the number of terms and m is the number of possible contexts. More efficient strategies are of three kinds: 938 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 938–947, c Singapore, 6-7 August 2009. 2009 ACL and AFNLP Smoothing: Techniques such as Latent Semantic Analysis reduce the context space by applying truncated Singular Value Decomposition (SVD) (Deerwester et al. 1990). Computing the matrix decomposition however does not"
D09-1098,P06-2004,0,0.0112683,"et (Paşca’s work also examines the advantages of combining these approaches). Semisupervised methods (including ours) are useful for extending finer grain entity classes, for which large unlabeled data sets are available. 2.3 Impact of corpus on system performance Previous work has examined the effect of using large, sometimes Web-size corpora, on system performance in the case of familiar NLP tasks. Banko and Brill (2001) show that Web-scale data helps with confusion set disambiguation while Lapata and Keller (2005) find that the Web is a good source of n-gram counts for unsupervised models. Atterer and Schutze (2006) examine the influence of corpus size on combining a supervised approach with an unsupervised one for relative clause and PP-attachment. Etzioni et al. (2005) and Pantel et al. (2004) show the advantages of using large quantities of generic Web text over smaller corpora for extracting relations and named entities. Overall, corpus size and quality are both found to be important for extraction. Our paper adds to this body of work by focusing on the task of similarity-based set expansion and providing a large empirical study quantify the relative corpus effects. 2.4 Impact of seeds on extraction"
D09-1098,P07-1028,0,0.0185866,"composition and seed size. We make public an experimental testbed for set expansion analysis that includes a large collection of diverse entity sets extracted from Wikipedia. 1 2 Related Work Below we review relevant work in optimizing similarity computations and automatic set expansion. Introduction Computing the semantic similarity between terms has many applications in NLP including word classification (Turney and Littman 2003), word sense disambiguation (Yuret and Yatbaz 2009), contextspelling correction (Jones and Martin 1997), fact extraction (Paşca et al. 2006), semantic role labeling (Erk 2007), and applications in IR such as query expansion (Cao et al. 2008) and textual advertising (Chang et al. 2009). 2.1 Computing Term Similarities The distributional hypothesis (Harris 1954), which links the meaning of words to their contexts, has inspired many algorithms for computing term similarities (Lund and Burgess 1996; Lin 1998; Lee 1999; Erk and Padó 2008; Agirre et al. 2009). Brute force similarity computation compares all the contexts for each pair of terms, with complexity O(n2m) where n is the number of terms and m is the number of possible contexts. More efficient strategies are of"
D09-1098,D08-1094,0,0.0241875,"any applications in NLP including word classification (Turney and Littman 2003), word sense disambiguation (Yuret and Yatbaz 2009), contextspelling correction (Jones and Martin 1997), fact extraction (Paşca et al. 2006), semantic role labeling (Erk 2007), and applications in IR such as query expansion (Cao et al. 2008) and textual advertising (Chang et al. 2009). 2.1 Computing Term Similarities The distributional hypothesis (Harris 1954), which links the meaning of words to their contexts, has inspired many algorithms for computing term similarities (Lund and Burgess 1996; Lin 1998; Lee 1999; Erk and Padó 2008; Agirre et al. 2009). Brute force similarity computation compares all the contexts for each pair of terms, with complexity O(n2m) where n is the number of terms and m is the number of possible contexts. More efficient strategies are of three kinds: 938 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 938–947, c Singapore, 6-7 August 2009. 2009 ACL and AFNLP Smoothing: Techniques such as Latent Semantic Analysis reduce the context space by applying truncated Singular Value Decomposition (SVD) (Deerwester et al. 1990). Computing the matrix decomposit"
D09-1098,P06-1046,0,0.0247881,"uch as Latent Semantic Analysis reduce the context space by applying truncated Singular Value Decomposition (SVD) (Deerwester et al. 1990). Computing the matrix decomposition however does not scale well to web-size term-context matrices. Other currently unscalable smoothing techniques include Probabilistic Latent Semantic Analysis (Hofmann 1999), Iterative Scaling (Ando 2000), and Latent Dirichlet Allocation (Blei et al. 2003). Randomized Algorithms: Randomized techniques for approximating various similarity measures have been successfully applied to term similarity (Ravichandran et al. 2005; Gorman and Curran 2006). Common techniques include Random Indexing based on Sparse Distributed Memory (Kanerva 1993) and Locality Sensitive Hashing (Broder 1997). Optimizations and Distributed Processing: Bayardo et al. (2007) present a sparse matrix optimization strategy capable of efficiently computing the similarity between terms which’s similarity exceeds a given threshold. Rychlý and Kilgarriff (2007), Elsayed et al. (2008) and Agirre et al. (2009) use reverse indexing and the MapReduce framework to distribute the similarity computations across several machines. Our proposed approach combines these two strategi"
D09-1098,P90-1034,0,0.535106,"Missing"
D09-1098,P99-1004,0,0.107422,"erms has many applications in NLP including word classification (Turney and Littman 2003), word sense disambiguation (Yuret and Yatbaz 2009), contextspelling correction (Jones and Martin 1997), fact extraction (Paşca et al. 2006), semantic role labeling (Erk 2007), and applications in IR such as query expansion (Cao et al. 2008) and textual advertising (Chang et al. 2009). 2.1 Computing Term Similarities The distributional hypothesis (Harris 1954), which links the meaning of words to their contexts, has inspired many algorithms for computing term similarities (Lund and Burgess 1996; Lin 1998; Lee 1999; Erk and Padó 2008; Agirre et al. 2009). Brute force similarity computation compares all the contexts for each pair of terms, with complexity O(n2m) where n is the number of terms and m is the number of possible contexts. More efficient strategies are of three kinds: 938 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 938–947, c Singapore, 6-7 August 2009. 2009 ACL and AFNLP Smoothing: Techniques such as Latent Semantic Analysis reduce the context space by applying truncated Singular Value Decomposition (SVD) (Deerwester et al. 1990). Computing th"
D09-1098,P98-2127,0,0.454805,"between terms has many applications in NLP including word classification (Turney and Littman 2003), word sense disambiguation (Yuret and Yatbaz 2009), contextspelling correction (Jones and Martin 1997), fact extraction (Paşca et al. 2006), semantic role labeling (Erk 2007), and applications in IR such as query expansion (Cao et al. 2008) and textual advertising (Chang et al. 2009). 2.1 Computing Term Similarities The distributional hypothesis (Harris 1954), which links the meaning of words to their contexts, has inspired many algorithms for computing term similarities (Lund and Burgess 1996; Lin 1998; Lee 1999; Erk and Padó 2008; Agirre et al. 2009). Brute force similarity computation compares all the contexts for each pair of terms, with complexity O(n2m) where n is the number of terms and m is the number of possible contexts. More efficient strategies are of three kinds: 938 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 938–947, c Singapore, 6-7 August 2009. 2009 ACL and AFNLP Smoothing: Techniques such as Latent Semantic Analysis reduce the context space by applying truncated Singular Value Decomposition (SVD) (Deerwester et al. 1990). Co"
D09-1098,P08-1003,0,0.412163,"mpanies) for which large training data sets are available. Unsu939 pervised approaches rely on no labeled data and use either bootstrapped class-specific extraction patterns (Etzioni et al. 2005) to find new elements of a given class (for targeted extraction) or corpusbased term similarity (Pantel and Lin 2002) to find term clusters (in an open extraction framework). Finally, semi-supervised methods have shown great promise for identifying and labeling entities (Riloff and Shepherd 1997; Riloff and Jones 1999; Banko et al. 2007; Downey et al. 2007; Paşca et al. 2006; Paşca 2007a; Paşca 2007b; Paşca and Durme 2008). Starting with a set of seed entities, semisupervised extraction methods use either classspecific patterns to populate an entity class or distributional similarity to find terms similar to the seed set (Paşca’s work also examines the advantages of combining these approaches). Semisupervised methods (including ours) are useful for extending finer grain entity classes, for which large unlabeled data sets are available. 2.3 Impact of corpus on system performance Previous work has examined the effect of using large, sometimes Web-size corpora, on system performance in the case of familiar NLP tas"
D09-1098,P06-1102,0,0.0786235,"rmance of corpus size, corpus quality, seed composition and seed size. We make public an experimental testbed for set expansion analysis that includes a large collection of diverse entity sets extracted from Wikipedia. 1 2 Related Work Below we review relevant work in optimizing similarity computations and automatic set expansion. Introduction Computing the semantic similarity between terms has many applications in NLP including word classification (Turney and Littman 2003), word sense disambiguation (Yuret and Yatbaz 2009), contextspelling correction (Jones and Martin 1997), fact extraction (Paşca et al. 2006), semantic role labeling (Erk 2007), and applications in IR such as query expansion (Cao et al. 2008) and textual advertising (Chang et al. 2009). 2.1 Computing Term Similarities The distributional hypothesis (Harris 1954), which links the meaning of words to their contexts, has inspired many algorithms for computing term similarities (Lund and Burgess 1996; Lin 1998; Lee 1999; Erk and Padó 2008; Agirre et al. 2009). Brute force similarity computation compares all the contexts for each pair of terms, with complexity O(n2m) where n is the number of terms and m is the number of possible contexts"
D09-1098,P05-1077,1,0.551314,"LP Smoothing: Techniques such as Latent Semantic Analysis reduce the context space by applying truncated Singular Value Decomposition (SVD) (Deerwester et al. 1990). Computing the matrix decomposition however does not scale well to web-size term-context matrices. Other currently unscalable smoothing techniques include Probabilistic Latent Semantic Analysis (Hofmann 1999), Iterative Scaling (Ando 2000), and Latent Dirichlet Allocation (Blei et al. 2003). Randomized Algorithms: Randomized techniques for approximating various similarity measures have been successfully applied to term similarity (Ravichandran et al. 2005; Gorman and Curran 2006). Common techniques include Random Indexing based on Sparse Distributed Memory (Kanerva 1993) and Locality Sensitive Hashing (Broder 1997). Optimizations and Distributed Processing: Bayardo et al. (2007) present a sparse matrix optimization strategy capable of efficiently computing the similarity between terms which’s similarity exceeds a given threshold. Rychlý and Kilgarriff (2007), Elsayed et al. (2008) and Agirre et al. (2009) use reverse indexing and the MapReduce framework to distribute the similarity computations across several machines. Our proposed approach co"
D09-1098,W97-0313,0,0.113274,"pus-level features. While very precise, these methods are typically used for coarse grained entity classes (People, Organizations, Companies) for which large training data sets are available. Unsu939 pervised approaches rely on no labeled data and use either bootstrapped class-specific extraction patterns (Etzioni et al. 2005) to find new elements of a given class (for targeted extraction) or corpusbased term similarity (Pantel and Lin 2002) to find term clusters (in an open extraction framework). Finally, semi-supervised methods have shown great promise for identifying and labeling entities (Riloff and Shepherd 1997; Riloff and Jones 1999; Banko et al. 2007; Downey et al. 2007; Paşca et al. 2006; Paşca 2007a; Paşca 2007b; Paşca and Durme 2008). Starting with a set of seed entities, semisupervised extraction methods use either classspecific patterns to populate an entity class or distributional similarity to find terms similar to the seed set (Paşca’s work also examines the advantages of combining these approaches). Semisupervised methods (including ours) are useful for extending finer grain entity classes, for which large unlabeled data sets are available. 2.3 Impact of corpus on system performance Previ"
D09-1098,P07-2011,0,0.0285034,"nd Latent Dirichlet Allocation (Blei et al. 2003). Randomized Algorithms: Randomized techniques for approximating various similarity measures have been successfully applied to term similarity (Ravichandran et al. 2005; Gorman and Curran 2006). Common techniques include Random Indexing based on Sparse Distributed Memory (Kanerva 1993) and Locality Sensitive Hashing (Broder 1997). Optimizations and Distributed Processing: Bayardo et al. (2007) present a sparse matrix optimization strategy capable of efficiently computing the similarity between terms which’s similarity exceeds a given threshold. Rychlý and Kilgarriff (2007), Elsayed et al. (2008) and Agirre et al. (2009) use reverse indexing and the MapReduce framework to distribute the similarity computations across several machines. Our proposed approach combines these two strategies and efficiently computes the exact similarity (cosine, Jaccard, Dice, and Overlap) between all pairs. 2.2 Entity extraction and classification Building entity lexicons is a task of great interest for which structured, semi-structured and unstructured data have all been explored (GoogleSets; Sarmento et al. 2007; Wang and Cohen 2007; Bunescu and Mooney 2004; Etzioni et al. 2005; Pa"
D09-1098,J90-1003,0,\N,Missing
D09-1098,H01-1052,0,\N,Missing
D09-1098,J10-1004,0,\N,Missing
D09-1098,J95-4004,0,\N,Missing
D09-1098,P08-2067,0,\N,Missing
D09-1098,W03-0430,0,\N,Missing
D09-1098,C98-2122,0,\N,Missing
D14-1002,P14-1066,1,0.0924512,"de PLSA (Hofmann 1990) and LDA (Blei et al. 2003). Recently, these models have been extended to handle cross-lingual cases, where there are pairs of corresponding documents in different languages (e.g., Dumais et al. 1997; Gao et al. 2011; Platt et al. 2010; Yih et al. 2011). By exploiting deep architectures, deep learning techniques are able to automatically discover from training data the hidden structures and the associated features at different levels of abstraction useful for a variety of tasks (e.g., Collobert et al. 2011; Hinton et al. 2012; Socher et al. 2012; Krizhevsky et al., 2012; Gao et al. 2014). Hinton and Salakhutdinov (2010) propose the most original approach based on an unsupervised version of the deep neural network to discover the hierarchical semantic structure embedded in queries and documents. Huang et al. (2013) significantly extends the approach so that the deep neural network can be trained on large-scale query-document pairs giving much better performance. The use of the convolutional neural network for text processing, central to our DSSM, was also described in Collobert et al. (2011) and Shen et al. (2014) but with very different applications. The DSSM described in Sec"
D14-1002,J93-2003,0,0.0385454,"combination (Row 9) is obtained by incorporating the DSSM feature vectors of source and target documents (i.e., 600 features in total) in the ranker. We thus conclude that on both tasks, automatic highlighting and contextual entity search, features drawn from the output layers of our deep semantic model result in significant gains after being added to a set of non-semantic features, and in comparison to other types of semantic models used in the past. |is the unigram probability of word where | in , and is the probability of translating into , trained on source-target document pairs using EM (Brown et al. 1993). The translation-based approach allows any pair of non-identical but semantically related words to have a nonzero matching score. As a result, it significantly outperforms BM25. BTLM (Row 4) follows the best performing bilingual topic model described in Gao et al. (2011), which is an extension of PLSA (Hofmann 1999). The model is trained on source-target document pairs using the EM algorithm with a constraint enforcing a source document and its target document to not only share the same prior topic distribution, but to also have similar fractions of words assigned to each topic. BLTM defines"
D14-1002,C14-1140,1,0.829571,"Missing"
D14-1002,D10-1025,0,0.0126544,"for interestingness. The task of contextual entity search, which is formulated as an information retrieval problem in this paper, is also related to research on entity resolution (Stefanidis et al. 2013). Latent Semantic Analysis (Deerwester et al. 1990) is arguably the earliest semantic model designed for IR. Generative topic models widely used for IR include PLSA (Hofmann 1990) and LDA (Blei et al. 2003). Recently, these models have been extended to handle cross-lingual cases, where there are pairs of corresponding documents in different languages (e.g., Dumais et al. 1997; Gao et al. 2011; Platt et al. 2010; Yih et al. 2011). By exploiting deep architectures, deep learning techniques are able to automatically discover from training data the hidden structures and the associated features at different levels of abstraction useful for a variety of tasks (e.g., Collobert et al. 2011; Hinton et al. 2012; Socher et al. 2012; Krizhevsky et al., 2012; Gao et al. 2014). Hinton and Salakhutdinov (2010) propose the most original approach based on an unsupervised version of the deep neural network to discover the hierarchical semantic structure embedded in queries and documents. Huang et al. (2013) significa"
D14-1002,D12-1110,0,0.00801862,"erative topic models widely used for IR include PLSA (Hofmann 1990) and LDA (Blei et al. 2003). Recently, these models have been extended to handle cross-lingual cases, where there are pairs of corresponding documents in different languages (e.g., Dumais et al. 1997; Gao et al. 2011; Platt et al. 2010; Yih et al. 2011). By exploiting deep architectures, deep learning techniques are able to automatically discover from training data the hidden structures and the associated features at different levels of abstraction useful for a variety of tasks (e.g., Collobert et al. 2011; Hinton et al. 2012; Socher et al. 2012; Krizhevsky et al., 2012; Gao et al. 2014). Hinton and Salakhutdinov (2010) propose the most original approach based on an unsupervised version of the deep neural network to discover the hierarchical semantic structure embedded in queries and documents. Huang et al. (2013) significantly extends the approach so that the deep neural network can be trained on large-scale query-document pairs giving much better performance. The use of the convolutional neural network for text processing, central to our DSSM, was also described in Collobert et al. (2011) and Shen et al. (2014) but with very differ"
D14-1002,W11-0329,0,0.0767306,"eywords of local feature vectors, the document. Figure 3 presents a sample of document snippets and their keywords detected by the DSSM according to the procedure elaborated in Figure 2. It is interesting to see that many names are identified as keywords although the DSSM is not designed explicitly for named entity recognition. useful to the corresponding tasks, with a manageable vector size. tanh 1 where ces. 3.2 and tanh (3) tanh (4) are learned linear projection matriTraining the DSSM To optimize the parameters of the DSSM of Figure 1, i.e., , , , we use a pair-wise rank loss as objective (Yih et al. 2011). Consider a source document and two candidate target documents and , where is more interesting than to a user when reading . We construct and , , where two pairs of documents , the former is preferred and should have a higher (2) where the max operation is performed for each dimension of across 1, … , respectively. 5 … the comedy festival formerly known as the us comedy arts festival is a comedy festival held each year in las vegas nevada from its 1985 inception to 2008 . it was held annually at the wheeler opera house and other venues in aspen colorado . the primary sponsor of the festival w"
D14-1002,2011.iwslt-evaluation.19,0,\N,Missing
D14-1002,N09-1054,0,\N,Missing
D15-1174,D14-1165,0,0.0272354,"follows: ef (es ,r,eo ;Θ) f (es ,r,e0 ;Θ) e0 ∈N eg(es ,r,?) e p(eo |es , r; Θ) = P Conditional probabilities for subject entities p(es |eo , r; Θ) are defined analogously. Here Θ denotes all the parameters of latent features. The denominator is defined using a set of entities that do not fill the object position in any relation triple (es , r, ?) in the training knowledge graph. Since the number of such entities is impractically large, we sample negative triples from the full set. We also limit the candidate entities to ones that have types consistent with the position in the relation triple (Chang et al., 2014; Yang et al., 2015), where the types are approximated following Toutanova and Chen (2015). Additionally, since the task of predicting textual relations is auxiliary to the main task, we use a weighting factor τ for the loss on predicting the arguments of textual relations (Toutanova and Chen, 2015). Denote T as a set of triples, we define the loss L(T ; Θ) as: X L(T ; Θ) = − log p(eo |es , r; Θ) (es ,r,eo )∈T − log p(es |eo , r; Θ) (es ,r,eo )∈T Let TKB and Ttext represent the set of knowledge base triples and textual relation triples respectively. The final training loss function is de2 All"
D15-1174,D13-1080,0,0.025555,"from the incompleteness of knowledge bases and text collections (Ritter et al., 2013), inter alia. Our work focuses on representing the compositional structure of sentential context for learning joint continuous representations of text and knowledge bases. Combining knowledge base and text information A combination of knowledge base and textual information was first shown to outperform either source alone in the framework of path-ranking algorithms in a combined knowledge base and text graph (Lao et al., 2012). To alleviate the sparsity of textual relations arising in such a combined graph, (Gardner et al., 2013; Gardner et al., 2014) showed how to incorporate clusters or continuous representations of textual relations. Note that these vector representations are based on the co-occurrence patterns for the textual relations and not on their compositional structure. Cooccurrence based textual relation representations were also learned in (Neelakantan et al., 2015). Wang et al. (2014a) combined knowledge base and text information by embedding knowledge base entities and the words in their names in the same vector space, but did not model the textual cooccurrences of entity pairs and the expressed textua"
D15-1174,D14-1044,0,0.362479,"ss of knowledge bases and text collections (Ritter et al., 2013), inter alia. Our work focuses on representing the compositional structure of sentential context for learning joint continuous representations of text and knowledge bases. Combining knowledge base and text information A combination of knowledge base and textual information was first shown to outperform either source alone in the framework of path-ranking algorithms in a combined knowledge base and text graph (Lao et al., 2012). To alleviate the sparsity of textual relations arising in such a combined graph, (Gardner et al., 2013; Gardner et al., 2014) showed how to incorporate clusters or continuous representations of textual relations. Note that these vector representations are based on the co-occurrence patterns for the textual relations and not on their compositional structure. Cooccurrence based textual relation representations were also learned in (Neelakantan et al., 2015). Wang et al. (2014a) combined knowledge base and text information by embedding knowledge base entities and the words in their names in the same vector space, but did not model the textual cooccurrences of entity pairs and the expressed textual relations. Weston et"
D15-1174,D15-1205,0,0.0233037,"use scoring function f (es , r, eo ) to represent the model’s confidence in the existence of the triple. We present the models and then the loss function used to train Continuous representations for supervised relation extraction In contrast to the work reviewed so far, work on sentence-level relation extraction using direct supervision has focused heavily on representing sentence context. Models using hand-crafted features have evolved for more than a decade, and recently, models using continuous representations have been found to achieve new state-of-the-art performance (Zeng et al., 2014; Gormley et al., 2015). Compared to work on representation learning for sentence-level context, such as this recent work using LSTM models on constituency or dependency trees (Tai et al., 2015), our approach using a one-hidden-layer convolutional neural network is relatively simple. However, even such a simple approach has been shown to be very competitive (Kim, 2014). 3 1501 nsubj r tween entities and the subject and object positions of relations. For each relation type r, the model learns two latent feature vectors v(rs ) and v(ro ) of dimension K. For each entity (node) ei , the model also learns a latent featur"
D15-1174,P11-1055,0,0.224699,"of textual relations. 1 http://lemurproject.org/clueweb12/ FACC1/ Relation extraction using distant supervision A number of works have focused on extracting new instances of relations using information from textual mentions, without sophisticated modeling of prior knowledge from the knowledge base. Mintz et al. (2009) demonstrated that both surface context and dependency path context were helpful for the task, but did not model the compositional sub-structure of this context. Other work proposed more sophisticated models that reason about sentence-level hidden variables (Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012) or model the noise arising from the incompleteness of knowledge bases and text collections (Ritter et al., 2013), inter alia. Our work focuses on representing the compositional structure of sentential context for learning joint continuous representations of text and knowledge bases. Combining knowledge base and text information A combination of knowledge base and textual information was first shown to outperform either source alone in the framework of path-ranking algorithms in a combined knowledge base and text graph (Lao et al., 2012). To alleviate the sparsity of te"
D15-1174,D14-1181,0,0.00486836,"Missing"
D15-1174,D11-1049,0,0.132637,"ithout requiring sentence-level annotations of textual mentions at training time. We group such related work into three groups based on whether KB, text, or both sources of information are used. Additionally, we discuss related work in the area of supervised relation extraction using continuous representations of text, even though we do not use supervision at the level of textual mentions. Knowledge base completion Nickel et al. (2015) provide a broad overview of machine learning models for knowledge graphs, including models based on observed graph features such as the path ranking algorithm (Lao et al., 2011), models based on continuous representations (latent features), and model combinations (Dong et al., 2014). These models predict new facts in a given knowledge base, based on information from existing entities and relations. From this line of work, most relevant to our study is prior work evaluating continuous representation models on the FB15k dataset. Yang et al. (2015) showed that a simple variant of a bilinear model D IST M ULT outperformed T RANS E (Bordes et al., 2013) and more richly parameterized models on this dataset. We therefore build upon the best performing prior model D IST M UL"
D15-1174,D12-1093,0,0.0611575,"iables (Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012) or model the noise arising from the incompleteness of knowledge bases and text collections (Ritter et al., 2013), inter alia. Our work focuses on representing the compositional structure of sentential context for learning joint continuous representations of text and knowledge bases. Combining knowledge base and text information A combination of knowledge base and textual information was first shown to outperform either source alone in the framework of path-ranking algorithms in a combined knowledge base and text graph (Lao et al., 2012). To alleviate the sparsity of textual relations arising in such a combined graph, (Gardner et al., 2013; Gardner et al., 2014) showed how to incorporate clusters or continuous representations of textual relations. Note that these vector representations are based on the co-occurrence patterns for the textual relations and not on their compositional structure. Cooccurrence based textual relation representations were also learned in (Neelakantan et al., 2015). Wang et al. (2014a) combined knowledge base and text information by embedding knowledge base entities and the words in their names in the"
D15-1174,P09-1113,0,0.533576,"ed models on this dataset. We therefore build upon the best performing prior model D IST M ULT from this line of work, as well as additional models E and F developed in the context of text-augmented knowledge graphs (Riedel et al., 2013), and extend them to incorporate compositional representations of textual relations. 1 http://lemurproject.org/clueweb12/ FACC1/ Relation extraction using distant supervision A number of works have focused on extracting new instances of relations using information from textual mentions, without sophisticated modeling of prior knowledge from the knowledge base. Mintz et al. (2009) demonstrated that both surface context and dependency path context were helpful for the task, but did not model the compositional sub-structure of this context. Other work proposed more sophisticated models that reason about sentence-level hidden variables (Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012) or model the noise arising from the incompleteness of knowledge bases and text collections (Ritter et al., 2013), inter alia. Our work focuses on representing the compositional structure of sentential context for learning joint continuous representations of text and knowled"
D15-1174,P15-1016,0,0.389139,"rmation was first shown to outperform either source alone in the framework of path-ranking algorithms in a combined knowledge base and text graph (Lao et al., 2012). To alleviate the sparsity of textual relations arising in such a combined graph, (Gardner et al., 2013; Gardner et al., 2014) showed how to incorporate clusters or continuous representations of textual relations. Note that these vector representations are based on the co-occurrence patterns for the textual relations and not on their compositional structure. Cooccurrence based textual relation representations were also learned in (Neelakantan et al., 2015). Wang et al. (2014a) combined knowledge base and text information by embedding knowledge base entities and the words in their names in the same vector space, but did not model the textual cooccurrences of entity pairs and the expressed textual relations. Weston et al. (2013) combined continuous representations from a knowledge base and textual mentions for prediction of new relations. The two representations were trained independently of each other and using different loss functions, and were only combined at inference time. Additionally, the employed representations of text were non-composit"
D15-1174,N13-1008,0,0.290368,"of Text and Knowledge Bases Kristina Toutanova Microsoft Research Redmond, WA, USA Danqi Chen∗ Computer Science Department Stanford University Patrick Pantel Microsoft Research Redmond, WA, USA Hoifung Poon Microsoft Research Redmond, WA, USA Pallavi Choudhury Microsoft Research Redmond, WA, USA Michael Gamon Microsoft Research Redmond, WA, USA Abstract Knowledge Base Models that learn to represent textual and knowledge base relations in the same continuous latent space are able to perform joint inferences among the two kinds of relations and obtain high accuracy on knowledge base completion (Riedel et al., 2013). In this paper we propose a model that captures the compositional structure of textual relations, and jointly optimizes entity, knowledge base, and textual relation representations. The proposed model significantly improves performance over a model that does not share parameters among textual relations with common sub-structure. 1 place_of_birth Honolulu Barack Obama city_of United States nationality Textual Mentions Barack Obama is the 44th and current President of United States. Obama was born in the United States just as he has always said. … ClueWeb Figure 1: A knowledge base fragment cou"
D15-1174,Q13-1030,0,0.00682238,"cused on extracting new instances of relations using information from textual mentions, without sophisticated modeling of prior knowledge from the knowledge base. Mintz et al. (2009) demonstrated that both surface context and dependency path context were helpful for the task, but did not model the compositional sub-structure of this context. Other work proposed more sophisticated models that reason about sentence-level hidden variables (Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012) or model the noise arising from the incompleteness of knowledge bases and text collections (Ritter et al., 2013), inter alia. Our work focuses on representing the compositional structure of sentential context for learning joint continuous representations of text and knowledge bases. Combining knowledge base and text information A combination of knowledge base and textual information was first shown to outperform either source alone in the framework of path-ranking algorithms in a combined knowledge base and text graph (Lao et al., 2012). To alleviate the sparsity of textual relations arising in such a combined graph, (Gardner et al., 2013; Gardner et al., 2014) showed how to incorporate clusters or cont"
D15-1174,D12-1042,0,0.309075,"1 http://lemurproject.org/clueweb12/ FACC1/ Relation extraction using distant supervision A number of works have focused on extracting new instances of relations using information from textual mentions, without sophisticated modeling of prior knowledge from the knowledge base. Mintz et al. (2009) demonstrated that both surface context and dependency path context were helpful for the task, but did not model the compositional sub-structure of this context. Other work proposed more sophisticated models that reason about sentence-level hidden variables (Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012) or model the noise arising from the incompleteness of knowledge bases and text collections (Ritter et al., 2013), inter alia. Our work focuses on representing the compositional structure of sentential context for learning joint continuous representations of text and knowledge bases. Combining knowledge base and text information A combination of knowledge base and textual information was first shown to outperform either source alone in the framework of path-ranking algorithms in a combined knowledge base and text graph (Lao et al., 2012). To alleviate the sparsity of textual relations arising"
D15-1174,P15-1150,0,0.0210913,"uous representations for supervised relation extraction In contrast to the work reviewed so far, work on sentence-level relation extraction using direct supervision has focused heavily on representing sentence context. Models using hand-crafted features have evolved for more than a decade, and recently, models using continuous representations have been found to achieve new state-of-the-art performance (Zeng et al., 2014; Gormley et al., 2015). Compared to work on representation learning for sentence-level context, such as this recent work using LSTM models on constituency or dependency trees (Tai et al., 2015), our approach using a one-hidden-layer convolutional neural network is relatively simple. However, even such a simple approach has been shown to be very competitive (Kim, 2014). 3 1501 nsubj r tween entities and the subject and object positions of relations. For each relation type r, the model learns two latent feature vectors v(rs ) and v(ro ) of dimension K. For each entity (node) ei , the model also learns a latent feature vector of the same dimensionality. The score of a candidate triple (es , r, eo ) is defined as f (es , r, eo ) = v(rs ) |v(es ) + v(ro ) |v(eo ). It can be seen that whe"
D15-1174,W15-4007,1,0.610739,"odel this sub-structure and share parameters among related dependency paths, using a unified loss function learning entity and relation representations to maximize performance on the knowledge base link prediction task. We evaluate our approach on the FB15k-237 dataset, a knowledge base derived from the Free1499 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1499–1509, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. base subset FB15k (Bordes et al., 2013) and filtered to remove highly redundant relations (Toutanova and Chen, 2015). The knowledge base is paired with textual mentions for all entity pairs derived from ClueWeb121 with Freebase entity mention annotations (Gabrilovich et al., 2013). We show that using a convolutional neural network to derive continuous representations for textual relations boosts the overall performance on link prediction, with larger improvement on entity pairs that have textual mentions. 2 Related Work There has been a growing body of work on learning to predict relations between entities without requiring sentence-level annotations of textual mentions at training time. We group such relat"
D15-1174,P10-1040,0,0.0350795,"rformance. L-BFGS (Liu and Nocedal, 1989) and RProp (Riedmiller and Braun, 1993) were found to converge to similar function values, with RProp converging significantly faster. We thus used RProp for optimization. We initialized the KB+text models from the KB-only models and also from random initial values (sampled from a Gaussian distribution), and stopped optimization when the overall MRR on the validation set decreased. For each model type, we chose the better of random and KB-only initialization. The word embeddings in the C ONV models were initialized using the 50-dimensional vectors from Turian et al. (2010) in the main experiments, with a slight positive impact. The effect of initialization is discussed at the end of the section. The number of negative examples for each triple was set to 200. Performance improved substantially when the number of negative examples was increased and reached a plateau around 200. We chose the optimal number of latent feature dimensions via a grid search to optimize MRR on the validation set, testing the values 5, 10, 15, 35, 50, 100, 200 and 500. We also performed a grid search over the values of the parameter τ , testing values in the set {0.01, 0.1, 0.25, 0.5, 1}"
D15-1174,D14-1167,0,0.541781,"outperform either source alone in the framework of path-ranking algorithms in a combined knowledge base and text graph (Lao et al., 2012). To alleviate the sparsity of textual relations arising in such a combined graph, (Gardner et al., 2013; Gardner et al., 2014) showed how to incorporate clusters or continuous representations of textual relations. Note that these vector representations are based on the co-occurrence patterns for the textual relations and not on their compositional structure. Cooccurrence based textual relation representations were also learned in (Neelakantan et al., 2015). Wang et al. (2014a) combined knowledge base and text information by embedding knowledge base entities and the words in their names in the same vector space, but did not model the textual cooccurrences of entity pairs and the expressed textual relations. Weston et al. (2013) combined continuous representations from a knowledge base and textual mentions for prediction of new relations. The two representations were trained independently of each other and using different loss functions, and were only combined at inference time. Additionally, the employed representations of text were non-compositional. In this work"
D15-1174,D13-1136,0,0.0553672,"al., 2014) showed how to incorporate clusters or continuous representations of textual relations. Note that these vector representations are based on the co-occurrence patterns for the textual relations and not on their compositional structure. Cooccurrence based textual relation representations were also learned in (Neelakantan et al., 2015). Wang et al. (2014a) combined knowledge base and text information by embedding knowledge base entities and the words in their names in the same vector space, but did not model the textual cooccurrences of entity pairs and the expressed textual relations. Weston et al. (2013) combined continuous representations from a knowledge base and textual mentions for prediction of new relations. The two representations were trained independently of each other and using different loss functions, and were only combined at inference time. Additionally, the employed representations of text were non-compositional. In this work we train continuous representations of knowledge base and textual relations jointly, which allows for deeper interactions between the 1500 sources of information. We directly build on the universal schema approach of Riedel et al. (2013) as well as the uni"
D15-1174,C14-1220,0,0.0333623,"triple. The models use scoring function f (es , r, eo ) to represent the model’s confidence in the existence of the triple. We present the models and then the loss function used to train Continuous representations for supervised relation extraction In contrast to the work reviewed so far, work on sentence-level relation extraction using direct supervision has focused heavily on representing sentence context. Models using hand-crafted features have evolved for more than a decade, and recently, models using continuous representations have been found to achieve new state-of-the-art performance (Zeng et al., 2014; Gormley et al., 2015). Compared to work on representation learning for sentence-level context, such as this recent work using LSTM models on constituency or dependency trees (Tai et al., 2015), our approach using a one-hidden-layer convolutional neural network is relatively simple. However, even such a simple approach has been shown to be very competitive (Kim, 2014). 3 1501 nsubj r tween entities and the subject and object positions of relations. For each relation type r, the model learns two latent feature vectors v(rs ) and v(ro ) of dimension K. For each entity (node) ei , the model also"
I05-1069,P99-1008,0,0.0246703,"Missing"
I05-1069,W04-3205,1,0.924948,"ual path being noisy, multiple indirect paths can provide sufficient evidence for adding, removing, or altering a relation between two nodes. As illustrated by the earlier example, inferring a relation based on the presence of an indirect path relies on the semantics of the links that make up the path, like transitivity or equivalence classes. As an evaluation and a sample practical application, we apply our refinement framework to the task of refining the temporal precedence relations in VERBOCEAN, a broad-coverage noisy network of semantic relations between verbs extracted by mining the Web [2]. Examples of new edges discovered (added) by applying the R. Dale et al. (Eds.): IJCNLP 2005, LNAI 3651, pp. 792 – 803, 2005. © Springer-Verlag Berlin Heidelberg 2005 Global Path-Based Refinement of Noisy Graphs Applied to Verb Semantics 793 framework include: “ascertain happens-before evaluate”, “approve happens-before back”, “coat happens-before bake”, “plan happens-before complete”, and “interrogate happens-before extradite”. Examples of edges that are removed by applying our framework include: “induce happens-before treat”, “warm happens-before heat”, “halve happens-before slice”, and “fl"
I05-1069,N04-1041,1,0.827104,"Missing"
I05-1069,N04-1010,0,0.0636318,"Missing"
I05-7009,W04-2709,1,\N,Missing
I05-7009,reeder-etal-2004-interlingual,1,\N,Missing
I05-7009,agirre-de-lacalle-2004-publicly,0,\N,Missing
I05-7009,W03-1007,1,\N,Missing
I05-7009,J05-2004,0,\N,Missing
I05-7009,P05-1016,1,\N,Missing
I05-7009,P98-1013,0,\N,Missing
I05-7009,C98-1013,0,\N,Missing
I05-7009,magnini-cavaglia-2000-integrating,0,\N,Missing
I05-7009,W98-0713,0,\N,Missing
I05-7009,P03-1001,1,\N,Missing
N03-4011,P90-1034,0,0.106721,"nstrate the output of a distributional clustering algorithm called Clustering by Committee that automatically discovers word senses from text1. 1 Introduction Using word senses versus word forms is useful in many applications such as information retrieval (Voorhees 1998), machine translation (Hutchins and Sommers 1992), and question-answering (Pasca and Harabagiu 2001). The Distributional Hypothesis (Harris 1985) states that words that occur in the same contexts tend to be similar. There have been many approaches to compute the similarity between words based on their distribution in a corpus (Hindle 1990; Landauer and Dumais 1997; Lin 1998). The output of these programs is a ranked list of similar words to each word. For example, Lin’s approach outputs the following similar words for wine and suit: wine: beer, white wine, red wine, Chardonnay, champagne, fruit, food, coffee, juice, Cabernet, cognac, vinegar, Pinot noir, milk, vodka,… suit: lawsuit, jacket, shirt, pant, dress, case, sweater, coat, trouser, claim, business suit, blouse, skirt, litigation, … The similar words of wine represent the meaning of wine. However, the similar words of suit represent a mixture of its clothing and litigat"
N03-4011,C94-1079,1,0.657444,"ss, soul, mind) ) Each entry shows the clusters to which the headword belongs along with its similarity to the cluster. The lists of words are the top-4 most similar members to the cluster centroid. Each cluster corresponds to a sense of the headword. 2 Feature Representation Following (Lin 1998), we represent each word by a feature vector. Each feature corresponds to a context in which the word occurs. For example, “sip __” is a verbobject context. If the word wine occurred in this context, the context is a feature of wine. These features are obtained by parsing a large corpus using Minipar (Lin 1994), a broad-coverage English parser. The value of the feature is the pointwise mutual information (Manning and Schütze 1999) between the feature and the word. Let c be a context and Fc(w) be the frequency count of a word w occurring in context c. The pointwise mutual information, miw,c, between c and w is defined as: miw,c = Fc ( w ) N References ∑ Fi ( w ) ∑j Fc ( j ) i × N N where N is the total frequency counts of all words and their contexts. We compute the similarity between two words wi and wj using the cosine coefficient (Salton and McGill 1983) of their mutual information vectors: ∑c miw"
N03-4011,P98-2127,1,0.759627,"clustering algorithm called Clustering by Committee that automatically discovers word senses from text1. 1 Introduction Using word senses versus word forms is useful in many applications such as information retrieval (Voorhees 1998), machine translation (Hutchins and Sommers 1992), and question-answering (Pasca and Harabagiu 2001). The Distributional Hypothesis (Harris 1985) states that words that occur in the same contexts tend to be similar. There have been many approaches to compute the similarity between words based on their distribution in a corpus (Hindle 1990; Landauer and Dumais 1997; Lin 1998). The output of these programs is a ranked list of similar words to each word. For example, Lin’s approach outputs the following similar words for wine and suit: wine: beer, white wine, red wine, Chardonnay, champagne, fruit, food, coffee, juice, Cabernet, cognac, vinegar, Pinot noir, milk, vodka,… suit: lawsuit, jacket, shirt, pant, dress, case, sweater, coat, trouser, claim, business suit, blouse, skirt, litigation, … The similar words of wine represent the meaning of wine. However, the similar words of suit represent a mixture of its clothing and litigation senses. Such lists of similar wor"
N03-4011,C98-2122,1,\N,Missing
N04-1041,P99-1008,0,0.819166,"Missing"
N04-1041,P89-1010,0,0.0188637,"hen the feature vector for wave will have value 217 for its “object-of catch” feature. In Section 4.1, we describe how we obtain these features. We then construct a mutual information vector MI(e) = (mie1, mie2, …, miem) for each word e, where mief is the pointwise mutual information between word e and feature f, which is defined as: mief = log cef N ∑ cif i =1 N (1) m n × ∑ cej j =1 N n m where n is the number of words and N = ∑ ∑ cij is the i =1 j =1 total frequency count of all features of all words. Mutual information is commonly used to measure the association strength between two words (Church and Hanks 1989). A well-known problem is that mutual information is biased towards infrequent elements/features. We therefore multiply mief with the following discounting factor: (2) sim(ei , e j ) = ∑ mi ei f f ∑ mi f 2 ei f × mie j f × ∑ mie j f (3) 2 f For each word e, we then cluster its most similar instances using group-average clustering (Han and Kamber 2001) and we store as a candidate committee the highest scoring cluster c&apos; according to the following metric: |c&apos; |× avgsim(c&apos;) (4) where |c&apos; |is the number of elements in c&apos; and avgsim(c&apos;) is the average pairwise similarity between words in c&apos;. The as"
N04-1041,P03-1001,0,0.0380221,". The remainder of this paper is organized as follows. In the next section, we review previous algorithms for extracting semantic classes and hyponym relationships. Section 3 describes our algorithm for labeling concepts and for extracting hyponym relationships. Experimental results are presented in Section 4 and finally, we conclude with a discussion and future work. 2 Previous Work There have been several approaches to automatically discovering lexico-semantic information from text (Hearst 1992; Riloff and Shepherd 1997; Riloff and Jones 1999; Berland and Charniak 1999; Pantel and Lin 2002; Fleischman et al. 2003; Girju et al. 2003). One approach constructs automatic thesauri by computing the similarity between words based on their distribution in a corpus (Hindle 1990; Lin 1998). The output of these programs is a ranked list of similar words to each word. For example, Lin’s approach outputs the following top-20 similar words of orange: (D) peach, grapefruit, yellow, lemon, pink, avocado, tangerine, banana, purple, Santa Ana, strawberry, tomato, red, pineapple, pear, Apricot, apple, green, citrus, mango A common problem of such lists is that they do not discriminate between the senses of polysemous wo"
N04-1041,N03-1011,0,0.15287,"Missing"
N04-1041,C92-2082,0,0.256535,"Missing"
N04-1041,P90-1034,0,0.726335,"Missing"
N04-1041,J98-1006,0,0.0614642,"Missing"
N04-1041,P98-2127,0,0.276626,"Missing"
N04-1041,W97-0313,0,0.670171,"Missing"
N04-1041,J90-1003,0,\N,Missing
N04-1041,C94-1079,0,\N,Missing
N04-1041,W02-1111,0,\N,Missing
N04-1041,C98-2122,0,\N,Missing
N07-1017,C92-2082,0,0.184991,"elations are ubiquitous, and affect ontology reliability, if used to populate it, as the relation drives the wrong type of ontological knowledge. Erroneous or false relations. These are particularly harmful, since they directly affect algorithm precision. A pattern-based relation extraction algorithm is particularly likely to extract erroneous relations if it uses generic patterns, which are defined in (Pantel and Pennacchiotti, 2006) as broad coverage, noisy patterns with high recall and low precision (e.g. “X of Y” for part-of relation). Harvesting algorithms either ignore generic patterns (Hearst, 1992) (affecting system recall) or use manually supervised filtering approaches (Girju et al., 2006) or use completely unsupervised Web-filtering methods (Pantel and Pennacchiotti, 2006). Yet, these methods still do not sufficiently mitigate the problem of erroneous relations. Background knowledge. Another aspect that makes relation harvesting difficult is related to the 131 Proceedings of NAACL HLT 2007, pages 131–138, c Rochester, NY, April 2007. 2007 Association for Computational Linguistics nature of semantic relations: relations among entities are mostly paradigmatic (de Saussure, 1922), and a"
N07-1017,magnini-cavaglia-2000-integrating,0,0.10389,"Missing"
N07-1017,P06-1015,1,0.878152,"ng that very high precision can be reached. 1 Introduction Relation extraction is a fundamental step in many natural language processing applications such as learning ontologies from texts (Buitelaar et al., 2005) and Question Answering (Pasca and Harabagiu, 2001). The most common approach for acquiring concepts, instances and relations is to harvest semantic knowledge from texts. These techniques have been largely explored and today they achieve reasonable accuracy. Harvested lexical resources, such as concept lists (Pantel and Lin, 2002), facts (Etzioni et al., 2002) and semantic relations (Pantel and Pennacchiotti, 2006) could be then successfully used in different frameworks and applications. The state of the art technology for relation extraction primarily relies on pattern-based approaches Irrelevant relations. These are valid relations that are not of interest in the domain at hand. For example, in a political domain, “Condoleezza Rice is a football fan” is not as relevant as “Condoleezza Rice is the Secretary of State of the United States”. Irrelevant relations are ubiquitous, and affect ontology reliability, if used to populate it, as the relation drives the wrong type of ontological knowledge. Erroneou"
N07-1017,P02-1006,0,0.00953961,"OStagged corpus, Espresso takes as input few seed instances (e.g. nitrogen is-a element) or seed surface patterns (e.g. X/NN such/JJ as/IN Y/NN). It then incrementally learns new patterns and instances by iterating on the following three phases, until a specific stop condition is met (i.e., new patterns are below a pre-defined threshold of reliability). Pattern Induction. Given an input set of seed instances I, Espresso infers new patterns connecting as many instances as possible in the given corpus. To do so, Espresso uses a slight modification of the state of the art algorithm described in (Ravichandran and Hovy, 2002). For each instance in input, the sentences containing it are first retrieved and then 134 generalized, by replacing term expressions with a terminological label using regular expressions on the POS-tags. This generalization allows to ease the problem of data sparseness in small corpora. Unfortunately, as patterns become more generic, they are more prone to low precision. Pattern Ranking and Selection. Espresso ranks all extracted patterns using a reliability measure rπ and discards all but the top-k P patterns, where k is set to the number of patterns from the previous iteration plus one. rπ"
N07-1017,P06-1101,0,0.0450548,"Missing"
N07-1017,E06-2016,1,\N,Missing
N07-1017,N03-1011,0,\N,Missing
N07-1071,P01-1008,0,0.0721351,"automatically. Manual collections of semantic classes include the hierarchies of WordNet (Fellbaum 1998), Levin verb classes (Levin 1993), and FrameNet (Baker et al. 1998). Automatic derivation of semantic classes can take a variety of approaches, but often uses corpus methods and the Distributional Hypothesis (Harris 1964) to automatically cluster similar entities into classes, e.g. CBC (Pantel and Lin 2002). In this paper, we experiment with two sets of semantic classes, one from WordNet and one from CBC. Another thread related to our work includes extracting from text corpora paraphrases (Barzilay and McKeown 2001) and inference rules, e.g. TEASE1 (Szpektor et al. 2004) and DIRT (Lin and Pantel 2001). While these systems differ in their approaches, neither provides for the extracted in1 Some systems refer to inferences they extract as entailments; the two terms are sometimes used interchangeably. 565 ference rules to hold or fail based on SPs. Zanzotto et al. (2006) recently explored a different interplay between SPs and inferences. Rather than examine the role of SPs in inferences, they use SPs of a particular type to derive inferences. For instance the preference of win for the subject player, a nomin"
N07-1071,P98-1013,0,0.0143219,"Light and Greiff 2002). Rather than venture into learning inferential SPs, much previous work has focused on learning SPs for simpler structures. Resnik (1996), the seminal paper on this topic, introduced a statistical model for learning SPs for predicates using an unsupervised method. Learning SPs often relies on an underlying set of semantic classes, as in both Resnik’s and our approach. Semantic classes can be specified manually or derived automatically. Manual collections of semantic classes include the hierarchies of WordNet (Fellbaum 1998), Levin verb classes (Levin 1993), and FrameNet (Baker et al. 1998). Automatic derivation of semantic classes can take a variety of approaches, but often uses corpus methods and the Distributional Hypothesis (Harris 1964) to automatically cluster similar entities into classes, e.g. CBC (Pantel and Lin 2002). In this paper, we experiment with two sets of semantic classes, one from WordNet and one from CBC. Another thread related to our work includes extracting from text corpora paraphrases (Barzilay and McKeown 2001) and inference rules, e.g. TEASE1 (Szpektor et al. 2004) and DIRT (Lin and Pantel 2001). While these systems differ in their approaches, neither p"
N07-1071,P06-1114,0,0.0274597,"tors for murder and conspiracy in the Oklahoma City bombing. and an inference rule such as: X is charged by Y ⇒ Y announced the arrest of X (1) Using this rule, we can infer that “federal prosecutors announced the arrest of Terry Nichols”. However, given the sentence: Fraud was suspected when accounts were charged by CCM telemarketers without obtaining consumer authorization. Introduction Semantic inference is a key component for advanced natural language understanding. Several important applications are already relying heavily on inference, including question answering (Moldovan et al. 2003; Harabagiu and Hickl 2006), information extraction (Romano et al. 2006), and textual entailment (Szpektor et al. 2004). In response, several researchers have created resources for enabling semantic inference. Among manual resources used for this task are WordNet (Fellbaum 1998) and Cyc (Lenat 1995). Although important and useful, these resources primarily contain prescriptive inference rules such as “X divorces Y ⇒ X married Y”. In practical NLP applications, however, plausible inference rules such as “X married Y” ⇒ “X dated Y” are very useful. This, along with the difficulty and labor-intensiveness of generating exha"
N07-1071,P93-1016,0,0.279352,"Missing"
N07-1071,N03-1022,0,0.0131744,"ged by federal prosecutors for murder and conspiracy in the Oklahoma City bombing. and an inference rule such as: X is charged by Y ⇒ Y announced the arrest of X (1) Using this rule, we can infer that “federal prosecutors announced the arrest of Terry Nichols”. However, given the sentence: Fraud was suspected when accounts were charged by CCM telemarketers without obtaining consumer authorization. Introduction Semantic inference is a key component for advanced natural language understanding. Several important applications are already relying heavily on inference, including question answering (Moldovan et al. 2003; Harabagiu and Hickl 2006), information extraction (Romano et al. 2006), and textual entailment (Szpektor et al. 2004). In response, several researchers have created resources for enabling semantic inference. Among manual resources used for this task are WordNet (Fellbaum 1998) and Cyc (Lenat 1995). Although important and useful, these resources primarily contain prescriptive inference rules such as “X divorces Y ⇒ X married Y”. In practical NLP applications, however, plausible inference rules such as “X married Y” ⇒ “X dated Y” are very useful. This, along with the difficulty and labor-inten"
N07-1071,E06-1052,0,0.0362033,"Missing"
N07-1071,W04-3206,1,0.918889,"charged by Y ⇒ Y announced the arrest of X (1) Using this rule, we can infer that “federal prosecutors announced the arrest of Terry Nichols”. However, given the sentence: Fraud was suspected when accounts were charged by CCM telemarketers without obtaining consumer authorization. Introduction Semantic inference is a key component for advanced natural language understanding. Several important applications are already relying heavily on inference, including question answering (Moldovan et al. 2003; Harabagiu and Hickl 2006), information extraction (Romano et al. 2006), and textual entailment (Szpektor et al. 2004). In response, several researchers have created resources for enabling semantic inference. Among manual resources used for this task are WordNet (Fellbaum 1998) and Cyc (Lenat 1995). Although important and useful, these resources primarily contain prescriptive inference rules such as “X divorces Y ⇒ X married Y”. In practical NLP applications, however, plausible inference rules such as “X married Y” ⇒ “X dated Y” are very useful. This, along with the difficulty and labor-intensiveness of generating exhaustive lists of rules, has led rethe plausible inference rule (1) would incorrectly infer th"
N07-1071,P06-1107,0,0.138975,"tities into classes, e.g. CBC (Pantel and Lin 2002). In this paper, we experiment with two sets of semantic classes, one from WordNet and one from CBC. Another thread related to our work includes extracting from text corpora paraphrases (Barzilay and McKeown 2001) and inference rules, e.g. TEASE1 (Szpektor et al. 2004) and DIRT (Lin and Pantel 2001). While these systems differ in their approaches, neither provides for the extracted in1 Some systems refer to inferences they extract as entailments; the two terms are sometimes used interchangeably. 565 ference rules to hold or fail based on SPs. Zanzotto et al. (2006) recently explored a different interplay between SPs and inferences. Rather than examine the role of SPs in inferences, they use SPs of a particular type to derive inferences. For instance the preference of win for the subject player, a nominalization of play, is used to derive that “win ⇒ play”. Our work can be viewed as complementary to the work on extracting semantic inferences and paraphrases, since we seek to refine when a given inference applies, filtering out incorrect inferences. 3 Selectional Preference Models The aim of this paper is to learn inferential selectional preferences for f"
N07-1071,C98-1013,0,\N,Missing
N09-1033,P01-1005,0,0.0547877,"pansion errors. Using user feedback to improve a system’s performance is a common theme within many information retrieval and machine learning tasks. One form of user feedback is active learning (Cohn et al. 1994), where one or more classifiers are used to focus human annotation efforts on the most beneficial test cases. Active learning has been successfully applied to various natural language tasks such as parsing (Tang et al. 2001), POS tagging (Dagan and Engelson 1995) and providing large amounts of annotations for common natural language processing tasks such as word sense disambiguation (Banko and Brill 2001). Relevance feedback is another popular feedback paradigm commonly used in information retrieval (Harman 1992), where user feedback (either explicit or implicit) is used to refine the search results of an IR system. Relevance feedback has been successfully applied to many IR applications including content-based image retrieval (Zhouand Huang 2003) and web search (Vishwa et al. 2005). Within NLP applications relevance feedback has also been used to generate sense tagged examples for WSD tasks (Stevenson et al. 2008), and Question Answering (Negri 2004). Our methods use relevance feedback in the"
N09-1033,J95-4004,0,0.256374,"’s negative example and was given to the system as an error. In the next section, we report R-precision gains at each iteration in the editorial process for our two methods described in Section 3. Our baseline method simply measures the gains obtained by removing the first incorrect entry in a candidate expansion set at each iteration. This simulates the process of manually cleaning a set by removing one error at a time. 5 5.1 Experimental Results Experimental Setup Wikipedia5 served as the source corpus for our algorithms described in Sections 3.1 and 3.2. All articles were POS-tagged using (Brill 1995) and later chunked using a variant of (Abney 1991). Corpus statistics from this processed text were collected to build the similarity matrix for the SIM method (Section 3.1) and to extract the features required for the FMM method (Section 3.2). In both cases corpus statistics were extracted over the semi-syntactic contexts (chunks) to approximate term meanings. The minimum similarity thresholds were experimentally set to 0.15 and 0.11 for the SIM and FMM algorithms respectively. Each experimental trial described in Section 4.2, which consists of a set of seed instances of one of our 50 random"
N09-1033,P04-1056,0,0.0726498,"ithms for refining sets of named entities. The datasets and our evaluation methodology used to perform our experiments are presented in Section 4 and in Section 5 we describe experimental results. Finally, we conclude with some discussion and future work. 2 Related Work There is a large body of work for automatically building sets of named entities using various techniques including supervised, unsupervised and semi-supervised methods. Supervised techniques use large amounts of training data to detect and classify entities into coarse grained classes such as People, Organizations, and Places (Bunescu and Mooney 2004; Etzioni et al. 2005). On the other hand, unsupervised methods require no training 2 See http://demo.patrickpantel.com/ for a demonstration of the distributional thesaurus. 3 In practice, this problem is rare since most terms that are similar in one of their senses tend not to be similar in their other senses. 291 data and rely on approaches such as clustering, targeted patterns and co-occurrences to extract sets of entities (Pantel and Lin 2002; Downey et al. 2007). Semi-supervised approaches are often used in practice since they allow for targeting specific entity classes. These methods rel"
N09-1033,C92-2082,0,0.187291,"Missing"
N09-1033,P08-1119,0,0.295514,"Missing"
N09-1033,P06-1102,0,0.0123714,"ties (Pantel and Lin 2002; Downey et al. 2007). Semi-supervised approaches are often used in practice since they allow for targeting specific entity classes. These methods rely on a small set of seed examples to extract sets of entities. They either are based on distributional approaches or employ lexico-syntactic patterns to expand a small set of seeds to a larger set of candidate expansions. Some methods such as (Riloff and Shepherd 1997; Riloff and Jones 1999; Banko et al. 2007;Paşca 2007a) use lexico-syntactic patterns to expand a set of seeds from web text and query logs. Others such as (Paşca et al. 2006; Paşca 2007b; Paşca and Durme 2008) use distributional approaches. Wang and Cohen (2007) use structural cues in semistructured text to expand sets of seed elements. In all methods however, expansion errors inevitably occur. This paper focuses on the task of post processing any such system’s expansion output using minimal human judgments in order to remove expansion errors. Using user feedback to improve a system’s performance is a common theme within many information retrieval and machine learning tasks. One form of user feedback is active learning (Cohn et al. 1994), where one or more classi"
N09-1033,P08-1003,0,0.127739,"y et al. 2007). Semi-supervised approaches are often used in practice since they allow for targeting specific entity classes. These methods rely on a small set of seed examples to extract sets of entities. They either are based on distributional approaches or employ lexico-syntactic patterns to expand a small set of seeds to a larger set of candidate expansions. Some methods such as (Riloff and Shepherd 1997; Riloff and Jones 1999; Banko et al. 2007;Paşca 2007a) use lexico-syntactic patterns to expand a set of seeds from web text and query logs. Others such as (Paşca et al. 2006; Paşca 2007b; Paşca and Durme 2008) use distributional approaches. Wang and Cohen (2007) use structural cues in semistructured text to expand sets of seed elements. In all methods however, expansion errors inevitably occur. This paper focuses on the task of post processing any such system’s expansion output using minimal human judgments in order to remove expansion errors. Using user feedback to improve a system’s performance is a common theme within many information retrieval and machine learning tasks. One form of user feedback is active learning (Cohn et al. 1994), where one or more classifiers are used to focus human annota"
N09-1033,W97-0313,0,0.0638149,"in one of their senses tend not to be similar in their other senses. 291 data and rely on approaches such as clustering, targeted patterns and co-occurrences to extract sets of entities (Pantel and Lin 2002; Downey et al. 2007). Semi-supervised approaches are often used in practice since they allow for targeting specific entity classes. These methods rely on a small set of seed examples to extract sets of entities. They either are based on distributional approaches or employ lexico-syntactic patterns to expand a small set of seeds to a larger set of candidate expansions. Some methods such as (Riloff and Shepherd 1997; Riloff and Jones 1999; Banko et al. 2007;Paşca 2007a) use lexico-syntactic patterns to expand a set of seeds from web text and query logs. Others such as (Paşca et al. 2006; Paşca 2007b; Paşca and Durme 2008) use distributional approaches. Wang and Cohen (2007) use structural cues in semistructured text to expand sets of seed elements. In all methods however, expansion errors inevitably occur. This paper focuses on the task of post processing any such system’s expansion output using minimal human judgments in order to remove expansion errors. Using user feedback to improve a system’s perform"
N09-1033,C08-1102,0,0.028258,"for common natural language processing tasks such as word sense disambiguation (Banko and Brill 2001). Relevance feedback is another popular feedback paradigm commonly used in information retrieval (Harman 1992), where user feedback (either explicit or implicit) is used to refine the search results of an IR system. Relevance feedback has been successfully applied to many IR applications including content-based image retrieval (Zhouand Huang 2003) and web search (Vishwa et al. 2005). Within NLP applications relevance feedback has also been used to generate sense tagged examples for WSD tasks (Stevenson et al. 2008), and Question Answering (Negri 2004). Our methods use relevance feedback in the form of negative examples to refine the results of a set expansion system. 3 Dynamic Similarity Modeling The set expansion algorithms discussed in Section 2 often produce high quality entity sets, however inevitably errors are introduced. Applications requiring high precision sets must invest significantly in editorial efforts to clean up the sets. Although companies like Yahoo! and Google can afford to routinely support such manual labor, there is a large opportunity to reduce the refinement cost (i.e., number of"
N09-1033,P02-1060,0,\N,Missing
N09-1033,P98-2127,0,\N,Missing
N09-1033,C98-2122,0,\N,Missing
N09-1033,P02-1016,0,\N,Missing
N12-1074,N10-1020,0,0.132636,"ertile avenues of research is modeling users and their interactions on Twitter. An extensive line of work characterizes users (Pear Analytics, 2009) and quantifies user influence (Cha et al., 2010; Romero et al., 2011; Wu et al., 2011; Bakshy et al., 2011). Popescu and Jain (2011) explored how businesses use Twitter to connect with their customer base. Popescu and Pennacchiotti (2011) and Qu et al. (2011) investigated how users react to events on social media. There also has been extensive work on modeling conversational interactions on Twitter (Honeycutt and Herring, 2009; Boyd et al., 2010; Ritter et al., 2010; Danescu-Niculescu-Mizil et al., 2011). Our work builds on these findings to predict response behavior on a large scale. Mining Twitter Social media has been used to detect events (Sakaki et al., 2010; Popescu and Pennacchiotti, 2010; Popescu et al., 2011), and even predict their outcomes (Asur and Huberman, 2010; Culotta, 2010). Similarly to this line of work, we mine the social network for event prediction. In contrast, our focus is on predicting events within the network. Response Prediction There has been significant work addressing the task of response prediction in news articles (Tsagki"
N12-1074,D11-1054,0,0.013986,"l networks has been investigated previously: Hong et al. (2011) focused on predicting responses for highly popular items, Rowe et al. (2011) targeted the prediction of conversations and their length and Suh et al. (2010) and Petrovic et al. (2011) predicted retweets. In contrast, our work targets tweets regardless of their popularity and attempts to predict both replies and retweets. Furthermore, we present a scalable method to use linguistic lexical features in discriminative models by leveraging global network statistics. A related task to ours is that of response generation, as explored by Ritter et al. (2011). Our work complements their approach by allowing to detect when the generation of a response is appropriate. Lastly, the task of predicting the spread of hashtags in microblogging networks (Tsur and Rappoport, 2012) is also closely related to our work and both approaches supplement each other as measures of impact. Ranking in News Feeds Different approaches were suggested for ranking items in social media (Das Sarma et al., 2010; Lakkaraju et al., 2011). Our work provides an important signal, which can be incorporated into any ranking approach. 3 Response Prediction on Twitter Our goal is to"
N12-1074,N09-1054,0,0.0112984,"uilds on these findings to predict response behavior on a large scale. Mining Twitter Social media has been used to detect events (Sakaki et al., 2010; Popescu and Pennacchiotti, 2010; Popescu et al., 2011), and even predict their outcomes (Asur and Huberman, 2010; Culotta, 2010). Similarly to this line of work, we mine the social network for event prediction. In contrast, our focus is on predicting events within the network. Response Prediction There has been significant work addressing the task of response prediction in news articles (Tsagkias et al., 2009; Tsagkias et al., 2010) and blogs (Yano et al., 2009; Yano and Smith, 2010; Balasubramanyan et al., 2011). The task of predicting responses in social networks has been investigated previously: Hong et al. (2011) focused on predicting responses for highly popular items, Rowe et al. (2011) targeted the prediction of conversations and their length and Suh et al. (2010) and Petrovic et al. (2011) predicted retweets. In contrast, our work targets tweets regardless of their popularity and attempts to predict both replies and retweets. Furthermore, we present a scalable method to use linguistic lexical features in discriminative models by leveraging g"
N12-1074,J96-1002,0,\N,Missing
N16-1171,P08-1107,0,0.0310575,"l., 2008), analyzing how topics change over time (Wang and McCallum, 2006), understanding entity relations (Balasubramanyan and Cohen, 2011), analyzing communication networks (Nguyen et al., 2013), for authorship attribution (Seroussi et al., 2012), and discovering topics associated with authors (McCallum et al., 2005). Other generative models have also been used for analyzing email communication behavior (Navaroli et al., 2012), identifying links between an email sender and a recipient to detect potential anomalous communication (Huang and Zeng, 2006), and resolving personal names in emails (Elsayed et al., 2008). Representing workplace activities of the emails with probabilistic inference in graphical models where observed information is personalized to the email senders is what sets our work apart from previous research in computational models for emails. 2.3 Email Recipient Recommendation For recommending email recipients, interactions among email participants and content similarity are the signals that have been explored most. Carvalho and Cohen (2007) leveraged content similarity by creating tf-idf centroid vectors and determining k-nearest neighbors of a target email. Pal et al. (2007) presented"
N16-1171,N12-3006,1,0.715863,"OI) phrases. = b] = t] Subject and Body Token Representations Previous work in modeling email content mostly explored bag of words (e.g., (Graus et al., 2014)) or tf-idf vectors (e.g., (Carvalho and Cohen, 2007)) as the content representation of an email. For modeling activities in emails, we experiment with different linguistic representations of the email content. They are: • Lexical: as the lexical representation, we use the bag of words (BOW) from email body and subject, after Penn Tree Bank (PTB) style tokenization. • Syntactic: using heuristics on the output of a PTB constituent parser (Quirk et al., 2012), we identify Nouns (N) and Verb Phrases (VP) in email body and subject. • Semantic: we identify phrases in emails that represent topics, concept and entities discussed in the emails. We refer to them as Thing of Interest (TOI). To extract these key phrases, we use Web search queries as a source of information. Using queries as a dictionary of possible key phrases is useful but has limited coverage since many topics/concepts are discussed in emails but absent or not widely available in Web search queries. Instead of using queries directly, we use them to construct a training set of key phrases"
N16-1171,P12-2052,0,0.0154431,"various linguistic representations of content. 2.2 Generative models for Emails Latent Dirichlet Allocation (LDA) (Blei et al., 2003) is a frequently used generative topic model. Assuming a Dirichlet prior, LDA models learn probability distributions of words as latent topics in a corpus. In emails, LDA models have been used for learn1453 ing summary keywords (Dredze et al., 2008), analyzing how topics change over time (Wang and McCallum, 2006), understanding entity relations (Balasubramanyan and Cohen, 2011), analyzing communication networks (Nguyen et al., 2013), for authorship attribution (Seroussi et al., 2012), and discovering topics associated with authors (McCallum et al., 2005). Other generative models have also been used for analyzing email communication behavior (Navaroli et al., 2012), identifying links between an email sender and a recipient to detect potential anomalous communication (Huang and Zeng, 2006), and resolving personal names in emails (Elsayed et al., 2008). Representing workplace activities of the emails with probabilistic inference in graphical models where observed information is personalized to the email senders is what sets our work apart from previous research in computatio"
P00-1014,J95-4004,0,0.0278631,"Missing"
P00-1014,C94-2195,0,0.60927,"Missing"
P00-1014,W95-0103,0,0.849537,"textually similar words. Attachment scores are obtained using a linear combination of features of the 4-tuple. Finally, we combine the average attachment scores with the attachment score of N2 attaching to the original V and the attachment score of N2 attaching to the original N1. The proposed classification represents the attachment site that scored highest. 1 Previous Work Altmann and Steedman (1988) showed that current discourse context is often required for disambiguating attachments. Recent work shows that it is generally sufficient to utilize lexical information (Brill and Resnik, 1994; Collins and Brooks, 1995; Hindle and Rooth, 1993; Ratnaparkhi et al., 1994). One of the earliest corpus-based approaches to prepositional phrase attachment used lexical preference by computing co-occurrence frequencies (lexical associations) of verbs and nouns with prepositions (Hindle and Rooth, 1993). Training data was obtained by extracting all phrases of the form (V, N1, P, N2) from a large parsed corpus. Supervised methods later improved attachment accuracy. Ratnaparkhi et al. (1994) used a maximum entropy model considering only lexical information from within the verb phrase (ignoring N2). They experimented wit"
P00-1014,P98-2177,0,0.59429,"VScore(Wk, Pk, N2k) if Wk = Vk; otherwise Score(Wk, Pk, N2k) = NScore(Wk, Pk, N2k). Suppose that after the initial frequency table is set NScore(man, in, park) = 1.23, VScore(saw, with, telescope) = 3.65, and NScore(dog, with, telescope) = 0.35. Then, the updated cooccurrence frequencies for (man, in, park) and (saw, with, telescope) are: fr(man, in, park) = fr(saw, with, telescope) = 1.23 1.23 3.65 3.65 + 0.35 = 1.0 = 0.913 Table 3 shows the updated frequency table after the first iteration of the algorithm. The resulting database contained 8,900,000 triples. 3.2 Unambiguous Data Set As in (Ratnaparkhi, 1998), we constructed a training data set consisting of only unambiguous attachments of the form (V, P, N2) and (N1, P, N2). We only extract a 3-tuple from a sentence when our program finds no alternative attachment site for its preposition. Each extracted 3-tuple is assigned a frequency count of 1. For example, in the previous sentence, (man, in, park) is extracted since it contains only one attachment site; (dog, with, telescope) is not extracted since with has an alternative attachment site. The resulting database contained 4,400,000 triples. 4 Classification Model Roth (1998) presented a unifie"
P00-1014,P99-1041,1,0.318605,"ion P, we increment the frequency of the corresponding 3-tuple by 1/k. For example, Table 2 shows the initial cooccurrence frequency table for the corresponding 3-tuples of the above sentence. 3Available at www.cs.ualberta.ca/~lindek/minipar.htm. P N2 FREQUENCY man in park 1.0 saw with telescope 0.5 dog with telescope 0.5 Table 3. Co-occurrence frequency table entries for A man in the park saw a dog with a telescope after one iteration. Training Data Extraction We parsed a 125-million word newspaper corpus with Minipar3, a descendent of Principar (Lin, 1994). Minipar outputs dependency trees (Lin, 1999) from the input sentences. For example, the following sentence is decomposed into a dependency tree: V OR N1 V OR N1 P N2 FREQUENCY man in park saw with telescope 0.913 dog with telescope 0.087 1.0 In the following iterations of the algorithm, we update the frequency table as follows. For each k possible attachment site of a preposition P, we refine its attachment score using the formulas described in Section 4: VScore(Vk, Pk, N2k) and NScore(N1k, Pk, N2k). For any tuple (Wk, Pk, N2k), where Wk is either Vk or N2k, we update its frequency as: fr(Wk , Pk , N 2 )= k Score (Wk , Pk , N 2 k ) ∑i ="
P00-1014,P98-2127,1,0.689557,"kid 22, patient 7, refugee 2, rider 1, Russian 1, shark 2, something 19, We 239, wolf 5, ... salad: adj-modifier: assorted 1, crisp 4, fresh 13, good 3, grilled 5, leftover 3, mixed 4, olive 3, prepared 3, side 4, small 6, special 5, vegetable 3, ... object-of: add 3, consume 1, dress 1, grow 1, harvest 2, have 20, like 5, love 1, mix 1, pick 1, place 3, prepare 4, return 3, rinse 1, season 1, serve 8, sprinkle 1, taste 1, test 1, Toss 8, try 3, ... Figure 1. Excepts of entries in the collocation database for eat and salad. Table 1. The top 20 most similar words of eat and salad as given by (Lin, 1998b). WORD SIMILAR WORDS (WITH SIMILARITY SCORE) EAT cook 0.127, drink 0.108, consume 0.101, feed 0.094, taste 0.093, like 0.092, serve 0.089, bake 0.087, sleep 0.086, pick 0.085, fry 0.084, freeze 0.081, enjoy 0.079, smoke 0.078, harvest 0.076, love 0.076, chop 0.074, sprinkle 0.072, Toss 0.072, chew 0.072 SALAD soup 0.172, sandwich 0.169, sauce 0.152, pasta 0.149, dish 0.135, vegetable 0.135, cheese 0.132, dessert 0.13, entree 0.121, bread 0.116, meat 0.116, chicken 0.115, pizza 0.114, rice 0.112, seafood 0.11, dressing 0.109, cake 0.107, steak 0.105, noodle 0.105, bean 0.102 occurrence freque"
P00-1014,C94-1079,1,0.64883,"each k possible attachment site of a preposition P, we increment the frequency of the corresponding 3-tuple by 1/k. For example, Table 2 shows the initial cooccurrence frequency table for the corresponding 3-tuples of the above sentence. 3Available at www.cs.ualberta.ca/~lindek/minipar.htm. P N2 FREQUENCY man in park 1.0 saw with telescope 0.5 dog with telescope 0.5 Table 3. Co-occurrence frequency table entries for A man in the park saw a dog with a telescope after one iteration. Training Data Extraction We parsed a 125-million word newspaper corpus with Minipar3, a descendent of Principar (Lin, 1994). Minipar outputs dependency trees (Lin, 1999) from the input sentences. For example, the following sentence is decomposed into a dependency tree: V OR N1 V OR N1 P N2 FREQUENCY man in park saw with telescope 0.913 dog with telescope 0.087 1.0 In the following iterations of the algorithm, we update the frequency table as follows. For each k possible attachment site of a preposition P, we refine its attachment score using the formulas described in Section 4: VScore(Vk, Pk, N2k) and NScore(N1k, Pk, N2k). For any tuple (Wk, Pk, N2k), where Wk is either Vk or N2k, we update its frequency as: fr(Wk"
P00-1014,H94-1048,0,0.766158,"ained using a linear combination of features of the 4-tuple. Finally, we combine the average attachment scores with the attachment score of N2 attaching to the original V and the attachment score of N2 attaching to the original N1. The proposed classification represents the attachment site that scored highest. 1 Previous Work Altmann and Steedman (1988) showed that current discourse context is often required for disambiguating attachments. Recent work shows that it is generally sufficient to utilize lexical information (Brill and Resnik, 1994; Collins and Brooks, 1995; Hindle and Rooth, 1993; Ratnaparkhi et al., 1994). One of the earliest corpus-based approaches to prepositional phrase attachment used lexical preference by computing co-occurrence frequencies (lexical associations) of verbs and nouns with prepositions (Hindle and Rooth, 1993). Training data was obtained by extracting all phrases of the form (V, N1, P, N2) from a large parsed corpus. Supervised methods later improved attachment accuracy. Ratnaparkhi et al. (1994) used a maximum entropy model considering only lexical information from within the verb phrase (ignoring N2). They experimented with both word features and word class features, their"
P00-1014,W97-0109,0,0.750863,"Missing"
P00-1014,J93-1005,0,\N,Missing
P00-1014,C98-2122,1,\N,Missing
P00-1014,C98-2172,0,\N,Missing
P05-1016,N03-1011,0,0.0330543,"Missing"
P05-1016,W99-0501,0,0.0516797,"hich are induced by our framework). The sense whose feature vector is most similar is selected. It remains to be seen in future work how well this approach performs on ontologizing various semantic resources. In this paper, we focus on the general framework for inducing the ontological co-occurrence vectors and we apply it to the task of linking new terms into the ontology. 2 Relevant work Our framework aims at enriching WordNet-like ontologies with syntactic features derived from a non-annotated corpus. Others have also made significant additions to WordNet. For example, in eXtended WordNet (Harabagiu et al. 1999), the rich glosses in WordNet are enriched by disambiguating the nouns, verbs, adverbs, and adjectives with synsets. Another work has enriched WordNet synsets with topically related words extracted from the Web (Agirre et al. 2001). While this method takes advantage of the redundancy of the web, our source of information is a local document collection, which opens the possibility for domain specific applications. Distributional approaches to building semantic repositories have shown remarkable power. The underlying assumption, called the Distributional Hypothesis (Harris 1985), links the seman"
P05-1016,P98-1013,0,0.0617254,"gescale and internally consistent collections of semantic information. 1 Ontologizing semantic resources Introduction Despite considerable effort, there is still today no commonly accepted semantic corpus, semantic framework, notation, or even agreement on precisely which aspects of semantics are most useful (if at all). We believe that one important reason for this rather startling fact is the absence of truly wide-coverage semantic resources. Recognizing this, some recent work on wide coverage term banks, like WordNet (Miller 1990) and CYC (Lenat 1995), and annotated corpora, like FrameNet (Baker et al. 1998), Propbank (Kingsbury et al. 2002) and Nombank (Meyers et al. 2004), seeks to address the problem. But manual efforts such as these suffer from two drawbacks: they are difficult to tailor to new domains, and they have internal inconsistencies that can make automating the acquisition process difficult. Recently, researchers have applied text- and web-mining algorithms for automatically creating lexical semantic resources like similarity lists (Lin 1998), semantic lexicons (Riloff and Shepherd 1997), hyponymy lists (Shinzato and Torisawa 2004; Pantel and Ravichandran 2004), partwhole lists (Girg"
P05-1016,W04-3205,1,0.82917,"Missing"
P05-1016,P98-2127,0,0.913009,", some recent work on wide coverage term banks, like WordNet (Miller 1990) and CYC (Lenat 1995), and annotated corpora, like FrameNet (Baker et al. 1998), Propbank (Kingsbury et al. 2002) and Nombank (Meyers et al. 2004), seeks to address the problem. But manual efforts such as these suffer from two drawbacks: they are difficult to tailor to new domains, and they have internal inconsistencies that can make automating the acquisition process difficult. Recently, researchers have applied text- and web-mining algorithms for automatically creating lexical semantic resources like similarity lists (Lin 1998), semantic lexicons (Riloff and Shepherd 1997), hyponymy lists (Shinzato and Torisawa 2004; Pantel and Ravichandran 2004), partwhole lists (Girgu et al. 2003), and verb relation graphs (Chklovski and Pantel 2004). However, none of these resources have been directly linked into an ontological framework. For example, in VERBOCEAN (Chklovski and Pantel 2004), we find the verb relation “to surpass is-stronger-than to hit”, but it is not specified that it is the achieving sense of hit where this relation applies. We term ontologizing a lexical-semantic resource as the task of sense disambiguating t"
P05-1016,C94-1079,0,0.0232221,"ity and syntactic features. Consider the following sentence: The tsunami left a trail of horror. In a proximity approach, a word is represented by a window of words surrounding it. For the above sentence, a window of size 1 would yield two features (-1:the and +1:left) for the word tsunami. In a syntactic approach, more linguistically rich features are extracted by using each grammatical relation in which a word is involved (e.g. the features for tsunami are determiner:the and subject-of:leave). For the purposes of this work, we consider the propagation of syntactic features. We used Minipar (Lin 1994), a broad coverage parser, to analyze text. We collected the statistics on the grammatical relations (contexts) output by Minipar and used these as the feature vectors. Following Lin (1998), we measure each feature f for a word e not by its frequency but by its pointwise mutual information, mief: Input: Decomposable object A node n and a corpus C. Termination Condition: If n is a leaf node then assign to n its lexical feature vector as described in Section 3. Step 2: Recursion Step: For each child c of n, reecurse on c and C. Assign a feature vector to n by propagating features from its childr"
P05-1016,meyers-etal-2004-annotating,0,0.0226023,"Missing"
P05-1016,W97-0313,0,0.0529255,"erage term banks, like WordNet (Miller 1990) and CYC (Lenat 1995), and annotated corpora, like FrameNet (Baker et al. 1998), Propbank (Kingsbury et al. 2002) and Nombank (Meyers et al. 2004), seeks to address the problem. But manual efforts such as these suffer from two drawbacks: they are difficult to tailor to new domains, and they have internal inconsistencies that can make automating the acquisition process difficult. Recently, researchers have applied text- and web-mining algorithms for automatically creating lexical semantic resources like similarity lists (Lin 1998), semantic lexicons (Riloff and Shepherd 1997), hyponymy lists (Shinzato and Torisawa 2004; Pantel and Ravichandran 2004), partwhole lists (Girgu et al. 2003), and verb relation graphs (Chklovski and Pantel 2004). However, none of these resources have been directly linked into an ontological framework. For example, in VERBOCEAN (Chklovski and Pantel 2004), we find the verb relation “to surpass is-stronger-than to hit”, but it is not specified that it is the achieving sense of hit where this relation applies. We term ontologizing a lexical-semantic resource as the task of sense disambiguating the resource. This problem is different but not"
P05-1016,N04-1010,0,0.0206906,") and CYC (Lenat 1995), and annotated corpora, like FrameNet (Baker et al. 1998), Propbank (Kingsbury et al. 2002) and Nombank (Meyers et al. 2004), seeks to address the problem. But manual efforts such as these suffer from two drawbacks: they are difficult to tailor to new domains, and they have internal inconsistencies that can make automating the acquisition process difficult. Recently, researchers have applied text- and web-mining algorithms for automatically creating lexical semantic resources like similarity lists (Lin 1998), semantic lexicons (Riloff and Shepherd 1997), hyponymy lists (Shinzato and Torisawa 2004; Pantel and Ravichandran 2004), partwhole lists (Girgu et al. 2003), and verb relation graphs (Chklovski and Pantel 2004). However, none of these resources have been directly linked into an ontological framework. For example, in VERBOCEAN (Chklovski and Pantel 2004), we find the verb relation “to surpass is-stronger-than to hit”, but it is not specified that it is the achieving sense of hit where this relation applies. We term ontologizing a lexical-semantic resource as the task of sense disambiguating the resource. This problem is different but not orthogonal to word-sense disambiguation. If"
P05-1016,C98-1013,0,\N,Missing
P05-1016,P90-1034,0,\N,Missing
P05-1016,C98-2122,0,\N,Missing
P05-1077,P89-1010,0,0.173444,"rally not easily web scalable. This kind of feature set does not seem to affect our results. Curran and Moens (2002) also report comparable results for Minipar features and simple word based proximity features. Table 1 gives the characteristics of both corpora. Since we use grammatical context, the feature set is considerably larger than the simple word based proximity feature set for the newspaper corpus. 4.3 Calculating Feature Vectors Having collected all nouns and their features, we now proceed to construct feature vectors (and values) for nouns from both corpora using mutual information (Church and Hanks, 1989). We first construct a frequency count vector C(e) = (ce1 , ce2 , ..., cek ), where k is the total number of features and cef is the frequency count of feature f occurring in word e. Here, cef is the number of times word e occurred in context f . We then construct a mutual information vector M I(e) = (mie1 , mie2 , ..., miek ) for each word e, where mief is the pointwise mutual information between word e and feature f , which is defined as: mief = log Pn cif i=1 N cef N × cej j=1 N Pk (6) where n is the number of words and N = 5 We perform this operation so that we can compare the performance"
P05-1077,P02-1030,0,0.00716698,"pus Corpus Size Unique Nouns Feature size Newspaper 6GB 65,547 940,154 Web 138GB 655,495 1,306,482 the left and right of each noun. We use the context words as features of the noun vector. 4.2 Newspaper Corpus We parse a 6 GB newspaper (TREC9 and TREC2002 collection) corpus using the dependency parser Minipar (Lin, 1994). We identify all nouns. For each noun we take the grammatical context of the noun as identified by Minipar5 . We do not use grammatical features in the web corpus since parsing is generally not easily web scalable. This kind of feature set does not seem to affect our results. Curran and Moens (2002) also report comparable results for Minipar features and simple word based proximity features. Table 1 gives the characteristics of both corpora. Since we use grammatical context, the feature set is considerably larger than the simple word based proximity feature set for the newspaper corpus. 4.3 Calculating Feature Vectors Having collected all nouns and their features, we now proceed to construct feature vectors (and values) for nouns from both corpora using mutual information (Church and Hanks, 1989). We first construct a frequency count vector C(e) = (ce1 , ce2 , ..., cek ), where k is the"
P05-1077,P90-1034,0,0.0927163,"er in the sorted lists. Overall, the algorithm takes O(nk + nlogn) time. However, for noun clustering, we generally have the number of nouns, n, smaller than the number of features, k. (i.e., n &lt; k). This implies logn &lt;&lt; k and nlogn &lt;&lt; nk. Hence the time complexity of our algorithm is O(nk + nlogn) ≈ O(nk). This is a huge saving from the original O(n2 k) algorithm. In the next section, we proceed to apply this technique for generating noun similarity lists. 4 Building Noun Similarity Lists A lot of work has been done in the NLP community on clustering words according to their meaning in text (Hindle, 1990; Lin, 1998). The basic intuition is that words that are similar to each other tend to occur in similar contexts, thus linking the semantics of words with their lexical usage in text. One may ask why is clustering of words necessary in the first place? There may be several reasons for clustering, but generally it boils down to one basic reason: if the words that occur rarely in a corpus are found to be distributionally similar to more frequently occurring words, then one may be able to make better inferences on rare words. However, to unleash the real power of clustering one has to work with l"
P05-1077,P98-2127,0,0.058624,"ed lists. Overall, the algorithm takes O(nk + nlogn) time. However, for noun clustering, we generally have the number of nouns, n, smaller than the number of features, k. (i.e., n &lt; k). This implies logn &lt;&lt; k and nlogn &lt;&lt; nk. Hence the time complexity of our algorithm is O(nk + nlogn) ≈ O(nk). This is a huge saving from the original O(n2 k) algorithm. In the next section, we proceed to apply this technique for generating noun similarity lists. 4 Building Noun Similarity Lists A lot of work has been done in the NLP community on clustering words according to their meaning in text (Hindle, 1990; Lin, 1998). The basic intuition is that words that are similar to each other tend to occur in similar contexts, thus linking the semantics of words with their lexical usage in text. One may ask why is clustering of words necessary in the first place? There may be several reasons for clustering, but generally it boils down to one basic reason: if the words that occur rarely in a corpus are found to be distributionally similar to more frequently occurring words, then one may be able to make better inferences on rare words. However, to unleash the real power of clustering one has to work with large amounts"
P05-1077,C94-1079,0,0.00934932,"the total frequency count of all i=1 features of all words. Having thus obtained the feature representation of each noun we can apply the algorithm described in Section 3 to discover similarity lists. We report results in the next section for both corpora. Table 1: Corpus description Corpus Corpus Size Unique Nouns Feature size Newspaper 6GB 65,547 940,154 Web 138GB 655,495 1,306,482 the left and right of each noun. We use the context words as features of the noun vector. 4.2 Newspaper Corpus We parse a 6 GB newspaper (TREC9 and TREC2002 collection) corpus using the dependency parser Minipar (Lin, 1994). We identify all nouns. For each noun we take the grammatical context of the noun as identified by Minipar5 . We do not use grammatical features in the web corpus since parsing is generally not easily web scalable. This kind of feature set does not seem to affect our results. Curran and Moens (2002) also report comparable results for Minipar features and simple word based proximity features. Table 1 gives the characteristics of both corpora. Since we use grammatical context, the feature set is considerably larger than the simple word based proximity feature set for the newspaper corpus. 4.3 C"
P05-1077,J90-1003,0,\N,Missing
P05-1077,H01-1052,0,\N,Missing
P05-1077,C98-2122,0,\N,Missing
P06-1015,P99-1008,0,0.691734,"tem precision dramatically decreases from the introduced noise and bootstrapping quickly spins out of control. 2 Relevant Work To date, most research on relation harvesting has focused on is-a and part-of. Approaches fall into two categories: pattern- and clustering-based. Most common are pattern-based approaches. Hearst (1992) pioneered using patterns to extract hyponym (is-a) relations. Manually building three lexico-syntactic patterns, Hearst sketched a bootstrapping algorithm to learn more patterns from instances, which has served as the model for most subsequent pattern-based algorithms. Berland and Charniak (1999) proposed a system for part-of relation extraction, based on the (Hearst 1992) approach. Seed instances are used to infer linguistic patterns that are used to extract new instances. While this study introduces statistical measures to evaluate instance quality, it remains vulnerable to data sparseness and has the limitation of considering only one-word terms. Improving upon (Berland and Charniak 1999), Girju et al. (2006) employ machine learning algorithms and WordNet (Fellbaum 1998) to disambiguate part-of generic patterns like “X’s Y” and “X of Y”. This study is the first extensive attempt to"
P06-1015,P99-1016,0,0.234956,"used part of speech patterns to extract a subset of is-a relations involving proper nouns, and (Downey et al. 2005) who formalized the problem of relation extraction in a coherent and effective combinatorial model that is shown to outperform previous probabilistic frameworks. Clustering approaches have so far been applied only to is-a extraction. These methods use clustering algorithms to group words according to their meanings in text, label the clusters using its members’ lexical or syntactic dependencies, and then extract an is-a relation between each cluster member and the cluster label. Caraballo (1999) proposed the first attempt, which used conjunction and apposition features to build noun clusters. Recently, Pantel and Ravichandran (2004) extended this approach by making use of all syntactic dependency features for each noun. The advantage of clustering approaches is that they permit algorithms to identify is-a relations that do not explicitly appear in text, however they generally fail to produce coherent clusters from fewer than 100 million words; hence they are unreliable for small corpora. 3 The Espresso Algorithm Espresso is based on the framework adopted in (Hearst 1992). It is a min"
P06-1015,A97-1051,0,0.0054802,"art systems on the task of extracting various semantic relations. 4.1 Experimental Setup We perform our experiments using the following two datasets: • TREC: This dataset consists of a sample of articles from the Aquaint (TREC-9) newswire text collection. The sample consists of 5,951,432 words extracted from the following data files: AP890101 – AP890131, AP890201 – AP890228, and AP890310 – AP890319. • CHEM: This small dataset of 313,590 words consists of a college level textbook of introductory chemistry (Brown et al. 2003). Each corpus is pre-processed using the Alembic Workbench POS-tagger (Day et al. 1997). Below we describe the systems used in our empirical evaluation of Espresso. • RH02: The algorithm by Ravichandran and Hovy (2002) described in Section 2. • GI03: The algorithm by Girju et al. (2006) described in Section 2. • PR04: The algorithm by Pantel and Ravichandran (2004) described in Section 2. • ESP-: The Espresso algorithm using the pattern and instance reliability measures, but without using generic patterns. • ESP+: The full Espresso algorithm described in this paper exploiting generic patterns. For ESP+, we experimentally set τ from Section 3.3 to τ = 0.4 for TREC and τ = 0.3 for"
P06-1015,P05-1014,0,0.00410646,"ering incorrect instances using the Web; and ii) a principled measure of pattern and instance reliability enabling the filtering algorithm. We present an empirical comparison of Espresso with various state of the art systems, on different size and genre corpora, on extracting various general and specific relations. Experimental results show that our exploitation of generic patterns substantially increases system recall with small effect on overall precision. 1 Introduction Recent attention to knowledge-rich problems such as question answering (Pasca and Harabagiu 2001) and textual entailment (Geffet and Dagan 2005) has encouraged natural language processing researchers to develop algorithms for automatically harvesting shallow semantic resources. With seemingly endless amounts of textual data at our disposal, we have a tremendous opportunity to automatically grow semantic term banks and ontological resources. To date, researchers have harvested, with varying success, several resources, including concept lists (Lin and Pantel 2002), topic signatures (Lin and Hovy 2000), facts (Etzioni et al. 2005), and word similarity lists (Hindle 1990). Many recent efforts have also focused on extracting semantic relat"
P06-1015,J06-1005,0,0.825778,"grow semantic term banks and ontological resources. To date, researchers have harvested, with varying success, several resources, including concept lists (Lin and Pantel 2002), topic signatures (Lin and Hovy 2000), facts (Etzioni et al. 2005), and word similarity lists (Hindle 1990). Many recent efforts have also focused on extracting semantic relations between entities, such as Marco Pennacchiotti ART Group - DISP University of Rome “Tor Vergata” Viale del Politecnico 1 Rome, Italy pennacchiotti@info.uniroma2.it entailments (Szpektor et al. 2004), is-a (Ravichandran and Hovy 2002), part-of (Girju et al. 2006), and other relations. The following desiderata outline the properties of an ideal relation harvesting algorithm: • Performance: it must generate both high precision and high recall relation instances; • Minimal supervision: it must require little or no human annotation; • Breadth: it must be applicable to varying corpus sizes and domains; and • Generality: it must be applicable to a wide variety of relations (i.e., not just is-a or part-of). To our knowledge, no previous harvesting algorithm addresses all these properties concurrently. In this paper, we present Espresso, a generalpurpose, bro"
P06-1015,C92-2082,0,0.761293,"res as input only a small number of seed instances; • Breadth: Espresso works on both small and large corpora – it uses Web and syntactic expansions to compensate for lacks of redundancy in small corpora; • Generality: Espresso is amenable to a wide variety of binary relations, from classical is-a and part-of to specific ones such as reaction and succession. Previous work like (Girju et al. 2006) that has made use of generic patterns through filtering has shown both high precision and high recall, at the expensive cost of much manual semantic annotation. Minimally supervised algorithms, like (Hearst 1992; Pantel et al. 2004), typically ignore generic patterns since system precision dramatically decreases from the introduced noise and bootstrapping quickly spins out of control. 2 Relevant Work To date, most research on relation harvesting has focused on is-a and part-of. Approaches fall into two categories: pattern- and clustering-based. Most common are pattern-based approaches. Hearst (1992) pioneered using patterns to extract hyponym (is-a) relations. Manually building three lexico-syntactic patterns, Hearst sketched a bootstrapping algorithm to learn more patterns from instances, which has"
P06-1015,P90-1034,0,0.304833,"wering (Pasca and Harabagiu 2001) and textual entailment (Geffet and Dagan 2005) has encouraged natural language processing researchers to develop algorithms for automatically harvesting shallow semantic resources. With seemingly endless amounts of textual data at our disposal, we have a tremendous opportunity to automatically grow semantic term banks and ontological resources. To date, researchers have harvested, with varying success, several resources, including concept lists (Lin and Pantel 2002), topic signatures (Lin and Hovy 2000), facts (Etzioni et al. 2005), and word similarity lists (Hindle 1990). Many recent efforts have also focused on extracting semantic relations between entities, such as Marco Pennacchiotti ART Group - DISP University of Rome “Tor Vergata” Viale del Politecnico 1 Rome, Italy pennacchiotti@info.uniroma2.it entailments (Szpektor et al. 2004), is-a (Ravichandran and Hovy 2002), part-of (Girju et al. 2006), and other relations. The following desiderata outline the properties of an ideal relation harvesting algorithm: • Performance: it must generate both high precision and high recall relation instances; • Minimal supervision: it must require little or no human annota"
P06-1015,C00-1072,0,0.0620359,"ntroduction Recent attention to knowledge-rich problems such as question answering (Pasca and Harabagiu 2001) and textual entailment (Geffet and Dagan 2005) has encouraged natural language processing researchers to develop algorithms for automatically harvesting shallow semantic resources. With seemingly endless amounts of textual data at our disposal, we have a tremendous opportunity to automatically grow semantic term banks and ontological resources. To date, researchers have harvested, with varying success, several resources, including concept lists (Lin and Pantel 2002), topic signatures (Lin and Hovy 2000), facts (Etzioni et al. 2005), and word similarity lists (Hindle 1990). Many recent efforts have also focused on extracting semantic relations between entities, such as Marco Pennacchiotti ART Group - DISP University of Rome “Tor Vergata” Viale del Politecnico 1 Rome, Italy pennacchiotti@info.uniroma2.it entailments (Szpektor et al. 2004), is-a (Ravichandran and Hovy 2002), part-of (Girju et al. 2006), and other relations. The following desiderata outline the properties of an ideal relation harvesting algorithm: • Performance: it must generate both high precision and high recall relation insta"
P06-1015,C02-1144,1,0.233369,"h small effect on overall precision. 1 Introduction Recent attention to knowledge-rich problems such as question answering (Pasca and Harabagiu 2001) and textual entailment (Geffet and Dagan 2005) has encouraged natural language processing researchers to develop algorithms for automatically harvesting shallow semantic resources. With seemingly endless amounts of textual data at our disposal, we have a tremendous opportunity to automatically grow semantic term banks and ontological resources. To date, researchers have harvested, with varying success, several resources, including concept lists (Lin and Pantel 2002), topic signatures (Lin and Hovy 2000), facts (Etzioni et al. 2005), and word similarity lists (Hindle 1990). Many recent efforts have also focused on extracting semantic relations between entities, such as Marco Pennacchiotti ART Group - DISP University of Rome “Tor Vergata” Viale del Politecnico 1 Rome, Italy pennacchiotti@info.uniroma2.it entailments (Szpektor et al. 2004), is-a (Ravichandran and Hovy 2002), part-of (Girju et al. 2006), and other relations. The following desiderata outline the properties of an ideal relation harvesting algorithm: • Performance: it must generate both high pr"
P06-1015,W02-1111,0,0.0931338,"and partof. Pantel et al. (2004) proposed a similar, highly scalable approach, based on an edit-distance technique, to learn lexico-POS patterns, showing both good performance and efficiency. Espresso uses a similar approach to infer patterns, but we make use of generic patterns and apply refining techniques to deal with wide variety of relations. Other pattern-based algorithms include (Riloff and Shepherd 1997), who used a semi-automatic method for discovering similar words using a few seed examples, KnowItAll (Etzioni et al. 2005) that performs large-scale extraction of facts from the Web, Mann (2002) who used part of speech patterns to extract a subset of is-a relations involving proper nouns, and (Downey et al. 2005) who formalized the problem of relation extraction in a coherent and effective combinatorial model that is shown to outperform previous probabilistic frameworks. Clustering approaches have so far been applied only to is-a extraction. These methods use clustering algorithms to group words according to their meanings in text, label the clusters using its members’ lexical or syntactic dependencies, and then extract an is-a relation between each cluster member and the cluster lab"
P06-1015,N04-1041,1,0.823584,"lized the problem of relation extraction in a coherent and effective combinatorial model that is shown to outperform previous probabilistic frameworks. Clustering approaches have so far been applied only to is-a extraction. These methods use clustering algorithms to group words according to their meanings in text, label the clusters using its members’ lexical or syntactic dependencies, and then extract an is-a relation between each cluster member and the cluster label. Caraballo (1999) proposed the first attempt, which used conjunction and apposition features to build noun clusters. Recently, Pantel and Ravichandran (2004) extended this approach by making use of all syntactic dependency features for each noun. The advantage of clustering approaches is that they permit algorithms to identify is-a relations that do not explicitly appear in text, however they generally fail to produce coherent clusters from fewer than 100 million words; hence they are unreliable for small corpora. 3 The Espresso Algorithm Espresso is based on the framework adopted in (Hearst 1992). It is a minimally supervised bootstrapping algorithm that takes as input a few seed instances of a particular relation and iteratively learns surface p"
P06-1015,P02-1006,0,0.967396,"remendous opportunity to automatically grow semantic term banks and ontological resources. To date, researchers have harvested, with varying success, several resources, including concept lists (Lin and Pantel 2002), topic signatures (Lin and Hovy 2000), facts (Etzioni et al. 2005), and word similarity lists (Hindle 1990). Many recent efforts have also focused on extracting semantic relations between entities, such as Marco Pennacchiotti ART Group - DISP University of Rome “Tor Vergata” Viale del Politecnico 1 Rome, Italy pennacchiotti@info.uniroma2.it entailments (Szpektor et al. 2004), is-a (Ravichandran and Hovy 2002), part-of (Girju et al. 2006), and other relations. The following desiderata outline the properties of an ideal relation harvesting algorithm: • Performance: it must generate both high precision and high recall relation instances; • Minimal supervision: it must require little or no human annotation; • Breadth: it must be applicable to varying corpus sizes and domains; and • Generality: it must be applicable to a wide variety of relations (i.e., not just is-a or part-of). To our knowledge, no previous harvesting algorithm addresses all these properties concurrently. In this paper, we present Es"
P06-1015,W97-0313,0,0.25279,"t of instance seeds by extracting substrings relating seeds in corpus sentences. The approach gives good results on specific relations such as birthdates, however it has low precision on generic ones like is-a and partof. Pantel et al. (2004) proposed a similar, highly scalable approach, based on an edit-distance technique, to learn lexico-POS patterns, showing both good performance and efficiency. Espresso uses a similar approach to infer patterns, but we make use of generic patterns and apply refining techniques to deal with wide variety of relations. Other pattern-based algorithms include (Riloff and Shepherd 1997), who used a semi-automatic method for discovering similar words using a few seed examples, KnowItAll (Etzioni et al. 2005) that performs large-scale extraction of facts from the Web, Mann (2002) who used part of speech patterns to extract a subset of is-a relations involving proper nouns, and (Downey et al. 2005) who formalized the problem of relation extraction in a coherent and effective combinatorial model that is shown to outperform previous probabilistic frameworks. Clustering approaches have so far been applied only to is-a extraction. These methods use clustering algorithms to group wo"
P06-1015,W04-3206,0,0.0568728,"at our disposal, we have a tremendous opportunity to automatically grow semantic term banks and ontological resources. To date, researchers have harvested, with varying success, several resources, including concept lists (Lin and Pantel 2002), topic signatures (Lin and Hovy 2000), facts (Etzioni et al. 2005), and word similarity lists (Hindle 1990). Many recent efforts have also focused on extracting semantic relations between entities, such as Marco Pennacchiotti ART Group - DISP University of Rome “Tor Vergata” Viale del Politecnico 1 Rome, Italy pennacchiotti@info.uniroma2.it entailments (Szpektor et al. 2004), is-a (Ravichandran and Hovy 2002), part-of (Girju et al. 2006), and other relations. The following desiderata outline the properties of an ideal relation harvesting algorithm: • Performance: it must generate both high precision and high recall relation instances; • Minimal supervision: it must require little or no human annotation; • Breadth: it must be applicable to varying corpus sizes and domains; and • Generality: it must be applicable to a wide variety of relations (i.e., not just is-a or part-of). To our knowledge, no previous harvesting algorithm addresses all these properties concurr"
P06-1100,W05-1203,0,0.0351582,"Missing"
P06-1100,N03-1011,0,0.751635,"present an empirical evaluation on the task of attaching partof and causation relations, showing an improvement on F-score over a baseline model. 1 Introduction NLP researchers have developed many algorithms for mining knowledge from text and the Web, including facts (Etzioni et al. 2005), semantic lexicons (Riloff and Shepherd 1997), concept lists (Lin and Pantel 2002), and word similarity lists (Hindle 1990). Many recent efforts have also focused on extracting binary semantic relations between entities, such as entailments (Szpektor et al. 2004), is-a (Ravichandran and Hovy 2002), part-of (Girju et al. 2003), and other relations. The output of most of these systems is flat lists of lexical semantic knowledge such as “Italy is-a country” and “orange similar-to blue”. However, using this knowledge beyond simple keyword matching, for example in inferences, requires it to be linked into formal semantic repositories such as ontologies or term banks like WordNet (Fellbaum 1998). Pantel (2005) defined the task of ontologizing a lexical semantic resource as linking its terms to the concepts in a WordNet-like hierarchy. For example, “orange similar-to blue” ontologizes in WordNet to “orange#2 similar-to b"
P06-1100,W03-1210,0,0.0876504,"Missing"
P06-1100,W99-0501,0,0.0831763,"ns, such as part-of, was untested. Girju et al. (2003) presented a highly supervised machine learning algorithm to infer semantic constraints on part-of relations, such as (object#1, PART-OF, social_event#1). These constraints are then used as selectional restrictions in harvesting part-of instances from ambiguous lexical patterns, like “X of Y”. The approach shows high performance in terms of precision and recall, but, as the authors acknowledge, it requires large human effort during the training phase. Others have also made significant additions to WordNet. For example, in eXtended WordNet (Harabagiu et al. 1999), the glosses in WordNet are enriched by disambiguating the nouns, verbs, adverbs, and adjectives with synsets. Another work has enriched WordNet synsets with topically related words extracted from the Web (Agirre et al. 2001). Finally, the general task of word sense disambiguation (Gale et al. 1991) is relevant since there the task is to ontologize each term in a passage into a WordNet-like sense inventory. If we had a large collection of sensetagged text, then our mining algorithms could directly discover WordNet attachment points at harvest time. However, since there is little high precisio"
P06-1100,P90-1034,0,0.216063,"owever few have linked the mined knowledge into formal knowledge repositories. In this paper, we propose two algorithms for automatically ontologizing (attaching) semantic relations into WordNet. We present an empirical evaluation on the task of attaching partof and causation relations, showing an improvement on F-score over a baseline model. 1 Introduction NLP researchers have developed many algorithms for mining knowledge from text and the Web, including facts (Etzioni et al. 2005), semantic lexicons (Riloff and Shepherd 1997), concept lists (Lin and Pantel 2002), and word similarity lists (Hindle 1990). Many recent efforts have also focused on extracting binary semantic relations between entities, such as entailments (Szpektor et al. 2004), is-a (Ravichandran and Hovy 2002), part-of (Girju et al. 2003), and other relations. The output of most of these systems is flat lists of lexical semantic knowledge such as “Italy is-a country” and “orange similar-to blue”. However, using this knowledge beyond simple keyword matching, for example in inferences, requires it to be linked into formal semantic repositories such as ontologies or term banks like WordNet (Fellbaum 1998). Pantel (2005) defined t"
P06-1100,C02-1144,1,0.428062,"eveloped to harvest lexical semantic resources, however few have linked the mined knowledge into formal knowledge repositories. In this paper, we propose two algorithms for automatically ontologizing (attaching) semantic relations into WordNet. We present an empirical evaluation on the task of attaching partof and causation relations, showing an improvement on F-score over a baseline model. 1 Introduction NLP researchers have developed many algorithms for mining knowledge from text and the Web, including facts (Etzioni et al. 2005), semantic lexicons (Riloff and Shepherd 1997), concept lists (Lin and Pantel 2002), and word similarity lists (Hindle 1990). Many recent efforts have also focused on extracting binary semantic relations between entities, such as entailments (Szpektor et al. 2004), is-a (Ravichandran and Hovy 2002), part-of (Girju et al. 2003), and other relations. The output of most of these systems is flat lists of lexical semantic knowledge such as “Italy is-a country” and “orange similar-to blue”. However, using this knowledge beyond simple keyword matching, for example in inferences, requires it to be linked into formal semantic repositories such as ontologies or term banks like WordNet"
P06-1100,P05-1016,1,0.673527,"s and 44th Annual Meeting of the ACL, pages 793–800, c Sydney, July 2006. 2006 Association for Computational Linguistics ceptual instances. In this paper, we develop a clustering algorithm for generalizing a set of relation instances to conceptual instances by looking up the WordNet hypernymy hierarchy for common ancestors, as specific as possible, that subsume as many instances as possible. An instance is then attached to its senses that are subsumed by the highest scoring conceptual instances. 2 Relevant Work Several researchers have worked on ontologizing semantic resources. Most recently, Pantel (2005) developed a method to propagate lexical cooccurrence vectors to WordNet synsets, forming ontological co-occurrence vectors. Adopting an extension of the distributional hypothesis (Harris 1985), the co-occurrence vectors are used to compute the similarity between synset/synset and between lexical term/synset. An unknown term is then attached to the WordNet synset whose cooccurrence vector is most similar to the term’s co-occurrence vector. Though the author suggests a method for attaching more complex lexical structures like binary semantic relations, the paper focused only on attaching terms."
P06-1100,P02-1006,0,0.0137035,"g) semantic relations into WordNet. We present an empirical evaluation on the task of attaching partof and causation relations, showing an improvement on F-score over a baseline model. 1 Introduction NLP researchers have developed many algorithms for mining knowledge from text and the Web, including facts (Etzioni et al. 2005), semantic lexicons (Riloff and Shepherd 1997), concept lists (Lin and Pantel 2002), and word similarity lists (Hindle 1990). Many recent efforts have also focused on extracting binary semantic relations between entities, such as entailments (Szpektor et al. 2004), is-a (Ravichandran and Hovy 2002), part-of (Girju et al. 2003), and other relations. The output of most of these systems is flat lists of lexical semantic knowledge such as “Italy is-a country” and “orange similar-to blue”. However, using this knowledge beyond simple keyword matching, for example in inferences, requires it to be linked into formal semantic repositories such as ontologies or term banks like WordNet (Fellbaum 1998). Pantel (2005) defined the task of ontologizing a lexical semantic resource as linking its terms to the concepts in a WordNet-like hierarchy. For example, “orange similar-to blue” ontologizes in Word"
P06-1100,W97-0313,0,0.137381,"a2.it Abstract Many algorithms have been developed to harvest lexical semantic resources, however few have linked the mined knowledge into formal knowledge repositories. In this paper, we propose two algorithms for automatically ontologizing (attaching) semantic relations into WordNet. We present an empirical evaluation on the task of attaching partof and causation relations, showing an improvement on F-score over a baseline model. 1 Introduction NLP researchers have developed many algorithms for mining knowledge from text and the Web, including facts (Etzioni et al. 2005), semantic lexicons (Riloff and Shepherd 1997), concept lists (Lin and Pantel 2002), and word similarity lists (Hindle 1990). Many recent efforts have also focused on extracting binary semantic relations between entities, such as entailments (Szpektor et al. 2004), is-a (Ravichandran and Hovy 2002), part-of (Girju et al. 2003), and other relations. The output of most of these systems is flat lists of lexical semantic knowledge such as “Italy is-a country” and “orange similar-to blue”. However, using this knowledge beyond simple keyword matching, for example in inferences, requires it to be linked into formal semantic repositories such as"
P06-1100,W04-3206,0,0.0232098,"ically ontologizing (attaching) semantic relations into WordNet. We present an empirical evaluation on the task of attaching partof and causation relations, showing an improvement on F-score over a baseline model. 1 Introduction NLP researchers have developed many algorithms for mining knowledge from text and the Web, including facts (Etzioni et al. 2005), semantic lexicons (Riloff and Shepherd 1997), concept lists (Lin and Pantel 2002), and word similarity lists (Hindle 1990). Many recent efforts have also focused on extracting binary semantic relations between entities, such as entailments (Szpektor et al. 2004), is-a (Ravichandran and Hovy 2002), part-of (Girju et al. 2003), and other relations. The output of most of these systems is flat lists of lexical semantic knowledge such as “Italy is-a country” and “orange similar-to blue”. However, using this knowledge beyond simple keyword matching, for example in inferences, requires it to be linked into formal semantic repositories such as ontologies or term banks like WordNet (Fellbaum 1998). Pantel (2005) defined the task of ontologizing a lexical semantic resource as linking its terms to the concepts in a WordNet-like hierarchy. For example, “orange s"
P06-1100,C96-1005,0,\N,Missing
P11-1009,P08-1003,0,0.0193845,"Missing"
P11-1009,N06-1052,0,0.0480897,"Missing"
P11-1009,D09-1098,1,\N,Missing
P11-1009,N09-1003,0,\N,Missing
P12-1059,C92-2082,0,0.0352979,"for Computational Linguistics • We propose an efficient learning technique and a robust implementation of our models, using real-world query data, and a realistic large set of entity types. • We empirically show that our models outperform the state of the art and that modeling latent intent contributes significantly to these results. 2 Related Work 2.1 Finding Semantic Classes A closely related problem is that of finding the semantic classes of entities. Automatic techniques for finding semantic classes include unsupervised clustering (Sch¨utze, 1998; Pantel and Lin, 2002), hyponym patterns (Hearst, 1992; Pantel et al., 2004; Kozareva et al., 2008), extraction patterns (Etzioni et al., 2005), hidden Markov models (Ritter et al., 2009), classification (Rahman and Ng, 2010) and many others. These techniques typically leverage large corpora, while projects such as WordNet (Miller et al., 1990) and Freebase (Bollacker et al., 2008) have employed editors to manually enumerate words and entities with their semantic classes. The aforementioned methods do not use query logs or explicitly determine the relative probabilities of different entity senses. A method might learn that there is independently"
P12-1059,P08-1119,0,0.0234511,"propose an efficient learning technique and a robust implementation of our models, using real-world query data, and a realistic large set of entity types. • We empirically show that our models outperform the state of the art and that modeling latent intent contributes significantly to these results. 2 Related Work 2.1 Finding Semantic Classes A closely related problem is that of finding the semantic classes of entities. Automatic techniques for finding semantic classes include unsupervised clustering (Sch¨utze, 1998; Pantel and Lin, 2002), hyponym patterns (Hearst, 1992; Pantel et al., 2004; Kozareva et al., 2008), extraction patterns (Etzioni et al., 2005), hidden Markov models (Ritter et al., 2009), classification (Rahman and Ng, 2010) and many others. These techniques typically leverage large corpora, while projects such as WordNet (Miller et al., 1990) and Freebase (Bollacker et al., 2008) have employed editors to manually enumerate words and entities with their semantic classes. The aforementioned methods do not use query logs or explicitly determine the relative probabilities of different entity senses. A method might learn that there is independently a high chance of eBay being a website and an"
P12-1059,C10-1105,0,0.0155835,"large set of entity types. • We empirically show that our models outperform the state of the art and that modeling latent intent contributes significantly to these results. 2 Related Work 2.1 Finding Semantic Classes A closely related problem is that of finding the semantic classes of entities. Automatic techniques for finding semantic classes include unsupervised clustering (Sch¨utze, 1998; Pantel and Lin, 2002), hyponym patterns (Hearst, 1992; Pantel et al., 2004; Kozareva et al., 2008), extraction patterns (Etzioni et al., 2005), hidden Markov models (Ritter et al., 2009), classification (Rahman and Ng, 2010) and many others. These techniques typically leverage large corpora, while projects such as WordNet (Miller et al., 1990) and Freebase (Bollacker et al., 2008) have employed editors to manually enumerate words and entities with their semantic classes. The aforementioned methods do not use query logs or explicitly determine the relative probabilities of different entity senses. A method might learn that there is independently a high chance of eBay being a website and an employer, but does not specify which usage is more common. This is especially problematic, for example, if one wishes to lever"
P12-1059,P11-1097,0,0.035427,"Missing"
P12-1059,J98-1004,0,0.0640862,"Missing"
P14-1143,N12-1092,0,0.0698924,"Missing"
P14-1143,W09-1119,0,0.0317926,"natural language processing task, called smart selection, which aims to address an important problem in text selection for touch-enabled devices; • We conduct a large crowd-sourced user study to collect a dataset of intended selections and simulated user selections, which we release to the academic community; • We propose a machine-learned ensemble model for smart selection, which combines various linguistic annotation methods with information from a large knowledge base and web graph; • We empirically show that our model can effectively address the smart selection task. Finkel et al., 2005; Ratinov and Roth, 2009). We found in our dataset (see Section 3.2) that only a quarter of what users intend to select consists in fact of named entities. Although an NER approach can be very useful, it is certainly not sufficient. The remainder of the data can be partially addressed with noun phrase (NP) detectors (Abney, 1991; Ramshaw and Marcus, 1995; Mu˜noz et al., 1999; Kudo and Matsumoto, 2001) and lists of items in a knowledge base (KB), but again, each is not alone sufficient. NP detectors and KB-based methods are further very susceptible to the generation of false positives (i.e., text contains many nested n"
P14-1143,W02-1001,0,0.256616,"subsumes the user selection is treated as a smart selection candidate. Scoring of candidates is by normalized length, under the assumption that in general the most specific (longest) unit is more likely to be the intended selection. 6 Note that this training set is generated automatically and is, by design, of a different nature than the manually labeled data we use to train and test the ensemble model. 1528 Our first unit spotter, labeled NER is geared towards recognizing named entities. We use a commercial and proprietary state-of-the-art NER system, trained using the perceptron algorithm (Collins, 2002) over more than a million hand-annotated labels. Our second approach uses purely syntactic information and treats noun phrases as units. We label this model as NP. For this purpose we parse the sentence containing the user selection with a syntactic parser following (Ratnaparkhi, 1999). We then treat every noun phrase that subsumes the user selection as a candidate smart selection. Finally, our third unit spotter, labeled KB, is based on the assumption that concepts and other entries in a knowledge base are, by nature, things that can be of interest to people. For our knowledge base lookup, we"
P14-1143,P05-1045,0,0.0407966,"• We introduce a new natural language processing task, called smart selection, which aims to address an important problem in text selection for touch-enabled devices; • We conduct a large crowd-sourced user study to collect a dataset of intended selections and simulated user selections, which we release to the academic community; • We propose a machine-learned ensemble model for smart selection, which combines various linguistic annotation methods with information from a large knowledge base and web graph; • We empirically show that our model can effectively address the smart selection task. Finkel et al., 2005; Ratinov and Roth, 2009). We found in our dataset (see Section 3.2) that only a quarter of what users intend to select consists in fact of named entities. Although an NER approach can be very useful, it is certainly not sufficient. The remainder of the data can be partially addressed with noun phrase (NP) detectors (Abney, 1991; Ramshaw and Marcus, 1995; Mu˜noz et al., 1999; Kudo and Matsumoto, 2001) and lists of items in a knowledge base (KB), but again, each is not alone sufficient. NP detectors and KB-based methods are further very susceptible to the generation of false positives (i.e., te"
P14-1143,N01-1025,0,0.105669,", which combines various linguistic annotation methods with information from a large knowledge base and web graph; • We empirically show that our model can effectively address the smart selection task. Finkel et al., 2005; Ratinov and Roth, 2009). We found in our dataset (see Section 3.2) that only a quarter of what users intend to select consists in fact of named entities. Although an NER approach can be very useful, it is certainly not sufficient. The remainder of the data can be partially addressed with noun phrase (NP) detectors (Abney, 1991; Ramshaw and Marcus, 1995; Mu˜noz et al., 1999; Kudo and Matsumoto, 2001) and lists of items in a knowledge base (KB), but again, each is not alone sufficient. NP detectors and KB-based methods are further very susceptible to the generation of false positives (i.e., text contains many nested noun phrases and knowledge base items include highly ambiguous terms). In our work, we leverage all three techniques in order to benefit from their complementary coverage of user selections. We further create a novel unit detector, called the hyperlink intent model. Based on the assumption that Wikipedia anchor texts are similar in nature to what users would select in a researc"
P14-1143,W99-0621,0,0.200999,"Missing"
P14-1143,W95-0107,0,0.297475,"hine-learned ensemble model for smart selection, which combines various linguistic annotation methods with information from a large knowledge base and web graph; • We empirically show that our model can effectively address the smart selection task. Finkel et al., 2005; Ratinov and Roth, 2009). We found in our dataset (see Section 3.2) that only a quarter of what users intend to select consists in fact of named entities. Although an NER approach can be very useful, it is certainly not sufficient. The remainder of the data can be partially addressed with noun phrase (NP) detectors (Abney, 1991; Ramshaw and Marcus, 1995; Mu˜noz et al., 1999; Kudo and Matsumoto, 2001) and lists of items in a knowledge base (KB), but again, each is not alone sufficient. NP detectors and KB-based methods are further very susceptible to the generation of false positives (i.e., text contains many nested noun phrases and knowledge base items include highly ambiguous terms). In our work, we leverage all three techniques in order to benefit from their complementary coverage of user selections. We further create a novel unit detector, called the hyperlink intent model. Based on the assumption that Wikipedia anchor texts are similar i"
W04-3205,P98-1013,0,0.0183966,"linked by the similarity relation will provide a way to automatically extend the existing verb classes to cover more of the English lexicon. 2.2 Existing resources Some existing broad-coverage resources on verbs have focused on organizing verbs into classes or annotating their frames or thematic roles. EVCA (English Verb Classes and Alternations) (Levin 1993) organizes verbs by similarity and participation / nonparticipation in alternation patterns. It contains 3200 verbs classified into 191 classes. Additional manually constructed resources include PropBank (Kingsbury et al. 2002), FrameNet (Baker et al. 1998), VerbNet (Kipper et al. 2000), and the resource on verb selectional restrictions developed by Gomez (2001). Our approach differs from the above in its focus. We relate verbs to each other rather than organize them into classes or identify their frames or thematic roles. WordNet does provide relations between verbs, but at a coarser level. We provide finer-grained relations such as strength, enablement and temporal information. Also, in contrast with WordNet, we cover more than the prescriptive cases. 2.3 Mining semantics from text Previous web mining work has rarely addressed extracting many"
W04-3205,N01-1012,0,0.0127364,"ore of the English lexicon. 2.2 Existing resources Some existing broad-coverage resources on verbs have focused on organizing verbs into classes or annotating their frames or thematic roles. EVCA (English Verb Classes and Alternations) (Levin 1993) organizes verbs by similarity and participation / nonparticipation in alternation patterns. It contains 3200 verbs classified into 191 classes. Additional manually constructed resources include PropBank (Kingsbury et al. 2002), FrameNet (Baker et al. 1998), VerbNet (Kipper et al. 2000), and the resource on verb selectional restrictions developed by Gomez (2001). Our approach differs from the above in its focus. We relate verbs to each other rather than organize them into classes or identify their frames or thematic roles. WordNet does provide relations between verbs, but at a coarser level. We provide finer-grained relations such as strength, enablement and temporal information. Also, in contrast with WordNet, we cover more than the prescriptive cases. 2.3 Mining semantics from text Previous web mining work has rarely addressed extracting many different semantic relations from Web-sized corpus. Most work on extracting semantic information from large"
W04-3205,C92-2082,0,0.346394,"ch (i) relates verbs to one another and (ii) provides broad coverage of the verbs in the target language. In this paper, we present an algorithm that semiautomatically discovers fine-grained verb semantics by querying the Web using simple lexicosyntactic patterns. The verb relations we discover are similarity, strength, antonymy, enablement, and temporal relations. Identifying these relations over 29,165 verb pairs results in a broad-coverage resource we call VERBOCEAN. Our approach extends previously formulated ones that use surface patterns as indicators of semantic relations between nouns (Hearst 1992; Etzioni 2003; Ravichandran and Hovy 2002). We extend these approaches in two ways: (i) our patterns indicate verb conjugation to increase their expressiveness and specificity and (ii) we use a measure similar to mutual information to account for both the frequency of the verbs whose semantic relations are being discovered as well as for the frequency of the pattern. 2 Relevant Work In this section, we describe application domains that can benefit from a resource of verb semantics. We then introduce some existing resources and describe previous attempts at mining semantics from text. 2.1 Appl"
W04-3205,J03-3005,0,0.128258,"Missing"
W04-3205,N04-1041,1,0.171704,"Missing"
W04-3205,P02-1006,0,0.388606,"another and (ii) provides broad coverage of the verbs in the target language. In this paper, we present an algorithm that semiautomatically discovers fine-grained verb semantics by querying the Web using simple lexicosyntactic patterns. The verb relations we discover are similarity, strength, antonymy, enablement, and temporal relations. Identifying these relations over 29,165 verb pairs results in a broad-coverage resource we call VERBOCEAN. Our approach extends previously formulated ones that use surface patterns as indicators of semantic relations between nouns (Hearst 1992; Etzioni 2003; Ravichandran and Hovy 2002). We extend these approaches in two ways: (i) our patterns indicate verb conjugation to increase their expressiveness and specificity and (ii) we use a measure similar to mutual information to account for both the frequency of the verbs whose semantic relations are being discovered as well as for the frequency of the pattern. 2 Relevant Work In this section, we describe application domains that can benefit from a resource of verb semantics. We then introduce some existing resources and describe previous attempts at mining semantics from text. 2.1 Applications Question answering is often approa"
W04-3205,P98-2180,0,0.0408247,"4) present a learning algorithm to detect 35 fine-grained noun phrase relations. Turney (2001) studied word relatedness and synonym extraction, while Lin et al. (2003) present an algorithm that queries the Web using lexical patterns for distinguishing noun synonymy and antonymy. Our approach addresses verbs and provides for a richer and finer-grained set of semantics. Reliability of estimating bigram counts on the web via search engines has been investigated by Keller and Lapata (2003). Semantic networks have also been extracted from dictionaries and other machine-readable resources. MindNet (Richardson et al. 1998) extracts a collection of triples of the type “ducks have wings” and “duck capable-of flying”. This resource, however, does not relate verbs to each other or provide verb semantics. 3 Semantic relations among verbs In this section, we introduce and motivate the specific relations that we extract. Whilst the natural language literature is rich in theories of semantics (Barwise and Perry 1985; Schank and Abelson 1977), large-coverage manually created semantic resources typically only organize verbs into a flat or shallow hierarchy of classes (such as those described in Section 2.2). WordNet iden"
W04-3205,W04-2609,0,\N,Missing
W04-3205,C98-2175,0,\N,Missing
W04-3205,C98-1013,0,\N,Missing
W06-1650,P05-1015,0,0.509608,"(e.g., product feature mentions), and meta-data (e.g., star rating). 2 Relevant Work The task of automatically assessing product review helpfulness is related to these broader areas 423 Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 423–430, c Sydney, July 2006. 2006 Association for Computational Linguistics of research: automatic analysis of product reviews, opinion and sentiment analysis, and text classification. In the thriving area of research on automatic analysis and processing of product reviews (Hu and Liu 2004; Turney 2002; Pang and Lee 2005), little attention has been paid to the important task studied here – assessing review helpfulness. Pang and Lee (2005) have studied prediction of product ratings, which may be particularly relevant due to the correlation we find between product rating and the helpfulness of the review (discussed in Section 5). However, a user’s overall rating for the product is often already available. Helpfulness, on the other hand, is valuable to assess because it is not explicitly known in current approaches until many users vote on the helpfulness of a review. In opinion and sentiment analysis, the focus"
W06-1650,W03-1014,0,0.0847164,"Missing"
W06-1650,W03-0404,0,0.0262738,"Missing"
W06-1650,W03-0210,0,0.00323758,"se it is not explicitly known in current approaches until many users vote on the helpfulness of a review. In opinion and sentiment analysis, the focus is on distinguishing between statements of fact vs. opinion, and on detecting the polarity of sentiments being expressed. Many researchers have worked in various facets of opinion analysis. Pang et al. (2002) and Turney (2002) classified sentiment polarity of reviews at the document level. Wiebe et al. (1999) classified sentence level subjectivity using syntactic classes such as adjectives, pronouns and modal verbs as features. Riloff and Wiebe (2003) extracted subjective expressions from sentences using a bootstrapping pattern learning process. Yu and Hatzivassiloglou (2003) identified the polarity of opinion sentences using semantically oriented words. These techniques were applied and examined in different domains, such as customer reviews (Hu and Liu 2004) and news articles (TREC novelty track 2003 and 2004). In text classification, systems typically use bag-of-words models, although there is some evidence of benefits when introducing relevant semantic knowledge (Gabrilovich and Markovitch, 2005). In this paper, we explore the use of s"
W06-1650,P02-1053,0,0.0274355,"ns), semantic (e.g., product feature mentions), and meta-data (e.g., star rating). 2 Relevant Work The task of automatically assessing product review helpfulness is related to these broader areas 423 Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 423–430, c Sydney, July 2006. 2006 Association for Computational Linguistics of research: automatic analysis of product reviews, opinion and sentiment analysis, and text classification. In the thriving area of research on automatic analysis and processing of product reviews (Hu and Liu 2004; Turney 2002; Pang and Lee 2005), little attention has been paid to the important task studied here – assessing review helpfulness. Pang and Lee (2005) have studied prediction of product ratings, which may be particularly relevant due to the correlation we find between product rating and the helpfulness of the review (discussed in Section 5). However, a user’s overall rating for the product is often already available. Helpfulness, on the other hand, is valuable to assess because it is not explicitly known in current approaches until many users vote on the helpfulness of a review. In opinion and sentiment"
W06-1650,P99-1032,0,0.13748,"Missing"
W06-1650,W03-1017,0,0.0361299,"w. In opinion and sentiment analysis, the focus is on distinguishing between statements of fact vs. opinion, and on detecting the polarity of sentiments being expressed. Many researchers have worked in various facets of opinion analysis. Pang et al. (2002) and Turney (2002) classified sentiment polarity of reviews at the document level. Wiebe et al. (1999) classified sentence level subjectivity using syntactic classes such as adjectives, pronouns and modal verbs as features. Riloff and Wiebe (2003) extracted subjective expressions from sentences using a bootstrapping pattern learning process. Yu and Hatzivassiloglou (2003) identified the polarity of opinion sentences using semantically oriented words. These techniques were applied and examined in different domains, such as customer reviews (Hu and Liu 2004) and news articles (TREC novelty track 2003 and 2004). In text classification, systems typically use bag-of-words models, although there is some evidence of benefits when introducing relevant semantic knowledge (Gabrilovich and Markovitch, 2005). In this paper, we explore the use of some semantic features for review helpfulness ranking. Another potential relevant classification task is academic and commercial"
W06-1650,C04-1200,1,\N,Missing
W06-1650,W02-1011,0,\N,Missing
W06-3909,P99-1008,0,0.280639,"and linguistic analysis, exploiting the seeds as much as possible. 2. Relevant Work To date, most research on lexical relation harvesting has focused on is-a and part-of relations. Approaches fall into two main categories: pattern- and clustering-based. Most common are pattern-based approaches. Hearst [12] pioneered using patterns to extract hyponym (is-a) relations. Manually building three lexico-syntactic patterns, Hearst sketched a bootstrapping algorithm to learn more patterns from instances, which has served as the model for most subsequent pattern-based algorithms. Berland and Charniak [1] propose a system for part-of relation extraction, based on the Hearst approach [12]. Seed instances are used to infer linguistic patterns that, in turn, are used to extract new instances, ranked according to various statistical measures. While this study introduces statistical measures to evaluate instance reliability, it remains vulnerable to data sparseness and has the limitation of taking into consideration only one-word terms. Improving upon Berland and Charniak [1], Girju et al. [11] employ machine learning algorithms and WordNet [8] to disambiguate part-of generic patterns, like [whole-"
W06-3909,P99-1016,0,0.0366489,"s-a relations involving proper nouns, and Downey et al. [6] who formalized the problem of relation extraction in a coherent and effective combinatorial model that is shown to outperform previous probabilistic frameworks. Clustering approaches to relation extraction are less common and have insofar been applied only to is-a extraction. These methods employ clustering algorithms to group words according to their meanings in text, label the clusters using its members’ lexical or syntactic dependencies, and then extract an is-a relation between each cluster member and the cluster label. Caraballo [3] proposed the first attempt, which used conjunction and apposition features to build noun clusters. Recently, Pantel and Ravichandran [16] extended this approach by making use of all syntactic dependency features for each noun. The advantage of clustering approaches is that they permit algorithms to identify is-a relations that do not explicitly appear in text, however they generally fail to produce coherent clusters from fewer than 100 million words; hence they are unreliable for small corpora. 3. The Espresso Algorithm The Espresso algorithm is based on a similar framework to the one adopted"
W06-3909,A97-1051,0,0.0396063,"so with two state of the art systems on the task of extracting various semantic relations. 4.1.1. Datasets We perform our experiments using the following two datasets:  TREC-9: This dataset consists of a sample of articles from the Aquaint (TREC-9) newswire text collection. The sample consists of 5,951,432 words extracted from the following data files: AP890101 – AP890131, AP890201 – AP890228, and AP890310 – AP890319.  CHEM: This small dataset of 313,590 words consists of a college level textbook of introductory chemistry [2]. We preprocess the corpora using the Alembic Workbench POS-tagger [5]. 4.1.2. Systems We compare the results of Espresso with the following two state of the art extraction systems:  RH02: This algorithm by Ravichandran and Hovy [20] learns lexical extraction patterns from a set of seed instances of a particular relation (see Section 2.)  PR04: This is-a extraction algorithm from Pantel and Ravichandran [16] first automatically induces concepts (clusters) from a raw corpus, names the concepts, and then extracts an is-a relation between each cluster member and its cluster label. For each cluster member, the system may generate multiple possible is-a relations,"
W06-3909,P03-1001,0,0.0211095,"a similar, highly scalable approach, based on an edit-distance technique, to learn lexico-POS patterns, showing both good performances and efficiency. Espresso uses a similar approach to infer patterns, but we then apply refining techniques to deal with various types of relations. Other pattern-based algorithms include Riloff and Shepherd [21], who used a semi-automatic method for discovering similar words using a few seed examples by using pattern-based techniques and human supervision, KnowItAll [7] that performs large-scale extraction of facts from the Web, Mann [15] and Fleischman et al. [9] who used part of speech patterns to extract a subset of is-a relations involving proper nouns, and Downey et al. [6] who formalized the problem of relation extraction in a coherent and effective combinatorial model that is shown to outperform previous probabilistic frameworks. Clustering approaches to relation extraction are less common and have insofar been applied only to is-a extraction. These methods employ clustering algorithms to group words according to their meanings in text, label the clusters using its members’ lexical or syntactic dependencies, and then extract an is-a relation bet"
W06-3909,P05-1014,0,0.0185685,"ervised iterative algorithm combined with a web-based knowledge expansion technique, for extracting binary semantic relations. Given a small set of seed instances for a particular relation, the system learns lexical patterns, applies them to extract new instances, and then uses the Web to filter and expand the instances. Preliminary experiments show that Espresso extracts highly precise lists of a wide variety of semantic relations when compared with two state of the art systems. 1. Introduction Recent attention to knowledge-rich problems such as question answering [18] and textual entailment [10] has encouraged Natural Language Processing (NLP) researchers to develop algorithms for automatically harvesting shallow semantic resources. With seemingly endless amounts of textual data at our disposal, we have a tremendous opportunity to automatically grow semantic term banks and ontological resources. Methods must be accurate, adaptable and scalable to the varying sizes of domain corpora (e.g., textbooks vs. World Wide Web), and independent or weakly dependent on human supervision. In this paper we present Espresso, a novel bootstrapping algorithm for automatically harvesting semantic rela"
W06-3909,N03-1011,0,0.253804,"nstances, which has served as the model for most subsequent pattern-based algorithms. Berland and Charniak [1] propose a system for part-of relation extraction, based on the Hearst approach [12]. Seed instances are used to infer linguistic patterns that, in turn, are used to extract new instances, ranked according to various statistical measures. While this study introduces statistical measures to evaluate instance reliability, it remains vulnerable to data sparseness and has the limitation of taking into consideration only one-word terms. Improving upon Berland and Charniak [1], Girju et al. [11] employ machine learning algorithms and WordNet [8] to disambiguate part-of generic patterns, like [whole-NP’s partNP]. This study is the first extensive attempt to solve the problem of generic relational patterns, that is, those expressive patterns that have high recall while suffering low precision, as they subsume a large set of instances. In order to discard incorrect instances, Girju et al. learn WordNet-based selectional restrictions, like [whole-NP(scene#4)’s part-NP(movie#1)]. While making huge grounds on improving precision/recall, the system requires heavy supervision through manual"
W06-3909,C92-2082,0,0.544263,"is-a relation.) In our experience, a handful of seed instances, in general, is sufficient for large corpora while for smaller corpora, a slightly larger set is required. To guarantee weakest supervision, Espresso combines its bootstrapping approach with a web-based knowledge expansion technique and linguistic analysis, exploiting the seeds as much as possible. 2. Relevant Work To date, most research on lexical relation harvesting has focused on is-a and part-of relations. Approaches fall into two main categories: pattern- and clustering-based. Most common are pattern-based approaches. Hearst [12] pioneered using patterns to extract hyponym (is-a) relations. Manually building three lexico-syntactic patterns, Hearst sketched a bootstrapping algorithm to learn more patterns from instances, which has served as the model for most subsequent pattern-based algorithms. Berland and Charniak [1] propose a system for part-of relation extraction, based on the Hearst approach [12]. Seed instances are used to infer linguistic patterns that, in turn, are used to extract new instances, ranked according to various statistical measures. While this study introduces statistical measures to evaluate insta"
W06-3909,C94-1079,0,0.0584074,"ndran and Hovy [20] learns lexical extraction patterns from a set of seed instances of a particular relation (see Section 2.)  PR04: This is-a extraction algorithm from Pantel and Ravichandran [16] first automatically induces concepts (clusters) from a raw corpus, names the concepts, and then extracts an is-a relation between each cluster member and its cluster label. For each cluster member, the system may generate multiple possible is-a relations, but in this evaluation we only keep the highest scoring one. To apply this algorithm, both datasets were first analyzed using the Minipar parser [14].  ESP: This is the algorithm described in this paper (details in Section 3). 4.1.3. Semantic Relations Espresso is designed to extract various semantic relations exemplified by a given small set of seed instances. For our preliminary evaluation, we consider the standard is-a and part-of relations as well as three novel relations:  succession: This relation indicates that one proper noun succeeds another in a position or title. For example, George Bush succeeded Bill Clinton and Pope Benedict XVI succeeded Pope John Paul II. We evaluate this relation on the TREC-9 corpus.  reaction: This re"
W06-3909,W02-1111,0,0.0156431,"Pantel et al. [17] proposed a similar, highly scalable approach, based on an edit-distance technique, to learn lexico-POS patterns, showing both good performances and efficiency. Espresso uses a similar approach to infer patterns, but we then apply refining techniques to deal with various types of relations. Other pattern-based algorithms include Riloff and Shepherd [21], who used a semi-automatic method for discovering similar words using a few seed examples by using pattern-based techniques and human supervision, KnowItAll [7] that performs large-scale extraction of facts from the Web, Mann [15] and Fleischman et al. [9] who used part of speech patterns to extract a subset of is-a relations involving proper nouns, and Downey et al. [6] who formalized the problem of relation extraction in a coherent and effective combinatorial model that is shown to outperform previous probabilistic frameworks. Clustering approaches to relation extraction are less common and have insofar been applied only to is-a extraction. These methods employ clustering algorithms to group words according to their meanings in text, label the clusters using its members’ lexical or syntactic dependencies, and then ex"
W06-3909,N04-1041,1,0.859175,"combinatorial model that is shown to outperform previous probabilistic frameworks. Clustering approaches to relation extraction are less common and have insofar been applied only to is-a extraction. These methods employ clustering algorithms to group words according to their meanings in text, label the clusters using its members’ lexical or syntactic dependencies, and then extract an is-a relation between each cluster member and the cluster label. Caraballo [3] proposed the first attempt, which used conjunction and apposition features to build noun clusters. Recently, Pantel and Ravichandran [16] extended this approach by making use of all syntactic dependency features for each noun. The advantage of clustering approaches is that they permit algorithms to identify is-a relations that do not explicitly appear in text, however they generally fail to produce coherent clusters from fewer than 100 million words; hence they are unreliable for small corpora. 3. The Espresso Algorithm The Espresso algorithm is based on a similar framework to the one adopted in [12]. For a specific semantic binary relation (e.g., is-a), the algorithm requires as input a small set of seed instances Is and a cor"
W06-3909,P02-1006,0,0.832099,"Net [8] to disambiguate part-of generic patterns, like [whole-NP’s partNP]. This study is the first extensive attempt to solve the problem of generic relational patterns, that is, those expressive patterns that have high recall while suffering low precision, as they subsume a large set of instances. In order to discard incorrect instances, Girju et al. learn WordNet-based selectional restrictions, like [whole-NP(scene#4)’s part-NP(movie#1)]. While making huge grounds on improving precision/recall, the system requires heavy supervision through manual semantic annotations. Ravichandran and Hovy [20] focus on efficiency issues for scaling relation extraction to terabytes of data. A simple and effective algorithm is proposed to infer surface patterns from a small set of instance seeds by extracting all substrings relating seeds in corpus sentences. The frequencies of the substrings in the corpus are then used to retain the best patterns. The approach gives good results on specific relations such as birthdates, however it has low precision on generic ones like is-a and part-of. Pantel et al. [17] proposed a similar, highly scalable approach, based on an edit-distance technique, to learn lex"
W06-3909,W97-0313,0,0.0154589,"e frequencies of the substrings in the corpus are then used to retain the best patterns. The approach gives good results on specific relations such as birthdates, however it has low precision on generic ones like is-a and part-of. Pantel et al. [17] proposed a similar, highly scalable approach, based on an edit-distance technique, to learn lexico-POS patterns, showing both good performances and efficiency. Espresso uses a similar approach to infer patterns, but we then apply refining techniques to deal with various types of relations. Other pattern-based algorithms include Riloff and Shepherd [21], who used a semi-automatic method for discovering similar words using a few seed examples by using pattern-based techniques and human supervision, KnowItAll [7] that performs large-scale extraction of facts from the Web, Mann [15] and Fleischman et al. [9] who used part of speech patterns to extract a subset of is-a relations involving proper nouns, and Downey et al. [6] who formalized the problem of relation extraction in a coherent and effective combinatorial model that is shown to outperform previous probabilistic frameworks. Clustering approaches to relation extraction are less common and"
W11-0319,W99-0613,0,0.265572,"rvesting generalized names (e.g., diseases, locations), where labeled negative examples for a given class are taken from positive seed examples of ‘competing’ classes (e.g. examples of diseases are used as negatives for locations). The approach is semi-supervised, in that it requires some manually annotated seeds. The study shows that using competing categories improves the accuracy of the system, by avoiding sematic drift, which is a common cause of divergence in boostrapping approaches. Similar approaches are used among others in (Thelen and Riloff, 2002) for learning semantic lexicons, in (Collins and Singer, 1999) for namedentity recognition, and in (Fagni and Sebastiani, 2007) for hierarchical text categorization. Some of 170 our methods rely on the same intuition described above, i.e., using instances of other classes as negative training examples. Yet, the ES framework allows us to add further restrictions to improve the quality of the data. 6 Conclusion We presented simple and general techniques for automatically acquiring training data, and then tested them in the context of the Ensemble Semantics framework. Experimental results show that our methods can compete with supervised systems using manua"
W11-0319,C92-2082,0,0.0287263,"are used to increase the labeled set of the other. These two phases are repeated until a stop condition is met. Co-training has been successfully applied to various applications, such as statistical parsing (Sarkar, 2001) and web pages classification (Yarowsky, 1998). Selftraining techniques (or bootsrapping) (Yarowsky, 1995) start with a small set of labeled data, and iteratively classify unlabeled data, selecting the most confident predictions as additional training. Selftraining has been applied in many NLP tasks, such as word sense disambiguation (Yarowsky, 1995) and relation extraction (Hearst, 1992). Unlike typical semi-supervised approaches, our approach reduces the needed amount of labeled data not by acting on the learning algorithm itself (any algorithm can be used in our approach), but on the method to acquire the labeled training data. Our work also relates to the automatic acquisition of labeled negative training data. Yangarber et al. (2002) propose a pattern-based bootstrapping approach for harvesting generalized names (e.g., diseases, locations), where labeled negative examples for a given class are taken from positive seed examples of ‘competing’ classes (e.g. examples of dise"
W11-0319,D10-1035,0,0.0218961,"Missing"
W11-0319,P06-2075,0,0.128846,"sts of actors, musicians, cities). Search engines such as Bing, Yahoo, and Google collect large sets of entities to better interpret queries (Tan and Peng, 2006), to improve query suggestions (Cao et al., 2008) and to understand query intents (Hu et al., 2009). In response, automated techniques for entity extraction have been proposed (Pas¸ca, 2007; Wang and Cohen, 2008; Chaudhuri et al., 2009; Pantel et al., 2009). There is mounting evidence that combining knowledge sources and information extraction systems yield significant improvements over applying each in isolation (Pas¸ca et al., 2006; Mirkin et al., 2006). This intuition is explored by the Ensemble Semantics (ES) framework proposed by Pennacchiotti and Pantel (2009), which outperforms previous state-of-the-art systems. A severe limitation of this type of extraction system is its reliance on In this paper, we propose simple and intuitively appealing solutions to automatically build training sets. Positive and negative training sets for a target semantic class are acquired by leveraging: i) ‘trusted’ sources such as structured databases (e.g., IMDB or Wikipedia for acquiring a list of Actors); ii) automatically constructed semantic lexicons; and"
W11-0319,D09-1025,1,0.931371,"f entities to better interpret queries (Tan and Peng, 2006), to improve query suggestions (Cao et al., 2008) and to understand query intents (Hu et al., 2009). In response, automated techniques for entity extraction have been proposed (Pas¸ca, 2007; Wang and Cohen, 2008; Chaudhuri et al., 2009; Pantel et al., 2009). There is mounting evidence that combining knowledge sources and information extraction systems yield significant improvements over applying each in isolation (Pas¸ca et al., 2006; Mirkin et al., 2006). This intuition is explored by the Ensemble Semantics (ES) framework proposed by Pennacchiotti and Pantel (2009), which outperforms previous state-of-the-art systems. A severe limitation of this type of extraction system is its reliance on In this paper, we propose simple and intuitively appealing solutions to automatically build training sets. Positive and negative training sets for a target semantic class are acquired by leveraging: i) ‘trusted’ sources such as structured databases (e.g., IMDB or Wikipedia for acquiring a list of Actors); ii) automatically constructed semantic lexicons; and iii) instances of semantic classes other than the target class. Our models focus on extracting training sets tha"
W11-0319,N01-1023,0,0.0383528,"rning system. Most common techniques are based on co-training and self-training. Co-training uses a small set of labeled examples to train two classifiers at the same time. The classifiers use independent views (i.e. ‘conditionally independent’ feature sets) to represent the labeled examples. After the learning phase, the most confident predictions of each classifier on the unlabeled data are used to increase the labeled set of the other. These two phases are repeated until a stop condition is met. Co-training has been successfully applied to various applications, such as statistical parsing (Sarkar, 2001) and web pages classification (Yarowsky, 1998). Selftraining techniques (or bootsrapping) (Yarowsky, 1995) start with a small set of labeled data, and iteratively classify unlabeled data, selecting the most confident predictions as additional training. Selftraining has been applied in many NLP tasks, such as word sense disambiguation (Yarowsky, 1995) and relation extraction (Hearst, 1992). Unlike typical semi-supervised approaches, our approach reduces the needed amount of labeled data not by acting on the learning algorithm itself (any algorithm can be used in our approach), but on the method"
W11-0319,W02-1028,0,0.195477,"(2002) propose a pattern-based bootstrapping approach for harvesting generalized names (e.g., diseases, locations), where labeled negative examples for a given class are taken from positive seed examples of ‘competing’ classes (e.g. examples of diseases are used as negatives for locations). The approach is semi-supervised, in that it requires some manually annotated seeds. The study shows that using competing categories improves the accuracy of the system, by avoiding sematic drift, which is a common cause of divergence in boostrapping approaches. Similar approaches are used among others in (Thelen and Riloff, 2002) for learning semantic lexicons, in (Collins and Singer, 1999) for namedentity recognition, and in (Fagni and Sebastiani, 2007) for hierarchical text categorization. Some of 170 our methods rely on the same intuition described above, i.e., using instances of other classes as negative training examples. Yet, the ES framework allows us to add further restrictions to improve the quality of the data. 6 Conclusion We presented simple and general techniques for automatically acquiring training data, and then tested them in the context of the Ensemble Semantics framework. Experimental results show th"
W11-0319,C02-1154,0,0.368831,"set of labeled data, and iteratively classify unlabeled data, selecting the most confident predictions as additional training. Selftraining has been applied in many NLP tasks, such as word sense disambiguation (Yarowsky, 1995) and relation extraction (Hearst, 1992). Unlike typical semi-supervised approaches, our approach reduces the needed amount of labeled data not by acting on the learning algorithm itself (any algorithm can be used in our approach), but on the method to acquire the labeled training data. Our work also relates to the automatic acquisition of labeled negative training data. Yangarber et al. (2002) propose a pattern-based bootstrapping approach for harvesting generalized names (e.g., diseases, locations), where labeled negative examples for a given class are taken from positive seed examples of ‘competing’ classes (e.g. examples of diseases are used as negatives for locations). The approach is semi-supervised, in that it requires some manually annotated seeds. The study shows that using competing categories improves the accuracy of the system, by avoiding sematic drift, which is a common cause of divergence in boostrapping approaches. Similar approaches are used among others in (Thelen"
W11-0319,P95-1026,0,0.0704772,"l set of labeled examples to train two classifiers at the same time. The classifiers use independent views (i.e. ‘conditionally independent’ feature sets) to represent the labeled examples. After the learning phase, the most confident predictions of each classifier on the unlabeled data are used to increase the labeled set of the other. These two phases are repeated until a stop condition is met. Co-training has been successfully applied to various applications, such as statistical parsing (Sarkar, 2001) and web pages classification (Yarowsky, 1998). Selftraining techniques (or bootsrapping) (Yarowsky, 1995) start with a small set of labeled data, and iteratively classify unlabeled data, selecting the most confident predictions as additional training. Selftraining has been applied in many NLP tasks, such as word sense disambiguation (Yarowsky, 1995) and relation extraction (Hearst, 1992). Unlike typical semi-supervised approaches, our approach reduces the needed amount of labeled data not by acting on the learning algorithm itself (any algorithm can be used in our approach), but on the method to acquire the labeled training data. Our work also relates to the automatic acquisition of labeled negat"
W11-0319,D09-1098,1,\N,Missing
