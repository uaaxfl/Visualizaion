2021.acl-short.121,P16-1154,0,0.0863315,"Missing"
2021.acl-short.121,D17-1084,0,0.0330225,"Missing"
2021.acl-short.121,Q15-1042,0,0.0631939,"Missing"
2021.acl-short.121,P14-1026,0,0.080211,"Missing"
2021.rocling-1.5,N19-1423,0,0.0523954,"Missing"
2021.rocling-1.5,D19-1170,0,0.0393739,"Missing"
2021.rocling-1.5,D19-1630,0,0.0309004,"Missing"
2021.rocling-1.5,O11-1010,0,0.0572486,"Missing"
2021.rocling-1.5,P19-1334,0,0.0473568,"Missing"
2021.rocling-1.5,W13-3819,0,0.0645537,"Missing"
2021.rocling-1.5,D19-1258,0,0.0248831,"Missing"
2021.rocling-1.5,S18-2023,0,0.0652759,"Missing"
2021.rocling-1.5,D16-1264,0,0.116042,"Missing"
C16-1035,P07-1056,0,0.0477674,"7 0.819 0.852 0.835 0.833 0.852 0.862 Kitchen 0.824 0.858 0.860 0.857 0.884 0.860 0.871 0.884 0.890 Average 0.790 0.826 0.824 0.813 0.842 0.826 0.831 0.842 0.853 Table 1: Experimental results on sentiment analysis achieved by the proposed EV model and other baseline features, including unigrams, bigrams, PCA, and the combinations. 4 4.1 Experimental Setup & Results Experiments on the EV Model for Sentiment Analysis At the outset, we evaluate the proposed EV model on the sentiment polarity classification task. Four widely-used benchmark multi-domain sentiment datasets are used in this study 1 (Blitzer et al., 2007). They are product reviews taken from Amazon.com in four different domains: Books, DVD, Electronics, and Kitchen. Each of the reviews, ranging from Star-1 to Star-5, were rated by a customer. The reviews with Star-1 and Star-2 were labelled as Negative, and those with Star-4 and Star-5 were labeled as Positive. Each of the four datasets contains 1,000 positive reviews, 1,000 negative reviews, and a number of unlabeled reviews. Labeled reviews in each domain are randomly split up into ten folds (with nine folds serving as the training set and the remaining one as the test set). All of the follo"
C16-1035,P15-2136,0,0.026887,"r, we leverage a density peaks clustering summarization method (Rodriguez and Laio, 2014; Zhang et al., 2015), which can take both relevance and redundancy information into account at the same time. That is, a concise summary for a given document set can be automatically generated through a one-pass process instead of an iterative process. Recently, the summarization method has proven its empirical effectiveness (Zhang et al., 2015). For evaluation, we adopt the widely-used automatic evaluation metric ROUGE (Lin, 2003), and take ROUGE-1 and ROUGE-2 (in F-scores) as the main measures following Cao et al., (2015). We compare the proposed EV model with two baseline systems (the vector space model (VSM) (Gong and Liu, 2001) and the LexRank (Erkan and Radev, 2004) method), the best peer systems (including Peer T, Peer 26, and Peer 65) participating DUC evaluations, and the recently elaborated DNN-based systems (including CNN and PriorSum) (Cao et al., 2015). Owing to the space limitation, we omit the detailed introduction to these summarization methods; interested readers may refer to Penn and Zhu (2008), Liu and Hakkani-Tur (2011), Nenkova and McKeown (2011), and Cao et al., (2015) for more in-depth ela"
C16-1035,W14-1504,0,0.0251256,"ph (or sentence and document) by simply taking an average over the word embeddings corresponding to the words occurring in the paragraph. By doing so, this thread of methods has recently This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http://creativecommons.org/licenses/by/4.0/ 358 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 358–368, Osaka, Japan, December 11-17 2016. enjoyed substantial success in many NLP-related tasks (Collobert and Weston, 2008; Tang et al., 2014; Kageback et al., 2014). Although the empirical effectiveness of word embedding methods has been proven recently, the composite representation for a paragraph (or sentence and document) is a bit queer. Theoretically, paragraph-based representation learning is expected to be more suitable for such tasks as information retrieval, sentiment analysis and document summarization (Huang et al., 2013; Le and Mikolov, 2014; Palangi et al., 2015), to name but a few. However, to the best of our knowledge, unsupervised paragraph embedding has been largely under-explored on these tasks. Classic paragraph embedding methods infer"
C16-1035,N10-1134,0,0.0322079,"rpus/MATBN-corpus.htm https://catalog.ldc.upenn.edu/LDC2011T13 365 deficiency of the EV model in spoken document summarization; we thus believe that it is more suitable for use in spoken content processing. In the last set of experiments, we compare the results mentioned above with that of several wellpracticed, state-of-the-art unsupervised summarization methods, including the graph-based methods (i.e., the Markov random walk (MRW) method (Wan and Yang, 2008) and the LexRank method (Erkan and Radev, 2004)) and the combinatorial optimization methods (i.e., the submodularity-based (SM) method (Lin and Bilmes, 2010) and the integer linear programming (ILP) method (Riedhammer et al., 2010)). Among them, the ability of reducing redundant information has been aptly incorporated into the submodular-based method and the ILP method. Interested readers may refer to Penn and Zhu (2008), Liu and Hakkani-Tur (2011), and Nenkova and McKeown (2011) for comprehensive reviews and new insights into the major methods that have been developed and applied with good success to a wide range of spoken document summarization tasks. The results are also listed in Table 3. Several noteworthy observations can be drawn from the r"
C16-1035,P08-1054,0,0.142534,"on metric ROUGE (Lin, 2003), and take ROUGE-1 and ROUGE-2 (in F-scores) as the main measures following Cao et al., (2015). We compare the proposed EV model with two baseline systems (the vector space model (VSM) (Gong and Liu, 2001) and the LexRank (Erkan and Radev, 2004) method), the best peer systems (including Peer T, Peer 26, and Peer 65) participating DUC evaluations, and the recently elaborated DNN-based systems (including CNN and PriorSum) (Cao et al., 2015). Owing to the space limitation, we omit the detailed introduction to these summarization methods; interested readers may refer to Penn and Zhu (2008), Liu and Hakkani-Tur (2011), Nenkova and McKeown (2011), and Cao et al., (2015) for more in-depth elaboration. It is worthy to note that the proposed EV model, the two baseline systems, and the best peer systems are unsupervised methods, while the DNN-based systems are supervised ones. The experimental results are listed in Table 2. Several interesting observations can be concluded from the results. First, the proposed EV model outperforms VSM by a large margin in all cases, and performs comparably to other well-designed unsupervised summarization methods. Second, both LexRank and EV (with th"
C16-1035,P14-1146,0,0.0395017,"ent a given paragraph (or sentence and document) by simply taking an average over the word embeddings corresponding to the words occurring in the paragraph. By doing so, this thread of methods has recently This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http://creativecommons.org/licenses/by/4.0/ 358 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 358–368, Osaka, Japan, December 11-17 2016. enjoyed substantial success in many NLP-related tasks (Collobert and Weston, 2008; Tang et al., 2014; Kageback et al., 2014). Although the empirical effectiveness of word embedding methods has been proven recently, the composite representation for a paragraph (or sentence and document) is a bit queer. Theoretically, paragraph-based representation learning is expected to be more suitable for such tasks as information retrieval, sentiment analysis and document summarization (Huang et al., 2013; Le and Mikolov, 2014; Palangi et al., 2015), to name but a few. However, to the best of our knowledge, unsupervised paragraph embedding has been largely under-explored on these tasks. Classic paragraph"
C16-1035,O05-3004,1,0.749163,"Missing"
C16-1035,N15-1136,0,0.0213232,"and were grouped into various thematic clusters. The summary length was limited to 100 words for both DUC 2001 and DUC 2002, and 665 bytes for DUC 2004. The general background information was inferred from the LDC Gigaword corpus 3 (including Associated Press Worldstream (AP), New York Times Newswire Service (NYT), and Xinhua News Agency (XIN)). The most common belief in the document summarization community is that relevance and redundancy are two key factors for generating a concise summary. In this paper, we leverage a density peaks clustering summarization method (Rodriguez and Laio, 2014; Zhang et al., 2015), which can take both relevance and redundancy information into account at the same time. That is, a concise summary for a given document set can be automatically generated through a one-pass process instead of an iterative process. Recently, the summarization method has proven its empirical effectiveness (Zhang et al., 2015). For evaluation, we adopt the widely-used automatic evaluation metric ROUGE (Lin, 2003), and take ROUGE-1 and ROUGE-2 (in F-scores) as the main measures following Cao et al., (2015). We compare the proposed EV model with two baseline systems (the vector space model (VSM)"
D14-1156,H91-1057,0,0.152474,"Missing"
D14-1156,P07-1085,0,0.0519061,"Missing"
D14-1156,P11-5003,0,0.0325849,"aneous (or in-domain) corpus. Finally, the enhanced query model (that is P(w|H) in speech recognition) can be estimated by RM, SMM, RSMM or QMM, and further combined with the background n-gram (e.g., trigram) language model to form an adaptive language model to guide the speech recognition process. 4.2 Speech Summarization On the other hand, extractive speech summarization aims at producing a concise summary by selecting salient sentences or paragraphs from the original spoken document according to a predefined target summarization ratio (Carbonell and Goldstein, 1998; Mani and Maybury, 1999; Nenkova and McKeown, 2011; Liu and Hakkani-Tur, 2011). Intuitively, this task could be framed as an ad-hoc IR problem, where the spoken document is treated as an information need and each sentence of the document is regarded as a candidate information unit to be retrieved according to its relevance to the information need. Therefore, KLM can be used to quantify how close the document D and one of its sentences S are: the closer the sentence model P(w|S) to the document model P(w|D), the more PRM ( w |Q )   D r DTop P( w |Dr )  DrDTop P(Q |Dr ) P( Dr )   L    P ( w |D r ) P ( Dr |Q )    wV  Dr D"
D14-1156,W01-0100,0,\N,Missing
H01-1050,1998.amta-tutorials.5,0,\N,Missing
H01-1050,W98-1005,0,\N,Missing
H01-1050,J95-4004,0,\N,Missing
H01-1050,A97-1029,0,\N,Missing
I13-1158,P11-1015,0,0.0545216,"d   P   Pw | nw,d d , (7) wd where Pw |     exp  T w  bw , T  exp  w'  bw' w'V   wd (9) The integral over  d in Eq. (9) is intractable. To simplify the estimation, we assume that the posterior distribution is highly peaked around the MAP estimate of  d . By adding regularization terms for R and  d and taking the logarithm, the parameters of LBDM are approximately estimated by Rˆ , bˆ  arg max    nw, d log P w |ˆd ; R, b d  D wd  R, b 2 2   R   ˆd ,  Log-Bilinear Document Modeling Log-Bilinear document modeling (LBDM) (Maas and Ng, 2010; Maas et al., 2011) can be considered as a relaxed version of LTM. LBDM attempts to learn the word representation with a semantic space and use training documents to constrain those semantically similar words to be represented in near vicinity. The major difference of LBDM and LTM is that LBDM aims to directly parameterize the model for capturing word representations, while LTM focuses on estimating a set of latent topics (Maas and Ng, 2010). d D  (10) where ˆd denotes the MAP estimate of  d for each document d  D . Since the objective function in Eq. (10) is not convex, a coordinate ascent process is perfo"
O06-2003,J96-1002,0,0.0568166,"he evaluation of perplexity and character-error rate versus different factors is conducted. The final conclusions drawn from this study are discussed in Section 5. 40 Chuang-Hua Chueh et al. 2. Maximum Entropy Principle 2.1 ME Language Modeling The underlying idea of the ME principle [Jaynes 1957] is to subtly model what we know, and assume nothing about what we do not know. Accordingly, we choose a model that satisfies all the information we have and that makes the model distribution as uniform as possible. Using the ME model, we can combine different knowledge sources for language modeling [Berger et al. 1996]. Each knowledge source provides a set of constraints, which must be satisfied to find a unique ME solution. These constraints are typically expressed as marginal distributions. Given features f1 , , f N , which specify the properties extracted from observed data, the expectation of f i with respect to empirical distribution p(h, w) of history h and word w is calculated by p ( fi ) = ∑ p ( h, w) fi (h, w) , (4) h, w where fi (⋅) is a binary-valued feature function. Also, using conditional probabilities in language modeling, we yield the expectation with respect to the target conditional distr"
O06-2003,O04-3003,1,0.84505,"thm, called expectation maximization iterative scaling (EM-IS), was used. The authors also applied the LME principle to incorporate probabilistic latent semantic analysis [Hofmann 1999] into n-gram modeling by serving the semantic information as the latent variables [Wang et al. 2003]. In this study, we use the semantic information as explicit features for ME language modeling. Latent semantic analysis (LSA) is adopted to build semantic topics. 3. Integration of Semantic Information and N-Gram Models Modeling long-distance information is crucial for language modeling. In [Chien and Chen 2004; Chien et al. 2004], we successfully incorporated long-distance association patterns and latent semantic knowledge in language models. In [Wu and Khudanpur 2002], the integration of statistical n-gram and topic unigram using the ME approach was presented. Clustering of document vectors in the original document space was performed to extract topic information. However, the original document space was generally sparse and filled with noises caused by polysemy and synonymy [Deerwester et al. 1990]. To explore robust and representative topic characteristics, here we introduce a new knowledge source to extract long-"
O06-2003,H92-1020,0,0.177466,"Missing"
O06-2003,P99-1022,0,0.0800192,"Missing"
O06-4002,O05-2001,1,\N,Missing
O13-1001,W97-0703,0,0.473494,"Missing"
O13-1001,P04-1085,0,0.0956261,"Missing"
O13-1001,P08-1054,0,0.0388536,"Missing"
O13-1001,N07-2054,0,0.0562749,"Missing"
O14-1002,N10-1134,0,0.08846,"Missing"
O14-1002,P11-5003,0,0.0392463,"Missing"
O14-1002,P08-1054,0,0.0612,"Missing"
O14-1002,C10-1111,0,0.0751714,"Missing"
O14-1002,N07-2054,0,0.0711078,"Missing"
O15-1001,O05-3004,1,0.74818,"Missing"
O15-3004,mochizuki-okumura-2000-comparison,0,0.184053,"Missing"
O15-3004,W04-1013,0,0.0235877,"Missing"
O15-3004,W97-0707,0,0.294225,"Missing"
O15-3004,O05-3004,1,0.78245,"Missing"
O15-3005,O11-2001,1,0.521578,"Missing"
O15-3005,O09-1004,0,0.0128541,"Missing"
O16-1012,P11-5003,0,0.123639,"Missing"
O16-1012,D15-1229,0,0.0740231,"Missing"
O16-1012,D15-1044,0,0.0889799,"Missing"
O16-1012,K16-1028,0,0.0360188,"Missing"
O16-1012,D14-1179,0,0.0116194,"Missing"
O16-1012,D15-1166,0,0.0695141,"Missing"
O17-2001,P04-1085,0,0.107934,"Missing"
O17-2001,N10-1134,0,0.105237,"Missing"
O98-4005,H94-1097,0,0.030332,"monosyllabic, and the total number of phonologically allowed syllables is only about 1345. Although the majority of Chinese words are composed of two or more syllables or characters, most of the characters can also be considered as monosyllabic words. This is why accurate recognition of all 1345 Mandarin syllables is believed to be the first key problem in Mandarin speech recognition with a very large vocabulary, and this is also why syllables are often chosen as the basic recognition target, very similar to the words used in systems for other alphabetic languages [Lee, Hon, and Reddy, 1990; Ney et al., 1994]. Of course, this small number of syllables also implies that a large number of homonym characters share the same syllable, and that there is a high degree of ambiguity. For example, on average, every syllable is shared by about 7-8 (10,000/1345) possible homonym characters. This one-to-many mapping relation from syllables to characters is certainly another key issue in Mandarin speech recognition with a very large vocabulary, and some relevant problems have been discussed in many papers [Lee et al., 1993a; Lee et al., 1993b; Lyu and Lee et al., 1995; Wang and Lee et al., 1995; Shen, 1996]. T"
O98-4005,O97-3003,0,0.0231416,"ition systems in realistic operating environments will *Institute of Infromation E-mail:whm@iis.sinica.edu.tw Science, Academia Sinica, Taipei, Taiwan, R. O. C. 94 H. M. Wang require much better speech data to help us model the inherent variability in speech signals among different speakers and in different environments, and to help us evaluate performance under near realistic conditions. Thus, researchers all over the world have also participated in many efforts devoted to collecting speech databases of their own languages [Akira et al., 1990; Yu and Liu, 1990; Zue et al., 1990; Tseng, 1995; Wang, 1997], in addition to developing robust algorithms for speech recognition. Because the Chinese language is not alphabetic and keyboard input of Chinese characters into computers requires a considerable amount of effort and training, Mandarin speech recognition is highly desired especially in the Chinese community. In Taiwan, speech recognition systems have been developed for a wide variety of applications, such as small to large vocabulary keyword spotting [Huang, Wang, and Soong, 1994; Bai, Tseng, and Lee, 1997], medium size vocabulary isolated word recognition for voice command and control [Chan"
O98-4005,C92-1019,0,\N,Missing
W00-0504,P99-1027,0,0.0327709,"ed words may be helpful. We thus plan to exp]Iore the potential for integrated sequential model]ling of both words and syllables [Meng et al., 20013]. 4. Multiseale Embedded Translation Figures 1 and 2 illustrate two translingual retrieval strategies. In query translation, English text queries are transformed into Mandarin and then used to retrieve Mandarin documents. For document translation, Mandarin documents are translated into English before they are indexed and then matched with English queries. McCarley has reported improved effectiveness from techniques that couple the two techniques [McCarley, 1999], but time constraints may limit us to explonng only the query translation strategy dunng the six-week Workshop. 4,1 W o r d T r a n s l a t i o n While we make use of sub-word transcription to smooth out-of-vocabulary(OOV) problems in speech recognition as described above, and to alleviate the OOV problem :for translation as we discuss in the next section, accurate translation generally relies on the additional information available at the word and phrase levels. Since the &quot;bag of words&quot; information retrieval techniques do not incorporate any meaningful degree of language understanding to as"
W00-0504,O99-2001,1,0.804809,"t takes place frequently...) The above considerations lead to a number of techniques we plan to use for our task. We concentrate on three equally critical problems related to our theme of translingual speech retrieval: (i) indexing Mandarin Chinese audio with word and subword units, (ii) translating variable-size units for cross-language information retrieval, and (iii) devising effective retrieval strategies for English text queries and Mandarin Chinese news audio. 3. Multiscale Audio Indexing A popular approach to retrieval is to apply spoken document Large-Vocabulary s Examples drawn from [Meng and Ip, 1999]. Continuous Speech Recognition (LVCSR) 9 for audio indexing, followed by text retrieval techniques. Mandarin Chinese presents a challenge for word-level indexing by LVCSR, because of the ambiguity in tokenizing a sentence into words (as mentioned earlier). Furthermore, LVCSR with a static vocabulary is hampered by the out-of-vocabulary (OOV) problem, especially when searching sources with topical coverage as diverse as that found in broadcast news. By virtue of the monosyllabic nature of the Chinese language and its dialects, the syllable inventory can provide a complete phonological coverag"
W00-0504,P97-1017,0,0.139368,"Missing"
W00-0504,1998.amta-tutorials.5,0,\N,Missing
W00-0504,W98-1005,0,\N,Missing
W00-0504,J95-4004,0,\N,Missing
W00-0504,J98-4003,0,\N,Missing
W00-0504,X93-1008,0,\N,Missing
W00-0504,A97-1029,0,\N,Missing
W13-4414,O11-1001,1,0.842514,"n such a document are assumed to be independent of each other (the so-called “bag-of-words” assumption). When we calculate the conditional probability P( wL |W1L 1 ) , we can linearly combine the associated WTM models of the words occurring in W1L 1 to form a composite WTM model for predicting wL : PWTM( wL |W1L 1 ) In addition to topic models, many other language modeling techniques have been proposed to complement the n-gram model in different ways, such as recurrent neural network language modeling (RNNLM) (Tomáš et al., 2010), discriminative language modeling (DLM) (Roark et al., 2007; Lai et al., 2011; Chen et al., 2012), and relevance modeling (RM) (Lavrenko and Croft, 2001; Chen and Chen, 2011; Chen and Chen, 2013). RNNLM tries to project W1L 1 and wL into a continuous space, and estimate the conditional probability in a recursive way by incorporating the full information about W1L 1 . DLM takes an objective function corresponding to minimizing the word error rate for speech recognition or maximizing the ROUGE score for summarization as a holy grail and updates the language model parameters to achieve the goal. RM assumes that each word sequence W1L is associated with a relevance class"
W13-4414,P10-1009,0,0.0130967,"fly review the n-gram and topic language models. Section 3 details our proposed CSC system. A series of experiments are presented in Section 4. Finally, conclusions and future work are given in Section 5. 2 2.1 Language Modeling N-gram Language Modeling From the early 20th century, statistical language modeling has been successfully applied to various applications related to natural language processing (NLP), such as speech recognition (Chen and Goodman, 1999; Chen and Chen, 2011), information retrieval (Ponte and Croft, 1998; Lavrenko and Croft, 2001; Lavrenko, 2009), document summarization (Lin and Chen, 2010), and spelling correction (Chen et al., 2009; Liu et al., 2011; Wu et al., 2010). The most widely-used and well-practiced language model, by far, is the n-gram language model (Jelinek, 1999), because of its simplicity and fair predictive power. Quantifying the quality of a word string in a natural language is the most commonly executed task. Take the tri-gram model for example, when given a word string W1L  w1 , w2 ,, wL , the probability of the word string is approximated by the Since most Chinese characters have other characters similar to them in either shape or pronunciation, an intuitiv"
W13-4414,W10-4107,0,0.69511,"ing the long-span semantic information for language modeling for CSC. Moreover, we make a step forward to incorporate a search engine to provide extra information from the Web resources to make a more robust system. Introduction Chinese is a tonal syllabic and character (symbol) language, in which each character is pronounced as a tonal syllable. A Chinese “word” usually comprises two or more characters. The difficulty of Chinese processing is that many Chinese characters have similar shapes or similar (or same) pronunciations. Some characters are even similar in both shape and pronunciation (Wu et al., 2010; Liu et al., 2011). However, the meanings of these characters (or words composed of the characters) may be widely divergent. Due to this reason, all the students in elementary school in Taiwan or the foreign Chinese learners need to practice to identify and correct “erroneous words” in a Chinese sentence, which is called the Incorrect Character Correction (ICC) test. In fact, the ICC test is not a simple task even for some adult native speakers in Taiwan. The rest of this paper is organized as follows. In Section 2, we briefly review the n-gram and topic language models. Section 3 details our"
W13-4414,W03-1726,0,0.102448,"Missing"
W13-4414,O09-2007,0,0.0175181,"s. Section 3 details our proposed CSC system. A series of experiments are presented in Section 4. Finally, conclusions and future work are given in Section 5. 2 2.1 Language Modeling N-gram Language Modeling From the early 20th century, statistical language modeling has been successfully applied to various applications related to natural language processing (NLP), such as speech recognition (Chen and Goodman, 1999; Chen and Chen, 2011), information retrieval (Ponte and Croft, 1998; Lavrenko and Croft, 2001; Lavrenko, 2009), document summarization (Lin and Chen, 2010), and spelling correction (Chen et al., 2009; Liu et al., 2011; Wu et al., 2010). The most widely-used and well-practiced language model, by far, is the n-gram language model (Jelinek, 1999), because of its simplicity and fair predictive power. Quantifying the quality of a word string in a natural language is the most commonly executed task. Take the tri-gram model for example, when given a word string W1L  w1 , w2 ,, wL , the probability of the word string is approximated by the Since most Chinese characters have other characters similar to them in either shape or pronunciation, an intuitive idea for CSC is to construct a confusion s"
