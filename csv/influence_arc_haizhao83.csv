2020.acl-main.571,C18-1048,1,0.913414,"Missing"
2020.acl-main.571,D09-1015,0,0.191268,"Missing"
2020.acl-main.571,P19-1585,0,0.113624,"respectively. Metric Precision (P ), recall (R) and F-score (F1 ) are used to evaluate the predicted entities. An entity is confirmed correct if it exists in the target labels, regardless of the layer at which the model makes this prediction. 1 http://www.geniaproject.org/genia-corpus/posannotation 2 https://catalog.ldc.upenn.edu/LDC2006T06 (ACE2005) 6412 Model LSTM-CRF (Lample et al., 2016) Multi-CRF layered-CRF (Ju et al., 2018) LSTM. hyp (Katiyar and Cardie, 2018) Segm. hyp [POS] (Wang and Lu, 2018) Exhaustive (Sohrab and Miwa, 2018) 4 Anchor-Region [POS] (Lin et al., 2019a) Merge & Label (Fisher and Vlachos, 2019) Boundary-aware (Zheng et al., 2019) GEANN [Gazetter] (Lin et al., 2019b) KBP2017 Overview (Ji et al., 2017) BiFlaG P 70.3 69.7 74.2 70.6 76.8 76.2 75.1 77.1 75.0 ACE2005 R F1 55.7 62.2 61.3 65.2 70.3 72.2 70.4 70.5 72.3 74.5 73.6 74.9 74.1 74.6† 73.3 75.2∗ 75.2 75.1 (-0.1∗ ) P 75.2 73.1 78.5 79.8 77.0 73.3 75.8 75.9 77.4 GENIA R F1 64.6 69.5 64.9 68.8 71.3 74.7 68.2 73.6 73.3 75.1∗ 68.3 70.7 73.9 74.8 73.6 74.7† 74.6 76.0 (+0.9∗ ) P 71.5 69.7 79.2 77.7 72.6 77.1 KBP2017 R F1 53.3 61.1 60.8 64.9 66.5 72.3 71.8 74.6∗ 73.0 72.8† 74.3 75.6 (+1.0∗ ) Table 2: Experimental results5 on ACE2005, GENIA"
2020.acl-main.571,P19-1136,0,0.04814,"Missing"
2020.acl-main.571,D19-1538,1,0.910091,"Missing"
2020.acl-main.571,P18-1192,1,0.915369,"Missing"
2020.acl-main.571,P18-4024,1,0.88519,"Missing"
2020.acl-main.571,D18-1321,1,0.890294,"Missing"
2020.acl-main.571,N18-1131,0,0.238125,"). NER is commonly regarded as a sequence labeling task (Lample et al., 2016; Ma and Hovy, 2016; ∗ Corresponding author. This paper was partially supported by National Key Research and Development Program of China (No. 2017YFB0304100), Key Projects of National Natural Science Foundation of China (U1836222 and 61733011), Huawei-SJTU long term AI project, Cutting-edge Machine reading comprehension and language model. Peters et al., 2017). These approaches only work for non-nested entities (or flat entities), but neglect nested entities. There have been efforts to deal with the nested structure. Ju et al. 2018 introduced a layered sequence labeling model to first recognize innermost entities, and then feed them into the next layer to extract outer entities. However, this model suffers from obvious error propagation. The wrong entities extracted by the previous layer will affect the performance of the next layer. Also, such layered model suffers from the sparsity of entities at high levels. For instance, in the well-known ACE2005 training dataset, there are only two entities in the sixth level. Sohrab and Miwa 2018 proposed a region-based method that enumerates all possible regions and classifies th"
2020.acl-main.571,N18-1079,0,0.199538,"Missing"
2020.acl-main.571,N16-1030,0,0.732687,"V7 V5 GPE Figure 1: An example of nested named entity mentions. Solid lines connect the starting and ending indices of inner nested entities. Introduction Named entity recognition (NER) aims to identify words or phrases that contain the names of predefined categories like location, organization or medical codes. Nested NER further deals with entities that can be nested with each other, such as the United States and third president of the United States shown in Figure 1, such phenomenon is quite common in natural language processing (NLP). NER is commonly regarded as a sequence labeling task (Lample et al., 2016; Ma and Hovy, 2016; ∗ Corresponding author. This paper was partially supported by National Key Research and Development Program of China (No. 2017YFB0304100), Key Projects of National Natural Science Foundation of China (U1836222 and 61733011), Huawei-SJTU long term AI project, Cutting-edge Machine reading comprehension and language model. Peters et al., 2017). These approaches only work for non-nested entities (or flat entities), but neglect nested entities. There have been efforts to deal with the nested structure. Ju et al. 2018 introduced a layered sequence labeling model to first recogni"
2020.acl-main.571,C18-1271,1,0.904393,"Missing"
2020.acl-main.571,D18-1262,1,0.917243,"Missing"
2020.acl-main.571,P19-1511,0,0.250575,"Missing"
2020.acl-main.571,D19-1646,0,0.0348655,"Missing"
2020.acl-main.571,D15-1102,0,0.408355,"Missing"
2020.acl-main.571,D18-1019,0,0.174678,"on these datasets are tagged with † in Table 2, we make improvements of 0.5/1.3/2.8 F1 on ACE2005, GENIA, and KBP2017 respectively. KBP2017 contains much more entities than ACE2005 and GE3 Code is available at: https://github.com/cslydia/BiFlaG. 4 This result is reported by (Zheng et al., 2019), consistent with our own re-implemented results. NIA. The number of entities on test set is four times that of ACE2005. Our model has the most significant improvement on such dataset, proving the effectiveness of our BiFlaG model. More notably, our model without POS tags surpasses the previous models (Wang and Lu, 2018; Lin et al., 2019a), which use POS tags as additional representations on all three datasets. Besides, (Lin et al., 2019b) incorporate gazetteer information on ACE2005 dataset, our model also makes comparable results with theirs. Other works like (Strakov´a et al., 2019) 4 , which train their model on both training and development sets, are thus not comparable to our model directly. Table 3 makes a detailed comparison on the five categories of GENIA test dataset with a layered model (Ju et al., 2018) and a region-based model (Zheng et al., 2019). Compared with region-based model, layered model"
2020.acl-main.571,P19-1298,1,0.860959,"Missing"
2020.acl-main.571,P16-1101,0,0.751246,"An example of nested named entity mentions. Solid lines connect the starting and ending indices of inner nested entities. Introduction Named entity recognition (NER) aims to identify words or phrases that contain the names of predefined categories like location, organization or medical codes. Nested NER further deals with entities that can be nested with each other, such as the United States and third president of the United States shown in Figure 1, such phenomenon is quite common in natural language processing (NLP). NER is commonly regarded as a sequence labeling task (Lample et al., 2016; Ma and Hovy, 2016; ∗ Corresponding author. This paper was partially supported by National Key Research and Development Program of China (No. 2017YFB0304100), Key Projects of National Natural Science Foundation of China (U1836222 and 61733011), Huawei-SJTU long term AI project, Cutting-edge Machine reading comprehension and language model. Peters et al., 2017). These approaches only work for non-nested entities (or flat entities), but neglect nested entities. There have been efforts to deal with the nested structure. Ju et al. 2018 introduced a layered sequence labeling model to first recognize innermost entiti"
2020.acl-main.571,D17-1159,0,0.0467546,"e decoding, we search the tag sequences with maxi6410 • Entity graph G1 : for all the nodes in an extracted entity extracted from the flat module, edges are added between any two nodes eij = (vi , vj ), where start ≤ i < j ≤ end, as shown in Figure 2, allowing the outermost entity information to be utilized. • Adjacent graph G2 : for each pair of adjacent words in the sentence, we add one directed edge from the left word to the right one, allowing local contextual information to be utilized. Bi-GCN. In order to consider both incoming and outgoing features for each node, we follow the work of (Marcheggiani and Titov, 2017; Fu et al., 2019), which uses Bi-GCN to extract graph features. Given a graph G = (V, E), and the word representation X = {x1 , x2 , ..., xN }, the graph feature f ∈ RN ×df learned from Bi-GCN is expressed as follows. X −→ → − → − fi = ReLU ( (Wf xj + bf )) eij ∈E X ←− ← − ← − fi = ReLU ( (Wf xj + bf )) (5) Algorithm 1 Bipartite Flat-Graph Algorithm Input: word representations X = {x1 , .., xN }, number of entity types L the dimension of word embeddings dx , the hidden size of GCN df Output: all the entities in this sequence 1: for numbers of training iterations do 2: y ← B I LSTM-CRF(X) 3: c"
2020.acl-main.571,D17-1276,0,0.34694,"Missing"
2020.acl-main.571,D14-1162,0,0.0839587,"the same settings with our experiments, ∗ represents state-of-the-art results with POS tags or gazetteers, values in parentheses are also compared with them. 3.2 Parameter Settings Our model 3 is based on the framework of (Yang and Zhang, 2018). We conduct optimization with the stochastic gradient descent (SGD) and Adam for flat and GCN modules, respectively. For GENIA dataset, we use the same 200-dimension pretrained word embedding as (Ju et al., 2018; Sohrab and Miwa, 2018; Zheng et al., 2019). For ACE2005 and KBP2017 datasets, we use the publicly available pre-trained 100-dimension GloVe (Pennington et al., 2014) embedding. We train the character embedding as in (Xin et al., 2018). The learning rate is set to 0.015 and 0.001 for flat and GCN modules, respectively. We apply dropout to embeddings and the hidden states with a rate of 0.5. The hidden sizes of BiLSTM and GCN are both set to 256. The bias weights λ1 and λ2 are both set to 1.5. 3.3 Results and Comparisons Table 2 compares our model to some existing state-of-the-art approaches on the three benchmark datasets. Given only standard training data and publicly available word embeddings, the results in Table 2 show that our model outperforms all th"
2020.acl-main.571,P17-1161,0,0.0240488,"be nested with each other, such as the United States and third president of the United States shown in Figure 1, such phenomenon is quite common in natural language processing (NLP). NER is commonly regarded as a sequence labeling task (Lample et al., 2016; Ma and Hovy, 2016; ∗ Corresponding author. This paper was partially supported by National Key Research and Development Program of China (No. 2017YFB0304100), Key Projects of National Natural Science Foundation of China (U1836222 and 61733011), Huawei-SJTU long term AI project, Cutting-edge Machine reading comprehension and language model. Peters et al., 2017). These approaches only work for non-nested entities (or flat entities), but neglect nested entities. There have been efforts to deal with the nested structure. Ju et al. 2018 introduced a layered sequence labeling model to first recognize innermost entities, and then feed them into the next layer to extract outer entities. However, this model suffers from obvious error propagation. The wrong entities extracted by the previous layer will affect the performance of the next layer. Also, such layered model suffers from the sparsity of entities at high levels. For instance, in the well-known ACE20"
2020.acl-main.571,N19-1082,0,0.0201852,"(2) i=1 where Tyi ,yi+1 represents the transmission score from yi to yi+1 , Pi,yi is the score of the j th tag of the ith word from BiLSTM encoder. CRF model defines a family of conditional probability p(y|x) over all possible tag sequences y: exps(x,y) p(y|x) = P s(x,˜ y) y˜∈y exp mum score: y ∗ = arg max score(x, y˜) y˜∈y 2.3 (4) Graph Module Since the original input sentences are plain texts without inherent graphical structure, we first construct graphs based on the sequential information of texts and the entity information from the flat module. Then, we apply GCN (Kipf and Welling, 2017; Qian et al., 2019) which propagates information between neighboring nodes in the graphs, to extract the inner entities. Graph Construction. We create two types of graphs for each sentence as in Figure 2. Each graph is defined as G = (V, E), where V is the set of nodes (words), E is the set of edges. (3) during training phase, we consider the maximum log probability of the correct predictions. While decoding, we search the tag sequences with maxi6410 • Entity graph G1 : for all the nodes in an extracted entity extracted from the flat module, edges are added between any two nodes eij = (vi , vj ), where start ≤ i"
2020.acl-main.571,D18-1309,0,0.294822,"Missing"
2020.acl-main.571,P19-1527,0,0.122241,"Missing"
2020.acl-main.571,D18-1279,0,0.0691265,"ntity graph G1 as in Figure 2. For the graph module, we use GCN which iteratively propagates information between the start and end nodes of potential entities to learn inner entities. Finally, the learned representation from the graph module is further fed back to the flat module for better outermost predictions. 2.1 Token Representation Given a sequence consisting of N tokens {t1 , t2 , ..., tN }, for each token ti , we first concatenate the word-level and character-level embedding ti = [wi ; ci ], wi is the pre-trained word embedding, character embedding ci is learned following the work of (Xin et al., 2018). Then we use a BiLSTM to capture sequential information for each token xi = B I LSTM(ti ). We take xi as the word representation and feed it to subsequent modules. 2.2 Flat NER Module We adopt BiLSTM-CRF architecture (Lample et al., 2016; Ma and Hovy, 2016; Yang and Zhang, 2018; Luo et al., 2020) in our flat module to recognize flat entities, which consists of a bidirectional LSTM (BiLSTM) encoder and a conditional random field (CRF) decoder. BiLSTM captures bidirectional contextual information of sequences and can effectively represent the hidden states of words in context. BiLSTM represents"
2020.acl-main.571,P18-4013,0,0.264916,"dule for better outermost predictions. 2.1 Token Representation Given a sequence consisting of N tokens {t1 , t2 , ..., tN }, for each token ti , we first concatenate the word-level and character-level embedding ti = [wi ; ci ], wi is the pre-trained word embedding, character embedding ci is learned following the work of (Xin et al., 2018). Then we use a BiLSTM to capture sequential information for each token xi = B I LSTM(ti ). We take xi as the word representation and feed it to subsequent modules. 2.2 Flat NER Module We adopt BiLSTM-CRF architecture (Lample et al., 2016; Ma and Hovy, 2016; Yang and Zhang, 2018; Luo et al., 2020) in our flat module to recognize flat entities, which consists of a bidirectional LSTM (BiLSTM) encoder and a conditional random field (CRF) decoder. BiLSTM captures bidirectional contextual information of sequences and can effectively represent the hidden states of words in context. BiLSTM represents the sequential information at each step, the hidden state h of BiLSTM can be expressed as follows. → − → − → − hi = LST M (xi , h i−1 ; θ ) ← − ← − ← − (1) hi = LST M (xi , h i−1 ; θ ) → − ← − hi = [ hi ; hi ] → − ← − → − where θ and θ are trainable parameters. hi and ← − hi re"
2020.acl-main.571,P19-1154,1,0.755594,"Missing"
2020.acl-main.571,C18-1038,1,0.907768,"Missing"
2020.acl-main.571,D19-1034,0,0.327988,"ies, and then feed them into the next layer to extract outer entities. However, this model suffers from obvious error propagation. The wrong entities extracted by the previous layer will affect the performance of the next layer. Also, such layered model suffers from the sparsity of entities at high levels. For instance, in the well-known ACE2005 training dataset, there are only two entities in the sixth level. Sohrab and Miwa 2018 proposed a region-based method that enumerates all possible regions and classifies their entity types. However, this model may ignore explicit boundary information. Zheng et al. 2019 combined the layered sequence labeling model and region-based method to locate the entity boundary first, and then utilized the region classification model to predict entities. This model, however, cares less interaction among entities located in outer and inner layers. In this paper, we propose a bipartite flat-graph 6408 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6408–6418 c July 5 - 10, 2020. 2020 Association for Computational Linguistics network (BiFlaG) for nested NER, which models a nested structure containing arbitrary many layers int"
2020.acl-main.571,P19-1230,1,0.884482,"Missing"
2020.acl-main.571,W07-1009,0,\N,Missing
2020.emnlp-main.317,W06-1655,0,0.322285,"l Key Research and Development Program of China (No. 2017YFB0304100), Key Projects of National Natural Science Foundation of China (U1836222 and 61733011), Huawei-SJTU long term AI project, Cutting-edge Machine reading comprehension and language model. Markov models such as (Ng and Low, 2004) and (Zheng et al., 2013) depend on the maximum entropy model or maximum entropy Markov model both with Viterbi decoding. Besides, conditional random field (CRF) or Semi-CRF for sequence labeling has been used for both traditional and neural models though with different representations (Peng et al., 2004; Andrew, 2006; Wang and Xu, 2017; Ma et al., 2018). Recent neural CWS research have been concerned about the following three perspectives (Emerson, 2005). Decoder. As CWS is a kind of structure learning task, the decoder module generally determines which type of detailed algorithm should be adopted for segmentation, also it may limit the capability of defining feature. As shown in Table 2, not all models can support the word-level features as CWS is a task to predict word boundary. Thus recent works focus on finding more general or flexible decoder design to make model learn the representation of segmentat"
2020.emnlp-main.317,P16-1039,1,0.908174,"l., 2018). Recent neural CWS research have been concerned about the following three perspectives (Emerson, 2005). Decoder. As CWS is a kind of structure learning task, the decoder module generally determines which type of detailed algorithm should be adopted for segmentation, also it may limit the capability of defining feature. As shown in Table 2, not all models can support the word-level features as CWS is a task to predict word boundary. Thus recent works focus on finding more general or flexible decoder design to make model learn the representation of segmentation more effective such as (Cai and Zhao, 2016; Cai et al., 2017). Encoder. Practice in various natural language processing tasks has shown that effective representation is essential to the performance improvement. For such a module in neural models, it is more than an encoder now, which is regarded as the most improvement perspective against traditional models. Thus for better CWS, it is crucial to encode the input character, word or sentence into a distinguishable representation. Table 2 summarizes regular feature sets for typical CWS models including ours as well. The building blocks that encoders use include recurrent neural network ("
2020.emnlp-main.317,C18-1233,1,0.86586,"Missing"
2020.emnlp-main.317,D15-1141,0,0.76184,"k (CNN), and long short-term memory (LSTM) network. External resources and pre-trained embedding. Using external resource such as pre-trained embeddings or language representation provides 3862 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 3862–3872, c November 16–20, 2020. 2020 Association for Computational Linguistics Traditional Models Greedy Model Markov Model Sequence Labeling Model General Graph Model Decoding Algorithm Neural Models Ours - Greedy (Ng and Low, 2004), (Low et al., 2005) MMTNN: (Pei et al., 2014) (Zheng et al., 2013), LSTM: (Chen et al., 2015) CRF: (Peng et al., 2004), semi-CRF: (Andrew, 2006), (Sun et al., 2009) CNN+CRF:(Wang and Xu, 2017), BiLSTM+CRF:(Ma et al., 2018) (Zhang and Clark, 2007) LSTM+GCNN: (Cai and Zhao, 2016), LSTM+GCNN: (Cai et al., 2017) (Wang et al., 2019a) Viterbi Beam search Table 1: The classification of Chinese word segmentation model. Models character based word based Ours (Zheng et al., 2013), . . . (Chen et al., 2015) (Zhang and Clark, 2007), . . . (Cai and Zhao, 2016; Cai et al., 2017) Characters c0 , c1 , . . . , ci , ci+1 , . . . , cn ci−2 , ci−1 , ci , ci+1 , ci+2 c0 , c1 , . . . , ci , ci+1 , ci+2 c i"
2020.emnlp-main.317,P17-1110,0,0.561525,"Missing"
2020.emnlp-main.317,I05-3017,0,0.941205,"22 and 61733011), Huawei-SJTU long term AI project, Cutting-edge Machine reading comprehension and language model. Markov models such as (Ng and Low, 2004) and (Zheng et al., 2013) depend on the maximum entropy model or maximum entropy Markov model both with Viterbi decoding. Besides, conditional random field (CRF) or Semi-CRF for sequence labeling has been used for both traditional and neural models though with different representations (Peng et al., 2004; Andrew, 2006; Wang and Xu, 2017; Ma et al., 2018). Recent neural CWS research have been concerned about the following three perspectives (Emerson, 2005). Decoder. As CWS is a kind of structure learning task, the decoder module generally determines which type of detailed algorithm should be adopted for segmentation, also it may limit the capability of defining feature. As shown in Table 2, not all models can support the word-level features as CWS is a task to predict word boundary. Thus recent works focus on finding more general or flexible decoder design to make model learn the representation of segmentation more effective such as (Cai and Zhao, 2016; Cai et al., 2017). Encoder. Practice in various natural language processing tasks has shown"
2020.emnlp-main.317,D19-1538,1,0.790442,"d 3866 dh are dimensions of model and one head, respectively. 3.2 T Biaffine Attention Scorer + Our model straightforwardly predicts gap between two adjacent characters as word boundary or not. In detail, we set a label value 1 to indicate word boundary, and 0 means no word boundary. Such a gap labeling task thus requires information of the two adjacent characters. In the meantime, the relationship between adjacent characters can be represented as the gap label. Biaffine attention scorer is used to label the gap (Dozat and Manning, 2017; Li et al., 2018; Cai et al., 2018; Zhou and Zhao, 2019; He et al., 2019). The distribution of labels in a labeling task is often uneven. Biaffine attention uses bias terms to alleviate the burden of the fixed bias term and get the prior probability which makes it different from bilinear attention. The distribution of the gap is uneven that is similar as other labeling task, which makes biaffine available for our task. Biaffine attention scorer labels the target depending on information of independent unit and the joint information of two units. In biaffine attention, the score sij of characters ci and cj (i < j) is calculated by: sij = Biaf f inalScorer(vif , vjb"
2020.emnlp-main.317,K18-2006,1,0.802657,"ter matrices of Rdk ×dk to generate the attention, dk and 3866 dh are dimensions of model and one head, respectively. 3.2 T Biaffine Attention Scorer + Our model straightforwardly predicts gap between two adjacent characters as word boundary or not. In detail, we set a label value 1 to indicate word boundary, and 0 means no word boundary. Such a gap labeling task thus requires information of the two adjacent characters. In the meantime, the relationship between adjacent characters can be represented as the gap label. Biaffine attention scorer is used to label the gap (Dozat and Manning, 2017; Li et al., 2018; Cai et al., 2018; Zhou and Zhao, 2019; He et al., 2019). The distribution of labels in a labeling task is often uneven. Biaffine attention uses bias terms to alleviate the burden of the fixed bias term and get the prior probability which makes it different from bilinear attention. The distribution of the gap is uneven that is similar as other labeling task, which makes biaffine available for our task. Biaffine attention scorer labels the target depending on information of independent unit and the joint information of two units. In biaffine attention, the score sij of characters ci and cj (i"
2020.emnlp-main.317,I05-3025,0,0.237478,"Missing"
2020.emnlp-main.317,D18-1529,0,0.65877,"Missing"
2020.emnlp-main.317,W04-3236,0,0.419794,"rpus with segmentation, the CWS task may be generally modeled as a decoder which performs segmentation based on a scoring module in terms of contextual feature based representations. Table 1 summarizes typical CWS models according to their decoding ways. ∗ Corresponding author. This paper was partially supported by National Key Research and Development Program of China (No. 2017YFB0304100), Key Projects of National Natural Science Foundation of China (U1836222 and 61733011), Huawei-SJTU long term AI project, Cutting-edge Machine reading comprehension and language model. Markov models such as (Ng and Low, 2004) and (Zheng et al., 2013) depend on the maximum entropy model or maximum entropy Markov model both with Viterbi decoding. Besides, conditional random field (CRF) or Semi-CRF for sequence labeling has been used for both traditional and neural models though with different representations (Peng et al., 2004; Andrew, 2006; Wang and Xu, 2017; Ma et al., 2018). Recent neural CWS research have been concerned about the following three perspectives (Emerson, 2005). Decoder. As CWS is a kind of structure learning task, the decoder module generally determines which type of detailed algorithm should be ad"
2020.emnlp-main.317,P14-1028,0,0.516642,"Missing"
2020.emnlp-main.317,C04-1081,0,0.914232,"upported by National Key Research and Development Program of China (No. 2017YFB0304100), Key Projects of National Natural Science Foundation of China (U1836222 and 61733011), Huawei-SJTU long term AI project, Cutting-edge Machine reading comprehension and language model. Markov models such as (Ng and Low, 2004) and (Zheng et al., 2013) depend on the maximum entropy model or maximum entropy Markov model both with Viterbi decoding. Besides, conditional random field (CRF) or Semi-CRF for sequence labeling has been used for both traditional and neural models though with different representations (Peng et al., 2004; Andrew, 2006; Wang and Xu, 2017; Ma et al., 2018). Recent neural CWS research have been concerned about the following three perspectives (Emerson, 2005). Decoder. As CWS is a kind of structure learning task, the decoder module generally determines which type of detailed algorithm should be adopted for segmentation, also it may limit the capability of defining feature. As shown in Table 2, not all models can support the word-level features as CWS is a task to predict word boundary. Thus recent works focus on finding more general or flexible decoder design to make model learn the representatio"
2020.emnlp-main.317,N18-2074,0,0.0257982,"from simple idea, including the least building block type for encoder (attention only), the least feature type for scoring (unigram only) and the least computational complexity for decoding (greedy segmentation). The original Transformer encoder consists of a stack of N identical layers and each layer has one multi-head self-attention layer and one positionwise fully connected feed-forward layer (Vaswani et al., 2017). One residual connection is around two sub-layers and followed by layer normalization. Several variants are proposed to enhance ability of capturing the localness relationship. (Shaw et al., 2018) propose an effcient way to incorporate relative and absolute position representation. (Yang et al., 2018) cast localness modeling as a learnable Gaussian bias to enhance the ability of capturing useful local context. (Kim et al., 2020) propose a Transformer with Gaussian-weighted self-attention to improved speech-enhancement performance. (Zhang et al., 2020b) propose using syntax to guide the text modeling based on selfattention network sponsored Transformer-based encoder. Transformer based pre-trained language 3864 Segmentation Decoding Layer Decoding Layer Biaffine Attention Scorer Biaffine"
2020.emnlp-main.317,W03-1719,0,0.349496,"Missing"
2020.emnlp-main.317,N09-1007,0,0.0555664,"Missing"
2020.emnlp-main.317,I17-1017,0,0.187861,"and Development Program of China (No. 2017YFB0304100), Key Projects of National Natural Science Foundation of China (U1836222 and 61733011), Huawei-SJTU long term AI project, Cutting-edge Machine reading comprehension and language model. Markov models such as (Ng and Low, 2004) and (Zheng et al., 2013) depend on the maximum entropy model or maximum entropy Markov model both with Viterbi decoding. Besides, conditional random field (CRF) or Semi-CRF for sequence labeling has been used for both traditional and neural models though with different representations (Peng et al., 2004; Andrew, 2006; Wang and Xu, 2017; Ma et al., 2018). Recent neural CWS research have been concerned about the following three perspectives (Emerson, 2005). Decoder. As CWS is a kind of structure learning task, the decoder module generally determines which type of detailed algorithm should be adopted for segmentation, also it may limit the capability of defining feature. As shown in Table 2, not all models can support the word-level features as CWS is a task to predict word boundary. Thus recent works focus on finding more general or flexible decoder design to make model learn the representation of segmentation more effective"
2020.emnlp-main.317,O03-4002,0,0.716834,"paper can be summarized as follows. • To especially enhance the representation of localness information and directional information, we propose a new Gaussian-masked Directional Transformer encoder. • Motivated from a simple design idea, we present a new CWS model which is stacked with only attention blocks. • With a powerful enough encoder, for the first time, we show that unigram (character) features plus greedy segmentation algorithm can support yielding strong performance instead of using diverse n-gram (character and word) features and highly complex decoding algorithms. 2 Related Work (Xue, 2003) first formalize CWS as a sequence labeling task, considering CWS as a supervised learning from annotated corpus with human segmentation. (Peng et al., 2004) further adopt standard sequence labeling tool CRFs for CWS modeling, achieving new state-of-the-art. (Zhao et al., 2006b) show that different character tag sets can make essential impact for segmentation performance. (Zhao et al., 2006a) propose a CWS system developed for Bakeoff-2006 based on CRF, which is based on their proposed 6-tag set for character position tagging and achieved state-of-the-art performance at then. (Zhao and Kit, 20"
2020.emnlp-main.317,D18-1475,0,0.0209286,"type for scoring (unigram only) and the least computational complexity for decoding (greedy segmentation). The original Transformer encoder consists of a stack of N identical layers and each layer has one multi-head self-attention layer and one positionwise fully connected feed-forward layer (Vaswani et al., 2017). One residual connection is around two sub-layers and followed by layer normalization. Several variants are proposed to enhance ability of capturing the localness relationship. (Shaw et al., 2018) propose an effcient way to incorporate relative and absolute position representation. (Yang et al., 2018) cast localness modeling as a learnable Gaussian bias to enhance the ability of capturing useful local context. (Kim et al., 2020) propose a Transformer with Gaussian-weighted self-attention to improved speech-enhancement performance. (Zhang et al., 2020b) propose using syntax to guide the text modeling based on selfattention network sponsored Transformer-based encoder. Transformer based pre-trained language 3864 Segmentation Decoding Layer Decoding Layer Biaffine Attention Scorer Biaffine Attention Scorer Highway-O vfʼ HiRED rfʼ Sum v bʼ rcʼ GD-Transformer Sum r bʼ rf vf Sum Forward Encoder O"
2020.emnlp-main.317,Y06-1012,1,0.499001,"h is stacked with only attention blocks. • With a powerful enough encoder, for the first time, we show that unigram (character) features plus greedy segmentation algorithm can support yielding strong performance instead of using diverse n-gram (character and word) features and highly complex decoding algorithms. 2 Related Work (Xue, 2003) first formalize CWS as a sequence labeling task, considering CWS as a supervised learning from annotated corpus with human segmentation. (Peng et al., 2004) further adopt standard sequence labeling tool CRFs for CWS modeling, achieving new state-of-the-art. (Zhao et al., 2006b) show that different character tag sets can make essential impact for segmentation performance. (Zhao et al., 2006a) propose a CWS system developed for Bakeoff-2006 based on CRF, which is based on their proposed 6-tag set for character position tagging and achieved state-of-the-art performance at then. (Zhao and Kit, 2007) present a novel Character tagging based CRF framework which is capable of exploiting global information for performance enhancement. Neural word segmentation has been widely used to minimize the efforts in feature engineering. (Zheng et al., 2013) first introduce the neura"
2020.emnlp-main.317,P17-1078,0,0.428154,"Missing"
2020.emnlp-main.317,P07-1106,0,0.140568,"Missing"
2020.emnlp-main.317,D13-1061,0,0.435461,"Missing"
2020.emnlp-main.317,D17-1079,0,0.606209,"Missing"
2020.emnlp-main.317,P19-1230,1,0.835983,"the attention, dk and 3866 dh are dimensions of model and one head, respectively. 3.2 T Biaffine Attention Scorer + Our model straightforwardly predicts gap between two adjacent characters as word boundary or not. In detail, we set a label value 1 to indicate word boundary, and 0 means no word boundary. Such a gap labeling task thus requires information of the two adjacent characters. In the meantime, the relationship between adjacent characters can be represented as the gap label. Biaffine attention scorer is used to label the gap (Dozat and Manning, 2017; Li et al., 2018; Cai et al., 2018; Zhou and Zhao, 2019; He et al., 2019). The distribution of labels in a labeling task is often uneven. Biaffine attention uses bias terms to alleviate the burden of the fixed bias term and get the prior probability which makes it different from bilinear attention. The distribution of the gap is uneven that is similar as other labeling task, which makes biaffine available for our task. Biaffine attention scorer labels the target depending on information of independent unit and the joint information of two units. In biaffine attention, the score sij of characters ci and cj (i < j) is calculated by: sij = Biaf f ina"
2020.emnlp-main.317,W06-0127,1,0.642979,"h is stacked with only attention blocks. • With a powerful enough encoder, for the first time, we show that unigram (character) features plus greedy segmentation algorithm can support yielding strong performance instead of using diverse n-gram (character and word) features and highly complex decoding algorithms. 2 Related Work (Xue, 2003) first formalize CWS as a sequence labeling task, considering CWS as a supervised learning from annotated corpus with human segmentation. (Peng et al., 2004) further adopt standard sequence labeling tool CRFs for CWS modeling, achieving new state-of-the-art. (Zhao et al., 2006b) show that different character tag sets can make essential impact for segmentation performance. (Zhao et al., 2006a) propose a CWS system developed for Bakeoff-2006 based on CRF, which is based on their proposed 6-tag set for character position tagging and achieved state-of-the-art performance at then. (Zhao and Kit, 2007) present a novel Character tagging based CRF framework which is capable of exploiting global information for performance enhancement. Neural word segmentation has been widely used to minimize the efforts in feature engineering. (Zheng et al., 2013) first introduce the neura"
2020.emnlp-main.723,W99-0613,0,0.662795,"ohnson, 2016), etc. Whereas the development of unsupervised NE recognition is still kept unsatisfactory. (Liu et al., 2019) design a Knowledge-Augmented Language Model for unsupervised NE recognition, they perform NE recognition by controlling whether a particular word is modeled as a general word or as a reference to an entity in the training of language models. However, their model still requires type-specific entity vocabularies for computing the type probabilities and the probability of the word under given type. Early unsupervised NE systems relied on labeled seeds and discrete features (Collins and Singer, 1999), open web text (Etzioni et al., 2005; Nadeau et al., 2006), shallow syntactic knowledge (Zhang and Elhadad, 2013), etc. Word embeddings learned from unlabeled text provide representation with rich syntax and semantics and have shown to be valuable as features in unsupervised learning prob8995 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 8995–9005, c November 16–20, 2020. 2020 Association for Computational Linguistics lems (Lin et al., 2015; He et al., 2018). In this work, we propose an NE recognition model with word embeddings as the unique fea"
2020.emnlp-main.723,D18-1160,0,0.0742001,"ciently select the set of samples for labeling, thus greatly reduce the annotation budget; (Ren et al., 2015; Shang et al., 2018; Fries et al., 2017; Yang et al., 2018b; Jie et al., 2019) use partially annotated data or external resources such as NE dictionary, knowledge base, POS tags as a replacement of hand-labeled data to train distant supervision systems. However, these methods still have certain requirements for annotation resources. Unsupervised models have achieved excellent results in the fields of part-of-speech induction (Lin et al., 2015; Stratos et al., 2016), dependency parsing (He et al., 2018; Pate and Johnson, 2016), etc. Whereas the development of unsupervised NE recognition is still kept unsatisfactory. (Liu et al., 2019) design a Knowledge-Augmented Language Model for unsupervised NE recognition, they perform NE recognition by controlling whether a particular word is modeled as a general word or as a reference to an entity in the training of language models. However, their model still requires type-specific entity vocabularies for computing the type probabilities and the probability of the word under given type. Early unsupervised NE systems relied on labeled seeds and discret"
2020.emnlp-main.723,N19-1079,0,0.182896,"018; Luo et al., 2020; Luo and Zhao, 2020). However, these existing methods require large amounts of manually annotated data for training supervised models. There have been efforts to deal with the lack of annotation data in NE recognition, (Talukdar and Pereira, 2010) train a weak supervision model and use label propagation methods to identify more entities of each type; (Shen et al., 2017) employ Deep Active Learning to efficiently select the set of samples for labeling, thus greatly reduce the annotation budget; (Ren et al., 2015; Shang et al., 2018; Fries et al., 2017; Yang et al., 2018b; Jie et al., 2019) use partially annotated data or external resources such as NE dictionary, knowledge base, POS tags as a replacement of hand-labeled data to train distant supervision systems. However, these methods still have certain requirements for annotation resources. Unsupervised models have achieved excellent results in the fields of part-of-speech induction (Lin et al., 2015; Stratos et al., 2016), dependency parsing (He et al., 2018; Pate and Johnson, 2016), etc. Whereas the development of unsupervised NE recognition is still kept unsatisfactory. (Liu et al., 2019) design a Knowledge-Augmented Languag"
2020.emnlp-main.723,N16-1030,0,0.695046,"-grained annotations through neural networks. Extensive experiments on two CoNLL benchmark NER datasets (CoNLL2003 English dataset and CoNLL-2002 Spanish dataset) demonstrate that our proposed light NE recognition model achieves remarkable performance without using any annotated lexicon or corpus. 1 Introduction Named Entity (NE) recognition is a major natural language processing task that intends to identify words or phrases that contain the names of PER (Person), ORG (Organization), LOC (Location), etc. Recent advances in deep neural models allow us to build reliable NE recognition systems (Lample et al., 2016; Ma and Hovy, 2016; Liu et al., ∗ Corresponding author. This paper was partially supported by National Key Research and Development Program of China (No. 2017YFB0304100), Key Projects of National Natural Science Foundation of China (U1836222 and 61733011), Huawei-SJTU long term AI project, Cutting-edge Machine reading comprehension and language model. 2018; Yang and Zhang, 2018; Luo et al., 2020; Luo and Zhao, 2020). However, these existing methods require large amounts of manually annotated data for training supervised models. There have been efforts to deal with the lack of annotation data"
2020.emnlp-main.723,C18-1271,1,0.750081,"deling. Unsupervised models have fewer parameters and simpler training phase, thus there is no guarantee that the language model will retain its key properties when it is reduced to low dimensions. We further add the pre-trained language model BERT as the additional embeddings for the NE Tagger to refine the output of Gaussian-HMM and DAGMM, which slightly improves our result to 69.99 for CoNLL-2003 English NER and 56.66 for CoNLL-2002 Spanish NER. 5 Related work Deep neural network models have helped peoples released from handcrafted features in a wide range of NLP tasks (Zhang et al., 2019; Li et al., 2018a,b, 2019; Zhou and Zhao, 2019; Xiao et al., 2019; Zhang et al., 2020a,b,c). LSTM-CRF (Lample et al., 2016; Ma and Hovy, 2016) is the most stateof-the-art model for NE recognition. In order to reduce the requirements of training corpus, distant supervised models (Shang et al., 2018; Yang et al., 2018b; Ren et al., 2015; He, 2017; Fries et al., 2017) have been applied to NE recognition. Recently, (Liu et al., 2019) proposed a KnowledgeAugmented Language Model, which trains language models and at the same time compute the probability of the next word being different entity types according to the"
2020.emnlp-main.723,D18-1262,1,0.621783,"deling. Unsupervised models have fewer parameters and simpler training phase, thus there is no guarantee that the language model will retain its key properties when it is reduced to low dimensions. We further add the pre-trained language model BERT as the additional embeddings for the NE Tagger to refine the output of Gaussian-HMM and DAGMM, which slightly improves our result to 69.99 for CoNLL-2003 English NER and 56.66 for CoNLL-2002 Spanish NER. 5 Related work Deep neural network models have helped peoples released from handcrafted features in a wide range of NLP tasks (Zhang et al., 2019; Li et al., 2018a,b, 2019; Zhou and Zhao, 2019; Xiao et al., 2019; Zhang et al., 2020a,b,c). LSTM-CRF (Lample et al., 2016; Ma and Hovy, 2016) is the most stateof-the-art model for NE recognition. In order to reduce the requirements of training corpus, distant supervised models (Shang et al., 2018; Yang et al., 2018b; Ren et al., 2015; He, 2017; Fries et al., 2017) have been applied to NE recognition. Recently, (Liu et al., 2019) proposed a KnowledgeAugmented Language Model, which trains language models and at the same time compute the probability of the next word being different entity types according to the"
2020.emnlp-main.723,N15-1144,0,0.273478,"type; (Shen et al., 2017) employ Deep Active Learning to efficiently select the set of samples for labeling, thus greatly reduce the annotation budget; (Ren et al., 2015; Shang et al., 2018; Fries et al., 2017; Yang et al., 2018b; Jie et al., 2019) use partially annotated data or external resources such as NE dictionary, knowledge base, POS tags as a replacement of hand-labeled data to train distant supervision systems. However, these methods still have certain requirements for annotation resources. Unsupervised models have achieved excellent results in the fields of part-of-speech induction (Lin et al., 2015; Stratos et al., 2016), dependency parsing (He et al., 2018; Pate and Johnson, 2016), etc. Whereas the development of unsupervised NE recognition is still kept unsatisfactory. (Liu et al., 2019) design a Knowledge-Augmented Language Model for unsupervised NE recognition, they perform NE recognition by controlling whether a particular word is modeled as a general word or as a reference to an entity in the training of language models. However, their model still requires type-specific entity vocabularies for computing the type probabilities and the probability of the word under given type. Early"
2020.emnlp-main.723,N19-1117,0,0.155382,"es et al., 2017; Yang et al., 2018b; Jie et al., 2019) use partially annotated data or external resources such as NE dictionary, knowledge base, POS tags as a replacement of hand-labeled data to train distant supervision systems. However, these methods still have certain requirements for annotation resources. Unsupervised models have achieved excellent results in the fields of part-of-speech induction (Lin et al., 2015; Stratos et al., 2016), dependency parsing (He et al., 2018; Pate and Johnson, 2016), etc. Whereas the development of unsupervised NE recognition is still kept unsatisfactory. (Liu et al., 2019) design a Knowledge-Augmented Language Model for unsupervised NE recognition, they perform NE recognition by controlling whether a particular word is modeled as a general word or as a reference to an entity in the training of language models. However, their model still requires type-specific entity vocabularies for computing the type probabilities and the probability of the word under given type. Early unsupervised NE systems relied on labeled seeds and discrete features (Collins and Singer, 1999), open web text (Etzioni et al., 2005; Nadeau et al., 2006), shallow syntactic knowledge (Zhang an"
2020.emnlp-main.723,2020.acl-main.571,1,0.770413,"r phrases that contain the names of PER (Person), ORG (Organization), LOC (Location), etc. Recent advances in deep neural models allow us to build reliable NE recognition systems (Lample et al., 2016; Ma and Hovy, 2016; Liu et al., ∗ Corresponding author. This paper was partially supported by National Key Research and Development Program of China (No. 2017YFB0304100), Key Projects of National Natural Science Foundation of China (U1836222 and 61733011), Huawei-SJTU long term AI project, Cutting-edge Machine reading comprehension and language model. 2018; Yang and Zhang, 2018; Luo et al., 2020; Luo and Zhao, 2020). However, these existing methods require large amounts of manually annotated data for training supervised models. There have been efforts to deal with the lack of annotation data in NE recognition, (Talukdar and Pereira, 2010) train a weak supervision model and use label propagation methods to identify more entities of each type; (Shen et al., 2017) employ Deep Active Learning to efficiently select the set of samples for labeling, thus greatly reduce the annotation budget; (Ren et al., 2015; Shang et al., 2018; Fries et al., 2017; Yang et al., 2018b; Jie et al., 2019) use partially annotated"
2020.emnlp-main.723,P16-1101,0,0.29156,"through neural networks. Extensive experiments on two CoNLL benchmark NER datasets (CoNLL2003 English dataset and CoNLL-2002 Spanish dataset) demonstrate that our proposed light NE recognition model achieves remarkable performance without using any annotated lexicon or corpus. 1 Introduction Named Entity (NE) recognition is a major natural language processing task that intends to identify words or phrases that contain the names of PER (Person), ORG (Organization), LOC (Location), etc. Recent advances in deep neural models allow us to build reliable NE recognition systems (Lample et al., 2016; Ma and Hovy, 2016; Liu et al., ∗ Corresponding author. This paper was partially supported by National Key Research and Development Program of China (No. 2017YFB0304100), Key Projects of National Natural Science Foundation of China (U1836222 and 61733011), Huawei-SJTU long term AI project, Cutting-edge Machine reading comprehension and language model. 2018; Yang and Zhang, 2018; Luo et al., 2020; Luo and Zhao, 2020). However, these existing methods require large amounts of manually annotated data for training supervised models. There have been efforts to deal with the lack of annotation data in NE recognition,"
2020.emnlp-main.723,C16-1003,0,0.0164958,"e set of samples for labeling, thus greatly reduce the annotation budget; (Ren et al., 2015; Shang et al., 2018; Fries et al., 2017; Yang et al., 2018b; Jie et al., 2019) use partially annotated data or external resources such as NE dictionary, knowledge base, POS tags as a replacement of hand-labeled data to train distant supervision systems. However, these methods still have certain requirements for annotation resources. Unsupervised models have achieved excellent results in the fields of part-of-speech induction (Lin et al., 2015; Stratos et al., 2016), dependency parsing (He et al., 2018; Pate and Johnson, 2016), etc. Whereas the development of unsupervised NE recognition is still kept unsatisfactory. (Liu et al., 2019) design a Knowledge-Augmented Language Model for unsupervised NE recognition, they perform NE recognition by controlling whether a particular word is modeled as a general word or as a reference to an entity in the training of language models. However, their model still requires type-specific entity vocabularies for computing the type probabilities and the probability of the word under given type. Early unsupervised NE systems relied on labeled seeds and discrete features (Collins and S"
2020.emnlp-main.723,D14-1162,0,0.0891203,"Missing"
2020.emnlp-main.723,D18-1230,0,0.208981,"ng comprehension and language model. 2018; Yang and Zhang, 2018; Luo et al., 2020; Luo and Zhao, 2020). However, these existing methods require large amounts of manually annotated data for training supervised models. There have been efforts to deal with the lack of annotation data in NE recognition, (Talukdar and Pereira, 2010) train a weak supervision model and use label propagation methods to identify more entities of each type; (Shen et al., 2017) employ Deep Active Learning to efficiently select the set of samples for labeling, thus greatly reduce the annotation budget; (Ren et al., 2015; Shang et al., 2018; Fries et al., 2017; Yang et al., 2018b; Jie et al., 2019) use partially annotated data or external resources such as NE dictionary, knowledge base, POS tags as a replacement of hand-labeled data to train distant supervision systems. However, these methods still have certain requirements for annotation resources. Unsupervised models have achieved excellent results in the fields of part-of-speech induction (Lin et al., 2015; Stratos et al., 2016), dependency parsing (He et al., 2018; Pate and Johnson, 2016), etc. Whereas the development of unsupervised NE recognition is still kept unsatisfacto"
2020.emnlp-main.723,D18-1309,0,0.0491275,"Missing"
2020.emnlp-main.723,P18-4013,0,0.0536712,"ng task that intends to identify words or phrases that contain the names of PER (Person), ORG (Organization), LOC (Location), etc. Recent advances in deep neural models allow us to build reliable NE recognition systems (Lample et al., 2016; Ma and Hovy, 2016; Liu et al., ∗ Corresponding author. This paper was partially supported by National Key Research and Development Program of China (No. 2017YFB0304100), Key Projects of National Natural Science Foundation of China (U1836222 and 61733011), Huawei-SJTU long term AI project, Cutting-edge Machine reading comprehension and language model. 2018; Yang and Zhang, 2018; Luo et al., 2020; Luo and Zhao, 2020). However, these existing methods require large amounts of manually annotated data for training supervised models. There have been efforts to deal with the lack of annotation data in NE recognition, (Talukdar and Pereira, 2010) train a weak supervision model and use label propagation methods to identify more entities of each type; (Shen et al., 2017) employ Deep Active Learning to efficiently select the set of samples for labeling, thus greatly reduce the annotation budget; (Ren et al., 2015; Shang et al., 2018; Fries et al., 2017; Yang et al., 2018b; Jie"
2020.emnlp-main.723,C18-1183,0,0.211228,"8; Yang and Zhang, 2018; Luo et al., 2020; Luo and Zhao, 2020). However, these existing methods require large amounts of manually annotated data for training supervised models. There have been efforts to deal with the lack of annotation data in NE recognition, (Talukdar and Pereira, 2010) train a weak supervision model and use label propagation methods to identify more entities of each type; (Shen et al., 2017) employ Deep Active Learning to efficiently select the set of samples for labeling, thus greatly reduce the annotation budget; (Ren et al., 2015; Shang et al., 2018; Fries et al., 2017; Yang et al., 2018b; Jie et al., 2019) use partially annotated data or external resources such as NE dictionary, knowledge base, POS tags as a replacement of hand-labeled data to train distant supervision systems. However, these methods still have certain requirements for annotation resources. Unsupervised models have achieved excellent results in the fields of part-of-speech induction (Lin et al., 2015; Stratos et al., 2016), dependency parsing (He et al., 2018; Pate and Johnson, 2016), etc. Whereas the development of unsupervised NE recognition is still kept unsatisfactory. (Liu et al., 2019) design a Knowled"
2020.emnlp-main.723,P19-1154,1,0.583374,"Missing"
2020.emnlp-main.723,Q16-1018,0,0.0233852,", 2017) employ Deep Active Learning to efficiently select the set of samples for labeling, thus greatly reduce the annotation budget; (Ren et al., 2015; Shang et al., 2018; Fries et al., 2017; Yang et al., 2018b; Jie et al., 2019) use partially annotated data or external resources such as NE dictionary, knowledge base, POS tags as a replacement of hand-labeled data to train distant supervision systems. However, these methods still have certain requirements for annotation resources. Unsupervised models have achieved excellent results in the fields of part-of-speech induction (Lin et al., 2015; Stratos et al., 2016), dependency parsing (He et al., 2018; Pate and Johnson, 2016), etc. Whereas the development of unsupervised NE recognition is still kept unsatisfactory. (Liu et al., 2019) design a Knowledge-Augmented Language Model for unsupervised NE recognition, they perform NE recognition by controlling whether a particular word is modeled as a general word or as a reference to an entity in the training of language models. However, their model still requires type-specific entity vocabularies for computing the type probabilities and the probability of the word under given type. Early unsupervised NE system"
2020.emnlp-main.723,P10-1149,0,0.0339814,"Missing"
2020.emnlp-main.723,W02-2024,0,0.118924,"ratively. In each round, the instance selector first selects sentences from the training data, and then the positive sentences are used to train the NE tagger, the tagger updates the reward to the selector to optimize the policy function. Different from the work of (Yang et al., 2018b), we relabel the negative sentences by the NE tagger after each round, and merge them with the positive sentences for the next selection. 4 Experiments We conduct experiments 2 on two standard NER datasets: CoNLL 2003 English dataset (Tjong Kim Sang and De Meulder, 2003) and CoNLL 2002 Spanish dataset (Tjong Kim Sang, 2002) that consist of news articles. These datasets contain four entity types: LOC (location), MISC (miscellaneous), ORG (organization), and PER (person). We adopt the standard data splitting and use the micro-averaged F1 score as the evaluation metric. 4.1 Setup Pre-trained Word Embeddings. For the CoNLL 2003 dataset, we use the pre-trained 50D SENNA embeddings released by (Collobert et al., 2011) and 100D GloVe (Pennington et al., 2014) embeddings for clustering and training, respectively. For CoNLL 2002 Spanish dataset, we train 64D GloVe embeddings with the minimum frequency of occurrence as 5,"
2020.emnlp-main.723,W04-1216,0,0.0980505,"hbors, since NEs are more similar in position in the corpus and syntactically and semantically related. Based on the discoveries above, we perform K-Means clustering algorithm on the word embeddings of the whole vocabulary. According to the clusters, we assign words in the cluster with fewer words 1 tags, and the other cluster 0 tags (according to the statics of (Jie et al., 2019), the proportion of NEs is very small on CoNLL datasets), and generate a coarse NE dictionary using the words with 1 tag. 2.2 Gaussian HMM Hidden Markov model is a classic model for NE recognition (Zhou and Su, 2002; Zhao, 2004), since hidden transition matrix exists in the IOB format of the NE labels (Sarkar, 2015). We follow the Gaussian hidden Markov model introduced by (Lin et al., 2015; He et al., 2018). Given a sentence of length l, we denote the latent NE labels as z = {zi }li=1 , the cluster embeddings as v = {vi }li=1 , observed (pre-trained) word embeddings as x = {xi }li=1 , transition parameters as θ. The joint distribution of observations and latent labels is given as following: p(z, x, v; θ) = l Y p(zi |zi−1 ; θ)p(xi |zi )p(vi |zi ) i=1 (1) where p(zi |zi−1 ; θ) is the multinomial transition probability"
2020.emnlp-main.723,P02-1060,0,0.105077,"ly to be their neighbors, since NEs are more similar in position in the corpus and syntactically and semantically related. Based on the discoveries above, we perform K-Means clustering algorithm on the word embeddings of the whole vocabulary. According to the clusters, we assign words in the cluster with fewer words 1 tags, and the other cluster 0 tags (according to the statics of (Jie et al., 2019), the proportion of NEs is very small on CoNLL datasets), and generate a coarse NE dictionary using the words with 1 tag. 2.2 Gaussian HMM Hidden Markov model is a classic model for NE recognition (Zhou and Su, 2002; Zhao, 2004), since hidden transition matrix exists in the IOB format of the NE labels (Sarkar, 2015). We follow the Gaussian hidden Markov model introduced by (Lin et al., 2015; He et al., 2018). Given a sentence of length l, we denote the latent NE labels as z = {zi }li=1 , the cluster embeddings as v = {vi }li=1 , observed (pre-trained) word embeddings as x = {xi }li=1 , transition parameters as θ. The joint distribution of observations and latent labels is given as following: p(z, x, v; θ) = l Y p(zi |zi−1 ; θ)p(xi |zi )p(vi |zi ) i=1 (1) where p(zi |zi−1 ; θ) is the multinomial transitio"
2020.emnlp-main.723,P19-1298,1,0.831585,"rs and simpler training phase, thus there is no guarantee that the language model will retain its key properties when it is reduced to low dimensions. We further add the pre-trained language model BERT as the additional embeddings for the NE Tagger to refine the output of Gaussian-HMM and DAGMM, which slightly improves our result to 69.99 for CoNLL-2003 English NER and 56.66 for CoNLL-2002 Spanish NER. 5 Related work Deep neural network models have helped peoples released from handcrafted features in a wide range of NLP tasks (Zhang et al., 2019; Li et al., 2018a,b, 2019; Zhou and Zhao, 2019; Xiao et al., 2019; Zhang et al., 2020a,b,c). LSTM-CRF (Lample et al., 2016; Ma and Hovy, 2016) is the most stateof-the-art model for NE recognition. In order to reduce the requirements of training corpus, distant supervised models (Shang et al., 2018; Yang et al., 2018b; Ren et al., 2015; He, 2017; Fries et al., 2017) have been applied to NE recognition. Recently, (Liu et al., 2019) proposed a KnowledgeAugmented Language Model, which trains language models and at the same time compute the probability of the next word being different entity types according to the context given type-specific entity/general vocab"
2020.emnlp-main.723,C18-1327,0,0.0845134,"8; Yang and Zhang, 2018; Luo et al., 2020; Luo and Zhao, 2020). However, these existing methods require large amounts of manually annotated data for training supervised models. There have been efforts to deal with the lack of annotation data in NE recognition, (Talukdar and Pereira, 2010) train a weak supervision model and use label propagation methods to identify more entities of each type; (Shen et al., 2017) employ Deep Active Learning to efficiently select the set of samples for labeling, thus greatly reduce the annotation budget; (Ren et al., 2015; Shang et al., 2018; Fries et al., 2017; Yang et al., 2018b; Jie et al., 2019) use partially annotated data or external resources such as NE dictionary, knowledge base, POS tags as a replacement of hand-labeled data to train distant supervision systems. However, these methods still have certain requirements for annotation resources. Unsupervised models have achieved excellent results in the fields of part-of-speech induction (Lin et al., 2015; Stratos et al., 2016), dependency parsing (He et al., 2018; Pate and Johnson, 2016), etc. Whereas the development of unsupervised NE recognition is still kept unsatisfactory. (Liu et al., 2019) design a Knowled"
2020.emnlp-main.723,P19-1230,1,0.642342,"s have fewer parameters and simpler training phase, thus there is no guarantee that the language model will retain its key properties when it is reduced to low dimensions. We further add the pre-trained language model BERT as the additional embeddings for the NE Tagger to refine the output of Gaussian-HMM and DAGMM, which slightly improves our result to 69.99 for CoNLL-2003 English NER and 56.66 for CoNLL-2002 Spanish NER. 5 Related work Deep neural network models have helped peoples released from handcrafted features in a wide range of NLP tasks (Zhang et al., 2019; Li et al., 2018a,b, 2019; Zhou and Zhao, 2019; Xiao et al., 2019; Zhang et al., 2020a,b,c). LSTM-CRF (Lample et al., 2016; Ma and Hovy, 2016) is the most stateof-the-art model for NE recognition. In order to reduce the requirements of training corpus, distant supervised models (Shang et al., 2018; Yang et al., 2018b; Ren et al., 2015; He, 2017; Fries et al., 2017) have been applied to NE recognition. Recently, (Liu et al., 2019) proposed a KnowledgeAugmented Language Model, which trains language models and at the same time compute the probability of the next word being different entity types according to the context given type-specific e"
2020.findings-emnlp.102,W19-3712,0,0.0205236,"Missing"
2020.findings-emnlp.102,W09-1206,0,0.124486,"Missing"
2020.findings-emnlp.102,D17-1003,0,0.0215483,"ctorized baseline models on all 7 languages High-order parsing is one of the research hotspots in which first-order parsers meet performance bottlenecks; this has been extensively studied in the literature of syntactic dependency parsing(McDonald et al., 2005; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010; Martins et al., 2011; Ma and Zhao, 2012; Gormley et al., 2015; Zhang et al., 2020). In semantic parsing, Martins and Almeida (2014) proposed a way to encode high-order parts with hand-crafted features and introduced a novel co-parent part for semantic dependency parsing. Cao et al. (2017) proposed a quasi-second-order semantic dependency parser with dynamic programming. Wang et al. (2019) trained a second-order parser in an end-toend manner with the help of mean field variational inference and loopy belief propagation approximation. In SRL or related research field, there is also some related work on the improvement of performance by high-order structural information. On the Japanese NAIST Predict-Argument Structure (PAS) dataset, some works (Yoshikawa et al., 2011; Ouchi et al., 2015; Iida et al., 2015; Shibata et al., 2016; Ouchi et al., 2017; Matsubayashi and Inui, 2018) ma"
2020.findings-emnlp.102,D07-1101,0,0.5658,"rm AI Project, Cuttingedge Machine Reading Comprehension and Language Model. Rui Wang was partially supported by JSPS grant-in-aid for early-career scientists (19K20354): “Unsupervised Neural Machine Translation in Universal Scenarios” and NICT tenuretrack researcher startup fund “Toward Intelligent Machine Translation”. arcs. The types of features that the model can exploit in the inference depend on the information included in the factorized parts. Before the introduction of deep neural networks, in syntactic parsing (a kind of linguistic parsing), several works (McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010; Zhang and McDonald, 2012; Ma and Zhao, 2012) showed that high-order parsers utilizing richer factorization information achieve higher accuracy than low-order ones due to the extensive decision history that can lead to significant improvements in inference (Chen et al., 2010). Semantic role labeling (SRL) (Gildea and Jurafsky, 2002; Zhao and Kit, 2008; Zhao et al., 2009b, 2013) captures the predicate-argument structure of a given sentence, and it is defined as a shallow semantic parsing task, which is also a typical linguistic parsing task. Recent high-performing SRL mo"
2020.findings-emnlp.102,C10-2015,0,0.0284524,"gent Machine Translation”. arcs. The types of features that the model can exploit in the inference depend on the information included in the factorized parts. Before the introduction of deep neural networks, in syntactic parsing (a kind of linguistic parsing), several works (McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010; Zhang and McDonald, 2012; Ma and Zhao, 2012) showed that high-order parsers utilizing richer factorization information achieve higher accuracy than low-order ones due to the extensive decision history that can lead to significant improvements in inference (Chen et al., 2010). Semantic role labeling (SRL) (Gildea and Jurafsky, 2002; Zhao and Kit, 2008; Zhao et al., 2009b, 2013) captures the predicate-argument structure of a given sentence, and it is defined as a shallow semantic parsing task, which is also a typical linguistic parsing task. Recent high-performing SRL models (He et al., 2017; Marcheggiani et al., 2017; He et al., 2018a; Strubell et al., 2018; He et al., 2018b; Cai et al., 2018), whether labeling arguments for a single predicate using sequence tagging model at a time or classifying the candidate predicateargument pairs, are (mainly) belong to first-"
2020.findings-emnlp.102,D19-1544,0,0.0301468,"Missing"
2020.findings-emnlp.102,N19-1423,0,0.00658232,"4 90.54 90.75 91.60 91.74 System German Zhao et al. (2009a) Lyu et al. (2019) Our baseline +HO Our baseline+B +HO+B Table 3: Precision, Recall, and Semantic-F 1 scores on German and Czech out-of-domain test sets. strategy from (Dozat and Manning, 2017; Wang et al., 2019). Please refer to Appendix A.1 for details. 4.2 Results And Analysis Main Results3 Table 1 presents the results on the standard English test set, WSJ (in-domain) and Brown (out-of-domain). For a fair comparison with previous works, we report three cases: not using pre-training, using ELMo (Peters et al., 2018), and using BERT (Devlin et al., 2019). Our single model achieves the best performance on the in-domain test set without syntactic information and extra resources for both types of setup, w/ and w/o preidentified predicate. On the out-of-domain test set, even though Zhou et al. (2019) obtains the highest score, their model is joint and likely achieves domain adaptation due to external tasks and resources. In general, our model achieves significant performance improvements in both in-domain and out-of-domain settings, especially while using pretraining out-of-domain. Furthermore, the results of using ELMo and BERT show that the str"
2020.findings-emnlp.102,J02-3001,0,0.0299432,"ures that the model can exploit in the inference depend on the information included in the factorized parts. Before the introduction of deep neural networks, in syntactic parsing (a kind of linguistic parsing), several works (McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010; Zhang and McDonald, 2012; Ma and Zhao, 2012) showed that high-order parsers utilizing richer factorization information achieve higher accuracy than low-order ones due to the extensive decision history that can lead to significant improvements in inference (Chen et al., 2010). Semantic role labeling (SRL) (Gildea and Jurafsky, 2002; Zhao and Kit, 2008; Zhao et al., 2009b, 2013) captures the predicate-argument structure of a given sentence, and it is defined as a shallow semantic parsing task, which is also a typical linguistic parsing task. Recent high-performing SRL models (He et al., 2017; Marcheggiani et al., 2017; He et al., 2018a; Strubell et al., 2018; He et al., 2018b; Cai et al., 2018), whether labeling arguments for a single predicate using sequence tagging model at a time or classifying the candidate predicateargument pairs, are (mainly) belong to first-order parsers. High-order information is an overlooked po"
2020.findings-emnlp.102,Q15-1035,0,0.0131809,"with special focus on the impact of syntax and contextualized word representations and achieved new state-ofthe-art results on the CoNLL-2009 benchmarks of all languages, resulting in an effective model and outperforming strong factorized baseline models on all 7 languages High-order parsing is one of the research hotspots in which first-order parsers meet performance bottlenecks; this has been extensively studied in the literature of syntactic dependency parsing(McDonald et al., 2005; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010; Martins et al., 2011; Ma and Zhao, 2012; Gormley et al., 2015; Zhang et al., 2020). In semantic parsing, Martins and Almeida (2014) proposed a way to encode high-order parts with hand-crafted features and introduced a novel co-parent part for semantic dependency parsing. Cao et al. (2017) proposed a quasi-second-order semantic dependency parser with dynamic programming. Wang et al. (2019) trained a second-order parser in an end-toend manner with the help of mean field variational inference and loopy belief propagation approximation. In SRL or related research field, there is also some related work on the improvement of performance by high-order structur"
2020.findings-emnlp.102,L18-1550,0,0.0356114,"Missing"
2020.findings-emnlp.102,P18-2058,0,0.168978,"Ma and Zhao, 2012) showed that high-order parsers utilizing richer factorization information achieve higher accuracy than low-order ones due to the extensive decision history that can lead to significant improvements in inference (Chen et al., 2010). Semantic role labeling (SRL) (Gildea and Jurafsky, 2002; Zhao and Kit, 2008; Zhao et al., 2009b, 2013) captures the predicate-argument structure of a given sentence, and it is defined as a shallow semantic parsing task, which is also a typical linguistic parsing task. Recent high-performing SRL models (He et al., 2017; Marcheggiani et al., 2017; He et al., 2018a; Strubell et al., 2018; He et al., 2018b; Cai et al., 2018), whether labeling arguments for a single predicate using sequence tagging model at a time or classifying the candidate predicateargument pairs, are (mainly) belong to first-order parsers. High-order information is an overlooked potential performance enhancer; however, it does suffer from an enormous spatial complexity and an expensive time cost in the inference stage. As a result, most of the previous algorithms for highorder syntactic dependency tree parsing are not directly applicable to neural parsing. In addition, the target of"
2020.findings-emnlp.102,P17-1044,0,0.0338225,"and Collins, 2010; Zhang and McDonald, 2012; Ma and Zhao, 2012) showed that high-order parsers utilizing richer factorization information achieve higher accuracy than low-order ones due to the extensive decision history that can lead to significant improvements in inference (Chen et al., 2010). Semantic role labeling (SRL) (Gildea and Jurafsky, 2002; Zhao and Kit, 2008; Zhao et al., 2009b, 2013) captures the predicate-argument structure of a given sentence, and it is defined as a shallow semantic parsing task, which is also a typical linguistic parsing task. Recent high-performing SRL models (He et al., 2017; Marcheggiani et al., 2017; He et al., 2018a; Strubell et al., 2018; He et al., 2018b; Cai et al., 2018), whether labeling arguments for a single predicate using sequence tagging model at a time or classifying the candidate predicateargument pairs, are (mainly) belong to first-order parsers. High-order information is an overlooked potential performance enhancer; however, it does suffer from an enormous spatial complexity and an expensive time cost in the inference stage. As a result, most of the previous algorithms for highorder syntactic dependency tree parsing are not directly applicable to"
2020.findings-emnlp.102,D19-1538,1,0.876976,"el}. (cop−dep) , hj (gp−dep) , hj (gp−head) , hk ), where the weight matrix U2nd is (d × (d + 1) × (d + 1))-dimensional. 3.4 k6=i,j (t−1) (gp) Sk→i→j +Qk,i ( arc + G (t−1) ), Arc i → j exist exp(Si→j i,j = 1, Otherwise (t−1) where Gi,j (0) Qi,k (t−1) (gp) + Qj,k Si→j→k }, is the second-order voting scores, arc ), and t is the updating step. = softmax(Si,j Zheng et al. (2015) stated that multiple meanfield update iterations can be implemented by stacking Recurrent Neural Network (RNN) layers, as Pre-training w/ pre-identified predicate Cai et al. (2018) Kasai et al. (2019)∗ Zhou et al. (2019)† He et al. (2019)∗ Ours Variational Inference Layers In the first-order model, we adopt the negative likelihood of the golden structure as the loss to train the model, but in the second-order module of our proposed model, a similar approach will encounter the sparsity problem, as the maximum likelihood estimates cannot be obtained when the number of trainable variables is much larger than the number of observations. In other words, it is not feasible to directly approximate the real distribution with the output distribution of the second-order scorer because of the sparsity of the real distribution. Computing"
2020.findings-emnlp.102,P18-1192,1,0.780741,"Ma and Zhao, 2012) showed that high-order parsers utilizing richer factorization information achieve higher accuracy than low-order ones due to the extensive decision history that can lead to significant improvements in inference (Chen et al., 2010). Semantic role labeling (SRL) (Gildea and Jurafsky, 2002; Zhao and Kit, 2008; Zhao et al., 2009b, 2013) captures the predicate-argument structure of a given sentence, and it is defined as a shallow semantic parsing task, which is also a typical linguistic parsing task. Recent high-performing SRL models (He et al., 2017; Marcheggiani et al., 2017; He et al., 2018a; Strubell et al., 2018; He et al., 2018b; Cai et al., 2018), whether labeling arguments for a single predicate using sequence tagging model at a time or classifying the candidate predicateargument pairs, are (mainly) belong to first-order parsers. High-order information is an overlooked potential performance enhancer; however, it does suffer from an enormous spatial complexity and an expensive time cost in the inference stage. As a result, most of the previous algorithms for highorder syntactic dependency tree parsing are not directly applicable to neural parsing. In addition, the target of"
2020.findings-emnlp.102,W07-1522,0,0.0152328,"L example presented in right part of Figure 1, our second-order SRL model looks at several pairs of arcs: • sibling (Smith and Eisner, 2008; Martins et al., 2009): arguments of the same predicate; • co-parents (Martins and Almeida, 2014): predicates sharing the same argument; • grandparent (Carreras, 2007): predicate that is the argument of another predicate. Though some high-order structures have been studied by some related works (Yoshikawa et al., 2011; Ouchi et al., 2015; Shibata et al., 2016; Ouchi et al., 2017; Matsubayashi and Inui, 2018) in Japanese Predicate Argument Structure (PAS) (Iida et al., 2007) analysis and English SRL (Yang and Zong, 2014), the integration of multiple high-order structures into a single framework and exploring the high-order effects on multiple languages, different high-order structure combinations in a comprehensive way on popular CoNLL-2009 benchmark is the first considered in this paper and thus takes the shape of the main novelties of our work. 3 3.1 Model Overview SRL can be decomposed into four subtasks: predicate identification, predicate disambiguation, argument identification, and argument classification. Since the CoNLL-2009 shared task identified all pre"
2020.findings-emnlp.102,C18-1271,1,0.825621,"semantic role. For simple sentences (without HO), the baseline can already parse it very well, which also explains the reason why the improvement in some languages is not great. 5 Related Work The CoNLL-2009 shared task advocated performing SRL for multiple languages to promote multilingual NLP applications. (Zhao et al., 2009a) proposed an integrated approach by exploiting large1141 scale feature sets, while (Bj¨orkelund et al., 2009) used a generic feature selection procedure, which yielded significant gains in the multilingual SRL shared task. With the development of deep neural networks (Li et al., 2018a; Xiao et al., 2019; Zhou and Zhao, 2019; Zhang et al., 2019c,a; Li et al., 2019c; Luo et al., 2020; Li et al., 2019b; Zhang et al., 2019b) for NLP, most subsequent SRL works have focused on improving the performance of English, with occasional comparisons to other languages (Lei et al., 2015; Swayamdipta et al., 2016; Roth and Lapata, 2016; Marcheggiani et al., 2017; He et al., 2018b; Li et al., 2018b; Cai et al., 2018). Mulcaire et al. (2018) built a polyglot semantic role labeling system by combining resources from all languages in the CoNLL2009 shared task for exploiting the similarities"
2020.findings-emnlp.102,D15-1260,0,0.0357786,"Missing"
2020.findings-emnlp.102,D18-1262,1,0.868122,"semantic role. For simple sentences (without HO), the baseline can already parse it very well, which also explains the reason why the improvement in some languages is not great. 5 Related Work The CoNLL-2009 shared task advocated performing SRL for multiple languages to promote multilingual NLP applications. (Zhao et al., 2009a) proposed an integrated approach by exploiting large1141 scale feature sets, while (Bj¨orkelund et al., 2009) used a generic feature selection procedure, which yielded significant gains in the multilingual SRL shared task. With the development of deep neural networks (Li et al., 2018a; Xiao et al., 2019; Zhou and Zhao, 2019; Zhang et al., 2019c,a; Li et al., 2019c; Luo et al., 2020; Li et al., 2019b; Zhang et al., 2019b) for NLP, most subsequent SRL works have focused on improving the performance of English, with occasional comparisons to other languages (Lei et al., 2015; Swayamdipta et al., 2016; Roth and Lapata, 2016; Marcheggiani et al., 2017; He et al., 2018b; Li et al., 2018b; Cai et al., 2018). Mulcaire et al. (2018) built a polyglot semantic role labeling system by combining resources from all languages in the CoNLL2009 shared task for exploiting the similarities"
2020.findings-emnlp.102,N19-1075,0,0.148608,"Missing"
2020.findings-emnlp.102,W04-3250,0,0.0346504,"Missing"
2020.findings-emnlp.102,K19-2004,1,0.880526,"Missing"
2020.findings-emnlp.102,P10-1001,0,0.441175,"uttingedge Machine Reading Comprehension and Language Model. Rui Wang was partially supported by JSPS grant-in-aid for early-career scientists (19K20354): “Unsupervised Neural Machine Translation in Universal Scenarios” and NICT tenuretrack researcher startup fund “Toward Intelligent Machine Translation”. arcs. The types of features that the model can exploit in the inference depend on the information included in the factorized parts. Before the introduction of deep neural networks, in syntactic parsing (a kind of linguistic parsing), several works (McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010; Zhang and McDonald, 2012; Ma and Zhao, 2012) showed that high-order parsers utilizing richer factorization information achieve higher accuracy than low-order ones due to the extensive decision history that can lead to significant improvements in inference (Chen et al., 2010). Semantic role labeling (SRL) (Gildea and Jurafsky, 2002; Zhao and Kit, 2008; Zhao et al., 2009b, 2013) captures the predicate-argument structure of a given sentence, and it is defined as a shallow semantic parsing task, which is also a typical linguistic parsing task. Recent high-performing SRL models (He et al., 2017;"
2020.findings-emnlp.102,N18-2108,0,0.0519954,"Missing"
2020.findings-emnlp.102,N15-1121,0,0.049307,"Missing"
2020.findings-emnlp.102,P81-1022,0,0.452873,"Missing"
2020.findings-emnlp.102,D11-1022,0,0.0256301,"019) boosted multilingual SRL performance with special focus on the impact of syntax and contextualized word representations and achieved new state-ofthe-art results on the CoNLL-2009 benchmarks of all languages, resulting in an effective model and outperforming strong factorized baseline models on all 7 languages High-order parsing is one of the research hotspots in which first-order parsers meet performance bottlenecks; this has been extensively studied in the literature of syntactic dependency parsing(McDonald et al., 2005; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010; Martins et al., 2011; Ma and Zhao, 2012; Gormley et al., 2015; Zhang et al., 2020). In semantic parsing, Martins and Almeida (2014) proposed a way to encode high-order parts with hand-crafted features and introduced a novel co-parent part for semantic dependency parsing. Cao et al. (2017) proposed a quasi-second-order semantic dependency parser with dynamic programming. Wang et al. (2019) trained a second-order parser in an end-toend manner with the help of mean field variational inference and loopy belief propagation approximation. In SRL or related research field, there is also some related work on the improvem"
2020.findings-emnlp.102,P09-1039,0,0.0550447,"the neural network model. Given an input sentence with length L, order J of parsing model, the memory required is O(LJ+1 ). In the current GPU memory conditions, second-order J = 2 is the upper limit that can be explored in practice if without pruning. Therefore, we enumerate all three second-order structures as objects of study in SRL, as shown in the left part of Figure 1, namely sibling (sib), co-parents (cop), and grandparent (gp). As shown in the SRL example presented in right part of Figure 1, our second-order SRL model looks at several pairs of arcs: • sibling (Smith and Eisner, 2008; Martins et al., 2009): arguments of the same predicate; • co-parents (Martins and Almeida, 2014): predicates sharing the same argument; • grandparent (Carreras, 2007): predicate that is the argument of another predicate. Though some high-order structures have been studied by some related works (Yoshikawa et al., 2011; Ouchi et al., 2015; Shibata et al., 2016; Ouchi et al., 2017; Matsubayashi and Inui, 2018) in Japanese Predicate Argument Structure (PAS) (Iida et al., 2007) analysis and English SRL (Yang and Zong, 2014), the integration of multiple high-order structures into a single framework and exploring the hig"
2020.findings-emnlp.102,S14-2082,0,0.0170798,"and achieved new state-ofthe-art results on the CoNLL-2009 benchmarks of all languages, resulting in an effective model and outperforming strong factorized baseline models on all 7 languages High-order parsing is one of the research hotspots in which first-order parsers meet performance bottlenecks; this has been extensively studied in the literature of syntactic dependency parsing(McDonald et al., 2005; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010; Martins et al., 2011; Ma and Zhao, 2012; Gormley et al., 2015; Zhang et al., 2020). In semantic parsing, Martins and Almeida (2014) proposed a way to encode high-order parts with hand-crafted features and introduced a novel co-parent part for semantic dependency parsing. Cao et al. (2017) proposed a quasi-second-order semantic dependency parser with dynamic programming. Wang et al. (2019) trained a second-order parser in an end-toend manner with the help of mean field variational inference and loopy belief propagation approximation. In SRL or related research field, there is also some related work on the improvement of performance by high-order structural information. On the Japanese NAIST Predict-Argument Structure (PAS)"
2020.findings-emnlp.102,D14-1162,0,0.0877038,"Missing"
2020.findings-emnlp.102,N18-1202,0,0.0330803,".88 90.03 91.23 91.61 82.66 87.20 86.04 90.54 90.75 91.60 91.74 System German Zhao et al. (2009a) Lyu et al. (2019) Our baseline +HO Our baseline+B +HO+B Table 3: Precision, Recall, and Semantic-F 1 scores on German and Czech out-of-domain test sets. strategy from (Dozat and Manning, 2017; Wang et al., 2019). Please refer to Appendix A.1 for details. 4.2 Results And Analysis Main Results3 Table 1 presents the results on the standard English test set, WSJ (in-domain) and Brown (out-of-domain). For a fair comparison with previous works, we report three cases: not using pre-training, using ELMo (Peters et al., 2018), and using BERT (Devlin et al., 2019). Our single model achieves the best performance on the in-domain test set without syntactic information and extra resources for both types of setup, w/ and w/o preidentified predicate. On the out-of-domain test set, even though Zhou et al. (2019) obtains the highest score, their model is joint and likely achieves domain adaptation due to external tasks and resources. In general, our model achieves significant performance improvements in both in-domain and out-of-domain settings, especially while using pretraining out-of-domain. Furthermore, the results of"
2020.findings-emnlp.371,abdelali-etal-2014-amara,0,0.0286749,"erence agreement. Following previous studies, newstest 2016 was used to evaluate the en-ro language pair. For fr-ro, we sampled 5K sentence pairs from OPUS (Tiedemann, 2012) for evaluation, while for zh-ro, we use the religious and educational parallel data for out-of-domain evaluation and collected 2K news parallel sentences for in-domain evaluation. In detail, as data for fr-ro, we used GlobalVoices2 , OpenSubtitles (Lison and Tiedemann, 2016), and MultiParaCrawl3 , whereas for zh-ro, Bible-uedin (Christodouloupoulos and Steedman, 2015), Tanzil, and the QCRI Educational Domain Corpus (QED) (Abdelali et al., 2014) were used. Because these parallel corpora between zh-ro are in religious and educational domains only, which are far away from the news domain of training data, we also collected a parallel corpus (2K in size) of zh-ro for in-domain evaluation. The Moses scripts (Koehn and Knowles, 2017) were used for tokenization of en, fr, and ro, and the jieba toolkit4 was used for word segmentation on zh. In particular, following Sennrich et al. (2016), we removed diacritics from ro. For zh, to avoid confusion between Hong Kong Standard Traditional Chinese (zh hk: QED), Taiwan Standard Traditional Chinese"
2020.findings-emnlp.371,N19-1388,0,0.0312615,"2005)). PBSMT + NMT: (Lample et al., 2018b), XLM: (Conneau and Lample, 2019), MASS: (Song et al., 2019). In the form x[y], x and y respectively indicate results on in-domain and out-of-domain sets. Note, the BLEU used in ro→zh is based on Chinese words segmented by the jieba toolkit. en-ro fr-ro zh-ro en-fr-ro en-zh-ro en fr ro zh 6.5 / 64.3 6.9 / 60.1 7.4 / 53.8 4.1 / 68.7 4.2 / 68.4 - 4.9 / 68.3 4.9 / 68.5 5.3 / 65.8 5.0 / 68.1 5.5 / 64.9 11.5 / 52.9 11.4 / 53.4 Table 2: Perplexity / Accuracy for masked language modeling in different languages joint pre-training. UNMT Lample et al. (2018a); Aharoni et al. (2019); Song et al. (2019) have demonstrated the importance of pre-training, which is a key ingredient of UNMT. Conneau and Lample (2019) used masked language modeling (MLM) to pretrain the full model for the initialization step before applying a denoising autoencoder and BT training step. Therefore, we take the XLM architecture proposed by Conneau and Lample (2019) as our backbone baseline model. MUNMT Our method studies the impact of adding a reference language to the existing UNMT language pair, which makes our model essentially multilingual. Therefore, MUNMT is the baseline for comparison. We ad"
2020.findings-emnlp.371,N19-1121,0,0.059622,"irs, including unseen language pairs, transfer learning should be considered when low-resource languages are trained together with rich-resource ones. As discussed by Arivazhagan et al. (2019), MUNMT usually performs worse than pivot-based supervised NMT; however, the pivot-based method easily experiences a computationally expensive quadratic growth in the number of source languages and suffers from the error propagation problem. Arivazhagan et al. (2019) addressed the zeroshot generalization problem that some translation directions have not been optimized well due to a lack of parallel data. Al-Shedivat and Parikh (2019) introduced a consistent agreement-based training method that encourages the model to produce equivalent translations of parallel sentences in zero-shot translation, which share similarities with our RAT approach. However, in terms of a specific implementation, because of the differences between UNMT and NMT, we have provided three new UNMT methods, and have alleviated the problem of uncontrollable intermediate BT quality in UNMT. Arivazhagan et al. (2019) addressed the issue of transfer learning between language pairs with parallel data where there is a lack of parallel corpora in multilingua"
2020.findings-emnlp.371,P17-1042,0,0.0856606,"Missing"
2020.findings-emnlp.371,1981.tc-1.7,0,0.709837,"Missing"
2020.findings-emnlp.371,C18-1233,1,0.900017,"Missing"
2020.findings-emnlp.371,P05-1066,0,0.0305915,"Missing"
2020.findings-emnlp.371,2020.findings-emnlp.283,0,0.202912,"ize is set to 60K, and the model hyperparameters are consistent with those of XLM. The smoothing value  in RAT is set to 0.1. 4.3 Main Results and Analysis This section examines the effectiveness of the proposed RUNMT framework6 . The main results7 are presented in Table 1. Row #4 reports the replicated results of the XLM architecture (Conneau and Lample, 2019) based on the training of each language pair individually. Our UNMT basically reproduces XLM’s results, and it also 6 Code available at https://github.com/ bcmi220/runmt. 7 Notably, concurrent works (Liu et al., 2020; Bai et al., 2020; Garcia et al., 2020) also explore the case of using auxiliary parallel data effects under the MUNMT setting, where all of these works share similarities in multilingualism motivation. Due to the inconsistency of the parallel corpora used, the results are not directly comparable, so we don’t include their results in the table. 4156 makes some improvements over the original (probably because of differences in data sampling). Thus, our approach offers a strong baseline performance. Compared with the current stateof-the-art method MASS (Song et al., 2019), our baseline performance is slightly lower. This is because M"
2020.findings-emnlp.371,P18-1192,1,0.906335,"Missing"
2020.findings-emnlp.371,D19-1080,0,0.0441941,"Missing"
2020.findings-emnlp.371,W17-3204,0,0.0440239,"and collected 2K news parallel sentences for in-domain evaluation. In detail, as data for fr-ro, we used GlobalVoices2 , OpenSubtitles (Lison and Tiedemann, 2016), and MultiParaCrawl3 , whereas for zh-ro, Bible-uedin (Christodouloupoulos and Steedman, 2015), Tanzil, and the QCRI Educational Domain Corpus (QED) (Abdelali et al., 2014) were used. Because these parallel corpora between zh-ro are in religious and educational domains only, which are far away from the news domain of training data, we also collected a parallel corpus (2K in size) of zh-ro for in-domain evaluation. The Moses scripts (Koehn and Knowles, 2017) were used for tokenization of en, fr, and ro, and the jieba toolkit4 was used for word segmentation on zh. In particular, following Sennrich et al. (2016), we removed diacritics from ro. For zh, to avoid confusion between Hong Kong Standard Traditional Chinese (zh hk: QED), Taiwan Standard Traditional Chinese (zh tw: Bibleuedin), and Simplified Chinese (zh: Tanzil and monolingual training data), we used opencc5 to convert zh hk and zh tw to simplified Chinese. 4.2 Baselines Our baseline models follow XLM (Conneau and Lample, 2019), with the following refinements: 4155 2 http://casmacat.eu/cor"
2020.findings-emnlp.371,J82-2005,0,0.623357,"Missing"
2020.findings-emnlp.371,P19-1017,0,0.0906893,"ich share similarities with our RAT approach. However, in terms of a specific implementation, because of the differences between UNMT and NMT, we have provided three new UNMT methods, and have alleviated the problem of uncontrollable intermediate BT quality in UNMT. Arivazhagan et al. (2019) addressed the issue of transfer learning between language pairs with parallel data where there is a lack of parallel corpora in multilingual supervised NMT. As for the agreement in UNMT, (Sun et al., 2019) investigate the enhancement of unsupervised bilingual word embedding agreement in the UNMT training. Leng et al. (2019) propose a multi-hop UNMT that automatically selects a good translation path for a distant language pair during UNMT. Baijun et al. (2019) proposed a cross-lingual pre-training approach that makes use of the source–pivot data to pre-train the language model. As for the multilingualism, Liu et al. (2020) proposes a multilingual denoising pre-training technique to improve machine translation tasks. Bai et al. (2020) and Garcia et al. (2020) both studied the agreement across language pairs. Their method is much the same as one of our proposed approaches, XBT, which relies on the supervision signa"
2020.findings-emnlp.371,D18-1262,1,0.884281,"Missing"
2020.findings-emnlp.371,L16-1147,0,0.020699,"language pair parallel dataset is about 10M. In both scenarios, we evaluated each language pair except for en-fr and en-zh, for which the relevant parallel data was used for reference agreement. Following previous studies, newstest 2016 was used to evaluate the en-ro language pair. For fr-ro, we sampled 5K sentence pairs from OPUS (Tiedemann, 2012) for evaluation, while for zh-ro, we use the religious and educational parallel data for out-of-domain evaluation and collected 2K news parallel sentences for in-domain evaluation. In detail, as data for fr-ro, we used GlobalVoices2 , OpenSubtitles (Lison and Tiedemann, 2016), and MultiParaCrawl3 , whereas for zh-ro, Bible-uedin (Christodouloupoulos and Steedman, 2015), Tanzil, and the QCRI Educational Domain Corpus (QED) (Abdelali et al., 2014) were used. Because these parallel corpora between zh-ro are in religious and educational domains only, which are far away from the news domain of training data, we also collected a parallel corpus (2K in size) of zh-ro for in-domain evaluation. The Moses scripts (Koehn and Knowles, 2017) were used for tokenization of en, fr, and ro, and the jieba toolkit4 was used for word segmentation on zh. In particular, following Sennr"
2020.findings-emnlp.371,2020.tacl-1.47,0,0.167936,"the byte pair encoding (BPE) code size is set to 60K, and the model hyperparameters are consistent with those of XLM. The smoothing value  in RAT is set to 0.1. 4.3 Main Results and Analysis This section examines the effectiveness of the proposed RUNMT framework6 . The main results7 are presented in Table 1. Row #4 reports the replicated results of the XLM architecture (Conneau and Lample, 2019) based on the training of each language pair individually. Our UNMT basically reproduces XLM’s results, and it also 6 Code available at https://github.com/ bcmi220/runmt. 7 Notably, concurrent works (Liu et al., 2020; Bai et al., 2020; Garcia et al., 2020) also explore the case of using auxiliary parallel data effects under the MUNMT setting, where all of these works share similarities in multilingualism motivation. Due to the inconsistency of the parallel corpora used, the results are not directly comparable, so we don’t include their results in the table. 4156 makes some improvements over the original (probably because of differences in data sampling). Thus, our approach offers a strong baseline performance. Compared with the current stateof-the-art method MASS (Song et al., 2019), our baseline performa"
2020.findings-emnlp.371,N09-2056,1,0.895514,"Missing"
2020.findings-emnlp.371,D18-1549,0,0.061576,"benchmarks has achieved great success (Wu et al., 2016; Gehring et al., 2017; Vaswani et al., 2017) because of advances in deep learning and the availability of large-scale parallel corpora; however, the applicability of MT systems is limited because of their reliance on large parallel corpora for the majority of language pairs. In real-world situations, the majority of language pairs have very little parallel data, although large volumes of monolingual data are available for each language. UNMT removes the dependence on parallel corpora, relying only on monolingual corpora in each language (Reddi et al., 2018; Lample et al., 2018a,b; Conneau and Lample, 2019; Li et al., 2019b). UNMT uses translation symmetry for dual learning in each language direction. Existing UNMT models are mainly built on the encoder– decoder schema. The essence of UNMT is to 4151 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 4151–4162 c November 16 - 20, 2020. 2020 Association for Computational Linguistics learn unsupervised cross-lingual word alignment and/or sentence alignment. For unsupervised word alignment, the most popular methods are word embedding mapping (Conneau et al., 2017; Lample e"
2020.findings-emnlp.371,W16-2323,0,0.058197,"2016), and MultiParaCrawl3 , whereas for zh-ro, Bible-uedin (Christodouloupoulos and Steedman, 2015), Tanzil, and the QCRI Educational Domain Corpus (QED) (Abdelali et al., 2014) were used. Because these parallel corpora between zh-ro are in religious and educational domains only, which are far away from the news domain of training data, we also collected a parallel corpus (2K in size) of zh-ro for in-domain evaluation. The Moses scripts (Koehn and Knowles, 2017) were used for tokenization of en, fr, and ro, and the jieba toolkit4 was used for word segmentation on zh. In particular, following Sennrich et al. (2016), we removed diacritics from ro. For zh, to avoid confusion between Hong Kong Standard Traditional Chinese (zh hk: QED), Taiwan Standard Traditional Chinese (zh tw: Bibleuedin), and Simplified Chinese (zh: Tanzil and monolingual training data), we used opencc5 to convert zh hk and zh tw to simplified Chinese. 4.2 Baselines Our baseline models follow XLM (Conneau and Lample, 2019), with the following refinements: 4155 2 http://casmacat.eu/corpus/global-voices.html http://paracrawl.eu 4 https://github.com/fxsjy/jieba 5 https://github.com/BYVoid/OpenCC 3 en-fr-ro en-zh-ro en→ro ro→en fr→ro ro→fr"
2020.findings-emnlp.371,P19-1119,1,0.807626,"Missing"
2020.findings-emnlp.371,2020.acl-main.324,1,0.653027,"ation models (as the generation direction is the same as the training direction), but also train the BT models, i.e., T → S and T → R. This gives the RABT training approach shown in Figure 2(c). The learning objective of RABT can be described as: LRABT (S, T , R) = L(θT →S ) + L(θT →R ). 3.4 (8) Cross-lingual Back-translation The traditional BT analyzed in Section 2 and illustrated in Figure 2(a) allows us to train a T → S model with the help of an S → T model, and vice versa; however, this mutually beneficial training is performed entirely within one language pair. Multilingual UNMT (MUNMT) (Sun et al., 2020) is a special case of UNMT that is capable of translating between multiple source and target languages. Although multiple language pairs are trained jointly in MUNMT, there is an obvious shortcoming for BT: translating between language pairs that do not occur together during training, i.e., lack of optimization across language pairs. Joint training across language pairs can be performed through forced high-order BT in UNMT, which takes the form L1 → L2 → ... → LO+1 → L1 , where O is the translation order indicating the number of bridge languages in BT. This approach may fail because decoding t"
2020.findings-emnlp.371,N07-1061,1,0.653675,"Missing"
2020.findings-emnlp.371,P07-1108,0,0.24056,"Missing"
2020.findings-emnlp.371,P19-1230,1,0.787194,"Missing"
2020.findings-emnlp.371,L16-1561,0,0.0615968,"olingual sentences as those extracted from the WMT News Crawl datasets for the period 2007–2017 by Conneau and Lample (2019) for a fair comparison and limited the maximum number of sentences in each language to 50 million(M), which results in 50M, 50M, and 14M sentences, respectively. For Chinese, we combined all of the sentences available in the WMT News Crawl datasets with the source sentences from the WMT’17 Chinese–English translation task, leading to 26M sentences. For the parallel data of en-fr and en-zh introduced by the two experimental settings, we only use those provided by MultiUN (Ziemski et al., 2016). Finally, the size of the resulting language pair parallel dataset is about 10M. In both scenarios, we evaluated each language pair except for en-fr and en-zh, for which the relevant parallel data was used for reference agreement. Following previous studies, newstest 2016 was used to evaluate the en-ro language pair. For fr-ro, we sampled 5K sentence pairs from OPUS (Tiedemann, 2012) for evaluation, while for zh-ro, we use the religious and educational parallel data for out-of-domain evaluation and collected 2K news parallel sentences for in-domain evaluation. In detail, as data for fr-ro, we"
2020.findings-emnlp.371,tiedemann-2012-parallel,0,\N,Missing
2020.findings-emnlp.398,C18-1233,1,0.922689,"Missing"
2020.findings-emnlp.398,D19-1094,0,0.0226876,"Missing"
2020.findings-emnlp.398,W05-0620,0,0.158689,"Missing"
2020.findings-emnlp.398,D19-1544,0,0.0324815,"Missing"
2020.findings-emnlp.398,N19-1423,0,0.0380258,"one for syntactic tree based on joint span syntactic parsing algorithm (Zhou and Zhao, 2019), another for uniform SRL. 3.2 Token Representation In our model, token representation xi is composed of characters, words, and part-of-speech (POS) representation. For character-level representation, we use CharLSTM (Ling et al., 2015). For word-level representation, we concatenate randomly initialized and pre-trained word embeddings. We concatenate character representation and word representation as our token representation xi =[xchar ;xword ;xP OS ]. In addition, we also augment our model with BERT (Devlin et al., 2019) or XLNet (Yang et al., 2019) as the sole token representation to compare with other pre-training models. Since BERT and XLNet are based on sub-word, we only take the last sub-word vector of the word in the last layer of BERT or XLNet as our sole token representation xi . 3.3 Self-Attention Encoder The encoder in our model is adapted from (Vaswani et al.) and factor explicit content and position information in the self-attention process. The input matrices X = [x1 , x2 , . . . , xn ] in which xi is concatenated with position embedding are transformed by a self-attention encoder. We factor the"
2020.findings-emnlp.398,N19-1076,0,0.0993399,"Missing"
2020.findings-emnlp.398,N18-1091,0,0.0891591,"Missing"
2020.findings-emnlp.398,P18-2058,0,0.219545,"ming search. − →−← Since we use the same end-point span sij = [− pr pli ] j to represent the dependency arguments for our uniform SRL, ← − →) we distinguish the left and right end-point vector (pli and − pr i to avoid having the zero vector as a span representation sij . and the individual score of semantic role label r is denoted by: 4 Φr (p, a, r) = [Φr (p, a)]r . 5 When i=j, it means a uniform representation of dependency semantic role. 4441 Since the total of predicate-argument pairs are O(n3 ), which is computationally impractical. We apply candidates pruning method in (Li et al., 2019b; He et al., 2018a). First of all, we train separate scorers (φp and φa ) for predicates and arguments by two one-layer feedforward networks. Then, the predicate and argument candidates are ranked according to their predicted score (φp and φa ), and we select the top np and na predicate and argument candidates, respectively: Algorithm 1 Joint span syntactic parsing algorithm Input: sentence leng n, span and dependency score s(i, j, `), d(r, h), 1 ≤ i ≤ j ≤ n, ∀r, h, ` Output: maximum value SH (T ) of tree T Initialization: sc [i][j][h] = si [i][j][h] = 0, ∀i, j, h for len = 1 to n do for i = 1 to n − len + 1 d"
2020.findings-emnlp.398,P17-1044,0,0.018678,"ent Program of China (No. 2017YFB0304100), Key Projects of National Natural Science Foundation of China (U1836222 and 61733011), Huawei-SJTU long term AI project, Cutting-edge Machine reading comprehension and language model. (Chomsky, 1981; Li et al., 2019b). Second, as semantics is usually considered as a higher layer of linguistics over syntax, most previous studies focus on how the latter helps the former. Though there comes a trend that syntactic clues show less impact on enhancing semantic parsing since neural models were introduced (Marcheggiani and Titov, 2017). In fact, recent works (He et al., 2017; Marcheggiani et al., 2017) propose syntax-agnostic models for semantic parsing and achieve competitive and even state-of-the-art results. However, semantics may not only benefit from syntax which has been well known, but syntax may also benefit from semantics, which is an obvious gap in explicit linguistic structure parsing and few attempts were ever reported. To our best knowledge, few previous works focus on the relationship between syntax and semantic which only based on dependency structure (Swayamdipta et al., 2016; Henderson et al., 2013; Shi et al., 2016). To fill such a gap, in this"
2020.findings-emnlp.398,D19-1538,1,0.839416,"trate that our improved uniform SRL representation can be adapted to perform dependency SRL and achieves impressive performance gains. 5 Table 6: Dependency SRL results on CoNLL-2009 Propbank test sets. Related Work In the early work of SRL, most of the researchers focus on feature engineering based on training corpus. The traditional approaches to SRL focused on developing rich sets of linguistic features templates and then employ linear classifiers such as SVM (Zhao et al., 2009a). With the impressive success of deep neural networks in various NLP tasks (Luo and Zhao, 2020; Li et al., 2020; He et al., 2019; Luo et al., 2020b; Zhang et al., 2018a; Li et al., 2018a; Zhang et al., 2018b; Luo et al., 2020a; Zhang et al., 2019; Li et al., 2019a; Zhao and Kit, 2008; Zhao et al., 2009b, 2013), considerable attention has been paid to syntactic features (Strubell et al., 2018; Kasai et al., 2019; He et al., 2018b). (Lewis et al., 2015; Strubell et al., 2018; Kasai et al., 2019; He et al., 2018b; Li et al., 2018b) modeled syntactic parsing and SRL jointly, (Lewis et al., 2015) jointly modeled SRL and CCG parsing, and (Kasai et al., 2019) combined the supertags 4445 extracted from dependency parses with S"
2020.findings-emnlp.398,P18-1192,1,0.950493,"ming search. − →−← Since we use the same end-point span sij = [− pr pli ] j to represent the dependency arguments for our uniform SRL, ← − →) we distinguish the left and right end-point vector (pli and − pr i to avoid having the zero vector as a span representation sij . and the individual score of semantic role label r is denoted by: 4 Φr (p, a, r) = [Φr (p, a)]r . 5 When i=j, it means a uniform representation of dependency semantic role. 4441 Since the total of predicate-argument pairs are O(n3 ), which is computationally impractical. We apply candidates pruning method in (Li et al., 2019b; He et al., 2018a). First of all, we train separate scorers (φp and φa ) for predicates and arguments by two one-layer feedforward networks. Then, the predicate and argument candidates are ranked according to their predicted score (φp and φa ), and we select the top np and na predicate and argument candidates, respectively: Algorithm 1 Joint span syntactic parsing algorithm Input: sentence leng n, span and dependency score s(i, j, `), d(r, h), 1 ≤ i ≤ j ≤ n, ∀r, h, ` Output: maximum value SH (T ) of tree T Initialization: sc [i][j][h] = si [i][j][h] = 0, ∀i, j, h for len = 1 to n do for i = 1 to n − len + 1 d"
2020.findings-emnlp.398,J13-4006,0,0.102745,"(Marcheggiani and Titov, 2017). In fact, recent works (He et al., 2017; Marcheggiani et al., 2017) propose syntax-agnostic models for semantic parsing and achieve competitive and even state-of-the-art results. However, semantics may not only benefit from syntax which has been well known, but syntax may also benefit from semantics, which is an obvious gap in explicit linguistic structure parsing and few attempts were ever reported. To our best knowledge, few previous works focus on the relationship between syntax and semantic which only based on dependency structure (Swayamdipta et al., 2016; Henderson et al., 2013; Shi et al., 2016). To fill such a gap, in this work, we further exploit both strengths of the span and dependency representation of both semantic role labeling (SRL) (Strubell et al., 2018) and syntax, and propose a joint model1 with multi-task learning in a balanced mode which improves both semantic and syntactic parsing. Moreover, in our model, semantics is learned in an end-to-end way with a uniform representation and syntactic parsing is represented as a joint span structure (Zhou and Zhao, 2019) relating to head-driven phrase structure grammar (HPSG) (Pollard and Sag, 1994) which can in"
2020.findings-emnlp.398,P19-1237,0,0.203145,"Missing"
2020.findings-emnlp.398,D08-1008,0,0.319879,"Missing"
2020.findings-emnlp.398,N19-1075,0,0.0560325,"s injecting state-of-the-art predicted parses. WSJ System P R Brown F1 P R F1 End-to-end Li et al. (2019b) - - 85.1 - - - Ours (wo/dep) Ours (w/dep) 84.24 83.73 87.55 86.94 85.86 85.30 76.46 76.21 78.52 77.89 77.47 77.04 + Pre-training He et al. (2018b) Cai et al. (2018) Li et al. (2019b) 83.9 84.7 84.5 82.7 85.2 86.1 83.3 85.0 85.3 74.6 73.8 72.5 74.2 Ours (wo/dep) + BERT Ours (w/dep) + BERT Ours (wo/dep) + XLNet Ours (w/dep) + XLNet 87.40 86.77 86.58 86.35 88.96 89.14 90.40 90.16 88.17 87.94 88.44 88.21 80.32 79.71 80.96 80.90 82.89 82.40 85.31 85.38 81.58 81.03 83.08 83.08 Given predicate (Kasai et al., 2019) 89.0 88.2 88.6 78.0 77.2 77.6 Ours (wo/dep) Ours (w/dep) 88.73 88.02 89.83 89.03 89.28 88.52 82.46 80.98 83.20 82.10 82.82 81.54 + Pre-training He et al. (2018b) Cai et al. (2018) Li et al. (2019b) Kasai et al. (2019) Lyu et al. (2019) Chen et al. (2019) Cai and Lapata (2019) 89.7 89.9 89.6 90.3 90.74 91.7 89.3 89.2 91.2 90.0 91.38 90.8 89.5 89.6 90.4 90.2 90.99 91.06 91.2 81.9 79.8 81.7 81.0 82.66 83.2 76.9 78.3 81.4 80.5 82.78 81.9 79.3 79.0 81.5 80.8 82.18 82.72 82.5 Ours (wo/dep) + BERT Ours (w/dep) + BERT Ours (wo/dep) + XLNet Ours (w/dep) + XLNet 91.21 91.14 91.16 90.80 91.19 91.03 91.6"
2020.findings-emnlp.398,P19-1340,0,0.0378303,"For dependency head scorer, we employ two 1024dimensional MLP layers with the ReLU as the activation function for learning specific representation and a 1024-dimensional parameter matrix for biaffine attention. In addition, when augmenting our model with BERT and XLNet, we set 2 layers of self-attention for BERT and XLNet. Training Details we use 0.33 dropout for biaffine attention and MLP layers. All models are trained for up to 150 epochs with batch size 150 on a single NVIDIA GeForce GTX 1080Ti GPU with Intel i7-7800X CPU. We use the same training settings as (Kitaev and Klein, 2018) and (Kitaev et al., 2019). 9 http://nlp.cs.nyu.edu/evalb/ Figure 3: Syntactic parsing performance of different parameter λH on PTB dev set. Model separate constituent converted dependency separate dependency joint span λH = 1.0 joint span λH = 0.0 joint span λH = 0.8 converted dependency F1 93.98 − 93.89 − 93.98 UAS − 95.38 95.80 − 95.90 95.99 95.70 LAS − 94.06 94.40 − 94.50 94.53 94.60 Table 1: PTB dev set performance of joint span syntactic parsing. The converted means the corresponding dependency syntactic parsing results are from the corresponding constituent parse tree using head rules. 4.2 Joint Span Syntactic P"
2020.findings-emnlp.398,P18-1249,0,0.389713,"which needs to distinguish the single word span arguments and dependency arguments. Thus, we represent all the span arguments A = {(wi , . . . , wj )|1 ≤ i ≤ j ≤ n} as span S(i − 1, j) and all the dependency arguments A = {(wi )|1 ≤ i ≤ n} as span S(i, i). We set a special start token at the beginning of sentence. 3 3.1 Our Model Overview As shown in Figure 2, our model includes four modules: token representation, self-attention encoder, scorer module, and two decoders. Using an encoder-decoder backbone, we apply self-attention encoder (Vaswani et al.) that is modified by position partition (Kitaev and Klein, 2018). We take multi-task learning (MTL) approach sharing the parameters of token representation and self-attention encoder. Since we convert two syntactic representations as joint span structure and apply uniform semantic representation, we only need two decoders, one for syntactic tree based on joint span syntactic parsing algorithm (Zhou and Zhao, 2019), another for uniform SRL. 3.2 Token Representation In our model, token representation xi is composed of characters, words, and part-of-speech (POS) representation. For character-level representation, we use CharLSTM (Ling et al., 2015). For word-"
2020.findings-emnlp.398,D15-1169,0,0.0231212,"e traditional approaches to SRL focused on developing rich sets of linguistic features templates and then employ linear classifiers such as SVM (Zhao et al., 2009a). With the impressive success of deep neural networks in various NLP tasks (Luo and Zhao, 2020; Li et al., 2020; He et al., 2019; Luo et al., 2020b; Zhang et al., 2018a; Li et al., 2018a; Zhang et al., 2018b; Luo et al., 2020a; Zhang et al., 2019; Li et al., 2019a; Zhao and Kit, 2008; Zhao et al., 2009b, 2013), considerable attention has been paid to syntactic features (Strubell et al., 2018; Kasai et al., 2019; He et al., 2018b). (Lewis et al., 2015; Strubell et al., 2018; Kasai et al., 2019; He et al., 2018b; Li et al., 2018b) modeled syntactic parsing and SRL jointly, (Lewis et al., 2015) jointly modeled SRL and CCG parsing, and (Kasai et al., 2019) combined the supertags 4445 extracted from dependency parses with SRL . There are a few studies on joint learning of syntactic and semantic parsing which only focus on dependency structure (Swayamdipta et al., 2016; Henderson et al., 2013; Shi et al., 2016). Such as (Henderson et al., 2013) based on dependency structure only focus on shared representation without explicitly analyzing whethe"
2020.findings-emnlp.398,N19-1307,1,0.574024,"representations for both semantics and syntax, which have been well studied and discussed from both linguistic and computational perspective, though few works comprehensively considered the impact of either/both representation styles over the respective parsing ∗ Corresponding author. This paper was partially supported by National Key Research and Development Program of China (No. 2017YFB0304100), Key Projects of National Natural Science Foundation of China (U1836222 and 61733011), Huawei-SJTU long term AI project, Cutting-edge Machine reading comprehension and language model. (Chomsky, 1981; Li et al., 2019b). Second, as semantics is usually considered as a higher layer of linguistics over syntax, most previous studies focus on how the latter helps the former. Though there comes a trend that syntactic clues show less impact on enhancing semantic parsing since neural models were introduced (Marcheggiani and Titov, 2017). In fact, recent works (He et al., 2017; Marcheggiani et al., 2017) propose syntax-agnostic models for semantic parsing and achieve competitive and even state-of-the-art results. However, semantics may not only benefit from syntax which has been well known, but syntax may also ben"
2020.findings-emnlp.398,C18-1271,1,0.876331,"adapted to perform dependency SRL and achieves impressive performance gains. 5 Table 6: Dependency SRL results on CoNLL-2009 Propbank test sets. Related Work In the early work of SRL, most of the researchers focus on feature engineering based on training corpus. The traditional approaches to SRL focused on developing rich sets of linguistic features templates and then employ linear classifiers such as SVM (Zhao et al., 2009a). With the impressive success of deep neural networks in various NLP tasks (Luo and Zhao, 2020; Li et al., 2020; He et al., 2019; Luo et al., 2020b; Zhang et al., 2018a; Li et al., 2018a; Zhang et al., 2018b; Luo et al., 2020a; Zhang et al., 2019; Li et al., 2019a; Zhao and Kit, 2008; Zhao et al., 2009b, 2013), considerable attention has been paid to syntactic features (Strubell et al., 2018; Kasai et al., 2019; He et al., 2018b). (Lewis et al., 2015; Strubell et al., 2018; Kasai et al., 2019; He et al., 2018b; Li et al., 2018b) modeled syntactic parsing and SRL jointly, (Lewis et al., 2015) jointly modeled SRL and CCG parsing, and (Kasai et al., 2019) combined the supertags 4445 extracted from dependency parses with SRL . There are a few studies on joint learning of syntact"
2020.findings-emnlp.398,D18-1262,1,0.845656,"adapted to perform dependency SRL and achieves impressive performance gains. 5 Table 6: Dependency SRL results on CoNLL-2009 Propbank test sets. Related Work In the early work of SRL, most of the researchers focus on feature engineering based on training corpus. The traditional approaches to SRL focused on developing rich sets of linguistic features templates and then employ linear classifiers such as SVM (Zhao et al., 2009a). With the impressive success of deep neural networks in various NLP tasks (Luo and Zhao, 2020; Li et al., 2020; He et al., 2019; Luo et al., 2020b; Zhang et al., 2018a; Li et al., 2018a; Zhang et al., 2018b; Luo et al., 2020a; Zhang et al., 2019; Li et al., 2019a; Zhao and Kit, 2008; Zhao et al., 2009b, 2013), considerable attention has been paid to syntactic features (Strubell et al., 2018; Kasai et al., 2019; He et al., 2018b). (Lewis et al., 2015; Strubell et al., 2018; Kasai et al., 2019; He et al., 2018b; Li et al., 2018b) modeled syntactic parsing and SRL jointly, (Lewis et al., 2015) jointly modeled SRL and CCG parsing, and (Kasai et al., 2019) combined the supertags 4445 extracted from dependency parses with SRL . There are a few studies on joint learning of syntact"
2020.findings-emnlp.398,D15-1176,0,0.0680823,"Missing"
2020.findings-emnlp.398,2020.acl-main.571,1,0.795512,"out-domain text. These results demonstrate that our improved uniform SRL representation can be adapted to perform dependency SRL and achieves impressive performance gains. 5 Table 6: Dependency SRL results on CoNLL-2009 Propbank test sets. Related Work In the early work of SRL, most of the researchers focus on feature engineering based on training corpus. The traditional approaches to SRL focused on developing rich sets of linguistic features templates and then employ linear classifiers such as SVM (Zhao et al., 2009a). With the impressive success of deep neural networks in various NLP tasks (Luo and Zhao, 2020; Li et al., 2020; He et al., 2019; Luo et al., 2020b; Zhang et al., 2018a; Li et al., 2018a; Zhang et al., 2018b; Luo et al., 2020a; Zhang et al., 2019; Li et al., 2019a; Zhao and Kit, 2008; Zhao et al., 2009b, 2013), considerable attention has been paid to syntactic features (Strubell et al., 2018; Kasai et al., 2019; He et al., 2018b). (Lewis et al., 2015; Strubell et al., 2018; Kasai et al., 2019; He et al., 2018b; Li et al., 2018b) modeled syntactic parsing and SRL jointly, (Lewis et al., 2015) jointly modeled SRL and CCG parsing, and (Kasai et al., 2019) combined the supertags 4445 extra"
2020.findings-emnlp.398,D14-1162,0,0.0922494,", 2003). In addition, we use two SRL setups: end-to-end and pre-identified predicates. For the predicate disambiguation task in dependency SRL, we follow (Marcheggiani and Titov, 2017) and use the off-the-shelf disambiguator from (Roth and Lapata, 2016). For constituent syntactic parsing, we use the standard evalb9 tool to evaluate the F1 score. For dependency syntactic parsing, following previous work (Dozat and Manning, 2017), we report the results without punctuations of the labeled and unlabeled attachment scores (LAS, UAS). 4.1 Setup Hyperparameters In our experiments, we use 100D GloVe (Pennington et al., 2014) pre-trained embeddings. For the self-attention encoder, we set 12 self-attention layers and use the same other hyperparameters settings as (Kitaev and Klein, 2018). For semantic role scorer, we use 512-dimensional MLP layers and 256-dimensional feed-forward networks. For candidates pruning, we set λp = 0.4 and λa = 0.6 for pruning predicates and arguments, mp = 30 and ma = 300 for max numbers of predicates and arguments respectively. For constituent span scorer, we apply a hidden size of 250-dimensional feed-forward networks. For dependency head scorer, we employ two 1024dimensional MLP layer"
2020.findings-emnlp.398,2020.emnlp-main.723,1,0.748993,"proved uniform SRL representation can be adapted to perform dependency SRL and achieves impressive performance gains. 5 Table 6: Dependency SRL results on CoNLL-2009 Propbank test sets. Related Work In the early work of SRL, most of the researchers focus on feature engineering based on training corpus. The traditional approaches to SRL focused on developing rich sets of linguistic features templates and then employ linear classifiers such as SVM (Zhao et al., 2009a). With the impressive success of deep neural networks in various NLP tasks (Luo and Zhao, 2020; Li et al., 2020; He et al., 2019; Luo et al., 2020b; Zhang et al., 2018a; Li et al., 2018a; Zhang et al., 2018b; Luo et al., 2020a; Zhang et al., 2019; Li et al., 2019a; Zhao and Kit, 2008; Zhao et al., 2009b, 2013), considerable attention has been paid to syntactic features (Strubell et al., 2018; Kasai et al., 2019; He et al., 2018b). (Lewis et al., 2015; Strubell et al., 2018; Kasai et al., 2019; He et al., 2018b; Li et al., 2018b) modeled syntactic parsing and SRL jointly, (Lewis et al., 2015) jointly modeled SRL and CCG parsing, and (Kasai et al., 2019) combined the supertags 4445 extracted from dependency parses with SRL . There are a f"
2020.findings-emnlp.398,J08-2005,0,0.150718,"} + d(r, h) } sc [i][j][h] = max { splitl , splitr }+ max s(i, j, `) p∈P,a∈A,r∈R = r≤k<h si [k + 1][j][h] } + d(r, h) } splitr = max { max { si [i][k][h]+ where λH in the range of 0 to 1. In addition, we can merely generate constituent or dependency syntactic parsing tree by setting λH to 1 or 0, respectively. Decoder for Uniform Semantic Role Since we apply uniform span for both dependency and span semantic role, we use a single dynamic programming decoder to generate two semantic forms following the non-overlapping constraints: span semantic arguments for the same predicate do not overlap (Punyakanok et al., 2008). 4 Experiments We evaluate our model on CoNLL-2009 shared task (Hajiˇc et al., 2009) for dependency-style SRL, CoNLL-2005 shared task (Carreras and M`arquez, 2005) for span-style SRL both using the Propbank convention (Palmer et al., 2005), and English Penn Treebank (PTB) (Marcus et al., 1993) for constituent syntactic parsing, Stanford basic dependencies (SD) representation (de Marneffe et al., 2006) 8 s(i, j, `) = λH Scateg (i, j, `), d(i, j) = (1−λH )αij , converted by the Stanford parser for dependency syntactic parsing. We follow standard data splitting: 6 For further details, see (Zhou"
2020.findings-emnlp.398,D19-1099,0,0.0237713,"Missing"
2020.findings-emnlp.398,P16-1113,0,0.0191504,"ng take section 2-21 of Wall Street Journal (WSJ) data as training set, SRL takes section 24 as development set while syntactic parsing takes section 22 as development set, SRL takes section 23 of WSJ together with 3 sections from Brown corpus as test set while syntactic parsing only takes section 23. POS tags are predicted using the Stanford tagger (Toutanova et al., 2003). In addition, we use two SRL setups: end-to-end and pre-identified predicates. For the predicate disambiguation task in dependency SRL, we follow (Marcheggiani and Titov, 2017) and use the off-the-shelf disambiguator from (Roth and Lapata, 2016). For constituent syntactic parsing, we use the standard evalb9 tool to evaluate the F1 score. For dependency syntactic parsing, following previous work (Dozat and Manning, 2017), we report the results without punctuations of the labeled and unlabeled attachment scores (LAS, UAS). 4.1 Setup Hyperparameters In our experiments, we use 100D GloVe (Pennington et al., 2014) pre-trained embeddings. For the self-attention encoder, we set 12 self-attention layers and use the same other hyperparameters settings as (Kitaev and Klein, 2018). For semantic role scorer, we use 512-dimensional MLP layers and"
2020.findings-emnlp.398,P18-1130,0,0.0659682,"Missing"
2020.findings-emnlp.398,C88-2121,0,0.600178,"Missing"
2020.findings-emnlp.398,K17-1041,0,0.0165771,"ina (No. 2017YFB0304100), Key Projects of National Natural Science Foundation of China (U1836222 and 61733011), Huawei-SJTU long term AI project, Cutting-edge Machine reading comprehension and language model. (Chomsky, 1981; Li et al., 2019b). Second, as semantics is usually considered as a higher layer of linguistics over syntax, most previous studies focus on how the latter helps the former. Though there comes a trend that syntactic clues show less impact on enhancing semantic parsing since neural models were introduced (Marcheggiani and Titov, 2017). In fact, recent works (He et al., 2017; Marcheggiani et al., 2017) propose syntax-agnostic models for semantic parsing and achieve competitive and even state-of-the-art results. However, semantics may not only benefit from syntax which has been well known, but syntax may also benefit from semantics, which is an obvious gap in explicit linguistic structure parsing and few attempts were ever reported. To our best knowledge, few previous works focus on the relationship between syntax and semantic which only based on dependency structure (Swayamdipta et al., 2016; Henderson et al., 2013; Shi et al., 2016). To fill such a gap, in this work, we further exploit bot"
2020.findings-emnlp.398,D16-1098,0,0.0181647,", 2017). In fact, recent works (He et al., 2017; Marcheggiani et al., 2017) propose syntax-agnostic models for semantic parsing and achieve competitive and even state-of-the-art results. However, semantics may not only benefit from syntax which has been well known, but syntax may also benefit from semantics, which is an obvious gap in explicit linguistic structure parsing and few attempts were ever reported. To our best knowledge, few previous works focus on the relationship between syntax and semantic which only based on dependency structure (Swayamdipta et al., 2016; Henderson et al., 2013; Shi et al., 2016). To fill such a gap, in this work, we further exploit both strengths of the span and dependency representation of both semantic role labeling (SRL) (Strubell et al., 2018) and syntax, and propose a joint model1 with multi-task learning in a balanced mode which improves both semantic and syntactic parsing. Moreover, in our model, semantics is learned in an end-to-end way with a uniform representation and syntactic parsing is represented as a joint span structure (Zhou and Zhao, 2019) relating to head-driven phrase structure grammar (HPSG) (Pollard and Sag, 1994) which can incorporate both head"
2020.findings-emnlp.398,D17-1159,0,0.142607,"tially supported by National Key Research and Development Program of China (No. 2017YFB0304100), Key Projects of National Natural Science Foundation of China (U1836222 and 61733011), Huawei-SJTU long term AI project, Cutting-edge Machine reading comprehension and language model. (Chomsky, 1981; Li et al., 2019b). Second, as semantics is usually considered as a higher layer of linguistics over syntax, most previous studies focus on how the latter helps the former. Though there comes a trend that syntactic clues show less impact on enhancing semantic parsing since neural models were introduced (Marcheggiani and Titov, 2017). In fact, recent works (He et al., 2017; Marcheggiani et al., 2017) propose syntax-agnostic models for semantic parsing and achieve competitive and even state-of-the-art results. However, semantics may not only benefit from syntax which has been well known, but syntax may also benefit from semantics, which is an obvious gap in explicit linguistic structure parsing and few attempts were ever reported. To our best knowledge, few previous works focus on the relationship between syntax and semantic which only based on dependency structure (Swayamdipta et al., 2016; Henderson et al., 2013; Shi et"
2020.findings-emnlp.398,J93-2004,0,0.0738247,"dency and constituent syntactic parsing. We verify the effectiveness and applicability of the proposed model on Propbank semantic parsing 2 in both span style (CoNLL-2005) (Carreras and 1 Our code : https://github.com/DoodleJZ/ParsingAll. It is also called semantic role labeling (SRL) for the semantic parsing task over the Propbank. 2 4438 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 4438–4449 c November 16 - 20, 2020. 2020 Association for Computational Linguistics S M`arquez, 2005) and dependency style (CoNLL2009) (Hajiˇc et al., 2009) and Penn Treebank (PTB) (Marcus et al., 1993) for both constituent and dependency syntactic parsing. Our empirical results show that semantics and syntax can indeed benefit each other, and our single model reaches new stateof-the-art or competitive performance for all four tasks: span and dependency SRL, constituent and dependency syntactic parsing. 2 (1,9) NP NNP NNP Federal Paper 1 2 ROOT Structure Representation Syntactic Representation The joint span structure which is related to the HEAD FEATURE PRINCIPLE (HFP) of HPSG (Pollard and Sag, 1994) consists of all its children phrases in the constituent tree and all dependency arcs betwee"
2020.findings-emnlp.398,de-marneffe-etal-2006-generating,0,0.220571,"Missing"
2020.findings-emnlp.398,D17-1178,0,0.0479171,"Missing"
2020.findings-emnlp.398,D18-1548,0,0.0492466,"Missing"
2020.findings-emnlp.398,K16-1019,0,0.0455413,"ral models were introduced (Marcheggiani and Titov, 2017). In fact, recent works (He et al., 2017; Marcheggiani et al., 2017) propose syntax-agnostic models for semantic parsing and achieve competitive and even state-of-the-art results. However, semantics may not only benefit from syntax which has been well known, but syntax may also benefit from semantics, which is an obvious gap in explicit linguistic structure parsing and few attempts were ever reported. To our best knowledge, few previous works focus on the relationship between syntax and semantic which only based on dependency structure (Swayamdipta et al., 2016; Henderson et al., 2013; Shi et al., 2016). To fill such a gap, in this work, we further exploit both strengths of the span and dependency representation of both semantic role labeling (SRL) (Strubell et al., 2018) and syntax, and propose a joint model1 with multi-task learning in a balanced mode which improves both semantic and syntactic parsing. Moreover, in our model, semantics is learned in an end-to-end way with a uniform representation and syntactic parsing is represented as a joint span structure (Zhou and Zhao, 2019) relating to head-driven phrase structure grammar (HPSG) (Pollard and"
2020.findings-emnlp.398,D18-1191,0,0.0635268,"Missing"
2020.findings-emnlp.398,J05-1004,0,0.204164,"ndency syntactic parsing tree by setting λH to 1 or 0, respectively. Decoder for Uniform Semantic Role Since we apply uniform span for both dependency and span semantic role, we use a single dynamic programming decoder to generate two semantic forms following the non-overlapping constraints: span semantic arguments for the same predicate do not overlap (Punyakanok et al., 2008). 4 Experiments We evaluate our model on CoNLL-2009 shared task (Hajiˇc et al., 2009) for dependency-style SRL, CoNLL-2005 shared task (Carreras and M`arquez, 2005) for span-style SRL both using the Propbank convention (Palmer et al., 2005), and English Penn Treebank (PTB) (Marcus et al., 1993) for constituent syntactic parsing, Stanford basic dependencies (SD) representation (de Marneffe et al., 2006) 8 s(i, j, `) = λH Scateg (i, j, `), d(i, j) = (1−λH )αij , converted by the Stanford parser for dependency syntactic parsing. We follow standard data splitting: 6 For further details, see (Zhou and Zhao, 2019) which has discussed the different between constituent syntactic parsing CKY-style algorithm, how to binarize the joint span tree and the time, space complexity. 7 We also try to incorporate the head information in constituen"
2020.findings-emnlp.398,N03-1033,0,0.230485,"more complex and unstable. Thus we employ a parameter to balance two different scores in joint decoder which is easily implemented with better performance. 8 http://nlp.stanford.edu/software/lex-parser.html 4442 semantic (SRL) and syntactic parsing take section 2-21 of Wall Street Journal (WSJ) data as training set, SRL takes section 24 as development set while syntactic parsing takes section 22 as development set, SRL takes section 23 of WSJ together with 3 sections from Brown corpus as test set while syntactic parsing only takes section 23. POS tags are predicted using the Stanford tagger (Toutanova et al., 2003). In addition, we use two SRL setups: end-to-end and pre-identified predicates. For the predicate disambiguation task in dependency SRL, we follow (Marcheggiani and Titov, 2017) and use the off-the-shelf disambiguator from (Roth and Lapata, 2016). For constituent syntactic parsing, we use the standard evalb9 tool to evaluate the F1 score. For dependency syntactic parsing, following previous work (Dozat and Manning, 2017), we report the results without punctuations of the labeled and unlabeled attachment scores (LAS, UAS). 4.1 Setup Hyperparameters In our experiments, we use 100D GloVe (Penning"
2020.findings-emnlp.398,N03-1000,0,0.420954,"Missing"
2020.findings-emnlp.398,P19-1230,1,0.700117,"en syntax and semantic which only based on dependency structure (Swayamdipta et al., 2016; Henderson et al., 2013; Shi et al., 2016). To fill such a gap, in this work, we further exploit both strengths of the span and dependency representation of both semantic role labeling (SRL) (Strubell et al., 2018) and syntax, and propose a joint model1 with multi-task learning in a balanced mode which improves both semantic and syntactic parsing. Moreover, in our model, semantics is learned in an end-to-end way with a uniform representation and syntactic parsing is represented as a joint span structure (Zhou and Zhao, 2019) relating to head-driven phrase structure grammar (HPSG) (Pollard and Sag, 1994) which can incorporate both head and phrase information of dependency and constituent syntactic parsing. We verify the effectiveness and applicability of the proposed model on Propbank semantic parsing 2 in both span style (CoNLL-2005) (Carreras and 1 Our code : https://github.com/DoodleJZ/ParsingAll. It is also called semantic role labeling (SRL) for the semantic parsing task over the Propbank. 2 4438 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 4438–4449 c November 16 - 20, 2020. 2"
2020.findings-emnlp.398,D18-1511,1,0.776723,"representation can be adapted to perform dependency SRL and achieves impressive performance gains. 5 Table 6: Dependency SRL results on CoNLL-2009 Propbank test sets. Related Work In the early work of SRL, most of the researchers focus on feature engineering based on training corpus. The traditional approaches to SRL focused on developing rich sets of linguistic features templates and then employ linear classifiers such as SVM (Zhao et al., 2009a). With the impressive success of deep neural networks in various NLP tasks (Luo and Zhao, 2020; Li et al., 2020; He et al., 2019; Luo et al., 2020b; Zhang et al., 2018a; Li et al., 2018a; Zhang et al., 2018b; Luo et al., 2020a; Zhang et al., 2019; Li et al., 2019a; Zhao and Kit, 2008; Zhao et al., 2009b, 2013), considerable attention has been paid to syntactic features (Strubell et al., 2018; Kasai et al., 2019; He et al., 2018b). (Lewis et al., 2015; Strubell et al., 2018; Kasai et al., 2019; He et al., 2018b; Li et al., 2018b) modeled syntactic parsing and SRL jointly, (Lewis et al., 2015) jointly modeled SRL and CCG parsing, and (Kasai et al., 2019) combined the supertags 4445 extracted from dependency parses with SRL . There are a few studies on joint l"
2020.findings-emnlp.398,P19-1154,1,0.843853,"Missing"
2020.findings-emnlp.398,C18-1317,1,0.819893,"representation can be adapted to perform dependency SRL and achieves impressive performance gains. 5 Table 6: Dependency SRL results on CoNLL-2009 Propbank test sets. Related Work In the early work of SRL, most of the researchers focus on feature engineering based on training corpus. The traditional approaches to SRL focused on developing rich sets of linguistic features templates and then employ linear classifiers such as SVM (Zhao et al., 2009a). With the impressive success of deep neural networks in various NLP tasks (Luo and Zhao, 2020; Li et al., 2020; He et al., 2019; Luo et al., 2020b; Zhang et al., 2018a; Li et al., 2018a; Zhang et al., 2018b; Luo et al., 2020a; Zhang et al., 2019; Li et al., 2019a; Zhao and Kit, 2008; Zhao et al., 2009b, 2013), considerable attention has been paid to syntactic features (Strubell et al., 2018; Kasai et al., 2019; He et al., 2018b). (Lewis et al., 2015; Strubell et al., 2018; Kasai et al., 2019; He et al., 2018b; Li et al., 2018b) modeled syntactic parsing and SRL jointly, (Lewis et al., 2015) jointly modeled SRL and CCG parsing, and (Kasai et al., 2019) combined the supertags 4445 extracted from dependency parses with SRL . There are a few studies on joint l"
2020.findings-emnlp.398,W09-1209,1,0.662743,"tain new stateof-the-art both of end-to-end and given predicate mode and both of in-domain and out-domain text. These results demonstrate that our improved uniform SRL representation can be adapted to perform dependency SRL and achieves impressive performance gains. 5 Table 6: Dependency SRL results on CoNLL-2009 Propbank test sets. Related Work In the early work of SRL, most of the researchers focus on feature engineering based on training corpus. The traditional approaches to SRL focused on developing rich sets of linguistic features templates and then employ linear classifiers such as SVM (Zhao et al., 2009a). With the impressive success of deep neural networks in various NLP tasks (Luo and Zhao, 2020; Li et al., 2020; He et al., 2019; Luo et al., 2020b; Zhang et al., 2018a; Li et al., 2018a; Zhang et al., 2018b; Luo et al., 2020a; Zhang et al., 2019; Li et al., 2019a; Zhao and Kit, 2008; Zhao et al., 2009b, 2013), considerable attention has been paid to syntactic features (Strubell et al., 2018; Kasai et al., 2019; He et al., 2018b). (Lewis et al., 2015; Strubell et al., 2018; Kasai et al., 2019; He et al., 2018b; Li et al., 2018b) modeled syntactic parsing and SRL jointly, (Lewis et al., 2015)"
2020.findings-emnlp.398,D09-1004,1,0.743006,"tain new stateof-the-art both of end-to-end and given predicate mode and both of in-domain and out-domain text. These results demonstrate that our improved uniform SRL representation can be adapted to perform dependency SRL and achieves impressive performance gains. 5 Table 6: Dependency SRL results on CoNLL-2009 Propbank test sets. Related Work In the early work of SRL, most of the researchers focus on feature engineering based on training corpus. The traditional approaches to SRL focused on developing rich sets of linguistic features templates and then employ linear classifiers such as SVM (Zhao et al., 2009a). With the impressive success of deep neural networks in various NLP tasks (Luo and Zhao, 2020; Li et al., 2020; He et al., 2019; Luo et al., 2020b; Zhang et al., 2018a; Li et al., 2018a; Zhang et al., 2018b; Luo et al., 2020a; Zhang et al., 2019; Li et al., 2019a; Zhao and Kit, 2008; Zhao et al., 2009b, 2013), considerable attention has been paid to syntactic features (Strubell et al., 2018; Kasai et al., 2019; He et al., 2018b). (Lewis et al., 2015; Strubell et al., 2018; Kasai et al., 2019; He et al., 2018b; Li et al., 2018b) modeled syntactic parsing and SRL jointly, (Lewis et al., 2015)"
2020.findings-emnlp.398,W08-2127,1,0.537442,"ncy SRL results on CoNLL-2009 Propbank test sets. Related Work In the early work of SRL, most of the researchers focus on feature engineering based on training corpus. The traditional approaches to SRL focused on developing rich sets of linguistic features templates and then employ linear classifiers such as SVM (Zhao et al., 2009a). With the impressive success of deep neural networks in various NLP tasks (Luo and Zhao, 2020; Li et al., 2020; He et al., 2019; Luo et al., 2020b; Zhang et al., 2018a; Li et al., 2018a; Zhang et al., 2018b; Luo et al., 2020a; Zhang et al., 2019; Li et al., 2019a; Zhao and Kit, 2008; Zhao et al., 2009b, 2013), considerable attention has been paid to syntactic features (Strubell et al., 2018; Kasai et al., 2019; He et al., 2018b). (Lewis et al., 2015; Strubell et al., 2018; Kasai et al., 2019; He et al., 2018b; Li et al., 2018b) modeled syntactic parsing and SRL jointly, (Lewis et al., 2015) jointly modeled SRL and CCG parsing, and (Kasai et al., 2019) combined the supertags 4445 extracted from dependency parses with SRL . There are a few studies on joint learning of syntactic and semantic parsing which only focus on dependency structure (Swayamdipta et al., 2016; Henders"
2020.findings-emnlp.399,C18-1139,0,0.0732333,"Missing"
2020.findings-emnlp.399,P18-1246,0,0.0432878,"Missing"
2020.findings-emnlp.399,D15-1075,0,0.015242,"g and evaluate setting as (Zhou et al., 2020) and use end-to-end SRL setups of both span and dependency SRL. Since LIMIT-BERT involves all syntactic and semantic parsing tasks, it is possible to directly apply LIMIT-BERT to each task without fine-tuning and we also compare these results. In order to evaluate the language model pretraining performance of our LIMIT-BERT, we also evaluate LIMIT-BERT on two widely-used datasets, The General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2018) which is a collection of nine NLU tasks and Stanford Natural Language Inference (SNLI) (Bowman et al., 2015) to show the superiority. models : Implementation Details Our implementation of LIMIT-BERT is based on the PyTorch implementation of BERT11 . We use a learning rate of 3e-5 and a batch size of 32 with 1 million training steps. The optimizer and other training settings are the same as BERT (Devlin et al., 2019). For task-specific layers including syntactic and semantic scorers and decoders, we set the same hyperparameters settings as (Zhou et al., 2020). LIMIT-BERT model is trained on 32 NVIDIA GeForce GTX 1080Ti GPUs. 4.3 Main Results Syntactic Parsing Results As shown in Table 1 and 2, LIMIT-"
2020.findings-emnlp.399,C18-1233,1,0.90008,"Missing"
2020.findings-emnlp.399,W05-0620,0,0.204623,"Missing"
2020.findings-emnlp.399,P05-1022,0,0.0225676,"988) , Combinatory Categorial Grammar (CCG) (Steedman, 2000) and head-driven phrase structure grammar (HPSG) (Pollard and Sag, 1994) which is a constraintbased highly lexicalized non-derivational generative grammar framework. To absorb both strengths of span and dependency structure, we apply both span (constituent) and dependency representations of semantic role labeling and syntactic parsing. Thus, it is a natural idea to study the relationship between constituent and dependency structures, and 4457 the joint learning of constituent and dependency syntactic parsing (Klein and Manning, 2004; Charniak and Johnson, 2005; Farkas et al., 2011; Green ˇ and Zabokrtsk´ y, 2012; Ren et al., 2013; Xu et al., 2014; Yoshikawa et al., 2017). Pre-trained Language Modeling Recently, deep contextual language model has been shown effective for learning universal language representations by leveraging large amounts of unlabeled data, achieving various state-of-the-art results in a series of NLU benchmarks. Some prominent examples are Embedding from Language models (ELMo) (Peters et al., 2018), Bidirectional Encoder Representations from Transformers (BERT) (Devlin et al., 2019) and Generalized Autoregressive Pretraining (XL"
2020.findings-emnlp.399,D14-1082,0,0.0080309,"he perforSentences Length Performance mance of baseline model and LIMIT-BERT while varying the sentence length of four linguistics tasks on the English dev set is shown in Figure 3. The statistics show that our LIMIT-BERT outperforms the baseline model of over all sentence lengths. For different sentence lengths, LIMIT-BERT outperforms much better than baseline model on long sentence (larger than 50) of both syntactic and semantic parsing. The possible explanation is that LIMITRelated Work Linguistics Inspired NLP Tasks With the impressive success of deep neural networks in various NLP tasks (Chen and Manning, 2014; Dozat and Manning, 2017; Ma et al., 2018; Strubell et al., 2018; Luo and Zhao, 2020; Li et al., 2020; He et al., 2019; Luo et al., 2020; Zhang et al., 2018a; Li et al., 2018a; Zhang et al., 2018b), syntactic parsing and semantic role labeling have been well developed with neural network and achieve very high performance (Chen and Manning, 2014; Dozat and Manning, 2017; Ma et al., 2018; Kitaev and Klein, 2018; Zhou and Zhao, 2019). Semantic role labeling is deeply related to syntactic structure and a number of works try to incorporate syntactic information in semantic role labeling models by"
2020.findings-emnlp.399,W09-1201,0,0.115493,"Missing"
2020.findings-emnlp.399,N19-1423,0,0.621004,"ect, Cutting-edge Machine reading comprehension and language model. so on (Zhou and Zhao, 2019; Zhou et al., 2020; Ouchi et al., 2018; He et al., 2018b; Li et al., 2019), when taking the latter as downstream tasks for the former. In the meantime, introducing linguistic clues such as syntax and semantics into the pre-trained language models may furthermore enhance other downstream tasks such as various Natural Language Understanding (NLU) tasks (Zhang et al., 2020a,b). However, nearly all existing language models are usually trained on large amounts of unlabeled text data (Peters et al., 2018; Devlin et al., 2019), without explicitly exploiting linguistic knowledge. Such observations motivate us to jointly consider both types of tasks, pre-training language models, and solving linguistics inspired NLP problems. We argue such a treatment may benefit from two-fold. (1) Joint learning is a better way to let the former help the latter in a bidirectional mode, rather than in a unidirectional mode, taking the latter as downstream tasks of the former. (2) Naturally empowered by linguistic clues from joint learning, pre-trained language models will be more powerful for enhancing downstream tasks. Thus we propo"
2020.findings-emnlp.399,W11-2924,0,0.019979,"l Grammar (CCG) (Steedman, 2000) and head-driven phrase structure grammar (HPSG) (Pollard and Sag, 1994) which is a constraintbased highly lexicalized non-derivational generative grammar framework. To absorb both strengths of span and dependency structure, we apply both span (constituent) and dependency representations of semantic role labeling and syntactic parsing. Thus, it is a natural idea to study the relationship between constituent and dependency structures, and 4457 the joint learning of constituent and dependency syntactic parsing (Klein and Manning, 2004; Charniak and Johnson, 2005; Farkas et al., 2011; Green ˇ and Zabokrtsk´ y, 2012; Ren et al., 2013; Xu et al., 2014; Yoshikawa et al., 2017). Pre-trained Language Modeling Recently, deep contextual language model has been shown effective for learning universal language representations by leveraging large amounts of unlabeled data, achieving various state-of-the-art results in a series of NLU benchmarks. Some prominent examples are Embedding from Language models (ELMo) (Peters et al., 2018), Bidirectional Encoder Representations from Transformers (BERT) (Devlin et al., 2019) and Generalized Autoregressive Pretraining (XLNet) (Yang et al., 20"
2020.findings-emnlp.399,N19-1076,0,0.027581,"Missing"
2020.findings-emnlp.399,N18-1091,0,0.0491756,"Missing"
2020.findings-emnlp.399,W12-0503,0,0.0238384,"Missing"
2020.findings-emnlp.399,2020.acl-main.740,0,0.0613948,"Missing"
2020.findings-emnlp.399,D19-1538,1,0.867953,"tics tasks on the English dev set is shown in Figure 3. The statistics show that our LIMIT-BERT outperforms the baseline model of over all sentence lengths. For different sentence lengths, LIMIT-BERT outperforms much better than baseline model on long sentence (larger than 50) of both syntactic and semantic parsing. The possible explanation is that LIMITRelated Work Linguistics Inspired NLP Tasks With the impressive success of deep neural networks in various NLP tasks (Chen and Manning, 2014; Dozat and Manning, 2017; Ma et al., 2018; Strubell et al., 2018; Luo and Zhao, 2020; Li et al., 2020; He et al., 2019; Luo et al., 2020; Zhang et al., 2018a; Li et al., 2018a; Zhang et al., 2018b), syntactic parsing and semantic role labeling have been well developed with neural network and achieve very high performance (Chen and Manning, 2014; Dozat and Manning, 2017; Ma et al., 2018; Kitaev and Klein, 2018; Zhou and Zhao, 2019). Semantic role labeling is deeply related to syntactic structure and a number of works try to incorporate syntactic information in semantic role labeling models by different methods such as concatenation of lexicalized embedding (He et al., 2018b), usage of syntactic GCN (Li et al.,"
2020.findings-emnlp.399,P18-1192,1,0.935644,". 1 Introduction Recently, pre-trained language models have shown greatly effective across a range of linguistics inspired natural language processing (NLP) tasks such as syntactic parsing, semantic parsing and ∗ Corresponding author. This paper was partially supported by National Key Research and Development Program of China (No. 2017YFB0304100), Key Projects of National Natural Science Foundation of China (U1836222 and 61733011), Huawei-SJTU long term AI project, Cutting-edge Machine reading comprehension and language model. so on (Zhou and Zhao, 2019; Zhou et al., 2020; Ouchi et al., 2018; He et al., 2018b; Li et al., 2019), when taking the latter as downstream tasks for the former. In the meantime, introducing linguistic clues such as syntax and semantics into the pre-trained language models may furthermore enhance other downstream tasks such as various Natural Language Understanding (NLU) tasks (Zhang et al., 2020a,b). However, nearly all existing language models are usually trained on large amounts of unlabeled text data (Peters et al., 2018; Devlin et al., 2019), without explicitly exploiting linguistic knowledge. Such observations motivate us to jointly consider both types of tasks, pre-t"
2020.findings-emnlp.399,J13-4006,0,0.0235662,"rate syntactic information in semantic role labeling models by different methods such as concatenation of lexicalized embedding (He et al., 2018b), usage of syntactic GCN (Li et al., 2018b) and multi-task learning (Strubell et al., 2018; Zhou et al., 2020). Besides semantic role labeling and syntactic parsing are two key tasks of semantics and syntax so that they are included into our linguistics tasks for multi-task learning. In addition, both span and dependency are popularly adopted annotation styles for both semantics and syntax and some work on jointly learning of semantic and syntactic (Henderson et al., 2013; Llu´ıs et al., 2013; Swayamdipta et al., 2016) . Researchers are interested in two styles of SRL models that may benefit from each other rather than their separated development, which has been roughly discussed in (Johansson and Nugues, 2008). On the other hand, researchers have discussed how to encode lexical dependencies in phrase structures, like lexicalized tree adjoining grammar (LTAG) (Schabes et al., 1988) , Combinatory Categorial Grammar (CCG) (Steedman, 2000) and head-driven phrase structure grammar (HPSG) (Pollard and Sag, 1994) which is a constraintbased highly lexicalized non-der"
2020.findings-emnlp.399,P19-1237,0,0.0275976,"Missing"
2020.findings-emnlp.399,D08-1008,0,0.055095,"Missing"
2020.findings-emnlp.399,P19-1340,0,0.0522905,"Missing"
2020.findings-emnlp.399,P18-1249,0,0.023056,"yntactic and semantic parsing. The possible explanation is that LIMITRelated Work Linguistics Inspired NLP Tasks With the impressive success of deep neural networks in various NLP tasks (Chen and Manning, 2014; Dozat and Manning, 2017; Ma et al., 2018; Strubell et al., 2018; Luo and Zhao, 2020; Li et al., 2020; He et al., 2019; Luo et al., 2020; Zhang et al., 2018a; Li et al., 2018a; Zhang et al., 2018b), syntactic parsing and semantic role labeling have been well developed with neural network and achieve very high performance (Chen and Manning, 2014; Dozat and Manning, 2017; Ma et al., 2018; Kitaev and Klein, 2018; Zhou and Zhao, 2019). Semantic role labeling is deeply related to syntactic structure and a number of works try to incorporate syntactic information in semantic role labeling models by different methods such as concatenation of lexicalized embedding (He et al., 2018b), usage of syntactic GCN (Li et al., 2018b) and multi-task learning (Strubell et al., 2018; Zhou et al., 2020). Besides semantic role labeling and syntactic parsing are two key tasks of semantics and syntax so that they are included into our linguistics tasks for multi-task learning. In addition, both span and dependency are pop"
2020.findings-emnlp.399,P04-1061,0,0.0427281,"(LTAG) (Schabes et al., 1988) , Combinatory Categorial Grammar (CCG) (Steedman, 2000) and head-driven phrase structure grammar (HPSG) (Pollard and Sag, 1994) which is a constraintbased highly lexicalized non-derivational generative grammar framework. To absorb both strengths of span and dependency structure, we apply both span (constituent) and dependency representations of semantic role labeling and syntactic parsing. Thus, it is a natural idea to study the relationship between constituent and dependency structures, and 4457 the joint learning of constituent and dependency syntactic parsing (Klein and Manning, 2004; Charniak and Johnson, 2005; Farkas et al., 2011; Green ˇ and Zabokrtsk´ y, 2012; Ren et al., 2013; Xu et al., 2014; Yoshikawa et al., 2017). Pre-trained Language Modeling Recently, deep contextual language model has been shown effective for learning universal language representations by leveraging large amounts of unlabeled data, achieving various state-of-the-art results in a series of NLU benchmarks. Some prominent examples are Embedding from Language models (ELMo) (Peters et al., 2018), Bidirectional Encoder Representations from Transformers (BERT) (Devlin et al., 2019) and Generalized Au"
2020.findings-emnlp.399,C18-1271,1,0.844723,"The statistics show that our LIMIT-BERT outperforms the baseline model of over all sentence lengths. For different sentence lengths, LIMIT-BERT outperforms much better than baseline model on long sentence (larger than 50) of both syntactic and semantic parsing. The possible explanation is that LIMITRelated Work Linguistics Inspired NLP Tasks With the impressive success of deep neural networks in various NLP tasks (Chen and Manning, 2014; Dozat and Manning, 2017; Ma et al., 2018; Strubell et al., 2018; Luo and Zhao, 2020; Li et al., 2020; He et al., 2019; Luo et al., 2020; Zhang et al., 2018a; Li et al., 2018a; Zhang et al., 2018b), syntactic parsing and semantic role labeling have been well developed with neural network and achieve very high performance (Chen and Manning, 2014; Dozat and Manning, 2017; Ma et al., 2018; Kitaev and Klein, 2018; Zhou and Zhao, 2019). Semantic role labeling is deeply related to syntactic structure and a number of works try to incorporate syntactic information in semantic role labeling models by different methods such as concatenation of lexicalized embedding (He et al., 2018b), usage of syntactic GCN (Li et al., 2018b) and multi-task learning (Strubell et al., 2018;"
2020.findings-emnlp.399,D18-1262,1,0.852536,"The statistics show that our LIMIT-BERT outperforms the baseline model of over all sentence lengths. For different sentence lengths, LIMIT-BERT outperforms much better than baseline model on long sentence (larger than 50) of both syntactic and semantic parsing. The possible explanation is that LIMITRelated Work Linguistics Inspired NLP Tasks With the impressive success of deep neural networks in various NLP tasks (Chen and Manning, 2014; Dozat and Manning, 2017; Ma et al., 2018; Strubell et al., 2018; Luo and Zhao, 2020; Li et al., 2020; He et al., 2019; Luo et al., 2020; Zhang et al., 2018a; Li et al., 2018a; Zhang et al., 2018b), syntactic parsing and semantic role labeling have been well developed with neural network and achieve very high performance (Chen and Manning, 2014; Dozat and Manning, 2017; Ma et al., 2018; Kitaev and Klein, 2018; Zhou and Zhao, 2019). Semantic role labeling is deeply related to syntactic structure and a number of works try to incorporate syntactic information in semantic role labeling models by different methods such as concatenation of lexicalized embedding (He et al., 2018b), usage of syntactic GCN (Li et al., 2018b) and multi-task learning (Strubell et al., 2018;"
2020.findings-emnlp.399,P19-1441,0,0.289605,"ined linguistics model2 to annotate large amounts of unlabeled text data and to combine with gold linguistics task dataset as our final training data. For such pre-processing, it is easy to train our LIMIT-BERT on large amounts of data with many tasks concurrently by simply summing up all the concerned losses together. Moreover, since every sentence has labeled with predicted syntax and semantics, we can furthermore improve the masked training objective by fully exploiting the known syntactic or semantic constituents during the language model training process. Unlike the previous work MT-DNN (Liu et al., 2019b) which only fine-tunes BERT on GLUE tasks, our LIMIT-BERT is trained on large amounts of data in a semi-supervised way and firmly supported by explicit linguistic clues. We verify the effectiveness and applicability of LIMIT-BERT on Propbank semantic parsing 3 in both span style (CoNLL-2005) (Carreras and M`arquez, 2005) and dependency style, (CoNLL2009) (Hajiˇc et al., 2009) and Penn Treebank (PTB) (Marcus et al., 1993) for both constituent and dependency syntactic parsing. Our empirical results show that semantics and syntax can indeed benefit the language representation model via multitas"
2020.findings-emnlp.399,Q13-1018,0,0.0528096,"Missing"
2020.findings-emnlp.399,2020.acl-main.571,1,0.730691,"g the sentence length of four linguistics tasks on the English dev set is shown in Figure 3. The statistics show that our LIMIT-BERT outperforms the baseline model of over all sentence lengths. For different sentence lengths, LIMIT-BERT outperforms much better than baseline model on long sentence (larger than 50) of both syntactic and semantic parsing. The possible explanation is that LIMITRelated Work Linguistics Inspired NLP Tasks With the impressive success of deep neural networks in various NLP tasks (Chen and Manning, 2014; Dozat and Manning, 2017; Ma et al., 2018; Strubell et al., 2018; Luo and Zhao, 2020; Li et al., 2020; He et al., 2019; Luo et al., 2020; Zhang et al., 2018a; Li et al., 2018a; Zhang et al., 2018b), syntactic parsing and semantic role labeling have been well developed with neural network and achieve very high performance (Chen and Manning, 2014; Dozat and Manning, 2017; Ma et al., 2018; Kitaev and Klein, 2018; Zhou and Zhao, 2019). Semantic role labeling is deeply related to syntactic structure and a number of works try to incorporate syntactic information in semantic role labeling models by different methods such as concatenation of lexicalized embedding (He et al., 2018b),"
2020.findings-emnlp.399,2020.emnlp-main.723,1,0.754231,"English dev set is shown in Figure 3. The statistics show that our LIMIT-BERT outperforms the baseline model of over all sentence lengths. For different sentence lengths, LIMIT-BERT outperforms much better than baseline model on long sentence (larger than 50) of both syntactic and semantic parsing. The possible explanation is that LIMITRelated Work Linguistics Inspired NLP Tasks With the impressive success of deep neural networks in various NLP tasks (Chen and Manning, 2014; Dozat and Manning, 2017; Ma et al., 2018; Strubell et al., 2018; Luo and Zhao, 2020; Li et al., 2020; He et al., 2019; Luo et al., 2020; Zhang et al., 2018a; Li et al., 2018a; Zhang et al., 2018b), syntactic parsing and semantic role labeling have been well developed with neural network and achieve very high performance (Chen and Manning, 2014; Dozat and Manning, 2017; Ma et al., 2018; Kitaev and Klein, 2018; Zhou and Zhao, 2019). Semantic role labeling is deeply related to syntactic structure and a number of works try to incorporate syntactic information in semantic role labeling models by different methods such as concatenation of lexicalized embedding (He et al., 2018b), usage of syntactic GCN (Li et al., 2018b) and multi-"
2020.findings-emnlp.399,P18-1130,0,0.0136602,"seline model and LIMIT-BERT while varying the sentence length of four linguistics tasks on the English dev set is shown in Figure 3. The statistics show that our LIMIT-BERT outperforms the baseline model of over all sentence lengths. For different sentence lengths, LIMIT-BERT outperforms much better than baseline model on long sentence (larger than 50) of both syntactic and semantic parsing. The possible explanation is that LIMITRelated Work Linguistics Inspired NLP Tasks With the impressive success of deep neural networks in various NLP tasks (Chen and Manning, 2014; Dozat and Manning, 2017; Ma et al., 2018; Strubell et al., 2018; Luo and Zhao, 2020; Li et al., 2020; He et al., 2019; Luo et al., 2020; Zhang et al., 2018a; Li et al., 2018a; Zhang et al., 2018b), syntactic parsing and semantic role labeling have been well developed with neural network and achieve very high performance (Chen and Manning, 2014; Dozat and Manning, 2017; Ma et al., 2018; Kitaev and Klein, 2018; Zhou and Zhao, 2019). Semantic role labeling is deeply related to syntactic structure and a number of works try to incorporate syntactic information in semantic role labeling models by different methods such as concatenation of"
2020.findings-emnlp.399,J93-2004,0,0.0864329,"e propose Linguistics Informed Multi-Task BERT (LIMIT-BERT), making an attempt to incorporate linguistic knowledge into pre-training language representation models. The proposed LIMIT-BERT is implemented in terms of MultiTask Learning (MTL) (Caruana, 1993) which has shown useful, by alleviating overfitting to a specific task, thus making the learned representations universal across tasks. Since universal language representations are learning by leveraging large amounts of unlabeled data which has quite different data volume compared with linguistics tasks dataset such as Penn Treebank (PTB)1 (Marcus et al., 1993). To alleviate such data unbalance on multi-task 1 PTB is an English treebank with syntactic tree annotation which only contains 50k sentences. 4450 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 4450–4461 c November 16 - 20, 2020. 2020 Association for Computational Linguistics learning, we apply semi-supervised learning approach that uses a pre-trained linguistics model2 to annotate large amounts of unlabeled text data and to combine with gold linguistics task dataset as our final training data. For such pre-processing, it is easy to train our LIMIT-BERT on large"
2020.findings-emnlp.399,de-marneffe-etal-2006-generating,0,0.00916053,"Missing"
2020.findings-emnlp.399,D19-1257,0,0.033793,"Missing"
2020.findings-emnlp.399,D18-1191,0,0.0158014,"g tasks once for all. 1 Introduction Recently, pre-trained language models have shown greatly effective across a range of linguistics inspired natural language processing (NLP) tasks such as syntactic parsing, semantic parsing and ∗ Corresponding author. This paper was partially supported by National Key Research and Development Program of China (No. 2017YFB0304100), Key Projects of National Natural Science Foundation of China (U1836222 and 61733011), Huawei-SJTU long term AI project, Cutting-edge Machine reading comprehension and language model. so on (Zhou and Zhao, 2019; Zhou et al., 2020; Ouchi et al., 2018; He et al., 2018b; Li et al., 2019), when taking the latter as downstream tasks for the former. In the meantime, introducing linguistic clues such as syntax and semantics into the pre-trained language models may furthermore enhance other downstream tasks such as various Natural Language Understanding (NLU) tasks (Zhang et al., 2020a,b). However, nearly all existing language models are usually trained on large amounts of unlabeled text data (Peters et al., 2018; Devlin et al., 2019), without explicitly exploiting linguistic knowledge. Such observations motivate us to jointly consider both type"
2020.findings-emnlp.399,J05-1004,0,0.11571,"on We use the model of (Zhou et al., 2020) with finetuned uncased BERTWWM (whole word masking) as the baseline9 . For fairly compared to the baseline 9 Our codes and the pre-trained https://github.com/DoodleJZ/LIMIT-BERT. BERTWWM , we also extract the language modeling layer of LIMIT-BERT and use the same model of (Zhou et al., 2020) to fine-tune. We evaluate our proposed model LIMIT-BERT and baseline model BERTWWM on CoNLL-2009 shared task (Hajiˇc et al., 2009) for dependency-style SRL, CoNLL2005 shared task (Carreras and M`arquez, 2005) for span-style SRL both using the Propbank convention (Palmer et al., 2005), and English Penn Treebank (PTB) (Marcus et al., 1993) for constituent syntactic parsing, Stanford basic dependencies (SD) representation (de Marneffe et al., 2006) converted by the Stanford parser10 for dependency syntactic parsing using the same model of (Zhou et al., 2020) to fine-tune. We follow standard data splitting and evaluate setting as (Zhou et al., 2020) and use end-to-end SRL setups of both span and dependency SRL. Since LIMIT-BERT involves all syntactic and semantic parsing tasks, it is possible to directly apply LIMIT-BERT to each task without fine-tuning and we also compare th"
2020.findings-emnlp.399,N18-1202,0,0.271512,"JTU long term AI project, Cutting-edge Machine reading comprehension and language model. so on (Zhou and Zhao, 2019; Zhou et al., 2020; Ouchi et al., 2018; He et al., 2018b; Li et al., 2019), when taking the latter as downstream tasks for the former. In the meantime, introducing linguistic clues such as syntax and semantics into the pre-trained language models may furthermore enhance other downstream tasks such as various Natural Language Understanding (NLU) tasks (Zhang et al., 2020a,b). However, nearly all existing language models are usually trained on large amounts of unlabeled text data (Peters et al., 2018; Devlin et al., 2019), without explicitly exploiting linguistic knowledge. Such observations motivate us to jointly consider both types of tasks, pre-training language models, and solving linguistics inspired NLP problems. We argue such a treatment may benefit from two-fold. (1) Joint learning is a better way to let the former help the latter in a bidirectional mode, rather than in a unidirectional mode, taking the latter as downstream tasks of the former. (2) Naturally empowered by linguistic clues from joint learning, pre-trained language models will be more powerful for enhancing downstrea"
2020.findings-emnlp.399,J08-2005,0,0.0766136,"task loss Jlt (θ) for training: Jlt (θ) = J1 (θ) + J2 (θ) + J3 (θ) + J4 (θ). At last, our LIMIT-BERT is trained for simply minimizing the overall loss: 4.2 Joverall (θ) = Jlm (θ) + Jlt (θ). Decoder Layer For syntactic parsing, we apply the joint span CKY-style algorithm to generate constituent and dependency syntactic tree simultaneously by following (Zhou and Zhao, 2019). For span and dependency SRL, we use a single dynamic programming decoder according to the uniform semantic role score following the nonoverlapping constraints: span semantic arguments for the same predicate do not overlap (Punyakanok et al., 2008). For further details of the scoring and decoder layer, please refer to (Zhou et al., 2020). 4 4.1 Experiments Evaluation We use the model of (Zhou et al., 2020) with finetuned uncased BERTWWM (whole word masking) as the baseline9 . For fairly compared to the baseline 9 Our codes and the pre-trained https://github.com/DoodleJZ/LIMIT-BERT. BERTWWM , we also extract the language modeling layer of LIMIT-BERT and use the same model of (Zhou et al., 2020) to fine-tune. We evaluate our proposed model LIMIT-BERT and baseline model BERTWWM on CoNLL-2009 shared task (Hajiˇc et al., 2009) for dependency"
2020.findings-emnlp.399,C88-2121,0,0.199348,"k learning. In addition, both span and dependency are popularly adopted annotation styles for both semantics and syntax and some work on jointly learning of semantic and syntactic (Henderson et al., 2013; Llu´ıs et al., 2013; Swayamdipta et al., 2016) . Researchers are interested in two styles of SRL models that may benefit from each other rather than their separated development, which has been roughly discussed in (Johansson and Nugues, 2008). On the other hand, researchers have discussed how to encode lexical dependencies in phrase structures, like lexicalized tree adjoining grammar (LTAG) (Schabes et al., 1988) , Combinatory Categorial Grammar (CCG) (Steedman, 2000) and head-driven phrase structure grammar (HPSG) (Pollard and Sag, 1994) which is a constraintbased highly lexicalized non-derivational generative grammar framework. To absorb both strengths of span and dependency structure, we apply both span (constituent) and dependency representations of semantic role labeling and syntactic parsing. Thus, it is a natural idea to study the relationship between constituent and dependency structures, and 4457 the joint learning of constituent and dependency syntactic parsing (Klein and Manning, 2004; Char"
2020.findings-emnlp.399,D18-1548,0,0.0397001,"Missing"
2020.findings-emnlp.399,K16-1019,0,0.0116904,"abeling models by different methods such as concatenation of lexicalized embedding (He et al., 2018b), usage of syntactic GCN (Li et al., 2018b) and multi-task learning (Strubell et al., 2018; Zhou et al., 2020). Besides semantic role labeling and syntactic parsing are two key tasks of semantics and syntax so that they are included into our linguistics tasks for multi-task learning. In addition, both span and dependency are popularly adopted annotation styles for both semantics and syntax and some work on jointly learning of semantic and syntactic (Henderson et al., 2013; Llu´ıs et al., 2013; Swayamdipta et al., 2016) . Researchers are interested in two styles of SRL models that may benefit from each other rather than their separated development, which has been roughly discussed in (Johansson and Nugues, 2008). On the other hand, researchers have discussed how to encode lexical dependencies in phrase structures, like lexicalized tree adjoining grammar (LTAG) (Schabes et al., 1988) , Combinatory Categorial Grammar (CCG) (Steedman, 2000) and head-driven phrase structure grammar (HPSG) (Pollard and Sag, 1994) which is a constraintbased highly lexicalized non-derivational generative grammar framework. To absor"
2020.findings-emnlp.399,W18-5446,0,0.0658492,"Missing"
2020.findings-emnlp.399,P14-1021,0,0.0215205,"ar (HPSG) (Pollard and Sag, 1994) which is a constraintbased highly lexicalized non-derivational generative grammar framework. To absorb both strengths of span and dependency structure, we apply both span (constituent) and dependency representations of semantic role labeling and syntactic parsing. Thus, it is a natural idea to study the relationship between constituent and dependency structures, and 4457 the joint learning of constituent and dependency syntactic parsing (Klein and Manning, 2004; Charniak and Johnson, 2005; Farkas et al., 2011; Green ˇ and Zabokrtsk´ y, 2012; Ren et al., 2013; Xu et al., 2014; Yoshikawa et al., 2017). Pre-trained Language Modeling Recently, deep contextual language model has been shown effective for learning universal language representations by leveraging large amounts of unlabeled data, achieving various state-of-the-art results in a series of NLU benchmarks. Some prominent examples are Embedding from Language models (ELMo) (Peters et al., 2018), Bidirectional Encoder Representations from Transformers (BERT) (Devlin et al., 2019) and Generalized Autoregressive Pretraining (XLNet) (Yang et al., 2019). Many latest works make attempts to modify the language model b"
2020.findings-emnlp.399,N18-1089,0,0.0483443,"Missing"
2020.findings-emnlp.399,P17-1026,0,0.0159332,"d and Sag, 1994) which is a constraintbased highly lexicalized non-derivational generative grammar framework. To absorb both strengths of span and dependency structure, we apply both span (constituent) and dependency representations of semantic role labeling and syntactic parsing. Thus, it is a natural idea to study the relationship between constituent and dependency structures, and 4457 the joint learning of constituent and dependency syntactic parsing (Klein and Manning, 2004; Charniak and Johnson, 2005; Farkas et al., 2011; Green ˇ and Zabokrtsk´ y, 2012; Ren et al., 2013; Xu et al., 2014; Yoshikawa et al., 2017). Pre-trained Language Modeling Recently, deep contextual language model has been shown effective for learning universal language representations by leveraging large amounts of unlabeled data, achieving various state-of-the-art results in a series of NLU benchmarks. Some prominent examples are Embedding from Language models (ELMo) (Peters et al., 2018), Bidirectional Encoder Representations from Transformers (BERT) (Devlin et al., 2019) and Generalized Autoregressive Pretraining (XLNet) (Yang et al., 2019). Many latest works make attempts to modify the language model based on BERT such as ELEC"
2020.findings-emnlp.399,D18-1511,1,0.822253,"s shown in Figure 3. The statistics show that our LIMIT-BERT outperforms the baseline model of over all sentence lengths. For different sentence lengths, LIMIT-BERT outperforms much better than baseline model on long sentence (larger than 50) of both syntactic and semantic parsing. The possible explanation is that LIMITRelated Work Linguistics Inspired NLP Tasks With the impressive success of deep neural networks in various NLP tasks (Chen and Manning, 2014; Dozat and Manning, 2017; Ma et al., 2018; Strubell et al., 2018; Luo and Zhao, 2020; Li et al., 2020; He et al., 2019; Luo et al., 2020; Zhang et al., 2018a; Li et al., 2018a; Zhang et al., 2018b), syntactic parsing and semantic role labeling have been well developed with neural network and achieve very high performance (Chen and Manning, 2014; Dozat and Manning, 2017; Ma et al., 2018; Kitaev and Klein, 2018; Zhou and Zhao, 2019). Semantic role labeling is deeply related to syntactic structure and a number of works try to incorporate syntactic information in semantic role labeling models by different methods such as concatenation of lexicalized embedding (He et al., 2018b), usage of syntactic GCN (Li et al., 2018b) and multi-task learning (Strub"
2020.findings-emnlp.399,C18-1317,1,0.736155,"s shown in Figure 3. The statistics show that our LIMIT-BERT outperforms the baseline model of over all sentence lengths. For different sentence lengths, LIMIT-BERT outperforms much better than baseline model on long sentence (larger than 50) of both syntactic and semantic parsing. The possible explanation is that LIMITRelated Work Linguistics Inspired NLP Tasks With the impressive success of deep neural networks in various NLP tasks (Chen and Manning, 2014; Dozat and Manning, 2017; Ma et al., 2018; Strubell et al., 2018; Luo and Zhao, 2020; Li et al., 2020; He et al., 2019; Luo et al., 2020; Zhang et al., 2018a; Li et al., 2018a; Zhang et al., 2018b), syntactic parsing and semantic role labeling have been well developed with neural network and achieve very high performance (Chen and Manning, 2014; Dozat and Manning, 2017; Ma et al., 2018; Kitaev and Klein, 2018; Zhou and Zhao, 2019). Semantic role labeling is deeply related to syntactic structure and a number of works try to incorporate syntactic information in semantic role labeling models by different methods such as concatenation of lexicalized embedding (He et al., 2018b), usage of syntactic GCN (Li et al., 2018b) and multi-task learning (Strub"
2020.findings-emnlp.399,2020.findings-emnlp.398,1,0.81577,"language processing tasks once for all. 1 Introduction Recently, pre-trained language models have shown greatly effective across a range of linguistics inspired natural language processing (NLP) tasks such as syntactic parsing, semantic parsing and ∗ Corresponding author. This paper was partially supported by National Key Research and Development Program of China (No. 2017YFB0304100), Key Projects of National Natural Science Foundation of China (U1836222 and 61733011), Huawei-SJTU long term AI project, Cutting-edge Machine reading comprehension and language model. so on (Zhou and Zhao, 2019; Zhou et al., 2020; Ouchi et al., 2018; He et al., 2018b; Li et al., 2019), when taking the latter as downstream tasks for the former. In the meantime, introducing linguistic clues such as syntax and semantics into the pre-trained language models may furthermore enhance other downstream tasks such as various Natural Language Understanding (NLU) tasks (Zhang et al., 2020a,b). However, nearly all existing language models are usually trained on large amounts of unlabeled text data (Peters et al., 2018; Devlin et al., 2019), without explicitly exploiting linguistic knowledge. Such observations motivate us to jointl"
2020.findings-emnlp.399,P19-1230,1,0.864611,"ti-purpose of natural language processing tasks once for all. 1 Introduction Recently, pre-trained language models have shown greatly effective across a range of linguistics inspired natural language processing (NLP) tasks such as syntactic parsing, semantic parsing and ∗ Corresponding author. This paper was partially supported by National Key Research and Development Program of China (No. 2017YFB0304100), Key Projects of National Natural Science Foundation of China (U1836222 and 61733011), Huawei-SJTU long term AI project, Cutting-edge Machine reading comprehension and language model. so on (Zhou and Zhao, 2019; Zhou et al., 2020; Ouchi et al., 2018; He et al., 2018b; Li et al., 2019), when taking the latter as downstream tasks for the former. In the meantime, introducing linguistic clues such as syntax and semantics into the pre-trained language models may furthermore enhance other downstream tasks such as various Natural Language Understanding (NLU) tasks (Zhang et al., 2020a,b). However, nearly all existing language models are usually trained on large amounts of unlabeled text data (Peters et al., 2018; Devlin et al., 2019), without explicitly exploiting linguistic knowledge. Such observations mo"
2020.wmt-1.22,D18-1399,0,0.0372296,"Missing"
2020.wmt-1.22,N18-1118,0,0.0256328,"al., 2017), sentence-level NMT has been based on strong independence and locality assumptions generally, in which the interrelations among these discourse (Jurafsky, 2000) elements were ignored. This results in that the translations may be perfect at the sentence-level but lack crucial properties of the text, hindering understanding (Maruf et al., 2019). To help to resolve ambiguities and inconsistencies in translations, some MT pioneers (Bar-Hillel, 1960; Xiong et al., 2013; Sennrich, 2018) exploit the underlying discourse structure information of a text to address this issue, while others (Bawden et al., 2018; Voita et al., 2018; Jean and Cho, 2019; Wang et al., 2019; Scherrer et al., 2019) extend the translation units with the context or use an additional context encoder and attention. It is worth noting that the essence of the document-level NMT claimed with additional context and attention is still sentence-level MT, whose translation is still output sentence by sentence. We named it as documentenhanced NMT more precisely. Due to computational efficiency and tractability concerns, the document-enhanced NMT models mostly used document embedding, document topic information, and limited past or fu"
2020.wmt-1.22,P18-1192,1,0.83718,"019) is adopted. In the unsupervised and low-resource track, we draw on the successful experience of the XLM framework (Conneau et al., 2019), and used the two-stage training mode of masked language modeling (MLM) pre-training + back-translation (BT) finetune to obtain a very strong baseline performance. Marian (JunczysDowmunt et al., 2018) toolkit is utilized for training the decoder in reranking using machine translation targets instead of common GPT-style language modeling targets. In order to better play the role of WMT evaluation in polishing the methods proposed or improved by our team (He et al., 2018; Li et al., 2018; Zhang et al., 2018; Zhang and Zhao, 2018; Xiao et al., 2019; Zhou and Zhao, 2019; Li et al., 2019b; Luo and Zhao, 2020), we divided the three language pairs we participated in into three categories: 1. Traditional language pair with rich parallel corpus: EN-PL, 2. Language pair with document-level information: EN-ZH, 3. Language pair with no or low parallel resources: DE-HSB. In the supervised PL→EN translation direction, we based on the XLM framework to pre-train a Polish language model using common crawl and news crawl monolingual data, and proposed the XLM enhanced NMT mo"
2020.wmt-1.22,D19-5603,1,0.814693,"rmance degradation caused by domain inconsistency. For the final submission, an ensemble of several different trained models outputs the n-best predictions, and used the decoder trained with Marian toolkit to performs reranking to get the final system output. 2 2.1 Methodology XLM-enhanced NMT Pre-trained language models such as ELMo (Peters et al., 2018), BERT (Devlin et al., 2019), XLM (Conneau et al., 2019), XLNet (Yang et al., 2019), ALBERT (Lan et al., 2019) etc. have recently demonstrated a very dominant effect on natural language processing tasks. Several works (Clinchant et al., 2019; Imamura and Sumita, 2019; Zhu et al., 2020) leveraged a pre-trained BERT model for improving NMT and found that BERT can bring significantly better results over the baseline. Since BERT and other pre-trained language models are trained on large scale corpus beyond the data provided by the WMT20 organizers, the direct use of BERT will make the system submitted unconstrained. Using an XLM model, a variant of BERT, pre-trained from scratch on the monolingual data provided by the official to enhance our NMT model, is a good choice to keep the system constrained. Moreover, the XLM model has the advantages of simple traini"
2020.wmt-1.22,P19-4007,0,0.0555909,"Missing"
2020.wmt-1.22,J82-2005,0,0.698509,"Missing"
2020.wmt-1.22,P19-1285,0,0.0196835,"ting that the essence of the document-level NMT claimed with additional context and attention is still sentence-level MT, whose translation is still output sentence by sentence. We named it as documentenhanced NMT more precisely. Due to computational efficiency and tractability concerns, the document-enhanced NMT models mostly used document embedding, document topic information, and limited past or future context sentences, etc., rather than the truly whole document information. Recently, with the increase in computational power available to us and the well-designed neural network structures (Dai et al., 2019; Kitaev et al., 2019; Beltagy et al., 2020) for long sequence encoding, we are finally in a position to employ the whole document information for enhancing sentence-level NMT. In addition, we argue that since long sequences encoding is easier than decoding, truly whole document-level translation is still a long way off, since the bidirectional context is available in the encoder, but only the past is visible by the decoder. Longformer To make the long documents processed with Transformer (Vaswani et al., 2017) architecture feasible or easier, a modified Transformer architecture named Longform"
2020.wmt-1.22,P19-1120,0,0.0384069,"Missing"
2020.wmt-1.22,N19-1423,0,0.0103748,"the TF-IDF algorithm is employed to filter the training set according to the input of the test set, a training subset whose domain is more similar to the test set is obtained, and then used to finetune the model for reducing the performance degradation caused by domain inconsistency. For the final submission, an ensemble of several different trained models outputs the n-best predictions, and used the decoder trained with Marian toolkit to performs reranking to get the final system output. 2 2.1 Methodology XLM-enhanced NMT Pre-trained language models such as ELMo (Peters et al., 2018), BERT (Devlin et al., 2019), XLM (Conneau et al., 2019), XLNet (Yang et al., 2019), ALBERT (Lan et al., 2019) etc. have recently demonstrated a very dominant effect on natural language processing tasks. Several works (Clinchant et al., 2019; Imamura and Sumita, 2019; Zhu et al., 2020) leveraged a pre-trained BERT model for improving NMT and found that BERT can bring significantly better results over the baseline. Since BERT and other pre-trained language models are trained on large scale corpus beyond the data provided by the WMT20 organizers, the direct use of BERT will make the system submitted unconstrained. Using an"
2020.wmt-1.22,J93-1004,0,0.207938,"ull model with the obtained model. The PLM-encoder attention attnP and PLM-decoder attention attnPC are randomly initialized. EN-PL On the language pair EN-PL, we explored performance in two training data settings. The first is base data, including Europarl v10, Tilde Rapid corpus, and WikiMatrix bitext data, whose raw data is on the sentence-level. In the second setting base data + paracrawl, we converted the paragraph-level alignment data in Paracrawl to sentence-level alignment and incorporated it with the base data. In the conversion process, we adopted the method and program proposed by (Gale and Church, 1993) for aligning sentences based on a simple statistical model of character lengths, which uses the fact that longer sentences in one language tend to be translated into longer sentences in the other language, and that shorter sentences tend to be translated into shorter 224 Systems Transformer big +D2GPo XLM-enhanced Document-enhanced Ensemble ++TF-IDF finetune ++Re-ranking 19test Test BLEU BLEU chrF 37.2 37.7 38.9 39.2 40.0 40.2 40.5 48.6 48.8 49.1 0.418 0.422 0.427 (NSP) classification model provided by Google for document interval prediction to recover the documents. DE-HSB In RUNMT on EN-DE-"
2020.wmt-1.22,P07-2045,0,0.0121171,"s the adjustable parameters. ∀t ∈ Dterms ], (12) where Dterms indicates the all terms set in corpus D. We calculate the cosine similarity as final scores between the query and every source sentence in corpus, and ranked on the scores to get the topK pairs (K=1000 in our experiments) as the subtraining set for finetuning. 3 Data Preprocessing and Model Setup Before model training, we preprocessed the data uniformly and customized the processing according to the requirements of each model. We normalized punctuation, remove non-printing characters, and tokenize all data with the Moses tokenizer (Koehn et al., 2007) except for the Chinese. For Chinese, we removed the segmentation space in some training data and then use PKUSeg (Luo et al., 2019) toolkit to cut all Chinese sentences, so as to obtain unified word segmentation annotations. We use joint byte pair encodings (BPE) with 40K split operations for subword segmentation (Sennrich et al., 2016). In XLM-enhanced NMT and Documentenhanced NMT, we first train a basic NMT (Transformer big) model on the sentence-level data until convergence, then initialize the encoder and decoder of the XLM-enhanced NMT and Document-enhanced NMT full model with the obtain"
2020.wmt-1.22,C18-1271,1,0.818947,"In the unsupervised and low-resource track, we draw on the successful experience of the XLM framework (Conneau et al., 2019), and used the two-stage training mode of masked language modeling (MLM) pre-training + back-translation (BT) finetune to obtain a very strong baseline performance. Marian (JunczysDowmunt et al., 2018) toolkit is utilized for training the decoder in reranking using machine translation targets instead of common GPT-style language modeling targets. In order to better play the role of WMT evaluation in polishing the methods proposed or improved by our team (He et al., 2018; Li et al., 2018; Zhang et al., 2018; Zhang and Zhao, 2018; Xiao et al., 2019; Zhou and Zhao, 2019; Li et al., 2019b; Luo and Zhao, 2020), we divided the three language pairs we participated in into three categories: 1. Traditional language pair with rich parallel corpus: EN-PL, 2. Language pair with document-level information: EN-ZH, 3. Language pair with no or low parallel resources: DE-HSB. In the supervised PL→EN translation direction, we based on the XLM framework to pre-train a Polish language model using common crawl and news crawl monolingual data, and proposed the XLM enhanced NMT model inspired from"
2020.wmt-1.22,N19-4009,0,0.0819852,"(U1836222 and 61733011), Huawei-SJTU Long Term AI Project, Cuttingedge Machine Reading Comprehension and Language Model. Rui Wang was partially supported by JSPS grant-in-aid for early-career scientists (19K20354): “Unsupervised Neural Machine Translation in Universal Scenarios” and NICT tenuretrack researcher startup fund “Toward Intelligent Machine Translation”. (DE) ↔ Upper Sorbian (HSB) both directions are focused. Our baseline system in supervised track is based on the Transformer big architecture proposed by Vaswani et al. (2017), in which its opensource implementation version Fairseq (Ott et al., 2019) is adopted. In the unsupervised and low-resource track, we draw on the successful experience of the XLM framework (Conneau et al., 2019), and used the two-stage training mode of masked language modeling (MLM) pre-training + back-translation (BT) finetune to obtain a very strong baseline performance. Marian (JunczysDowmunt et al., 2018) toolkit is utilized for training the decoder in reranking using machine translation targets instead of common GPT-style language modeling targets. In order to better play the role of WMT evaluation in polishing the methods proposed or improved by our team (He e"
2020.wmt-1.22,N09-2056,1,0.713764,"eference agreement mechanism. Specifically, we proposed three kinds of reference agreement utilization approaches in (Li et al., 2020b): reference agreement translation (RAT), reference agreement back-translation (RABT), and cross-lingual back-translation (XBT). (6) LRABT (S, T , R) = L(θT →S ) + L(θT →R ). (9) XBT The parallel corpus between languages S and R can not only bring agreement in the translations of the same target language T , but also cross-lingual agreement, that is, using the target language as the bridge to form pivot translation (Wu and Wang, 2007; Utiyama and Isahara, 2007; Paul et al., 2009) patterns: S → T → R and R → T → S. In XBT, paired sentences s and r are translated to language T : t˜s and t˜r , and forms two new pseudo-parallel pairs: ht˜s , ri and ht˜r , si, which promote the training of translation T → R and T → S. The objective function of XBT is: RAT RAT utilizes the principle for translating paired sentences into the target language T of the source S and reference R language. Since the input the parallel, the both translation outputs should be the same. Given a parallel sentence pair hs, ri between language S and R, we would ideally have P(·|s; θS→T ) = P(·|r; θR→T )"
2020.wmt-1.22,N18-1202,0,0.00847445,"model training is finished, the TF-IDF algorithm is employed to filter the training set according to the input of the test set, a training subset whose domain is more similar to the test set is obtained, and then used to finetune the model for reducing the performance degradation caused by domain inconsistency. For the final submission, an ensemble of several different trained models outputs the n-best predictions, and used the decoder trained with Marian toolkit to performs reranking to get the final system output. 2 2.1 Methodology XLM-enhanced NMT Pre-trained language models such as ELMo (Peters et al., 2018), BERT (Devlin et al., 2019), XLM (Conneau et al., 2019), XLNet (Yang et al., 2019), ALBERT (Lan et al., 2019) etc. have recently demonstrated a very dominant effect on natural language processing tasks. Several works (Clinchant et al., 2019; Imamura and Sumita, 2019; Zhu et al., 2020) leveraged a pre-trained BERT model for improving NMT and found that BERT can bring significantly better results over the baseline. Since BERT and other pre-trained language models are trained on large scale corpus beyond the data provided by the WMT20 organizers, the direct use of BERT will make the system submi"
2020.wmt-1.22,2020.findings-emnlp.371,1,0.64243,"Missing"
2020.wmt-1.22,2021.ccl-1.108,0,0.100832,"Missing"
2020.wmt-1.22,P12-3005,0,0.0427849,"gned to each proposed correspondence of sentences, based on the scaled difference of lengths of the two sentences (in characters) and the variance of this difference. This probabilistic score is used in a dynamic programming framework to find the maximum likelihood alignment of sentences. For the Polish pre-trained XLM language model, we used all NewsCrawl monolingual data and some CommonCrawl monolingual data. Since the CommonCrawl data is very large and noisy and can potentially decrease the performance of LM if it is used in its raw form. We apply language identification filtering (langid; Lui and Baldwin (2012)), keeping sentences with correct languages. In order to filter out the sentences shorter than 5 words or longer than 150 words more precisely, we re-split sentences using Spacy (Honnibal and Montani, 2017) toolkit. EN-ZH In EN-ZH, the pre-training of Longformer as a document encoder is unique. As described in (Beltagy et al., 2020), the Longformer needs a large number of gradient updates to learn the local context first; before learning to utilize longer context. In the first phase of the staged training procedure, an initial RoBERTa (Liu et al., 2019) model implemented in Fairseq (Ott et al."
2020.wmt-1.22,2020.acl-main.571,1,0.800756,", 2019), and used the two-stage training mode of masked language modeling (MLM) pre-training + back-translation (BT) finetune to obtain a very strong baseline performance. Marian (JunczysDowmunt et al., 2018) toolkit is utilized for training the decoder in reranking using machine translation targets instead of common GPT-style language modeling targets. In order to better play the role of WMT evaluation in polishing the methods proposed or improved by our team (He et al., 2018; Li et al., 2018; Zhang et al., 2018; Zhang and Zhao, 2018; Xiao et al., 2019; Zhou and Zhao, 2019; Li et al., 2019b; Luo and Zhao, 2020), we divided the three language pairs we participated in into three categories: 1. Traditional language pair with rich parallel corpus: EN-PL, 2. Language pair with document-level information: EN-ZH, 3. Language pair with no or low parallel resources: DE-HSB. In the supervised PL→EN translation direction, we based on the XLM framework to pre-train a Polish language model using common crawl and news crawl monolingual data, and proposed the XLM enhanced NMT model inspired from the idea of incorporating BERT into NMT (Zhu et al., 2020). Besides, we trained a bidirectional translation model of EN-"
2020.wmt-1.22,W18-6319,0,0.0144898,"uent phase, we trained the model on the paragraph text, doubled the window size and the sequence length, and halve the learning rate. For the paragraph text, the Wikidumps and NewsCommentary v15 have document intervals and can be used directly, while UN v1.0 has no document intervals but the sentence order is not interrupted. Therefore, we use the BERT Next Sentence Prediction Results and Analysis Results and ablations for PL→EN2 are shown in Table 1, EN→ZH in Table 2, unsupervised DE↔HSB in Table 3 and low-resource DE↔HSB in Table 4. We report case-sensitive SacreBLEU scores using SacreBLEU (Post, 2018) for EN-PL, DE-HSB, and BLEU based on characters for ENZH. In the results, “+” means addition based on baseline, and “++” means cumulative addition based on the previous one. In PL→EN, the introduction of ParaCrawl data improves the baseline performance on the dev dataset by about 4.2 BLEU. +D2GPo, XLM-enhanced NMT, Bidirectional NMT, and ensembling outperforms our strong baseline by 2 BLEU point. Finally, finetuning and reranking further gives another 0.5 BLEU. For EN→ZH, as with PL→EN, we see similar improvements with +D2GPo, XLM-enhanced NMT, ensembling and reranking. We also observe that t"
2020.wmt-1.22,D19-6506,0,0.0216909,"irections, the differences are also very obvious. To further expose the model to the direction difference and improve the effect of unidirectional translation, we further finetune the 220 bidirectional pre-trained model on the bilingual data. Take S2T translation as an example; the model is optimized as follows: L(θS→T ) = N X log p(y (n) |x(n) ), (5) n=1 where θS→T is the parameters of child model which is initialized with θparent . Similarly, the T2S child model can also be obtained. Due to the introduction of bidirectional translation in one model, follow the practice of Conneau and Lample (2019), shared subword vocabulary and shared encoder-decoder (source and target) embedding were employed to improves the alignment of embedding spaces across languages. In addition, since the encoder and decoder need to be able to handle two languages simultaneously, a language embedding was used to indicate the language being processed, so as to reduce confusion of the model. 2.3 Document-enhanced NMT In spite of its success (Vaswani et al., 2017), sentence-level NMT has been based on strong independence and locality assumptions generally, in which the interrelations among these discourse (Jurafsky"
2020.wmt-1.22,P16-1162,0,0.0703717,"cessing and Model Setup Before model training, we preprocessed the data uniformly and customized the processing according to the requirements of each model. We normalized punctuation, remove non-printing characters, and tokenize all data with the Moses tokenizer (Koehn et al., 2007) except for the Chinese. For Chinese, we removed the segmentation space in some training data and then use PKUSeg (Luo et al., 2019) toolkit to cut all Chinese sentences, so as to obtain unified word segmentation annotations. We use joint byte pair encodings (BPE) with 40K split operations for subword segmentation (Sennrich et al., 2016). In XLM-enhanced NMT and Documentenhanced NMT, we first train a basic NMT (Transformer big) model on the sentence-level data until convergence, then initialize the encoder and decoder of the XLM-enhanced NMT and Document-enhanced NMT full model with the obtained model. The PLM-encoder attention attnP and PLM-decoder attention attnPC are randomly initialized. EN-PL On the language pair EN-PL, we explored performance in two training data settings. The first is base data, including Europarl v10, Tilde Rapid corpus, and WikiMatrix bitext data, whose raw data is on the sentence-level. In the secon"
2020.wmt-1.22,2021.naacl-main.311,1,0.626638,"Missing"
2020.wmt-1.22,P95-1026,0,0.836605,"e unlabeled dataset U = {x(j) }L j=1 is used for the synthesis of pseudo-parallel corpora. While in UNMT, since the model is trained with backtranslation on unpaired monolingual data, the pseudo-parallel corpora is synthesized by the monolingual data, i.e., U = {x(m) }M m=1 . Considering the translation quality can’t effectively be evaluated across languages in machine translation with only the monolingual data, therefore the selection of the subset Q, is one of the key factors for self-training. It is usually selected based on some confidence scores (e.g. log probability or perplexity, PPL) (Yarowsky, 1995), but it is also possible for S to be the whole pseudo parallel data (Zhu and Goldberg, 2009). In the backward translation based on the pseudo-parallel data, the DAE method widely used in UNMT can alleviate the impact of the noise resulted from the synthesized sentences on model training, since the synthesized sentences are only used as input. However, in the forward translation training, the quality of noisy targets will directly affect the success of the model training. Therefore, the selection of synthetic parallel corpus becomes particularly critical. fθT →S 5: Calculate BT-BLEU B for two"
2020.wmt-1.22,N07-1061,1,0.7001,"Missing"
2020.wmt-1.22,P18-1117,0,0.0215559,"-level NMT has been based on strong independence and locality assumptions generally, in which the interrelations among these discourse (Jurafsky, 2000) elements were ignored. This results in that the translations may be perfect at the sentence-level but lack crucial properties of the text, hindering understanding (Maruf et al., 2019). To help to resolve ambiguities and inconsistencies in translations, some MT pioneers (Bar-Hillel, 1960; Xiong et al., 2013; Sennrich, 2018) exploit the underlying discourse structure information of a text to address this issue, while others (Bawden et al., 2018; Voita et al., 2018; Jean and Cho, 2019; Wang et al., 2019; Scherrer et al., 2019) extend the translation units with the context or use an additional context encoder and attention. It is worth noting that the essence of the document-level NMT claimed with additional context and attention is still sentence-level MT, whose translation is still output sentence by sentence. We named it as documentenhanced NMT more precisely. Due to computational efficiency and tractability concerns, the document-enhanced NMT models mostly used document embedding, document topic information, and limited past or future context sentenc"
2020.wmt-1.22,P07-1108,0,0.0458195,"truction training of UNMT through a proposed reference agreement mechanism. Specifically, we proposed three kinds of reference agreement utilization approaches in (Li et al., 2020b): reference agreement translation (RAT), reference agreement back-translation (RABT), and cross-lingual back-translation (XBT). (6) LRABT (S, T , R) = L(θT →S ) + L(θT →R ). (9) XBT The parallel corpus between languages S and R can not only bring agreement in the translations of the same target language T , but also cross-lingual agreement, that is, using the target language as the bridge to form pivot translation (Wu and Wang, 2007; Utiyama and Isahara, 2007; Paul et al., 2009) patterns: S → T → R and R → T → S. In XBT, paired sentences s and r are translated to language T : t˜s and t˜r , and forms two new pseudo-parallel pairs: ht˜s , ri and ht˜r , si, which promote the training of translation T → R and T → S. The objective function of XBT is: RAT RAT utilizes the principle for translating paired sentences into the target language T of the source S and reference R language. Since the input the parallel, the both translation outputs should be the same. Given a parallel sentence pair hs, ri between language S and R, we w"
2020.wmt-1.22,P19-1298,1,0.820418,"successful experience of the XLM framework (Conneau et al., 2019), and used the two-stage training mode of masked language modeling (MLM) pre-training + back-translation (BT) finetune to obtain a very strong baseline performance. Marian (JunczysDowmunt et al., 2018) toolkit is utilized for training the decoder in reranking using machine translation targets instead of common GPT-style language modeling targets. In order to better play the role of WMT evaluation in polishing the methods proposed or improved by our team (He et al., 2018; Li et al., 2018; Zhang et al., 2018; Zhang and Zhao, 2018; Xiao et al., 2019; Zhou and Zhao, 2019; Li et al., 2019b; Luo and Zhao, 2020), we divided the three language pairs we participated in into three categories: 1. Traditional language pair with rich parallel corpus: EN-PL, 2. Language pair with document-level information: EN-ZH, 3. Language pair with no or low parallel resources: DE-HSB. In the supervised PL→EN translation direction, we based on the XLM framework to pre-train a Polish language model using common crawl and news crawl monolingual data, and proposed the XLM enhanced NMT model inspired from the idea of incorporating BERT into NMT (Zhu et al., 2020)."
2020.wmt-1.22,D13-1163,0,0.0303897,"dicate the language being processed, so as to reduce confusion of the model. 2.3 Document-enhanced NMT In spite of its success (Vaswani et al., 2017), sentence-level NMT has been based on strong independence and locality assumptions generally, in which the interrelations among these discourse (Jurafsky, 2000) elements were ignored. This results in that the translations may be perfect at the sentence-level but lack crucial properties of the text, hindering understanding (Maruf et al., 2019). To help to resolve ambiguities and inconsistencies in translations, some MT pioneers (Bar-Hillel, 1960; Xiong et al., 2013; Sennrich, 2018) exploit the underlying discourse structure information of a text to address this issue, while others (Bawden et al., 2018; Voita et al., 2018; Jean and Cho, 2019; Wang et al., 2019; Scherrer et al., 2019) extend the translation units with the context or use an additional context encoder and attention. It is worth noting that the essence of the document-level NMT claimed with additional context and attention is still sentence-level MT, whose translation is still output sentence by sentence. We named it as documentenhanced NMT more precisely. Due to computational efficiency and"
2020.wmt-1.22,C18-1317,1,0.818967,"ed and low-resource track, we draw on the successful experience of the XLM framework (Conneau et al., 2019), and used the two-stage training mode of masked language modeling (MLM) pre-training + back-translation (BT) finetune to obtain a very strong baseline performance. Marian (JunczysDowmunt et al., 2018) toolkit is utilized for training the decoder in reranking using machine translation targets instead of common GPT-style language modeling targets. In order to better play the role of WMT evaluation in polishing the methods proposed or improved by our team (He et al., 2018; Li et al., 2018; Zhang et al., 2018; Zhang and Zhao, 2018; Xiao et al., 2019; Zhou and Zhao, 2019; Li et al., 2019b; Luo and Zhao, 2020), we divided the three language pairs we participated in into three categories: 1. Traditional language pair with rich parallel corpus: EN-PL, 2. Language pair with document-level information: EN-ZH, 3. Language pair with no or low parallel resources: DE-HSB. In the supervised PL→EN translation direction, we based on the XLM framework to pre-train a Polish language model using common crawl and news crawl monolingual data, and proposed the XLM enhanced NMT model inspired from the idea of incorpo"
2020.wmt-1.22,P19-1230,1,0.815888,"ce of the XLM framework (Conneau et al., 2019), and used the two-stage training mode of masked language modeling (MLM) pre-training + back-translation (BT) finetune to obtain a very strong baseline performance. Marian (JunczysDowmunt et al., 2018) toolkit is utilized for training the decoder in reranking using machine translation targets instead of common GPT-style language modeling targets. In order to better play the role of WMT evaluation in polishing the methods proposed or improved by our team (He et al., 2018; Li et al., 2018; Zhang et al., 2018; Zhang and Zhao, 2018; Xiao et al., 2019; Zhou and Zhao, 2019; Li et al., 2019b; Luo and Zhao, 2020), we divided the three language pairs we participated in into three categories: 1. Traditional language pair with rich parallel corpus: EN-PL, 2. Language pair with document-level information: EN-ZH, 3. Language pair with no or low parallel resources: DE-HSB. In the supervised PL→EN translation direction, we based on the XLM framework to pre-train a Polish language model using common crawl and news crawl monolingual data, and proposed the XLM enhanced NMT model inspired from the idea of incorporating BERT into NMT (Zhu et al., 2020). Besides, we trained a"
2020.wmt-1.22,D16-1163,0,0.0162403,"t , where [0, pnet 2 ) is the probability of attending to L, the final sum for the first attn in HEL and HD pnet pnet [ 2 , 1 − 2 ) is the probability for the whole HEL L equation, [1 − pnet , 1] is the probability and HD 2 L. for the second attn in in HEL and HD 2.2 Bidirectional NMT Machine translation, in general, is unidirectional, that is, from the source language to the target language. The encoder-decoder framework for NMT has been shown effective in large data scenarios, and the more high-quality bilingual training data, the better performance the model tends to achieve. Recent works (Zoph et al., 2016; Kim et al., 2019) on translation transfer learning (Torrey and Shavlik, 2010; Pan and Yang, 2009) from rich-resource language pairs to low-resource language pairs demonstrate that translation has some universal nature in essence between different language pairs. As the sourceto-target (S2T) forward translation and target-tosource (T2S) backward translation can be seen as two special language pairs in bilingual translation, it can make use of the translation universal nature to improve each other, i.e., dual learning (He et al., 2016). Based on this motivation, we developed a bidirectional NM"
2021.acl-long.398,D19-5509,0,0.0254508,"tor architecture. The aforementioned models may seemingly handle different sized input sequences, but all of them focus on sentence-level specific representation still for each word, which may cause unsatisfactory performance in real-world situations. There are a series of downstream NLP tasks especially on question answering which may be conveniently and effectively solved through ULR like solution. Actually, though in different forms, these tasks more and more tend to be solved by our suggested ULR model, including dialogue utterance regularization (Cao et al., 2020), question paraphrasing (Bonadiman et al., 2019), measuring QA similarities in FAQ tasks (Damani et al., 2020; Sakata et al., 2019). Related Work Previous language representation learning methods such as Word2Vec (Mikolov et al., 2013), GloVe (Pennington et al., 2014), LASER (Artetxe and Schwenk, 2019), InferSent (Conneau et al., 2017) and USE (Cer et al., 2018) focus on specific granular linguistic units, e.g., words or sentences. Later proposed ELMo (Peters et al., 2018), OpenAI GPT (Radford et al., 2018), BERT (Devlin et al., 2019) and XLNet (Yang et al., 2020) learns contextualized representation for each input token. Although such pre-"
2021.acl-long.398,2020.acl-main.608,0,0.0181969,"raining through a generator and discriminator architecture. The aforementioned models may seemingly handle different sized input sequences, but all of them focus on sentence-level specific representation still for each word, which may cause unsatisfactory performance in real-world situations. There are a series of downstream NLP tasks especially on question answering which may be conveniently and effectively solved through ULR like solution. Actually, though in different forms, these tasks more and more tend to be solved by our suggested ULR model, including dialogue utterance regularization (Cao et al., 2020), question paraphrasing (Bonadiman et al., 2019), measuring QA similarities in FAQ tasks (Damani et al., 2020; Sakata et al., 2019). Related Work Previous language representation learning methods such as Word2Vec (Mikolov et al., 2013), GloVe (Pennington et al., 2014), LASER (Artetxe and Schwenk, 2019), InferSent (Conneau et al., 2017) and USE (Cer et al., 2018) focus on specific granular linguistic units, e.g., words or sentences. Later proposed ELMo (Peters et al., 2018), OpenAI GPT (Radford et al., 2018), BERT (Devlin et al., 2019) and XLNet (Yang et al., 2020) learns contextualized represe"
2021.acl-long.398,S17-2001,0,0.0148814,"ingle-Sentence Classification Single-sentence classification tasks includes SST-2 (Socher et al., 2013), a sentiment classification task, and CoLA (Warstadt et al., 2019), a task that is to determine whether a sentence is grammatically acceptable. Natural Language Inference GLUE contains four NLI tasks: MNLI (Williams et al., 2018), QNIL (Rajpurkar et al., 2016), RTE (Bentivogli et al., 2009) and WNLI (Levesque et al., 2012). However, we exclude the problematic WNLI in accordance with Devlin et al. (2019). Semantic Similarity MRPC (Dolan and Brockett, 2005), QQP (Chen et al., 2018) and STS-B (Cer et al., 2017) are semantic similarity tasks, where the model is required to either determine whether the two sentences are equivalent or assign a similarity score for them. In the fine-tuning stage, pairs of sentences are concatenated into a single sequence with a special token [SEP] in between. For both single sentence and sentence pair tasks, the hidden state of the first token [CLS] is used for softmax classification. We use the same sets of hyperparameters for all the evaluated models. Experiments are ran with batch sizes in {8, 16, 32, 64} and learning rate of 3e-5 for 3 epochs. 5.1.3 G EO G RANNO G E"
2021.acl-long.398,D18-2029,0,0.0484783,"Missing"
2021.acl-long.398,2020.emnlp-main.651,0,0.0205882,"upported by Huawei Noah’s Ark Lab. Then given two documents one of which contains “England” and “capital”, the other contains “London”, we consider them relevant. While a ULR model may generalize such good analogy features onto free text with all language levels involved together. For example, Eat an onion : Vegetable :: Eat a pear : Fruit. ULR has practical values in dialogue systems, by which human-computer communication will go far beyond executing instructions. One of the main challenges of dialogue systems is Dialogue State Tracking (DST). It can be formulated as a semantic parsing task (Cheng et al., 2020), namely, converting natural language utterances with any length into unified representations. Thus this is essentially a problem that can be conveniently solved by mapping sequences with similar semantic meanings into similar representations in the same vector space according to a ULR model. Another use of ULR is in the Frequently Asked Questions (FAQ) retrieval task, where the goal is to answer a user’s question by retrieving question 5122 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Pr"
2021.acl-long.398,P89-1010,0,0.301763,"tion. Second, inspired by word and phrase representation 5123 and their compositionality, we introduce a novel pre-training objective that directly models the input sequences and the extracted n-grams through manipulating their representations. Finally, we implement a normalized score for each n-gram to guide their sampling for training. 3.1 n-gram Extracting Given a symbol sentence, Joshi et al. (2020) utilize span-level information by randomly masking and predicting contiguous segments. Different from such random sampling strategy, our method is based on point-wise mutual information (PMI) (Church and Hanks, 1989) that makes efficient use of statistics and automatically extracts meaningful n-grams from unlabeled corpus. Mutual information (MI) describes the association between two tokens by comparing the probability of observing them together with the probabilities of observing them independently. Higher mutual information indicates stronger association between the tokens. To be specific, an n-gram is denoted as w = (x1 , . . . , x|w |), where |w |is the number of tokens in w and |w |&gt; 1. Therefore, we present an extended PMI formula as follows:  P M I(w) = 1  logP (w) − |w| |w| X  logP (xk ) k=1 w"
2021.acl-long.398,D17-1070,0,0.218313,"sks especially on question answering which may be conveniently and effectively solved through ULR like solution. Actually, though in different forms, these tasks more and more tend to be solved by our suggested ULR model, including dialogue utterance regularization (Cao et al., 2020), question paraphrasing (Bonadiman et al., 2019), measuring QA similarities in FAQ tasks (Damani et al., 2020; Sakata et al., 2019). Related Work Previous language representation learning methods such as Word2Vec (Mikolov et al., 2013), GloVe (Pennington et al., 2014), LASER (Artetxe and Schwenk, 2019), InferSent (Conneau et al., 2017) and USE (Cer et al., 2018) focus on specific granular linguistic units, e.g., words or sentences. Later proposed ELMo (Peters et al., 2018), OpenAI GPT (Radford et al., 2018), BERT (Devlin et al., 2019) and XLNet (Yang et al., 2020) learns contextualized representation for each input token. Although such pre-trained language models (PrLMs) more or less are capable of offering universal language representation through their general-purpose training objectives, all the PrLMs devote into the contex3 Model As pre-trained contextualized language models show their powerfulness in generic language r"
2021.acl-long.398,N19-1423,0,0.656319,"n for Computational Linguistics paraphrases that already have an answer from the database. Such task can be accurately done by only manipulating vectors such as calculating and ranking vector distance (i.e., cosine similarity). The core is to embed sequences of different lengths in the same vector space. Then a ULR model retrieves the correct question-answer pair for the user query according to vector distance. In this paper, we propose a universal language representation learning method that generates fixedsized vectors for sequences of different lengths based on pre-trained language models (Devlin et al., 2019; Lan et al., 2019; Clark et al., 2020). We first introduce an efficient approach to extract and prune meaningful n-grams from unlabeled corpus. Then we present a new pre-training objective, Minimizing Symbol-vector Algorithmic Difference (MiSAD), that explicitly applies a penalty over different levels of linguistic units if their representations tend not to be in the same vector space. To investigate our model’s ability of capturing different levels of language information, we introduce an original universal analogy task derived from Google’s word analogy dataset, where our model significantl"
2021.acl-long.398,I05-5002,0,0.00869191,"LU tasks from the GLUE benchmark into three main categories. Single-Sentence Classification Single-sentence classification tasks includes SST-2 (Socher et al., 2013), a sentiment classification task, and CoLA (Warstadt et al., 2019), a task that is to determine whether a sentence is grammatically acceptable. Natural Language Inference GLUE contains four NLI tasks: MNLI (Williams et al., 2018), QNIL (Rajpurkar et al., 2016), RTE (Bentivogli et al., 2009) and WNLI (Levesque et al., 2012). However, we exclude the problematic WNLI in accordance with Devlin et al. (2019). Semantic Similarity MRPC (Dolan and Brockett, 2005), QQP (Chen et al., 2018) and STS-B (Cer et al., 2017) are semantic similarity tasks, where the model is required to either determine whether the two sentences are equivalent or assign a similarity score for them. In the fine-tuning stage, pairs of sentences are concatenated into a single sequence with a special token [SEP] in between. For both single sentence and sentence pair tasks, the hidden state of the first token [CLS] is used for softmax classification. We use the same sets of hyperparameters for all the evaluated models. Experiments are ran with batch sizes in {8, 16, 32, 64} and lear"
2021.acl-long.398,D19-1394,0,0.0121403,"ic similarity tasks, where the model is required to either determine whether the two sentences are equivalent or assign a similarity score for them. In the fine-tuning stage, pairs of sentences are concatenated into a single sequence with a special token [SEP] in between. For both single sentence and sentence pair tasks, the hidden state of the first token [CLS] is used for softmax classification. We use the same sets of hyperparameters for all the evaluated models. Experiments are ran with batch sizes in {8, 16, 32, 64} and learning rate of 3e-5 for 3 epochs. 5.1.3 G EO G RANNO G EO G RANNO (Herzig and Berant, 2019) contains natural language paraphrases paired with logical forms. The dataset is manually annotated: For each natural language utterance, a correct canonical utterance paraphrase is selected. The train/dev sets have 487 and 59 paraphrase pairs, respectively. In our experiments, we focus on question paraphrase retrieval, whose task is to retrieve the correct paraphrase from all 158 different sentences when given a question. Most of the queries have only one correct answer while some have two or more matches. Evaluation metrics are Top-1/5/10 accuracy. For G EO G RANNO and the universal analogy"
2021.acl-long.398,2020.tacl-1.5,0,0.296236,"applications in an extremely convenient way. 2 tualized representations from a generic text background and pay little attention on our concerned universal language presentation. As a typical PrLM, BERT is trained on a large amount of unlabeled data including two training targets: Masked Language Model (MLM), and Next Sentence Prediction (NSP). ALBERT (Lan et al., 2019) is trained with Sentence-Order Prediction (SOP) as a replacement of NSP. StructBERT (Wang et al., 2020) combines NSP and SOP to learn inter-sentence structural information. Nevertheless, RoBERTa (Liu et al., 2019) and SpanBERT (Joshi et al., 2020) show that single-sequence training is better than the sentence-pair scenario. Besides, BERT-wwm (Cui et al., 2019), StructBERT (Joshi et al., 2020), SpanBERT (Wang et al., 2020) perform MLM on higher linguistic levels, augmenting the MLM objective by masking whole words, trigrams or spans, respectively. ELECTRA (Clark et al., 2020) further improves pre-training through a generator and discriminator architecture. The aforementioned models may seemingly handle different sized input sequences, but all of them focus on sentence-level specific representation still for each word, which may cause un"
2021.acl-long.398,D14-1162,0,0.0992771,"rmance in real-world situations. There are a series of downstream NLP tasks especially on question answering which may be conveniently and effectively solved through ULR like solution. Actually, though in different forms, these tasks more and more tend to be solved by our suggested ULR model, including dialogue utterance regularization (Cao et al., 2020), question paraphrasing (Bonadiman et al., 2019), measuring QA similarities in FAQ tasks (Damani et al., 2020; Sakata et al., 2019). Related Work Previous language representation learning methods such as Word2Vec (Mikolov et al., 2013), GloVe (Pennington et al., 2014), LASER (Artetxe and Schwenk, 2019), InferSent (Conneau et al., 2017) and USE (Cer et al., 2018) focus on specific granular linguistic units, e.g., words or sentences. Later proposed ELMo (Peters et al., 2018), OpenAI GPT (Radford et al., 2018), BERT (Devlin et al., 2019) and XLNet (Yang et al., 2020) learns contextualized representation for each input token. Although such pre-trained language models (PrLMs) more or less are capable of offering universal language representation through their general-purpose training objectives, all the PrLMs devote into the contex3 Model As pre-trained context"
2021.acl-long.398,N18-1202,0,0.0434417,"t forms, these tasks more and more tend to be solved by our suggested ULR model, including dialogue utterance regularization (Cao et al., 2020), question paraphrasing (Bonadiman et al., 2019), measuring QA similarities in FAQ tasks (Damani et al., 2020; Sakata et al., 2019). Related Work Previous language representation learning methods such as Word2Vec (Mikolov et al., 2013), GloVe (Pennington et al., 2014), LASER (Artetxe and Schwenk, 2019), InferSent (Conneau et al., 2017) and USE (Cer et al., 2018) focus on specific granular linguistic units, e.g., words or sentences. Later proposed ELMo (Peters et al., 2018), OpenAI GPT (Radford et al., 2018), BERT (Devlin et al., 2019) and XLNet (Yang et al., 2020) learns contextualized representation for each input token. Although such pre-trained language models (PrLMs) more or less are capable of offering universal language representation through their general-purpose training objectives, all the PrLMs devote into the contex3 Model As pre-trained contextualized language models show their powerfulness in generic language representation for various downstream NLP tasks, we present a BERT-style ULR model that is especially designed to effectively learn universal"
2021.acl-long.398,D16-1264,0,0.0225142,"al Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2018) is a collection of tasks that are widely used to evaluate the performance of a model in language understanding. We divide NLU tasks from the GLUE benchmark into three main categories. Single-Sentence Classification Single-sentence classification tasks includes SST-2 (Socher et al., 2013), a sentiment classification task, and CoLA (Warstadt et al., 2019), a task that is to determine whether a sentence is grammatically acceptable. Natural Language Inference GLUE contains four NLI tasks: MNLI (Williams et al., 2018), QNIL (Rajpurkar et al., 2016), RTE (Bentivogli et al., 2009) and WNLI (Levesque et al., 2012). However, we exclude the problematic WNLI in accordance with Devlin et al. (2019). Semantic Similarity MRPC (Dolan and Brockett, 2005), QQP (Chen et al., 2018) and STS-B (Cer et al., 2017) are semantic similarity tasks, where the model is required to either determine whether the two sentences are equivalent or assign a similarity score for them. In the fine-tuning stage, pairs of sentences are concatenated into a single sequence with a special token [SEP] in between. For both single sentence and sentence pair tasks, the hidden st"
2021.acl-long.398,Q19-1040,0,0.0113676,"our types of semantic analogy and three kinds of syntactic analogy. Please refer to Appendix A for details about our approach of constructing the universal analogy dataset. 5.1.2 GLUE The General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2018) is a collection of tasks that are widely used to evaluate the performance of a model in language understanding. We divide NLU tasks from the GLUE benchmark into three main categories. Single-Sentence Classification Single-sentence classification tasks includes SST-2 (Socher et al., 2013), a sentiment classification task, and CoLA (Warstadt et al., 2019), a task that is to determine whether a sentence is grammatically acceptable. Natural Language Inference GLUE contains four NLI tasks: MNLI (Williams et al., 2018), QNIL (Rajpurkar et al., 2016), RTE (Bentivogli et al., 2009) and WNLI (Levesque et al., 2012). However, we exclude the problematic WNLI in accordance with Devlin et al. (2019). Semantic Similarity MRPC (Dolan and Brockett, 2005), QQP (Chen et al., 2018) and STS-B (Cer et al., 2017) are semantic similarity tasks, where the model is required to either determine whether the two sentences are equivalent or assign a similarity score for"
2021.acl-long.398,N18-1101,0,0.011019,"dataset. 5.1.2 GLUE The General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2018) is a collection of tasks that are widely used to evaluate the performance of a model in language understanding. We divide NLU tasks from the GLUE benchmark into three main categories. Single-Sentence Classification Single-sentence classification tasks includes SST-2 (Socher et al., 2013), a sentiment classification task, and CoLA (Warstadt et al., 2019), a task that is to determine whether a sentence is grammatically acceptable. Natural Language Inference GLUE contains four NLI tasks: MNLI (Williams et al., 2018), QNIL (Rajpurkar et al., 2016), RTE (Bentivogli et al., 2009) and WNLI (Levesque et al., 2012). However, we exclude the problematic WNLI in accordance with Devlin et al. (2019). Semantic Similarity MRPC (Dolan and Brockett, 2005), QQP (Chen et al., 2018) and STS-B (Cer et al., 2017) are semantic similarity tasks, where the model is required to either determine whether the two sentences are equivalent or assign a similarity score for them. In the fine-tuning stage, pairs of sentences are concatenated into a single sequence with a special token [SEP] in between. For both single sentence and sen"
2021.acl-long.398,2020.acl-main.357,0,0.0500207,"Missing"
2021.acl-long.398,W18-5446,0,0.0367977,"Missing"
2021.acl-long.399,2020.acl-main.9,0,0.0404475,"urther enhance the capacity of PrLMs, such as XLNet (Yang et al., 2019), RoBERTa (Liu et al., 2019), ALBERT (Lan et al., 2020), ELECTRA (Clark et al., 2020). For simplicity and convenient comparison with public studies, we select the most widely used BERT as the backbone in this work. There are two ways of training PrLMs on dialogue scenarios, including open-domain pretraining and domain-adaptive post-training. Some studies perform training on open-domain conversational data like Reddit for response selection or generation tasks (Wolf et al., 2019; Zhang et al., 2020c; Henderson et al., 2020; Bao et al., 2020), but they are limited to the original pre-training tasks and ignore the dialogue related features. For domain-adaptive post-training, prior works have indicated that the order information would be important in the text representation, and the well-known next-sentence-prediction (Devlin et al., 2019b) and sentence-order-prediction (Lan et al., 2020) can be viewed as special cases of order prediction. Especially in the dialogue scenario, predicting the word order of utterance, as well as the utterance order in the context, has shown effectiveness in the dialogue generation task (Kumar et al., 2"
2021.acl-long.399,P19-1174,0,0.0280652,"dialogue related features. For domain-adaptive post-training, prior works have indicated that the order information would be important in the text representation, and the well-known next-sentence-prediction (Devlin et al., 2019b) and sentence-order-prediction (Lan et al., 2020) can be viewed as special cases of order prediction. Especially in the dialogue scenario, predicting the word order of utterance, as well as the utterance order in the context, has shown effectiveness in the dialogue generation task (Kumar et al., 2020; Gu et al., 2020b), where the order information is well recognized (Chen et al., 2019). However, there is little attention paid to dialogue comprehension tasks such as response selection (Lowe et al., 2015; Wu et al., 2017; Zhang et al., 2018). The potential difficulty 5135 is that utterance order restoration involves much more ordering possibilities for utterances that may have a quite flexible order inside dialogue text than NSP and SOP which only handle the predication of two-class ordering. Our work is also profoundly related to auxiliary multi-task learning, whose common theme is to guide the language modeling Transformers with explicit knowledge and complementing objectiv"
2021.acl-long.399,2020.acl-main.130,0,0.118007,"e graphs (Eric et al., 2017; Liu et al., 2018), the SVO triplets extracted in our sentence backbone predication objective (SBP) method, appear more widely in the text itself. Such triplets ensure the correctness of SVO and enable our model to discover the salient facts from the lengthy texts, sensing the intuition of “who did what”. 2.2 Multi-turn Dialogue Comprehension Multi-turn dialogue comprehension aims to teach machines to read dialogue contexts and solve tasks such as response selection (Lowe et al., 2015; Wu et al., 2017; Zhang et al., 2018) and answering questions (Sun et al., 2019a; Cui et al., 2020), whose common application is building intelligent humancomputer interactive systems (Chen et al., 2017a; Shum et al., 2018; Li et al., 2017; Zhu et al., 2018b). Early studies mainly focus on the matching between the dialogue context and question (Huang et al., 2019; Zhu et al., 2018a). Recently, inspired by the impressive performance of PrLMs, the mainstream is employing PrLMs to handle the whole input texts of context and question, as a linear sequence of successive tokens and implicitly capture the contextualized representations of those tokens through self-attention (Qu et al., 2019; Liu e"
2021.acl-long.399,N19-1423,0,0.145637,"t; 2) sentence backbone regularization, which regularizes the model to improve the factual correctness of summarized subject-verb-object triplets. Experimental results on widely used dialogue benchmarks verify the effectiveness of the newly introduced self-supervised tasks. 1 Figure 1: A multi-turn dialogue example. Different colors indicate the utterances from different speakers. Introduction Recent advances in large-scale pre-training language models (PrLMs) have achieved remarkable successes in a variety of natural language processing (NLP) tasks (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2019a; Yang et al., 2019; Clark et al., 2020; Zhang et al., 2020d). Providing fine-grained contextualized embedding, these pre-trained models are widely employed as encoders for various downstream NLP tasks. Although the PrLMs demonstrate superior perfor∗ Corresponding author. This paper was partially supported by National Key Research and Development Program of China (No. 2017YFB0304100), Key Projects of National Natural Science Foundation of China (U1836222 and 61733011), Huawei-SJTU long term AI project, Cuttingedge Machine Reading Comprehension and Language Model. This work was supported by Hu"
2021.acl-long.399,W17-5506,0,0.0281725,"ose both sides of intra- and inter- utterance objectives. In contrast, the four objectives proposed in Xu et al. (2020a) are natural variants of NSP in BERT, which are all utterancelevel. 3) Training: we empirically evaluate domainadaptive training and multi-task learning, instead of only employing multi-task learning, which requires many efforts of optimizing coefficients in the loss functions, which would be time-consuming. In terms of factual backbone modeling, compared with the existing studies that enhance the PrLMs by annotating named entities or incorporating external knowledge graphs (Eric et al., 2017; Liu et al., 2018), the SVO triplets extracted in our sentence backbone predication objective (SBP) method, appear more widely in the text itself. Such triplets ensure the correctness of SVO and enable our model to discover the salient facts from the lengthy texts, sensing the intuition of “who did what”. 2.2 Multi-turn Dialogue Comprehension Multi-turn dialogue comprehension aims to teach machines to read dialogue contexts and solve tasks such as response selection (Lowe et al., 2015; Wu et al., 2017; Zhang et al., 2018) and answering questions (Sun et al., 2019a; Cui et al., 2020), whose co"
2021.acl-long.399,2020.acl-main.740,0,0.0443849,"Missing"
2021.acl-long.399,2021.naacl-main.122,0,0.0187383,"t al., 2020a; Bai and Zhao, 2018; Qin et al., 2016, 2017). A critical challenge is the learning of rich and robust context representations and interactive relationships of dialogue utterances, so that the resulting model is capable of adequately capturing the semantics of each utterance, and the relationships among all the utterances inside the dialogue. Inspired by the effectiveness for learning universal language representations of PrLMs, there are increasing studies that employ PrLMs for conversation modeling (Mehri et al., 2019; Zhang et al., 2020b; Rothe et al., 2020; Whang et al., 2020; Han et al., 2021). These studies typically model the response selection with only the context-response matching task and overlook many potential training signals contained in dialogue data. Although the PrLMs have learned contextualized semantic representation from token-level or sentence-level pre-training tasks like MLM, NSP, they all do not consider dialogue related features like speaker role, continuity and consistency. One obvious issue of these approaches is that the relationships between utterances are harder to capture using word-level semantics. Besides, some latent features, such as user intent and c"
2021.acl-long.399,2020.findings-emnlp.196,0,0.0549039,"Missing"
2021.acl-long.399,2020.coling-main.238,0,0.0197949,"which is the focus in this work. How5134 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 5134–5145 August 1–6, 2021. ©2021 Association for Computational Linguistics ever, selecting a coherent and informative response for a given dialogue context remains a challenge. The multi-turn dialogue typically involves two or more speakers that engage in various conversation topics, intentions, thus the utterances are rich in interactions, e.g., with criss-cross discourse structures (Li et al., 2020a; Bai and Zhao, 2018; Qin et al., 2016, 2017). A critical challenge is the learning of rich and robust context representations and interactive relationships of dialogue utterances, so that the resulting model is capable of adequately capturing the semantics of each utterance, and the relationships among all the utterances inside the dialogue. Inspired by the effectiveness for learning universal language representations of PrLMs, there are increasing studies that employ PrLMs for conversation modeling (Mehri et al., 2019; Zhang et al., 2020b; Rothe et al., 2020; Whang et al., 2020; Han et al.,"
2021.acl-long.399,P18-1138,0,0.0250186,"ntra- and inter- utterance objectives. In contrast, the four objectives proposed in Xu et al. (2020a) are natural variants of NSP in BERT, which are all utterancelevel. 3) Training: we empirically evaluate domainadaptive training and multi-task learning, instead of only employing multi-task learning, which requires many efforts of optimizing coefficients in the loss functions, which would be time-consuming. In terms of factual backbone modeling, compared with the existing studies that enhance the PrLMs by annotating named entities or incorporating external knowledge graphs (Eric et al., 2017; Liu et al., 2018), the SVO triplets extracted in our sentence backbone predication objective (SBP) method, appear more widely in the text itself. Such triplets ensure the correctness of SVO and enable our model to discover the salient facts from the lengthy texts, sensing the intuition of “who did what”. 2.2 Multi-turn Dialogue Comprehension Multi-turn dialogue comprehension aims to teach machines to read dialogue contexts and solve tasks such as response selection (Lowe et al., 2015; Wu et al., 2017; Zhang et al., 2018) and answering questions (Sun et al., 2019a; Cui et al., 2020), whose common application is"
2021.acl-long.399,2021.ccl-1.108,0,0.0634135,"Missing"
2021.acl-long.399,W15-4640,0,0.307919,"h fine-tuning paradigm of exploiting PrLMs would be suboptimal to model dialogue task which holds exclusive text features that plain text for PrLM training may hardly embody. Therefore, we explore a fundamental way to alleviate this difficulty by improving the training of PrLM. This work devotes itself to designing the natural way of adapting the language modeling to the dialogue scenario motivated by the natural characteristics of dialogue contexts. As an active research topic in the NLP field, multi-turn dialogue modeling has attracted great interest. The typical task is response selection (Lowe et al., 2015; Wu et al., 2017; Zhang et al., 2018) that aims to select the appropriate response according to a given dialogue context containing a number of utterances, which is the focus in this work. How5134 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 5134–5145 August 1–6, 2021. ©2021 Association for Computational Linguistics ever, selecting a coherent and informative response for a given dialogue context remains a challenge. The multi-turn dialogue typically involves two or more"
2021.acl-long.399,2020.acl-main.571,1,0.825693,"hitecture choices and training objectives for large-scale LM pre-training (Zhou et al., 2020b,a; Xu et al., 2020a,b; Li et al., 2021, 2020b). Most of the PrLMs are based on the encoder in Transformer, among which Bidirectional Encoder Representations from Transformers (BERT) (Devlin et al., 2019b) is one of the most representative work. BERT uses multiple layers of stacked Transformer Encoder to obtain contextualized representations of the language at different levels. BERT has helped achieve great performance improvement in a broad range of NLP tasks (Bai and Zhao, 2018; Zhang et al., 2020a; Luo and Zhao, 2020; Zhang et al., 2021). Several subsequent variants have been proposed to further enhance the capacity of PrLMs, such as XLNet (Yang et al., 2019), RoBERTa (Liu et al., 2019), ALBERT (Lan et al., 2020), ELECTRA (Clark et al., 2020). For simplicity and convenient comparison with public studies, we select the most widely used BERT as the backbone in this work. There are two ways of training PrLMs on dialogue scenarios, including open-domain pretraining and domain-adaptive post-training. Some studies perform training on open-domain conversational data like Reddit for response selection or generati"
2021.acl-long.399,Q19-1014,0,0.135895,"le attention paid to dialogue comprehension tasks such as response selection (Lowe et al., 2015; Wu et al., 2017; Zhang et al., 2018). The potential difficulty 5135 is that utterance order restoration involves much more ordering possibilities for utterances that may have a quite flexible order inside dialogue text than NSP and SOP which only handle the predication of two-class ordering. Our work is also profoundly related to auxiliary multi-task learning, whose common theme is to guide the language modeling Transformers with explicit knowledge and complementing objectives (Zhang et al., 2019; Sun et al., 2019b; Xu et al., 2020a). A most related work is Xu et al. (2020a), which introduces four self-supervised tasks including next session prediction, utterance restoration, incoherence detection and consistency discrimination. Our work differs from Xu et al. (2020a) by three sides. 1) Motivation: our method is designed for a general-purpose in broad dialogue comprehension tasks whose goals may be either utterancelevel discourse coherence or inner-utterance factual correctness, instead of only motivated for downstream context-response matching, whose goal is to measure if two sequences are related or"
2021.acl-long.399,P19-1373,0,0.0211584,"ances are rich in interactions, e.g., with criss-cross discourse structures (Li et al., 2020a; Bai and Zhao, 2018; Qin et al., 2016, 2017). A critical challenge is the learning of rich and robust context representations and interactive relationships of dialogue utterances, so that the resulting model is capable of adequately capturing the semantics of each utterance, and the relationships among all the utterances inside the dialogue. Inspired by the effectiveness for learning universal language representations of PrLMs, there are increasing studies that employ PrLMs for conversation modeling (Mehri et al., 2019; Zhang et al., 2020b; Rothe et al., 2020; Whang et al., 2020; Han et al., 2021). These studies typically model the response selection with only the context-response matching task and overlook many potential training signals contained in dialogue data. Although the PrLMs have learned contextualized semantic representation from token-level or sentence-level pre-training tasks like MLM, NSP, they all do not consider dialogue related features like speaker role, continuity and consistency. One obvious issue of these approaches is that the relationships between utterances are harder to capture usin"
2021.acl-long.399,N18-1202,0,0.0215095,"the permuted utterances in dialogue context; 2) sentence backbone regularization, which regularizes the model to improve the factual correctness of summarized subject-verb-object triplets. Experimental results on widely used dialogue benchmarks verify the effectiveness of the newly introduced self-supervised tasks. 1 Figure 1: A multi-turn dialogue example. Different colors indicate the utterances from different speakers. Introduction Recent advances in large-scale pre-training language models (PrLMs) have achieved remarkable successes in a variety of natural language processing (NLP) tasks (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2019a; Yang et al., 2019; Clark et al., 2020; Zhang et al., 2020d). Providing fine-grained contextualized embedding, these pre-trained models are widely employed as encoders for various downstream NLP tasks. Although the PrLMs demonstrate superior perfor∗ Corresponding author. This paper was partially supported by National Key Research and Development Program of China (No. 2017YFB0304100), Key Projects of National Natural Science Foundation of China (U1836222 and 61733011), Huawei-SJTU long term AI project, Cuttingedge Machine Reading Comprehension and La"
2021.acl-long.399,D16-1246,1,0.823639,"34 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 5134–5145 August 1–6, 2021. ©2021 Association for Computational Linguistics ever, selecting a coherent and informative response for a given dialogue context remains a challenge. The multi-turn dialogue typically involves two or more speakers that engage in various conversation topics, intentions, thus the utterances are rich in interactions, e.g., with criss-cross discourse structures (Li et al., 2020a; Bai and Zhao, 2018; Qin et al., 2016, 2017). A critical challenge is the learning of rich and robust context representations and interactive relationships of dialogue utterances, so that the resulting model is capable of adequately capturing the semantics of each utterance, and the relationships among all the utterances inside the dialogue. Inspired by the effectiveness for learning universal language representations of PrLMs, there are increasing studies that employ PrLMs for conversation modeling (Mehri et al., 2019; Zhang et al., 2020b; Rothe et al., 2020; Whang et al., 2020; Han et al., 2021). These studies typically model t"
2021.acl-long.399,P17-1093,1,0.899595,"Missing"
2021.acl-long.399,D19-1011,0,0.0308898,"Missing"
2021.acl-long.399,P19-1001,0,0.0248771,"Missing"
2021.acl-long.399,P17-1046,0,0.208461,"igm of exploiting PrLMs would be suboptimal to model dialogue task which holds exclusive text features that plain text for PrLM training may hardly embody. Therefore, we explore a fundamental way to alleviate this difficulty by improving the training of PrLM. This work devotes itself to designing the natural way of adapting the language modeling to the dialogue scenario motivated by the natural characteristics of dialogue contexts. As an active research topic in the NLP field, multi-turn dialogue modeling has attracted great interest. The typical task is response selection (Lowe et al., 2015; Wu et al., 2017; Zhang et al., 2018) that aims to select the appropriate response according to a given dialogue context containing a number of utterances, which is the focus in this work. How5134 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 5134–5145 August 1–6, 2021. ©2021 Association for Computational Linguistics ever, selecting a coherent and informative response for a given dialogue context remains a challenge. The multi-turn dialogue typically involves two or more speakers that en"
2021.acl-long.399,2020.acl-demos.30,0,0.256885,"model to improve the factual correctness of summarized subject-verb-object triplets. Experimental results on widely used dialogue benchmarks verify the effectiveness of the newly introduced self-supervised tasks. 1 Figure 1: A multi-turn dialogue example. Different colors indicate the utterances from different speakers. Introduction Recent advances in large-scale pre-training language models (PrLMs) have achieved remarkable successes in a variety of natural language processing (NLP) tasks (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2019a; Yang et al., 2019; Clark et al., 2020; Zhang et al., 2020d). Providing fine-grained contextualized embedding, these pre-trained models are widely employed as encoders for various downstream NLP tasks. Although the PrLMs demonstrate superior perfor∗ Corresponding author. This paper was partially supported by National Key Research and Development Program of China (No. 2017YFB0304100), Key Projects of National Natural Science Foundation of China (U1836222 and 61733011), Huawei-SJTU long term AI project, Cuttingedge Machine Reading Comprehension and Language Model. This work was supported by Huawei Noah’s Ark Lab. mance due to their strong representatio"
2021.acl-long.399,P19-1139,0,0.0235521,"wever, there is little attention paid to dialogue comprehension tasks such as response selection (Lowe et al., 2015; Wu et al., 2017; Zhang et al., 2018). The potential difficulty 5135 is that utterance order restoration involves much more ordering possibilities for utterances that may have a quite flexible order inside dialogue text than NSP and SOP which only handle the predication of two-class ordering. Our work is also profoundly related to auxiliary multi-task learning, whose common theme is to guide the language modeling Transformers with explicit knowledge and complementing objectives (Zhang et al., 2019; Sun et al., 2019b; Xu et al., 2020a). A most related work is Xu et al. (2020a), which introduces four self-supervised tasks including next session prediction, utterance restoration, incoherence detection and consistency discrimination. Our work differs from Xu et al. (2020a) by three sides. 1) Motivation: our method is designed for a general-purpose in broad dialogue comprehension tasks whose goals may be either utterancelevel discourse coherence or inner-utterance factual correctness, instead of only motivated for downstream context-response matching, whose goal is to measure if two sequenc"
2021.acl-long.399,C18-1317,1,0.59155,"PrLMs would be suboptimal to model dialogue task which holds exclusive text features that plain text for PrLM training may hardly embody. Therefore, we explore a fundamental way to alleviate this difficulty by improving the training of PrLM. This work devotes itself to designing the natural way of adapting the language modeling to the dialogue scenario motivated by the natural characteristics of dialogue contexts. As an active research topic in the NLP field, multi-turn dialogue modeling has attracted great interest. The typical task is response selection (Lowe et al., 2015; Wu et al., 2017; Zhang et al., 2018) that aims to select the appropriate response according to a given dialogue context containing a number of utterances, which is the focus in this work. How5134 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 5134–5145 August 1–6, 2021. ©2021 Association for Computational Linguistics ever, selecting a coherent and informative response for a given dialogue context remains a challenge. The multi-turn dialogue typically involves two or more speakers that engage in various conve"
2021.acl-long.399,2020.findings-emnlp.398,1,0.746506,"hich predicts the order of the permuted utterances in dialogue context; 2) sentence backbone regularization (SBR), which regularizes the model to improve the factual correctness of summarized subject-verb-object (SVO) triplets. Experimental results on widely used benchmarks show that SPDER boosts the model performance for various multi-turn dialogue comprehension tasks including response selection and dialogue reasoning. 2 2.1 Background and Related Work Pre-trained Language Models Recent works have explored various architecture choices and training objectives for large-scale LM pre-training (Zhou et al., 2020b,a; Xu et al., 2020a,b; Li et al., 2021, 2020b). Most of the PrLMs are based on the encoder in Transformer, among which Bidirectional Encoder Representations from Transformers (BERT) (Devlin et al., 2019b) is one of the most representative work. BERT uses multiple layers of stacked Transformer Encoder to obtain contextualized representations of the language at different levels. BERT has helped achieve great performance improvement in a broad range of NLP tasks (Bai and Zhao, 2018; Zhang et al., 2020a; Luo and Zhao, 2020; Zhang et al., 2021). Several subsequent variants have been proposed to f"
2021.acl-long.399,2020.findings-emnlp.399,1,0.730224,"hich predicts the order of the permuted utterances in dialogue context; 2) sentence backbone regularization (SBR), which regularizes the model to improve the factual correctness of summarized subject-verb-object (SVO) triplets. Experimental results on widely used benchmarks show that SPDER boosts the model performance for various multi-turn dialogue comprehension tasks including response selection and dialogue reasoning. 2 2.1 Background and Related Work Pre-trained Language Models Recent works have explored various architecture choices and training objectives for large-scale LM pre-training (Zhou et al., 2020b,a; Xu et al., 2020a,b; Li et al., 2021, 2020b). Most of the PrLMs are based on the encoder in Transformer, among which Bidirectional Encoder Representations from Transformers (BERT) (Devlin et al., 2019b) is one of the most representative work. BERT uses multiple layers of stacked Transformer Encoder to obtain contextualized representations of the language at different levels. BERT has helped achieve great performance improvement in a broad range of NLP tasks (Bai and Zhao, 2018; Zhang et al., 2020a; Luo and Zhao, 2020; Zhang et al., 2021). Several subsequent variants have been proposed to f"
2021.acl-long.399,P18-1103,0,0.0367521,"Missing"
2021.acl-long.399,C18-2024,1,0.855334,"he text itself. Such triplets ensure the correctness of SVO and enable our model to discover the salient facts from the lengthy texts, sensing the intuition of “who did what”. 2.2 Multi-turn Dialogue Comprehension Multi-turn dialogue comprehension aims to teach machines to read dialogue contexts and solve tasks such as response selection (Lowe et al., 2015; Wu et al., 2017; Zhang et al., 2018) and answering questions (Sun et al., 2019a; Cui et al., 2020), whose common application is building intelligent humancomputer interactive systems (Chen et al., 2017a; Shum et al., 2018; Li et al., 2017; Zhu et al., 2018b). Early studies mainly focus on the matching between the dialogue context and question (Huang et al., 2019; Zhu et al., 2018a). Recently, inspired by the impressive performance of PrLMs, the mainstream is employing PrLMs to handle the whole input texts of context and question, as a linear sequence of successive tokens and implicitly capture the contextualized representations of those tokens through self-attention (Qu et al., 2019; Liu et al., 2020). Such a way of modeling would be suboptimal to capture the high-level relationships between utterances in the dialogue history. In this work, we"
2021.eacl-tutorials.5,D18-1399,0,0.0156808,"Missing"
2021.eacl-tutorials.5,P19-1019,0,0.0172519,"ination of diverse mechanisms such as an initialization with bilingual word embeddings, denoising auto-encoder, back-translation, and shared latent representation. 1.2 1.3 Recent Advances USMT and UNMT. Since 2016, statistical MT (SMT) has been significantly over-passed by NMT. Lample et al. (2018c) and Artetxe et al. (2018a) proposed an alternative method, that is, unsupervised statistical machine translation (USMT) method. However, in the supervised scenario, the performance of USMT method is comparable with that of UNMT. In addition, several works (Marie and Fujita, 2018; Ren et al., 2019; Artetxe et al., 2019) combined UNMT and USMT to improve unsupervised machine translation performance. In WMTMethods Cross-lingual language representation initialization. In supervised NMT, language representation initialization is not so necessary, because the bilingual corpus can help NMT learn the crosslingual representation. In comparison, there is only monolingual corpus for UNMT. Therefore, the pretrained unsupervised bilingual word embedding 1 https://wangruinlp.github.io/unmt. html 17 Proceedings of EACL: Tutorials, pages 17–21 April 19 - 20, 2021. ©2020 Association for Computational Linguistics 2019, the u"
2021.eacl-tutorials.5,P19-1017,0,0.0216804,"-lingual language model (Lample and Conneau, 2019; Song et al., 2019) achieve better UNMT performance than the bilingual word embedding. In high-resource scenario, UNMT has achieved remarkable performance. However, the performance of low-resource UNMT is still far below expectations Multilingualism. To improve the low-resource UNMT, multi-lingual UNMT (MUNMT) is proposed (Sun et al., 2020; Liu et al., 2020). The translation of low-resouce and zero-shot language pairs can be enhanced by the similar languages in the shared latent representation. In addition, the pivotbased methods are proposed. Leng et al. (2019) introduced unsupervised pivot translation for distant language pairs. The SJTU-NICT team used monolingual corpus together with parallel third-party languages to enhance the low-resource UNMT performance (Li et al., 2020b) and their system achieved the best performance in WMT-2020 unsupervised task (Li et al., 2020a). 1.4 supervision, it is very difficult for UNMT to learn the syntactic correspondence. 2) There are too few shared words/subwords in the distant language pair to learn the shared latent representation for UNMT. Finally, we will show some potential solutions, such as 1) syntactic m"
2021.eacl-tutorials.5,J82-2005,0,0.513875,"Missing"
2021.eacl-tutorials.5,2020.wmt-1.22,1,0.763207,"Missing"
2021.eacl-tutorials.5,2020.findings-emnlp.371,1,0.880271,"Missing"
2021.eacl-tutorials.5,2020.tacl-1.47,0,0.0208541,"s on improving the language representation pre-training. Sun et al. (2019b) proposed to train UNMT jointly with bilingual word embedding agreement. More recently, it has been shown that the pre-trained cross-lingual language model (Lample and Conneau, 2019; Song et al., 2019) achieve better UNMT performance than the bilingual word embedding. In high-resource scenario, UNMT has achieved remarkable performance. However, the performance of low-resource UNMT is still far below expectations Multilingualism. To improve the low-resource UNMT, multi-lingual UNMT (MUNMT) is proposed (Sun et al., 2020; Liu et al., 2020). The translation of low-resouce and zero-shot language pairs can be enhanced by the similar languages in the shared latent representation. In addition, the pivotbased methods are proposed. Leng et al. (2019) introduced unsupervised pivot translation for distant language pairs. The SJTU-NICT team used monolingual corpus together with parallel third-party languages to enhance the low-resource UNMT performance (Li et al., 2020b) and their system achieved the best performance in WMT-2020 unsupervised task (Li et al., 2020a). 1.4 supervision, it is very difficult for UNMT to learn the syntactic co"
2021.eacl-tutorials.5,W19-5330,1,0.788036,"l language representation initialization. In supervised NMT, language representation initialization is not so necessary, because the bilingual corpus can help NMT learn the crosslingual representation. In comparison, there is only monolingual corpus for UNMT. Therefore, the pretrained unsupervised bilingual word embedding 1 https://wangruinlp.github.io/unmt. html 17 Proceedings of EACL: Tutorials, pages 17–21 April 19 - 20, 2021. ©2020 Association for Computational Linguistics 2019, the unsupervised MT task (German-Czech) first-time became the officially task of WMT, and the system from NICT (Marie et al., 2019) won the first place and achieved state-of-the-art performances by combining the USMT and UNMT. However, after the advanced pre-training technologies was developed, USMT became less important. Advanced Pre-Training Technologies. Similar as other NLP tasks, the quality of language representation pre-training significantly affects the performance of UNMT. Several works focus on improving the language representation pre-training. Sun et al. (2019b) proposed to train UNMT jointly with bilingual word embedding agreement. More recently, it has been shown that the pre-trained cross-lingual language m"
2021.eacl-tutorials.5,P16-1078,0,0.0253765,"ced unsupervised pivot translation for distant language pairs. The SJTU-NICT team used monolingual corpus together with parallel third-party languages to enhance the low-resource UNMT performance (Li et al., 2020b) and their system achieved the best performance in WMT-2020 unsupervised task (Li et al., 2020a). 1.4 supervision, it is very difficult for UNMT to learn the syntactic correspondence. 2) There are too few shared words/subwords in the distant language pair to learn the shared latent representation for UNMT. Finally, we will show some potential solutions, such as 1) syntactic methods (Eriguchi et al., 2016; Chen et al., 2017, 2018) and 2) artificial shared words/code-switching methods (Yang et al., 2020) and show the initial results. Domain adaptation methods for UNMT have not been well-studied although UNMT has recently achieved remarkable results in some specific domains for several language pairs. For UNMT, addition to inconsistent domains between training data and test data for supervised NMT, there also exist other inconsistent domains between monolingual training data in two languages. Actually, it is difficult for some language pairs to obtain enough source and target monolingual corpora"
2021.eacl-tutorials.5,P16-1009,0,0.021246,"ranslation to generate pseudo-parallel corpora at the beginning of the UNMT training. Denoising auto-encoder: Noise obtained by randomly performing local substitutions and word reorderings (Vincent et al., 2010), is added to the input sentences to improve model learning ability and regularization. The denoising auto-encoder model objective function would be optimized by maximizing the probability of encoding a noisy sentence and reconstructing it. Back-translation: The back-translation plays a key role in achieving unsupervised translation relying only on monolingual corpora in each language (Sennrich et al., 2016). The pseudo-parallel sentence pairs produced by the model at the previous iteration have been used to train the new translation model. Sharing latent representations: Encoders and decoders are (partially) shared for two languages. Therefore, the two languages must use the same vocabulary. The entire training of UNMT needs to consider back-translation between the two languages and their respective denoising processing. Unsupervised cross-lingual language representation initialization methods, together with mechanisms such as denoising and backtranslation, have advanced unsupervised neural mach"
2021.eacl-tutorials.5,P19-1119,1,0.846643,"Missing"
2021.eacl-tutorials.5,2020.acl-main.324,1,0.772073,"Several works focus on improving the language representation pre-training. Sun et al. (2019b) proposed to train UNMT jointly with bilingual word embedding agreement. More recently, it has been shown that the pre-trained cross-lingual language model (Lample and Conneau, 2019; Song et al., 2019) achieve better UNMT performance than the bilingual word embedding. In high-resource scenario, UNMT has achieved remarkable performance. However, the performance of low-resource UNMT is still far below expectations Multilingualism. To improve the low-resource UNMT, multi-lingual UNMT (MUNMT) is proposed (Sun et al., 2020; Liu et al., 2020). The translation of low-resouce and zero-shot language pairs can be enhanced by the similar languages in the shared latent representation. In addition, the pivotbased methods are proposed. Leng et al. (2019) introduced unsupervised pivot translation for distant language pairs. The SJTU-NICT team used monolingual corpus together with parallel third-party languages to enhance the low-resource UNMT performance (Li et al., 2020b) and their system achieved the best performance in WMT-2020 unsupervised task (Li et al., 2020a). 1.4 supervision, it is very difficult for UNMT to lea"
2021.eacl-tutorials.5,2020.emnlp-main.208,0,0.0688424,"us together with parallel third-party languages to enhance the low-resource UNMT performance (Li et al., 2020b) and their system achieved the best performance in WMT-2020 unsupervised task (Li et al., 2020a). 1.4 supervision, it is very difficult for UNMT to learn the syntactic correspondence. 2) There are too few shared words/subwords in the distant language pair to learn the shared latent representation for UNMT. Finally, we will show some potential solutions, such as 1) syntactic methods (Eriguchi et al., 2016; Chen et al., 2017, 2018) and 2) artificial shared words/code-switching methods (Yang et al., 2020) and show the initial results. Domain adaptation methods for UNMT have not been well-studied although UNMT has recently achieved remarkable results in some specific domains for several language pairs. For UNMT, addition to inconsistent domains between training data and test data for supervised NMT, there also exist other inconsistent domains between monolingual training data in two languages. Actually, it is difficult for some language pairs to obtain enough source and target monolingual corpora from the same domain in the real-world scenario. In this tutorial, we will empirically show differe"
2021.emnlp-demo.1,D18-1045,0,0.01542,"ation of the translation y is always k tokens behind reading x; that is, at the t-th decoding step, we generate token yt based on x ≤ t − k + 1. We thus adopt a Transformerbased NMT model with the wait-k strategy, aiming for balance between translation performance and efficiency. 2.3 translation’ of a translated sequence back into its original language – is a potential method of generating reference sentences for comparison that utilizes the duality of direction in translation (He et al., 2016). Back-translation is currently mainly used as a data-enhancement method for supervised NMT systems (Edunov et al., 2018) and as a crucial training method for unsupervised NMT systems (Conneau and Lample, 2019), though it has been more controversial as a method of assessing translations. According to (Behr, 2017)’s conclusion, while back-translation can give some evaluation of the translation, it often raises issues not noted by human assessors, and more importantly, is less reliable in general, as many problems remain hidden. These shortcomings are mainly are a result of commonly used automatic evaluation methods (like BLEU) using only surface-level similarity; they do not strictly measure , Semantic Equivalenc"
2021.emnlp-demo.1,W19-4822,0,0.0146487,"tics as Google and Baidu have introduced simultaneous translation feature, due to the integration of simultaneous translation and whole-sentence translation, users cannot easily control whether the system uses simultaneous translation or whole-sentence translation, and the automated control of commercial systems sometimes does not follow the user’s wishes. Since user input errors are unavoidable for any human-computer interaction system, the quality of NMT system also has been shown to significantly degrade when confronted with source-side noise (Heigold et al., 2018; Belinkov and Bisk, 2018; Anastasopoulos, 2019). The previous grammatical error detection and correction work focused on computer-aided writing systems. Some existing computer-aided writing systems (Grammarly2 and Pigai3 , Write&Improve4 , and LinggleWrite5 ) detect and correct grammatical errors; however, systems such as these have had little attention when considered in the context of input error detection or correction for commercial machine translation systems, as their main focus is generally posttranslation editing. High quality domain specific machine translation systems are in high demand whereas general purpose MT has limited appl"
2021.emnlp-demo.1,D19-1435,0,0.0143281,"ain better translation sequences. We wanted to emphasize efficient inference, so we adopted a Transformer (Base) setting with fewer parameters. The training data used was the same as that in the multi-style NMT model. We formulated the GEC task as a sequence labeling problem and thus adopted a neural sequence tagging model to handle the task. We followed (Omelianchuk et al., 2020)’s model architecture, which was an encoder consisting of a pre-trained BERT-like transformer stacked with two linear layers with softmax layers on the top - one for error detection and one for error labeling. As in (Awasthi et al., 2019), the architecture uses an iterative correction strategy in which predicted transformations are applied to the input sequence successively. After errors are detected and predicted, a modified Levenshtein distance guides the generation of a corrected sentence. We limit the maximum number of inference iterations to 4 to speed up the overall correction process while still maintaining good correction accuracy. The training data we used for GEC is shown in Table 3. We trained our English GEC model at the word level and our Chinese and Japanese models at the character level. We used pre-trained lang"
2021.emnlp-demo.1,W18-1807,0,0.0687644,"Missing"
2021.emnlp-demo.1,P18-4024,1,0.847809,"apan charlee@sjtu.edu.cn, {mutiyama, eiichiro.sumita}@nict.go.jp, zhaohai@cs.sjtu.edu.cn Abstract and machine translation has correspondingly risen in popularity (Hutchins and Somers, 1992). Recently, Neural Machine Translation (NMT), especially Transformer-based NMT, has emerged as a promising approach with the potential to address many of the shortcomings of traditional rulebased or statistics-based machine translation systems (Sutskever et al., 2014; Bahdanau et al., 2015; Vaswani et al., 2017). This has significantly improved the performance of machine translation and other related tasks (Huang et al., 2018; Li et al., 2018a,b). Although neural machine translation has made tremendous improvements and is relatively highperforming, because human language is so complex, machine translation is often still only used as an assistance tool rather than the sole entity responsible for translation. There are several popular and large existing commercial machine translation systems that provide users with effective translation (e.g., Google Translator, Bing Translator, Amazon Translate, and Baidu Translate). As NMT is still very imprecise, however, these web services fall short, as they do not provide suff"
2021.emnlp-demo.1,N12-1067,0,0.0355332,"016; Omelianchuk et al., 2020) and used precision (P), recall (R), and F0 .5 to evaluate our models on all three languages. We evaluated English at the word level and Chinese and Japanese at the character level. We chose the test set of the CoNLL-2014 shared task as our evaluation set for our English GEC model. For Chinese and Japanese, we extracted 5000 sentences from the original training set for the development set and 5000 sentences for the test set and used the rest as the training set. ERRANT6 was used to convert parallel files to the m2 format for subsequent scoring with the M2 Scorer (Dahlmeier and Ng, 2012). The results of our models for standard NMT and simultaneous NMT are shown in Table 4. First, for the evaluation results of standard NMT, we found that the joint training of multiple styles of data does not bring performance improvement compared to separate training, especially when the corpora sizes of the two styles are similar. The translation performance gap between different styles demonstrates that the level difficulty of translation in different styles is different. Since style essentially refers to deviation from standard textual norms, the greater the deviation, the greater the trans"
2021.emnlp-demo.1,P19-1289,0,0.0633091,"Missing"
2021.emnlp-demo.1,2016.amta-researchers.9,0,0.0252712,"I SS, users can get real-time translations while writing, flexible control in switching between real-time translation and whole-sentence translation, informative back-translation feedback and scoring, and input error detection and revision suggestions. In addition, the system also supports user interactions that modify the translations or inputs, which provides crowdsourced data for further improving the performance of our machine translation and grammatical error correction. Notably, there were also several interactive translation systems in the past, such as CASMACAT (Alabau et al., 2014), (Knowles and Koehn, 2016), (Peris et al., 2017), and INMT (Santy et al., 2019). The distinctions lie in the abilities of the systems and the features to adapt to the latest user needs. 2 The M I SS System There are 5 features in our M I SS translation system: simultaneous translation, back-translation for quality evaluation, grammatical error correction, multi-style translation, and crowdsourcing data collection. The system is available at http: //miss.x2brain.com/ until November 12, 2021. We show a screenshot of the system in Figure 1. In the following subsections, we will describe each component of the system. 2.1 B"
2021.emnlp-demo.1,W17-3204,0,0.0152215,"nd LinggleWrite5 ) detect and correct grammatical errors; however, systems such as these have had little attention when considered in the context of input error detection or correction for commercial machine translation systems, as their main focus is generally posttranslation editing. High quality domain specific machine translation systems are in high demand whereas general purpose MT has limited applications because different machine translation users want to generate translations that can be used in the scenario. On the one hand, general purpose translation systems usually perform poorly (Koehn and Knowles, 2017). On the other hand, appropriate translation is also a very important goal to pursue. There are two typical methods to achieve this goal. One is to use the domain adaptation method to obtain a domainspecific model from the existing general machine translation model through transfer learning. The other is to adopt an conditional translation decoder to integrate various domains into the same model and generate translations according to different input conditions (Keskar et al., 2019). At present, the commercial machine translation system mainly adopts the former one, but it also brings the addit"
2021.emnlp-demo.1,D18-1512,0,0.0469907,"Missing"
2021.emnlp-demo.1,C18-1271,1,0.743867,"u.cn, {mutiyama, eiichiro.sumita}@nict.go.jp, zhaohai@cs.sjtu.edu.cn Abstract and machine translation has correspondingly risen in popularity (Hutchins and Somers, 1992). Recently, Neural Machine Translation (NMT), especially Transformer-based NMT, has emerged as a promising approach with the potential to address many of the shortcomings of traditional rulebased or statistics-based machine translation systems (Sutskever et al., 2014; Bahdanau et al., 2015; Vaswani et al., 2017). This has significantly improved the performance of machine translation and other related tasks (Huang et al., 2018; Li et al., 2018a,b). Although neural machine translation has made tremendous improvements and is relatively highperforming, because human language is so complex, machine translation is often still only used as an assistance tool rather than the sole entity responsible for translation. There are several popular and large existing commercial machine translation systems that provide users with effective translation (e.g., Google Translator, Bing Translator, Amazon Translate, and Baidu Translate). As NMT is still very imprecise, however, these web services fall short, as they do not provide sufficient informatio"
2021.emnlp-demo.1,2020.bea-1.16,0,0.389675,"NMT model, making this decoder also conditioned on a variety of control codes (Pfaff, 1979; Poplack, 2000). We call our system CTRL-NMT. Formally speaking, the target distribution of CTRL-NMT can be decomposed using the chain rule of probability and trained with a loss that takes the control code into account: Feature #3: Grammatical Error Correction Detecting potential grammatical errors and offering corrective suggestions for them sentence is also a very important feature in M I SS. We chose the tag-based modeling approach for this feature based on the fresearch field’s latest achievements (Omelianchuk et al., 2020) and our recent work (Parnow et al., 2020, 2021) in the Grammatical Error Correction (GEC). Specifically, the g-transformations developed by (Omelianchuk et al., 2020) were included in our system in the hopes of providing learners more specific suggestions (i.e., the edit type of an error) to revise the users’ input. Predicting edits rather than tokens also increases the generalization of our GEC model. G-transformations are based on several basic transformations: $KEEP (keep the current token unchanged), $DELETE (delete current token), $APPEND_t1 (append new token t1 next to the current token"
2021.emnlp-demo.1,D18-1262,1,0.848954,"u.cn, {mutiyama, eiichiro.sumita}@nict.go.jp, zhaohai@cs.sjtu.edu.cn Abstract and machine translation has correspondingly risen in popularity (Hutchins and Somers, 1992). Recently, Neural Machine Translation (NMT), especially Transformer-based NMT, has emerged as a promising approach with the potential to address many of the shortcomings of traditional rulebased or statistics-based machine translation systems (Sutskever et al., 2014; Bahdanau et al., 2015; Vaswani et al., 2017). This has significantly improved the performance of machine translation and other related tasks (Huang et al., 2018; Li et al., 2018a,b). Although neural machine translation has made tremendous improvements and is relatively highperforming, because human language is so complex, machine translation is often still only used as an assistance tool rather than the sole entity responsible for translation. There are several popular and large existing commercial machine translation systems that provide users with effective translation (e.g., Google Translator, Bing Translator, Amazon Translate, and Baidu Translate). As NMT is still very imprecise, however, these web services fall short, as they do not provide sufficient informatio"
2021.emnlp-demo.1,N19-4009,0,0.0214697,"models. As (Zhang et al., 2019, 2020) observed that fine-tuning the pre-trained conSE X X X Table 1: User operations used for our crowdsourcing data collection in our M I SS system. 3 Style Implementation and Training The full system consists of 4 neural models: (1) a multi-style NMT model, (2) a simultaneous NMT model, (3) a grammatical error correction model, and (4) a BERT model. In our current M I SS release, we translate between three languages (English (EN), Chinse (ZH), and Japanese (JA)) for demonstration. For the multi-style NMT model, we implement CTRL-NMT using the public fairseq (Ott et al., 2019) toolkit. In our system, we adopt the Transformer (big) setting as in (Vaswani et al., 2017). We did not choose a deeper or wider Transformer (Wang et al., 2019; Sun et al., 2019) model because we wanted to balance performance and efficiency. As in (Li et al., 2019), we used a data-dependent gaussian prior objective (D2GPo) during the NMT model training process for better generalization. Due to resource constraints, our currently deployed model does not perform back-translation of larger sentences. Table 2 lists all our training corpora and their sizes. For the simultaneous translation model,"
2021.emnlp-demo.1,2021.findings-acl.290,1,0.806502,"Missing"
2021.emnlp-demo.1,2020.findings-emnlp.371,1,0.876743,"Missing"
2021.emnlp-demo.1,P16-1112,0,0.0293237,"ous translation model, we implemented the wait-k strategy and replaced the bi5 Provider Num. PIE-synthetic Lang-8 NUCLE FCE W&I+LOCNESS 9M 947K 56K 34K 34K ZH NLPCC2018-GEC HSK+Lang8 CGED 1.3M JA Lang8 3.1M EN EN ZH JA PrLM Dict P R F0.5 − +XLNet − − +BERT − − +BERT Word Word Word Char Char Word Char Char 53.46 76.92 38.72 45.06 50.34 36.83 45.68 46.56 37.45 41.03 15.07 19.55 33.46 20.52 16.49 27.34 54.22 65.47 29.47 35.73 45.72 31.78 33.74 40.82 Table 5: The performance of our GEC models Table 3: The GEC training data Models For the GEC component, we followed common practice in the GEC task (Rei and Yannakoudakis, 2016; Omelianchuk et al., 2020) and used precision (P), recall (R), and F0 .5 to evaluate our models on all three languages. We evaluated English at the word level and Chinese and Japanese at the character level. We chose the test set of the CoNLL-2014 shared task as our evaluation set for our English GEC model. For Chinese and Japanese, we extracted 5000 sentences from the original training set for the development set and 5000 sentences for the test set and used the rest as the training set. ERRANT6 was used to convert parallel files to the m2 format for subsequent scoring with the M2 Scorer (Dah"
2021.emnlp-demo.1,D19-3018,0,0.0188754,"flexible control in switching between real-time translation and whole-sentence translation, informative back-translation feedback and scoring, and input error detection and revision suggestions. In addition, the system also supports user interactions that modify the translations or inputs, which provides crowdsourced data for further improving the performance of our machine translation and grammatical error correction. Notably, there were also several interactive translation systems in the past, such as CASMACAT (Alabau et al., 2014), (Knowles and Koehn, 2016), (Peris et al., 2017), and INMT (Santy et al., 2019). The distinctions lie in the abilities of the systems and the features to adapt to the latest user needs. 2 The M I SS System There are 5 features in our M I SS translation system: simultaneous translation, back-translation for quality evaluation, grammatical error correction, multi-style translation, and crowdsourcing data collection. The system is available at http: //miss.x2brain.com/ until November 12, 2021. We show a screenshot of the system in Figure 1. In the following subsections, we will describe each component of the system. 2.1 Basis: Transformer-based NMT Transformer (Vaswani et a"
2021.emnlp-demo.1,W19-5341,0,0.017823,"em. 3 Style Implementation and Training The full system consists of 4 neural models: (1) a multi-style NMT model, (2) a simultaneous NMT model, (3) a grammatical error correction model, and (4) a BERT model. In our current M I SS release, we translate between three languages (English (EN), Chinse (ZH), and Japanese (JA)) for demonstration. For the multi-style NMT model, we implement CTRL-NMT using the public fairseq (Ott et al., 2019) toolkit. In our system, we adopt the Transformer (big) setting as in (Vaswani et al., 2017). We did not choose a deeper or wider Transformer (Wang et al., 2019; Sun et al., 2019) model because we wanted to balance performance and efficiency. As in (Li et al., 2019), we used a data-dependent gaussian prior objective (D2GPo) during the NMT model training process for better generalization. Due to resource constraints, our currently deployed model does not perform back-translation of larger sentences. Table 2 lists all our training corpora and their sizes. For the simultaneous translation model, we implemented the wait-k strategy and replaced the bi5 Provider Num. PIE-synthetic Lang-8 NUCLE FCE W&I+LOCNESS 9M 947K 56K 34K 34K ZH NLPCC2018-GEC HSK+Lang8 CGED 1.3M JA Lang8"
2021.emnlp-demo.1,P19-1176,0,0.0137142,"in our M I SS system. 3 Style Implementation and Training The full system consists of 4 neural models: (1) a multi-style NMT model, (2) a simultaneous NMT model, (3) a grammatical error correction model, and (4) a BERT model. In our current M I SS release, we translate between three languages (English (EN), Chinse (ZH), and Japanese (JA)) for demonstration. For the multi-style NMT model, we implement CTRL-NMT using the public fairseq (Ott et al., 2019) toolkit. In our system, we adopt the Transformer (big) setting as in (Vaswani et al., 2017). We did not choose a deeper or wider Transformer (Wang et al., 2019; Sun et al., 2019) model because we wanted to balance performance and efficiency. As in (Li et al., 2019), we used a data-dependent gaussian prior objective (D2GPo) during the NMT model training process for better generalization. Due to resource constraints, our currently deployed model does not perform back-translation of larger sentences. Table 2 lists all our training corpora and their sizes. For the simultaneous translation model, we implemented the wait-k strategy and replaced the bi5 Provider Num. PIE-synthetic Lang-8 NUCLE FCE W&I+LOCNESS 9M 947K 56K 34K 34K ZH NLPCC2018-GEC HSK+Lang8"
2021.emnlp-main.261,D18-1024,0,0.0183798,"erformance even declines compared to baseline. C ONST BTLM can adapt to larger γ and higher phrase utilization proportions, it achieves better results. Method Cosine sim. L2 dist. Pearson cor. Concat Fasttext MUSE 0.36 0.38 4.89 5.13 0.52 0.65 XLM + C ONST BTLM‡ 0.55 0.60 2.64 2.55 0.69 0.71 Table 4: Unsupervised cross-lingual alignment evaluation with word embedding Cosine similarity (Cosine sim.), L2 distance (L2 dist.), and Pearson correlation (Pearson cor.) between source words and their translations. We adopted the same vocabulary size for Concat Fasttext (Bojanowski et al., 2016), MUSE (Alaux et al., 2018), and XLM baselines, and our best EnDe UNMT model and extracted the embeddings for comparison. The results are shown in Table 4. As the results show, our method is not only better than pure embedding training methods, Concat Fasttest and MUSE, on the three evaluation metrics, but also surpasses our strong XLM baseline, which demonstrates that the alignment of the UGUNMT model is indeed improved with the weak alignment information from syntactic categories. 4.4 Universal Constituent Labels To illustrate the universal nature of the phrase grammar, we calculate statistics on the labels of the con"
2021.emnlp-main.261,P17-1042,0,0.0443604,"Missing"
2021.emnlp-main.261,D18-1399,0,0.0182256,"in machine translation. On the one hand, the development of deep neural networks such as Transformer (Vaswani et al., 2017; Li et al., 2021a) has played a significant role in NMT’s improvements. On the other hand, large-scale parallel corpora like the UN corpus (Ziemski et al., 2016) have also played an important role. Despite the recent success of NMT in standard benchmarks, the need for large-scale parallel corpora has limited the effectiveness of NMT in many language pairs, especially in low-resource language pairs (Koehn and Knowles, 2017). Unsupervised Neural Machine Translation (UNMT) (Artetxe et al., 2018b) was proposed to alleviate this issue by completely removing the need for parallel data and training an NMT system in a completely unsupervised manner, relying on nothing but monolingual corpora. Unsupervised machine translation does not need the parallel information from parallel sentences; rather, it generally uses embedding alignments, initializes parameters with pretrained language models, and uses iterative backtranslation between two languages to synthesize pseudo parallel corpora for model training (Lample et al., 2018a,c; Yang et al., 2018; Sun et al., 2019; Conneau and Lample, 2019;"
2021.emnlp-main.261,J82-2005,0,0.644622,"Missing"
2021.emnlp-main.261,D17-1209,0,0.0602426,"Missing"
2021.emnlp-main.261,S17-2002,0,0.0139469,"on label UNMT model is obtained using UG and our pro- proportions, indicating the UD annotation’s uniposed training approaches, we conducted an ex- versality and the effectiveness of our conversion perimental exploration of embedding alignment in preserving grammatical features. This does not according to the experimental settings of (Conneau explain more complicated issues such as language and Lample, 2019) and evaluated models on the similarity or commonality but rather indicates the SemEval’17 En-De cross-lingual semantic word overlap of grammatical phenomena and universal similarity task (Camacho-Collados et al., 2017). features in the annotations and parser predictions. 3256 4.5 Effects of SpanBERT, LIMIT-BERT, and C ONST BTLM for UNMT From the main experiments, the UNMT performance is improved, especially for the small-scale data setting. To find out that if the improvements are caused by C ONST MLM/C ONST BTLM and the syntactic information is really necessary, we compare our approaches with LIMIT-BERT which apply a linguistically guided span based MLM objective during UNMT training, and SpanBERT which is with a non-syntax based span masking strategy. Compared with SpanBERT and LIMIT-BERT in our UNMT fram"
2021.emnlp-main.261,D19-1538,1,0.900451,"Missing"
2021.emnlp-main.261,P18-1192,1,0.892196,"Missing"
2021.emnlp-main.261,2020.acl-main.703,0,0.0326422,"Missing"
2021.emnlp-main.261,D18-1262,1,0.8931,"Missing"
2021.emnlp-main.261,2020.findings-emnlp.371,1,0.856979,"Missing"
2021.emnlp-main.261,H94-1020,0,0.851239,"Missing"
2021.emnlp-main.261,J93-2004,0,0.0748203,"rged IWSLT’14 En-Fr and En-De tasks. † means the to evaluate the En-Fr translation model and encoder-only version is adopted, and ‡ means the dev2010, dev2012, tst2010, tst2011, and tst2012 in encoder-decoder version is adopted. IWSLT14.TED to evaluate the En-De model. To acquire constituent parse trees for monolingual sentences, we adopted the current state-of-the- 3.2 Results and Analysis art Berkeley Neural Parser (Kitaev and Klein, 2018) The results of the UNMT experiment are mainly as our parsing model and trained an En parser us- shown in Table 1. When a large-scale monolingual ing PTB (Marcus et al., 1993), Fr and De parsers corpus is used, our baseline model outperforms using the SPMRL14 multilingual constituent tree- XLM’s reported results. This may be due to the use bank (Seddah et al., 2014), and a Zh Parser using of the larger epoch size, which makes for more adeCTB (Xue et al., 2005). Since a constituent tree- quate training. Based on our strong baseline model, bank is not available in Ro and for the consistency the four implementations of our C ONST MLM and 3254 In the small-scale monolingual training data scenario, the performance of the baseline model has a large decline compared with"
2021.emnlp-main.261,W17-4707,0,0.0590694,"Missing"
2021.emnlp-main.261,W08-2119,0,0.078955,"Missing"
2021.emnlp-main.261,P11-1002,0,0.0926002,"Missing"
2021.emnlp-main.261,P16-1009,0,0.23002,"s compared to the baselines in these tasks. We also present a significantly boosted performance on several low-resource semisupervised tasks. These results verify that universal grammar commonalities can bring additional supervision information to bolster the training of unsupervised and low-resource translation models. 2 2.1 The Proposed Approaches Background We formally present the background of our baseline UNMT system in terms of unsupervised machine translation between languages L1 and L2 . Our UNMT model follows an encoder-decoder architecture as in standard NMT. We use a joint subword (Sennrich et al., 2016b) vocabulary shared between languages and share parameters between source→target and target→source models to take advantage of multilingualism (Edwards, 2002). In this framework, three training methods are indispensable for the feasibility of unsupervised machine translation: initialization, denoising generation, and iterative back-translation. UNMT models typically use denoising generation and iterative back translation simultaneously by alternating between the two methods in a single phase rather Masked Language Modeling (MLM) is a com- than separately in multiple phases. The model is 3250"
2021.emnlp-main.261,P16-1162,0,0.135289,"s compared to the baselines in these tasks. We also present a significantly boosted performance on several low-resource semisupervised tasks. These results verify that universal grammar commonalities can bring additional supervision information to bolster the training of unsupervised and low-resource translation models. 2 2.1 The Proposed Approaches Background We formally present the background of our baseline UNMT system in terms of unsupervised machine translation between languages L1 and L2 . Our UNMT model follows an encoder-decoder architecture as in standard NMT. We use a joint subword (Sennrich et al., 2016b) vocabulary shared between languages and share parameters between source→target and target→source models to take advantage of multilingualism (Edwards, 2002). In this framework, three training methods are indispensable for the feasibility of unsupervised machine translation: initialization, denoising generation, and iterative back-translation. UNMT models typically use denoising generation and iterative back translation simultaneously by alternating between the two methods in a single phase rather Masked Language Modeling (MLM) is a com- than separately in multiple phases. The model is 3250"
2021.emnlp-main.261,P19-1119,1,0.827825,"ne Translation (UNMT) (Artetxe et al., 2018b) was proposed to alleviate this issue by completely removing the need for parallel data and training an NMT system in a completely unsupervised manner, relying on nothing but monolingual corpora. Unsupervised machine translation does not need the parallel information from parallel sentences; rather, it generally uses embedding alignments, initializes parameters with pretrained language models, and uses iterative backtranslation between two languages to synthesize pseudo parallel corpora for model training (Lample et al., 2018a,c; Yang et al., 2018; Sun et al., 2019; Conneau and Lample, 2019; Li et al., 2020a). The pseudo parallel data created by iterative back-translation is the key to the success of unsupervised NMT model training (Kim et al., 2020). ∗ Corresponding author. † This paper was finished when It takes advantage of the equivalence of translaZuchao Li was a fixed term technical researcher at NICT. tion languages to bring supervision (albeit weak This work was supported by Key Projects of National Natural Science Foundation of China (U1836222 and 61733011). supervision) to model training. Recent results in 3249 Proceedings of the 2021 Conferen"
2021.emnlp-main.261,2020.acl-main.324,1,0.803643,"Missing"
2021.emnlp-main.261,P15-1150,0,0.0327086,"Missing"
2021.emnlp-main.261,J10-2004,0,0.0915598,"Missing"
2021.emnlp-main.261,W18-6459,0,0.0630501,"Missing"
2021.emnlp-main.261,P17-1179,0,0.0596818,"Missing"
2021.emnlp-main.261,P08-1064,0,0.109236,"Missing"
2021.emnlp-main.261,2020.findings-emnlp.398,1,0.838011,"T training. The loss, LB , is defined as the sequence after masking. The masked positions 3251 set M consists of randomly sampled discrete positions, that is, M = TopK([randi (0, 1)]ni=1 ). Here, TopK is a function that selects positions by probability until the masking budget has been spent. In span-based MLM like (Joshi et al., 2020), a span of length ` is first sampled from a geometric distribution ` ∼ Geo(p), and the start position of a span is sampled in the same manner as in MLM, giving final masked span set MS = {(Mi , `i )}. In another linguistically guided language modeling approach, Zhou et al. (2020b) proposed Syntactic/Semantic Phrase Masking (SPM) for their model LIMIT-BERT. In SPM, the masked positions set consists of tuples randomly sampled from the linguistic span set instead of the discrete token position set. Only the span boundary information, however, is used in SPM; the linguistic label is ignored, so we remedy this and propose C ONST MLM. In C ONST MLM, we first extract and filter the constituent span set CS = {(s, e, c)i }m i=1 , where s, e, and c represent the start position, end position, and syntactic category, respectively. During filtering, constituent parse trees with a"
2021.emnlp-main.261,2020.findings-emnlp.399,1,0.781794,"T training. The loss, LB , is defined as the sequence after masking. The masked positions 3251 set M consists of randomly sampled discrete positions, that is, M = TopK([randi (0, 1)]ni=1 ). Here, TopK is a function that selects positions by probability until the masking budget has been spent. In span-based MLM like (Joshi et al., 2020), a span of length ` is first sampled from a geometric distribution ` ∼ Geo(p), and the start position of a span is sampled in the same manner as in MLM, giving final masked span set MS = {(Mi , `i )}. In another linguistically guided language modeling approach, Zhou et al. (2020b) proposed Syntactic/Semantic Phrase Masking (SPM) for their model LIMIT-BERT. In SPM, the masked positions set consists of tuples randomly sampled from the linguistic span set instead of the discrete token position set. Only the span boundary information, however, is used in SPM; the linguistic label is ignored, so we remedy this and propose C ONST MLM. In C ONST MLM, we first extract and filter the constituent span set CS = {(s, e, c)i }m i=1 , where s, e, and c represent the start position, end position, and syntactic category, respectively. During filtering, constituent parse trees with a"
2021.emnlp-main.261,L16-1561,0,0.020456,"f constituent trees from English Penn Treebank (PTB) and German dataset of SPMRL14 shared task. The dotted box indicates the constituents that can be masked for prediction. Introduction Recently, Neural Machine Translation (NMT) (Bahdanau et al., 2014; Sutskever et al., 2014) has been greatly developed and become the dominant paradigm in machine translation. On the one hand, the development of deep neural networks such as Transformer (Vaswani et al., 2017; Li et al., 2021a) has played a significant role in NMT’s improvements. On the other hand, large-scale parallel corpora like the UN corpus (Ziemski et al., 2016) have also played an important role. Despite the recent success of NMT in standard benchmarks, the need for large-scale parallel corpora has limited the effectiveness of NMT in many language pairs, especially in low-resource language pairs (Koehn and Knowles, 2017). Unsupervised Neural Machine Translation (UNMT) (Artetxe et al., 2018b) was proposed to alleviate this issue by completely removing the need for parallel data and training an NMT system in a completely unsupervised manner, relying on nothing but monolingual corpora. Unsupervised machine translation does not need the parallel informa"
2021.emnlp-main.261,P18-1005,0,0.0165051,"rvised Neural Machine Translation (UNMT) (Artetxe et al., 2018b) was proposed to alleviate this issue by completely removing the need for parallel data and training an NMT system in a completely unsupervised manner, relying on nothing but monolingual corpora. Unsupervised machine translation does not need the parallel information from parallel sentences; rather, it generally uses embedding alignments, initializes parameters with pretrained language models, and uses iterative backtranslation between two languages to synthesize pseudo parallel corpora for model training (Lample et al., 2018a,c; Yang et al., 2018; Sun et al., 2019; Conneau and Lample, 2019; Li et al., 2020a). The pseudo parallel data created by iterative back-translation is the key to the success of unsupervised NMT model training (Kim et al., 2020). ∗ Corresponding author. † This paper was finished when It takes advantage of the equivalence of translaZuchao Li was a fixed term technical researcher at NICT. tion languages to bring supervision (albeit weak This work was supported by Key Projects of National Natural Science Foundation of China (U1836222 and 61733011). supervision) to model training. Recent results in 3249 Proceedings of"
2021.emnlp-main.261,N19-1118,0,0.0470832,"Missing"
2021.emnlp-main.299,L16-1432,0,0.0159602,"f possible edges derived from the discourse parsing, namely, default-in, default-out, reverse-in, reverse-out, self, and global. Furthermore, to build the relationship with the background user scenario, we add an extra global vertex of the user scenario that connects all the other vertices. As a result, there are three types of vertices, including the rule conditions, discourse relations, and the global scenario vertex. For rule condition and user scenario vertices, 2 This discourse parser gives a state-of-the-art performance on STAC so far. There are 16 discourse relations according to STAC (Asher et al., 2016), including comment, clarificationquestion, elaboration, acknowledgment, continuation, explanation, conditional, question-answer, alternation, questionelaboration, result, background, narration, correction, parallel, and contrast. we fetch the contextualized representation of the special tokens [RULE] and [CLS] before the corresponding sequences, respectively. For relation vertices, they are initialized as the conventional embedding layer, whose representations are obtained through a lookup table. For each rule document that is composed of multiple rule conditions, i.e., EDUs, let hp denote th"
2021.emnlp-main.299,N19-1124,0,0.0608009,"Missing"
2021.emnlp-main.299,D18-1241,0,0.0170919,"framework where the dialogue states for decision making are employed for question generation, in contrast to the independent models or pipeline systems in previous studies. Besides, a variety of strategies are empirically studied for smoothing the two dialogue states in only one decoder. 3) Experiments on the ShARC dataset show the effectiveness of our model, which achieves the new state-of-the-art results. A series of analyses show the contributing factors. 2 Related Work Most of the current conversation-based reading comprehension tasks are formed as either spanbased QA (Reddy et al., 2019; Choi et al., 2018) or multi-choice tasks (Sun et al., 2019; Cui et al., 2020), both of which neglect the vital process of question generation for confirmation during the human-machine interaction. In this work, we are interested in building a machine that can not only make the right decisions but also raise questions when necessary. The related task is called conversational machine reading (Saeidi et al., 2018) which consists of two separate subtasks: decision making and question generation. Compared with conversation-based reading comprehension tasks, our concerned CMR task is more challenging as it involves r"
2021.emnlp-main.299,2020.acl-main.130,0,0.0166896,"employed for question generation, in contrast to the independent models or pipeline systems in previous studies. Besides, a variety of strategies are empirically studied for smoothing the two dialogue states in only one decoder. 3) Experiments on the ShARC dataset show the effectiveness of our model, which achieves the new state-of-the-art results. A series of analyses show the contributing factors. 2 Related Work Most of the current conversation-based reading comprehension tasks are formed as either spanbased QA (Reddy et al., 2019; Choi et al., 2018) or multi-choice tasks (Sun et al., 2019; Cui et al., 2020), both of which neglect the vital process of question generation for confirmation during the human-machine interaction. In this work, we are interested in building a machine that can not only make the right decisions but also raise questions when necessary. The related task is called conversational machine reading (Saeidi et al., 2018) which consists of two separate subtasks: decision making and question generation. Compared with conversation-based reading comprehension tasks, our concerned CMR task is more challenging as it involves rule documents, scenarios, asking clarifi1 Our source codes"
2021.emnlp-main.299,N19-1423,0,0.00556007,"are packed from Hc . Then a gate control λ is computed ˆ + Uλ H e ) to get the final as sigmoid(Wλ H ˆ representation H = H e + λH. (7) The overall loss for decision making is: Ld = Ldecision + λLentail . (8) Question Generation If the decision is made to be Inquire, the machine needs to ask a follow-up question to further clarify. Question generation in this part is mainly based on the uncovered information in the rule document, and then that information will be rephrased into a question. We predict the position of an under-specified span within a rule document in a supervised way. Following Devlin et al. (2019), our model learns a start vector ws ∈ Rd and end vector we ∈ Rd to indicate the start and end positions of the desired span: span = arg min(wsT tk,i + weT tk,j ), H is then passed to the BART decoder to generate the follow-up question. At the i-th time-step, H is used to generate the target token yi by P (yi |y&lt;i , x; θ) ∝ exp(Wd tanh(Ww H)), (10) where θ denotes all the trainable parameters. Wd and Ww are projection matrices. The training objective is computed by Lg = arg max (9) I X log P (yi |y&lt;i , x; θ). (11) i=1 i,j,k where tk,i denote the i-th token in the k-th rule sen- is tence. The g"
2021.emnlp-main.299,2020.acl-main.88,0,0.257863,"eventually hintion questions. ders the model performance. In this work, we propose an effective gating strategy by smoothA variety of methods have been proposed for ing the two dialogue states in only one decoder the CMR task, including 1) sequential models that and bridge decision making and question genencode all the elements and model the matching reeration to provide a richer dialogue state referlationships with attention mechanisms (Zhong and ence. Experiments on the OR-ShARC dataset Zettlemoyer, 2019; Lawrence et al., 2019; Verma show the effectiveness of our method, which et al., 2020; Gao et al., 2020a,b); 2) graph-based achieves new state-of-the-art results. methods that capture the discourse structures of the 1 Introduction rule texts and user scenario for better interactions (Ouyang et al., 2021). However, there are two sides The ultimate goal of multi-turn dialogue is to enof challenges that have been neglected: able the machine to interact with human beings and 1) Open-retrieval of supporting evidence. The solve practical problems (Zhu et al., 2018; Zhang above existing methods assume that the relevant et al., 2018; Zaib et al., 2020; Huang et al., 2020; rule documents are given befor"
2021.emnlp-main.299,2020.emnlp-main.191,0,0.35027,"eventually hintion questions. ders the model performance. In this work, we propose an effective gating strategy by smoothA variety of methods have been proposed for ing the two dialogue states in only one decoder the CMR task, including 1) sequential models that and bridge decision making and question genencode all the elements and model the matching reeration to provide a richer dialogue state referlationships with attention mechanisms (Zhong and ence. Experiments on the OR-ShARC dataset Zettlemoyer, 2019; Lawrence et al., 2019; Verma show the effectiveness of our method, which et al., 2020; Gao et al., 2020a,b); 2) graph-based achieves new state-of-the-art results. methods that capture the discourse structures of the 1 Introduction rule texts and user scenario for better interactions (Ouyang et al., 2021). However, there are two sides The ultimate goal of multi-turn dialogue is to enof challenges that have been neglected: able the machine to interact with human beings and 1) Open-retrieval of supporting evidence. The solve practical problems (Zhu et al., 2018; Zhang above existing methods assume that the relevant et al., 2018; Zaib et al., 2020; Huang et al., 2020; rule documents are given befor"
2021.emnlp-main.299,2021.acl-long.285,0,0.0126044,"ate-of-the-art results. methods that capture the discourse structures of the 1 Introduction rule texts and user scenario for better interactions (Ouyang et al., 2021). However, there are two sides The ultimate goal of multi-turn dialogue is to enof challenges that have been neglected: able the machine to interact with human beings and 1) Open-retrieval of supporting evidence. The solve practical problems (Zhu et al., 2018; Zhang above existing methods assume that the relevant et al., 2018; Zaib et al., 2020; Huang et al., 2020; rule documents are given before the system interFan et al., 2020; Gu et al., 2021). It usually adopts acts with users, which is in a closed-book style. the form of question answering (QA) according to In real-world applications, the machines are ofthe user’s query along with the dialogue context (Sun et al., 2019; Reddy et al., 2019; Choi et al., ten required to retrieve supporting information to 2018). The machine may also actively ask ques- respond to incoming high-level queries in an intertions for confirmation (Wu et al., 2018; Cai et al., active manner, which results in an open-retrieval setting (Gao et al., 2021). The comparison of the 2019; Zhang et al., 2020b; Gu et"
2021.emnlp-main.299,D19-1001,0,0.255904,"actilogue context, and make decisions or ask clarificavate question generation, which eventually hintion questions. ders the model performance. In this work, we propose an effective gating strategy by smoothA variety of methods have been proposed for ing the two dialogue states in only one decoder the CMR task, including 1) sequential models that and bridge decision making and question genencode all the elements and model the matching reeration to provide a richer dialogue state referlationships with attention mechanisms (Zhong and ence. Experiments on the OR-ShARC dataset Zettlemoyer, 2019; Lawrence et al., 2019; Verma show the effectiveness of our method, which et al., 2020; Gao et al., 2020a,b); 2) graph-based achieves new state-of-the-art results. methods that capture the discourse structures of the 1 Introduction rule texts and user scenario for better interactions (Ouyang et al., 2021). However, there are two sides The ultimate goal of multi-turn dialogue is to enof challenges that have been neglected: able the machine to interact with human beings and 1) Open-retrieval of supporting evidence. The solve practical problems (Zhu et al., 2018; Zhang above existing methods assume that the relevant e"
2021.emnlp-main.299,2020.emnlp-main.120,0,0.0111371,"eak the rule text into clauselike units called elementary discourse units (EDUs) to extract the in-line rule conditions from the rule texts. Embedding We employ pre-trained language model (PrLM) model as the backbone of the encoder. As shown in the figure, the input of our model includes rule document which has already be parsed into EDUs with explicit discourse relation tagging, user initial question, user scenario and the dialog history. Instead of inserting a [CLS] token before each rule condition to get a sentence-level representation, we use [RULE] which is proved to enhance performance (Lee et al., 2020). Formally, the sequence is organized as: {[RULE] EDU0 [RULE] EDU1 [RULE] EDUk [CLS] Question [CLS] Scenario [CLS] History [SEP]}. Then we feed the sequence to the PrLM to obtain the contextualized representation. Interaction To explicitly model the discourse structure among the rule conditions, we first annotate the discourse relationships between the rule conditions and employ a relational graph convolutional network following Ouyang et al. (2021) by regarding the rule conditions as the vertices. The graph is formed as a Levi graph (Levi, 1942) that regards the relation edges as additional v"
2021.emnlp-main.299,2021.findings-acl.279,1,0.782265,"e encoder together with other necessary information. Lawrence et al., 2019; Verma et al., 2020; Gao et al., 2020a,b; Ouyang et al., 2021) have made progress in modeling the matching relationships between the rule document and other elements such as user scenarios and questions. These studies are based on the hypothesis that the supporting information for answering the question is provided, which does not meet the real-world applications. Therefore, we are motivated to investigate the open-retrieval settings (Qu et al., 2020), where the retrieved background knowledge would be noisy. Gao et al. (2021) makes the initial attempts of open-retrieval for CMR. However, like previous studies, the common solution is training independent or pipeline systems for the two subtasks and does not consider the information flow between decision making and question generation, which would eventually hinder the model performance. Compared to existing methods, our method makes the first attempt to bridge the gap between decision making and question generation, by smoothing the two dialogue states in only one decoder. In addition, we improve the retrieval process by taking advantage of the traditional TF-IDF m"
2021.emnlp-main.299,H90-1020,0,0.0999002,"Tong University 2 Key Laboratory of Shanghai Education Commission for Intelligent Interaction and Cognitive Engineering, Shanghai Jiao Tong University 3 MoE Key Lab of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University 4 National Institute of Information and Communications Technology (NICT), Kyoto, Japan {zhangzs,oysr0926}@sjtu.edu.cn,zhaohai@cs.sjtu.edu.cn {mutiyama,eiichiro.sumita}@nict.go.jp Abstract with people according to the dialogue states, and completes specific tasks, such as ordering meals Conversational machine reading (CMR) re(Liu et al., 2013) and air tickets (Price, 1990). In quires machines to communicate with humans real-world scenario, annotating data such as inthrough multi-turn interactions between two tents and slots is expensive. Inspired by the studies salient dialogue states of decision making and of reading comprehension (Rajpurkar et al., 2016, question generation processes. In open CMR 2018; Zhang et al., 2020c, 2021), there appears a settings, as the more realistic scenario, the retrieved background knowledge would be noisy, more general task — conversational machine readwhich results in severe challenges in the ining (CMR) (Saeidi et al., 2018):"
2021.emnlp-main.299,2020.emnlp-main.550,0,0.10908,"R. However, like previous studies, the common solution is training independent or pipeline systems for the two subtasks and does not consider the information flow between decision making and question generation, which would eventually hinder the model performance. Compared to existing methods, our method makes the first attempt to bridge the gap between decision making and question generation, by smoothing the two dialogue states in only one decoder. In addition, we improve the retrieval process by taking advantage of the traditional TF-IDF method and the latest dense passage retrieval model (Karpukhin et al., 2020). and a generator G(·, ·) on {R, Us , Uq , C} for question generation. 4 Model Our model is composed of three main modules: retriever, encoder, and decoder. The retriever is employed to retrieve the related rule texts for the given user scenario and question. The encoder takes the tuple {R, Us , Uq , C} as the input, encodes the elements into vectors and captures the contextualized representations. The decoder makes a decision or generates a question once the decision is “inquiry”. Figure 1 overviews the model architecture, we will elaborate the details in the following part. 4.1 Retrieval To"
2021.emnlp-main.299,P18-2124,0,0.0621738,"Missing"
2021.emnlp-main.299,D16-1264,0,0.0496147,"Communications Technology (NICT), Kyoto, Japan {zhangzs,oysr0926}@sjtu.edu.cn,zhaohai@cs.sjtu.edu.cn {mutiyama,eiichiro.sumita}@nict.go.jp Abstract with people according to the dialogue states, and completes specific tasks, such as ordering meals Conversational machine reading (CMR) re(Liu et al., 2013) and air tickets (Price, 1990). In quires machines to communicate with humans real-world scenario, annotating data such as inthrough multi-turn interactions between two tents and slots is expensive. Inspired by the studies salient dialogue states of decision making and of reading comprehension (Rajpurkar et al., 2016, question generation processes. In open CMR 2018; Zhang et al., 2020c, 2021), there appears a settings, as the more realistic scenario, the retrieved background knowledge would be noisy, more general task — conversational machine readwhich results in severe challenges in the ining (CMR) (Saeidi et al., 2018): given the inquiry, formation transmission. Existing studies comthe machine is required to retrieve relevant supmonly train independent or pipeline systems porting rule documents, the machine should judge for the two subtasks. However, those methods whether the goal is satisfied according"
2021.emnlp-main.299,2020.emnlp-main.589,0,0.0933312,"ques. BART-base *relations obtained by tagging model &lt;rule&gt; user scenario e1 Document k ... e2 e2 e3 ... e4 e3 ... ...... Rule Conditions Tagging Double-channel Decoder relations e1 e3 e1 Comment us GCN layer e 4 Document 1 ...... r3 e4 &lt;rule&gt; ... Rule e5 &lt;/s&gt; ... Span e6 &lt;/s&gt; ... BART-base &lt;rule&gt; EDU0 &lt;rule&gt; EDU1 &lt;rule&gt; EDU... &lt;s&gt; Scen. &lt;/s&gt; Ques. &lt;/s&gt; QA... &lt;/s&gt; Figure 2: The overall structure of our model O SCAR. The left part introduces the retrieval and tagging process for rule documents, which is then fed into the encoder together with other necessary information. Lawrence et al., 2019; Verma et al., 2020; Gao et al., 2020a,b; Ouyang et al., 2021) have made progress in modeling the matching relationships between the rule document and other elements such as user scenarios and questions. These studies are based on the hypothesis that the supporting information for answering the question is provided, which does not meet the real-world applications. Therefore, we are motivated to investigate the open-retrieval settings (Qu et al., 2020), where the retrieved background knowledge would be noisy. Gao et al. (2021) makes the initial attempts of open-retrieval for CMR. However, like previous studies, t"
2021.emnlp-main.299,Q19-1016,0,0.115503,"hallenges that have been neglected: able the machine to interact with human beings and 1) Open-retrieval of supporting evidence. The solve practical problems (Zhu et al., 2018; Zhang above existing methods assume that the relevant et al., 2018; Zaib et al., 2020; Huang et al., 2020; rule documents are given before the system interFan et al., 2020; Gu et al., 2021). It usually adopts acts with users, which is in a closed-book style. the form of question answering (QA) according to In real-world applications, the machines are ofthe user’s query along with the dialogue context (Sun et al., 2019; Reddy et al., 2019; Choi et al., ten required to retrieve supporting information to 2018). The machine may also actively ask ques- respond to incoming high-level queries in an intertions for confirmation (Wu et al., 2018; Cai et al., active manner, which results in an open-retrieval setting (Gao et al., 2021). The comparison of the 2019; Zhang et al., 2020b; Gu et al., 2020). closed-book setting and open-retrieval setting is In the classic spoken language understanding tasks (Tur and De Mori, 2011; Zhang et al., 2020a; shown in Figure 1. 2) The gap between decision making and quesRen et al., 2018; Qin et al., 2"
2021.emnlp-main.299,D18-1299,0,0.0339546,"Missing"
2021.emnlp-main.299,D18-1233,0,0.202927,"tickets (Price, 1990). In quires machines to communicate with humans real-world scenario, annotating data such as inthrough multi-turn interactions between two tents and slots is expensive. Inspired by the studies salient dialogue states of decision making and of reading comprehension (Rajpurkar et al., 2016, question generation processes. In open CMR 2018; Zhang et al., 2020c, 2021), there appears a settings, as the more realistic scenario, the retrieved background knowledge would be noisy, more general task — conversational machine readwhich results in severe challenges in the ining (CMR) (Saeidi et al., 2018): given the inquiry, formation transmission. Existing studies comthe machine is required to retrieve relevant supmonly train independent or pipeline systems porting rule documents, the machine should judge for the two subtasks. However, those methods whether the goal is satisfied according to the diaare trivial by using hard-label decisions to actilogue context, and make decisions or ask clarificavate question generation, which eventually hintion questions. ders the model performance. In this work, we propose an effective gating strategy by smoothA variety of methods have been proposed for ing"
2021.emnlp-main.299,Q19-1014,0,0.0881508,"logue is to enof challenges that have been neglected: able the machine to interact with human beings and 1) Open-retrieval of supporting evidence. The solve practical problems (Zhu et al., 2018; Zhang above existing methods assume that the relevant et al., 2018; Zaib et al., 2020; Huang et al., 2020; rule documents are given before the system interFan et al., 2020; Gu et al., 2021). It usually adopts acts with users, which is in a closed-book style. the form of question answering (QA) according to In real-world applications, the machines are ofthe user’s query along with the dialogue context (Sun et al., 2019; Reddy et al., 2019; Choi et al., ten required to retrieve supporting information to 2018). The machine may also actively ask ques- respond to incoming high-level queries in an intertions for confirmation (Wu et al., 2018; Cai et al., active manner, which results in an open-retrieval setting (Gao et al., 2021). The comparison of the 2019; Zhang et al., 2020b; Gu et al., 2020). closed-book setting and open-retrieval setting is In the classic spoken language understanding tasks (Tur and De Mori, 2011; Zhang et al., 2020a; shown in Figure 1. 2) The gap between decision making and quesRen et al.,"
2021.emnlp-main.299,2020.acl-demos.30,0,0.237205,"u.cn,zhaohai@cs.sjtu.edu.cn {mutiyama,eiichiro.sumita}@nict.go.jp Abstract with people according to the dialogue states, and completes specific tasks, such as ordering meals Conversational machine reading (CMR) re(Liu et al., 2013) and air tickets (Price, 1990). In quires machines to communicate with humans real-world scenario, annotating data such as inthrough multi-turn interactions between two tents and slots is expensive. Inspired by the studies salient dialogue states of decision making and of reading comprehension (Rajpurkar et al., 2016, question generation processes. In open CMR 2018; Zhang et al., 2020c, 2021), there appears a settings, as the more realistic scenario, the retrieved background knowledge would be noisy, more general task — conversational machine readwhich results in severe challenges in the ining (CMR) (Saeidi et al., 2018): given the inquiry, formation transmission. Existing studies comthe machine is required to retrieve relevant supmonly train independent or pipeline systems porting rule documents, the machine should judge for the two subtasks. However, those methods whether the goal is satisfied according to the diaare trivial by using hard-label decisions to actilogue con"
2021.emnlp-main.299,C18-1317,1,0.893958,"Missing"
2021.emnlp-main.299,P19-1223,0,0.0225589,"Missing"
2021.emnlp-main.299,C18-2024,1,0.833381,"eriments on the OR-ShARC dataset Zettlemoyer, 2019; Lawrence et al., 2019; Verma show the effectiveness of our method, which et al., 2020; Gao et al., 2020a,b); 2) graph-based achieves new state-of-the-art results. methods that capture the discourse structures of the 1 Introduction rule texts and user scenario for better interactions (Ouyang et al., 2021). However, there are two sides The ultimate goal of multi-turn dialogue is to enof challenges that have been neglected: able the machine to interact with human beings and 1) Open-retrieval of supporting evidence. The solve practical problems (Zhu et al., 2018; Zhang above existing methods assume that the relevant et al., 2018; Zaib et al., 2020; Huang et al., 2020; rule documents are given before the system interFan et al., 2020; Gu et al., 2021). It usually adopts acts with users, which is in a closed-book style. the form of question answering (QA) according to In real-world applications, the machines are ofthe user’s query along with the dialogue context (Sun et al., 2019; Reddy et al., 2019; Choi et al., ten required to retrieve supporting information to 2018). The machine may also actively ask ques- respond to incoming high-level queries in an"
2021.emnlp-main.318,2020.findings-emnlp.382,0,0.0195947,", the ASTE is transformed as MRC task (Chen et al., 2021; Mao et al., 2021). Recently, Huang et al. (2021) proposed a sequence taggingbased model to perform representation learning on the ASTE task. 6.2 AESC Task The AESC task is to perform aspect terms extraction and sentiment classification simultaneously. Hu et al. (2019) and Zhou et al. (2019) used a span-level sequence tagging method to tackle huge search space and sentiment inconsistency problems. Although the huge search space issue has been solved by Hu et al. (2019), there still exists a lowperformance problem. Addressing this issue, Lin and Yang (2020) utilized a BERT encoder to contextualize shared information of target extraction and target classification subtasks. Meanwhile, they used two BiLSTM networks to encode the private information of each subtask, which greatly boosted the model performance. 6.3 Dual-encoder Structure Productive efforts were put into the research of dual-encoder structure for natural language processing tasks in the last few years because of the natural ability to model representational similarity maximization associated tasks (Chidambaram et al., 2019; Yu et al., 2020; Bhowmik et al., 2021). Generally, these appr"
2021.emnlp-main.318,2021.ccl-1.108,0,0.0809366,"Missing"
2021.emnlp-main.318,2021.findings-acl.290,1,0.736843,"long-range dependency, which was particularly paid attention to but not solved well by Zhang et al. (2020a). It is due to incorporating the self-attention mechanism and GRU in two dimensions, and our model is sensitive to the difference between the proposed dual-encoder architecture. Collectively, these aforementioned cases demonstrate the robustness of our dual-encoder model. 6 Related Work Recently, NLP has been developed rapidly (He et al., 2018; Li et al., 2018; Cai et al., 2018; Li et al., 2019b; Jiang et al., 2020; Zhang et al., 2021), and the process is further by deep neural networks (Parnow et al., 2021; Li et al., 2021a) and pre-trained language models (Li et al., 2021b; Zhang et al., 2020b). Aspect-based sentiment analysis was proposed by Pontiki et al. (2014) and also received lots of attention in recent years. 6.1 ASTE Task The ASTE task aims to make triplet extraction of aspect terms, opinion terms, and sentiment polarity, which was introduced by Peng et al. (2020). In their work, they leveraged the sequence labeling method to extract aspect terms and target sentiment and utilized graph neural networks to detect candidate opinion terms. Zhang et al. (2020a) proposed a multi-task framewo"
2021.emnlp-main.318,D14-1162,0,0.0844273,"Missing"
2021.emnlp-main.318,S15-2082,0,0.0608487,"Missing"
2021.emnlp-main.318,S14-2004,0,0.63218,"s attention on sequence labeling. Empirical results show that our proposed model shows robustness and significantly outperforms the previous state-ofthe-art on four benchmark datasets. 1 Introduction Sentiment analysis is a task that aims to retrieve the sentiment polarity based on three levels of granularities: document level, sentence level, and entity and aspect level (Liu, 2012), which is under the urgent demands of several society scenarios (Preethi et al., 2017; Cobos et al., 2019; Islam and Zibran, 2017; Novielli et al., 2018). Recently, the aspect-based sentiment analysis (ABSA) task (Pontiki et al., 2014), focusing on excavating the specific aspect from an annotated review, has aroused much attention from researchers, in which this paper mainly concerns the aspect/opinion term extraction and sentiment classification task. The latest benchmark proposed by Peng et al. (2020) formulates the relevant information into a triplet: target aspect object, opinion clue, and sentiment polarity orientation. Thus, the concerned aspect term extraction becomes a task of Aspect Sentiment Triplet ∗ Corresponding author.        positive positive The view is spectac"
2021.emnlp-main.318,2020.emnlp-main.133,0,0.190036,"e sentiment polarity tagging style of the dataset, the SC subtask is divided into two categories: ASTE, tagging SP on AT and OT, and AESC, which tags SP only on AT. In particular, we denote AT, OT and SP as the set of predefined aspect terms, opinion terms and sentiment polarities, respectively, where AT ∈ AT, OT ∈ OT, and SP ∈ SP = {POS, NEU, NEG}. Given a sentence s consisting of n tokens ω1 , ω2 , ..., ωn , we denote T as the sentence output of our model. Specifically, for the ASTE task, T = {(AT, OT, SP )}, and for the AESC task, T = {(AT, SP )}. 2.2 Model Overview Inspired by the work of Wang and Lu (2020) which utilize the dual-encoder structure, our approach for the ABSA task is designed to subtly modeling high affinity between aspect/opinion pair and ground truth by effectively leveraging the pair representation. As shown in Figure 2, our dual-encoder comprises two modules: (1) a sequence encoder, a Transformer network initialized with the pretrained language model to represent AT and OT with the corresponding context. (2) a pair encoder, encoding the aspect-opinion pair (for ASTE) or aspect-aspect pair (for AESC) for each sentiment polarity. 2.3 Token Representation For a length-n input sen"
2021.emnlp-main.318,2020.findings-emnlp.234,0,0.449246,"uring structural information of term span. Meanwhile, the many-target to one-opinion issue is not effectively handled. The promising results achieved by machine reading comprehension (MRC) frameworks on solving many other NLP tasks (Li et al., 2020, 2019a) also inspires the ASTE task. Mao et al. (2021) and Chen et al. (2021) attempted to design question-answer pair in terms of MRC to formulate the triplet extraction. Nevertheless, both need to make the converted question correspond one-to-one to the designed question manually, hence increasing computation complexity. Among these joint models, Wu et al. (2020) transformed the sequence representation into the two-dimension space and argued that the word-pair under at least one assumption could represent the aspect-opinion pair as input of different encoders. Although this model indicated significant improvement, it treated the word-pair without taking span boundary of aspect term and opinion term into consideration and incorporated nonexistent pre-defined aspect-opinion pairs. Considering the problems mentioned above, we propose a dual-encoder model based on a pretrained language model by jointly fine-tuning multiple encoders on the ABSA task. Simil"
2021.emnlp-main.318,P18-2094,0,0.0193138,"information fed into our model. The tagging scheme of our model is illustrated in Figure 3, in which the main diagonal are filled with AT and OT accompanying entries to the right of the main diagonal with span pairs. Compared to (Wu et al., 2020), our method may heavily reduce the redundancy aroused by AT and OT tags at the right of the main diagonal. Second, we consider the context information on both two-dimension spaces and the historical information with the utilization of the recurrent neural network (RNN). However, Wu et al. (2020) merely adopted a single encoder which based on DE-CNN (Xu et al., 2018)/BiLSTM/BERT (Devlin et al., 2019) to establish token representation, and they formulated the final word-pair representation by a                  of each word to feed into the dual-encoder:  Figure 2: The framework of our model. Dashed lines are for optional components.              Figure 3: A tagging example for our model. simplified method of attention-guided word-pair concatenation. Thus, our dual-encoder could jointly encode AT, OT (with the corresponding context on both dimensions), a"
2021.emnlp-main.318,2020.emnlp-main.183,0,0.0652476,"Missing"
2021.emnlp-main.318,2020.findings-emnlp.72,0,0.370301,"Figure 1: The subtasks in our proposed model. Extraction (ASTE). Similarly, the relevant information is formulated into a pair with aspect term and sentiment polarity, and the task is defined as Aspect Term Extraction and Sentiment Classification (AESC). Figure 1 shows an example of ASTE and AESC. Two early methods handle the triplet extraction task efficiently (Zhang et al., 2020a; Huang et al., 2021). Both are typically composed of a sequence representation layer to predict the aspect/opinion term mentions and a classification layer to infer the sentiment polarity of the predicted mention pair of the last layer. However, as is often the case, such model design may easily result in that the errors of the upper prediction layer would hurt the accuracy of the lower layer during the training procedure. To tackle the error cascading phenomenon on the pipeline model, a growing trend of jointly modeling these subtasks in one shot appears. Xu et al. (2020) proposed a joint m"
2021.emnlp-tutorials.6,D17-1304,1,0.866418,"Missing"
2021.emnlp-tutorials.6,2020.acl-main.153,0,0.0183686,"et al., 2020) and so on. On the other hand, linguistic prior knowledge (i.e. alignment, bilingual lexicon, phrase table, and knowledge graphs) to alleviate the problem of inadequacy target translations which are caused by the language model property of the encoderdecoder framework (Feng et al., 2017; Zhang et al., 2017; Zhao et al., 2018a; Wang et al., 2018b). Moreover, linguistic differences between the source language and target language can learn natural language representations that are easy to be understood by the translation model, for example, word order difference (Chen et al., 2019; Ding et al., 2020), morphological differences (Ji et al., 2019) and so on. Meanwhile, linguistic shared feature between the source language and target language can also enhance the understanding and generation of natural language in MT, for example, shared words (Artetxe et al., 2018), image information (Yin et al., 2020), video information (Wang et al., 2020) and so on. 2 Relevance to the Computational Linguistics Community The topics included in this tutorial, i.e., syntax parsing, SRL, and MT, are all the classic ones to the entire NLP/CL community. This tutorial is primarily towards researchers who have a b"
2021.emnlp-tutorials.6,P17-1044,0,0.0994261,"other researchers. 6 Diversity Considerations N/A 7 Specification of Any Prerequisites for the Attendees This tutorial is primarily aimed at researchers who have a basic understanding of NLP. 8 Presenters Small reading list • Deep Learning: Deep learning (LeCun et al., 2015) • Syntactic Parsing: Deep biaffine attention for neural dependency parsing (Dozat and Manning, 2016) and Constituency parsing with a self-attentive encoder (Kitaev and Klein, 2018). • SRL: Syntax for semantic role labeling, to be, or not to be (He et al., 2018b) and Deep semantic role labeling: What works and whats next (He et al., 2017b). • Machine Translation: Statistical machine translation (Koehn, 2009) and Neural machine translation by jointly learning to align and translate (Bahdanau et al., 2015). 29 conferences, such as CWMT, CCL, etc. He served as the area chairs of ICLR-2021 and NAACL2021. 3. Dr. Kehai Chen, Postdoctoral Researcher, Advanced Translation Technology Laboratory, National Institute of Information and Communications Technology (NICT), Japan khchen@nict.go.jp https://chenkehai.github.io His research focuses on linguistic-motivated machine translation (MT), a classic NLP task in AI. He has published more"
2021.emnlp-tutorials.6,N19-1205,0,0.0200961,"tion based transformer (Vaswani et al., 2017) has become new state-of-the-art architecture in NMT and gives a series of new state-of-the-art benchmarks (Bojar et al., 2018; Marie et al., 2018; Wang et al., 2018a; Marie et al., 2019). Syntax information has been shown that it can improve the performances of the recurrent neural network (RNN) based NMT on conditions (Eriguchi et al., 2016, 2017; Chen et al., 2017a; Li et al., 2017; Wu et al., 2017; Chen et al., 2017b, 2018). However, so far it has not been shown significantly widely useful in self-attention based NMT. There are only a few work (Ma et al., 2019) adopted the syntactic information into the positional embedding of Transformer. We will give a detailed analysis on this issue by surveying the key technique details. common sense, such as over/under-translation questions (Tu et al., 2016), troublesome words modeling (Zhao et al., 2018b) and so on; (2) to have some basic abilities of human translator, for example, word importance modeling (Chen et al., 2020), translation refinement (Song et al., 2020), structured information (Xu et al., 2020), diverse feature (Chen et al., 2020) and so on. On the other hand, linguistic prior knowledge (i.e. a"
2021.emnlp-tutorials.6,P18-1192,1,0.87376,"Missing"
2021.emnlp-tutorials.6,K17-1041,0,0.0300525,"Missing"
2021.emnlp-tutorials.6,W19-5330,1,0.835416,"Knight, 2001; Mi et al., 2008). In some scenarios, especially when the domain of the MT corpus is similar to the domain of the parsing corpus, the performance of tree based SMT is better than phrase based SMT (Koehn, 2009). For NMT, it so far achieves significant progress by using end-toend based structure since 2014 (Sutskever et al., 2014; Bahdanau et al., 2015). Recently, selfattention based transformer (Vaswani et al., 2017) has become new state-of-the-art architecture in NMT and gives a series of new state-of-the-art benchmarks (Bojar et al., 2018; Marie et al., 2018; Wang et al., 2018a; Marie et al., 2019). Syntax information has been shown that it can improve the performances of the recurrent neural network (RNN) based NMT on conditions (Eriguchi et al., 2016, 2017; Chen et al., 2017a; Li et al., 2017; Wu et al., 2017; Chen et al., 2017b, 2018). However, so far it has not been shown significantly widely useful in self-attention based NMT. There are only a few work (Ma et al., 2019) adopted the syntactic information into the positional embedding of Transformer. We will give a detailed analysis on this issue by surveying the key technique details. common sense, such as over/under-translation que"
2021.emnlp-tutorials.6,W18-6419,1,0.827852,"ds have been well developed (Yamada and Knight, 2001; Mi et al., 2008). In some scenarios, especially when the domain of the MT corpus is similar to the domain of the parsing corpus, the performance of tree based SMT is better than phrase based SMT (Koehn, 2009). For NMT, it so far achieves significant progress by using end-toend based structure since 2014 (Sutskever et al., 2014; Bahdanau et al., 2015). Recently, selfattention based transformer (Vaswani et al., 2017) has become new state-of-the-art architecture in NMT and gives a series of new state-of-the-art benchmarks (Bojar et al., 2018; Marie et al., 2018; Wang et al., 2018a; Marie et al., 2019). Syntax information has been shown that it can improve the performances of the recurrent neural network (RNN) based NMT on conditions (Eriguchi et al., 2016, 2017; Chen et al., 2017a; Li et al., 2017; Wu et al., 2017; Chen et al., 2017b, 2018). However, so far it has not been shown significantly widely useful in self-attention based NMT. There are only a few work (Ma et al., 2019) adopted the syntactic information into the positional embedding of Transformer. We will give a detailed analysis on this issue by surveying the key technique details. common"
2021.emnlp-tutorials.6,P17-1065,0,0.0143432,"). For NMT, it so far achieves significant progress by using end-toend based structure since 2014 (Sutskever et al., 2014; Bahdanau et al., 2015). Recently, selfattention based transformer (Vaswani et al., 2017) has become new state-of-the-art architecture in NMT and gives a series of new state-of-the-art benchmarks (Bojar et al., 2018; Marie et al., 2018; Wang et al., 2018a; Marie et al., 2019). Syntax information has been shown that it can improve the performances of the recurrent neural network (RNN) based NMT on conditions (Eriguchi et al., 2016, 2017; Chen et al., 2017a; Li et al., 2017; Wu et al., 2017; Chen et al., 2017b, 2018). However, so far it has not been shown significantly widely useful in self-attention based NMT. There are only a few work (Ma et al., 2019) adopted the syntactic information into the positional embedding of Transformer. We will give a detailed analysis on this issue by surveying the key technique details. common sense, such as over/under-translation questions (Tu et al., 2016), troublesome words modeling (Zhao et al., 2018b) and so on; (2) to have some basic abilities of human translator, for example, word importance modeling (Chen et al., 2020), translation refinem"
2021.emnlp-tutorials.6,P08-1023,0,0.0723227,"0” denotes mainly studies in zero/lowresource scenarios; “-” denotes negative or little impact. The mark in the rightmost column indicates whether it is overall effective when all marked factors to the left are combined. Syntax in MT also endures a methodology change from statistical machine translation (SMT) (Brown et al., 1993) to neural machine translation (NMT) (Sutskever et al., 2014; Bahdanau et al., 2015) as the task of SRL. For typical SMT, besides phrase based SMT (Och et al., 1999; Koehn et al., 2003), syntactic (tree) based methods have been well developed (Yamada and Knight, 2001; Mi et al., 2008). In some scenarios, especially when the domain of the MT corpus is similar to the domain of the parsing corpus, the performance of tree based SMT is better than phrase based SMT (Koehn, 2009). For NMT, it so far achieves significant progress by using end-toend based structure since 2014 (Sutskever et al., 2014; Bahdanau et al., 2015). Recently, selfattention based transformer (Vaswani et al., 2017) has become new state-of-the-art architecture in NMT and gives a series of new state-of-the-art benchmarks (Bojar et al., 2018; Marie et al., 2018; Wang et al., 2018a; Marie et al., 2019). Syntax in"
2021.emnlp-tutorials.6,2020.acl-main.37,0,0.0482381,"Missing"
2021.emnlp-tutorials.6,W99-0604,0,0.716643,"Missing"
2021.emnlp-tutorials.6,P01-1067,0,0.347375,"moderate contribution; “0” denotes mainly studies in zero/lowresource scenarios; “-” denotes negative or little impact. The mark in the rightmost column indicates whether it is overall effective when all marked factors to the left are combined. Syntax in MT also endures a methodology change from statistical machine translation (SMT) (Brown et al., 1993) to neural machine translation (NMT) (Sutskever et al., 2014; Bahdanau et al., 2015) as the task of SRL. For typical SMT, besides phrase based SMT (Och et al., 1999; Koehn et al., 2003), syntactic (tree) based methods have been well developed (Yamada and Knight, 2001; Mi et al., 2008). In some scenarios, especially when the domain of the MT corpus is similar to the domain of the parsing corpus, the performance of tree based SMT is better than phrase based SMT (Koehn, 2009). For NMT, it so far achieves significant progress by using end-toend based structure since 2014 (Sutskever et al., 2014; Bahdanau et al., 2015). Recently, selfattention based transformer (Vaswani et al., 2017) has become new state-of-the-art architecture in NMT and gives a series of new state-of-the-art benchmarks (Bojar et al., 2018; Marie et al., 2018; Wang et al., 2018a; Marie et al."
2021.emnlp-tutorials.6,J05-1004,0,0.426004,"Missing"
2021.emnlp-tutorials.6,2020.acl-main.273,0,0.0253252,"ng et al., 2017; Zhao et al., 2018a; Wang et al., 2018b). Moreover, linguistic differences between the source language and target language can learn natural language representations that are easy to be understood by the translation model, for example, word order difference (Chen et al., 2019; Ding et al., 2020), morphological differences (Ji et al., 2019) and so on. Meanwhile, linguistic shared feature between the source language and target language can also enhance the understanding and generation of natural language in MT, for example, shared words (Artetxe et al., 2018), image information (Yin et al., 2020), video information (Wang et al., 2020) and so on. 2 Relevance to the Computational Linguistics Community The topics included in this tutorial, i.e., syntax parsing, SRL, and MT, are all the classic ones to the entire NLP/CL community. This tutorial is primarily towards researchers who have a basic understanding of deep learning based NLP. We believe that this tutorial would help the audience more deeply understand the relationship between three classic NLP tasks, i.e., syntax parsing and SRL/MT. Linguistic in MT. In addition, we will investigate why linguistic cognition and prior knowledge ca"
2021.emnlp-tutorials.6,P17-1139,0,0.0115986,"Tu et al., 2016), troublesome words modeling (Zhao et al., 2018b) and so on; (2) to have some basic abilities of human translator, for example, word importance modeling (Chen et al., 2020), translation refinement (Song et al., 2020), structured information (Xu et al., 2020), diverse feature (Chen et al., 2020) and so on. On the other hand, linguistic prior knowledge (i.e. alignment, bilingual lexicon, phrase table, and knowledge graphs) to alleviate the problem of inadequacy target translations which are caused by the language model property of the encoderdecoder framework (Feng et al., 2017; Zhang et al., 2017; Zhao et al., 2018a; Wang et al., 2018b). Moreover, linguistic differences between the source language and target language can learn natural language representations that are easy to be understood by the translation model, for example, word order difference (Chen et al., 2019; Ding et al., 2020), morphological differences (Ji et al., 2019) and so on. Meanwhile, linguistic shared feature between the source language and target language can also enhance the understanding and generation of natural language in MT, for example, shared words (Artetxe et al., 2018), image information (Yin et al., 202"
2021.emnlp-tutorials.6,P16-1008,0,0.0123334,"information has been shown that it can improve the performances of the recurrent neural network (RNN) based NMT on conditions (Eriguchi et al., 2016, 2017; Chen et al., 2017a; Li et al., 2017; Wu et al., 2017; Chen et al., 2017b, 2018). However, so far it has not been shown significantly widely useful in self-attention based NMT. There are only a few work (Ma et al., 2019) adopted the syntactic information into the positional embedding of Transformer. We will give a detailed analysis on this issue by surveying the key technique details. common sense, such as over/under-translation questions (Tu et al., 2016), troublesome words modeling (Zhao et al., 2018b) and so on; (2) to have some basic abilities of human translator, for example, word importance modeling (Chen et al., 2020), translation refinement (Song et al., 2020), structured information (Xu et al., 2020), diverse feature (Chen et al., 2020) and so on. On the other hand, linguistic prior knowledge (i.e. alignment, bilingual lexicon, phrase table, and knowledge graphs) to alleviate the problem of inadequacy target translations which are caused by the language model property of the encoderdecoder framework (Feng et al., 2017; Zhang et al., 20"
2021.emnlp-tutorials.6,P15-1109,0,0.0900917,"Missing"
2021.findings-acl.235,2020.acl-main.9,0,0.242632,"t rules, and one speaker may continuously shoot multiple utterances. Although some existing works have noticed such 2663 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 2663–2673 August 1–6, 2021. ©2021 Association for Computational Linguistics nonlinear nature of multi-turn dialogues, they are limited to conducting post-training or pre-training in a specific domain and do not provide generalpurpose dialogue-oriented PrLMs to fundamentally solve this problem (Xu et al., 2021a; Whang et al., 2021; Wolf et al., 2019; Zhang et al., 2020b; Henderson et al., 2020; Bao et al., 2020). In this work, we make the first attempt to train a general-purpose dialogue-oriented PrLM. However, such a PrLM should be trained on huge dialogue data, which is hard to collect. Thus we propose three novel pre-training strategies (i.e., Insertion, Deletion, Replacement), so that we facilitate plain text originally for common PrLM training to simulate dialogue-like features. The resulted model, we denote as Dialog-PrLM, then is capable of effectively learning speaker awareness, continuity and consistency in a general way. Especially, for the convenient use of the downstream dialogue tasks, w"
2021.findings-acl.235,N19-1423,0,0.193087,"a (No. 2017YFB0304100), Key Projects of National Natural Science Foundation of China (U1836222 and 61733011), Huawei-SJTU long term AI project, Cuttingedge Machine Reading Comprehension and Language Model. This work was supported by Huawei Noah’s Ark Lab. most suitable response from a collection of answers (Zhou et al., 2016; Wu et al., 2017; Zhou et al., 2018b; Zhu et al., 2018; Zhang et al., 2018; Tao et al., 2019; Gu et al., 2019). Pre-training tasks of all these PrLMs almost concentrate on two aspects: token prediction and sentence relation prediction. For example, the genetic BERT model (Devlin et al., 2019) uses masked language modeling (MLM) and next sentence prediction (NSP) objectives; ALBERT (Lan et al., 2020) predicts sentence order rather than NSP; ELECTRA (Clark et al., 2020) transfers MLM into a generating and then discriminating process like GAN (Goodfellow et al., 2014). However, these tasks are just devoted to incorporating token-level and sentence-level semantic information into embeddings, and cannot be sufficiently compatible with its dialogue-oriented characteristics. Table 1 shows a multi-turn dialogue example. Compared with plain text, the utterance turn and speaker role keep sh"
2021.findings-acl.235,D19-1193,0,0.0240653,"s, which takes a dialogue history as input and aims to select a ∗ Corresponding author. This paper was partially supported by National Key Research and Development Program of China (No. 2017YFB0304100), Key Projects of National Natural Science Foundation of China (U1836222 and 61733011), Huawei-SJTU long term AI project, Cuttingedge Machine Reading Comprehension and Language Model. This work was supported by Huawei Noah’s Ark Lab. most suitable response from a collection of answers (Zhou et al., 2016; Wu et al., 2017; Zhou et al., 2018b; Zhu et al., 2018; Zhang et al., 2018; Tao et al., 2019; Gu et al., 2019). Pre-training tasks of all these PrLMs almost concentrate on two aspects: token prediction and sentence relation prediction. For example, the genetic BERT model (Devlin et al., 2019) uses masked language modeling (MLM) and next sentence prediction (NSP) objectives; ALBERT (Lan et al., 2020) predicts sentence order rather than NSP; ELECTRA (Clark et al., 2020) transfers MLM into a generating and then discriminating process like GAN (Goodfellow et al., 2014). However, these tasks are just devoted to incorporating token-level and sentence-level semantic information into embeddings, and cannot be"
2021.findings-acl.235,2020.findings-emnlp.196,0,0.0422896,"Missing"
2021.findings-acl.235,2020.emnlp-main.150,0,0.0158819,"s tend to model the interaction between each dialogue utterance and the response, which usually adopt the encoding-matching-aggregation paradigm (Zhou et al., 2016; Wu et al., 2017; Zhang et al., 2018; Zhou et al., 2018a,b; Tao et al., 2019; Yuan et al., 2019). After encoding, distinct matching networks generate features for each utterance which are usually passed to GRU (Cho et al., 2014) for aggregating into a final matching score. Besides, some works adopt topic information (Xing et al., 2017; Wu et al., 2018; Xu et al., 2021b) or conversation disentanglement to select the proper response (Jia et al., 2020; Wang et al., 2020). There are more and more practice using powerful PrLMs as the model encoder (Zhang et al., 2021, 2020a; Zhu et al., 2020) like BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019), ALBERT (Lan et al., 2020) and ELECTRA (Clark et al., 2020). Considering task domain difference from the general corpus for PrLM pre-training, recent studies start to conduct post-training on target multi-turn dialogue datasets to incorporate in-domain knowledge (Whang et al., 2020; Lu et al., 2020; Gu et al., 2020; Xu et al., 2021a; Whang et al., 2021). Whang et al. (2020) conduct post-trainin"
2021.findings-acl.235,2021.ccl-1.108,0,0.0777108,"Missing"
2021.findings-acl.235,W15-4640,0,0.218247,"erance. We mimic dialogue-related features on conventional plain text, which can bring up the possibility that similar techniques could be adopted in other domains not only for dialogues. Our pre-trained Dialog-PrLM is fine-tuned on three multi-turn dialogue response selection benchmarks, and obtains significant and consistent improvements over the plain PrLMs. 2 Related Work For multi-turn dialogue response selection task, earlier works conduct single-turn match, which concatenates all the utterances in the history dialogue or just considers the last one to match with the candidate response (Lowe et al., 2015; Kadlec et al., 2015; Yan et al., 2016; Tan et al., 2016; Wan et al., 2016; Wang and Jiang, 2016). Recently, existing works tend to model the interaction between each dialogue utterance and the response, which usually adopt the encoding-matching-aggregation paradigm (Zhou et al., 2016; Wu et al., 2017; Zhang et al., 2018; Zhou et al., 2018a,b; Tao et al., 2019; Yuan et al., 2019). After encoding, distinct matching networks generate features for each utterance which are usually passed to GRU (Cho et al., 2014) for aggregating into a final matching score. Besides, some works adopt topic informa"
2021.findings-acl.235,2021.findings-acl.279,1,0.750672,", 2021, 2020a; Zhu et al., 2020) like BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019), ALBERT (Lan et al., 2020) and ELECTRA (Clark et al., 2020). Considering task domain difference from the general corpus for PrLM pre-training, recent studies start to conduct post-training on target multi-turn dialogue datasets to incorporate in-domain knowledge (Whang et al., 2020; Lu et al., 2020; Gu et al., 2020; Xu et al., 2021a; Whang et al., 2021). Whang et al. (2020) conduct post-training of MLM and NSP tasks as BERT. Rather than using the same tasks as PrLMs, Xu et al. (2021a) and Whang et al. (2021) both considers auxiliary tasks through posttraining to enhance response selection. Although the PrLMs which are trained on plain text have learned contextual semantic representation from token-level or sentence-level pre-training tasks like MLM, NSP, they all do not consider dialogue related features like speaker role, continuity and consistency. Despite some existing works (Xu et al., 2021a; Whang et al., 2021) considers that when conducting post-training, they are limited to a specific domain. (Wolf et al., 2019; Zhang et al., 2020b; Henderson et al., 2020; Bao et al., 2020) train on open-d"
2021.findings-acl.235,P17-1046,0,0.364902,"et al., 2020; Li et al., 2021), including the response selection task for multi-turn dialogues, which takes a dialogue history as input and aims to select a ∗ Corresponding author. This paper was partially supported by National Key Research and Development Program of China (No. 2017YFB0304100), Key Projects of National Natural Science Foundation of China (U1836222 and 61733011), Huawei-SJTU long term AI project, Cuttingedge Machine Reading Comprehension and Language Model. This work was supported by Huawei Noah’s Ark Lab. most suitable response from a collection of answers (Zhou et al., 2016; Wu et al., 2017; Zhou et al., 2018b; Zhu et al., 2018; Zhang et al., 2018; Tao et al., 2019; Gu et al., 2019). Pre-training tasks of all these PrLMs almost concentrate on two aspects: token prediction and sentence relation prediction. For example, the genetic BERT model (Devlin et al., 2019) uses masked language modeling (MLM) and next sentence prediction (NSP) objectives; ALBERT (Lan et al., 2020) predicts sentence order rather than NSP; ELECTRA (Clark et al., 2020) transfers MLM into a generating and then discriminating process like GAN (Goodfellow et al., 2014). However, these tasks are just devoted to in"
2021.findings-acl.235,D19-1011,0,0.0250428,"ulti-turn dialogues, which takes a dialogue history as input and aims to select a ∗ Corresponding author. This paper was partially supported by National Key Research and Development Program of China (No. 2017YFB0304100), Key Projects of National Natural Science Foundation of China (U1836222 and 61733011), Huawei-SJTU long term AI project, Cuttingedge Machine Reading Comprehension and Language Model. This work was supported by Huawei Noah’s Ark Lab. most suitable response from a collection of answers (Zhou et al., 2016; Wu et al., 2017; Zhou et al., 2018b; Zhu et al., 2018; Zhang et al., 2018; Tao et al., 2019; Gu et al., 2019). Pre-training tasks of all these PrLMs almost concentrate on two aspects: token prediction and sentence relation prediction. For example, the genetic BERT model (Devlin et al., 2019) uses masked language modeling (MLM) and next sentence prediction (NSP) objectives; ALBERT (Lan et al., 2020) predicts sentence order rather than NSP; ELECTRA (Clark et al., 2020) transfers MLM into a generating and then discriminating process like GAN (Goodfellow et al., 2014). However, these tasks are just devoted to incorporating token-level and sentence-level semantic information into embeddi"
2021.findings-acl.235,N16-1170,0,0.0306029,"possibility that similar techniques could be adopted in other domains not only for dialogues. Our pre-trained Dialog-PrLM is fine-tuned on three multi-turn dialogue response selection benchmarks, and obtains significant and consistent improvements over the plain PrLMs. 2 Related Work For multi-turn dialogue response selection task, earlier works conduct single-turn match, which concatenates all the utterances in the history dialogue or just considers the last one to match with the candidate response (Lowe et al., 2015; Kadlec et al., 2015; Yan et al., 2016; Tan et al., 2016; Wan et al., 2016; Wang and Jiang, 2016). Recently, existing works tend to model the interaction between each dialogue utterance and the response, which usually adopt the encoding-matching-aggregation paradigm (Zhou et al., 2016; Wu et al., 2017; Zhang et al., 2018; Zhou et al., 2018a,b; Tao et al., 2019; Yuan et al., 2019). After encoding, distinct matching networks generate features for each utterance which are usually passed to GRU (Cho et al., 2014) for aggregating into a final matching score. Besides, some works adopt topic information (Xing et al., 2017; Wu et al., 2018; Xu et al., 2021b) or conversation disentanglement to sel"
2021.findings-acl.235,2020.emnlp-main.533,0,0.0425098,"Missing"
2021.findings-acl.235,2020.emnlp-main.66,0,0.100681,"or sentence-level pre-training tasks like MLM, NSP, they all do not consider dialogue related features like speaker role, continuity and consistency. Despite some existing works (Xu et al., 2021a; Whang et al., 2021) considers that when conducting post-training, they are limited to a specific domain. (Wolf et al., 2019; Zhang et al., 2020b; Henderson et al., 2020; Bao et al., 2020) train on open-domain conversational data like Reddit for response selection or generation tasks, but they are limited to original pre-training tasks on plain text and ignore the dialogue related features. Besides, Wu et al. (2020) and Li et al. (2020) conduct task-specific training on collected dialogue corpora, but they also suffer from biased and limited amount of dialogue data. Different from all the previous studies, we still make an attempt in obtaining a general-purpose PrLM but not aiming at any specific tasks like post-training methods. Meanwhile, our proposed dialogue-oriented pre-training enables the resulted PrLMs to especially capture dialogue related features in a general way. 3 Dialogue-oriented Pre-training For dialogue-oriented pre-training, we split and sample sentences as ”utterances” from the general"
2021.findings-acl.235,2020.acl-demos.30,0,0.603239,"ility of learning dialogue related features including speaker awareness, continuity and consistency. The resulted Dialog-PrLM is fine-tuned on three public multi-turn dialogue datasets and helps achieve significant and consistent improvement over the plain PrLMs. 1 Table 1: A multi-turn dialogue example with interleaved or continuous utterances between two speakers. Introduction Recently, pre-trained language models (PrLMs) have shown impressive improvements for various downstream NLP tasks (Zhou et al., 2020; Ouyang et al., 2021; Zhang and Zhao, 2021; Radford et al., 2018; Yang et al., 2019; Zhang et al., 2020c; Clark et al., 2020; Li et al., 2021), including the response selection task for multi-turn dialogues, which takes a dialogue history as input and aims to select a ∗ Corresponding author. This paper was partially supported by National Key Research and Development Program of China (No. 2017YFB0304100), Key Projects of National Natural Science Foundation of China (U1836222 and 61733011), Huawei-SJTU long term AI project, Cuttingedge Machine Reading Comprehension and Language Model. This work was supported by Huawei Noah’s Ark Lab. most suitable response from a collection of answers (Zhou et al"
2021.findings-acl.235,C18-1317,1,0.937822,"selection task for multi-turn dialogues, which takes a dialogue history as input and aims to select a ∗ Corresponding author. This paper was partially supported by National Key Research and Development Program of China (No. 2017YFB0304100), Key Projects of National Natural Science Foundation of China (U1836222 and 61733011), Huawei-SJTU long term AI project, Cuttingedge Machine Reading Comprehension and Language Model. This work was supported by Huawei Noah’s Ark Lab. most suitable response from a collection of answers (Zhou et al., 2016; Wu et al., 2017; Zhou et al., 2018b; Zhu et al., 2018; Zhang et al., 2018; Tao et al., 2019; Gu et al., 2019). Pre-training tasks of all these PrLMs almost concentrate on two aspects: token prediction and sentence relation prediction. For example, the genetic BERT model (Devlin et al., 2019) uses masked language modeling (MLM) and next sentence prediction (NSP) objectives; ALBERT (Lan et al., 2020) predicts sentence order rather than NSP; ELECTRA (Clark et al., 2020) transfers MLM into a generating and then discriminating process like GAN (Goodfellow et al., 2014). However, these tasks are just devoted to incorporating token-level and sentence-level semantic inform"
2021.findings-acl.235,2020.findings-emnlp.399,1,0.724816,"may yield a generalpurpose PrLM and does not individualize to any detailed task while keeping the capability of learning dialogue related features including speaker awareness, continuity and consistency. The resulted Dialog-PrLM is fine-tuned on three public multi-turn dialogue datasets and helps achieve significant and consistent improvement over the plain PrLMs. 1 Table 1: A multi-turn dialogue example with interleaved or continuous utterances between two speakers. Introduction Recently, pre-trained language models (PrLMs) have shown impressive improvements for various downstream NLP tasks (Zhou et al., 2020; Ouyang et al., 2021; Zhang and Zhao, 2021; Radford et al., 2018; Yang et al., 2019; Zhang et al., 2020c; Clark et al., 2020; Li et al., 2021), including the response selection task for multi-turn dialogues, which takes a dialogue history as input and aims to select a ∗ Corresponding author. This paper was partially supported by National Key Research and Development Program of China (No. 2017YFB0304100), Key Projects of National Natural Science Foundation of China (U1836222 and 61733011), Huawei-SJTU long term AI project, Cuttingedge Machine Reading Comprehension and Language Model. This work"
2021.findings-acl.235,D16-1036,0,0.0239369,"Missing"
2021.findings-acl.235,P18-1103,0,0.0770017,"et al., 2021), including the response selection task for multi-turn dialogues, which takes a dialogue history as input and aims to select a ∗ Corresponding author. This paper was partially supported by National Key Research and Development Program of China (No. 2017YFB0304100), Key Projects of National Natural Science Foundation of China (U1836222 and 61733011), Huawei-SJTU long term AI project, Cuttingedge Machine Reading Comprehension and Language Model. This work was supported by Huawei Noah’s Ark Lab. most suitable response from a collection of answers (Zhou et al., 2016; Wu et al., 2017; Zhou et al., 2018b; Zhu et al., 2018; Zhang et al., 2018; Tao et al., 2019; Gu et al., 2019). Pre-training tasks of all these PrLMs almost concentrate on two aspects: token prediction and sentence relation prediction. For example, the genetic BERT model (Devlin et al., 2019) uses masked language modeling (MLM) and next sentence prediction (NSP) objectives; ALBERT (Lan et al., 2020) predicts sentence order rather than NSP; ELECTRA (Clark et al., 2020) transfers MLM into a generating and then discriminating process like GAN (Goodfellow et al., 2014). However, these tasks are just devoted to incorporating token-l"
2021.findings-acl.235,C18-2024,1,0.843212,"ding the response selection task for multi-turn dialogues, which takes a dialogue history as input and aims to select a ∗ Corresponding author. This paper was partially supported by National Key Research and Development Program of China (No. 2017YFB0304100), Key Projects of National Natural Science Foundation of China (U1836222 and 61733011), Huawei-SJTU long term AI project, Cuttingedge Machine Reading Comprehension and Language Model. This work was supported by Huawei Noah’s Ark Lab. most suitable response from a collection of answers (Zhou et al., 2016; Wu et al., 2017; Zhou et al., 2018b; Zhu et al., 2018; Zhang et al., 2018; Tao et al., 2019; Gu et al., 2019). Pre-training tasks of all these PrLMs almost concentrate on two aspects: token prediction and sentence relation prediction. For example, the genetic BERT model (Devlin et al., 2019) uses masked language modeling (MLM) and next sentence prediction (NSP) objectives; ALBERT (Lan et al., 2020) predicts sentence order rather than NSP; ELECTRA (Clark et al., 2020) transfers MLM into a generating and then discriminating process like GAN (Goodfellow et al., 2014). However, these tasks are just devoted to incorporating token-level and sentence-l"
2021.findings-acl.237,D19-5611,0,0.161572,"ina (No. 2017YFB0304100) and Key Projects of National Natural Science Foundation of China (U1836222 and 61733011). This work was supported by Huawei Noah’s Ark Lab. an improved method that exploits the checkpoint of the PrLM into the Transformer models (Vaswani et al., 2017). The existing methods for leveraging PrLM in NLG tasks can be roughly classified into two categories: Reusing the PrLM as a starting point and Integrating the intermediate output of the PrLM. The former, the widely used in various NLP tasks, denotes to initialize the part of Transformer from the PrLM for generation tasks (Clinchant et al., 2019; Edunov et al., 2019; Rothe et al., 2020) or replace the input embedding with the PrLM. The latter is an approach that first extracts the contextualized representation from a LM for an input sentence and fuses it into a neural model (Yang et al., 2020; Zhu et al., 2020; Chen et al., 2020). As our preliminary experiment shows, we expand this approach and explore in many ways towards better performance. In both of the preceding approaches, whether to freeze or fine-tun the parameter of PrLM is also an important issue. For the former (Reusing the PrLM), several works demonstrated that freezing t"
2021.findings-acl.237,P07-2045,0,0.0160817,"Missing"
2021.findings-acl.237,W04-1013,0,0.0221253,"rging the two different flows, and it gains better results than previous works such as gate network (Yang et al., 2020) and dropnet (Zhu et al., 2020). 3 Experiments To demonstrate the effectiveness of the proposed method, we perform extensive experiments on two NMT and abstractive text summarization tasks. For translation, we use BLEU (Papineni et al., 2002) for the evaluation of translation quality, and for text summarization, we report unigram and bigram overlap (ROUGE-1 and ROUGE2) to assess informativeness, and the longest common subsequence (ROUGE-L) to assess fluency with ROUGE scores (Lin, 2004). All the model training is on a single NVIDIA Tesla V100 GPU (16130MiB, Google Colab). 3.1 Datasets and Experimental Setting We evaluate our approach on language generation tasks such as translation and text summarization. For translation tasks, we use two machine translation datasets: AIHub Ko→En 2 (containing 1.6M 2 2687 http://www.aihub.or.kr/ Systems Ko→Ch Ko→En 30.35 (-) 31.33 (+0.9) 31.75 (+1.4) 41.19 (-) 41.97 (+0.7) 41.55 (+0.4) Korean-specific PrLM +KoBERT 31.20 (+0.8) +HanBERT 31.39 (+0.9) +DistilKoBERT 30.94 (+0.5) +ELEC. small 31.51 (+1.1) +ELEC. base 32.17 (+1.8) +KoGPT2 30.38 (+"
2021.findings-acl.237,2021.ccl-1.108,0,0.0469824,"Missing"
2021.findings-acl.237,N19-4009,0,0.045539,"Missing"
2021.findings-acl.237,P02-1040,0,0.112876,"1 , HSn−1 , HSn−1 ), (3) (4) (5) where PrLMAttn is the PrLM-dedicated attention module that takes the previous hidden state HSn−1 as a query and the PrLM representation HP as a key and a value and Attn is the original one. We adopt the summation strategy for merging the two different flows, and it gains better results than previous works such as gate network (Yang et al., 2020) and dropnet (Zhu et al., 2020). 3 Experiments To demonstrate the effectiveness of the proposed method, we perform extensive experiments on two NMT and abstractive text summarization tasks. For translation, we use BLEU (Papineni et al., 2002) for the evaluation of translation quality, and for text summarization, we report unigram and bigram overlap (ROUGE-1 and ROUGE2) to assess informativeness, and the longest common subsequence (ROUGE-L) to assess fluency with ROUGE scores (Lin, 2004). All the model training is on a single NVIDIA Tesla V100 GPU (16130MiB, Google Colab). 3.1 Datasets and Experimental Setting We evaluate our approach on language generation tasks such as translation and text summarization. For translation tasks, we use two machine translation datasets: AIHub Ko→En 2 (containing 1.6M 2 2687 http://www.aihub.or.kr/ S"
2021.findings-acl.237,2020.tacl-1.18,0,0.221613,"National Natural Science Foundation of China (U1836222 and 61733011). This work was supported by Huawei Noah’s Ark Lab. an improved method that exploits the checkpoint of the PrLM into the Transformer models (Vaswani et al., 2017). The existing methods for leveraging PrLM in NLG tasks can be roughly classified into two categories: Reusing the PrLM as a starting point and Integrating the intermediate output of the PrLM. The former, the widely used in various NLP tasks, denotes to initialize the part of Transformer from the PrLM for generation tasks (Clinchant et al., 2019; Edunov et al., 2019; Rothe et al., 2020) or replace the input embedding with the PrLM. The latter is an approach that first extracts the contextualized representation from a LM for an input sentence and fuses it into a neural model (Yang et al., 2020; Zhu et al., 2020; Chen et al., 2020). As our preliminary experiment shows, we expand this approach and explore in many ways towards better performance. In both of the preceding approaches, whether to freeze or fine-tun the parameter of PrLM is also an important issue. For the former (Reusing the PrLM), several works demonstrated that freezing the PrLM at training time led to a signific"
2021.findings-acl.237,P16-1162,0,0.0300174,"Missing"
2021.findings-acl.279,L16-1432,0,0.148281,"rule document, number of rule EDUs n Output: Final decision in Yes/No/Irrelevant or a follow-up question Preprocessing EDU Segmentation. We first separate the rule document into several units each containing exactly one condition. Here we follow D ISCERN (Gao et al., 2020b) adopting the discourse segmenter (Li et al., 2018) to break the rule document into EDUs. Discourse Relation. Unlike EDU segmentation which only concerns with constituency-based logical structures, discourse relation allows relations between the non-adjacent EDUs. There are in total 16 discourse relations according to STAC (Asher et al., 2016), namely, comment, clarificationquestion, elaboration, acknowledgement, continuation, explanation, conditional, question-answer, alternation, question-elaboration, result, background, narration, correction, parallel and contrast. We adopt a pre-trained discourse parser (Shi 1 2 3 4 5 6 7 8 9 10 for i in epochs do build explicit discourse graph G(E, D) Gn×d ← GCN (G) build implicit discourse graph by calculating adjacent matrix Ml , Mc get rule EDU representation Cn×d by Eq.(6) and (7) combined representation for [RULE]r˜i ← self-Attn(C + G)i entailment state fi ← LINEAR(r˜i ) make the decision"
2021.findings-acl.279,C18-1048,1,0.844796,"lling, 2017; Luo and Zhao, 2020), efforts towards better performance on MRC utilizing GCNs have sprung up, such as BAG (Cao et al., 2019), GraphRel (Fu et al., 2019) and social information reasoning (Li and Goldwasser, 2019). Unlike the previous works However, the aforementioned cascaded methods tend to model in a holistic way, i.e. interpreting the rule document with other elements quite plainly, which have the following drawbacks. First, very little attention is paid to the inner dependencies of rule conditions such as the discourse structure and discourse relations (Qin et al., 2016, 2017; Bai and Zhao, 2018). Second, existing methods do not dig deep enough into mining the interactions between the rule document and other elements, especially user scenarios. 3159 who just apply the graph framework mechanically to turn the entire passage or document into a graph, the discourse graph we proposed is delicately designed to mine the relationships of multiple elements in CMR task and to facilitate information flow over the graph. 3 Model As illustrated in Figure 2, our model mainly consists of three parts to generate the final answer. 1. Rule document is segmented into rule EDUs, which is then tagged dis"
2021.findings-acl.279,P18-1026,0,0.0262227,"user initial question, user scenario and dialog history are sent for embedding and graph modeling to make the final decision. If the decision is Inquire, the question generation stage will be activated and use the under-specified span of rule document to generate a follow-up question. we add a directed edge e = (U1 , U2 ) with relation R assigned to Continuation. The corresponding Levi graph can be expressed as G = (VL , EL , RL ) where VL = V ∪ R. EL is the set of edges with format (U1 , Continuation) and (Continuation, U2 ). As for RL , previous works such as (Marcheggiani and Titov, 2017; Beck et al., 2018) designed three types of edges RL = def ault, reverse, self to enhance information flow. Here with our settings, we extend it into six types: default-in, default-out, reverse-in, reverseout, self, global, corresponding to the direction of the edges towards the relation vertices. An example of constructing Levi graph is shown in Figure 3. To construct the discourse structure of other elements, a global vertex representing user scenario is added and connected with all the other vertices. We use a relational graph convolutional network (Schlichtkrull et al., 2018) to implement explicit discourse"
2021.findings-acl.279,N19-1032,0,0.0190441,"elements, user scenarios and dialogue history updates. As to our best knowledge we are the first to explicitly model the relationships among rules and user scenario with Graph Convolutional Networks (GCNs) (Schlichtkrull et al., 2018). Experimental results show that our proposed model outperforms the baseline models in terms of official evaluation metrics and achieves the new state-of-the-art reGraph Modeling in MRC. Inspired by the impressive performance of GCN (Kipf and Welling, 2017; Luo and Zhao, 2020), efforts towards better performance on MRC utilizing GCNs have sprung up, such as BAG (Cao et al., 2019), GraphRel (Fu et al., 2019) and social information reasoning (Li and Goldwasser, 2019). Unlike the previous works However, the aforementioned cascaded methods tend to model in a holistic way, i.e. interpreting the rule document with other elements quite plainly, which have the following drawbacks. First, very little attention is paid to the inner dependencies of rule conditions such as the discourse structure and discourse relations (Qin et al., 2016, 2017; Bai and Zhao, 2018). Second, existing methods do not dig deep enough into mining the interactions between the rule document and other ele"
2021.findings-acl.279,2020.acl-main.130,0,0.0933005,"idi et al. (2018). It contains up to 948 dialog trees clawed from government websites. Those dialog trees are then flattened into 32,436 examples consisting of utterance id, tree id, rule document, initial question, user scenario, dialog history, evidence and the decision. It is worth noting that evidence is the information that we need to extract from user information and thus will not be given in the testing phase. The sizes of train, dev and test are 21,890, 2,270 and 8,276 respectively. We also showed the generalizability of our model on the Multi-Turn Dialogue Reasoning (MuTual) dataset (Cui et al., 2020), which has 8,678 multiple choice samples and is divided into 7,376, 651, 651 of train, dev and test sets respectively. Evaluation. For the decision-making subtask, ShARC evaluates the Micro- and Macro- Acc. for the results of classification. If both the prediction and ground truth of decision is Inquire, 3 Leaderboard can be found at website https:// sharc-data.github.io/leaderboard.html 3163 BLEU(Papineni et al., 2002) score (particularly BLEU1 and BLEU4) will be evaluated on the follow-up question generation subtask. Implementation Details. For rule EDU relation prediction, we keep all the"
2021.findings-acl.279,N19-1423,0,0.0454102,"decision and z has the score for all the four possible states. The corresponding training loss is: Ldecision = − log softmax(z)l , (11) The overall loss for decision making is: L = Ldecision + λLentail . (12) Qustion Generation. If the decision is made to be Inquire, the machine need to ask a follow-up question to further clarify. Question generation in this part is mainly based on the uncovered information in the rule document, and then that information will be rephrased into a question. We predict the position of an under-specified span within a rule document in a supervised way. Following Devlin et al. (2019), our model learns a start vector ws ∈ Rd and end vector we ∈ Rd to indicate the start and end positions of the desired span: span = arg min(wsT tk,i + weT tk,j ), i,j,k where tk,i denote the i-th token in the k-th rule sentence. The ground-truth span labels are generated by calculating the edit-distance between the rule span and the follow-up questions. Intuitively, the shortest rule span with the minimum edit-distance is selected to be the under-specified span. Finally, we concatenate the rule document and the predicted span as an input sequence to finetune UniLM (Dong et al., 2019) and gene"
2021.findings-acl.279,P19-1136,0,0.0544768,"Missing"
2021.findings-acl.279,2020.acl-main.88,0,0.546016,"estion to help with decision making. The corresponding rule document and the question are marked in the same color in the figure. The major challenges for the conversational machine reading include the rule document interpretation, and reasoning with the background 3158 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 3158–3169 August 1–6, 2021. ©2021 Association for Computational Linguistics knowledge, e.g., the provided rule document, user scenario and the input question. Existing works (Zhong and Zettlemoyer, 2019; Lawrence et al., 2019; Verma et al., 2020; Gao et al., 2020a,b) have made progress in improving the reasoning ability by modeling the interactions among rule document, user scenario and the other elements implicitly. As for rule document interpretation, most existing approaches simply split the rule document into several rule conditions to be satisfied. In general, they first track the entailment state of each rule condition for decision making and then form a certain under-specified rule span into a follow-up question. sults on ShARC, the benchmark dataset for CMR (Saeidi et al., 2018). In addition, our model enjoys strong interpretability by modelin"
2021.findings-acl.279,2020.emnlp-main.191,0,0.607467,"estion to help with decision making. The corresponding rule document and the question are marked in the same color in the figure. The major challenges for the conversational machine reading include the rule document interpretation, and reasoning with the background 3158 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 3158–3169 August 1–6, 2021. ©2021 Association for Computational Linguistics knowledge, e.g., the provided rule document, user scenario and the input question. Existing works (Zhong and Zettlemoyer, 2019; Lawrence et al., 2019; Verma et al., 2020; Gao et al., 2020a,b) have made progress in improving the reasoning ability by modeling the interactions among rule document, user scenario and the other elements implicitly. As for rule document interpretation, most existing approaches simply split the rule document into several rule conditions to be satisfied. In general, they first track the entailment state of each rule condition for decision making and then form a certain under-specified rule span into a follow-up question. sults on ShARC, the benchmark dataset for CMR (Saeidi et al., 2018). In addition, our model enjoys strong interpretability by modelin"
2021.findings-acl.279,P17-1147,0,0.10998,"ot my loan last year. It was for 450,000. Initial Question: Does this loan meet my needs? Decision: No Inquire Irrelevant Follow-up Q1: Do you need a direct loan? Follow-up A1: Yes. Decision: Yes No Inquire Irrelevant Follow-up Q2: Is your loan for less than 300,000? Follow-up A2: No. //github.com/ozyyshr/DGM 1 Yes Decision: Introduction Yes No Inquire Irrelevant Follow-up Q3: Is your loan less than 1,392,000? Training machines to understand documents is the major goal of machine reading comprehension (MRC) (Hermann et al., 2015; Hill et al., 2016; Rajpurkar et al., 2016; Nguyen et al., 2016; Joshi et al., 2017; Rajpurkar et al., 2018; Choi et al., 2018; Zhang et al., 2018; Reddy et al., 2019; Zhang et al., 2020c, 2021). Especially, in the recent challenging conversational machine reading (CMR) task, ∗ Equal contribution. †Corresponding author. This paper was partially supported by National Key Research and Development Program of China (No. 2017YFB0304100), Key Projects of National Natural Science Foundation of China (U1836222 and 61733011), Huawei-SJTU long term AI project, Cutting-edge Machine Reading Comprehension and Language Model. This work was supported by Huawei Noah’s Ark Lab. Follow-up A2:"
2021.findings-acl.279,D19-1001,0,0.416827,"re, the machine will ask a clarification question to help with decision making. The corresponding rule document and the question are marked in the same color in the figure. The major challenges for the conversational machine reading include the rule document interpretation, and reasoning with the background 3158 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 3158–3169 August 1–6, 2021. ©2021 Association for Computational Linguistics knowledge, e.g., the provided rule document, user scenario and the input question. Existing works (Zhong and Zettlemoyer, 2019; Lawrence et al., 2019; Verma et al., 2020; Gao et al., 2020a,b) have made progress in improving the reasoning ability by modeling the interactions among rule document, user scenario and the other elements implicitly. As for rule document interpretation, most existing approaches simply split the rule document into several rule conditions to be satisfied. In general, they first track the entailment state of each rule condition for decision making and then form a certain under-specified rule span into a follow-up question. sults on ShARC, the benchmark dataset for CMR (Saeidi et al., 2018). In addition, our model enj"
2021.findings-acl.279,2020.emnlp-main.120,0,0.463685,"dependencies between EDUs and the corresponding relation types with the structured representation of each EDU. 3.2 Encoding Block Embedding. We select the pre-trained language model (PrLM) model ELECTRA (Clark et al., 2020) for encoding. As shown in the figure, the input of our model includes rule document which has already be parsed into EDUs with explicit discourse relation tagging, user initial question, user scenario and the dialog history. Instead of inserting a [CLS] token before each rule EDU to get a sentence-level representation, we use [RULE] which is proved to enhance performance (Lee et al., 2020). Note that we also insert [SEP] between every two adjacent utterances. Explicit Discourse Graph. We first construct the explicit discourse graph as a Levi graph (Levi, 1942) which turns the labeled edges into additional vertices. Suppose G = (V, E, R) is the graph constructed in the following way: if utterance U1 is the continuation of utterance U2 , 1 This discourse parser gives a state-of-the-art performance on STAC so far 3160 Yes No f1 r1 f2 r2 f3 r3 E C U E C U E C U r1 Decoder Block Irrelevant Inquire r2 follow-up ques. E :Entailment r3 C :Contradiction UniLM U :Unmentioned [CLS] Rule ["
2021.findings-acl.279,P19-1247,0,0.0245949,"we are the first to explicitly model the relationships among rules and user scenario with Graph Convolutional Networks (GCNs) (Schlichtkrull et al., 2018). Experimental results show that our proposed model outperforms the baseline models in terms of official evaluation metrics and achieves the new state-of-the-art reGraph Modeling in MRC. Inspired by the impressive performance of GCN (Kipf and Welling, 2017; Luo and Zhao, 2020), efforts towards better performance on MRC utilizing GCNs have sprung up, such as BAG (Cao et al., 2019), GraphRel (Fu et al., 2019) and social information reasoning (Li and Goldwasser, 2019). Unlike the previous works However, the aforementioned cascaded methods tend to model in a holistic way, i.e. interpreting the rule document with other elements quite plainly, which have the following drawbacks. First, very little attention is paid to the inner dependencies of rule conditions such as the discourse structure and discourse relations (Qin et al., 2016, 2017; Bai and Zhao, 2018). Second, existing methods do not dig deep enough into mining the interactions between the rule document and other elements, especially user scenarios. 3159 who just apply the graph framework mechanically"
2021.findings-acl.279,2020.acl-main.571,1,0.837437,"ods, our method makes the first attempt to explicitly capture elaborate interactions among all the document elements, user scenarios and dialogue history updates. As to our best knowledge we are the first to explicitly model the relationships among rules and user scenario with Graph Convolutional Networks (GCNs) (Schlichtkrull et al., 2018). Experimental results show that our proposed model outperforms the baseline models in terms of official evaluation metrics and achieves the new state-of-the-art reGraph Modeling in MRC. Inspired by the impressive performance of GCN (Kipf and Welling, 2017; Luo and Zhao, 2020), efforts towards better performance on MRC utilizing GCNs have sprung up, such as BAG (Cao et al., 2019), GraphRel (Fu et al., 2019) and social information reasoning (Li and Goldwasser, 2019). Unlike the previous works However, the aforementioned cascaded methods tend to model in a holistic way, i.e. interpreting the rule document with other elements quite plainly, which have the following drawbacks. First, very little attention is paid to the inner dependencies of rule conditions such as the discourse structure and discourse relations (Qin et al., 2016, 2017; Bai and Zhao, 2018). Second, exi"
2021.findings-acl.279,D17-1159,0,0.209508,"elations, the inputs including user initial question, user scenario and dialog history are sent for embedding and graph modeling to make the final decision. If the decision is Inquire, the question generation stage will be activated and use the under-specified span of rule document to generate a follow-up question. we add a directed edge e = (U1 , U2 ) with relation R assigned to Continuation. The corresponding Levi graph can be expressed as G = (VL , EL , RL ) where VL = V ∪ R. EL is the set of edges with format (U1 , Continuation) and (Continuation, U2 ). As for RL , previous works such as (Marcheggiani and Titov, 2017; Beck et al., 2018) designed three types of edges RL = def ault, reverse, self to enhance information flow. Here with our settings, we extend it into six types: default-in, default-out, reverse-in, reverseout, self, global, corresponding to the direction of the edges towards the relation vertices. An example of constructing Levi graph is shown in Figure 3. To construct the discourse structure of other elements, a global vertex representing user scenario is added and connected with all the other vertices. We use a relational graph convolutional network (Schlichtkrull et al., 2018) to implement"
2021.findings-acl.279,P02-1040,0,0.109467,". The sizes of train, dev and test are 21,890, 2,270 and 8,276 respectively. We also showed the generalizability of our model on the Multi-Turn Dialogue Reasoning (MuTual) dataset (Cui et al., 2020), which has 8,678 multiple choice samples and is divided into 7,376, 651, 651 of train, dev and test sets respectively. Evaluation. For the decision-making subtask, ShARC evaluates the Micro- and Macro- Acc. for the results of classification. If both the prediction and ground truth of decision is Inquire, 3 Leaderboard can be found at website https:// sharc-data.github.io/leaderboard.html 3163 BLEU(Papineni et al., 2002) score (particularly BLEU1 and BLEU4) will be evaluated on the follow-up question generation subtask. Implementation Details. For rule EDU relation prediction, we keep all the default parameters of the original discourse relation parser4 , with F 1 score achieving 55. In the decision-making stage, we finetune an ELECTRA-based model. The dimension of hidden states is 1024 for both the encoder and decoder. The training process uses Adam (Kingma and Ba, 2015) for 5 epochs with learning rate set to 5e-5. We also use gradient clipping with a maximum gradient norm of 2, and a total batch size of 16."
2021.findings-acl.279,D16-1246,1,0.761285,"ance of GCN (Kipf and Welling, 2017; Luo and Zhao, 2020), efforts towards better performance on MRC utilizing GCNs have sprung up, such as BAG (Cao et al., 2019), GraphRel (Fu et al., 2019) and social information reasoning (Li and Goldwasser, 2019). Unlike the previous works However, the aforementioned cascaded methods tend to model in a holistic way, i.e. interpreting the rule document with other elements quite plainly, which have the following drawbacks. First, very little attention is paid to the inner dependencies of rule conditions such as the discourse structure and discourse relations (Qin et al., 2016, 2017; Bai and Zhao, 2018). Second, existing methods do not dig deep enough into mining the interactions between the rule document and other elements, especially user scenarios. 3159 who just apply the graph framework mechanically to turn the entire passage or document into a graph, the discourse graph we proposed is delicately designed to mine the relationships of multiple elements in CMR task and to facilitate information flow over the graph. 3 Model As illustrated in Figure 2, our model mainly consists of three parts to generate the final answer. 1. Rule document is segmented into rule EDU"
2021.findings-acl.279,P17-1093,1,0.88211,"Missing"
2021.findings-acl.279,P18-2124,0,0.176267,". It was for 450,000. Initial Question: Does this loan meet my needs? Decision: No Inquire Irrelevant Follow-up Q1: Do you need a direct loan? Follow-up A1: Yes. Decision: Yes No Inquire Irrelevant Follow-up Q2: Is your loan for less than 300,000? Follow-up A2: No. //github.com/ozyyshr/DGM 1 Yes Decision: Introduction Yes No Inquire Irrelevant Follow-up Q3: Is your loan less than 1,392,000? Training machines to understand documents is the major goal of machine reading comprehension (MRC) (Hermann et al., 2015; Hill et al., 2016; Rajpurkar et al., 2016; Nguyen et al., 2016; Joshi et al., 2017; Rajpurkar et al., 2018; Choi et al., 2018; Zhang et al., 2018; Reddy et al., 2019; Zhang et al., 2020c, 2021). Especially, in the recent challenging conversational machine reading (CMR) task, ∗ Equal contribution. †Corresponding author. This paper was partially supported by National Key Research and Development Program of China (No. 2017YFB0304100), Key Projects of National Natural Science Foundation of China (U1836222 and 61733011), Huawei-SJTU long term AI project, Cutting-edge Machine Reading Comprehension and Language Model. This work was supported by Huawei Noah’s Ark Lab. Follow-up A2: Yes. Decision: Yes No I"
2021.findings-acl.279,D16-1264,0,0.426472,"d annually for inflation). User Scenario: I got my loan last year. It was for 450,000. Initial Question: Does this loan meet my needs? Decision: No Inquire Irrelevant Follow-up Q1: Do you need a direct loan? Follow-up A1: Yes. Decision: Yes No Inquire Irrelevant Follow-up Q2: Is your loan for less than 300,000? Follow-up A2: No. //github.com/ozyyshr/DGM 1 Yes Decision: Introduction Yes No Inquire Irrelevant Follow-up Q3: Is your loan less than 1,392,000? Training machines to understand documents is the major goal of machine reading comprehension (MRC) (Hermann et al., 2015; Hill et al., 2016; Rajpurkar et al., 2016; Nguyen et al., 2016; Joshi et al., 2017; Rajpurkar et al., 2018; Choi et al., 2018; Zhang et al., 2018; Reddy et al., 2019; Zhang et al., 2020c, 2021). Especially, in the recent challenging conversational machine reading (CMR) task, ∗ Equal contribution. †Corresponding author. This paper was partially supported by National Key Research and Development Program of China (No. 2017YFB0304100), Key Projects of National Natural Science Foundation of China (U1836222 and 61733011), Huawei-SJTU long term AI project, Cutting-edge Machine Reading Comprehension and Language Model. This work was supporte"
2021.findings-acl.279,Q19-1016,0,0.0394022,"needs? Decision: No Inquire Irrelevant Follow-up Q1: Do you need a direct loan? Follow-up A1: Yes. Decision: Yes No Inquire Irrelevant Follow-up Q2: Is your loan for less than 300,000? Follow-up A2: No. //github.com/ozyyshr/DGM 1 Yes Decision: Introduction Yes No Inquire Irrelevant Follow-up Q3: Is your loan less than 1,392,000? Training machines to understand documents is the major goal of machine reading comprehension (MRC) (Hermann et al., 2015; Hill et al., 2016; Rajpurkar et al., 2016; Nguyen et al., 2016; Joshi et al., 2017; Rajpurkar et al., 2018; Choi et al., 2018; Zhang et al., 2018; Reddy et al., 2019; Zhang et al., 2020c, 2021). Especially, in the recent challenging conversational machine reading (CMR) task, ∗ Equal contribution. †Corresponding author. This paper was partially supported by National Key Research and Development Program of China (No. 2017YFB0304100), Key Projects of National Natural Science Foundation of China (U1836222 and 61733011), Huawei-SJTU long term AI project, Cutting-edge Machine Reading Comprehension and Language Model. This work was supported by Huawei Noah’s Ark Lab. Follow-up A2: Yes. Decision: Yes No Inquire Irrelevant Final Answer: Yes. Figure 1: An example d"
2021.findings-acl.279,D18-1233,0,0.168815,"Missing"
2021.findings-acl.279,2020.emnlp-main.589,0,0.298042,"k a clarification question to help with decision making. The corresponding rule document and the question are marked in the same color in the figure. The major challenges for the conversational machine reading include the rule document interpretation, and reasoning with the background 3158 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 3158–3169 August 1–6, 2021. ©2021 Association for Computational Linguistics knowledge, e.g., the provided rule document, user scenario and the input question. Existing works (Zhong and Zettlemoyer, 2019; Lawrence et al., 2019; Verma et al., 2020; Gao et al., 2020a,b) have made progress in improving the reasoning ability by modeling the interactions among rule document, user scenario and the other elements implicitly. As for rule document interpretation, most existing approaches simply split the rule document into several rule conditions to be satisfied. In general, they first track the entailment state of each rule condition for decision making and then form a certain under-specified rule span into a follow-up question. sults on ShARC, the benchmark dataset for CMR (Saeidi et al., 2018). In addition, our model enjoys strong interpret"
2021.findings-acl.279,C18-1317,1,0.814877,"s this loan meet my needs? Decision: No Inquire Irrelevant Follow-up Q1: Do you need a direct loan? Follow-up A1: Yes. Decision: Yes No Inquire Irrelevant Follow-up Q2: Is your loan for less than 300,000? Follow-up A2: No. //github.com/ozyyshr/DGM 1 Yes Decision: Introduction Yes No Inquire Irrelevant Follow-up Q3: Is your loan less than 1,392,000? Training machines to understand documents is the major goal of machine reading comprehension (MRC) (Hermann et al., 2015; Hill et al., 2016; Rajpurkar et al., 2016; Nguyen et al., 2016; Joshi et al., 2017; Rajpurkar et al., 2018; Choi et al., 2018; Zhang et al., 2018; Reddy et al., 2019; Zhang et al., 2020c, 2021). Especially, in the recent challenging conversational machine reading (CMR) task, ∗ Equal contribution. †Corresponding author. This paper was partially supported by National Key Research and Development Program of China (No. 2017YFB0304100), Key Projects of National Natural Science Foundation of China (U1836222 and 61733011), Huawei-SJTU long term AI project, Cutting-edge Machine Reading Comprehension and Language Model. This work was supported by Huawei Noah’s Ark Lab. Follow-up A2: Yes. Decision: Yes No Inquire Irrelevant Final Answer: Yes. Fi"
2021.findings-acl.279,P19-1223,0,0.348168,"Missing"
2021.findings-acl.287,D18-1316,0,0.33648,"., 2020, 2019) are widely adopted as an essential component for various NLP systems. However, as DNN-based models, PrLMs can still be easily fooled by textual adversarial samples (Wallace et al., 2019; Jin et al., 2019; Nie et al., 2020; Zang et al., 2020). Such vulnerability of PrLMs keeps raising potential security concerns, therefore researches on defense techniques to help PrLMs against textual adversarial samples are imperatively needed. Different kinds of textual attack methods have been proposed, ranging from character-level word misspelling (Gao et al., 2018), word-level substitution (Alzantot et al., 2018; Ebrahimi et al., 2018; Ren et al., 2019; Jin et al., 2019; Zang et al., 2020; Li et al., 2020; Garg and Ramakrishnan, 2020), phrase-level insertion and removal (Liang et al., 2018), to sentence-level paraphrasing (Ribeiro et al., 2018; Iyyer et al., 2018). Thanks to the discrete nature of natural language, attack approaches that result in illegal or unnatural sentences can be easily detected and restored by spelling correction and grammar error correction (Islam and Inkpen, 2009; Sakaguchi et al., 2017; Pruthi et al., 2019). However, attack approaches based on adversarial word substitution c"
2021.findings-acl.287,N19-1423,0,0.461328,"t al., 2019; Li et al., 2020; Garg and Ramakrishnan, 2020) adopt heuristic algorithms to locate vulnerable tokens in sentences. To illustrate, for a given sample and a target model, the attacker iteratively masks the tokens and checks the output of the model. The tokens which have significant influence on the final output logits are regarded as vulnerable. Previous works leverage word embeddings such as GloVe (Pennington et al., 2014) and counterfitted vectors (Mrkˇsi´c et al., 2016) to search the suitable synonym set of a given token. Li et al. (2020); Garg and Ramakrishnan (2020) uses BERT (Devlin et al., 2019) to generate perturbation for better semantic consistency and language fluency. 2.2 Defense against AWS For general attack approaches, adversarial training (Goodfellow et al., 2015; Jiang et al., 2020) is widely adopted to mitigate adversarial effect, but (Alzantot et al., 2018; Jin et al., 2019) shows that this method is still vulnerable to AWS. This is because AWS models leverage dynamic algorithms to attack the target model, while adversarial training only involves a static training set. Methods proposed by Jia et al. (2019); Huang et al. (2019) are proved effective for defence against AWS,"
2021.findings-acl.287,C18-1055,0,0.0620844,"ly adopted as an essential component for various NLP systems. However, as DNN-based models, PrLMs can still be easily fooled by textual adversarial samples (Wallace et al., 2019; Jin et al., 2019; Nie et al., 2020; Zang et al., 2020). Such vulnerability of PrLMs keeps raising potential security concerns, therefore researches on defense techniques to help PrLMs against textual adversarial samples are imperatively needed. Different kinds of textual attack methods have been proposed, ranging from character-level word misspelling (Gao et al., 2018), word-level substitution (Alzantot et al., 2018; Ebrahimi et al., 2018; Ren et al., 2019; Jin et al., 2019; Zang et al., 2020; Li et al., 2020; Garg and Ramakrishnan, 2020), phrase-level insertion and removal (Liang et al., 2018), to sentence-level paraphrasing (Ribeiro et al., 2018; Iyyer et al., 2018). Thanks to the discrete nature of natural language, attack approaches that result in illegal or unnatural sentences can be easily detected and restored by spelling correction and grammar error correction (Islam and Inkpen, 2009; Sakaguchi et al., 2017; Pruthi et al., 2019). However, attack approaches based on adversarial word substitution can produce high-quality"
2021.findings-acl.287,2020.emnlp-main.498,0,0.0349097,"Missing"
2021.findings-acl.287,D19-1419,0,0.0370464,"Missing"
2021.findings-acl.287,D09-1129,0,0.0605234,"hods have been proposed, ranging from character-level word misspelling (Gao et al., 2018), word-level substitution (Alzantot et al., 2018; Ebrahimi et al., 2018; Ren et al., 2019; Jin et al., 2019; Zang et al., 2020; Li et al., 2020; Garg and Ramakrishnan, 2020), phrase-level insertion and removal (Liang et al., 2018), to sentence-level paraphrasing (Ribeiro et al., 2018; Iyyer et al., 2018). Thanks to the discrete nature of natural language, attack approaches that result in illegal or unnatural sentences can be easily detected and restored by spelling correction and grammar error correction (Islam and Inkpen, 2009; Sakaguchi et al., 2017; Pruthi et al., 2019). However, attack approaches based on adversarial word substitution can produce high-quality and efficient adversarial samples which are still hard to be detected by existing methods. Thus, the adversarial word substitution keeps posing a larger and more profound challenge for the robustness of PrLMs. Therefore, this paper is devoted to overcome the challenge posed by adversarial word substitution. Several approaches are already proposed to mitigate issues posed by adversarial word substitution (Zhou et al., 2019; Jia et al., 2019; Huang et al., 20"
2021.findings-acl.287,N18-1170,0,0.0223553,"uch vulnerability of PrLMs keeps raising potential security concerns, therefore researches on defense techniques to help PrLMs against textual adversarial samples are imperatively needed. Different kinds of textual attack methods have been proposed, ranging from character-level word misspelling (Gao et al., 2018), word-level substitution (Alzantot et al., 2018; Ebrahimi et al., 2018; Ren et al., 2019; Jin et al., 2019; Zang et al., 2020; Li et al., 2020; Garg and Ramakrishnan, 2020), phrase-level insertion and removal (Liang et al., 2018), to sentence-level paraphrasing (Ribeiro et al., 2018; Iyyer et al., 2018). Thanks to the discrete nature of natural language, attack approaches that result in illegal or unnatural sentences can be easily detected and restored by spelling correction and grammar error correction (Islam and Inkpen, 2009; Sakaguchi et al., 2017; Pruthi et al., 2019). However, attack approaches based on adversarial word substitution can produce high-quality and efficient adversarial samples which are still hard to be detected by existing methods. Thus, the adversarial word substitution keeps posing a larger and more profound challenge for the robustness of PrLMs. Therefore, this paper i"
2021.findings-acl.287,D19-1423,0,0.0575849,"Missing"
2021.findings-acl.287,2020.acl-main.197,0,0.0804764,"Missing"
2021.findings-acl.287,2020.emnlp-main.500,0,0.0729133,"based models, PrLMs can still be easily fooled by textual adversarial samples (Wallace et al., 2019; Jin et al., 2019; Nie et al., 2020; Zang et al., 2020). Such vulnerability of PrLMs keeps raising potential security concerns, therefore researches on defense techniques to help PrLMs against textual adversarial samples are imperatively needed. Different kinds of textual attack methods have been proposed, ranging from character-level word misspelling (Gao et al., 2018), word-level substitution (Alzantot et al., 2018; Ebrahimi et al., 2018; Ren et al., 2019; Jin et al., 2019; Zang et al., 2020; Li et al., 2020; Garg and Ramakrishnan, 2020), phrase-level insertion and removal (Liang et al., 2018), to sentence-level paraphrasing (Ribeiro et al., 2018; Iyyer et al., 2018). Thanks to the discrete nature of natural language, attack approaches that result in illegal or unnatural sentences can be easily detected and restored by spelling correction and grammar error correction (Islam and Inkpen, 2009; Sakaguchi et al., 2017; Pruthi et al., 2019). However, attack approaches based on adversarial word substitution can produce high-quality and efficient adversarial samples which are still hard to be detected b"
2021.findings-acl.287,2021.ccl-1.108,0,0.0821426,"Missing"
2021.findings-acl.287,P11-1015,0,0.0915558,"Missing"
2021.findings-acl.287,2020.emnlp-demos.16,0,0.0353929,"Missing"
2021.findings-acl.287,W17-5301,0,0.0565702,"Missing"
2021.findings-acl.287,2020.acl-main.441,0,0.0329392,"Pretrained language models (PrLMs) (Devlin et al., * Corresponding author. This paper was partially supported by National Key Research and Development Program of China (No. 2017YFB0304100), Key Projects of National Natural Science Foundation of China (U1836222 and 61733011). This work was supported by Huawei Noah’s Ask Lab 2019; Liu et al., 2019; Clark et al., 2020; Zhang et al., 2020, 2019) are widely adopted as an essential component for various NLP systems. However, as DNN-based models, PrLMs can still be easily fooled by textual adversarial samples (Wallace et al., 2019; Jin et al., 2019; Nie et al., 2020; Zang et al., 2020). Such vulnerability of PrLMs keeps raising potential security concerns, therefore researches on defense techniques to help PrLMs against textual adversarial samples are imperatively needed. Different kinds of textual attack methods have been proposed, ranging from character-level word misspelling (Gao et al., 2018), word-level substitution (Alzantot et al., 2018; Ebrahimi et al., 2018; Ren et al., 2019; Jin et al., 2019; Zang et al., 2020; Li et al., 2020; Garg and Ramakrishnan, 2020), phrase-level insertion and removal (Liang et al., 2018), to sentence-level paraphrasing"
2021.findings-acl.287,P05-1015,0,0.444523,"Missing"
2021.findings-acl.287,D14-1162,0,0.0951936,"ermine the vulnerable tokens to be perturbed, and then choose suitable synonyms to replace them. Current AWS models (Alzantot et al., 2018; Ebrahimi et al., 2018; Ren et al., 2019; Jin et al., 2019; Li et al., 2020; Garg and Ramakrishnan, 2020) adopt heuristic algorithms to locate vulnerable tokens in sentences. To illustrate, for a given sample and a target model, the attacker iteratively masks the tokens and checks the output of the model. The tokens which have significant influence on the final output logits are regarded as vulnerable. Previous works leverage word embeddings such as GloVe (Pennington et al., 2014) and counterfitted vectors (Mrkˇsi´c et al., 2016) to search the suitable synonym set of a given token. Li et al. (2020); Garg and Ramakrishnan (2020) uses BERT (Devlin et al., 2019) to generate perturbation for better semantic consistency and language fluency. 2.2 Defense against AWS For general attack approaches, adversarial training (Goodfellow et al., 2015; Jiang et al., 2020) is widely adopted to mitigate adversarial effect, but (Alzantot et al., 2018; Jin et al., 2019) shows that this method is still vulnerable to AWS. This is because AWS models leverage dynamic algorithms to attack the"
2021.findings-acl.287,P19-1561,0,0.0175847,"level word misspelling (Gao et al., 2018), word-level substitution (Alzantot et al., 2018; Ebrahimi et al., 2018; Ren et al., 2019; Jin et al., 2019; Zang et al., 2020; Li et al., 2020; Garg and Ramakrishnan, 2020), phrase-level insertion and removal (Liang et al., 2018), to sentence-level paraphrasing (Ribeiro et al., 2018; Iyyer et al., 2018). Thanks to the discrete nature of natural language, attack approaches that result in illegal or unnatural sentences can be easily detected and restored by spelling correction and grammar error correction (Islam and Inkpen, 2009; Sakaguchi et al., 2017; Pruthi et al., 2019). However, attack approaches based on adversarial word substitution can produce high-quality and efficient adversarial samples which are still hard to be detected by existing methods. Thus, the adversarial word substitution keeps posing a larger and more profound challenge for the robustness of PrLMs. Therefore, this paper is devoted to overcome the challenge posed by adversarial word substitution. Several approaches are already proposed to mitigate issues posed by adversarial word substitution (Zhou et al., 2019; Jia et al., 2019; Huang et al., 2019; Cohen et al., 2019; Ye et al., 2020; Si et"
2021.findings-acl.287,P19-1103,0,0.155008,"ial component for various NLP systems. However, as DNN-based models, PrLMs can still be easily fooled by textual adversarial samples (Wallace et al., 2019; Jin et al., 2019; Nie et al., 2020; Zang et al., 2020). Such vulnerability of PrLMs keeps raising potential security concerns, therefore researches on defense techniques to help PrLMs against textual adversarial samples are imperatively needed. Different kinds of textual attack methods have been proposed, ranging from character-level word misspelling (Gao et al., 2018), word-level substitution (Alzantot et al., 2018; Ebrahimi et al., 2018; Ren et al., 2019; Jin et al., 2019; Zang et al., 2020; Li et al., 2020; Garg and Ramakrishnan, 2020), phrase-level insertion and removal (Liang et al., 2018), to sentence-level paraphrasing (Ribeiro et al., 2018; Iyyer et al., 2018). Thanks to the discrete nature of natural language, attack approaches that result in illegal or unnatural sentences can be easily detected and restored by spelling correction and grammar error correction (Islam and Inkpen, 2009; Sakaguchi et al., 2017; Pruthi et al., 2019). However, attack approaches based on adversarial word substitution can produce high-quality and efficient adv"
2021.findings-acl.287,P18-1079,0,0.0190115,"Zang et al., 2020). Such vulnerability of PrLMs keeps raising potential security concerns, therefore researches on defense techniques to help PrLMs against textual adversarial samples are imperatively needed. Different kinds of textual attack methods have been proposed, ranging from character-level word misspelling (Gao et al., 2018), word-level substitution (Alzantot et al., 2018; Ebrahimi et al., 2018; Ren et al., 2019; Jin et al., 2019; Zang et al., 2020; Li et al., 2020; Garg and Ramakrishnan, 2020), phrase-level insertion and removal (Liang et al., 2018), to sentence-level paraphrasing (Ribeiro et al., 2018; Iyyer et al., 2018). Thanks to the discrete nature of natural language, attack approaches that result in illegal or unnatural sentences can be easily detected and restored by spelling correction and grammar error correction (Islam and Inkpen, 2009; Sakaguchi et al., 2017; Pruthi et al., 2019). However, attack approaches based on adversarial word substitution can produce high-quality and efficient adversarial samples which are still hard to be detected by existing methods. Thus, the adversarial word substitution keeps posing a larger and more profound challenge for the robustness of PrLMs. Th"
2021.findings-acl.287,I17-2062,0,0.0666599,"Missing"
2021.findings-acl.287,D13-1170,0,0.0252781,"Missing"
2021.findings-acl.287,D19-1221,0,0.0169613,"se the model to make false predictions. Pretrained language models (PrLMs) (Devlin et al., * Corresponding author. This paper was partially supported by National Key Research and Development Program of China (No. 2017YFB0304100), Key Projects of National Natural Science Foundation of China (U1836222 and 61733011). This work was supported by Huawei Noah’s Ask Lab 2019; Liu et al., 2019; Clark et al., 2020; Zhang et al., 2020, 2019) are widely adopted as an essential component for various NLP systems. However, as DNN-based models, PrLMs can still be easily fooled by textual adversarial samples (Wallace et al., 2019; Jin et al., 2019; Nie et al., 2020; Zang et al., 2020). Such vulnerability of PrLMs keeps raising potential security concerns, therefore researches on defense techniques to help PrLMs against textual adversarial samples are imperatively needed. Different kinds of textual attack methods have been proposed, ranging from character-level word misspelling (Gao et al., 2018), word-level substitution (Alzantot et al., 2018; Ebrahimi et al., 2018; Ren et al., 2019; Jin et al., 2019; Zang et al., 2020; Li et al., 2020; Garg and Ramakrishnan, 2020), phrase-level insertion and removal (Liang et al., 20"
2021.findings-acl.287,D19-1670,0,0.024626,"two times faster in inference speed and can achieve better accuracy for sentence-level anomaly detection. 3.3 Framework In this section, we elaborate the framework of ADFAR in both training and inference. 3.3.1 Training Figure 2 shows the framework of ADFAR in training. We extend the baseline PrLMs by three major modifications: 1) the construction of training data, 2) the auxiliary anomaly detector and 3) the training objective, which will be introduced in this section. Construction of Training Data As shown in Figure 2, we combine the idea of both adversarial training and data augmentation (Wei and Zou, 2019) to construct our randomization augmented adversarial training data. Firstly, we use a heuristic AWS model (e.g. TextFooler) to generate adversarial samples based on the original training set. Following the common practice of adversarial training, we then combine the adversarial samples with the original ones to form an adversarial training set. Secondly, in order to let PrLMs better cope with randomized samples in inference, we apply the frequency-aware randomization on the adversarial training set to generate a randomized adversarial training set. Lastly, the adversarial training set and the"
2021.findings-acl.287,2020.acl-main.317,0,0.48824,"; Pruthi et al., 2019). However, attack approaches based on adversarial word substitution can produce high-quality and efficient adversarial samples which are still hard to be detected by existing methods. Thus, the adversarial word substitution keeps posing a larger and more profound challenge for the robustness of PrLMs. Therefore, this paper is devoted to overcome the challenge posed by adversarial word substitution. Several approaches are already proposed to mitigate issues posed by adversarial word substitution (Zhou et al., 2019; Jia et al., 2019; Huang et al., 2019; Cohen et al., 2019; Ye et al., 2020; Si et al., 2021). Although these defense methods manage to alleviate the negative impact of adversarial word 3248 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 3248–3258 August 1–6, 2021. ©2021 Association for Computational Linguistics substitution, they sometimes reduce the prediction accuracy for non-adversarial samples to a notable extent. Given the uncertainty of the existence of attack in real application, it is impractical to sacrifice the original prediction accuracy for the purpose of defense. Moreover, previous defense methods either have strong l"
2021.findings-acl.287,2020.acl-main.540,0,0.0334177,"e models (PrLMs) (Devlin et al., * Corresponding author. This paper was partially supported by National Key Research and Development Program of China (No. 2017YFB0304100), Key Projects of National Natural Science Foundation of China (U1836222 and 61733011). This work was supported by Huawei Noah’s Ask Lab 2019; Liu et al., 2019; Clark et al., 2020; Zhang et al., 2020, 2019) are widely adopted as an essential component for various NLP systems. However, as DNN-based models, PrLMs can still be easily fooled by textual adversarial samples (Wallace et al., 2019; Jin et al., 2019; Nie et al., 2020; Zang et al., 2020). Such vulnerability of PrLMs keeps raising potential security concerns, therefore researches on defense techniques to help PrLMs against textual adversarial samples are imperatively needed. Different kinds of textual attack methods have been proposed, ranging from character-level word misspelling (Gao et al., 2018), word-level substitution (Alzantot et al., 2018; Ebrahimi et al., 2018; Ren et al., 2019; Jin et al., 2019; Zang et al., 2020; Li et al., 2020; Garg and Ramakrishnan, 2020), phrase-level insertion and removal (Liang et al., 2018), to sentence-level paraphrasing (Ribeiro et al., 201"
2021.findings-acl.287,D19-1169,0,0.0555817,"Missing"
2021.findings-acl.287,D19-1496,0,0.220493,"and grammar error correction (Islam and Inkpen, 2009; Sakaguchi et al., 2017; Pruthi et al., 2019). However, attack approaches based on adversarial word substitution can produce high-quality and efficient adversarial samples which are still hard to be detected by existing methods. Thus, the adversarial word substitution keeps posing a larger and more profound challenge for the robustness of PrLMs. Therefore, this paper is devoted to overcome the challenge posed by adversarial word substitution. Several approaches are already proposed to mitigate issues posed by adversarial word substitution (Zhou et al., 2019; Jia et al., 2019; Huang et al., 2019; Cohen et al., 2019; Ye et al., 2020; Si et al., 2021). Although these defense methods manage to alleviate the negative impact of adversarial word 3248 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 3248–3258 August 1–6, 2021. ©2021 Association for Computational Linguistics substitution, they sometimes reduce the prediction accuracy for non-adversarial samples to a notable extent. Given the uncertainty of the existence of attack in real application, it is impractical to sacrifice the original prediction accuracy for the"
2021.findings-acl.290,D19-1435,0,0.31275,"left-to-right sequence generation, our model still faces exposure bias because of the iterative correction process, which, through its iterative correction In our model, the GEL module can be considered a discriminator, as it must differentiate whether tokens are erroneous, and by adding a sampling module to the GED module, we can create a generator that outputs grammatical errors (rather than corrections) that are increasingly realistic. We can then pair these sampling outputs with their golden sequence in the training dataset to create new training 3286 GEC system Ens. (Zhao et al., 2019) (Awasthi et al., 2019) (Kiyono et al., 2019) (Kaneko et al., 2020) (Lichtarge et al., 2019) (Zhao et al., 2019) (Awasthi et al., 2019) (Kiyono et al., 2019) (Kantor et al., 2019) (Kaneko et al., 2020) X X X X X X Baseline (BERT-base) +GST Baseline (RoBERTa-base) +GST Baseline (XLNet-base) +GST CoNLL-2014 (test) BEA-2019 (test) JFLEG (test) P R F0.5 P R F0.5 GLEU 67.7 66.1 67.9 69.2 40.6 43.0 44.1 45.6 59.8 59.7 61.3 62.6 − − 65.5 67.1 − − 59.4 60.1 − − 64.2 65.6 − 60.3 59.7 61.3 66.7 71.6 68.3 72.4 − 72.6 43.9 38.7 43.2 46.1 − 46.4 60.4 61.2 61.2 65.0 − 65.2 − − − 74.7 78.3 72.3 − − − 56.7 58.0 61.4 − − − 70.2 73.2"
2021.findings-acl.290,W19-4406,0,0.0136463,"ferent error rates. 2 The Gumbel(0, 1) distribution can be sampled using inverse transform sampling by drawing u ∼ Uniform(0, 1) and computing g = − log(− log(u)). 4 4.1 Experiments Setup To isolate our GAN-like Sequence Labeling Training (GST) approach, we use the same model setting and training details as in (Omelianchuk et al., 2020). The training data includes PIE’s synthetic data (Awasthi et al., 2019), NUCLE (Dahlmeier et al., 2013), Lang-8 (Tajiri et al., 2012), FCE (Yannakoudakis et al., 2011), Cambridge Learner Corpus (the publicly available portion) (Nicholls, 2003), and WI+LOCNESS (Bryant et al., 2019). Our models are evaluated on the test sets of CoNLL2014 (Ng et al., 2014), BEA-2019 (Bryant et al., 2019), and JFLEG (Napoles et al., 2017) with the official M 2 (Dahlmeier and Ng, 2012), ERRANT (Bryant et al., 2017), and GLEU(Napoles et al., 2015) scorers, respectively. 4.2 Results and Analysis Our results on the three test datasets are listed in Table 1. Our baseline model achieves the best single model CoNLL-2014 F0.5 , BEA-2019 F0.5 , and JFLEG GLEU scores, showing that the baseline we use is very strong. The results on the three 3287 Baseline Baseline + Extended Training Intermediate Out"
2021.findings-acl.290,P17-1074,0,0.0251609,"nce Labeling Training (GST) approach, we use the same model setting and training details as in (Omelianchuk et al., 2020). The training data includes PIE’s synthetic data (Awasthi et al., 2019), NUCLE (Dahlmeier et al., 2013), Lang-8 (Tajiri et al., 2012), FCE (Yannakoudakis et al., 2011), Cambridge Learner Corpus (the publicly available portion) (Nicholls, 2003), and WI+LOCNESS (Bryant et al., 2019). Our models are evaluated on the test sets of CoNLL2014 (Ng et al., 2014), BEA-2019 (Bryant et al., 2019), and JFLEG (Napoles et al., 2017) with the official M 2 (Dahlmeier and Ng, 2012), ERRANT (Bryant et al., 2017), and GLEU(Napoles et al., 2015) scorers, respectively. 4.2 Results and Analysis Our results on the three test datasets are listed in Table 1. Our baseline model achieves the best single model CoNLL-2014 F0.5 , BEA-2019 F0.5 , and JFLEG GLEU scores, showing that the baseline we use is very strong. The results on the three 3287 Baseline Baseline + Extended Training Intermediate Outputs GST BERT+GST CoNLL-2014 (test) P R F0.5 72.1 73.5 73.2 72.6 42.0 40.1 39.8 42.5 RoBERTa+GST 63.0 63.0 62.7 63.6 F0.5 score Model Table 3: Comparing GST training with additional baselines. benchmarks are further i"
2021.findings-acl.290,N12-1067,0,0.110038,"tup To isolate our GAN-like Sequence Labeling Training (GST) approach, we use the same model setting and training details as in (Omelianchuk et al., 2020). The training data includes PIE’s synthetic data (Awasthi et al., 2019), NUCLE (Dahlmeier et al., 2013), Lang-8 (Tajiri et al., 2012), FCE (Yannakoudakis et al., 2011), Cambridge Learner Corpus (the publicly available portion) (Nicholls, 2003), and WI+LOCNESS (Bryant et al., 2019). Our models are evaluated on the test sets of CoNLL2014 (Ng et al., 2014), BEA-2019 (Bryant et al., 2019), and JFLEG (Napoles et al., 2017) with the official M 2 (Dahlmeier and Ng, 2012), ERRANT (Bryant et al., 2017), and GLEU(Napoles et al., 2015) scorers, respectively. 4.2 Results and Analysis Our results on the three test datasets are listed in Table 1. Our baseline model achieves the best single model CoNLL-2014 F0.5 , BEA-2019 F0.5 , and JFLEG GLEU scores, showing that the baseline we use is very strong. The results on the three 3287 Baseline Baseline + Extended Training Intermediate Outputs GST BERT+GST CoNLL-2014 (test) P R F0.5 72.1 73.5 73.2 72.6 42.0 40.1 39.8 42.5 RoBERTa+GST 63.0 63.0 62.7 63.6 F0.5 score Model Table 3: Comparing GST training with additional basel"
2021.findings-acl.290,W13-1703,0,0.029661,"= P|C| , (2) i exp((log([P ] ) + g )/τ ) j j j=1 GEL where |C |is the number of classes, τ is the softmax temperature. Altering γ and β allows us to synthesize input samples of different error rates. 2 The Gumbel(0, 1) distribution can be sampled using inverse transform sampling by drawing u ∼ Uniform(0, 1) and computing g = − log(− log(u)). 4 4.1 Experiments Setup To isolate our GAN-like Sequence Labeling Training (GST) approach, we use the same model setting and training details as in (Omelianchuk et al., 2020). The training data includes PIE’s synthetic data (Awasthi et al., 2019), NUCLE (Dahlmeier et al., 2013), Lang-8 (Tajiri et al., 2012), FCE (Yannakoudakis et al., 2011), Cambridge Learner Corpus (the publicly available portion) (Nicholls, 2003), and WI+LOCNESS (Bryant et al., 2019). Our models are evaluated on the test sets of CoNLL2014 (Ng et al., 2014), BEA-2019 (Bryant et al., 2019), and JFLEG (Napoles et al., 2017) with the official M 2 (Dahlmeier and Ng, 2012), ERRANT (Bryant et al., 2017), and GLEU(Napoles et al., 2015) scorers, respectively. 4.2 Results and Analysis Our results on the three test datasets are listed in Table 1. Our baseline model achieves the best single model CoNLL-2014 F"
2021.findings-acl.290,N19-1423,0,0.0225099,"the basic GEC model. 2.1 Background and Notation First, in training, given incorrect input sentence X = x1 , x2 , ..., xn and its corrected version n X log p(ti |x, θ), i=1 where p is the conditional probability that the model outputs at each position i. 2.2 Deep Pre-trained Transformer Encoder As in most neural sequence labeling models (Ma and Hovy, 2016), a neural encoder such as a BiLSTM (Hochreiter and Schmidhuber, 1997) or a Transformer (Vaswani et al., 2017; Li et al., 2021) is used to extract context-aware features from the input sequence. Deep pre-trained language models such as BERT (Devlin et al., 2019; Zhang et al., 2020b), RoBERTa (Liu et al., 2019), and XLNet (Yang et al., 2019) have recently demonstrated the efficacy of Transformer models trained on largescale unlabeled data in various NLP tasks. We leveraged these very beneficial models by using a pre-trained language model as our encoder. We define the contextualized features captured by the neural encoder as: hi = [Enc(X)]i , where Enc represents the encoder, and [·]i represents the output of the i-th position after encoding. 2.3 Grammatical Error Detector and Labeler Next, we adopt a a Grammatical Error Detector (GED) to detect the"
2021.findings-acl.290,2020.acl-main.391,0,0.0475688,"Missing"
2021.findings-acl.290,W19-4414,0,0.0260165,"Missing"
2021.findings-acl.290,D19-1119,0,0.0229203,"Missing"
2021.findings-acl.290,C18-1271,1,0.802068,"s in inference compared to in training, where it handles static data and does not use multiple-round corrections. To address this issue, we borrow the idea of a GAN (Goodfellow et al., 2014) and propose a GAN-like iterative training approach for a sequence labeling GEC model. GANs, whose training objective can be formulated as a minimax game between a generator that creates increasingly realistic fake outputs and a discriminator that must differentiate these outputs from their real counterparts, have been suggested for sequence-tosequence text generation (Li et al., 2020; Zhang et al., 2020a; Li et al., 2018) as they do not suffer from exposure bias. i PGED = softmax(MLPGED (hi )), Require: Genuine GEC parallel dataset D = {(X, Y)} Synthesized GEC parallel dataset DSYN = {} Number of training stages N Number of training epochs M Sentence error probability threshold γ Additional confidence β for label $KEP 1: for i in 1, ..., N do 2: Initialize model parameters from previous training stage θi ← θi-1 when i &gt; 1 3: for j in 1, ..., M do 4: for k in 1, ..., |D ∪ DSYN |do 5: Encode each sentence Xk as Hk k 6: PGED = Softmax(MLPGED (Hk )) k 7: PGEL = Softmax(MLPGEL (Hk )) k 8: lossGED = CrossEntropy(PGE"
2021.findings-acl.290,N19-1333,0,0.0393304,"Missing"
2021.findings-acl.290,2021.ccl-1.108,0,0.0867205,"Missing"
2021.findings-acl.290,P16-1101,0,0.049102,"here are three main components in our basic neural GEC model: a deep pre-trained Transformer Encoder, a Grammatical Error Detector, and a Grammatical Error Labeler. To accommodate our new GAN-like training process, we add a Gumbelsoftmax sampling component to the basic GEC model. 2.1 Background and Notation First, in training, given incorrect input sentence X = x1 , x2 , ..., xn and its corrected version n X log p(ti |x, θ), i=1 where p is the conditional probability that the model outputs at each position i. 2.2 Deep Pre-trained Transformer Encoder As in most neural sequence labeling models (Ma and Hovy, 2016), a neural encoder such as a BiLSTM (Hochreiter and Schmidhuber, 1997) or a Transformer (Vaswani et al., 2017; Li et al., 2021) is used to extract context-aware features from the input sequence. Deep pre-trained language models such as BERT (Devlin et al., 2019; Zhang et al., 2020b), RoBERTa (Liu et al., 2019), and XLNet (Yang et al., 2019) have recently demonstrated the efficacy of Transformer models trained on largescale unlabeled data in various NLP tasks. We leveraged these very beneficial models by using a pre-trained language model as our encoder. We define the contextualized features ca"
2021.findings-acl.290,P12-2039,0,0.0376471,"g )/τ ) j j j=1 GEL where |C |is the number of classes, τ is the softmax temperature. Altering γ and β allows us to synthesize input samples of different error rates. 2 The Gumbel(0, 1) distribution can be sampled using inverse transform sampling by drawing u ∼ Uniform(0, 1) and computing g = − log(− log(u)). 4 4.1 Experiments Setup To isolate our GAN-like Sequence Labeling Training (GST) approach, we use the same model setting and training details as in (Omelianchuk et al., 2020). The training data includes PIE’s synthetic data (Awasthi et al., 2019), NUCLE (Dahlmeier et al., 2013), Lang-8 (Tajiri et al., 2012), FCE (Yannakoudakis et al., 2011), Cambridge Learner Corpus (the publicly available portion) (Nicholls, 2003), and WI+LOCNESS (Bryant et al., 2019). Our models are evaluated on the test sets of CoNLL2014 (Ng et al., 2014), BEA-2019 (Bryant et al., 2019), and JFLEG (Napoles et al., 2017) with the official M 2 (Dahlmeier and Ng, 2012), ERRANT (Bryant et al., 2017), and GLEU(Napoles et al., 2015) scorers, respectively. 4.2 Results and Analysis Our results on the three test datasets are listed in Table 1. Our baseline model achieves the best single model CoNLL-2014 F0.5 , BEA-2019 F0.5 , and JFLE"
2021.findings-acl.290,P15-2097,0,0.0235093,"roach, we use the same model setting and training details as in (Omelianchuk et al., 2020). The training data includes PIE’s synthetic data (Awasthi et al., 2019), NUCLE (Dahlmeier et al., 2013), Lang-8 (Tajiri et al., 2012), FCE (Yannakoudakis et al., 2011), Cambridge Learner Corpus (the publicly available portion) (Nicholls, 2003), and WI+LOCNESS (Bryant et al., 2019). Our models are evaluated on the test sets of CoNLL2014 (Ng et al., 2014), BEA-2019 (Bryant et al., 2019), and JFLEG (Napoles et al., 2017) with the official M 2 (Dahlmeier and Ng, 2012), ERRANT (Bryant et al., 2017), and GLEU(Napoles et al., 2015) scorers, respectively. 4.2 Results and Analysis Our results on the three test datasets are listed in Table 1. Our baseline model achieves the best single model CoNLL-2014 F0.5 , BEA-2019 F0.5 , and JFLEG GLEU scores, showing that the baseline we use is very strong. The results on the three 3287 Baseline Baseline + Extended Training Intermediate Outputs GST BERT+GST CoNLL-2014 (test) P R F0.5 72.1 73.5 73.2 72.6 42.0 40.1 39.8 42.5 RoBERTa+GST 63.0 63.0 62.7 63.6 F0.5 score Model Table 3: Comparing GST training with additional baselines. benchmarks are further improved using the GST approach,"
2021.findings-acl.290,E17-2037,0,0.0131188,"ing g = − log(− log(u)). 4 4.1 Experiments Setup To isolate our GAN-like Sequence Labeling Training (GST) approach, we use the same model setting and training details as in (Omelianchuk et al., 2020). The training data includes PIE’s synthetic data (Awasthi et al., 2019), NUCLE (Dahlmeier et al., 2013), Lang-8 (Tajiri et al., 2012), FCE (Yannakoudakis et al., 2011), Cambridge Learner Corpus (the publicly available portion) (Nicholls, 2003), and WI+LOCNESS (Bryant et al., 2019). Our models are evaluated on the test sets of CoNLL2014 (Ng et al., 2014), BEA-2019 (Bryant et al., 2019), and JFLEG (Napoles et al., 2017) with the official M 2 (Dahlmeier and Ng, 2012), ERRANT (Bryant et al., 2017), and GLEU(Napoles et al., 2015) scorers, respectively. 4.2 Results and Analysis Our results on the three test datasets are listed in Table 1. Our baseline model achieves the best single model CoNLL-2014 F0.5 , BEA-2019 F0.5 , and JFLEG GLEU scores, showing that the baseline we use is very strong. The results on the three 3287 Baseline Baseline + Extended Training Intermediate Outputs GST BERT+GST CoNLL-2014 (test) P R F0.5 72.1 73.5 73.2 72.6 42.0 40.1 39.8 42.5 RoBERTa+GST 63.0 63.0 62.7 63.6 F0.5 score Model Table"
2021.findings-acl.290,W14-1701,0,0.0273346,"se transform sampling by drawing u ∼ Uniform(0, 1) and computing g = − log(− log(u)). 4 4.1 Experiments Setup To isolate our GAN-like Sequence Labeling Training (GST) approach, we use the same model setting and training details as in (Omelianchuk et al., 2020). The training data includes PIE’s synthetic data (Awasthi et al., 2019), NUCLE (Dahlmeier et al., 2013), Lang-8 (Tajiri et al., 2012), FCE (Yannakoudakis et al., 2011), Cambridge Learner Corpus (the publicly available portion) (Nicholls, 2003), and WI+LOCNESS (Bryant et al., 2019). Our models are evaluated on the test sets of CoNLL2014 (Ng et al., 2014), BEA-2019 (Bryant et al., 2019), and JFLEG (Napoles et al., 2017) with the official M 2 (Dahlmeier and Ng, 2012), ERRANT (Bryant et al., 2017), and GLEU(Napoles et al., 2015) scorers, respectively. 4.2 Results and Analysis Our results on the three test datasets are listed in Table 1. Our baseline model achieves the best single model CoNLL-2014 F0.5 , BEA-2019 F0.5 , and JFLEG GLEU scores, showing that the baseline we use is very strong. The results on the three 3287 Baseline Baseline + Extended Training Intermediate Outputs GST BERT+GST CoNLL-2014 (test) P R F0.5 72.1 73.5 73.2 72.6 42.0 40.1"
2021.findings-acl.290,2020.bea-1.16,0,0.586804,"y 3284 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 3284–3290 August 1–6, 2021. ©2021 Association for Computational Linguistics Xc = y1 , y2 , ..., ym , the model predicts a corrective label sequence T = t1 , t2 , ..., tn by minimizing the token-level Levenshtein distance on the span-based alignments of X and Xc . The corrective label set is given as T = {$KEP, $DEL, $APP, $REP} ∪ {$CAS, $MRG, $SPL, $NNUM, $VFORM}, in which the first set consists of the basic text editing transformation operations and the second consists of g-transformations as defined by (Omelianchuk et al., 2020) for GEC1 . Aligning sentences using these transformations in preprocessing, reduces what would be a sequence generation task that handles unequal source-target lengths to a set of label classification problems. In this formulation, the neural sequence labeling model trains to optimize the input sequence’s negative log-likelihood loss for an input sequence:        γ       ∑PGED              β    …   ∑PGEL     hi … … XSYN X Y Figure 1: An overview of our mod"
2021.findings-acl.290,2020.findings-emnlp.275,0,0.0212548,"s a sentence with progressively fewer errors. This of course causes an exposure bias problem, as the training data does not match the test data, and suggests that providing the model with training data with varying error rates will lead to better performance. To combat this exposure bias, we propose a new approach for training a sequence labeling GEC model that draws from GANs (Goodfellow et al., 2014), which consist of a generator that generates increasingly realistic fake inputs and a discriminator that is tasked with differentiating these fake inputs from real inputs. Other GEC works like (Raheja and Alikaniotis, 2020) directly used GANs to produce grammatically correct sentences given grammatically incorrect ones. This contrasts our work, which uses aspects of a GAN to enhance the training process rather than using a GAN itself as the correcting model. Our model consists of three components: an encoder, a Grammatical Error Detector, and a Grammatical Error Labeler. By sampling from the error distribution in the error labeler, our model can synthesize sentences with new errors creating new sentence pairs for further training data. As a result, our Detector continually 3284 Findings of the Association for Co"
2021.findings-acl.290,P11-1019,0,0.0520985,"|C |is the number of classes, τ is the softmax temperature. Altering γ and β allows us to synthesize input samples of different error rates. 2 The Gumbel(0, 1) distribution can be sampled using inverse transform sampling by drawing u ∼ Uniform(0, 1) and computing g = − log(− log(u)). 4 4.1 Experiments Setup To isolate our GAN-like Sequence Labeling Training (GST) approach, we use the same model setting and training details as in (Omelianchuk et al., 2020). The training data includes PIE’s synthetic data (Awasthi et al., 2019), NUCLE (Dahlmeier et al., 2013), Lang-8 (Tajiri et al., 2012), FCE (Yannakoudakis et al., 2011), Cambridge Learner Corpus (the publicly available portion) (Nicholls, 2003), and WI+LOCNESS (Bryant et al., 2019). Our models are evaluated on the test sets of CoNLL2014 (Ng et al., 2014), BEA-2019 (Bryant et al., 2019), and JFLEG (Napoles et al., 2017) with the official M 2 (Dahlmeier and Ng, 2012), ERRANT (Bryant et al., 2017), and GLEU(Napoles et al., 2015) scorers, respectively. 4.2 Results and Analysis Our results on the three test datasets are listed in Table 1. Our baseline model achieves the best single model CoNLL-2014 F0.5 , BEA-2019 F0.5 , and JFLEG GLEU scores, showing that the ba"
2021.findings-acl.290,N19-1014,0,0.0269469,"Missing"
2021.findings-acl.93,2020.acl-main.449,0,0.368694,"Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 1078–1090 August 1–6, 2021. ©2021 Association for Computational Linguistics Structuresensitive LSTM Tree-LSTM Transformer LSTM + SBT Transformer + SBT SiT Long-term dependency Feat-model match X X X X X X X X X Table 2: Comparison of the previous models with proposed SiT model. The last column refers to whether input features match with the corresponding model. sequences due to its poor long-term dependency. For instance, a normal snippet of Java as shown in Table 1 usually has hundreds of tokens. More recently, Ahmad et al. (2020) used an enhanced Transformer-based model to capture long-term and non-sequential information of source code, which outperformed previous RNN-based models by a large margin. On the other hand, in the light of the structural nature of programming languages, structure clues are supposed to greatly enhance programming language processing task like code summarization (Fernandes et al., 2019). Indeed, substantial empirical studies showed that Abstract Syntax Tree may help models better comprehend code snippets and achieve more sensible generation results. Previous approaches could be divided into t"
2021.findings-acl.93,2020.emnlp-main.19,0,0.0703139,"Missing"
2021.findings-acl.93,P19-1285,0,0.0136142,"t al. (2020) introduced type information to assist training, which also gained promising results. Additionally, reinforce learning (Wan et al., 2018) and dual learning (Wei et al., 2019; Ye et al., 2020) are also shown effective to boost model performance. Transformer-based Approaches It is known that RNN-based models may encounter bottleneck when modeling long code sequences. Ahmad et al. (2020) proposed an enhanced Transformer with copy attention and relative position encoding while Gupta (2020); Dowdell and Zhang (2020) proposed to use Transformer (Vaswani et al., 2017) and Transformer-XL (Dai et al., 2019), all of which outperformed previous RNN-based models by a large margin. 1085 Structure-based Approaches Recent works on code summarization pay more and more attention on structural information, which usually treats the source code in form of its Abstract Syntax Tree (AST). Hu et al. (2018a); LeClair et al. (2019); Uri et al. (2019) leveraged flattened ASTs as inputs and trained with LSTMs. Mou et al. (2016); Bui et al. (2021a); Shido et al. (2019); Harer et al. (2019) proposed TBCNN, TreeCaps, Tree-LSTM and Tree-Transformer to directly encode tree-style inputs. Differ from modeling code with"
2021.findings-acl.93,N19-1423,0,0.298404,"ingasan/astruc We try to adjust the weights of three views, showing little performance variant, which suggests that self-attention network itself may balance the relative significance between the three. 2 (b) Window (c) Random (d) Structure-induced Figure 5: Comparison of different types of selfattention pattern. (b) Window attention with w = 2. (c) Random attention with r = 2. Pre-trained Language Model We also compare our model with CodeBERT (Feng et al., 2020), a pre-trained language model on both natural and programming languages. It is pre-trained over six programming languages with MLM (Devlin et al., 2019) and RTD (Clark et al., 2020). 3.3 Baselines (a) Full Training Details We train our model on a single nVidia Titan RTX with batch size in {32, 64}. The learning rate is in {3e-5, 5e-5} with warm-up rate of 0.06 and L2 weight decay of 0.01. The maximum number of epochs is set to 150 for Transformer and 30 for CodeBERT. For validation, we simply use greedy search, while for evaluation, we use beam search with beam size in {4, 5, 8} and choose the best result3 . 3.4 Main Results Scores Table 3 shows the overall results on Java and Python benchmarks. The Transformer baseline is strong enough as it"
2021.findings-acl.93,P16-1195,0,0.314901,"ional Natural Science Foundation of China (U1836222 and 61733011), Huawei-SJTU long term AI project, Cutting-edge Machine Reading Comprehension and Language Model. This work was supported by Huawei Noah’s Ark Lab. In early days, code summarization was a derivative problem of information retrieval (Haiduc et al., 2010; Eddy et al., 2013; Wong et al., 2013, 2015) by matching the most similar code snippets which are labeled with summaries. Such method lacks generalization and performs unsatisfactorily. Thus in recent years, researchers treated code summarization as a task of language generation (Iyer et al., 2016; Liang and Zhu, 2018), which usually depends on RNN-based Seq2Seq models (Cho et al., 2014; Bahdanau et al., 2015). It is already known that RNN-based models may encounter bottleneck when modeling long 1078 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 1078–1090 August 1–6, 2021. ©2021 Association for Computational Linguistics Structuresensitive LSTM Tree-LSTM Transformer LSTM + SBT Transformer + SBT SiT Long-term dependency Feat-model match X X X X X X X X X Table 2: Comparison of the previous models with proposed SiT model. The last column refers to wheth"
2021.findings-acl.93,P16-1078,0,0.112802,"robustness and avoid over-pruning, we introduce structure-induced module, which is a stack of two layers, SAN and Si-SAN. In each module, SAN is followed by Si-SAN and the output is the combination of both layers. Specifically, given input sequence X = (x1 , . . . , xl ), where l denotes sequence length, we first pass it through an SAN layer to obtain hidden representation denoted as H = (h1 , . . . , hl ): H = Concat(SAN1 (X), . . . , SANh (X)) (4) where h refers to number of heads of multi-head attention while SANi refers to self-attention of 1081 Model CODE-NN (Iyer et al., 2016) Tree2Seq (Eriguchi et al., 2016) Hybrid2Seq (Wan et al., 2018) DeepCom (Hu et al., 2018a) API + Code (Hu et al., 2018b) Dual Model (Wei et al., 2019) Transformer (Ahmad et al., 2020) Transformer∗ (Ahmad et al., 2020) SiT CodeBERT∗ † (Feng et al., 2020) SiT on CodeBERT† BLEU 27.60 37.88 38.22 39.75 41.31 42.39 44.58 44.87 45.76(↑1.18) 43.33 45.19(↑0.61) Java ROUGE-L 41.10 51.50 51.91 52.67 52.25 53.61 54.76 54.95 55.58(↑0.82) 54.64 55.87(↑1.11) METEOR 12.61 22.55 22.75 23.06 23.73 25.77 26.43 26.58 27.58(↑1.15) 26.20 27.52(↑1.09) BLEU 17.36 20.07 19.28 20.78 15.36 21.80 32.52 32.85 34.11(↑1.59) 33.47 34.31(↑1.79) Python ROUGE"
2021.findings-acl.93,2021.ccl-1.108,0,0.0217344,"Missing"
2021.findings-acl.93,2020.emnlp-main.339,0,0.0608038,"Missing"
2021.findings-acl.93,P17-1099,0,0.0272805,"er vocabulary than natural language, including vast operators and identifiers. We have to introduce vast out-of-vocabulary (OOV) tokens (usually replaced by hUNKi) (Hu et al., 2018a) to keep it in a regular size. To avoid OVV problem, we apply CamelCase and snake case tokenizers (Ahmad et al., 2020) to reduce code vocabulary and remove all extra nodes which do not correspond to specific tokens. 3.2 We take all three categories of state-of-the-art models as our baselines for comparison. Transformer We refer to the enhanced Transformer in (Ahmad et al., 2020) which equipped with copy attention (See et al., 2017) and relative position encoding (RPE) (Shaw et al., 2018). For fair enough comparison, we run their model on our machine under the same environment with SiT. Note that we also utilize RPE in SiT because of its better capability in capturing long sequences, while we do not utilize copy attention. LSTM This group includes all relevant LSTM models with sequential and non-sequential inputs (Iyer et al., 2016; Eriguchi et al., 2016; Wan et al., 2018; Hu et al., 2018a,b; Wei et al., 2019). 1 https://github.com/gingasan/astruc We try to adjust the weights of three views, showing little performance va"
2021.findings-acl.93,N18-2074,0,0.0226337,"ators and identifiers. We have to introduce vast out-of-vocabulary (OOV) tokens (usually replaced by hUNKi) (Hu et al., 2018a) to keep it in a regular size. To avoid OVV problem, we apply CamelCase and snake case tokenizers (Ahmad et al., 2020) to reduce code vocabulary and remove all extra nodes which do not correspond to specific tokens. 3.2 We take all three categories of state-of-the-art models as our baselines for comparison. Transformer We refer to the enhanced Transformer in (Ahmad et al., 2020) which equipped with copy attention (See et al., 2017) and relative position encoding (RPE) (Shaw et al., 2018). For fair enough comparison, we run their model on our machine under the same environment with SiT. Note that we also utilize RPE in SiT because of its better capability in capturing long sequences, while we do not utilize copy attention. LSTM This group includes all relevant LSTM models with sequential and non-sequential inputs (Iyer et al., 2016; Eriguchi et al., 2016; Wan et al., 2018; Hu et al., 2018a,b; Wei et al., 2019). 1 https://github.com/gingasan/astruc We try to adjust the weights of three views, showing little performance variant, which suggests that self-attention network itself"
2021.findings-acl.93,2020.acl-main.451,0,0.025819,"h is different from CPG. which is original for C/C++ only and we do not find an appropriate analysis platform for other languages. 1 Expression Num 2.2 print(b) Structure-induced Transformer Call print Name b Name Figure 2: A Python code sample of multi-view graph used in Si-SAN. The code snippet is referred from Liu et al. (2020), which is original in Java. Followed by appropriate structure representation and graph construction, we now propose our structure-induced Transformer (SiT) for code summarization, which is a structure-sensitive transformer (Zhang et al., 2020b; Narayan et al., 2020; Xu et al., 2020) model and is able to comprehend code snippets both semantically and syntac1080 ×n Value Transformer Decoder Attention Query Value Code Attention Query + Key Key + Summary Figure 3: Overall architecture of Structure-induced Transformer (SiT). tically. Meanwhile, we do not introduce extra parameters in SiT so that guarantee the training efficiency. In this section, we first review the selfattention network (SAN) of Transformer in terms of attention graph. Then we correspondingly propose structure-induced self-attention to build the structure-induced Transformer. Vanilla Self-Attention Transform"
2021.findings-emnlp.169,2021.ccl-1.108,0,0.0687924,"Missing"
2021.findings-emnlp.169,D15-1075,0,0.180365,"texts. AMBERT (Zhang and Li, 2020) achieves better performance than its precursors in NLU tasks by incorporating both sub-token-level and span-level tokenization in pretraining. The upper mentioned studies all focus on introducing span-level information in pre-training. To the best of our knowledge, the introduction of span-level information in fine-tuning is still a white space to explore, which makes our approach a valuable attempt. 2.3 Integration of fine-grained representation Different formats of downstream tasks require sentence-level representations, such as natural language inference (Bowman et al., 2015; Nangia et al., 2017), semantic textual similarity (Cer et al., 2 Related Work 2017) and sentiment classification (Socher et al., 2.1 Pre-trained language models 2013). Besides directly pre-training the repreLearning reliable and broadly applicable word rep- sentation of coarser granularity (Le and Mikolov, resentations has been an ongoing heated focus for 2014; Logeswaran and Lee, 2018), a lot of methnatural language processing community. Language ods have been explored to obtain a task-specific modeling objectives are proved to be effective for sentence-level representation by integrating f"
2021.findings-emnlp.169,S17-2001,0,0.0218957,"Missing"
2021.findings-emnlp.169,D17-1070,0,0.0158971,"89.5 90.9 92.2 92.2 89.9 90.1 92.4 92.4 86.5 87.6 89.0 89.8 Table 4: Results on test sets of GLUE benchmark with stronger baseline, we average results from three different random seeds. Method Dev Test BERTWWM BERTWWM + SF SemBERTWWM 92.0 92.3 92.2 91.4 91.7 91.9 result of experiments indicate that the performance improvement is primarily a result of our unique segmentation method. 5.2 Table 5: Accuracy on dev and test sets of SNLI. SemBERTWWM (Zhang et al., 2019b) is the published SoTA on SNLI. method BERTBASE BERTBASE + CNN BERTBASE + Random SF§ BERTBASE + NLTK SF¶ BERTBASE + SF Avg Score (Conneau et al., 2017) mentions that the influence of sentence encoder architectures on PrLM performance varies a lot from case to case. (Toshniwal et al., 2020) also suggests that different span representations can affect NLPs tasks greatly. 82.6 82.5 83.0 83.7 84.2 Table 6: Ablation studied on dev sets of GLUE benchmark, we average results from three different random seeds. The results of the experiment BERTBASE + CNN suggest that the improvement is unlikely to come from the extra parameters, since it reduce the overall performance by 0.1 percent. The experiment BERTBASE + Random SF and BERTBASE + NLTK SF indicat"
2021.findings-emnlp.169,D14-1181,0,0.0248311,"Missing"
2021.findings-emnlp.169,W17-5301,0,0.0791325,"ficient way. The code is available to case. The methods of introducing span-level at https://github.com/BAORONGZHOU/spaninformation in pre-training phase, proposed by prefine-tuning. vious works, do not fit into the requirements and cannot improve the performance for all NLU tasks. 1 Introduction For instance, ERNIE models (Sun et al., 2019) perPre-trained language models (PrLM), including form remarkably well in Relation Classification, ELECTRA (Clark et al., 2020), RoBERTa(Liu while underperforms BERT in language inference et al., 2019b), and BERT (Devlin et al., 2018), tasks, such as MNLI (Nangia et al., 2017). Therehave demonstrated strong performance in down- fore, it is imperative to develop a strategy to incorstream tasks (Wang et al., 2018). Leveraging a porate span-level information into PrLMs in a more self-supervised training on large text corpora, these flexible and universally adaptive way. This paper proposes a novel approach, Span Fine-tuning (SF), * Corresponding author. This paper was partially supto leverage span-level information in fine-tuning ported by National Key Research and Development Program of China (No. 2017YFB0304100), Key Projects of phase and therefore formulate a task-"
2021.findings-emnlp.169,N18-1202,0,0.017248,"and broadly applicable word rep- sentation of coarser granularity (Le and Mikolov, resentations has been an ongoing heated focus for 2014; Logeswaran and Lee, 2018), a lot of methnatural language processing community. Language ods have been explored to obtain a task-specific modeling objectives are proved to be effective for sentence-level representation by integrating finedistributed representation generation (Mnih and grained token-level representations(Conneau et al., Hinton, 2009). By generating deep contextualized 2017). Kim (2014) shows that by applying a conword representations, ELMo (Peters et al., 2018) volutional neural network (CNN) on top of preadvances state of the art for several NLU tasks. trained word vectors, we can get a sentence-level Leveraging Transformer (Vaswani et al., 2017), representation that is well adapted to classification 1971 Figure 1: Overview of the framework of our proposed method tasks. Lin et al. (2017) leverage a self-attentive module over hidden states of a BiLSTM to generate sentence-level representations. Zhang et al. (2019b) use a CNN layer to extract word-level representations form sub-word representations and combine them with word-level semantic role repre"
2021.findings-emnlp.169,D13-1170,0,0.0142604,"Missing"
2021.findings-emnlp.169,2020.repl4nlp-1.20,0,0.0301672,"verage results from three different random seeds. Method Dev Test BERTWWM BERTWWM + SF SemBERTWWM 92.0 92.3 92.2 91.4 91.7 91.9 result of experiments indicate that the performance improvement is primarily a result of our unique segmentation method. 5.2 Table 5: Accuracy on dev and test sets of SNLI. SemBERTWWM (Zhang et al., 2019b) is the published SoTA on SNLI. method BERTBASE BERTBASE + CNN BERTBASE + Random SF§ BERTBASE + NLTK SF¶ BERTBASE + SF Avg Score (Conneau et al., 2017) mentions that the influence of sentence encoder architectures on PrLM performance varies a lot from case to case. (Toshniwal et al., 2020) also suggests that different span representations can affect NLPs tasks greatly. 82.6 82.5 83.0 83.7 84.2 Table 6: Ablation studied on dev sets of GLUE benchmark, we average results from three different random seeds. The results of the experiment BERTBASE + CNN suggest that the improvement is unlikely to come from the extra parameters, since it reduce the overall performance by 0.1 percent. The experiment BERTBASE + Random SF and BERTBASE + NLTK SF indicate that the segmentation generated by a pre-train chunker or even random segmentation can also achieve enhancement under the Span Fine-tunin"
2021.findings-emnlp.169,W18-5446,0,0.159763,"raining phase, proposed by prefine-tuning. vious works, do not fit into the requirements and cannot improve the performance for all NLU tasks. 1 Introduction For instance, ERNIE models (Sun et al., 2019) perPre-trained language models (PrLM), including form remarkably well in Relation Classification, ELECTRA (Clark et al., 2020), RoBERTa(Liu while underperforms BERT in language inference et al., 2019b), and BERT (Devlin et al., 2018), tasks, such as MNLI (Nangia et al., 2017). Therehave demonstrated strong performance in down- fore, it is imperative to develop a strategy to incorstream tasks (Wang et al., 2018). Leveraging a porate span-level information into PrLMs in a more self-supervised training on large text corpora, these flexible and universally adaptive way. This paper proposes a novel approach, Span Fine-tuning (SF), * Corresponding author. This paper was partially supto leverage span-level information in fine-tuning ported by National Key Research and Development Program of China (No. 2017YFB0304100), Key Projects of phase and therefore formulate a task-specific stratNational Natural Science Foundation of China (U1836222 egy. Compared to existing works, our approach and 61733011). This wor"
2021.findings-emnlp.169,P19-1441,0,0.143162,"he span-level representations are merged with subtoken-representations into a sentence-level representation. In this way, the sentence-level representation is able to contain and maximize the utilization of both sub-token-level and span-level information. Experiments are conducted on the GLUE benchmark (Wang et al., 2018), which includes many NLU tasks, such as text classification, semantic similarity, and natural language inference. Empirical results demonstrate that Span Fine-tuning is able to further improve the performance of different PrLMs, including BERT (Devlin et al., 2018), RoBERTa (Liu et al., 2019b) and SpanBERT (Joshi et al., 2019). The result of the experiments with SpanBERT indicates that Span Fine-tuning leverages span-level information differently compared to PrLMs pre-trained with span-level information, which shows the distinguishness of our approach. It is also verified in ablation studies and analysis that Span Fine-tuning is essential for further performance improvement for PrLMs. BERT (Devlin et al., 2018) further advances the field of transfer learning. Recent PrLMs are established based on the various extensions of BERT, including using GAN-style architecture (Clark et al."
2021.findings-emnlp.169,P19-1139,0,0.122944,"ion can be strengthened to setting to be adaptively determined by specific downstream tasks during the fine-tuning phase. some extent. For example, base on BERT, SpanIn detail, any sentences processed by the PrLM BERT (Joshi et al., 2019) focuses on masking and will be segmented into multiple spans acpredicting text spans, instead of sub-token-level cording to a pre-sampled dictionary. Then information for pre-training. Entity-level masking the segmentation information will be sent is used as a pre-training strategy by ERNIE modthrough a hierarchical CNN module together els (Sun et al., 2019; Zhang et al., 2019a). The with the representation outputs of the PrLM upper mentioned methods prove that the introducand ultimately generate a span-enhanced reption of span-level information in pre-training to be resentation. Experiments on GLUE benchmark show that the proposed span fine-tuning effective for different NLU tasks. method significantly enhances the PrLM, and However, the requirements for span-level inforat the same time, offer more flexibility in mation of various NLU tasks differs a lot from case an efficient way. The code is available to case. The methods of introducing span-level at https://git"
2021.findings-emnlp.176,K17-1023,0,0.0417705,"Missing"
2021.findings-emnlp.176,2020.acl-main.130,0,0.0105544,"al., 2015; Wu et al., 2017; Zhang et al., 2018). In this paper, we focus on question answering (QA) over dialogue, which tests the capability of a model to understand a dialogue by asking it questions with respect to the dialogue context. QA over dialogue is of more challenge than QA over plain text (Rajpurkar et al., 2016; Reddy et al., 2019; Yang and Choi, 2019) owing to the fact that conversations are full of informal, colloquial expressions and discontinuous semantics. Among this, multi-party dialogue brings even more tremendous challenge compared to two-party dialogue (Sun et al., 2019; Cui et al., 2020) since it involves Findings of the Association for Computational Linguistics: EMNLP 2021, pages 2053–2063 November 7–11, 2021. ©2021 Association for Computational Linguistics flow from U9 to U8 , which means Rachel speaks to Chandler. Q2 follows a similar process, a model should be aware of that U10 is a continuation of U9 and solves the above coreference resolution problem as well. To tackle the aforementioned obstacles, we design a self-supervised speaker prediction task to implicitly model the speaker information flows, and a pseudo-self-supervised key-utterance prediction task to capture s"
2021.findings-emnlp.176,N19-1423,0,0.00808908,"Liu et al. (2021) propose a Mask-based Decoupling-Fusing Network (MDFN) to decouple speaker information from dialogue contexts, by adding inter-speaker and intra-speaker masks to the self-attention blocks of Transformer layers. However, their approach is restricted to two-party dialogue since they have to specify the sender and receiver roles of each ut2 Related work terance. Gu et al. (2020) propose Speaker-Aware 2.1 Pre-trained Language Models BERT (SA-BERT) to capture speaker information Recently, pre-trained language models (PrLMs), by adding speaker embedding at token represenlike BERT (Devlin et al., 2019), RoBERTa (Liu tation stage of the Transformer architecture, then et al., 2019), ALBERT (Lan et al., 2019), XLNet pre-train the model using next sentence prediction (Yang et al., 2019) and ELECTRA (Clark et al., (NSP) and masked language model (MLM) losses. 2020), have reached remarkable achievements in Nonetheless, their speaker embedding lacks of welllearning universal natural language representations designed pre-training task to refine, resulting in by pre-training large language models on mas- inadequate speaker-specific information. Differsive general corpus and fine-tuning them on down-"
2021.findings-emnlp.176,D19-1015,0,0.0633003,"Missing"
2021.findings-emnlp.176,2020.emnlp-main.150,0,0.0123096,"t is chosen as the basic architecture of our SIDB (Section 3.4) and KIDB (Section 3.3) instead of vanilla GAT. 2.2 Multi-party Dialogue Modeling There are several previous works that study multiparty dialogue modeling on different downstream tasks such as response selection and dialogue emotion recognition. Hu et al. (2019) utilize the response to (@) labels and a Graph Neural Network (GNN) to explicitly model the speaker information flows. Wang et al. (2020) design a pre-training task named Topic Prediction to equip PrLMs with the ability of tracking parallel topics in a multiparty dialogue. Jia et al. (2020) make use of an additional labeled dataset to train a dependency parser, then utilize the dependency parser to disentangle parallel threads in multi-party dialogues. Ghosal et al. (2019) propose a window-based heterogeneous Graph Convolutional Network (GCN) to model the emotion flow in multi-party dialogues. 2.3 Speaker Information Incorporation In dialogue MRC, speaker information plays a significant role in comprehending the dialogue context. In the latest studies, Liu et al. (2021) propose a Mask-based Decoupling-Fusing Network (MDFN) to decouple speaker information from dialogue contexts,"
2021.findings-emnlp.176,D17-1082,0,0.0152227,"t al., 2019) and ELECTRA (Clark et al., (NSP) and masked language model (MLM) losses. 2020), have reached remarkable achievements in Nonetheless, their speaker embedding lacks of welllearning universal natural language representations designed pre-training task to refine, resulting in by pre-training large language models on mas- inadequate speaker-specific information. Differsive general corpus and fine-tuning them on down- ent from previous models, our model is suitable stream tasks (Socher et al., 2013; Wang et al., 2018; for the more challenging multi-party dialogue and Wang et al., 2019; Lai et al., 2017). We argue that is equipped with carefully-designed task to better the self-attention mechanism (Vaswani et al., 2017) capture the speaker information. 2054 3 Methodology In this part, we will formulate our task and present our proposed model as shown in Figure 2. There are four main parts in our model, a shared Transformer encoder, a key-utterance information decoupling block, a speaker information decoupling block and a final fusion-prediction layer. In the following sections, we will introduce these modules in detail. 3.1 Task Formulation Let C = {U1 , U2 , ..., UN } be a dialogue context w"
2021.findings-emnlp.176,2020.acl-main.505,0,0.0111928,"ni, we set batch size to 8, learning rate to 1.2e-5 and maximum input sequence length of the Transformer blocks to 384. For FriendsQA, they are 4, 4e-6 and 512 respectively. Note that in FriendsQA, there are dialogue contexts whose length (in tokens) are larger than 512. We split those contexts to pieces and choose the answer with highest span probability pstart ⇤ pend as the final prediction1 . 1 L = LU + LS + LSE (+LA ) Codes and data are available at https://github. (15) com/EricLee8/Multi-party-Dialogue-MRC 2058 4.3 Baseline Models For FriendsQA, we adopt BERT as the baseline model follow Li and Choi (2020) and Liu et al. (2020). For Molweni, we follow Li et al. (2021) who also employ BERT as the baseline model. In addition, we also adpot ELECTRA (Clark et al., 2020) as a strong baseline in both datasets to see if our model still holds on top of stronger PrLMs. 4.4 We see from the the table that our model outperforms strong baselines and the current SOTA model by a large margin, even under the condition that we do not make use of additional discourse annotations. Model BERTpublic basline (Li et al.) BERTour basline BERTDADGraph (Li et al.) BERTour ELECTRAbasline ELECTRAour Results Table 1 shows"
2021.findings-emnlp.176,2020.coling-main.238,0,0.020965,"Missing"
2021.findings-emnlp.176,P16-2022,0,0.0221385,"Here WiQ 2 Rd⇥dq , WiK 2 Rd⇥dk , WiV 2 Rd⇥dv , W O 2 Rhdv ⇥d are matrices with trainable weights, h is the number of attention heads and [; ] denotes the concatenation operation. After stacking L layers of multi-head selfattention: MultiHead([EU ; Eq ; ET ]) to fully exchange information between these nodes, we get a question representation Hq 2 Rd , the utterance representations HU = {Hui 2 Rd }N i=1 , and the token representations HT = {Hti 2 Rd }ni=1 . Hq is then paired with each Hui to conduct the key-utterance prediction task. In detail, we use a heuristic matching mechanism proposed by (Mou et al., 2016) to calculate the matching score of the question representation and utterance representation. Here we define a matching function M atch(X, Y , activ), where X, Y 2 Rd⇤N , as follows: To fully utilize the powerful representational ability of PrLMs, we employ a pack and separate method as Zhang et al. (2021), which is supposed to take advantage of the deep Transformer blocks to make the context and question better interacted with each other. We first pack the context and question as a joint input to feed into the Transformer blocks and separate them according to the position for further interact"
2021.findings-emnlp.176,D16-1264,0,0.0146366,"of China (U1836222 and 61733011). must be obtained by considering the information 2053 Dialogue machine reading comprehension (MRC, Hermann et al., 2015) aims to teach machines to understand dialogue contexts so that solves multiple downstream tasks (Yang and Choi, 2019; Li et al., 2020; Lowe et al., 2015; Wu et al., 2017; Zhang et al., 2018). In this paper, we focus on question answering (QA) over dialogue, which tests the capability of a model to understand a dialogue by asking it questions with respect to the dialogue context. QA over dialogue is of more challenge than QA over plain text (Rajpurkar et al., 2016; Reddy et al., 2019; Yang and Choi, 2019) owing to the fact that conversations are full of informal, colloquial expressions and discontinuous semantics. Among this, multi-party dialogue brings even more tremendous challenge compared to two-party dialogue (Sun et al., 2019; Cui et al., 2020) since it involves Findings of the Association for Computational Linguistics: EMNLP 2021, pages 2053–2063 November 7–11, 2021. ©2021 Association for Computational Linguistics flow from U9 to U8 , which means Rachel speaks to Chandler. Q2 follows a similar process, a model should be aware of that U10 is a co"
2021.findings-emnlp.176,Q19-1016,0,0.0219374,"61733011). must be obtained by considering the information 2053 Dialogue machine reading comprehension (MRC, Hermann et al., 2015) aims to teach machines to understand dialogue contexts so that solves multiple downstream tasks (Yang and Choi, 2019; Li et al., 2020; Lowe et al., 2015; Wu et al., 2017; Zhang et al., 2018). In this paper, we focus on question answering (QA) over dialogue, which tests the capability of a model to understand a dialogue by asking it questions with respect to the dialogue context. QA over dialogue is of more challenge than QA over plain text (Rajpurkar et al., 2016; Reddy et al., 2019; Yang and Choi, 2019) owing to the fact that conversations are full of informal, colloquial expressions and discontinuous semantics. Among this, multi-party dialogue brings even more tremendous challenge compared to two-party dialogue (Sun et al., 2019; Cui et al., 2020) since it involves Findings of the Association for Computational Linguistics: EMNLP 2021, pages 2053–2063 November 7–11, 2021. ©2021 Association for Computational Linguistics flow from U9 to U8 , which means Rachel speaks to Chandler. Q2 follows a similar process, a model should be aware of that U10 is a continuation of U9 and"
2021.findings-emnlp.176,P16-1162,0,0.0118751,"sequence: X = {[CLS]Q[SEP]S1 :U1 [SEP]. . . SN :UN [SEP]}, where [CLS] and [SEP] are two special tokens and each Si :Ui pair is the name and utterance of G = [X; Y ; X Y ; X Y ] 2 R4d⇥N (2) a speaker separated by a colon. This sequence X P = activ(aT G) 2 RN is then fed into Lall L layers of Transformer blocks to gain its contextualized representation Here denotes element-wise multiplication and E 2 RJ⇥d where J is the length of the sequence a 2 R4d is a vector with trainable weights. The after tokenized by Byte-Pair Encoding (BPE) tok- activ is an activation function to get a probabilenizer (Sennrich et al., 2016) and d is the hidden ity distribution according to the downstream loss dimension of the Transformer block. Here Lall is function, which can be chosen from sof tmax and the total number of Transformer layers specified sigmoid. In span-based dialogue MRC datasets, by the type of the PrLM, L is a hyper-parameter we set the pseudo-self-supervised key-utterance which means the number of decoupling layers. target based on the position of the answer span. 2055 Shared Encoding Transformer Blocks speaker-aware token representation attention mask … ESEPQ beam search masked speaker token speaker nodes …"
2021.findings-emnlp.176,D13-1170,0,0.00336096,"en et al., 2019), ALBERT (Lan et al., 2019), XLNet pre-train the model using next sentence prediction (Yang et al., 2019) and ELECTRA (Clark et al., (NSP) and masked language model (MLM) losses. 2020), have reached remarkable achievements in Nonetheless, their speaker embedding lacks of welllearning universal natural language representations designed pre-training task to refine, resulting in by pre-training large language models on mas- inadequate speaker-specific information. Differsive general corpus and fine-tuning them on down- ent from previous models, our model is suitable stream tasks (Socher et al., 2013; Wang et al., 2018; for the more challenging multi-party dialogue and Wang et al., 2019; Lai et al., 2017). We argue that is equipped with carefully-designed task to better the self-attention mechanism (Vaswani et al., 2017) capture the speaker information. 2054 3 Methodology In this part, we will formulate our task and present our proposed model as shown in Figure 2. There are four main parts in our model, a shared Transformer encoder, a key-utterance information decoupling block, a speaker information decoupling block and a final fusion-prediction layer. In the following sections, we will i"
2021.findings-emnlp.176,2020.coling-main.219,0,0.0317592,"Missing"
2021.findings-emnlp.176,Q19-1014,0,0.0245281,"al., 2020; Lowe et al., 2015; Wu et al., 2017; Zhang et al., 2018). In this paper, we focus on question answering (QA) over dialogue, which tests the capability of a model to understand a dialogue by asking it questions with respect to the dialogue context. QA over dialogue is of more challenge than QA over plain text (Rajpurkar et al., 2016; Reddy et al., 2019; Yang and Choi, 2019) owing to the fact that conversations are full of informal, colloquial expressions and discontinuous semantics. Among this, multi-party dialogue brings even more tremendous challenge compared to two-party dialogue (Sun et al., 2019; Cui et al., 2020) since it involves Findings of the Association for Computational Linguistics: EMNLP 2021, pages 2053–2063 November 7–11, 2021. ©2021 Association for Computational Linguistics flow from U9 to U8 , which means Rachel speaks to Chandler. Q2 follows a similar process, a model should be aware of that U10 is a continuation of U9 and solves the above coreference resolution problem as well. To tackle the aforementioned obstacles, we design a self-supervised speaker prediction task to implicitly model the speaker information flows, and a pseudo-self-supervised key-utterance predictio"
2021.findings-emnlp.176,2021.ccl-1.108,0,0.0430808,"Missing"
2021.findings-emnlp.176,W15-4640,0,0.0997057,"el should first notice that it is Rachel who had a dream and locate U9 , then solve the coreference resolution problem that I refers to Rachel and you *Corresponding author. This paper was partially suprefers to Chandler. This coreference knowledge ported by Key Projects of National Natural Science Foundation of China (U1836222 and 61733011). must be obtained by considering the information 2053 Dialogue machine reading comprehension (MRC, Hermann et al., 2015) aims to teach machines to understand dialogue contexts so that solves multiple downstream tasks (Yang and Choi, 2019; Li et al., 2020; Lowe et al., 2015; Wu et al., 2017; Zhang et al., 2018). In this paper, we focus on question answering (QA) over dialogue, which tests the capability of a model to understand a dialogue by asking it questions with respect to the dialogue context. QA over dialogue is of more challenge than QA over plain text (Rajpurkar et al., 2016; Reddy et al., 2019; Yang and Choi, 2019) owing to the fact that conversations are full of informal, colloquial expressions and discontinuous semantics. Among this, multi-party dialogue brings even more tremendous challenge compared to two-party dialogue (Sun et al., 2019; Cui et al."
2021.findings-emnlp.176,W18-5446,0,0.0110643,"ERT (Lan et al., 2019), XLNet pre-train the model using next sentence prediction (Yang et al., 2019) and ELECTRA (Clark et al., (NSP) and masked language model (MLM) losses. 2020), have reached remarkable achievements in Nonetheless, their speaker embedding lacks of welllearning universal natural language representations designed pre-training task to refine, resulting in by pre-training large language models on mas- inadequate speaker-specific information. Differsive general corpus and fine-tuning them on down- ent from previous models, our model is suitable stream tasks (Socher et al., 2013; Wang et al., 2018; for the more challenging multi-party dialogue and Wang et al., 2019; Lai et al., 2017). We argue that is equipped with carefully-designed task to better the self-attention mechanism (Vaswani et al., 2017) capture the speaker information. 2054 3 Methodology In this part, we will formulate our task and present our proposed model as shown in Figure 2. There are four main parts in our model, a shared Transformer encoder, a key-utterance information decoupling block, a speaker information decoupling block and a final fusion-prediction layer. In the following sections, we will introduce these modu"
2021.findings-emnlp.176,2020.emnlp-main.533,0,0.0743258,"Missing"
2021.findings-emnlp.176,2020.emnlp-demos.6,0,0.0122978,"ments. FriendsQA excerpts 1,222 scenes and 10,610 opendomain questions from the first four seasons of a well-known American TV show Friends to tackle dialogue MRC on everyday conversations. Each dialogue is longer in length and involves more speakers, resulting in more complicated speaker information flows compared to Molweni. For each dialogue context, at least 4 out of 6 types (5W1H) of questions, are generated. This dataset features in its colloquial language style filled with sarcasms, metaphors, humors, etc. 4.2 Implementation Details We implement our model based on Transformers Library (Wolf et al., 2020). The number of information decoupling layers L is chosen from 3 - 5 according to the type of the PrLM in our experiments. For Molweni, we set batch size to 8, learning rate to 1.2e-5 and maximum input sequence length of the Transformer blocks to 384. For FriendsQA, they are 4, 4e-6 and 512 respectively. Note that in FriendsQA, there are dialogue contexts whose length (in tokens) are larger than 512. We split those contexts to pieces and choose the answer with highest span probability pstart ⇤ pend as the final prediction1 . 1 L = LU + LS + LSE (+LA ) Codes and data are available at https://gi"
2021.findings-emnlp.176,P17-1046,0,0.0257368,"ice that it is Rachel who had a dream and locate U9 , then solve the coreference resolution problem that I refers to Rachel and you *Corresponding author. This paper was partially suprefers to Chandler. This coreference knowledge ported by Key Projects of National Natural Science Foundation of China (U1836222 and 61733011). must be obtained by considering the information 2053 Dialogue machine reading comprehension (MRC, Hermann et al., 2015) aims to teach machines to understand dialogue contexts so that solves multiple downstream tasks (Yang and Choi, 2019; Li et al., 2020; Lowe et al., 2015; Wu et al., 2017; Zhang et al., 2018). In this paper, we focus on question answering (QA) over dialogue, which tests the capability of a model to understand a dialogue by asking it questions with respect to the dialogue context. QA over dialogue is of more challenge than QA over plain text (Rajpurkar et al., 2016; Reddy et al., 2019; Yang and Choi, 2019) owing to the fact that conversations are full of informal, colloquial expressions and discontinuous semantics. Among this, multi-party dialogue brings even more tremendous challenge compared to two-party dialogue (Sun et al., 2019; Cui et al., 2020) since it"
2021.findings-emnlp.176,W19-5923,0,0.349887,"nding questions from FriendsQA, whose answers are marked with wavy lines. Left part: The speaker information flows of this dialogue. Introduction multiple speakers at one dialogue, resulting in complicated discourse structure (Li et al., 2020) and intricate speaker information flows. Besides this, Zhang et al. (2021) also pointed that for long dialogue contexts, not all utterances contribute to the final answer prediction since a lot of them are noisy and carry no useful information. To illustrate the challenge of multi-party dialogue MRC, we extract a dialogue example from FriendsQA dataset (Yang and Choi, 2019) which is shown in Figure 1. This single dialogue involves four different speakers with intricate speaker information flows. The arrows here represent the direction of information flows, from senders to receivers. Let us consider the reasoning process of Q1 : a model should first notice that it is Rachel who had a dream and locate U9 , then solve the coreference resolution problem that I refers to Rachel and you *Corresponding author. This paper was partially suprefers to Chandler. This coreference knowledge ported by Key Projects of National Natural Science Foundation of China (U1836222 and 6"
2021.findings-emnlp.176,2020.acl-main.444,0,0.0175601,"ethod using pretrain-fine-tune form. They first pre-train BERT on FriendsQA and additional transcripts from Seasons 5-10 of Friends using well designed pre-training tasks Utterance-level-MaskedLM (ULM) and Utterance-Order-Prediction (UOP), then fine-tune it on dialogue MRC task. BERTgraph (Liu et al., 2020) is a graph-based model that integrates relation knowledge and coreference knowledge using Relational Graph Convolution Networks (R-GCNs) (Schlichtkrull et al., 2018). Note that this model utilizes additional labeled data on coreference resolution (Chen et al., 2017) and character relation (Yu et al., 2020). We adopt the same evaluation metrics as Li et al. (2020): exactly match (EM) and F1 score. Our model reaches new stateof-the-art (SOTA) result on EM metric and comparable result on F1 metric, even without any additional labeled data. Besides, our model still gains great performance improvement under ELECTRAbased condition, which demonstrates the effectiveness of our model over strong PrLMs. Model BERTbasline BERTULM+UOP (Li and Choi) BERTgraph (Liu et al.) BERTour ELECTRAbasline ELECTRAour EM 43.3 46.8 46.4 46.9 52.8 55.8 Table 1: Results on FriendsQA F1 59.3 63.1 64.3 63.9 70.1 72.3 EM 45.3"
2021.findings-emnlp.176,C18-1317,1,0.835149,"chel who had a dream and locate U9 , then solve the coreference resolution problem that I refers to Rachel and you *Corresponding author. This paper was partially suprefers to Chandler. This coreference knowledge ported by Key Projects of National Natural Science Foundation of China (U1836222 and 61733011). must be obtained by considering the information 2053 Dialogue machine reading comprehension (MRC, Hermann et al., 2015) aims to teach machines to understand dialogue contexts so that solves multiple downstream tasks (Yang and Choi, 2019; Li et al., 2020; Lowe et al., 2015; Wu et al., 2017; Zhang et al., 2018). In this paper, we focus on question answering (QA) over dialogue, which tests the capability of a model to understand a dialogue by asking it questions with respect to the dialogue context. QA over dialogue is of more challenge than QA over plain text (Rajpurkar et al., 2016; Reddy et al., 2019; Yang and Choi, 2019) owing to the fact that conversations are full of informal, colloquial expressions and discontinuous semantics. Among this, multi-party dialogue brings even more tremendous challenge compared to two-party dialogue (Sun et al., 2019; Cui et al., 2020) since it involves Findings of"
2021.naacl-industry.12,N19-1121,0,0.0183821,"system has First, we test the baseline supervised system, that is, only En → F r and F r → En are conducted on the model. Due to difference in model architecture, the performance Robustness on Parallel Data Scale As shown in Table 4, C UNMT is robust to the parallel data 93 System Supervised Training C UNMT + Forward C UNMT + Backward En-Fr 39.70 39.26 39.12 Fr-En 36.62 36.82 36.20 Wang et al., 2018; Gu et al., 2018; Edunov et al., 2018; Yang et al., 2020). Others also try to utilize parallel data of rich resource language pairs and also monolingual data (Ren et al., 2018; Wang et al., 2019; Al-Shedivat and Parikh, 2019; Lin et al., 2020). (Ren et al., 2018) also proposed a triangular architecture, but their work still relied on parallel data of low resource language pairs. With the joint support of parallel and monolingual data, the performance of a low resource system can be improved. Table 4: Translation performance on supervised directions of C UNMT. of C UNMT is slightly lower than that of its state of the art counterparts. Also, some techniques such as model average are not applied, and two directions are trained in one model. In C UNMT, the performance of supervised directions drops a little, but in e"
2021.naacl-industry.12,N18-1032,0,0.021685,"e important than the auxiliary data scale. Benefits as All in One Model In table 4, the performance of supervised directions are shown to illustrate the effects on which jointly training a single system has First, we test the baseline supervised system, that is, only En → F r and F r → En are conducted on the model. Due to difference in model architecture, the performance Robustness on Parallel Data Scale As shown in Table 4, C UNMT is robust to the parallel data 93 System Supervised Training C UNMT + Forward C UNMT + Backward En-Fr 39.70 39.26 39.12 Fr-En 36.62 36.82 36.20 Wang et al., 2018; Gu et al., 2018; Edunov et al., 2018; Yang et al., 2020). Others also try to utilize parallel data of rich resource language pairs and also monolingual data (Ren et al., 2018; Wang et al., 2019; Al-Shedivat and Parikh, 2019; Lin et al., 2020). (Ren et al., 2018) also proposed a triangular architecture, but their work still relied on parallel data of low resource language pairs. With the joint support of parallel and monolingual data, the performance of a low resource system can be improved. Table 4: Translation performance on supervised directions of C UNMT. of C UNMT is slightly lower than that of its state"
2021.naacl-industry.12,P17-1042,0,0.0117438,"n supervised directions of C UNMT. of C UNMT is slightly lower than that of its state of the art counterparts. Also, some techniques such as model average are not applied, and two directions are trained in one model. In C UNMT, the performance of supervised directions drops a little, but in exchange, the performances of zero-shot directions are greatly improved and the model is convenient to serve for multiple translation directions. Unsupervised NMT In 2017, pure unsupervised machine translation method with only monolingual data was proven to be feasible. On the basis of embedding alignment (Artetxe et al., 2017; Lample et al., 2018b), (Lample et al., 2018a) and (Artetxe et al., 2018b) devised similar methods for fully unsupervised machine translation. Considerable work has been done to improve the unsupervised machine translation systems by methods such as statistical machine translation (Lample et al., 2018c; Artetxe et al., 2018a; Ren et al., 2019; Artetxe et al., 2019), pretraining models (Lample and Conneau, 2019; Song et al., 2019), or others (Wu et al., 2019), and all of which greatly improve the performance of unsupervised machine translation. Our work attempts to utilize both monolingual and"
2021.naacl-industry.12,D18-1399,0,0.0746721,"ural machine translation (NMT) has achieved great success and reached satisfactory translation performance for several language pairs (Bahdanau et al., 2015; Gehring et al., 2017; Vaswani et al., 2017). Such breakthroughs heavily depend on the availability of colossal amounts of bilingual sentence pairs, such as the some 40 million parallel sentence pairs used in the training of WMT14 English French Task. As bilingual sentence pairs are costly to collect, the success of NMT has not been fully duplicated in the vast majority of language pairs, especially for zero-resource languages. Recently, (Artetxe et al., 2018b; Lample et al., 2018a; ?) tackled this challenge by training unsupervised neural machine translation (UNMT) models using only monolingual data, which achieves considerably high accuracy, but still not on par with that of the state of the art supervised models. 89 Proceedings of NAACL HLT 2021: IndustryTrack Papers, pages 89–96 June 6–11, 2021. ©2021 Association for Computational Linguistics tion (Figure 1(d)). We introduce cross-lingual supervision which aims at modeling explicit translation probabilities across languages. Taking three languages as an example, suppose the target unsupervised"
2021.naacl-industry.12,P19-1019,0,0.0178482,"the model is convenient to serve for multiple translation directions. Unsupervised NMT In 2017, pure unsupervised machine translation method with only monolingual data was proven to be feasible. On the basis of embedding alignment (Artetxe et al., 2017; Lample et al., 2018b), (Lample et al., 2018a) and (Artetxe et al., 2018b) devised similar methods for fully unsupervised machine translation. Considerable work has been done to improve the unsupervised machine translation systems by methods such as statistical machine translation (Lample et al., 2018c; Artetxe et al., 2018a; Ren et al., 2019; Artetxe et al., 2019), pretraining models (Lample and Conneau, 2019; Song et al., 2019), or others (Wu et al., 2019), and all of which greatly improve the performance of unsupervised machine translation. Our work attempts to utilize both monolingual and parallel data, and combine unsupervised and supervised machine translation through multilingual translation method into a single model C UNMT to ensure better performance for unsupervised language pairs. Strategies of Synthetic Data Generation For the synthetic data generation, the reported results are from greedy decoding for time efficiency. We compared the effec"
2021.naacl-industry.12,J82-2005,0,0.441809,"Missing"
2021.naacl-industry.12,P17-1176,0,0.0217729,", the performance of beam search is slightly inferior. A possible reason is that the beam search makes the synthetic data further biased on the learned pattern. The results suggest that C UNMT is exceedingly robust to the sampling strategies when performing forward and backward cross translation. 5 Related Work 6 Multilingual NMT It has been proven low resource machine translation can adopt methods to utilize other rich resource data in order to develop a better system. These methods include multilingual translation system (Firat et al., 2016; Johnson et al., 2017), teacher-student framework (Chen et al., 2017), or others (Zheng et al., 2017). Apart from parallel data as an entry point, many attempts have been made to explore the usefulness of monolingual data, including semi-supervised methods and unsupervised methods which only monolingual data is used. Much work also has been done to attempt to marry monolingual data with supervised data to create a better system, some of which include using small amounts of parallel data and augment the system with monolingual data (Sennrich et al., 2016; He et al., 2016; Conclusion In this work, we propose a multilingual machine translation framework C UNMT inc"
2021.naacl-industry.12,D18-1045,0,0.02103,"the auxiliary data scale. Benefits as All in One Model In table 4, the performance of supervised directions are shown to illustrate the effects on which jointly training a single system has First, we test the baseline supervised system, that is, only En → F r and F r → En are conducted on the model. Due to difference in model architecture, the performance Robustness on Parallel Data Scale As shown in Table 4, C UNMT is robust to the parallel data 93 System Supervised Training C UNMT + Forward C UNMT + Backward En-Fr 39.70 39.26 39.12 Fr-En 36.62 36.82 36.20 Wang et al., 2018; Gu et al., 2018; Edunov et al., 2018; Yang et al., 2020). Others also try to utilize parallel data of rich resource language pairs and also monolingual data (Ren et al., 2018; Wang et al., 2019; Al-Shedivat and Parikh, 2019; Lin et al., 2020). (Ren et al., 2018) also proposed a triangular architecture, but their work still relied on parallel data of low resource language pairs. With the joint support of parallel and monolingual data, the performance of a low resource system can be improved. Table 4: Translation performance on supervised directions of C UNMT. of C UNMT is slightly lower than that of its state of the art counterpa"
2021.naacl-industry.12,2020.emnlp-main.210,1,0.72213,"baseline supervised system, that is, only En → F r and F r → En are conducted on the model. Due to difference in model architecture, the performance Robustness on Parallel Data Scale As shown in Table 4, C UNMT is robust to the parallel data 93 System Supervised Training C UNMT + Forward C UNMT + Backward En-Fr 39.70 39.26 39.12 Fr-En 36.62 36.82 36.20 Wang et al., 2018; Gu et al., 2018; Edunov et al., 2018; Yang et al., 2020). Others also try to utilize parallel data of rich resource language pairs and also monolingual data (Ren et al., 2018; Wang et al., 2019; Al-Shedivat and Parikh, 2019; Lin et al., 2020). (Ren et al., 2018) also proposed a triangular architecture, but their work still relied on parallel data of low resource language pairs. With the joint support of parallel and monolingual data, the performance of a low resource system can be improved. Table 4: Translation performance on supervised directions of C UNMT. of C UNMT is slightly lower than that of its state of the art counterparts. Also, some techniques such as model average are not applied, and two directions are trained in one model. In C UNMT, the performance of supervised directions drops a little, but in exchange, the perfor"
2021.naacl-industry.12,D16-1026,0,0.0305797,"Missing"
2021.naacl-industry.12,2020.tacl-1.47,0,0.0624579,"man, 20 million sentences from available WMT monolingual News Crawl datasets were randomly selected. For Romanian monolingual data, all of the available Romanian sentences from News Crawl dataset were used and and were supplemented with WMT16 monolingual data to yield a total of in 2.9 million sentences. For parallel data, we use the standard WMT 2014 English-French dataset consisting of about 36M sentence pairs, and the 91 Supervised Transformer Comparison systems of UNMT UNMT (Lample et al., 2018c) EMB (Lample and Conneau, 2019) MLM (Lample and Conneau, 2019) MASS (Song et al., 2019) MBART (Liu et al., 2020) C UNMT C UNMT w/o Para. C UNMT w/ Para. C UNMT + Forward C UNMT + Backward + Forward (Fr, En, De) En-Fr Fr-En 41.0 - (De, En, Fr) En-De De-En 34.0 38.6 (Ro, En, Fr) En-Ro Ro-En 34.3 34.0 25.1 29.4 33.4 37.5 - 24.2 29.4 33.3 34.9 - 17.2 21.3 26.4 28.3 29.8 21.0 27.3 34.3 35.2 34.0 21.2 27.5 33.3 35.2 35.0 19.4 26.6 31.8 33.1 30.5 32.90 34.37 35.88 37.60 31.93 32.77 33.64 35.18 23.03 23.99 26.50 27.60 31.01 31.98 33.11 34.10 33.23 33.95 34.12 35.09 32.34 33.15 33.61 33.95 Table 1: Main results comparisons. MASS uses large scale pre-training and back translation during fine-tuning. MBART employ"
2021.naacl-industry.12,P16-1009,0,0.0259438,"s include multilingual translation system (Firat et al., 2016; Johnson et al., 2017), teacher-student framework (Chen et al., 2017), or others (Zheng et al., 2017). Apart from parallel data as an entry point, many attempts have been made to explore the usefulness of monolingual data, including semi-supervised methods and unsupervised methods which only monolingual data is used. Much work also has been done to attempt to marry monolingual data with supervised data to create a better system, some of which include using small amounts of parallel data and augment the system with monolingual data (Sennrich et al., 2016; He et al., 2016; Conclusion In this work, we propose a multilingual machine translation framework C UNMT incorporating distant supervision to tackle the challenge of the unsupervised translation task. By mixing different training schemes into one model and utilizing unrelated bilingual corpus, we greatly improve the performance of the unsupervised NMT direction. By joint training, C UNMT can serve all translation directions in one model. Empirically, C UNMT has been proven to deliver substantial improvements over several strong UNMT competitors and even achieve comparable performance to supe"
2021.naacl-industry.12,N19-1120,0,0.0358227,"Missing"
2021.wat-1.4,D17-1098,0,0.0211247,"(Chu and Wang, 2018), which is also crucial for the practical application of NMT. Since there is still a need for manual interventions for the new NMT paradigm, much effort is spent in studying how to incorporate this explicit control into the end-to-end neural translation (Arthur et al., 2016). Among these efforts, Constrained Decoding (CD) has gained a lot of attention in this research field, which is a modification to commonly adopted beam search in ordinary NMT models. Hokamp and Liu (2017) proposed grid beam search, which expands beam search to include pre-specified lexical constraints. Anderson et al. (2017) used constrained beam search to force the inclusion of restricted words in the output, and employed fixed pre-trained word embeddings to facilitate vocabulary expansion to unseen words in training. While these works accomplish the goal of explicit translation control, the time complexity of their decoding algorithm and resultant decoding speed falls short of the expectations. The complexity of grid beam search and constrained beam search is linear and exponential to the number of constraints, respectively. These algorithms are thus too inefficient to be practical for large-scale use. To allev"
2021.wat-1.4,N19-1090,0,0.0512166,"Missing"
2021.wat-1.4,D16-1162,0,0.0114004,"of Information and Communications Technology (NICT), Kyoto, Japan 1 charlee@sjtu.edu.cn, {mutiyama, eiichiro.sumita}, zhaohai@cs.sjtu.edu.cn Abstract control over translation output, which is effective in a variety of translation settings, including interactive machine translation (Peris et al., 2017) and domain adaptation (Chu and Wang, 2018), which is also crucial for the practical application of NMT. Since there is still a need for manual interventions for the new NMT paradigm, much effort is spent in studying how to incorporate this explicit control into the end-to-end neural translation (Arthur et al., 2016). Among these efforts, Constrained Decoding (CD) has gained a lot of attention in this research field, which is a modification to commonly adopted beam search in ordinary NMT models. Hokamp and Liu (2017) proposed grid beam search, which expands beam search to include pre-specified lexical constraints. Anderson et al. (2017) used constrained beam search to force the inclusion of restricted words in the output, and employed fixed pre-trained word embeddings to facilitate vocabulary expansion to unseen words in training. While these works accomplish the goal of explicit translation control, the"
2021.wat-1.4,C18-1111,0,0.0241363,"ong University 2 Key Laboratory of Shanghai Education Commission for Intelligent Interaction and Cognitive Engineering, Shanghai Jiao Tong University, Shanghai, China 3 MoE Key Lab of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University 4 National Institute of Information and Communications Technology (NICT), Kyoto, Japan 1 charlee@sjtu.edu.cn, {mutiyama, eiichiro.sumita}, zhaohai@cs.sjtu.edu.cn Abstract control over translation output, which is effective in a variety of translation settings, including interactive machine translation (Peris et al., 2017) and domain adaptation (Chu and Wang, 2018), which is also crucial for the practical application of NMT. Since there is still a need for manual interventions for the new NMT paradigm, much effort is spent in studying how to incorporate this explicit control into the end-to-end neural translation (Arthur et al., 2016). Among these efforts, Constrained Decoding (CD) has gained a lot of attention in this research field, which is a modification to commonly adopted beam search in ordinary NMT models. Hokamp and Liu (2017) proposed grid beam search, which expands beam search to include pre-specified lexical constraints. Anderson et al. (2017"
2021.wat-1.4,J10-4005,0,0.0298736,"ese sampled positions to achieve the goal of restrict translation with soft constraints on the model: Convk (H) = DepthConvk (HW V )W out DepthConvk (H) = k X Softmax( d X Q Wj,c Hi,c ) c=1 j=1  · Hi+j−d k+1 e,c , 2 Conv(H) = K X i=1 LSCC = − exp (αi ) Convki (X) n P exp (αj ) (1 + γ 1(yi ∈ C)) i=1  logP (yi |X; C; y<i ; θ) , j=1 where 1(·) is the indicator function and γ is the penalty factor. in which DepthConv(·) is the depth convolution structure proposed in Wu et al. (2019). And Pointwise(·) refers to a position-wise feed-forward network: 2.4 Lexically Constrained Decoding Beam search (Koehn, 2010) is a common approximate search algorithm for sequence generation task. Lexically constrained decoding is a modification to the beam search algorithm, which is proposed to enforce hard constraints that force a given constrained sequence to appear in the generated sequence. Specifically, beam search maintains a beam Bt on time step t, which contains only the b most likely partial sequences, where b is known as the beam size. The beam Bt is updated by retaining the b most likely sequences in the candidate set Et generated by considering all possible next word predictions: Pointwise(H) = max(0, H"
2021.wat-1.4,N03-1017,0,0.112871,"Missing"
2021.wat-1.4,W19-6721,0,0.0563198,"Missing"
2021.wat-1.4,D18-1262,1,0.831361,"ECT En→Ja test sets. ∗ indicates that the official evaluation results are reported. Dataset Sentences ParaCrawl-v5.1 Wiki Titles v2 ASPEC 10.12M 3.64M 3.01M Table 2: Training data statistics. [  s0 (Yˆt−1 , w) |Yˆt−1 ∈ Bt−1 , w ∈ V, δ(s0 , w) = s , where δ : S × V 7→ S is the FSM state-transition function that maps states and predicted words to states. System Details Our implementation of the Transformer models and lexically constrained decoding algorithm are based on the Fairseq toolkit1 . We follow the settings and pre-processing methods in our previous models and systems (He et al., 2018; Li et al., 2018; He et al., 2019; Li et al., 2019; Zhou et al., 2020; Li et al., 2020b,d,c; Zhang et al., 2020). We use Transformer-big as our basic model, which has 6 layers in both the encoder and decoder, respectively. For each layer, it consists of a multi-head attention sublayer with 16 heads and a feed-forward sublayer with an inner dimension 4096. The word embedding dimensions and the hidden state dimensions are set to 1024 for both the encoder and decoder. In the training phase, the dropout rate is set to 0.1. Our model training consists of two phases. In the first NMT pre-training phase, the ParaCra"
2021.wat-1.4,D19-1538,1,0.835069,"ts. ∗ indicates that the official evaluation results are reported. Dataset Sentences ParaCrawl-v5.1 Wiki Titles v2 ASPEC 10.12M 3.64M 3.01M Table 2: Training data statistics. [  s0 (Yˆt−1 , w) |Yˆt−1 ∈ Bt−1 , w ∈ V, δ(s0 , w) = s , where δ : S × V 7→ S is the FSM state-transition function that maps states and predicted words to states. System Details Our implementation of the Transformer models and lexically constrained decoding algorithm are based on the Fairseq toolkit1 . We follow the settings and pre-processing methods in our previous models and systems (He et al., 2018; Li et al., 2018; He et al., 2019; Li et al., 2019; Zhou et al., 2020; Li et al., 2020b,d,c; Zhang et al., 2020). We use Transformer-big as our basic model, which has 6 layers in both the encoder and decoder, respectively. For each layer, it consists of a multi-head attention sublayer with 16 heads and a feed-forward sublayer with an inner dimension 4096. The word embedding dimensions and the hidden state dimensions are set to 1024 for both the encoder and decoder. In the training phase, the dropout rate is set to 0.1. Our model training consists of two phases. In the first NMT pre-training phase, the ParaCrawlv5.1 (Espl`a et"
2021.wat-1.4,D18-1458,0,0.0396717,"Missing"
2021.wat-1.4,2020.wmt-1.22,1,0.828689,"Missing"
2021.wat-1.4,2020.findings-emnlp.371,1,0.890157,"Missing"
2021.wat-1.4,K19-2004,1,0.848833,"hat the official evaluation results are reported. Dataset Sentences ParaCrawl-v5.1 Wiki Titles v2 ASPEC 10.12M 3.64M 3.01M Table 2: Training data statistics. [  s0 (Yˆt−1 , w) |Yˆt−1 ∈ Bt−1 , w ∈ V, δ(s0 , w) = s , where δ : S × V 7→ S is the FSM state-transition function that maps states and predicted words to states. System Details Our implementation of the Transformer models and lexically constrained decoding algorithm are based on the Fairseq toolkit1 . We follow the settings and pre-processing methods in our previous models and systems (He et al., 2018; Li et al., 2018; He et al., 2019; Li et al., 2019; Zhou et al., 2020; Li et al., 2020b,d,c; Zhang et al., 2020). We use Transformer-big as our basic model, which has 6 layers in both the encoder and decoder, respectively. For each layer, it consists of a multi-head attention sublayer with 16 heads and a feed-forward sublayer with an inner dimension 4096. The word embedding dimensions and the hidden state dimensions are set to 1024 for both the encoder and decoder. In the training phase, the dropout rate is set to 0.1. Our model training consists of two phases. In the first NMT pre-training phase, the ParaCrawlv5.1 (Espl`a et al., 2019) and W"
2021.wat-1.4,N18-1119,0,0.0866255,"usion of restricted words in the output, and employed fixed pre-trained word embeddings to facilitate vocabulary expansion to unseen words in training. While these works accomplish the goal of explicit translation control, the time complexity of their decoding algorithm and resultant decoding speed falls short of the expectations. The complexity of grid beam search and constrained beam search is linear and exponential to the number of constraints, respectively. These algorithms are thus too inefficient to be practical for large-scale use. To alleviate the shortcomings in constrained decoding, Post and Vilar (2018) proposed a new constrained decoding algorithm with a claimed complexity of O(1) in the number of constraints - dynamic beam allocation which allocates the slots in a fixed-size beam. However, their approach still processes sentence constraints sequentially rather than batch processing, limiting the GPU’s parallel processing capabilities. Based on Post and Vilar (2018), a vectorized dynamic beam allocation approach was proposed in Hu et al. (2019), which which vectorThis paper describes our system (Team ID: nictrb) for participating in the WAT’21 restricted machine translation task. In our sub"
2021.wat-1.4,2020.findings-emnlp.398,1,0.72695,"evaluation results are reported. Dataset Sentences ParaCrawl-v5.1 Wiki Titles v2 ASPEC 10.12M 3.64M 3.01M Table 2: Training data statistics. [  s0 (Yˆt−1 , w) |Yˆt−1 ∈ Bt−1 , w ∈ V, δ(s0 , w) = s , where δ : S × V 7→ S is the FSM state-transition function that maps states and predicted words to states. System Details Our implementation of the Transformer models and lexically constrained decoding algorithm are based on the Fairseq toolkit1 . We follow the settings and pre-processing methods in our previous models and systems (He et al., 2018; Li et al., 2018; He et al., 2019; Li et al., 2019; Zhou et al., 2020; Li et al., 2020b,d,c; Zhang et al., 2020). We use Transformer-big as our basic model, which has 6 layers in both the encoder and decoder, respectively. For each layer, it consists of a multi-head attention sublayer with 16 heads and a feed-forward sublayer with an inner dimension 4096. The word embedding dimensions and the hidden state dimensions are set to 1024 for both the encoder and decoder. In the training phase, the dropout rate is set to 0.1. Our model training consists of two phases. In the first NMT pre-training phase, the ParaCrawlv5.1 (Espl`a et al., 2019) and Wiki Titles v2 datas"
C12-2077,W06-2922,0,0.00948837,"cribed this algorithm. Zhang and McDonald (2012) generalized the Eisner (1996) algorithm to handle arbitrary features over higher-order dependencies. However, their generalizing algorithm suffers quite high complexities of time and space – for instance, the parsing complexity of time is O(n5 ) for a third-order factored model. In order to achieve asymptotic efficiency of cost, cube pruning for decoding is utilized (Chiang, 2007). Another dominant category of data-driven dependency parsing systems is local-and-greedy transition-based parsing (Yamada and Matsumoto, 2003; Nivre and Scholz, 2004; Attardi, 2006; McDonald and Nivre, 2007) which parameterizes models over transitions from state to another in an abstract state-machine. In these models, dependency trees are constructed by making a series of incremental decisions. Parameters in these models are typically learned using standard classification techniques. 3 Fourth-Order Parsing Algorithm In this section, we propose our fourth-order dependency parsing algorithm, which factors each dependency tree into a set of grand-tri-sibling parts. Specifically, a grand-tri-sibling is a 5-tuple of indices (g, s, r, m, t) where (s, r, m, t) is a tri-siblin"
C12-2077,W06-2920,0,0.0235482,"22, the National Basic Research Program of China (Grant No. 2009CB320901), and the European Union Seventh Framework Program (Grant No. 247619). † Corresponding author Proceedings of COLING 2012: Posters, pages 785–796, COLING 2012, Mumbai, December 2012. 785 1 Introduction In recent years, dependency parsing has gained universal interest due to its usefulness in a wide range of applications such as synonym generation (Shinyama et al., 2002), relation extraction (Nguyen et al., 2009) and machine translation (Katz-Brown et al., 2011; Xie et al., 2011). CoNLL-X shared task on dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007) made a comparison of many algorithms, and graph-based parsing models have achieved stateof-the-art accuracy for a wide range of languages. Graph-based dependency parsing algorithms usually use the factored representations of dependency trees: a set of small parts with special structures. The types of features that the model can exploit depend on the information included in the factorizations. Several previous works have shown that higher-order parsers utilizing richer contextual information achieve higher accuracy than lower-order ones— Chen et al. (2010) illustrated that"
C12-2077,D07-1101,0,0.369014,"Graph-based dependency parsing algorithms usually use the factored representations of dependency trees: a set of small parts with special structures. The types of features that the model can exploit depend on the information included in the factorizations. Several previous works have shown that higher-order parsers utilizing richer contextual information achieve higher accuracy than lower-order ones— Chen et al. (2010) illustrated that a wide range of decision history can lead to significant improvements in accuracy for graph-based dependency parsing models. Meanwhile, several previous works (Carreras, 2007; Koo and Collins, 2010) have shown that grandchild interactions provide important information for dependency parsing. However, the computational cost of the parsing algorithm increases with the need for more expressive factorizations. Consequently, the existing most powerful parser (Koo and Collins, 2010) is limited to third-order parts, which requires O(n4 ) time and O(n3 ) space. In this paper, we further present a fourth-order parsing algorithm that can utilize more richer information by enclosing grand-sibling and tri-sibling parts into a grand-tri-sibling part. Koo and Collins (2010) dis"
C12-2077,W08-2102,0,0.0192275,"Missing"
C12-2077,C10-2015,0,0.0235945,"y parsing (Buchholz and Marsi, 2006; Nivre et al., 2007) made a comparison of many algorithms, and graph-based parsing models have achieved stateof-the-art accuracy for a wide range of languages. Graph-based dependency parsing algorithms usually use the factored representations of dependency trees: a set of small parts with special structures. The types of features that the model can exploit depend on the information included in the factorizations. Several previous works have shown that higher-order parsers utilizing richer contextual information achieve higher accuracy than lower-order ones— Chen et al. (2010) illustrated that a wide range of decision history can lead to significant improvements in accuracy for graph-based dependency parsing models. Meanwhile, several previous works (Carreras, 2007; Koo and Collins, 2010) have shown that grandchild interactions provide important information for dependency parsing. However, the computational cost of the parsing algorithm increases with the need for more expressive factorizations. Consequently, the existing most powerful parser (Koo and Collins, 2010) is limited to third-order parts, which requires O(n4 ) time and O(n3 ) space. In this paper, we furt"
C12-2077,J07-2003,0,0.0162997,"Collins (2010) also discussed the possibility that the third-order parsers are extended to fourth-order by increasing vertical context or horizontal context and Koo (2010) first described this algorithm. Zhang and McDonald (2012) generalized the Eisner (1996) algorithm to handle arbitrary features over higher-order dependencies. However, their generalizing algorithm suffers quite high complexities of time and space – for instance, the parsing complexity of time is O(n5 ) for a third-order factored model. In order to achieve asymptotic efficiency of cost, cube pruning for decoding is utilized (Chiang, 2007). Another dominant category of data-driven dependency parsing systems is local-and-greedy transition-based parsing (Yamada and Matsumoto, 2003; Nivre and Scholz, 2004; Attardi, 2006; McDonald and Nivre, 2007) which parameterizes models over transitions from state to another in an abstract state-machine. In these models, dependency trees are constructed by making a series of incremental decisions. Parameters in these models are typically learned using standard classification techniques. 3 Fourth-Order Parsing Algorithm In this section, we propose our fourth-order dependency parsing algorithm, w"
C12-2077,C96-1058,0,0.495152,"events of grandchild parts for outermost grandchildren. Koo and Collins (2010) proposed a third-order grand-sibling parser that decomposes each tree into set of grand-sibling parts—parts combined with sibling parts and grandchild parts. This factorization defines all grandchild and sibling parts and still requires O(n4 ) time and O(n3 ) space. Koo and Collins (2010) also discussed the possibility that the third-order parsers are extended to fourth-order by increasing vertical context or horizontal context and Koo (2010) first described this algorithm. Zhang and McDonald (2012) generalized the Eisner (1996) algorithm to handle arbitrary features over higher-order dependencies. However, their generalizing algorithm suffers quite high complexities of time and space – for instance, the parsing complexity of time is O(n5 ) for a third-order factored model. In order to achieve asymptotic efficiency of cost, cube pruning for decoding is utilized (Chiang, 2007). Another dominant category of data-driven dependency parsing systems is local-and-greedy transition-based parsing (Yamada and Matsumoto, 2003; Nivre and Scholz, 2004; Attardi, 2006; McDonald and Nivre, 2007) which parameterizes models over trans"
C12-2077,gimenez-marquez-2004-svmtool,0,0.023794,"Missing"
C12-2077,P10-1110,0,0.0125608,"marked † or ‡ are based on the Carreras (2007) parser, which might be replaced by our fourth-order parser to get an even better performance. 791 Parser McDonald and Pereira (2006), 1st order McDonald and Pereira (2006), 2nd order Zhang and Clark (2008) Zhang and Nivre (2011) Koo and Collins (2010), model 2 Koo and Collins (2010), model 1 Zhang and McDonald (2012) this paper Koo et al. (2008)† Carreras et al. (2008)‡ Suzuki et al. (2009)† UAS 90.9 91.5 92.1 92.9 92.9 93.0 93.1 93.4 93.2 93.5 93.8 CM 36.7 42.1 45.4 48.0 – – – 50.3 – – – Table 3: UAS and CM of different parsers on PTB 3.0 Parser Huang and Sagae (2010) Zhang and Clark (2008) Zhang and Nivre (2011) 3 rd order grand-sibling Zhang and McDonald (2012) this paper Zhang and Clark (2009)‡ UAS 85.2 85.7 86.0 86.8 86.9 87.4 86.6 CM 33.7 34.4 36.9 35.5 – 36.8 36.1 Table 4: UAS and CM of different parsers on CTB 5.0 Next, we turn to the impact of our fourth-order parser on Chinese. Table 4 shows the comparative results for Chinese. Here we compare our method to an implement of the third-order grand-sibling parser — whose parsing performance on CTB is not reported in Koo and Collins (2010), and the dynamic programming transition-based parser of Huang a"
C12-2077,D11-1017,0,0.0158054,"ation for the Doctoral Program of Higher Education of China under Grant No.20110073120022, the National Basic Research Program of China (Grant No. 2009CB320901), and the European Union Seventh Framework Program (Grant No. 247619). † Corresponding author Proceedings of COLING 2012: Posters, pages 785–796, COLING 2012, Mumbai, December 2012. 785 1 Introduction In recent years, dependency parsing has gained universal interest due to its usefulness in a wide range of applications such as synonym generation (Shinyama et al., 2002), relation extraction (Nguyen et al., 2009) and machine translation (Katz-Brown et al., 2011; Xie et al., 2011). CoNLL-X shared task on dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007) made a comparison of many algorithms, and graph-based parsing models have achieved stateof-the-art accuracy for a wide range of languages. Graph-based dependency parsing algorithms usually use the factored representations of dependency trees: a set of small parts with special structures. The types of features that the model can exploit depend on the information included in the factorizations. Several previous works have shown that higher-order parsers utilizing richer contextual inform"
C12-2077,P08-1068,0,0.0679127,"Missing"
C12-2077,P10-1001,0,0.495685,"ndency parsing algorithms usually use the factored representations of dependency trees: a set of small parts with special structures. The types of features that the model can exploit depend on the information included in the factorizations. Several previous works have shown that higher-order parsers utilizing richer contextual information achieve higher accuracy than lower-order ones— Chen et al. (2010) illustrated that a wide range of decision history can lead to significant improvements in accuracy for graph-based dependency parsing models. Meanwhile, several previous works (Carreras, 2007; Koo and Collins, 2010) have shown that grandchild interactions provide important information for dependency parsing. However, the computational cost of the parsing algorithm increases with the need for more expressive factorizations. Consequently, the existing most powerful parser (Koo and Collins, 2010) is limited to third-order parts, which requires O(n4 ) time and O(n3 ) space. In this paper, we further present a fourth-order parsing algorithm that can utilize more richer information by enclosing grand-sibling and tri-sibling parts into a grand-tri-sibling part. Koo and Collins (2010) discussed the possibility t"
C12-2077,J93-2004,0,0.0426964,"part. Koo and Collins (2010) discussed the possibility that the third-order parsers are extended to fourth-order by increasing vertical context (e.g. from grand-siblings to “great-grand-siblings”) or horizontal context (e.g. from grand-siblings to “grand-tri-siblings”), and Koo (2010) first described this algorithm. In this work, we show that grand-tri-siblings can effectively work. The computational requirements of this algorithm are O(n5 ) time and O(n4 ) space. To achieve empirical evaluations of our parser, we implement and evaluate the proposed parsing algorithm on the Penn WSJ Treebank (Marcus et al., 1993) for English, and Penn Chinese Treebank (Xue et al., 2005) for Chinese, both achieving state-of-the-art accuracy. A free distribution of our implementation in C++ has been put on the Internet.1 2 Related Work There have been several existing graph-based dependency parsing algorithms, which are the backbones of the new fourth-order dependency parser. In this section, we mainly describe four graph-based dependency parsers with different types of factorization. The first-order parser (McDonald et al., 2005) decomposes a dependency tree into its individual edges. Eisner (2000) introduced a widely-"
C12-2077,P05-1012,0,0.781575,"r parser, we implement and evaluate the proposed parsing algorithm on the Penn WSJ Treebank (Marcus et al., 1993) for English, and Penn Chinese Treebank (Xue et al., 2005) for Chinese, both achieving state-of-the-art accuracy. A free distribution of our implementation in C++ has been put on the Internet.1 2 Related Work There have been several existing graph-based dependency parsing algorithms, which are the backbones of the new fourth-order dependency parser. In this section, we mainly describe four graph-based dependency parsers with different types of factorization. The first-order parser (McDonald et al., 2005) decomposes a dependency tree into its individual edges. Eisner (2000) introduced a widely-used dynamic programming algorithm for first-order parsing, which is to parse the left and right dependents of a word independently, and combine them at a later stage. This algorithm introduces two types of dynamic programming structures: complete spans, and incomplete spans (McDonald, 2006). Larger spans are created from two smaller, adjacent spans by recursive combination in a bottom-up procedure. McDonald and Pereira (2006) defined a second-order sibling dependency parser in which interactions between"
C12-2077,D07-1013,0,0.0142415,"orithm. Zhang and McDonald (2012) generalized the Eisner (1996) algorithm to handle arbitrary features over higher-order dependencies. However, their generalizing algorithm suffers quite high complexities of time and space – for instance, the parsing complexity of time is O(n5 ) for a third-order factored model. In order to achieve asymptotic efficiency of cost, cube pruning for decoding is utilized (Chiang, 2007). Another dominant category of data-driven dependency parsing systems is local-and-greedy transition-based parsing (Yamada and Matsumoto, 2003; Nivre and Scholz, 2004; Attardi, 2006; McDonald and Nivre, 2007) which parameterizes models over transitions from state to another in an abstract state-machine. In these models, dependency trees are constructed by making a series of incremental decisions. Parameters in these models are typically learned using standard classification techniques. 3 Fourth-Order Parsing Algorithm In this section, we propose our fourth-order dependency parsing algorithm, which factors each dependency tree into a set of grand-tri-sibling parts. Specifically, a grand-tri-sibling is a 5-tuple of indices (g, s, r, m, t) where (s, r, m, t) is a tri-sibling part and (g, s, r, m) and"
C12-2077,E06-1011,0,0.92874,"d dependency parsers with different types of factorization. The first-order parser (McDonald et al., 2005) decomposes a dependency tree into its individual edges. Eisner (2000) introduced a widely-used dynamic programming algorithm for first-order parsing, which is to parse the left and right dependents of a word independently, and combine them at a later stage. This algorithm introduces two types of dynamic programming structures: complete spans, and incomplete spans (McDonald, 2006). Larger spans are created from two smaller, adjacent spans by recursive combination in a bottom-up procedure. McDonald and Pereira (2006) defined a second-order sibling dependency parser in which interactions between adjacent siblings are allowed. Koo and Collins (2010) proposed an algorithm 1 http://sourceforge.net/projects/maxparser/ 786 that factors each dependency tree into a set of grandchild parts. Formally, a grandchild part is a triple of indices (g, s, t) where g is the head of s and s is the head of t. In order to parse this factorization, it is necessary to augment both complete and incomplete spans with grandparent indices. Following Koo and Collins (2010), we refer to these augmented structures as g-spans. The seco"
C12-2077,D09-1143,0,0.013333,"ant No. 61170114), the National Research Foundation for the Doctoral Program of Higher Education of China under Grant No.20110073120022, the National Basic Research Program of China (Grant No. 2009CB320901), and the European Union Seventh Framework Program (Grant No. 247619). † Corresponding author Proceedings of COLING 2012: Posters, pages 785–796, COLING 2012, Mumbai, December 2012. 785 1 Introduction In recent years, dependency parsing has gained universal interest due to its usefulness in a wide range of applications such as synonym generation (Shinyama et al., 2002), relation extraction (Nguyen et al., 2009) and machine translation (Katz-Brown et al., 2011; Xie et al., 2011). CoNLL-X shared task on dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007) made a comparison of many algorithms, and graph-based parsing models have achieved stateof-the-art accuracy for a wide range of languages. Graph-based dependency parsing algorithms usually use the factored representations of dependency trees: a set of small parts with special structures. The types of features that the model can exploit depend on the information included in the factorizations. Several previous works have shown that higher"
C12-2077,C04-1010,0,0.0998741,"and Koo (2010) first described this algorithm. Zhang and McDonald (2012) generalized the Eisner (1996) algorithm to handle arbitrary features over higher-order dependencies. However, their generalizing algorithm suffers quite high complexities of time and space – for instance, the parsing complexity of time is O(n5 ) for a third-order factored model. In order to achieve asymptotic efficiency of cost, cube pruning for decoding is utilized (Chiang, 2007). Another dominant category of data-driven dependency parsing systems is local-and-greedy transition-based parsing (Yamada and Matsumoto, 2003; Nivre and Scholz, 2004; Attardi, 2006; McDonald and Nivre, 2007) which parameterizes models over transitions from state to another in an abstract state-machine. In these models, dependency trees are constructed by making a series of incremental decisions. Parameters in these models are typically learned using standard classification techniques. 3 Fourth-Order Parsing Algorithm In this section, we propose our fourth-order dependency parsing algorithm, which factors each dependency tree into a set of grand-tri-sibling parts. Specifically, a grand-tri-sibling is a 5-tuple of indices (g, s, r, m, t) where (s, r, m, t)"
C12-2077,D09-1058,0,0.0302719,"Missing"
C12-2077,D11-1020,0,0.0252194,"ogram of Higher Education of China under Grant No.20110073120022, the National Basic Research Program of China (Grant No. 2009CB320901), and the European Union Seventh Framework Program (Grant No. 247619). † Corresponding author Proceedings of COLING 2012: Posters, pages 785–796, COLING 2012, Mumbai, December 2012. 785 1 Introduction In recent years, dependency parsing has gained universal interest due to its usefulness in a wide range of applications such as synonym generation (Shinyama et al., 2002), relation extraction (Nguyen et al., 2009) and machine translation (Katz-Brown et al., 2011; Xie et al., 2011). CoNLL-X shared task on dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007) made a comparison of many algorithms, and graph-based parsing models have achieved stateof-the-art accuracy for a wide range of languages. Graph-based dependency parsing algorithms usually use the factored representations of dependency trees: a set of small parts with special structures. The types of features that the model can exploit depend on the information included in the factorizations. Several previous works have shown that higher-order parsers utilizing richer contextual information achieve highe"
C12-2077,W03-3023,0,0.0654522,"ntext or horizontal context and Koo (2010) first described this algorithm. Zhang and McDonald (2012) generalized the Eisner (1996) algorithm to handle arbitrary features over higher-order dependencies. However, their generalizing algorithm suffers quite high complexities of time and space – for instance, the parsing complexity of time is O(n5 ) for a third-order factored model. In order to achieve asymptotic efficiency of cost, cube pruning for decoding is utilized (Chiang, 2007). Another dominant category of data-driven dependency parsing systems is local-and-greedy transition-based parsing (Yamada and Matsumoto, 2003; Nivre and Scholz, 2004; Attardi, 2006; McDonald and Nivre, 2007) which parameterizes models over transitions from state to another in an abstract state-machine. In these models, dependency trees are constructed by making a series of incremental decisions. Parameters in these models are typically learned using standard classification techniques. 3 Fourth-Order Parsing Algorithm In this section, we propose our fourth-order dependency parsing algorithm, which factors each dependency tree into a set of grand-tri-sibling parts. Specifically, a grand-tri-sibling is a 5-tuple of indices (g, s, r, m"
C12-2077,D12-1030,0,0.36007,"cial limitation that it can only evaluate events of grandchild parts for outermost grandchildren. Koo and Collins (2010) proposed a third-order grand-sibling parser that decomposes each tree into set of grand-sibling parts—parts combined with sibling parts and grandchild parts. This factorization defines all grandchild and sibling parts and still requires O(n4 ) time and O(n3 ) space. Koo and Collins (2010) also discussed the possibility that the third-order parsers are extended to fourth-order by increasing vertical context or horizontal context and Koo (2010) first described this algorithm. Zhang and McDonald (2012) generalized the Eisner (1996) algorithm to handle arbitrary features over higher-order dependencies. However, their generalizing algorithm suffers quite high complexities of time and space – for instance, the parsing complexity of time is O(n5 ) for a third-order factored model. In order to achieve asymptotic efficiency of cost, cube pruning for decoding is utilized (Chiang, 2007). Another dominant category of data-driven dependency parsing systems is local-and-greedy transition-based parsing (Yamada and Matsumoto, 2003; Nivre and Scholz, 2004; Attardi, 2006; McDonald and Nivre, 2007) which p"
C12-2077,D08-1059,0,0.279524,"Missing"
C12-2077,W09-3825,0,0.0781358,"2010), two versions of POS tags are used for any features involve POS: one using is normal POS tags and another is a coarsened version of the POS tags. 2 5 Experiments The proposed fourth-order dependency parsing algorithm is evaluated on the Penn English Treebank (PTB 3.0) (Marcus et al., 1993) and the Penn Chinese Treebank (CTB 5.0). For English, the PTB data is prepared by using the standard split: sections 2-21 are used for training, section 22 is for development, and section 23 for test. For Chinese, we adopt the identical training/validation/testing data split and experimental set-up as Zhang and Clark (2009). Dependencies are extracted by using Penn2Malt3 tool. Parsing accuracy is measured with unlabeled attachment score (UAS): the percentage of words with the correct head, and the percentage of complete matches (CM).4 The k-best version of the Margin Infused Relaxed Algorithm (MIRA) (Crammer and Singer, 2003; Crammer et al., 2006; McDonald, 2006) for the max-margin models (Taskar et al., 2003) is chosen for parameter estimation of our parsing model, In practice, we set k = 10 and exclude the sentences containing more than 100 words in both the training data sets of English and Chinese in all exp"
C12-2077,P11-2033,0,0.0649127,"Missing"
C12-2077,D07-1096,0,\N,Missing
C12-2131,J96-2004,0,0.0781749,"through all these reviews and marked each review a spam or not, see Table 2. Finally, we get a dataset of 1800 reviews (900 positive and 900 negative opinions) for the balance of data. To evaluate the accuracy of our Chinese non-gold dataset, we annotated 800 reviews randomly selected instances twice and Kappa coefficient (κ) was calculated to compare the result of each annotator by the following formula: κ= P r(a) − P r(e) 1 − P r(e) (1) where P r(a) is the proportion of times that the two annotators agree and P r(e) is the proportion of times that they would are expected to agree by chance(Carletta, 1996). The Kappa coefficient result is 0.97. This result shows that our annotators reach a high agreement in our deceptive opinion annotation task. 4 Deceptive Opinion Detection To train an effective classifier, Ott et al. (2011) mainly focus on the following three approaches: • Genre identification; • Psycholinguist deceptive detection; • Text categorization; Another approach to identify deceptive opinion spam is using structural features of sentences. We are inspired by the idea that the genre of the text can be used to detect deceptive opinion spam. The structure of the sentence shows the genre"
C12-2131,de-marneffe-etal-2006-generating,0,0.0142346,"Missing"
C12-2131,P11-1032,0,0.423636,"r kinds of spam, such as web spam, e-mail spam and so on. Research focused on opinion spam is rarely reported till now. Jindal and Liu (2007) was one of the earliest work about Internet review spam. Further more, most previous work in this area focused on finding methods of detecting opinion spam which can be identified by a human reader. Only detecting this kind of opinion spam is not enough, because users can easily recognize the useless information by themselves and will not be misled. A much more challenging task, detection of deceptive opinion has been proposed by (Yoo and Gretzel, 2009; Ott et al., 2011) which is based on a gold-standard dataset. They have done the data selection and initial analysis work on this interesting topic. Our work uses gold-standard dataset collected by (Ott et al., 2011) and non-gold standard Chinese dataset collected by ourselves. We give a machine learning model which is about 2 percent better than previous works on gold standard dataset and is also very effective on non-gold standard dataset. Later, we analyze the close relationship between sentence structure and deceptive opinion. Finally, statistical methodologies have been used to analyze all the feature sets"
C12-2131,D11-1147,0,0.0227212,"(Sahami et al., 1998; Drucker et al., 1999), web spam (Fetterly et al., 2004; Ntoulas et al., 2006),blog spam (Bhattarai et al., 2009), Twitter spam (Grier et al., 2010) and 1 http://tripadvisor.com 1342 review spam (Jindal and Liu, 2008). They used statistics and machine learning methodologies to analyze and investigate this topic extensively. In recent years, some researchers have begun to pay attention to the detection of spam which is deceptive. They analyzed review spam (Yoo and Gretzel, 2009; Wu et al., 2010; Ott et al., 2011; Li et al., 2011; Lau et al., 2012) and rumors on microblogs (Qazvinian et al., 2011). Following these works, our work deals with the second problem, deceptive opinion detection. Although, opinion spam is widely spread on the Internet(Jindal and Liu, 2008). It is quite difficult to obtain a first-hand deceptive opinion dataset. Jindal and Liu (2008) used duplicate reviews as positive data and other views as negative examples2 . Wu et al. (2010) tried to detect deceptive opinion spam by comparing popularity rankings. Qazvinian et al. (2011) annotated rumors (a similar concept to deceptive opinion) by experts manually which is a huge project. Our work may save such human judgeme"
C12-2131,W06-0127,1,0.528859,"Missing"
C12-2131,J96-1002,0,\N,Missing
C12-3067,P07-1032,0,0.0296533,"247619). † Corresponding author Proceedings of COLING 2012: Demonstration Papers, pages 535–542, COLING 2012, Mumbai, December 2012. 535 1 Introduction Much has been done in cross-framework performance analysis of parsers, and nearly all such analysis applies a converter across different grammar frameworks. Matsuzaki and Tsujii (2008) compared a Head-driven Phrase Structure Grammar (HPSG) parser with several Context Free Grammar (CFG) parsers by converting the parsing result to a shallow CFG analysis using an automatic tree converter based on Stochastic Synchronous Tree-Substitution Grammar. Clark and Curran (2007) converted the CCG dependencies of the CCG parser into those in Depbank by developing mapping rules via inspection so as to compare the performance of their CCG parser with the RASP parser. Performing such a conversion has been proven to be a time-consuming and non-trivial task (Clark and Curran, 2007). Although CCGBank (Hockenmaier and Steedman, 2007) is a translation of the Penn Treebank (Marcus et al., 1993) into a corpus of Combinatory Categorial Grammar derivations, little work has been done in conversion from CCG derivations back to PTB trees besides (Clark and Curran, 2009) and (Kummerf"
C12-3067,P09-2014,0,0.21725,"ion Grammar. Clark and Curran (2007) converted the CCG dependencies of the CCG parser into those in Depbank by developing mapping rules via inspection so as to compare the performance of their CCG parser with the RASP parser. Performing such a conversion has been proven to be a time-consuming and non-trivial task (Clark and Curran, 2007). Although CCGBank (Hockenmaier and Steedman, 2007) is a translation of the Penn Treebank (Marcus et al., 1993) into a corpus of Combinatory Categorial Grammar derivations, little work has been done in conversion from CCG derivations back to PTB trees besides (Clark and Curran, 2009) and (Kummerfeld et al., 2012). In the work of Clark and Curran (2009), they associated conversion rules with each local tree and developed 32 unary and 776 binary rule instances by manual inspection. Considerable time and effort were spent on the creation of these schemas. They show that although CCGBank is derived from PTB, the conversion from CCG back to PTB trees is far from trivial due to the non-isomorphic property of tree structures and the non-correspondence of tree labels. In the work of Kummerfeld et al. (2012), although no ad-hoc rules over non-local features were required, a set of"
C12-3067,J07-3004,0,0.113165,"a Head-driven Phrase Structure Grammar (HPSG) parser with several Context Free Grammar (CFG) parsers by converting the parsing result to a shallow CFG analysis using an automatic tree converter based on Stochastic Synchronous Tree-Substitution Grammar. Clark and Curran (2007) converted the CCG dependencies of the CCG parser into those in Depbank by developing mapping rules via inspection so as to compare the performance of their CCG parser with the RASP parser. Performing such a conversion has been proven to be a time-consuming and non-trivial task (Clark and Curran, 2007). Although CCGBank (Hockenmaier and Steedman, 2007) is a translation of the Penn Treebank (Marcus et al., 1993) into a corpus of Combinatory Categorial Grammar derivations, little work has been done in conversion from CCG derivations back to PTB trees besides (Clark and Curran, 2009) and (Kummerfeld et al., 2012). In the work of Clark and Curran (2009), they associated conversion rules with each local tree and developed 32 unary and 776 binary rule instances by manual inspection. Considerable time and effort were spent on the creation of these schemas. They show that although CCGBank is derived from PTB, the conversion from CCG back to PTB tre"
C12-3067,P12-2021,0,0.0109015,"(2007) converted the CCG dependencies of the CCG parser into those in Depbank by developing mapping rules via inspection so as to compare the performance of their CCG parser with the RASP parser. Performing such a conversion has been proven to be a time-consuming and non-trivial task (Clark and Curran, 2007). Although CCGBank (Hockenmaier and Steedman, 2007) is a translation of the Penn Treebank (Marcus et al., 1993) into a corpus of Combinatory Categorial Grammar derivations, little work has been done in conversion from CCG derivations back to PTB trees besides (Clark and Curran, 2009) and (Kummerfeld et al., 2012). In the work of Clark and Curran (2009), they associated conversion rules with each local tree and developed 32 unary and 776 binary rule instances by manual inspection. Considerable time and effort were spent on the creation of these schemas. They show that although CCGBank is derived from PTB, the conversion from CCG back to PTB trees is far from trivial due to the non-isomorphic property of tree structures and the non-correspondence of tree labels. In the work of Kummerfeld et al. (2012), although no ad-hoc rules over non-local features were required, a set of instructions, operations, and"
C12-3067,J93-2004,0,0.0385921,"ext Free Grammar (CFG) parsers by converting the parsing result to a shallow CFG analysis using an automatic tree converter based on Stochastic Synchronous Tree-Substitution Grammar. Clark and Curran (2007) converted the CCG dependencies of the CCG parser into those in Depbank by developing mapping rules via inspection so as to compare the performance of their CCG parser with the RASP parser. Performing such a conversion has been proven to be a time-consuming and non-trivial task (Clark and Curran, 2007). Although CCGBank (Hockenmaier and Steedman, 2007) is a translation of the Penn Treebank (Marcus et al., 1993) into a corpus of Combinatory Categorial Grammar derivations, little work has been done in conversion from CCG derivations back to PTB trees besides (Clark and Curran, 2009) and (Kummerfeld et al., 2012). In the work of Clark and Curran (2009), they associated conversion rules with each local tree and developed 32 unary and 776 binary rule instances by manual inspection. Considerable time and effort were spent on the creation of these schemas. They show that although CCGBank is derived from PTB, the conversion from CCG back to PTB trees is far from trivial due to the non-isomorphic property of"
C12-3067,C08-1069,0,0.0191044,"nt No. 60903119 and Grant No. 61170114), the National Research Foundation for the Doctoral Program of Higher Education of China under Grant No. 20110073120022, the National Basic Research Program of China (Grant No. 2009CB320901), and the European Union Seventh Framework Program (Grant No. 247619). † Corresponding author Proceedings of COLING 2012: Demonstration Papers, pages 535–542, COLING 2012, Mumbai, December 2012. 535 1 Introduction Much has been done in cross-framework performance analysis of parsers, and nearly all such analysis applies a converter across different grammar frameworks. Matsuzaki and Tsujii (2008) compared a Head-driven Phrase Structure Grammar (HPSG) parser with several Context Free Grammar (CFG) parsers by converting the parsing result to a shallow CFG analysis using an automatic tree converter based on Stochastic Synchronous Tree-Substitution Grammar. Clark and Curran (2007) converted the CCG dependencies of the CCG parser into those in Depbank by developing mapping rules via inspection so as to compare the performance of their CCG parser with the RASP parser. Performing such a conversion has been proven to be a time-consuming and non-trivial task (Clark and Curran, 2007). Although"
C12-3067,W11-0328,0,0.0154131,"l with the predicted PTB label. Then, an intermediate PTB tree is built. Secondly, C lassi f ier2 examines each leaf in the intermediate PTB tree and predicts whether a parent node should be added to dominate the leaf. The feature sets for the two classifiers are listed in Table 1. 4 Experiments We evaluate the proposed approach on CCGBank and PTB. As in (Clark and Curran, 2009), we use Section 01-22 as training set3 , Section 00 as development set to tune the parameters and Section 23 as test set. Experiments are done separately with gold POS tags and auto POS tags predicted by Lapos tagger (Tsuruoka et al., 2011). 1 According to our inspection, these cases always occur in certain language structures, such as complex objects, “of” phrases and so on. 2 We use the implementation by Apache OpenNLP http://incubator.apache.org/opennlp/index.html 3 Since the converter is trained on the gold banks, it doesn’t have access to parsing output. And thus, the converting process is general and not specific to parser errors. 538 Feature Set 1 FeaturesC C GN od e Label ParentLabel LSiblingLabel LSiblingWordForm RSiblingLabel RSiblingWordForm Children ChildCnt Features P T BN od e Children LSiblingLabel POS POS−1 Featu"
C16-1180,D15-1263,0,0.0231179,"ed model obtains state-of-the-art results. 1 Introduction It is widely agreed that in a formal text, units including clauses and sentences are not isolated but instead connected logically, semantically, and syntactically. Discourse parsing is a fundamental task in natural language processing (NLP) that analyzes the latent relation structure and discovers those connections across text units. It could benefit various downstream NLP applications such as question answering (Chai and Jin, 2004; Verberne et al., 2007), machine translation (Hardmeier, 2012; Guzm´an et al., 2014), sentiment analysis (Bhatia et al., 2015; Hu et al., 2016b), and automatic summarization (Maskey and Hirschberg, 2005; Murray et al., 2006). For discourse parsing, Penn Discourse Treebank (PDTB) (Prasad et al., 2008) provides the lexicallygrounded annotations of discourse relations. Each discourse relation consists of two abstract object arguments and the corresponding sense annotations, which can be roughly characterized according to whether explicit connectives could be drawn from the texts. In Explicit relations, explicit connectives can be found in the texts; when such indicators are not given directly, an inferred connective ex"
C16-1180,P13-2013,0,0.425761,"for discourse relation recognition. Moreover, implicit relation recognition calls for semantic understanding, which needs to encode the word meaning in the context and the sentence-level understanding for the argument pairs. Considering the complexity of natural language, the task is quite nontrivial and requires more effective encoding of the arguments. Conventional methods for implicit discourse relation recognition are based on manually specified indicator features, such as bag-of-words, production rules, and other linguistically-informed features (Zhou et al., 2010; Park and Cardie, 2012; Biran and McKeown, 2013; Rutherford and Xue, 2014). Recently, embedding based neural models have been proved effective to address the data sparsity problem that is not well solved in traditional methods. The key techniques include real-valued dense embeddings for feature representations and non-linear neural models for feature combinations and transformations. However, most of the neural models take words as the smallest processing units, which can suffer a lot from insufficient training on rare words. Discourse parsing, as the highest level language processing at present, covers word and sentence levels for feature"
C16-1180,D15-1262,0,0.358395,"Missing"
C16-1180,P16-1039,1,0.744854,"f e wb The wf e wb defense wf e wb The wf e wb worked f e w b CNN share parameters The s1 MLP softmax defense worked The defense Arg1 bi-LSTMs CNN ... ... Word-Level Module Enhanced-Embedding Arg2 CNN s2 defense won Word Look-up Table Figure 1: Architecture of the proposed model. Xue (2015) and Ji et al. (2015) add automatically-labeled instances to expand data. Fisher and Simmons (2015) incorporate a mixture of labeled and unlabeled data to reduce the need for annotated data. More recently, neural network models have been proved effective for NLP tasks (Wang et al., 2016; Zhang et al., 2016; Cai and Zhao, 2016; Hu et al., 2016a) and also utilized for implicit discourse relation recognition. Ji and Eisenstein (2015) adopt recursive neural network and incorporated with entityaugmented distributed semantics. Zhang et al. (2015a) propose a simplified neural network which contains only one hidden layer and use three different pooling operations (max, min, average). Chen et al. (2016) adopt a deep gated neural model to capture the semantic interactions between argument pairs. Ji et al. (2016) propose a latent variable recurrent neural network architecture for jointly modeling sequences of words. (Qin et"
C16-1180,W04-2504,0,0.0287159,"rawbacks of the current word level representation. Our experiments show that the enhanced embeddings work well and the proposed model obtains state-of-the-art results. 1 Introduction It is widely agreed that in a formal text, units including clauses and sentences are not isolated but instead connected logically, semantically, and syntactically. Discourse parsing is a fundamental task in natural language processing (NLP) that analyzes the latent relation structure and discovers those connections across text units. It could benefit various downstream NLP applications such as question answering (Chai and Jin, 2004; Verberne et al., 2007), machine translation (Hardmeier, 2012; Guzm´an et al., 2014), sentiment analysis (Bhatia et al., 2015; Hu et al., 2016b), and automatic summarization (Maskey and Hirschberg, 2005; Murray et al., 2006). For discourse parsing, Penn Discourse Treebank (PDTB) (Prasad et al., 2008) provides the lexicallygrounded annotations of discourse relations. Each discourse relation consists of two abstract object arguments and the corresponding sense annotations, which can be roughly characterized according to whether explicit connectives could be drawn from the texts. In Explicit rel"
C16-1180,K15-2005,1,0.571344,"Society Science Foundation of China (No. 15-ZDA041). 1914 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 1914–1924, Osaka, Japan, December 11-17 2016. clauses are effectively indicated by explicit connectives like “but” and “so”, implicit discourse relation recognition is much more difficult. Without effective indicators, the relations could only be inferred from indirect plain texts, which makes implicit discourse relation recognition the bottleneck of the entire discourse parsing system (Qin et al., 2016a; Li et al., 2016; Chen et al., 2015). This paper attempts to deal with this challenging task. The challenge stems from the fact that, without connective cues, recognizing implicit relation has to rely solely on two textual arguments, and it must capture latent semantic and logical relationship between two arguments in discourse-level. First, given limited amount of annotated corpus, both the traditional indicator feature based methods and the recent embedding based neural methods (Wang et al., 2015) can suffer from insufficient data. The training is especially difficult for rare words, which appear rarely in the corpus but gener"
C16-1180,P16-1163,0,0.397616,"Simmons (2015) incorporate a mixture of labeled and unlabeled data to reduce the need for annotated data. More recently, neural network models have been proved effective for NLP tasks (Wang et al., 2016; Zhang et al., 2016; Cai and Zhao, 2016; Hu et al., 2016a) and also utilized for implicit discourse relation recognition. Ji and Eisenstein (2015) adopt recursive neural network and incorporated with entityaugmented distributed semantics. Zhang et al. (2015a) propose a simplified neural network which contains only one hidden layer and use three different pooling operations (max, min, average). Chen et al. (2016) adopt a deep gated neural model to capture the semantic interactions between argument pairs. Ji et al. (2016) propose a latent variable recurrent neural network architecture for jointly modeling sequences of words. (Qin et al., 2016b) propose a stacking neural network model to solve the classification problem. In their model, convolutional neural network is utilized for sentence modeling and a collaborative gated neural network is proposed for feature transformation. 3 3.1 Model Architecture The architecture of our model is shown in Figure 1. The model is a hybrid neural network including a c"
C16-1180,P15-2015,0,0.016416,"terns to identify the meaning in text. Li and Nenkova (2014) introduce a syntactic representation to reduce sparsity. Rutherford and 1915 Character-Level Module The ploy d e worked f e n s e ... ... The The w ploy ploy wf e wb worked wf e wb The wf e wb defense wf e wb The wf e wb worked f e w b CNN share parameters The s1 MLP softmax defense worked The defense Arg1 bi-LSTMs CNN ... ... Word-Level Module Enhanced-Embedding Arg2 CNN s2 defense won Word Look-up Table Figure 1: Architecture of the proposed model. Xue (2015) and Ji et al. (2015) add automatically-labeled instances to expand data. Fisher and Simmons (2015) incorporate a mixture of labeled and unlabeled data to reduce the need for annotated data. More recently, neural network models have been proved effective for NLP tasks (Wang et al., 2016; Zhang et al., 2016; Cai and Zhao, 2016; Hu et al., 2016a) and also utilized for implicit discourse relation recognition. Ji and Eisenstein (2015) adopt recursive neural network and incorporated with entityaugmented distributed semantics. Zhang et al. (2015a) propose a simplified neural network which contains only one hidden layer and use three different pooling operations (max, min, average). Chen et al. (2"
C16-1180,P14-1065,0,0.0942749,"Missing"
C16-1180,P16-1228,0,0.180828,"e-of-the-art results. 1 Introduction It is widely agreed that in a formal text, units including clauses and sentences are not isolated but instead connected logically, semantically, and syntactically. Discourse parsing is a fundamental task in natural language processing (NLP) that analyzes the latent relation structure and discovers those connections across text units. It could benefit various downstream NLP applications such as question answering (Chai and Jin, 2004; Verberne et al., 2007), machine translation (Hardmeier, 2012; Guzm´an et al., 2014), sentiment analysis (Bhatia et al., 2015; Hu et al., 2016b), and automatic summarization (Maskey and Hirschberg, 2005; Murray et al., 2006). For discourse parsing, Penn Discourse Treebank (PDTB) (Prasad et al., 2008) provides the lexicallygrounded annotations of discourse relations. Each discourse relation consists of two abstract object arguments and the corresponding sense annotations, which can be roughly characterized according to whether explicit connectives could be drawn from the texts. In Explicit relations, explicit connectives can be found in the texts; when such indicators are not given directly, an inferred connective expression could be"
C16-1180,D16-1173,0,0.201555,"e-of-the-art results. 1 Introduction It is widely agreed that in a formal text, units including clauses and sentences are not isolated but instead connected logically, semantically, and syntactically. Discourse parsing is a fundamental task in natural language processing (NLP) that analyzes the latent relation structure and discovers those connections across text units. It could benefit various downstream NLP applications such as question answering (Chai and Jin, 2004; Verberne et al., 2007), machine translation (Hardmeier, 2012; Guzm´an et al., 2014), sentiment analysis (Bhatia et al., 2015; Hu et al., 2016b), and automatic summarization (Maskey and Hirschberg, 2005; Murray et al., 2006). For discourse parsing, Penn Discourse Treebank (PDTB) (Prasad et al., 2008) provides the lexicallygrounded annotations of discourse relations. Each discourse relation consists of two abstract object arguments and the corresponding sense annotations, which can be roughly characterized according to whether explicit connectives could be drawn from the texts. In Explicit relations, explicit connectives can be found in the texts; when such indicators are not given directly, an inferred connective expression could be"
C16-1180,Q15-1024,0,0.3578,"defense worked The defense Arg1 bi-LSTMs CNN ... ... Word-Level Module Enhanced-Embedding Arg2 CNN s2 defense won Word Look-up Table Figure 1: Architecture of the proposed model. Xue (2015) and Ji et al. (2015) add automatically-labeled instances to expand data. Fisher and Simmons (2015) incorporate a mixture of labeled and unlabeled data to reduce the need for annotated data. More recently, neural network models have been proved effective for NLP tasks (Wang et al., 2016; Zhang et al., 2016; Cai and Zhao, 2016; Hu et al., 2016a) and also utilized for implicit discourse relation recognition. Ji and Eisenstein (2015) adopt recursive neural network and incorporated with entityaugmented distributed semantics. Zhang et al. (2015a) propose a simplified neural network which contains only one hidden layer and use three different pooling operations (max, min, average). Chen et al. (2016) adopt a deep gated neural model to capture the semantic interactions between argument pairs. Ji et al. (2016) propose a latent variable recurrent neural network architecture for jointly modeling sequences of words. (Qin et al., 2016b) propose a stacking neural network model to solve the classification problem. In their model, co"
C16-1180,D15-1264,0,0.104561,"pairs to represent discourse relation and incorporate coreference patterns to identify the meaning in text. Li and Nenkova (2014) introduce a syntactic representation to reduce sparsity. Rutherford and 1915 Character-Level Module The ploy d e worked f e n s e ... ... The The w ploy ploy wf e wb worked wf e wb The wf e wb defense wf e wb The wf e wb worked f e w b CNN share parameters The s1 MLP softmax defense worked The defense Arg1 bi-LSTMs CNN ... ... Word-Level Module Enhanced-Embedding Arg2 CNN s2 defense won Word Look-up Table Figure 1: Architecture of the proposed model. Xue (2015) and Ji et al. (2015) add automatically-labeled instances to expand data. Fisher and Simmons (2015) incorporate a mixture of labeled and unlabeled data to reduce the need for annotated data. More recently, neural network models have been proved effective for NLP tasks (Wang et al., 2016; Zhang et al., 2016; Cai and Zhao, 2016; Hu et al., 2016a) and also utilized for implicit discourse relation recognition. Ji and Eisenstein (2015) adopt recursive neural network and incorporated with entityaugmented distributed semantics. Zhang et al. (2015a) propose a simplified neural network which contains only one hidden layer"
C16-1180,N16-1037,0,0.277435,"ecently, neural network models have been proved effective for NLP tasks (Wang et al., 2016; Zhang et al., 2016; Cai and Zhao, 2016; Hu et al., 2016a) and also utilized for implicit discourse relation recognition. Ji and Eisenstein (2015) adopt recursive neural network and incorporated with entityaugmented distributed semantics. Zhang et al. (2015a) propose a simplified neural network which contains only one hidden layer and use three different pooling operations (max, min, average). Chen et al. (2016) adopt a deep gated neural model to capture the semantic interactions between argument pairs. Ji et al. (2016) propose a latent variable recurrent neural network architecture for jointly modeling sequences of words. (Qin et al., 2016b) propose a stacking neural network model to solve the classification problem. In their model, convolutional neural network is utilized for sentence modeling and a collaborative gated neural network is proposed for feature transformation. 3 3.1 Model Architecture The architecture of our model is shown in Figure 1. The model is a hybrid neural network including a character-level module and a word-level module. The character-level module receives inputs of character-level e"
C16-1180,W14-4327,0,0.0152778,"r features and linear models. Pitler et al. (2009) use several linguistically informed features, including polarity tags, Levin verb classes and length of verb phrases. Zhou et al (2010) improve the performance through predicting connective words as extra features. Park and Cardie (2012) propose a method using a locally-optimal feature set. Biran and McKeown (2013) collect word pairs from arguments of explicit examples to help the learning. Rutherford and Xue (2014) employ Brown cluster pairs to represent discourse relation and incorporate coreference patterns to identify the meaning in text. Li and Nenkova (2014) introduce a syntactic representation to reduce sparsity. Rutherford and 1915 Character-Level Module The ploy d e worked f e n s e ... ... The The w ploy ploy wf e wb worked wf e wb The wf e wb defense wf e wb The wf e wb worked f e w b CNN share parameters The s1 MLP softmax defense worked The defense Arg1 bi-LSTMs CNN ... ... Word-Level Module Enhanced-Embedding Arg2 CNN s2 defense won Word Look-up Table Figure 1: Architecture of the proposed model. Xue (2015) and Ji et al. (2015) add automatically-labeled instances to expand data. Fisher and Simmons (2015) incorporate a mixture of labeled a"
C16-1180,K16-2008,1,0.714606,"ject of National Society Science Foundation of China (No. 15-ZDA041). 1914 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 1914–1924, Osaka, Japan, December 11-17 2016. clauses are effectively indicated by explicit connectives like “but” and “so”, implicit discourse relation recognition is much more difficult. Without effective indicators, the relations could only be inferred from indirect plain texts, which makes implicit discourse relation recognition the bottleneck of the entire discourse parsing system (Qin et al., 2016a; Li et al., 2016; Chen et al., 2015). This paper attempts to deal with this challenging task. The challenge stems from the fact that, without connective cues, recognizing implicit relation has to rely solely on two textual arguments, and it must capture latent semantic and logical relationship between two arguments in discourse-level. First, given limited amount of annotated corpus, both the traditional indicator feature based methods and the recent embedding based neural methods (Wang et al., 2015) can suffer from insufficient data. The training is especially difficult for rare words, which appear rarely in"
C16-1180,D09-1036,0,0.656919,"s, the data contain 1 http://www.seas.upenn.edu/ pdtb/ 1918 16,224 implicit relations. It provides three hierarchies of relations: Level 1 Class, Level 2 Type, and Level 3 Subtypes. The first level consists of four major relation Class: C OMPARISON, C ONTINGENCY, E XPANSION and T EMPORAL. There are 16 Level 2 relation types of implicit relations. The third level of Subtypes is types that are only available for specific types. For the evaluation of implicit relation classification, there are two settings in previous works: one is multi-class classification for second-level discourse relations (Lin et al., 2009); the other is the “OneVersus-Others” setting which employs binary classification only for Level 1 Class, which is first used by Pitler et at. (2009). Note that the results for the latter setting can be also derived from the specific statistics over the results of the former setting. In this paper, we will focus on the more practical multiclass classification, which is a necessary component for building a complete discourse parser such as that for the shared tasks of CoNLL-2015 and 2016 (Xue et al., 2015; Xue et al., 2016). For the model analysis, we perform the experiments with the multi-clas"
C16-1180,D15-1176,0,0.0377227,"ng, as the highest level language processing at present, covers word and sentence levels for feature representation. This work extends the current wordlevel representation onto more fine-grained character-level which is helpful for encoding morphology information and alleviating the rare word problem. In summary, this paper presents a neural model with context-aware character-enhanced embeddings to address implicit discourse relation recognition task. Recently, character-aware models have been popular for English and other morphologically rich languages (Kim et al., 2016; Zhang et al., 2015b; Ling et al., 2015). The proposed model enhanced the word embeddings with character-aware representations learned from stacked convolutional and recurrent neural models. Utilizing these enhanced embeddings, the model covers information of three levels from character, word, to sentence. Through extensive experiments on standard discourse corpus, we analyze several models and show the superiority of the proposed method. The remaining of the paper is organized as follows: Section 2 discusses related work; Section 3 describes the proposed model; Section 4 provides the details of experiments and model analysis; and S"
C16-1180,N06-1047,0,0.0205418,"t, units including clauses and sentences are not isolated but instead connected logically, semantically, and syntactically. Discourse parsing is a fundamental task in natural language processing (NLP) that analyzes the latent relation structure and discovers those connections across text units. It could benefit various downstream NLP applications such as question answering (Chai and Jin, 2004; Verberne et al., 2007), machine translation (Hardmeier, 2012; Guzm´an et al., 2014), sentiment analysis (Bhatia et al., 2015; Hu et al., 2016b), and automatic summarization (Maskey and Hirschberg, 2005; Murray et al., 2006). For discourse parsing, Penn Discourse Treebank (PDTB) (Prasad et al., 2008) provides the lexicallygrounded annotations of discourse relations. Each discourse relation consists of two abstract object arguments and the corresponding sense annotations, which can be roughly characterized according to whether explicit connectives could be drawn from the texts. In Explicit relations, explicit connectives can be found in the texts; when such indicators are not given directly, an inferred connective expression could be inserted, forming Implicit relations. The following two examples describes these"
C16-1180,W12-1614,0,0.674379,"g in high perplexities for discourse relation recognition. Moreover, implicit relation recognition calls for semantic understanding, which needs to encode the word meaning in the context and the sentence-level understanding for the argument pairs. Considering the complexity of natural language, the task is quite nontrivial and requires more effective encoding of the arguments. Conventional methods for implicit discourse relation recognition are based on manually specified indicator features, such as bag-of-words, production rules, and other linguistically-informed features (Zhou et al., 2010; Park and Cardie, 2012; Biran and McKeown, 2013; Rutherford and Xue, 2014). Recently, embedding based neural models have been proved effective to address the data sparsity problem that is not well solved in traditional methods. The key techniques include real-valued dense embeddings for feature representations and non-linear neural models for feature combinations and transformations. However, most of the neural models take words as the smallest processing units, which can suffer a lot from insufficient training on rare words. Discourse parsing, as the highest level language processing at present, covers word and se"
C16-1180,P09-1077,0,0.807876,"e inserted, forming Implicit relations. The following two examples describes these two kinds of discourse relations: the former has an explicit connective “so” which reveals the Explicit relation, while in the latter case, an inferred Implicit connective “that is” has to be inserted to express the relation. (1) Arg1: We’re standing in gasoline. Arg2: So don’t smoke. (Contingency.Cause.Result - wsj 0596) (2) Arg1: The ploy worked. Arg2: Implicit=that is The defense won. (Contingency.Cause - wsj 1267) It has been shown that discourse connective is crucial for high-accuracy relation recognition (Pitler et al., 2009; Lin et al., 2014). Compared to explicit discourse relations in which senses between adjacent ∗ Corresponding author. This paper was partially supported by Cai Yuanpei Program (CSC No. 201304490199 and No. 201304490171), National Natural Science Foundation of China (No. 61170114, No. 61672343 and No. 61272248), National Basic Research Program of China (No. 2013CB329401), Major Basic Research Program of Shanghai Science and Technology Committee (No. 15JC1400103), Art and Science Interdisciplinary Funds of Shanghai Jiao Tong University (No. 14JCRZ04), and Key Project of National Society Science"
C16-1180,prasad-etal-2008-penn,0,0.347227,"ed logically, semantically, and syntactically. Discourse parsing is a fundamental task in natural language processing (NLP) that analyzes the latent relation structure and discovers those connections across text units. It could benefit various downstream NLP applications such as question answering (Chai and Jin, 2004; Verberne et al., 2007), machine translation (Hardmeier, 2012; Guzm´an et al., 2014), sentiment analysis (Bhatia et al., 2015; Hu et al., 2016b), and automatic summarization (Maskey and Hirschberg, 2005; Murray et al., 2006). For discourse parsing, Penn Discourse Treebank (PDTB) (Prasad et al., 2008) provides the lexicallygrounded annotations of discourse relations. Each discourse relation consists of two abstract object arguments and the corresponding sense annotations, which can be roughly characterized according to whether explicit connectives could be drawn from the texts. In Explicit relations, explicit connectives can be found in the texts; when such indicators are not given directly, an inferred connective expression could be inserted, forming Implicit relations. The following two examples describes these two kinds of discourse relations: the former has an explicit connective “so”"
C16-1180,K16-2010,1,0.89221,"CRZ04), and Key Project of National Society Science Foundation of China (No. 15-ZDA041). 1914 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 1914–1924, Osaka, Japan, December 11-17 2016. clauses are effectively indicated by explicit connectives like “but” and “so”, implicit discourse relation recognition is much more difficult. Without effective indicators, the relations could only be inferred from indirect plain texts, which makes implicit discourse relation recognition the bottleneck of the entire discourse parsing system (Qin et al., 2016a; Li et al., 2016; Chen et al., 2015). This paper attempts to deal with this challenging task. The challenge stems from the fact that, without connective cues, recognizing implicit relation has to rely solely on two textual arguments, and it must capture latent semantic and logical relationship between two arguments in discourse-level. First, given limited amount of annotated corpus, both the traditional indicator feature based methods and the recent embedding based neural methods (Wang et al., 2015) can suffer from insufficient data. The training is especially difficult for rare words, which"
C16-1180,D16-1246,1,0.896434,"CRZ04), and Key Project of National Society Science Foundation of China (No. 15-ZDA041). 1914 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 1914–1924, Osaka, Japan, December 11-17 2016. clauses are effectively indicated by explicit connectives like “but” and “so”, implicit discourse relation recognition is much more difficult. Without effective indicators, the relations could only be inferred from indirect plain texts, which makes implicit discourse relation recognition the bottleneck of the entire discourse parsing system (Qin et al., 2016a; Li et al., 2016; Chen et al., 2015). This paper attempts to deal with this challenging task. The challenge stems from the fact that, without connective cues, recognizing implicit relation has to rely solely on two textual arguments, and it must capture latent semantic and logical relationship between two arguments in discourse-level. First, given limited amount of annotated corpus, both the traditional indicator feature based methods and the recent embedding based neural methods (Wang et al., 2015) can suffer from insufficient data. The training is especially difficult for rare words, which"
C16-1180,E14-1068,0,0.559746,"cognition. Moreover, implicit relation recognition calls for semantic understanding, which needs to encode the word meaning in the context and the sentence-level understanding for the argument pairs. Considering the complexity of natural language, the task is quite nontrivial and requires more effective encoding of the arguments. Conventional methods for implicit discourse relation recognition are based on manually specified indicator features, such as bag-of-words, production rules, and other linguistically-informed features (Zhou et al., 2010; Park and Cardie, 2012; Biran and McKeown, 2013; Rutherford and Xue, 2014). Recently, embedding based neural models have been proved effective to address the data sparsity problem that is not well solved in traditional methods. The key techniques include real-valued dense embeddings for feature representations and non-linear neural models for feature combinations and transformations. However, most of the neural models take words as the smallest processing units, which can suffer a lot from insufficient training on rare words. Discourse parsing, as the highest level language processing at present, covers word and sentence levels for feature representation. This work"
C16-1180,N15-1081,0,0.51153,"Missing"
C16-1180,N16-1064,1,0.748355,".. The The w ploy ploy wf e wb worked wf e wb The wf e wb defense wf e wb The wf e wb worked f e w b CNN share parameters The s1 MLP softmax defense worked The defense Arg1 bi-LSTMs CNN ... ... Word-Level Module Enhanced-Embedding Arg2 CNN s2 defense won Word Look-up Table Figure 1: Architecture of the proposed model. Xue (2015) and Ji et al. (2015) add automatically-labeled instances to expand data. Fisher and Simmons (2015) incorporate a mixture of labeled and unlabeled data to reduce the need for annotated data. More recently, neural network models have been proved effective for NLP tasks (Wang et al., 2016; Zhang et al., 2016; Cai and Zhao, 2016; Hu et al., 2016a) and also utilized for implicit discourse relation recognition. Ji and Eisenstein (2015) adopt recursive neural network and incorporated with entityaugmented distributed semantics. Zhang et al. (2015a) propose a simplified neural network which contains only one hidden layer and use three different pooling operations (max, min, average). Chen et al. (2016) adopt a deep gated neural model to capture the semantic interactions between argument pairs. Ji et al. (2016) propose a latent variable recurrent neural network architecture for joint"
C16-1180,K15-2001,0,0.301562,"Missing"
C16-1180,K16-2001,0,0.352439,"rom character, word, to sentence. Through extensive experiments on standard discourse corpus, we analyze several models and show the superiority of the proposed method. The remaining of the paper is organized as follows: Section 2 discusses related work; Section 3 describes the proposed model; Section 4 provides the details of experiments and model analysis; and Section 5 concludes the paper. 2 Related work Implicit discourse relation recognition is the subcomponent of the end-to-end discourse parsing system, which is also used as the share-task in CoNLL 2015 and CoNLL 2016 (Xue et al., 2015; Xue et al., 2016). In the share-task, the classification task concerns other Non-Explicit types including EntRel and AltLex, in addition to the Implicit relations. Early work for implicit discourse relation recognition focuses on typical machine learning solutions with sparse indicator features and linear models. Pitler et al. (2009) use several linguistically informed features, including polarity tags, Levin verb classes and length of verb phrases. Zhou et al (2010) improve the performance through predicting connective words as extra features. Park and Cardie (2012) propose a method using a locally-optimal fe"
C16-1180,D15-1266,0,0.459068,"ords. Discourse parsing, as the highest level language processing at present, covers word and sentence levels for feature representation. This work extends the current wordlevel representation onto more fine-grained character-level which is helpful for encoding morphology information and alleviating the rare word problem. In summary, this paper presents a neural model with context-aware character-enhanced embeddings to address implicit discourse relation recognition task. Recently, character-aware models have been popular for English and other morphologically rich languages (Kim et al., 2016; Zhang et al., 2015b; Ling et al., 2015). The proposed model enhanced the word embeddings with character-aware representations learned from stacked convolutional and recurrent neural models. Utilizing these enhanced embeddings, the model covers information of three levels from character, word, to sentence. Through extensive experiments on standard discourse corpus, we analyze several models and show the superiority of the proposed method. The remaining of the paper is organized as follows: Section 2 discusses related work; Section 3 describes the proposed model; Section 4 provides the details of experiments and"
C16-1180,P16-1131,1,0.71013,"loy wf e wb worked wf e wb The wf e wb defense wf e wb The wf e wb worked f e w b CNN share parameters The s1 MLP softmax defense worked The defense Arg1 bi-LSTMs CNN ... ... Word-Level Module Enhanced-Embedding Arg2 CNN s2 defense won Word Look-up Table Figure 1: Architecture of the proposed model. Xue (2015) and Ji et al. (2015) add automatically-labeled instances to expand data. Fisher and Simmons (2015) incorporate a mixture of labeled and unlabeled data to reduce the need for annotated data. More recently, neural network models have been proved effective for NLP tasks (Wang et al., 2016; Zhang et al., 2016; Cai and Zhao, 2016; Hu et al., 2016a) and also utilized for implicit discourse relation recognition. Ji and Eisenstein (2015) adopt recursive neural network and incorporated with entityaugmented distributed semantics. Zhang et al. (2015a) propose a simplified neural network which contains only one hidden layer and use three different pooling operations (max, min, average). Chen et al. (2016) adopt a deep gated neural model to capture the semantic interactions between argument pairs. Ji et al. (2016) propose a latent variable recurrent neural network architecture for jointly modeling sequence"
C16-1180,C10-2172,0,0.554587,"entations, resulting in high perplexities for discourse relation recognition. Moreover, implicit relation recognition calls for semantic understanding, which needs to encode the word meaning in the context and the sentence-level understanding for the argument pairs. Considering the complexity of natural language, the task is quite nontrivial and requires more effective encoding of the arguments. Conventional methods for implicit discourse relation recognition are based on manually specified indicator features, such as bag-of-words, production rules, and other linguistically-informed features (Zhou et al., 2010; Park and Cardie, 2012; Biran and McKeown, 2013; Rutherford and Xue, 2014). Recently, embedding based neural models have been proved effective to address the data sparsity problem that is not well solved in traditional methods. The key techniques include real-valued dense embeddings for feature representations and non-linear neural models for feature combinations and transformations. However, most of the neural models take words as the smallest processing units, which can suffer a lot from insufficient training on rare words. Discourse parsing, as the highest level language processing at pres"
C16-1295,D11-1033,0,0.575583,"corpora, which are also called in-domain or related-domain corpora, can enhance the performance of SMT effectively. Otherwise the irrelevant additional corpora, which are also called outof-domain corpora, may not benefit SMT (Koehn and Schroeder, 2007). SMT adaptation means selecting useful part from mix-domain (mixture of in-domain and out-ofdomain) data, for SMT performance enhancement. The core task in adaptation is about how to select the useful data. Existing works have considered selection strategies with various granularities, though most of them only focus on sentence-level selection (Axelrod et al., 2011; Banerjee et al., 2012; Duh et al., 2013; Hoang and Sima’an, 2014a; Hoang and Sima’an, 2014b). There is a potential problem for sentence level adaptation: different parts of a sentence may belong to different domains. That is, it is possible that a sentence is overall out-of-domain, although part of it can be in-domain. Therefore a few works consider more granular level for selection. They build lexicon, Translation Models (TMs), reordering models or Language Models (LMs) to select fragment or directly adapt the models (Bellegarda, 2004; Deng et al., 2008; Moore and Lewis, 2010; Foster et al."
C16-1295,2011.iwslt-evaluation.18,0,0.251266,"ion probabilities of connecting phrases calculated by NN can also be used to enhance SMT, and the experimental results will be shown in Section 5.4. 3.3 Integration into SMT The thresholds of Pop and Dminus are tuned using development data. Selected phrase pairs are added into the in-domain PT. Because they are not so useful as the in-domain ones, a penalty score is added. For in-domain phrase pairs, the penalty is set as 1; for the out-of-domain ones the penalty is set as e (= 2.71828...). Other phrase scores (lexical weights et. al.) are used as they are. This penalty setting is similar to (Bisazza et al., 2011). Penalty weights, together with all of existing score weights, will be further tuned by MERT (Och, 2003). The phrase pairs in re-ordering model are selected using the same way as PT. The selected monolingual n-grams are added to the original LM, and the probabilities are re-normalized by SRILM (Stolcke, 2002; Stolcke et al., 2011). 4 Experiments 4.1 Data sets The proposed methods are evaluated on two data sets. 1) IWSLT 2014 French (FR) to English (EN) corpus4 is used as in-domain data and dev2010 and test2010/2011 (Niehues and Waibel, 2012), are selected as development (dev) and test data, r"
C16-1295,P16-1039,1,0.833935,"Missing"
C16-1295,P13-1141,0,0.0645321,"Missing"
C16-1295,N13-1114,0,0.0207534,"Sima’an, 2014a; Hoang and Sima’an, 2014b). There is a potential problem for sentence level adaptation: different parts of a sentence may belong to different domains. That is, it is possible that a sentence is overall out-of-domain, although part of it can be in-domain. Therefore a few works consider more granular level for selection. They build lexicon, Translation Models (TMs), reordering models or Language Models (LMs) to select fragment or directly adapt the models (Bellegarda, 2004; Deng et al., 2008; Moore and Lewis, 2010; Foster et al., 2010; Mansour and Ney, 2013; Carpuat et al., 2013; Chen et al., 2013a; Chen et al., 2013b; Sennrich et al., 2013; Mathur et al., 2014; Shi et al., 2015). One typical example of these methods is to train two Neural Network (NN) models (one from in-domain and the other from out-of-domain) and penalize the sentences/phrases similar to out-of-domain corpora (Duh et al., 2013; Joty et al., 2015; Durrani et al., 2015). As we know, Phrase Based SMT (PBSMT) mainly contains two models: translation model and LM, whose components are bilingual phrase pairs and monolingual n-grams. Meanwhile, most of the above methods enhance SMT performance by adapting single specific mo"
C16-1295,P13-1126,0,0.0188404,"Sima’an, 2014a; Hoang and Sima’an, 2014b). There is a potential problem for sentence level adaptation: different parts of a sentence may belong to different domains. That is, it is possible that a sentence is overall out-of-domain, although part of it can be in-domain. Therefore a few works consider more granular level for selection. They build lexicon, Translation Models (TMs), reordering models or Language Models (LMs) to select fragment or directly adapt the models (Bellegarda, 2004; Deng et al., 2008; Moore and Lewis, 2010; Foster et al., 2010; Mansour and Ney, 2013; Carpuat et al., 2013; Chen et al., 2013a; Chen et al., 2013b; Sennrich et al., 2013; Mathur et al., 2014; Shi et al., 2015). One typical example of these methods is to train two Neural Network (NN) models (one from in-domain and the other from out-of-domain) and penalize the sentences/phrases similar to out-of-domain corpora (Duh et al., 2013; Joty et al., 2015; Durrani et al., 2015). As we know, Phrase Based SMT (PBSMT) mainly contains two models: translation model and LM, whose components are bilingual phrase pairs and monolingual n-grams. Meanwhile, most of the above methods enhance SMT performance by adapting single specific mo"
C16-1295,P08-1010,0,0.178554,"focus on sentence-level selection (Axelrod et al., 2011; Banerjee et al., 2012; Duh et al., 2013; Hoang and Sima’an, 2014a; Hoang and Sima’an, 2014b). There is a potential problem for sentence level adaptation: different parts of a sentence may belong to different domains. That is, it is possible that a sentence is overall out-of-domain, although part of it can be in-domain. Therefore a few works consider more granular level for selection. They build lexicon, Translation Models (TMs), reordering models or Language Models (LMs) to select fragment or directly adapt the models (Bellegarda, 2004; Deng et al., 2008; Moore and Lewis, 2010; Foster et al., 2010; Mansour and Ney, 2013; Carpuat et al., 2013; Chen et al., 2013a; Chen et al., 2013b; Sennrich et al., 2013; Mathur et al., 2014; Shi et al., 2015). One typical example of these methods is to train two Neural Network (NN) models (one from in-domain and the other from out-of-domain) and penalize the sentences/phrases similar to out-of-domain corpora (Duh et al., 2013; Joty et al., 2015; Durrani et al., 2015). As we know, Phrase Based SMT (PBSMT) mainly contains two models: translation model and LM, whose components are bilingual phrase pairs and mono"
C16-1295,P13-2119,0,0.636704,"elated-domain corpora, can enhance the performance of SMT effectively. Otherwise the irrelevant additional corpora, which are also called outof-domain corpora, may not benefit SMT (Koehn and Schroeder, 2007). SMT adaptation means selecting useful part from mix-domain (mixture of in-domain and out-ofdomain) data, for SMT performance enhancement. The core task in adaptation is about how to select the useful data. Existing works have considered selection strategies with various granularities, though most of them only focus on sentence-level selection (Axelrod et al., 2011; Banerjee et al., 2012; Duh et al., 2013; Hoang and Sima’an, 2014a; Hoang and Sima’an, 2014b). There is a potential problem for sentence level adaptation: different parts of a sentence may belong to different domains. That is, it is possible that a sentence is overall out-of-domain, although part of it can be in-domain. Therefore a few works consider more granular level for selection. They build lexicon, Translation Models (TMs), reordering models or Language Models (LMs) to select fragment or directly adapt the models (Bellegarda, 2004; Deng et al., 2008; Moore and Lewis, 2010; Foster et al., 2010; Mansour and Ney, 2013; Carpuat et"
C16-1295,2015.mtsummit-papers.10,0,0.834828,"ild lexicon, Translation Models (TMs), reordering models or Language Models (LMs) to select fragment or directly adapt the models (Bellegarda, 2004; Deng et al., 2008; Moore and Lewis, 2010; Foster et al., 2010; Mansour and Ney, 2013; Carpuat et al., 2013; Chen et al., 2013a; Chen et al., 2013b; Sennrich et al., 2013; Mathur et al., 2014; Shi et al., 2015). One typical example of these methods is to train two Neural Network (NN) models (one from in-domain and the other from out-of-domain) and penalize the sentences/phrases similar to out-of-domain corpora (Duh et al., 2013; Joty et al., 2015; Durrani et al., 2015). As we know, Phrase Based SMT (PBSMT) mainly contains two models: translation model and LM, whose components are bilingual phrase pairs and monolingual n-grams. Meanwhile, most of the above methods enhance SMT performance by adapting single specific model. ∗ Corresponding authors. H. Zhao and B. L. Lu were partially supported by Cai Yuanpei Program (CSC No. 201304490199 and 201304490171), National Natural Science Foundation of China (No. 61672343, 61170114 and 61272248), National Basic Research Program of China (No. 2013CB329401), Major Basic Research Program of Shanghai Science and Technolog"
C16-1295,D10-1044,0,0.0735489,"et al., 2011; Banerjee et al., 2012; Duh et al., 2013; Hoang and Sima’an, 2014a; Hoang and Sima’an, 2014b). There is a potential problem for sentence level adaptation: different parts of a sentence may belong to different domains. That is, it is possible that a sentence is overall out-of-domain, although part of it can be in-domain. Therefore a few works consider more granular level for selection. They build lexicon, Translation Models (TMs), reordering models or Language Models (LMs) to select fragment or directly adapt the models (Bellegarda, 2004; Deng et al., 2008; Moore and Lewis, 2010; Foster et al., 2010; Mansour and Ney, 2013; Carpuat et al., 2013; Chen et al., 2013a; Chen et al., 2013b; Sennrich et al., 2013; Mathur et al., 2014; Shi et al., 2015). One typical example of these methods is to train two Neural Network (NN) models (one from in-domain and the other from out-of-domain) and penalize the sentences/phrases similar to out-of-domain corpora (Duh et al., 2013; Joty et al., 2015; Durrani et al., 2015). As we know, Phrase Based SMT (PBSMT) mainly contains two models: translation model and LM, whose components are bilingual phrase pairs and monolingual n-grams. Meanwhile, most of the abov"
C16-1295,D14-1062,0,0.238959,"Missing"
C16-1295,C14-1182,0,0.301219,"Missing"
C16-1295,D15-1147,0,0.484005,"selection. They build lexicon, Translation Models (TMs), reordering models or Language Models (LMs) to select fragment or directly adapt the models (Bellegarda, 2004; Deng et al., 2008; Moore and Lewis, 2010; Foster et al., 2010; Mansour and Ney, 2013; Carpuat et al., 2013; Chen et al., 2013a; Chen et al., 2013b; Sennrich et al., 2013; Mathur et al., 2014; Shi et al., 2015). One typical example of these methods is to train two Neural Network (NN) models (one from in-domain and the other from out-of-domain) and penalize the sentences/phrases similar to out-of-domain corpora (Duh et al., 2013; Joty et al., 2015; Durrani et al., 2015). As we know, Phrase Based SMT (PBSMT) mainly contains two models: translation model and LM, whose components are bilingual phrase pairs and monolingual n-grams. Meanwhile, most of the above methods enhance SMT performance by adapting single specific model. ∗ Corresponding authors. H. Zhao and B. L. Lu were partially supported by Cai Yuanpei Program (CSC No. 201304490199 and 201304490171), National Natural Science Foundation of China (No. 61672343, 61170114 and 61272248), National Basic Research Program of China (No. 2013CB329401), Major Basic Research Program of Shangha"
C16-1295,W07-0733,0,0.496687,"ed method is evaluated on IWSLT/NIST data sets, and the results show that phrase based SMT performances are significantly improved (up to +1.6 in comparison with phrase based SMT baseline system and +0.9 in comparison with existing methods). 1 Introduction Large corpora are important for Statistical Machine Translation (SMT) training. However only the relevant additional corpora, which are also called in-domain or related-domain corpora, can enhance the performance of SMT effectively. Otherwise the irrelevant additional corpora, which are also called outof-domain corpora, may not benefit SMT (Koehn and Schroeder, 2007). SMT adaptation means selecting useful part from mix-domain (mixture of in-domain and out-ofdomain) data, for SMT performance enhancement. The core task in adaptation is about how to select the useful data. Existing works have considered selection strategies with various granularities, though most of them only focus on sentence-level selection (Axelrod et al., 2011; Banerjee et al., 2012; Duh et al., 2013; Hoang and Sima’an, 2014a; Hoang and Sima’an, 2014b). There is a potential problem for sentence level adaptation: different parts of a sentence may belong to different domains. That is, it i"
C16-1295,W04-3250,0,0.163106,"methods have been applied to a series of NLP tasks, such as Chinese word segmentation and parsing (Cai and Zhao, 2016; Zhang et al., 2016). 4 https://wit3.fbk.eu/mt.php?release=2014-01 5 http://statmt.org/wmt15/translation-task.html 6 http://www.itl.nist.gov/iad/mig/tests/mt/2006/ 3138 4.2 Common Setting The basic settings of IWSLT-2014 FR to EN and NIST-06 CN to EN phrase based translation baseline systems are followed. 5-gram interpolated KN (Kneser and Ney, 1995) LMs are trained. Translation performances are measured by case-insensitive BLEU (Papineni et al., 2002) with significance test (Koehn, 2004) and METEOR (Lavie and Agarwal, 2007). MERT (Och, 2003) (BLEU based) is run three times for each system and the average BLEU/METEOR scores are recorded. 4-layer CSTM (Schwenk, 2012) are applied to NN translation models: phrase length limit is set as seven, shared projection layer of dimension 320 for each word (that is 2240 for seven words), projection layer of dimension 768, hidden layer of dimension 512. The dimensions of input/output layers for both in/out-of-domain CSTMs follows the size of vocabularies of source/target words from in-domain corpora. That is 72K/57K for IWSLT and 149/112K f"
C16-1295,W07-0734,0,0.0163639,"d to a series of NLP tasks, such as Chinese word segmentation and parsing (Cai and Zhao, 2016; Zhang et al., 2016). 4 https://wit3.fbk.eu/mt.php?release=2014-01 5 http://statmt.org/wmt15/translation-task.html 6 http://www.itl.nist.gov/iad/mig/tests/mt/2006/ 3138 4.2 Common Setting The basic settings of IWSLT-2014 FR to EN and NIST-06 CN to EN phrase based translation baseline systems are followed. 5-gram interpolated KN (Kneser and Ney, 1995) LMs are trained. Translation performances are measured by case-insensitive BLEU (Papineni et al., 2002) with significance test (Koehn, 2004) and METEOR (Lavie and Agarwal, 2007). MERT (Och, 2003) (BLEU based) is run three times for each system and the average BLEU/METEOR scores are recorded. 4-layer CSTM (Schwenk, 2012) are applied to NN translation models: phrase length limit is set as seven, shared projection layer of dimension 320 for each word (that is 2240 for seven words), projection layer of dimension 768, hidden layer of dimension 512. The dimensions of input/output layers for both in/out-of-domain CSTMs follows the size of vocabularies of source/target words from in-domain corpora. That is 72K/57K for IWSLT and 149/112K for NIST. Since out-of-domain corpora"
C16-1295,D12-1088,0,0.0610705,"Missing"
C16-1295,N13-1074,0,0.0171997,"ee et al., 2012; Duh et al., 2013; Hoang and Sima’an, 2014a; Hoang and Sima’an, 2014b). There is a potential problem for sentence level adaptation: different parts of a sentence may belong to different domains. That is, it is possible that a sentence is overall out-of-domain, although part of it can be in-domain. Therefore a few works consider more granular level for selection. They build lexicon, Translation Models (TMs), reordering models or Language Models (LMs) to select fragment or directly adapt the models (Bellegarda, 2004; Deng et al., 2008; Moore and Lewis, 2010; Foster et al., 2010; Mansour and Ney, 2013; Carpuat et al., 2013; Chen et al., 2013a; Chen et al., 2013b; Sennrich et al., 2013; Mathur et al., 2014; Shi et al., 2015). One typical example of these methods is to train two Neural Network (NN) models (one from in-domain and the other from out-of-domain) and penalize the sentences/phrases similar to out-of-domain corpora (Duh et al., 2013; Joty et al., 2015; Durrani et al., 2015). As we know, Phrase Based SMT (PBSMT) mainly contains two models: translation model and LM, whose components are bilingual phrase pairs and monolingual n-grams. Meanwhile, most of the above methods enhance SMT p"
C16-1295,C14-1105,0,0.0122047,"problem for sentence level adaptation: different parts of a sentence may belong to different domains. That is, it is possible that a sentence is overall out-of-domain, although part of it can be in-domain. Therefore a few works consider more granular level for selection. They build lexicon, Translation Models (TMs), reordering models or Language Models (LMs) to select fragment or directly adapt the models (Bellegarda, 2004; Deng et al., 2008; Moore and Lewis, 2010; Foster et al., 2010; Mansour and Ney, 2013; Carpuat et al., 2013; Chen et al., 2013a; Chen et al., 2013b; Sennrich et al., 2013; Mathur et al., 2014; Shi et al., 2015). One typical example of these methods is to train two Neural Network (NN) models (one from in-domain and the other from out-of-domain) and penalize the sentences/phrases similar to out-of-domain corpora (Duh et al., 2013; Joty et al., 2015; Durrani et al., 2015). As we know, Phrase Based SMT (PBSMT) mainly contains two models: translation model and LM, whose components are bilingual phrase pairs and monolingual n-grams. Meanwhile, most of the above methods enhance SMT performance by adapting single specific model. ∗ Corresponding authors. H. Zhao and B. L. Lu were partially"
C16-1295,P10-2041,0,0.0842928,"evel selection (Axelrod et al., 2011; Banerjee et al., 2012; Duh et al., 2013; Hoang and Sima’an, 2014a; Hoang and Sima’an, 2014b). There is a potential problem for sentence level adaptation: different parts of a sentence may belong to different domains. That is, it is possible that a sentence is overall out-of-domain, although part of it can be in-domain. Therefore a few works consider more granular level for selection. They build lexicon, Translation Models (TMs), reordering models or Language Models (LMs) to select fragment or directly adapt the models (Bellegarda, 2004; Deng et al., 2008; Moore and Lewis, 2010; Foster et al., 2010; Mansour and Ney, 2013; Carpuat et al., 2013; Chen et al., 2013a; Chen et al., 2013b; Sennrich et al., 2013; Mathur et al., 2014; Shi et al., 2015). One typical example of these methods is to train two Neural Network (NN) models (one from in-domain and the other from out-of-domain) and penalize the sentences/phrases similar to out-of-domain corpora (Duh et al., 2013; Joty et al., 2015; Durrani et al., 2015). As we know, Phrase Based SMT (PBSMT) mainly contains two models: translation model and LM, whose components are bilingual phrase pairs and monolingual n-grams. Meanwh"
C16-1295,2012.iwslt-papers.3,0,0.0240338,"e used as they are. This penalty setting is similar to (Bisazza et al., 2011). Penalty weights, together with all of existing score weights, will be further tuned by MERT (Och, 2003). The phrase pairs in re-ordering model are selected using the same way as PT. The selected monolingual n-grams are added to the original LM, and the probabilities are re-normalized by SRILM (Stolcke, 2002; Stolcke et al., 2011). 4 Experiments 4.1 Data sets The proposed methods are evaluated on two data sets. 1) IWSLT 2014 French (FR) to English (EN) corpus4 is used as in-domain data and dev2010 and test2010/2011 (Niehues and Waibel, 2012), are selected as development (dev) and test data, respectively. Out-of-domain corpora contain Common Crawl, Europarl v7, News Commentary v10 and United Nation (UN) FR-EN parallel corpora5 . 2) NIST 2006 Chinese (CN) to English corpus6 is used as in-domain corpus, which follows the setting of (Wang et al., 2014b) and mainly consists of news and blog texts. Chinese to English UN data set (LDC2013T06) and NTCIR-9 (Goto et al., 2011) patent data are used as out-of-domain bilingual (Bil) parallel corpora. The English patent data in NTCIR-8 (Fujii et al., 2010) is also used as additional out-of-dom"
C16-1295,P03-1021,0,0.0187056,"ts will be shown in Section 5.4. 3.3 Integration into SMT The thresholds of Pop and Dminus are tuned using development data. Selected phrase pairs are added into the in-domain PT. Because they are not so useful as the in-domain ones, a penalty score is added. For in-domain phrase pairs, the penalty is set as 1; for the out-of-domain ones the penalty is set as e (= 2.71828...). Other phrase scores (lexical weights et. al.) are used as they are. This penalty setting is similar to (Bisazza et al., 2011). Penalty weights, together with all of existing score weights, will be further tuned by MERT (Och, 2003). The phrase pairs in re-ordering model are selected using the same way as PT. The selected monolingual n-grams are added to the original LM, and the probabilities are re-normalized by SRILM (Stolcke, 2002; Stolcke et al., 2011). 4 Experiments 4.1 Data sets The proposed methods are evaluated on two data sets. 1) IWSLT 2014 French (FR) to English (EN) corpus4 is used as in-domain data and dev2010 and test2010/2011 (Niehues and Waibel, 2012), are selected as development (dev) and test data, respectively. Out-of-domain corpora contain Common Crawl, Europarl v7, News Commentary v10 and United Nati"
C16-1295,P02-1040,0,0.0955457,"ics on data sets (‘B’ for billions). 3 NN based methods have been applied to a series of NLP tasks, such as Chinese word segmentation and parsing (Cai and Zhao, 2016; Zhang et al., 2016). 4 https://wit3.fbk.eu/mt.php?release=2014-01 5 http://statmt.org/wmt15/translation-task.html 6 http://www.itl.nist.gov/iad/mig/tests/mt/2006/ 3138 4.2 Common Setting The basic settings of IWSLT-2014 FR to EN and NIST-06 CN to EN phrase based translation baseline systems are followed. 5-gram interpolated KN (Kneser and Ney, 1995) LMs are trained. Translation performances are measured by case-insensitive BLEU (Papineni et al., 2002) with significance test (Koehn, 2004) and METEOR (Lavie and Agarwal, 2007). MERT (Och, 2003) (BLEU based) is run three times for each system and the average BLEU/METEOR scores are recorded. 4-layer CSTM (Schwenk, 2012) are applied to NN translation models: phrase length limit is set as seven, shared projection layer of dimension 320 for each word (that is 2240 for seven words), projection layer of dimension 768, hidden layer of dimension 512. The dimensions of input/output layers for both in/out-of-domain CSTMs follows the size of vocabularies of source/target words from in-domain corpora. Tha"
C16-1295,C12-2104,0,0.173123,"and Pout (E|F ) by N N T Mout should be lower. This hypothesis is partially motivated by (Axelrod et al., 2011), which use bilingual cross-entropy difference to distinguish in-domain and out-of-domain data. The translation probability of a phrase-pair is estimated as, P (E|F ) = P (e1 , ..., eq |f1 , ..., fp ), (1) where fs (s ∈ [1, p]) and et (t ∈ [1, q]) are source and target words, respectively. Originally, P (e1 , ..., eq |f1 , ..., fp ) = q ∏ P (ek |e1 , ..., ek−1 , f1 , ...fp ). (2) k=1 The structure of NN based translation model is similar to Continuous Space Translation Model (CSTM) (Schwenk, 2012). For the purpose of adaptation, the dependence between target words is dropped2 and the probabilities of different length target phrase are normalized. For an incomplete source phrase, i.e. with less than seven words, we set the projections of the missing words to zero. The normalized translation probability Q(E|F ) can be approximately computed by the following equation, v u q u∏ q P (ek |f1 , ...fp ). Q(E|F ) ≈ t (3) k=1 Finally, the minus Dminus (E|F ) is used to rank connecting phrase pairs from mix-domain PT, Dminus (E|F ) = Qin (E|F ) − Qin (E|F ). (4) where Qin (E|F ) and Qin (E|F ) ar"
C16-1295,P13-1082,0,0.203536,"). There is a potential problem for sentence level adaptation: different parts of a sentence may belong to different domains. That is, it is possible that a sentence is overall out-of-domain, although part of it can be in-domain. Therefore a few works consider more granular level for selection. They build lexicon, Translation Models (TMs), reordering models or Language Models (LMs) to select fragment or directly adapt the models (Bellegarda, 2004; Deng et al., 2008; Moore and Lewis, 2010; Foster et al., 2010; Mansour and Ney, 2013; Carpuat et al., 2013; Chen et al., 2013a; Chen et al., 2013b; Sennrich et al., 2013; Mathur et al., 2014; Shi et al., 2015). One typical example of these methods is to train two Neural Network (NN) models (one from in-domain and the other from out-of-domain) and penalize the sentences/phrases similar to out-of-domain corpora (Duh et al., 2013; Joty et al., 2015; Durrani et al., 2015). As we know, Phrase Based SMT (PBSMT) mainly contains two models: translation model and LM, whose components are bilingual phrase pairs and monolingual n-grams. Meanwhile, most of the above methods enhance SMT performance by adapting single specific model. ∗ Corresponding authors. H. Zhao and B."
C16-1295,E12-1055,0,0.0662449,"input/output layers for both in/out-of-domain CSTMs follows the size of vocabularies of source/target words from in-domain corpora. That is 72K/57K for IWSLT and 149/112K for NIST. Since out-of-domain corpora are huge, part of them are resampled (resample coefficient 0.01 for IWSLT and NIST). Several related existing methods are selected as baselines7 : Koehn and Schroeder (2007)’s method for using two (in and out-of-domain) TMs and LMs together, entropy based method for TM (Ling et al., 2012) and LM (Stolcke, 1998) adaptation (pruning), (Duh et al., 2013) for NNLM based sentence adaptation, (Sennrich, 2012) for TM weights combination, and (Bisazza et al., 2011) for TM fill-up. In Table Tables 2 and 3, ‘in-domain’, ‘out-of-domain’ and ‘mix-domain’ indicate training all models using corresponding corpora, ‘in+NN’ indicates applying NN based adaptation directly for all phrases, and ‘in+connect’ indicates adding all connecting phrases and n-grams to in-domain PT and LM, respectively. For tuning methods, ‘in+connect+OP/NN’ indicates tuning connecting phrase pairs and n-grams using Occurring Probability (OP) and NN, respectively. Only the best preforming systems (for both the baselines and proposed me"
C16-1295,D14-1023,1,0.921449,"Missing"
C16-1295,P14-2122,1,0.886787,"ails: http://creativecommons.org/licenses/by/4.0/ 3135 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 3135–3145, Osaka, Japan, December 11-17 2016. Instead of focusing on sentence selection or single model adaptation, we propose a phrase adaptation method, which is applied to both bilingual phrase pair and monolingual n-gram selection. It is based on a linguistic observation that the translation hypotheses of a phrase-based SMT system are concatenations of phrases from Phrase Table (PT), which has been applied to LM growing (Wang et al., 2014a; Wang et al., 2015). As a straightforward linear method, it is much efficient in comparison with NN based non-linear methods. The remainder of this paper is organized as follows. Section 2 will introduce the connecting phrase based adaptation method. The size of adapted connecting phrase will be tuned in Section 3. Empirical results will be shown in Section 4. We will discuss the methods and conduct extension experiments in Section 5. The last section will conclude this paper. 2 Connecting Phrase based Adaptation Suppose that two phrases ‘would like to learn’ and ‘Chinese as second language’"
C16-1295,P16-1131,1,0.811994,"Missing"
C16-1295,W14-3348,0,\N,Missing
C16-1295,C12-1010,0,\N,Missing
C18-1038,P15-1034,0,0.024196,"cially the common words (world market) of the question and corresponding answer. Besides, the label in the question is also assigned a high attention, indicating the label works essentially. 4 4.1 Related Work Question Answering Various neural models have been proposed to tackle the tasks of QA task and related knowledge representation (Wang et al., 2017b; Chen et al., 2016b; Todor and Anette, 2018; Kundu and Ng, 2018). Previous work mainly focused on factoid questions, falling into the architecture of knowledge base embedding and question answering from knowledge base (Khashabi et al., 2016; Angeli et al., 2015). Yang et al. (2014) proposed a method that transforms natural questions into their corresponding logical forms and conducted question answering by leveraging semantic associations between lexical representations and knowledge base properties in the latent space. Yin et al. (2016) proposed an end-to-end neural model that can generate answers to simple factoid questions from a knowledge base. Nonetheless, the knowledge base construction requires a lot of human workload, and the related semantic parsing and knowledge representation will become much more complicated as the scale of the knowledge"
C18-1038,C18-1233,1,0.826789,"in total scoring and is extremely difficult in the exams. (Cheng et al., 2016) made a preliminary attempt to take up the Gaokao challenge, trying to solve multiple-choice questions via retrieving and ranking evidence from Wikipedia articles to determine the right choices. Differently, this task is to solve comprehensive questions and has to be based on knowledge representation and semantic computation rather than word form matching in the previous work. Although deep learning methods shine at various natural language processing tasks (Wang et al., 2015; Zhang et al., 2016b; Qin et al., 2017; Cai et al., 2018; Zhang et al., 2018b; Huang et al., 2018; Zhang et al., 2018a; He et al., 2018; Zhang et al., 2018c; Li et al., 2018), they usually rely on a large scale of dataset for effective learning. The concerned task, unfortunately, cannot receive sufficient training data under ordinary circumstances. Different from previous typical QA tasks such as community QA (Zhang et al., 2016a) which can enjoy the advantage of holding a very large known QA pair set, the concerned task is equal to retrieving a proper answer from textbooks organized as plain texts with guidelines of very limited number of known QA"
C18-1038,P16-1163,0,0.0607365,"Missing"
C18-1038,P16-1076,0,0.0201373,"erature and art of ancient Greece and Rome. However, it is actually an ideological liberation movement and a revolution of thoughts from a deeper inspection. Table 1: Comprehensive question examples from Gaokao history exams. Generally speaking, for the Gaokao challenge, knowledge sources are extensive and no sufficient structured dataset is available, while most existing work on knowledge representation focused on structured and semi-structured types (Khot et al., 2017; Khashabi et al., 2016; Vieira, 2016). With regard to the answer retrieval, most current research focused on the factoid QA (Dai et al., 2016; Yin et al., 2016), community QA (Zhang et al., 2017; Lu and Kong, 2017) and episodic QA (Samothrakis et al., 2017; Xiong et al., 2016; Vinyals et al., 2016). Compared with these existing studies, the concerned task is more compositive and more comprehensive and has to be done from unstructured knowledge sources like textbooks. Moreover, the answers of our Gaokao challenge QA task are always a group of detail-riddled sentences with various lengths rather than merely short sentences as focused on in previous work (Yin et al., 2016; Yih et al., 2014). Recent research has turned to semi-supervis"
C18-1038,fujita-etal-2014-overview,0,0.0176216,"tance of extra labels given by a neural turing machine labeler. Empirical study shows that the labeler works well with only a small training dataset and the gated mechanism is good at fetching the semantic representation of lengthy answers. Experiments on question answering demonstrate the proposed model obtains substantial performance gains over various neural model baselines in terms of multiple evaluation metrics. 1 Introduction Teaching a machine to pass admission exams is a hot AI challenge which has aroused a growing number of research (Guo et al., 2017; Cheng et al., 2016; Clark, 2015; Fujita et al., 2014; Henaff et al., 2016). Among these studies, deep question-answering (QA) task (Ferrucci et al., 2010; Wang et al., 2014) is especially difficult, aiming to answer complex questions via deep feature learning from multisource datasets. Recently, a highly challenging deep QA task is from the Chinese University Admission Examination (Gaokao in Chinese), which is a national-wide standard examination for all senior middle school students in China and has been known as its large scale and strictness. This work focuses on comprehensive question-answering in Gaokao history exams as shown in Table 11 ,"
C18-1038,E17-1011,0,0.0506791,"d neural network to retrieve answers with the assistance of extra labels given by a neural turing machine labeler. Empirical study shows that the labeler works well with only a small training dataset and the gated mechanism is good at fetching the semantic representation of lengthy answers. Experiments on question answering demonstrate the proposed model obtains substantial performance gains over various neural model baselines in terms of multiple evaluation metrics. 1 Introduction Teaching a machine to pass admission exams is a hot AI challenge which has aroused a growing number of research (Guo et al., 2017; Cheng et al., 2016; Clark, 2015; Fujita et al., 2014; Henaff et al., 2016). Among these studies, deep question-answering (QA) task (Ferrucci et al., 2010; Wang et al., 2014) is especially difficult, aiming to answer complex questions via deep feature learning from multisource datasets. Recently, a highly challenging deep QA task is from the Chinese University Admission Examination (Gaokao in Chinese), which is a national-wide standard examination for all senior middle school students in China and has been known as its large scale and strictness. This work focuses on comprehensive question-an"
C18-1038,P18-1192,1,0.698167,"made a preliminary attempt to take up the Gaokao challenge, trying to solve multiple-choice questions via retrieving and ranking evidence from Wikipedia articles to determine the right choices. Differently, this task is to solve comprehensive questions and has to be based on knowledge representation and semantic computation rather than word form matching in the previous work. Although deep learning methods shine at various natural language processing tasks (Wang et al., 2015; Zhang et al., 2016b; Qin et al., 2017; Cai et al., 2018; Zhang et al., 2018b; Huang et al., 2018; Zhang et al., 2018a; He et al., 2018; Zhang et al., 2018c; Li et al., 2018), they usually rely on a large scale of dataset for effective learning. The concerned task, unfortunately, cannot receive sufficient training data under ordinary circumstances. Different from previous typical QA tasks such as community QA (Zhang et al., 2016a) which can enjoy the advantage of holding a very large known QA pair set, the concerned task is equal to retrieving a proper answer from textbooks organized as plain texts with guidelines of very limited number of known QA pairs. In addition, the questions are usually given in a quite indirect way to"
C18-1038,P18-4024,1,0.802256,"icult in the exams. (Cheng et al., 2016) made a preliminary attempt to take up the Gaokao challenge, trying to solve multiple-choice questions via retrieving and ranking evidence from Wikipedia articles to determine the right choices. Differently, this task is to solve comprehensive questions and has to be based on knowledge representation and semantic computation rather than word form matching in the previous work. Although deep learning methods shine at various natural language processing tasks (Wang et al., 2015; Zhang et al., 2016b; Qin et al., 2017; Cai et al., 2018; Zhang et al., 2018b; Huang et al., 2018; Zhang et al., 2018a; He et al., 2018; Zhang et al., 2018c; Li et al., 2018), they usually rely on a large scale of dataset for effective learning. The concerned task, unfortunately, cannot receive sufficient training data under ordinary circumstances. Different from previous typical QA tasks such as community QA (Zhang et al., 2016a) which can enjoy the advantage of holding a very large known QA pair set, the concerned task is equal to retrieving a proper answer from textbooks organized as plain texts with guidelines of very limited number of known QA pairs. In addition, the questions are us"
C18-1038,P17-2049,0,0.024344,"复兴古希腊罗马时期的哲学、文学和艺术的活 动，是一种复古运动，但从其深层的含义看，它却是一场思想解放运动，是一次思想领域里 的变革。 The Renaissance is seemingly regarded as a retro movement of philosophy, literature and art of ancient Greece and Rome. However, it is actually an ideological liberation movement and a revolution of thoughts from a deeper inspection. Table 1: Comprehensive question examples from Gaokao history exams. Generally speaking, for the Gaokao challenge, knowledge sources are extensive and no sufficient structured dataset is available, while most existing work on knowledge representation focused on structured and semi-structured types (Khot et al., 2017; Khashabi et al., 2016; Vieira, 2016). With regard to the answer retrieval, most current research focused on the factoid QA (Dai et al., 2016; Yin et al., 2016), community QA (Zhang et al., 2017; Lu and Kong, 2017) and episodic QA (Samothrakis et al., 2017; Xiong et al., 2016; Vinyals et al., 2016). Compared with these existing studies, the concerned task is more compositive and more comprehensive and has to be done from unstructured knowledge sources like textbooks. Moreover, the answers of our Gaokao challenge QA task are always a group of detail-riddled sentences with various lengths rathe"
C18-1038,N16-1153,0,0.314895,"et al., 2016; Vinyals et al., 2016). Compared with these existing studies, the concerned task is more compositive and more comprehensive and has to be done from unstructured knowledge sources like textbooks. Moreover, the answers of our Gaokao challenge QA task are always a group of detail-riddled sentences with various lengths rather than merely short sentences as focused on in previous work (Yin et al., 2016; Yih et al., 2014). Recent research has turned to semi-supervised methods to leverage unlabeled texts to enhance the performance of QA tasks via deep neural networks (Yang et al., 2017; Lei et al., 2016). This task is somewhat different from previous ones that the expected extra labels are difficult to be annotated and the entire unlabeled data is kept in a very small scale, so that semi-supervised methods cannot be conveniently applied. Notably, one-shot learning has been proved effective for image recognition with few samples (Li et al., 2006), which is a strategy similar to people learning concept. As an implementation of one-shot learning, neural turing machine (NTM) was proposed (Santoro et al., 2016; Vinyals et al., 2016) and showed great potential by learning effective features from a"
C18-1038,C18-1271,1,0.579028,"the Gaokao challenge, trying to solve multiple-choice questions via retrieving and ranking evidence from Wikipedia articles to determine the right choices. Differently, this task is to solve comprehensive questions and has to be based on knowledge representation and semantic computation rather than word form matching in the previous work. Although deep learning methods shine at various natural language processing tasks (Wang et al., 2015; Zhang et al., 2016b; Qin et al., 2017; Cai et al., 2018; Zhang et al., 2018b; Huang et al., 2018; Zhang et al., 2018a; He et al., 2018; Zhang et al., 2018c; Li et al., 2018), they usually rely on a large scale of dataset for effective learning. The concerned task, unfortunately, cannot receive sufficient training data under ordinary circumstances. Different from previous typical QA tasks such as community QA (Zhang et al., 2016a) which can enjoy the advantage of holding a very large known QA pair set, the concerned task is equal to retrieving a proper answer from textbooks organized as plain texts with guidelines of very limited number of known QA pairs. In addition, the questions are usually given in a quite indirect way to ask students to dig the exactly expect"
C18-1038,P17-1093,1,0.838653,"e major proportion in total scoring and is extremely difficult in the exams. (Cheng et al., 2016) made a preliminary attempt to take up the Gaokao challenge, trying to solve multiple-choice questions via retrieving and ranking evidence from Wikipedia articles to determine the right choices. Differently, this task is to solve comprehensive questions and has to be based on knowledge representation and semantic computation rather than word form matching in the previous work. Although deep learning methods shine at various natural language processing tasks (Wang et al., 2015; Zhang et al., 2016b; Qin et al., 2017; Cai et al., 2018; Zhang et al., 2018b; Huang et al., 2018; Zhang et al., 2018a; He et al., 2018; Zhang et al., 2018c; Li et al., 2018), they usually rely on a large scale of dataset for effective learning. The concerned task, unfortunately, cannot receive sufficient training data under ordinary circumstances. Different from previous typical QA tasks such as community QA (Zhang et al., 2016a) which can enjoy the advantage of holding a very large known QA pair set, the concerned task is equal to retrieving a proper answer from textbooks organized as plain texts with guidelines of very limited"
C18-1038,D16-1264,0,0.0587089,"to represent QA pairs and compare the distance in vector space to match answer text (Tan et al., 2015; Feng et al., 2015). However, the performance of these neural models greatly depends on a large amount of labeled data. Our concerned task cannot be simply regarded as either factoid or non-factoid. In fact, questions in our task consist of both factoid and non-factoid ones with purely unstructured corresponding answers. Conventional networks might not be sufficient to represent these QA pairs and we need to seek more powerful models. A recent hot comprehensive QA task is the SQuAD challenge (Rajpurkar et al., 2016) which aims to find a text span from given paragraph to answer the question. However, our task is quite different from SQuAD. With sufficient learning data, neural models have shown satisfying performance in the SQuAD challenge. In real world practice, examinees are acquired to learn and comprehend metaknowledge from textbooks so as to pass the exams through review and reasoning among the whole knowledge space instead of simply finding an answer span from a given paragraph, which shows to be challenging for neural networks. 4.2 Semi-supervised Learning For real-world applications of specific d"
C18-1038,P17-1018,0,0.197811,"answer retrieval, and NTM as one-shot learning module for extra labeling. As shown in Figure 2, we use NTM to classify the question type and annotate the corresponding label. Then, the concatenated label vectors and question representation are fed to CGNN for jointly scoring candidate answers. 2.1 Main framework For the QA task, the key problem is how to effectively capture the semantic connections between a question and the corresponding answer. However, the questions and answers are lengthy noisy, resulting in poor feature extraction. Inspired by the success of the popular gated mechanism (Wang et al., 2017a; Chen et al., 2016a; Lei et al., 2016) and the gradient-easing Highway Network (Srivastava et al., 2015), a CGNN is proposed to score the correspondence of inputted QA pairs. The architecture is shown in Figure 3. Embedding Our model reads each word of questions and answers as the input. For an input sequence, the embedding is represented as M ∈ Rd×n where d is dimension of word vector and n is the maximum length. When using the NTM module for question type labeling, the question embedding will be refined as the concatenation of the label vectors and its original embedding. Considering the c"
C18-1038,D14-1071,0,0.0209484,"s (world market) of the question and corresponding answer. Besides, the label in the question is also assigned a high attention, indicating the label works essentially. 4 4.1 Related Work Question Answering Various neural models have been proposed to tackle the tasks of QA task and related knowledge representation (Wang et al., 2017b; Chen et al., 2016b; Todor and Anette, 2018; Kundu and Ng, 2018). Previous work mainly focused on factoid questions, falling into the architecture of knowledge base embedding and question answering from knowledge base (Khashabi et al., 2016; Angeli et al., 2015). Yang et al. (2014) proposed a method that transforms natural questions into their corresponding logical forms and conducted question answering by leveraging semantic associations between lexical representations and knowledge base properties in the latent space. Yin et al. (2016) proposed an end-to-end neural model that can generate answers to simple factoid questions from a knowledge base. Nonetheless, the knowledge base construction requires a lot of human workload, and the related semantic parsing and knowledge representation will become much more complicated as the scale of the knowledge base increases. 457"
C18-1038,P17-1096,0,0.128732,"t al., 2017; Xiong et al., 2016; Vinyals et al., 2016). Compared with these existing studies, the concerned task is more compositive and more comprehensive and has to be done from unstructured knowledge sources like textbooks. Moreover, the answers of our Gaokao challenge QA task are always a group of detail-riddled sentences with various lengths rather than merely short sentences as focused on in previous work (Yin et al., 2016; Yih et al., 2014). Recent research has turned to semi-supervised methods to leverage unlabeled texts to enhance the performance of QA tasks via deep neural networks (Yang et al., 2017; Lei et al., 2016). This task is somewhat different from previous ones that the expected extra labels are difficult to be annotated and the entire unlabeled data is kept in a very small scale, so that semi-supervised methods cannot be conveniently applied. Notably, one-shot learning has been proved effective for image recognition with few samples (Li et al., 2006), which is a strategy similar to people learning concept. As an implementation of one-shot learning, neural turing machine (NTM) was proposed (Santoro et al., 2016; Vinyals et al., 2016) and showed great potential by learning effecti"
C18-1038,P14-2105,0,0.0245112,"t current research focused on the factoid QA (Dai et al., 2016; Yin et al., 2016), community QA (Zhang et al., 2017; Lu and Kong, 2017) and episodic QA (Samothrakis et al., 2017; Xiong et al., 2016; Vinyals et al., 2016). Compared with these existing studies, the concerned task is more compositive and more comprehensive and has to be done from unstructured knowledge sources like textbooks. Moreover, the answers of our Gaokao challenge QA task are always a group of detail-riddled sentences with various lengths rather than merely short sentences as focused on in previous work (Yin et al., 2016; Yih et al., 2014). Recent research has turned to semi-supervised methods to leverage unlabeled texts to enhance the performance of QA tasks via deep neural networks (Yang et al., 2017; Lei et al., 2016). This task is somewhat different from previous ones that the expected extra labels are difficult to be annotated and the entire unlabeled data is kept in a very small scale, so that semi-supervised methods cannot be conveniently applied. Notably, one-shot learning has been proved effective for image recognition with few samples (Li et al., 2006), which is a strategy similar to people learning concept. As an imp"
C18-1038,W16-0106,0,0.0929773,"ancient Greece and Rome. However, it is actually an ideological liberation movement and a revolution of thoughts from a deeper inspection. Table 1: Comprehensive question examples from Gaokao history exams. Generally speaking, for the Gaokao challenge, knowledge sources are extensive and no sufficient structured dataset is available, while most existing work on knowledge representation focused on structured and semi-structured types (Khot et al., 2017; Khashabi et al., 2016; Vieira, 2016). With regard to the answer retrieval, most current research focused on the factoid QA (Dai et al., 2016; Yin et al., 2016), community QA (Zhang et al., 2017; Lu and Kong, 2017) and episodic QA (Samothrakis et al., 2017; Xiong et al., 2016; Vinyals et al., 2016). Compared with these existing studies, the concerned task is more compositive and more comprehensive and has to be done from unstructured knowledge sources like textbooks. Moreover, the answers of our Gaokao challenge QA task are always a group of detail-riddled sentences with various lengths rather than merely short sentences as focused on in previous work (Yin et al., 2016; Yih et al., 2014). Recent research has turned to semi-supervised methods to lever"
C18-1038,P16-1131,1,0.784202,"which accounts for the major proportion in total scoring and is extremely difficult in the exams. (Cheng et al., 2016) made a preliminary attempt to take up the Gaokao challenge, trying to solve multiple-choice questions via retrieving and ranking evidence from Wikipedia articles to determine the right choices. Differently, this task is to solve comprehensive questions and has to be based on knowledge representation and semantic computation rather than word form matching in the previous work. Although deep learning methods shine at various natural language processing tasks (Wang et al., 2015; Zhang et al., 2016b; Qin et al., 2017; Cai et al., 2018; Zhang et al., 2018b; Huang et al., 2018; Zhang et al., 2018a; He et al., 2018; Zhang et al., 2018c; Li et al., 2018), they usually rely on a large scale of dataset for effective learning. The concerned task, unfortunately, cannot receive sufficient training data under ordinary circumstances. Different from previous typical QA tasks such as community QA (Zhang et al., 2016a) which can enjoy the advantage of holding a very large known QA pair set, the concerned task is equal to retrieving a proper answer from textbooks organized as plain texts with guidelin"
C18-1038,C18-1153,1,0.713175,"and is extremely difficult in the exams. (Cheng et al., 2016) made a preliminary attempt to take up the Gaokao challenge, trying to solve multiple-choice questions via retrieving and ranking evidence from Wikipedia articles to determine the right choices. Differently, this task is to solve comprehensive questions and has to be based on knowledge representation and semantic computation rather than word form matching in the previous work. Although deep learning methods shine at various natural language processing tasks (Wang et al., 2015; Zhang et al., 2016b; Qin et al., 2017; Cai et al., 2018; Zhang et al., 2018b; Huang et al., 2018; Zhang et al., 2018a; He et al., 2018; Zhang et al., 2018c; Li et al., 2018), they usually rely on a large scale of dataset for effective learning. The concerned task, unfortunately, cannot receive sufficient training data under ordinary circumstances. Different from previous typical QA tasks such as community QA (Zhang et al., 2016a) which can enjoy the advantage of holding a very large known QA pair set, the concerned task is equal to retrieving a proper answer from textbooks organized as plain texts with guidelines of very limited number of known QA pairs. In addition,"
C18-1038,S18-1147,1,0.681819,"and is extremely difficult in the exams. (Cheng et al., 2016) made a preliminary attempt to take up the Gaokao challenge, trying to solve multiple-choice questions via retrieving and ranking evidence from Wikipedia articles to determine the right choices. Differently, this task is to solve comprehensive questions and has to be based on knowledge representation and semantic computation rather than word form matching in the previous work. Although deep learning methods shine at various natural language processing tasks (Wang et al., 2015; Zhang et al., 2016b; Qin et al., 2017; Cai et al., 2018; Zhang et al., 2018b; Huang et al., 2018; Zhang et al., 2018a; He et al., 2018; Zhang et al., 2018c; Li et al., 2018), they usually rely on a large scale of dataset for effective learning. The concerned task, unfortunately, cannot receive sufficient training data under ordinary circumstances. Different from previous typical QA tasks such as community QA (Zhang et al., 2016a) which can enjoy the advantage of holding a very large known QA pair set, the concerned task is equal to retrieving a proper answer from textbooks organized as plain texts with guidelines of very limited number of known QA pairs. In addition,"
C18-1038,C18-1317,1,0.601338,"and is extremely difficult in the exams. (Cheng et al., 2016) made a preliminary attempt to take up the Gaokao challenge, trying to solve multiple-choice questions via retrieving and ranking evidence from Wikipedia articles to determine the right choices. Differently, this task is to solve comprehensive questions and has to be based on knowledge representation and semantic computation rather than word form matching in the previous work. Although deep learning methods shine at various natural language processing tasks (Wang et al., 2015; Zhang et al., 2016b; Qin et al., 2017; Cai et al., 2018; Zhang et al., 2018b; Huang et al., 2018; Zhang et al., 2018a; He et al., 2018; Zhang et al., 2018c; Li et al., 2018), they usually rely on a large scale of dataset for effective learning. The concerned task, unfortunately, cannot receive sufficient training data under ordinary circumstances. Different from previous typical QA tasks such as community QA (Zhang et al., 2016a) which can enjoy the advantage of holding a very large known QA pair set, the concerned task is equal to retrieving a proper answer from textbooks organized as plain texts with guidelines of very limited number of known QA pairs. In addition,"
C18-1038,W06-0127,1,0.773232,"tion, all our source for the answer should come from standard textbook which contains unstructured plain texts as we initialized our system building. The corpus is from the standard history textbook2 . First, to compose an answer set, 1,929 text fragments3 were extracted by human experts. Then equal numbers of questions were collected from past Gaokao exam papers, and their answers were manually assigned from the answer set to give 1,929 annotated QA pairs, which were equally split for training and test. All text fragments in either questions or answers are segmented into words using BaseSeg (Zhao et al., 2006). We publish it to research communities to facilitate the research 4 . Data statistics are in Table 2. Embedding CGNN NTM Max number of words Word embedding size Hidden unit number Initial learning rate Regularization Dropout Filter Width Controller size Read heads Memory shape Initial learning rate n = 100 d = 200 h = 200 lr = 10−3 ∇ = 10−5 p = 0.2 k=4 s = 200 r=4 m = (128, 100) lr = 10−3 Table 3: Hyper-parameters of our model. 2 Standard Middle-school History Textbook (Vol. 1-3), published by the People’s Education Press in May, 2015. 1,929 is the number of all must-to-be-mastered history fa"
C18-1048,D15-1075,0,0.0182948,"entation is a key component in many NLP tasks. Usually, better representation means better performance. Plenty of work on language modeling has been done, as language modeling can supply better sentence representations. Since the pioneering work of Bengio et al. (2006), neural language models have been well developed (Mikolov and Zweig, 2012; Williams et al., 2015; Kim et al., 2016). Sentence representation is directly handled in a series of work. Lin et al. (2017) used self attention mechanism and used matrix to represent sentence, and Conneau et al. (2017) used encoders pre-trained on SNLI (Bowman et al., 2015) and MultiNLI (Williams et al., 2017). Different from all the existing work, for the first time to our best knowledge, this work is devoted to an empirical study on different levels of representation enhancement for implicit discourse relation classification task. 3 3.1 Model Overview Figure 1 illustrates an overview of our model, which is mainly consisted of three parts: word-level module, sentence-level module, and pair-level module. Token sequences of sentence pairs (Arg1 and Arg2) are encoded by word-level module first and every token becomes a word embedding augmented by subword and ELMo."
C18-1048,D15-1262,0,0.0893515,"f the task by empirically evaluating the impact of surface features. Feature based methods (Pitler et al., 2009; Zhou et al., 2010; Chen et al., 2015; Li et al., 2016) mainly focused on using linguistic, or semantic features from the discourse units, or the relations between unit pairs and word pairs. Zhang et al. (2015) is the first one who modeled this task using end-to-end neural network and gained great performance improvement. Neural network methods also used by lots of works (Zhang et al., 2016; Cai and Zhao, 2016) for better performance. Since then, a lot of methods have been proposed. Braud and Denis (2015) found that word embeddings trained by neural networks is very useful to this task. Qin et al. (2016a) augmented their system with character-level and contextualized embeddings. Recurrent networks and convolutional networks have been used as basic blocks in many works (Ji et al., 2016; R¨onnqvist et al., 2017; Qin et al., 2016b). Ji and Eisenstein (2015) used recursive neural networks. Attention mechanism was used by Liu and Li (2016), Cai and Zhao (2017) and others. Wu et al. (2016) and Lan et al. (2017) applied multi-task component. Qin et al. (2017) utilized adversarial nets to migrate the"
C18-1048,P16-1039,1,0.857182,"ed task. Lin et al. (2009) is the first work who considered the second-level classification of the task by empirically evaluating the impact of surface features. Feature based methods (Pitler et al., 2009; Zhou et al., 2010; Chen et al., 2015; Li et al., 2016) mainly focused on using linguistic, or semantic features from the discourse units, or the relations between unit pairs and word pairs. Zhang et al. (2015) is the first one who modeled this task using end-to-end neural network and gained great performance improvement. Neural network methods also used by lots of works (Zhang et al., 2016; Cai and Zhao, 2016) for better performance. Since then, a lot of methods have been proposed. Braud and Denis (2015) found that word embeddings trained by neural networks is very useful to this task. Qin et al. (2016a) augmented their system with character-level and contextualized embeddings. Recurrent networks and convolutional networks have been used as basic blocks in many works (Ji et al., 2016; R¨onnqvist et al., 2017; Qin et al., 2016b). Ji and Eisenstein (2015) used recursive neural networks. Attention mechanism was used by Liu and Li (2016), Cai and Zhao (2017) and others. Wu et al. (2016) and Lan et al."
C18-1048,K15-2005,1,0.864693,"and demonstrate state-of-the-art performance to verify its effectiveness. This paper is organized as follows. Section 2 reviews related work. Section 3 introduces our model. Section 4 shows our experiments and analyses the results. Section 5 concludes this work. 2 Related Work After the release of Penn Discourse Treebank 2.0, many works have been made to solve this concerned task. Lin et al. (2009) is the first work who considered the second-level classification of the task by empirically evaluating the impact of surface features. Feature based methods (Pitler et al., 2009; Zhou et al., 2010; Chen et al., 2015; Li et al., 2016) mainly focused on using linguistic, or semantic features from the discourse units, or the relations between unit pairs and word pairs. Zhang et al. (2015) is the first one who modeled this task using end-to-end neural network and gained great performance improvement. Neural network methods also used by lots of works (Zhang et al., 2016; Cai and Zhao, 2016) for better performance. Since then, a lot of methods have been proposed. Braud and Denis (2015) found that word embeddings trained by neural networks is very useful to this task. Qin et al. (2016a) augmented their system w"
C18-1048,P16-1163,0,0.432125,"Missing"
C18-1048,D14-1179,0,0.010973,"Missing"
C18-1048,D17-1070,0,0.0338858,"nnective-based features to implicit ones. Sentence representation is a key component in many NLP tasks. Usually, better representation means better performance. Plenty of work on language modeling has been done, as language modeling can supply better sentence representations. Since the pioneering work of Bengio et al. (2006), neural language models have been well developed (Mikolov and Zweig, 2012; Williams et al., 2015; Kim et al., 2016). Sentence representation is directly handled in a series of work. Lin et al. (2017) used self attention mechanism and used matrix to represent sentence, and Conneau et al. (2017) used encoders pre-trained on SNLI (Bowman et al., 2015) and MultiNLI (Williams et al., 2017). Different from all the existing work, for the first time to our best knowledge, this work is devoted to an empirical study on different levels of representation enhancement for implicit discourse relation classification task. 3 3.1 Model Overview Figure 1 illustrates an overview of our model, which is mainly consisted of three parts: word-level module, sentence-level module, and pair-level module. Token sequences of sentence pairs (Arg1 and Arg2) are encoded by word-level module first and every token"
C18-1048,P18-1192,1,0.820321,"· · , Cn ) Finally, the K outputs are concatenated, u = [u1 ; u2 ; · · · ; uK ] ∈ Rds to feed a highway network (Srivastava et al., 2015), g = σ(Wg uT + bg ) ∈ Rds esi = g ReLU (Wh uT + bh ) + (1 − g) u ∈ Rds (2) where g denotes the gate, and Wg ∈ Rds ×ds , bg ∈ Rds , Wh ∈ Rds ×ds , bh ∈ Rds are parameters. is element-wise multiplication. The above Eq. 2 gives the subword-level embedding for the i-th word. ELMo ELMo (Embeddings from Language Models) (Peters et al., 2018) is a pre-trained contextualized word embeddings involving character-level representation. It is shown useful in some works (He et al., 2018; Lee et al., 2018). This embedding is trained by bidirectional language models on large corpus using character sequence for each word token as input. The ELMo encoder employs CNN and highway networks over characters, whose output is given to a multiple-layer biLSTM with residual connections. Then the output is contextualized embeddings for each word. It is also can be seen as a hybrid encoder for character, word, and sentence. This encoder can add lots of contextual information to each word, and can ease the semantics learning of the model. 574 For the pre-trained ELMo encoder, the output is"
C18-1048,P14-1092,0,0.0894495,"Missing"
C18-1048,Q15-1024,0,0.418555,"is task using end-to-end neural network and gained great performance improvement. Neural network methods also used by lots of works (Zhang et al., 2016; Cai and Zhao, 2016) for better performance. Since then, a lot of methods have been proposed. Braud and Denis (2015) found that word embeddings trained by neural networks is very useful to this task. Qin et al. (2016a) augmented their system with character-level and contextualized embeddings. Recurrent networks and convolutional networks have been used as basic blocks in many works (Ji et al., 2016; R¨onnqvist et al., 2017; Qin et al., 2016b). Ji and Eisenstein (2015) used recursive neural networks. Attention mechanism was used by Liu and Li (2016), Cai and Zhao (2017) and others. Wu et al. (2016) and Lan et al. (2017) applied multi-task component. Qin et al. (2017) utilized adversarial nets to migrate the connective-based features to implicit ones. Sentence representation is a key component in many NLP tasks. Usually, better representation means better performance. Plenty of work on language modeling has been done, as language modeling can supply better sentence representations. Since the pioneering work of Bengio et al. (2006), neural language models hav"
C18-1048,N16-1037,0,0.0177383,"ord pairs. Zhang et al. (2015) is the first one who modeled this task using end-to-end neural network and gained great performance improvement. Neural network methods also used by lots of works (Zhang et al., 2016; Cai and Zhao, 2016) for better performance. Since then, a lot of methods have been proposed. Braud and Denis (2015) found that word embeddings trained by neural networks is very useful to this task. Qin et al. (2016a) augmented their system with character-level and contextualized embeddings. Recurrent networks and convolutional networks have been used as basic blocks in many works (Ji et al., 2016; R¨onnqvist et al., 2017; Qin et al., 2016b). Ji and Eisenstein (2015) used recursive neural networks. Attention mechanism was used by Liu and Li (2016), Cai and Zhao (2017) and others. Wu et al. (2016) and Lan et al. (2017) applied multi-task component. Qin et al. (2017) utilized adversarial nets to migrate the connective-based features to implicit ones. Sentence representation is a key component in many NLP tasks. Usually, better representation means better performance. Plenty of work on language modeling has been done, as language modeling can supply better sentence representations. Since"
C18-1048,D17-1134,0,0.74878,"Zhao, 2016) for better performance. Since then, a lot of methods have been proposed. Braud and Denis (2015) found that word embeddings trained by neural networks is very useful to this task. Qin et al. (2016a) augmented their system with character-level and contextualized embeddings. Recurrent networks and convolutional networks have been used as basic blocks in many works (Ji et al., 2016; R¨onnqvist et al., 2017; Qin et al., 2016b). Ji and Eisenstein (2015) used recursive neural networks. Attention mechanism was used by Liu and Li (2016), Cai and Zhao (2017) and others. Wu et al. (2016) and Lan et al. (2017) applied multi-task component. Qin et al. (2017) utilized adversarial nets to migrate the connective-based features to implicit ones. Sentence representation is a key component in many NLP tasks. Usually, better representation means better performance. Plenty of work on language modeling has been done, as language modeling can supply better sentence representations. Since the pioneering work of Bengio et al. (2006), neural language models have been well developed (Mikolov and Zweig, 2012; Williams et al., 2015; Kim et al., 2016). Sentence representation is directly handled in a series of work."
C18-1048,N18-2108,0,0.0189489,"y, the K outputs are concatenated, u = [u1 ; u2 ; · · · ; uK ] ∈ Rds to feed a highway network (Srivastava et al., 2015), g = σ(Wg uT + bg ) ∈ Rds esi = g ReLU (Wh uT + bh ) + (1 − g) u ∈ Rds (2) where g denotes the gate, and Wg ∈ Rds ×ds , bg ∈ Rds , Wh ∈ Rds ×ds , bh ∈ Rds are parameters. is element-wise multiplication. The above Eq. 2 gives the subword-level embedding for the i-th word. ELMo ELMo (Embeddings from Language Models) (Peters et al., 2018) is a pre-trained contextualized word embeddings involving character-level representation. It is shown useful in some works (He et al., 2018; Lee et al., 2018). This embedding is trained by bidirectional language models on large corpus using character sequence for each word token as input. The ELMo encoder employs CNN and highway networks over characters, whose output is given to a multiple-layer biLSTM with residual connections. Then the output is contextualized embeddings for each word. It is also can be seen as a hybrid encoder for character, word, and sentence. This encoder can add lots of contextual information to each word, and can ease the semantics learning of the model. 574 For the pre-trained ELMo encoder, the output is the result of the l"
C18-1048,P14-2047,0,0.0650219,"Missing"
C18-1048,K16-2008,1,0.855617,"te-of-the-art performance to verify its effectiveness. This paper is organized as follows. Section 2 reviews related work. Section 3 introduces our model. Section 4 shows our experiments and analyses the results. Section 5 concludes this work. 2 Related Work After the release of Penn Discourse Treebank 2.0, many works have been made to solve this concerned task. Lin et al. (2009) is the first work who considered the second-level classification of the task by empirically evaluating the impact of surface features. Feature based methods (Pitler et al., 2009; Zhou et al., 2010; Chen et al., 2015; Li et al., 2016) mainly focused on using linguistic, or semantic features from the discourse units, or the relations between unit pairs and word pairs. Zhang et al. (2015) is the first one who modeled this task using end-to-end neural network and gained great performance improvement. Neural network methods also used by lots of works (Zhang et al., 2016; Cai and Zhao, 2016) for better performance. Since then, a lot of methods have been proposed. Braud and Denis (2015) found that word embeddings trained by neural networks is very useful to this task. Qin et al. (2016a) augmented their system with character-leve"
C18-1048,D09-1036,0,0.687795,"g, implicit discourse relation recognition task is to find the relation between two spans without explicit connectives (e.g., but, so, etc.), and needs recovering the relation from semantic understanding of texts. The Penn Discourse Treebank 2.0 (PDTB 2.0) (Prasad et al., 2008) is a benchmark corpus for discourse relations. In PDTB style, the connectives can be explicit or implicit, and one entry of the data is separated into Arg1 and Arg2, accompanied with a relation sense. Since the release of PDTB 2.0 dataset, many methods have been proposed, ranging from traditional feature-based methods (Lin et al., 2009; Pitler et al., 2009) to latest neural-based methods (Qin et al., 2017; Lan et al., 2017). Especially through many neural network methods used for this task such as convolutional neural network (CNN) (Qin et al., 2016b), recursive neural network (Ji and Eisenstein, 2015), embedding improvement (Wu et al., 2017), attention mechanism (Liu and Li, 2016), gate mechanism (Chen et al., 2016), multi-task method (Lan et al., 2017), the performance of this task has improved a lot since it was first introduced. However, this task is still very challenging with the highest reported accuracy still lower"
C18-1048,D16-1130,0,0.369612,"network methods also used by lots of works (Zhang et al., 2016; Cai and Zhao, 2016) for better performance. Since then, a lot of methods have been proposed. Braud and Denis (2015) found that word embeddings trained by neural networks is very useful to this task. Qin et al. (2016a) augmented their system with character-level and contextualized embeddings. Recurrent networks and convolutional networks have been used as basic blocks in many works (Ji et al., 2016; R¨onnqvist et al., 2017; Qin et al., 2016b). Ji and Eisenstein (2015) used recursive neural networks. Attention mechanism was used by Liu and Li (2016), Cai and Zhao (2017) and others. Wu et al. (2016) and Lan et al. (2017) applied multi-task component. Qin et al. (2017) utilized adversarial nets to migrate the connective-based features to implicit ones. Sentence representation is a key component in many NLP tasks. Usually, better representation means better performance. Plenty of work on language modeling has been done, as language modeling can supply better sentence representations. Since the pioneering work of Bengio et al. (2006), neural language models have been well developed (Mikolov and Zweig, 2012; Williams et al., 2015; Kim et al.,"
C18-1048,N18-1202,0,0.0718735,"ings sej to sej+ki −1 is   Cj = tanh Convi [sej : sej+ki −1 ] The final output of Convi after max pooling is ui = maxpool (C1 , · · · , Cj , · · · , Cn ) Finally, the K outputs are concatenated, u = [u1 ; u2 ; · · · ; uK ] ∈ Rds to feed a highway network (Srivastava et al., 2015), g = σ(Wg uT + bg ) ∈ Rds esi = g ReLU (Wh uT + bh ) + (1 − g) u ∈ Rds (2) where g denotes the gate, and Wg ∈ Rds ×ds , bg ∈ Rds , Wh ∈ Rds ×ds , bh ∈ Rds are parameters. is element-wise multiplication. The above Eq. 2 gives the subword-level embedding for the i-th word. ELMo ELMo (Embeddings from Language Models) (Peters et al., 2018) is a pre-trained contextualized word embeddings involving character-level representation. It is shown useful in some works (He et al., 2018; Lee et al., 2018). This embedding is trained by bidirectional language models on large corpus using character sequence for each word token as input. The ELMo encoder employs CNN and highway networks over characters, whose output is given to a multiple-layer biLSTM with residual connections. Then the output is contextualized embeddings for each word. It is also can be seen as a hybrid encoder for character, word, and sentence. This encoder can add lots of"
C18-1048,P09-1077,0,0.223899,"be evaluated on the benchmark PDTB 2.0 and demonstrate state-of-the-art performance to verify its effectiveness. This paper is organized as follows. Section 2 reviews related work. Section 3 introduces our model. Section 4 shows our experiments and analyses the results. Section 5 concludes this work. 2 Related Work After the release of Penn Discourse Treebank 2.0, many works have been made to solve this concerned task. Lin et al. (2009) is the first work who considered the second-level classification of the task by empirically evaluating the impact of surface features. Feature based methods (Pitler et al., 2009; Zhou et al., 2010; Chen et al., 2015; Li et al., 2016) mainly focused on using linguistic, or semantic features from the discourse units, or the relations between unit pairs and word pairs. Zhang et al. (2015) is the first one who modeled this task using end-to-end neural network and gained great performance improvement. Neural network methods also used by lots of works (Zhang et al., 2016; Cai and Zhao, 2016) for better performance. Since then, a lot of methods have been proposed. Braud and Denis (2015) found that word embeddings trained by neural networks is very useful to this task. Qin e"
C18-1048,prasad-etal-2008-penn,0,0.273206,"dule. The sentence-level module is composed of stacked encoder blocks. The block in each layer receives output of the previous layer as input and sends output to next layer. It also sends its output to the pair-level module. Parameters in different layers are not the same. We consider two encoder types, convolutional type and recurrent type. We only use one encoder type in one experiment. For the sentence-level module for different arguments (Arg1 and Arg2), many previous works used same parameters to encode different arguments, that is, one encoder for two type arguments. But as indicated by Prasad et al. (2008), Arg1 and Arg2 may have different semantic perspective, we thus introduce argument-aware parameter settings for different arguments. GLU GLU GLU Conv Conv Conv FFN GLU Res 1 Conv FFN FFN Res 1 FFN biGRU Figure 3: Convolutional encoder block. Figure 4: Recurrent encoder block. Convolutional Encoder Block Figure 3 is the convolutional encoder block. Suppose the input for the encoder block is xi (i = 1, · · · , N ), then xi ∈ Rde . The input is sent to a convolutional layer and mapped to output yi = [Ai Bi ] ∈ R2de . After the convolutional operation, gated linear units (GLU) (Dauphin et al., 20"
C18-1048,C16-1180,1,0.855514,"2009; Zhou et al., 2010; Chen et al., 2015; Li et al., 2016) mainly focused on using linguistic, or semantic features from the discourse units, or the relations between unit pairs and word pairs. Zhang et al. (2015) is the first one who modeled this task using end-to-end neural network and gained great performance improvement. Neural network methods also used by lots of works (Zhang et al., 2016; Cai and Zhao, 2016) for better performance. Since then, a lot of methods have been proposed. Braud and Denis (2015) found that word embeddings trained by neural networks is very useful to this task. Qin et al. (2016a) augmented their system with character-level and contextualized embeddings. Recurrent networks and convolutional networks have been used as basic blocks in many works (Ji et al., 2016; R¨onnqvist et al., 2017; Qin et al., 2016b). Ji and Eisenstein (2015) used recursive neural networks. Attention mechanism was used by Liu and Li (2016), Cai and Zhao (2017) and others. Wu et al. (2016) and Lan et al. (2017) applied multi-task component. Qin et al. (2017) utilized adversarial nets to migrate the connective-based features to implicit ones. Sentence representation is a key component in many NLP t"
C18-1048,K16-2010,1,0.88791,"2009; Zhou et al., 2010; Chen et al., 2015; Li et al., 2016) mainly focused on using linguistic, or semantic features from the discourse units, or the relations between unit pairs and word pairs. Zhang et al. (2015) is the first one who modeled this task using end-to-end neural network and gained great performance improvement. Neural network methods also used by lots of works (Zhang et al., 2016; Cai and Zhao, 2016) for better performance. Since then, a lot of methods have been proposed. Braud and Denis (2015) found that word embeddings trained by neural networks is very useful to this task. Qin et al. (2016a) augmented their system with character-level and contextualized embeddings. Recurrent networks and convolutional networks have been used as basic blocks in many works (Ji et al., 2016; R¨onnqvist et al., 2017; Qin et al., 2016b). Ji and Eisenstein (2015) used recursive neural networks. Attention mechanism was used by Liu and Li (2016), Cai and Zhao (2017) and others. Wu et al. (2016) and Lan et al. (2017) applied multi-task component. Qin et al. (2017) utilized adversarial nets to migrate the connective-based features to implicit ones. Sentence representation is a key component in many NLP t"
C18-1048,D16-1246,1,0.861843,"2009; Zhou et al., 2010; Chen et al., 2015; Li et al., 2016) mainly focused on using linguistic, or semantic features from the discourse units, or the relations between unit pairs and word pairs. Zhang et al. (2015) is the first one who modeled this task using end-to-end neural network and gained great performance improvement. Neural network methods also used by lots of works (Zhang et al., 2016; Cai and Zhao, 2016) for better performance. Since then, a lot of methods have been proposed. Braud and Denis (2015) found that word embeddings trained by neural networks is very useful to this task. Qin et al. (2016a) augmented their system with character-level and contextualized embeddings. Recurrent networks and convolutional networks have been used as basic blocks in many works (Ji et al., 2016; R¨onnqvist et al., 2017; Qin et al., 2016b). Ji and Eisenstein (2015) used recursive neural networks. Attention mechanism was used by Liu and Li (2016), Cai and Zhao (2017) and others. Wu et al. (2016) and Lan et al. (2017) applied multi-task component. Qin et al. (2017) utilized adversarial nets to migrate the connective-based features to implicit ones. Sentence representation is a key component in many NLP t"
C18-1048,P17-1093,1,0.923706,"a lot of methods have been proposed. Braud and Denis (2015) found that word embeddings trained by neural networks is very useful to this task. Qin et al. (2016a) augmented their system with character-level and contextualized embeddings. Recurrent networks and convolutional networks have been used as basic blocks in many works (Ji et al., 2016; R¨onnqvist et al., 2017; Qin et al., 2016b). Ji and Eisenstein (2015) used recursive neural networks. Attention mechanism was used by Liu and Li (2016), Cai and Zhao (2017) and others. Wu et al. (2016) and Lan et al. (2017) applied multi-task component. Qin et al. (2017) utilized adversarial nets to migrate the connective-based features to implicit ones. Sentence representation is a key component in many NLP tasks. Usually, better representation means better performance. Plenty of work on language modeling has been done, as language modeling can supply better sentence representations. Since the pioneering work of Bengio et al. (2006), neural language models have been well developed (Mikolov and Zweig, 2012; Williams et al., 2015; Kim et al., 2016). Sentence representation is directly handled in a series of work. Lin et al. (2017) used self attention mechanism"
C18-1048,P17-2040,0,0.180775,"Missing"
C18-1048,P16-1162,0,0.0358758,"is merged from single-character segmentation and the input of ELMo encoder is also character. Subword Encoder Character-level embeddings have been used widely in lots of works and its effectiveness is verified for out-of-vocabulary (OOV) or rare word representation. However, character is not a natural minimal unit for there exists word internal structure, we thus introduce a subword-level embedding instead. Subword units can be computationally discovered by unsupervised segmentation over words that are regarded as character sequences. We adopt byte pair encoding (BPE) algorithm introduced by Sennrich et al. (2016) for this segmentation. BPE segmentation actually relies on a series of iterative merging operation over bigrams with the highest frequency. The number of merging operation times is roughly equal to the result subword vocabulary size. 573 Highway Concat Conv1 + max Sub Conv2 + max Sub Sub Figure 2: Subword encoder. For each word, the subword-level embedding is encoded by a subword encoder as in Figure 2. Firstly, the subword sequence (of length n) of the word is mapped to subword embedding sequence (se1 , se2 , se3 , · · · , sen ) (after padding), which is randomly initialized. Then K (we empi"
C18-1048,D16-1253,0,0.0732191,"t al., 2016; Cai and Zhao, 2016) for better performance. Since then, a lot of methods have been proposed. Braud and Denis (2015) found that word embeddings trained by neural networks is very useful to this task. Qin et al. (2016a) augmented their system with character-level and contextualized embeddings. Recurrent networks and convolutional networks have been used as basic blocks in many works (Ji et al., 2016; R¨onnqvist et al., 2017; Qin et al., 2016b). Ji and Eisenstein (2015) used recursive neural networks. Attention mechanism was used by Liu and Li (2016), Cai and Zhao (2017) and others. Wu et al. (2016) and Lan et al. (2017) applied multi-task component. Qin et al. (2017) utilized adversarial nets to migrate the connective-based features to implicit ones. Sentence representation is a key component in many NLP tasks. Usually, better representation means better performance. Plenty of work on language modeling has been done, as language modeling can supply better sentence representations. Since the pioneering work of Bengio et al. (2006), neural language models have been well developed (Mikolov and Zweig, 2012; Williams et al., 2015; Kim et al., 2016). Sentence representation is directly handle"
C18-1048,P17-2042,0,0.139603,"Missing"
C18-1048,D15-1266,0,0.225656,"l. Section 4 shows our experiments and analyses the results. Section 5 concludes this work. 2 Related Work After the release of Penn Discourse Treebank 2.0, many works have been made to solve this concerned task. Lin et al. (2009) is the first work who considered the second-level classification of the task by empirically evaluating the impact of surface features. Feature based methods (Pitler et al., 2009; Zhou et al., 2010; Chen et al., 2015; Li et al., 2016) mainly focused on using linguistic, or semantic features from the discourse units, or the relations between unit pairs and word pairs. Zhang et al. (2015) is the first one who modeled this task using end-to-end neural network and gained great performance improvement. Neural network methods also used by lots of works (Zhang et al., 2016; Cai and Zhao, 2016) for better performance. Since then, a lot of methods have been proposed. Braud and Denis (2015) found that word embeddings trained by neural networks is very useful to this task. Qin et al. (2016a) augmented their system with character-level and contextualized embeddings. Recurrent networks and convolutional networks have been used as basic blocks in many works (Ji et al., 2016; R¨onnqvist et"
C18-1048,P16-1131,1,0.845914,"o solve this concerned task. Lin et al. (2009) is the first work who considered the second-level classification of the task by empirically evaluating the impact of surface features. Feature based methods (Pitler et al., 2009; Zhou et al., 2010; Chen et al., 2015; Li et al., 2016) mainly focused on using linguistic, or semantic features from the discourse units, or the relations between unit pairs and word pairs. Zhang et al. (2015) is the first one who modeled this task using end-to-end neural network and gained great performance improvement. Neural network methods also used by lots of works (Zhang et al., 2016; Cai and Zhao, 2016) for better performance. Since then, a lot of methods have been proposed. Braud and Denis (2015) found that word embeddings trained by neural networks is very useful to this task. Qin et al. (2016a) augmented their system with character-level and contextualized embeddings. Recurrent networks and convolutional networks have been used as basic blocks in many works (Ji et al., 2016; R¨onnqvist et al., 2017; Qin et al., 2016b). Ji and Eisenstein (2015) used recursive neural networks. Attention mechanism was used by Liu and Li (2016), Cai and Zhao (2017) and others. Wu et al. ("
C18-1048,C10-2172,0,0.20652,"benchmark PDTB 2.0 and demonstrate state-of-the-art performance to verify its effectiveness. This paper is organized as follows. Section 2 reviews related work. Section 3 introduces our model. Section 4 shows our experiments and analyses the results. Section 5 concludes this work. 2 Related Work After the release of Penn Discourse Treebank 2.0, many works have been made to solve this concerned task. Lin et al. (2009) is the first work who considered the second-level classification of the task by empirically evaluating the impact of surface features. Feature based methods (Pitler et al., 2009; Zhou et al., 2010; Chen et al., 2015; Li et al., 2016) mainly focused on using linguistic, or semantic features from the discourse units, or the relations between unit pairs and word pairs. Zhang et al. (2015) is the first one who modeled this task using end-to-end neural network and gained great performance improvement. Neural network methods also used by lots of works (Zhang et al., 2016; Cai and Zhao, 2016) for better performance. Since then, a lot of methods have been proposed. Braud and Denis (2015) found that word embeddings trained by neural networks is very useful to this task. Qin et al. (2016a) augme"
C18-1048,W18-2501,0,\N,Missing
C18-1153,E17-2067,0,0.0364362,"s, such as word segmentation (Cai et al., 2017), machine translation (Luong and Manning, 2016), tagging (Yang et al., 2016; Li et al., 2018) and language modeling (Verwimp et al., 2017; Miyamoto and Cho, 2016). However, character embedding only shows marginal improvement due to a lack internal semantics. Lexical, syntactic and morphological information are also considered to improve word representation (Cao and Rei, 2016; Bergmanis and Goldwater, 2017). Bojanowski et al. (2017) proposed to learn representations for character n-gram vectors and represent words as the sum of the n-gram vectors. Avraham and Goldberg (2017) built a model inspired by (Joulin et al., 2017), who used morphological tags instead of n-grams. They jointly trained their morphological and semantic embeddings, implicitly assuming that morphological and semantic information should live in the same space. However, the linguistic knowledge resulting subwords, typically, morphological suffix, prefix or stem, may not be suitable for different kinds of languages and tasks. Sennrich et al. (2016) introduced the byte pair encoding (BPE) compression algorithm into neural machine translation for being capable of open-vocabulary translation by encod"
C18-1153,C18-1048,1,0.843929,"ument for accurate answer selection. Wang et al. (2017b) employed gated self-matching networks (R-net) on passage against passage itself to refine passage representation with information from the whole passage. Cui et al. (2017a) introduced an “attended attention” mechanism (AoA) where query-to-document and document-to-query are mutually attentive and interactive to each other. 5.2 Augmented Word Embedding Distributed word representation plays a fundamental role in neural models (Cai and Zhao, 2016; Qin et al., 2016; Zhao et al., 2017; Peters et al., 2018; He et al., 2018; Wang et al., 2018a; Bai and Zhao, 2018; Zhang et al., 2018a). Recently, character embeddings are widely used to enrich word representations (Kim et al., 2016; Yang et al., 2017; Luong and Manning, 2016; Huang et al., 2018). Yang et al. (2017) explored a fine-grained gating mechanism (FG Reader) to dynamically combine word-level and character-level representations based on properties of the words. However, this method is computationally complex and it is not end-to-end, requiring extra labels such as NE and POS tags. Seo et al. (2017) 1810 concatenated the character and word embedding to feed a two-layer Highway Network. Not only f"
C18-1153,E17-1032,0,0.0268016,"word embedding to feed a two-layer Highway Network. Not only for machine reading comprehension tasks, character embedding has also benefit other natural language process tasks, such as word segmentation (Cai et al., 2017), machine translation (Luong and Manning, 2016), tagging (Yang et al., 2016; Li et al., 2018) and language modeling (Verwimp et al., 2017; Miyamoto and Cho, 2016). However, character embedding only shows marginal improvement due to a lack internal semantics. Lexical, syntactic and morphological information are also considered to improve word representation (Cao and Rei, 2016; Bergmanis and Goldwater, 2017). Bojanowski et al. (2017) proposed to learn representations for character n-gram vectors and represent words as the sum of the n-gram vectors. Avraham and Goldberg (2017) built a model inspired by (Joulin et al., 2017), who used morphological tags instead of n-grams. They jointly trained their morphological and semantic embeddings, implicitly assuming that morphological and semantic information should live in the same space. However, the linguistic knowledge resulting subwords, typically, morphological suffix, prefix or stem, may not be suitable for different kinds of languages and tasks. Sen"
C18-1153,Q17-1010,0,0.0615449,"er Highway Network. Not only for machine reading comprehension tasks, character embedding has also benefit other natural language process tasks, such as word segmentation (Cai et al., 2017), machine translation (Luong and Manning, 2016), tagging (Yang et al., 2016; Li et al., 2018) and language modeling (Verwimp et al., 2017; Miyamoto and Cho, 2016). However, character embedding only shows marginal improvement due to a lack internal semantics. Lexical, syntactic and morphological information are also considered to improve word representation (Cao and Rei, 2016; Bergmanis and Goldwater, 2017). Bojanowski et al. (2017) proposed to learn representations for character n-gram vectors and represent words as the sum of the n-gram vectors. Avraham and Goldberg (2017) built a model inspired by (Joulin et al., 2017), who used morphological tags instead of n-grams. They jointly trained their morphological and semantic embeddings, implicitly assuming that morphological and semantic information should live in the same space. However, the linguistic knowledge resulting subwords, typically, morphological suffix, prefix or stem, may not be suitable for different kinds of languages and tasks. Sennrich et al. (2016) introd"
C18-1153,P16-1039,1,0.84958,"d of the document using the entire query representation to build query-specific representations of words in the document for accurate answer selection. Wang et al. (2017b) employed gated self-matching networks (R-net) on passage against passage itself to refine passage representation with information from the whole passage. Cui et al. (2017a) introduced an “attended attention” mechanism (AoA) where query-to-document and document-to-query are mutually attentive and interactive to each other. 5.2 Augmented Word Embedding Distributed word representation plays a fundamental role in neural models (Cai and Zhao, 2016; Qin et al., 2016; Zhao et al., 2017; Peters et al., 2018; He et al., 2018; Wang et al., 2018a; Bai and Zhao, 2018; Zhang et al., 2018a). Recently, character embeddings are widely used to enrich word representations (Kim et al., 2016; Yang et al., 2017; Luong and Manning, 2016; Huang et al., 2018). Yang et al. (2017) explored a fine-grained gating mechanism (FG Reader) to dynamically combine word-level and character-level representations based on properties of the words. However, this method is computationally complex and it is not end-to-end, requiring extra labels such as NE and POS tags. S"
C18-1153,P17-2096,1,0.835811,"ong and Manning, 2016; Huang et al., 2018). Yang et al. (2017) explored a fine-grained gating mechanism (FG Reader) to dynamically combine word-level and character-level representations based on properties of the words. However, this method is computationally complex and it is not end-to-end, requiring extra labels such as NE and POS tags. Seo et al. (2017) 1810 concatenated the character and word embedding to feed a two-layer Highway Network. Not only for machine reading comprehension tasks, character embedding has also benefit other natural language process tasks, such as word segmentation (Cai et al., 2017), machine translation (Luong and Manning, 2016), tagging (Yang et al., 2016; Li et al., 2018) and language modeling (Verwimp et al., 2017; Miyamoto and Cho, 2016). However, character embedding only shows marginal improvement due to a lack internal semantics. Lexical, syntactic and morphological information are also considered to improve word representation (Cao and Rei, 2016; Bergmanis and Goldwater, 2017). Bojanowski et al. (2017) proposed to learn representations for character n-gram vectors and represent words as the sum of the n-gram vectors. Avraham and Goldberg (2017) built a model inspi"
C18-1153,W16-1603,0,0.100131,"for dealing with rare word representation. However, the minimal meaningful unit below word usually is not character, which motivates researchers to explore the potential unit (subword) between character and word to model sub-word morphologies or lexical semantics. In fact, morphological compounding (e.g. sunshine or playground) is one of the most common and productive methods of word formation across human languages, which inspires us to represent word by meaningful sub-word units. Recently, researchers have started to work on morphologically informed word embeddings (Botha and Blunsom, 2014; Cao and Rei, 2016), aiming at better capturing syntactic, lexical and morphological information. With ready subwords, we do not have to work with characters, and segmentation could be stopped at the subword-level to reach a meaningful representation. In this paper, we present various simple yet accurate subword-augmented embedding (SAW) strategies and propose SAW Reader as an instance. Specifically, we adopt subword information to enrich word embedding and survey different SAW operations to integrate word-level and subword-level embedding for a fine-grained representation. To ensure adequate training of OOV and"
C18-1153,P16-1223,0,0.0408595,"After attention learning, the key evidence of the answer would be collected and irrelevant parts would be ignored. This shows our SAW Reader is effective at selecting the vital points at the fundamental embedding layer, guiding the attention layers to collect more relevant pieces. 5 5.1 Related Work Machine Reading Comprehension Recently, many deep learning models have been proposed for reading comprehension (Sordoni et al., 2016; Trischler et al., 2016; Wang and Jiang, 2016; Munkhdalai and Yu, 2017; Wang et al., 2017a; Dhingra et al., 2017; Zhang et al., 2018b; Wang et al., 2018b). Notably, Chen et al. (2016) conducted an in-depth and thoughtful examination on the comprehension task based on an attentive neural network and an entity-centric classifier with a careful analysis based on handful features. Kadlec et al. (2016) proposed the Attention Sum Reader (AS Reader) that uses attention to directly pick the answer from the context, which is motivated by the Pointer Network (Vinyals et al., 2015). Instead of summing the attention of query-to-document, GA Reader (Dhingra et al., 2017) defined an element-wise product to endowing attention on each word of the document using the entire query representa"
C18-1153,C16-1167,0,0.251713,"t word embedding with a short list to handle rare words effectively. A thorough examination is conducted to evaluate the comprehensive performance and generalization ability of the proposed reader. Experimental results show that the proposed approach helps the reader significantly outperform the state-of-the-art baselines on various public datasets. 1 Introduction A recent hot challenge is to train machines to read and comprehend human languages. Towards this end, various machine reading comprehension datasets have been released, including cloze-style (Hermann et al., 2015; Hill et al., 2015; Cui et al., 2016) and user-query types (Joshi et al., 2017; Rajpurkar et al., 2016). Meanwhile, a number of deep learning models are designed to take up the challenges, most of which focus on attention mechanism (Wang et al., 2017b; Seo et al., 2017; Cui et al., 2017a; Kadlec et al., 2016; Dhingra et al., 2017; Zhang and Zhao, 2018). However, how to represent word in an effective way remains an open problem for diverse natural language processing tasks, including machine reading comprehension for different languages. Particularly, for a language like Chinese with a large set of characters (typically, thousands"
C18-1153,P17-1055,0,0.212967,"helps the reader significantly outperform the state-of-the-art baselines on various public datasets. 1 Introduction A recent hot challenge is to train machines to read and comprehend human languages. Towards this end, various machine reading comprehension datasets have been released, including cloze-style (Hermann et al., 2015; Hill et al., 2015; Cui et al., 2016) and user-query types (Joshi et al., 2017; Rajpurkar et al., 2016). Meanwhile, a number of deep learning models are designed to take up the challenges, most of which focus on attention mechanism (Wang et al., 2017b; Seo et al., 2017; Cui et al., 2017a; Kadlec et al., 2016; Dhingra et al., 2017; Zhang and Zhao, 2018). However, how to represent word in an effective way remains an open problem for diverse natural language processing tasks, including machine reading comprehension for different languages. Particularly, for a language like Chinese with a large set of characters (typically, thousands of), lots of which are semantically ambiguous, using either word-level or character-level embedding alone to build the word representations would not be accurate enough. This work especially focuses on a cloze-style reading comprehension task over f"
C18-1153,P17-1168,0,0.384446,"m the state-of-the-art baselines on various public datasets. 1 Introduction A recent hot challenge is to train machines to read and comprehend human languages. Towards this end, various machine reading comprehension datasets have been released, including cloze-style (Hermann et al., 2015; Hill et al., 2015; Cui et al., 2016) and user-query types (Joshi et al., 2017; Rajpurkar et al., 2016). Meanwhile, a number of deep learning models are designed to take up the challenges, most of which focus on attention mechanism (Wang et al., 2017b; Seo et al., 2017; Cui et al., 2017a; Kadlec et al., 2016; Dhingra et al., 2017; Zhang and Zhao, 2018). However, how to represent word in an effective way remains an open problem for diverse natural language processing tasks, including machine reading comprehension for different languages. Particularly, for a language like Chinese with a large set of characters (typically, thousands of), lots of which are semantically ambiguous, using either word-level or character-level embedding alone to build the word representations would not be accurate enough. This work especially focuses on a cloze-style reading comprehension task over fairy stories, which is highly challenging du"
C18-1153,P18-1192,1,0.69419,"c representations of words in the document for accurate answer selection. Wang et al. (2017b) employed gated self-matching networks (R-net) on passage against passage itself to refine passage representation with information from the whole passage. Cui et al. (2017a) introduced an “attended attention” mechanism (AoA) where query-to-document and document-to-query are mutually attentive and interactive to each other. 5.2 Augmented Word Embedding Distributed word representation plays a fundamental role in neural models (Cai and Zhao, 2016; Qin et al., 2016; Zhao et al., 2017; Peters et al., 2018; He et al., 2018; Wang et al., 2018a; Bai and Zhao, 2018; Zhang et al., 2018a). Recently, character embeddings are widely used to enrich word representations (Kim et al., 2016; Yang et al., 2017; Luong and Manning, 2016; Huang et al., 2018). Yang et al. (2017) explored a fine-grained gating mechanism (FG Reader) to dynamically combine word-level and character-level representations based on properties of the words. However, this method is computationally complex and it is not end-to-end, requiring extra labels such as NE and POS tags. Seo et al. (2017) 1810 concatenated the character and word embedding to feed"
C18-1153,P18-4024,1,0.660925,"ion from the whole passage. Cui et al. (2017a) introduced an “attended attention” mechanism (AoA) where query-to-document and document-to-query are mutually attentive and interactive to each other. 5.2 Augmented Word Embedding Distributed word representation plays a fundamental role in neural models (Cai and Zhao, 2016; Qin et al., 2016; Zhao et al., 2017; Peters et al., 2018; He et al., 2018; Wang et al., 2018a; Bai and Zhao, 2018; Zhang et al., 2018a). Recently, character embeddings are widely used to enrich word representations (Kim et al., 2016; Yang et al., 2017; Luong and Manning, 2016; Huang et al., 2018). Yang et al. (2017) explored a fine-grained gating mechanism (FG Reader) to dynamically combine word-level and character-level representations based on properties of the words. However, this method is computationally complex and it is not end-to-end, requiring extra labels such as NE and POS tags. Seo et al. (2017) 1810 concatenated the character and word embedding to feed a two-layer Highway Network. Not only for machine reading comprehension tasks, character embedding has also benefit other natural language process tasks, such as word segmentation (Cai et al., 2017), machine translation (Lu"
C18-1153,P17-1147,0,0.0122583,"ndle rare words effectively. A thorough examination is conducted to evaluate the comprehensive performance and generalization ability of the proposed reader. Experimental results show that the proposed approach helps the reader significantly outperform the state-of-the-art baselines on various public datasets. 1 Introduction A recent hot challenge is to train machines to read and comprehend human languages. Towards this end, various machine reading comprehension datasets have been released, including cloze-style (Hermann et al., 2015; Hill et al., 2015; Cui et al., 2016) and user-query types (Joshi et al., 2017; Rajpurkar et al., 2016). Meanwhile, a number of deep learning models are designed to take up the challenges, most of which focus on attention mechanism (Wang et al., 2017b; Seo et al., 2017; Cui et al., 2017a; Kadlec et al., 2016; Dhingra et al., 2017; Zhang and Zhao, 2018). However, how to represent word in an effective way remains an open problem for diverse natural language processing tasks, including machine reading comprehension for different languages. Particularly, for a language like Chinese with a large set of characters (typically, thousands of), lots of which are semantically ambi"
C18-1153,E17-2068,0,0.0205335,"e translation (Luong and Manning, 2016), tagging (Yang et al., 2016; Li et al., 2018) and language modeling (Verwimp et al., 2017; Miyamoto and Cho, 2016). However, character embedding only shows marginal improvement due to a lack internal semantics. Lexical, syntactic and morphological information are also considered to improve word representation (Cao and Rei, 2016; Bergmanis and Goldwater, 2017). Bojanowski et al. (2017) proposed to learn representations for character n-gram vectors and represent words as the sum of the n-gram vectors. Avraham and Goldberg (2017) built a model inspired by (Joulin et al., 2017), who used morphological tags instead of n-grams. They jointly trained their morphological and semantic embeddings, implicitly assuming that morphological and semantic information should live in the same space. However, the linguistic knowledge resulting subwords, typically, morphological suffix, prefix or stem, may not be suitable for different kinds of languages and tasks. Sennrich et al. (2016) introduced the byte pair encoding (BPE) compression algorithm into neural machine translation for being capable of open-vocabulary translation by encoding rare and unknown words as subword units. Ins"
C18-1153,P16-1086,0,0.126242,"gnificantly outperform the state-of-the-art baselines on various public datasets. 1 Introduction A recent hot challenge is to train machines to read and comprehend human languages. Towards this end, various machine reading comprehension datasets have been released, including cloze-style (Hermann et al., 2015; Hill et al., 2015; Cui et al., 2016) and user-query types (Joshi et al., 2017; Rajpurkar et al., 2016). Meanwhile, a number of deep learning models are designed to take up the challenges, most of which focus on attention mechanism (Wang et al., 2017b; Seo et al., 2017; Cui et al., 2017a; Kadlec et al., 2016; Dhingra et al., 2017; Zhang and Zhao, 2018). However, how to represent word in an effective way remains an open problem for diverse natural language processing tasks, including machine reading comprehension for different languages. Particularly, for a language like Chinese with a large set of characters (typically, thousands of), lots of which are semantically ambiguous, using either word-level or character-level embedding alone to build the word representations would not be accurate enough. This work especially focuses on a cloze-style reading comprehension task over fairy stories, which is"
C18-1153,P16-1100,0,0.0605792,"resentation with information from the whole passage. Cui et al. (2017a) introduced an “attended attention” mechanism (AoA) where query-to-document and document-to-query are mutually attentive and interactive to each other. 5.2 Augmented Word Embedding Distributed word representation plays a fundamental role in neural models (Cai and Zhao, 2016; Qin et al., 2016; Zhao et al., 2017; Peters et al., 2018; He et al., 2018; Wang et al., 2018a; Bai and Zhao, 2018; Zhang et al., 2018a). Recently, character embeddings are widely used to enrich word representations (Kim et al., 2016; Yang et al., 2017; Luong and Manning, 2016; Huang et al., 2018). Yang et al. (2017) explored a fine-grained gating mechanism (FG Reader) to dynamically combine word-level and character-level representations based on properties of the words. However, this method is computationally complex and it is not end-to-end, requiring extra labels such as NE and POS tags. Seo et al. (2017) 1810 concatenated the character and word embedding to feed a two-layer Highway Network. Not only for machine reading comprehension tasks, character embedding has also benefit other natural language process tasks, such as word segmentation (Cai et al., 2017), ma"
C18-1153,D16-1209,0,0.0246897,"aracter-level representations based on properties of the words. However, this method is computationally complex and it is not end-to-end, requiring extra labels such as NE and POS tags. Seo et al. (2017) 1810 concatenated the character and word embedding to feed a two-layer Highway Network. Not only for machine reading comprehension tasks, character embedding has also benefit other natural language process tasks, such as word segmentation (Cai et al., 2017), machine translation (Luong and Manning, 2016), tagging (Yang et al., 2016; Li et al., 2018) and language modeling (Verwimp et al., 2017; Miyamoto and Cho, 2016). However, character embedding only shows marginal improvement due to a lack internal semantics. Lexical, syntactic and morphological information are also considered to improve word representation (Cao and Rei, 2016; Bergmanis and Goldwater, 2017). Bojanowski et al. (2017) proposed to learn representations for character n-gram vectors and represent words as the sum of the n-gram vectors. Avraham and Goldberg (2017) built a model inspired by (Joulin et al., 2017), who used morphological tags instead of n-grams. They jointly trained their morphological and semantic embeddings, implicitly assumin"
C18-1153,N18-1202,0,0.0461159,"o build query-specific representations of words in the document for accurate answer selection. Wang et al. (2017b) employed gated self-matching networks (R-net) on passage against passage itself to refine passage representation with information from the whole passage. Cui et al. (2017a) introduced an “attended attention” mechanism (AoA) where query-to-document and document-to-query are mutually attentive and interactive to each other. 5.2 Augmented Word Embedding Distributed word representation plays a fundamental role in neural models (Cai and Zhao, 2016; Qin et al., 2016; Zhao et al., 2017; Peters et al., 2018; He et al., 2018; Wang et al., 2018a; Bai and Zhao, 2018; Zhang et al., 2018a). Recently, character embeddings are widely used to enrich word representations (Kim et al., 2016; Yang et al., 2017; Luong and Manning, 2016; Huang et al., 2018). Yang et al. (2017) explored a fine-grained gating mechanism (FG Reader) to dynamically combine word-level and character-level representations based on properties of the words. However, this method is computationally complex and it is not end-to-end, requiring extra labels such as NE and POS tags. Seo et al. (2017) 1810 concatenated the character and word"
C18-1153,D16-1246,1,0.857959,"ing the entire query representation to build query-specific representations of words in the document for accurate answer selection. Wang et al. (2017b) employed gated self-matching networks (R-net) on passage against passage itself to refine passage representation with information from the whole passage. Cui et al. (2017a) introduced an “attended attention” mechanism (AoA) where query-to-document and document-to-query are mutually attentive and interactive to each other. 5.2 Augmented Word Embedding Distributed word representation plays a fundamental role in neural models (Cai and Zhao, 2016; Qin et al., 2016; Zhao et al., 2017; Peters et al., 2018; He et al., 2018; Wang et al., 2018a; Bai and Zhao, 2018; Zhang et al., 2018a). Recently, character embeddings are widely used to enrich word representations (Kim et al., 2016; Yang et al., 2017; Luong and Manning, 2016; Huang et al., 2018). Yang et al. (2017) explored a fine-grained gating mechanism (FG Reader) to dynamically combine word-level and character-level representations based on properties of the words. However, this method is computationally complex and it is not end-to-end, requiring extra labels such as NE and POS tags. Seo et al. (2017) 1"
C18-1153,D16-1264,0,0.0316737,"ctively. A thorough examination is conducted to evaluate the comprehensive performance and generalization ability of the proposed reader. Experimental results show that the proposed approach helps the reader significantly outperform the state-of-the-art baselines on various public datasets. 1 Introduction A recent hot challenge is to train machines to read and comprehend human languages. Towards this end, various machine reading comprehension datasets have been released, including cloze-style (Hermann et al., 2015; Hill et al., 2015; Cui et al., 2016) and user-query types (Joshi et al., 2017; Rajpurkar et al., 2016). Meanwhile, a number of deep learning models are designed to take up the challenges, most of which focus on attention mechanism (Wang et al., 2017b; Seo et al., 2017; Cui et al., 2017a; Kadlec et al., 2016; Dhingra et al., 2017; Zhang and Zhao, 2018). However, how to represent word in an effective way remains an open problem for diverse natural language processing tasks, including machine reading comprehension for different languages. Particularly, for a language like Chinese with a large set of characters (typically, thousands of), lots of which are semantically ambiguous, using either word-"
C18-1153,P16-1162,0,0.394315,"able-length character sequences, making it a very suitable word segmentation strategy for neural network models. The generalized framework can be described as follows. Firstly, all the input sequences (strings) are tokenized into a sequence of single-character subwords, then we repeat, 1. Count all bigrams under the current segmentation status of all sequences. 2. Find the bigram with the highest frequency and merge them in all the sequences. Note the segmentation status is updating now. 3. If the merging times do not reach the specified number, go back to 1, otherwise the algorithm ends. In (Sennrich et al., 2016), BPE is adopted to segment infrequent words into sub-word units for machine translation. However, there is a key difference between the motivations for subword segmentation. We aim to refine the word representations by using subwords, for both frequent and infrequent words, which is more generally motivated. To this end, we adaptively tokenize words in multi-granularity by controlling the merging times. 2.2 Subword-augmented Word Embedding Our subwords are also formed as character n-grams, do not cross word boundaries. After using unsupervised segmentation methods to split each word into a su"
C18-1153,D16-1013,0,0.0161683,"n the document can be focused after the pair-wise matching of document and query and the right answer (“The mole”) could obtain a high weight at the very beginning. After attention learning, the key evidence of the answer would be collected and irrelevant parts would be ignored. This shows our SAW Reader is effective at selecting the vital points at the fundamental embedding layer, guiding the attention layers to collect more relevant pieces. 5 5.1 Related Work Machine Reading Comprehension Recently, many deep learning models have been proposed for reading comprehension (Sordoni et al., 2016; Trischler et al., 2016; Wang and Jiang, 2016; Munkhdalai and Yu, 2017; Wang et al., 2017a; Dhingra et al., 2017; Zhang et al., 2018b; Wang et al., 2018b). Notably, Chen et al. (2016) conducted an in-depth and thoughtful examination on the comprehension task based on an attentive neural network and an entity-centric classifier with a careful analysis based on handful features. Kadlec et al. (2016) proposed the Attention Sum Reader (AS Reader) that uses attention to directly pick the answer from the context, which is motivated by the Pointer Network (Vinyals et al., 2015). Instead of summing the attention of query-to"
C18-1153,E17-1040,0,0.0735249,"Missing"
C18-1153,P17-1018,0,0.389479,"sults show that the proposed approach helps the reader significantly outperform the state-of-the-art baselines on various public datasets. 1 Introduction A recent hot challenge is to train machines to read and comprehend human languages. Towards this end, various machine reading comprehension datasets have been released, including cloze-style (Hermann et al., 2015; Hill et al., 2015; Cui et al., 2016) and user-query types (Joshi et al., 2017; Rajpurkar et al., 2016). Meanwhile, a number of deep learning models are designed to take up the challenges, most of which focus on attention mechanism (Wang et al., 2017b; Seo et al., 2017; Cui et al., 2017a; Kadlec et al., 2016; Dhingra et al., 2017; Zhang and Zhao, 2018). However, how to represent word in an effective way remains an open problem for diverse natural language processing tasks, including machine reading comprehension for different languages. Particularly, for a language like Chinese with a large set of characters (typically, thousands of), lots of which are semantically ambiguous, using either word-level or character-level embedding alone to build the word representations would not be accurate enough. This work especially focuses on a cloze-st"
C18-1153,P18-1178,0,0.0539433,"weight at the very beginning. After attention learning, the key evidence of the answer would be collected and irrelevant parts would be ignored. This shows our SAW Reader is effective at selecting the vital points at the fundamental embedding layer, guiding the attention layers to collect more relevant pieces. 5 5.1 Related Work Machine Reading Comprehension Recently, many deep learning models have been proposed for reading comprehension (Sordoni et al., 2016; Trischler et al., 2016; Wang and Jiang, 2016; Munkhdalai and Yu, 2017; Wang et al., 2017a; Dhingra et al., 2017; Zhang et al., 2018b; Wang et al., 2018b). Notably, Chen et al. (2016) conducted an in-depth and thoughtful examination on the comprehension task based on an attentive neural network and an entity-centric classifier with a careful analysis based on handful features. Kadlec et al. (2016) proposed the Attention Sum Reader (AS Reader) that uses attention to directly pick the answer from the context, which is motivated by the Pointer Network (Vinyals et al., 2015). Instead of summing the attention of query-to-document, GA Reader (Dhingra et al., 2017) defined an element-wise product to endowing attention on each word of the document us"
C18-1153,C18-1038,1,0.713413,"baselines on various public datasets. 1 Introduction A recent hot challenge is to train machines to read and comprehend human languages. Towards this end, various machine reading comprehension datasets have been released, including cloze-style (Hermann et al., 2015; Hill et al., 2015; Cui et al., 2016) and user-query types (Joshi et al., 2017; Rajpurkar et al., 2016). Meanwhile, a number of deep learning models are designed to take up the challenges, most of which focus on attention mechanism (Wang et al., 2017b; Seo et al., 2017; Cui et al., 2017a; Kadlec et al., 2016; Dhingra et al., 2017; Zhang and Zhao, 2018). However, how to represent word in an effective way remains an open problem for diverse natural language processing tasks, including machine reading comprehension for different languages. Particularly, for a language like Chinese with a large set of characters (typically, thousands of), lots of which are semantically ambiguous, using either word-level or character-level embedding alone to build the word representations would not be accurate enough. This work especially focuses on a cloze-style reading comprehension task over fairy stories, which is highly challenging due to diverse semantic p"
C18-1153,S18-1147,1,0.804002,") could obtain a high weight at the very beginning. After attention learning, the key evidence of the answer would be collected and irrelevant parts would be ignored. This shows our SAW Reader is effective at selecting the vital points at the fundamental embedding layer, guiding the attention layers to collect more relevant pieces. 5 5.1 Related Work Machine Reading Comprehension Recently, many deep learning models have been proposed for reading comprehension (Sordoni et al., 2016; Trischler et al., 2016; Wang and Jiang, 2016; Munkhdalai and Yu, 2017; Wang et al., 2017a; Dhingra et al., 2017; Zhang et al., 2018b; Wang et al., 2018b). Notably, Chen et al. (2016) conducted an in-depth and thoughtful examination on the comprehension task based on an attentive neural network and an entity-centric classifier with a careful analysis based on handful features. Kadlec et al. (2016) proposed the Attention Sum Reader (AS Reader) that uses attention to directly pick the answer from the context, which is motivated by the Pointer Network (Vinyals et al., 2015). Instead of summing the attention of query-to-document, GA Reader (Dhingra et al., 2017) defined an element-wise product to endowing attention on each wor"
C18-1153,C18-1317,1,0.773461,") could obtain a high weight at the very beginning. After attention learning, the key evidence of the answer would be collected and irrelevant parts would be ignored. This shows our SAW Reader is effective at selecting the vital points at the fundamental embedding layer, guiding the attention layers to collect more relevant pieces. 5 5.1 Related Work Machine Reading Comprehension Recently, many deep learning models have been proposed for reading comprehension (Sordoni et al., 2016; Trischler et al., 2016; Wang and Jiang, 2016; Munkhdalai and Yu, 2017; Wang et al., 2017a; Dhingra et al., 2017; Zhang et al., 2018b; Wang et al., 2018b). Notably, Chen et al. (2016) conducted an in-depth and thoughtful examination on the comprehension task based on an attentive neural network and an entity-centric classifier with a careful analysis based on handful features. Kadlec et al. (2016) proposed the Attention Sum Reader (AS Reader) that uses attention to directly pick the answer from the context, which is motivated by the Pointer Network (Vinyals et al., 2015). Instead of summing the attention of query-to-document, GA Reader (Dhingra et al., 2017) defined an element-wise product to endowing attention on each wor"
C18-1233,C18-1048,1,0.837634,"lly, SRL task can be put into two categories: constituent-based (i.e., phrase or span) SRL and dependency-based SRL. This paper will focus on the latter one popularized by CoNLL-2008 and 2009 shared tasks (Surdeanu et al., 2008; Hajiˇc et al., 2009). Most conventional SRL systems relied on sophisticated handcraft features or some declarative constraints (Pradhan et al., 2005; Zhao et al., 2009a), which suffers from poor efficiency and generalization ability. A recently tendency for SRL is adopting neural networks methods attributed to their significant success in a wide range of applications (Bai and Zhao, 2018; Zhang and Zhao, 2018). However, most of those works still heavily resort to syntactic features. Since the syntactic parsing task is equally hard as SRL and comes with its own errors, it is better to get rid of such prerequisite as in other NLP tasks. Accordingly, Marcheggiani et al. (2017) presented a neural model putting syntax aside for dependency-based SRL and obtain favorable results, which overturns the inherent belief that syntax is indispensable in SRL task (Punyakanok et al., 2008). Besides, SRL task is generally formulated as multi-step classification subtasks in pipeline systems, c"
C18-1233,W13-3820,0,0.0812249,"the state-of-the-art syntax-aware SRL systems on the CoNLL-2008, 2009 benchmarks for both English and Chinese. To our best knowledge, we report the first syntax-agnostic SRL model that surpasses all known syntax-aware models. 1 Introduction Semantic role labeling (SRL) is a shallow semantic parsing, which is dedicated to identifying the semantic arguments of a predicate and labeling them with their semantic roles. SRL is considered as one of the core tasks in the natural language processing (NLP), which has been successfully applied to various downstream tasks, such as information extraction (Bastianelli et al., 2013), question answering (Shen and Lapata, 2007; Berant et al., 2013), machine translation (Xiong et al., 2012; Shi et al., 2016). Typically, SRL task can be put into two categories: constituent-based (i.e., phrase or span) SRL and dependency-based SRL. This paper will focus on the latter one popularized by CoNLL-2008 and 2009 shared tasks (Surdeanu et al., 2008; Hajiˇc et al., 2009). Most conventional SRL systems relied on sophisticated handcraft features or some declarative constraints (Pradhan et al., 2005; Zhao et al., 2009a), which suffers from poor efficiency and generalization ability. A re"
C18-1233,D13-1160,0,0.0609907,"benchmarks for both English and Chinese. To our best knowledge, we report the first syntax-agnostic SRL model that surpasses all known syntax-aware models. 1 Introduction Semantic role labeling (SRL) is a shallow semantic parsing, which is dedicated to identifying the semantic arguments of a predicate and labeling them with their semantic roles. SRL is considered as one of the core tasks in the natural language processing (NLP), which has been successfully applied to various downstream tasks, such as information extraction (Bastianelli et al., 2013), question answering (Shen and Lapata, 2007; Berant et al., 2013), machine translation (Xiong et al., 2012; Shi et al., 2016). Typically, SRL task can be put into two categories: constituent-based (i.e., phrase or span) SRL and dependency-based SRL. This paper will focus on the latter one popularized by CoNLL-2008 and 2009 shared tasks (Surdeanu et al., 2008; Hajiˇc et al., 2009). Most conventional SRL systems relied on sophisticated handcraft features or some declarative constraints (Pradhan et al., 2005; Zhao et al., 2009a), which suffers from poor efficiency and generalization ability. A recently tendency for SRL is adopting neural networks methods attri"
C18-1233,W09-1206,0,0.274059,"Missing"
C18-1233,C10-3009,0,0.220829,"Missing"
C18-1233,P17-2096,1,0.88885,"Missing"
C18-1233,D15-1112,0,0.262702,"Missing"
C18-1233,S15-1033,0,0.0885476,"Missing"
C18-1233,J02-3001,0,0.731976,"Missing"
C18-1233,P17-1044,0,0.109522,"nal Natural Science Foundation of China (No. 61672343 and No. 61733011), Key Project of National Society Science Foundation of China (No. 15-ZDA041), The Art and Science Interdisciplinary Funds of Shanghai Jiao Tong University (No. 14JCRZ04). This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http: //creativecommons.org/licenses/by/4.0/ 2753 Proceedings of the 27th International Conference on Computational Linguistics, pages 2753–2765 Santa Fe, New Mexico, USA, August 20-26, 2018. after another. Until recently, some works (Zhou and Xu, 2015; He et al., 2017) introduce end-to-end models for span-based SRL, which motivates us to explore integrative model for dependency SRL. In this work, we propose a syntactic-agnostic end-to-end system, dealing with predicate disambiguation and argument labeling in one model, unlike previous systems that treat the predicate disambiguation as a subtask and handle it separately. In detail, our model contains (1) a deep BiLSTM encoder, which is able to distinguish the predicates and arguments by mapping them into two different vector spaces, and (2) a biaffine attentional (Dozat and Manning, 2017) scorer, which unifi"
C18-1233,P18-1192,1,0.373742,"irs for semantic role label classification. Score vector + . . LSTM LSTM . BiLSTM Encoder &lt;VR&gt; + + = Biaffine Scorer + + + LSTM LSTM LSTM LSTM LSTM LSTM Sentence Representation &lt;VR&gt; He soon Figure 2: An overview of our model. 2755 adjusts adjusts 3.1 Bidirectional LSTM Encoder Word Representation The word representation of our model is the concatenation of several vectors: a randomly initialized word embedding e(r) , a pre-trained word embedding e(p) , a randomly initialized part-of-speech (POS) tag embedding e(pos) , a randomly initialized lemma embedding e(l) . Besides, since previous work (He et al., 2018) demonstrated that the predicate-specific feature is helpful in promoting the role labeling process, we employ an indicator embedding e(i) to indicate whether a word is a predicate when predicting and labeling the arguments for each given predicate. The final word representation is given by e = e(r) ⊕ e(p) ⊕ e(l) ⊕ e(pos) ⊕ e(i) , where ⊕ is the concatenation operator. Encoder As commonly used to model the sequential input in most NLP tasks (Wang et al., 2016; He et al., 2018), BiLSTM is adopted for our sentence encoder. By incorporating a stack of two distinct LSTMs, BiLSTM processes an input"
C18-1233,W08-2123,0,0.0595137,"Missing"
C18-1233,N15-1121,0,0.27569,"Missing"
C18-1233,D15-1166,0,0.362193,"ically, to predict and label arguments for a given predicate, a role classifier is employed on top of the BiLSTM encoder. Some work like (Marcheggiani et al., 2017) shows that incorporating the predicate’s hidden state in their role classifier enhances the model performance, while we argue that a more natural way to incorporate the syntactic information carried by the predicate is to employ the attentional mechanism. Our model adopts the recently introduced biaffine attention (Dozat and Manning, 2017) to enhance our role scorer. Biaffine attention is a natural extension of bilinear attention (Luong et al., 2015) which is widely used in neural machine translation (NMT). Nonlinear Affine Transformation Usually, a BiLSTM decoder takes the concatenation gi of the hidden state vectors as output for each hidden state. However, in the SRL context, the encoder is supposed to distinguish the currently considered predicate from its candidate arguments. To this end, we perform two distinct affine transformations with a nonlinear activation on the hidden state gi , mapping it to vectors with smaller dimensionality:     (pred) (arg) hi = ReLU W (pred) gi + b(pred) , hi = ReLU W (arg) gi + b(arg) , (pred) wher"
C18-1233,D17-1159,0,0.228092,"Missing"
C18-1233,K17-1041,0,0.583365,"ied on sophisticated handcraft features or some declarative constraints (Pradhan et al., 2005; Zhao et al., 2009a), which suffers from poor efficiency and generalization ability. A recently tendency for SRL is adopting neural networks methods attributed to their significant success in a wide range of applications (Bai and Zhao, 2018; Zhang and Zhao, 2018). However, most of those works still heavily resort to syntactic features. Since the syntactic parsing task is equally hard as SRL and comes with its own errors, it is better to get rid of such prerequisite as in other NLP tasks. Accordingly, Marcheggiani et al. (2017) presented a neural model putting syntax aside for dependency-based SRL and obtain favorable results, which overturns the inherent belief that syntax is indispensable in SRL task (Punyakanok et al., 2008). Besides, SRL task is generally formulated as multi-step classification subtasks in pipeline systems, consisting of predicate identification, predicate disambiguation, argument identification and argument classification. Most previous SRL approaches adopt a pipeline framework to handle these subtasks one ∗ Corresponding author. This paper was partially supported by National Key Research and D"
C18-1233,D14-1162,0,0.0833981,"Missing"
C18-1233,N18-1202,0,0.0838452,"Missing"
C18-1233,P05-1072,0,0.790369,"en successfully applied to various downstream tasks, such as information extraction (Bastianelli et al., 2013), question answering (Shen and Lapata, 2007; Berant et al., 2013), machine translation (Xiong et al., 2012; Shi et al., 2016). Typically, SRL task can be put into two categories: constituent-based (i.e., phrase or span) SRL and dependency-based SRL. This paper will focus on the latter one popularized by CoNLL-2008 and 2009 shared tasks (Surdeanu et al., 2008; Hajiˇc et al., 2009). Most conventional SRL systems relied on sophisticated handcraft features or some declarative constraints (Pradhan et al., 2005; Zhao et al., 2009a), which suffers from poor efficiency and generalization ability. A recently tendency for SRL is adopting neural networks methods attributed to their significant success in a wide range of applications (Bai and Zhao, 2018; Zhang and Zhao, 2018). However, most of those works still heavily resort to syntactic features. Since the syntactic parsing task is equally hard as SRL and comes with its own errors, it is better to get rid of such prerequisite as in other NLP tasks. Accordingly, Marcheggiani et al. (2017) presented a neural model putting syntax aside for dependency-based"
C18-1233,J08-2005,0,0.550127,"is adopting neural networks methods attributed to their significant success in a wide range of applications (Bai and Zhao, 2018; Zhang and Zhao, 2018). However, most of those works still heavily resort to syntactic features. Since the syntactic parsing task is equally hard as SRL and comes with its own errors, it is better to get rid of such prerequisite as in other NLP tasks. Accordingly, Marcheggiani et al. (2017) presented a neural model putting syntax aside for dependency-based SRL and obtain favorable results, which overturns the inherent belief that syntax is indispensable in SRL task (Punyakanok et al., 2008). Besides, SRL task is generally formulated as multi-step classification subtasks in pipeline systems, consisting of predicate identification, predicate disambiguation, argument identification and argument classification. Most previous SRL approaches adopt a pipeline framework to handle these subtasks one ∗ Corresponding author. This paper was partially supported by National Key Research and Development Program of China (No. 2017YFB0304100), National Natural Science Foundation of China (No. 61672343 and No. 61733011), Key Project of National Society Science Foundation of China (No. 15-ZDA041),"
C18-1233,P17-1093,1,0.906833,"Missing"
C18-1233,P16-1113,0,0.61445,"Missing"
C18-1233,D07-1002,0,0.157099,"n the CoNLL-2008, 2009 benchmarks for both English and Chinese. To our best knowledge, we report the first syntax-agnostic SRL model that surpasses all known syntax-aware models. 1 Introduction Semantic role labeling (SRL) is a shallow semantic parsing, which is dedicated to identifying the semantic arguments of a predicate and labeling them with their semantic roles. SRL is considered as one of the core tasks in the natural language processing (NLP), which has been successfully applied to various downstream tasks, such as information extraction (Bastianelli et al., 2013), question answering (Shen and Lapata, 2007; Berant et al., 2013), machine translation (Xiong et al., 2012; Shi et al., 2016). Typically, SRL task can be put into two categories: constituent-based (i.e., phrase or span) SRL and dependency-based SRL. This paper will focus on the latter one popularized by CoNLL-2008 and 2009 shared tasks (Surdeanu et al., 2008; Hajiˇc et al., 2009). Most conventional SRL systems relied on sophisticated handcraft features or some declarative constraints (Pradhan et al., 2005; Zhao et al., 2009a), which suffers from poor efficiency and generalization ability. A recently tendency for SRL is adopting neural"
C18-1233,P16-1212,0,0.0218005,"we report the first syntax-agnostic SRL model that surpasses all known syntax-aware models. 1 Introduction Semantic role labeling (SRL) is a shallow semantic parsing, which is dedicated to identifying the semantic arguments of a predicate and labeling them with their semantic roles. SRL is considered as one of the core tasks in the natural language processing (NLP), which has been successfully applied to various downstream tasks, such as information extraction (Bastianelli et al., 2013), question answering (Shen and Lapata, 2007; Berant et al., 2013), machine translation (Xiong et al., 2012; Shi et al., 2016). Typically, SRL task can be put into two categories: constituent-based (i.e., phrase or span) SRL and dependency-based SRL. This paper will focus on the latter one popularized by CoNLL-2008 and 2009 shared tasks (Surdeanu et al., 2008; Hajiˇc et al., 2009). Most conventional SRL systems relied on sophisticated handcraft features or some declarative constraints (Pradhan et al., 2005; Zhao et al., 2009a), which suffers from poor efficiency and generalization ability. A recently tendency for SRL is adopting neural networks methods attributed to their significant success in a wide range of applic"
C18-1233,W08-2121,0,0.355239,"Missing"
C18-1233,N16-1064,1,0.847838,"domly initialized part-of-speech (POS) tag embedding e(pos) , a randomly initialized lemma embedding e(l) . Besides, since previous work (He et al., 2018) demonstrated that the predicate-specific feature is helpful in promoting the role labeling process, we employ an indicator embedding e(i) to indicate whether a word is a predicate when predicting and labeling the arguments for each given predicate. The final word representation is given by e = e(r) ⊕ e(p) ⊕ e(l) ⊕ e(pos) ⊕ e(i) , where ⊕ is the concatenation operator. Encoder As commonly used to model the sequential input in most NLP tasks (Wang et al., 2016; He et al., 2018), BiLSTM is adopted for our sentence encoder. By incorporating a stack of two distinct LSTMs, BiLSTM processes an input sequence in both forward and backward directions. In this way, the BiLSTM encoder provides the ability to incorporate the contextual information for each word. Given a sequence of word representation S = {e1 , e2 , · · · , eN } as input, the i-th hidden state gi is encoded as follows:     f b gif = LST M F ei , gi−1 , gib = LST M B ei , gi+1 , gi = gif ⊕ gib , where LST M F denotes the forward LSTM transformation and LST M B denotes the backward LSTM tra"
C18-1233,P12-1095,0,0.0490242,"our best knowledge, we report the first syntax-agnostic SRL model that surpasses all known syntax-aware models. 1 Introduction Semantic role labeling (SRL) is a shallow semantic parsing, which is dedicated to identifying the semantic arguments of a predicate and labeling them with their semantic roles. SRL is considered as one of the core tasks in the natural language processing (NLP), which has been successfully applied to various downstream tasks, such as information extraction (Bastianelli et al., 2013), question answering (Shen and Lapata, 2007; Berant et al., 2013), machine translation (Xiong et al., 2012; Shi et al., 2016). Typically, SRL task can be put into two categories: constituent-based (i.e., phrase or span) SRL and dependency-based SRL. This paper will focus on the latter one popularized by CoNLL-2008 and 2009 shared tasks (Surdeanu et al., 2008; Hajiˇc et al., 2009). Most conventional SRL systems relied on sophisticated handcraft features or some declarative constraints (Pradhan et al., 2005; Zhao et al., 2009a), which suffers from poor efficiency and generalization ability. A recently tendency for SRL is adopting neural networks methods attributed to their significant success in a w"
C18-1233,C18-1038,1,0.827707,"put into two categories: constituent-based (i.e., phrase or span) SRL and dependency-based SRL. This paper will focus on the latter one popularized by CoNLL-2008 and 2009 shared tasks (Surdeanu et al., 2008; Hajiˇc et al., 2009). Most conventional SRL systems relied on sophisticated handcraft features or some declarative constraints (Pradhan et al., 2005; Zhao et al., 2009a), which suffers from poor efficiency and generalization ability. A recently tendency for SRL is adopting neural networks methods attributed to their significant success in a wide range of applications (Bai and Zhao, 2018; Zhang and Zhao, 2018). However, most of those works still heavily resort to syntactic features. Since the syntactic parsing task is equally hard as SRL and comes with its own errors, it is better to get rid of such prerequisite as in other NLP tasks. Accordingly, Marcheggiani et al. (2017) presented a neural model putting syntax aside for dependency-based SRL and obtain favorable results, which overturns the inherent belief that syntax is indispensable in SRL task (Punyakanok et al., 2008). Besides, SRL task is generally formulated as multi-step classification subtasks in pipeline systems, consisting of predicate"
C18-1233,P16-1131,1,0.885424,"Missing"
C18-1233,W08-2127,1,0.717203,"vely. • Our work is the first attempt to apply end-to-end model for dependency-based SRL, which tackles the predicate disambiguation and the argument labeling subtasks in one shot. 2 Semantic Structure Decomposition SRL includes two subtasks: predicate identification/disambiguation and argument identification/labeling. Since the CoNLL-2009 dataset provides the gold predicates, most previous neural SRL systems use a default model to perform predicate disambiguation and focus on argument identification/labeling. Despite nearly all SRL work adopted the pipeline model with two or more components, Zhao and Kit (2008) and Zhao et al. (2013) presented an end-to-end solution for the entire SRL task with a word pair classifier. Following the same formulization, we propose the first neural SRL system that uniformly handles the tasks of predicate disambiguation and argument identification/labeling. In semantic dependency parsing, we can always identify two types of words, semantic head (predicate) and semantic dependent (argument). To build the needed predicate-argument structure, the model only needs to predict the role of any word pair from the given sentence. For the purpose, an additional role label None an"
C18-1233,W09-1209,1,0.953825,"d to various downstream tasks, such as information extraction (Bastianelli et al., 2013), question answering (Shen and Lapata, 2007; Berant et al., 2013), machine translation (Xiong et al., 2012; Shi et al., 2016). Typically, SRL task can be put into two categories: constituent-based (i.e., phrase or span) SRL and dependency-based SRL. This paper will focus on the latter one popularized by CoNLL-2008 and 2009 shared tasks (Surdeanu et al., 2008; Hajiˇc et al., 2009). Most conventional SRL systems relied on sophisticated handcraft features or some declarative constraints (Pradhan et al., 2005; Zhao et al., 2009a), which suffers from poor efficiency and generalization ability. A recently tendency for SRL is adopting neural networks methods attributed to their significant success in a wide range of applications (Bai and Zhao, 2018; Zhang and Zhao, 2018). However, most of those works still heavily resort to syntactic features. Since the syntactic parsing task is equally hard as SRL and comes with its own errors, it is better to get rid of such prerequisite as in other NLP tasks. Accordingly, Marcheggiani et al. (2017) presented a neural model putting syntax aside for dependency-based SRL and obtain fav"
C18-1233,D09-1004,1,0.949705,"d to various downstream tasks, such as information extraction (Bastianelli et al., 2013), question answering (Shen and Lapata, 2007; Berant et al., 2013), machine translation (Xiong et al., 2012; Shi et al., 2016). Typically, SRL task can be put into two categories: constituent-based (i.e., phrase or span) SRL and dependency-based SRL. This paper will focus on the latter one popularized by CoNLL-2008 and 2009 shared tasks (Surdeanu et al., 2008; Hajiˇc et al., 2009). Most conventional SRL systems relied on sophisticated handcraft features or some declarative constraints (Pradhan et al., 2005; Zhao et al., 2009a), which suffers from poor efficiency and generalization ability. A recently tendency for SRL is adopting neural networks methods attributed to their significant success in a wide range of applications (Bai and Zhao, 2018; Zhang and Zhao, 2018). However, most of those works still heavily resort to syntactic features. Since the syntactic parsing task is equally hard as SRL and comes with its own errors, it is better to get rid of such prerequisite as in other NLP tasks. Accordingly, Marcheggiani et al. (2017) presented a neural model putting syntax aside for dependency-based SRL and obtain fav"
C18-1233,W09-1208,1,0.929428,"d to various downstream tasks, such as information extraction (Bastianelli et al., 2013), question answering (Shen and Lapata, 2007; Berant et al., 2013), machine translation (Xiong et al., 2012; Shi et al., 2016). Typically, SRL task can be put into two categories: constituent-based (i.e., phrase or span) SRL and dependency-based SRL. This paper will focus on the latter one popularized by CoNLL-2008 and 2009 shared tasks (Surdeanu et al., 2008; Hajiˇc et al., 2009). Most conventional SRL systems relied on sophisticated handcraft features or some declarative constraints (Pradhan et al., 2005; Zhao et al., 2009a), which suffers from poor efficiency and generalization ability. A recently tendency for SRL is adopting neural networks methods attributed to their significant success in a wide range of applications (Bai and Zhao, 2018; Zhang and Zhao, 2018). However, most of those works still heavily resort to syntactic features. Since the syntactic parsing task is equally hard as SRL and comes with its own errors, it is better to get rid of such prerequisite as in other NLP tasks. Accordingly, Marcheggiani et al. (2017) presented a neural model putting syntax aside for dependency-based SRL and obtain fav"
C18-1233,P15-1109,0,0.0947973,"7YFB0304100), National Natural Science Foundation of China (No. 61672343 and No. 61733011), Key Project of National Society Science Foundation of China (No. 15-ZDA041), The Art and Science Interdisciplinary Funds of Shanghai Jiao Tong University (No. 14JCRZ04). This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http: //creativecommons.org/licenses/by/4.0/ 2753 Proceedings of the 27th International Conference on Computational Linguistics, pages 2753–2765 Santa Fe, New Mexico, USA, August 20-26, 2018. after another. Until recently, some works (Zhou and Xu, 2015; He et al., 2017) introduce end-to-end models for span-based SRL, which motivates us to explore integrative model for dependency SRL. In this work, we propose a syntactic-agnostic end-to-end system, dealing with predicate disambiguation and argument labeling in one model, unlike previous systems that treat the predicate disambiguation as a subtask and handle it separately. In detail, our model contains (1) a deep BiLSTM encoder, which is able to distinguish the predicates and arguments by mapping them into two different vector spaces, and (2) a biaffine attentional (Dozat and Manning, 2017) s"
C18-1271,P16-1231,0,0.284729,"th high-order feature. With the impressive success of deep neural networks in a wide range of NLP tasks (Cai and Zhao, 2017; Qin et al., 2017; He et al., 2018; Cai et al., 2018; Zhang et al., 2018; Bai and Zhao, 2018; Huang et al., 2018), neural models have been successfully applied to various dependency parsers (Li et al., 2018; Wang et al., 2017; Zhang et al., 2016). Dyer et al. (2015) introduced the stack LSTM to promote the transition-based parsing. Kiperwasser and Goldberg (2016) incorporated the bidirectional Long Short-Term Memory (BiLSTM) into both graph- and transition-based parsers. Andor et al. (2016) proposed globally normalized networks and achieved the best results of transition-based parsing, while the state-of-the-art result was reported in Dozat and Manning (2016), which proposed a deep biaffine model for graph-based parser. Though previous neural parsers have achieved such inspiring progresses, the majority of them heavily rely on the primitive parsing framework. Usually, those neural parsers build a network for feature extracting and use the neural features in place of handcrafted ones to predict some discrete actions in a traditional parsing algorithm. Until recently, few work (Wi"
C18-1271,C18-1048,1,0.807283,"ally design a beam search algorithm in the decoder side, which always guarantees the output to be a well-formed dependency tree. • To deal with the long-distance dependency, we introduce sub-root decomposition and it also alleviates the problem caused by too long seq2seq learning in the meantime. 2 Related Work Early researches (Ma and Zhao, 2012; Martins et al., 2013) achieved comparable results with high-order feature. With the impressive success of deep neural networks in a wide range of NLP tasks (Cai and Zhao, 2017; Qin et al., 2017; He et al., 2018; Cai et al., 2018; Zhang et al., 2018; Bai and Zhao, 2018; Huang et al., 2018), neural models have been successfully applied to various dependency parsers (Li et al., 2018; Wang et al., 2017; Zhang et al., 2016). Dyer et al. (2015) introduced the stack LSTM to promote the transition-based parsing. Kiperwasser and Goldberg (2016) incorporated the bidirectional Long Short-Term Memory (BiLSTM) into both graph- and transition-based parsers. Andor et al. (2016) proposed globally normalized networks and achieved the best results of transition-based parsing, while the state-of-the-art result was reported in Dozat and Manning (2016), which proposed a deep b"
C18-1271,C18-1233,1,0.763718,"nsition parsing algorithm. • We especially design a beam search algorithm in the decoder side, which always guarantees the output to be a well-formed dependency tree. • To deal with the long-distance dependency, we introduce sub-root decomposition and it also alleviates the problem caused by too long seq2seq learning in the meantime. 2 Related Work Early researches (Ma and Zhao, 2012; Martins et al., 2013) achieved comparable results with high-order feature. With the impressive success of deep neural networks in a wide range of NLP tasks (Cai and Zhao, 2017; Qin et al., 2017; He et al., 2018; Cai et al., 2018; Zhang et al., 2018; Bai and Zhao, 2018; Huang et al., 2018), neural models have been successfully applied to various dependency parsers (Li et al., 2018; Wang et al., 2017; Zhang et al., 2016). Dyer et al. (2015) introduced the stack LSTM to promote the transition-based parsing. Kiperwasser and Goldberg (2016) incorporated the bidirectional Long Short-Term Memory (BiLSTM) into both graph- and transition-based parsers. Andor et al. (2016) proposed globally normalized networks and achieved the best results of transition-based parsing, while the state-of-the-art result was reported in Dozat and"
C18-1271,D14-1082,0,0.0358514,"ay processing the PTB and CTB: For PTB, applying Stanford basic dependencies (SD) representation (De Marneffe et al., 2006), using sections 2-21 for training, section 22 for development and section 23 for testing as same as the standard splitting and tagging POS tag with the Stanford tagger (Toutanova et al., 2003). For CTB, adopting Penn2Malt tool for conversion, splitting the dataset by sections 001-815, 10011136 for training, sections 886-931, 1148-1151 for development, and sections 816-885, 1137-1147 for testing as (Zhang and Clark, 2008) and using the golden segmentation and POS tags as (Chen and Manning, 2014). 5.1 Setup In the experiments, our seq2seq parser implemented based on the OpenNMT-py project (Klein et al., 2017), in which employs a 4-layer Bi-LSTM as encoder and a 2-layer LSTM as decoder. The parameters are randomly initialized and optimized using the Adam algorithm with mini-batch size of 64. The initial learning rate is set to 0.001 (with β1 = 0.9, β2 = 0.999), and decays by 0.5 every epoch after running 20 epochs. In training phase, we adopt a dropout rate of 0.3. The size of the vocabulary in the target side is limited to 100 by setting the maximum relative position to 50, namely, th"
C18-1271,de-marneffe-etal-2006-generating,0,0.441972,"Missing"
C18-1271,P15-1033,0,0.027223,"we introduce sub-root decomposition and it also alleviates the problem caused by too long seq2seq learning in the meantime. 2 Related Work Early researches (Ma and Zhao, 2012; Martins et al., 2013) achieved comparable results with high-order feature. With the impressive success of deep neural networks in a wide range of NLP tasks (Cai and Zhao, 2017; Qin et al., 2017; He et al., 2018; Cai et al., 2018; Zhang et al., 2018; Bai and Zhao, 2018; Huang et al., 2018), neural models have been successfully applied to various dependency parsers (Li et al., 2018; Wang et al., 2017; Zhang et al., 2016). Dyer et al. (2015) introduced the stack LSTM to promote the transition-based parsing. Kiperwasser and Goldberg (2016) incorporated the bidirectional Long Short-Term Memory (BiLSTM) into both graph- and transition-based parsers. Andor et al. (2016) proposed globally normalized networks and achieved the best results of transition-based parsing, while the state-of-the-art result was reported in Dozat and Manning (2016), which proposed a deep biaffine model for graph-based parser. Though previous neural parsers have achieved such inspiring progresses, the majority of them heavily rely on the primitive parsing frame"
C18-1271,P96-1011,0,0.192981,"Missing"
C18-1271,P18-1192,1,0.56054,"using a known transition parsing algorithm. • We especially design a beam search algorithm in the decoder side, which always guarantees the output to be a well-formed dependency tree. • To deal with the long-distance dependency, we introduce sub-root decomposition and it also alleviates the problem caused by too long seq2seq learning in the meantime. 2 Related Work Early researches (Ma and Zhao, 2012; Martins et al., 2013) achieved comparable results with high-order feature. With the impressive success of deep neural networks in a wide range of NLP tasks (Cai and Zhao, 2017; Qin et al., 2017; He et al., 2018; Cai et al., 2018; Zhang et al., 2018; Bai and Zhao, 2018; Huang et al., 2018), neural models have been successfully applied to various dependency parsers (Li et al., 2018; Wang et al., 2017; Zhang et al., 2016). Dyer et al. (2015) introduced the stack LSTM to promote the transition-based parsing. Kiperwasser and Goldberg (2016) incorporated the bidirectional Long Short-Term Memory (BiLSTM) into both graph- and transition-based parsers. Andor et al. (2016) proposed globally normalized networks and achieved the best results of transition-based parsing, while the state-of-the-art result was rep"
C18-1271,P18-4024,1,0.509308,"earch algorithm in the decoder side, which always guarantees the output to be a well-formed dependency tree. • To deal with the long-distance dependency, we introduce sub-root decomposition and it also alleviates the problem caused by too long seq2seq learning in the meantime. 2 Related Work Early researches (Ma and Zhao, 2012; Martins et al., 2013) achieved comparable results with high-order feature. With the impressive success of deep neural networks in a wide range of NLP tasks (Cai and Zhao, 2017; Qin et al., 2017; He et al., 2018; Cai et al., 2018; Zhang et al., 2018; Bai and Zhao, 2018; Huang et al., 2018), neural models have been successfully applied to various dependency parsers (Li et al., 2018; Wang et al., 2017; Zhang et al., 2016). Dyer et al. (2015) introduced the stack LSTM to promote the transition-based parsing. Kiperwasser and Goldberg (2016) incorporated the bidirectional Long Short-Term Memory (BiLSTM) into both graph- and transition-based parsers. Andor et al. (2016) proposed globally normalized networks and achieved the best results of transition-based parsing, while the state-of-the-art result was reported in Dozat and Manning (2016), which proposed a deep biaffine model for gra"
C18-1271,Q16-1023,0,0.21733,"ong seq2seq learning in the meantime. 2 Related Work Early researches (Ma and Zhao, 2012; Martins et al., 2013) achieved comparable results with high-order feature. With the impressive success of deep neural networks in a wide range of NLP tasks (Cai and Zhao, 2017; Qin et al., 2017; He et al., 2018; Cai et al., 2018; Zhang et al., 2018; Bai and Zhao, 2018; Huang et al., 2018), neural models have been successfully applied to various dependency parsers (Li et al., 2018; Wang et al., 2017; Zhang et al., 2016). Dyer et al. (2015) introduced the stack LSTM to promote the transition-based parsing. Kiperwasser and Goldberg (2016) incorporated the bidirectional Long Short-Term Memory (BiLSTM) into both graph- and transition-based parsers. Andor et al. (2016) proposed globally normalized networks and achieved the best results of transition-based parsing, while the state-of-the-art result was reported in Dozat and Manning (2016), which proposed a deep biaffine model for graph-based parser. Though previous neural parsers have achieved such inspiring progresses, the majority of them heavily rely on the primitive parsing framework. Usually, those neural parsers build a network for feature extracting and use the neural featu"
C18-1271,P17-4012,0,0.0125941,"006), using sections 2-21 for training, section 22 for development and section 23 for testing as same as the standard splitting and tagging POS tag with the Stanford tagger (Toutanova et al., 2003). For CTB, adopting Penn2Malt tool for conversion, splitting the dataset by sections 001-815, 10011136 for training, sections 886-931, 1148-1151 for development, and sections 816-885, 1137-1147 for testing as (Zhang and Clark, 2008) and using the golden segmentation and POS tags as (Chen and Manning, 2014). 5.1 Setup In the experiments, our seq2seq parser implemented based on the OpenNMT-py project (Klein et al., 2017), in which employs a 4-layer Bi-LSTM as encoder and a 2-layer LSTM as decoder. The parameters are randomly initialized and optimized using the Adam algorithm with mini-batch size of 64. The initial learning rate is set to 0.001 (with β1 = 0.9, β2 = 0.999), and decays by 0.5 every epoch after running 20 epochs. In training phase, we adopt a dropout rate of 0.3. The size of the vocabulary in the target side is limited to 100 by setting the maximum relative position to 50, namely, the target side vocabulary is: Vt = {L1, L2, · · · , L50, R1, R2, · · · , R50} In our experiments, the beam size is s"
C18-1271,E17-1117,0,0.0395164,"Missing"
C18-1271,D15-1166,0,0.011044,"convert each head into a position representation, encoding relative distance between word and its head. When working, the parser picks a head for each word by predicting the relative position of the head. Our experiments show that the conversion is effective in bounding the output of our seq2seq parser. Given a word wi and its head word wj , the relative position representation of wj is obtained by:  Ri,j = Lj−i if Ri−j if i &lt; j, i &gt; j. Figure 2 illustrates our relative position tag encoding for the output dependency structures. 3.3 Attention Mechanism As usual, we employ an attention layer (Luong et al., 2015) to encode the context for each word. The context vector cj at the j-step of the decoding process is calculated as the weighted sum of the hidden states of the input sequence: N X cj = αj (i)hi . i=1 The attention weight αj (i) between the i-th encoder hidden state hi and the j-th decoder hidden state hj is computed by a softmax function: v T tanh(W (a) [hi ; hj ]) αj (i) = PN , T tanh(W (a) [h ; h ]) v j k k=1 where [hi ; hj ] and [hk ; hj ] denote the vector concatenation of the rows, W (a) and v are learnable parameters. 3206 &lt;ROOT&gt; nsubj aux dobj That has outraged some fans . R2 R1 L3 R1 L"
C18-1271,C12-2077,1,0.879729,"h does not rely on any transition sequence by directly predicting the head position for each word in the sentence, unlike all previous work that first predicted a transition sequence and then built dependency tree using a known transition parsing algorithm. • We especially design a beam search algorithm in the decoder side, which always guarantees the output to be a well-formed dependency tree. • To deal with the long-distance dependency, we introduce sub-root decomposition and it also alleviates the problem caused by too long seq2seq learning in the meantime. 2 Related Work Early researches (Ma and Zhao, 2012; Martins et al., 2013) achieved comparable results with high-order feature. With the impressive success of deep neural networks in a wide range of NLP tasks (Cai and Zhao, 2017; Qin et al., 2017; He et al., 2018; Cai et al., 2018; Zhang et al., 2018; Bai and Zhao, 2018; Huang et al., 2018), neural models have been successfully applied to various dependency parsers (Li et al., 2018; Wang et al., 2017; Zhang et al., 2016). Dyer et al. (2015) introduced the stack LSTM to promote the transition-based parsing. Kiperwasser and Goldberg (2016) incorporated the bidirectional Long Short-Term Memory (B"
C18-1271,P13-2109,0,0.0177811,"any transition sequence by directly predicting the head position for each word in the sentence, unlike all previous work that first predicted a transition sequence and then built dependency tree using a known transition parsing algorithm. • We especially design a beam search algorithm in the decoder side, which always guarantees the output to be a well-formed dependency tree. • To deal with the long-distance dependency, we introduce sub-root decomposition and it also alleviates the problem caused by too long seq2seq learning in the meantime. 2 Related Work Early researches (Ma and Zhao, 2012; Martins et al., 2013) achieved comparable results with high-order feature. With the impressive success of deep neural networks in a wide range of NLP tasks (Cai and Zhao, 2017; Qin et al., 2017; He et al., 2018; Cai et al., 2018; Zhang et al., 2018; Bai and Zhao, 2018; Huang et al., 2018), neural models have been successfully applied to various dependency parsers (Li et al., 2018; Wang et al., 2017; Zhang et al., 2016). Dyer et al. (2015) introduced the stack LSTM to promote the transition-based parsing. Kiperwasser and Goldberg (2016) incorporated the bidirectional Long Short-Term Memory (BiLSTM) into both graph-"
C18-1271,P05-1012,0,0.0609872,"Missing"
C18-1271,P08-1108,0,0.116949,"Missing"
C18-1271,W03-3017,0,0.514111,"Missing"
C18-1271,D14-1162,0,0.0812892,"rocesses an input sequence in both directions by incorporating a stack of two distinct LSTMs. Given an input sentence X = {w1 , · · · , wL }, the i-th element wi is encoded by first feeding its embedding representation ei in to two distinguish LSTMs: the forward LSTM and the backward LSTM, to obtain two hidden state vectors: hfi and hbi respectively, and then concatenating the two vectors as hi = [hfi ; hbi ]. 3.1.1 Source Representation Given a vocabulary W , each individual word wi ∈ W is mapped into a real-valued vector (word embedding) w ∈ Rm where m is the dimension. We employ the GloVe (Pennington et al., 2014) and Node2Vec (Grover and Leskovec, 2016) to generate the pre-trained word embedding, obtaining two distinct embedding for each word. It is worth noting that Node2Vec is an embedding method that is capable of encoding the topological information of inside structure. To further incorporate the structural information of dependency tree, we perform a Node2Vec pre-training on the training set. Besides, our model adopts subword which is obtained though BPE segmentation (Sennrich et al., 2015) broadly used in neural machine translation (NMT), and character augment embedding with AllenNLP toolkit acc"
C18-1271,N18-1202,0,0.0187954,"egmentation (Sennrich et al., 2015) broadly used in neural machine translation (NMT), and character augment embedding with AllenNLP toolkit according to (Gardner et al., 2017). A subword dictionary is built by running BPE on the Wikipedia, which segments each word into several subwords. Each subword in the dictionary is mapped into a real-valued vector (subword embedding). In our experiments, the subword embedding is randomly initialized and then trained jointly with other components of the network. To get the representation of the original word, we use an additional neural network component (Peters et al., 2018) to distill the character and subword embedding for representation. Our model also employs the part-of-speech (POS) tag embedding following previous works (Kiperwasser and Goldberg, 2016; Dozat and Manning, 2016). The 3205 POS embedding is randomly initialized and jointly trained with other model parameters too. The adopted word representation of our model is the concatenation of all above-mentioned embeddings: e = [eg ; en ; es ; ec ; ep ], where eg is the GloVe embedding, en is the Node2Vec embedding, es is the subword embedding, ec is the character embedding and ep is the POS tag embedding."
C18-1271,P17-1093,1,0.677303,"t dependency tree using a known transition parsing algorithm. • We especially design a beam search algorithm in the decoder side, which always guarantees the output to be a well-formed dependency tree. • To deal with the long-distance dependency, we introduce sub-root decomposition and it also alleviates the problem caused by too long seq2seq learning in the meantime. 2 Related Work Early researches (Ma and Zhao, 2012; Martins et al., 2013) achieved comparable results with high-order feature. With the impressive success of deep neural networks in a wide range of NLP tasks (Cai and Zhao, 2017; Qin et al., 2017; He et al., 2018; Cai et al., 2018; Zhang et al., 2018; Bai and Zhao, 2018; Huang et al., 2018), neural models have been successfully applied to various dependency parsers (Li et al., 2018; Wang et al., 2017; Zhang et al., 2016). Dyer et al. (2015) introduced the stack LSTM to promote the transition-based parsing. Kiperwasser and Goldberg (2016) incorporated the bidirectional Long Short-Term Memory (BiLSTM) into both graph- and transition-based parsers. Andor et al. (2016) proposed globally normalized networks and achieved the best results of transition-based parsing, while the state-of-the-a"
C18-1271,N03-1033,0,0.0688859,"of the proposed beam search decoding with tree constraint. 5 Experiments The proposed seq2seq parser is evaluated on the Penn Treebank (PTB) and the Chinese Treebank (CTB 5.1) with the unlabled attachment score (UAS) and the labeled attachment score (LAS) metrics (excluding punctuation). Follow the usual way processing the PTB and CTB: For PTB, applying Stanford basic dependencies (SD) representation (De Marneffe et al., 2006), using sections 2-21 for training, section 22 for development and section 23 for testing as same as the standard splitting and tagging POS tag with the Stanford tagger (Toutanova et al., 2003). For CTB, adopting Penn2Malt tool for conversion, splitting the dataset by sections 001-815, 10011136 for training, sections 886-931, 1148-1151 for development, and sections 816-885, 1137-1147 for testing as (Zhang and Clark, 2008) and using the golden segmentation and POS tags as (Chen and Manning, 2014). 5.1 Setup In the experiments, our seq2seq parser implemented based on the OpenNMT-py project (Klein et al., 2017), in which employs a 4-layer Bi-LSTM as encoder and a 2-layer LSTM as decoder. The parameters are randomly initialized and optimized using the Adam algorithm with mini-batch size"
C18-1271,P16-1218,0,0.0391538,"Missing"
C18-1271,K17-3020,1,0.87191,"deal with the long-distance dependency, we introduce sub-root decomposition and it also alleviates the problem caused by too long seq2seq learning in the meantime. 2 Related Work Early researches (Ma and Zhao, 2012; Martins et al., 2013) achieved comparable results with high-order feature. With the impressive success of deep neural networks in a wide range of NLP tasks (Cai and Zhao, 2017; Qin et al., 2017; He et al., 2018; Cai et al., 2018; Zhang et al., 2018; Bai and Zhao, 2018; Huang et al., 2018), neural models have been successfully applied to various dependency parsers (Li et al., 2018; Wang et al., 2017; Zhang et al., 2016). Dyer et al. (2015) introduced the stack LSTM to promote the transition-based parsing. Kiperwasser and Goldberg (2016) incorporated the bidirectional Long Short-Term Memory (BiLSTM) into both graph- and transition-based parsers. Andor et al. (2016) proposed globally normalized networks and achieved the best results of transition-based parsing, while the state-of-the-art result was reported in Dozat and Manning (2016), which proposed a deep biaffine model for graph-based parser. Though previous neural parsers have achieved such inspiring progresses, the majority of them he"
C18-1271,P15-1032,0,0.0274142,"orms a fully generative decoding in one pass by using an attention layer to keep around the context of each word. Another line of studies related to this work focus on beam search, which have attracted much interest and have been adopted to improve the performance of greedy parsers. Beam search commonly adopts approximate strategy for computational tractability, which aims at including more sophisticated features within reasonable cost. Zhang and Clark (2008) proposed a beam search based parser applied to integrate graph-based and transition-based parsers. Most transition-based neural models (Weiss et al., 2015; Zhou et al., 2015; Andor et al., 2016) incorporated the structured perceptron with beam search decoding, resulting in substantial improvement of accuracy. Additionally, beam search has been also incorporated in the seq2seq framework (Wiseman and Rush, 2016). Following the previous works, we also adopt a beam search in our decoder. However, since our parser aims at predicting a structure instead of sequence, we especially design a beam search algorithm with several searching constraints, which enforces the tree structure of the outputs. 3204 X That has outraged some fans . &lt;EOS&gt; Encoder h0 h1"
C18-1271,D16-1137,0,0.111035,"opt rich features but subjected to limited searching space. Besides, ensemble or hybrid methods on both the basic models have been well studied (Nivre and Mcdonald, 2008; Zhang and Clark, 2008). Most traditional dependency parsers rely heavily on feature engineering, especially for graph-based parser, which suffers from poor efficiency and generalization ability. Recent tendency for dependency parsing is adopting neural networks due to their significant success in a wide range of applications. Specially, leveraging sequence-to-sequence (seq2seq) model for dependency parsing started to appear (Wiseman and Rush, 2016; Zhang et al., 2017b). The recently proposed seq2seq parsers focus on predicting a transition sequence to build a dependency tree, which makes them actually fall back into the transition-based model constrained by the adopted transition parsing algorithm. In this paper, we propose a seq2seq parser with a novel parsing structure encoding independent of transition parsing operation sequence. The proposed parser first directly generates the head position for each word in an input sentence using a seq2seq model, and then employs a BiLSTM-CRF model (Huang et al., 2015) to predict the relation labe"
C18-1271,D08-1059,0,0.390302,"ng algorithm. Secondly, each time the head selection parser selects a head, it relies on referring to each pair of words in the sentence. While our parser performs a fully generative decoding in one pass by using an attention layer to keep around the context of each word. Another line of studies related to this work focus on beam search, which have attracted much interest and have been adopted to improve the performance of greedy parsers. Beam search commonly adopts approximate strategy for computational tractability, which aims at including more sophisticated features within reasonable cost. Zhang and Clark (2008) proposed a beam search based parser applied to integrate graph-based and transition-based parsers. Most transition-based neural models (Weiss et al., 2015; Zhou et al., 2015; Andor et al., 2016) incorporated the structured perceptron with beam search decoding, resulting in substantial improvement of accuracy. Additionally, beam search has been also incorporated in the seq2seq framework (Wiseman and Rush, 2016). Following the previous works, we also adopt a beam search in our decoder. However, since our parser aims at predicting a structure instead of sequence, we especially design a beam sear"
C18-1271,D12-1030,0,0.0586133,"Missing"
C18-1271,P14-2107,0,0.0391151,"Missing"
C18-1271,P16-1131,1,0.86545,"distance dependency, we introduce sub-root decomposition and it also alleviates the problem caused by too long seq2seq learning in the meantime. 2 Related Work Early researches (Ma and Zhao, 2012; Martins et al., 2013) achieved comparable results with high-order feature. With the impressive success of deep neural networks in a wide range of NLP tasks (Cai and Zhao, 2017; Qin et al., 2017; He et al., 2018; Cai et al., 2018; Zhang et al., 2018; Bai and Zhao, 2018; Huang et al., 2018), neural models have been successfully applied to various dependency parsers (Li et al., 2018; Wang et al., 2017; Zhang et al., 2016). Dyer et al. (2015) introduced the stack LSTM to promote the transition-based parsing. Kiperwasser and Goldberg (2016) incorporated the bidirectional Long Short-Term Memory (BiLSTM) into both graph- and transition-based parsers. Andor et al. (2016) proposed globally normalized networks and achieved the best results of transition-based parsing, while the state-of-the-art result was reported in Dozat and Manning (2016), which proposed a deep biaffine model for graph-based parser. Though previous neural parsers have achieved such inspiring progresses, the majority of them heavily rely on the pri"
C18-1271,E17-1063,0,0.0760283,"bjected to limited searching space. Besides, ensemble or hybrid methods on both the basic models have been well studied (Nivre and Mcdonald, 2008; Zhang and Clark, 2008). Most traditional dependency parsers rely heavily on feature engineering, especially for graph-based parser, which suffers from poor efficiency and generalization ability. Recent tendency for dependency parsing is adopting neural networks due to their significant success in a wide range of applications. Specially, leveraging sequence-to-sequence (seq2seq) model for dependency parsing started to appear (Wiseman and Rush, 2016; Zhang et al., 2017b). The recently proposed seq2seq parsers focus on predicting a transition sequence to build a dependency tree, which makes them actually fall back into the transition-based model constrained by the adopted transition parsing algorithm. In this paper, we propose a seq2seq parser with a novel parsing structure encoding independent of transition parsing operation sequence. The proposed parser first directly generates the head position for each word in an input sentence using a seq2seq model, and then employs a BiLSTM-CRF model (Huang et al., 2015) to predict the relation label for each determine"
C18-1271,D17-1175,0,0.479829,"bjected to limited searching space. Besides, ensemble or hybrid methods on both the basic models have been well studied (Nivre and Mcdonald, 2008; Zhang and Clark, 2008). Most traditional dependency parsers rely heavily on feature engineering, especially for graph-based parser, which suffers from poor efficiency and generalization ability. Recent tendency for dependency parsing is adopting neural networks due to their significant success in a wide range of applications. Specially, leveraging sequence-to-sequence (seq2seq) model for dependency parsing started to appear (Wiseman and Rush, 2016; Zhang et al., 2017b). The recently proposed seq2seq parsers focus on predicting a transition sequence to build a dependency tree, which makes them actually fall back into the transition-based model constrained by the adopted transition parsing algorithm. In this paper, we propose a seq2seq parser with a novel parsing structure encoding independent of transition parsing operation sequence. The proposed parser first directly generates the head position for each word in an input sentence using a seq2seq model, and then employs a BiLSTM-CRF model (Huang et al., 2015) to predict the relation label for each determine"
C18-1271,C18-1317,1,0.601604,"gorithm. • We especially design a beam search algorithm in the decoder side, which always guarantees the output to be a well-formed dependency tree. • To deal with the long-distance dependency, we introduce sub-root decomposition and it also alleviates the problem caused by too long seq2seq learning in the meantime. 2 Related Work Early researches (Ma and Zhao, 2012; Martins et al., 2013) achieved comparable results with high-order feature. With the impressive success of deep neural networks in a wide range of NLP tasks (Cai and Zhao, 2017; Qin et al., 2017; He et al., 2018; Cai et al., 2018; Zhang et al., 2018; Bai and Zhao, 2018; Huang et al., 2018), neural models have been successfully applied to various dependency parsers (Li et al., 2018; Wang et al., 2017; Zhang et al., 2016). Dyer et al. (2015) introduced the stack LSTM to promote the transition-based parsing. Kiperwasser and Goldberg (2016) incorporated the bidirectional Long Short-Term Memory (BiLSTM) into both graph- and transition-based parsers. Andor et al. (2016) proposed globally normalized networks and achieved the best results of transition-based parsing, while the state-of-the-art result was reported in Dozat and Manning (2016), whi"
C18-1271,P15-1117,0,0.0124083,"ive decoding in one pass by using an attention layer to keep around the context of each word. Another line of studies related to this work focus on beam search, which have attracted much interest and have been adopted to improve the performance of greedy parsers. Beam search commonly adopts approximate strategy for computational tractability, which aims at including more sophisticated features within reasonable cost. Zhang and Clark (2008) proposed a beam search based parser applied to integrate graph-based and transition-based parsers. Most transition-based neural models (Weiss et al., 2015; Zhou et al., 2015; Andor et al., 2016) incorporated the structured perceptron with beam search decoding, resulting in substantial improvement of accuracy. Additionally, beam search has been also incorporated in the seq2seq framework (Wiseman and Rush, 2016). Following the previous works, we also adopt a beam search in our decoder. However, since our parser aims at predicting a structure instead of sequence, we especially design a beam search algorithm with several searching constraints, which enforces the tree structure of the outputs. 3204 X That has outraged some fans . &lt;EOS&gt; Encoder h0 h1 h2 h3 h4 h5 h6 Dec"
C18-1271,P15-1112,0,0.0608782,"Missing"
C18-2024,P17-4017,0,0.0317586,"ion retrieval augmented chatbot which is able to answer questions based on given product introduction document and deal with multi-turn conversations. We will introduce a finegrained pipeline processing to distill responses based on unstructured documents, and attentive sequential context-response matching for multi-turn conversations. 1 Introduction Recently, dialogue and interactive systems have been emerging with huge commercial values (Qiu et al., 2017; Yan et al., 2016a; Zhang et al., 2017; Huang et al., 2018; Zhang et al., 2018b; Zhang et al., 2018a), especially in the e-commerce field (Cui et al., 2017; Yan et al., 2016b). Building a chatbot mainly faces two challenges, the lack of dialogue data and poor performance for multi-turn conversations. This paper describes a fine-grained information retrieval (IR) augmented multi-turn chatbot - Lingke. It can learn knowledge without human supervision from conversation records or given product introduction documents and generate proper response, which alleviates the problem of lacking dialogue corpus to train a chatbot. First, by using Apache Lucene1 to select top 2 sentences most relevant to the question and extracting subject-verb-object (SVO) tr"
C18-2024,D11-1142,0,0.0132992,"re precise processing, we need to roughly select sentences which are relevant with the current message. We used Apache Lucene to accomplish the retrieval. Given sentence collection A from step 1, we retrieve k relevant sentences E = {E1 , E2 , ..., Ek }. In our system, the value of k is 2. Candidate Responses Generation Generally, the response to a conversation can be expressed as a simple sentence, even a few of words. However, sentences from a product introduction document are usually complicated with much information. To extract SVO, we used an open information extraction framework ReVerb (Fader et al., 2011), which is able to recognize more than one group of SVO triples (including triples from the clauses). Figure 2 shows an example. Based on an utterance Ei from E, we extract its SVO triples Es = {Es1 , Es2 , ..., Esn }, Ev = {Ev1 , Ev2 , ..., Evn }, Eo = {Eo1 , Eo2 , ..., Eon }, and by concatenating each triple, we obtain multiple of simple sentences T = {T1 , T2 , ..., Tn }. The above first three steps generate all sentences and phrases as candidate responses, which are denoted as R = E ∪ T . What we need to do next is to rerank the candidates for the most proper response. Dialogue Manager We"
C18-2024,P18-4024,1,0.833637,"multi-turn the performance is usually unsatisfactory. In this paper, we present Lingke, an information retrieval augmented chatbot which is able to answer questions based on given product introduction document and deal with multi-turn conversations. We will introduce a finegrained pipeline processing to distill responses based on unstructured documents, and attentive sequential context-response matching for multi-turn conversations. 1 Introduction Recently, dialogue and interactive systems have been emerging with huge commercial values (Qiu et al., 2017; Yan et al., 2016a; Zhang et al., 2017; Huang et al., 2018; Zhang et al., 2018b; Zhang et al., 2018a), especially in the e-commerce field (Cui et al., 2017; Yan et al., 2016b). Building a chatbot mainly faces two challenges, the lack of dialogue data and poor performance for multi-turn conversations. This paper describes a fine-grained information retrieval (IR) augmented multi-turn chatbot - Lingke. It can learn knowledge without human supervision from conversation records or given product introduction documents and generate proper response, which alleviates the problem of lacking dialogue corpus to train a chatbot. First, by using Apache Lucene1 to"
C18-2024,W15-4640,0,0.0552763,"al., 2014) achieved by a generic deep learning framework OpenNMT 3 . The model is trained on twitter conversation data, which has 370K query-reply pairs, and 300K non-duplicate pairs are selected for training. 3 Experiment Dataset We evaluate Lingke on a dataset from our Taobao4 partners, which is a collection of conversation records between customers and customer service staffs. It contains over five kinds of conversations, including chit-chat, product and discount consultation, querying delivery progress and after-sales feedback. We converted it into the structured multi-turn format as in (Lowe et al., 2015; Wu et al., 2017). The training set has 1 million multi-turn dialogues totally, and 10K respectively in validation and test set. R10 @1 R10 @2 R10 @5 TF-IDF 0.159 0.256 0.477 RNN 0.325 0.463 0.775 CNN 0.328 0.515 0.792 LSTM 0.365 0.536 0.828 BiLSTM 0.355 0.525 0.825 Multi-View 0.421 0.601 0.861 SMN 0.453 0.654 0.886 Our model 0.476 0.672 0.893 Table 1: Comparison of different models. Evaluation Our model is compared with recent single-turn and multi-turn models, of which the former are in (Kadlec et al., 2015; Lowe et al., 2015) including TF-IDF, Convolutional Neural Network (CNN), Recurrent"
C18-2024,P14-5010,0,0.00285434,"core CPU. An i7-7700HQ quad-core processor is twice as powerful as a dualcore CPU. Figure 2: Example of SVO Extraction. r GRU Self-matching Attention Response Matching Attentive Turns Aggregation Figure 3: Structure overview of the dialogue manager. Coreference Resolution and Document Separation Since the response is usually supposed to be concise and coherent, we first separate a given document into sentences. However, long documents commonly involve complex reference relations. A direct segmentation might result in severe information loss. So before the separation, we used Stanford CoreNLP (Manning et al., 2014) 2 to accomplish the coreference resolution. After the resolution, we cut the document into sentences A = {A1 , A2 , ..., An }. Target Sentences Retrieval There is abundant information in the whole document, but what current message cares about just exists in some paragraphs or even sentences. So before precise processing, we need to roughly select sentences which are relevant with the current message. We used Apache Lucene to accomplish the retrieval. Given sentence collection A from step 1, we retrieve k relevant sentences E = {E1 , E2 , ..., Ek }. In our system, the value of k is 2. Candida"
C18-2024,P17-2079,0,0.0184971,"can easily deal with single-turn question answering, for multi-turn the performance is usually unsatisfactory. In this paper, we present Lingke, an information retrieval augmented chatbot which is able to answer questions based on given product introduction document and deal with multi-turn conversations. We will introduce a finegrained pipeline processing to distill responses based on unstructured documents, and attentive sequential context-response matching for multi-turn conversations. 1 Introduction Recently, dialogue and interactive systems have been emerging with huge commercial values (Qiu et al., 2017; Yan et al., 2016a; Zhang et al., 2017; Huang et al., 2018; Zhang et al., 2018b; Zhang et al., 2018a), especially in the e-commerce field (Cui et al., 2017; Yan et al., 2016b). Building a chatbot mainly faces two challenges, the lack of dialogue data and poor performance for multi-turn conversations. This paper describes a fine-grained information retrieval (IR) augmented multi-turn chatbot - Lingke. It can learn knowledge without human supervision from conversation records or given product introduction documents and generate proper response, which alleviates the problem of lacking dialogue c"
C18-2024,P17-1018,0,0.0138137,"sequential utterance-response matching to develop a multi-turn dialogue manager. Figure 3 shows the structure. (1) Self-matching Attention Since not all of the information is useful, it is a natural thought that adopts self-matching attention strategy to filter redundant information. Before that, we transform raw dialogue data into word embedding (Mikolov et al., 2013) firstly. Each conversation utterance or candidate 2 https://stanfordnlp.github.io/CoreNLP/index.html 109 response is fed to the gated recurrent units (GRUs) (Cho et al., 2014). Then, we adopt a self-matching attention strategy (Wang et al., 2017) to directly match each utterance or response against itself to distill the pivotal information and filter irrelevant pieces. (2) Response Selection Following Sequential Matching Network (SMN) (Wu et al., 2017), we employ sequential matching for multi-turn response selection. Given the candidate response set, it matches each response with the conversation utterances in chronological order and obtains accumulated matching score of the utterance-response pairs to capture significant information and relations among utterances and each candidate response. The one with highest matching score is sel"
C18-2024,P17-1046,0,0.260709,"adopts self-matching attention strategy to filter redundant information. Before that, we transform raw dialogue data into word embedding (Mikolov et al., 2013) firstly. Each conversation utterance or candidate 2 https://stanfordnlp.github.io/CoreNLP/index.html 109 response is fed to the gated recurrent units (GRUs) (Cho et al., 2014). Then, we adopt a self-matching attention strategy (Wang et al., 2017) to directly match each utterance or response against itself to distill the pivotal information and filter irrelevant pieces. (2) Response Selection Following Sequential Matching Network (SMN) (Wu et al., 2017), we employ sequential matching for multi-turn response selection. Given the candidate response set, it matches each response with the conversation utterances in chronological order and obtains accumulated matching score of the utterance-response pairs to capture significant information and relations among utterances and each candidate response. The one with highest matching score is selected as final response. (3) Chit-chat Response Generation When given a question irrelevant to current introduction document, Target Sentences Retrieval may fail, so we adopt a chit-chat engine to give response"
C18-2024,P16-1049,0,0.0310963,"th single-turn question answering, for multi-turn the performance is usually unsatisfactory. In this paper, we present Lingke, an information retrieval augmented chatbot which is able to answer questions based on given product introduction document and deal with multi-turn conversations. We will introduce a finegrained pipeline processing to distill responses based on unstructured documents, and attentive sequential context-response matching for multi-turn conversations. 1 Introduction Recently, dialogue and interactive systems have been emerging with huge commercial values (Qiu et al., 2017; Yan et al., 2016a; Zhang et al., 2017; Huang et al., 2018; Zhang et al., 2018b; Zhang et al., 2018a), especially in the e-commerce field (Cui et al., 2017; Yan et al., 2016b). Building a chatbot mainly faces two challenges, the lack of dialogue data and poor performance for multi-turn conversations. This paper describes a fine-grained information retrieval (IR) augmented multi-turn chatbot - Lingke. It can learn knowledge without human supervision from conversation records or given product introduction documents and generate proper response, which alleviates the problem of lacking dialogue corpus to train a c"
C18-2024,P17-4003,0,0.0245773,"tion answering, for multi-turn the performance is usually unsatisfactory. In this paper, we present Lingke, an information retrieval augmented chatbot which is able to answer questions based on given product introduction document and deal with multi-turn conversations. We will introduce a finegrained pipeline processing to distill responses based on unstructured documents, and attentive sequential context-response matching for multi-turn conversations. 1 Introduction Recently, dialogue and interactive systems have been emerging with huge commercial values (Qiu et al., 2017; Yan et al., 2016a; Zhang et al., 2017; Huang et al., 2018; Zhang et al., 2018b; Zhang et al., 2018a), especially in the e-commerce field (Cui et al., 2017; Yan et al., 2016b). Building a chatbot mainly faces two challenges, the lack of dialogue data and poor performance for multi-turn conversations. This paper describes a fine-grained information retrieval (IR) augmented multi-turn chatbot - Lingke. It can learn knowledge without human supervision from conversation records or given product introduction documents and generate proper response, which alleviates the problem of lacking dialogue corpus to train a chatbot. First, by usi"
C18-2024,C18-1153,1,0.751809,"rmance is usually unsatisfactory. In this paper, we present Lingke, an information retrieval augmented chatbot which is able to answer questions based on given product introduction document and deal with multi-turn conversations. We will introduce a finegrained pipeline processing to distill responses based on unstructured documents, and attentive sequential context-response matching for multi-turn conversations. 1 Introduction Recently, dialogue and interactive systems have been emerging with huge commercial values (Qiu et al., 2017; Yan et al., 2016a; Zhang et al., 2017; Huang et al., 2018; Zhang et al., 2018b; Zhang et al., 2018a), especially in the e-commerce field (Cui et al., 2017; Yan et al., 2016b). Building a chatbot mainly faces two challenges, the lack of dialogue data and poor performance for multi-turn conversations. This paper describes a fine-grained information retrieval (IR) augmented multi-turn chatbot - Lingke. It can learn knowledge without human supervision from conversation records or given product introduction documents and generate proper response, which alleviates the problem of lacking dialogue corpus to train a chatbot. First, by using Apache Lucene1 to select top 2 senten"
C18-2024,C18-1317,1,0.573198,"rmance is usually unsatisfactory. In this paper, we present Lingke, an information retrieval augmented chatbot which is able to answer questions based on given product introduction document and deal with multi-turn conversations. We will introduce a finegrained pipeline processing to distill responses based on unstructured documents, and attentive sequential context-response matching for multi-turn conversations. 1 Introduction Recently, dialogue and interactive systems have been emerging with huge commercial values (Qiu et al., 2017; Yan et al., 2016a; Zhang et al., 2017; Huang et al., 2018; Zhang et al., 2018b; Zhang et al., 2018a), especially in the e-commerce field (Cui et al., 2017; Yan et al., 2016b). Building a chatbot mainly faces two challenges, the lack of dialogue data and poor performance for multi-turn conversations. This paper describes a fine-grained information retrieval (IR) augmented multi-turn chatbot - Lingke. It can learn knowledge without human supervision from conversation records or given product introduction documents and generate proper response, which alleviates the problem of lacking dialogue corpus to train a chatbot. First, by using Apache Lucene1 to select top 2 senten"
C18-2024,D16-1036,0,0.108968,"Missing"
D09-1004,W09-1201,0,0.0466519,"Missing"
D09-1004,W05-0620,0,0.68883,"nal Institute of Information and Communications Technology 3-5 Hikari-dai, Seika-cho, Soraku-gun, Kyoto, Japan, 619-0289 haizhao@cityu.edu.hk, chenwl@nict.go.jp Abstract into multiple stages is two-fold, one is each subtask asks for its favorable features, the other is at the consideration of computational efficiency. Generally speaking, a joint system is slower than a pipeline system in training. (Xue and Palmer, 2004) fount out that different features suited for different sub-tasks of SRL, i.e. argument identification and classification. The results from CoNLL shared tasks in 2005 and 2008 (Carreras and Marquez, 2005; Koomen et al., 2005; Surdeanu et al., 2008; Johansson and Nugues, 2008), further show that SRL pipeline may be one of the standard to achieve a state-of-the-art performance in practice. In the recent years, most works on SRL, including two CoNLL shared task in 2004 and 2005, focus on verbal predicates with the availability of PropBank (Palmer et al., 2005). As a complement to PropBank, NomBank (Meyers et al., 2004) annotates nominal predicates and their corresponding semantic roles using similar semantic framework as PropBank. Though SRL for nominal predicates offers more challenge, it draws"
D09-1004,W08-2134,0,0.125114,"ish-only task, while CoNLL2009 is a multilingual one. Though the English corpus in CoNLL-2009 is almost identical to the corpus in the CoNLL2008 shared task evaluation, the latter holds more sophisticated input structure as in (Surdeanu et al., 2008). The most difference for these two tasks is that the identification of semantic predicates is required in the task of CoNLL-2008 but not in CoNLL-2009. 31 et al., 2005; Marquez et al., 2005; Dang and Palmer, 2005; Pradhan et al., 2005; Toutanova et al., 2005; Jiang and Ng, 2006; Liu and Ng, 2007; Surdeanu et al., 2007; Johansson and Nugues, 2008; Che et al., 2008). Most feature templates that we will adopt for this work will come from various combinations or integrations of the following basic elements. Word Property. This type of elements include word form (form and its split form, spForm)2 , lemma (lemma,spLemma), and part-of-speech tag (pos, spPos), syntactic dependency label (dprel), and semantic dependency label (semdprel)3 . Syntactic Connection. This includes syntactic head (h), left(right) farthest(nearest) child (lm, ln, rm, and rn), and high(low) support verb or noun. We explain the last item, support verb(noun). From a given word to the synt"
D09-1004,W08-2122,0,0.130649,"comes better. Though not so surprised, the results do show that the argument traverse scheme synP th always outperforms the other linP th. The result of this comparison partially shows that an integrated semantic role labeler is sensitive to the order of how argument candidates are traversed to some extent. The performance given by synP th is compared to some other systems that participated in the CoNLL-2008 shared task. They were chosen among the 20 participating systems either because they held better results (the first four participants) or because they used some joint learning techniques (Henderson et al., 2008). The results of (Titov et al., 2009) that use the similar joint learning technique as (Henderson et al., 2008) are also included9 . Results of these evaluations on the test set are in Table 6. Top three systems of CoNLL2008, (Johansson and Nugues, 2008; Ciaramita et al., 2008; Che et al., 2008), used SRL pipelines. In this work, we partially use the similar techniques (synP th) for our participation in the shared tasks of CoNLL-2008 and 2009 (Zhao and Kit, 2008; Zhao et al., 2009b; Zhao et al., 2009a). Here we report that all SRL sub-tasks are tackled in one integrated model, while the predic"
D09-1004,I08-1012,1,0.827391,"ependency Parsers We consider three types of syntactic information to feed the SRL task. One is gold-standard syntactic input, and other two are based on automatically parsing results of two parsers, the state-ofthe-art syntactic parser described in (Johansson and Nugues, 2008)7 (it is referred to Johansson) and an integrated parser described as the following (referred to MSTM E ). The parser is basically based on the MSTParser8 using all the features presented by (McDonald et al., 2006) with projective parsing. Moreover, we exploit three types of additional features to improve the parser. 1) Chen et al. (2008) used features derived from short dependency pairs based on large-scale auto-parsed data to enhance dependency parsing. Here, the same features are used, though all dependency pairs rather than short dependency pairs are extracted along with the dependency direction from training data rather than auto-parsed data. 2) Koo et al. (2008) presented new features based on word clusters obtained from large-scale unlabeled data and achieved large improvement for English and Czech. Here, the same features are also used as word clusters are generated only from the training data. 3) Nivre and McDonald (2"
D09-1004,W06-1617,0,0.283236,"u et al., 2008; Johansson and Nugues, 2008), further show that SRL pipeline may be one of the standard to achieve a state-of-the-art performance in practice. In the recent years, most works on SRL, including two CoNLL shared task in 2004 and 2005, focus on verbal predicates with the availability of PropBank (Palmer et al., 2005). As a complement to PropBank, NomBank (Meyers et al., 2004) annotates nominal predicates and their corresponding semantic roles using similar semantic framework as PropBank. Though SRL for nominal predicates offers more challenge, it draws relatively little attention (Jiang and Ng, 2006). (Pustejovsky et al., 2005) discussed the issue of merging various treebanks, including PropBank, NomBank, and others. The idea of merging these two different treebanks was implemented in the CoNLL-2008 shared task (Surdeanu et al., 2008). However, few empirical studies support the necessity of an integrated learning strategy from NomBank and PropBank. Though aiming at Chinese SRL, (Xue, 2006) reported that their experiments show that simply adding the verb data to the training set of NomBank and extracting the same features from the verb and noun instances will hurt the overall performance."
D09-1004,W08-2138,0,0.0466915,"Missing"
D09-1004,W08-2123,0,0.069069,"Missing"
D09-1004,P05-1006,0,0.0217851,"metimes takes itself as its argument. The above pruning algorithm has been shown effective. However, it is still inefficient for a SRL 1 CoNLL-2008 is an English-only task, while CoNLL2009 is a multilingual one. Though the English corpus in CoNLL-2009 is almost identical to the corpus in the CoNLL2008 shared task evaluation, the latter holds more sophisticated input structure as in (Surdeanu et al., 2008). The most difference for these two tasks is that the identification of semantic predicates is required in the task of CoNLL-2008 but not in CoNLL-2009. 31 et al., 2005; Marquez et al., 2005; Dang and Palmer, 2005; Pradhan et al., 2005; Toutanova et al., 2005; Jiang and Ng, 2006; Liu and Ng, 2007; Surdeanu et al., 2007; Johansson and Nugues, 2008; Che et al., 2008). Most feature templates that we will adopt for this work will come from various combinations or integrations of the following basic elements. Word Property. This type of elements include word form (form and its split form, spForm)2 , lemma (lemma,spLemma), and part-of-speech tag (pos, spPos), syntactic dependency label (dprel), and semantic dependency label (semdprel)3 . Syntactic Connection. This includes syntactic head (h), left(right) far"
D09-1004,P08-1068,0,0.0284694,"scribed as the following (referred to MSTM E ). The parser is basically based on the MSTParser8 using all the features presented by (McDonald et al., 2006) with projective parsing. Moreover, we exploit three types of additional features to improve the parser. 1) Chen et al. (2008) used features derived from short dependency pairs based on large-scale auto-parsed data to enhance dependency parsing. Here, the same features are used, though all dependency pairs rather than short dependency pairs are extracted along with the dependency direction from training data rather than auto-parsed data. 2) Koo et al. (2008) presented new features based on word clusters obtained from large-scale unlabeled data and achieved large improvement for English and Czech. Here, the same features are also used as word clusters are generated only from the training data. 3) Nivre and McDonald (2008) presented an integrating method to provide additional information for graph-based and transition-based parsers. Here, we represent features based on dependency relations predicted by transition-based parsers for the MSTParer. For the sake of efficiency, we use a fast transitionSyn-Parser LAS MSTM E Johansson Gold 88.39 89.28 100."
D09-1004,D08-1034,0,0.0312437,"mplate Selection Based on the above mentioned elements, 781 feature templates (hereafter the set of these templates is referred to F T )6 are initially considered. Feature templates in this initial set are constructed in a generalized way. For example, if we find that a feature template a.lm.lemma was once used in some existing work, then such three templates, a.rm.lemma, a.rn.lemma, a.ln.lemma will be also added into the set. As an optimal feature template subset cannot be expected to be extracted from so large a set by hand, a greedy feature selection similar to that in (Jiang and Ng, 2006; Ding and Chang, 2008) is applied. The detailed algorithm is described in Algorithm 1. Assuming that the number of feature templates in a given set is n, the algorithm of (Ding and Chang, 2008) requires O(n2 ) times of training/test routines, it cannot handle a set that consists of hundreds of templates. As the time complexity of Algorithm 1 is only O(n), it permits a large scale feature selection accomplished by paying a reasonable time cost. Though the time complexity of the algorithm given by (Jiang and Ng, 2006) is also linear, it should assume all feature templates in the initial selected set ‘good’ enough and"
D09-1004,W05-0625,0,0.022033,"and Communications Technology 3-5 Hikari-dai, Seika-cho, Soraku-gun, Kyoto, Japan, 619-0289 haizhao@cityu.edu.hk, chenwl@nict.go.jp Abstract into multiple stages is two-fold, one is each subtask asks for its favorable features, the other is at the consideration of computational efficiency. Generally speaking, a joint system is slower than a pipeline system in training. (Xue and Palmer, 2004) fount out that different features suited for different sub-tasks of SRL, i.e. argument identification and classification. The results from CoNLL shared tasks in 2005 and 2008 (Carreras and Marquez, 2005; Koomen et al., 2005; Surdeanu et al., 2008; Johansson and Nugues, 2008), further show that SRL pipeline may be one of the standard to achieve a state-of-the-art performance in practice. In the recent years, most works on SRL, including two CoNLL shared task in 2004 and 2005, focus on verbal predicates with the availability of PropBank (Palmer et al., 2005). As a complement to PropBank, NomBank (Meyers et al., 2004) annotates nominal predicates and their corresponding semantic roles using similar semantic framework as PropBank. Though SRL for nominal predicates offers more challenge, it draws relatively little at"
D09-1004,J02-3001,0,0.194096,"oLeft A1 f g A0 noRight h . Training samples will generated from c to g according to the above sequence. We use a Maximum Entropy classifier with a tunable Gaussian prior as usual. Our implementation of the model adopts L-BFGS algorithm for parameter optimization. 3 Feature Templates 3.1 Elements for Feature Generation Motivated by previous works, we carefully consider those factors from a wide range of features that can help semantic role labeling for both predicate disambiguation, argument’s identification and classification as the predicate is either verbal or nominal. These works include (Gildea and Jurafsky, 2002; Carreras and Marquez, 2005; Koomen 2 In CoNLL-2008, Treebank tokens are split at the position that a hyphen (-) or a forward slash (/) occurs. This leads to two types of feature columns, non-split and split. 3 Lemma and pos for either training or test are from automatically pre-analyzed columns in the input files. 4 Note that the meaning of support verb is slightly different between (Toutanova et al., 2005) and (Xue, 2006; Jiang and Ng, 2006) 32 first includes all syntactic children (children), the second also includes all but excludes the left most and the right most children (noFarChildren"
D09-1004,P07-1027,0,0.0120512,"e. However, it is still inefficient for a SRL 1 CoNLL-2008 is an English-only task, while CoNLL2009 is a multilingual one. Though the English corpus in CoNLL-2009 is almost identical to the corpus in the CoNLL2008 shared task evaluation, the latter holds more sophisticated input structure as in (Surdeanu et al., 2008). The most difference for these two tasks is that the identification of semantic predicates is required in the task of CoNLL-2008 but not in CoNLL-2009. 31 et al., 2005; Marquez et al., 2005; Dang and Palmer, 2005; Pradhan et al., 2005; Toutanova et al., 2005; Jiang and Ng, 2006; Liu and Ng, 2007; Surdeanu et al., 2007; Johansson and Nugues, 2008; Che et al., 2008). Most feature templates that we will adopt for this work will come from various combinations or integrations of the following basic elements. Word Property. This type of elements include word form (form and its split form, spForm)2 , lemma (lemma,spLemma), and part-of-speech tag (pos, spPos), syntactic dependency label (dprel), and semantic dependency label (semdprel)3 . Syntactic Connection. This includes syntactic head (h), left(right) farthest(nearest) child (lm, ln, rm, and rn), and high(low) support verb or noun. We ex"
D09-1004,P05-1073,0,0.109413,"Missing"
D09-1004,W04-3212,0,0.642466,"unyu Kit† (揭 揭春 雨 ) † Department of Chinese, Translation and Linguistics City University of Hong Kong Tat Chee Avenue, Kowloon, Hong Kong, China ∗ Language Infrastructure Group, MASTAR Project National Institute of Information and Communications Technology 3-5 Hikari-dai, Seika-cho, Soraku-gun, Kyoto, Japan, 619-0289 haizhao@cityu.edu.hk, chenwl@nict.go.jp Abstract into multiple stages is two-fold, one is each subtask asks for its favorable features, the other is at the consideration of computational efficiency. Generally speaking, a joint system is slower than a pipeline system in training. (Xue and Palmer, 2004) fount out that different features suited for different sub-tasks of SRL, i.e. argument identification and classification. The results from CoNLL shared tasks in 2005 and 2008 (Carreras and Marquez, 2005; Koomen et al., 2005; Surdeanu et al., 2008; Johansson and Nugues, 2008), further show that SRL pipeline may be one of the standard to achieve a state-of-the-art performance in practice. In the recent years, most works on SRL, including two CoNLL shared task in 2004 and 2005, focus on verbal predicates with the availability of PropBank (Palmer et al., 2005). As a complement to PropBank, NomBan"
D09-1004,W06-2932,0,0.0610293,"Missing"
D09-1004,N06-1055,0,0.371434,"al predicates and their corresponding semantic roles using similar semantic framework as PropBank. Though SRL for nominal predicates offers more challenge, it draws relatively little attention (Jiang and Ng, 2006). (Pustejovsky et al., 2005) discussed the issue of merging various treebanks, including PropBank, NomBank, and others. The idea of merging these two different treebanks was implemented in the CoNLL-2008 shared task (Surdeanu et al., 2008). However, few empirical studies support the necessity of an integrated learning strategy from NomBank and PropBank. Though aiming at Chinese SRL, (Xue, 2006) reported that their experiments show that simply adding the verb data to the training set of NomBank and extracting the same features from the verb and noun instances will hurt the overall performance. From the results of CoNLL-2008 shared task, the top system by (Johansson and Nugues, 2008) also used two We present an integrated dependencybased semantic role labeling system for English from both NomBank and PropBank. By introducing assistant argument labels and considering much more feature templates, two optimal feature template sets are obtained through an effective feature selection proce"
D09-1004,W04-2705,0,0.0326022,"unt out that different features suited for different sub-tasks of SRL, i.e. argument identification and classification. The results from CoNLL shared tasks in 2005 and 2008 (Carreras and Marquez, 2005; Koomen et al., 2005; Surdeanu et al., 2008; Johansson and Nugues, 2008), further show that SRL pipeline may be one of the standard to achieve a state-of-the-art performance in practice. In the recent years, most works on SRL, including two CoNLL shared task in 2004 and 2005, focus on verbal predicates with the availability of PropBank (Palmer et al., 2005). As a complement to PropBank, NomBank (Meyers et al., 2004) annotates nominal predicates and their corresponding semantic roles using similar semantic framework as PropBank. Though SRL for nominal predicates offers more challenge, it draws relatively little attention (Jiang and Ng, 2006). (Pustejovsky et al., 2005) discussed the issue of merging various treebanks, including PropBank, NomBank, and others. The idea of merging these two different treebanks was implemented in the CoNLL-2008 shared task (Surdeanu et al., 2008). However, few empirical studies support the necessity of an integrated learning strategy from NomBank and PropBank. Though aiming a"
D09-1004,W08-2127,1,0.845355,"performed on a much larger feature template set than that in previous work. This helps us find features that may be of benefit to all SRL sub-tasks as long as possible. As two optimal feature template sets have been proven available, for the first time we report that an integrated SRL system may provide a result close to the state-of-the-art achieved by those SRL pipelines or individual systems for some specific predicates. 2 Initialization: Set the given predicate as the current node; Adaptive Argument Pruning A word-pair classification is used to formulate semantic dependency parsing as in (Zhao and Kit, 2008). As for predicate identification or disambiguation, the first word is set as a virtual root (which is virtually set before the beginning of the sentence.) and the second as a predicate candidate. As for argument identification/classification, the first word in a word pair is specified as a predi(1) The current node and all of its syntactic children are selected as argument candidates (children are traversed from left to right.). (2) Reset the current node to its syntactic head and repeat step (1) until the root is reached. Note that this pruning algorithm is slightly different from that of (X"
D09-1004,P08-1108,0,0.0290597,"1) Chen et al. (2008) used features derived from short dependency pairs based on large-scale auto-parsed data to enhance dependency parsing. Here, the same features are used, though all dependency pairs rather than short dependency pairs are extracted along with the dependency direction from training data rather than auto-parsed data. 2) Koo et al. (2008) presented new features based on word clusters obtained from large-scale unlabeled data and achieved large improvement for English and Czech. Here, the same features are also used as word clusters are generated only from the training data. 3) Nivre and McDonald (2008) presented an integrating method to provide additional information for graph-based and transition-based parsers. Here, we represent features based on dependency relations predicted by transition-based parsers for the MSTParer. For the sake of efficiency, we use a fast transitionSyn-Parser LAS MSTM E Johansson Gold 88.39 89.28 100.00 synPth+F Tsyn Sem Sem-F1 F1 /LAS 80.53 91.10 80.94 90.66 84.57 84.57 linPth+F Tlin Sem Sem-F1 F1 /LAS 79.83 90.31 79.84 89.43 83.34 83.34 Table 5: Semantic Labeled F1 based parser based on maximum entropy as in Zhao and Kit (2008). We still use the similar feature"
D09-1004,W09-1209,1,0.855764,"em still holds some sort of merits, being easier to implement, a single-stage feature selection benefiting the whole system, an all-in-one model outputting all required semantic role information and so on. The shared tasks at the CoNLL 2008 and 2009 are devoted to the joint learning of syntactic and semantic dependencies, which show that SRL can be well performed using only dependency syntax input. Using data and evaluation settings of the CoNLL-2008 shared task, this work will only focus on semantic dependency parsing and compares the best-performing SRL system in the CoNLL-2009 shared Task (Zhao et al., 2009b) with those in the CoNLL-2008 shared task (Surdeanu et al., 2008; Hajiˇc et al., 2009)1 . Aiming at main drawbacks of an integrated approach, two key techniques will be applied. 1) Assistant argument labels are introduced for the further improvement of argument pruning. This helps the development of a fast and lightweight SRL system. 2) Using a greedy feature selection algorithm, a large-scale feature engineering is performed on a much larger feature template set than that in previous work. This helps us find features that may be of benefit to all SRL sub-tasks as long as possible. As two op"
D09-1004,J05-1004,0,0.0315932,"han a pipeline system in training. (Xue and Palmer, 2004) fount out that different features suited for different sub-tasks of SRL, i.e. argument identification and classification. The results from CoNLL shared tasks in 2005 and 2008 (Carreras and Marquez, 2005; Koomen et al., 2005; Surdeanu et al., 2008; Johansson and Nugues, 2008), further show that SRL pipeline may be one of the standard to achieve a state-of-the-art performance in practice. In the recent years, most works on SRL, including two CoNLL shared task in 2004 and 2005, focus on verbal predicates with the availability of PropBank (Palmer et al., 2005). As a complement to PropBank, NomBank (Meyers et al., 2004) annotates nominal predicates and their corresponding semantic roles using similar semantic framework as PropBank. Though SRL for nominal predicates offers more challenge, it draws relatively little attention (Jiang and Ng, 2006). (Pustejovsky et al., 2005) discussed the issue of merging various treebanks, including PropBank, NomBank, and others. The idea of merging these two different treebanks was implemented in the CoNLL-2008 shared task (Surdeanu et al., 2008). However, few empirical studies support the necessity of an integrated"
D09-1004,W09-1208,1,0.769841,"em still holds some sort of merits, being easier to implement, a single-stage feature selection benefiting the whole system, an all-in-one model outputting all required semantic role information and so on. The shared tasks at the CoNLL 2008 and 2009 are devoted to the joint learning of syntactic and semantic dependencies, which show that SRL can be well performed using only dependency syntax input. Using data and evaluation settings of the CoNLL-2008 shared task, this work will only focus on semantic dependency parsing and compares the best-performing SRL system in the CoNLL-2009 shared Task (Zhao et al., 2009b) with those in the CoNLL-2008 shared task (Surdeanu et al., 2008; Hajiˇc et al., 2009)1 . Aiming at main drawbacks of an integrated approach, two key techniques will be applied. 1) Assistant argument labels are introduced for the further improvement of argument pruning. This helps the development of a fast and lightweight SRL system. 2) Using a greedy feature selection algorithm, a large-scale feature engineering is performed on a much larger feature template set than that in previous work. This helps us find features that may be of benefit to all SRL sub-tasks as long as possible. As two op"
D09-1004,N04-1030,0,0.0522951,"idates are in Table 4. The statistics is conducted on three different syntactic inputs. The coverage rate in the table means the ratio of how many true arguments are covered by the selected pruning scheme. Note that the adaptive pruning of argument candidates using assistant labels does not change this rate. This ratio only depends on which path, either synP th or linP th, is chosen, and how good the syntactic input is (if synP th is the case). From the results, we see that more than a half of argument candidates can be effectively pruned for synP th and even 2/3 for linP th. As mentioned by (Pradhan et al., 2004), argument identification plays a bottleneck role in improving the performance of a SRL system. The effectiveness of the proposed additional pruning techniques may be seen as a significant improvement over the original algorithm of (Xue and Palmer, 2004). The results also indicate that such an assumption holds that arguments trend to close with their predicate, at either type of distance, syntactic or linear. Based on different syntactic inputs, we obtain different results on semantic dependency parsing 7 It is a 2-order maximum spanning tree parser with pseudo-projective techniques. A syntact"
D09-1004,P05-1072,0,0.478575,"its argument. The above pruning algorithm has been shown effective. However, it is still inefficient for a SRL 1 CoNLL-2008 is an English-only task, while CoNLL2009 is a multilingual one. Though the English corpus in CoNLL-2009 is almost identical to the corpus in the CoNLL2008 shared task evaluation, the latter holds more sophisticated input structure as in (Surdeanu et al., 2008). The most difference for these two tasks is that the identification of semantic predicates is required in the task of CoNLL-2008 but not in CoNLL-2009. 31 et al., 2005; Marquez et al., 2005; Dang and Palmer, 2005; Pradhan et al., 2005; Toutanova et al., 2005; Jiang and Ng, 2006; Liu and Ng, 2007; Surdeanu et al., 2007; Johansson and Nugues, 2008; Che et al., 2008). Most feature templates that we will adopt for this work will come from various combinations or integrations of the following basic elements. Word Property. This type of elements include word form (form and its split form, spForm)2 , lemma (lemma,spLemma), and part-of-speech tag (pos, spPos), syntactic dependency label (dprel), and semantic dependency label (semdprel)3 . Syntactic Connection. This includes syntactic head (h), left(right) farthest(nearest) child ("
D09-1004,W05-0302,0,0.0146086,"son and Nugues, 2008), further show that SRL pipeline may be one of the standard to achieve a state-of-the-art performance in practice. In the recent years, most works on SRL, including two CoNLL shared task in 2004 and 2005, focus on verbal predicates with the availability of PropBank (Palmer et al., 2005). As a complement to PropBank, NomBank (Meyers et al., 2004) annotates nominal predicates and their corresponding semantic roles using similar semantic framework as PropBank. Though SRL for nominal predicates offers more challenge, it draws relatively little attention (Jiang and Ng, 2006). (Pustejovsky et al., 2005) discussed the issue of merging various treebanks, including PropBank, NomBank, and others. The idea of merging these two different treebanks was implemented in the CoNLL-2008 shared task (Surdeanu et al., 2008). However, few empirical studies support the necessity of an integrated learning strategy from NomBank and PropBank. Though aiming at Chinese SRL, (Xue, 2006) reported that their experiments show that simply adding the verb data to the training set of NomBank and extracting the same features from the verb and noun instances will hurt the overall performance. From the results of CoNLL-20"
D09-1004,W08-2121,0,0.0732528,"Missing"
D09-1004,H05-1081,0,\N,Missing
D09-1133,P98-1013,0,0.0167375,"n and Harabagiu, 2004), information extraction (Surdeanu et al., 2003), and co-reference resolution (Ponzetto and Strube, 2006). Given a sentence and a predicate (either a verb or a noun) in it, SRL recognizes and maps all the constituents in the sentence into their corresponding semantic arguments (roles) ∗ of the predicate. According to the predicate types, SRL could be divided into SRL for verbal predicates (verbal SRL, in short) and SRL for nominal predicates (nominal SRL, in short). During the past few years, verbal SRL has dominated the research on SRL with the availability of FrameNet (Baker et al., 1998), PropBank (Palmer et al., 2005), and the consecutive CoNLL shared tasks (Carreras and Màrquez, 2004 & 2005) in English language. As a complement to PropBank on verbal predicates, NomBank (Meyers et al., 2004) annotates nominal predicates and their corresponding semantic roles using similar semantic framework as PropBank. As a representative, Jiang and Ng (2006) pioneered the exploration of various nominal SRL-specific features besides the traditional verbal SRL-related features on NomBank. They achieved the performance of 72.73 and 69.14 in F1-measure on golden and automatic syntactic parse t"
D09-1133,W04-2412,0,0.0205686,"lution (Ponzetto and Strube, 2006). Given a sentence and a predicate (either a verb or a noun) in it, SRL recognizes and maps all the constituents in the sentence into their corresponding semantic arguments (roles) ∗ of the predicate. According to the predicate types, SRL could be divided into SRL for verbal predicates (verbal SRL, in short) and SRL for nominal predicates (nominal SRL, in short). During the past few years, verbal SRL has dominated the research on SRL with the availability of FrameNet (Baker et al., 1998), PropBank (Palmer et al., 2005), and the consecutive CoNLL shared tasks (Carreras and Màrquez, 2004 & 2005) in English language. As a complement to PropBank on verbal predicates, NomBank (Meyers et al., 2004) annotates nominal predicates and their corresponding semantic roles using similar semantic framework as PropBank. As a representative, Jiang and Ng (2006) pioneered the exploration of various nominal SRL-specific features besides the traditional verbal SRL-related features on NomBank. They achieved the performance of 72.73 and 69.14 in F1-measure on golden and automatic syntactic parse trees, respectively, given golden nominal predicates. For SRL in Chinese, Sun and Jurafsky (2004) and"
D09-1133,W05-0620,0,0.21267,"Missing"
D09-1133,D08-1034,0,0.0979658,"Missing"
D09-1133,W06-1617,0,0.63749,"L could be divided into SRL for verbal predicates (verbal SRL, in short) and SRL for nominal predicates (nominal SRL, in short). During the past few years, verbal SRL has dominated the research on SRL with the availability of FrameNet (Baker et al., 1998), PropBank (Palmer et al., 2005), and the consecutive CoNLL shared tasks (Carreras and Màrquez, 2004 & 2005) in English language. As a complement to PropBank on verbal predicates, NomBank (Meyers et al., 2004) annotates nominal predicates and their corresponding semantic roles using similar semantic framework as PropBank. As a representative, Jiang and Ng (2006) pioneered the exploration of various nominal SRL-specific features besides the traditional verbal SRL-related features on NomBank. They achieved the performance of 72.73 and 69.14 in F1-measure on golden and automatic syntactic parse trees, respectively, given golden nominal predicates. For SRL in Chinese, Sun and Jurafsky (2004) and Pradhan et al. (2004) pioneered the research on Chinese verbal and nominal SRLs, respectively, on small private datasets. Taking the advantage of recent release of Chinese PropBank (Xue and Palmer, 2003) and Chinese NomBank (Xue, 2006a), Xue and his colleagues (X"
D09-1133,P07-1027,0,0.0832518,"Missing"
D09-1133,meyers-etal-2004-annotating,0,0.0395649,"s and maps all the constituents in the sentence into their corresponding semantic arguments (roles) ∗ of the predicate. According to the predicate types, SRL could be divided into SRL for verbal predicates (verbal SRL, in short) and SRL for nominal predicates (nominal SRL, in short). During the past few years, verbal SRL has dominated the research on SRL with the availability of FrameNet (Baker et al., 1998), PropBank (Palmer et al., 2005), and the consecutive CoNLL shared tasks (Carreras and Màrquez, 2004 & 2005) in English language. As a complement to PropBank on verbal predicates, NomBank (Meyers et al., 2004) annotates nominal predicates and their corresponding semantic roles using similar semantic framework as PropBank. As a representative, Jiang and Ng (2006) pioneered the exploration of various nominal SRL-specific features besides the traditional verbal SRL-related features on NomBank. They achieved the performance of 72.73 and 69.14 in F1-measure on golden and automatic syntactic parse trees, respectively, given golden nominal predicates. For SRL in Chinese, Sun and Jurafsky (2004) and Pradhan et al. (2004) pioneered the research on Chinese verbal and nominal SRLs, respectively, on small priv"
D09-1133,P04-1043,0,0.12312,"arsing, we employ a Chinese word segmenter, similar to Ng and Low (2004), to obtain the best automatic segmentation result for a given sentence, which is then fed into Berkeley parser for further syntactic parsing. Both the word segmenter and Berkeley parser are developed with the same training and development datasets as our SRL experiments. The word segmenter achieves the performance of 96.1 in F1-measure while the Berkeley parser gives a performance of 82.5 and 85.5 in F1measure on golden and automatic word segmentation, respectively 2 . In addition, SVMLight with the tree kernel function (Moschitti, 2004) 3 is selected as our classifier. In order to handle multi-classification 1 Berkeley Parser. http://code.google.com/p/berkeleyparser/ POSs are not counted in evaluating the performance of word-based syntactic parser, but they are counted in evaluating the performance of character-based parser. Therefore the F1-measure for the later is higher than that for the former. 3 SVM-LIGHT-TK. http://dit.unitn.it/~moschitt/ 2 1285 problem in argument classification, we apply the one vs. others strategy, which builds K classifiers so as to separate one class from all others. For argument identification an"
D09-1133,C04-1100,0,0.0221351,"nominal SRL system much outperforms the state-of-the-art ones. 1. Introduction Semantic parsing maps a natural language sentence into a formal representation of its meaning. Due to the difficulty in deep semantic parsing, most of previous work focuses on shallow semantic parsing, which assigns a simple structure (such as WHO did WHAT to WHOM, WHEN, WHERE, WHY, HOW) to each predicate in a sentence. In particular, the well-defined semantic role labeling (SRL) task has been drawing more and more attention in recent years due to its importance in deep NLP applications, such as question answering (Narayanan and Harabagiu, 2004), information extraction (Surdeanu et al., 2003), and co-reference resolution (Ponzetto and Strube, 2006). Given a sentence and a predicate (either a verb or a noun) in it, SRL recognizes and maps all the constituents in the sentence into their corresponding semantic arguments (roles) ∗ of the predicate. According to the predicate types, SRL could be divided into SRL for verbal predicates (verbal SRL, in short) and SRL for nominal predicates (nominal SRL, in short). During the past few years, verbal SRL has dominated the research on SRL with the availability of FrameNet (Baker et al., 1998), P"
D09-1133,W04-3236,0,0.0211059,"les (chtb_001 to 040.fid and chtb_900 to 931.fid) are held out as the test data, and 40 files (chtb_041 to 080.fid) as the development data, with 8642, 1124, and 731 propositions, respectively. As Chinese words are not naturally segmented in raw sentences, two Chinese automatic parsers are constructed: word-based parser (assuming golden word segmentation) and character-based parser (with automatic word segmentation). Here, Berkeley parser (Petrov and Klein, 2007) 1 is chosen as the Chinese automatic parser. With regard to character-based parsing, we employ a Chinese word segmenter, similar to Ng and Low (2004), to obtain the best automatic segmentation result for a given sentence, which is then fed into Berkeley parser for further syntactic parsing. Both the word segmenter and Berkeley parser are developed with the same training and development datasets as our SRL experiments. The word segmenter achieves the performance of 96.1 in F1-measure while the Berkeley parser gives a performance of 82.5 and 85.5 in F1measure on golden and automatic word segmentation, respectively 2 . In addition, SVMLight with the tree kernel function (Moschitti, 2004) 3 is selected as our classifier. In order to handle mul"
D09-1133,J05-1004,0,0.108985,"tion extraction (Surdeanu et al., 2003), and co-reference resolution (Ponzetto and Strube, 2006). Given a sentence and a predicate (either a verb or a noun) in it, SRL recognizes and maps all the constituents in the sentence into their corresponding semantic arguments (roles) ∗ of the predicate. According to the predicate types, SRL could be divided into SRL for verbal predicates (verbal SRL, in short) and SRL for nominal predicates (nominal SRL, in short). During the past few years, verbal SRL has dominated the research on SRL with the availability of FrameNet (Baker et al., 1998), PropBank (Palmer et al., 2005), and the consecutive CoNLL shared tasks (Carreras and Màrquez, 2004 & 2005) in English language. As a complement to PropBank on verbal predicates, NomBank (Meyers et al., 2004) annotates nominal predicates and their corresponding semantic roles using similar semantic framework as PropBank. As a representative, Jiang and Ng (2006) pioneered the exploration of various nominal SRL-specific features besides the traditional verbal SRL-related features on NomBank. They achieved the performance of 72.73 and 69.14 in F1-measure on golden and automatic syntactic parse trees, respectively, given golden"
D09-1133,N07-1051,0,0.0314267,"ese Penn TreeBank 5.1. Following the experimental setting in Xue (2008), 648 files (chtb_081 to 899.fid) are selected as the training data, 72 files (chtb_001 to 040.fid and chtb_900 to 931.fid) are held out as the test data, and 40 files (chtb_041 to 080.fid) as the development data, with 8642, 1124, and 731 propositions, respectively. As Chinese words are not naturally segmented in raw sentences, two Chinese automatic parsers are constructed: word-based parser (assuming golden word segmentation) and character-based parser (with automatic word segmentation). Here, Berkeley parser (Petrov and Klein, 2007) 1 is chosen as the Chinese automatic parser. With regard to character-based parsing, we employ a Chinese word segmenter, similar to Ng and Low (2004), to obtain the best automatic segmentation result for a given sentence, which is then fed into Berkeley parser for further syntactic parsing. Both the word segmenter and Berkeley parser are developed with the same training and development datasets as our SRL experiments. The word segmenter achieves the performance of 96.1 in F1-measure while the Berkeley parser gives a performance of 82.5 and 85.5 in F1measure on golden and automatic word segmen"
D09-1133,E06-2015,0,0.310819,"vided into SRL for verbal predicates (verbal SRL, in short) and SRL for nominal predicates (nominal SRL, in short). During the past few years, verbal SRL has dominated the research on SRL with the availability of FrameNet (Baker et al., 1998), PropBank (Palmer et al., 2005), and the consecutive CoNLL shared tasks (Carreras and Màrquez, 2004 & 2005) in English language. As a complement to PropBank on verbal predicates, NomBank (Meyers et al., 2004) annotates nominal predicates and their corresponding semantic roles using similar semantic framework as PropBank. As a representative, Jiang and Ng (2006) pioneered the exploration of various nominal SRL-specific features besides the traditional verbal SRL-related features on NomBank. They achieved the performance of 72.73 and 69.14 in F1-measure on golden and automatic syntactic parse trees, respectively, given golden nominal predicates. For SRL in Chinese, Sun and Jurafsky (2004) and Pradhan et al. (2004) pioneered the research on Chinese verbal and nominal SRLs, respectively, on small private datasets. Taking the advantage of recent release of Chinese PropBank (Xue and Palmer, 2003) and Chinese NomBank (Xue, 2006a), Xue and his colleagues (X"
D09-1133,N04-4036,0,0.0189779,"& 2005) in English language. As a complement to PropBank on verbal predicates, NomBank (Meyers et al., 2004) annotates nominal predicates and their corresponding semantic roles using similar semantic framework as PropBank. As a representative, Jiang and Ng (2006) pioneered the exploration of various nominal SRL-specific features besides the traditional verbal SRL-related features on NomBank. They achieved the performance of 72.73 and 69.14 in F1-measure on golden and automatic syntactic parse trees, respectively, given golden nominal predicates. For SRL in Chinese, Sun and Jurafsky (2004) and Pradhan et al. (2004) pioneered the research on Chinese verbal and nominal SRLs, respectively, on small private datasets. Taking the advantage of recent release of Chinese PropBank (Xue and Palmer, 2003) and Chinese NomBank (Xue, 2006a), Xue and his colleagues (Xue and Palmer 2005; Xue 2006b; Xue, 2008) pioneered the exploration of Chinese verbal and nominal SRLs, given golden predicates. Among them, Xue and Palmer (2005) studied Chinese verbal SRL using Chinese PropBank and achieved the performance of 91.3 and 61.3 in F1-measure on golden and automatic syntactic parse trees, respectively. Xue (2006b) extended the"
D09-1133,N04-1032,0,0.0110768,"(Carreras and Màrquez, 2004 & 2005) in English language. As a complement to PropBank on verbal predicates, NomBank (Meyers et al., 2004) annotates nominal predicates and their corresponding semantic roles using similar semantic framework as PropBank. As a representative, Jiang and Ng (2006) pioneered the exploration of various nominal SRL-specific features besides the traditional verbal SRL-related features on NomBank. They achieved the performance of 72.73 and 69.14 in F1-measure on golden and automatic syntactic parse trees, respectively, given golden nominal predicates. For SRL in Chinese, Sun and Jurafsky (2004) and Pradhan et al. (2004) pioneered the research on Chinese verbal and nominal SRLs, respectively, on small private datasets. Taking the advantage of recent release of Chinese PropBank (Xue and Palmer, 2003) and Chinese NomBank (Xue, 2006a), Xue and his colleagues (Xue and Palmer 2005; Xue 2006b; Xue, 2008) pioneered the exploration of Chinese verbal and nominal SRLs, given golden predicates. Among them, Xue and Palmer (2005) studied Chinese verbal SRL using Chinese PropBank and achieved the performance of 91.3 and 61.3 in F1-measure on golden and automatic syntactic parse trees, respectively"
D09-1133,P03-1002,0,0.0353596,"ones. 1. Introduction Semantic parsing maps a natural language sentence into a formal representation of its meaning. Due to the difficulty in deep semantic parsing, most of previous work focuses on shallow semantic parsing, which assigns a simple structure (such as WHO did WHAT to WHOM, WHEN, WHERE, WHY, HOW) to each predicate in a sentence. In particular, the well-defined semantic role labeling (SRL) task has been drawing more and more attention in recent years due to its importance in deep NLP applications, such as question answering (Narayanan and Harabagiu, 2004), information extraction (Surdeanu et al., 2003), and co-reference resolution (Ponzetto and Strube, 2006). Given a sentence and a predicate (either a verb or a noun) in it, SRL recognizes and maps all the constituents in the sentence into their corresponding semantic arguments (roles) ∗ of the predicate. According to the predicate types, SRL could be divided into SRL for verbal predicates (verbal SRL, in short) and SRL for nominal predicates (nominal SRL, in short). During the past few years, verbal SRL has dominated the research on SRL with the availability of FrameNet (Baker et al., 1998), PropBank (Palmer et al., 2005), and the consecuti"
D09-1133,W03-1707,0,0.0158673,"similar semantic framework as PropBank. As a representative, Jiang and Ng (2006) pioneered the exploration of various nominal SRL-specific features besides the traditional verbal SRL-related features on NomBank. They achieved the performance of 72.73 and 69.14 in F1-measure on golden and automatic syntactic parse trees, respectively, given golden nominal predicates. For SRL in Chinese, Sun and Jurafsky (2004) and Pradhan et al. (2004) pioneered the research on Chinese verbal and nominal SRLs, respectively, on small private datasets. Taking the advantage of recent release of Chinese PropBank (Xue and Palmer, 2003) and Chinese NomBank (Xue, 2006a), Xue and his colleagues (Xue and Palmer 2005; Xue 2006b; Xue, 2008) pioneered the exploration of Chinese verbal and nominal SRLs, given golden predicates. Among them, Xue and Palmer (2005) studied Chinese verbal SRL using Chinese PropBank and achieved the performance of 91.3 and 61.3 in F1-measure on golden and automatic syntactic parse trees, respectively. Xue (2006b) extended their study on Chinese nominal SRL and attempted to improve the performance of nominal SRL by simply inCorresponding author 1280 Proceedings of the 2009 Conference on Empirical Methods"
D09-1133,xue-2006-annotating,0,0.493591,"a representative, Jiang and Ng (2006) pioneered the exploration of various nominal SRL-specific features besides the traditional verbal SRL-related features on NomBank. They achieved the performance of 72.73 and 69.14 in F1-measure on golden and automatic syntactic parse trees, respectively, given golden nominal predicates. For SRL in Chinese, Sun and Jurafsky (2004) and Pradhan et al. (2004) pioneered the research on Chinese verbal and nominal SRLs, respectively, on small private datasets. Taking the advantage of recent release of Chinese PropBank (Xue and Palmer, 2003) and Chinese NomBank (Xue, 2006a), Xue and his colleagues (Xue and Palmer 2005; Xue 2006b; Xue, 2008) pioneered the exploration of Chinese verbal and nominal SRLs, given golden predicates. Among them, Xue and Palmer (2005) studied Chinese verbal SRL using Chinese PropBank and achieved the performance of 91.3 and 61.3 in F1-measure on golden and automatic syntactic parse trees, respectively. Xue (2006b) extended their study on Chinese nominal SRL and attempted to improve the performance of nominal SRL by simply inCorresponding author 1280 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,"
D09-1133,N06-1055,0,0.353081,"a representative, Jiang and Ng (2006) pioneered the exploration of various nominal SRL-specific features besides the traditional verbal SRL-related features on NomBank. They achieved the performance of 72.73 and 69.14 in F1-measure on golden and automatic syntactic parse trees, respectively, given golden nominal predicates. For SRL in Chinese, Sun and Jurafsky (2004) and Pradhan et al. (2004) pioneered the research on Chinese verbal and nominal SRLs, respectively, on small private datasets. Taking the advantage of recent release of Chinese PropBank (Xue and Palmer, 2003) and Chinese NomBank (Xue, 2006a), Xue and his colleagues (Xue and Palmer 2005; Xue 2006b; Xue, 2008) pioneered the exploration of Chinese verbal and nominal SRLs, given golden predicates. Among them, Xue and Palmer (2005) studied Chinese verbal SRL using Chinese PropBank and achieved the performance of 91.3 and 61.3 in F1-measure on golden and automatic syntactic parse trees, respectively. Xue (2006b) extended their study on Chinese nominal SRL and attempted to improve the performance of nominal SRL by simply inCorresponding author 1280 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,"
D09-1133,J08-2004,0,0.609138,"arious nominal SRL-specific features besides the traditional verbal SRL-related features on NomBank. They achieved the performance of 72.73 and 69.14 in F1-measure on golden and automatic syntactic parse trees, respectively, given golden nominal predicates. For SRL in Chinese, Sun and Jurafsky (2004) and Pradhan et al. (2004) pioneered the research on Chinese verbal and nominal SRLs, respectively, on small private datasets. Taking the advantage of recent release of Chinese PropBank (Xue and Palmer, 2003) and Chinese NomBank (Xue, 2006a), Xue and his colleagues (Xue and Palmer 2005; Xue 2006b; Xue, 2008) pioneered the exploration of Chinese verbal and nominal SRLs, given golden predicates. Among them, Xue and Palmer (2005) studied Chinese verbal SRL using Chinese PropBank and achieved the performance of 91.3 and 61.3 in F1-measure on golden and automatic syntactic parse trees, respectively. Xue (2006b) extended their study on Chinese nominal SRL and attempted to improve the performance of nominal SRL by simply inCorresponding author 1280 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1280–1288, c Singapore, 6-7 August 2009. 2009 ACL and AFNLP por"
D09-1133,W08-2121,0,\N,Missing
D09-1133,C98-1013,0,\N,Missing
D13-1082,P96-1041,0,0.290232,"translations. However, CSLMs have not been used in the first pass decoding of SMT, because using CSLMs in decoding takes a lot of time. In contrast, we propose a method for converting CSLMs into back-off n-gram language models (BNLMs) so that we can use converted CSLMs in decoding. We show that they outperform the original BNLMs and are comparable with the traditional use of CSLMs in reranking. 1 Introduction Language models are important in natural language processing tasks such as speech recognition and statistical machine translation. Traditionally, backoff n-gram language models (BNLMs) (Chen and Goodman, 1996; Chen and Goodman, 1998; Stolcke, 2002) are being widely used for these tasks. Recently, neural network language models, or continuous-space language models (CSLMs) (Bengio et al., 2003; Schwenk, 2007; Le et al., 2011) are being used in statistical machine translation (SMT) (Schwenk et al., 2006; Son et al., 2010; Schwenk et al., 2012; Son et al., 2012; Niehues and Waibel, 2012). These works have shown that CSLMs can improve the BLEU (Papineni et al., 2002) scores of SMT when compared with BNLMs, on the condition that the training data for language modeling are the same size. However, in prac"
D13-1082,W04-3250,0,0.0751864,"development data. After we selected the interpolation weight, we applied MERT again to the 2,000 sentence development data to tune the weight parameters.2 We call this BNLM CONV42. We also obtained CONV746 by re-writing BNLM746 with CSLM42 2 We aware that the interpolation weight might be determined by minimizing the perplexity on the development data. However, we opted to directly maximize the BLEU score. LMs BNLM42 CONV42 BNLM746 CONV746 1st pass 31.60 32.58 32.83 33.22 rerank 32.44 32.98 33.36 33.54 Table 1: Comparison of BLEU scores We also performed the paired bootstrap resampling test (Koehn, 2004).3 We sampled 2000 samples for each significance test. Table 2 shows the results of a statistical significance test, in which the “1st” is short for the “1st pass”. The marks indicate whether the LM to the left of a mark is significantly better than that above the mark at a certain level. (“≫”: significantly better at α = 0.01, “>”: α = 0.05, “−”: not significantly better at α = 0.05) First, as shown in the tables, the reranking by applying CSLM42 increased the BLEU scores for all language models. This observation is in accordance with those of previous work (Schwenk, 2010; Huang et al., 2013)"
D13-1082,2012.iwslt-papers.3,0,0.182408,"CSLMs in reranking. 1 Introduction Language models are important in natural language processing tasks such as speech recognition and statistical machine translation. Traditionally, backoff n-gram language models (BNLMs) (Chen and Goodman, 1996; Chen and Goodman, 1998; Stolcke, 2002) are being widely used for these tasks. Recently, neural network language models, or continuous-space language models (CSLMs) (Bengio et al., 2003; Schwenk, 2007; Le et al., 2011) are being used in statistical machine translation (SMT) (Schwenk et al., 2006; Son et al., 2010; Schwenk et al., 2012; Son et al., 2012; Niehues and Waibel, 2012). These works have shown that CSLMs can improve the BLEU (Papineni et al., 2002) scores of SMT when compared with BNLMs, on the condition that the training data for language modeling are the same size. However, in practice, CSLMs have not been widely used in SMT. One reason is that the computational costs of training and using CSLMs are very high. Various methods have been proposed to tackle the training cost issues (Son et al., 2010; Schwenk et al., 2012; Mikolov et al., 2011). However, there has been little work on reducing using costs. Since the using costs of CSLMs are very high, it is dif"
D13-1082,J03-1002,0,0.01,"n that case, we use the probabilities in the BNLM as they are. 4 Experiments 4.1 Common settings We used the patent data for the Chinese to English patent translation subtask from the NTCIR-9 patent translation task (Goto et al., 2011). The parallel training, development, and test data consisted of 1 M, 2,000, and 2,000 sentences, respectively. We followed the settings of the NTCIR-9 Chinese to English translation baseline system (Goto et al., 2011) except that we used various language models to compare them. We used the MOSES phrasebased SMT system (Koehn et al., 2003), together with Giza++ (Och and Ney, 2003) for alignment and MERT (Och, 2003) for tuning on the development data. The translation performance was measured by the case-insensitive BLEU scores on the tokenized test data. We used mteval-v13a.pl for calculating BLEU scores.1 1 It is available at http://www.itl.nist.gov/iad/ mig/tests/mt/2009/ 847 We used the 14 standard SMT features: five translation model scores, one word penalty score, seven distortion scores and one language model score. Each of the different language models was used to calculate the language model score. As the baseline BNLM, we trained a 5-gram BNLM with modified Kne"
D13-1082,P03-1021,0,0.0144364,"e BNLM as they are. 4 Experiments 4.1 Common settings We used the patent data for the Chinese to English patent translation subtask from the NTCIR-9 patent translation task (Goto et al., 2011). The parallel training, development, and test data consisted of 1 M, 2,000, and 2,000 sentences, respectively. We followed the settings of the NTCIR-9 Chinese to English translation baseline system (Goto et al., 2011) except that we used various language models to compare them. We used the MOSES phrasebased SMT system (Koehn et al., 2003), together with Giza++ (Och and Ney, 2003) for alignment and MERT (Och, 2003) for tuning on the development data. The translation performance was measured by the case-insensitive BLEU scores on the tokenized test data. We used mteval-v13a.pl for calculating BLEU scores.1 1 It is available at http://www.itl.nist.gov/iad/ mig/tests/mt/2009/ 847 We used the 14 standard SMT features: five translation model scores, one word penalty score, seven distortion scores and one language model score. Each of the different language models was used to calculate the language model score. As the baseline BNLM, we trained a 5-gram BNLM with modified Kneser-Ney smoothing using the English"
D13-1082,P02-1040,0,0.0964055,"e processing tasks such as speech recognition and statistical machine translation. Traditionally, backoff n-gram language models (BNLMs) (Chen and Goodman, 1996; Chen and Goodman, 1998; Stolcke, 2002) are being widely used for these tasks. Recently, neural network language models, or continuous-space language models (CSLMs) (Bengio et al., 2003; Schwenk, 2007; Le et al., 2011) are being used in statistical machine translation (SMT) (Schwenk et al., 2006; Son et al., 2010; Schwenk et al., 2012; Son et al., 2012; Niehues and Waibel, 2012). These works have shown that CSLMs can improve the BLEU (Papineni et al., 2002) scores of SMT when compared with BNLMs, on the condition that the training data for language modeling are the same size. However, in practice, CSLMs have not been widely used in SMT. One reason is that the computational costs of training and using CSLMs are very high. Various methods have been proposed to tackle the training cost issues (Son et al., 2010; Schwenk et al., 2012; Mikolov et al., 2011). However, there has been little work on reducing using costs. Since the using costs of CSLMs are very high, it is difficult to use CSLMs in decoding directly. A common approach in SMT using CSLMs i"
D13-1082,P06-2093,0,0.271464,"ey outperform the original BNLMs and are comparable with the traditional use of CSLMs in reranking. 1 Introduction Language models are important in natural language processing tasks such as speech recognition and statistical machine translation. Traditionally, backoff n-gram language models (BNLMs) (Chen and Goodman, 1996; Chen and Goodman, 1998; Stolcke, 2002) are being widely used for these tasks. Recently, neural network language models, or continuous-space language models (CSLMs) (Bengio et al., 2003; Schwenk, 2007; Le et al., 2011) are being used in statistical machine translation (SMT) (Schwenk et al., 2006; Son et al., 2010; Schwenk et al., 2012; Son et al., 2012; Niehues and Waibel, 2012). These works have shown that CSLMs can improve the BLEU (Papineni et al., 2002) scores of SMT when compared with BNLMs, on the condition that the training data for language modeling are the same size. However, in practice, CSLMs have not been widely used in SMT. One reason is that the computational costs of training and using CSLMs are very high. Various methods have been proposed to tackle the training cost issues (Son et al., 2010; Schwenk et al., 2012; Mikolov et al., 2011). However, there has been little"
D13-1082,W12-2702,0,0.0213596,"comparable with the traditional use of CSLMs in reranking. 1 Introduction Language models are important in natural language processing tasks such as speech recognition and statistical machine translation. Traditionally, backoff n-gram language models (BNLMs) (Chen and Goodman, 1996; Chen and Goodman, 1998; Stolcke, 2002) are being widely used for these tasks. Recently, neural network language models, or continuous-space language models (CSLMs) (Bengio et al., 2003; Schwenk, 2007; Le et al., 2011) are being used in statistical machine translation (SMT) (Schwenk et al., 2006; Son et al., 2010; Schwenk et al., 2012; Son et al., 2012; Niehues and Waibel, 2012). These works have shown that CSLMs can improve the BLEU (Papineni et al., 2002) scores of SMT when compared with BNLMs, on the condition that the training data for language modeling are the same size. However, in practice, CSLMs have not been widely used in SMT. One reason is that the computational costs of training and using CSLMs are very high. Various methods have been proposed to tackle the training cost issues (Son et al., 2010; Schwenk et al., 2012; Mikolov et al., 2011). However, there has been little work on reducing using costs. Since the"
D13-1082,D10-1076,0,0.552441,"inal BNLMs and are comparable with the traditional use of CSLMs in reranking. 1 Introduction Language models are important in natural language processing tasks such as speech recognition and statistical machine translation. Traditionally, backoff n-gram language models (BNLMs) (Chen and Goodman, 1996; Chen and Goodman, 1998; Stolcke, 2002) are being widely used for these tasks. Recently, neural network language models, or continuous-space language models (CSLMs) (Bengio et al., 2003; Schwenk, 2007; Le et al., 2011) are being used in statistical machine translation (SMT) (Schwenk et al., 2006; Son et al., 2010; Schwenk et al., 2012; Son et al., 2012; Niehues and Waibel, 2012). These works have shown that CSLMs can improve the BLEU (Papineni et al., 2002) scores of SMT when compared with BNLMs, on the condition that the training data for language modeling are the same size. However, in practice, CSLMs have not been widely used in SMT. One reason is that the computational costs of training and using CSLMs are very high. Various methods have been proposed to tackle the training cost issues (Son et al., 2010; Schwenk et al., 2012; Mikolov et al., 2011). However, there has been little work on reducing u"
D13-1082,N12-1005,0,0.0565609,"raditional use of CSLMs in reranking. 1 Introduction Language models are important in natural language processing tasks such as speech recognition and statistical machine translation. Traditionally, backoff n-gram language models (BNLMs) (Chen and Goodman, 1996; Chen and Goodman, 1998; Stolcke, 2002) are being widely used for these tasks. Recently, neural network language models, or continuous-space language models (CSLMs) (Bengio et al., 2003; Schwenk, 2007; Le et al., 2011) are being used in statistical machine translation (SMT) (Schwenk et al., 2006; Son et al., 2010; Schwenk et al., 2012; Son et al., 2012; Niehues and Waibel, 2012). These works have shown that CSLMs can improve the BLEU (Papineni et al., 2002) scores of SMT when compared with BNLMs, on the condition that the training data for language modeling are the same size. However, in practice, CSLMs have not been widely used in SMT. One reason is that the computational costs of training and using CSLMs are very high. Various methods have been proposed to tackle the training cost issues (Son et al., 2010; Schwenk et al., 2012; Mikolov et al., 2011). However, there has been little work on reducing using costs. Since the using costs of CSL"
D13-1082,N03-1017,0,\N,Missing
D14-1022,C10-2044,0,0.0352255,"translation spans, respectively. A source word is marked as beginning (ending) boundary if it is the first (last) word of a translation span. However, a source span whose first and last words are both boundaries is not always a translation span. In Figure 1, “I” is a beginning boundary since it is the first word of translation span “I will” and “experiment” is an ending boundary since it is the last word of translation span “finish this experiment” , but “I will finish this experiment” is not a translation span. This happens because the translation spans are nested or hierarchical. Note that (He et al., 2010) also learned phrase boundaries to constrain decoding, but their approach identified boundaries only for monotone translation. In this paper, taking fully into account that translation spans being nested, we propose an approach to learn hierarchical translation spans directly from an aligned parallel corpus that makes more accurate identification over translation spans. The rest of the paper is structured as follows: In Section 2, we briefly review the HPB translation model. Section 3 describes our approach. We describe experiments in Section 4 and conclude in Section 5. 2 w (X → hγ, α, ∼i) ="
D14-1022,N03-1017,0,0.0383474,"118M 104M 150k 273k Table 1: Data sets. sets were both provided for CE task while only the test set was provided for JE task. Therefore, we used the sentences from the NTCIR-8 JE test set as the development set. Word segmentation was done by BaseSeg (Zhao et al., 2006; Zhao and Kit, 2008; Zhao et al., 2010; Zhao and Kit, 2011; Zhao et al., 2013) for Chinese and Mecab 2 for Japanese. To learn the classifiers for each translation task, the training set and development set were put together to obtain symmetric word alignment using GIZA++ (Och and Ney, 2003) and the growdiag-final-and heuristic (Koehn et al., 2003). The source span instances extracted from the aligned training and development sets were used as the training and validation data for the classifiers. The toolkit Wapiti (Lavergne et al., 2010) was adopted to train ME classifiers using the classical quasi-newton optimization algorithm with limited memory. The NNs are trained by the toolkit NPLM (Vaswani et al., 2013). We chose “rectifier” as the activation function and the logarithmic loss function for NNs. The number of epochs was set to 20. Other parameters were set to default Classifiers We compare two machine learning methods for learning"
D14-1022,P07-2045,0,0.0296419,"Missing"
D14-1022,W04-3250,0,0.0165102,"word order difference of JE task is much more significant than that of CE task, there are more negative Japanese translation span instances than Chinese. In JE tasks, the ME classifiers C8 , C9 and C10 predicted all new instances to be negative due to the heavily unbalanced instance distribution. We compare our method with the baseline and the boundary learning method (BLM) (Xiong et al., 2010) based on Maximum Entropy Markov Models with Markov order 2. Table 3 reports BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) scores. Significance tests are conducted using bootstrap sampling (Koehn, 2004). Our ME classifiers achieve comparable translation improvement with the BLM and NN classifiers enhance translation system significantly compared to others. Table 3 also shows that the relative gain was higher for higher n-grams, which is reasonable since the higher n-grams have higher ambiguities in the translation rule application. It is true that because of multiple parallel sentences, a source span can be applied with translaAs shown in Table 2, NN outperformed ME approach for our classification tasks. As the span length growing, the advantage of NN became more significant. Since the class"
D14-1022,P10-1052,0,0.0701371,"Missing"
D14-1022,2011.mtsummit-papers.28,0,0.029711,"ine system. 1 我 会 在 明天 之前 完成 这个 实验 Figure 1: A translation example. model, which represents if the source span covered by a translation rule is a syntactic constituent. However, the experimental results showed this feature gave no significant improvement. Instead of using the undifferentiated constituency feature, (Marton and Resnik, 2008) defined different soft syntactic features for different constituent types and obtained substantial performance improvement. Later, (Mylonakis and Sima’an, 2011) introduced joint probability synchronous grammars to integrate flexible linguistic information. (Liu et al., 2011) proposed the soft syntactic constraint model based on discriminative classifiers for each constituent type and integrated all of them into the translation model. (Cui et al., 2010) focused on hierarchical rule selection using many features including syntax constituents. These works have demonstrated the benefits of using syntactic features in the HPB model. However, high quality syntax parsers are not always easily obtained for many languages. Without this problem, word alignment constraints can also be used to guide the application of the rules. Suppose that we want to translate the English"
D14-1022,P08-1114,0,0.0223738,"t require parsers. Rich source side contextual features and advanced machine learning methods were utilized for this learning task. The proposed approach was evaluated on NTCIR-9 Chinese-English and Japanese-English translation tasks and showed significant improvement over the baseline system. 1 我 会 在 明天 之前 完成 这个 实验 Figure 1: A translation example. model, which represents if the source span covered by a translation rule is a syntactic constituent. However, the experimental results showed this feature gave no significant improvement. Instead of using the undifferentiated constituency feature, (Marton and Resnik, 2008) defined different soft syntactic features for different constituent types and obtained substantial performance improvement. Later, (Mylonakis and Sima’an, 2011) introduced joint probability synchronous grammars to integrate flexible linguistic information. (Liu et al., 2011) proposed the soft syntactic constraint model based on discriminative classifiers for each constituent type and integrated all of them into the translation model. (Cui et al., 2010) focused on hierarchical rule selection using many features including syntax constituents. These works have demonstrated the benefits of using"
D14-1022,J96-1002,0,0.0295998,"ta for the classifiers. The toolkit Wapiti (Lavergne et al., 2010) was adopted to train ME classifiers using the classical quasi-newton optimization algorithm with limited memory. The NNs are trained by the toolkit NPLM (Vaswani et al., 2013). We chose “rectifier” as the activation function and the logarithmic loss function for NNs. The number of epochs was set to 20. Other parameters were set to default Classifiers We compare two machine learning methods for learning a series of binary classifiers. For the first method, each Ck is individually learned using the maximum entropy (ME) approach (Berger et al., 1996):  P exp t µt ht (v, f (D) , i, j)  P Pk (v|f (D) , i, j) = P 0 v 0 exp t µt ht (v , f (D) , i, j) (5) where ht is a feature function and µt is weight of ht . We use rich source contextual features: unigram, bigram and trigram of the phrase [fi−3 , ..., fj+3 ]. As the second method, these classification tasks are learned in the continuous space using feedforward neural networks (NNs). Each Ck has the similar structure with the NN language model (Vaswani et al., 2013). The inputs to the NN are indices of the words: [fi−3 , ..., fj+3 ]. Each source word is projected into an N dimensional vecto"
D14-1022,P05-1033,0,0.124086,"wever, high quality syntax parsers are not always easily obtained for many languages. Without this problem, word alignment constraints can also be used to guide the application of the rules. Suppose that we want to translate the English sentence into the Chinese sentence in Figure 1, a translation rule can be applied to the source span “finish this experiment by tomorrow”. Nonetheless, if a rule is applied to “experiment by”, then the Chinese translation can not be correctly obtained, because the target span projected from “exIntroduction The hierarchical phrase-based (HPB) translation model (Chiang, 2005) has been widely adopted in statistical machine translation (SMT) tasks. The HPB translation rules based on the synchronous context free grammar (SCFG) are simple and powerful. One drawback of the HPB model is the applications of translation rules to the input sentence are highly ambiguous. For example, a rule whose English side is “X1 by X2” can be applied to any word sequence that has “by” in them. In Figure 1, this rule can be applied to the whole sentence as well as to “experiment by tomorrow”. In order to tackle rule application ambiguities, a few previous works used syntax trees. Chiang"
D14-1022,P11-1065,0,0.036752,"Missing"
D14-1022,P10-2002,0,0.0140531,"experimental results showed this feature gave no significant improvement. Instead of using the undifferentiated constituency feature, (Marton and Resnik, 2008) defined different soft syntactic features for different constituent types and obtained substantial performance improvement. Later, (Mylonakis and Sima’an, 2011) introduced joint probability synchronous grammars to integrate flexible linguistic information. (Liu et al., 2011) proposed the soft syntactic constraint model based on discriminative classifiers for each constituent type and integrated all of them into the translation model. (Cui et al., 2010) focused on hierarchical rule selection using many features including syntax constituents. These works have demonstrated the benefits of using syntactic features in the HPB model. However, high quality syntax parsers are not always easily obtained for many languages. Without this problem, word alignment constraints can also be used to guide the application of the rules. Suppose that we want to translate the English sentence into the Chinese sentence in Figure 1, a translation rule can be applied to the source span “finish this experiment by tomorrow”. Nonetheless, if a rule is applied to “expe"
D14-1022,P02-1038,0,0.10792,"0.97 0.14 0.98 0 1 0 1 0 1 NN P N 0.86 0.80 0.71 0.87 0.63 0.90 0.54 0.93 0.47 0.95 0.41 0.96 0.33 0.97 0.32 0.97 0.25 0.98 0.23 0.99 Table 2: Classification accuracies. The Rate column represents ratio of positive instances to negative instances; the P and N columns give classification accuracies for positive and negative instances. LM Toolkit 3 with improved Kneser-Ney smoothing. {C1 , ..., C10 } were integrated into the baseline with different weights, which were tuned by MERT (Och, 2003) together with other feature weights (language model, word penalty,...) under the log-linear framework (Och and Ney, 2002). values. The training time of one classifier on a 12-core 3.47GHz Xeon X5690 machine was 0.5h (2.5h) using ME (NN) approach for CE task; 1h (4h) using ME (NN) approach for JE task . The classification results are shown in Table 2. Instead of the undifferentiated classification accuracy, we present separate classification accuracies for positive and negative instances. The big difference between classification accuracies for positive and negative instances was caused by the unbalanced rate of positive and negative instances in the training corpus. For example, if there are more positive traini"
D14-1022,J03-1002,0,0.0125398,"rds #Vocab SOURCE TARGET 954k 37.2M 40.4M 288k 504k 3.14M 118M 104M 150k 273k Table 1: Data sets. sets were both provided for CE task while only the test set was provided for JE task. Therefore, we used the sentences from the NTCIR-8 JE test set as the development set. Word segmentation was done by BaseSeg (Zhao et al., 2006; Zhao and Kit, 2008; Zhao et al., 2010; Zhao and Kit, 2011; Zhao et al., 2013) for Chinese and Mecab 2 for Japanese. To learn the classifiers for each translation task, the training set and development set were put together to obtain symmetric word alignment using GIZA++ (Och and Ney, 2003) and the growdiag-final-and heuristic (Koehn et al., 2003). The source span instances extracted from the aligned training and development sets were used as the training and validation data for the classifiers. The toolkit Wapiti (Lavergne et al., 2010) was adopted to train ME classifiers using the classical quasi-newton optimization algorithm with limited memory. The NNs are trained by the toolkit NPLM (Vaswani et al., 2013). We chose “rectifier” as the activation function and the logarithmic loss function for NNs. The number of epochs was set to 20. Other parameters were set to default Classi"
D14-1022,P03-1021,0,0.0214277,".08 0.73 0.52 0.36 0.26 0.20 0.16 0.13 0.10 0.08 JE ME P N 0.85 0.79 0.69 0.84 0.56 0.89 0.48 0.93 0.30 0.96 0.25 0.97 0.14 0.98 0 1 0 1 0 1 NN P N 0.86 0.80 0.71 0.87 0.63 0.90 0.54 0.93 0.47 0.95 0.41 0.96 0.33 0.97 0.32 0.97 0.25 0.98 0.23 0.99 Table 2: Classification accuracies. The Rate column represents ratio of positive instances to negative instances; the P and N columns give classification accuracies for positive and negative instances. LM Toolkit 3 with improved Kneser-Ney smoothing. {C1 , ..., C10 } were integrated into the baseline with different weights, which were tuned by MERT (Och, 2003) together with other feature weights (language model, word penalty,...) under the log-linear framework (Och and Ney, 2002). values. The training time of one classifier on a 12-core 3.47GHz Xeon X5690 machine was 0.5h (2.5h) using ME (NN) approach for CE task; 1h (4h) using ME (NN) approach for JE task . The classification results are shown in Table 2. Instead of the undifferentiated classification accuracy, we present separate classification accuracies for positive and negative instances. The big difference between classification accuracies for positive and negative instances was caused by the"
D14-1022,P02-1040,0,0.0924408,"erence at the p < 0.01 level and - represents a significant difference at the p < 0.05 level against the BLM. Since the word order difference of JE task is much more significant than that of CE task, there are more negative Japanese translation span instances than Chinese. In JE tasks, the ME classifiers C8 , C9 and C10 predicted all new instances to be negative due to the heavily unbalanced instance distribution. We compare our method with the baseline and the boundary learning method (BLM) (Xiong et al., 2010) based on Maximum Entropy Markov Models with Markov order 2. Table 3 reports BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) scores. Significance tests are conducted using bootstrap sampling (Koehn, 2004). Our ME classifiers achieve comparable translation improvement with the BLM and NN classifiers enhance translation system significantly compared to others. Table 3 also shows that the relative gain was higher for higher n-grams, which is reasonable since the higher n-grams have higher ambiguities in the translation rule application. It is true that because of multiple parallel sentences, a source span can be applied with translaAs shown in Table 2, NN outperformed ME approach for our"
D14-1022,2006.amta-papers.25,0,0.020084,"- represents a significant difference at the p < 0.05 level against the BLM. Since the word order difference of JE task is much more significant than that of CE task, there are more negative Japanese translation span instances than Chinese. In JE tasks, the ME classifiers C8 , C9 and C10 predicted all new instances to be negative due to the heavily unbalanced instance distribution. We compare our method with the baseline and the boundary learning method (BLM) (Xiong et al., 2010) based on Maximum Entropy Markov Models with Markov order 2. Table 3 reports BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) scores. Significance tests are conducted using bootstrap sampling (Koehn, 2004). Our ME classifiers achieve comparable translation improvement with the BLM and NN classifiers enhance translation system significantly compared to others. Table 3 also shows that the relative gain was higher for higher n-grams, which is reasonable since the higher n-grams have higher ambiguities in the translation rule application. It is true that because of multiple parallel sentences, a source span can be applied with translaAs shown in Table 2, NN outperformed ME approach for our classification tasks. As the s"
D14-1022,D13-1140,0,0.074174,"ab 2 for Japanese. To learn the classifiers for each translation task, the training set and development set were put together to obtain symmetric word alignment using GIZA++ (Och and Ney, 2003) and the growdiag-final-and heuristic (Koehn et al., 2003). The source span instances extracted from the aligned training and development sets were used as the training and validation data for the classifiers. The toolkit Wapiti (Lavergne et al., 2010) was adopted to train ME classifiers using the classical quasi-newton optimization algorithm with limited memory. The NNs are trained by the toolkit NPLM (Vaswani et al., 2013). We chose “rectifier” as the activation function and the logarithmic loss function for NNs. The number of epochs was set to 20. Other parameters were set to default Classifiers We compare two machine learning methods for learning a series of binary classifiers. For the first method, each Ck is individually learned using the maximum entropy (ME) approach (Berger et al., 1996):  P exp t µt ht (v, f (D) , i, j)  P Pk (v|f (D) , i, j) = P 0 v 0 exp t µt ht (v , f (D) , i, j) (5) where ht is a feature function and µt is weight of ht . We use rich source contextual features: unigram, bigram and t"
D14-1022,W06-0127,1,0.879402,"i to n do 3: if ∃eqp , 1 ≤ p ≤ q ≤ m & ∃ (k, t) ∈ A, i ≤ k ≤ j, p ≤ t ≤ q & ∀ (k, t) ∈ A, i ≤ k ≤ j ↔ p ≤ t ≤ q then 4: fij is a positive instance for Cj−i+1 5: else 6: fij is a negative instance for Cj−i+1 7: end if 8: end for 9: end for 3.3 Experiment CE JE #Sents #Words #Vocab #Sents #Words #Vocab SOURCE TARGET 954k 37.2M 40.4M 288k 504k 3.14M 118M 104M 150k 273k Table 1: Data sets. sets were both provided for CE task while only the test set was provided for JE task. Therefore, we used the sentences from the NTCIR-8 JE test set as the development set. Word segmentation was done by BaseSeg (Zhao et al., 2006; Zhao and Kit, 2008; Zhao et al., 2010; Zhao and Kit, 2011; Zhao et al., 2013) for Chinese and Mecab 2 for Japanese. To learn the classifiers for each translation task, the training set and development set were put together to obtain symmetric word alignment using GIZA++ (Och and Ney, 2003) and the growdiag-final-and heuristic (Koehn et al., 2003). The source span instances extracted from the aligned training and development sets were used as the training and validation data for the classifiers. The toolkit Wapiti (Lavergne et al., 2010) was adopted to train ME classifiers using the classical"
D14-1022,N10-1016,0,\N,Missing
D14-1023,P14-1142,1,0.292552,"i ∈ V0 Pb (wi |hi ) otherwise 0 (1) where V0 is the short-list,∑Pc (·) is the probability calculated by CSLM, w∈V0 Pc (w|hi ) is the summary of probabilities of the neuron for all the words in the short-list, Pb (·) is the probability calculated by the BNLM, and Ps (hi ) = ∑ Pb (v|hi ). (2) v∈V0 We may regard that CSLM redistributes the probability mass of all words in the short-list, which is calculated by using the n-gram LM. Existing CSLM Converting Methods 2.2 Existing Converting Methods Traditional Backoff N -gram LMs (BNLMs) have been widely used in many NLP tasks (Zhang and Zhao, 2013; Jia and Zhao, 2014; Zhao et al., 2013; Zhang et al., 2012; Xu and Zhao, 2012; Wang et al., 2013b; Jia and Zhao, 2013; Wang et al., 2014). Recently, CSLMs become popular because they can obtain more accurate probability estimation. As baseline systems, our approach proposed in (Wang et al., 2013a) only re-writes the probabilities from CSLM into the BNLM, so it can only conduct a convert LM with the same size as the original one. The main difference between our proposed method in this paper and our previous approach is that n-grams outside the corpus are generated firstly and the probabilities using CSLM are calc"
D14-1023,W07-0733,0,0.109437,"Missing"
D14-1023,P07-2045,0,0.00563399,"ased and the BLEU scores trended to increase. These indicated that our proposed method can give better probability estimation for LM and better performance for SMT. (2) In comparison with the grown LMs in ArGrown n-grams Input Corpus CSLM Interpolate BNLM Output Grown n-grams with Probabilities Grown LM Figure 1: NN based bilingual LM growing. 4 Experiments and Results 4.1 Experiment Setting up The same setting up of the NTCIR-9 Chinese to English translation baseline system (Goto et al., 2011) was followed, only with various LMs to compare them. The Moses phrase-based SMT system was applied (Koehn et al., 2007), together with GIZA++ (Och and Ney, 2003) for alignment and MERT (Och, 2003) for tuning on the development data. Fourteen standard SMT features were used: five translation model scores, one word penalty score, seven distortion scores, and one LM score. The translation performance was measured by the case-insensitive BLEU on the tokenized test data. We used the patent data for the Chinese to English patent translation subtask from the NTCIR-9 patent translation task (Goto et al., 2011). The parallel training, development, and test data sets consist of 1 million (M), 2,000, and 2,000 sentences,"
D14-1023,D13-1106,0,0.0135132,"accurately predict the probabilities of the n-grams, which are not in the training corpus. However, in practice, CSLMs have not been widely used in the current SMT systems, due to their too high computational cost. Vaswani and colleagues (2013) propose a method for reducing the training cost of CSLM and apply it to SMT decoder. However, they do not show their improvement for decoding speed, and their method is still slower than the n-gram LM. There are several other methods for attempting to implement neural network based LM or translation model for SMT (Devlin et al., 2014; Liu et al., 2014; Auli et al., 2013). However, the decoding speed using n-gram LM is still state-ofthe-art one. Some approaches calculate the probabilities of the n-grams n-grams before decoding, and store them in the n-gram format (Wang et al., 2013a; Arsoy et al., 2013; Arsoy et al., 2014). The ‘converted CSLM’ can be directly used in SMT. Though more n-grams which are not in the trainSince larger n-gram Language Model (LM) usually performs better in Statistical Machine Translation (SMT), how to construct efficient large LM is an important topic in SMT. However, most of the existing LM growing methods need an extra monolingual"
D14-1023,W04-3250,0,0.0727048,"est lists of SMT. Our previous converted LM, Arsoy’s grown LMs and bilingual grown LMs were interpolated with the original BNLMs, using default setting of SRILM5 . To reduce the randomness of MERT, we used two methods for tuning the weights of different SMT features, and two BLEU scores are corresponding to these two methods. The BLEU-s indicated that the same weights of the BNLM (BN) features were used for all the SMT systems. The BLEU-i indicated that the MERT was run independently by three times and the average BLEU scores were taken. We also performed the paired bootstrap resampling test (Koehn, 2004)6 . Two thousands samples were sampled for each significance test. The marks at the right of the BLEU score indicated whether the LMs were significantly better/worse than the Arsoy’s grown LMs with the same IDs for SMT (“++/−−”: significantly better/worse at α = 0.01, “+/−”: α = 0.05, no mark: not significantly better/worse at α = 0.05). From the results shown in Table 1, we can get the following observations: (1) Nearly all the bilingual grown LMs outperformed both BNLM and our previous converted LM on PPL and BLEU. As the size of grown LMs is increased, the PPL always decreased and the BLEU"
D14-1023,2012.eamt-1.60,0,0.0244855,"200240, China 2 Multilingual Translation Laboratory, MASTAR Project, National Institute of Information and Communications Technology, 3-5 Hikaridai, Keihanna Science City, Kyoto, 619-0289, Japan 3 Key Laboratory of Shanghai Education Commission for Intelligent Interaction and Cognitive Engineering, Shanghai Jiao Tong University, Shanghai, 200240, China wangrui.nlp@gmail.com, {zhaohai, blu}@cs.sjtu.edu.cn, {mutiyama, eiichiro.sumita}@nict.go.jp Abstract 2007). In addition, it is very difficult or even impossible to collect an extra large corpus for some special domains such as the TED corpus (Cettolo et al., 2012) or for some rare languages. Therefore, to improve the performance of LMs, without assistance of extra corpus, is one of important research topics in SMT. Recently, Continues Space Language Model (CSLM), especially Neural Network based Language Model (NNLM) (Bengio et al., 2003; Schwenk, 2007; Mikolov et al., 2010; Le et al., 2011), is being actively used in SMT (Schwenk et al., 2006; Son et al., 2010; Schwenk, 2010; Schwenk et al., 2012; Son et al., 2012; Niehues and Waibel, 2012). One of the main advantages of CSLM is that it can more accurately predict the probabilities of the n-grams, whic"
D14-1023,P14-1140,0,0.0260099,"Missing"
D14-1023,P14-1129,0,0.0325731,"Missing"
D14-1023,2012.iwslt-papers.3,0,0.044911,"s very difficult or even impossible to collect an extra large corpus for some special domains such as the TED corpus (Cettolo et al., 2012) or for some rare languages. Therefore, to improve the performance of LMs, without assistance of extra corpus, is one of important research topics in SMT. Recently, Continues Space Language Model (CSLM), especially Neural Network based Language Model (NNLM) (Bengio et al., 2003; Schwenk, 2007; Mikolov et al., 2010; Le et al., 2011), is being actively used in SMT (Schwenk et al., 2006; Son et al., 2010; Schwenk, 2010; Schwenk et al., 2012; Son et al., 2012; Niehues and Waibel, 2012). One of the main advantages of CSLM is that it can more accurately predict the probabilities of the n-grams, which are not in the training corpus. However, in practice, CSLMs have not been widely used in the current SMT systems, due to their too high computational cost. Vaswani and colleagues (2013) propose a method for reducing the training cost of CSLM and apply it to SMT decoder. However, they do not show their improvement for decoding speed, and their method is still slower than the n-gram LM. There are several other methods for attempting to implement neural network based LM or translati"
D14-1023,J03-1002,0,0.00542098,"e. These indicated that our proposed method can give better probability estimation for LM and better performance for SMT. (2) In comparison with the grown LMs in ArGrown n-grams Input Corpus CSLM Interpolate BNLM Output Grown n-grams with Probabilities Grown LM Figure 1: NN based bilingual LM growing. 4 Experiments and Results 4.1 Experiment Setting up The same setting up of the NTCIR-9 Chinese to English translation baseline system (Goto et al., 2011) was followed, only with various LMs to compare them. The Moses phrase-based SMT system was applied (Koehn et al., 2007), together with GIZA++ (Och and Ney, 2003) for alignment and MERT (Och, 2003) for tuning on the development data. Fourteen standard SMT features were used: five translation model scores, one word penalty score, seven distortion scores, and one LM score. The translation performance was measured by the case-insensitive BLEU on the tokenized test data. We used the patent data for the Chinese to English patent translation subtask from the NTCIR-9 patent translation task (Goto et al., 2011). The parallel training, development, and test data sets consist of 1 million (M), 2,000, and 2,000 sentences, respectively. Using SRILM (Stolcke, 2002;"
D14-1023,I13-1170,1,0.794008,"ed by CSLM, w∈V0 Pc (w|hi ) is the summary of probabilities of the neuron for all the words in the short-list, Pb (·) is the probability calculated by the BNLM, and Ps (hi ) = ∑ Pb (v|hi ). (2) v∈V0 We may regard that CSLM redistributes the probability mass of all words in the short-list, which is calculated by using the n-gram LM. Existing CSLM Converting Methods 2.2 Existing Converting Methods Traditional Backoff N -gram LMs (BNLMs) have been widely used in many NLP tasks (Zhang and Zhao, 2013; Jia and Zhao, 2014; Zhao et al., 2013; Zhang et al., 2012; Xu and Zhao, 2012; Wang et al., 2013b; Jia and Zhao, 2013; Wang et al., 2014). Recently, CSLMs become popular because they can obtain more accurate probability estimation. As baseline systems, our approach proposed in (Wang et al., 2013a) only re-writes the probabilities from CSLM into the BNLM, so it can only conduct a convert LM with the same size as the original one. The main difference between our proposed method in this paper and our previous approach is that n-grams outside the corpus are generated firstly and the probabilities using CSLM are calculated by using the same method as our previous approach. That is, the proposed new method is the"
D14-1023,P03-1021,0,0.0161605,"can give better probability estimation for LM and better performance for SMT. (2) In comparison with the grown LMs in ArGrown n-grams Input Corpus CSLM Interpolate BNLM Output Grown n-grams with Probabilities Grown LM Figure 1: NN based bilingual LM growing. 4 Experiments and Results 4.1 Experiment Setting up The same setting up of the NTCIR-9 Chinese to English translation baseline system (Goto et al., 2011) was followed, only with various LMs to compare them. The Moses phrase-based SMT system was applied (Koehn et al., 2007), together with GIZA++ (Och and Ney, 2003) for alignment and MERT (Och, 2003) for tuning on the development data. Fourteen standard SMT features were used: five translation model scores, one word penalty score, seven distortion scores, and one LM score. The translation performance was measured by the case-insensitive BLEU on the tokenized test data. We used the patent data for the Chinese to English patent translation subtask from the NTCIR-9 patent translation task (Goto et al., 2011). The parallel training, development, and test data sets consist of 1 million (M), 2,000, and 2,000 sentences, respectively. Using SRILM (Stolcke, 2002; Stolcke et al., 2011), we trained"
D14-1023,P95-1030,0,0.0389552,"rpus in SMT. The results show that our method can improve both the perplexity score for LM evaluation and BLEU score for SMT, and significantly outperforms the existing LM growing methods without extra corpus. 1 Introduction ‘Language Model (LM) Growing’ refers to adding n-grams outside the corpus together with their probabilities into the original LM. This operation is useful as it can make LM perform better through letting it become larger and larger, by only using a small training corpus. There are various methods for adding n-grams selected by different criteria from a monolingual corpus (Ristad and Thomas, 1995; Niesler and Woodland, 1996; Siu and Ostendorf, 2000; Siivola et al., 2007). However, all of these approaches need additional corpora. Meanwhile the extra corpora from different domains will not result in better LMs (Clarkson and Robinson, 1997; Iyer et al., 1997; Bellegarda, 2004; Koehn and Schroeder, ∗ Part of this work was done as Rui Wang visited in NICT. 189 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 189–195, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics ing corpus can be generated by using so"
D14-1023,D13-1140,0,0.0268942,"Missing"
D14-1023,P06-2093,0,0.0965172,", blu}@cs.sjtu.edu.cn, {mutiyama, eiichiro.sumita}@nict.go.jp Abstract 2007). In addition, it is very difficult or even impossible to collect an extra large corpus for some special domains such as the TED corpus (Cettolo et al., 2012) or for some rare languages. Therefore, to improve the performance of LMs, without assistance of extra corpus, is one of important research topics in SMT. Recently, Continues Space Language Model (CSLM), especially Neural Network based Language Model (NNLM) (Bengio et al., 2003; Schwenk, 2007; Mikolov et al., 2010; Le et al., 2011), is being actively used in SMT (Schwenk et al., 2006; Son et al., 2010; Schwenk, 2010; Schwenk et al., 2012; Son et al., 2012; Niehues and Waibel, 2012). One of the main advantages of CSLM is that it can more accurately predict the probabilities of the n-grams, which are not in the training corpus. However, in practice, CSLMs have not been widely used in the current SMT systems, due to their too high computational cost. Vaswani and colleagues (2013) propose a method for reducing the training cost of CSLM and apply it to SMT decoder. However, they do not show their improvement for decoding speed, and their method is still slower than the n-gram"
D14-1023,D13-1082,1,0.598465,"Missing"
D14-1023,W12-2702,0,0.0279048,".go.jp Abstract 2007). In addition, it is very difficult or even impossible to collect an extra large corpus for some special domains such as the TED corpus (Cettolo et al., 2012) or for some rare languages. Therefore, to improve the performance of LMs, without assistance of extra corpus, is one of important research topics in SMT. Recently, Continues Space Language Model (CSLM), especially Neural Network based Language Model (NNLM) (Bengio et al., 2003; Schwenk, 2007; Mikolov et al., 2010; Le et al., 2011), is being actively used in SMT (Schwenk et al., 2006; Son et al., 2010; Schwenk, 2010; Schwenk et al., 2012; Son et al., 2012; Niehues and Waibel, 2012). One of the main advantages of CSLM is that it can more accurately predict the probabilities of the n-grams, which are not in the training corpus. However, in practice, CSLMs have not been widely used in the current SMT systems, due to their too high computational cost. Vaswani and colleagues (2013) propose a method for reducing the training cost of CSLM and apply it to SMT decoder. However, they do not show their improvement for decoding speed, and their method is still slower than the n-gram LM. There are several other methods for attempting to i"
D14-1023,I13-1069,1,0.740408,"Missing"
D14-1023,C12-2131,1,0.530178,"ist,∑Pc (·) is the probability calculated by CSLM, w∈V0 Pc (w|hi ) is the summary of probabilities of the neuron for all the words in the short-list, Pb (·) is the probability calculated by the BNLM, and Ps (hi ) = ∑ Pb (v|hi ). (2) v∈V0 We may regard that CSLM redistributes the probability mass of all words in the short-list, which is calculated by using the n-gram LM. Existing CSLM Converting Methods 2.2 Existing Converting Methods Traditional Backoff N -gram LMs (BNLMs) have been widely used in many NLP tasks (Zhang and Zhao, 2013; Jia and Zhao, 2014; Zhao et al., 2013; Zhang et al., 2012; Xu and Zhao, 2012; Wang et al., 2013b; Jia and Zhao, 2013; Wang et al., 2014). Recently, CSLMs become popular because they can obtain more accurate probability estimation. As baseline systems, our approach proposed in (Wang et al., 2013a) only re-writes the probabilities from CSLM into the BNLM, so it can only conduct a convert LM with the same size as the original one. The main difference between our proposed method in this paper and our previous approach is that n-grams outside the corpus are generated firstly and the probabilities using CSLM are calculated by using the same method as our previous approach."
D14-1023,C12-3067,1,0.224044,"re V0 is the short-list,∑Pc (·) is the probability calculated by CSLM, w∈V0 Pc (w|hi ) is the summary of probabilities of the neuron for all the words in the short-list, Pb (·) is the probability calculated by the BNLM, and Ps (hi ) = ∑ Pb (v|hi ). (2) v∈V0 We may regard that CSLM redistributes the probability mass of all words in the short-list, which is calculated by using the n-gram LM. Existing CSLM Converting Methods 2.2 Existing Converting Methods Traditional Backoff N -gram LMs (BNLMs) have been widely used in many NLP tasks (Zhang and Zhao, 2013; Jia and Zhao, 2014; Zhao et al., 2013; Zhang et al., 2012; Xu and Zhao, 2012; Wang et al., 2013b; Jia and Zhao, 2013; Wang et al., 2014). Recently, CSLMs become popular because they can obtain more accurate probability estimation. As baseline systems, our approach proposed in (Wang et al., 2013a) only re-writes the probabilities from CSLM into the BNLM, so it can only conduct a convert LM with the same size as the original one. The main difference between our proposed method in this paper and our previous approach is that n-grams outside the corpus are generated firstly and the probabilities using CSLM are calculated by using the same method as our"
D14-1023,D10-1076,0,0.35607,"{mutiyama, eiichiro.sumita}@nict.go.jp Abstract 2007). In addition, it is very difficult or even impossible to collect an extra large corpus for some special domains such as the TED corpus (Cettolo et al., 2012) or for some rare languages. Therefore, to improve the performance of LMs, without assistance of extra corpus, is one of important research topics in SMT. Recently, Continues Space Language Model (CSLM), especially Neural Network based Language Model (NNLM) (Bengio et al., 2003; Schwenk, 2007; Mikolov et al., 2010; Le et al., 2011), is being actively used in SMT (Schwenk et al., 2006; Son et al., 2010; Schwenk, 2010; Schwenk et al., 2012; Son et al., 2012; Niehues and Waibel, 2012). One of the main advantages of CSLM is that it can more accurately predict the probabilities of the n-grams, which are not in the training corpus. However, in practice, CSLMs have not been widely used in the current SMT systems, due to their too high computational cost. Vaswani and colleagues (2013) propose a method for reducing the training cost of CSLM and apply it to SMT decoder. However, they do not show their improvement for decoding speed, and their method is still slower than the n-gram LM. There are seve"
D14-1023,N12-1005,0,0.0314071,"In addition, it is very difficult or even impossible to collect an extra large corpus for some special domains such as the TED corpus (Cettolo et al., 2012) or for some rare languages. Therefore, to improve the performance of LMs, without assistance of extra corpus, is one of important research topics in SMT. Recently, Continues Space Language Model (CSLM), especially Neural Network based Language Model (NNLM) (Bengio et al., 2003; Schwenk, 2007; Mikolov et al., 2010; Le et al., 2011), is being actively used in SMT (Schwenk et al., 2006; Son et al., 2010; Schwenk, 2010; Schwenk et al., 2012; Son et al., 2012; Niehues and Waibel, 2012). One of the main advantages of CSLM is that it can more accurately predict the probabilities of the n-grams, which are not in the training corpus. However, in practice, CSLMs have not been widely used in the current SMT systems, due to their too high computational cost. Vaswani and colleagues (2013) propose a method for reducing the training cost of CSLM and apply it to SMT decoder. However, they do not show their improvement for decoding speed, and their method is still slower than the n-gram LM. There are several other methods for attempting to implement neural ne"
D16-1246,P13-2013,0,0.0609801,"tion of China (No. 61170114, No. 61672343 and No. 61272248), National Basic Research Program of China (No. 2013CB329401), Major Basic Research Program of Shanghai Science and Technology Committee (No. 15JC1400103), Art and Science Interdisciplinary Funds of Shanghai Jiao Tong University (No. 14JCRZ04), and Key Project of National Society Science Foundation of China (No. 15-ZDA041). Most previous works on PDTB implicit relation recognition only focus on one-versus-others binary classification problems of the top level four classes (Pitler et al., 2009; Zhou et al., 2010; Park and Cardie, 2012; Biran and McKeown, 2013; Rutherford and Xue, 2014; Braud and Denis, 2015). Traditional classification methods directly rely on feature engineering, based on bag-of-words, production rules, and some linguistically-informed features (Zhou et al., 2010; Rutherford and Xue, 2014). However, discourse relations root in semantics, which may be hard to recover from surface level feature, thus these methods did not report satisfactory performance. Recently, neural network (NN) models have shown competitive or even better results than traditional linear models with handcrafted sparse features (Wang et al., 2016b; Zhang et al."
D16-1246,D15-1262,0,0.184582,"1272248), National Basic Research Program of China (No. 2013CB329401), Major Basic Research Program of Shanghai Science and Technology Committee (No. 15JC1400103), Art and Science Interdisciplinary Funds of Shanghai Jiao Tong University (No. 14JCRZ04), and Key Project of National Society Science Foundation of China (No. 15-ZDA041). Most previous works on PDTB implicit relation recognition only focus on one-versus-others binary classification problems of the top level four classes (Pitler et al., 2009; Zhou et al., 2010; Park and Cardie, 2012; Biran and McKeown, 2013; Rutherford and Xue, 2014; Braud and Denis, 2015). Traditional classification methods directly rely on feature engineering, based on bag-of-words, production rules, and some linguistically-informed features (Zhou et al., 2010; Rutherford and Xue, 2014). However, discourse relations root in semantics, which may be hard to recover from surface level feature, thus these methods did not report satisfactory performance. Recently, neural network (NN) models have shown competitive or even better results than traditional linear models with handcrafted sparse features (Wang et al., 2016b; Zhang et al., 2016a; Jia and Zhao, 2014). They have been prove"
D16-1246,P16-1039,1,0.827444,"Missing"
D16-1246,K15-2005,1,0.834986,"ification Lianhui Qin1,2 , Zhisong Zhang1,2 , Hai Zhao1,2,∗ Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, 200240, China 2 Key Laboratory of Shanghai Education Commission for Intelligent Interaction and Cognitive Engineering, Shanghai Jiao Tong University, Shanghai, 200240, China {qinlianhui, zzs2011}@sjtu.edu.cn,zhaohai@cs.sjtu.edu.cn 1 Abstract Yoshida et al., 2014). Discourse parsing is also the shared task of CoNLL 2015 and 2016 (Xue et al., 2015; Xue et al., 2016), and many previous works previous on this task (Qin et al., 2016b; Li et al., 2016; Chen et al., 2015; Wang and Lan, 2016). In a discourse parser, implicit relation recognition has been the bottleneck due to lack of explicit connectives (like “because” or “and”) that can be strong indicators for the senses between adjacent clauses (Qin et al., 2016b; Pitler et al., 2009; Lin et al., 2014). This work therefore focuses on implicit relation recognition that infers the senses of the discourse relations within adjacent sentence pairs. Discourse parsing is considered as one of the most challenging natural language processing (NLP) tasks. Implicit discourse relation classification is the bottleneck"
D16-1246,P16-1163,0,0.671911,"Missing"
D16-1246,P14-1092,0,0.0250113,"twork model to solve the classification problem in which a convolutional neural network (CNN) is utilized for sentence modeling and a collaborative gated neural network (CGNN) is proposed for feature transformation. Our evaluation and comparisons show that the proposed model outperforms previous state-of-the-art systems. 1 Introduction As a fundamental task in natural language processing (NLP), discourse parsing entails the discovery of the latent relational structure in multi-sentence level analysis. It is also central to many practical tasks such as question answering (Liakata et al., 2013; Jansen et al., 2014), machine translation (Meyer and Popescu-Belis, 2012; Meyer and Webber, 2013) and automatic summarization (Murray et al., 2006; ∗ Corresponding author. This paper was partially supported by Cai Yuanpei Program (CSC No. 201304490199 and No. 201304490171), National Natural Science Foundation of China (No. 61170114, No. 61672343 and No. 61272248), National Basic Research Program of China (No. 2013CB329401), Major Basic Research Program of Shanghai Science and Technology Committee (No. 15JC1400103), Art and Science Interdisciplinary Funds of Shanghai Jiao Tong University (No. 14JCRZ04), and Key Pr"
D16-1246,Q15-1024,0,0.0805524,"eural network (NN) models have shown competitive or even better results than traditional linear models with handcrafted sparse features (Wang et al., 2016b; Zhang et al., 2016a; Jia and Zhao, 2014). They have been proved to be effective for many tasks (Qin et al., 2016a; Wang et al., 2016a; Zhang et al., 2016b; Wang et al., 2015; Wang et al., 2014; Cai and 2263 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2263–2270, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics Zhao, 2016), also including discourse parsing. Ji and Eisenstein (2015) adopt recursive neural network and incorporate with entity-augmented distributed semantics. Zhang et al. (2015) explore a shallow convolutional neural network and achieve competitive performance. Although simple neural network has been shown effective, the result has not been quite satisfactory which suggests that there is still space for improving. The concerned task could be straightforwardly formalized as a sentence-pair classification problem, which needs inferring senses solely based on the two arguments without cues of connectives. Two problems should be carefully handled in this task:"
D16-1246,P14-1142,1,0.766725,"ord and Xue, 2014; Braud and Denis, 2015). Traditional classification methods directly rely on feature engineering, based on bag-of-words, production rules, and some linguistically-informed features (Zhou et al., 2010; Rutherford and Xue, 2014). However, discourse relations root in semantics, which may be hard to recover from surface level feature, thus these methods did not report satisfactory performance. Recently, neural network (NN) models have shown competitive or even better results than traditional linear models with handcrafted sparse features (Wang et al., 2016b; Zhang et al., 2016a; Jia and Zhao, 2014). They have been proved to be effective for many tasks (Qin et al., 2016a; Wang et al., 2016a; Zhang et al., 2016b; Wang et al., 2015; Wang et al., 2014; Cai and 2263 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2263–2270, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics Zhao, 2016), also including discourse parsing. Ji and Eisenstein (2015) adopt recursive neural network and incorporate with entity-augmented distributed semantics. Zhang et al. (2015) explore a shallow convolutional neural network and achieve c"
D16-1246,P14-1062,0,0.00339599,"ple neural network has been shown effective, the result has not been quite satisfactory which suggests that there is still space for improving. The concerned task could be straightforwardly formalized as a sentence-pair classification problem, which needs inferring senses solely based on the two arguments without cues of connectives. Two problems should be carefully handled in this task: how to model sentences and how to capture the interactions between the two arguments. The former could be addressed by Convolutional Neural Network (CNN) which has been proved effective for sentence modeling (Kalchbrenner et al., 2014; Kim, 2014), while the latter is the key problem, which might need deep semantic analysis for the interaction of two arguments. To solve the latter problem, we propose collaborative gated neural network (CGNN) which is partially inspired by Highway Network whose gate mechanism achieves success (Srivastava et al., 2015). Our method will be evaluated on the benchmark dataset against state-of-the-art methods. The rest of the paper is organized as follows: Section 2 briefly describes our model, introducing the stacking architecture of CNN and CGNN, Section 3 shows the experiments and analysis, an"
D16-1246,D14-1181,0,0.00487045,"shown effective, the result has not been quite satisfactory which suggests that there is still space for improving. The concerned task could be straightforwardly formalized as a sentence-pair classification problem, which needs inferring senses solely based on the two arguments without cues of connectives. Two problems should be carefully handled in this task: how to model sentences and how to capture the interactions between the two arguments. The former could be addressed by Convolutional Neural Network (CNN) which has been proved effective for sentence modeling (Kalchbrenner et al., 2014; Kim, 2014), while the latter is the key problem, which might need deep semantic analysis for the interaction of two arguments. To solve the latter problem, we propose collaborative gated neural network (CGNN) which is partially inspired by Highway Network whose gate mechanism achieves success (Srivastava et al., 2015). Our method will be evaluated on the benchmark dataset against state-of-the-art methods. The rest of the paper is organized as follows: Section 2 briefly describes our model, introducing the stacking architecture of CNN and CGNN, Section 3 shows the experiments and analysis, and Section 4"
D16-1246,K16-2008,1,0.830753,"se Relation Classification Lianhui Qin1,2 , Zhisong Zhang1,2 , Hai Zhao1,2,∗ Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, 200240, China 2 Key Laboratory of Shanghai Education Commission for Intelligent Interaction and Cognitive Engineering, Shanghai Jiao Tong University, Shanghai, 200240, China {qinlianhui, zzs2011}@sjtu.edu.cn,zhaohai@cs.sjtu.edu.cn 1 Abstract Yoshida et al., 2014). Discourse parsing is also the shared task of CoNLL 2015 and 2016 (Xue et al., 2015; Xue et al., 2016), and many previous works previous on this task (Qin et al., 2016b; Li et al., 2016; Chen et al., 2015; Wang and Lan, 2016). In a discourse parser, implicit relation recognition has been the bottleneck due to lack of explicit connectives (like “because” or “and”) that can be strong indicators for the senses between adjacent clauses (Qin et al., 2016b; Pitler et al., 2009; Lin et al., 2014). This work therefore focuses on implicit relation recognition that infers the senses of the discourse relations within adjacent sentence pairs. Discourse parsing is considered as one of the most challenging natural language processing (NLP) tasks. Implicit discourse relation classification"
D16-1246,D13-1070,0,0.0681088,"Missing"
D16-1246,W12-0117,0,0.0119952,"blem in which a convolutional neural network (CNN) is utilized for sentence modeling and a collaborative gated neural network (CGNN) is proposed for feature transformation. Our evaluation and comparisons show that the proposed model outperforms previous state-of-the-art systems. 1 Introduction As a fundamental task in natural language processing (NLP), discourse parsing entails the discovery of the latent relational structure in multi-sentence level analysis. It is also central to many practical tasks such as question answering (Liakata et al., 2013; Jansen et al., 2014), machine translation (Meyer and Popescu-Belis, 2012; Meyer and Webber, 2013) and automatic summarization (Murray et al., 2006; ∗ Corresponding author. This paper was partially supported by Cai Yuanpei Program (CSC No. 201304490199 and No. 201304490171), National Natural Science Foundation of China (No. 61170114, No. 61672343 and No. 61272248), National Basic Research Program of China (No. 2013CB329401), Major Basic Research Program of Shanghai Science and Technology Committee (No. 15JC1400103), Art and Science Interdisciplinary Funds of Shanghai Jiao Tong University (No. 14JCRZ04), and Key Project of National Society Science Foundation of Chin"
D16-1246,W13-3303,0,0.0230922,"eural network (CNN) is utilized for sentence modeling and a collaborative gated neural network (CGNN) is proposed for feature transformation. Our evaluation and comparisons show that the proposed model outperforms previous state-of-the-art systems. 1 Introduction As a fundamental task in natural language processing (NLP), discourse parsing entails the discovery of the latent relational structure in multi-sentence level analysis. It is also central to many practical tasks such as question answering (Liakata et al., 2013; Jansen et al., 2014), machine translation (Meyer and Popescu-Belis, 2012; Meyer and Webber, 2013) and automatic summarization (Murray et al., 2006; ∗ Corresponding author. This paper was partially supported by Cai Yuanpei Program (CSC No. 201304490199 and No. 201304490171), National Natural Science Foundation of China (No. 61170114, No. 61672343 and No. 61272248), National Basic Research Program of China (No. 2013CB329401), Major Basic Research Program of Shanghai Science and Technology Committee (No. 15JC1400103), Art and Science Interdisciplinary Funds of Shanghai Jiao Tong University (No. 14JCRZ04), and Key Project of National Society Science Foundation of China (No. 15-ZDA041). Most p"
D16-1246,N06-1047,0,0.0248579,"and a collaborative gated neural network (CGNN) is proposed for feature transformation. Our evaluation and comparisons show that the proposed model outperforms previous state-of-the-art systems. 1 Introduction As a fundamental task in natural language processing (NLP), discourse parsing entails the discovery of the latent relational structure in multi-sentence level analysis. It is also central to many practical tasks such as question answering (Liakata et al., 2013; Jansen et al., 2014), machine translation (Meyer and Popescu-Belis, 2012; Meyer and Webber, 2013) and automatic summarization (Murray et al., 2006; ∗ Corresponding author. This paper was partially supported by Cai Yuanpei Program (CSC No. 201304490199 and No. 201304490171), National Natural Science Foundation of China (No. 61170114, No. 61672343 and No. 61272248), National Basic Research Program of China (No. 2013CB329401), Major Basic Research Program of Shanghai Science and Technology Committee (No. 15JC1400103), Art and Science Interdisciplinary Funds of Shanghai Jiao Tong University (No. 14JCRZ04), and Key Project of National Society Science Foundation of China (No. 15-ZDA041). Most previous works on PDTB implicit relation recogniti"
D16-1246,W12-1614,0,0.0385696,"Natural Science Foundation of China (No. 61170114, No. 61672343 and No. 61272248), National Basic Research Program of China (No. 2013CB329401), Major Basic Research Program of Shanghai Science and Technology Committee (No. 15JC1400103), Art and Science Interdisciplinary Funds of Shanghai Jiao Tong University (No. 14JCRZ04), and Key Project of National Society Science Foundation of China (No. 15-ZDA041). Most previous works on PDTB implicit relation recognition only focus on one-versus-others binary classification problems of the top level four classes (Pitler et al., 2009; Zhou et al., 2010; Park and Cardie, 2012; Biran and McKeown, 2013; Rutherford and Xue, 2014; Braud and Denis, 2015). Traditional classification methods directly rely on feature engineering, based on bag-of-words, production rules, and some linguistically-informed features (Zhou et al., 2010; Rutherford and Xue, 2014). However, discourse relations root in semantics, which may be hard to recover from surface level feature, thus these methods did not report satisfactory performance. Recently, neural network (NN) models have shown competitive or even better results than traditional linear models with handcrafted sparse features (Wang et"
D16-1246,P09-1077,0,0.0641879,"hanghai Jiao Tong University, Shanghai, 200240, China {qinlianhui, zzs2011}@sjtu.edu.cn,zhaohai@cs.sjtu.edu.cn 1 Abstract Yoshida et al., 2014). Discourse parsing is also the shared task of CoNLL 2015 and 2016 (Xue et al., 2015; Xue et al., 2016), and many previous works previous on this task (Qin et al., 2016b; Li et al., 2016; Chen et al., 2015; Wang and Lan, 2016). In a discourse parser, implicit relation recognition has been the bottleneck due to lack of explicit connectives (like “because” or “and”) that can be strong indicators for the senses between adjacent clauses (Qin et al., 2016b; Pitler et al., 2009; Lin et al., 2014). This work therefore focuses on implicit relation recognition that infers the senses of the discourse relations within adjacent sentence pairs. Discourse parsing is considered as one of the most challenging natural language processing (NLP) tasks. Implicit discourse relation classification is the bottleneck for discourse parsing. Without the guide of explicit discourse connectives, the relation of sentence pairs are very hard to be inferred. This paper proposes a stacking neural network model to solve the classification problem in which a convolutional neural network (CNN)"
D16-1246,prasad-etal-2008-penn,0,0.0338221,"because they are only influenced by 2265 Output and Training After the transformation of the CGNN unit, the transformed vector h will be sent to a conventional softmax for classification. The training object J will be the cross-entropy error E with L2 regularization: E(ˆ y , y) = − l X j yj × log(P r(ˆ yj )) 3.2 m J(θ) = 1 X λ E(ˆ y (k) , y (k) ) + kθk2 m 2 k where yj is the gold label and yˆj is the predicted one. We adopt the diagonal variant of AdaGrad (Duchi et al., 2011) for the optimization process. 3 Experiments 3.1 Setting As for the benchmark dataset, Penn Discourse Treebank (PDTB) (Prasad et al., 2008) corpus1 is used for evaluation. In the PDTB, each discourse relation is annotated between two argument spans. To be consistent with the setups of prior works, we formulate the implicit relation classification task as four one-versus-other binary classification problems only using the four top level classes: C OM PARISON (C OMP.), C ONTINGENCY (C ONT.), E X PANSION (E XP.) and T EMPORAL (T EMP.). While different works include different relations of varying specificities, all of them include these four core relations (Pitler et al., 2009). Following dataset splitting convention of the previous"
D16-1246,C16-1180,1,0.896595,"or Implicit Discourse Relation Classification Lianhui Qin1,2 , Zhisong Zhang1,2 , Hai Zhao1,2,∗ Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, 200240, China 2 Key Laboratory of Shanghai Education Commission for Intelligent Interaction and Cognitive Engineering, Shanghai Jiao Tong University, Shanghai, 200240, China {qinlianhui, zzs2011}@sjtu.edu.cn,zhaohai@cs.sjtu.edu.cn 1 Abstract Yoshida et al., 2014). Discourse parsing is also the shared task of CoNLL 2015 and 2016 (Xue et al., 2015; Xue et al., 2016), and many previous works previous on this task (Qin et al., 2016b; Li et al., 2016; Chen et al., 2015; Wang and Lan, 2016). In a discourse parser, implicit relation recognition has been the bottleneck due to lack of explicit connectives (like “because” or “and”) that can be strong indicators for the senses between adjacent clauses (Qin et al., 2016b; Pitler et al., 2009; Lin et al., 2014). This work therefore focuses on implicit relation recognition that infers the senses of the discourse relations within adjacent sentence pairs. Discourse parsing is considered as one of the most challenging natural language processing (NLP) tasks. Implicit discourse relat"
D16-1246,K16-2010,1,0.832151,"or Implicit Discourse Relation Classification Lianhui Qin1,2 , Zhisong Zhang1,2 , Hai Zhao1,2,∗ Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, 200240, China 2 Key Laboratory of Shanghai Education Commission for Intelligent Interaction and Cognitive Engineering, Shanghai Jiao Tong University, Shanghai, 200240, China {qinlianhui, zzs2011}@sjtu.edu.cn,zhaohai@cs.sjtu.edu.cn 1 Abstract Yoshida et al., 2014). Discourse parsing is also the shared task of CoNLL 2015 and 2016 (Xue et al., 2015; Xue et al., 2016), and many previous works previous on this task (Qin et al., 2016b; Li et al., 2016; Chen et al., 2015; Wang and Lan, 2016). In a discourse parser, implicit relation recognition has been the bottleneck due to lack of explicit connectives (like “because” or “and”) that can be strong indicators for the senses between adjacent clauses (Qin et al., 2016b; Pitler et al., 2009; Lin et al., 2014). This work therefore focuses on implicit relation recognition that infers the senses of the discourse relations within adjacent sentence pairs. Discourse parsing is considered as one of the most challenging natural language processing (NLP) tasks. Implicit discourse relat"
D16-1246,E14-1068,0,0.0791638,"14, No. 61672343 and No. 61272248), National Basic Research Program of China (No. 2013CB329401), Major Basic Research Program of Shanghai Science and Technology Committee (No. 15JC1400103), Art and Science Interdisciplinary Funds of Shanghai Jiao Tong University (No. 14JCRZ04), and Key Project of National Society Science Foundation of China (No. 15-ZDA041). Most previous works on PDTB implicit relation recognition only focus on one-versus-others binary classification problems of the top level four classes (Pitler et al., 2009; Zhou et al., 2010; Park and Cardie, 2012; Biran and McKeown, 2013; Rutherford and Xue, 2014; Braud and Denis, 2015). Traditional classification methods directly rely on feature engineering, based on bag-of-words, production rules, and some linguistically-informed features (Zhou et al., 2010; Rutherford and Xue, 2014). However, discourse relations root in semantics, which may be hard to recover from surface level feature, thus these methods did not report satisfactory performance. Recently, neural network (NN) models have shown competitive or even better results than traditional linear models with handcrafted sparse features (Wang et al., 2016b; Zhang et al., 2016a; Jia and Zhao, 201"
D16-1246,K16-2004,0,0.0208973,"in1,2 , Zhisong Zhang1,2 , Hai Zhao1,2,∗ Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, 200240, China 2 Key Laboratory of Shanghai Education Commission for Intelligent Interaction and Cognitive Engineering, Shanghai Jiao Tong University, Shanghai, 200240, China {qinlianhui, zzs2011}@sjtu.edu.cn,zhaohai@cs.sjtu.edu.cn 1 Abstract Yoshida et al., 2014). Discourse parsing is also the shared task of CoNLL 2015 and 2016 (Xue et al., 2015; Xue et al., 2016), and many previous works previous on this task (Qin et al., 2016b; Li et al., 2016; Chen et al., 2015; Wang and Lan, 2016). In a discourse parser, implicit relation recognition has been the bottleneck due to lack of explicit connectives (like “because” or “and”) that can be strong indicators for the senses between adjacent clauses (Qin et al., 2016b; Pitler et al., 2009; Lin et al., 2014). This work therefore focuses on implicit relation recognition that infers the senses of the discourse relations within adjacent sentence pairs. Discourse parsing is considered as one of the most challenging natural language processing (NLP) tasks. Implicit discourse relation classification is the bottleneck for discourse parsing"
D16-1246,D14-1023,1,0.844235,"s, and some linguistically-informed features (Zhou et al., 2010; Rutherford and Xue, 2014). However, discourse relations root in semantics, which may be hard to recover from surface level feature, thus these methods did not report satisfactory performance. Recently, neural network (NN) models have shown competitive or even better results than traditional linear models with handcrafted sparse features (Wang et al., 2016b; Zhang et al., 2016a; Jia and Zhao, 2014). They have been proved to be effective for many tasks (Qin et al., 2016a; Wang et al., 2016a; Zhang et al., 2016b; Wang et al., 2015; Wang et al., 2014; Cai and 2263 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2263–2270, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics Zhao, 2016), also including discourse parsing. Ji and Eisenstein (2015) adopt recursive neural network and incorporate with entity-augmented distributed semantics. Zhang et al. (2015) explore a shallow convolutional neural network and achieve competitive performance. Although simple neural network has been shown effective, the result has not been quite satisfactory which suggests that there is"
D16-1246,N16-1064,1,0.687689,"e, 2012; Biran and McKeown, 2013; Rutherford and Xue, 2014; Braud and Denis, 2015). Traditional classification methods directly rely on feature engineering, based on bag-of-words, production rules, and some linguistically-informed features (Zhou et al., 2010; Rutherford and Xue, 2014). However, discourse relations root in semantics, which may be hard to recover from surface level feature, thus these methods did not report satisfactory performance. Recently, neural network (NN) models have shown competitive or even better results than traditional linear models with handcrafted sparse features (Wang et al., 2016b; Zhang et al., 2016a; Jia and Zhao, 2014). They have been proved to be effective for many tasks (Qin et al., 2016a; Wang et al., 2016a; Zhang et al., 2016b; Wang et al., 2015; Wang et al., 2014; Cai and 2263 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2263–2270, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics Zhao, 2016), also including discourse parsing. Ji and Eisenstein (2015) adopt recursive neural network and incorporate with entity-augmented distributed semantics. Zhang et al. (2015) explore a shallow"
D16-1246,D14-1196,0,0.0323429,"Missing"
D16-1246,D15-1266,0,0.169575,"ed sparse features (Wang et al., 2016b; Zhang et al., 2016a; Jia and Zhao, 2014). They have been proved to be effective for many tasks (Qin et al., 2016a; Wang et al., 2016a; Zhang et al., 2016b; Wang et al., 2015; Wang et al., 2014; Cai and 2263 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2263–2270, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics Zhao, 2016), also including discourse parsing. Ji and Eisenstein (2015) adopt recursive neural network and incorporate with entity-augmented distributed semantics. Zhang et al. (2015) explore a shallow convolutional neural network and achieve competitive performance. Although simple neural network has been shown effective, the result has not been quite satisfactory which suggests that there is still space for improving. The concerned task could be straightforwardly formalized as a sentence-pair classification problem, which needs inferring senses solely based on the two arguments without cues of connectives. Two problems should be carefully handled in this task: how to model sentences and how to capture the interactions between the two arguments. The former could be addres"
D16-1246,P16-1131,1,0.258524,"Missing"
D16-1246,C10-2172,0,0.0327639,"04490171), National Natural Science Foundation of China (No. 61170114, No. 61672343 and No. 61272248), National Basic Research Program of China (No. 2013CB329401), Major Basic Research Program of Shanghai Science and Technology Committee (No. 15JC1400103), Art and Science Interdisciplinary Funds of Shanghai Jiao Tong University (No. 14JCRZ04), and Key Project of National Society Science Foundation of China (No. 15-ZDA041). Most previous works on PDTB implicit relation recognition only focus on one-versus-others binary classification problems of the top level four classes (Pitler et al., 2009; Zhou et al., 2010; Park and Cardie, 2012; Biran and McKeown, 2013; Rutherford and Xue, 2014; Braud and Denis, 2015). Traditional classification methods directly rely on feature engineering, based on bag-of-words, production rules, and some linguistically-informed features (Zhou et al., 2010; Rutherford and Xue, 2014). However, discourse relations root in semantics, which may be hard to recover from surface level feature, thus these methods did not report satisfactory performance. Recently, neural network (NN) models have shown competitive or even better results than traditional linear models with handcrafted s"
D16-1246,K15-2001,0,\N,Missing
D18-1262,D13-1160,0,0.0320649,"ational Society Science Foundation of China (No. 15ZDA041), The Art and Science Interdisciplinary Funds of Shanghai Jiao Tong University (No. 14JCRZ04) and the joint research project with Youtu Lab of Tencent. into multiple subtasks in pipeline framework, consisting of predicate identification (makes), predicate disambiguation (make.02), argument identification (e.g., Someone) and argument classification (Someone is A0 for the predicate makes). SRL is beneficial to a wide range of natural language processing (NLP) tasks, including machine translation (Shi et al., 2016) and question answering (Berant et al., 2013; Yih et al., 2016). Most traditional SRL methods rely heavily on feature templates that struggle to capture sufficient discriminative information, while neural models are capable of extracting features automatically. In particular, recent works (Zhou and Xu, 2015; He et al., 2017; Marcheggiani et al., 2017) propose syntax-agnostic models for SRL and achieve favorable results, which seems to be in conflict with the belief that syntactic information is an absolutely necessary prerequisite for high-performance SRL (Gildea and Palmer, 2002). Despite the success of these models, the main reasons f"
D18-1262,C10-3009,0,0.0789533,"Missing"
D18-1262,W09-1206,0,0.277259,"Missing"
D18-1262,P16-1039,1,0.904877,"Missing"
D18-1262,C18-1233,1,0.856068,"s work (Marcheggiani and Titov, 2017). Specifically, each word embedding representation ei of input sentence is the concatenation of several features, a randomly initialized word embedding eri , a pretrained word embedding epi , a randomly initialized lemma embedding eli , a randomly initialized POS tag embedding epos i , and a predicate-specific feature efi , which is a binary flag set 0 or 1 indicating whether the current word is the given predicate. To further enhance the word representation, we leverage an external embedding ELMo (Embeddings from Language Models) proposed by Peters et al. (2018). ELMo is obtained by deep bidirectional language model that takes characters as input, enriching subword information and contextual information, which has expressive representation power. Eventually, the resulting word representation is concatenated as ei = f [eri , epi , eli , epos i , ei , ELMoi ]. BiLSTM encoder We use bi-directional Long Short-term Memory neural network (BiLSTM) (Hochreiter and Schmidhuber, 1997) as the sentence encoder to model sequential inputs. Given an input sequence (e1 , . . . , en ), the BiLSTM processes these embedding vectors sequentially from both directions to"
D18-1262,D15-1112,0,0.705882,"units. Specifically, the main difference between TreeLSTM unit and the standard one is that the memory cell updating and the calculation of gating vectors are depended on multiple child units. A TreeLSTM unit can be connected to arbitrary number of child units and assigns a single forget gate for each child unit. This provides Tree-LSTM the flexibility to incorporate or drop the information from each child unit. Given a syntactic tree, the Tree-LSTM transformation is defined on node nk and its children set C(k), which can be formulated as follows (Tai 2404 System Local model Lei et al. (2015) FitzGerald et al. (2015) Roth and Lapata (2016) Marcheggiani et al. (2017) Marcheggiani and Titov (2017) He et al. (2018) Cai et al. (2018) Ours (Syn-GCN) Ours (SA-LSTM) Ours (Tree-LSTM) Global model Bj¨orkelund et al. (2010) FitzGerald et al. (2015) Roth and Lapata (2016) Ensemble model FitzGerald et al. (2015) Roth and Lapata (2016) Marcheggiani and Titov (2017) et al., 2015): ˜k = h X hk , (1) j∈C(k) ˜ k + b(i) ), ig = σ(W (i) xk + U (i) h fgk,j = σ(W (f ) xk + U (f ) hj + b(f ) ), ˜ k + b(o) ), og = σ(W (o) xk + U (o) h (2) ˜ k + b(u) ), u = tanh(W (u) xk + U (u) h X fgk,j cj , ck = ig u + j∈C(k) hk = og tanh(ck"
D18-1262,S15-1033,0,0.038785,"Related Work Semantic role labeling was pioneered by Gildea and Jurafsky (2002), also known as shallow semantic parsing. In early works of SRL, considerable attention has been paid to feature engineering (Pradhan et al., 2005; Zhao and Kit, 2008; Zhao et al., 2009a,b,c; Li et al., 2009; Bj¨orkelund et al., 2009; Zhao et al., 2013). Along with the the impressive success of deep neural networks (Zhang et al., 2016; Cai and Zhao, 2016; Qin et al., 2016; Wang et al., 2016b,a; Zhang et al., 2018; Li et al., 2018; Huang et al., 2018), a series of neural SRL systems have been proposed. For instance, Foland and Martin (2015) presented a semantic role labeler using convolutional and time-domain neural networks. FitzGerald et al. (2015) exploited neural network to jointly embed arguments and semantic roles, akin to the work (Lei et al., 2015), which induced a compact feature representation applying tensor-based approach. Recently, people have attempted to build endto-end systems for span SRL without syntactic input (Zhou and Xu, 2015; He et al., 2017; Tan et al., 2018). Similarly, Marcheggiani et al. (2017) also proposed a syntax-agnostic model for dependency SRL and obtained favorable results. Despite the success"
D18-1262,J02-3001,0,0.76716,"Missing"
D18-1262,P02-1031,0,0.0984173,"achine translation (Shi et al., 2016) and question answering (Berant et al., 2013; Yih et al., 2016). Most traditional SRL methods rely heavily on feature templates that struggle to capture sufficient discriminative information, while neural models are capable of extracting features automatically. In particular, recent works (Zhou and Xu, 2015; He et al., 2017; Marcheggiani et al., 2017) propose syntax-agnostic models for SRL and achieve favorable results, which seems to be in conflict with the belief that syntactic information is an absolutely necessary prerequisite for high-performance SRL (Gildea and Palmer, 2002). Despite the success of these models, the main reasons for putting syntax aside are two-fold. First, it is still challenging to effectively incorporate syntactic information into neural SRL models, due to the sophisticated tree structure of syntactic relation. Second, the syntactic parsers are unreliable on account of the risk of erroneous syntactic input, which may lead to error propagation and an unsatisfactory SRL performance. However, syntactic information is considered closely related to semantic relation and plays an essential role in SRL task (Punyakanok et al., 2008). Recently, Marche"
D18-1262,P17-1044,0,0.2754,"ification (makes), predicate disambiguation (make.02), argument identification (e.g., Someone) and argument classification (Someone is A0 for the predicate makes). SRL is beneficial to a wide range of natural language processing (NLP) tasks, including machine translation (Shi et al., 2016) and question answering (Berant et al., 2013; Yih et al., 2016). Most traditional SRL methods rely heavily on feature templates that struggle to capture sufficient discriminative information, while neural models are capable of extracting features automatically. In particular, recent works (Zhou and Xu, 2015; He et al., 2017; Marcheggiani et al., 2017) propose syntax-agnostic models for SRL and achieve favorable results, which seems to be in conflict with the belief that syntactic information is an absolutely necessary prerequisite for high-performance SRL (Gildea and Palmer, 2002). Despite the success of these models, the main reasons for putting syntax aside are two-fold. First, it is still challenging to effectively incorporate syntactic information into neural SRL models, due to the sophisticated tree structure of syntactic relation. Second, the syntactic parsers are unreliable on account of the risk of erron"
D18-1262,D17-1159,0,0.0668081,"2002). Despite the success of these models, the main reasons for putting syntax aside are two-fold. First, it is still challenging to effectively incorporate syntactic information into neural SRL models, due to the sophisticated tree structure of syntactic relation. Second, the syntactic parsers are unreliable on account of the risk of erroneous syntactic input, which may lead to error propagation and an unsatisfactory SRL performance. However, syntactic information is considered closely related to semantic relation and plays an essential role in SRL task (Punyakanok et al., 2008). Recently, Marcheggiani and Titov (2017) 2401 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2401–2411 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics proposed a syntactic graph convolutional networks (GCNs) based SRL model and further improved the SRL performance with relatively better syntactic parser as input. Since syntax can provide rich structure and information for SRL, we seek to effectively model complex syntactic tree structure for incorporating syntax into neural SRL. In this paper, we present a general framework1 for SRL, wh"
D18-1262,P18-1192,1,0.422579,"Missing"
D18-1262,P82-1020,0,0.843118,"Missing"
D18-1262,D14-1162,0,0.0824892,"= hk . 4 Experiments We evaluate our models performance of syntactic GCN (henceforth Syn-GCN), SA-LSTM and Tree-LSTM on CoNLL-2009 datasets both for English and Chinese with standard training, development and test splits. For predicate disambiguation, we follow previous work (Marcheggiani and Titov, 2017), using the off-the-shelf disambiguator from Roth and Lapata (2016). For syntactic dependency tree, we parse the corpus with Biaffine Parser (Dozat and Manning, 2017). 4.1 Experimental Settings In our experiments, the pre-trained word embeddings for English are 100-dimensional GloVe vectors (Pennington et al., 2014). For Chinese, we P R F1 − − 88.1 88.7 89.1 89.7 89.9 90.3 90.8 90.0 − − 85.3 86.8 86.8 89.3 89.2 89.3 88.6 88.8 86.6 86.7 86.7 87.7 88.0 89.5 89.6 89.8 89.7 89.4 88.6 85.2 86.9 − − 87.3 90.0 85.5 87.7 − − 87.7 90.3 85.7 87.9 90.5 87.7 89.1 Table 1: Results on the English in-domain test set. exploit Wikipedia documents to train the same dimensional Word2Vec embeddings (Mikolov et al., 2013). All other vectors are randomly initialized, the dimension of lemma embeddings is 100, and the dimension of POS tag embedding is 32. In addition, we use 300-dimensional ELMo embedding for English2 . During"
D18-1262,P18-4024,1,0.765756,"y indeed enhance SRL, which is consistent with the conclusion in (He et al., 2017). 5 Related Work Semantic role labeling was pioneered by Gildea and Jurafsky (2002), also known as shallow semantic parsing. In early works of SRL, considerable attention has been paid to feature engineering (Pradhan et al., 2005; Zhao and Kit, 2008; Zhao et al., 2009a,b,c; Li et al., 2009; Bj¨orkelund et al., 2009; Zhao et al., 2013). Along with the the impressive success of deep neural networks (Zhang et al., 2016; Cai and Zhao, 2016; Qin et al., 2016; Wang et al., 2016b,a; Zhang et al., 2018; Li et al., 2018; Huang et al., 2018), a series of neural SRL systems have been proposed. For instance, Foland and Martin (2015) presented a semantic role labeler using convolutional and time-domain neural networks. FitzGerald et al. (2015) exploited neural network to jointly embed arguments and semantic roles, akin to the work (Lei et al., 2015), which induced a compact feature representation applying tensor-based approach. Recently, people have attempted to build endto-end systems for span SRL without syntactic input (Zhou and Xu, 2015; He et al., 2017; Tan et al., 2018). Similarly, Marcheggiani et al. (2017) also proposed a sy"
D18-1262,Q16-1023,0,0.0295115,"89.2 89.8 90.0 R 87.9 89.3 88.6 88.8 88.0 88.8 87.8 F1 88.7 89.8 89.7 89.4 88.6 89.3 88.9 Table 6: Comparison of models with deep encoder and M&T encoder (Marcheggiani and Titov, 2017) on the English test set. Syntactic Input Four types of syntactic inputs are used to explore the role of syntax in our unified framework, (1) the automatically predicted parse provided by CoNLL-2009 shared task, (2) the parsing results of the CoNLL-2009 data by state-of-theart syntactic parser, the Biaffine Parser (used in our previous experiments), (3) corresponding results from another parser, the BIST Parser (Kiperwasser and Goldberg, 2016), which is also adopted by Marcheggiani and Titov (2017), (4) the gold syntax available from the official data set. Evaluation Metric It is worth noting that for SRL task, the standard evaluation metric is the semantic labeled F1 score (Sem-F1 ), and we use the labeled attachment score (LAS) to quantify the quality of syntactic input. In addition, the ratio between labeled F1 score for semantic dependencies and the LAS for syntactic dependencies (Sem-F1 /LAS) proposed by CoNLL2008 shared task3 (Surdeanu et al., 2008), are also given for reference. To a certain extent, the ratio Sem-F1 /LAS cou"
D18-1262,N15-1121,0,0.274848,"Missing"
D18-1262,D09-1133,1,0.800247,"Missing"
D18-1262,N18-1202,0,0.0430818,"lowing previous work (Marcheggiani and Titov, 2017). Specifically, each word embedding representation ei of input sentence is the concatenation of several features, a randomly initialized word embedding eri , a pretrained word embedding epi , a randomly initialized lemma embedding eli , a randomly initialized POS tag embedding epos i , and a predicate-specific feature efi , which is a binary flag set 0 or 1 indicating whether the current word is the given predicate. To further enhance the word representation, we leverage an external embedding ELMo (Embeddings from Language Models) proposed by Peters et al. (2018). ELMo is obtained by deep bidirectional language model that takes characters as input, enriching subword information and contextual information, which has expressive representation power. Eventually, the resulting word representation is concatenated as ei = f [eri , epi , eli , epos i , ei , ELMoi ]. BiLSTM encoder We use bi-directional Long Short-term Memory neural network (BiLSTM) (Hochreiter and Schmidhuber, 1997) as the sentence encoder to model sequential inputs. Given an input sequence (e1 , . . . , en ), the BiLSTM processes these embedding vectors sequentially from both directions to"
D18-1262,P05-1072,0,0.513969,"Missing"
D18-1262,J08-2005,0,0.142582,"-performance SRL (Gildea and Palmer, 2002). Despite the success of these models, the main reasons for putting syntax aside are two-fold. First, it is still challenging to effectively incorporate syntactic information into neural SRL models, due to the sophisticated tree structure of syntactic relation. Second, the syntactic parsers are unreliable on account of the risk of erroneous syntactic input, which may lead to error propagation and an unsatisfactory SRL performance. However, syntactic information is considered closely related to semantic relation and plays an essential role in SRL task (Punyakanok et al., 2008). Recently, Marcheggiani and Titov (2017) 2401 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2401–2411 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics proposed a syntactic graph convolutional networks (GCNs) based SRL model and further improved the SRL performance with relatively better syntactic parser as input. Since syntax can provide rich structure and information for SRL, we seek to effectively model complex syntactic tree structure for incorporating syntax into neural SRL. In this paper, we"
D18-1262,W17-4305,0,0.475622,"ncorporating syntax into neural SRL. In this paper, we present a general framework1 for SRL, which enables us to integrate syntax into SRL in diverse ways. Following Marcheggiani and Titov (2017), we focus on argument labeling and formulate SRL as sequence labeling problem. However, we differ by (1) leveraging enhanced word representation, (2) applying recent advances in recurrent neural networks (RNNs), such as highway connections (Srivastava et al., 2015), (3) using deep encoder with residual connections (He et al., 2016), (4) further extending Syntax Aware Long Short-Term Memory (SA-LSTM) (Qian et al., 2017) for SRL, and (5) introducing the Tree-Structured Long Short-Term Memory (Tree-LSTM) (Tai et al., 2015) to model syntactic information for SRL. In addition, as pointed out by He et al. (2017) for span SRL, the worse syntactic input will hurt performance if the syntactically-driven SRL model trusts syntactic information too much, and high-quality syntax can still make a large impact on SRL, which motivates us to investigate the effect of syntactic quality on dependency SRL. In summary, our major contributions are as follows: • We propose a unified neural framework for dependency SRL to more eff"
D18-1262,C16-1180,1,0.848487,"taking gold syntax as input. It suggests that high-quality syntactic parse may indeed enhance SRL, which is consistent with the conclusion in (He et al., 2017). 5 Related Work Semantic role labeling was pioneered by Gildea and Jurafsky (2002), also known as shallow semantic parsing. In early works of SRL, considerable attention has been paid to feature engineering (Pradhan et al., 2005; Zhao and Kit, 2008; Zhao et al., 2009a,b,c; Li et al., 2009; Bj¨orkelund et al., 2009; Zhao et al., 2013). Along with the the impressive success of deep neural networks (Zhang et al., 2016; Cai and Zhao, 2016; Qin et al., 2016; Wang et al., 2016b,a; Zhang et al., 2018; Li et al., 2018; Huang et al., 2018), a series of neural SRL systems have been proposed. For instance, Foland and Martin (2015) presented a semantic role labeler using convolutional and time-domain neural networks. FitzGerald et al. (2015) exploited neural network to jointly embed arguments and semantic roles, akin to the work (Lei et al., 2015), which induced a compact feature representation applying tensor-based approach. Recently, people have attempted to build endto-end systems for span SRL without syntactic input (Zhou and Xu, 2015; He et al., 2"
D18-1262,C18-1271,1,0.638043,"yntactic parse may indeed enhance SRL, which is consistent with the conclusion in (He et al., 2017). 5 Related Work Semantic role labeling was pioneered by Gildea and Jurafsky (2002), also known as shallow semantic parsing. In early works of SRL, considerable attention has been paid to feature engineering (Pradhan et al., 2005; Zhao and Kit, 2008; Zhao et al., 2009a,b,c; Li et al., 2009; Bj¨orkelund et al., 2009; Zhao et al., 2013). Along with the the impressive success of deep neural networks (Zhang et al., 2016; Cai and Zhao, 2016; Qin et al., 2016; Wang et al., 2016b,a; Zhang et al., 2018; Li et al., 2018; Huang et al., 2018), a series of neural SRL systems have been proposed. For instance, Foland and Martin (2015) presented a semantic role labeler using convolutional and time-domain neural networks. FitzGerald et al. (2015) exploited neural network to jointly embed arguments and semantic roles, akin to the work (Lei et al., 2015), which induced a compact feature representation applying tensor-based approach. Recently, people have attempted to build endto-end systems for span SRL without syntactic input (Zhou and Xu, 2015; He et al., 2017; Tan et al., 2018). Similarly, Marcheggiani et al. (201"
D18-1262,P16-1113,0,0.272085,"main difference between TreeLSTM unit and the standard one is that the memory cell updating and the calculation of gating vectors are depended on multiple child units. A TreeLSTM unit can be connected to arbitrary number of child units and assigns a single forget gate for each child unit. This provides Tree-LSTM the flexibility to incorporate or drop the information from each child unit. Given a syntactic tree, the Tree-LSTM transformation is defined on node nk and its children set C(k), which can be formulated as follows (Tai 2404 System Local model Lei et al. (2015) FitzGerald et al. (2015) Roth and Lapata (2016) Marcheggiani et al. (2017) Marcheggiani and Titov (2017) He et al. (2018) Cai et al. (2018) Ours (Syn-GCN) Ours (SA-LSTM) Ours (Tree-LSTM) Global model Bj¨orkelund et al. (2010) FitzGerald et al. (2015) Roth and Lapata (2016) Ensemble model FitzGerald et al. (2015) Roth and Lapata (2016) Marcheggiani and Titov (2017) et al., 2015): ˜k = h X hk , (1) j∈C(k) ˜ k + b(i) ), ig = σ(W (i) xk + U (i) h fgk,j = σ(W (f ) xk + U (f ) hj + b(f ) ), ˜ k + b(o) ), og = σ(W (o) xk + U (o) h (2) ˜ k + b(u) ), u = tanh(W (u) xk + U (u) h X fgk,j cj , ck = ig u + j∈C(k) hk = og tanh(ck ). where j ∈ C(k), hj i"
D18-1262,K17-1041,0,0.585082,", predicate disambiguation (make.02), argument identification (e.g., Someone) and argument classification (Someone is A0 for the predicate makes). SRL is beneficial to a wide range of natural language processing (NLP) tasks, including machine translation (Shi et al., 2016) and question answering (Berant et al., 2013; Yih et al., 2016). Most traditional SRL methods rely heavily on feature templates that struggle to capture sufficient discriminative information, while neural models are capable of extracting features automatically. In particular, recent works (Zhou and Xu, 2015; He et al., 2017; Marcheggiani et al., 2017) propose syntax-agnostic models for SRL and achieve favorable results, which seems to be in conflict with the belief that syntactic information is an absolutely necessary prerequisite for high-performance SRL (Gildea and Palmer, 2002). Despite the success of these models, the main reasons for putting syntax aside are two-fold. First, it is still challenging to effectively incorporate syntactic information into neural SRL models, due to the sophisticated tree structure of syntactic relation. Second, the syntactic parsers are unreliable on account of the risk of erroneous syntactic input, which"
D18-1262,P16-1212,0,0.0372414,"Missing"
D18-1262,W08-2121,0,0.294179,"Missing"
D18-1262,P15-1150,0,0.179857,"Missing"
D18-1262,C16-1295,1,0.90072,"Missing"
D18-1262,P16-2033,0,0.0414918,"ce Foundation of China (No. 15ZDA041), The Art and Science Interdisciplinary Funds of Shanghai Jiao Tong University (No. 14JCRZ04) and the joint research project with Youtu Lab of Tencent. into multiple subtasks in pipeline framework, consisting of predicate identification (makes), predicate disambiguation (make.02), argument identification (e.g., Someone) and argument classification (Someone is A0 for the predicate makes). SRL is beneficial to a wide range of natural language processing (NLP) tasks, including machine translation (Shi et al., 2016) and question answering (Berant et al., 2013; Yih et al., 2016). Most traditional SRL methods rely heavily on feature templates that struggle to capture sufficient discriminative information, while neural models are capable of extracting features automatically. In particular, recent works (Zhou and Xu, 2015; He et al., 2017; Marcheggiani et al., 2017) propose syntax-agnostic models for SRL and achieve favorable results, which seems to be in conflict with the belief that syntactic information is an absolutely necessary prerequisite for high-performance SRL (Gildea and Palmer, 2002). Despite the success of these models, the main reasons for putting syntax a"
D18-1262,W09-1209,1,0.83556,"Missing"
D18-1262,D09-1004,1,0.943973,"Missing"
D18-1262,W09-1208,1,0.896196,"Missing"
D18-1262,W08-2127,1,0.879914,"Missing"
D18-1262,P15-1109,0,0.230713,"of predicate identification (makes), predicate disambiguation (make.02), argument identification (e.g., Someone) and argument classification (Someone is A0 for the predicate makes). SRL is beneficial to a wide range of natural language processing (NLP) tasks, including machine translation (Shi et al., 2016) and question answering (Berant et al., 2013; Yih et al., 2016). Most traditional SRL methods rely heavily on feature templates that struggle to capture sufficient discriminative information, while neural models are capable of extracting features automatically. In particular, recent works (Zhou and Xu, 2015; He et al., 2017; Marcheggiani et al., 2017) propose syntax-agnostic models for SRL and achieve favorable results, which seems to be in conflict with the belief that syntactic information is an absolutely necessary prerequisite for high-performance SRL (Gildea and Palmer, 2002). Despite the success of these models, the main reasons for putting syntax aside are two-fold. First, it is still challenging to effectively incorporate syntactic information into neural SRL models, due to the sophisticated tree structure of syntactic relation. Second, the syntactic parsers are unreliable on account of"
D18-1262,P15-1112,0,0.0293036,"into a neural SRL model. Qian et al. (2017) proposed SALSTM to model the whole tree structure of dependency relation in an architecture engineering way. Besides, syntax encoding has also successfully promoted other NLP tasks. Tree-LSTM (Tai et al., 2015) is a variant of the standard LSTM that can encode a dependency tree with arbitrary branching factors, which has shown effectiveness on semantic relatedness and the sentiment classification tasks. In this work, we extend the Tree-LSTM with a relation specific gate and employ it to recursively encode the syntactic dependency tree for SRL. RCNN (Zhu et al., 2015) is an extension of the recursive neural network (Socher et al., 2010) which has been popularly used to encode trees with fixed branching factors. The RCNN is able to encode a tree structure with arbitrary number of factors and is useful in a re-ranking model for dependency parsing (Zhu et al., 2015). 2408 In our experiments, we simplify and reformulate the RCNN model. However, the simplified model performs poorly on the development and the test sets. The reason might be that the RCNN model with a single global composition parameter is too simple to cover all types of syntactic relation in a d"
D18-1262,P16-1131,1,0.914273,"Missing"
D18-1262,C18-1153,1,0.786404,"Missing"
D18-1321,P00-1031,0,\N,Missing
D18-1321,I13-1170,1,\N,Missing
D18-1321,P14-1142,1,\N,Missing
D18-1321,W13-4416,1,\N,Missing
D18-1321,P18-1192,1,\N,Missing
D18-1321,P18-2025,1,\N,Missing
D18-1321,P18-4024,1,\N,Missing
D18-1321,C18-1153,1,\N,Missing
D18-1321,C18-1271,1,\N,Missing
D18-1321,Y15-1052,1,\N,Missing
D18-1321,P17-1046,0,\N,Missing
D19-1538,W09-1206,0,0.158309,"Missing"
D19-1538,C18-1233,1,0.846952,"Missing"
D19-1538,K18-2005,0,0.0641464,"Missing"
D19-1538,D15-1112,0,0.0842371,"Missing"
D19-1538,S15-1033,0,0.0136373,"also a promising direction and we leave it for future work. 4 In this work, we use gold syntax rather than other better parse to explore the greatest syntactic contribution, considering the current state-of-the-art syntactic parsers are being upgraded so fast now. 5 Related Work In early work of semantic role labeling, most of researchers were dedicated to feature engineering (Pradhan et al., 2005; Punyakanok et al., 2008; Zhao et al., 2009b, 2013). The first neural SRL model was proposed by Collobert et al. (2011), which used convolutional neural network but their efforts fell short. Later, Foland and Martin (2015) effectively extended their work by using syntactic features as input. Roth and Lapata (2016) introduced syntactic paths to guide neural architectures for dependency SRL. However, putting syntax aside has sparked much research interest since Zhou and Xu (2015) employed deep BiLSTMs for span SRL. A series of neural SRL models without syntactic inputs were proposed. Marcheggiani et al. (2017) applied a simple LSTM model with effective word representation, achieving encouraging results on English, Chinese, Czech and Spanish. Cai et al. (2018) built a full end-to-end SRL model with biaffine attent"
D19-1538,L18-1550,0,0.0307583,"Missing"
D19-1538,P18-1192,1,0.932506,"yntactic input, leading to undesired error propagation. To alleviate this inconvenience, researchers as early as Zhou and Xu (2015) propose neural SRL models without syntactic input. Cai et al. (2018) employ the biaffine attentional mechanism (Dozat and Manning, 2017) for dependency-based SRL. In the meantime, a series of studies (Roth and Lapata, 2016; Marcheggiani and Titov, 2017; Strubell et al., 2018; Li et al., 2018) have introduced syntactic clue in creative ways for further performance improvement, which achieve favorable results. However, applying the k-order syntactic tree pruning of He et al. (2018) to the biaffine SRL model (Cai et al., 2018) does not boost the performance as expected, which indicates that exploiting syntactic clue in state-of-theart SRL models still deserves deep exploration. Besides, most of SRL literature is dedicated to impressive performance gains on English and Chinese, but other multiple languages have received relatively little attention. We even observe that to date the best reported results of some languages (Catalan and Japanese) are still from the initial CoNLL-2009 shared task (Hajiˇc et al., 2009). Therefore, we launch this multilingual SRL study to fill t"
D19-1538,P82-1020,0,0.716336,"Missing"
D19-1538,N15-1121,0,0.0765269,"Missing"
D19-1538,D18-1262,1,0.896264,"translation (Xiong et al., 2012) and question answering (Yih et al., 2016). Almost all of traditional SRL methods relied heavily on syntactic features, which suffered the risk of erroneous syntactic input, leading to undesired error propagation. To alleviate this inconvenience, researchers as early as Zhou and Xu (2015) propose neural SRL models without syntactic input. Cai et al. (2018) employ the biaffine attentional mechanism (Dozat and Manning, 2017) for dependency-based SRL. In the meantime, a series of studies (Roth and Lapata, 2016; Marcheggiani and Titov, 2017; Strubell et al., 2018; Li et al., 2018) have introduced syntactic clue in creative ways for further performance improvement, which achieve favorable results. However, applying the k-order syntactic tree pruning of He et al. (2018) to the biaffine SRL model (Cai et al., 2018) does not boost the performance as expected, which indicates that exploiting syntactic clue in state-of-theart SRL models still deserves deep exploration. Besides, most of SRL literature is dedicated to impressive performance gains on English and Chinese, but other multiple languages have received relatively little attention. We even observe that to date the bes"
D19-1538,K17-1041,0,0.747735,"or biaffine scorer, we employ two 300-dimensional affine transfor2 There were two tracks in the CoNLL-2009 shared task, SRL-only and joint. For the former, all participants did not have to develop their own syntactic parsers and focused on the SRL model development, while for the latter, the participants had to build their own syntactic parser as well. For the sake of focusing the SRL work, in this work, we will take the official syntax provided by CoNLL-2009. 5353 English Model Chinese P R F1 P R F1 Zhao et al. (2009a) Bj¨orkelund et al. (2009) FitzGerald et al. (2015) Roth and Lapata (2016) Marcheggiani et al. (2017) Marcheggiani and Titov (2017) He et al. (2018) (with ELMo) Cai et al. (2018) Li et al. (2018) (with ELMo) Li et al. (2019) (with ELMo) − 88.6 − 90.0 88.7 89.1 89.7 89.9 90.3 89.6 − 85.2 − 85.5 86.8 86.8 89.3 89.2 89.3 91.2 86.2 86.9 87.3 87.7 87.7 88.0 89.5 89.6 89.8 90.4 80.4 82.4 − 83.2 83.4 84.6 84.2 84.7 84.8 − 75.2 75.1 − 75.9 79.1 80.4 81.5 84.0 81.2 − 77.7 78.6 − 79.4 81.2 82.5 82.8 84.3 83.0 − Our baseline + AP + BERT + AP + ELMo + AP + BERT 89.30 89.96 89.80 90.00 90.41 89.93 89.96 91.20 90.65 91.32 89.61 89.96 90.50 90.32 90.86 82.88 84.60 85.76 84.44 86.15 85.26 84.50 86.50 84.95 8"
D19-1538,D17-1159,0,0.482936,"mation extraction (Christensen et al., 2011), machine translation (Xiong et al., 2012) and question answering (Yih et al., 2016). Almost all of traditional SRL methods relied heavily on syntactic features, which suffered the risk of erroneous syntactic input, leading to undesired error propagation. To alleviate this inconvenience, researchers as early as Zhou and Xu (2015) propose neural SRL models without syntactic input. Cai et al. (2018) employ the biaffine attentional mechanism (Dozat and Manning, 2017) for dependency-based SRL. In the meantime, a series of studies (Roth and Lapata, 2016; Marcheggiani and Titov, 2017; Strubell et al., 2018; Li et al., 2018) have introduced syntactic clue in creative ways for further performance improvement, which achieve favorable results. However, applying the k-order syntactic tree pruning of He et al. (2018) to the biaffine SRL model (Cai et al., 2018) does not boost the performance as expected, which indicates that exploiting syntactic clue in state-of-theart SRL models still deserves deep exploration. Besides, most of SRL literature is dedicated to impressive performance gains on English and Chinese, but other multiple languages have received relatively little attent"
D19-1538,P18-2106,0,0.0588996,"Missing"
D19-1538,D14-1162,0,0.0813937,"Missing"
D19-1538,N18-1202,0,0.0433936,"a sentence and marked predicates, we adopt the bidirectional Long Short-term Memory neural network (BiLSTM) (Hochreiter and Schmidhuber, 1997) to encode sentence, which takes as input the word representation. Following Cai et al. (2018), the word representation is the concatenation of five vectors: randomly initialized word embedding, lemma embedding, part-of-speech (POS) tag embedding, pre-trained word embedding and predicate-specific indicator embedding. Besides, the latest work (Li et al., 2018) has demonstrated that the contextualized representation ELMo (Embeddings from Language Models) (Peters et al., 2018) could boost performance of dependency SRL model on English and Chinese. To explore whether the deep enhanced representation can help other multiple languages, we further enhance the word representation by concatenating an external embedding from the recent successful language models, ELMo and BERT (Bidirectional Encoder Representations from Transformers) (Devlin et al., 2018), which are both contextualized representations. It is worth noting that we use ELMo or BERT to obtain pre-trained contextual embeddings rather than fine-tune the model, which are fixed contextual representations. 2.2 Arg"
D19-1538,P05-1072,0,0.281319,"in this work, due to strong syntax-agnostic baseline. Therefore, more effective methods to incorporate syntax into neural SRL model are worth exploring. Besides, utilization of syntactic dependency labels information is also a promising direction and we leave it for future work. 4 In this work, we use gold syntax rather than other better parse to explore the greatest syntactic contribution, considering the current state-of-the-art syntactic parsers are being upgraded so fast now. 5 Related Work In early work of semantic role labeling, most of researchers were dedicated to feature engineering (Pradhan et al., 2005; Punyakanok et al., 2008; Zhao et al., 2009b, 2013). The first neural SRL model was proposed by Collobert et al. (2011), which used convolutional neural network but their efforts fell short. Later, Foland and Martin (2015) effectively extended their work by using syntactic features as input. Roth and Lapata (2016) introduced syntactic paths to guide neural architectures for dependency SRL. However, putting syntax aside has sparked much research interest since Zhou and Xu (2015) employed deep BiLSTMs for span SRL. A series of neural SRL models without syntactic inputs were proposed. Marcheggia"
D19-1538,J08-2005,0,0.345373,"trong syntax-agnostic baseline. Therefore, more effective methods to incorporate syntax into neural SRL model are worth exploring. Besides, utilization of syntactic dependency labels information is also a promising direction and we leave it for future work. 4 In this work, we use gold syntax rather than other better parse to explore the greatest syntactic contribution, considering the current state-of-the-art syntactic parsers are being upgraded so fast now. 5 Related Work In early work of semantic role labeling, most of researchers were dedicated to feature engineering (Pradhan et al., 2005; Punyakanok et al., 2008; Zhao et al., 2009b, 2013). The first neural SRL model was proposed by Collobert et al. (2011), which used convolutional neural network but their efforts fell short. Later, Foland and Martin (2015) effectively extended their work by using syntactic features as input. Roth and Lapata (2016) introduced syntactic paths to guide neural architectures for dependency SRL. However, putting syntax aside has sparked much research interest since Zhou and Xu (2015) employed deep BiLSTMs for span SRL. A series of neural SRL models without syntactic inputs were proposed. Marcheggiani et al. (2017) applied"
D19-1538,P16-1113,0,0.385825,"tasks, including information extraction (Christensen et al., 2011), machine translation (Xiong et al., 2012) and question answering (Yih et al., 2016). Almost all of traditional SRL methods relied heavily on syntactic features, which suffered the risk of erroneous syntactic input, leading to undesired error propagation. To alleviate this inconvenience, researchers as early as Zhou and Xu (2015) propose neural SRL models without syntactic input. Cai et al. (2018) employ the biaffine attentional mechanism (Dozat and Manning, 2017) for dependency-based SRL. In the meantime, a series of studies (Roth and Lapata, 2016; Marcheggiani and Titov, 2017; Strubell et al., 2018; Li et al., 2018) have introduced syntactic clue in creative ways for further performance improvement, which achieve favorable results. However, applying the k-order syntactic tree pruning of He et al. (2018) to the biaffine SRL model (Cai et al., 2018) does not boost the performance as expected, which indicates that exploiting syntactic clue in state-of-theart SRL models still deserves deep exploration. Besides, most of SRL literature is dedicated to impressive performance gains on English and Chinese, but other multiple languages have rec"
D19-1538,D18-1548,0,0.230434,"Missing"
D19-1538,K16-1019,0,0.0500551,"t al. (2018) presented a unified neural framework to provide multiple methods for syntactic integration. Our method is closely related to the one of He et al. (2018), designed to prune as many unlikely arguments as possible. Multilingual SRL To promote NLP applications, the CoNLL-2009 shared task advocated performing SRL for multiple languages. Among the participating systems, Zhao et al. (2009a) proposed an integrated approach by exploiting largescale feature set, while Bj¨orkelund et al. (2009) used a generic feature selection procedure. Until now, only a few of work (Lei et al., 2015; 5357 Swayamdipta et al., 2016; Mulcaire et al., 2018) seriously considered multilingual SRL. Among them, Mulcaire et al. (2018) built a polyglot model (training one model on multiple languages) for multilingual SRL, but their results were far from satisfactory. Therefore, this work aims to complete the overall upgrade since CoNLL-2009 shared task and leaves polyglot training as our future work. 6 Conclusion This paper is dedicated to filling the long-term performance gap of multilingual SRL since a long time ago with a newly proposed syntax-based argument pruning method. Experimental results demonstrate its effectiveness"
D19-1538,P12-1095,0,0.0428707,"L annotates the syntactic head of argument rather than the entire constituent (span), and this paper will focus on the ∗ These authors made equal contribution.† Corresponding author. This paper was partially supported by National Key Research and Development Program of China (No. 2017YFB0304100) and Key Projects of National Natural Science Foundation of China (No. U1836222 and No. 61733011). dependency-based SRL. Be it dependency or span, SRL plays a critical role in many natural language processing (NLP) tasks, including information extraction (Christensen et al., 2011), machine translation (Xiong et al., 2012) and question answering (Yih et al., 2016). Almost all of traditional SRL methods relied heavily on syntactic features, which suffered the risk of erroneous syntactic input, leading to undesired error propagation. To alleviate this inconvenience, researchers as early as Zhou and Xu (2015) propose neural SRL models without syntactic input. Cai et al. (2018) employ the biaffine attentional mechanism (Dozat and Manning, 2017) for dependency-based SRL. In the meantime, a series of studies (Roth and Lapata, 2016; Marcheggiani and Titov, 2017; Strubell et al., 2018; Li et al., 2018) have introduced"
D19-1538,W04-3212,0,0.0816855,"noted as Rootarg ) according to the syntax tree. Therefore, the relative position representation of predicate and argument is (0, 1), so it is for likes and fish. As for the right one in (b), suppose the marked predicate has two arguments−arg1 and arg2, the common ancestors of predicate and arguments are respectively Rootarg1 and Rootarg2 . In this case, the relative position representations are (0, 1) and (1, 2). Argument Pruning Method To maintain the integrity of sequential inputs from the whole sentence, we propose a novel syntax-based method to prune arguments, unlike most existing work (Xue and Palmer, 2004; Zhao et al., 2009a; He et al., 5352 2018) which prunes argument candidates in the pre-processing stage. As shown in Figure 1, the way to perform argument pruning strategy is very straightforward. In the argument pruning layer, our model drops these candidate arguments (more exactly, BiLSTM representations) which do not comply with the syntactic rule. In other words, only the predicates and arguments that satisfy the syntactic rule will be output to next layer. For example (in Figure 1), given the sentence Keep your heart and mind open, the predicate Keep and the corresponding syntactic depen"
D19-1538,P16-2033,0,0.0222586,"ather than the entire constituent (span), and this paper will focus on the ∗ These authors made equal contribution.† Corresponding author. This paper was partially supported by National Key Research and Development Program of China (No. 2017YFB0304100) and Key Projects of National Natural Science Foundation of China (No. U1836222 and No. 61733011). dependency-based SRL. Be it dependency or span, SRL plays a critical role in many natural language processing (NLP) tasks, including information extraction (Christensen et al., 2011), machine translation (Xiong et al., 2012) and question answering (Yih et al., 2016). Almost all of traditional SRL methods relied heavily on syntactic features, which suffered the risk of erroneous syntactic input, leading to undesired error propagation. To alleviate this inconvenience, researchers as early as Zhou and Xu (2015) propose neural SRL models without syntactic input. Cai et al. (2018) employ the biaffine attentional mechanism (Dozat and Manning, 2017) for dependency-based SRL. In the meantime, a series of studies (Roth and Lapata, 2016; Marcheggiani and Titov, 2017; Strubell et al., 2018; Li et al., 2018) have introduced syntactic clue in creative ways for furthe"
D19-1538,P16-1131,1,0.903057,"Missing"
D19-1538,W09-1209,1,0.833171,"ording to the syntax tree. Therefore, the relative position representation of predicate and argument is (0, 1), so it is for likes and fish. As for the right one in (b), suppose the marked predicate has two arguments−arg1 and arg2, the common ancestors of predicate and arguments are respectively Rootarg1 and Rootarg2 . In this case, the relative position representations are (0, 1) and (1, 2). Argument Pruning Method To maintain the integrity of sequential inputs from the whole sentence, we propose a novel syntax-based method to prune arguments, unlike most existing work (Xue and Palmer, 2004; Zhao et al., 2009a; He et al., 5352 2018) which prunes argument candidates in the pre-processing stage. As shown in Figure 1, the way to perform argument pruning strategy is very straightforward. In the argument pruning layer, our model drops these candidate arguments (more exactly, BiLSTM representations) which do not comply with the syntactic rule. In other words, only the predicates and arguments that satisfy the syntactic rule will be output to next layer. For example (in Figure 1), given the sentence Keep your heart and mind open, the predicate Keep and the corresponding syntactic dependency (bottom), by"
D19-1538,D09-1004,1,0.83697,"ording to the syntax tree. Therefore, the relative position representation of predicate and argument is (0, 1), so it is for likes and fish. As for the right one in (b), suppose the marked predicate has two arguments−arg1 and arg2, the common ancestors of predicate and arguments are respectively Rootarg1 and Rootarg2 . In this case, the relative position representations are (0, 1) and (1, 2). Argument Pruning Method To maintain the integrity of sequential inputs from the whole sentence, we propose a novel syntax-based method to prune arguments, unlike most existing work (Xue and Palmer, 2004; Zhao et al., 2009a; He et al., 5352 2018) which prunes argument candidates in the pre-processing stage. As shown in Figure 1, the way to perform argument pruning strategy is very straightforward. In the argument pruning layer, our model drops these candidate arguments (more exactly, BiLSTM representations) which do not comply with the syntactic rule. In other words, only the predicates and arguments that satisfy the syntactic rule will be output to next layer. For example (in Figure 1), given the sentence Keep your heart and mind open, the predicate Keep and the corresponding syntactic dependency (bottom), by"
D19-1538,P15-1109,0,0.270467,"04100) and Key Projects of National Natural Science Foundation of China (No. U1836222 and No. 61733011). dependency-based SRL. Be it dependency or span, SRL plays a critical role in many natural language processing (NLP) tasks, including information extraction (Christensen et al., 2011), machine translation (Xiong et al., 2012) and question answering (Yih et al., 2016). Almost all of traditional SRL methods relied heavily on syntactic features, which suffered the risk of erroneous syntactic input, leading to undesired error propagation. To alleviate this inconvenience, researchers as early as Zhou and Xu (2015) propose neural SRL models without syntactic input. Cai et al. (2018) employ the biaffine attentional mechanism (Dozat and Manning, 2017) for dependency-based SRL. In the meantime, a series of studies (Roth and Lapata, 2016; Marcheggiani and Titov, 2017; Strubell et al., 2018; Li et al., 2018) have introduced syntactic clue in creative ways for further performance improvement, which achieve favorable results. However, applying the k-order syntactic tree pruning of He et al. (2018) to the biaffine SRL model (Cai et al., 2018) does not boost the performance as expected, which indicates that expl"
D19-1538,P19-1230,1,0.751795,"ng encouraging results on English, Chinese, Czech and Spanish. Cai et al. (2018) built a full end-to-end SRL model with biaffine attention and provided strong performance on English and Chinese. Li et al. (2019) also proposed an end-to-end model for both dependency and span SRL with a unified argument representation, obtaining favorable results on English. Despite the success of syntax-agnostic SRL models, more recent work attempts to further improve performance by integrating syntactic information, with the impressive success of deep neural networks in dependency parsing (Zhang et al., 2016; Zhou and Zhao, 2019). Marcheggiani and Titov (2017) used graph convolutional network to encode syntax into dependency SRL. He et al. (2018) proposed an extended k-order argument pruning algorithm based on syntactic tree and boosted SRL performance. Li et al. (2018) presented a unified neural framework to provide multiple methods for syntactic integration. Our method is closely related to the one of He et al. (2018), designed to prune as many unlikely arguments as possible. Multilingual SRL To promote NLP applications, the CoNLL-2009 shared task advocated performing SRL for multiple languages. Among the participat"
E09-1100,D07-1098,0,0.0245754,"e function for that character. The feature preactn returns the previous parsing action type, and the subscript n stands for the action order before the current action. 3.4 Decoding Without Markovian feature like preact−1 , a shiftreduce parser can scan through an input sequence in linear time. That is, the decoding of a parsing method for word segmentation will be extremely fast. The time complexity of decoding will be 2L for Scheme E, and L for Scheme B, where L is the length of the input sequence. However, it is somewhat complicated as Markovian features are involved. Following the work of (Duan et al., 2007), the decoding in this case is to search a parsing action sequence with the maximal probability. Y Sdi = argmax p(di |di−1 di−2 ...), AV (s) = min{Lav (s), Rav (s)}, where the left and right AV values Lav (s) and Rav (s) are defined, respectively, as the numbers of its distinct predecessor and successor characters. In this work, AV values for substrings are derived from unlabeled training and test corpora by substring counting. Multiple features are used to represent substrings of various lengths identified by the AV criterion. Formally put, the feature function for a n-character substring s w"
E09-1100,I05-3017,0,0.0624901,"lations among these characters and segment it later for a proper tackling as the above first example. All these examples suggest that dependencies exist between discontinuous characters, and word boundary representation is insufficient to handle these cases. This motivates us to introduce character dependencies. n 2 To Segment or Not: That Is the Question Though most words can be unambiguously defined in Chinese text, some word boundaries are not so easily determined. We show such three examples as the following. The first example is from the MSRA segmented corpus of Bakeoff-2 (Bakeoff-2005) (Emerson, 2005): (ÏÊ  / Ü / / / ®®ìÊÆéÇ¬ / ¬ l /  |/ y / 0 a / piece of / “ / Beijing City Beijing Opera OK Sodality / member / entrance / ticket / ” As the guideline of MSRA standard requires any organization’s full name as a word, many long words in this form are frequently encountered. Though this type of ‘words’ may be regarded as an effective unit to some extent, some smaller meaningful constituents can be still identified inside them. Some researchers argue that these should be seen as phrases rather than words. In fact, e.g., a machine translation system will have to segment this type of words int"
E09-1100,W03-3017,0,0.0413149,", Ê  (Ï /  / n / Ê Week / one / three / five An unlabeled dependency graph for a string of cliques (i.e., words and characters) W = (Monday, Wednesday and Friday) 880 3.2 Shift-reduce Parsing According to (McDonald and Nivre, 2007), all data-driven models for dependency parsing that have been proposed in recent years can be described as either graph-based or transition-based. Since both dependency schemes that we construct for parsing are well-formed and projective, the latter is chosen as the parsing framework for the sake of efficiency. In detail, a shift-reduce method is adopted as in (Nivre, 2003). The method is step-wise and a classifier is used to make a parsing decision step by step. In each step, the classifier checks a clique pair 4 , namely, TOP, the top of a stack that consists of the processed cliques, and, INPUT, the first clique in the unprocessed sequence, to determine if a dependent relation should be established between them. Besides two arc-building actions, a shift action and a reduce action are also defined, as follows, Left-arc: Add an arc from INPUT to TOP and pop the stack. Right-arc: Add an arc from TOP to INPUT and push INPUT onto the stack. Reduce: Pop TOP from th"
E09-1100,J04-1004,0,0.0343245,"head, either left or right. The feature curroot returns the root of a partial parsing tree that includes a specified node. The feature cnseq returns a substring started from a given character. It checks the direction of the arc that passes the given character and collects all characters with the same arc direction to yield an output substring until the arc direction is changed. Note that all combinational features concerned with this one can be regarded as word-level features. The feature av is derived from unsupervised segmentation as in (Zhao and Kit, 2008a), and the accessor variety (AV) (Feng et al., 2004) is adopted as the unsupervised segmentation criterion. The AV value of a substring s is defined as the greatest AV score to activate the above feature function for that character. The feature preactn returns the previous parsing action type, and the subscript n stands for the action order before the current action. 3.4 Decoding Without Markovian feature like preact−1 , a shiftreduce parser can scan through an input sequence in linear time. That is, the decoding of a parsing method for word segmentation will be extremely fast. The time complexity of decoding will be 2L for Scheme E, and L for"
E09-1100,E06-1010,0,0.0222318,"cter dependency schemes w1 ...wn is an unlabeled directed graph D = (W, A), where (a) W is the set of ordered nodes, i.e. clique tokens in the input string, ordered by a linear precedence relation &lt;, (b) A is a set of unlabeled arcs (wi , wj ), where wi , wj ∈ W , If (wi , wj ) ∈ A, wi is called the head of wj and wj a dependent of wi . Traditionally, the notation wi → wj means (wi , wj ) ∈ A; wi →∗ wj denotes the reflexive and transitive closure of the (unlabeled) arc relation. We assume that the designed dependency structure satisfies the following common constraints in existing literature (Nivre, 2006). (1) D is weakly connected, that is, the corresponding undirected graph is connected. (CONNECTEDNESS) (2) The graph D is acyclic, i.e., if wi → wj then not wj →∗ wi . (ACYCLICITY) (3) There is at most one arc (wi , wj ) ∈ A, ∀wj ∈ W . (SINGLE-HEAD) (4) An arc wi → wk is projective iff, for every word wj occurring between wi and wk in the string (wi &lt; wj &lt; wk or wi &gt; wj &gt; wk ), wi →∗ wj . (PROJECTIVITY) We say that D is well-formed iff it is acyclic and connected, and D is projective iff every arcs in A are projective. Note that the above four conditions entail that the graph D is a single-roo"
E09-1100,C04-1081,0,0.0187637,"hm is adopted to find a tag sequence with the maximal joint probability from all legal tag sequences. If such a dynamic programming decoding is adopted, then this method for word segmentation is regarded as character tagging 5 . The time complexity of character-based classification method for decoding is L, which is the best result in decoding velocity. As dynamic programming is applied, the time complexity will be 16L with four tags. Recently, conditional random fields (CRFs) becomes popular for word segmentation since it provides slightly better performance than maximum entropy method does (Peng et al., 2004). However, CRFs is a structural learning tool rather than a simple classification framework. As shift-reduce parsing is a typical step-wise method that checks Table 4: The results of parsing and classification/tagging approaches using different feature combinations S.a Feature AS CityU MSRA PKU Basicb .935 .922 .950 .917 B +AVc .941 .933 .956 .927 +Prevd .937 .923 .951 .918 +AV+Prev .942 .935 .958 .929 Basic .940 .932 .957 .926 E +AV .948 .947 .964 .942 +Prev .944 .940 .962 .931 +AV+Prev .949 .951 .967 .943 n-gram/ce .933 .923 .948 .923 Cf +AV/c .942 .936 .957 .933 n-gram/dg .945 .938 .956 .93"
E09-1100,W04-1109,0,0.0115507,"(di |di−1 ...) is the conditional probability, and di is i-th parsing action. We use a beam search algorithm as in (Ratnaparkhi, 1996) to find the object parsing action sequence. The time complexity of this beam search algorithm will be 4BL for Scheme E and 3BL for Scheme B, where B is the beam width. (1) 3.5 Related Methods where t is an integer to logarithmize the score and taken as the feature value. For an overlap character of several substrings, we only choose the one with Among character-based learning techniques for word segmentation, we may identify two main 882 types, classification (GOH et al., 2004) and tagging (Low et al., 2005). Both character classification and tagging need to define the position of character inside a word. Traditionally, the four tags, b, m, e, and s stand, respectively, for the beginning, midle, end of a word, and a singlecharacter as word since (Xue, 2003). The following n-gram features from (Xue, 2003; Low et al., 2005) are used as basic features, each character one by one, it is reasonable to compare it to a classification method over characters. 3.6 Evaluation Results Table 3: Corpus size of Bakeoff-2 in number of words Training(M) Test(K) (a) Cn (n = −2, −1, 0,"
E09-1100,D07-1097,0,0.0295562,"Missing"
E09-1100,W06-0115,0,0.0219908,"e whole sequence. As full features are used, the former and the latter provide the similar performance. Due to using a global model like CRFs, our previous work in (Zhao et al., 2006; Zhao and Kit, 2008c) reported the best results over the evaluated corpora of Bakeoff-2 until now7 . Though those results are slightly better than the results here, we still see that the results of character-level dependency parsing approach (Scheme E) are comparable to those state-of-the-art ones on each evaluated corpus. now, we launched an annotation job based on UPUC segmented corpus of Bakeoff-3(Bakeoff2006)(Levow, 2006). The training corpus is with 880K characters and test corpus 270K. However, the essential of the annotation job is actually conducted in a lexicon. After a lexicon is extracted from CTB segmented corpus, we use a top-down strategy to annotate internal dependencies inside these words from the lexicon. A long word is first split into some smaller constituents, and dependencies among these constituents are determined, character dependencies inside each constituents are then annotated. Some simple rules are adopted to determine dependency relation, e.g., modifiers are kept marking as dependants a"
E09-1100,I05-3025,0,0.559331,"al probability, and di is i-th parsing action. We use a beam search algorithm as in (Ratnaparkhi, 1996) to find the object parsing action sequence. The time complexity of this beam search algorithm will be 4BL for Scheme E and 3BL for Scheme B, where B is the beam width. (1) 3.5 Related Methods where t is an integer to logarithmize the score and taken as the feature value. For an overlap character of several substrings, we only choose the one with Among character-based learning techniques for word segmentation, we may identify two main 882 types, classification (GOH et al., 2004) and tagging (Low et al., 2005). Both character classification and tagging need to define the position of character inside a word. Traditionally, the four tags, b, m, e, and s stand, respectively, for the beginning, midle, end of a word, and a singlecharacter as word since (Xue, 2003). The following n-gram features from (Xue, 2003; Low et al., 2005) are used as basic features, each character one by one, it is reasonable to compare it to a classification method over characters. 3.6 Evaluation Results Table 3: Corpus size of Bakeoff-2 in number of words Training(M) Test(K) (a) Cn (n = −2, −1, 0, 1, 2), (b) Cn Cn+1 (n = −2, −1"
E09-1100,W03-1025,0,0.219689,"thods Approach a basic +AV +Prevb +AV+Prev Class/Tagc .918 .935 .928 .941 Parsing/wod .921 .937 .924 .942 Parsing/w e .925 .940 .929 .945 a The highest F-score in Bakeoff-3 is 0.933. b As for the tagging method, this means dynamic programming decoding; As for the parsing method, this means three Markovian features. c Character-based classification or tagging method d Using trivial internal dependencies in Scheme E. e Using annotated internal character dependencies. 5 Is Word Still Necessary? Note that this work is not about joint learning of word boundaries and syntactic dependencies such as (Luo, 2003), where a character-based tagging method is used for syntactic constituent parsing from unsegmented Chinese text. Instead, this work is to explore an alternative way to represent “word-hood” in Chinese, which is based on character-level dependencies instead of traditional word boundaries definition. Though considering dependencies among words is not novel (Gao and Suzuki, 2004), we recognize that this study is the first work concerned with character dependency. This study originally intends to lead us to consider an alternative way that can play the similar role as word boundary annotations. I"
E09-1100,D07-1013,0,0.017719,"if an organization name is segmented into pieces. Though the word ‘ ’(embassy) is right after ‘ ’(South Africa) in the above phrase, the embassy does not belong to South Africa but China, and it is only located in South Africa. The third example is an abbreviation that makes use of the characteristics of Chinese characters. H (Ï (Ïn 3 Character-Level Dependency Parsing ¥I / 7 / H / , , Ê  (Ï /  / n / Ê Week / one / three / five An unlabeled dependency graph for a string of cliques (i.e., words and characters) W = (Monday, Wednesday and Friday) 880 3.2 Shift-reduce Parsing According to (McDonald and Nivre, 2007), all data-driven models for dependency parsing that have been proposed in recent years can be described as either graph-based or transition-based. Since both dependency schemes that we construct for parsing are well-formed and projective, the latter is chosen as the parsing framework for the sake of efficiency. In detail, a shift-reduce method is adopted as in (Nivre, 2003). The method is step-wise and a classifier is used to make a parsing decision step by step. In each step, the classifier checks a clique pair 4 , namely, TOP, the top of a stack that consists of the processed cliques, and,"
E09-1100,P05-1013,0,0.0177441,"n China / in / South Africa / embassy Using a character-level dependency representation, we first show how a word segmentation task can be transformed into a dependency parsing problem. Since word segmentation is traditionally formularized as an unlabeled character chunking task since (Xue, 2003), only unlabeled dependencies are concerned in the transformation. There are many ways to transform chunks in a sequence into dependency representation. However, for the sake of simplicity, only well-formed and projective output sequences are considered for our processing. Borrowing the notation from (Nivre and Nilsson, 2005), an unlabeled dependency graph is formally defined as follows: (the Chinese embassy in South Africa) This example demonstrates how researchers can also feel inconvenient if an organization name is segmented into pieces. Though the word ‘ ’(embassy) is right after ‘ ’(South Africa) in the above phrase, the embassy does not belong to South Africa but China, and it is only located in South Africa. The third example is an abbreviation that makes use of the characteristics of Chinese characters. H (Ï (Ïn 3 Character-Level Dependency Parsing ¥I / 7 / H / , , Ê  (Ï /  / n / Ê Week / one / thr"
E09-1100,W03-1719,0,0.156904,"Missing"
E09-1100,O03-4002,0,0.665102,"r words are usually annotated. We will initially develop a character-level dependency parsing scheme in this section. Especially, we show character dependencies, even those trivial ones that are equally transformed from pre-defined word boundaries, can be effectively captured in a parsing way. 3.1 Formularization China / in / South Africa / embassy Using a character-level dependency representation, we first show how a word segmentation task can be transformed into a dependency parsing problem. Since word segmentation is traditionally formularized as an unlabeled character chunking task since (Xue, 2003), only unlabeled dependencies are concerned in the transformation. There are many ways to transform chunks in a sequence into dependency representation. However, for the sake of simplicity, only well-formed and projective output sequences are considered for our processing. Borrowing the notation from (Nivre and Nilsson, 2005), an unlabeled dependency graph is formally defined as follows: (the Chinese embassy in South Africa) This example demonstrates how researchers can also feel inconvenient if an organization name is segmented into pieces. Though the word ‘ ’(embassy) is right after ‘ ’(Sout"
E09-1100,W08-2127,1,0.940215,"dency parsing, dprel means the arc direction from the head, either left or right. The feature curroot returns the root of a partial parsing tree that includes a specified node. The feature cnseq returns a substring started from a given character. It checks the direction of the arc that passes the given character and collects all characters with the same arc direction to yield an output substring until the arc direction is changed. Note that all combinational features concerned with this one can be regarded as word-level features. The feature av is derived from unsupervised segmentation as in (Zhao and Kit, 2008a), and the accessor variety (AV) (Feng et al., 2004) is adopted as the unsupervised segmentation criterion. The AV value of a substring s is defined as the greatest AV score to activate the above feature function for that character. The feature preactn returns the previous parsing action type, and the subscript n stands for the action order before the current action. 3.4 Decoding Without Markovian feature like preact−1 , a shiftreduce parser can scan through an input sequence in linear time. That is, the decoding of a parsing method for word segmentation will be extremely fast. The time compl"
E09-1100,I08-4017,1,0.852991,"dency parsing, dprel means the arc direction from the head, either left or right. The feature curroot returns the root of a partial parsing tree that includes a specified node. The feature cnseq returns a substring started from a given character. It checks the direction of the arc that passes the given character and collects all characters with the same arc direction to yield an output substring until the arc direction is changed. Note that all combinational features concerned with this one can be regarded as word-level features. The feature av is derived from unsupervised segmentation as in (Zhao and Kit, 2008a), and the accessor variety (AV) (Feng et al., 2004) is adopted as the unsupervised segmentation criterion. The AV value of a substring s is defined as the greatest AV score to activate the above feature function for that character. The feature preactn returns the previous parsing action type, and the subscript n stands for the action order before the current action. 3.4 Decoding Without Markovian feature like preact−1 , a shiftreduce parser can scan through an input sequence in linear time. That is, the decoding of a parsing method for word segmentation will be extremely fast. The time compl"
E09-1100,Y06-1012,1,0.887285,"nvolved, a beam search algorithm with width 5 is used to decode, otherwise, a simple shift-reduce decoding is used. We see that the performance given by Scheme E is much better than that by Scheme B. The results of character-based classification and tagging methods are at the bottom of Table 46 . It is observed that the parsing method outperforms classification and tagging method without Markovian features or decoding throughout the whole sequence. As full features are used, the former and the latter provide the similar performance. Due to using a global model like CRFs, our previous work in (Zhao et al., 2006; Zhao and Kit, 2008c) reported the best results over the evaluated corpora of Bakeoff-2 until now7 . Though those results are slightly better than the results here, we still see that the results of character-level dependency parsing approach (Scheme E) are comparable to those state-of-the-art ones on each evaluated corpus. now, we launched an annotation job based on UPUC segmented corpus of Bakeoff-3(Bakeoff2006)(Levow, 2006). The training corpus is with 880K characters and test corpus 270K. However, the essential of the annotation job is actually conducted in a lexicon. After a lexicon is ex"
E09-1100,J03-4003,0,\N,Missing
E09-1100,W03-1726,0,\N,Missing
I08-1002,A00-2032,0,0.0978954,"y. In this sense, one may consider them the discrete and continuous formulation of the same idea. 5 All evaluations will be represented in terms of word F-measure if not otherwise specified. A standard scoring tool with this metric can be found in SIGHAN website, http://www.sighan.org/bakeoff2003/score. However, to compare with related work, we will also adopt boundary F-measure Fb = 2Rb Pb /(Rb + Pb ), where the boundary recall Rb and boundary precision Pb are, respectively, the proportions of the correctly recognized boundaries to all boundaries in the goldstandard and a segmenter’s output (Ando and Lee, 2000). 11 Table 1: Bakeoff-3 Corpora Corpus Training(M) Test(K) AS 8.42 146 CityU 2.71 364 CTB 0.83 256 MSRA 2.17 173 Table 2: Performance with decoding algorithm (1) M. L.a GoodTraining corpus ness AS CityU CTB MSRA FSR .400 .454 .462 .432 DLG/d .592 .610 .604 .603 2 AV .568 .595 .596 .577 BE .559 .587 .592 .572 FSR .193 .251 .268 .235 DLG/d .331 .397 .409 .379 7 AV .399 .423 .430 .407 BE .390 .419 .428 .403 a M.L.: Maximal length allowable for word candidates. for computation. There are two ways to get this score: (1) computed by the goodness measure, which is applicable only if the measure allow"
I08-1002,O97-4005,0,0.455799,"tation of Chinese text. Four existing approaches to unsupervised segmentations or word extraction are considered as its special cases, each with its own goodness measurement to quantify word likelihood. The output by each approach will be evaluated using benchmark data sets of Bakeoff-32 (Levow, 2006). Note that unsupervised segmentation is different from, if not more complex than, word extraction, in that the former must carry out the segmentation task for a text, for which a segmentation (decoding) algorithm is indispensable, whereas the latter only acquires a word candidate list as output (Chang and Su, 1997; Zhang et al., 2000). This paper reports our empirical evaluation and comparison of several popular goodness measures for unsupervised segmentation of Chinese texts using Bakeoff-3 data sets with a unified framework. Assuming no prior knowledge about Chinese, this framework relies on a goodness measure to identify word candidates from unlabeled texts and then applies a generalized decoding algorithm to find the optimal segmentation of a sentence into such candidates with the greatest sum of goodness scores. Experiments show that description length gain outperforms other measures because of it"
I08-1002,J04-1004,0,0.548443,"gs. L(·) is the empirical description length of a corpus in bits that can be estimated by the Shannon-Fano code or Huffman code as below, following classic information theory (Shannon, 1948). X . L(X) = −|X| pˆ(x)log2 pˆ(x) (5) x∈V where |· |denotes string length, V is the character vocabulary of X and pˆ(x) x’s frequency in X. For a given word candidate w, we define gDLG (w) = DLG(w). In principle, a substring with a negative DLG do not bring any positive compression effect by itself. Thus only substrings with a positive DLG value are added into our word candidate list. Accessor Variety (AV) Feng et al. (2004) propose AV as a statistical criterion to measure how likely a substring is a word. It is reported to handle lowfrequent words particularly well. The AV of a substring xi..j is defined as AV (xi..j ) = min{Lav (xi..j ), Rav (xi..j )} (6) 3 Although there have been many existing works in this direction (Lua and Gan, 1994; Chien, 1997; Sun et al., 1998; Zhang et al., 2000; SUN et al., 2004), we have to skip the details of comparing MI due to the length limitation of this paper. However, our experiments with MI provide no evidence against the conclusions in this paper. 10 where the left and right"
I08-1002,Y03-1017,0,0.0160045,"iments with MI provide no evidence against the conclusions in this paper. 10 where the left and right accessor variety Lav (xi..j ) and Rav (xi..j ) are, respectively, the number of distinct predecessor and successor characters. For a similar reason as to FSR, the logarithm of AV is used as goodness measure, and only substrings with AV &gt; 1 are considered word candidates. That is, we have gAV (w) = logAV (w) for a word candidate w. Boundary Entropy (Branching Entropy, BE) It is proposed as a criterion for unsupervised segmentation in some existing works (Tung and Lee, 1994; Chang and Su, 1997; Huang and Powers, 2003; Jin and Tanaka-Ishii, 2006). The local entropy for a given xi..j , defined as X h(xi..j ) = − p(x|xi..j )log p(x|xi..j ), (7) x∈V indicates the average uncertainty after (or before) xi..j in the text, where p(x|xi..j ) is the co-occurrence probability for x and xi..j . Two types of h(xi..j ), namely hL (xi..j ) and hR (xi..j ), can be defined for the two directions to extend xi..j (Tung and Lee, 1994). Also, we can define hmin = min{hR , hL } in a similar way as in (6). In this study, only substrings with BE &gt; 0 are considered word candidates. For a candidate w, we have gBE (w) = hmin (w)4 ."
I08-1002,P06-2056,0,0.662748,"o evidence against the conclusions in this paper. 10 where the left and right accessor variety Lav (xi..j ) and Rav (xi..j ) are, respectively, the number of distinct predecessor and successor characters. For a similar reason as to FSR, the logarithm of AV is used as goodness measure, and only substrings with AV &gt; 1 are considered word candidates. That is, we have gAV (w) = logAV (w) for a word candidate w. Boundary Entropy (Branching Entropy, BE) It is proposed as a criterion for unsupervised segmentation in some existing works (Tung and Lee, 1994; Chang and Su, 1997; Huang and Powers, 2003; Jin and Tanaka-Ishii, 2006). The local entropy for a given xi..j , defined as X h(xi..j ) = − p(x|xi..j )log p(x|xi..j ), (7) x∈V indicates the average uncertainty after (or before) xi..j in the text, where p(x|xi..j ) is the co-occurrence probability for x and xi..j . Two types of h(xi..j ), namely hL (xi..j ) and hR (xi..j ), can be defined for the two directions to extend xi..j (Tung and Lee, 1994). Also, we can define hmin = min{hR , hL } in a similar way as in (6). In this study, only substrings with BE &gt; 0 are considered word candidates. For a candidate w, we have gBE (w) = hmin (w)4 . 4 Evaluation The evaluation"
I08-1002,W99-0701,1,0.866648,"odness score. It works on T to output the best current word w∗ repeatedly with T =t∗ for the next round as follows, {w∗ , t∗ } = argmax g(w) (2) wt = T gF SR (w) = log(ˆ p(w)) (3) where pˆ(w) is w’s frequency in the corpus. This allows the arithmetic addition in (1). According to Zipf’s Law (Zipf, 1949), it approximates the use of the rank of w as its goodness, which would give it some statistical significance. For the sake of efficiency, only those substrings that occur more than once are considered qualified word candidates. Description Length Gain (DLG) The goodness measure is proposed in (Kit and Wilks, 1999) for compression-based unsupervised segmentation. The DLG from extracting all occurrences of xi xi+1 ...xj (also denoted as xi..j ) from a corpus X= x1 x2 ...xn as a word is defined as DLG(xi..j ) = L(X) − L(X[r → xi..j ] ⊕ xi..j ) (4) with each {w, g(w)} ∈ W . This algorithm will back off to forward maximal matching algorithm if the goodness function is set to word length. Thus the former may be regarded as a generalization of the latter. Symmetrically, it has an inverse version that works the other way around. 3 as the goodness for a word candidate, i.e., Goodness Measurement An unsupervised"
I08-1002,W06-0115,0,0.0334186,"are often involved in most existing works, and there has not been a comprehensive comparison of their performance in a unified way with available large-scale “gold standard” data sets, especially, multi-standard ones since Bakeoff-1 1 . In this paper we will propose a unified framework for unsupervised segmentation of Chinese text. Four existing approaches to unsupervised segmentations or word extraction are considered as its special cases, each with its own goodness measurement to quantify word likelihood. The output by each approach will be evaluated using benchmark data sets of Bakeoff-32 (Levow, 2006). Note that unsupervised segmentation is different from, if not more complex than, word extraction, in that the former must carry out the segmentation task for a text, for which a segmentation (decoding) algorithm is indispensable, whereas the latter only acquires a word candidate list as output (Chang and Su, 1997; Zhang et al., 2000). This paper reports our empirical evaluation and comparison of several popular goodness measures for unsupervised segmentation of Chinese texts using Bakeoff-3 data sets with a unified framework. Assuming no prior knowledge about Chinese, this framework relies o"
I08-1002,P98-2206,0,0.400207,"te w, we define gDLG (w) = DLG(w). In principle, a substring with a negative DLG do not bring any positive compression effect by itself. Thus only substrings with a positive DLG value are added into our word candidate list. Accessor Variety (AV) Feng et al. (2004) propose AV as a statistical criterion to measure how likely a substring is a word. It is reported to handle lowfrequent words particularly well. The AV of a substring xi..j is defined as AV (xi..j ) = min{Lav (xi..j ), Rav (xi..j )} (6) 3 Although there have been many existing works in this direction (Lua and Gan, 1994; Chien, 1997; Sun et al., 1998; Zhang et al., 2000; SUN et al., 2004), we have to skip the details of comparing MI due to the length limitation of this paper. However, our experiments with MI provide no evidence against the conclusions in this paper. 10 where the left and right accessor variety Lav (xi..j ) and Rav (xi..j ) are, respectively, the number of distinct predecessor and successor characters. For a similar reason as to FSR, the logarithm of AV is used as goodness measure, and only substrings with AV &gt; 1 are considered word candidates. That is, we have gAV (w) = logAV (w) for a word candidate w. Boundary Entropy ("
I08-1002,W00-1219,0,0.0456081,"xt. Four existing approaches to unsupervised segmentations or word extraction are considered as its special cases, each with its own goodness measurement to quantify word likelihood. The output by each approach will be evaluated using benchmark data sets of Bakeoff-32 (Levow, 2006). Note that unsupervised segmentation is different from, if not more complex than, word extraction, in that the former must carry out the segmentation task for a text, for which a segmentation (decoding) algorithm is indispensable, whereas the latter only acquires a word candidate list as output (Chang and Su, 1997; Zhang et al., 2000). This paper reports our empirical evaluation and comparison of several popular goodness measures for unsupervised segmentation of Chinese texts using Bakeoff-3 data sets with a unified framework. Assuming no prior knowledge about Chinese, this framework relies on a goodness measure to identify word candidates from unlabeled texts and then applies a generalized decoding algorithm to find the optimal segmentation of a sentence into such candidates with the greatest sum of goodness scores. Experiments show that description length gain outperforms other measures because of its strength for identi"
I08-1002,Y06-1012,1,0.889942,"Missing"
I08-1002,C98-2201,0,\N,Missing
I08-4017,O97-4005,0,0.0750261,"erformance improvement on both word segmentation and NER for all tracks except CTB segmentation, as highlighted in Table 6. We are unable explain this yet, and can only attribute it to some unique text characteristics of the CTB segmented corpus. An unsupervised segmentation criterion provides a kind of global information over the whole text of a corpus (Zhao and Kit, 2007). Its effectiveness is certainly sensitive to text characteristics. Quite a number of other unsupervised segmentation criteria are available for word discovery in unlabeled texts, e.g., boundary entropy (Tung and Lee, 1994; Chang and Su, 1997; Huang and Powers, 2003; Jin and Tanaka-Ishii, 2006) and descriptionlength-gain (DLG) (Kit and Wilks, 1999). We found that among them AV could help the CRFs model to achieve a better performance than others, although the overall unsupervised segmentation by DLG was slightly better than that by AV. Combining any two of these criteria did not give any further performance 110 5 Conclusion Without doubt our achievements in Bakeoff-4 owes not only to the careful selection of character tag set and feature templates for exerting the strength of CRFs learning but also to the effectiveness of our unsu"
I08-4017,W06-0116,0,0.0135173,"ndom fields (CRFs) learning successfully, using similar character tags and feature templates for both word segmentation and NER. It ranks at the top in all closed tests of word segmentation and gives promising results for all closed and open NER tasks in the Bakeoff. Tag set selection and unsupervised segmentation play a critical role in this success. 1 Introduction A number of recent studies show that character sequence labeling is a simple but effective formulation of Chinese word segmentation and name entity recognition for machine learning (Xue, 2003; Low et al., 2005; Zhao et al., 2006a; Chen et al., 2006). Character tagging becomes a prevailing technique for this kind of labeling task for Chinese language processing, following the current trend of applying machine learning as a core technology in the field of natural language processing. In particular, when a full-fledged general-purpose sequence learning model such as CRFs is involved, the only work to do for a given application is to identify an ideal set of features and hyperparameters for the purpose 1 The Fourth International Chinese Language Processing Bakeoff & the First CIPS Chinese Language Processing Evaluation, at http://www.china-l"
I08-4017,I05-3017,0,0.152128,"segmenter. Table 3 lists the training corpora for the assistant CRFs segmenter and the ANERs for various open NER tests. The third group consists of feature templates generated from seven NE lists acquired from Chinese Wikipedia.4 The categories and numbers of these NE items are summarized in Table 4. 3 Evaluation Results The performance of both word segmentation and NER is measured in terms of the F-measure F = 2RP/(R + P ), where R and P are the recall and precision of segmentation or NER. We tested the techniques described above with the previous Bakeoffs’ data5 (Sproat and Emerson, 2003; Emerson, 2005; Levow, 2006). The evaluation results for the closed tests of word segmentation are reported in Table 5 and those for the NER on two corpora of Bakeoff-3 are in the upper part of Table 7. ‘+/–AV’ indicates whether AV features are applied. For Bakeoff-4, we participated in all five closed tracks of word segmentation, namely, CityU, CKIP, CTB, NCC, and SXU, and in all closed and open NER tracks of CityU and MSRA.6 The evaluation 3 It consists of about 108K words of one to four characterslong, available at http://ccl.pku.edu.cn/doubtfire/Course/Chi nese%20Information%20Processing/Source Code/Cha"
I08-4017,J04-1004,0,0.102102,"r, respectively. 107 ì E-ORG  O q S-LOC  ¯ ü Ñ O O O O In addition to these n-gram features, unsupervised segmentation outputs are also used as features, for the purpose of providing more word boundary information via global statistics derived from all unlabeled texts of the training and test corpora. The basic idea is to inform a supervised leaner of which substrings are recognized as word candidates by a given unsupervised segmentation criterion and how likely they are to be true words in terms of that criterion (Zhao and Kit, 2007; Kit and Zhao, 2007). We adopt the accessor variety (AV) (Feng et al., 2004a; Feng et al., 2004b) as our unsupervised segmentation criterion. It formulates an idea similar to linguist Harris’ (1955; 1970) for segmenting utterances of an unfamiliar language into morphemes to facilitate word extraction from Chinese raw texts. It is found more effective than other criteria in supporting CRFs learning of character tagging for word segmentation (Zhao and Kit, 2007). The AV value of a substring s is defined as AV (s) = min{Lav (s), Rav (s)}, where the left and right AV values Lav (s) and Rav (s) are defined, respectively, as the numbers of its distinct predecessor and succ"
I08-4017,Y03-1017,0,0.0536962,"nt on both word segmentation and NER for all tracks except CTB segmentation, as highlighted in Table 6. We are unable explain this yet, and can only attribute it to some unique text characteristics of the CTB segmented corpus. An unsupervised segmentation criterion provides a kind of global information over the whole text of a corpus (Zhao and Kit, 2007). Its effectiveness is certainly sensitive to text characteristics. Quite a number of other unsupervised segmentation criteria are available for word discovery in unlabeled texts, e.g., boundary entropy (Tung and Lee, 1994; Chang and Su, 1997; Huang and Powers, 2003; Jin and Tanaka-Ishii, 2006) and descriptionlength-gain (DLG) (Kit and Wilks, 1999). We found that among them AV could help the CRFs model to achieve a better performance than others, although the overall unsupervised segmentation by DLG was slightly better than that by AV. Combining any two of these criteria did not give any further performance 110 5 Conclusion Without doubt our achievements in Bakeoff-4 owes not only to the careful selection of character tag set and feature templates for exerting the strength of CRFs learning but also to the effectiveness of our unsupervised segmentation ap"
I08-4017,P06-2056,0,0.0146146,"tion and NER for all tracks except CTB segmentation, as highlighted in Table 6. We are unable explain this yet, and can only attribute it to some unique text characteristics of the CTB segmented corpus. An unsupervised segmentation criterion provides a kind of global information over the whole text of a corpus (Zhao and Kit, 2007). Its effectiveness is certainly sensitive to text characteristics. Quite a number of other unsupervised segmentation criteria are available for word discovery in unlabeled texts, e.g., boundary entropy (Tung and Lee, 1994; Chang and Su, 1997; Huang and Powers, 2003; Jin and Tanaka-Ishii, 2006) and descriptionlength-gain (DLG) (Kit and Wilks, 1999). We found that among them AV could help the CRFs model to achieve a better performance than others, although the overall unsupervised segmentation by DLG was slightly better than that by AV. Combining any two of these criteria did not give any further performance 110 5 Conclusion Without doubt our achievements in Bakeoff-4 owes not only to the careful selection of character tag set and feature templates for exerting the strength of CRFs learning but also to the effectiveness of our unsupervised segmentation approach. It is for the sake of"
I08-4017,W99-0701,1,0.674328,"ghted in Table 6. We are unable explain this yet, and can only attribute it to some unique text characteristics of the CTB segmented corpus. An unsupervised segmentation criterion provides a kind of global information over the whole text of a corpus (Zhao and Kit, 2007). Its effectiveness is certainly sensitive to text characteristics. Quite a number of other unsupervised segmentation criteria are available for word discovery in unlabeled texts, e.g., boundary entropy (Tung and Lee, 1994; Chang and Su, 1997; Huang and Powers, 2003; Jin and Tanaka-Ishii, 2006) and descriptionlength-gain (DLG) (Kit and Wilks, 1999). We found that among them AV could help the CRFs model to achieve a better performance than others, although the overall unsupervised segmentation by DLG was slightly better than that by AV. Combining any two of these criteria did not give any further performance 110 5 Conclusion Without doubt our achievements in Bakeoff-4 owes not only to the careful selection of character tag set and feature templates for exerting the strength of CRFs learning but also to the effectiveness of our unsupervised segmentation approach. It is for the sake of simplicity that similar sets of character tags and fea"
I08-4017,W06-0115,0,0.0295768,"le 3 lists the training corpora for the assistant CRFs segmenter and the ANERs for various open NER tests. The third group consists of feature templates generated from seven NE lists acquired from Chinese Wikipedia.4 The categories and numbers of these NE items are summarized in Table 4. 3 Evaluation Results The performance of both word segmentation and NER is measured in terms of the F-measure F = 2RP/(R + P ), where R and P are the recall and precision of segmentation or NER. We tested the techniques described above with the previous Bakeoffs’ data5 (Sproat and Emerson, 2003; Emerson, 2005; Levow, 2006). The evaluation results for the closed tests of word segmentation are reported in Table 5 and those for the NER on two corpora of Bakeoff-3 are in the upper part of Table 7. ‘+/–AV’ indicates whether AV features are applied. For Bakeoff-4, we participated in all five closed tracks of word segmentation, namely, CityU, CKIP, CTB, NCC, and SXU, and in all closed and open NER tracks of CityU and MSRA.6 The evaluation 3 It consists of about 108K words of one to four characterslong, available at http://ccl.pku.edu.cn/doubtfire/Course/Chi nese%20Information%20Processing/Source Code/Chapter 8/ Lexico"
I08-4017,I05-3025,0,0.514394,"rvised segmentation and conditional random fields (CRFs) learning successfully, using similar character tags and feature templates for both word segmentation and NER. It ranks at the top in all closed tests of word segmentation and gives promising results for all closed and open NER tasks in the Bakeoff. Tag set selection and unsupervised segmentation play a critical role in this success. 1 Introduction A number of recent studies show that character sequence labeling is a simple but effective formulation of Chinese word segmentation and name entity recognition for machine learning (Xue, 2003; Low et al., 2005; Zhao et al., 2006a; Chen et al., 2006). Character tagging becomes a prevailing technique for this kind of labeling task for Chinese language processing, following the current trend of applying machine learning as a core technology in the field of natural language processing. In particular, when a full-fledged general-purpose sequence learning model such as CRFs is involved, the only work to do for a given application is to identify an ideal set of features and hyperparameters for the purpose 1 The Fourth International Chinese Language Processing Bakeoff & the First CIPS Chinese Language Proc"
I08-4017,W03-1719,0,0.0324663,"sentation as the assistant segmenter. Table 3 lists the training corpora for the assistant CRFs segmenter and the ANERs for various open NER tests. The third group consists of feature templates generated from seven NE lists acquired from Chinese Wikipedia.4 The categories and numbers of these NE items are summarized in Table 4. 3 Evaluation Results The performance of both word segmentation and NER is measured in terms of the F-measure F = 2RP/(R + P ), where R and P are the recall and precision of segmentation or NER. We tested the techniques described above with the previous Bakeoffs’ data5 (Sproat and Emerson, 2003; Emerson, 2005; Levow, 2006). The evaluation results for the closed tests of word segmentation are reported in Table 5 and those for the NER on two corpora of Bakeoff-3 are in the upper part of Table 7. ‘+/–AV’ indicates whether AV features are applied. For Bakeoff-4, we participated in all five closed tracks of word segmentation, namely, CityU, CKIP, CTB, NCC, and SXU, and in all closed and open NER tracks of CityU and MSRA.6 The evaluation 3 It consists of about 108K words of one to four characterslong, available at http://ccl.pku.edu.cn/doubtfire/Course/Chi nese%20Information%20Processing/"
I08-4017,W06-0120,0,0.102785,"R trained on the MSRA NER training corpus of Bakeoff-3. This makes our official evaluation results extremely high but trivial, for a part of this corpus is used as the MSRA NER test corpus for Bakeoff-4. Presented here are the results without using this ANER. b Open2 is the result of Open1 using no NE list feature. results of word segmentation and NER for our system are presented in Tables 6 and 7, respectively. For the purpose of comparison, the word segmentation performance of our system on Bakeoff-4 data using the 2- and 4-tag sets and the best corresponding n-gram feature templates as in (Tsai et al., 2006; Low et al., 2005) are presented in Table 8.7 This comparison reconfirms the conclusion in (Zhao et CityU data sets in any other situation than the Bakeoff. 7 The templates for the 2-tag set, adopted from (Tsai et al., 2006), include C−2 , C−1 , C0 , C1 , C−3 C−1 , C−2 C0 , C−2 C−1 , C−1 C0 , C−1 C1 and C0 C1 . Those for the 4-tag set, adopted from (Xue, 2003) and (Low et al., 2005), include C−2 , C−1 , C0 , C1 , C2 , C−2 C−1 , C−1 C0 , C−1 C1 , C0 C1 and C1 C2 . 109 POOV .6960 .7013 .7719 .5182 .6223 .7912 .7649 .7761 .5984 .7159 ROOV .7168 .7216 .8141 .6280 .7116 .7495 .7404 .7730 .6179 .74"
I08-4017,O03-4002,0,0.860294,"ates unsupervised segmentation and conditional random fields (CRFs) learning successfully, using similar character tags and feature templates for both word segmentation and NER. It ranks at the top in all closed tests of word segmentation and gives promising results for all closed and open NER tasks in the Bakeoff. Tag set selection and unsupervised segmentation play a critical role in this success. 1 Introduction A number of recent studies show that character sequence labeling is a simple but effective formulation of Chinese word segmentation and name entity recognition for machine learning (Xue, 2003; Low et al., 2005; Zhao et al., 2006a; Chen et al., 2006). Character tagging becomes a prevailing technique for this kind of labeling task for Chinese language processing, following the current trend of applying machine learning as a core technology in the field of natural language processing. In particular, when a full-fledged general-purpose sequence learning model such as CRFs is involved, the only work to do for a given application is to identify an ideal set of features and hyperparameters for the purpose 1 The Fourth International Chinese Language Processing Bakeoff & the First CIPS Chi"
I08-4017,W06-0126,0,0.0390691,"we have opted for AV for Bakeoff-4. Table 9: Comparison of computational cost Tags 2 4 6 2 4 6 2 4 6 Templates AS CityU CTB Training time (Minutes) Tsai 112 52 16 Xue 206 79 28 Zhao 402 146 47 6 Feature numbers (×10 ) Tsai 13.2 7.3 3.1 Xue 16.1 9.0 3.9 Zhao 15.6 8.8 3.8 Memory cost (Giga bytes) Tsai 5.4 2.4 0.9 Xue 6.6 2.8 1.1 Zhao 6.4 2.7 1.0 MSRA 4.3 35 73 117 NE List Features for Open NER We realize that the NE lists available to us are far from sufficient for coping with all NEs in Bakeoff4. It is reasonable that using richer external NE lists gives a better NER performance in many cases (Zhang et al., 2006). Surprisingly, however, the NE list features used in our NER do not lead to any significant performance improvement, according to the evaluation results in Table 7. This is certainly another issue for our further inspection. 5.5 6.8 6.6 1.8 2.2 2.1 tainly, a possible way out of this problem is the computer hardware advancement, which is predicted by Moore’s Law (Moore, 1965) to be improving at an exponential rate in general, including processing speed and memory capacity. Specifically, CPU can be made twice faster every other year or even 18 months. It is predictable that computational cost w"
I08-4017,W06-0127,1,0.845739,"n and conditional random fields (CRFs) learning successfully, using similar character tags and feature templates for both word segmentation and NER. It ranks at the top in all closed tests of word segmentation and gives promising results for all closed and open NER tasks in the Bakeoff. Tag set selection and unsupervised segmentation play a critical role in this success. 1 Introduction A number of recent studies show that character sequence labeling is a simple but effective formulation of Chinese word segmentation and name entity recognition for machine learning (Xue, 2003; Low et al., 2005; Zhao et al., 2006a; Chen et al., 2006). Character tagging becomes a prevailing technique for this kind of labeling task for Chinese language processing, following the current trend of applying machine learning as a core technology in the field of natural language processing. In particular, when a full-fledged general-purpose sequence learning model such as CRFs is involved, the only work to do for a given application is to identify an ideal set of features and hyperparameters for the purpose 1 The Fourth International Chinese Language Processing Bakeoff & the First CIPS Chinese Language Processing Evaluation,"
I08-4017,Y06-1012,1,0.918116,"n and conditional random fields (CRFs) learning successfully, using similar character tags and feature templates for both word segmentation and NER. It ranks at the top in all closed tests of word segmentation and gives promising results for all closed and open NER tasks in the Bakeoff. Tag set selection and unsupervised segmentation play a critical role in this success. 1 Introduction A number of recent studies show that character sequence labeling is a simple but effective formulation of Chinese word segmentation and name entity recognition for machine learning (Xue, 2003; Low et al., 2005; Zhao et al., 2006a; Chen et al., 2006). Character tagging becomes a prevailing technique for this kind of labeling task for Chinese language processing, following the current trend of applying machine learning as a core technology in the field of natural language processing. In particular, when a full-fledged general-purpose sequence learning model such as CRFs is involved, the only work to do for a given application is to identify an ideal set of features and hyperparameters for the purpose 1 The Fourth International Chinese Language Processing Bakeoff & the First CIPS Chinese Language Processing Evaluation,"
I08-4017,W03-1726,0,\N,Missing
I13-1069,H05-1079,0,0.022352,"uch as simple rewriting rules, lexical relations and passiveactive transform, as well as a group of detailed • A labeled alignment scheme is proposed for RTE; • An RTE data set annotated with the proposed scheme is released; • High prediction accuracies are achieved on two RTE data sets. 2 Related Work RTE has attracted extensive attention in the past decade, and a wide range of approaches have been proposed besides the alignment-based methods (Androutsopoulos and Malakasiotis, 2009). The logic-based methods interpret sentences to first-order-logic expressions and then invoke theorem provers (Bos and Markert, 2005). Similaritybased methods employ classifiers to learn from 2 The notation means that the expression “read into” in P is connected to the expression “interpreted” in H. 3 NULL means an empty expression. 606 (a) Alignment on entailment pair (b) Weakness on non-entailment pair (c) Labeled alignment on non-entailment pair Figure 1: Illustration of Alignment for RTE. Each subfigure presents an RTE sample. The vertical text is the premise, and the horizontal text is the hypothesis. The solid squares represent positive links, and the crosses represent negative links. (a) is of entailment relation, wh"
I13-1069,J93-2003,0,0.0258722,"“Chr´etien visited Peugeot’s newly renovated car factory” entails the hypothesized answer form “Peugeot manufactures cars” (Dagan et al., 2006). Similarly, in Machine Translation (MT) evaluation, a correct translation should be semantically equivalent to the gold translation, that is, both translations should entail each other (Pad´o et al., 2009). RTE has attracted extensive attention ever since it was proposed. A wide range of methods have been proposed, and quite a few successful approaches treat RTE as an alignment problem. Alignment is originally developed for MT to bridge two languages (Brown et al., 1993). Alignment is to establish links between the semantically equivalent atom expressions in two sentences. (Marsi and Krahmer, 2005) first advocates pipelined system architectures that contain distinct alignment components. This latter becomes a strategy crucial to the top-performing systems of (Hickl et al., 2006). In addition, human-generated alignment annotations for the second PASCAL 1 RTE challenge is released by Microsoft Research to facilitate related research (Brockett, 2007). The principle of the existing alignment-based RTE methods is that a sufficiently good alignment between P and H"
I13-1069,W07-1427,0,0.0446611,"Missing"
I13-1069,P10-1122,0,0.0243825,"er, however, revises the alignment scheme to support RTE, especially to address the third difference. (MacCartney et al., 2006) argues that some critical RTE-related linguistic phenomena such as negations and modalities cannot be captured by alignment. They propose a wide range of features to represent them, and employ a classifier to learn from these specialized features as well as the alignment features to predict the entailment relation. The proposed labeled alignment in this paper, however, can natively process these phenomena, e.g., Fig. (2g) solves negations and (2h) solves modalities. (Sammons et al., 2010) argues that a single label (whether entailment or not) is insufficient to effectively evaluate the performance of RTE system as well as to guide researchers. They raise a group of detailed entailment phenomena such as simple rewriting rules, lexical relations and passiveactive transform, as well as a group of detailed • A labeled alignment scheme is proposed for RTE; • An RTE data set annotated with the proposed scheme is released; • High prediction accuracies are achieved on two RTE data sets. 2 Related Work RTE has attracted extensive attention in the past decade, and a wide range of approa"
I13-1069,D09-1082,0,0.0245832,"200). H. Zhao is supported by the National Natural Science Foundation of China (Grant No. 60903119 and Grant No. 61170114). 1 PASCAL is the European Commission’s ICT-funded Network of Excellence for Cognitive Systems, Interaction & Robotics. 605 International Joint Conference on Natural Language Processing, pages 605–613, Nagoya, Japan, 14-18 October 2013. multiple similarity measures including lexical similarities (Watanabe et al., 2012),edit distance (Rios and Gelbukh, 2012), measurements from MT (Volokh and Neumann, 2011), syntactic tree similarity (Mehdad, 2009) and dependency similarity (Wang and Zhang, 2009). Transformbased methods take entailment as finding a credible transform from the premise to the hypothesis (Kouylekov et al., 2011). (MacCartney et al., 2008) argues the alignment techniques and tools for MT such as GIZA++ (Och and Ney, 2003) do not readily transfer to RTE. They compare the alignment for RTE with that for MT, and state the following differences: relation can be correctly predicted through recognizing “read into” → “interpreted”2 and “what he wanted” → “in his own way”. However, the alignment developed in MT does not solve the non-alignment samples well. It usually links the w"
I13-1069,N06-1006,0,0.0581277,"Missing"
I13-1069,D08-1084,0,0.0917934,"ion’s ICT-funded Network of Excellence for Cognitive Systems, Interaction & Robotics. 605 International Joint Conference on Natural Language Processing, pages 605–613, Nagoya, Japan, 14-18 October 2013. multiple similarity measures including lexical similarities (Watanabe et al., 2012),edit distance (Rios and Gelbukh, 2012), measurements from MT (Volokh and Neumann, 2011), syntactic tree similarity (Mehdad, 2009) and dependency similarity (Wang and Zhang, 2009). Transformbased methods take entailment as finding a credible transform from the premise to the hypothesis (Kouylekov et al., 2011). (MacCartney et al., 2008) argues the alignment techniques and tools for MT such as GIZA++ (Och and Ney, 2003) do not readily transfer to RTE. They compare the alignment for RTE with that for MT, and state the following differences: relation can be correctly predicted through recognizing “read into” → “interpreted”2 and “what he wanted” → “in his own way”. However, the alignment developed in MT does not solve the non-alignment samples well. It usually links the words in H, which have no counterparts in P , to NULL regardless their impacts on the entailment relation. For example, in Fig. (1b), “ferry sinking”, “cause” a"
I13-1069,W05-1201,0,0.0313646,"agan et al., 2006). Similarly, in Machine Translation (MT) evaluation, a correct translation should be semantically equivalent to the gold translation, that is, both translations should entail each other (Pad´o et al., 2009). RTE has attracted extensive attention ever since it was proposed. A wide range of methods have been proposed, and quite a few successful approaches treat RTE as an alignment problem. Alignment is originally developed for MT to bridge two languages (Brown et al., 1993). Alignment is to establish links between the semantically equivalent atom expressions in two sentences. (Marsi and Krahmer, 2005) first advocates pipelined system architectures that contain distinct alignment components. This latter becomes a strategy crucial to the top-performing systems of (Hickl et al., 2006). In addition, human-generated alignment annotations for the second PASCAL 1 RTE challenge is released by Microsoft Research to facilitate related research (Brockett, 2007). The principle of the existing alignment-based RTE methods is that a sufficiently good alignment between P and H means a close lexical and structural correspondence, thus P probably entails H. For example, Fig. (1a) shows that the entailment R"
I13-1069,P09-2073,0,0.0199604,"Shanghai Municipality (Grant No. 13511500200). H. Zhao is supported by the National Natural Science Foundation of China (Grant No. 60903119 and Grant No. 61170114). 1 PASCAL is the European Commission’s ICT-funded Network of Excellence for Cognitive Systems, Interaction & Robotics. 605 International Joint Conference on Natural Language Processing, pages 605–613, Nagoya, Japan, 14-18 October 2013. multiple similarity measures including lexical similarities (Watanabe et al., 2012),edit distance (Rios and Gelbukh, 2012), measurements from MT (Volokh and Neumann, 2011), syntactic tree similarity (Mehdad, 2009) and dependency similarity (Wang and Zhang, 2009). Transformbased methods take entailment as finding a credible transform from the premise to the hypothesis (Kouylekov et al., 2011). (MacCartney et al., 2008) argues the alignment techniques and tools for MT such as GIZA++ (Och and Ney, 2003) do not readily transfer to RTE. They compare the alignment for RTE with that for MT, and state the following differences: relation can be correctly predicted through recognizing “read into” → “interpreted”2 and “what he wanted” → “in his own way”. However, the alignment developed in MT does not solve the n"
I13-1069,J03-1002,0,0.00804781,"International Joint Conference on Natural Language Processing, pages 605–613, Nagoya, Japan, 14-18 October 2013. multiple similarity measures including lexical similarities (Watanabe et al., 2012),edit distance (Rios and Gelbukh, 2012), measurements from MT (Volokh and Neumann, 2011), syntactic tree similarity (Mehdad, 2009) and dependency similarity (Wang and Zhang, 2009). Transformbased methods take entailment as finding a credible transform from the premise to the hypothesis (Kouylekov et al., 2011). (MacCartney et al., 2008) argues the alignment techniques and tools for MT such as GIZA++ (Och and Ney, 2003) do not readily transfer to RTE. They compare the alignment for RTE with that for MT, and state the following differences: relation can be correctly predicted through recognizing “read into” → “interpreted”2 and “what he wanted” → “in his own way”. However, the alignment developed in MT does not solve the non-alignment samples well. It usually links the words in H, which have no counterparts in P , to NULL regardless their impacts on the entailment relation. For example, in Fig. (1b), “ferry sinking”, “cause” and “that” are all linked to NULL3 , while only “ferry sinking” is the cause for non-"
I13-1069,I08-4017,1,0.884109,"Missing"
I13-1069,P09-1034,0,0.062609,"Missing"
I13-1069,W06-0127,1,0.803924,"Missing"
I13-1170,N03-1020,0,0.198867,"Missing"
I13-1170,W04-1013,0,0.0550008,"Missing"
I13-1170,P02-1040,0,0.0967493,"Missing"
I13-1170,yang-etal-2012-spell,1,0.824458,"Missing"
I13-1170,Y06-1012,1,0.786988,"Missing"
I13-1170,P11-2085,0,0.172077,"Missing"
I13-1170,P00-1031,0,\N,Missing
K15-2005,P09-2004,0,0.0525303,"n it may become much more important. ‘Arg1’ is shown in italic, and ‘Arg2’ is shown in bold. The discourse connective is underlined and the sense of this explicit relation is ‘Comparison.Concession’. 2.1.1 Explicit Classifier There are 100 explicit connectives in PDTB annotation (Prasad et al., 2008). However, some connectives, e.g., ‘and’, do not express a discourse relation. We use a level-order traverse to scan every node in the constituent parse tree to select the connective candidates. This method gives us a high recall in the train set as shown in Table 1. Seven features are considered (Pitler and Nenkova, 2009): a) Self Category The highest dominated node which covers the connective. b) Parent Caterogy The category of the parent of the self category. c) Left Sibling Category The syntactic category of immediate left sibling of the self-category. It would be ‘NONE’ if the connective is the leftmost node. d) Right Sibling Category The immediate right sibling of the self category. It also would be assigned ‘NONE’ if the self-category has been the rightmost node. e) VP Existence We set a binary feature to indicate whether the right sibling contains a VP. f) Connective In addition to those features propos"
K15-2005,P09-1077,0,0.53987,"Missing"
K15-2005,prasad-etal-2008-penn,0,0.197859,"ing of the connective. c) Con-iLSib Number of left sibling of the connective. d) Con-iRSib Number of right sibling of the connective. The second part consists of features from the He added that ”having just one firm do this isn’t going to mean a hill of beans. But if this prompts others to consider the same thing, then it may become much more important. ‘Arg1’ is shown in italic, and ‘Arg2’ is shown in bold. The discourse connective is underlined and the sense of this explicit relation is ‘Comparison.Concession’. 2.1.1 Explicit Classifier There are 100 explicit connectives in PDTB annotation (Prasad et al., 2008). However, some connectives, e.g., ‘and’, do not express a discourse relation. We use a level-order traverse to scan every node in the constituent parse tree to select the connective candidates. This method gives us a high recall in the train set as shown in Table 1. Seven features are considered (Pitler and Nenkova, 2009): a) Self Category The highest dominated node which covers the connective. b) Parent Caterogy The category of the parent of the self category. c) Left Sibling Category The syntactic category of immediate left sibling of the self-category. It would be ‘NONE’ if the connective"
K15-2005,W14-1710,1,0.878194,"Missing"
K15-2005,W08-2127,1,0.909238,"Missing"
K15-2005,W09-1209,1,0.887273,"Missing"
K15-2005,C10-2172,0,0.783895,"Missing"
K15-2005,D14-1008,0,\N,Missing
K15-2005,W13-3610,1,\N,Missing
K16-2008,P09-1077,0,0.286232,"showed maximum entropy classifier performed well in relative tasks, so we apply it to our classification problem∗ . According to official evaluation, F1 score of this part in our system is 0.9905 on the dev set and 0.9838 on the blind test set, comparing to 0.9514 and 0.9186, the best result of CoNLL-2015. The detailed results are shown in Table 1. Our system consists of six parts, and the general workflow refers to the shallow discourse parser based on the constituent parse tree (Chen et al., 2015). Feature extraction for training follows previous works (Kong et al., 2014; Lin et al., 2014; Pitler et al., 2009; Pitler and Nenkova, 2009). We deduce each sentence into a constituent parse tree. Relative information is extracted from these constituent parse trees to train models and predict discourse relations. PDTB Connective classifier Explicit parser Explicit argument labeler Explicit sense classifier Filter Filter Non-explicit argument labeler Nonexplicit Parser Non-explicit sense classifier 3.1.1 P R F dev 0.9971 0.9840 0.9905 test 0.9967 0.9819 0.9892 blind 0.9856 0.9821 0.9838 Table 1: Official scores of connective classifier From the comparison, we can learn that (1)From the constituent parse t"
K16-2008,W09-1208,1,0.888348,"Missing"
K16-2008,K15-2002,0,0.0931103,"Missing"
K16-2008,E09-1100,1,0.879273,"Missing"
K16-2008,W14-1710,1,0.897414,"Missing"
K16-2008,D14-1023,1,0.913411,"Missing"
K16-2008,K16-2001,0,0.106317,"Missing"
K16-2008,W08-2127,1,0.729041,"ious word of the connective and the connective itself. (3) PrevPos The category of the previous word of the connective. (4) PrevPosConnPos The category of previous word and category of the connective. (5) ConnNext The connective itself and the next word of the connective. (6) NextPos The category of the next word of the connective. (7) ConnPosNextPos The category of the connective itself and category of the next word. After extracting the mentioned feature for each connective, we annotate it as 1 or 0 according to whether this word in PDTB functions as discourse connective. (Jia et al., 2013; Zhao and Kit, 2008) showed maximum entropy classifier performed well in relative tasks, so we apply it to our classification problem∗ . According to official evaluation, F1 score of this part in our system is 0.9905 on the dev set and 0.9838 on the blind test set, comparing to 0.9514 and 0.9186, the best result of CoNLL-2015. The detailed results are shown in Table 1. Our system consists of six parts, and the general workflow refers to the shallow discourse parser based on the constituent parse tree (Chen et al., 2015). Feature extraction for training follows previous works (Kong et al., 2014; Lin et al., 2014;"
K16-2008,D09-1004,1,0.884329,"Missing"
K16-2008,P09-1007,1,\N,Missing
K16-2008,prasad-etal-2008-penn,0,\N,Missing
K16-2008,P09-2004,0,\N,Missing
K16-2008,K15-2005,1,\N,Missing
K16-2008,K15-2004,0,\N,Missing
K16-2008,D14-1008,0,\N,Missing
K16-2008,D13-1082,1,\N,Missing
K16-2008,K16-2010,1,\N,Missing
K16-2010,K15-2011,0,0.0190555,"ourse connectives which provide strong indications, the NonExplicit relations between adjacent sentences are difficult to figure out. Therefore, our primary work is to improve sense classification components, especially on Non-Explicit relations. For other components such as connectives detection and arguments extraction, we just follow the top ranked system (Wang and Lan, 2015) in CoNLL-2015, which is as the baseline system in this paper. In CoNLL-2015, various approaches were explored to conquer the sense classification problem, which is a straightforward multi-category classification task (Okita et al., 2015; Wang and Lan, 2015; Chiarcos and Schenk, 2015; Song et al., 2015; Stepanov et al., 2015; Yoshida et al., 2015; Sun et al., 2015; Nguyen et al., 2015; Laali et al., 2015). Typical data-driven machine learning methods, like Maximum Entropy and Support Vector Machine, were adopted. Some of them selected lexical and syntactic features over the arguments, including linguistically motivated word groupings such as Levin verb classes and polarity tags. Brown cluster features, surface features and entity semantics were also effective to enhance sense classification. Additionally, paragraph embeddings"
K16-2010,P16-1039,1,0.221514,"jor Basic Research Program of Shanghai Science and Technology Committee (No. 15JC1400103), Art and Science Interdisciplinary Funds of Shanghai Jiao Tong University (No. 14JCRZ04), and Key Project of National Society Science Foundation of China (No. 15-ZDA041). 70 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 70–77, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics than traditional linear models with hand-crafted sparse features for some Nature Language Process (NLP) tasks (Wang et al., 2013; Wang et al., 2014; Cai and Zhao, 2016; Wang et al., 2016; Zhang and Zhao, 2016), such as sentence modeling (Kalchbrenner et al., 2014; Kim, 2014). In Non-Explicit sense classification, due to the absence of discourse connectives, the task is exactly to classify a sentence pair, where CNN could be utilized. For Explicit sense classification which has strong discourse relation information provided by the connectives, we will use traditional linear methods with novel dependency features. The rest of the paper is organized as follows: Section 2 briefly describes our system, Section 3 introduces the CNN model for modeling sentence pai"
K16-2010,K15-2005,1,0.424054,"al., 2015; Nguyen et al., 2015; Laali et al., 2015). Typical data-driven machine learning methods, like Maximum Entropy and Support Vector Machine, were adopted. Some of them selected lexical and syntactic features over the arguments, including linguistically motivated word groupings such as Levin verb classes and polarity tags. Brown cluster features, surface features and entity semantics were also effective to enhance sense classification. Additionally, paragraph embeddings were also used to determine the senses (Okita et al., 2015). In other previous work of implicit sense classification, Chen et al (2015) used word-pair features for predicting missing connectives, Zhou et al. (2010) attempted to insert discourse connectives between arguments with the use of a language model, Lin et al. (2009) applied various feature selection methods. Although traditional methods have performed well on semantic tasks through feature engineering (Zhao et al., 2009a; Zhao et al., 2009b; Zhao et al., 2013), they still suffer from data sparsity problems. Recently, Neural Network (NN) methods have shown competitive or even better performance This paper describes a discourse parsing system for our participation in t"
K16-2010,C08-2022,0,0.105076,"Missing"
K16-2010,K15-2006,0,0.0169712,"ndications, the NonExplicit relations between adjacent sentences are difficult to figure out. Therefore, our primary work is to improve sense classification components, especially on Non-Explicit relations. For other components such as connectives detection and arguments extraction, we just follow the top ranked system (Wang and Lan, 2015) in CoNLL-2015, which is as the baseline system in this paper. In CoNLL-2015, various approaches were explored to conquer the sense classification problem, which is a straightforward multi-category classification task (Okita et al., 2015; Wang and Lan, 2015; Chiarcos and Schenk, 2015; Song et al., 2015; Stepanov et al., 2015; Yoshida et al., 2015; Sun et al., 2015; Nguyen et al., 2015; Laali et al., 2015). Typical data-driven machine learning methods, like Maximum Entropy and Support Vector Machine, were adopted. Some of them selected lexical and syntactic features over the arguments, including linguistically motivated word groupings such as Levin verb classes and polarity tags. Brown cluster features, surface features and entity semantics were also effective to enhance sense classification. Additionally, paragraph embeddings were also used to determine the senses (Okita"
K16-2010,prasad-etal-2008-penn,0,0.291604,"Missing"
K16-2010,P14-1062,0,0.0372578,", Art and Science Interdisciplinary Funds of Shanghai Jiao Tong University (No. 14JCRZ04), and Key Project of National Society Science Foundation of China (No. 15-ZDA041). 70 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 70–77, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics than traditional linear models with hand-crafted sparse features for some Nature Language Process (NLP) tasks (Wang et al., 2013; Wang et al., 2014; Cai and Zhao, 2016; Wang et al., 2016; Zhang and Zhao, 2016), such as sentence modeling (Kalchbrenner et al., 2014; Kim, 2014). In Non-Explicit sense classification, due to the absence of discourse connectives, the task is exactly to classify a sentence pair, where CNN could be utilized. For Explicit sense classification which has strong discourse relation information provided by the connectives, we will use traditional linear methods with novel dependency features. The rest of the paper is organized as follows: Section 2 briefly describes our system, Section 3 introduces the CNN model for modeling sentence pairs, Section 4 discusses our main works including Explicit sense classification and Non-Explicit"
K16-2010,K15-2012,0,0.0204144,"relations between adjacent sentences are difficult to figure out. Therefore, our primary work is to improve sense classification components, especially on Non-Explicit relations. For other components such as connectives detection and arguments extraction, we just follow the top ranked system (Wang and Lan, 2015) in CoNLL-2015, which is as the baseline system in this paper. In CoNLL-2015, various approaches were explored to conquer the sense classification problem, which is a straightforward multi-category classification task (Okita et al., 2015; Wang and Lan, 2015; Chiarcos and Schenk, 2015; Song et al., 2015; Stepanov et al., 2015; Yoshida et al., 2015; Sun et al., 2015; Nguyen et al., 2015; Laali et al., 2015). Typical data-driven machine learning methods, like Maximum Entropy and Support Vector Machine, were adopted. Some of them selected lexical and syntactic features over the arguments, including linguistically motivated word groupings such as Levin verb classes and polarity tags. Brown cluster features, surface features and entity semantics were also effective to enhance sense classification. Additionally, paragraph embeddings were also used to determine the senses (Okita et al., 2015). In o"
K16-2010,D14-1181,0,0.0315772,"Missing"
K16-2010,K15-2008,0,0.0546855,"Missing"
K16-2010,D09-1036,0,0.149041,"Missing"
K16-2010,K15-2013,0,0.0207869,"t. Therefore, our primary work is to improve sense classification components, especially on Non-Explicit relations. For other components such as connectives detection and arguments extraction, we just follow the top ranked system (Wang and Lan, 2015) in CoNLL-2015, which is as the baseline system in this paper. In CoNLL-2015, various approaches were explored to conquer the sense classification problem, which is a straightforward multi-category classification task (Okita et al., 2015; Wang and Lan, 2015; Chiarcos and Schenk, 2015; Song et al., 2015; Stepanov et al., 2015; Yoshida et al., 2015; Sun et al., 2015; Nguyen et al., 2015; Laali et al., 2015). Typical data-driven machine learning methods, like Maximum Entropy and Support Vector Machine, were adopted. Some of them selected lexical and syntactic features over the arguments, including linguistically motivated word groupings such as Levin verb classes and polarity tags. Brown cluster features, surface features and entity semantics were also effective to enhance sense classification. Additionally, paragraph embeddings were also used to determine the senses (Okita et al., 2015). In other previous work of implicit sense classification, Chen et al"
K16-2010,K15-2002,0,0.15812,"Shanghai Education Commission for Intelligent Interaction and Cognitive Engineering, Shanghai Jiao Tong University, Shanghai, 200240, China {qinlianhui, zzs2011}@sjtu.edu.cn,zhaohai@cs.sjtu.edu.cn Abstract parsing system. Without the discourse connectives which provide strong indications, the NonExplicit relations between adjacent sentences are difficult to figure out. Therefore, our primary work is to improve sense classification components, especially on Non-Explicit relations. For other components such as connectives detection and arguments extraction, we just follow the top ranked system (Wang and Lan, 2015) in CoNLL-2015, which is as the baseline system in this paper. In CoNLL-2015, various approaches were explored to conquer the sense classification problem, which is a straightforward multi-category classification task (Okita et al., 2015; Wang and Lan, 2015; Chiarcos and Schenk, 2015; Song et al., 2015; Stepanov et al., 2015; Yoshida et al., 2015; Sun et al., 2015; Nguyen et al., 2015; Laali et al., 2015). Typical data-driven machine learning methods, like Maximum Entropy and Support Vector Machine, were adopted. Some of them selected lexical and syntactic features over the arguments, includin"
K16-2010,K15-2010,0,0.0467772,"primary work is to improve sense classification components, especially on Non-Explicit relations. For other components such as connectives detection and arguments extraction, we just follow the top ranked system (Wang and Lan, 2015) in CoNLL-2015, which is as the baseline system in this paper. In CoNLL-2015, various approaches were explored to conquer the sense classification problem, which is a straightforward multi-category classification task (Okita et al., 2015; Wang and Lan, 2015; Chiarcos and Schenk, 2015; Song et al., 2015; Stepanov et al., 2015; Yoshida et al., 2015; Sun et al., 2015; Nguyen et al., 2015; Laali et al., 2015). Typical data-driven machine learning methods, like Maximum Entropy and Support Vector Machine, were adopted. Some of them selected lexical and syntactic features over the arguments, including linguistically motivated word groupings such as Levin verb classes and polarity tags. Brown cluster features, surface features and entity semantics were also effective to enhance sense classification. Additionally, paragraph embeddings were also used to determine the senses (Okita et al., 2015). In other previous work of implicit sense classification, Chen et al (2015) used word-pai"
K16-2010,C10-2172,0,0.319746,"ine learning methods, like Maximum Entropy and Support Vector Machine, were adopted. Some of them selected lexical and syntactic features over the arguments, including linguistically motivated word groupings such as Levin verb classes and polarity tags. Brown cluster features, surface features and entity semantics were also effective to enhance sense classification. Additionally, paragraph embeddings were also used to determine the senses (Okita et al., 2015). In other previous work of implicit sense classification, Chen et al (2015) used word-pair features for predicting missing connectives, Zhou et al. (2010) attempted to insert discourse connectives between arguments with the use of a language model, Lin et al. (2009) applied various feature selection methods. Although traditional methods have performed well on semantic tasks through feature engineering (Zhao et al., 2009a; Zhao et al., 2009b; Zhao et al., 2013), they still suffer from data sparsity problems. Recently, Neural Network (NN) methods have shown competitive or even better performance This paper describes a discourse parsing system for our participation in the CoNLL 2016 Shared Task. We focus on the supplementary task: Sense Classifica"
K16-2010,D14-1023,1,0.852485,". 2013CB329401), Major Basic Research Program of Shanghai Science and Technology Committee (No. 15JC1400103), Art and Science Interdisciplinary Funds of Shanghai Jiao Tong University (No. 14JCRZ04), and Key Project of National Society Science Foundation of China (No. 15-ZDA041). 70 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 70–77, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics than traditional linear models with hand-crafted sparse features for some Nature Language Process (NLP) tasks (Wang et al., 2013; Wang et al., 2014; Cai and Zhao, 2016; Wang et al., 2016; Zhang and Zhao, 2016), such as sentence modeling (Kalchbrenner et al., 2014; Kim, 2014). In Non-Explicit sense classification, due to the absence of discourse connectives, the task is exactly to classify a sentence pair, where CNN could be utilized. For Explicit sense classification which has strong discourse relation information provided by the connectives, we will use traditional linear methods with novel dependency features. The rest of the paper is organized as follows: Section 2 briefly describes our system, Section 3 introduces the CNN model for m"
K16-2010,N16-1064,1,0.311912,"rogram of Shanghai Science and Technology Committee (No. 15JC1400103), Art and Science Interdisciplinary Funds of Shanghai Jiao Tong University (No. 14JCRZ04), and Key Project of National Society Science Foundation of China (No. 15-ZDA041). 70 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 70–77, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics than traditional linear models with hand-crafted sparse features for some Nature Language Process (NLP) tasks (Wang et al., 2013; Wang et al., 2014; Cai and Zhao, 2016; Wang et al., 2016; Zhang and Zhao, 2016), such as sentence modeling (Kalchbrenner et al., 2014; Kim, 2014). In Non-Explicit sense classification, due to the absence of discourse connectives, the task is exactly to classify a sentence pair, where CNN could be utilized. For Explicit sense classification which has strong discourse relation information provided by the connectives, we will use traditional linear methods with novel dependency features. The rest of the paper is organized as follows: Section 2 briefly describes our system, Section 3 introduces the CNN model for modeling sentence pairs, Section 4 discu"
K16-2010,K15-2001,0,0.288695,"plore a traditional linear model with novel dependency features for Explicit sense classification. Compared with the best system in CoNLL-2015, our system achieves competitive performances. Moreover, as shown in the results, our system has higher F1 score on Non-Explicit sense classification. 1 Introduction This paper presents the Shanghai Jiao Tong University discourse parsing system for the CoNLL 2016 Shared Task (Xue et al., 2016) on Shallow Discourse Parsing and the supplementary tasks of sense classification for English and Chinese. As shown by the results of the same task in CoNLL 2015 (Xue et al., 2015), sense classification has been found more difficult than other subtasks, especially determining Non-Explicit senses which is the bottleneck of the end-to-end discourse ∗ Corresponding author. This paper was partially supported by Cai Yuanpei Program (CSC No. 201304490199 and No. 201304490171), National Natural Science Foundation of China (No. 61170114 and No. 61272248), National Basic Research Program of China (No. 2013CB329401), Major Basic Research Program of Shanghai Science and Technology Committee (No. 15JC1400103), Art and Science Interdisciplinary Funds of Shanghai Jiao Tong University"
K16-2010,K16-2001,0,0.116252,"g system. To improve Non-Explicit sense classification, we propose a Convolutional Neural Network (CNN) model to determine the senses for both English and Chinese tasks. We also explore a traditional linear model with novel dependency features for Explicit sense classification. Compared with the best system in CoNLL-2015, our system achieves competitive performances. Moreover, as shown in the results, our system has higher F1 score on Non-Explicit sense classification. 1 Introduction This paper presents the Shanghai Jiao Tong University discourse parsing system for the CoNLL 2016 Shared Task (Xue et al., 2016) on Shallow Discourse Parsing and the supplementary tasks of sense classification for English and Chinese. As shown by the results of the same task in CoNLL 2015 (Xue et al., 2015), sense classification has been found more difficult than other subtasks, especially determining Non-Explicit senses which is the bottleneck of the end-to-end discourse ∗ Corresponding author. This paper was partially supported by Cai Yuanpei Program (CSC No. 201304490199 and No. 201304490171), National Natural Science Foundation of China (No. 61170114 and No. 61272248), National Basic Research Program of China (No."
K16-2010,K15-2015,0,0.020374,"difficult to figure out. Therefore, our primary work is to improve sense classification components, especially on Non-Explicit relations. For other components such as connectives detection and arguments extraction, we just follow the top ranked system (Wang and Lan, 2015) in CoNLL-2015, which is as the baseline system in this paper. In CoNLL-2015, various approaches were explored to conquer the sense classification problem, which is a straightforward multi-category classification task (Okita et al., 2015; Wang and Lan, 2015; Chiarcos and Schenk, 2015; Song et al., 2015; Stepanov et al., 2015; Yoshida et al., 2015; Sun et al., 2015; Nguyen et al., 2015; Laali et al., 2015). Typical data-driven machine learning methods, like Maximum Entropy and Support Vector Machine, were adopted. Some of them selected lexical and syntactic features over the arguments, including linguistically motivated word groupings such as Levin verb classes and polarity tags. Brown cluster features, surface features and entity semantics were also effective to enhance sense classification. Additionally, paragraph embeddings were also used to determine the senses (Okita et al., 2015). In other previous work of implicit sense classifi"
K16-2010,P16-1131,1,0.674358,"Science and Technology Committee (No. 15JC1400103), Art and Science Interdisciplinary Funds of Shanghai Jiao Tong University (No. 14JCRZ04), and Key Project of National Society Science Foundation of China (No. 15-ZDA041). 70 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 70–77, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics than traditional linear models with hand-crafted sparse features for some Nature Language Process (NLP) tasks (Wang et al., 2013; Wang et al., 2014; Cai and Zhao, 2016; Wang et al., 2016; Zhang and Zhao, 2016), such as sentence modeling (Kalchbrenner et al., 2014; Kim, 2014). In Non-Explicit sense classification, due to the absence of discourse connectives, the task is exactly to classify a sentence pair, where CNN could be utilized. For Explicit sense classification which has strong discourse relation information provided by the connectives, we will use traditional linear methods with novel dependency features. The rest of the paper is organized as follows: Section 2 briefly describes our system, Section 3 introduces the CNN model for modeling sentence pairs, Section 4 discusses our main works inc"
K16-2010,D09-1004,1,0.863581,"atures, surface features and entity semantics were also effective to enhance sense classification. Additionally, paragraph embeddings were also used to determine the senses (Okita et al., 2015). In other previous work of implicit sense classification, Chen et al (2015) used word-pair features for predicting missing connectives, Zhou et al. (2010) attempted to insert discourse connectives between arguments with the use of a language model, Lin et al. (2009) applied various feature selection methods. Although traditional methods have performed well on semantic tasks through feature engineering (Zhao et al., 2009a; Zhao et al., 2009b; Zhao et al., 2013), they still suffer from data sparsity problems. Recently, Neural Network (NN) methods have shown competitive or even better performance This paper describes a discourse parsing system for our participation in the CoNLL 2016 Shared Task. We focus on the supplementary task: Sense Classification, especially the Non-Explicit one which is the bottleneck of discourse parsing system. To improve Non-Explicit sense classification, we propose a Convolutional Neural Network (CNN) model to determine the senses for both English and Chinese tasks. We also explore a"
K16-2010,W09-1208,1,0.804105,"atures, surface features and entity semantics were also effective to enhance sense classification. Additionally, paragraph embeddings were also used to determine the senses (Okita et al., 2015). In other previous work of implicit sense classification, Chen et al (2015) used word-pair features for predicting missing connectives, Zhou et al. (2010) attempted to insert discourse connectives between arguments with the use of a language model, Lin et al. (2009) applied various feature selection methods. Although traditional methods have performed well on semantic tasks through feature engineering (Zhao et al., 2009a; Zhao et al., 2009b; Zhao et al., 2013), they still suffer from data sparsity problems. Recently, Neural Network (NN) methods have shown competitive or even better performance This paper describes a discourse parsing system for our participation in the CoNLL 2016 Shared Task. We focus on the supplementary task: Sense Classification, especially the Non-Explicit one which is the bottleneck of discourse parsing system. To improve Non-Explicit sense classification, we propose a Convolutional Neural Network (CNN) model to determine the senses for both English and Chinese tasks. We also explore a"
K16-2010,D13-1082,1,\N,Missing
K16-2010,K15-2003,0,\N,Missing
K17-3020,P16-1231,0,0.107565,"Missing"
K17-3020,D14-1082,0,0.073012,"Missing"
K17-3020,W02-1001,0,0.0202115,"rst two types which we refer to as Known Language, they will be dealt by the Known Language Parser, while the Surprise Language Parser will dispose with the surprise languages. 3.2.2 Tagger In the pipeline of dealing known languages, the second step is to provide several light-weighted syntactical and morphological features for the tokenized texts, which will be utilized as the input features in the final parsing step. In our system, we adopt the tagger in UDPipe, whose tagging method is based on MorphoDita (Strakov´a et al., 2014) and the training method is the classical Averaged Perceptron (Collins, 2002), and the training parameters of UDPipe Tagger are provided in Table 2. In this step, the tagger will provide the following outputs: As for Parallel Test Set, we use its corresponding treebank without specific domain in treebank name.3 3 It may be better if we use the whole treebanks of corresponding language 193 Parameter Name iteration hidden layer batch size learning rate dimension of upostag dimension of feats dimension of xpostag dimension of form dimension of deprel 1. Lemma: Lemma or stem of word forms. 2. UPOS: Universal POS tags. 3. XPOS: Language-specific POS tags. 4. FEATS: Morpholo"
K17-3020,P15-1033,0,0.0625928,"Missing"
K17-3020,C96-1058,0,0.668903,"Missing"
K17-3020,P05-1012,0,0.15038,"n macro-averaged LAS F1-score. 1 In this paper, we describe the system of team Wanghao-ftd-SJTU for the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies (Zeman et al., 2017). For this task, we only use provided treebanks to train models without any other resources including pretrained embeddings. For dependency parsing, there have been two major parsing methods: graph-based and transition-based. The former searches for the final tree through graph algorithms by decomposing trees into factors, utilizing ingenious dynamic programming algorithms (Eisner, 1996; McDonald et al., 2005; McDonald and Pereira, 2006); while the latter parses sentences by making a series of shift-reduce decisions (Yamada and Matsumoto, 2003; Nivre, 2003). In our system, we will utilize the transition-based system for its simplicity and relatively lower computation cost. Introduction Universal Dependencies (UD) (Nivre et al., 2016, 2017b) and universal dependency parsing take efforts to build cross-linguistically treebank annotation and develop cross-lingual learning to parse many languages even low-resource languages. Universal Dependencies release 2.02 (Nivre et al., 2017b) includes rich langu"
K17-3020,W03-3017,0,0.193869,"xt to Universal Dependencies (Zeman et al., 2017). For this task, we only use provided treebanks to train models without any other resources including pretrained embeddings. For dependency parsing, there have been two major parsing methods: graph-based and transition-based. The former searches for the final tree through graph algorithms by decomposing trees into factors, utilizing ingenious dynamic programming algorithms (Eisner, 1996; McDonald et al., 2005; McDonald and Pereira, 2006); while the latter parses sentences by making a series of shift-reduce decisions (Yamada and Matsumoto, 2003; Nivre, 2003). In our system, we will utilize the transition-based system for its simplicity and relatively lower computation cost. Introduction Universal Dependencies (UD) (Nivre et al., 2016, 2017b) and universal dependency parsing take efforts to build cross-linguistically treebank annotation and develop cross-lingual learning to parse many languages even low-resource languages. Universal Dependencies release 2.02 (Nivre et al., 2017b) includes rich languages and treebanks resources and the parsing task in CoNLL 2017 is Transition-based dependency parsing takes linear time complexity and utilizes rich f"
K17-3020,L16-1680,0,0.0986402,"Missing"
K17-3020,J08-4003,0,0.044329,"languages. Universal Dependencies release 2.02 (Nivre et al., 2017b) includes rich languages and treebanks resources and the parsing task in CoNLL 2017 is Transition-based dependency parsing takes linear time complexity and utilizes rich features to make structural prediction (Zhang and Clark, 2008; Zhang and Nivre, 2011). Specifically, a buffer for input words, a stack for partially built structure and shift-reduce actions are basic elements in a transition-based dependency parsing. For the transition systems of dependency parsing, there have been two major ones: arc-standard and arc-eager (Nivre, 2008). Our system adopts the former, whose basic algorithm can be described as ∗ Correspondence author. This paper was partially supported by Cai Yuanpei Program (CSC No. 201304490199 and No. 201304490171), National Natural Science Foundation of China (No. 61170114, No. 61672343 and No. 61272248), National Basic Research Program of China (No. 2013CB329401), Major Basic Research Program of Shanghai Science and Technology Committee (No. 15JC1400103), Art and Science Interdisciplinary Funds of Shanghai Jiao Tong University (No. 14JCRZ04), Key Project of National Society Science Foundation of China (No"
K17-3020,P14-5003,0,0.0847773,"Missing"
K17-3020,P16-1218,0,0.0593645,"Missing"
K17-3020,P15-1032,0,0.0474639,"Missing"
K17-3020,W03-3023,0,0.199207,"ilingual Parsing from Raw Text to Universal Dependencies (Zeman et al., 2017). For this task, we only use provided treebanks to train models without any other resources including pretrained embeddings. For dependency parsing, there have been two major parsing methods: graph-based and transition-based. The former searches for the final tree through graph algorithms by decomposing trees into factors, utilizing ingenious dynamic programming algorithms (Eisner, 1996; McDonald et al., 2005; McDonald and Pereira, 2006); while the latter parses sentences by making a series of shift-reduce decisions (Yamada and Matsumoto, 2003; Nivre, 2003). In our system, we will utilize the transition-based system for its simplicity and relatively lower computation cost. Introduction Universal Dependencies (UD) (Nivre et al., 2016, 2017b) and universal dependency parsing take efforts to build cross-linguistically treebank annotation and develop cross-lingual learning to parse many languages even low-resource languages. Universal Dependencies release 2.02 (Nivre et al., 2017b) includes rich languages and treebanks resources and the parsing task in CoNLL 2017 is Transition-based dependency parsing takes linear time complexity and u"
K17-3020,P15-1031,0,0.0242118,"Missing"
K17-3020,K17-3001,0,0.0416656,"Missing"
K17-3020,I08-3008,0,0.064489,"uages and corresponding source languages. method could employ rich features effectively. In our system, we use the linguistic features generated by previous taggers, including lemma, POS tags and morphological features as described in Section 3.2.2. The parameters for the parser training are shown in Table 3. 3.3 Surprise Language Parser This sub-system deals with the surprise languages without enough training data. We use a simple delexicalized and cross-lingual method, that is, parsing these low resource languages based on the models learned from other languages. This follows the method of (Zeman and Resnik, 2008), which shows that transfer learning for another language based on delexicalized parser can perform well. Although different languages may have different word forms, the underlying syntactic information could overlap and the universal POS tags could be utilized to explore the correlations. To achieve this, we train a dependency parser in a close-relation language (source language) for a surprise language, and then feed the delexicalized POS tag sequence of the surprise language to the source language parser. We consider language family and close area to find the source language for surprise la"
K17-3020,D08-1059,0,0.0606357,"ansition-based system for its simplicity and relatively lower computation cost. Introduction Universal Dependencies (UD) (Nivre et al., 2016, 2017b) and universal dependency parsing take efforts to build cross-linguistically treebank annotation and develop cross-lingual learning to parse many languages even low-resource languages. Universal Dependencies release 2.02 (Nivre et al., 2017b) includes rich languages and treebanks resources and the parsing task in CoNLL 2017 is Transition-based dependency parsing takes linear time complexity and utilizes rich features to make structural prediction (Zhang and Clark, 2008; Zhang and Nivre, 2011). Specifically, a buffer for input words, a stack for partially built structure and shift-reduce actions are basic elements in a transition-based dependency parsing. For the transition systems of dependency parsing, there have been two major ones: arc-standard and arc-eager (Nivre, 2008). Our system adopts the former, whose basic algorithm can be described as ∗ Correspondence author. This paper was partially supported by Cai Yuanpei Program (CSC No. 201304490199 and No. 201304490171), National Natural Science Foundation of China (No. 61170114, No. 61672343 and No. 61272"
K17-3020,P11-2033,0,0.0478588,"or its simplicity and relatively lower computation cost. Introduction Universal Dependencies (UD) (Nivre et al., 2016, 2017b) and universal dependency parsing take efforts to build cross-linguistically treebank annotation and develop cross-lingual learning to parse many languages even low-resource languages. Universal Dependencies release 2.02 (Nivre et al., 2017b) includes rich languages and treebanks resources and the parsing task in CoNLL 2017 is Transition-based dependency parsing takes linear time complexity and utilizes rich features to make structural prediction (Zhang and Clark, 2008; Zhang and Nivre, 2011). Specifically, a buffer for input words, a stack for partially built structure and shift-reduce actions are basic elements in a transition-based dependency parsing. For the transition systems of dependency parsing, there have been two major ones: arc-standard and arc-eager (Nivre, 2008). Our system adopts the former, whose basic algorithm can be described as ∗ Correspondence author. This paper was partially supported by Cai Yuanpei Program (CSC No. 201304490199 and No. 201304490171), National Natural Science Foundation of China (No. 61170114, No. 61672343 and No. 61272248), National Basic Res"
K17-3020,E06-1011,0,\N,Missing
K17-3020,W06-2920,0,\N,Missing
K17-3020,L16-1262,0,\N,Missing
K18-2006,P10-1001,0,0.0167105,"in more than one valid selection for each time step, 67 Treebank Breton KEB Czech PUD English PUD Faroese OFT Sampling English, Irish Czech PDT English EWT Norwegian, English, Danish, Swedish, German, Dutch Finnish PUD Finnish TDT Japanese Modern Japanese GSD Naija NSC English Swedish PUD Swedish Talbanken Thai PUD English, Chinese, Hindi, Vietnamese which might confuse the decoder. To address this problem, the parser introduces an inside-outside order to utilize second-order sibling information, which has been proven to be an important feature for parsing process (McDonald and Pereira, 2006; Koo and Collins, 2010). To utilize the secondorder information, the parser replaces the input of decoder from si as follows: βi = ss ◦ sh ◦ si where s and h indicate the sibling and head index of node i, ◦ is the element-wise sum operation to ensure no additional model parameters. 2.4 Table 1: Language substitution for treebanks without training data Loss Function The training objective of pur system is to learn the probability of UPOS tags Pθpos (ypos |x) and 0 the dependency trees Pθdep (ydep |x, ypos ). Given a sentence x, the probabilities are factorized as: Pθpos (ypos |x) = k X et al., 2016) to provide a pipe"
K18-2006,C18-1271,1,0.902643,"es for Multilingual Universal Dependency Parsing Zuchao Li1,2,∗, Shexia He1,2,∗ , Zhuosheng Zhang1,2 , Hai Zhao1,2,† 1 Department of Computer Science and Engineering, Shanghai Jiao Tong University 2 Key Laboratory of Shanghai Education Commission for Intelligent Interaction and Cognitive Engineering, Shanghai Jiao Tong University, Shanghai, China {charlee,heshexia,zhangzs}@sjtu.edu.cn, zhaohai@cs.sjtu.edu.cn Abstract Dependency parsing that aims to predict the existence and type of linguistic dependency relations between words, is a fundamental part in natural language processing (NLP) tasks (Li et al., 2018c; He et al., 2018). Many referential natural language processing studies (Zhang et al., 2018; Bai and Zhao, 2018; Cai et al., 2018; Li et al., 2018b; Wang et al., 2018; Qin et al., 2017) can also contribute to the universal dependency parsing system. Universal dependency parsing focuses on learning syntactic dependency structure over many typologically different languages, even low-resource languages in a real-world setting. Within the dependency parsing literature, there are two dominant techniques, graph-based (McDonald et al., 2005; Ma and Zhao, 2012; Kiperwasser and Goldberg, 2016; Dozat"
K18-2006,D18-1262,1,0.835564,"es for Multilingual Universal Dependency Parsing Zuchao Li1,2,∗, Shexia He1,2,∗ , Zhuosheng Zhang1,2 , Hai Zhao1,2,† 1 Department of Computer Science and Engineering, Shanghai Jiao Tong University 2 Key Laboratory of Shanghai Education Commission for Intelligent Interaction and Cognitive Engineering, Shanghai Jiao Tong University, Shanghai, China {charlee,heshexia,zhangzs}@sjtu.edu.cn, zhaohai@cs.sjtu.edu.cn Abstract Dependency parsing that aims to predict the existence and type of linguistic dependency relations between words, is a fundamental part in natural language processing (NLP) tasks (Li et al., 2018c; He et al., 2018). Many referential natural language processing studies (Zhang et al., 2018; Bai and Zhao, 2018; Cai et al., 2018; Li et al., 2018b; Wang et al., 2018; Qin et al., 2017) can also contribute to the universal dependency parsing system. Universal dependency parsing focuses on learning syntactic dependency structure over many typologically different languages, even low-resource languages in a real-world setting. Within the dependency parsing literature, there are two dominant techniques, graph-based (McDonald et al., 2005; Ma and Zhao, 2012; Kiperwasser and Goldberg, 2016; Dozat"
K18-2006,P17-1093,1,0.821108,"Missing"
K18-2006,P16-1101,0,0.0247246,"s into the representation. Therefore, we jointly predict the UPOS tag in our system. The architecture for the POS tagger in our model is almost identical to that of the parser (Dozat et al., 2017). The tagger uses a BiLSTM over the concatenation of word embeddings and character embeddings: c spos = BiLST M pos (ew i ei ) i Representation Representation is a key component in various NLP models, and good representations should ideally model both complex characteristics and linguistic contexts. In our system, we follow the bidirectional LSTM-CNN architecture (BiLSTMCNNs) (Chiu and Nichols, 2016; Ma and Hovy, 2016), where CNNs encode word information into character-level representation and BiLSTM models context information of each word. Then we calculate the probability of tag for each type using affine classifiers as follows: hpos = M LP pos (spos i i ) ripos = W pos hpos + bpos i yipos = arg max(ri ) The tag classifier is trained jointly using crossentropy losses that are summed together with the dependency parser loss during optimization. Character Level Representation Though word embedding is popular in many existing parsers, they are not ideal for languages with high out-ofvocabulary (OOV) ratios."
K18-2006,P18-1130,0,0.114686,"2015; Zhang et al., 2017). Graph-based dependency parsers enjoy the advantage of the global search which learns the scoring functions for all possible parsing trees to find the globally highest scoring one while transition-based dependency parsers build dependency trees from left to right incrementally, which makes the series of multiple choice decisions locally. In our system, we adopt the transition-based dependency parsing in view of its relatively lower time complexity. Our system implements universal dependency parsing based on the stack-pointer networks (STACKPTR) parser introduced by (Ma et al., 2018). Furthermore, previous work (Straka et al., 2016; Nguyen et al., 2017) showed that POS tags are helpful to dependency parsing. In particular, (Nguyen et al., 2017) pointed out that parsing performance could be improved by the merit of accurate POS tags and the context of syntactic parse tree could help resolve POS ambiguities. Therefore, we seek to jointly learn POS tagging and dependency parsing. This paper describes the system of team LeisureX in the CoNLL 2018 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies. Our system predicts the part-of-speech tag and dependenc"
K18-2006,L16-1680,0,0.0883456,"Missing"
K18-2006,C12-2077,1,0.935559,"Missing"
K18-2006,P05-1012,0,0.315381,"Missing"
K18-2006,E06-1011,0,0.0623666,"specific head. This results in more than one valid selection for each time step, 67 Treebank Breton KEB Czech PUD English PUD Faroese OFT Sampling English, Irish Czech PDT English EWT Norwegian, English, Danish, Swedish, German, Dutch Finnish PUD Finnish TDT Japanese Modern Japanese GSD Naija NSC English Swedish PUD Swedish Talbanken Thai PUD English, Chinese, Hindi, Vietnamese which might confuse the decoder. To address this problem, the parser introduces an inside-outside order to utilize second-order sibling information, which has been proven to be an important feature for parsing process (McDonald and Pereira, 2006; Koo and Collins, 2010). To utilize the secondorder information, the parser replaces the input of decoder from si as follows: βi = ss ◦ sh ◦ si where s and h indicate the sibling and head index of node i, ◦ is the element-wise sum operation to ensure no additional model parameters. 2.4 Table 1: Language substitution for treebanks without training data Loss Function The training objective of pur system is to learn the probability of UPOS tags Pθpos (ypos |x) and 0 the dependency trees Pθdep (ydep |x, ypos ). Given a sentence x, the probabilities are factorized as: Pθpos (ypos |x) = k X et al.,"
K18-2006,K17-3014,0,0.0381188,"Missing"
K18-2006,W03-3017,0,0.297626,"Missing"
K18-2006,D17-1175,0,0.0392203,"Missing"
K18-2006,C18-1153,1,0.708339,"Missing"
K18-2006,zhao-etal-2010-large,1,0.692802,"Missing"
K18-2006,P09-1007,1,0.840787,"Missing"
K18-2007,Q14-1000,0,0.329535,"Missing"
K18-2007,C18-1271,1,0.889537,"Missing"
K18-2007,L16-1262,0,0.108869,"Missing"
K18-2007,C18-1048,1,0.858681,"Missing"
K18-2007,C18-1233,1,0.867831,"Missing"
K18-2007,K18-2001,0,0.0596661,"Missing"
K18-2007,P17-1093,1,0.849959,"Missing"
K18-2007,L16-1680,0,0.0509626,"Missing"
K18-2007,K17-3009,0,0.0520106,"Missing"
K18-2007,P14-5003,0,0.0608992,"Missing"
K18-2007,I08-3008,0,0.0886815,"Missing"
K18-2007,C18-1317,1,0.759308,"Missing"
K18-2007,W15-2137,0,0.059889,"Missing"
K18-2007,C18-1038,1,0.79374,"Missing"
K18-2007,K17-3020,1,0.848229,"Missing"
K19-2004,P18-2058,0,0.0637148,"rsing at the 2019 CoNLL, pages 45–54 c Hong Kong, November 3, 2019. 2019 Association for Computational Linguistics https://doi.org/10.18653/v1/K19-2004 task combines the following five frameworks for graph-based meaning representation: DM, PSD, EDS, UCCA, and AMR. corresponding graph of the directed graph of EDS, our system further treats EDS as one type, and DM and PSD as another type. Based on the experiences of Jiang et al. (2019) and Zhang et al. (2019a) and our previous works on the Dependency Parsing (Li et al., 2018a,b,d; Zhou and Zhao, 2019; Zhou et al., 2019), Semantic Role Labeling (He et al., 2018b; Cai et al., 2018; Li et al., 2018c, 2019b; He et al., 2019), Universal Conceptual Cognitive Annotation (Jiang et al., 2019), Abstract Mean Representation (Zhang et al., 2019a), Machine Translation (Xiao et al., 2019; Sun et al., 2019; Chen et al., 2019), Language Modeling (Li et al., 2019a; Zhang et al., 2019c,b) tasks, we create three graph parsing models based on the semantic graph flavors: (1) Strictly anchored (DM, PSD, EDS): scores the surface lexical units as nodes of the graph, and performs edge training based on the expression of the candidate graph nodes, (2) Non-strictly anchored"
K19-2004,P13-1023,0,0.215914,"of transducing natural language text into AMR, which is a graph-based formalism used for capturing sentence-level semantics. The AMR framework backgrounds notions of compositionality and derivation, therefore, without explicit correspondence between graph nodes and lexical units. In the representation of AMR framework, the graph nodes are obtained by composition, derivation, lexical decomposition, normalization towards verb senses and so on. The features of the AMR graphs built on these graph nodes is similar UCCA UCCA is a multi-layer linguistic framework for semantic annotation proposed by Abend and Rappoport (2013). UCCA aims to recognize the level of semantic granularity which abstracts away from syntactic paraphrases in a typologicallymotivated, cross-linguistic fashion and does not need to rely on language-specific resources. In the representation of the UCCA framework, some nodes have a one-to-one correspondence with the span in the sentence, which is called 3 MRP-transformed UCCA graph differs from on the terminal nodes from the original UCCA graph. In the original UCCA graph representation, terminal nodes refer to words, and in the MRP-transformed UCCA graph, terminal nodes refer to the lowest lay"
K19-2004,D19-1538,1,0.812819,"2019. 2019 Association for Computational Linguistics https://doi.org/10.18653/v1/K19-2004 task combines the following five frameworks for graph-based meaning representation: DM, PSD, EDS, UCCA, and AMR. corresponding graph of the directed graph of EDS, our system further treats EDS as one type, and DM and PSD as another type. Based on the experiences of Jiang et al. (2019) and Zhang et al. (2019a) and our previous works on the Dependency Parsing (Li et al., 2018a,b,d; Zhou and Zhao, 2019; Zhou et al., 2019), Semantic Role Labeling (He et al., 2018b; Cai et al., 2018; Li et al., 2018c, 2019b; He et al., 2019), Universal Conceptual Cognitive Annotation (Jiang et al., 2019), Abstract Mean Representation (Zhang et al., 2019a), Machine Translation (Xiao et al., 2019; Sun et al., 2019; Chen et al., 2019), Language Modeling (Li et al., 2019a; Zhang et al., 2019c,b) tasks, we create three graph parsing models based on the semantic graph flavors: (1) Strictly anchored (DM, PSD, EDS): scores the surface lexical units as nodes of the graph, and performs edge training based on the expression of the candidate graph nodes, (2) Non-strictly anchored (UCCA): treats it as a special constituent tree parsing task a"
K19-2004,W13-2322,0,0.120818,"Missing"
K19-2004,P18-1192,1,0.922788,"rsing at the 2019 CoNLL, pages 45–54 c Hong Kong, November 3, 2019. 2019 Association for Computational Linguistics https://doi.org/10.18653/v1/K19-2004 task combines the following five frameworks for graph-based meaning representation: DM, PSD, EDS, UCCA, and AMR. corresponding graph of the directed graph of EDS, our system further treats EDS as one type, and DM and PSD as another type. Based on the experiences of Jiang et al. (2019) and Zhang et al. (2019a) and our previous works on the Dependency Parsing (Li et al., 2018a,b,d; Zhou and Zhao, 2019; Zhou et al., 2019), Semantic Role Labeling (He et al., 2018b; Cai et al., 2018; Li et al., 2018c, 2019b; He et al., 2019), Universal Conceptual Cognitive Annotation (Jiang et al., 2019), Abstract Mean Representation (Zhang et al., 2019a), Machine Translation (Xiao et al., 2019; Sun et al., 2019; Chen et al., 2019), Language Modeling (Li et al., 2019a; Zhang et al., 2019c,b) tasks, we create three graph parsing models based on the semantic graph flavors: (1) Strictly anchored (DM, PSD, EDS): scores the surface lexical units as nodes of the graph, and performs edge training based on the expression of the candidate graph nodes, (2) Non-strictly anchored"
K19-2004,W12-3602,0,0.323392,"BERT as the encoder. In the training phase, in order to prevent the nodes from falling into local optimum and the edges unable to get enough training, we use the random sampling method on the golden graph nodes to push as many correct nodes as possible to join the edge training. According to the official results of the evaluation, our system ranked second place in the overall F1 metric among the 16 participating systems. On the DM framework, our system achieved the best results. Our system on other 4 frameworks (PSD, EDS, UCCA, and AMR) are all ranked the third place. 2 2.1 DM and PSD The DM (Ivanova et al., 2012) and PSD (Hajic et al., 2012; Miyao et al., 2014) are two independently developed syntactic-semantic annotations which project semantic forms onto bilexical dependencies in a lossy manner. In the representation of the DM and PSD frameworks, the graph nodes and surface lexical units are strictly anchored. There is an explicit, one-to-many anchoring onto sub-strings of the underlying sentence. These graphs are neither fully connected nor rooted. The graphs of DM and PSD have the following features: • There is only a one-to-one correspondence1 between the graph node and the span in the sentence."
K19-2004,C18-1233,1,0.869457,"Missing"
K19-2004,S19-2002,0,0.248133,"ersal Scenarios” and NICT tenuretrack researcher startup fund “Toward Intelligent Machine Translation”. 45 Proceedings of the Shared Task on Cross-Framework Meaning Representation Parsing at the 2019 CoNLL, pages 45–54 c Hong Kong, November 3, 2019. 2019 Association for Computational Linguistics https://doi.org/10.18653/v1/K19-2004 task combines the following five frameworks for graph-based meaning representation: DM, PSD, EDS, UCCA, and AMR. corresponding graph of the directed graph of EDS, our system further treats EDS as one type, and DM and PSD as another type. Based on the experiences of Jiang et al. (2019) and Zhang et al. (2019a) and our previous works on the Dependency Parsing (Li et al., 2018a,b,d; Zhou and Zhao, 2019; Zhou et al., 2019), Semantic Role Labeling (He et al., 2018b; Cai et al., 2018; Li et al., 2018c, 2019b; He et al., 2019), Universal Conceptual Cognitive Annotation (Jiang et al., 2019), Abstract Mean Representation (Zhang et al., 2019a), Machine Translation (Xiao et al., 2019; Sun et al., 2019; Chen et al., 2019), Language Modeling (Li et al., 2019a; Zhang et al., 2019c,b) tasks, we create three graph parsing models based on the semantic graph flavors: (1) Strictly anchored ("
K19-2004,N19-1423,0,0.0142873,"edge target label. Overall, we use multi-tasking learning strategy, shared hidden representation, The top node uses the same mechanism as node scoring, using binary crossentropy as loss implementation. The node pos tag and node frame label use independent feed-forward classifier, using cross-entropy as loss implementation. The edge source label and edge target label use a biaffine scorer consistent with the edge label, using cross-entropy loss as well. We accumulate the loss of all goals together. Neural Architecture Our model builds the candidate graph nodes representation based on the BERT (Devlin et al., 2019) encoder outputs, i.e., for each token wi , the contextualized vector from BERT encoder is denoted as xi . The candidate span (i, j) representation h consists of two endpoint contextualized vectors (xi , xj ) where i and j are the start and end position of the span in the sentence: h = [xi ; xj ]. 4.2 For the UCCA framework, we directly adopt the minimal span-based parser of Stern et al. (2017) on the converted constituent trees. A constituency tree can be regarded as a collection of labeled spans over a sentence. There are two components in the constituent parsing model: one is to assign the"
K19-2004,C18-1271,1,0.840306,"lation”. 45 Proceedings of the Shared Task on Cross-Framework Meaning Representation Parsing at the 2019 CoNLL, pages 45–54 c Hong Kong, November 3, 2019. 2019 Association for Computational Linguistics https://doi.org/10.18653/v1/K19-2004 task combines the following five frameworks for graph-based meaning representation: DM, PSD, EDS, UCCA, and AMR. corresponding graph of the directed graph of EDS, our system further treats EDS as one type, and DM and PSD as another type. Based on the experiences of Jiang et al. (2019) and Zhang et al. (2019a) and our previous works on the Dependency Parsing (Li et al., 2018a,b,d; Zhou and Zhao, 2019; Zhou et al., 2019), Semantic Role Labeling (He et al., 2018b; Cai et al., 2018; Li et al., 2018c, 2019b; He et al., 2019), Universal Conceptual Cognitive Annotation (Jiang et al., 2019), Abstract Mean Representation (Zhang et al., 2019a), Machine Translation (Xiao et al., 2019; Sun et al., 2019; Chen et al., 2019), Language Modeling (Li et al., 2019a; Zhang et al., 2019c,b) tasks, we create three graph parsing models based on the semantic graph flavors: (1) Strictly anchored (DM, PSD, EDS): scores the surface lexical units as nodes of the graph, and performs edge tr"
K19-2004,D18-1262,1,0.847064,"lation”. 45 Proceedings of the Shared Task on Cross-Framework Meaning Representation Parsing at the 2019 CoNLL, pages 45–54 c Hong Kong, November 3, 2019. 2019 Association for Computational Linguistics https://doi.org/10.18653/v1/K19-2004 task combines the following five frameworks for graph-based meaning representation: DM, PSD, EDS, UCCA, and AMR. corresponding graph of the directed graph of EDS, our system further treats EDS as one type, and DM and PSD as another type. Based on the experiences of Jiang et al. (2019) and Zhang et al. (2019a) and our previous works on the Dependency Parsing (Li et al., 2018a,b,d; Zhou and Zhao, 2019; Zhou et al., 2019), Semantic Role Labeling (He et al., 2018b; Cai et al., 2018; Li et al., 2018c, 2019b; He et al., 2019), Universal Conceptual Cognitive Annotation (Jiang et al., 2019), Abstract Mean Representation (Zhang et al., 2019a), Machine Translation (Xiao et al., 2019; Sun et al., 2019; Chen et al., 2019), Language Modeling (Li et al., 2019a; Zhang et al., 2019c,b) tasks, we create three graph parsing models based on the semantic graph flavors: (1) Strictly anchored (DM, PSD, EDS): scores the surface lexical units as nodes of the graph, and performs edge tr"
K19-2004,flickinger-etal-2014-towards,0,0.0124799,"ection, we will introduce this shared task and our modeling approach. Our key idea is to use a graph-based approach rather than a transition-based one; therefore, all the modeling and optimization methods we have on these frameworks are graph-based. The CoNLL shared 46 terminal nodes3 . Other nodes do not have any corresponding relationship with the span, which is introduced as a notion of a semantic constituency that transcends the pure dependency graphs to represent the semantic granularity. The UCCA graph has the following features: (2006) which encode the English Resource Semantics (ERS) (Flickinger et al., 2014). The EDS conversion from under-specified logical forms of the full ERS to variable-free graphs discards partial semantic information which makes the graph abstractly. In the representation of the EDS framework, the graph nodes are independent of surface lexical units. For each graph node, there is an explicit, many-to-many anchoring onto sub-strings of the underlying sentence. The EDS graph has the following features: • There is a one-to-one correspondence between the terminal nodes and the spans in the sentence. • Graph nodes may have multiple parents, among which one is annotated as the pri"
K19-2004,D18-1198,0,0.0419396,"ch span s ∈ Sd do 6: for each word w ∈ s do 7: find a maximum range parent node np of word w whose range size is less than s; 8: move node np to be the child of n(t), and concatenate the original edge label with “ancestor-d” where d represents the original number of edges between the ancestor of np and n(t); 9: remove all the children words of np from s; 10: end for 11: end for 12: until T (t) is a constituent tree 13: set Tc = T (t) Anonymization in AMR Framework Anonymization is an important AMR preprocessing method to reduce the data sparsity issue (Werling et al., 2015; Peng et al., 2017; Guo and Lu, 2018). Following the practice of Zhang et al. (2019a), we first remove senses, wiki links, and polarity attributes in the training dataset. Secondly, we anonymize sub-graphs of named entities which is labeled by one of AMR’s finegrained entity types that contain a name role, and other entities which end with -entity6 . 4 Models To handle different flavors of representation, our system has three types of models: Anchoring-based Pruning Parsing Model, Constituent Parsing Model, Seq2seq-based Parsing Model. 4.1 Anchoring-based Pruning Parsing Model The anchoring-based pruning parsing model is suitable"
K19-2004,K18-2006,1,0.849176,"lation”. 45 Proceedings of the Shared Task on Cross-Framework Meaning Representation Parsing at the 2019 CoNLL, pages 45–54 c Hong Kong, November 3, 2019. 2019 Association for Computational Linguistics https://doi.org/10.18653/v1/K19-2004 task combines the following five frameworks for graph-based meaning representation: DM, PSD, EDS, UCCA, and AMR. corresponding graph of the directed graph of EDS, our system further treats EDS as one type, and DM and PSD as another type. Based on the experiences of Jiang et al. (2019) and Zhang et al. (2019a) and our previous works on the Dependency Parsing (Li et al., 2018a,b,d; Zhou and Zhao, 2019; Zhou et al., 2019), Semantic Role Labeling (He et al., 2018b; Cai et al., 2018; Li et al., 2018c, 2019b; He et al., 2019), Universal Conceptual Cognitive Annotation (Jiang et al., 2019), Abstract Mean Representation (Zhang et al., 2019a), Machine Translation (Xiao et al., 2019; Sun et al., 2019; Chen et al., 2019), Language Modeling (Li et al., 2019a; Zhang et al., 2019c,b) tasks, we create three graph parsing models based on the semantic graph flavors: (1) Strictly anchored (DM, PSD, EDS): scores the surface lexical units as nodes of the graph, and performs edge tr"
K19-2004,K18-2001,0,0.0750259,"Missing"
K19-2004,hajic-etal-2012-announcing,0,0.427424,"Missing"
K19-2004,P14-5010,0,0.00246285,"ne correspondence with its usage pattern string (like “ACT PAT”) in the case of word determination, and the usage pattern has duplicates among different words, the number is much smaller than all item ids size; thus it is more suitable as a learning goal. In the subsequent recovery process, we can use lemma and the usage pattern to restore to the item id. Tokenization, Lemmatization, and Anchor conversion Since the sentence in the training dataset is the original text and no tokenization is performed, and the subsequent processing requires the word root form, we use the Stanford NLP toolkit4 (Manning et al., 2014) to tokenize and lemmatize the original text. As the graph node anchor in the original data is defined at the character level, we need to convert the anchor to the word level. In this process, due to the difference in tokenization criteria and the existence of tokenizing errors, some graph nodes will be converted into the same one in the process of conversion to word-level anchor. Therefore, we performed some post-processing modifications on the tokenization results of the Stanford NLP toolkit to ensure that the graph nodes after the conversion to the word level anchor correspond to the previo"
K19-2004,P19-1298,1,0.811368,"ing representation: DM, PSD, EDS, UCCA, and AMR. corresponding graph of the directed graph of EDS, our system further treats EDS as one type, and DM and PSD as another type. Based on the experiences of Jiang et al. (2019) and Zhang et al. (2019a) and our previous works on the Dependency Parsing (Li et al., 2018a,b,d; Zhou and Zhao, 2019; Zhou et al., 2019), Semantic Role Labeling (He et al., 2018b; Cai et al., 2018; Li et al., 2018c, 2019b; He et al., 2019), Universal Conceptual Cognitive Annotation (Jiang et al., 2019), Abstract Mean Representation (Zhang et al., 2019a), Machine Translation (Xiao et al., 2019; Sun et al., 2019; Chen et al., 2019), Language Modeling (Li et al., 2019a; Zhang et al., 2019c,b) tasks, we create three graph parsing models based on the semantic graph flavors: (1) Strictly anchored (DM, PSD, EDS): scores the surface lexical units as nodes of the graph, and performs edge training based on the expression of the candidate graph nodes, (2) Non-strictly anchored (UCCA): treats it as a special constituent tree parsing task and uses an additional component to recover the remote edges, and (3) Completely unanchored (i.e., AMR): uses the Seq2seq model to generate the nodes and the"
K19-2004,J93-2004,0,0.0644555,"propbank/accept-v.xml :: accept.01 :: 7] ACT()[accept.01 :: 0 :: :: 3]{} PAT()[accept.01 :: 1 :: :: 6]{} ORIG()[accept.01 :: 2 :: :: 3, accept.01 :: 1 :: :: 2]{} ev-w21f1 ACT PAT [propbank/access-v.xml :: access.01 :: 2] ACT()[access.01 :: 0 :: :: 1]{} PAT()[access.01 :: 1 :: :: 2]{} acclaim ev-w22f1 ACT PAT [propbank/acclaim-v.xml :: acclaim.01 :: 1] ACT(sub)[]{} PAT(obj1, ving)[acclaim.01 :: 1 :: :: 1]{} Data and Preprocessing ev-w22f2 3.1 Data The CoNLL shared task provides a training dataset of 5 subtasks, of which DM, PSD, and EDS are from Wall Street Journal (WSJ) text of Penn Treebank (Marcus et al., 1993) and contain 35,656 sentences. The UCCA training data comes from the English Web Treebank’s reviews text (Bies et al., 2012) and the English Wikipedia celebrity articles, with a total data volume of 5,672 sentences. AMR annotation data are drawn from a variety of texts, including online discussion forums, newswires, folktales, novels, and Wikipedia articles, which contain a total of 56,240 sentences. 3.2 ACT PAT ?CAUS ACT(sub)[]{} PAT(obj1)[]{} CAUS(for[objpp, ving])[]{} Figure 1: Examples of the most frequent frame-toframeset mapping extracted from “rng pb links.txt”. 3.3 Frame Label Projecti"
K19-2004,S14-2056,0,0.515659,"r to prevent the nodes from falling into local optimum and the edges unable to get enough training, we use the random sampling method on the golden graph nodes to push as many correct nodes as possible to join the edge training. According to the official results of the evaluation, our system ranked second place in the overall F1 metric among the 16 participating systems. On the DM framework, our system achieved the best results. Our system on other 4 frameworks (PSD, EDS, UCCA, and AMR) are all ranked the third place. 2 2.1 DM and PSD The DM (Ivanova et al., 2012) and PSD (Hajic et al., 2012; Miyao et al., 2014) are two independently developed syntactic-semantic annotations which project semantic forms onto bilexical dependencies in a lossy manner. In the representation of the DM and PSD frameworks, the graph nodes and surface lexical units are strictly anchored. There is an explicit, one-to-many anchoring onto sub-strings of the underlying sentence. These graphs are neither fully connected nor rooted. The graphs of DM and PSD have the following features: • There is only a one-to-one correspondence1 between the graph node and the span in the sentence. • Graph nodes can have multiple in-edges or out-e"
K19-2004,P19-1009,0,0.09286,"Missing"
K19-2004,P05-1013,0,0.103018,"transformation is carried out: the pseudo node has a one-to-one relationship with the span in the sentence. The edge between nodes in the graph is transformed into the edge of the pseudo node, and two attributes are added for the edge: the source node label and the target node label which are used to indicate the node label in the original EDS graph. In this way, the many-to-one relationship is converted into a one-to-one relationship. After conversion, we can model the problem using in the same way as DM and PSD as described in Subsection 2.1. 2.3 Based on the above features and inspired by Nivre and Nilsson (2005), we transform the tree composed of primary edges (and nodes) into a constituent syntax tree structure, which is modeled using the constituent syntax tree parsing schema. Use an additional classifier for the remote edges prediction and recovery. 2.4 AMR Abstract Meaning Representation (AMR) (Banarescu et al., 2013) parsing is the task of transducing natural language text into AMR, which is a graph-based formalism used for capturing sentence-level semantics. The AMR framework backgrounds notions of compositionality and derivation, therefore, without explicit correspondence between graph nodes a"
K19-2004,K19-2001,0,0.0683072,"Missing"
K19-2004,P19-1230,1,0.841784,"s of the Shared Task on Cross-Framework Meaning Representation Parsing at the 2019 CoNLL, pages 45–54 c Hong Kong, November 3, 2019. 2019 Association for Computational Linguistics https://doi.org/10.18653/v1/K19-2004 task combines the following five frameworks for graph-based meaning representation: DM, PSD, EDS, UCCA, and AMR. corresponding graph of the directed graph of EDS, our system further treats EDS as one type, and DM and PSD as another type. Based on the experiences of Jiang et al. (2019) and Zhang et al. (2019a) and our previous works on the Dependency Parsing (Li et al., 2018a,b,d; Zhou and Zhao, 2019; Zhou et al., 2019), Semantic Role Labeling (He et al., 2018b; Cai et al., 2018; Li et al., 2018c, 2019b; He et al., 2019), Universal Conceptual Cognitive Annotation (Jiang et al., 2019), Abstract Mean Representation (Zhang et al., 2019a), Machine Translation (Xiao et al., 2019; Sun et al., 2019; Chen et al., 2019), Language Modeling (Li et al., 2019a; Zhang et al., 2019c,b) tasks, we create three graph parsing models based on the semantic graph flavors: (1) Strictly anchored (DM, PSD, EDS): scores the surface lexical units as nodes of the graph, and performs edge training based on the expres"
K19-2004,E17-1035,0,0.0249758,"of n(t); 5: for each span s ∈ Sd do 6: for each word w ∈ s do 7: find a maximum range parent node np of word w whose range size is less than s; 8: move node np to be the child of n(t), and concatenate the original edge label with “ancestor-d” where d represents the original number of edges between the ancestor of np and n(t); 9: remove all the children words of np from s; 10: end for 11: end for 12: until T (t) is a constituent tree 13: set Tc = T (t) Anonymization in AMR Framework Anonymization is an important AMR preprocessing method to reduce the data sparsity issue (Werling et al., 2015; Peng et al., 2017; Guo and Lu, 2018). Following the practice of Zhang et al. (2019a), we first remove senses, wiki links, and polarity attributes in the training dataset. Secondly, we anonymize sub-graphs of named entities which is labeled by one of AMR’s finegrained entity types that contain a name role, and other entities which end with -entity6 . 4 Models To handle different flavors of representation, our system has three types of models: Anchoring-based Pruning Parsing Model, Constituent Parsing Model, Seq2seq-based Parsing Model. 4.1 Anchoring-based Pruning Parsing Model The anchoring-based pruning parsin"
K19-2004,P17-1076,0,0.0518095,"ent with the edge label, using cross-entropy loss as well. We accumulate the loss of all goals together. Neural Architecture Our model builds the candidate graph nodes representation based on the BERT (Devlin et al., 2019) encoder outputs, i.e., for each token wi , the contextualized vector from BERT encoder is denoted as xi . The candidate span (i, j) representation h consists of two endpoint contextualized vectors (xi , xj ) where i and j are the start and end position of the span in the sentence: h = [xi ; xj ]. 4.2 For the UCCA framework, we directly adopt the minimal span-based parser of Stern et al. (2017) on the converted constituent trees. A constituency tree can be regarded as a collection of labeled spans over a sentence. There are two components in the constituent parsing model: one is to assign the scores directly to span existence which determines the tree structure, and the other one assigns scores to span labels which provides the labeled outputs. (1) The node unary scorer φnode (·) is implemented with feed-forward networks based on the candidate graph nodes representation h: φnode (·) = sigmoid(MLPnode (h)). Constituent Parsing Model (2) Neural Architecture In this model, we also buil"
K19-2004,P19-1119,1,0.795869,"Missing"
K19-2004,P15-1095,0,0.0223824,"spans Sd in the range of n(t); 5: for each span s ∈ Sd do 6: for each word w ∈ s do 7: find a maximum range parent node np of word w whose range size is less than s; 8: move node np to be the child of n(t), and concatenate the original edge label with “ancestor-d” where d represents the original number of edges between the ancestor of np and n(t); 9: remove all the children words of np from s; 10: end for 11: end for 12: until T (t) is a constituent tree 13: set Tc = T (t) Anonymization in AMR Framework Anonymization is an important AMR preprocessing method to reduce the data sparsity issue (Werling et al., 2015; Peng et al., 2017; Guo and Lu, 2018). Following the practice of Zhang et al. (2019a), we first remove senses, wiki links, and polarity attributes in the training dataset. Secondly, we anonymize sub-graphs of named entities which is labeled by one of AMR’s finegrained entity types that contain a name role, and other entities which end with -entity6 . 4 Models To handle different flavors of representation, our system has three types of models: Anchoring-based Pruning Parsing Model, Constituent Parsing Model, Seq2seq-based Parsing Model. 4.1 Anchoring-based Pruning Parsing Model The anchoring-b"
K19-2008,P14-1134,0,0.129362,"Missing"
K19-2008,N19-1340,1,0.847059,"Missing"
K19-2008,C18-1048,1,0.833677,"rojects of National Natural Science Foundation of China (No. U1836222 and No. 61733011). 86 Proceedings of the Shared Task on Cross-Framework Meaning Representation Parsing at the 2019 CoNLL, pages 86–94 c Hong Kong, November 3, 2019. 2019 Association for Computational Linguistics https://doi.org/10.18653/v1/K19-2008 tence. TUPA (Transition-based UCCA Parser) (Hershcovich et al., 2017, 2018) in terms of neural networks, which is powerful in a lot of NLP tasks (Cai and Zhao, 2016; Zhang et al., 2016; Qin et al., 2016; Vaswani et al., 2017; Cai et al., 2017; Wang et al., 2017; Qin et al., 2017; Bai and Zhao, 2018; He et al., 2018; Cai et al., 2018; Zhang and Zhao, 2018; Zhang et al., 2018a,b; Zhu et al., 2018; Huang and Zhao, 2018; Li et al., 2018c; Wu et al., 2018; Zhang et al., 2019; Xiao et al., 2019). Neural networks can encode the texts into a dense representation. We put the parsing job of all the frameworks to one model and use a multi-task setting to jointly train the system. In the final official evaluation of the shared task, our system achieves 42%F1 unified MRP metric score. The rest of this paper is organized as follows. Section 2 introduces these frameworks. Section 3 shows our model. Se"
K19-2008,W13-2322,0,0.0811725,"Missing"
K19-2008,P18-1192,1,0.838504,"Natural Science Foundation of China (No. U1836222 and No. 61733011). 86 Proceedings of the Shared Task on Cross-Framework Meaning Representation Parsing at the 2019 CoNLL, pages 86–94 c Hong Kong, November 3, 2019. 2019 Association for Computational Linguistics https://doi.org/10.18653/v1/K19-2008 tence. TUPA (Transition-based UCCA Parser) (Hershcovich et al., 2017, 2018) in terms of neural networks, which is powerful in a lot of NLP tasks (Cai and Zhao, 2016; Zhang et al., 2016; Qin et al., 2016; Vaswani et al., 2017; Cai et al., 2017; Wang et al., 2017; Qin et al., 2017; Bai and Zhao, 2018; He et al., 2018; Cai et al., 2018; Zhang and Zhao, 2018; Zhang et al., 2018a,b; Zhu et al., 2018; Huang and Zhao, 2018; Li et al., 2018c; Wu et al., 2018; Zhang et al., 2019; Xiao et al., 2019). Neural networks can encode the texts into a dense representation. We put the parsing job of all the frameworks to one model and use a multi-task setting to jointly train the system. In the final official evaluation of the shared task, our system achieves 42%F1 unified MRP metric score. The rest of this paper is organized as follows. Section 2 introduces these frameworks. Section 3 shows our model. Section 4 gives the"
K19-2008,P16-1039,1,0.837366,"ing author. This paper was partially supported by National Key Research and Development Program of China (No. 2017YFB0304100) and Key Projects of National Natural Science Foundation of China (No. U1836222 and No. 61733011). 86 Proceedings of the Shared Task on Cross-Framework Meaning Representation Parsing at the 2019 CoNLL, pages 86–94 c Hong Kong, November 3, 2019. 2019 Association for Computational Linguistics https://doi.org/10.18653/v1/K19-2008 tence. TUPA (Transition-based UCCA Parser) (Hershcovich et al., 2017, 2018) in terms of neural networks, which is powerful in a lot of NLP tasks (Cai and Zhao, 2016; Zhang et al., 2016; Qin et al., 2016; Vaswani et al., 2017; Cai et al., 2017; Wang et al., 2017; Qin et al., 2017; Bai and Zhao, 2018; He et al., 2018; Cai et al., 2018; Zhang and Zhao, 2018; Zhang et al., 2018a,b; Zhu et al., 2018; Huang and Zhao, 2018; Li et al., 2018c; Wu et al., 2018; Zhang et al., 2019; Xiao et al., 2019). Neural networks can encode the texts into a dense representation. We put the parsing job of all the frameworks to one model and use a multi-task setting to jointly train the system. In the final official evaluation of the shared task, our system achieves 42%F1 unified"
K19-2008,P17-1104,0,0.0516767,"xpressive and arguably more adequate target structures for sentence-level analysis beyond ∗ Corresponding author. This paper was partially supported by National Key Research and Development Program of China (No. 2017YFB0304100) and Key Projects of National Natural Science Foundation of China (No. U1836222 and No. 61733011). 86 Proceedings of the Shared Task on Cross-Framework Meaning Representation Parsing at the 2019 CoNLL, pages 86–94 c Hong Kong, November 3, 2019. 2019 Association for Computational Linguistics https://doi.org/10.18653/v1/K19-2008 tence. TUPA (Transition-based UCCA Parser) (Hershcovich et al., 2017, 2018) in terms of neural networks, which is powerful in a lot of NLP tasks (Cai and Zhao, 2016; Zhang et al., 2016; Qin et al., 2016; Vaswani et al., 2017; Cai et al., 2017; Wang et al., 2017; Qin et al., 2017; Bai and Zhao, 2018; He et al., 2018; Cai et al., 2018; Zhang and Zhao, 2018; Zhang et al., 2018a,b; Zhu et al., 2018; Huang and Zhao, 2018; Li et al., 2018c; Wu et al., 2018; Zhang et al., 2019; Xiao et al., 2019). Neural networks can encode the texts into a dense representation. We put the parsing job of all the frameworks to one model and use a multi-task setting to jointly train th"
K19-2008,P18-1035,0,0.0624917,"e actions, actions 4, 5, 6, 7, 8 are used by all the frameworks, actions 1, 2 are used by EDS and AMR, action 3 is used by UCCA. If one action is not used by the framework, then the oracle will not generate this action for it, and during inference, the action is only selected from the legal actions for task-specified classifiers. 3.2 X1 X2 X3 State Specific BiLSTM X4 X5 X6 X7 Figure 1: Model overview. follows TUPA. The model uses a bi-directional LSTM (Hochreiter and Schmidhuber, 1997) to encode the sentence and a multi-layer perceptron (MLP) with a softmax layer for classification. Following Hershcovich et al. (2018), in the model, we have shared embedding components and a shared LSTM module, and for each framework, we have a task-specified LSTM module and a corresponding classifier. For each framework, the outputs of shared LSTM and task-specified LSTM are concatenated and fed into the task-specified classifier for action prediction. For the word embeddings, we use the pre-trained GloVe embeddings (Pennington et al., 2014) and the pre-trained BERT (Devlin et al., 2019). For each token, there are also embeddings for lemma, POS tag, and syntactic dependency label. These embeddings together with token embed"
K19-2008,K19-2002,0,0.0539402,"ers. If the length of one sentence is larger than the max length, then the exceeding tokens are discarded. Other features denote the node labels in the stack and buffer, and the previous actions introduced in Section 3.2. Test 3359 3359 3359 1131 1998 4.3 The evaluation is blindly conducted. The MRP score results are shown in Table 3. For framework specified metric, the SDP results for DM and PSD are reported in Table 4, the EDM results for EDS are reported in Table 7, and the SMATCH results for AMR are reported in Table 9. Table 3 also contains the comparison results with the TUPA baseline (Hershcovich and Arviv, 2019). For some of the frameworks, our model is better than the TUPA baseline. Table 5: Number of sentences of each framework in training set and test set. 4.2 Model Settings We implement our model with PyTorch1 and tuned on the development set. During inference, we use greedy decoding to get the action sequence. Table 6 shows the hyperparameter settings. The optimizer is Adam (Kingma and Ba, 2015). The dropout is applied to the embeddings, the outputs of BiLSTMs, and the outputs of the first MLP lay1 Results 4.4 Analysis Though following the same model architecture and dynamic oracle of TUPA, we a"
K19-2008,oepen-lonning-2006-discriminant,0,0.0209056,"ly appear to be more abstractive compared to the other frameworks. This shared task considers five meaning representation frameworks. In this section, we briefly introduce these frameworks and figure out the traits of these frameworks. 2.1 UCCA 2.5 Framework Summary Node Labels Node Properties Node Anchoring Generated Node Edge Attributes EDS DM PSD EDS UCCA AMR • • • • • • • • • • • • • ◦ • • - Table 1: Framework properties. Generated node means the nodes in the graph are not the superficial tokens and ◦ means in UCCA they are empty non-terminal nodes. Elementary Dependency Structures (EDS) (Oepen and Lønning, 2006) is a variable-free semantic dependency graph, where graph nodes correspond to logical predictions and edges to labeled argument positions. The variable-free feature makes these graphs quite similar to Abstract Meaning Representation (AMR). Nodes in EDS are in principle independent of surface lexical units, but for each node, there is an explicit and many-to-many anchoring onto sub-strings of the underlying senThese frameworks have different structures and different complexity. The graphs of these frameworks all have a top node or root node, and edges are all directed and labeled. Other proper"
K19-2008,D14-1162,0,0.083121,"A. The model uses a bi-directional LSTM (Hochreiter and Schmidhuber, 1997) to encode the sentence and a multi-layer perceptron (MLP) with a softmax layer for classification. Following Hershcovich et al. (2018), in the model, we have shared embedding components and a shared LSTM module, and for each framework, we have a task-specified LSTM module and a corresponding classifier. For each framework, the outputs of shared LSTM and task-specified LSTM are concatenated and fed into the task-specified classifier for action prediction. For the word embeddings, we use the pre-trained GloVe embeddings (Pennington et al., 2014) and the pre-trained BERT (Devlin et al., 2019). For each token, there are also embeddings for lemma, POS tag, and syntactic dependency label. These embeddings together with token embeddings and BERT outputs Model Figure 1 depicts our model. x1 , x2 , · · · , xi denotes the input tokens. Our model architecture 88 labels and convert these nodes and edges to properties. For DM and PSD, the pos node property is from XPOS in UDPipe data, and the frame property are predicted by additional classifiers. UCCA has edge attribute remote to reflect the reentrancy and we neglect the edge attribute in our"
K19-2008,D18-1321,1,0.796643,"Task on Cross-Framework Meaning Representation Parsing at the 2019 CoNLL, pages 86–94 c Hong Kong, November 3, 2019. 2019 Association for Computational Linguistics https://doi.org/10.18653/v1/K19-2008 tence. TUPA (Transition-based UCCA Parser) (Hershcovich et al., 2017, 2018) in terms of neural networks, which is powerful in a lot of NLP tasks (Cai and Zhao, 2016; Zhang et al., 2016; Qin et al., 2016; Vaswani et al., 2017; Cai et al., 2017; Wang et al., 2017; Qin et al., 2017; Bai and Zhao, 2018; He et al., 2018; Cai et al., 2018; Zhang and Zhao, 2018; Zhang et al., 2018a,b; Zhu et al., 2018; Huang and Zhao, 2018; Li et al., 2018c; Wu et al., 2018; Zhang et al., 2019; Xiao et al., 2019). Neural networks can encode the texts into a dense representation. We put the parsing job of all the frameworks to one model and use a multi-task setting to jointly train the system. In the final official evaluation of the shared task, our system achieves 42%F1 unified MRP metric score. The rest of this paper is organized as follows. Section 2 introduces these frameworks. Section 3 shows our model. Section 4 gives the settings of our model and test results. 2 2.3 Universal Conceptual Cognitive Annotation (UCCA) (Abend"
K19-2008,D16-1246,1,0.850755,"ported by National Key Research and Development Program of China (No. 2017YFB0304100) and Key Projects of National Natural Science Foundation of China (No. U1836222 and No. 61733011). 86 Proceedings of the Shared Task on Cross-Framework Meaning Representation Parsing at the 2019 CoNLL, pages 86–94 c Hong Kong, November 3, 2019. 2019 Association for Computational Linguistics https://doi.org/10.18653/v1/K19-2008 tence. TUPA (Transition-based UCCA Parser) (Hershcovich et al., 2017, 2018) in terms of neural networks, which is powerful in a lot of NLP tasks (Cai and Zhao, 2016; Zhang et al., 2016; Qin et al., 2016; Vaswani et al., 2017; Cai et al., 2017; Wang et al., 2017; Qin et al., 2017; Bai and Zhao, 2018; He et al., 2018; Cai et al., 2018; Zhang and Zhao, 2018; Zhang et al., 2018a,b; Zhu et al., 2018; Huang and Zhao, 2018; Li et al., 2018c; Wu et al., 2018; Zhang et al., 2019; Xiao et al., 2019). Neural networks can encode the texts into a dense representation. We put the parsing job of all the frameworks to one model and use a multi-task setting to jointly train the system. In the final official evaluation of the shared task, our system achieves 42%F1 unified MRP metric score. The rest of this pa"
K19-2008,W12-3602,0,0.239187,"Missing"
K19-2008,P17-1093,1,0.893541,"Missing"
K19-2008,C18-1271,1,0.863804,"k Meaning Representation Parsing at the 2019 CoNLL, pages 86–94 c Hong Kong, November 3, 2019. 2019 Association for Computational Linguistics https://doi.org/10.18653/v1/K19-2008 tence. TUPA (Transition-based UCCA Parser) (Hershcovich et al., 2017, 2018) in terms of neural networks, which is powerful in a lot of NLP tasks (Cai and Zhao, 2016; Zhang et al., 2016; Qin et al., 2016; Vaswani et al., 2017; Cai et al., 2017; Wang et al., 2017; Qin et al., 2017; Bai and Zhao, 2018; He et al., 2018; Cai et al., 2018; Zhang and Zhao, 2018; Zhang et al., 2018a,b; Zhu et al., 2018; Huang and Zhao, 2018; Li et al., 2018c; Wu et al., 2018; Zhang et al., 2019; Xiao et al., 2019). Neural networks can encode the texts into a dense representation. We put the parsing job of all the frameworks to one model and use a multi-task setting to jointly train the system. In the final official evaluation of the shared task, our system achieves 42%F1 unified MRP metric score. The rest of this paper is organized as follows. Section 2 introduces these frameworks. Section 3 shows our model. Section 4 gives the settings of our model and test results. 2 2.3 Universal Conceptual Cognitive Annotation (UCCA) (Abend and Rappoport, 20"
K19-2008,D18-1262,1,0.849056,"k Meaning Representation Parsing at the 2019 CoNLL, pages 86–94 c Hong Kong, November 3, 2019. 2019 Association for Computational Linguistics https://doi.org/10.18653/v1/K19-2008 tence. TUPA (Transition-based UCCA Parser) (Hershcovich et al., 2017, 2018) in terms of neural networks, which is powerful in a lot of NLP tasks (Cai and Zhao, 2016; Zhang et al., 2016; Qin et al., 2016; Vaswani et al., 2017; Cai et al., 2017; Wang et al., 2017; Qin et al., 2017; Bai and Zhao, 2018; He et al., 2018; Cai et al., 2018; Zhang and Zhao, 2018; Zhang et al., 2018a,b; Zhu et al., 2018; Huang and Zhao, 2018; Li et al., 2018c; Wu et al., 2018; Zhang et al., 2019; Xiao et al., 2019). Neural networks can encode the texts into a dense representation. We put the parsing job of all the frameworks to one model and use a multi-task setting to jointly train the system. In the final official evaluation of the shared task, our system achieves 42%F1 unified MRP metric score. The rest of this paper is organized as follows. Section 2 introduces these frameworks. Section 3 shows our model. Section 4 gives the settings of our model and test results. 2 2.3 Universal Conceptual Cognitive Annotation (UCCA) (Abend and Rappoport, 20"
K19-2008,K17-3020,1,0.846419,"China (No. 2017YFB0304100) and Key Projects of National Natural Science Foundation of China (No. U1836222 and No. 61733011). 86 Proceedings of the Shared Task on Cross-Framework Meaning Representation Parsing at the 2019 CoNLL, pages 86–94 c Hong Kong, November 3, 2019. 2019 Association for Computational Linguistics https://doi.org/10.18653/v1/K19-2008 tence. TUPA (Transition-based UCCA Parser) (Hershcovich et al., 2017, 2018) in terms of neural networks, which is powerful in a lot of NLP tasks (Cai and Zhao, 2016; Zhang et al., 2016; Qin et al., 2016; Vaswani et al., 2017; Cai et al., 2017; Wang et al., 2017; Qin et al., 2017; Bai and Zhao, 2018; He et al., 2018; Cai et al., 2018; Zhang and Zhao, 2018; Zhang et al., 2018a,b; Zhu et al., 2018; Huang and Zhao, 2018; Li et al., 2018c; Wu et al., 2018; Zhang et al., 2019; Xiao et al., 2019). Neural networks can encode the texts into a dense representation. We put the parsing job of all the frameworks to one model and use a multi-task setting to jointly train the system. In the final official evaluation of the shared task, our system achieves 42%F1 unified MRP metric score. The rest of this paper is organized as follows. Section 2 introduces these fra"
K19-2008,K18-2006,1,0.852586,"k Meaning Representation Parsing at the 2019 CoNLL, pages 86–94 c Hong Kong, November 3, 2019. 2019 Association for Computational Linguistics https://doi.org/10.18653/v1/K19-2008 tence. TUPA (Transition-based UCCA Parser) (Hershcovich et al., 2017, 2018) in terms of neural networks, which is powerful in a lot of NLP tasks (Cai and Zhao, 2016; Zhang et al., 2016; Qin et al., 2016; Vaswani et al., 2017; Cai et al., 2017; Wang et al., 2017; Qin et al., 2017; Bai and Zhao, 2018; He et al., 2018; Cai et al., 2018; Zhang and Zhao, 2018; Zhang et al., 2018a,b; Zhu et al., 2018; Huang and Zhao, 2018; Li et al., 2018c; Wu et al., 2018; Zhang et al., 2019; Xiao et al., 2019). Neural networks can encode the texts into a dense representation. We put the parsing job of all the frameworks to one model and use a multi-task setting to jointly train the system. In the final official evaluation of the shared task, our system achieves 42%F1 unified MRP metric score. The rest of this paper is organized as follows. Section 2 introduces these frameworks. Section 3 shows our model. Section 4 gives the settings of our model and test results. 2 2.3 Universal Conceptual Cognitive Annotation (UCCA) (Abend and Rappoport, 20"
K19-2008,K18-2001,0,0.023048,"4, 5, 6, 7, 8 are used by all the frameworks, actions 1, 2 are used by EDS and AMR, action 3 is used by UCCA. If one action is not used by the framework, then the oracle will not generate this action for it, and during inference, the action is only selected from the legal actions for task-specified classifiers. 3.2 X1 X2 X3 State Specific BiLSTM X4 X5 X6 X7 Figure 1: Model overview. follows TUPA. The model uses a bi-directional LSTM (Hochreiter and Schmidhuber, 1997) to encode the sentence and a multi-layer perceptron (MLP) with a softmax layer for classification. Following Hershcovich et al. (2018), in the model, we have shared embedding components and a shared LSTM module, and for each framework, we have a task-specified LSTM module and a corresponding classifier. For each framework, the outputs of shared LSTM and task-specified LSTM are concatenated and fed into the task-specified classifier for action prediction. For the word embeddings, we use the pre-trained GloVe embeddings (Pennington et al., 2014) and the pre-trained BERT (Devlin et al., 2019). For each token, there are also embeddings for lemma, POS tag, and syntactic dependency label. These embeddings together with token embed"
K19-2008,K18-2007,1,0.838581,"tation Parsing at the 2019 CoNLL, pages 86–94 c Hong Kong, November 3, 2019. 2019 Association for Computational Linguistics https://doi.org/10.18653/v1/K19-2008 tence. TUPA (Transition-based UCCA Parser) (Hershcovich et al., 2017, 2018) in terms of neural networks, which is powerful in a lot of NLP tasks (Cai and Zhao, 2016; Zhang et al., 2016; Qin et al., 2016; Vaswani et al., 2017; Cai et al., 2017; Wang et al., 2017; Qin et al., 2017; Bai and Zhao, 2018; He et al., 2018; Cai et al., 2018; Zhang and Zhao, 2018; Zhang et al., 2018a,b; Zhu et al., 2018; Huang and Zhao, 2018; Li et al., 2018c; Wu et al., 2018; Zhang et al., 2019; Xiao et al., 2019). Neural networks can encode the texts into a dense representation. We put the parsing job of all the frameworks to one model and use a multi-task setting to jointly train the system. In the final official evaluation of the shared task, our system achieves 42%F1 unified MRP metric score. The rest of this paper is organized as follows. Section 2 introduces these frameworks. Section 3 shows our model. Section 4 gives the settings of our model and test results. 2 2.3 Universal Conceptual Cognitive Annotation (UCCA) (Abend and Rappoport, 2013) targets to a m"
K19-2008,C12-2077,1,0.863417,"Missing"
K19-2008,S14-2056,0,0.0835387,"targets to a more semantic way rather than only syntactically and can be extended to crosslinguistic settings. UCCA representations are directed acyclic graphs (DAGs), where terminal nodes correspond to the text tokens and non-terminal nodes to semantic units with more abstract meanings. Edges are labeled, indicating the role of a child in the relation. UCCA enable reentrancy to allow a node to participate in several semantic relations. 2.4 Framework Schemes DM and PSD DELPH-IN MRS Bi-Lexical Dependencies (DM) (Ivanova et al., 2012) and Prague Semantic Dependencies (PSD) (Hajiˇc et al., 2012; Miyao et al., 2014) use bi-lexical semantic dependencies to represent the meaning with different annotations. Graph nodes in DM and PSD correspond to surface tokens, and graphs are neither fully connected nor rooted trees, that is, some tokens from the underlying sentence remain structurally isolated, and for some nodes, there are multiple incoming edges. 2.2 AMR Abstract Meaning Representation (AMR) (Banarescu et al., 2013) tries to abstract out all the semantic information from the sentences. The AMR graphs are rooted directed graphs, in which both nodes and edges are labeled, and reentrancy is also allowed. A"
K19-2008,P19-1298,1,0.805628,"es 86–94 c Hong Kong, November 3, 2019. 2019 Association for Computational Linguistics https://doi.org/10.18653/v1/K19-2008 tence. TUPA (Transition-based UCCA Parser) (Hershcovich et al., 2017, 2018) in terms of neural networks, which is powerful in a lot of NLP tasks (Cai and Zhao, 2016; Zhang et al., 2016; Qin et al., 2016; Vaswani et al., 2017; Cai et al., 2017; Wang et al., 2017; Qin et al., 2017; Bai and Zhao, 2018; He et al., 2018; Cai et al., 2018; Zhang and Zhao, 2018; Zhang et al., 2018a,b; Zhu et al., 2018; Huang and Zhao, 2018; Li et al., 2018c; Wu et al., 2018; Zhang et al., 2019; Xiao et al., 2019). Neural networks can encode the texts into a dense representation. We put the parsing job of all the frameworks to one model and use a multi-task setting to jointly train the system. In the final official evaluation of the shared task, our system achieves 42%F1 unified MRP metric score. The rest of this paper is organized as follows. Section 2 introduces these frameworks. Section 3 shows our model. Section 4 gives the settings of our model and test results. 2 2.3 Universal Conceptual Cognitive Annotation (UCCA) (Abend and Rappoport, 2013) targets to a more semantic way rather than only syntac"
K19-2008,P16-1131,1,0.903143,"Missing"
K19-2008,C18-1153,1,0.81922,"61733011). 86 Proceedings of the Shared Task on Cross-Framework Meaning Representation Parsing at the 2019 CoNLL, pages 86–94 c Hong Kong, November 3, 2019. 2019 Association for Computational Linguistics https://doi.org/10.18653/v1/K19-2008 tence. TUPA (Transition-based UCCA Parser) (Hershcovich et al., 2017, 2018) in terms of neural networks, which is powerful in a lot of NLP tasks (Cai and Zhao, 2016; Zhang et al., 2016; Qin et al., 2016; Vaswani et al., 2017; Cai et al., 2017; Wang et al., 2017; Qin et al., 2017; Bai and Zhao, 2018; He et al., 2018; Cai et al., 2018; Zhang and Zhao, 2018; Zhang et al., 2018a,b; Zhu et al., 2018; Huang and Zhao, 2018; Li et al., 2018c; Wu et al., 2018; Zhang et al., 2019; Xiao et al., 2019). Neural networks can encode the texts into a dense representation. We put the parsing job of all the frameworks to one model and use a multi-task setting to jointly train the system. In the final official evaluation of the shared task, our system achieves 42%F1 unified MRP metric score. The rest of this paper is organized as follows. Section 2 introduces these frameworks. Section 3 shows our model. Section 4 gives the settings of our model and test results. 2 2.3 Universal Con"
K19-2008,P19-1154,1,0.857749,"Missing"
K19-2008,C18-1317,1,0.814481,"61733011). 86 Proceedings of the Shared Task on Cross-Framework Meaning Representation Parsing at the 2019 CoNLL, pages 86–94 c Hong Kong, November 3, 2019. 2019 Association for Computational Linguistics https://doi.org/10.18653/v1/K19-2008 tence. TUPA (Transition-based UCCA Parser) (Hershcovich et al., 2017, 2018) in terms of neural networks, which is powerful in a lot of NLP tasks (Cai and Zhao, 2016; Zhang et al., 2016; Qin et al., 2016; Vaswani et al., 2017; Cai et al., 2017; Wang et al., 2017; Qin et al., 2017; Bai and Zhao, 2018; He et al., 2018; Cai et al., 2018; Zhang and Zhao, 2018; Zhang et al., 2018a,b; Zhu et al., 2018; Huang and Zhao, 2018; Li et al., 2018c; Wu et al., 2018; Zhang et al., 2019; Xiao et al., 2019). Neural networks can encode the texts into a dense representation. We put the parsing job of all the frameworks to one model and use a multi-task setting to jointly train the system. In the final official evaluation of the shared task, our system achieves 42%F1 unified MRP metric score. The rest of this paper is organized as follows. Section 2 introduces these frameworks. Section 3 shows our model. Section 4 gives the settings of our model and test results. 2 2.3 Universal Con"
K19-2008,C18-1038,1,0.848831,"(No. U1836222 and No. 61733011). 86 Proceedings of the Shared Task on Cross-Framework Meaning Representation Parsing at the 2019 CoNLL, pages 86–94 c Hong Kong, November 3, 2019. 2019 Association for Computational Linguistics https://doi.org/10.18653/v1/K19-2008 tence. TUPA (Transition-based UCCA Parser) (Hershcovich et al., 2017, 2018) in terms of neural networks, which is powerful in a lot of NLP tasks (Cai and Zhao, 2016; Zhang et al., 2016; Qin et al., 2016; Vaswani et al., 2017; Cai et al., 2017; Wang et al., 2017; Qin et al., 2017; Bai and Zhao, 2018; He et al., 2018; Cai et al., 2018; Zhang and Zhao, 2018; Zhang et al., 2018a,b; Zhu et al., 2018; Huang and Zhao, 2018; Li et al., 2018c; Wu et al., 2018; Zhang et al., 2019; Xiao et al., 2019). Neural networks can encode the texts into a dense representation. We put the parsing job of all the frameworks to one model and use a multi-task setting to jointly train the system. In the final official evaluation of the shared task, our system achieves 42%F1 unified MRP metric score. The rest of this paper is organized as follows. Section 2 introduces these frameworks. Section 3 shows our model. Section 4 gives the settings of our model and test results."
K19-2008,W09-1209,1,0.755357,"Missing"
K19-2008,D09-1004,1,0.820525,"Missing"
K19-2008,P19-1230,1,0.885515,"Missing"
K19-2008,C18-2024,1,0.819489,"ngs of the Shared Task on Cross-Framework Meaning Representation Parsing at the 2019 CoNLL, pages 86–94 c Hong Kong, November 3, 2019. 2019 Association for Computational Linguistics https://doi.org/10.18653/v1/K19-2008 tence. TUPA (Transition-based UCCA Parser) (Hershcovich et al., 2017, 2018) in terms of neural networks, which is powerful in a lot of NLP tasks (Cai and Zhao, 2016; Zhang et al., 2016; Qin et al., 2016; Vaswani et al., 2017; Cai et al., 2017; Wang et al., 2017; Qin et al., 2017; Bai and Zhao, 2018; He et al., 2018; Cai et al., 2018; Zhang and Zhao, 2018; Zhang et al., 2018a,b; Zhu et al., 2018; Huang and Zhao, 2018; Li et al., 2018c; Wu et al., 2018; Zhang et al., 2019; Xiao et al., 2019). Neural networks can encode the texts into a dense representation. We put the parsing job of all the frameworks to one model and use a multi-task setting to jointly train the system. In the final official evaluation of the shared task, our system achieves 42%F1 unified MRP metric score. The rest of this paper is organized as follows. Section 2 introduces these frameworks. Section 3 shows our model. Section 4 gives the settings of our model and test results. 2 2.3 Universal Conceptual Cognitive Ann"
N16-1064,W02-1001,0,0.217763,"n Figure1. To use trained representation, we initialize the weight matrix W1 with these external representations. For words without corresponding external representations, their representations are initialized with uniformly distributed random values, ranging from -0.1 to 0.1. Three typical tagging tasks are used for the evaluation: partof-speech tagging (POS), chunking (CHUNK) and named entity recognition (NER). • The POS tagging experiment is conducted on the Wall Street Journal data from Penn Treebank III (Marcus et al., 1993). Training, development and test sets are split according to in (Collins, 2002). Performance is evaluated by the accuracy of predicted tags on test set. • CHUNK experiment is conducted on the data of CoNLL-2000 shared task (Sang and Buchholz, 2000). Performance is assessed by the F1 score computed by the evaluation script re530 • NER experiment is conducted on the CoNLL2003 shared task (Tjong Kim Sang and De Meulder, 2003). Performance is measured by the F1 score calculated by the evaluation script of the CoNLL-2003 shared task 4 . To focus on the effect of word representation, for all tasks, we use the network with the same hidden structure and input features. The size"
N16-1064,P15-1033,0,0.0445432,"sk for natural language processing. Many primary processing tasks over sentence such as word segmentation, named entity recognition and part-of-speech tagging can be formalized as a tagging task (Zhao et al., 2006; Huang and Zhao, 2007; Zhao and Kit, 2008b; Zhao and Kit, 2008a; Zhao et al., 2010; Zhao and Kit, 2011). Recently, many state-of-the-art systems of tagging related tasks are implemented with bidirectional long short-term memory (BLSTM) recurrent neural network (RNN), for example, slot filling (Mesnil et al., 2013), part-of-speech tagging (Huang et al., 2015), and dependency parsing (Dyer et al., 2015) etc. All of these systems use distributed representation of words to involve word level information. Better trained word representations would further improve the state-of-the-art performance of these tasks which makes it worthy to research the training methods of word representations. The existing training methods of word representation can generally be divided into two classes: 1) Matrix factorization methods. These methods utilize low-rank approximation to decompose a large matrix that contains corpus statistics. One typical work is the latent semantic analysis (LSA) (Deerwester et al., 19"
N16-1064,J93-2004,0,0.0907678,"better. The structure of BLSTM-RNN to test word representations is the same as that in Figure1. To use trained representation, we initialize the weight matrix W1 with these external representations. For words without corresponding external representations, their representations are initialized with uniformly distributed random values, ranging from -0.1 to 0.1. Three typical tagging tasks are used for the evaluation: partof-speech tagging (POS), chunking (CHUNK) and named entity recognition (NER). • The POS tagging experiment is conducted on the Wall Street Journal data from Penn Treebank III (Marcus et al., 1993). Training, development and test sets are split according to in (Collins, 2002). Performance is evaluated by the accuracy of predicted tags on test set. • CHUNK experiment is conducted on the data of CoNLL-2000 shared task (Sang and Buchholz, 2000). Performance is assessed by the F1 score computed by the evaluation script re530 • NER experiment is conducted on the CoNLL2003 shared task (Tjong Kim Sang and De Meulder, 2003). Performance is measured by the F1 score calculated by the evaluation script of the CoNLL-2003 shared task 4 . To focus on the effect of word representation, for all tasks,"
N16-1064,D14-1162,0,0.100895,"vious several words. (Collobert et al., 2011) trains a neural network to judge the validity of a given context. (Mikolov et al., 2013a) proposes skip-gram and continuous bag-of-words (CBOW) models based on a single-layer network architecture. The objective of skip-gram model is to predict the context given the word itself, while the objective of CBOW is to predict a word given its context. Aside from these two sets of methods, distributed representation can also be obtained by training recurrent neural network (RNN) language model proposed by (Mikolov et al., 2010) or GloVe model proposed by (Pennington et al., 2014a) which trains a log-bilinear model on word-word co-occurrence counts. All of these methods suffer from shortcomings that might limit the quality of trained word distributions. The matrix factorization family only uses the statistics of co-occurrence counts, disregarding of the position of word in sentence and word order. The window-based methods only consider local context, which is incapable of involving information outside the context window. While RNN language model theoretically considers all information of the previous sequence, but fails to involve the information of the posterior sequ"
N16-1064,W00-0726,0,0.189932,"esentations, their representations are initialized with uniformly distributed random values, ranging from -0.1 to 0.1. Three typical tagging tasks are used for the evaluation: partof-speech tagging (POS), chunking (CHUNK) and named entity recognition (NER). • The POS tagging experiment is conducted on the Wall Street Journal data from Penn Treebank III (Marcus et al., 1993). Training, development and test sets are split according to in (Collins, 2002). Performance is evaluated by the accuracy of predicted tags on test set. • CHUNK experiment is conducted on the data of CoNLL-2000 shared task (Sang and Buchholz, 2000). Performance is assessed by the F1 score computed by the evaluation script re530 • NER experiment is conducted on the CoNLL2003 shared task (Tjong Kim Sang and De Meulder, 2003). Performance is measured by the F1 score calculated by the evaluation script of the CoNLL-2003 shared task 4 . To focus on the effect of word representation, for all tasks, we use the network with the same hidden structure and input features. The size of input layer is 100, size of BLSTM hidden layer is 128 and output layer size is set as the number of tag types according to the specific tagging task. Input features a"
N16-1064,W03-0419,0,0.169438,"Missing"
N16-1064,D13-1140,0,0.00623146,"hich words have been replaced by minimizing the binary classification error on the training corpus. When the network is trained, W1 contains all trained word representations. In our experiments, to reduce the vocabulary V , each letter of input word is transferred to its lowercase. The upper case information is kept in an additional features x2i which in practice is a threedimensional binary vector to indicate if x1i is full lowercase, full uppercase or leading with a capital letter. Our approach is similar to (Collobert and Weston, 2008) and (Gutmann and Hyv¨arinen, 2012; Mnih and Teh, 2012; Vaswani et al., 2013). All of these works introduce randomly sampled words and train a neural network on a binary classification task, while (Collobert and Weston, 2008) learns representations for a feedforward network and (Gutmann and Hyv¨arinen, 2012; Mnih and Teh, 2012; Vaswani et al., 2013) learns normalization parameters instead of representations. 4 4.1 x2i Experiments Experimental setup To construct corpus for training word representations, we use North American news (Graff, 2008) which contains about 536 million words as unlabeled data. The North American news data is first tokenized with the Penn Treebank"
N16-1064,I08-1002,1,0.148624,"fund 201304490199, 201304490171), and the art and science interdisciplinary funds of Shanghai Jiao Tong University, No. 14X190040031, and the Key Project of National Society Science Foundation of China, No. 15-ZDA041. † *Work performed as an intern in speech group, Microsoft Research Asia ‡ Corresponding author Sequence tagging is a basic structure learning task for natural language processing. Many primary processing tasks over sentence such as word segmentation, named entity recognition and part-of-speech tagging can be formalized as a tagging task (Zhao et al., 2006; Huang and Zhao, 2007; Zhao and Kit, 2008b; Zhao and Kit, 2008a; Zhao et al., 2010; Zhao and Kit, 2011). Recently, many state-of-the-art systems of tagging related tasks are implemented with bidirectional long short-term memory (BLSTM) recurrent neural network (RNN), for example, slot filling (Mesnil et al., 2013), part-of-speech tagging (Huang et al., 2015), and dependency parsing (Dyer et al., 2015) etc. All of these systems use distributed representation of words to involve word level information. Better trained word representations would further improve the state-of-the-art performance of these tasks which makes it worthy to rese"
N16-1064,I08-4017,1,0.772454,"fund 201304490199, 201304490171), and the art and science interdisciplinary funds of Shanghai Jiao Tong University, No. 14X190040031, and the Key Project of National Society Science Foundation of China, No. 15-ZDA041. † *Work performed as an intern in speech group, Microsoft Research Asia ‡ Corresponding author Sequence tagging is a basic structure learning task for natural language processing. Many primary processing tasks over sentence such as word segmentation, named entity recognition and part-of-speech tagging can be formalized as a tagging task (Zhao et al., 2006; Huang and Zhao, 2007; Zhao and Kit, 2008b; Zhao and Kit, 2008a; Zhao et al., 2010; Zhao and Kit, 2011). Recently, many state-of-the-art systems of tagging related tasks are implemented with bidirectional long short-term memory (BLSTM) recurrent neural network (RNN), for example, slot filling (Mesnil et al., 2013), part-of-speech tagging (Huang et al., 2015), and dependency parsing (Dyer et al., 2015) etc. All of these systems use distributed representation of words to involve word level information. Better trained word representations would further improve the state-of-the-art performance of these tasks which makes it worthy to rese"
N16-1064,W06-0127,1,0.0701014,"No. 247619), the Cai Yuanpei Program (CSC fund 201304490199, 201304490171), and the art and science interdisciplinary funds of Shanghai Jiao Tong University, No. 14X190040031, and the Key Project of National Society Science Foundation of China, No. 15-ZDA041. † *Work performed as an intern in speech group, Microsoft Research Asia ‡ Corresponding author Sequence tagging is a basic structure learning task for natural language processing. Many primary processing tasks over sentence such as word segmentation, named entity recognition and part-of-speech tagging can be formalized as a tagging task (Zhao et al., 2006; Huang and Zhao, 2007; Zhao and Kit, 2008b; Zhao and Kit, 2008a; Zhao et al., 2010; Zhao and Kit, 2011). Recently, many state-of-the-art systems of tagging related tasks are implemented with bidirectional long short-term memory (BLSTM) recurrent neural network (RNN), for example, slot filling (Mesnil et al., 2013), part-of-speech tagging (Huang et al., 2015), and dependency parsing (Dyer et al., 2015) etc. All of these systems use distributed representation of words to involve word level information. Better trained word representations would further improve the state-of-the-art performance of"
N19-1307,D18-1245,0,0.0222661,"Missing"
N19-1307,P14-1079,0,0.0393513,"Missing"
N19-1307,D18-1247,0,0.0275421,"Missing"
N19-1307,P11-1055,0,0.203854,"Missing"
N19-1307,P17-1004,0,0.0240559,"Missing"
N19-1307,P16-1200,0,0.324638,"Missing"
N19-1307,P17-1001,0,0.0671664,"Missing"
N19-1307,D18-1243,1,0.89332,"Missing"
N19-1307,D17-1189,0,0.0989015,"Missing"
N19-1307,P09-1113,0,0.524006,"Missing"
N19-1307,P18-1046,0,0.38432,"Missing"
N19-1307,P18-1199,0,0.116797,"Missing"
N19-1307,D17-1187,0,0.0732864,"Missing"
N19-1307,D15-1203,0,0.1747,"Missing"
N19-1307,D12-1042,0,0.204449,"Missing"
N19-1307,P12-1076,0,0.0310016,"Missing"
N19-1307,D18-1157,0,0.317079,"Missing"
N19-1307,D18-1248,0,0.0284302,"Missing"
N19-1340,P98-1013,0,0.310014,"ferent from all previous work, we utilize inter-sentence attention to help model leverage associated information from other known sentences in the memory. To our best knowledge, this is the first time to use memory network in the SRL task. Our evaluation on CoNLL-2009 benchmarks shows that our model outperforms or reaches other syntaxagnostic models on English, and achieves competitive results on Chinese, which indicates that memory network learning from known data is indeed helpful to SRL task. There are several SRL annotation conventions, such as PropBank (Bonial et al., 2012) and FrameNet (Baker et al., 1998). This paper focuses on the former convention. Under PropBank convention, there are two role representation forms, which are span-based SRL, such as CoNLL 2005 and CoNLL 2012 shared tasks, and dependencybased SRL, such as CoNLL 2009 shared task. The former uses span to represent argument, while the latter uses the headword of the span to represent the argument. As the latter has been more actively studied due to dependency style SRL for convenient machine learning, we will focus on dependency SRL only in this work. Given a sentence S, the goal of dependency SRL task is to find all the predicat"
N19-1340,K18-2005,0,0.0141129,") word embeddings. For English, We independently determine the best distance calculating method and the best merging method one after another. First, we select a distance according to the results on development set and then we determine the merging method with the selected distance method. At last we explore the impact of memory size. For Chinese, we obtain the result with similar parameters as for the best model in English. The English and Chinese GloVe word embeddings are both trained on Wikipedia. The pretrained English ELMo model is from (Peters et al., 2018), and the Chinese one is from (Che et al., 2018), which is hosted at (Fares et al., 2017). The model is trained for maximum 20 epochs for the nearly best model based on development set results. We re-run our model using different initialized parameters for 4 times and report the average performance4 . 3.1 Results For the predicate disambiguation, we use the same one from (He et al., 2018b) with the precisions 4 Our implementation is publicly available at https:// github.com/Frozenmad/AMN_SRL. 3365 System (syntax-aware single) (Zhao et al., 2009a) (Roth and Lapata, 2016) (Marcheggiani and Titov, 2017) (He et al., 2018b) (Li et al., 2018) Sys"
N19-1340,W17-0237,0,0.0144451,"pendently determine the best distance calculating method and the best merging method one after another. First, we select a distance according to the results on development set and then we determine the merging method with the selected distance method. At last we explore the impact of memory size. For Chinese, we obtain the result with similar parameters as for the best model in English. The English and Chinese GloVe word embeddings are both trained on Wikipedia. The pretrained English ELMo model is from (Peters et al., 2018), and the Chinese one is from (Che et al., 2018), which is hosted at (Fares et al., 2017). The model is trained for maximum 20 epochs for the nearly best model based on development set results. We re-run our model using different initialized parameters for 4 times and report the average performance4 . 3.1 Results For the predicate disambiguation, we use the same one from (He et al., 2018b) with the precisions 4 Our implementation is publicly available at https:// github.com/Frozenmad/AMN_SRL. 3365 System (syntax-aware single) (Zhao et al., 2009a) (Roth and Lapata, 2016) (Marcheggiani and Titov, 2017) (He et al., 2018b) (Li et al., 2018) System (syntax-agnostic single) (Marcheggian"
N19-1340,D15-1112,0,0.034862,"Missing"
N19-1340,P18-2058,0,0.206142,"017) delivered a full study on the influence of RNN training and decoding strategies. Whether to use the syntactic information for SRL is also studied actively (He et al., 2017, 2018b). Since the recent work of (Marcheggiani et al., 2017), which surprisingly shows syntax-agnostic dependency SRL for the first time can be rival of syntax-aware models, SRL has been more and more formulized into standard sequence labeling task on a basis of keeping syntax unavailable. A series of work on SRL received further performance improvement following this line through further refining neural model design (He et al., 2018a). Different from all previous work, we propose to introduce an associated memory network which builds memory from known data through the inter-sentence attention to enhance syntaxagnostic model even further. Inspired by the observation that people always refer to other similar problems and their solutions when dealing with a problem they have never seen, like query in their memory, we want to utilize similar known samples which include the associated sentences and their annotated labels to help model label target sentence. To reach such a goal, we adopt a memory network component, and use in"
N19-1340,P17-1044,0,0.461086,"bal et al., 2017). Generally, SRL is decomposed into four classification subtasks in pipeline systems, consisting of ∗ Corresponding author. This paper was partially supported by National Key Research and Development Program of China (No. 2017YFB0304100), National Natural Science Foundation of China (No. U1836222 and No. 61733011) and Key Project of National Society Science Foundation of China (No. 15-ZDA041). predicate identification, predicate disambiguation, argument identification, and argument classification. In recent years, great attention (Zhou and Xu, 2015; Marcheggiani et al., 2017; He et al., 2017, 2018a,b) has been turned to deep learning method, especially Long Short-term Memory (LSTM) network for learning with automatically extracted features. (Zhou and Xu, 2015) proposed the first end-to-end recurrent neural network (RNN) to solve the SRL task. (Marcheggiani et al., 2017) studied several predicate-specified embedding and decoding methods. (He et al., 2017) delivered a full study on the influence of RNN training and decoding strategies. Whether to use the syntactic information for SRL is also studied actively (He et al., 2017, 2018b). Since the recent work of (Marcheggiani et al., 2"
N19-1340,P18-1192,1,0.884022,"017) delivered a full study on the influence of RNN training and decoding strategies. Whether to use the syntactic information for SRL is also studied actively (He et al., 2017, 2018b). Since the recent work of (Marcheggiani et al., 2017), which surprisingly shows syntax-agnostic dependency SRL for the first time can be rival of syntax-aware models, SRL has been more and more formulized into standard sequence labeling task on a basis of keeping syntax unavailable. A series of work on SRL received further performance improvement following this line through further refining neural model design (He et al., 2018a). Different from all previous work, we propose to introduce an associated memory network which builds memory from known data through the inter-sentence attention to enhance syntaxagnostic model even further. Inspired by the observation that people always refer to other similar problems and their solutions when dealing with a problem they have never seen, like query in their memory, we want to utilize similar known samples which include the associated sentences and their annotated labels to help model label target sentence. To reach such a goal, we adopt a memory network component, and use in"
N19-1340,D18-1262,1,0.89867,"Missing"
N19-1340,P17-2031,0,0.0605937,"Missing"
N19-1340,K17-1041,0,0.315228,"d question answering (Abujabal et al., 2017). Generally, SRL is decomposed into four classification subtasks in pipeline systems, consisting of ∗ Corresponding author. This paper was partially supported by National Key Research and Development Program of China (No. 2017YFB0304100), National Natural Science Foundation of China (No. U1836222 and No. 61733011) and Key Project of National Society Science Foundation of China (No. 15-ZDA041). predicate identification, predicate disambiguation, argument identification, and argument classification. In recent years, great attention (Zhou and Xu, 2015; Marcheggiani et al., 2017; He et al., 2017, 2018a,b) has been turned to deep learning method, especially Long Short-term Memory (LSTM) network for learning with automatically extracted features. (Zhou and Xu, 2015) proposed the first end-to-end recurrent neural network (RNN) to solve the SRL task. (Marcheggiani et al., 2017) studied several predicate-specified embedding and decoding methods. (He et al., 2017) delivered a full study on the influence of RNN training and decoding strategies. Whether to use the syntactic information for SRL is also studied actively (He et al., 2017, 2018b). Since the recent work of (March"
N19-1340,D17-1159,0,0.0485552,"Missing"
N19-1340,D16-1147,0,0.205698,"and their solutions when dealing with a problem they have never seen, like query in their memory, we want to utilize similar known samples which include the associated sentences and their annotated labels to help model label target sentence. To reach such a goal, we adopt a memory network component, and use inter-sentence attention to fully exploit the information in memory. 3361 Proceedings of NAACL-HLT 2019, pages 3361–3371 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics Based on Memory Network (Weston et al., 2014; Sukhbaatar et al., 2015), (Miller et al., 2016) proposed Key-Value Memory Network (KV-MemNN) to solve Question Answering problem and gain large progress. Our proposed method is similar to KV-MemNN, but with a different definition of key-value and different information distilling process. Thus, we propose a carefully designed inter-sentence attention mechanism to handle it. Recently, there are also some attempts to make use of attention mechanism in SRL task. (Tan et al., 2018; Strubell et al., 2018) focus on selfattention, which only uses the information of the input sentence as the source of attention. (Cai et al., 2018) makes use of biaf"
N19-1340,D14-1162,0,0.0815224,"k (He et al., 2018b) to handle the predicate disambiguation subtask. This work will only focus on the argument labeling subtask through sequence labeling formalization. We first describe our base model in Section 2.1. Then we introduce the proposed associated memory network including the inter-sentence attention design and label merging strategies in Section 2.2. The full model architecture is shown in Figure 1. 2.1 Base Model Word Embedding We use the concatenation of the following embeddings as the representation for every word. (1) dre Random-initialized word embedding xre i ∈ R (2) GloVe (Pennington et al., 2014) word embedding xpe ∈ Rdpe pre-trained on 6B tokens i (3) Random-initialized part-of-speech (POS) tag embedding xpos ∈ Rdpos (4) Random-initialized i dle (5) Contextualized lemma embedding xle i ∈ R word embedding derived by applying fully condce (Penected layer on ELMo embedding xce i ∈R ters et al., 2018), and (6) Random-initialized predicate specified flag embedding xpred ∈ Rdpred . i The final representation of each word is: pe pos pred ce xi = xre ◦ xle i ◦ xi ◦ xi i ◦ xi ◦ xi where ◦ stands for concatenation operator. 1 In CoNLL-2009 task, the predicates information is already identified"
N19-1340,N18-1202,0,0.0232524,"cept the pre-trained GloVe (Pennington et al., 2014) word embeddings. For English, We independently determine the best distance calculating method and the best merging method one after another. First, we select a distance according to the results on development set and then we determine the merging method with the selected distance method. At last we explore the impact of memory size. For Chinese, we obtain the result with similar parameters as for the best model in English. The English and Chinese GloVe word embeddings are both trained on Wikipedia. The pretrained English ELMo model is from (Peters et al., 2018), and the Chinese one is from (Che et al., 2018), which is hosted at (Fares et al., 2017). The model is trained for maximum 20 epochs for the nearly best model based on development set results. We re-run our model using different initialized parameters for 4 times and report the average performance4 . 3.1 Results For the predicate disambiguation, we use the same one from (He et al., 2018b) with the precisions 4 Our implementation is publicly available at https:// github.com/Frozenmad/AMN_SRL. 3365 System (syntax-aware single) (Zhao et al., 2009a) (Roth and Lapata, 2016) (Marcheggiani and Titov"
N19-1340,P05-1072,0,0.0827457,"ottom. Their predicate is bolded. 3.7 Analysis of Performance on Distance We compare the performance concerning with the distance of argument and predicate on our best model and base model in Figure 2, from which we can observe that our model performs better nearly at any distance. 3.8 Case Study on AMN To explore how the AMN works in the model, we visualize the similarity matrix M of some sentences from development set in Figure 6. The input sentence is itA1 shouldAM −M OD runv foreverAM −T M P . And the associated sentence is itA1 woAM −M OD n´tAM −N EG happenv Related Works Early attempts (Pradhan et al., 2005; Zhao et al., 2009a,b, 2013; Roth and Woodsend, 2014) to the SRL task were mainly linear classifiers. The main focus was how to find proper feature templates that can best describe the sentences. (Pradhan et al., 2005) utilized a SVM classifier with rich syntactic features. (Toutanova et al., 2008) took the structural constraint into consideration by using a global reranker. (Zhao et al., 2009c) adopted a maximum entropy model with large scale feature template selection. (Roth and Woodsend, 2014) explored the distributional word representations as new feature to gain more powerful models. Rec"
N19-1340,P16-1113,0,0.0252131,"Missing"
N19-1340,D14-1045,0,0.0230776,"Performance on Distance We compare the performance concerning with the distance of argument and predicate on our best model and base model in Figure 2, from which we can observe that our model performs better nearly at any distance. 3.8 Case Study on AMN To explore how the AMN works in the model, we visualize the similarity matrix M of some sentences from development set in Figure 6. The input sentence is itA1 shouldAM −M OD runv foreverAM −T M P . And the associated sentence is itA1 woAM −M OD n´tAM −N EG happenv Related Works Early attempts (Pradhan et al., 2005; Zhao et al., 2009a,b, 2013; Roth and Woodsend, 2014) to the SRL task were mainly linear classifiers. The main focus was how to find proper feature templates that can best describe the sentences. (Pradhan et al., 2005) utilized a SVM classifier with rich syntactic features. (Toutanova et al., 2008) took the structural constraint into consideration by using a global reranker. (Zhao et al., 2009c) adopted a maximum entropy model with large scale feature template selection. (Roth and Woodsend, 2014) explored the distributional word representations as new feature to gain more powerful models. Recently, a great attention has been paid on neural netwo"
N19-1340,D18-1548,0,0.0282082,"Missing"
N19-1340,J08-2002,0,0.0413221,"on AMN To explore how the AMN works in the model, we visualize the similarity matrix M of some sentences from development set in Figure 6. The input sentence is itA1 shouldAM −M OD runv foreverAM −T M P . And the associated sentence is itA1 woAM −M OD n´tAM −N EG happenv Related Works Early attempts (Pradhan et al., 2005; Zhao et al., 2009a,b, 2013; Roth and Woodsend, 2014) to the SRL task were mainly linear classifiers. The main focus was how to find proper feature templates that can best describe the sentences. (Pradhan et al., 2005) utilized a SVM classifier with rich syntactic features. (Toutanova et al., 2008) took the structural constraint into consideration by using a global reranker. (Zhao et al., 2009c) adopted a maximum entropy model with large scale feature template selection. (Roth and Woodsend, 2014) explored the distributional word representations as new feature to gain more powerful models. Recently, a great attention has been paid on neural networks. (Zhou and Xu, 2015) proposed an end-to-end model using stacked BiLSTM network combined with CRF decoder without any syntactic input. (Marcheggiani et al., 2017) explored the predicate-specified encoding and decoding and also provided a synta"
N19-1340,W09-1209,1,0.886388,"M1raw , M2raw , ..., Mm Then, we perform softmax operation on every row in M raw to normalize the value so that it can be considered as probability from input sentence S to all associated sentences Aj . 3364 raw , M raw ..., M raw ]) γi = f ([Mi,1 i,2 i,nall Name dre dpe dpos dle dce dpred dae m ke ka de da rd lr Meaning random word embedding pre-trained word embedding POS embedding lemma embedding contextualized embedding flag embedding argument embedding memory size #LST Me layers #LST Ma layers LST Me hidden state LST Ma hidden state dropout rate learning rate System (syntax-aware single) (Zhao et al., 2009a) (Zhao et al., 2009c) (FitzGerald et al., 2015) (Roth and Lapata, 2016) (Marcheggiani and Titov, 2017) (He et al., 2018b) (Li et al., 2018) System (syntax-aware ensemble) (FitzGerald et al., 2015) (Roth and Lapata, 2016) (Marcheggiani and Titov, 2017) System (syntax-agnostic single) (Marcheggiani et al., 2017) (He et al., 2018b) (Cai et al., 2018) (Li et al., 2018) Ours ( + AMN + ELMo) Value 100 100 32 100 128 16 128 4 2 3 512 512 0.1 0.001 R 85.3 86.8 89.3 89.3 R 85.7 87.7 R 86.8 87.9 89.2 87.9 89.2 F1 86.2 85.4 86.7 86.7 88.0 89.5 89.8 F1 87.7 87.9 89.1 F1 87.7 88.7 89.6 88.7 89.6 Table 2:"
N19-1340,D09-1004,1,0.887887,"M1raw , M2raw , ..., Mm Then, we perform softmax operation on every row in M raw to normalize the value so that it can be considered as probability from input sentence S to all associated sentences Aj . 3364 raw , M raw ..., M raw ]) γi = f ([Mi,1 i,2 i,nall Name dre dpe dpos dle dce dpred dae m ke ka de da rd lr Meaning random word embedding pre-trained word embedding POS embedding lemma embedding contextualized embedding flag embedding argument embedding memory size #LST Me layers #LST Ma layers LST Me hidden state LST Ma hidden state dropout rate learning rate System (syntax-aware single) (Zhao et al., 2009a) (Zhao et al., 2009c) (FitzGerald et al., 2015) (Roth and Lapata, 2016) (Marcheggiani and Titov, 2017) (He et al., 2018b) (Li et al., 2018) System (syntax-aware ensemble) (FitzGerald et al., 2015) (Roth and Lapata, 2016) (Marcheggiani and Titov, 2017) System (syntax-agnostic single) (Marcheggiani et al., 2017) (He et al., 2018b) (Cai et al., 2018) (Li et al., 2018) Ours ( + AMN + ELMo) Value 100 100 32 100 128 16 128 4 2 3 512 512 0.1 0.001 R 85.3 86.8 89.3 89.3 R 85.7 87.7 R 86.8 87.9 89.2 87.9 89.2 F1 86.2 85.4 86.7 86.7 88.0 89.5 89.8 F1 87.7 87.9 89.1 F1 87.7 88.7 89.6 88.7 89.6 Table 2:"
N19-1340,W09-1208,1,0.854841,"M1raw , M2raw , ..., Mm Then, we perform softmax operation on every row in M raw to normalize the value so that it can be considered as probability from input sentence S to all associated sentences Aj . 3364 raw , M raw ..., M raw ]) γi = f ([Mi,1 i,2 i,nall Name dre dpe dpos dle dce dpred dae m ke ka de da rd lr Meaning random word embedding pre-trained word embedding POS embedding lemma embedding contextualized embedding flag embedding argument embedding memory size #LST Me layers #LST Ma layers LST Me hidden state LST Ma hidden state dropout rate learning rate System (syntax-aware single) (Zhao et al., 2009a) (Zhao et al., 2009c) (FitzGerald et al., 2015) (Roth and Lapata, 2016) (Marcheggiani and Titov, 2017) (He et al., 2018b) (Li et al., 2018) System (syntax-aware ensemble) (FitzGerald et al., 2015) (Roth and Lapata, 2016) (Marcheggiani and Titov, 2017) System (syntax-agnostic single) (Marcheggiani et al., 2017) (He et al., 2018b) (Cai et al., 2018) (Li et al., 2018) Ours ( + AMN + ELMo) Value 100 100 32 100 128 16 128 4 2 3 512 512 0.1 0.001 R 85.3 86.8 89.3 89.3 R 85.7 87.7 R 86.8 87.9 89.2 87.9 89.2 F1 86.2 85.4 86.7 86.7 88.0 89.5 89.8 F1 87.7 87.9 89.1 F1 87.7 88.7 89.6 88.7 89.6 Table 2:"
N19-1340,P15-1109,0,0.274382,"iu et al., 2016) and question answering (Abujabal et al., 2017). Generally, SRL is decomposed into four classification subtasks in pipeline systems, consisting of ∗ Corresponding author. This paper was partially supported by National Key Research and Development Program of China (No. 2017YFB0304100), National Natural Science Foundation of China (No. U1836222 and No. 61733011) and Key Project of National Society Science Foundation of China (No. 15-ZDA041). predicate identification, predicate disambiguation, argument identification, and argument classification. In recent years, great attention (Zhou and Xu, 2015; Marcheggiani et al., 2017; He et al., 2017, 2018a,b) has been turned to deep learning method, especially Long Short-term Memory (LSTM) network for learning with automatically extracted features. (Zhou and Xu, 2015) proposed the first end-to-end recurrent neural network (RNN) to solve the SRL task. (Marcheggiani et al., 2017) studied several predicate-specified embedding and decoding methods. (He et al., 2017) delivered a full study on the influence of RNN training and decoding strategies. Whether to use the syntactic information for SRL is also studied actively (He et al., 2017, 2018b). Sinc"
N19-1340,C98-1013,0,\N,Missing
P09-1007,J90-2002,0,0.510764,"Missing"
P09-1007,D08-1092,0,0.0385535,"from CTL, City University of Hong Kong. 1 It is a tradition to call an annotated syntactic corpus as treebank in parsing community. 55 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 55–63, c Suntec, Singapore, 2-7 August 2009. 2009 ACL and AFNLP target language can effectively make use of them by only considering the most related information extracted from the translated text. The basic idea to support this work is to make use of the semantic connection between different languages. In this sense, it is related to the work of (Merlo et al., 2002) and (Burkett and Klein, 2008). The former showed that complementary information about English verbs can be extracted from their translations in a second language (Chinese) and the use of multilingual features improves classification performance of the English verbs. The latter iteratively trained a model to maximize the marginal likelihood of tree pairs, with alignments treated as latent variables, and then jointly parsing bilingual sentences in a translation pair. The proposed parser using features from monolingual and mutual constraints helped its log-linear model to achieve better performance for both monolingual parse"
P09-1007,I08-1012,0,0.573078,"n draw some interests in recent years. Typical domain adaptation tasks often assume annotated data in new domain absent or insufficient and a large scale unlabeled data available. As unlabeled data are concerned, semi-supervised or unsupervised methods will be naturally adopted. In previous works, two basic types of methods can be identified to enhance an existing parser from additional resources. The first is usually focus on exploiting automatic generated labeled data from the unlabeled data (Steedman et al., 2003; McClosky et al., 2006; Reichart and Rappoport, 2007; Sagae and Tsujii, 2007; Chen et al., 2008), the second is on combining supervised and unsupervised methods, and only unlabeled data are considered (Smith and Eisner, 2006; Wang and Schuurmans, 2008; Koo et al., 2008). Our purpose in this study is to obtain a further performance enhancement by exploiting treebanks in other languages. This is similar to the above first type of methods, some assistant data should be automatically generated for the subsequent processing. The differences are what type of data are concerned with and how they are produced. In our method, a machine translation method is applied to tackle golden-standard treeb"
P09-1007,D07-1098,0,0.0104809,"ting overlapped features as our work in (Zhao and Kit, 2008), especially, those character-level ones for Chinese parsing. Our implementation of maximum entropy adopts L-BFGS algorithm for parameter optimization as usual. 4.2 Parsing using a Beam Search Algorithm In Table 2, the feature preactn returns the previous parsing action type, and the subscript n stands for the action order before the current action. These are a group of Markovian features. Without this type of features, a shift-reduce parser may directly scan through an input sequence in linear time. Otherwise, following the work of (Duan et al., 2007) and (Zhao, 2009), the parsing algorithm is to search a parsing action sequence with the maximal probability. Sdi = argmax Y p(di |di−1 di−2 ...), i where Sdi is the object parsing action sequence, p(di |di−1 ...) is the conditional probability, and di 58 Figure 1: A comparison before and after translation is i-th parsing action. We use a beam search algorithm to find the object parsing action sequence. Table 2: Features for Parsing 5 Exploiting the Translated Treebank in .f orm, n = 0, 1 i.f orm + i1 .f orm in .char2 + in+1 .char2 , n = −1, 0 i.char−1 + i1 .char−1 in .char−2 n = 0, 3 i1 .char"
P09-1007,P06-1072,0,0.011487,"nsufficient and a large scale unlabeled data available. As unlabeled data are concerned, semi-supervised or unsupervised methods will be naturally adopted. In previous works, two basic types of methods can be identified to enhance an existing parser from additional resources. The first is usually focus on exploiting automatic generated labeled data from the unlabeled data (Steedman et al., 2003; McClosky et al., 2006; Reichart and Rappoport, 2007; Sagae and Tsujii, 2007; Chen et al., 2008), the second is on combining supervised and unsupervised methods, and only unlabeled data are considered (Smith and Eisner, 2006; Wang and Schuurmans, 2008; Koo et al., 2008). Our purpose in this study is to obtain a further performance enhancement by exploiting treebanks in other languages. This is similar to the above first type of methods, some assistant data should be automatically generated for the subsequent processing. The differences are what type of data are concerned with and how they are produced. In our method, a machine translation method is applied to tackle golden-standard treebank, while all the previous works focus on the unlabeled data. Although cross-language technique has been used in other natural"
P09-1007,P08-1068,0,0.00863736,"ble. As unlabeled data are concerned, semi-supervised or unsupervised methods will be naturally adopted. In previous works, two basic types of methods can be identified to enhance an existing parser from additional resources. The first is usually focus on exploiting automatic generated labeled data from the unlabeled data (Steedman et al., 2003; McClosky et al., 2006; Reichart and Rappoport, 2007; Sagae and Tsujii, 2007; Chen et al., 2008), the second is on combining supervised and unsupervised methods, and only unlabeled data are considered (Smith and Eisner, 2006; Wang and Schuurmans, 2008; Koo et al., 2008). Our purpose in this study is to obtain a further performance enhancement by exploiting treebanks in other languages. This is similar to the above first type of methods, some assistant data should be automatically generated for the subsequent processing. The differences are what type of data are concerned with and how they are produced. In our method, a machine translation method is applied to tackle golden-standard treebank, while all the previous works focus on the unlabeled data. Although cross-language technique has been used in other natural language processing tasks, it is basically new"
P09-1007,E03-1008,0,0.0270136,"a resources to enhance an existing parser, it is related to domain adaption for parsing that has been draw some interests in recent years. Typical domain adaptation tasks often assume annotated data in new domain absent or insufficient and a large scale unlabeled data available. As unlabeled data are concerned, semi-supervised or unsupervised methods will be naturally adopted. In previous works, two basic types of methods can be identified to enhance an existing parser from additional resources. The first is usually focus on exploiting automatic generated labeled data from the unlabeled data (Steedman et al., 2003; McClosky et al., 2006; Reichart and Rappoport, 2007; Sagae and Tsujii, 2007; Chen et al., 2008), the second is on combining supervised and unsupervised methods, and only unlabeled data are considered (Smith and Eisner, 2006; Wang and Schuurmans, 2008; Koo et al., 2008). Our purpose in this study is to obtain a further performance enhancement by exploiting treebanks in other languages. This is similar to the above first type of methods, some assistant data should be automatically generated for the subsequent processing. The differences are what type of data are concerned with and how they are"
P09-1007,P06-1043,0,0.0262404,"an existing parser, it is related to domain adaption for parsing that has been draw some interests in recent years. Typical domain adaptation tasks often assume annotated data in new domain absent or insufficient and a large scale unlabeled data available. As unlabeled data are concerned, semi-supervised or unsupervised methods will be naturally adopted. In previous works, two basic types of methods can be identified to enhance an existing parser from additional resources. The first is usually focus on exploiting automatic generated labeled data from the unlabeled data (Steedman et al., 2003; McClosky et al., 2006; Reichart and Rappoport, 2007; Sagae and Tsujii, 2007; Chen et al., 2008), the second is on combining supervised and unsupervised methods, and only unlabeled data are considered (Smith and Eisner, 2006; Wang and Schuurmans, 2008; Koo et al., 2008). Our purpose in this study is to obtain a further performance enhancement by exploiting treebanks in other languages. This is similar to the above first type of methods, some assistant data should be automatically generated for the subsequent processing. The differences are what type of data are concerned with and how they are produced. In our metho"
P09-1007,P08-1061,0,0.0118388,"scale unlabeled data available. As unlabeled data are concerned, semi-supervised or unsupervised methods will be naturally adopted. In previous works, two basic types of methods can be identified to enhance an existing parser from additional resources. The first is usually focus on exploiting automatic generated labeled data from the unlabeled data (Steedman et al., 2003; McClosky et al., 2006; Reichart and Rappoport, 2007; Sagae and Tsujii, 2007; Chen et al., 2008), the second is on combining supervised and unsupervised methods, and only unlabeled data are considered (Smith and Eisner, 2006; Wang and Schuurmans, 2008; Koo et al., 2008). Our purpose in this study is to obtain a further performance enhancement by exploiting treebanks in other languages. This is similar to the above first type of methods, some assistant data should be automatically generated for the subsequent processing. The differences are what type of data are concerned with and how they are produced. In our method, a machine translation method is applied to tackle golden-standard treebank, while all the previous works focus on the unlabeled data. Although cross-language technique has been used in other natural language processing tasks,"
P09-1007,D07-1013,0,0.0336862,"om the source language to the target one. In detail, a word-based decoding is used, which adopts a loglinear framework as in (Och and Ney, 2002) with only two features, translation model and language model, P exp[ 2i=1 λi hi (c, e)] P (c|e) = P P2 c exp[ i=1 λi hi (c, e)] Where h1 (c, e) = log(pγ (c|e)) is the translation model, which is converted from the bilingual lexicon, and h2 (c, e) = log(pθ (c)) 4 Dependency Parsing: Baseline 4.1 Learning Model and Features is the language model, a word trigram model trained from the CTB. In our experiment, we set two weights λ1 = λ2 = 1. According to (McDonald and Nivre, 2007), all data-driven models for dependency parsing that have been proposed in recent years can be described as either graph-based or transition-based. 2 StarDict is an open source dictionary software, available at http://stardict.sourceforge.net/. 57 With notations defined in Table 1, a feature set as shown in Table 2 is adopted. Here, we explain some terms in Tables 1 and 2. We used a large scale feature selection approach as in (Zhao et al., 2009) to obtain the feature set in Table 2. Some feature notations in this paper are also borrowed from that work. The feature curroot returns the root of"
P09-1007,W05-1516,0,0.0299467,"Missing"
P09-1007,E06-1011,0,0.0426299,"Missing"
P09-1007,W03-3023,0,0.385266,"Missing"
P09-1007,P05-1012,0,0.211703,"d-by-word decoding, where not a parallel corpus but a bilingual lexicon is necessary, is adopted for the treebank translation. Using an ensemble method, the key information extracted from word pairs with dependency relations in the translated text is effectively integrated into the parser for the target language. The proposed method is evaluated in English and Chinese treebanks. It is shown that a translated English treebank helps a Chinese parser obtain a state-ofthe-art result. 1 Introduction Although supervised learning methods bring stateof-the-art outcome for dependency parser inferring (McDonald et al., 2005; Hall et al., 2007), a large enough data set is often required for specific parsing accuracy according to this type of methods. However, to annotate syntactic structure, either phrase- or dependency-based, is a costly job. Until now, the largest treebanks1 in various languages for syntax learning are with around one million words (or some other similar units). Limited data stand in the way of further performance enhancement. This is the case for each individual language at least. But, this is not the case as we observe all treebanks in different languages as a whole. For example, of ten treeb"
P09-1007,C08-1132,0,0.217068,"ted in (Wang et al., 2007). The experimental results in (McDonald and Nivre, 2007) show a negative impact on the parsing accuracy from too long dependency relation. For the proposed method, the improvement relative to dependency length is shown in Figure 2. From the figure, it is seen that our method gives observable better performance when dependency lengths are larger than 4. Although word order is changed, the results here show that the useful information from the translated treebank still help those long distance dependencies. 4 There is a slight exception: using the same data splitting, (Yu et al., 2008) reported UAS without p as 0.873 versus ours, 0.870. 61 chosen to generate some additional features to enhance the parser for the target language. The experimental results in English and Chinese treebanks show the proposed method is effective and helps the Chinese parser in this work achieve a state-of-the-art result. Note that our method is evaluated in two treebanks with a similar annotation style and it avoids using too many linguistic properties. Thus the method is in the hope of being used in other similarly annotated treebanks 5 . For an immediate example, we may adopt a translated Chine"
P09-1007,P02-1027,0,0.011287,"by a research fellowship from CTL, City University of Hong Kong. 1 It is a tradition to call an annotated syntactic corpus as treebank in parsing community. 55 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 55–63, c Suntec, Singapore, 2-7 August 2009. 2009 ACL and AFNLP target language can effectively make use of them by only considering the most related information extracted from the translated text. The basic idea to support this work is to make use of the semantic connection between different languages. In this sense, it is related to the work of (Merlo et al., 2002) and (Burkett and Klein, 2008). The former showed that complementary information about English verbs can be extracted from their translations in a second language (Chinese) and the use of multilingual features improves classification performance of the English verbs. The latter iteratively trained a model to maximize the marginal likelihood of tree pairs, with alignments treated as latent variables, and then jointly parsing bilingual sentences in a translation pair. The proposed parser using features from monolingual and mutual constraints helped its log-linear model to achieve better performa"
P09-1007,I08-3008,0,0.108101,"g bilingual sentences in a translation pair. The proposed parser using features from monolingual and mutual constraints helped its log-linear model to achieve better performance for both monolingual parsers and machine translation system. In this work, cross-language features will be also adopted as the latter work. However, although it is not essentially different, we only focus on dependency parsing itself, while the parsing scheme in (Burkett and Klein, 2008) based on a constituent representation. Among of existing works that we are aware of, we regard that the most similar one to ours is (Zeman and Resnik, 2008), who adapted a parser to a new language that is much poorer in linguistic resources than the source language. However, there are two main differences between their work and ours. The first is that they considered a pair of sufficiently related languages, Danish and Swedish, and made full use of the similar characteristics of two languages. Here we consider two quite different languages, English and Chinese. As fewer language properties are concerned, our approach holds the more possibility to be extended to other language pairs than theirs. The second is that a parallel corpus is required for"
P09-1007,W08-2127,1,0.799644,"g actions, a shift action and a reduce action are also defined to maintain the stack and the unprocessed sequence. In this work, we adopt a left-to-right arc-eager parsing model, that means that the parser scans the input sequence from left to right and right dependents are attached to their heads as soon as possible (Hall et al., 2007). While memory-based and margin-based learning approaches such as support vector machines are popularly applied to shift-reduce parsing, we apply maximum entropy model as the learning model for efficient training and adopting overlapped features as our work in (Zhao and Kit, 2008), especially, those character-level ones for Chinese parsing. Our implementation of maximum entropy adopts L-BFGS algorithm for parameter optimization as usual. 4.2 Parsing using a Beam Search Algorithm In Table 2, the feature preactn returns the previous parsing action type, and the subscript n stands for the action order before the current action. These are a group of Markovian features. Without this type of features, a shift-reduce parser may directly scan through an input sequence in linear time. Otherwise, following the work of (Duan et al., 2007) and (Zhao, 2009), the parsing algorithm i"
P09-1007,W03-3017,0,0.04529,"he first letter of POS tag of word coarse POS: the first two POS tags of word the left nearest verb The first character of a word The first two characters of a word The last character of a word The last two characters of a word ’s, i.e., ‘s.dprel’ means dependent label of character in the top of stack Feature combination, i.e., ‘s.char+i.char’ means both s.char and i.char work as a feature function. Although the former will be also used as comparison, the latter is chosen as the main parsing framework by this study for the sake of efficiency. In detail, a shift-reduce method is adopted as in (Nivre, 2003), where a classifier is used to make a parsing decision step by step. In each step, the classifier checks a word pair, namely, s, the top of a stack that consists of the processed words, and, i, the first word in the (input) unprocessed sequence, to determine if a dependent relation should be established between them. Besides two dependency arc building actions, a shift action and a reduce action are also defined to maintain the stack and the unprocessed sequence. In this work, we adopt a left-to-right arc-eager parsing model, that means that the parser scans the input sequence from left to ri"
P09-1007,W09-1208,1,0.28587,"nd Features is the language model, a word trigram model trained from the CTB. In our experiment, we set two weights λ1 = λ2 = 1. According to (McDonald and Nivre, 2007), all data-driven models for dependency parsing that have been proposed in recent years can be described as either graph-based or transition-based. 2 StarDict is an open source dictionary software, available at http://stardict.sourceforge.net/. 57 With notations defined in Table 1, a feature set as shown in Table 2 is adopted. Here, we explain some terms in Tables 1 and 2. We used a large scale feature selection approach as in (Zhao et al., 2009) to obtain the feature set in Table 2. Some feature notations in this paper are also borrowed from that work. The feature curroot returns the root of a partial parsing tree that includes a specified node. The feature charseq returns a character sequence whose members are collected from all identified children for a specified word. In Table 2, as for concatenating multiple substrings into a feature string, there are two ways, seq and bag. The former is to concatenate all substrings without do something special. The latter will remove all duplicated substrings, sort the rest and concatenate all"
P09-1007,P02-1038,0,0.0152292,"y reached in fact, as the following case is frequently encountered, multiple English words have to be translated into one Chinese word. To solve this problem, we use a policy that lets the output Chinese word only inherits the attached information of the highest syntactic head in the original multiple English words. 3.2 Translation A word-by-word statistical machine translation strategy is adopted to translate words attached with the respective dependency information from the source language to the target one. In detail, a word-based decoding is used, which adopts a loglinear framework as in (Och and Ney, 2002) with only two features, translation model and language model, P exp[ 2i=1 λi hi (c, e)] P (c|e) = P P2 c exp[ i=1 λi hi (c, e)] Where h1 (c, e) = log(pγ (c|e)) is the translation model, which is converted from the bilingual lexicon, and h2 (c, e) = log(pθ (c)) 4 Dependency Parsing: Baseline 4.1 Learning Model and Features is the language model, a word trigram model trained from the CTB. In our experiment, we set two weights λ1 = λ2 = 1. According to (McDonald and Nivre, 2007), all data-driven models for dependency parsing that have been proposed in recent years can be described as either grap"
P09-1007,P07-1078,0,0.0121418,"is related to domain adaption for parsing that has been draw some interests in recent years. Typical domain adaptation tasks often assume annotated data in new domain absent or insufficient and a large scale unlabeled data available. As unlabeled data are concerned, semi-supervised or unsupervised methods will be naturally adopted. In previous works, two basic types of methods can be identified to enhance an existing parser from additional resources. The first is usually focus on exploiting automatic generated labeled data from the unlabeled data (Steedman et al., 2003; McClosky et al., 2006; Reichart and Rappoport, 2007; Sagae and Tsujii, 2007; Chen et al., 2008), the second is on combining supervised and unsupervised methods, and only unlabeled data are considered (Smith and Eisner, 2006; Wang and Schuurmans, 2008; Koo et al., 2008). Our purpose in this study is to obtain a further performance enhancement by exploiting treebanks in other languages. This is similar to the above first type of methods, some assistant data should be automatically generated for the subsequent processing. The differences are what type of data are concerned with and how they are produced. In our method, a machine translation metho"
P09-1007,E09-1100,1,0.164926,"as our work in (Zhao and Kit, 2008), especially, those character-level ones for Chinese parsing. Our implementation of maximum entropy adopts L-BFGS algorithm for parameter optimization as usual. 4.2 Parsing using a Beam Search Algorithm In Table 2, the feature preactn returns the previous parsing action type, and the subscript n stands for the action order before the current action. These are a group of Markovian features. Without this type of features, a shift-reduce parser may directly scan through an input sequence in linear time. Otherwise, following the work of (Duan et al., 2007) and (Zhao, 2009), the parsing algorithm is to search a parsing action sequence with the maximal probability. Sdi = argmax Y p(di |di−1 di−2 ...), i where Sdi is the object parsing action sequence, p(di |di−1 ...) is the conditional probability, and di 58 Figure 1: A comparison before and after translation is i-th parsing action. We use a beam search algorithm to find the object parsing action sequence. Table 2: Features for Parsing 5 Exploiting the Translated Treebank in .f orm, n = 0, 1 i.f orm + i1 .f orm in .char2 + in+1 .char2 , n = −1, 0 i.char−1 + i1 .char−1 in .char−2 n = 0, 3 i1 .char−2 + i2 .char−2 +"
P09-1007,D07-1111,0,0.00895734,"for parsing that has been draw some interests in recent years. Typical domain adaptation tasks often assume annotated data in new domain absent or insufficient and a large scale unlabeled data available. As unlabeled data are concerned, semi-supervised or unsupervised methods will be naturally adopted. In previous works, two basic types of methods can be identified to enhance an existing parser from additional resources. The first is usually focus on exploiting automatic generated labeled data from the unlabeled data (Steedman et al., 2003; McClosky et al., 2006; Reichart and Rappoport, 2007; Sagae and Tsujii, 2007; Chen et al., 2008), the second is on combining supervised and unsupervised methods, and only unlabeled data are considered (Smith and Eisner, 2006; Wang and Schuurmans, 2008; Koo et al., 2008). Our purpose in this study is to obtain a further performance enhancement by exploiting treebanks in other languages. This is similar to the above first type of methods, some assistant data should be automatically generated for the subsequent processing. The differences are what type of data are concerned with and how they are produced. In our method, a machine translation method is applied to tackle g"
P09-1007,D07-1097,0,\N,Missing
P09-1007,D07-1096,0,\N,Missing
P14-1142,P00-1031,0,0.926392,"st characters usually have unique pinyin representations, while a few Chinese characters may be pronounced in several different ways, so they may have multiple pinyin representations. The advantage of pinyin IME is that it only adopts the pronunciation perspective of Chinese characters so that it is simple and easy to learn. But there are only less than 500 pinyin syllables in standard modern Chinese, compared with over 6,000 commonly used Chinese characters, which leads to serious ambiguities for pinyin-to-character mapping. Modern pinyin IMEs mostly use a “sentencebased” decoding technique (Chen and Lee, 2000) to alleviate the ambiguities. “Sentence based” means that IME generates a sequence of Chinese characters upon a sequence of pinyin inputs with respect to certain statistical criteria. 1.2 Typos and Chinese Spell Checking Written in Chinese characters but not alphabets, spell checking for Chinese language is quite different from the same task for other languages. Since Chinese characters are entered via IME, those user-made typos do not immediately lead to spelling errors. When a user types a wrong letter, IME will be very likely to fail to generate the expected Chinese character sequence. Nor"
P14-1142,W13-4414,0,0.0139599,"a certain meaning, instead of only in one word. A recent spell correction work is (Li et al., 2006), where a distributional similarity was introduced for spell correction of web queries. Early attempts for Chinese spelling checking could date back to (Chang, 1994) where character tables for similar shape, pronunciation, meaning, and input-method-code characters were proposed. More recently, the 7th SIGHAN Workshop on Chinese Language Processing (Yu et al., 2013) held a shared task on Chinese spell checking. Various approaches were made for the task including language model (LM) based methods (Chen et al., 2013), ME model (Han and Chang, 2013), CRF (Wang et al., 2013d; Wang et al., 2013a), SMT (Chiu et al., 2013; Liu et al., 2013), and graph model (Jia et al., 2013), etc. 3 Pinyin Input Method Model 3.1 From English Letter to Chinese Sentence It is a rather long journey from the first English letter typed on the keyboard to finally a completed Chinese sentence generated by IME. We will first take an overview of the entire process. The average length of pinyin syllables is about 3 letters. There are about 410 pinyin syllables used in the current pinyin system. Each pinyin sylla1513 ble has a bunch of"
P14-1142,W13-4408,0,0.0286348,"ere a distributional similarity was introduced for spell correction of web queries. Early attempts for Chinese spelling checking could date back to (Chang, 1994) where character tables for similar shape, pronunciation, meaning, and input-method-code characters were proposed. More recently, the 7th SIGHAN Workshop on Chinese Language Processing (Yu et al., 2013) held a shared task on Chinese spell checking. Various approaches were made for the task including language model (LM) based methods (Chen et al., 2013), ME model (Han and Chang, 2013), CRF (Wang et al., 2013d; Wang et al., 2013a), SMT (Chiu et al., 2013; Liu et al., 2013), and graph model (Jia et al., 2013), etc. 3 Pinyin Input Method Model 3.1 From English Letter to Chinese Sentence It is a rather long journey from the first English letter typed on the keyboard to finally a completed Chinese sentence generated by IME. We will first take an overview of the entire process. The average length of pinyin syllables is about 3 letters. There are about 410 pinyin syllables used in the current pinyin system. Each pinyin sylla1513 ble has a bunch of corresponding Chinese characters which share the same pronunciation represented by the syllable. The n"
P14-1142,W13-4413,0,0.0210596,"only in one word. A recent spell correction work is (Li et al., 2006), where a distributional similarity was introduced for spell correction of web queries. Early attempts for Chinese spelling checking could date back to (Chang, 1994) where character tables for similar shape, pronunciation, meaning, and input-method-code characters were proposed. More recently, the 7th SIGHAN Workshop on Chinese Language Processing (Yu et al., 2013) held a shared task on Chinese spell checking. Various approaches were made for the task including language model (LM) based methods (Chen et al., 2013), ME model (Han and Chang, 2013), CRF (Wang et al., 2013d; Wang et al., 2013a), SMT (Chiu et al., 2013; Liu et al., 2013), and graph model (Jia et al., 2013), etc. 3 Pinyin Input Method Model 3.1 From English Letter to Chinese Sentence It is a rather long journey from the first English letter typed on the keyboard to finally a completed Chinese sentence generated by IME. We will first take an overview of the entire process. The average length of pinyin syllables is about 3 letters. There are about 410 pinyin syllables used in the current pinyin system. Each pinyin sylla1513 ble has a bunch of corresponding Chinese characters"
P14-1142,P13-2121,0,0.0287581,"Missing"
P14-1142,W11-2123,0,0.011477,"shortest paths in typo correction An efficient heap data structure is required in K-shortest paths algorithm (Eppstein, 1998) for D 2K 83,765 T 100K 4,123,184 Table 1: Data set size K-Shortest Paths To reduce the scale of graph G, we filter graph Gc by searching its K-shortest paths first to get G′c and construct G on top of G′c . Figure 5 shows the 3shortest paths filtered graph G′c and Figure 6 shows the corresponding G for our running example. The scale of graph may be thus drastically reduced. T 1M 43,679,593 SRILM (Stolcke, 2002) is adopted for language model training and KenLM (Heafield, 2011; Heafield et al., 2013) for language model query. The Chinese part of the corpus is segmented into words before LM training. Maximum matching word segmentation is used with a large word vocabulary V extracted from web data provided by (Wang et al., 2013b). The pinyin part is segmented according to the Chinese part. This vocabulary V also serves as the PTC dictionary. The original vocabulary is not labeled with pinyin, thus we use the PTC dictionary of sunpinyin1 which is an open source Chinese pinyin IME, to label the 1517 1 http://code.google.com/p/sunpinyin/ We will use conventional sequenc"
P14-1142,I13-1170,1,0.715741,"mart-phones, pinyin typos worsen due to the limited size of soft keyboard, and the lack of physical feedback on the touch screen. However, existing practical IMEs only provide small patches to deal with typos such as Fuzzy Pinyin (Wu and Chen, 2004) and other language specific errors (Zheng et al., 2011b). Typo checking and correction has an important impact on IME performance. When IME fails to correct a typo and generate the expected sentence, the user will have to take much extra effort to move the cursor back to the mistyped letter and correct it, which leads to very poor user experience (Jia and Zhao, 2013). 2 Related Works The very first approach for Chinese input with typo correction was made by (Chen and Lee, 2000), which was also the initial attempt of “sentence-based” IME. The idea of “statistical input method” was proposed by modeling PTC conversion as a hidden Markov model (HMM), and using Viterbi (Viterbi, 1967) algorithm to decode the sequence. They solved the typo correction problem by decomposing the conditional probability P (H|P ) of Chinese character sequence H given pinyin sequence P into a language model P (wi |wi−1 ) and a typing model P (pi |wi ). The typing model that was esti"
P14-1142,W13-4416,1,0.863332,"l correction of web queries. Early attempts for Chinese spelling checking could date back to (Chang, 1994) where character tables for similar shape, pronunciation, meaning, and input-method-code characters were proposed. More recently, the 7th SIGHAN Workshop on Chinese Language Processing (Yu et al., 2013) held a shared task on Chinese spell checking. Various approaches were made for the task including language model (LM) based methods (Chen et al., 2013), ME model (Han and Chang, 2013), CRF (Wang et al., 2013d; Wang et al., 2013a), SMT (Chiu et al., 2013; Liu et al., 2013), and graph model (Jia et al., 2013), etc. 3 Pinyin Input Method Model 3.1 From English Letter to Chinese Sentence It is a rather long journey from the first English letter typed on the keyboard to finally a completed Chinese sentence generated by IME. We will first take an overview of the entire process. The average length of pinyin syllables is about 3 letters. There are about 410 pinyin syllables used in the current pinyin system. Each pinyin sylla1513 ble has a bunch of corresponding Chinese characters which share the same pronunciation represented by the syllable. The number of those homophones ranges from 1 to over 300. Ch"
P14-1142,P07-2045,0,0.00384466,"-Ney (IKN), and Witten-Bell (WB). All of the three smoothing methods for bigram and trigram LMs are examined both using back-off modMIU-Acc 4.3 0.74 Ch-Acc 4.2 Evaluation Metrics els and interpolated models. The number of N best candidates for PTC conversion is set to 10. The results on D are shown in Figure 7 in which the “-i” suffix indicates using interpolated model. According to the results, we then choose the trigram LM using Kneser-Ney smoothing with interpolation. MIU-Acc vocabulary V with pinyin. The emission probabilities are estimated using the lexical translation module of MOSES (Koehn et al., 2007) as “translation probability” from pinyin to Chinese. 0.955 0.7285 0.95 0.728 0.7275 0.945 0.727 0.94 0.7265 0.935 1 10 100 1000 Figure 8: MIU-Acc and Ch-Acc with different N s The parameter γ determines emission probability. Results with different γ on D is shown in Figure 9, of which the γ axis is drawn in logarithmic scale. γ = 0.03 is chosen at last. We compare our baseline system with several practical pinyin IMEs including sunpinyin and Google Input Tools (Online version)4 . The results on D are shown in Table 2. 1518 4 http://www.google.com/inputtools/try/ 0.98 pinyin word, this rat"
P14-1142,P06-1129,0,0.0142906,"for PTC conversion, there are also various methods such as: support vector machine (Jiang et al., 2007), maximum entropy (ME) model (Wang et al., 2006), conditional random field (CRF) (Li et al., 2009) and statistical machine translation (SMT) (Yang et al., 2012a; Wang et al., 2013c; Zhang and Zhao, 2013), etc. Spell checking or typo checking was first proposed for English (Peterson, 1980). (Mays et al., 1991) addressed that spell checking should be done within a context, i.e., a sentence or a long phrase with a certain meaning, instead of only in one word. A recent spell correction work is (Li et al., 2006), where a distributional similarity was introduced for spell correction of web queries. Early attempts for Chinese spelling checking could date back to (Chang, 1994) where character tables for similar shape, pronunciation, meaning, and input-method-code characters were proposed. More recently, the 7th SIGHAN Workshop on Chinese Language Processing (Yu et al., 2013) held a shared task on Chinese spell checking. Various approaches were made for the task including language model (LM) based methods (Chen et al., 2013), ME model (Han and Chang, 2013), CRF (Wang et al., 2013d; Wang et al., 2013a), S"
P14-1142,W13-4409,0,0.0261631,"l similarity was introduced for spell correction of web queries. Early attempts for Chinese spelling checking could date back to (Chang, 1994) where character tables for similar shape, pronunciation, meaning, and input-method-code characters were proposed. More recently, the 7th SIGHAN Workshop on Chinese Language Processing (Yu et al., 2013) held a shared task on Chinese spell checking. Various approaches were made for the task including language model (LM) based methods (Chen et al., 2013), ME model (Han and Chang, 2013), CRF (Wang et al., 2013d; Wang et al., 2013a), SMT (Chiu et al., 2013; Liu et al., 2013), and graph model (Jia et al., 2013), etc. 3 Pinyin Input Method Model 3.1 From English Letter to Chinese Sentence It is a rather long journey from the first English letter typed on the keyboard to finally a completed Chinese sentence generated by IME. We will first take an overview of the entire process. The average length of pinyin syllables is about 3 letters. There are about 410 pinyin syllables used in the current pinyin system. Each pinyin sylla1513 ble has a bunch of corresponding Chinese characters which share the same pronunciation represented by the syllable. The number of those homo"
P14-1142,yang-etal-2012-spell,1,0.909771,"ake corrections, which usually means doing a bunch of extra operations like cursor 1512 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1512–1523, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics movement, deletion and re-typing. Thus there are two separated sub-tasks for Chinese spell checking: 1. typo checking for user typed pinyin sequences which should be a built-in module in IME, and 2. spell checking for Chinese texts in its narrow sense, which is typically a module of word processing applications (Yang et al., 2012b). These two terms are often confused especially in IME related works such as (Chen and Lee, 2000) and (Wu et al., 2009). Pinyin typos have always been a serious problem for Chinese pinyin IMEs. The user may fail to input the completely right pinyin simply because he/she is a dialect speaker and does not know the exact pronunciation for the expected character. This may be a very common situation since there are about seven quite different dialects in Chinese, among which being spoken languages, six are far different from the standard modern Chinese, mandarin. With the boom of smart-phones, pi"
P14-1142,W13-4411,0,0.123263,"el depended on a very strong assumption that input pinyin sequence should have been segmented into pinyin words by the user. This assumption does not really hold in modern “sentence-based” IMEs. We release this assumption since our model solves segmentation, typo correction and PTC conversion jointly. Besides the common HMM approach for PTC conversion, there are also various methods such as: support vector machine (Jiang et al., 2007), maximum entropy (ME) model (Wang et al., 2006), conditional random field (CRF) (Li et al., 2009) and statistical machine translation (SMT) (Yang et al., 2012a; Wang et al., 2013c; Zhang and Zhao, 2013), etc. Spell checking or typo checking was first proposed for English (Peterson, 1980). (Mays et al., 1991) addressed that spell checking should be done within a context, i.e., a sentence or a long phrase with a certain meaning, instead of only in one word. A recent spell correction work is (Li et al., 2006), where a distributional similarity was introduced for spell correction of web queries. Early attempts for Chinese spelling checking could date back to (Chang, 1994) where character tables for similar shape, pronunciation, meaning, and input-method-code characters we"
P14-1142,D13-1082,1,0.459517,"el depended on a very strong assumption that input pinyin sequence should have been segmented into pinyin words by the user. This assumption does not really hold in modern “sentence-based” IMEs. We release this assumption since our model solves segmentation, typo correction and PTC conversion jointly. Besides the common HMM approach for PTC conversion, there are also various methods such as: support vector machine (Jiang et al., 2007), maximum entropy (ME) model (Wang et al., 2006), conditional random field (CRF) (Li et al., 2009) and statistical machine translation (SMT) (Yang et al., 2012a; Wang et al., 2013c; Zhang and Zhao, 2013), etc. Spell checking or typo checking was first proposed for English (Peterson, 1980). (Mays et al., 1991) addressed that spell checking should be done within a context, i.e., a sentence or a long phrase with a certain meaning, instead of only in one word. A recent spell correction work is (Li et al., 2006), where a distributional similarity was introduced for spell correction of web queries. Early attempts for Chinese spelling checking could date back to (Chang, 1994) where character tables for similar shape, pronunciation, meaning, and input-method-code characters we"
P14-1142,W13-4412,0,0.118322,"el depended on a very strong assumption that input pinyin sequence should have been segmented into pinyin words by the user. This assumption does not really hold in modern “sentence-based” IMEs. We release this assumption since our model solves segmentation, typo correction and PTC conversion jointly. Besides the common HMM approach for PTC conversion, there are also various methods such as: support vector machine (Jiang et al., 2007), maximum entropy (ME) model (Wang et al., 2006), conditional random field (CRF) (Li et al., 2009) and statistical machine translation (SMT) (Yang et al., 2012a; Wang et al., 2013c; Zhang and Zhao, 2013), etc. Spell checking or typo checking was first proposed for English (Peterson, 1980). (Mays et al., 1991) addressed that spell checking should be done within a context, i.e., a sentence or a long phrase with a certain meaning, instead of only in one word. A recent spell correction work is (Li et al., 2006), where a distributional similarity was introduced for spell correction of web queries. Early attempts for Chinese spelling checking could date back to (Chang, 1994) where character tables for similar shape, pronunciation, meaning, and input-method-code characters we"
P14-1142,W06-0127,1,0.451854,"of pinyin syllables is about 3 letters. There are about 410 pinyin syllables used in the current pinyin system. Each pinyin sylla1513 ble has a bunch of corresponding Chinese characters which share the same pronunciation represented by the syllable. The number of those homophones ranges from 1 to over 300. Chinese characters then form words. But word in Chinese is a rather vague concept. Without word delimiters, linguists have argued on what a Chinese word really is for a long time and that is why there is always a primary word segmentation treatment in most Chinese language processing tasks (Zhao et al., 2006; Huang and Zhao, 2007; Zhao and Kit, 2008; Zhao et al., 2010; Zhao and Kit, 2011; Zhao et al., 2013). A Chinese word may contain from 1 to over 10 characters due to different word segmentation conventions. Figure 1 demonstrates the relationship of pinyin and word, from pinyin letters “nihao” to the word “你好 (hello)”. Typically, an IME takes the pinyin input, segments it into syllables, looks up corresponding words in a dictionary and generates a sentence with the candidate words. n i h ni hao a o Pinyin characters ble segmentation by using rules. But as the pinyin input is not segmented, it i"
P14-1142,P11-2085,0,0.141876,"e is a dialect speaker and does not know the exact pronunciation for the expected character. This may be a very common situation since there are about seven quite different dialects in Chinese, among which being spoken languages, six are far different from the standard modern Chinese, mandarin. With the boom of smart-phones, pinyin typos worsen due to the limited size of soft keyboard, and the lack of physical feedback on the touch screen. However, existing practical IMEs only provide small patches to deal with typos such as Fuzzy Pinyin (Wu and Chen, 2004) and other language specific errors (Zheng et al., 2011b). Typo checking and correction has an important impact on IME performance. When IME fails to correct a typo and generate the expected sentence, the user will have to take much extra effort to move the cursor back to the mistyped letter and correct it, which leads to very poor user experience (Jia and Zhao, 2013). 2 Related Works The very first approach for Chinese input with typo correction was made by (Chen and Lee, 2000), which was also the initial attempt of “sentence-based” IME. The idea of “statistical input method” was proposed by modeling PTC conversion as a hidden Markov model (HMM),"
P14-1142,O13-1005,0,\N,Missing
P15-2089,C14-1108,0,0.0124885,"approaches to integrate rich contextual information for target side rule selection. Cui et al. (2010) proposed a joint model to select hierarchical rules for both source and target sides. 542 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 542–548, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics Algorithm 1 Extract training instances. could not be used in hierarchical phrase-based model directly. Nguyen and Vogel (2013) and Cao et al. (2014) proposed to integrate phrasebased reordering features into hierarchical phrasebased SMT. However, their work limited to learning the reordering of continuous phrases. For short phrases, in extreme cases, when phrase length is one, their model only learned reordering for continuous word pairs like Feng et al. (2013)’s work, while our model can be applied to word pairs with longer distances. 2 Require: A pair of parallel sentence f1l and em 1 with word alignments. Ensure: Training examples for M1 , M2 , . . . , MN . for i = 1 to l − 1 do for j = i + 1 to l do if j − i ≤ N then for u = 1 to πi d"
P15-2089,P05-1033,0,0.134178,"machine translation system. Existing word reordering models learn the reordering for any two source words in a sentence or only for two continuous words. This paper proposes a series of separate sub-models to learn reorderings for word pairs with different distances. Our experiments demonstrate that reordering sub-models for word pairs with distance less than a specific threshold are useful to improve translation quality. Compared with previous work, our method may more effectively and efficiently exploit helpful word reordering information. 1 Introduction The hierarchical phrase-based model (Chiang, 2005) is capable of capturing rich translation knowledge with the synchronous context-free grammar. But selecting proper translation rules during decoding is a challenge as a huge number of hierarchical rules can be applied to one source sentence. Chiang (2005) used a log-linear model to compute rule weights with features similar to Pharaoh (Koehn et al., 2003). However, to select appropriate rules, more effective criteria are required. A lot of work has been done for better rule selection. He et al. (2008) and Liu et al. (2008) used maximum entropy approaches to integrate rich contextual informati"
P15-2089,D08-1010,0,0.0211256,"eordering information. 1 Introduction The hierarchical phrase-based model (Chiang, 2005) is capable of capturing rich translation knowledge with the synchronous context-free grammar. But selecting proper translation rules during decoding is a challenge as a huge number of hierarchical rules can be applied to one source sentence. Chiang (2005) used a log-linear model to compute rule weights with features similar to Pharaoh (Koehn et al., 2003). However, to select appropriate rules, more effective criteria are required. A lot of work has been done for better rule selection. He et al. (2008) and Liu et al. (2008) used maximum entropy approaches to integrate rich contextual information for target side rule selection. Cui et al. (2010) proposed a joint model to select hierarchical rules for both source and target sides. 542 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 542–548, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics Algorithm 1 Extract training instances. could not be used in hierarchical phrase-based model directly. Nguyen"
P15-2089,J07-2003,0,0.23551,"Missing"
P15-2089,P13-1032,0,0.21843,"niversity, Shanghai 200240, China 4 Key Laboratory of Shanghai Education Commission for Intelligent Interaction and Cognitive Engineering, Shanghai Jiao Tong University, Shanghai 200240, China jingyizhang/mutiyama/eiichiro.sumita@nict.go.jp zhaohai@cs.sjtu.edu.cn 1 Abstract Hayashi et al. (2010) demonstrated the effectiveness of using word reordering information within hierarchical phrase-based SMT by integrating Tromble and Eisner (2009)’s word reordering model into decoder as a feature, which estimates the probability of any two source words in a sentence being reordered during translating. Feng et al. (2013) proposed a word reordering model to learn reorderings only for continuous words, which reduced computation cost a lot compared with Tromble and Eisner (2009)’s model and still achieved significant reordering improvement over the baseline system. In this paper, we incorporate word reordering information into hierarchical phrase-based SMT by training a series of separate reordering submodels for word pairs with different distances. We will demonstrate that the translation performance achieves consistent improvement as more sub-models for longer distance reorderings being integrated, but the imp"
P15-2089,P13-1156,0,0.0142256,"(2008) used maximum entropy approaches to integrate rich contextual information for target side rule selection. Cui et al. (2010) proposed a joint model to select hierarchical rules for both source and target sides. 542 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 542–548, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics Algorithm 1 Extract training instances. could not be used in hierarchical phrase-based model directly. Nguyen and Vogel (2013) and Cao et al. (2014) proposed to integrate phrasebased reordering features into hierarchical phrasebased SMT. However, their work limited to learning the reordering of continuous phrases. For short phrases, in extreme cases, when phrase length is one, their model only learned reordering for continuous word pairs like Feng et al. (2013)’s work, while our model can be applied to word pairs with longer distances. 2 Require: A pair of parallel sentence f1l and em 1 with word alignments. Ensure: Training examples for M1 , M2 , . . . , MN . for i = 1 to l − 1 do for j = i + 1 to l do if j − i ≤ N"
P15-2089,P09-2061,0,0.01765,"significantly. Compared with previous models (Tromble and Eisner, 2009; Feng et al., 2013), our method makes full use of helpful word reordering information and also avoids unnecessary computation cost for long distance reorderings. Besides, our reordering model is learned by feed-forward neural network (FNN) for better performance and uses efficient caching strategy to further reduce time cost. Phrase reordering models have also been integrated into hierarchical phrase-based SMT. Phrase reordering models were originally developed for phrase-based SMT (Koehn et al., 2005; Zens and Ney, 2006; Ni et al., 2009; Li et al., 2014) and Statistical models for reordering source words have been used to enhance the hierarchical phrase-based statistical machine translation system. Existing word reordering models learn the reordering for any two source words in a sentence or only for two continuous words. This paper proposes a series of separate sub-models to learn reorderings for word pairs with different distances. Our experiments demonstrate that reordering sub-models for word pairs with distance less than a specific threshold are useful to improve translation quality. Compared with previous work, our met"
P15-2089,C10-1050,0,0.314597,"al Institute of Information and Communications Technology, 3-5Hikaridai, Keihanna Science City, Kyoto 619-0289, Japan 2 Graduate School of Information Science, Nara Institute of Science and Technology, Takayama, Ikoma, Nara 630-0192, Japan 3 Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai 200240, China 4 Key Laboratory of Shanghai Education Commission for Intelligent Interaction and Cognitive Engineering, Shanghai Jiao Tong University, Shanghai 200240, China jingyizhang/mutiyama/eiichiro.sumita@nict.go.jp zhaohai@cs.sjtu.edu.cn 1 Abstract Hayashi et al. (2010) demonstrated the effectiveness of using word reordering information within hierarchical phrase-based SMT by integrating Tromble and Eisner (2009)’s word reordering model into decoder as a feature, which estimates the probability of any two source words in a sentence being reordered during translating. Feng et al. (2013) proposed a word reordering model to learn reorderings only for continuous words, which reduced computation cost a lot compared with Tromble and Eisner (2009)’s model and still achieved significant reordering improvement over the baseline system. In this paper, we incorporate w"
P15-2089,J03-1002,0,0.0123034,"se that r is applied to the input sentence f1l , where CE JE In NTCIR-9, the development and test sets were both provided for CE task while only the test set was provided for the JE task. Therefore, we used the sentences from the NTCIR-8 JE test set as the development set for JE task. The word segmentation was done by BaseSeg (Zhao et al., 2006; Zhao and Kit, 2008; Zhao et al., 2010; Zhao and Kit, 2011; Zhao et al., 2013) for Chinese and Mecab2 for Japanese. To learn neural reordering models, the training and development sets were put together to obtain symmetric word alignments using GIZA++ (Och and Ney, 2003) and the grow-diag-finaland heuristic (Koehn et al., 2003). The reordering instances extracted from the aligned training and development sets were used as the training and validation data respectively for learning neural reordering models. Neural reordering models were trained by the toolkit NPLM (Vaswani et al., 2013). For CE task, training instances extracted from all the 1M sentence pairs were used to train neural reordering models. For JE task, training instances were from 1M sentence pairs that were randomly selected from all the 3.14M sentence pairs. We also implemented Hayashi et al. (2"
P15-2089,P03-1021,0,0.0151868,"cial datasets for the patent machine translation task at NTCIR-9 (Goto et al., 2011) were used. The detailed statistics for training, development and test sets are given in Table 1. Integration into the Decoder In the hierarchical phrase-based model, a translation rule r is like: X → hγ, α, ∼i TRAINING where X is a nonterminal, γ and α are respectively source and target strings of terminals and nonterminals, and ∼ is the alignment between nonterminals and terminals in γ and α. Each rule has several features and the feature weights are tuned by the minimum error rate training (MERT) algorithm (Och, 2003). To integrate our model into the hierarchical phrase-based translation system, a new feature scoren (r) is added to each rule r for each Mn . The score of this feature is calculated during decoding. Note that these scores are correspondingly calculated for different sub-models Mn and the sub-model weights are tuned separately. Suppose that r is applied to the input sentence f1l , where CE JE In NTCIR-9, the development and test sets were both provided for CE task while only the test set was provided for the JE task. Therefore, we used the sentences from the NTCIR-8 JE test set as the developm"
P15-2089,C08-1041,0,0.0210894,"xploit helpful word reordering information. 1 Introduction The hierarchical phrase-based model (Chiang, 2005) is capable of capturing rich translation knowledge with the synchronous context-free grammar. But selecting proper translation rules during decoding is a challenge as a huge number of hierarchical rules can be applied to one source sentence. Chiang (2005) used a log-linear model to compute rule weights with features similar to Pharaoh (Koehn et al., 2003). However, to select appropriate rules, more effective criteria are required. A lot of work has been done for better rule selection. He et al. (2008) and Liu et al. (2008) used maximum entropy approaches to integrate rich contextual information for target side rule selection. Cui et al. (2010) proposed a joint model to select hierarchical rules for both source and target sides. 542 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 542–548, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics Algorithm 1 Extract training instances. could not be used in hierarchical phrase-based m"
P15-2089,D09-1105,0,0.152786,"e School of Information Science, Nara Institute of Science and Technology, Takayama, Ikoma, Nara 630-0192, Japan 3 Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai 200240, China 4 Key Laboratory of Shanghai Education Commission for Intelligent Interaction and Cognitive Engineering, Shanghai Jiao Tong University, Shanghai 200240, China jingyizhang/mutiyama/eiichiro.sumita@nict.go.jp zhaohai@cs.sjtu.edu.cn 1 Abstract Hayashi et al. (2010) demonstrated the effectiveness of using word reordering information within hierarchical phrase-based SMT by integrating Tromble and Eisner (2009)’s word reordering model into decoder as a feature, which estimates the probability of any two source words in a sentence being reordered during translating. Feng et al. (2013) proposed a word reordering model to learn reorderings only for continuous words, which reduced computation cost a lot compared with Tromble and Eisner (2009)’s model and still achieved significant reordering improvement over the baseline system. In this paper, we incorporate word reordering information into hierarchical phrase-based SMT by training a series of separate reordering submodels for word pairs with different"
P15-2089,N03-1017,0,0.104813,"less than a specific threshold are useful to improve translation quality. Compared with previous work, our method may more effectively and efficiently exploit helpful word reordering information. 1 Introduction The hierarchical phrase-based model (Chiang, 2005) is capable of capturing rich translation knowledge with the synchronous context-free grammar. But selecting proper translation rules during decoding is a challenge as a huge number of hierarchical rules can be applied to one source sentence. Chiang (2005) used a log-linear model to compute rule weights with features similar to Pharaoh (Koehn et al., 2003). However, to select appropriate rules, more effective criteria are required. A lot of work has been done for better rule selection. He et al. (2008) and Liu et al. (2008) used maximum entropy approaches to integrate rich contextual information for target side rule selection. Cui et al. (2010) proposed a joint model to select hierarchical rules for both source and target sides. 542 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 542–548, c Beijing, China, July"
P15-2089,D13-1140,0,0.124363,"ion of f1l = f1 , . . . , fl and A be word alignments bel tween em 1 and f1 , our model estimates the reordering probability of the source sentence as follows: Pr f1l , em 1 ,A ≈ N Q to learn reorderings for word pairs with different distances. That means, for the word pair hfi , fj i with distance j − i = n, its reordering   probability Pr oijuv |fi−3 , ..., fj+3 , eaiu , eajv is estimated by Mn . Different sub-models are trained and integrated into the translation system separately. Each sub-model Mn is implemented by an FNN, which has the same structure with the neural language model in (Vaswani et al., 2013). The input to Mn is a sequence of n + 9 words: fi−3 , ..., fj+3 , eaiu , eajv . The input layer projects each word into a high dimensional vector using a matrix of input word embeddings. Two hidden layers can combine all input data1. The output layer has two neurons  that give Pr oijuv = 1|fi−3 , ..., fj+3 , eaiu , eajv and  Q n=1 i,j:1≤i&lt;j≤l,j−i=n  Pr f1l , em 1 , A, i, j  (1)  where Pr f1l , em 1 , A, i, j is the reordering probability of the word pair hfi , fj i during translating; N is the maximum distance for source word reordering, which is empirically determined by supposing that"
P15-2089,2005.iwslt-1.8,0,0.0474646,"shold do not improve translation quality significantly. Compared with previous models (Tromble and Eisner, 2009; Feng et al., 2013), our method makes full use of helpful word reordering information and also avoids unnecessary computation cost for long distance reorderings. Besides, our reordering model is learned by feed-forward neural network (FNN) for better performance and uses efficient caching strategy to further reduce time cost. Phrase reordering models have also been integrated into hierarchical phrase-based SMT. Phrase reordering models were originally developed for phrase-based SMT (Koehn et al., 2005; Zens and Ney, 2006; Ni et al., 2009; Li et al., 2014) and Statistical models for reordering source words have been used to enhance the hierarchical phrase-based statistical machine translation system. Existing word reordering models learn the reordering for any two source words in a sentence or only for two continuous words. This paper proposes a series of separate sub-models to learn reorderings for word pairs with different distances. Our experiments demonstrate that reordering sub-models for word pairs with distance less than a specific threshold are useful to improve translation quality."
P15-2089,W06-3108,0,0.0271019,"translation quality significantly. Compared with previous models (Tromble and Eisner, 2009; Feng et al., 2013), our method makes full use of helpful word reordering information and also avoids unnecessary computation cost for long distance reorderings. Besides, our reordering model is learned by feed-forward neural network (FNN) for better performance and uses efficient caching strategy to further reduce time cost. Phrase reordering models have also been integrated into hierarchical phrase-based SMT. Phrase reordering models were originally developed for phrase-based SMT (Koehn et al., 2005; Zens and Ney, 2006; Ni et al., 2009; Li et al., 2014) and Statistical models for reordering source words have been used to enhance the hierarchical phrase-based statistical machine translation system. Existing word reordering models learn the reordering for any two source words in a sentence or only for two continuous words. This paper proposes a series of separate sub-models to learn reorderings for word pairs with different distances. Our experiments demonstrate that reordering sub-models for word pairs with distance less than a specific threshold are useful to improve translation quality. Compared with previ"
P15-2089,P07-2045,0,0.00963672,"Missing"
P15-2089,W06-0127,1,0.737146,"slation system, a new feature scoren (r) is added to each rule r for each Mn . The score of this feature is calculated during decoding. Note that these scores are correspondingly calculated for different sub-models Mn and the sub-model weights are tuned separately. Suppose that r is applied to the input sentence f1l , where CE JE In NTCIR-9, the development and test sets were both provided for CE task while only the test set was provided for the JE task. Therefore, we used the sentences from the NTCIR-8 JE test set as the development set for JE task. The word segmentation was done by BaseSeg (Zhao et al., 2006; Zhao and Kit, 2008; Zhao et al., 2010; Zhao and Kit, 2011; Zhao et al., 2013) for Chinese and Mecab2 for Japanese. To learn neural reordering models, the training and development sets were put together to obtain symmetric word alignments using GIZA++ (Och and Ney, 2003) and the grow-diag-finaland heuristic (Koehn et al., 2003). The reordering instances extracted from the aligned training and development sets were used as the training and validation data respectively for learning neural reordering models. Neural reordering models were trained by the toolkit NPLM (Vaswani et al., 2013). For CE"
P15-2089,W04-3250,0,0.0722837,"男生(guy)” is reversed. This is the reason why translation performance improves as more sub-models being integrated. As shown in Table 2, with 4 sub-models being integrated, our model improved baseline system significantly and also outperformed Hayashi model clearly. It is easy to understand, since our model was trained by feed-forward neural network on a high dimensional space and incorporated rich context information, while Hayashi model used the averaged perceptron algorithm and simple features. Table 3b shows the prediction accuracies (b) Significance test results using bootstrap sampling (Koehn, 2004) w.r.t. BLEU scores. The symbol  represents a significant difference at the p &lt; 0.01 level; &gt; represents a significant difference at the p &lt; 0.05 level; − means not significantly different at p = 0.05. Table 2: Translation results. For each translation task, the recent version of the Moses hierarchical phrase-based decoder (Koehn et al., 2007) with the training scripts was used as the baseline system Base. We used the default parameters for Moses. A 5-gram language model was trained on the target side of the training corpus by IRST LM Toolkit3 with the improved Kneser-Ney smoothing. We integr"
P15-2089,C14-1179,0,0.0221259,"ompared with previous models (Tromble and Eisner, 2009; Feng et al., 2013), our method makes full use of helpful word reordering information and also avoids unnecessary computation cost for long distance reorderings. Besides, our reordering model is learned by feed-forward neural network (FNN) for better performance and uses efficient caching strategy to further reduce time cost. Phrase reordering models have also been integrated into hierarchical phrase-based SMT. Phrase reordering models were originally developed for phrase-based SMT (Koehn et al., 2005; Zens and Ney, 2006; Ni et al., 2009; Li et al., 2014) and Statistical models for reordering source words have been used to enhance the hierarchical phrase-based statistical machine translation system. Existing word reordering models learn the reordering for any two source words in a sentence or only for two continuous words. This paper proposes a series of separate sub-models to learn reorderings for word pairs with different distances. Our experiments demonstrate that reordering sub-models for word pairs with distance less than a specific threshold are useful to improve translation quality. Compared with previous work, our method may more effec"
P15-2089,D11-1079,0,\N,Missing
P15-2089,Q13-1027,0,\N,Missing
P15-2089,P08-1114,0,\N,Missing
P15-2089,W13-2258,0,\N,Missing
P15-2089,P02-1038,0,\N,Missing
P15-2089,P10-2002,0,\N,Missing
P15-2089,D15-1164,0,\N,Missing
P15-2089,D11-1125,0,\N,Missing
P16-1039,D15-1032,0,0.0134464,"from previous segmentation y can be represented as a triple (y.score, y.h, y.c), where y.score, y.h, y.c indicate the current score, current hidden state vector and current memory cell vector respectively. Beam search ensures that the total time for segmenting a sentence of n characters is w × k × n, where w, k are maximum word length and beam size respectively. 5 Table 2: Hyper-parameter settings. The update for the i-th parameter at time step t is as follows: α θt,i = θt−1,i − qP t Training 2 τ =1 gτ,i We use the max-margin criterion (Taskar et al., 2005) to train our model. As reported in (Kummerfeld et al., 2015), the margin methods generally outperform both likelihood and perception methods. For a given character sequence x(i) , denote the correct segmented sentence for x(i) as y (i) . We define a structured margin loss ∆(y (i) , yˆ) for predicting a segmented sentence yˆ: (i) ∆(y , yˆ) = m X d = 50 H = 50 α = 0.2 µ = 0.2 λ = 10−6 p = 0.2 w=4 gt,i where α is the initial learning rate and gτ,i ∈ R|θi | is the subgradient at time step τ for parameter θi . 6 6.1 Experiments Datasets To evaluate the proposed segmenter, we use two popular datasets, PKU and MSR, from the second International Chinese Word S"
P16-1039,J96-1002,0,0.253717,"Missing"
P16-1039,P15-1168,0,0.321963,"t al., 2001; Peng et al., 2004; Zhao et al., 2006a). However, those methods heavily depend on the choice of handcrafted features. Recently, neural models have been widely used for NLP tasks for their ability to minimize the effort in feature engineering. For the task of CWS, Zheng et al. (2013) adapted the general neural network architecture for sequence labeling proposed in (Collobert et al., 2011), and used character embeddings as input to a two-layer network. Pei et al. (2014) improved upon (Zheng et al., 2013) by explicitly modeling the interactions between local context and previous tag. Chen et al. (2015a) proposed a gated recursive neural network to model the feature combinations of context characters. Chen et al. (2015b) used an LSTM architecture to capture potential long-distance dependencies, which alleviates the limitation of the size of context window but introduced another window for hidden states. Despite the differences, all these models are designed to solve CWS by assigning labels to the characters in the sequence one by one. At each time step of inference, these models compute the tag scores of character based on (i) context features within a fixed sized local window and (ii) tagg"
P16-1039,D12-1132,0,0.0181464,"lts in Figure 6 and the last row of Table 6) only with two cores of an Intel i7-5960X CPU. The requirement for RAM during training is less than 800MB. The trained model can be saved within 4MB on the hard disk. 7 Other Techniques. The proposed model might furthermore benefit from some techniques in recent state-of-the-art systems, such as semisupervised learning (Zhao and Kit, 2008b; Zhao and Kit, 2008a; Sun and Xu, 2011; Zhao and Kit, 2011; Zeng et al., 2013; Zhang et al., 2013), incorporating global information (Zhao and Kit, 2007; Zhang et al., 2016b), and joint models (Qian and Liu, 2012; Li and Zhou, 2012). Related Work 8 Neural Network Models. Most modern CWS methods followed (Xue, 2003) treated CWS as a sequence labeling problems (Zhao et al., 2006b). Recently, researchers have tended to explore neural network based approaches (Collobert et al., 2011) to reduce efforts of feature engineering (Zheng et al., 2013; Qi et al., 2014; Chen et al., 2015a; Chen et al., 2015b). They modeled CWS as tagging problem as well, scoring tags on individual characters. In those models, tag scores are decided by context information within local windows and the sentence-level score is obtained via context-indepe"
P16-1039,D15-1141,0,0.396889,"Missing"
P16-1039,I05-3025,0,0.111401,"Missing"
P16-1039,P15-1167,0,0.194294,"s sequence labeling schemes, Zhang and Clark (2007) proposed a word-based perceptron method. Zhang et al. (2012) used a linear-time incremental model which can also benefits from various kinds of features including word-based features. But both of them rely heavily on massive handcrafted features. Contemporary to this work, some neural models (Zhang et al., 2016a; Liu et al., 2016) also leverage word-level information. Specifically, Liu et al. (2016) use a semi-CRF taking segment-level embeddings as input and Zhang et al. (2016a) use a transition-based framework. Another notable exception is (Ma and Hinrichs, 2015), which is also an embedding-based model, but models CWS as configuration-action matching. However, again, this method only uses the context information within limited sized windows. Table 7: Results on MSR dataset with different maximum decoding word length settings. on PKU dataset but a competitive result on MSR dataset, which can attribute to too strict maximum word length setting for consistence as it is well known that MSR corpus has a much longer average word length (Zhao et al., 2010). Table 7 demonstrates the results on MSR corpus with different maximum decoding word lengths, in which"
P16-1039,I05-3017,0,0.610519,"generally outperform both likelihood and perception methods. For a given character sequence x(i) , denote the correct segmented sentence for x(i) as y (i) . We define a structured margin loss ∆(y (i) , yˆ) for predicting a segmented sentence yˆ: (i) ∆(y , yˆ) = m X d = 50 H = 50 α = 0.2 µ = 0.2 λ = 10−6 p = 0.2 w=4 gt,i where α is the initial learning rate and gτ,i ∈ R|θi | is the subgradient at time step τ for parameter θi . 6 6.1 Experiments Datasets To evaluate the proposed segmenter, we use two popular datasets, PKU and MSR, from the second International Chinese Word Segmentation Bakeoff (Emerson, 2005). These datasets are commonly used by previous state-of-the-art models and neural network models. Both datasets are preprocessed by replacing the continuous English characters and digits with a unique token. All experiments are conducted with standard Bakeoff scoring program1 calculating precision, recall, and F1 -score. µ1{y (i),t 6= yˆt } t=1 where m is the length of sequence x(i) and µ is the discount parameter. The calculation of margin loss could be regarded as to count the number of incorrectly segmented characters and then multiple it with a fixed discount parameter for smoothing. There"
P16-1039,P14-1028,0,0.62049,"t pervised learning methods such as Maximum Entropy (Berger et al., 1996; Low et al., 2005) and Conditional Random Fields (Lafferty et al., 2001; Peng et al., 2004; Zhao et al., 2006a). However, those methods heavily depend on the choice of handcrafted features. Recently, neural models have been widely used for NLP tasks for their ability to minimize the effort in feature engineering. For the task of CWS, Zheng et al. (2013) adapted the general neural network architecture for sequence labeling proposed in (Collobert et al., 2011), and used character embeddings as input to a two-layer network. Pei et al. (2014) improved upon (Zheng et al., 2013) by explicitly modeling the interactions between local context and previous tag. Chen et al. (2015a) proposed a gated recursive neural network to model the feature combinations of context characters. Chen et al. (2015b) used an LSTM architecture to capture potential long-distance dependencies, which alleviates the limitation of the size of context window but introduced another window for hidden states. Despite the differences, all these models are designed to solve CWS by assigning labels to the characters in the sequence one by one. At each time step of infe"
P16-1039,C04-1081,0,0.7827,"Missing"
P16-1039,Y06-1001,1,0.844322,"compute the tag scores of character based on (i) context features within a fixed sized local window and (ii) tagging history of previous one. Nevertheless, the tag-tag transition is insufficient to model the complicated influence from previous segmentation decisions, though it could sometimes be a crucial clue to later segmentation decisions. The fixed context window size, which is broadly adopted by these methods for feature engineering, also restricts the flexibility of modeling diverse distances. Moreover, word-level information, which is being the greater granularity unit as suggested in (Huang and Zhao, 2006), remains Most previous approaches to Chinese word segmentation formalize this problem as a character-based sequence labeling task so that only contextual information within fixed sized local windows and simple interactions between adjacent tags can be captured. In this paper, we propose a novel neural framework which thoroughly eliminates context windows and can utilize complete segmentation history. Our model employs a gated combination neural network over characters to produce distributed representations of word candidates, which are then given to a long shortterm memory (LSTM) language sco"
P16-1039,N16-1064,1,0.375566,"work. In order to obtain word representation through its characters, in the simplest strategy, character vectors are integrated into their word representation using a weight matrix W(L) that is shared across all words with the same length L, followed by a non-linear function g(·). Specifically, ci (1 ≤ i ≤ L) are d-dimensional character vector representations respectively, the corresponding word vector w will be d-dimensional as well:   c1   w = g(W(L)  ... ) (1) cL Word Score Character Embedding. While the scores are decided at the word-level, using word embedding (Bengio et al., 2003; Wang et al., 2016) immediately will lead to a remarkable issue that rare words and out-of-vocabulary words will be poorly estimated (Kim et al., 2015). In addition, the character-level information inside an n-gram can be helpful to judge whether it is a true word. Therefore, a lookup table of character embeddings is used as the bottom layer. Formally, we have a character dictionary D of where W(L) ∈ Rd×Ld and g is a non-linear function as mentioned above. Although the mechanism above seems to work well, it can not sufficiently model the complicated combination features in practice, yet. Gated structure in neura"
P16-1039,D12-1046,0,0.0350566,"ing epochs (for results in Figure 6 and the last row of Table 6) only with two cores of an Intel i7-5960X CPU. The requirement for RAM during training is less than 800MB. The trained model can be saved within 4MB on the hard disk. 7 Other Techniques. The proposed model might furthermore benefit from some techniques in recent state-of-the-art systems, such as semisupervised learning (Zhao and Kit, 2008b; Zhao and Kit, 2008a; Sun and Xu, 2011; Zhao and Kit, 2011; Zeng et al., 2013; Zhang et al., 2013), incorporating global information (Zhao and Kit, 2007; Zhang et al., 2016b), and joint models (Qian and Liu, 2012; Li and Zhou, 2012). Related Work 8 Neural Network Models. Most modern CWS methods followed (Xue, 2003) treated CWS as a sequence labeling problems (Zhao et al., 2006b). Recently, researchers have tended to explore neural network based approaches (Collobert et al., 2011) to reduce efforts of feature engineering (Zheng et al., 2013; Qi et al., 2014; Chen et al., 2015a; Chen et al., 2015b). They modeled CWS as tagging problem as well, scoring tags on individual characters. In those models, tag scores are decided by context information within local windows and the sentence-level score is obtaine"
P16-1039,O03-4002,0,0.85111,"mploys a gated combination neural network over characters to produce distributed representations of word candidates, which are then given to a long shortterm memory (LSTM) language scoring model. Experiments on the benchmark datasets show that without the help of feature engineering as most existing approaches, our models achieve competitive or better performances with previous stateof-the-art methods. 1 Introduction Most east Asian languages including Chinese are written without explicit word delimiters, therefore, word segmentation is a preliminary step for processing those languages. Since Xue (2003), most methods formalize the Chinese word segmentation (CWS) as a sequence labeling problem with character position tags, which can be handled with su∗ Corresponding author. This paper was partially supported by Cai Yuanpei Program (CSC No. 201304490199 and No. 201304490171), National Natural Science Foundation of China (No. 61170114 and No. 61272248), National Basic Research Program of China (No. 2013CB329401), Major Basic Research Program of Shanghai Science and Technology Committee (No. 15JC1400103), Art and Science Interdisciplinary Funds of Shanghai Jiao Tong University (No. 14JCRZ04), an"
P16-1039,P13-1076,0,0.028982,"For the running cost, we roughly report the current computation consuming on PKU dataset.7 It takes about two days to finish 50 training epochs (for results in Figure 6 and the last row of Table 6) only with two cores of an Intel i7-5960X CPU. The requirement for RAM during training is less than 800MB. The trained model can be saved within 4MB on the hard disk. 7 Other Techniques. The proposed model might furthermore benefit from some techniques in recent state-of-the-art systems, such as semisupervised learning (Zhao and Kit, 2008b; Zhao and Kit, 2008a; Sun and Xu, 2011; Zhao and Kit, 2011; Zeng et al., 2013; Zhang et al., 2013), incorporating global information (Zhao and Kit, 2007; Zhang et al., 2016b), and joint models (Qian and Liu, 2012; Li and Zhou, 2012). Related Work 8 Neural Network Models. Most modern CWS methods followed (Xue, 2003) treated CWS as a sequence labeling problems (Zhao et al., 2006b). Recently, researchers have tended to explore neural network based approaches (Collobert et al., 2011) to reduce efforts of feature engineering (Zheng et al., 2013; Qi et al., 2014; Chen et al., 2015a; Chen et al., 2015b). They modeled CWS as tagging problem as well, scoring tags on individual"
P16-1039,P13-1045,0,0.0367159,"Missing"
P16-1039,P07-1106,0,0.450676,"hem with a unique token. This treatment does not strictly follow the convention of closed-set setting defined by SIGHAN Bakeoff, as no linguistic resources, either dictionary or corpus, other than the training corpus, should be adopted. 5 To make comparisons fair, we re-run their code (https://github.com/dalstonChen) without their unspecified Chinese idiom dictionary. 6 416 http://code.google.com/p/word2vec/ Max. word length 4 5 6 F1 score 96.5 96.7 96.8 Time (Days) 4 5 6 actions taken by the segmenter should be considered. Alternatives to Sequence Labeling. Besides sequence labeling schemes, Zhang and Clark (2007) proposed a word-based perceptron method. Zhang et al. (2012) used a linear-time incremental model which can also benefits from various kinds of features including word-based features. But both of them rely heavily on massive handcrafted features. Contemporary to this work, some neural models (Zhang et al., 2016a; Liu et al., 2016) also leverage word-level information. Specifically, Liu et al. (2016) use a semi-CRF taking segment-level embeddings as input and Zhang et al. (2016a) use a transition-based framework. Another notable exception is (Ma and Hinrichs, 2015), which is also an embedding-"
P16-1039,W12-6308,0,0.0148156,"the convention of closed-set setting defined by SIGHAN Bakeoff, as no linguistic resources, either dictionary or corpus, other than the training corpus, should be adopted. 5 To make comparisons fair, we re-run their code (https://github.com/dalstonChen) without their unspecified Chinese idiom dictionary. 6 416 http://code.google.com/p/word2vec/ Max. word length 4 5 6 F1 score 96.5 96.7 96.8 Time (Days) 4 5 6 actions taken by the segmenter should be considered. Alternatives to Sequence Labeling. Besides sequence labeling schemes, Zhang and Clark (2007) proposed a word-based perceptron method. Zhang et al. (2012) used a linear-time incremental model which can also benefits from various kinds of features including word-based features. But both of them rely heavily on massive handcrafted features. Contemporary to this work, some neural models (Zhang et al., 2016a; Liu et al., 2016) also leverage word-level information. Specifically, Liu et al. (2016) use a semi-CRF taking segment-level embeddings as input and Zhang et al. (2016a) use a transition-based framework. Another notable exception is (Ma and Hinrichs, 2015), which is also an embedding-based model, but models CWS as configuration-action matching."
P16-1039,D11-1090,0,0.113901,"Missing"
P16-1039,D13-1031,0,0.824109,"2015a)* (Chen et al., 2015b)* This work P 92.8 93.7 94.6 94.6 95.5 PKU R 92.0 93.4 94.2 94.0 94.9 F 92.4 93.5 94.4 94.3 95.2 P 92.9 94.6 94.6 94.5 96.1 MSR R 93.6 94.2 95.6 95.5 96.7 F 93.3 94.4 95.1 95.0 96.4 93.5 94.4 94.8 95.1 95.8 92.2 93.6 94.1 94.4 95.2 92.8 94.0 94.5 94.8 95.5 94.2 95.2 94.9 95.1 96.3 93.7 94.6 95.9 96.2 96.8 93.9 94.9 95.4 95.6 96.5 Table 5: Comparison with previous neural network models. Results with * are from our runs on their released implementations.5 Models (Tseng et al., 2005) (Zhang and Clark, 2007) (Zhao and Kit, 2008b) (Sun et al., 2009) (Sun et al., 2012) (Zhang et al., 2013) (Chen et al., 2015a) (Chen et al., 2015b) This work PKU 95.0 94.5 95.4 95.2 95.4 94.5 94.8 95.5 MSR 96.4 97.2 97.6 97.3 97.4 95.4 95.6 96.5 PKU 96.1* 96.4* 96.5* - MSR 97.4* 97.6* 97.4* - Table 6: Comparison with previous state-of-the-art models. Results with * used external dictionary or corpus. We first compare our model with the latest neural network methods as shown in Table 4. However, (Chen et al., 2015a; Chen et al., 2015b) used an extra preprocess to filter out Chinese idioms according to an external dictionary.4 Table 4 lists the results (F1 -scores) with different dictionaries, whic"
P16-1039,N09-1007,0,0.566706,"Missing"
P16-1039,P16-1040,0,0.512358,") without their unspecified Chinese idiom dictionary. 6 416 http://code.google.com/p/word2vec/ Max. word length 4 5 6 F1 score 96.5 96.7 96.8 Time (Days) 4 5 6 actions taken by the segmenter should be considered. Alternatives to Sequence Labeling. Besides sequence labeling schemes, Zhang and Clark (2007) proposed a word-based perceptron method. Zhang et al. (2012) used a linear-time incremental model which can also benefits from various kinds of features including word-based features. But both of them rely heavily on massive handcrafted features. Contemporary to this work, some neural models (Zhang et al., 2016a; Liu et al., 2016) also leverage word-level information. Specifically, Liu et al. (2016) use a semi-CRF taking segment-level embeddings as input and Zhang et al. (2016a) use a transition-based framework. Another notable exception is (Ma and Hinrichs, 2015), which is also an embedding-based model, but models CWS as configuration-action matching. However, again, this method only uses the context information within limited sized windows. Table 7: Results on MSR dataset with different maximum decoding word length settings. on PKU dataset but a competitive result on MSR dataset, which can attribu"
P16-1039,P12-1027,0,0.362842,"Missing"
P16-1039,P16-1131,1,0.653525,") without their unspecified Chinese idiom dictionary. 6 416 http://code.google.com/p/word2vec/ Max. word length 4 5 6 F1 score 96.5 96.7 96.8 Time (Days) 4 5 6 actions taken by the segmenter should be considered. Alternatives to Sequence Labeling. Besides sequence labeling schemes, Zhang and Clark (2007) proposed a word-based perceptron method. Zhang et al. (2012) used a linear-time incremental model which can also benefits from various kinds of features including word-based features. But both of them rely heavily on massive handcrafted features. Contemporary to this work, some neural models (Zhang et al., 2016a; Liu et al., 2016) also leverage word-level information. Specifically, Liu et al. (2016) use a semi-CRF taking segment-level embeddings as input and Zhang et al. (2016a) use a transition-based framework. Another notable exception is (Ma and Hinrichs, 2015), which is also an embedding-based model, but models CWS as configuration-action matching. However, again, this method only uses the context information within limited sized windows. Table 7: Results on MSR dataset with different maximum decoding word length settings. on PKU dataset but a competitive result on MSR dataset, which can attribu"
P16-1039,I05-3027,0,0.252034,"Missing"
P16-1039,I08-4017,1,0.64696,"edding (Zheng et al., 2013) (Pei et al., 2014) (Chen et al., 2015a)* (Chen et al., 2015b)* This work P 92.8 93.7 94.6 94.6 95.5 PKU R 92.0 93.4 94.2 94.0 94.9 F 92.4 93.5 94.4 94.3 95.2 P 92.9 94.6 94.6 94.5 96.1 MSR R 93.6 94.2 95.6 95.5 96.7 F 93.3 94.4 95.1 95.0 96.4 93.5 94.4 94.8 95.1 95.8 92.2 93.6 94.1 94.4 95.2 92.8 94.0 94.5 94.8 95.5 94.2 95.2 94.9 95.1 96.3 93.7 94.6 95.9 96.2 96.8 93.9 94.9 95.4 95.6 96.5 Table 5: Comparison with previous neural network models. Results with * are from our runs on their released implementations.5 Models (Tseng et al., 2005) (Zhang and Clark, 2007) (Zhao and Kit, 2008b) (Sun et al., 2009) (Sun et al., 2012) (Zhang et al., 2013) (Chen et al., 2015a) (Chen et al., 2015b) This work PKU 95.0 94.5 95.4 95.2 95.4 94.5 94.8 95.5 MSR 96.4 97.2 97.6 97.3 97.4 95.4 95.6 96.5 PKU 96.1* 96.4* 96.5* - MSR 97.4* 97.6* 97.4* - Table 6: Comparison with previous state-of-the-art models. Results with * used external dictionary or corpus. We first compare our model with the latest neural network methods as shown in Table 4. However, (Chen et al., 2015a; Chen et al., 2015b) used an extra preprocess to filter out Chinese idioms according to an external dictionary.4 Table 4 lis"
P16-1039,W06-0127,1,0.319178,"B. The trained model can be saved within 4MB on the hard disk. 7 Other Techniques. The proposed model might furthermore benefit from some techniques in recent state-of-the-art systems, such as semisupervised learning (Zhao and Kit, 2008b; Zhao and Kit, 2008a; Sun and Xu, 2011; Zhao and Kit, 2011; Zeng et al., 2013; Zhang et al., 2013), incorporating global information (Zhao and Kit, 2007; Zhang et al., 2016b), and joint models (Qian and Liu, 2012; Li and Zhou, 2012). Related Work 8 Neural Network Models. Most modern CWS methods followed (Xue, 2003) treated CWS as a sequence labeling problems (Zhao et al., 2006b). Recently, researchers have tended to explore neural network based approaches (Collobert et al., 2011) to reduce efforts of feature engineering (Zheng et al., 2013; Qi et al., 2014; Chen et al., 2015a; Chen et al., 2015b). They modeled CWS as tagging problem as well, scoring tags on individual characters. In those models, tag scores are decided by context information within local windows and the sentence-level score is obtained via context-independently tag transitions. Pei et al. (2014) introduced the tag embedding as input to capture the combinations of context and tag history. However, i"
P16-1039,D13-1061,0,0.645687,"ab of Shanghai Education Commission for Intelligent Interaction and Cognitive Engineering Shanghai Jiao Tong University, Shanghai, China thisisjcykcd@gmail.com, zhaohai@cs.sjtu.edu.cn Abstract pervised learning methods such as Maximum Entropy (Berger et al., 1996; Low et al., 2005) and Conditional Random Fields (Lafferty et al., 2001; Peng et al., 2004; Zhao et al., 2006a). However, those methods heavily depend on the choice of handcrafted features. Recently, neural models have been widely used for NLP tasks for their ability to minimize the effort in feature engineering. For the task of CWS, Zheng et al. (2013) adapted the general neural network architecture for sequence labeling proposed in (Collobert et al., 2011), and used character embeddings as input to a two-layer network. Pei et al. (2014) improved upon (Zheng et al., 2013) by explicitly modeling the interactions between local context and previous tag. Chen et al. (2015a) proposed a gated recursive neural network to model the feature combinations of context characters. Chen et al. (2015b) used an LSTM architecture to capture potential long-distance dependencies, which alleviates the limitation of the size of context window but introduced anot"
P16-1039,Y06-1012,1,\N,Missing
P16-1131,P16-1039,1,0.495909,"ences of all words in a sentence is used so that sentence-level information can be effectively captured. Secondly, a linear layer is added to integrate different order neural models and trained with perceptron method. The proposed parsers are evaluated on English and Chinese Penn Treebanks and obtain competitive accuracies. 1 Introduction Neural network methods have shown great promise in the field of parsing and other related natural language processing tasks, exploiting more complex features with distributed representation and non-linear neural network (Wang et al., 2013; Wang et al., 2014; Cai and Zhao, 2016; Wang et al., 2016). In transition-based dependency parsing, neural models that can represent the partial or whole parsing histories have been explored (Weiss et al., 2015; Dyer et al., 2015). While for graphbased parsing, on which we focus in this work, Pei et al. (2015) also show the effectiveness of neural methods. ∗ Corresponding author. This paper was partially supported by Cai Yuanpei Program (CSC No. 201304490199 and No. 201304490171), National Natural Science Foundation of China (No. 61170114 and No. 61272248), National Basic Research Program of China (No. 2013CB329401), Major Basic R"
P16-1131,D14-1082,0,0.0685025,"Missing"
P16-1131,P15-1017,0,0.0080287,"proposed parser has output competitive performance for different dependency conversion conventions and treebanks. Compared with traditional graphbased linear models, neural models may benefit from better feature representations and more general non-linear transformations. The results and comparisons in Table 2 demonstrate the proposed models can obtain comparable accuracies, which show the effectiveness of combining local and global features through windowbased and convolutional neural networks. 5 Related Work CNN has been explored in recent work of relation classification (Zeng et al., 2014; Chen et al., 2015), which resembles the task of deciding dependency relations in parsing. However, relation classification usually involves labeling for given arguments and seldom needs to consider the global structure. Parsing is more complex for it needs to predict structures and the use of CNN should be incorporated with the searching algorithms. Neural network methods have been proved effective for graph-based parsing. Lei et al. (2014) explore a tensor scoring method, however, it needs to combine scores from linear models and we are not able to compare with it because of different datasets (they take datas"
P16-1131,P04-1015,0,0.033663,"ly fixed weights. For each model to be combined, we concatenate the output layer and all hidden layers (except embedding layer h0 ): vall = [s, h1 , h2 ] All vall from different models are again concatenated to form the input for the final linear layer and the final scores are obtained through a linear transformation (no bias adding): vcombine = [vall-o1 , vall-o2 , vall-o3 ] scombine = Wcombine · vcombine We no longer update weights for the underlying neural models, and the learning of the final layer is equally training a linear model, for which structured average perceptron (Collins, 2002; Collins and Roark, 2004) is adopted for simplicity. This ensemble scheme can be extended in several ways which might be explored in future work: (1) feed-forward network can be stacked rather than a single linear layer, (2) traditional sparse features can also be concatenated to vcombine to combine manually specified representations with distributed neural representations as in (Zhang and Zhang, 2015). 4 Experiments The proposed parsers are evaluated on English Penn Treebank (PTB) and Chinese Penn Treebank (CTB). Unlabeled attachment scores (UAS), labeled attachment scores (LAS) and unlabeled complete matches (CM) ar"
P16-1131,W02-1001,0,0.085939,"er with specially fixed weights. For each model to be combined, we concatenate the output layer and all hidden layers (except embedding layer h0 ): vall = [s, h1 , h2 ] All vall from different models are again concatenated to form the input for the final linear layer and the final scores are obtained through a linear transformation (no bias adding): vcombine = [vall-o1 , vall-o2 , vall-o3 ] scombine = Wcombine · vcombine We no longer update weights for the underlying neural models, and the learning of the final layer is equally training a linear model, for which structured average perceptron (Collins, 2002; Collins and Roark, 2004) is adopted for simplicity. This ensemble scheme can be extended in several ways which might be explored in future work: (1) feed-forward network can be stacked rather than a single linear layer, (2) traditional sparse features can also be concatenated to vcombine to combine manually specified representations with distributed neural representations as in (Zhang and Zhang, 2015). 4 Experiments The proposed parsers are evaluated on English Penn Treebank (PTB) and Chinese Penn Treebank (CTB). Unlabeled attachment scores (UAS), labeled attachment scores (LAS) and unlabele"
P16-1131,de-marneffe-etal-2006-generating,0,0.0888227,"Missing"
P16-1131,P15-1030,0,0.013493,"ikely parse tree, the other is the parameter estimation approach for the machine learning models. For the former, classical dynamic programming algorithms are usually adopted, while for the latter, there are various solutions. Like some previous neural methods (Socher et al., 2010; Socher et al., 2013), to tackle the structure prediction problems, Pei et al. (2015) utilize a max-margin training criterion, which does not include probabilistic explanations. Re-visiting the traditional probabilistic criteria in log-linear models, this work utilizes maximum likelihood for neural network training. Durrett and Klein (2015) adopt this method for constituency parsing, which scores the anchored rules with neural models and formalizes the probabilities with tree-structured random fields. Motivated by this work, we utilize the probabilistic treatment for dependency parsing: scoring the edges or high-order sub-trees with a neural model and calculating the gradients according to probabilistic criteria. Although scores are computed by a neural network, the existing dynamic programming algorithms for gradient calculation remain the same as those in log-linear models. Graph-based methods search globally through the whole"
P16-1131,P15-1033,0,0.00990828,"d with perceptron method. The proposed parsers are evaluated on English and Chinese Penn Treebanks and obtain competitive accuracies. 1 Introduction Neural network methods have shown great promise in the field of parsing and other related natural language processing tasks, exploiting more complex features with distributed representation and non-linear neural network (Wang et al., 2013; Wang et al., 2014; Cai and Zhao, 2016; Wang et al., 2016). In transition-based dependency parsing, neural models that can represent the partial or whole parsing histories have been explored (Weiss et al., 2015; Dyer et al., 2015). While for graphbased parsing, on which we focus in this work, Pei et al. (2015) also show the effectiveness of neural methods. ∗ Corresponding author. This paper was partially supported by Cai Yuanpei Program (CSC No. 201304490199 and No. 201304490171), National Natural Science Foundation of China (No. 61170114 and No. 61272248), National Basic Research Program of China (No. 2013CB329401), Major Basic Research Program of Shanghai Science and Technology Committee (No. 15JC1400103), Art and Science Interdisciplinary Funds of Shanghai Jiao Tong University (No. 14JCRZ04), and Key Project of Nati"
P16-1131,C96-1058,0,0.647947,"Figure 1: The decompositions of factors. define the order of the graph model. Three different ordered factorizations considered in this work and their sub-tree patterns are shown in Figure 1. The score for a dependency tree (T ) is defined as the sum of the scores of all its factors (p): Score(T ) = X Score(p) p∈T In this way, the dependency parsing task is to find a max-scoring tree. For projective dependency parsing considered in this work, this searching problem is conquered by dynamic programming algorithms with the key assumption that the factors are scored independently. Previous work (Eisner, 1996; McDonald et al., 2005; McDonald and Pereira, 2006; Koo and Collins, 2010; Ma and Zhao, 2012) explores ingenious algorithms for decoding ranging from first-order to higher-orders. Our proposed parsers also take these algorithms as backbones and use them for inference. 2.2 Probabilistic Model With the graph factorization and inference, the remaining problems are how to obtain the scores and how to train the scoring model. For the scoring models, traditional linear methods utilize manually specified features and linear scoring models, while we adopt neural network models, which may exploit bett"
P16-1131,P08-1109,0,0.0260606,"Missing"
P16-1131,W15-1508,0,0.127884,"Missing"
P16-1131,P02-1036,0,0.0326478,"g models depend on the underlying base parsers, which might already miss the correct trees. Generally, the re-ranking techniques play a role of additional enhancement for basic parsing models, and therefore they are not included in our comparisons. The conditional log-likelihood probabilistic criterion utilized in this work is actually a (conditioned) Markov Random Field for tree structures, and it has been applied to parsing since long time ago. Johnson et al. (1999) utilize the Markov Random Fields for stochastic grammars and gradient based methods are adopted for parameter estimations, and Geman and Johnson (2002) extend this with dynamic programming algorithms for inference and marginal-probability calculation. Collins (2000) uses the same probabilistic treatment for re-ranking and the denominator only includes the candidate trees which can be seen as an approximation for the whole space of trees. Finkel et al. (2008) utilize it for feature-based parsing. The probabilistic training criterion for linear graphbased dependency models have been also explored in (Li et al., 2014; Ma and Zhao, 2015). However, these previous methods usually exploit loglinear models utilizing sparse features for input represe"
P16-1131,N10-1112,0,0.0184658,"golden tree and a predicted tree, and its sub-gradient can be written in a similar form:    ∂Lm (θ) X ∂Score(p)   = − p ∈ Tg + p ∈ Tb ∂θ ∂θ p Here, the predicted tree Tb is the best-scored tree with a structured margin loss in the score. Comparing the derivatives, we can see that the one of probabilistic criteria can be viewed as a soft version of the max-margin criteria, and all the possible factors are considered when calculating gradients for the probabilistic way, while only wrongly predicted factors have non-zero subgradients for max-margin training. This observation is not new and Gimpel and Smith (2010) provide a good review of several training criteria. It might be interesting to explore the impacts of different training criteria on the parsing performance, and we will leave it for future research. 2.4 T 0 ∈T (p) Here, T (p) is the set of trees that contain the factor p, and the inner summation is defined as the marginal probability m(p): X m(p) = Pr(T 0 ) T 0 ∈T (p) which can be viewed as the mass of all the trees containing the specified factor p. The calculation of m(p) (Paskin, 2001; Ma and Zhao, 2015) is solved by a variant of inside-outside algorithm, which is of the same complexity c"
P16-1131,W07-2416,0,0.0481073,"Missing"
P16-1131,P99-1069,0,0.140134,"Network 1389 (Le and Zuidema, 2014) and Recursive CNN (Zhu et al., 2015), are utilized for capturing features with more contexts. However, re-ranking models depend on the underlying base parsers, which might already miss the correct trees. Generally, the re-ranking techniques play a role of additional enhancement for basic parsing models, and therefore they are not included in our comparisons. The conditional log-likelihood probabilistic criterion utilized in this work is actually a (conditioned) Markov Random Field for tree structures, and it has been applied to parsing since long time ago. Johnson et al. (1999) utilize the Markov Random Fields for stochastic grammars and gradient based methods are adopted for parameter estimations, and Geman and Johnson (2002) extend this with dynamic programming algorithms for inference and marginal-probability calculation. Collins (2000) uses the same probabilistic treatment for re-ranking and the denominator only includes the candidate trees which can be seen as an approximation for the whole space of trees. Finkel et al. (2008) utilize it for feature-based parsing. The probabilistic training criterion for linear graphbased dependency models have been also explor"
P16-1131,P10-1001,0,0.147789,"graph model. Three different ordered factorizations considered in this work and their sub-tree patterns are shown in Figure 1. The score for a dependency tree (T ) is defined as the sum of the scores of all its factors (p): Score(T ) = X Score(p) p∈T In this way, the dependency parsing task is to find a max-scoring tree. For projective dependency parsing considered in this work, this searching problem is conquered by dynamic programming algorithms with the key assumption that the factors are scored independently. Previous work (Eisner, 1996; McDonald et al., 2005; McDonald and Pereira, 2006; Koo and Collins, 2010; Ma and Zhao, 2012) explores ingenious algorithms for decoding ranging from first-order to higher-orders. Our proposed parsers also take these algorithms as backbones and use them for inference. 2.2 Probabilistic Model With the graph factorization and inference, the remaining problems are how to obtain the scores and how to train the scoring model. For the scoring models, traditional linear methods utilize manually specified features and linear scoring models, while we adopt neural network models, which may exploit better feature representations. For the training methods, in recent neural gra"
P16-1131,D14-1081,0,0.0147606,"it needs to combine scores from linear models and we are not able to compare with it because of different datasets (they take datasets from CoNLL shared task). Zhang and Zhao (2015) also explore a probabilistic treatment, but its model may give mass to illegal trees or non-trees. Fonseca and Alu´ısio (2015) utilize CNN for scoring edges, though only explore first-order parsing. Its model is based on head selection for each modifier and might be difficult to be extended to high-order parsing. Recently, several neural re-ranking models, like Inside-Outside Recursive Neural Network 1389 (Le and Zuidema, 2014) and Recursive CNN (Zhu et al., 2015), are utilized for capturing features with more contexts. However, re-ranking models depend on the underlying base parsers, which might already miss the correct trees. Generally, the re-ranking techniques play a role of additional enhancement for basic parsing models, and therefore they are not included in our comparisons. The conditional log-likelihood probabilistic criterion utilized in this work is actually a (conditioned) Markov Random Field for tree structures, and it has been applied to parsing since long time ago. Johnson et al. (1999) utilize the Ma"
P16-1131,P14-1130,0,0.0143447,"nd global features through windowbased and convolutional neural networks. 5 Related Work CNN has been explored in recent work of relation classification (Zeng et al., 2014; Chen et al., 2015), which resembles the task of deciding dependency relations in parsing. However, relation classification usually involves labeling for given arguments and seldom needs to consider the global structure. Parsing is more complex for it needs to predict structures and the use of CNN should be incorporated with the searching algorithms. Neural network methods have been proved effective for graph-based parsing. Lei et al. (2014) explore a tensor scoring method, however, it needs to combine scores from linear models and we are not able to compare with it because of different datasets (they take datasets from CoNLL shared task). Zhang and Zhao (2015) also explore a probabilistic treatment, but its model may give mass to illegal trees or non-trees. Fonseca and Alu´ısio (2015) utilize CNN for scoring edges, though only explore first-order parsing. Its model is based on head selection for each modifier and might be difficult to be extended to high-order parsing. Recently, several neural re-ranking models, like Inside-Outs"
P16-1131,P14-1043,0,0.013233,"e the Markov Random Fields for stochastic grammars and gradient based methods are adopted for parameter estimations, and Geman and Johnson (2002) extend this with dynamic programming algorithms for inference and marginal-probability calculation. Collins (2000) uses the same probabilistic treatment for re-ranking and the denominator only includes the candidate trees which can be seen as an approximation for the whole space of trees. Finkel et al. (2008) utilize it for feature-based parsing. The probabilistic training criterion for linear graphbased dependency models have been also explored in (Li et al., 2014; Ma and Zhao, 2015). However, these previous methods usually exploit loglinear models utilizing sparse features for input representations and linear models for score calculations, which are replaced by more sophisticated distributed representations and neural models, as shown in this work. 6 Conclusions This work presents neural probabilistic graphbased models for dependency parsing, together with a convolutional part which could capture the sentence-level information. With distributed vectors for representations and complex non-linear neural network for calculations, the model can effectivel"
P16-1131,D15-1154,0,0.0123421,"els with the probabilistic training criteria unchanged. Training Criteria Labeled Parsing In a dependency tree, each edge can be given a label indicating the type of the dependency relation, this labeling procedure can be integrated directly into the parsing task, instead of a second pass after obtaining the structure. For the probabilistic model, integrating labeled parsing only needs some extensions for the inference procedure and marginal probability calculations. For the simplicity, we only consider a single label for each factor (even for high-order ones) which corresponds to Model 1 in (Ma and Hovy, 2015): the label of the edge between head and modifier word, which will only multiply O(l) to the complexity. We find this direct approach not only achieves labeled parsing in one pass, but also improves unlabeled attachment accuracies (see Section 4.3), which may benefit from the joint learning with the labels. 3 Neural Model The task for the neural models is computing the labeled scores of the factors. The inputs are the words in a factor with contexts, and the outputs are the scores for this factor to be valid in the dependency tree. We propose neural models to in1384 Output s = Wsh2 + bs Hidden"
P16-1131,C12-2077,1,0.881885,"ferent ordered factorizations considered in this work and their sub-tree patterns are shown in Figure 1. The score for a dependency tree (T ) is defined as the sum of the scores of all its factors (p): Score(T ) = X Score(p) p∈T In this way, the dependency parsing task is to find a max-scoring tree. For projective dependency parsing considered in this work, this searching problem is conquered by dynamic programming algorithms with the key assumption that the factors are scored independently. Previous work (Eisner, 1996; McDonald et al., 2005; McDonald and Pereira, 2006; Koo and Collins, 2010; Ma and Zhao, 2012) explores ingenious algorithms for decoding ranging from first-order to higher-orders. Our proposed parsers also take these algorithms as backbones and use them for inference. 2.2 Probabilistic Model With the graph factorization and inference, the remaining problems are how to obtain the scores and how to train the scoring model. For the scoring models, traditional linear methods utilize manually specified features and linear scoring models, while we adopt neural network models, which may exploit better feature representations. For the training methods, in recent neural graph-based parsers, no"
P16-1131,P13-2109,0,0.0302591,"Missing"
P16-1131,P08-1108,0,0.0161474,". A simple adding scheme is often used. For nonlinear neural models, we use an explicit adding method. For example, in third-order parsing, the final score for the factor (g, h, m, s) will be: sadd (g, h, m, s) = so3 (g, h, m, s) + so2 (h, m, s) + so1 (h, m) Here, g, h, m and s represent the grandparent, head, modifier and sibling nodes in the grandsibling third-order factor; so1 , so2 and so3 stand for the corresponding lower-order scores from first, second and third order models, respectively. We notice that ensemble or stacking methods for dependency parsing have explored in previous work (Nivre and McDonald, 2008; Torres Martins et al., 2008). Recently, Weiss et al. (2015) stack a linear layer for the final scoring in a single model, and we extend this method to combine multiple models by stacking a linear layer on their output and hidden layers. The simple adding scheme can 1386 be viewed as adopting a final layer with specially fixed weights. For each model to be combined, we concatenate the output layer and all hidden layers (except embedding layer h0 ): vall = [s, h1 , h2 ] All vall from different models are again concatenated to form the input for the final linear layer and the final scores are o"
P16-1131,P15-1031,0,0.144414,"e Penn Treebanks and obtain competitive accuracies. 1 Introduction Neural network methods have shown great promise in the field of parsing and other related natural language processing tasks, exploiting more complex features with distributed representation and non-linear neural network (Wang et al., 2013; Wang et al., 2014; Cai and Zhao, 2016; Wang et al., 2016). In transition-based dependency parsing, neural models that can represent the partial or whole parsing histories have been explored (Weiss et al., 2015; Dyer et al., 2015). While for graphbased parsing, on which we focus in this work, Pei et al. (2015) also show the effectiveness of neural methods. ∗ Corresponding author. This paper was partially supported by Cai Yuanpei Program (CSC No. 201304490199 and No. 201304490171), National Natural Science Foundation of China (No. 61170114 and No. 61272248), National Basic Research Program of China (No. 2013CB329401), Major Basic Research Program of Shanghai Science and Technology Committee (No. 15JC1400103), Art and Science Interdisciplinary Funds of Shanghai Jiao Tong University (No. 14JCRZ04), and Key Project of National Society Science Foundation of China (No. 15-ZDA041). The graph-based parser"
P16-1131,P13-1045,0,0.0226319,"(No. 15JC1400103), Art and Science Interdisciplinary Funds of Shanghai Jiao Tong University (No. 14JCRZ04), and Key Project of National Society Science Foundation of China (No. 15-ZDA041). The graph-based parser generally consists of two components: one is the parsing algorithm for inference or searching the most likely parse tree, the other is the parameter estimation approach for the machine learning models. For the former, classical dynamic programming algorithms are usually adopted, while for the latter, there are various solutions. Like some previous neural methods (Socher et al., 2010; Socher et al., 2013), to tackle the structure prediction problems, Pei et al. (2015) utilize a max-margin training criterion, which does not include probabilistic explanations. Re-visiting the traditional probabilistic criteria in log-linear models, this work utilizes maximum likelihood for neural network training. Durrett and Klein (2015) adopt this method for constituency parsing, which scores the anchored rules with neural models and formalizes the probabilities with tree-structured random fields. Motivated by this work, we utilize the probabilistic treatment for dependency parsing: scoring the edges or high-o"
P16-1131,D08-1017,0,0.0135261,"Missing"
P16-1131,N03-1033,0,0.226409,"Missing"
P16-1131,D13-1082,1,0.850853,"olutional layer that absorbs the influences of all words in a sentence is used so that sentence-level information can be effectively captured. Secondly, a linear layer is added to integrate different order neural models and trained with perceptron method. The proposed parsers are evaluated on English and Chinese Penn Treebanks and obtain competitive accuracies. 1 Introduction Neural network methods have shown great promise in the field of parsing and other related natural language processing tasks, exploiting more complex features with distributed representation and non-linear neural network (Wang et al., 2013; Wang et al., 2014; Cai and Zhao, 2016; Wang et al., 2016). In transition-based dependency parsing, neural models that can represent the partial or whole parsing histories have been explored (Weiss et al., 2015; Dyer et al., 2015). While for graphbased parsing, on which we focus in this work, Pei et al. (2015) also show the effectiveness of neural methods. ∗ Corresponding author. This paper was partially supported by Cai Yuanpei Program (CSC No. 201304490199 and No. 201304490171), National Natural Science Foundation of China (No. 61170114 and No. 61272248), National Basic Research Program of"
P16-1131,D14-1023,1,0.740403,"t absorbs the influences of all words in a sentence is used so that sentence-level information can be effectively captured. Secondly, a linear layer is added to integrate different order neural models and trained with perceptron method. The proposed parsers are evaluated on English and Chinese Penn Treebanks and obtain competitive accuracies. 1 Introduction Neural network methods have shown great promise in the field of parsing and other related natural language processing tasks, exploiting more complex features with distributed representation and non-linear neural network (Wang et al., 2013; Wang et al., 2014; Cai and Zhao, 2016; Wang et al., 2016). In transition-based dependency parsing, neural models that can represent the partial or whole parsing histories have been explored (Weiss et al., 2015; Dyer et al., 2015). While for graphbased parsing, on which we focus in this work, Pei et al. (2015) also show the effectiveness of neural methods. ∗ Corresponding author. This paper was partially supported by Cai Yuanpei Program (CSC No. 201304490199 and No. 201304490171), National Natural Science Foundation of China (No. 61170114 and No. 61272248), National Basic Research Program of China (No. 2013CB32"
P16-1131,E06-1011,0,0.0305921,"ors. define the order of the graph model. Three different ordered factorizations considered in this work and their sub-tree patterns are shown in Figure 1. The score for a dependency tree (T ) is defined as the sum of the scores of all its factors (p): Score(T ) = X Score(p) p∈T In this way, the dependency parsing task is to find a max-scoring tree. For projective dependency parsing considered in this work, this searching problem is conquered by dynamic programming algorithms with the key assumption that the factors are scored independently. Previous work (Eisner, 1996; McDonald et al., 2005; McDonald and Pereira, 2006; Koo and Collins, 2010; Ma and Zhao, 2012) explores ingenious algorithms for decoding ranging from first-order to higher-orders. Our proposed parsers also take these algorithms as backbones and use them for inference. 2.2 Probabilistic Model With the graph factorization and inference, the remaining problems are how to obtain the scores and how to train the scoring model. For the scoring models, traditional linear methods utilize manually specified features and linear scoring models, while we adopt neural network models, which may exploit better feature representations. For the training method"
P16-1131,N16-1064,1,0.46789,"n a sentence is used so that sentence-level information can be effectively captured. Secondly, a linear layer is added to integrate different order neural models and trained with perceptron method. The proposed parsers are evaluated on English and Chinese Penn Treebanks and obtain competitive accuracies. 1 Introduction Neural network methods have shown great promise in the field of parsing and other related natural language processing tasks, exploiting more complex features with distributed representation and non-linear neural network (Wang et al., 2013; Wang et al., 2014; Cai and Zhao, 2016; Wang et al., 2016). In transition-based dependency parsing, neural models that can represent the partial or whole parsing histories have been explored (Weiss et al., 2015; Dyer et al., 2015). While for graphbased parsing, on which we focus in this work, Pei et al. (2015) also show the effectiveness of neural methods. ∗ Corresponding author. This paper was partially supported by Cai Yuanpei Program (CSC No. 201304490199 and No. 201304490171), National Natural Science Foundation of China (No. 61170114 and No. 61272248), National Basic Research Program of China (No. 2013CB329401), Major Basic Research Program of S"
P16-1131,P05-1012,0,0.233778,"decompositions of factors. define the order of the graph model. Three different ordered factorizations considered in this work and their sub-tree patterns are shown in Figure 1. The score for a dependency tree (T ) is defined as the sum of the scores of all its factors (p): Score(T ) = X Score(p) p∈T In this way, the dependency parsing task is to find a max-scoring tree. For projective dependency parsing considered in this work, this searching problem is conquered by dynamic programming algorithms with the key assumption that the factors are scored independently. Previous work (Eisner, 1996; McDonald et al., 2005; McDonald and Pereira, 2006; Koo and Collins, 2010; Ma and Zhao, 2012) explores ingenious algorithms for decoding ranging from first-order to higher-orders. Our proposed parsers also take these algorithms as backbones and use them for inference. 2.2 Probabilistic Model With the graph factorization and inference, the remaining problems are how to obtain the scores and how to train the scoring model. For the scoring models, traditional linear methods utilize manually specified features and linear scoring models, while we adopt neural network models, which may exploit better feature representati"
P16-1131,P15-1032,0,0.202881,"al models and trained with perceptron method. The proposed parsers are evaluated on English and Chinese Penn Treebanks and obtain competitive accuracies. 1 Introduction Neural network methods have shown great promise in the field of parsing and other related natural language processing tasks, exploiting more complex features with distributed representation and non-linear neural network (Wang et al., 2013; Wang et al., 2014; Cai and Zhao, 2016; Wang et al., 2016). In transition-based dependency parsing, neural models that can represent the partial or whole parsing histories have been explored (Weiss et al., 2015; Dyer et al., 2015). While for graphbased parsing, on which we focus in this work, Pei et al. (2015) also show the effectiveness of neural methods. ∗ Corresponding author. This paper was partially supported by Cai Yuanpei Program (CSC No. 201304490199 and No. 201304490171), National Natural Science Foundation of China (No. 61170114 and No. 61272248), National Basic Research Program of China (No. 2013CB329401), Major Basic Research Program of Shanghai Science and Technology Committee (No. 15JC1400103), Art and Science Interdisciplinary Funds of Shanghai Jiao Tong University (No. 14JCRZ04), and"
P16-1131,W03-3023,0,0.130908,"Missing"
P16-1131,C14-1220,0,0.016171,"n, we see that the proposed parser has output competitive performance for different dependency conversion conventions and treebanks. Compared with traditional graphbased linear models, neural models may benefit from better feature representations and more general non-linear transformations. The results and comparisons in Table 2 demonstrate the proposed models can obtain comparable accuracies, which show the effectiveness of combining local and global features through windowbased and convolutional neural networks. 5 Related Work CNN has been explored in recent work of relation classification (Zeng et al., 2014; Chen et al., 2015), which resembles the task of deciding dependency relations in parsing. However, relation classification usually involves labeling for given arguments and seldom needs to consider the global structure. Parsing is more complex for it needs to predict structures and the use of CNN should be incorporated with the searching algorithms. Neural network methods have been proved effective for graph-based parsing. Lei et al. (2014) explore a tensor scoring method, however, it needs to combine scores from linear models and we are not able to compare with it because of different datas"
P16-1131,D08-1059,0,0.0332872,"h might be explored in future work: (1) feed-forward network can be stacked rather than a single linear layer, (2) traditional sparse features can also be concatenated to vcombine to combine manually specified representations with distributed neural representations as in (Zhang and Zhang, 2015). 4 Experiments The proposed parsers are evaluated on English Penn Treebank (PTB) and Chinese Penn Treebank (CTB). Unlabeled attachment scores (UAS), labeled attachment scores (LAS) and unlabeled complete matches (CM) are the metrics. Punctuations2 are ignored as in previous work (Koo and Collins, 2010; Zhang and Clark, 2008). For English, we follow the splitting convention for PTB3: sections 2-21 for training, 22 for developing and 23 for test. We prepare three datasets of PTB, using different conversion tools: (1) Penn2Malt3 and the head rules of Yamada and Matsumoto (2003), noted as PTB-Y&M; (2) dependency converter in Stanford parser v3.3.0 with Stanford Basic Dependencies (De Marneffe et al., 2006), noted as PTB-SD; (3) LTH Constituentto-Dependency Conversion Tool4 (Johansson and 2 Tokens whose gold POS tags are one of {“ ” : , .} for PTB or P U for CTB. 3 http://stp.lingfil.uu.se/˜nivre/research/Penn2Malt.ht"
P16-1131,D15-1153,0,0.0192325,"mbine = Wcombine · vcombine We no longer update weights for the underlying neural models, and the learning of the final layer is equally training a linear model, for which structured average perceptron (Collins, 2002; Collins and Roark, 2004) is adopted for simplicity. This ensemble scheme can be extended in several ways which might be explored in future work: (1) feed-forward network can be stacked rather than a single linear layer, (2) traditional sparse features can also be concatenated to vcombine to combine manually specified representations with distributed neural representations as in (Zhang and Zhang, 2015). 4 Experiments The proposed parsers are evaluated on English Penn Treebank (PTB) and Chinese Penn Treebank (CTB). Unlabeled attachment scores (UAS), labeled attachment scores (LAS) and unlabeled complete matches (CM) are the metrics. Punctuations2 are ignored as in previous work (Koo and Collins, 2010; Zhang and Clark, 2008). For English, we follow the splitting convention for PTB3: sections 2-21 for training, 22 for developing and 23 for test. We prepare three datasets of PTB, using different conversion tools: (1) Penn2Malt3 and the head rules of Yamada and Matsumoto (2003), noted as PTB-Y&M"
P16-1131,Y15-1014,1,0.887382,"lso explore the influences of the convolution operations on dependencies of different lengths, as shown in Figure 4, the convolutional methods help the decisions of long-range dependencies generally. For the highorder parsing in the rest of this paper, we will all adopt the concatenate-add setting. In the third group, we can see that high-order parsing brings significant performance improvement. For high-order parsing, three ensemble schemes are examined: no combination, adding 1388 Methods Graph-NN:proposed o3-adding o3-perceptron Graph-NN:others Pei et al. (2015) Fonseca and Alu´ısio (2015) Zhang and Zhao (2015) Graph-Linear Koo and Collins (2010) Martins et al. (2013) Ma and Zhao (2015) Transition-NN Chen and Manning (2014) Dyer et al. (2015) Weiss et al. (2015) Zhou et al. (2015) PTB-Y&M UAS LAS CM PTB-SD UAS LAS CM PTB-LTH UAS LAS CM CTB UAS LAS 93.20 92.12 48.92 93.31 92.23 50.00 93.42 91.29 50.37 93.42 91.26 49.92 93.14 90.07 43.38 93.12 89.53 43.83 87.55 86.19 35.65 87.65 86.17 36.07 – – – 91.6– 88.9– – 92.52 – 41.10 – – 86.01 – – – – – 31.88 – – – – – 87.2– – – – – – 37.0– – – – – 83.9– 82.4– 87.2– 85.7– – – – – 93.29 92.13 – – – – 93.04 93.07 93.0– – – – – – – – – – 93.28 92.35 – – – – – – –"
P16-1131,D09-1004,1,0.630538,") T 0 ∈T (p) which can be viewed as the mass of all the trees containing the specified factor p. The calculation of m(p) (Paskin, 2001; Ma and Zhao, 2015) is solved by a variant of inside-outside algorithm, which is of the same complexity compared with the corresponding inference algorithms. Finally, the gradients can be represented as:   ∂L(θ) X ∂Score(p)   = − p ∈ Tg + m(p) ∂θ ∂θ p where [p ∈ Tg ] is a binary value which indicates whether p is in tree Tg . Traditional models usually utilize linear functions for the Score function, which might need carefully feature engineering such as (Zhao et al., 2009a; Zhao et al., 2009b; Zhao et al., 2009c; Zhao, 2009; Zhao et al., 2013), while we adopt neural models with the probabilistic training criteria unchanged. Training Criteria Labeled Parsing In a dependency tree, each edge can be given a label indicating the type of the dependency relation, this labeling procedure can be integrated directly into the parsing task, instead of a second pass after obtaining the structure. For the probabilistic model, integrating labeled parsing only needs some extensions for the inference procedure and marginal probability calculations. For the simplicity, we only"
P16-1131,W09-1208,1,0.54445,") T 0 ∈T (p) which can be viewed as the mass of all the trees containing the specified factor p. The calculation of m(p) (Paskin, 2001; Ma and Zhao, 2015) is solved by a variant of inside-outside algorithm, which is of the same complexity compared with the corresponding inference algorithms. Finally, the gradients can be represented as:   ∂L(θ) X ∂Score(p)   = − p ∈ Tg + m(p) ∂θ ∂θ p where [p ∈ Tg ] is a binary value which indicates whether p is in tree Tg . Traditional models usually utilize linear functions for the Score function, which might need carefully feature engineering such as (Zhao et al., 2009a; Zhao et al., 2009b; Zhao et al., 2009c; Zhao, 2009; Zhao et al., 2013), while we adopt neural models with the probabilistic training criteria unchanged. Training Criteria Labeled Parsing In a dependency tree, each edge can be given a label indicating the type of the dependency relation, this labeling procedure can be integrated directly into the parsing task, instead of a second pass after obtaining the structure. For the probabilistic model, integrating labeled parsing only needs some extensions for the inference procedure and marginal probability calculations. For the simplicity, we only"
P16-1131,P09-1007,1,0.833425,") T 0 ∈T (p) which can be viewed as the mass of all the trees containing the specified factor p. The calculation of m(p) (Paskin, 2001; Ma and Zhao, 2015) is solved by a variant of inside-outside algorithm, which is of the same complexity compared with the corresponding inference algorithms. Finally, the gradients can be represented as:   ∂L(θ) X ∂Score(p)   = − p ∈ Tg + m(p) ∂θ ∂θ p where [p ∈ Tg ] is a binary value which indicates whether p is in tree Tg . Traditional models usually utilize linear functions for the Score function, which might need carefully feature engineering such as (Zhao et al., 2009a; Zhao et al., 2009b; Zhao et al., 2009c; Zhao, 2009; Zhao et al., 2013), while we adopt neural models with the probabilistic training criteria unchanged. Training Criteria Labeled Parsing In a dependency tree, each edge can be given a label indicating the type of the dependency relation, this labeling procedure can be integrated directly into the parsing task, instead of a second pass after obtaining the structure. For the probabilistic model, integrating labeled parsing only needs some extensions for the inference procedure and marginal probability calculations. For the simplicity, we only"
P16-1131,E09-1100,1,0.775042,"es containing the specified factor p. The calculation of m(p) (Paskin, 2001; Ma and Zhao, 2015) is solved by a variant of inside-outside algorithm, which is of the same complexity compared with the corresponding inference algorithms. Finally, the gradients can be represented as:   ∂L(θ) X ∂Score(p)   = − p ∈ Tg + m(p) ∂θ ∂θ p where [p ∈ Tg ] is a binary value which indicates whether p is in tree Tg . Traditional models usually utilize linear functions for the Score function, which might need carefully feature engineering such as (Zhao et al., 2009a; Zhao et al., 2009b; Zhao et al., 2009c; Zhao, 2009; Zhao et al., 2013), while we adopt neural models with the probabilistic training criteria unchanged. Training Criteria Labeled Parsing In a dependency tree, each edge can be given a label indicating the type of the dependency relation, this labeling procedure can be integrated directly into the parsing task, instead of a second pass after obtaining the structure. For the probabilistic model, integrating labeled parsing only needs some extensions for the inference procedure and marginal probability calculations. For the simplicity, we only consider a single label for each factor (even for hig"
P16-1131,P15-1117,0,0.056877,"Missing"
P16-1131,P15-1112,0,0.0219552,"inear models and we are not able to compare with it because of different datasets (they take datasets from CoNLL shared task). Zhang and Zhao (2015) also explore a probabilistic treatment, but its model may give mass to illegal trees or non-trees. Fonseca and Alu´ısio (2015) utilize CNN for scoring edges, though only explore first-order parsing. Its model is based on head selection for each modifier and might be difficult to be extended to high-order parsing. Recently, several neural re-ranking models, like Inside-Outside Recursive Neural Network 1389 (Le and Zuidema, 2014) and Recursive CNN (Zhu et al., 2015), are utilized for capturing features with more contexts. However, re-ranking models depend on the underlying base parsers, which might already miss the correct trees. Generally, the re-ranking techniques play a role of additional enhancement for basic parsing models, and therefore they are not included in our comparisons. The conditional log-likelihood probabilistic criterion utilized in this work is actually a (conditioned) Markov Random Field for tree structures, and it has been applied to parsing since long time ago. Johnson et al. (1999) utilize the Markov Random Fields for stochastic gra"
P17-1093,P13-2013,0,0.0596199,"ipeline manner by first predicting the implicit connective with a language model and determining discourse relation accordingly. Instead of treating implicit connectives as intermediate prediction targets which can suffer from error propagation, we use the connectives to induce highly discriminative features to guide the learning of an implicit network, serving as an adaptive regularization mechanism for en1007 hanced robustness and generalization. Our framework is also end-to-end, avoiding costly feature engineering. Another notable line aims at adapting explicit examples for data synthesis (Biran and McKeown, 2013; Rutherford and Xue, 2015; Braud and Denis, 2015; Ji et al., 2015), multi-task learning (Lan et al., 2013; Liu et al., 2016), and word representation (Braud and Denis, 2016). Our work is orthogonal and complementary to these methods, as we use implicit connectives which have been annotated for implicit examples. 2.2 Adversarial Networks Deep neural networks have gained impressive success in various natural language processing tasks (Wang et al., 2016; Zhang et al., 2016b; Cai et al., 2017), in which adversarial networks have been shown especially effective in deep generative modeling (Goodfel"
P17-1093,D15-1262,0,0.320744,"g process. This paper aims to advance implicit parsing by making use of annotated implicit connectives available in training data. Few recent work has explored such combination. Zhou et al. (2010) developed a two-step approach by first predicting implicit connectives whose sense is then disambiguated to obtain the relation. However, the pipeline approach usually suffers from error propagation, and the method itself has relied on hand-crafted features which do not necessarily generalize well. Other research leveraged explicit connective examples for data augmentation (Rutherford and Xue, 2015; Braud and Denis, 2015; Ji et al., 2015; Braud and Denis, 2016). Our work is orthogonal and complementary to this line. In this paper, we propose a novel neural method that incorporates implicit connectives in a principled adversarial framework. We use deep neural models for relation classification, and take the intuition that, sentence arguments integrated with connectives would enable highly discriminative neural features for accurate relation inference, and an ideal implicit relation classifier, even though without access to connectives, should mimic the connective-augmented reasoning behavior by extracting simi"
P17-1093,D16-1020,0,0.147016,"Missing"
P17-1093,P17-2096,1,0.756078,"y feature engineering. Another notable line aims at adapting explicit examples for data synthesis (Biran and McKeown, 2013; Rutherford and Xue, 2015; Braud and Denis, 2015; Ji et al., 2015), multi-task learning (Lan et al., 2013; Liu et al., 2016), and word representation (Braud and Denis, 2016). Our work is orthogonal and complementary to these methods, as we use implicit connectives which have been annotated for implicit examples. 2.2 Adversarial Networks Deep neural networks have gained impressive success in various natural language processing tasks (Wang et al., 2016; Zhang et al., 2016b; Cai et al., 2017), in which adversarial networks have been shown especially effective in deep generative modeling (Goodfellow et al., 2014) and domain adaptation (Ganin et al., 2016). Generative adversarial nets (Goodfellow et al., 2014) learn to produce realistic samples through competition between a generator and a real/fake discriminator. Professor forcing (Lamb et al., 2016) applies a similar idea to improve long-term generation of a recurrent neural language model. Other approaches (Chen et al., 2016b; Hu et al., 2017; Liang et al., 2017) extend the framework for controllable image/text generation. Li et"
P17-1093,K15-2005,1,0.912243,"Missing"
P17-1093,P16-1163,0,0.603872,"Missing"
P17-1093,D14-1168,0,0.0670168,"riminator. Our method effectively transfers discriminability of connectives to the implicit features, and achieves state-of-the-art performance on the PDTB benchmark. 1 [Arg1]: Never mind. [Arg2]: You already know the answer. [Implicit connective]: Because [Discourse relation]: Cause Introduction Discourse relations connect linguistic units such as clauses and sentences to form coherent semantics. Identification of discourse relations can benefit a variety of downstream applications including question answering (Liakata et al., 2013), machine translation (Li et al., 2014), text summarization (Gerani et al., 2014), opinion spam detection (Chen and Zhao, 2015), and so forth. ∗ Corresponding authors. This paper was partially supported by Cai Yuanpei Program (CSC No. 201304490199 and No. 201304490171), National Natural Science Foundation of China (No. 61170114, No. 61672343 and No. 61272248), National Basic Research Program of China (No. 2013CB329401), Major Basic Research Program of Shanghai Science and Technology Committee (No. 15JC1400103), Art and Science Interdisciplinary Funds of Shanghai Jiao Tong University (No. 14JCRZ04), and Key Project of National Society Science Foundation of China (No. 15ZDA0"
P17-1093,P16-1228,1,0.784768,"extract highly discriminative features by mimicking a connective-augmented network. Our method achieved state-of-the-art performance for implicit discourse relation classification. Besides implicit connective examples, our model can naturally exploit enormous explicit connective data to further improve discourse parsing. The proposed adversarial feature imitation scheme is also generally applicable to other context to incorporate indicative side information available at training time for enhanced inference. Our framework shares a similar spirit of the iterative knowledge distillation method (Hu et al., 2016a,b) which train a “student” network to mimic the classification behavior of a knowledgeinformed “teacher” network. Our approach encourages imitation on the feature level instead of the final prediction level. This allows our approach to apply to regression tasks, and more interestingly, the context in which the student and teacher networks have different prediction outputs, e.g., performing different tasks, while transferring knowledge between each other can be beneficial. Besides, our adversarial mechanism provides an adaptive metric to measure and drive the imitation procedure. 1014 Referen"
P17-1093,D16-1173,1,0.838978,"extract highly discriminative features by mimicking a connective-augmented network. Our method achieved state-of-the-art performance for implicit discourse relation classification. Besides implicit connective examples, our model can naturally exploit enormous explicit connective data to further improve discourse parsing. The proposed adversarial feature imitation scheme is also generally applicable to other context to incorporate indicative side information available at training time for enhanced inference. Our framework shares a similar spirit of the iterative knowledge distillation method (Hu et al., 2016a,b) which train a “student” network to mimic the classification behavior of a knowledgeinformed “teacher” network. Our approach encourages imitation on the feature level instead of the final prediction level. This allows our approach to apply to regression tasks, and more interestingly, the context in which the student and teacher networks have different prediction outputs, e.g., performing different tasks, while transferring knowledge between each other can be beneficial. Besides, our adversarial mechanism provides an adaptive metric to measure and drive the imitation procedure. 1014 Referen"
P17-1093,Q15-1024,0,0.422786,"and future work. 2 2.1 Related Work Implicit Discourse Relation Recognition There has been a surge of interest in implicit discourse parsing since the release of PDTB (Prasad et al., 2008), the first large discourse corpus distinguishing implicit examples from explicit ones. A large set of work has focused on direct classification based on observed sentences, including structured methods with linguistically-informed features (Lin et al., 2009; Pitler et al., 2009; Zhou et al., 2010), end-to-end neural models (Qin et al., 2016b,c; Chen et al., 2016a; Liu and Li, 2016), and combined approaches (Ji and Eisenstein, 2015; Ji et al., 2016). However, the lacking of connective cues makes learning purely from contextual semantics full of challenges. Prior work has attempted to leverage connective information. Zhou et al. (2010) also incorporate implicit connectives, but in a pipeline manner by first predicting the implicit connective with a language model and determining discourse relation accordingly. Instead of treating implicit connectives as intermediate prediction targets which can suffer from error propagation, we use the connectives to induce highly discriminative features to guide the learning of an impli"
P17-1093,N16-1037,0,0.119389,"lated Work Implicit Discourse Relation Recognition There has been a surge of interest in implicit discourse parsing since the release of PDTB (Prasad et al., 2008), the first large discourse corpus distinguishing implicit examples from explicit ones. A large set of work has focused on direct classification based on observed sentences, including structured methods with linguistically-informed features (Lin et al., 2009; Pitler et al., 2009; Zhou et al., 2010), end-to-end neural models (Qin et al., 2016b,c; Chen et al., 2016a; Liu and Li, 2016), and combined approaches (Ji and Eisenstein, 2015; Ji et al., 2016). However, the lacking of connective cues makes learning purely from contextual semantics full of challenges. Prior work has attempted to leverage connective information. Zhou et al. (2010) also incorporate implicit connectives, but in a pipeline manner by first predicting the implicit connective with a language model and determining discourse relation accordingly. Instead of treating implicit connectives as intermediate prediction targets which can suffer from error propagation, we use the connectives to induce highly discriminative features to guide the learning of an implicit network, servi"
P17-1093,D15-1264,0,0.126217,"ims to advance implicit parsing by making use of annotated implicit connectives available in training data. Few recent work has explored such combination. Zhou et al. (2010) developed a two-step approach by first predicting implicit connectives whose sense is then disambiguated to obtain the relation. However, the pipeline approach usually suffers from error propagation, and the method itself has relied on hand-crafted features which do not necessarily generalize well. Other research leveraged explicit connective examples for data augmentation (Rutherford and Xue, 2015; Braud and Denis, 2015; Ji et al., 2015; Braud and Denis, 2016). Our work is orthogonal and complementary to this line. In this paper, we propose a novel neural method that incorporates implicit connectives in a principled adversarial framework. We use deep neural models for relation classification, and take the intuition that, sentence arguments integrated with connectives would enable highly discriminative neural features for accurate relation inference, and an ideal implicit relation classifier, even though without access to connectives, should mimic the connective-augmented reasoning behavior by extracting similarly salient fea"
P17-1093,P13-1047,0,0.04657,"on accordingly. Instead of treating implicit connectives as intermediate prediction targets which can suffer from error propagation, we use the connectives to induce highly discriminative features to guide the learning of an implicit network, serving as an adaptive regularization mechanism for en1007 hanced robustness and generalization. Our framework is also end-to-end, avoiding costly feature engineering. Another notable line aims at adapting explicit examples for data synthesis (Biran and McKeown, 2013; Rutherford and Xue, 2015; Braud and Denis, 2015; Ji et al., 2015), multi-task learning (Lan et al., 2013; Liu et al., 2016), and word representation (Braud and Denis, 2016). Our work is orthogonal and complementary to these methods, as we use implicit connectives which have been annotated for implicit examples. 2.2 Adversarial Networks Deep neural networks have gained impressive success in various natural language processing tasks (Wang et al., 2016; Zhang et al., 2016b; Cai et al., 2017), in which adversarial networks have been shown especially effective in deep generative modeling (Goodfellow et al., 2014) and domain adaptation (Ganin et al., 2016). Generative adversarial nets (Goodfellow et a"
P17-1093,P14-2047,0,0.0324499,"licit network and a rival feature discriminator. Our method effectively transfers discriminability of connectives to the implicit features, and achieves state-of-the-art performance on the PDTB benchmark. 1 [Arg1]: Never mind. [Arg2]: You already know the answer. [Implicit connective]: Because [Discourse relation]: Cause Introduction Discourse relations connect linguistic units such as clauses and sentences to form coherent semantics. Identification of discourse relations can benefit a variety of downstream applications including question answering (Liakata et al., 2013), machine translation (Li et al., 2014), text summarization (Gerani et al., 2014), opinion spam detection (Chen and Zhao, 2015), and so forth. ∗ Corresponding authors. This paper was partially supported by Cai Yuanpei Program (CSC No. 201304490199 and No. 201304490171), National Natural Science Foundation of China (No. 61170114, No. 61672343 and No. 61272248), National Basic Research Program of China (No. 2013CB329401), Major Basic Research Program of Shanghai Science and Technology Committee (No. 15JC1400103), Art and Science Interdisciplinary Funds of Shanghai Jiao Tong University (No. 14JCRZ04), and Key Project of National Socie"
P17-1093,K16-2008,1,0.907018,"Missing"
P17-1093,D13-1070,0,0.0212508,"n scheme through competition between the implicit network and a rival feature discriminator. Our method effectively transfers discriminability of connectives to the implicit features, and achieves state-of-the-art performance on the PDTB benchmark. 1 [Arg1]: Never mind. [Arg2]: You already know the answer. [Implicit connective]: Because [Discourse relation]: Cause Introduction Discourse relations connect linguistic units such as clauses and sentences to form coherent semantics. Identification of discourse relations can benefit a variety of downstream applications including question answering (Liakata et al., 2013), machine translation (Li et al., 2014), text summarization (Gerani et al., 2014), opinion spam detection (Chen and Zhao, 2015), and so forth. ∗ Corresponding authors. This paper was partially supported by Cai Yuanpei Program (CSC No. 201304490199 and No. 201304490171), National Natural Science Foundation of China (No. 61170114, No. 61672343 and No. 61272248), National Basic Research Program of China (No. 2013CB329401), Major Basic Research Program of Shanghai Science and Technology Committee (No. 15JC1400103), Art and Science Interdisciplinary Funds of Shanghai Jiao Tong University (No. 14JCR"
P17-1093,D09-1036,0,0.784133,"Missing"
P17-1093,D16-1130,0,0.531433,"ous methods. Section 5 discusses extensions and future work. 2 2.1 Related Work Implicit Discourse Relation Recognition There has been a surge of interest in implicit discourse parsing since the release of PDTB (Prasad et al., 2008), the first large discourse corpus distinguishing implicit examples from explicit ones. A large set of work has focused on direct classification based on observed sentences, including structured methods with linguistically-informed features (Lin et al., 2009; Pitler et al., 2009; Zhou et al., 2010), end-to-end neural models (Qin et al., 2016b,c; Chen et al., 2016a; Liu and Li, 2016), and combined approaches (Ji and Eisenstein, 2015; Ji et al., 2016). However, the lacking of connective cues makes learning purely from contextual semantics full of challenges. Prior work has attempted to leverage connective information. Zhou et al. (2010) also incorporate implicit connectives, but in a pipeline manner by first predicting the implicit connective with a language model and determining discourse relation accordingly. Instead of treating implicit connectives as intermediate prediction targets which can suffer from error propagation, we use the connectives to induce highly discrim"
P17-1093,P09-1077,0,0.834033,"201304490171), National Natural Science Foundation of China (No. 61170114, No. 61672343 and No. 61272248), National Basic Research Program of China (No. 2013CB329401), Major Basic Research Program of Shanghai Science and Technology Committee (No. 15JC1400103), Art and Science Interdisciplinary Funds of Shanghai Jiao Tong University (No. 14JCRZ04), and Key Project of National Society Science Foundation of China (No. 15ZDA041). Various attempts have been made to directly infer underlying relations by modeling the semantics of the arguments, ranging from feature-based methods (Lin et al., 2009; Pitler et al., 2009) to the very recent end-to-end neural models (Chen et al., 2016a; Qin et al., 2016c). Despite impressive performance, the absence of strong explicit connective cues has made the inference extremely hard and hindered further improvement. In fact, even the human annotators would make use of connectives to aid relation annotation. For instance, the popular Penn Discourse Treebank (PDTB) benchmark data (Prasad et al., 2008) was annotated by first inserting a connective expression (i.e., implicit connective, as shown in the above example) manually, and determining the abstract relation by combining"
P17-1093,prasad-etal-2008-penn,0,0.131206,"eatly improves over standalone neural models and previous bestperforming approaches. We also demonstrate that our implicit recognition network successfully imitates and extracts crucial hidden representations. We begin by briefly reviewing related work in section 2. Section 3 presents the proposed adversarial model. Section 4 shows substantially improved experimental results over previous methods. Section 5 discusses extensions and future work. 2 2.1 Related Work Implicit Discourse Relation Recognition There has been a surge of interest in implicit discourse parsing since the release of PDTB (Prasad et al., 2008), the first large discourse corpus distinguishing implicit examples from explicit ones. A large set of work has focused on direct classification based on observed sentences, including structured methods with linguistically-informed features (Lin et al., 2009; Pitler et al., 2009; Zhou et al., 2010), end-to-end neural models (Qin et al., 2016b,c; Chen et al., 2016a; Liu and Li, 2016), and combined approaches (Ji and Eisenstein, 2015; Ji et al., 2016). However, the lacking of connective cues makes learning purely from contextual semantics full of challenges. Prior work has attempted to leverage"
P17-1093,C16-1180,1,0.833631,"Missing"
P17-1093,K16-2010,1,0.883387,"43 and No. 61272248), National Basic Research Program of China (No. 2013CB329401), Major Basic Research Program of Shanghai Science and Technology Committee (No. 15JC1400103), Art and Science Interdisciplinary Funds of Shanghai Jiao Tong University (No. 14JCRZ04), and Key Project of National Society Science Foundation of China (No. 15ZDA041). Various attempts have been made to directly infer underlying relations by modeling the semantics of the arguments, ranging from feature-based methods (Lin et al., 2009; Pitler et al., 2009) to the very recent end-to-end neural models (Chen et al., 2016a; Qin et al., 2016c). Despite impressive performance, the absence of strong explicit connective cues has made the inference extremely hard and hindered further improvement. In fact, even the human annotators would make use of connectives to aid relation annotation. For instance, the popular Penn Discourse Treebank (PDTB) benchmark data (Prasad et al., 2008) was annotated by first inserting a connective expression (i.e., implicit connective, as shown in the above example) manually, and determining the abstract relation by combining both the implicit connective and contextual semantics. 1006 Proceedings of the 55"
P17-1093,D16-1246,1,0.897429,"43 and No. 61272248), National Basic Research Program of China (No. 2013CB329401), Major Basic Research Program of Shanghai Science and Technology Committee (No. 15JC1400103), Art and Science Interdisciplinary Funds of Shanghai Jiao Tong University (No. 14JCRZ04), and Key Project of National Society Science Foundation of China (No. 15ZDA041). Various attempts have been made to directly infer underlying relations by modeling the semantics of the arguments, ranging from feature-based methods (Lin et al., 2009; Pitler et al., 2009) to the very recent end-to-end neural models (Chen et al., 2016a; Qin et al., 2016c). Despite impressive performance, the absence of strong explicit connective cues has made the inference extremely hard and hindered further improvement. In fact, even the human annotators would make use of connectives to aid relation annotation. For instance, the popular Penn Discourse Treebank (PDTB) benchmark data (Prasad et al., 2008) was annotated by first inserting a connective expression (i.e., implicit connective, as shown in the above example) manually, and determining the abstract relation by combining both the implicit connective and contextual semantics. 1006 Proceedings of the 55"
P17-1093,N15-1081,0,0.19495,"tion to guide the reasoning process. This paper aims to advance implicit parsing by making use of annotated implicit connectives available in training data. Few recent work has explored such combination. Zhou et al. (2010) developed a two-step approach by first predicting implicit connectives whose sense is then disambiguated to obtain the relation. However, the pipeline approach usually suffers from error propagation, and the method itself has relied on hand-crafted features which do not necessarily generalize well. Other research leveraged explicit connective examples for data augmentation (Rutherford and Xue, 2015; Braud and Denis, 2015; Ji et al., 2015; Braud and Denis, 2016). Our work is orthogonal and complementary to this line. In this paper, we propose a novel neural method that incorporates implicit connectives in a principled adversarial framework. We use deep neural models for relation classification, and take the intuition that, sentence arguments integrated with connectives would enable highly discriminative neural features for accurate relation inference, and an ideal implicit relation classifier, even though without access to connectives, should mimic the connective-augmented reasoning beha"
P17-1093,N16-1064,1,0.80589,"ework is also end-to-end, avoiding costly feature engineering. Another notable line aims at adapting explicit examples for data synthesis (Biran and McKeown, 2013; Rutherford and Xue, 2015; Braud and Denis, 2015; Ji et al., 2015), multi-task learning (Lan et al., 2013; Liu et al., 2016), and word representation (Braud and Denis, 2016). Our work is orthogonal and complementary to these methods, as we use implicit connectives which have been annotated for implicit examples. 2.2 Adversarial Networks Deep neural networks have gained impressive success in various natural language processing tasks (Wang et al., 2016; Zhang et al., 2016b; Cai et al., 2017), in which adversarial networks have been shown especially effective in deep generative modeling (Goodfellow et al., 2014) and domain adaptation (Ganin et al., 2016). Generative adversarial nets (Goodfellow et al., 2014) learn to produce realistic samples through competition between a generator and a real/fake discriminator. Professor forcing (Lamb et al., 2016) applies a similar idea to improve long-term generation of a recurrent neural language model. Other approaches (Chen et al., 2016b; Hu et al., 2017; Liang et al., 2017) extend the framework for co"
P17-1093,D16-1037,0,0.030373,"o-end, avoiding costly feature engineering. Another notable line aims at adapting explicit examples for data synthesis (Biran and McKeown, 2013; Rutherford and Xue, 2015; Braud and Denis, 2015; Ji et al., 2015), multi-task learning (Lan et al., 2013; Liu et al., 2016), and word representation (Braud and Denis, 2016). Our work is orthogonal and complementary to these methods, as we use implicit connectives which have been annotated for implicit examples. 2.2 Adversarial Networks Deep neural networks have gained impressive success in various natural language processing tasks (Wang et al., 2016; Zhang et al., 2016b; Cai et al., 2017), in which adversarial networks have been shown especially effective in deep generative modeling (Goodfellow et al., 2014) and domain adaptation (Ganin et al., 2016). Generative adversarial nets (Goodfellow et al., 2014) learn to produce realistic samples through competition between a generator and a real/fake discriminator. Professor forcing (Lamb et al., 2016) applies a similar idea to improve long-term generation of a recurrent neural language model. Other approaches (Chen et al., 2016b; Hu et al., 2017; Liang et al., 2017) extend the framework for controllable image/tex"
P17-1093,K15-2001,0,0.0848951,"prior work of implicit discourse relation classification, we evaluate on two popular experimental settings: 1) multi-class classification for 2nd-level types (Lin et al., 2009; Ji and Eisenstein, 2015), and 2) oneversus-others binary classifications for 1st-level classes (Pitler et al., 2009). We describe the detailed configurations in the following respective sections. We will focus our analysis on the multiclass classification setting, which is most realistic in practice and serves as a building block for a complete discourse parser such as that for the shared tasks of CoNLL-2015 and 2016 (Xue et al., 2015, 2016). Model Training Here we provide the detailed architecture configurations of each component we used in the experiments. Discriminator The discriminator is a binary classifier to identify the correct source of an input feature vector. To make it a strong rival to the feature imitating network (i-CNN), we model the discriminator as a multi-layer perceptron (MLP) enhanced with gated mechanism for efficient information flow (Srivastava et al., 2015; Qin et al., 2016c), as shown in Figure 3. 4 Experiment Setup • Throughout the experiments i-CNN and aCNN contain 3 sets of convolutional filter"
P17-1093,P16-1131,1,0.805493,"Missing"
P17-1093,K16-2001,0,0.225587,"Missing"
P17-1093,C10-2172,0,0.715684,"tion network is then trained to correctly classify relations and simultaneously to fool the discriminator, resulting in an adversarial framework. The adversarial mechanism has been an emerging method in different context, especially for image generation (Goodfellow et al., 2014) and domain adaptation (Ganin et al., 2016; Chen et al., 2016c). Our adversarial framework is unique to address neural feature emulation between two models. Besides, to the best of our knowledge, this is the first adversarial approach in the context of discourse parsing. Compared to previous connective exploiting work (Zhou et al., 2010; Xu et al., 2012), our method provides a new integration paradigm and an end-to-end procedure that avoids inefficient feature engineering and error propagation. Our method is evaluated on the PDTB 2.0 benchmark in a variety of experimental settings. The proposed adversarial model greatly improves over standalone neural models and previous bestperforming approaches. We also demonstrate that our implicit recognition network successfully imitates and extracts crucial hidden representations. We begin by briefly reviewing related work in section 2. Section 3 presents the proposed adversarial model"
P17-2096,W06-1655,0,0.010766,"ast it as a characterbased tagging problem. Peng et al. (2004) showed CRF based model is particularly effective to solve CWS in the sequence labeling fashion. This method has been followed by most later segmenters (Tseng et al., 2005; Zhao et al., 2006; Zhao and Kit, 2008c; Zhao et al., 2010; Sun et al., 2012; Zhang et al., 2013). The same spirit has also be followed by most neural models (Zheng et al., 2013; Pei et al., 2014; Qi et al., 2014; Chen et al., 2015a,b; Ma and Hinrichs, 2015; Xu and Sun, 2016). Word based CWS to conveniently incorporate complete word features has also be explored. Andrew (2006) proposed a semi-CRF model. Zhang and Clark (2007, 2011) used a perceptron algorithm with inexact search. Both of them have been followed by neural model versions (Liu et al., 2016) and (Zhang et al., 2016) respectively. There are also works integrating both character-based and word-based segmenters (Huang and Zhao, 2006; Sun, 2010; Wang et al., 2014) and semisupervised learning (Zhao and Kit, 2008b, 2011; Zeng et al., 2013; Zhang et al., 2013). Unlike most previous works, which extract features within a fixed sized sliding window, Cai and Zhao (2016) proposed a direct segmentation framework t"
P17-2096,Y06-1001,1,0.727752,"ng et al., 2013). The same spirit has also be followed by most neural models (Zheng et al., 2013; Pei et al., 2014; Qi et al., 2014; Chen et al., 2015a,b; Ma and Hinrichs, 2015; Xu and Sun, 2016). Word based CWS to conveniently incorporate complete word features has also be explored. Andrew (2006) proposed a semi-CRF model. Zhang and Clark (2007, 2011) used a perceptron algorithm with inexact search. Both of them have been followed by neural model versions (Liu et al., 2016) and (Zhang et al., 2016) respectively. There are also works integrating both character-based and word-based segmenters (Huang and Zhao, 2006; Sun, 2010; Wang et al., 2014) and semisupervised learning (Zhao and Kit, 2008b, 2011; Zeng et al., 2013; Zhang et al., 2013). Unlike most previous works, which extract features within a fixed sized sliding window, Cai and Zhao (2016) proposed a direct segmentation framework that extends the feature window to cover complete input and segmentation history and uses beam search for decoding. In this work, we will make a series of significant improvement over the basic framework and especially adopt greedy search instead. Another notable exception of embedding based methods is (Ma and Hinrichs, 2"
P17-2096,P16-1039,1,0.759203,"en et al. (2015b) used a Long shortterm memory network (LSTM) (Hochreiter and Schmidhuber, 1997) to capture long-distance context. Xu and Sun (2016) integrated both GRNN and LSTM for deeper feature extraction. Besides sequence labeling schemes, Zhang et al. (2016) proposed a transition-based framework. Liu et al. (2016) used a zero-order semiCRF based model. However, these two models rely on either traditional discrete features or nonneural-network components for performance enhancement, their performance drops rapidly when solely depending on neural models. Most closely related to this work, Cai and Zhao (2016) proposed to score candidate segmented outputs directly, employing a gated combination neural network over characters for word representation generation and an LSTM scoring model for segmentation result evaluation. Despite the active progress of most existing works in terms of accuracy, their computational needs have been significantly increased to the extent that training a neural segmenter usually takes days even using cutting-edge hardwares. Meanwhile, different applications often require diverse segmenters and offer large-scale incoming data. The efficiency of a word segmenter either for t"
P17-2096,P15-1168,0,0.363545,"isty, Shanghai, China thisisjcykcd@gmail.com, {zhaohai@cs, zzs2011@}sjtu.edu.cn 3 Tencent Youtu Lab, Shanghai, China {macxin,littlekenwu,garyhuang}@tencent.com Abstract To minimize the efforts in feature engineering, neural word segmentation has been actively studied recently. Zheng et al. (2013) first adapted the sliding-window based sequence labeling (Collobert et al., 2011) with character embeddings as input. A number of other researchers have attempted to improve the segmenter of (Zheng et al., 2013) by augmenting it with additional complexity. Pei et al. (2014) introduced tag embeddings. Chen et al. (2015a) proposed to model ngram features via a gated recursive neural network (GRNN). Chen et al. (2015b) used a Long shortterm memory network (LSTM) (Hochreiter and Schmidhuber, 1997) to capture long-distance context. Xu and Sun (2016) integrated both GRNN and LSTM for deeper feature extraction. Besides sequence labeling schemes, Zhang et al. (2016) proposed a transition-based framework. Liu et al. (2016) used a zero-order semiCRF based model. However, these two models rely on either traditional discrete features or nonneural-network components for performance enhancement, their performance drops"
P17-2096,D15-1141,0,0.364738,"Missing"
P17-2096,P04-1015,0,0.0153534,"an update gate z (As in Figure 2), which has been shown helpless to the performance but requires huge computational cost according to our empirical study. 3.3 Training Criteria Our segmenter is trained using max-margin methods (Taskar et al., 2005) where the structured margin loss is defined as µ times the number of incorrectly segmented characters (Cai and Zhao, 2016). However, according to (Huang et al., 2012), standard parameter update cannot guarantee convergence in the case of inexact search. We thus additionally examine two strategies as follows. Early update This strategy proposed in (Collins and Roark, 2004) can be simplified into “update once the golden answer is unreachable”. In our case, when the considering character prefix can be correctly segmented but the correct one falls off the beam, an update operation will be conducted and the rest part will be ignored. LaSO update One drawback of early update is that the search may never reach the end of a training instance, which means the rest part of the instance is “wasted”. Differently, LaSO method of (Daum´e III and Marcu, 2005) continues on the same instance with correct hypothesis after each update. In our case, the beam will be emptied and t"
P17-2096,I05-3025,0,0.0302022,"er is truly end-toend, capable of performing segmentation much faster and even more accurate than state-of-the-art neural models on Chinese benchmark datasets. 1 Introduction Word segmentation is a fundamental task for processing most east Asian languages, typically Chinese. Almost all practical Chinese processing applications essentially rely on Chinese word segmentation (CWS), e.g., (Zhao et al., 2017). Since (Xue, 2003), most methods formalize this task as a sequence labeling problem. In a supervised learning fashion, sequence labeling may adopt various models such as Maximum Entropy (ME) (Low et al., 2005) and Conditional Random Fields (CRF) (Lafferty et al., 2001; Peng et al., 2004). However, these models rely heavily on hand-crafted features. ∗ Corresponding author. This paper was partially supported by Cai Yuanpei Program (CSC No. 201304490199 and No. 201304490171), National Natural Science Foundation of China (No. 61170114, No. 61672343 and No. 61272248), National Basic Research Program of China (No. 2013CB329401), Major Basic Research Program of Shanghai Science and Technology Committee (No. 15JC1400103), Art and Science Interdisciplinary Funds of Shanghai Jiao Tong University (No. 14JCRZ0"
P17-2096,P15-1167,0,0.145241,"ted Work Statistical Chinese word segmentation has been studied for decades (Huang and Zhao, 2007). (Xue, 2003) was the first to cast it as a characterbased tagging problem. Peng et al. (2004) showed CRF based model is particularly effective to solve CWS in the sequence labeling fashion. This method has been followed by most later segmenters (Tseng et al., 2005; Zhao et al., 2006; Zhao and Kit, 2008c; Zhao et al., 2010; Sun et al., 2012; Zhang et al., 2013). The same spirit has also be followed by most neural models (Zheng et al., 2013; Pei et al., 2014; Qi et al., 2014; Chen et al., 2015a,b; Ma and Hinrichs, 2015; Xu and Sun, 2016). Word based CWS to conveniently incorporate complete word features has also be explored. Andrew (2006) proposed a semi-CRF model. Zhang and Clark (2007, 2011) used a perceptron algorithm with inexact search. Both of them have been followed by neural model versions (Liu et al., 2016) and (Zhang et al., 2016) respectively. There are also works integrating both character-based and word-based segmenters (Huang and Zhao, 2006; Sun, 2010; Wang et al., 2014) and semisupervised learning (Zhao and Kit, 2008b, 2011; Zeng et al., 2013; Zhang et al., 2013). Unlike most previous works,"
P17-2096,P15-1033,0,0.00699845,"thods Standard Early update LaSO update Experiments F1 95.6 95.8 95.7 PKU #epochs 50 30 45 F1 96.7 97.1 97.0 MSR #epochs 50 30 30 Table 3: The effect of different update methods. #epochs denotes the number of training epochs to convergence. Datasets and Settings We conduct experiments on two popular benchmark datasets, namely PKU and MSR, from the second international Chinese word segmentation bakeoff (Emerson, 2005) (Bakeoff-2005). Data statistics are in Table 1. Throughout this paper, we use the same model setting as shown in Table 2. These numbers are tuned on development sets.3 We follow (Dyer et al., 2015) to train model parameters. The learning rate at epoch t is set as ηt = 0.2/(1 + γt), where γ = 0.1 for PKU dataset and γ = 0.2 for MSR dataset. The character embeddings are either randomly initialized or pre-trained by word2vec (Mikolov et al., 2013) toolkit on Chinese Wikipedia corpus (which will be indicated by +pre-train in tables.), while the word embeddings are always randomly initialized. The beam size is kept the same during training and working. By default, early update strategy is adopted and the word table H is top half of in-vocabulary (IV) words by frequency. 4.2 96 95 Table 2: Mo"
P17-2096,I05-3017,0,0.0428083,"bedding size Hidden unit number Margin loss discount Maximum word length 4.1 dc = 50 dw = 50 H = 50 µ = 0.2 w=4 PKU MSR 1 2 3 4 5 6 7 8 beam size Figure 3: The effect of different beam sizes. Methods Standard Early update LaSO update Experiments F1 95.6 95.8 95.7 PKU #epochs 50 30 45 F1 96.7 97.1 97.0 MSR #epochs 50 30 30 Table 3: The effect of different update methods. #epochs denotes the number of training epochs to convergence. Datasets and Settings We conduct experiments on two popular benchmark datasets, namely PKU and MSR, from the second international Chinese word segmentation bakeoff (Emerson, 2005) (Bakeoff-2005). Data statistics are in Table 1. Throughout this paper, we use the same model setting as shown in Table 2. These numbers are tuned on development sets.3 We follow (Dyer et al., 2015) to train model parameters. The learning rate at epoch t is set as ηt = 0.2/(1 + γt), where γ = 0.1 for PKU dataset and γ = 0.2 for MSR dataset. The character embeddings are either randomly initialized or pre-trained by word2vec (Mikolov et al., 2013) toolkit on Chinese Wikipedia corpus (which will be indicated by +pre-train in tables.), while the word embeddings are always randomly initialized. The"
P17-2096,O03-4002,0,0.889801,"s are computationally inefficient. This paper presents a greedy neural word segmenter with balanced word and character embedding inputs to alleviate the existing drawbacks. Our segmenter is truly end-toend, capable of performing segmentation much faster and even more accurate than state-of-the-art neural models on Chinese benchmark datasets. 1 Introduction Word segmentation is a fundamental task for processing most east Asian languages, typically Chinese. Almost all practical Chinese processing applications essentially rely on Chinese word segmentation (CWS), e.g., (Zhao et al., 2017). Since (Xue, 2003), most methods formalize this task as a sequence labeling problem. In a supervised learning fashion, sequence labeling may adopt various models such as Maximum Entropy (ME) (Low et al., 2005) and Conditional Random Fields (CRF) (Lafferty et al., 2001; Peng et al., 2004). However, these models rely heavily on hand-crafted features. ∗ Corresponding author. This paper was partially supported by Cai Yuanpei Program (CSC No. 201304490199 and No. 201304490171), National Natural Science Foundation of China (No. 61170114, No. 61672343 and No. 61272248), National Basic Research Program of China (No. 20"
P17-2096,P14-1028,0,0.260099,"nitive Engineering, Shanghai Jiao Tong Univeristy, Shanghai, China thisisjcykcd@gmail.com, {zhaohai@cs, zzs2011@}sjtu.edu.cn 3 Tencent Youtu Lab, Shanghai, China {macxin,littlekenwu,garyhuang}@tencent.com Abstract To minimize the efforts in feature engineering, neural word segmentation has been actively studied recently. Zheng et al. (2013) first adapted the sliding-window based sequence labeling (Collobert et al., 2011) with character embeddings as input. A number of other researchers have attempted to improve the segmenter of (Zheng et al., 2013) by augmenting it with additional complexity. Pei et al. (2014) introduced tag embeddings. Chen et al. (2015a) proposed to model ngram features via a gated recursive neural network (GRNN). Chen et al. (2015b) used a Long shortterm memory network (LSTM) (Hochreiter and Schmidhuber, 1997) to capture long-distance context. Xu and Sun (2016) integrated both GRNN and LSTM for deeper feature extraction. Besides sequence labeling schemes, Zhang et al. (2016) proposed a transition-based framework. Liu et al. (2016) used a zero-order semiCRF based model. However, these two models rely on either traditional discrete features or nonneural-network components for perf"
P17-2096,P13-1076,0,0.022877,", 2014; Qi et al., 2014; Chen et al., 2015a,b; Ma and Hinrichs, 2015; Xu and Sun, 2016). Word based CWS to conveniently incorporate complete word features has also be explored. Andrew (2006) proposed a semi-CRF model. Zhang and Clark (2007, 2011) used a perceptron algorithm with inexact search. Both of them have been followed by neural model versions (Liu et al., 2016) and (Zhang et al., 2016) respectively. There are also works integrating both character-based and word-based segmenters (Huang and Zhao, 2006; Sun, 2010; Wang et al., 2014) and semisupervised learning (Zhao and Kit, 2008b, 2011; Zeng et al., 2013; Zhang et al., 2013). Unlike most previous works, which extract features within a fixed sized sliding window, Cai and Zhao (2016) proposed a direct segmentation framework that extends the feature window to cover complete input and segmentation history and uses beam search for decoding. In this work, we will make a series of significant improvement over the basic framework and especially adopt greedy search instead. Another notable exception of embedding based methods is (Ma and Hinrichs, 2015), which used character-specified tags matching for fast decoding and resulted in a character-based gr"
P17-2096,C04-1081,0,0.926951,"more accurate than state-of-the-art neural models on Chinese benchmark datasets. 1 Introduction Word segmentation is a fundamental task for processing most east Asian languages, typically Chinese. Almost all practical Chinese processing applications essentially rely on Chinese word segmentation (CWS), e.g., (Zhao et al., 2017). Since (Xue, 2003), most methods formalize this task as a sequence labeling problem. In a supervised learning fashion, sequence labeling may adopt various models such as Maximum Entropy (ME) (Low et al., 2005) and Conditional Random Fields (CRF) (Lafferty et al., 2001; Peng et al., 2004). However, these models rely heavily on hand-crafted features. ∗ Corresponding author. This paper was partially supported by Cai Yuanpei Program (CSC No. 201304490199 and No. 201304490171), National Natural Science Foundation of China (No. 61170114, No. 61672343 and No. 61272248), National Basic Research Program of China (No. 2013CB329401), Major Basic Research Program of Shanghai Science and Technology Committee (No. 15JC1400103), Art and Science Interdisciplinary Funds of Shanghai Jiao Tong University (No. 14JCRZ04), Key Project of National Society Science Foundation of China (No. 15-ZDA041)"
P17-2096,D13-1031,0,0.031264,"king to overcome the existing efficiency obstacle. Our evaluation will be performed on Chinese benchmark datasets. 2 hi wi!1 wi LSTM layer Word embedding Related Work Statistical Chinese word segmentation has been studied for decades (Huang and Zhao, 2007). (Xue, 2003) was the first to cast it as a characterbased tagging problem. Peng et al. (2004) showed CRF based model is particularly effective to solve CWS in the sequence labeling fashion. This method has been followed by most later segmenters (Tseng et al., 2005; Zhao et al., 2006; Zhao and Kit, 2008c; Zhao et al., 2010; Sun et al., 2012; Zhang et al., 2013). The same spirit has also be followed by most neural models (Zheng et al., 2013; Pei et al., 2014; Qi et al., 2014; Chen et al., 2015a,b; Ma and Hinrichs, 2015; Xu and Sun, 2016). Word based CWS to conveniently incorporate complete word features has also be explored. Andrew (2006) proposed a semi-CRF model. Zhang and Clark (2007, 2011) used a perceptron algorithm with inexact search. Both of them have been followed by neural model versions (Liu et al., 2016) and (Zhang et al., 2016) respectively. There are also works integrating both character-based and word-based segmenters (Huang and Zhao,"
P17-2096,C10-2139,0,0.0102546,"same spirit has also be followed by most neural models (Zheng et al., 2013; Pei et al., 2014; Qi et al., 2014; Chen et al., 2015a,b; Ma and Hinrichs, 2015; Xu and Sun, 2016). Word based CWS to conveniently incorporate complete word features has also be explored. Andrew (2006) proposed a semi-CRF model. Zhang and Clark (2007, 2011) used a perceptron algorithm with inexact search. Both of them have been followed by neural model versions (Liu et al., 2016) and (Zhang et al., 2016) respectively. There are also works integrating both character-based and word-based segmenters (Huang and Zhao, 2006; Sun, 2010; Wang et al., 2014) and semisupervised learning (Zhao and Kit, 2008b, 2011; Zeng et al., 2013; Zhang et al., 2013). Unlike most previous works, which extract features within a fixed sized sliding window, Cai and Zhao (2016) proposed a direct segmentation framework that extends the feature window to cover complete input and segmentation history and uses beam search for decoding. In this work, we will make a series of significant improvement over the basic framework and especially adopt greedy search instead. Another notable exception of embedding based methods is (Ma and Hinrichs, 2015), which"
P17-2096,P16-1040,0,0.339491,"labeling (Collobert et al., 2011) with character embeddings as input. A number of other researchers have attempted to improve the segmenter of (Zheng et al., 2013) by augmenting it with additional complexity. Pei et al. (2014) introduced tag embeddings. Chen et al. (2015a) proposed to model ngram features via a gated recursive neural network (GRNN). Chen et al. (2015b) used a Long shortterm memory network (LSTM) (Hochreiter and Schmidhuber, 1997) to capture long-distance context. Xu and Sun (2016) integrated both GRNN and LSTM for deeper feature extraction. Besides sequence labeling schemes, Zhang et al. (2016) proposed a transition-based framework. Liu et al. (2016) used a zero-order semiCRF based model. However, these two models rely on either traditional discrete features or nonneural-network components for performance enhancement, their performance drops rapidly when solely depending on neural models. Most closely related to this work, Cai and Zhao (2016) proposed to score candidate segmented outputs directly, employing a gated combination neural network over characters for word representation generation and an LSTM scoring model for segmentation result evaluation. Despite the active progress of"
P17-2096,P12-1027,0,0.0323914,"h training and working to overcome the existing efficiency obstacle. Our evaluation will be performed on Chinese benchmark datasets. 2 hi wi!1 wi LSTM layer Word embedding Related Work Statistical Chinese word segmentation has been studied for decades (Huang and Zhao, 2007). (Xue, 2003) was the first to cast it as a characterbased tagging problem. Peng et al. (2004) showed CRF based model is particularly effective to solve CWS in the sequence labeling fashion. This method has been followed by most later segmenters (Tseng et al., 2005; Zhao et al., 2006; Zhao and Kit, 2008c; Zhao et al., 2010; Sun et al., 2012; Zhang et al., 2013). The same spirit has also be followed by most neural models (Zheng et al., 2013; Pei et al., 2014; Qi et al., 2014; Chen et al., 2015a,b; Ma and Hinrichs, 2015; Xu and Sun, 2016). Word based CWS to conveniently incorporate complete word features has also be explored. Andrew (2006) proposed a semi-CRF model. Zhang and Clark (2007, 2011) used a perceptron algorithm with inexact search. Both of them have been followed by neural model versions (Liu et al., 2016) and (Zhang et al., 2016) respectively. There are also works integrating both character-based and word-based segment"
P17-2096,P07-1106,0,0.0191233,"m. Peng et al. (2004) showed CRF based model is particularly effective to solve CWS in the sequence labeling fashion. This method has been followed by most later segmenters (Tseng et al., 2005; Zhao et al., 2006; Zhao and Kit, 2008c; Zhao et al., 2010; Sun et al., 2012; Zhang et al., 2013). The same spirit has also be followed by most neural models (Zheng et al., 2013; Pei et al., 2014; Qi et al., 2014; Chen et al., 2015a,b; Ma and Hinrichs, 2015; Xu and Sun, 2016). Word based CWS to conveniently incorporate complete word features has also be explored. Andrew (2006) proposed a semi-CRF model. Zhang and Clark (2007, 2011) used a perceptron algorithm with inexact search. Both of them have been followed by neural model versions (Liu et al., 2016) and (Zhang et al., 2016) respectively. There are also works integrating both character-based and word-based segmenters (Huang and Zhao, 2006; Sun, 2010; Wang et al., 2014) and semisupervised learning (Zhao and Kit, 2008b, 2011; Zeng et al., 2013; Zhang et al., 2013). Unlike most previous works, which extract features within a fixed sized sliding window, Cai and Zhao (2016) proposed a direct segmentation framework that extends the feature window to cover complete"
P17-2096,J11-1005,0,0.014803,"Missing"
P17-2096,Y06-1012,1,0.63945,"17-2096 ral word segmenter who searches greedily during both training and working to overcome the existing efficiency obstacle. Our evaluation will be performed on Chinese benchmark datasets. 2 hi wi!1 wi LSTM layer Word embedding Related Work Statistical Chinese word segmentation has been studied for decades (Huang and Zhao, 2007). (Xue, 2003) was the first to cast it as a characterbased tagging problem. Peng et al. (2004) showed CRF based model is particularly effective to solve CWS in the sequence labeling fashion. This method has been followed by most later segmenters (Tseng et al., 2005; Zhao et al., 2006; Zhao and Kit, 2008c; Zhao et al., 2010; Sun et al., 2012; Zhang et al., 2013). The same spirit has also be followed by most neural models (Zheng et al., 2013; Pei et al., 2014; Qi et al., 2014; Chen et al., 2015a,b; Ma and Hinrichs, 2015; Xu and Sun, 2016). Word based CWS to conveniently incorporate complete word features has also be explored. Andrew (2006) proposed a semi-CRF model. Zhang and Clark (2007, 2011) used a perceptron algorithm with inexact search. Both of them have been followed by neural model versions (Liu et al., 2016) and (Zhang et al., 2016) respectively. There are also wor"
P17-2096,I05-3027,0,0.241929,"oi.org/10.18653/v1/P17-2096 ral word segmenter who searches greedily during both training and working to overcome the existing efficiency obstacle. Our evaluation will be performed on Chinese benchmark datasets. 2 hi wi!1 wi LSTM layer Word embedding Related Work Statistical Chinese word segmentation has been studied for decades (Huang and Zhao, 2007). (Xue, 2003) was the first to cast it as a characterbased tagging problem. Peng et al. (2004) showed CRF based model is particularly effective to solve CWS in the sequence labeling fashion. This method has been followed by most later segmenters (Tseng et al., 2005; Zhao et al., 2006; Zhao and Kit, 2008c; Zhao et al., 2010; Sun et al., 2012; Zhang et al., 2013). The same spirit has also be followed by most neural models (Zheng et al., 2013; Pei et al., 2014; Qi et al., 2014; Chen et al., 2015a,b; Ma and Hinrichs, 2015; Xu and Sun, 2016). Word based CWS to conveniently incorporate complete word features has also be explored. Andrew (2006) proposed a semi-CRF model. Zhang and Clark (2007, 2011) used a perceptron algorithm with inexact search. Both of them have been followed by neural model versions (Liu et al., 2016) and (Zhang et al., 2016) respectively."
P17-2096,P14-2032,0,0.0185643,"has also be followed by most neural models (Zheng et al., 2013; Pei et al., 2014; Qi et al., 2014; Chen et al., 2015a,b; Ma and Hinrichs, 2015; Xu and Sun, 2016). Word based CWS to conveniently incorporate complete word features has also be explored. Andrew (2006) proposed a semi-CRF model. Zhang and Clark (2007, 2011) used a perceptron algorithm with inexact search. Both of them have been followed by neural model versions (Liu et al., 2016) and (Zhang et al., 2016) respectively. There are also works integrating both character-based and word-based segmenters (Huang and Zhao, 2006; Sun, 2010; Wang et al., 2014) and semisupervised learning (Zhao and Kit, 2008b, 2011; Zeng et al., 2013; Zhang et al., 2013). Unlike most previous works, which extract features within a fixed sized sliding window, Cai and Zhao (2016) proposed a direct segmentation framework that extends the feature window to cover complete input and segmentation history and uses beam search for decoding. In this work, we will make a series of significant improvement over the basic framework and especially adopt greedy search instead. Another notable exception of embedding based methods is (Ma and Hinrichs, 2015), which used character-spec"
P17-2096,I08-1002,1,0.735935,"gmenter who searches greedily during both training and working to overcome the existing efficiency obstacle. Our evaluation will be performed on Chinese benchmark datasets. 2 hi wi!1 wi LSTM layer Word embedding Related Work Statistical Chinese word segmentation has been studied for decades (Huang and Zhao, 2007). (Xue, 2003) was the first to cast it as a characterbased tagging problem. Peng et al. (2004) showed CRF based model is particularly effective to solve CWS in the sequence labeling fashion. This method has been followed by most later segmenters (Tseng et al., 2005; Zhao et al., 2006; Zhao and Kit, 2008c; Zhao et al., 2010; Sun et al., 2012; Zhang et al., 2013). The same spirit has also be followed by most neural models (Zheng et al., 2013; Pei et al., 2014; Qi et al., 2014; Chen et al., 2015a,b; Ma and Hinrichs, 2015; Xu and Sun, 2016). Word based CWS to conveniently incorporate complete word features has also be explored. Andrew (2006) proposed a semi-CRF model. Zhang and Clark (2007, 2011) used a perceptron algorithm with inexact search. Both of them have been followed by neural model versions (Liu et al., 2016) and (Zhang et al., 2016) respectively. There are also works integrating both"
P17-2096,P16-2092,0,0.0733199,"gmentation has been actively studied recently. Zheng et al. (2013) first adapted the sliding-window based sequence labeling (Collobert et al., 2011) with character embeddings as input. A number of other researchers have attempted to improve the segmenter of (Zheng et al., 2013) by augmenting it with additional complexity. Pei et al. (2014) introduced tag embeddings. Chen et al. (2015a) proposed to model ngram features via a gated recursive neural network (GRNN). Chen et al. (2015b) used a Long shortterm memory network (LSTM) (Hochreiter and Schmidhuber, 1997) to capture long-distance context. Xu and Sun (2016) integrated both GRNN and LSTM for deeper feature extraction. Besides sequence labeling schemes, Zhang et al. (2016) proposed a transition-based framework. Liu et al. (2016) used a zero-order semiCRF based model. However, these two models rely on either traditional discrete features or nonneural-network components for performance enhancement, their performance drops rapidly when solely depending on neural models. Most closely related to this work, Cai and Zhao (2016) proposed to score candidate segmented outputs directly, employing a gated combination neural network over characters for word re"
P17-2096,I08-4017,1,0.484954,"gmenter who searches greedily during both training and working to overcome the existing efficiency obstacle. Our evaluation will be performed on Chinese benchmark datasets. 2 hi wi!1 wi LSTM layer Word embedding Related Work Statistical Chinese word segmentation has been studied for decades (Huang and Zhao, 2007). (Xue, 2003) was the first to cast it as a characterbased tagging problem. Peng et al. (2004) showed CRF based model is particularly effective to solve CWS in the sequence labeling fashion. This method has been followed by most later segmenters (Tseng et al., 2005; Zhao et al., 2006; Zhao and Kit, 2008c; Zhao et al., 2010; Sun et al., 2012; Zhang et al., 2013). The same spirit has also be followed by most neural models (Zheng et al., 2013; Pei et al., 2014; Qi et al., 2014; Chen et al., 2015a,b; Ma and Hinrichs, 2015; Xu and Sun, 2016). Word based CWS to conveniently incorporate complete word features has also be explored. Andrew (2006) proposed a semi-CRF model. Zhang and Clark (2007, 2011) used a perceptron algorithm with inexact search. Both of them have been followed by neural model versions (Liu et al., 2016) and (Zhang et al., 2016) respectively. There are also works integrating both"
P17-2096,D13-1061,0,0.116324,"Missing"
P18-1192,D13-1160,0,0.0594886,"thors made equal contribution.† Corresponding author. This paper was partially supported by National Key Research and Development Program of China (No. 2017YFB0304100), National Natural Science Foundation of China (No. 61672343 and No. 61733011), Key Project of National Society Science Foundation of China (No. 15ZDA041), The Art and Science Interdisciplinary Funds of Shanghai Jiao Tong University (No. 14JCRZ04). tant method to obtain semantic information beneficial to a wide range of natural language processing (NLP) tasks, including machine translation (Shi et al., 2016), question answering (Berant et al., 2013; Yih et al., 2016) and discourse relation sense classification (Mihaylov and Frank, 2016). There are two formulizations for semantic predicate-argument structures, one is based on constituents (i.e., phrase or span), the other is based on dependencies. The latter proposed by the CoNLL-2008 shared task (Surdeanu et al., 2008) is also called semantic dependency parsing, which annotates the heads of arguments rather than phrasal arguments. Generally, SRL is decomposed into multi-step classification subtasks in pipeline systems, consisting of predicate identification and disambiguation, argument"
P18-1192,C10-3009,0,0.228753,"Missing"
P18-1192,W09-1206,0,0.515401,"Missing"
P18-1192,P17-2096,1,0.810946,"Pradhan et al., 2005; Zhao et al., 2009b; Bj¨orkelund et al., 2009). Among them, Pradhan et al. (2005) combined features derived from different syntactic parses based on SVM classifier, while Zhao et al. (2009b) presented an integrative approach for dependency SRL by greedy feature selection algorithm. Later, Collobert et al. (2011) proposed a convolutional neural network model of inducing word embeddings substituting for hand-crafted features, which was a breakthrough for SRL task. With the impressive success of deep neural networks in various NLP tasks (Zhang et al., 2016; Qin et al., 2017; Cai et al., 2017), a series of neural SRL systems have been proposed. Foland and Martin (2015) presented a dependency semantic role labeler using convolutional and time-domain neural networks, while FitzGerald et al. (2015) exploited neural network to jointly embed arguments and semantic roles, akin to the work (Lei et al., 2015), which induced a compact feature representation applying tensor-based approach. Recently, researchers consider multiple ways to effectively integrate syntax into SRL learning. Roth and Lapata (2016) introduced dependency path embedding to model syntactic information and exhibited a no"
P18-1192,D15-1112,0,0.258226,"al. (2009b) presented an integrative approach for dependency SRL by greedy feature selection algorithm. Later, Collobert et al. (2011) proposed a convolutional neural network model of inducing word embeddings substituting for hand-crafted features, which was a breakthrough for SRL task. With the impressive success of deep neural networks in various NLP tasks (Zhang et al., 2016; Qin et al., 2017; Cai et al., 2017), a series of neural SRL systems have been proposed. Foland and Martin (2015) presented a dependency semantic role labeler using convolutional and time-domain neural networks, while FitzGerald et al. (2015) exploited neural network to jointly embed arguments and semantic roles, akin to the work (Lei et al., 2015), which induced a compact feature representation applying tensor-based approach. Recently, researchers consider multiple ways to effectively integrate syntax into SRL learning. Roth and Lapata (2016) introduced dependency path embedding to model syntactic information and exhibited a notable success. Marcheggiani and Titov (2017) leveraged the graph convolutional network to incorporate syntax into neural models. Differently, Marcheggiani et al. (2017) proposed a syntax-agnostic model usin"
P18-1192,S15-1033,0,0.154628,"mong them, Pradhan et al. (2005) combined features derived from different syntactic parses based on SVM classifier, while Zhao et al. (2009b) presented an integrative approach for dependency SRL by greedy feature selection algorithm. Later, Collobert et al. (2011) proposed a convolutional neural network model of inducing word embeddings substituting for hand-crafted features, which was a breakthrough for SRL task. With the impressive success of deep neural networks in various NLP tasks (Zhang et al., 2016; Qin et al., 2017; Cai et al., 2017), a series of neural SRL systems have been proposed. Foland and Martin (2015) presented a dependency semantic role labeler using convolutional and time-domain neural networks, while FitzGerald et al. (2015) exploited neural network to jointly embed arguments and semantic roles, akin to the work (Lei et al., 2015), which induced a compact feature representation applying tensor-based approach. Recently, researchers consider multiple ways to effectively integrate syntax into SRL learning. Roth and Lapata (2016) introduced dependency path embedding to model syntactic information and exhibited a notable success. Marcheggiani and Titov (2017) leveraged the graph convolutiona"
P18-1192,J02-3001,0,0.911375,"Missing"
P18-1192,P02-1031,0,0.162592,"articular, syntactic information, including syntactic tree feature, has been show extremely beneficial to SRL since a larger scale of empirical verification of Punyakanok et al. (2008). However, all the work had to take the risk of erroneous syntactic input, leading to an unsatisfactory performance. To alleviate the above issues, Marcheggiani et al. (2017) propose a simple but effective model for dependency SRL without syntactic input. It seems that neural SRL does not have to rely on syntactic features, contradicting with the belief that syntax is a necessary prerequisite for SRL as early as Gildea and Palmer (2002). This dramatic contradiction motivates us to make a thorough exploration on syntactic contribution to SRL. This paper will focus on semantic dependency parsing and formulate SRL as one or two se2061 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 2061–2071 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics 2 ... ... ... ... ... Hidden Layer + Word Representation ie x re x pe x ce x le x pos x de x BiLSTM CNN To fully disclose the predicate-argument structure, typical SRL systems have to step b"
P18-1192,P17-1044,0,0.203363,", Marcheggiani et al. (2017) proposed a syntax-agnostic model using effective word representation for dependency SRL, which for the first time achieves comparable performance as stateof-the-art syntax-aware SRL models. However, most neural SRL works seldom pay much attention to the impact of input syntactic parse over the resulting SRL performance. This work is thus more than proposing a high performance SRL model through reviewing the highlights of previous models, and presenting an effective syntactic tree based argument pruning. Our work is also closely related to (Punyakanok et al., 2008; He et al., 2017). Under the traditional methods, Punyakanok et al. (2008) investigated the significance of syntax to SRL system and shown syntactic information most crucial in the pruning stage. He et al. (2017) presented extensive error analysis with deep learning model for span SRL, including discussion of how constituent syntactic parser could be used to improve SRL performance. 6 Conclusion and Future Work This paper presents a simple and effective neural model for dependency-based SRL, incorporating syntactic information with the proposed extended k-order pruning algorithm. With a large enough setting of"
P18-1192,P82-1020,0,0.851905,"Missing"
P18-1192,W08-2123,0,0.0793009,"Missing"
P18-1192,N15-1121,0,0.38992,"Missing"
P18-1192,K17-1041,0,0.599976,"ent identification and classification. In prior work of SRL, considerable attention has been paid to feature engineering that struggles to capture sufficient discriminative information, while neural network models are capable of extracting features automatically. In particular, syntactic information, including syntactic tree feature, has been show extremely beneficial to SRL since a larger scale of empirical verification of Punyakanok et al. (2008). However, all the work had to take the risk of erroneous syntactic input, leading to an unsatisfactory performance. To alleviate the above issues, Marcheggiani et al. (2017) propose a simple but effective model for dependency SRL without syntactic input. It seems that neural SRL does not have to rely on syntactic features, contradicting with the belief that syntax is a necessary prerequisite for SRL as early as Gildea and Palmer (2002). This dramatic contradiction motivates us to make a thorough exploration on syntactic contribution to SRL. This paper will focus on semantic dependency parsing and formulate SRL as one or two se2061 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 2061–2071 c Melbourne, Au"
P18-1192,D17-1159,0,0.621925,"parison of results on CoNLL-2009 data between our end-to-end and pipeline models. 3.5 Zhao et al. (2013) LAS 90.13 87.52 88.39 89.28 88.39 89.28 − 86.0 Syntactic Contribution Syntactic information plays an informative role in semantic role labeling. However, few studies were done to quantitatively evaluate the syntactic contribution to SRL. Furthermore, we observe that most of the above compared neural SRL systems took the syntactic parser of (Bj¨orkelund et al., 2010) as syntactic inputs instead of the one from CoNLL-2009 shared task, which adopted a much weaker syntactic parser. Especially (Marcheggiani and Titov, 2017), adopted an external syntactic parser with even higher parsing accuracy. Contrarily, our SRL model is based on the automatically predicted parse with moderate performance provided by CoNLL-2009 shared task, but outperforms their models. This section thus attempts to explore how much syntax contributes to dependency-based SRL in deep learning framework and how to effectively evaluate relative performance of syntax-based SRL. To this end, we conduct experiments for empirical analysis with different syntactic inputs. Syntactic Input In order to obtain different syntactic inputs, we design a faul"
P18-1192,K16-2014,0,0.0202271,"rted by National Key Research and Development Program of China (No. 2017YFB0304100), National Natural Science Foundation of China (No. 61672343 and No. 61733011), Key Project of National Society Science Foundation of China (No. 15ZDA041), The Art and Science Interdisciplinary Funds of Shanghai Jiao Tong University (No. 14JCRZ04). tant method to obtain semantic information beneficial to a wide range of natural language processing (NLP) tasks, including machine translation (Shi et al., 2016), question answering (Berant et al., 2013; Yih et al., 2016) and discourse relation sense classification (Mihaylov and Frank, 2016). There are two formulizations for semantic predicate-argument structures, one is based on constituents (i.e., phrase or span), the other is based on dependencies. The latter proposed by the CoNLL-2008 shared task (Surdeanu et al., 2008) is also called semantic dependency parsing, which annotates the heads of arguments rather than phrasal arguments. Generally, SRL is decomposed into multi-step classification subtasks in pipeline systems, consisting of predicate identification and disambiguation, argument identification and classification. In prior work of SRL, considerable attention has been p"
P18-1192,D14-1162,0,0.0827691,"ntation is ce re pe le pos pi = [pie i , pi , pi , pi , pi , pi ]. 3 60 20 Table 1: Hyperparameter values. 2.4 Coverage Reduction Experiments Our model2 is evaluated on the CoNLL-2009 shared task both for English and Chinese datasets, following the standard training, development and test splits. The hyperparameters in our model were selected based on the development set, and are summarized in Table 1. Note that the parameters of predicate model are the same as these in argument model. All real vectors are randomly initialized, and the pre-trained word embeddings for English are GloVe vectors (Pennington et al., 2014). For Chinese, we exploit Wikipedia documents to train Word2Vec embeddings (Mikolov 5 10 k 15 20 Figure 3: Changing curves of coverage and reduction with different k value on English training set. The coverage rate is the proportion of true arguments in pruning output, while the reduction is the one of pruned argument candidates in total tokens. et al., 2013). During training procedures, we use the categorical cross-entropy as objective, with Adam optimizer (Kingma and Ba, 2015). We train models for a maximum of 20 epochs and obtain the nearly best model based on development results. For argum"
P18-1192,N18-1202,0,0.182653,"Missing"
P18-1192,P05-1072,0,0.768178,"Missing"
P18-1192,W09-1209,1,0.909138,"ous syntactic parsers contribute different syntactic inputs with various range of quality levels, the ratio provides a fairer comparison between syntactically-driven SRL systems, which will be surveyed by our empirical study. CoNLL-2008 is an English-only task, while CoNLL2009 extends to a multilingual one. Their main difference is that predicates have been beforehand indicated for the latter. Figure 1: The Argument Labeling Model the arguments, and the most crucial contribution of full parsing is in the pruning stage. In this paper, we propose a k-order argument pruning algorithm inspired by Zhao et al. (2009b). First of all, for node n and its descendant nd in a syntactic dependency tree, we define the order to be the distance between the two nodes, denoted as D(n, nd ). Then we define k-order descendants of given node satisfying D(n, nd ) = k, and k-order traversal that visits each node from the given node to its descendant nodes within k-th order. Note that the definition of k-order traversal is somewhat different from tree traversal in terminology. A brief description of the proposed k-order pruning algorithm is given as follow. Initially, we set a given predicate as the current node in a synt"
P18-1192,J08-2005,0,0.887835,"hrasal arguments. Generally, SRL is decomposed into multi-step classification subtasks in pipeline systems, consisting of predicate identification and disambiguation, argument identification and classification. In prior work of SRL, considerable attention has been paid to feature engineering that struggles to capture sufficient discriminative information, while neural network models are capable of extracting features automatically. In particular, syntactic information, including syntactic tree feature, has been show extremely beneficial to SRL since a larger scale of empirical verification of Punyakanok et al. (2008). However, all the work had to take the risk of erroneous syntactic input, leading to an unsatisfactory performance. To alleviate the above issues, Marcheggiani et al. (2017) propose a simple but effective model for dependency SRL without syntactic input. It seems that neural SRL does not have to rely on syntactic features, contradicting with the belief that syntax is a necessary prerequisite for SRL as early as Gildea and Palmer (2002). This dramatic contradiction motivates us to make a thorough exploration on syntactic contribution to SRL. This paper will focus on semantic dependency parsing"
P18-1192,D09-1004,1,0.948946,"ous syntactic parsers contribute different syntactic inputs with various range of quality levels, the ratio provides a fairer comparison between syntactically-driven SRL systems, which will be surveyed by our empirical study. CoNLL-2008 is an English-only task, while CoNLL2009 extends to a multilingual one. Their main difference is that predicates have been beforehand indicated for the latter. Figure 1: The Argument Labeling Model the arguments, and the most crucial contribution of full parsing is in the pruning stage. In this paper, we propose a k-order argument pruning algorithm inspired by Zhao et al. (2009b). First of all, for node n and its descendant nd in a syntactic dependency tree, we define the order to be the distance between the two nodes, denoted as D(n, nd ). Then we define k-order descendants of given node satisfying D(n, nd ) = k, and k-order traversal that visits each node from the given node to its descendant nodes within k-th order. Note that the definition of k-order traversal is somewhat different from tree traversal in terminology. A brief description of the proposed k-order pruning algorithm is given as follow. Initially, we set a given predicate as the current node in a synt"
P18-1192,P17-1093,1,0.877555,"Missing"
P18-1192,W09-1208,1,0.947775,"ous syntactic parsers contribute different syntactic inputs with various range of quality levels, the ratio provides a fairer comparison between syntactically-driven SRL systems, which will be surveyed by our empirical study. CoNLL-2008 is an English-only task, while CoNLL2009 extends to a multilingual one. Their main difference is that predicates have been beforehand indicated for the latter. Figure 1: The Argument Labeling Model the arguments, and the most crucial contribution of full parsing is in the pruning stage. In this paper, we propose a k-order argument pruning algorithm inspired by Zhao et al. (2009b). First of all, for node n and its descendant nd in a syntactic dependency tree, we define the order to be the distance between the two nodes, denoted as D(n, nd ). Then we define k-order descendants of given node satisfying D(n, nd ) = k, and k-order traversal that visits each node from the given node to its descendant nodes within k-th order. Note that the definition of k-order traversal is somewhat different from tree traversal in terminology. A brief description of the proposed k-order pruning algorithm is given as follow. Initially, we set a given predicate as the current node in a synt"
P18-1192,P16-1113,0,0.260632,"success of deep neural networks in various NLP tasks (Zhang et al., 2016; Qin et al., 2017; Cai et al., 2017), a series of neural SRL systems have been proposed. Foland and Martin (2015) presented a dependency semantic role labeler using convolutional and time-domain neural networks, while FitzGerald et al. (2015) exploited neural network to jointly embed arguments and semantic roles, akin to the work (Lei et al., 2015), which induced a compact feature representation applying tensor-based approach. Recently, researchers consider multiple ways to effectively integrate syntax into SRL learning. Roth and Lapata (2016) introduced dependency path embedding to model syntactic information and exhibited a notable success. Marcheggiani and Titov (2017) leveraged the graph convolutional network to incorporate syntax into neural models. Differently, Marcheggiani et al. (2017) proposed a syntax-agnostic model using effective word representation for dependency SRL, which for the first time achieves comparable performance as stateof-the-art syntax-aware SRL models. However, most neural SRL works seldom pay much attention to the impact of input syntactic parse over the resulting SRL performance. This work is thus more"
P18-1192,W08-2127,1,0.913177,"Missing"
P18-1192,P16-1212,0,0.143345,"Missing"
P18-1192,W08-2121,0,0.607747,"Missing"
P18-1192,P16-2033,0,0.0284436,"ribution.† Corresponding author. This paper was partially supported by National Key Research and Development Program of China (No. 2017YFB0304100), National Natural Science Foundation of China (No. 61672343 and No. 61733011), Key Project of National Society Science Foundation of China (No. 15ZDA041), The Art and Science Interdisciplinary Funds of Shanghai Jiao Tong University (No. 14JCRZ04). tant method to obtain semantic information beneficial to a wide range of natural language processing (NLP) tasks, including machine translation (Shi et al., 2016), question answering (Berant et al., 2013; Yih et al., 2016) and discourse relation sense classification (Mihaylov and Frank, 2016). There are two formulizations for semantic predicate-argument structures, one is based on constituents (i.e., phrase or span), the other is based on dependencies. The latter proposed by the CoNLL-2008 shared task (Surdeanu et al., 2008) is also called semantic dependency parsing, which annotates the heads of arguments rather than phrasal arguments. Generally, SRL is decomposed into multi-step classification subtasks in pipeline systems, consisting of predicate identification and disambiguation, argument identification and"
P18-1192,P16-1131,1,0.683032,"ls rely heavily on feature templates (Pradhan et al., 2005; Zhao et al., 2009b; Bj¨orkelund et al., 2009). Among them, Pradhan et al. (2005) combined features derived from different syntactic parses based on SVM classifier, while Zhao et al. (2009b) presented an integrative approach for dependency SRL by greedy feature selection algorithm. Later, Collobert et al. (2011) proposed a convolutional neural network model of inducing word embeddings substituting for hand-crafted features, which was a breakthrough for SRL task. With the impressive success of deep neural networks in various NLP tasks (Zhang et al., 2016; Qin et al., 2017; Cai et al., 2017), a series of neural SRL systems have been proposed. Foland and Martin (2015) presented a dependency semantic role labeler using convolutional and time-domain neural networks, while FitzGerald et al. (2015) exploited neural network to jointly embed arguments and semantic roles, akin to the work (Lei et al., 2015), which induced a compact feature representation applying tensor-based approach. Recently, researchers consider multiple ways to effectively integrate syntax into SRL learning. Roth and Lapata (2016) introduced dependency path embedding to model syn"
P18-2025,W08-0312,0,0.0173426,"man correlation of BLEU-1, METEOR, and W-METEOR, showing that the BLEU-1 scores vary a lot given any fixed human score, appearing to be random noise, while the METEOR family exhibit strong consistency with human scores. Compared to W-METEOR, METEOR deviates from the regression line more frequently, esp. by assigning unexpectedly high scores to comments with low human grades. Notably, the best automatic metric, WMETEOR, achieves 0.59 Spearman and 0.57 Pearson, which is higher or comparable to automatic metrics in other generation tasks (Lowe et al., 2017; Liu et al., 2016; Sharma et al., 2017; Agarwal and Lavie, 2008), indicating a good supplement to human judgment for efficient evaluation and comparison. We use the metrics to evaluate the above models in the supplements. 6 Conclusions and Future Work We have introduced the new task and dataset for automatic article commenting, as well as developed quality-weighted automatic metrics that leverage valuable human bias on comment quality. The dataset and the study of metrics establish a testbed for the article commenting task. We are excited to study solutions for the task in the future, by building advanced deep generative models (Goodfellow et al., 2016; Hu"
P18-2025,P15-1034,0,0.0136403,"ments. 6 Conclusions and Future Work We have introduced the new task and dataset for automatic article commenting, as well as developed quality-weighted automatic metrics that leverage valuable human bias on comment quality. The dataset and the study of metrics establish a testbed for the article commenting task. We are excited to study solutions for the task in the future, by building advanced deep generative models (Goodfellow et al., 2016; Hu et al., 2018) that incorporate effective reading comprehension modules (Rajpurkar et al., 2016; Richardson et al., 2013) and rich external knowledge (Angeli et al., 2015; Hu et al., 2016). The large dataset is also potentially useful for a variety of other tasks, such as comment ranking (Hsu et al., 2009), upvotes prediction (Rizos et al., 2016), and article headline generation (Banko et al., 2000). We encourage the use of the dataset in these context. Acknowledgement. We would like to thank anonymous reviewers for their helpful suggestions and particularly the annotators for their contributions on the dataset. Hai Zhao was partially supported by National Key Research and Development Program of China (No. 2017YFB0304100), National Natural Science Foundation o"
P18-2025,W05-0909,0,0.493806,"e articles. Article commenting also differs from making product reviews (Tang et al., 2017; Li et al., 2017), as the latter takes structured data (e.g., product attributes) as input; while the input of article commenting is in plain text format, posing a much larger input space to explore. In this paper, we propose the new task of automatic article commenting, and release a largescale Chinese corpus with a human-annotated subset for scientific research and evaluation. We further develop a general approach of enhancing popular automatic metrics, such as BLEU (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005), to better fit the characteristics of the new task. In recent years, enormous efforts have been made in different contexts that analyze one or more aspects of online comments. For example, Kolhatkar and Taboada (2017) identify constructive news comments; Barker et al. (2016) study human summaries of online comment conversations. The datasets used in these works are typically not directly applicable in the context of article commenting, and are small in scale that is unable to support the unique complexity of the new task. In contrast, our dataset consists of around 200K news articles and 4.5M"
P18-2025,P00-1041,0,0.198504,"Missing"
P18-2025,W16-3605,0,0.288313,"explore. In this paper, we propose the new task of automatic article commenting, and release a largescale Chinese corpus with a human-annotated subset for scientific research and evaluation. We further develop a general approach of enhancing popular automatic metrics, such as BLEU (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005), to better fit the characteristics of the new task. In recent years, enormous efforts have been made in different contexts that analyze one or more aspects of online comments. For example, Kolhatkar and Taboada (2017) identify constructive news comments; Barker et al. (2016) study human summaries of online comment conversations. The datasets used in these works are typically not directly applicable in the context of article commenting, and are small in scale that is unable to support the unique complexity of the new task. In contrast, our dataset consists of around 200K news articles and 4.5M human comments along with rich meta data for article categories and user votes of comments. Different from traditional text generation tasks such as machine translation (Brown et al., 1990) that has a relatively small set of gold targets, human comments on an article live in"
P18-2025,J90-2002,0,0.751023,"ents. For example, Kolhatkar and Taboada (2017) identify constructive news comments; Barker et al. (2016) study human summaries of online comment conversations. The datasets used in these works are typically not directly applicable in the context of article commenting, and are small in scale that is unable to support the unique complexity of the new task. In contrast, our dataset consists of around 200K news articles and 4.5M human comments along with rich meta data for article categories and user votes of comments. Different from traditional text generation tasks such as machine translation (Brown et al., 1990) that has a relatively small set of gold targets, human comments on an article live in much larger space by involving diverse topics and personal views, and critically, are of vary151 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 151–156 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics Title: ˘úl¯iPhone 8 —⇤⇢ö(9 &gt;L (Apple’s iPhone 8 event is happening in Sept.) Score Criteria Content: ˘úl¯c✏⌘íS—⇤Ä˜ ˝ £⇤⌃é9 12ÂÏ ˘ú∞¡—⇤ ⇢ Âl¯⌃—⇤↵ „iPhone èKÙ∞ Ñÿ ˘úKh ˘úTV åiOSoˆ⇥Ÿ !—⇤⇢⌃&e &gt;∞iPhones⇢&OLED&gt; :O"
P18-2025,P15-2073,0,0.0206468,"ating comments, and develop a dataset that is orders-of-magnitude larger than previous related corpus. Instead of restricting to one or few specific aspects, we focus on the general comment quality aligned with human judgment, and provide over 27 gold references for each data instance to enable wide-coverage evaluation. Such setting also allows a large output space, and makes the task challenging and valuable for text generation research. Yao et al. (2017) explore defense approaches of spam or malicious reviews. We believe the proposed task and dataset can be potentially useful for the study. Galley et al. (2015) propose BLEU that weights multiple references for conversation generation evaluation. The quality weighted metrics developed in our work can be seen as a generalization of BLEU to many popular reference-based metrics (e.g., METEOR, ROUGE, and CIDEr). Our human survey demonstrates the effectiveness of the generalized metrics in the article commenting task. 3 Article Commenting Dataset The dataset is collected from Tencent News (news.qq.com), one of the most popular Chinese websites of news and opinion articles. Table 1 shows an example data instance in the dataset (For readability we also prov"
P18-2025,X98-1026,0,0.208952,"foster online communities. Besides, commenting on articles is one of the increasingly demanded skills of intelligent chatbot (Shum et al., 2018) to enable in-depth, content-rich conversations with humans. Article commenting poses new challenges for machines, as it involves multiple cognitive abil⇤ Work done while Lianhui interned at Tencent AI Lab The dataset is available on http://ai.tencent. com/upload/PapersUploads/article_ commenting.tgz 1 ities: understanding the given article, formulating opinions and arguments, and organizing natural language for expression. Compared to summarization (Hovy and Lin, 1998), a comment does not necessarily cover all salient ideas of the article; instead it is often desirable for a comment to carry additional information not explicitly presented in the articles. Article commenting also differs from making product reviews (Tang et al., 2017; Li et al., 2017), as the latter takes structured data (e.g., product attributes) as input; while the input of article commenting is in plain text format, posing a much larger input space to explore. In this paper, we propose the new task of automatic article commenting, and release a largescale Chinese corpus with a human-annot"
P18-2025,D13-1020,0,0.0324806,"the metrics to evaluate the above models in the supplements. 6 Conclusions and Future Work We have introduced the new task and dataset for automatic article commenting, as well as developed quality-weighted automatic metrics that leverage valuable human bias on comment quality. The dataset and the study of metrics establish a testbed for the article commenting task. We are excited to study solutions for the task in the future, by building advanced deep generative models (Goodfellow et al., 2016; Hu et al., 2018) that incorporate effective reading comprehension modules (Rajpurkar et al., 2016; Richardson et al., 2013) and rich external knowledge (Angeli et al., 2015; Hu et al., 2016). The large dataset is also potentially useful for a variety of other tasks, such as comment ranking (Hsu et al., 2009), upvotes prediction (Rizos et al., 2016), and article headline generation (Banko et al., 2000). We encourage the use of the dataset in these context. Acknowledgement. We would like to thank anonymous reviewers for their helpful suggestions and particularly the annotators for their contributions on the dataset. Hai Zhao was partially supported by National Key Research and Development Program of China (No. 2017Y"
P18-2025,W17-3002,0,0.139171,"nting is in plain text format, posing a much larger input space to explore. In this paper, we propose the new task of automatic article commenting, and release a largescale Chinese corpus with a human-annotated subset for scientific research and evaluation. We further develop a general approach of enhancing popular automatic metrics, such as BLEU (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005), to better fit the characteristics of the new task. In recent years, enormous efforts have been made in different contexts that analyze one or more aspects of online comments. For example, Kolhatkar and Taboada (2017) identify constructive news comments; Barker et al. (2016) study human summaries of online comment conversations. The datasets used in these works are typically not directly applicable in the context of article commenting, and are small in scale that is unable to support the unique complexity of the new task. In contrast, our dataset consists of around 200K news articles and 4.5M human comments along with rich meta data for article categories and user votes of comments. Different from traditional text generation tasks such as machine translation (Brown et al., 1990) that has a relatively small"
P18-2025,W04-1013,0,0.0457846,"Missing"
P18-2025,D16-1230,0,0.116055,"ead articles using an encoder and generate comments using a decoder with or without attentions (Bahdanau et al., 2014), which are denoted as Seq2seq and Att if only article titles are read. We also set up an attentional sequence-tosequence model that reads full article title/content, and denote with Att-TC. Again, these approaches are mainly for demonstration purpose and for evaluating the metrics, and are far from solving the difficult commenting task. We discard comments with over 50 words and use a truncated vocabulary of size 30K. Results We follow previous setting (Papineni et al., 2002; Liu et al., 2016; Lowe et al., 2017) to evaluate the metrics, by conducting human evaluations and calculating the correlation between the scores assigned by humans and the metrics. Specifically, for each article in the test set, we obtained six comments, five of which come from IRT, IR-TC, Seq2seq, Att, and Att-TC, respectively, and one randomly drawn from real comments that are different from the reference comments. The comments were then graded by human annotators following the same procedure of test set scoring (sec.3). Meanwhile, we measure each comment with the vanilla and weighted automatic metrics base"
P18-2025,P17-1103,0,0.329605,"c be a generated comment to evaluate, R = {rj } the set of references, each of which has a quality score sj by human annotators. We assume properly normalized sj 2 [0, 1]. Due to space limitations, here we only present the enhanced METEOR, and defer the formulations of enhancing BLEU, ROUGE, and CIDEr to the supplements. Specifically, METEOR performs word matching through an alignment between the candidate and references. The weighted METEOR extends the Figure 1: Scatter plots showing the correlation between metrics and human judgments. Left: BLEU1; Middle: METEOR; Right: W-METEOR. Following (Lowe et al., 2017), we added Gaussian noise drawn from N (0, 0.05) to the integer human scores to better visualize the density of points. original metric by weighting references with sj : W-METEOR(c, R) = (1 BP ) maxj sj Fmean,j , (1) where Fmean,j is a harmonic mean of the precision and recall between c and rj , and BP is the penalty (Banerjee and Lavie, 2005). Note that the new metrics fall back to the respective original metrics by setting sj = 1. 5 Experiments We demonstrate the use of the dataset and metrics with simple retrieval and generation models, and show the enhanced metrics consistently improve cor"
P18-2025,P02-1040,0,0.101647,"tion not explicitly presented in the articles. Article commenting also differs from making product reviews (Tang et al., 2017; Li et al., 2017), as the latter takes structured data (e.g., product attributes) as input; while the input of article commenting is in plain text format, posing a much larger input space to explore. In this paper, we propose the new task of automatic article commenting, and release a largescale Chinese corpus with a human-annotated subset for scientific research and evaluation. We further develop a general approach of enhancing popular automatic metrics, such as BLEU (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005), to better fit the characteristics of the new task. In recent years, enormous efforts have been made in different contexts that analyze one or more aspects of online comments. For example, Kolhatkar and Taboada (2017) identify constructive news comments; Barker et al. (2016) study human summaries of online comment conversations. The datasets used in these works are typically not directly applicable in the context of article commenting, and are small in scale that is unable to support the unique complexity of the new task. In contrast, our dataset consists"
P18-2025,D16-1264,0,0.0517994,"and comparison. We use the metrics to evaluate the above models in the supplements. 6 Conclusions and Future Work We have introduced the new task and dataset for automatic article commenting, as well as developed quality-weighted automatic metrics that leverage valuable human bias on comment quality. The dataset and the study of metrics establish a testbed for the article commenting task. We are excited to study solutions for the task in the future, by building advanced deep generative models (Goodfellow et al., 2016; Hu et al., 2018) that incorporate effective reading comprehension modules (Rajpurkar et al., 2016; Richardson et al., 2013) and rich external knowledge (Angeli et al., 2015; Hu et al., 2016). The large dataset is also potentially useful for a variety of other tasks, such as comment ranking (Hsu et al., 2009), upvotes prediction (Rizos et al., 2016), and article headline generation (Banko et al., 2000). We encourage the use of the dataset in these context. Acknowledgement. We would like to thank anonymous reviewers for their helpful suggestions and particularly the annotators for their contributions on the dataset. Hai Zhao was partially supported by National Key Research and Development P"
P18-2025,P16-2032,0,0.144391,"Work There is a surge of interest in natural language generation tasks, such as machine translation (Brown et al., 1990; Bahdanau et al., 2014), dialog (Williams and Young, 2007; Shum et al., 2018), text manipulation (Hu et al., 2017), visual description generation (Vinyals et al., 2015; Liang et al., 2017), and so forth. Automatic article commenting poses new challenges due to the large input and output spaces and the open-domain nature 152 of comments. Many efforts have been devoted to studying specific attributes of reader comments, such as constructiveness, persuasiveness, and sentiment (Wei et al., 2016; Kolhatkar and Taboada, 2017; Barker et al., 2016). We introduce the new task of generating comments, and develop a dataset that is orders-of-magnitude larger than previous related corpus. Instead of restricting to one or few specific aspects, we focus on the general comment quality aligned with human judgment, and provide over 27 gold references for each data instance to enable wide-coverage evaluation. Such setting also allows a large output space, and makes the task challenging and valuable for text generation research. Yao et al. (2017) explore defense approaches of spam or malicious revi"
P18-2025,D17-1239,0,0.0214441,"Table 2 provides the key data statistics. The dataset has a vocabulary size of 1,858,452. The average lengths of the article titles and content are 15 and 554 Chinese words (not characters), respectively. The average comment length is 17 words. Notably, the dataset contains an enormous volume of tokens, and is orders-of-magnitude larger than previous public data of article comment analysis (Wei et al., 2016; Barker et al., 2016). Moreover, each article in the dataset has on average over 27 human-written comments. Compared to other popular text generation tasks and datasets (Chen et al., 2015; Wiseman et al., 2017) which typically contain no more than 5 gold references, our dataset enables richer guidance for model training and wider coverage for evaluation, in order to fit the unique large output space of the commenting task. Each article is associated with one of 44 categories, whose distribution is shown in the supplements. The number of upvotes per comment ranges from 3.4 to 5.9 on average. Though the numbers look small, the distribution exhibits a long-tail pattern with popular comments having thousands of upvotes. Test Set Comment Quality Annotations Real human comments are of varying quality. Sel"
P18-4024,P17-2096,1,0.82377,"e a download link. 4 t user requirement for oversimplified modeling. It is worth mentioning that we delivery Moon IME as a type of IME service rather than a simple IME software because it can be adjusted to adapt to diverse domains with the Association Cloud Platform (Zhang et al., 2018b,c; Zhang and Zhao, 2018), which helps user type long sentences and predicts the whole expected inputs based on customized knowledge bases. Related Work There are variable referential natural language processing studies(Cai et al., 2018; Li et al., 2018b; He et al., 2018; Li et al., 2018a; Zhang et al., 2018a; Cai et al., 2017a,b) for IME development to refer to. Most of the engineering practice mainly focus on the matching correspondence between the Pinyin and Chinese characters, namely, pinyin-to-character converting with the highest accuracy. (Chen, 2003) introduced a conditional maximum entropy model with syllabification for grapheme-to-phoneme conversion. (Zhang et al., 2006) presented a rule-based error correction approach to improving preferable conversion rate. (Lin and Zhang, 2008) present a statistical model that associates a word with supporting context to offer a better solution to Chinese input. (Jiang"
P18-4024,Y15-1052,1,0.672822,"ing preferable conversion rate. (Lin and Zhang, 2008) present a statistical model that associates a word with supporting context to offer a better solution to Chinese input. (Jiang et al., 2007) put forward a PTC framework based on support vector machine. (Okuno and Mori, 2012) introduced an ensemble model of wordbased and character-based models for Japanese and Chinese IMEs. (Yang et al., 2012; Wang et al., 2018, 2016; Pang et al., 2016; Jia and Zhao, 2013, 2014) regarded the P2C conversion as a transformation between two languages and solved it by statistical machine translation framework. (Chen et al., 2015) firstly use natural machine thanslation method to translate pinyin to Chinese. (Zhang et al., 2017) introduced an online algorithm to construct an appropriate dictionary for IME. The recent trend on state-of-the-art techniques for Chinese input methods can be put into two lines. Speech-to-text input as iFly IM7 (Zhang et al., 2015; Saon et al., 2014; Lu et al., 2016) and the aided input methods which are capable of generating candidate sentences for users to choose to complete input tasks, means that users can yield coherent text with fewer keystrokes. The challenge is that the input pinyin s"
P18-4024,P18-1192,1,0.656478,"lay the main feature function of our platform and provide a download link. 4 t user requirement for oversimplified modeling. It is worth mentioning that we delivery Moon IME as a type of IME service rather than a simple IME software because it can be adjusted to adapt to diverse domains with the Association Cloud Platform (Zhang et al., 2018b,c; Zhang and Zhao, 2018), which helps user type long sentences and predicts the whole expected inputs based on customized knowledge bases. Related Work There are variable referential natural language processing studies(Cai et al., 2018; Li et al., 2018b; He et al., 2018; Li et al., 2018a; Zhang et al., 2018a; Cai et al., 2017a,b) for IME development to refer to. Most of the engineering practice mainly focus on the matching correspondence between the Pinyin and Chinese characters, namely, pinyin-to-character converting with the highest accuracy. (Chen, 2003) introduced a conditional maximum entropy model with syllabification for grapheme-to-phoneme conversion. (Zhang et al., 2006) presented a rule-based error correction approach to improving preferable conversion rate. (Lin and Zhang, 2008) present a statistical model that associates a word with supporting co"
P18-4024,P82-1020,0,0.652236,"Missing"
P18-4024,C16-1295,1,0.725041,"Missing"
P18-4024,I13-1170,1,0.778005,"aximum entropy model with syllabification for grapheme-to-phoneme conversion. (Zhang et al., 2006) presented a rule-based error correction approach to improving preferable conversion rate. (Lin and Zhang, 2008) present a statistical model that associates a word with supporting context to offer a better solution to Chinese input. (Jiang et al., 2007) put forward a PTC framework based on support vector machine. (Okuno and Mori, 2012) introduced an ensemble model of wordbased and character-based models for Japanese and Chinese IMEs. (Yang et al., 2012; Wang et al., 2018, 2016; Pang et al., 2016; Jia and Zhao, 2013, 2014) regarded the P2C conversion as a transformation between two languages and solved it by statistical machine translation framework. (Chen et al., 2015) firstly use natural machine thanslation method to translate pinyin to Chinese. (Zhang et al., 2017) introduced an online algorithm to construct an appropriate dictionary for IME. The recent trend on state-of-the-art techniques for Chinese input methods can be put into two lines. Speech-to-text input as iFly IM7 (Zhang et al., 2015; Saon et al., 2014; Lu et al., 2016) and the aided input methods which are capable of generating candidate se"
P18-4024,P14-1142,1,0.926181,"Missing"
P18-4024,P17-1046,0,0.0301894,"recognition, and speech recognition. 2 https://github.com/EasyIME/PIME 3 141 http://opennmt.net tor is computed as a weighted sum of previously hidden states. The probability of each candidate word as being the recommended one is predicted using a softmax layer over the inner-product between source and candidate target characters. Our model is initially trained on two datasets, namely the People’s Daily (PD) corpus and Douban (DC) corpus. The former is extracted from the People’s Daily from 1992 to 1998 that has word segmentation annotations by Peking University. The DC corpus is created by (Wu et al., 2017) from Chinese open domain conversations. One sentence of the DC corpus contains one complete utterance in a continuous dialogue situation. The statistics of two datasets is shown in Table 1. With character text available, the needed parallel corpus between pinyin and character texts is automatically created following the approach proposed by (Yang et al., 2012). # MIUs PD # Vocab # MIUs DC # Vocab text classification and information retrieval. The TF (term-frequency) term is simply a count of the number of times a word appearing in a given context, while the IDF (invert document frequency) ter"
P18-4024,C18-1271,1,0.517033,", in which we display the main feature function of our platform and provide a download link. 4 t user requirement for oversimplified modeling. It is worth mentioning that we delivery Moon IME as a type of IME service rather than a simple IME software because it can be adjusted to adapt to diverse domains with the Association Cloud Platform (Zhang et al., 2018b,c; Zhang and Zhao, 2018), which helps user type long sentences and predicts the whole expected inputs based on customized knowledge bases. Related Work There are variable referential natural language processing studies(Cai et al., 2018; Li et al., 2018b; He et al., 2018; Li et al., 2018a; Zhang et al., 2018a; Cai et al., 2017a,b) for IME development to refer to. Most of the engineering practice mainly focus on the matching correspondence between the Pinyin and Chinese characters, namely, pinyin-to-character converting with the highest accuracy. (Chen, 2003) introduced a conditional maximum entropy model with syllabification for grapheme-to-phoneme conversion. (Zhang et al., 2006) presented a rule-based error correction approach to improving preferable conversion rate. (Lin and Zhang, 2008) present a statistical model that associates a word"
P18-4024,S18-1147,1,0.688555,"ce the demands of the users are quite diverse, our platform to support such demands can be adapted to any specific domains with complex specialized terms. We provide a http://www.statmt.org/wmt17/translation-task.html 143 Demo homepage6 for better reference, in which we display the main feature function of our platform and provide a download link. 4 t user requirement for oversimplified modeling. It is worth mentioning that we delivery Moon IME as a type of IME service rather than a simple IME software because it can be adjusted to adapt to diverse domains with the Association Cloud Platform (Zhang et al., 2018b,c; Zhang and Zhao, 2018), which helps user type long sentences and predicts the whole expected inputs based on customized knowledge bases. Related Work There are variable referential natural language processing studies(Cai et al., 2018; Li et al., 2018b; He et al., 2018; Li et al., 2018a; Zhang et al., 2018a; Cai et al., 2017a,b) for IME development to refer to. Most of the engineering practice mainly focus on the matching correspondence between the Pinyin and Chinese characters, namely, pinyin-to-character converting with the highest accuracy. (Chen, 2003) introduced a conditional maximum e"
P18-4024,C18-1317,1,0.551668,"ce the demands of the users are quite diverse, our platform to support such demands can be adapted to any specific domains with complex specialized terms. We provide a http://www.statmt.org/wmt17/translation-task.html 143 Demo homepage6 for better reference, in which we display the main feature function of our platform and provide a download link. 4 t user requirement for oversimplified modeling. It is worth mentioning that we delivery Moon IME as a type of IME service rather than a simple IME software because it can be adjusted to adapt to diverse domains with the Association Cloud Platform (Zhang et al., 2018b,c; Zhang and Zhao, 2018), which helps user type long sentences and predicts the whole expected inputs based on customized knowledge bases. Related Work There are variable referential natural language processing studies(Cai et al., 2018; Li et al., 2018b; He et al., 2018; Li et al., 2018a; Zhang et al., 2018a; Cai et al., 2017a,b) for IME development to refer to. Most of the engineering practice mainly focus on the matching correspondence between the Pinyin and Chinese characters, namely, pinyin-to-character converting with the highest accuracy. (Chen, 2003) introduced a conditional maximum e"
P18-4024,D15-1166,0,0.0645188,"e Y , the encoder of the P2C model encodes pinyin representation in word-level, and the decoder is to generate the target Chinese sequence which maximizes P (Y |X) using maximum likelihood training. The encoder is a bi-directional long shortterm memory (LSTM) network (Hochreiter and Schmidhuber, 1997). The vectorized inputs are fed to forward LSTM and backward LSTM to obtain the internal features of two directions. The output for each input is the concatenation of the ← → → − ← − two vectors from both directions: ht = ht k ht . Our decoder is based on the global attentional model proposed by (Luong et al., 2015) which takes the hidden states of the encoder into consideration when deriving the context vector. The probability is conditioned on a distinct context vector for each target word. The context vecSystem Details Figure 1 illustrates the architecture of Moon IME. The Moon IME is based on Windows Text Services Framework (TSF) 1 . Our Moon IME extends the Open-source projects PIME2 with three main components: a) pinyin text segmentation, b) P2C conversion module, c) IR-based association module. The nub of our work is realizing an engine to stably convert pinyin to Chinese as well as giving reasona"
P18-4024,C18-1153,1,0.659639,"ce the demands of the users are quite diverse, our platform to support such demands can be adapted to any specific domains with complex specialized terms. We provide a http://www.statmt.org/wmt17/translation-task.html 143 Demo homepage6 for better reference, in which we display the main feature function of our platform and provide a download link. 4 t user requirement for oversimplified modeling. It is worth mentioning that we delivery Moon IME as a type of IME service rather than a simple IME software because it can be adjusted to adapt to diverse domains with the Association Cloud Platform (Zhang et al., 2018b,c; Zhang and Zhao, 2018), which helps user type long sentences and predicts the whole expected inputs based on customized knowledge bases. Related Work There are variable referential natural language processing studies(Cai et al., 2018; Li et al., 2018b; He et al., 2018; Li et al., 2018a; Zhang et al., 2018a; Cai et al., 2017a,b) for IME development to refer to. Most of the engineering practice mainly focus on the matching correspondence between the Pinyin and Chinese characters, namely, pinyin-to-character converting with the highest accuracy. (Chen, 2003) introduced a conditional maximum e"
P18-4024,C18-1038,1,0.800811,"sers are quite diverse, our platform to support such demands can be adapted to any specific domains with complex specialized terms. We provide a http://www.statmt.org/wmt17/translation-task.html 143 Demo homepage6 for better reference, in which we display the main feature function of our platform and provide a download link. 4 t user requirement for oversimplified modeling. It is worth mentioning that we delivery Moon IME as a type of IME service rather than a simple IME software because it can be adjusted to adapt to diverse domains with the Association Cloud Platform (Zhang et al., 2018b,c; Zhang and Zhao, 2018), which helps user type long sentences and predicts the whole expected inputs based on customized knowledge bases. Related Work There are variable referential natural language processing studies(Cai et al., 2018; Li et al., 2018b; He et al., 2018; Li et al., 2018a; Zhang et al., 2018a; Cai et al., 2017a,b) for IME development to refer to. Most of the engineering practice mainly focus on the matching correspondence between the Pinyin and Chinese characters, namely, pinyin-to-character converting with the highest accuracy. (Chen, 2003) introduced a conditional maximum entropy model with syllabif"
P18-4024,W06-0127,1,0.72037,"plex specialized terms. The rest of the paper is organized as follows: Section 2 demonstrates the details of our system. Section 3 presents the feature functions of our realized IME. Some related works are introduced in Section 4. Section 5 concludes this paper. 2 As (Zhang et al., 2017) proves that P2C conversion of IME may benefit from decoding longer pinyin sequence for more efficient inputting. When a given pinyin sequence becomes longer, the list of the corresponding legal character sequences will significantly reduce. Thus, we train our P2C model with segmented corpora. We used baseSeg (Zhao et al., 2006) to segment all text, and finish the training in both word-level and character-level. NMT-based P2C module Our P2C module is implemented through OpenNMT Toolkit3 as we formulize P2C as a translation between pinyin and character sequences. Given a pinyin sequence X and a Chinese character sequence Y , the encoder of the P2C model encodes pinyin representation in word-level, and the decoder is to generate the target Chinese sequence which maximizes P (Y |X) using maximum likelihood training. The encoder is a bi-directional long shortterm memory (LSTM) network (Hochreiter and Schmidhuber, 1997)."
P18-4024,W12-4802,0,0.384205,"focus on the matching correspondence between the Pinyin and Chinese characters, namely, pinyin-to-character converting with the highest accuracy. (Chen, 2003) introduced a conditional maximum entropy model with syllabification for grapheme-to-phoneme conversion. (Zhang et al., 2006) presented a rule-based error correction approach to improving preferable conversion rate. (Lin and Zhang, 2008) present a statistical model that associates a word with supporting context to offer a better solution to Chinese input. (Jiang et al., 2007) put forward a PTC framework based on support vector machine. (Okuno and Mori, 2012) introduced an ensemble model of wordbased and character-based models for Japanese and Chinese IMEs. (Yang et al., 2012; Wang et al., 2018, 2016; Pang et al., 2016; Jia and Zhao, 2013, 2014) regarded the P2C conversion as a transformation between two languages and solved it by statistical machine translation framework. (Chen et al., 2015) firstly use natural machine thanslation method to translate pinyin to Chinese. (Zhang et al., 2017) introduced an online algorithm to construct an appropriate dictionary for IME. The recent trend on state-of-the-art techniques for Chinese input methods can be"
P19-1154,P17-2096,1,0.649004,", 2018; Chen et al., 2018). As for improved word representation in IMEs, Hatori and Suzuki (2011) solved Japanese pronunciation inference combining word-based and character-based features within SMT-style framework to handle unknown words. Neubig et al. (2013) proposed character-based SMT to handle sparsity. Okuno and Mori (2012) introduced an ensemble model of word-based and character-based models for Japanese and Chinese IMEs. All the above-mentioned methods used similar solution about character representation for various tasks. Our work takes inspiration from (Luong and Manning, 2016) and (Cai et al., 2017). The former built a novel representation method to tackle the rare word for machine translation. In detail, they used word representation network with characters as the basic input units. Cai et al. (2017) presented a greedy neural word segmenter with balanced word and character embedding inputs. In the meantime, high-frequency word embeddings are attached to character embedding via average pooling while low-frequency words are computed from character embedding. Our embeddings also contain different granularity levels of embedding, but the word vocabulary is capable of being updated in accord"
P19-1154,W12-4804,0,0.0253754,"08) proposed an iterative algorithm to discover unseen words in corpus for building a Chinese language model. Mori et al. (2006) described a method enlarging the vocabulary which can capture the context information. For either pinyin-to-character for Chinese IMEs or kana-to-kanji for Japanese IMEs, a few language model training methods have been developed. Mori et al. (1998) proposed a probabilistic based language model for IME. Jiampojamarn et al. (2008) presented online discriminative training. Lin and Zhang (2008) proposed a statistic model using the frequent nearby set of the target word. Chen et al. (2012) used collocations and kmeans clustering to improve the n-pos model for Japanese IME. Jiang et al. (2007) put forward a PTC framework based on support vector machine. Hatori and Suzuki (2011) and Yang et al. (2012) respectively applied statistic machine translation (SMT) to Japanese pronunciation prediction and Chinese P2C tasks. Chen et al. (2015); Huang et al. (2018) regarded the P2C as a translation between two languages and solved it in neural machine translation framework. All the above-mentioned work, however, still rely on a predefined fixed vocabulary, and IME users have no chance to r"
P19-1154,Y15-1052,1,0.836936,"veloped. Mori et al. (1998) proposed a probabilistic based language model for IME. Jiampojamarn et al. (2008) presented online discriminative training. Lin and Zhang (2008) proposed a statistic model using the frequent nearby set of the target word. Chen et al. (2012) used collocations and kmeans clustering to improve the n-pos model for Japanese IME. Jiang et al. (2007) put forward a PTC framework based on support vector machine. Hatori and Suzuki (2011) and Yang et al. (2012) respectively applied statistic machine translation (SMT) to Japanese pronunciation prediction and Chinese P2C tasks. Chen et al. (2015); Huang et al. (2018) regarded the P2C as a translation between two languages and solved it in neural machine translation framework. All the above-mentioned work, however, still rely on a predefined fixed vocabulary, and IME users have no chance to refine their own dictionary through a user-friendly way. Zhang et al. (2017) is mostly related to this work, which also offers an online mechanism to adaptively update user vocabulary. The key difference between their work 1585 and ours lies on that this work presents the first neural solution with online vocabulary adaptation while (Zhang et al., 2"
P19-1154,P00-1031,0,0.249461,"ecoding procedure. In detail, first, a characterenhanced word embedding (CWE) mechanism is proposed to represent the word so that the proposed model can let IME generally work at the word level and pick a very small target vocabulary for each sentence. Second, every time the user makes a selection contradicted the prediction given by the P2C conversion module, the modRelated Work To effectively utilize words for IMEs, many natural language processing (NLP) techniques have been applied. Chen (2003) introduced a joint maximum n-gram model with syllabification for grapheme-to-phoneme conversion. Chen and Lee (2000) used a trigram language model and incorporated word segmentation to convert pinyin sequence to Chinese word sequence. Xiao et al. (2008) proposed an iterative algorithm to discover unseen words in corpus for building a Chinese language model. Mori et al. (2006) described a method enlarging the vocabulary which can capture the context information. For either pinyin-to-character for Chinese IMEs or kana-to-kanji for Japanese IMEs, a few language model training methods have been developed. Mori et al. (1998) proposed a probabilistic based language model for IME. Jiampojamarn et al. (2008) presen"
P19-1154,I05-3017,0,0.0851684,"ulary Vs for a sentence x = (P y1 , P y2 , ...) by merging all the candidates of all pinyin. In order to cover target un-aligned functional words, we also need top n most common target words Vc . In training procedure, the target vocabulary Vˆ for a sentence x needs to include the target words Vt in the reference y, Vˆ = Vs ∪ Vc ∪ Vy . In decoding procedure, the Vˆ may only contain two parts, Vˆ = Vs ∪ Vc . 6 6.1 Experiment Datasets and Evaluation Metrics We adopt two corpora for evaluation. The People’s Daily corpus is extracted from the People’s Daily from 1992 to 1998 by Peking University (Emerson, 2005). The bilingual corpus can be straightforwardly produced by the conversion proposed by (Yang et al., 2012). Contrast to the style of the People’s Daily, the TouchPal corpus (Zhang et al., 2017) is a large scale of user chat history collected by TouchPal IME, which are more colloquial. Hence, we use the latter to simulate user’s chatting input to verify our online model’s adaptability to different environments. The test set size is 2,000 MIUs in both corpora. Table 2 shows the statistics of two corpora1 . Two metrics are used for our evaluation by following previous work: Maximum Input Unit (MI"
P19-1154,I11-1014,0,0.122945,"pture the context information. For either pinyin-to-character for Chinese IMEs or kana-to-kanji for Japanese IMEs, a few language model training methods have been developed. Mori et al. (1998) proposed a probabilistic based language model for IME. Jiampojamarn et al. (2008) presented online discriminative training. Lin and Zhang (2008) proposed a statistic model using the frequent nearby set of the target word. Chen et al. (2012) used collocations and kmeans clustering to improve the n-pos model for Japanese IME. Jiang et al. (2007) put forward a PTC framework based on support vector machine. Hatori and Suzuki (2011) and Yang et al. (2012) respectively applied statistic machine translation (SMT) to Japanese pronunciation prediction and Chinese P2C tasks. Chen et al. (2015); Huang et al. (2018) regarded the P2C as a translation between two languages and solved it in neural machine translation framework. All the above-mentioned work, however, still rely on a predefined fixed vocabulary, and IME users have no chance to refine their own dictionary through a user-friendly way. Zhang et al. (2017) is mostly related to this work, which also offers an online mechanism to adaptively update user vocabulary. The key"
P19-1154,P18-4024,1,0.842233,"(1998) proposed a probabilistic based language model for IME. Jiampojamarn et al. (2008) presented online discriminative training. Lin and Zhang (2008) proposed a statistic model using the frequent nearby set of the target word. Chen et al. (2012) used collocations and kmeans clustering to improve the n-pos model for Japanese IME. Jiang et al. (2007) put forward a PTC framework based on support vector machine. Hatori and Suzuki (2011) and Yang et al. (2012) respectively applied statistic machine translation (SMT) to Japanese pronunciation prediction and Chinese P2C tasks. Chen et al. (2015); Huang et al. (2018) regarded the P2C as a translation between two languages and solved it in neural machine translation framework. All the above-mentioned work, however, still rely on a predefined fixed vocabulary, and IME users have no chance to refine their own dictionary through a user-friendly way. Zhang et al. (2017) is mostly related to this work, which also offers an online mechanism to adaptively update user vocabulary. The key difference between their work 1585 and ours lies on that this work presents the first neural solution with online vocabulary adaptation while (Zhang et al., 2017) sticks to a trad"
P19-1154,P15-1001,0,0.29286,"scusses relevant works. Sections 3 and 4 introduce the proposed model. Experimental results and the model analysis are respectively in Sections 5 and 6. Section 7 concludes this paper. ni 你 你 2 Table 1: The shorter the pinyin sequence is, the more character sequences will be mapped. from the mapping ambiguity. However, the effect of the work in P2C will be undermined with quite restricted vocabularies. The efficiency of IME conversion depends on the sufficiency of the vocabulary and previous work on machine translation has shown a large enough vocabulary is necessary to achieve good accuracy (Jean et al., 2015). In addition, some sampling techniques for vocabulary selection are proposed to balance the computational cost of conversion (Zhou et al., 2016; Wu et al., 2018). As IMEs work, users inputting style may change from time to time, let alone diverse user may input quite diverse contents, which makes a predefined fixed vocabulary can never be sufficient. For a convenient solution, most commercial IMEs have to manually update their vocabulary on schedule. Moreover, the training for word-based language model is especially difficult for rare words, which appear sparsely in the corpus but generally t"
P19-1154,I13-1170,1,0.805811,"rwardly produced by the conversion proposed by (Yang et al., 2012). Contrast to the style of the People’s Daily, the TouchPal corpus (Zhang et al., 2017) is a large scale of user chat history collected by TouchPal IME, which are more colloquial. Hence, we use the latter to simulate user’s chatting input to verify our online model’s adaptability to different environments. The test set size is 2,000 MIUs in both corpora. Table 2 shows the statistics of two corpora1 . Two metrics are used for our evaluation by following previous work: Maximum Input Unit (MIU) Accuracy and KeyStroke Score (KySS) (Jia and Zhao, 2013). The former measures the con1 The two corpora along with our codes are available at https://github.com/cooelf/OpenIME . version accuracy of MIU, which is defined as the longest uninterrupted Chinese character sequence inside a sentence. As the P2C conversion aims to output a rank list of corresponding character sequences candidates, the top-K MIU accuracy means the possibility of hitting the target in the first K predict items. We will follow the definition of (Zhang et al., 2017) about top-K accuracy. The KySS quantifies user experience by using keystroke count. An IME with higher KySS is su"
P19-1154,P14-1142,1,0.75865,"sentence). Actually, such a translation in P2C procedure is even more straightforward and simple by considering that the target Chinese character sequence keeps the same order as the source pinyin sequence, which means that we can decode the target sentence from left to right without any reordering. Meanwhile, there exists a well-known challenge in P2C procedure, too much ambiguity mapping pinyin syllable to character. In fact, there are only about 500 pinyin syllables corresponding to ten thousands of Chinese characters, even though the amount of the commonest characters is more than 6,000 (Jia and Zhao, 2014). As well known, the homophone and the polyphone are quite common in the Chinese language. Thus one pinyin may correspond to ten or more Chinese characters on the average. However, pinyin IME may benefit from decoding longer pinyin sequence for more efficient inputting. When a given pinyin sequence becomes longer, the list of the corresponding legal character sequences will significantly reduce. For example, IME being aware of that pinyin sequence bei jing can be only converted to either 背景(background) or 北京(Beijing) will greatly help it make the right and more efficient P2C decoding, as both"
P19-1154,P08-1103,0,0.0513371,"e conversion. Chen and Lee (2000) used a trigram language model and incorporated word segmentation to convert pinyin sequence to Chinese word sequence. Xiao et al. (2008) proposed an iterative algorithm to discover unseen words in corpus for building a Chinese language model. Mori et al. (2006) described a method enlarging the vocabulary which can capture the context information. For either pinyin-to-character for Chinese IMEs or kana-to-kanji for Japanese IMEs, a few language model training methods have been developed. Mori et al. (1998) proposed a probabilistic based language model for IME. Jiampojamarn et al. (2008) presented online discriminative training. Lin and Zhang (2008) proposed a statistic model using the frequent nearby set of the target word. Chen et al. (2012) used collocations and kmeans clustering to improve the n-pos model for Japanese IME. Jiang et al. (2007) put forward a PTC framework based on support vector machine. Hatori and Suzuki (2011) and Yang et al. (2012) respectively applied statistic machine translation (SMT) to Japanese pronunciation prediction and Chinese P2C tasks. Chen et al. (2015); Huang et al. (2018) regarded the P2C as a translation between two languages and solved it"
P19-1154,C18-1271,1,0.4769,"still rely on a predefined fixed vocabulary, and IME users have no chance to refine their own dictionary through a user-friendly way. Zhang et al. (2017) is mostly related to this work, which also offers an online mechanism to adaptively update user vocabulary. The key difference between their work 1585 and ours lies on that this work presents the first neural solution with online vocabulary adaptation while (Zhang et al., 2017) sticks to a traditional model for IME. Recently, neural networks have been adopted for a wide range of tasks (Li et al., 2019; Xiao et al., 2019; Zhou and Zhao, 2019; Li et al., 2018a,b). The effectiveness of neural models depends on the size of the vocabulary on the target side and previous work has shown that vocabularies of well over 50K word types are necessary to achieve good accuracy (Jean et al., 2015) (Zhou et al., 2016). Neural machine translation (NMT) systems compute the probability of the next target word given both the previously generated target words as well as the source sentence. Estimating this conditional distribution is linear in the size of the target vocabulary which can be very large for many translation tasks. Recent NMT work has adopted vocabulary"
P19-1154,D18-1262,1,0.516242,"still rely on a predefined fixed vocabulary, and IME users have no chance to refine their own dictionary through a user-friendly way. Zhang et al. (2017) is mostly related to this work, which also offers an online mechanism to adaptively update user vocabulary. The key difference between their work 1585 and ours lies on that this work presents the first neural solution with online vocabulary adaptation while (Zhang et al., 2017) sticks to a traditional model for IME. Recently, neural networks have been adopted for a wide range of tasks (Li et al., 2019; Xiao et al., 2019; Zhou and Zhao, 2019; Li et al., 2018a,b). The effectiveness of neural models depends on the size of the vocabulary on the target side and previous work has shown that vocabularies of well over 50K word types are necessary to achieve good accuracy (Jean et al., 2015) (Zhou et al., 2016). Neural machine translation (NMT) systems compute the probability of the next target word given both the previously generated target words as well as the source sentence. Estimating this conditional distribution is linear in the size of the target vocabulary which can be very large for many translation tasks. Recent NMT work has adopted vocabulary"
P19-1154,P16-1100,0,0.0259663,"l., 2017a,b, 2018; Wang et al., 2018; Chen et al., 2018). As for improved word representation in IMEs, Hatori and Suzuki (2011) solved Japanese pronunciation inference combining word-based and character-based features within SMT-style framework to handle unknown words. Neubig et al. (2013) proposed character-based SMT to handle sparsity. Okuno and Mori (2012) introduced an ensemble model of word-based and character-based models for Japanese and Chinese IMEs. All the above-mentioned methods used similar solution about character representation for various tasks. Our work takes inspiration from (Luong and Manning, 2016) and (Cai et al., 2017). The former built a novel representation method to tackle the rare word for machine translation. In detail, they used word representation network with characters as the basic input units. Cai et al. (2017) presented a greedy neural word segmenter with balanced word and character embedding inputs. In the meantime, high-frequency word embeddings are attached to character embedding via average pooling while low-frequency words are computed from character embedding. Our embeddings also contain different granularity levels of embedding, but the word vocabulary is capable of"
P19-1154,D15-1166,0,0.0681889,"Missing"
P19-1154,P06-1092,0,0.0471028,"ery time the user makes a selection contradicted the prediction given by the P2C conversion module, the modRelated Work To effectively utilize words for IMEs, many natural language processing (NLP) techniques have been applied. Chen (2003) introduced a joint maximum n-gram model with syllabification for grapheme-to-phoneme conversion. Chen and Lee (2000) used a trigram language model and incorporated word segmentation to convert pinyin sequence to Chinese word sequence. Xiao et al. (2008) proposed an iterative algorithm to discover unseen words in corpus for building a Chinese language model. Mori et al. (2006) described a method enlarging the vocabulary which can capture the context information. For either pinyin-to-character for Chinese IMEs or kana-to-kanji for Japanese IMEs, a few language model training methods have been developed. Mori et al. (1998) proposed a probabilistic based language model for IME. Jiampojamarn et al. (2008) presented online discriminative training. Lin and Zhang (2008) proposed a statistic model using the frequent nearby set of the target word. Chen et al. (2012) used collocations and kmeans clustering to improve the n-pos model for Japanese IME. Jiang et al. (2007) put"
P19-1154,W12-4802,0,0.0210335,"e effects of word representation on various NLP tasks, such as language modeling (Verwimp et al., 2017), question answering (Zhang and Zhao, 2018; Zhang et al., 2018b), dialogue systems (Zhang et al., 2018c; Zhu et al., 2018) and machine translation (Wang et al., 2017a,b, 2018; Wang et al., 2018; Chen et al., 2018). As for improved word representation in IMEs, Hatori and Suzuki (2011) solved Japanese pronunciation inference combining word-based and character-based features within SMT-style framework to handle unknown words. Neubig et al. (2013) proposed character-based SMT to handle sparsity. Okuno and Mori (2012) introduced an ensemble model of word-based and character-based models for Japanese and Chinese IMEs. All the above-mentioned methods used similar solution about character representation for various tasks. Our work takes inspiration from (Luong and Manning, 2016) and (Cai et al., 2017). The former built a novel representation method to tackle the rare word for machine translation. In detail, they used word representation network with characters as the basic input units. Cai et al. (2017) presented a greedy neural word segmenter with balanced word and character embedding inputs. In the meantime"
P19-1154,E17-1040,0,0.050303,"Missing"
P19-1154,P17-2089,0,0.0613591,"Missing"
P19-1154,C18-1269,0,0.0195313,"lary which can be very large for many translation tasks. Recent NMT work has adopted vocabulary selection techniques from language modeling which do not directly generate the vocabulary from all the source sentences (L’Hostis et al., 2016; Wu et al., 2018). The latest studies on deep neural network prove the demonstrable effects of word representation on various NLP tasks, such as language modeling (Verwimp et al., 2017), question answering (Zhang and Zhao, 2018; Zhang et al., 2018b), dialogue systems (Zhang et al., 2018c; Zhu et al., 2018) and machine translation (Wang et al., 2017a,b, 2018; Wang et al., 2018; Chen et al., 2018). As for improved word representation in IMEs, Hatori and Suzuki (2011) solved Japanese pronunciation inference combining word-based and character-based features within SMT-style framework to handle unknown words. Neubig et al. (2013) proposed character-based SMT to handle sparsity. Okuno and Mori (2012) introduced an ensemble model of word-based and character-based models for Japanese and Chinese IMEs. All the above-mentioned methods used similar solution about character representation for various tasks. Our work takes inspiration from (Luong and Manning, 2016) and (Cai et"
P19-1154,D17-1155,0,0.0587536,"Missing"
P19-1154,P18-2048,0,0.0196588,"lary which can be very large for many translation tasks. Recent NMT work has adopted vocabulary selection techniques from language modeling which do not directly generate the vocabulary from all the source sentences (L’Hostis et al., 2016; Wu et al., 2018). The latest studies on deep neural network prove the demonstrable effects of word representation on various NLP tasks, such as language modeling (Verwimp et al., 2017), question answering (Zhang and Zhao, 2018; Zhang et al., 2018b), dialogue systems (Zhang et al., 2018c; Zhu et al., 2018) and machine translation (Wang et al., 2017a,b, 2018; Wang et al., 2018; Chen et al., 2018). As for improved word representation in IMEs, Hatori and Suzuki (2011) solved Japanese pronunciation inference combining word-based and character-based features within SMT-style framework to handle unknown words. Neubig et al. (2013) proposed character-based SMT to handle sparsity. Okuno and Mori (2012) introduced an ensemble model of word-based and character-based models for Japanese and Chinese IMEs. All the above-mentioned methods used similar solution about character representation for various tasks. Our work takes inspiration from (Luong and Manning, 2016) and (Cai et"
P19-1154,P19-1298,1,0.706524,"All the above-mentioned work, however, still rely on a predefined fixed vocabulary, and IME users have no chance to refine their own dictionary through a user-friendly way. Zhang et al. (2017) is mostly related to this work, which also offers an online mechanism to adaptively update user vocabulary. The key difference between their work 1585 and ours lies on that this work presents the first neural solution with online vocabulary adaptation while (Zhang et al., 2017) sticks to a traditional model for IME. Recently, neural networks have been adopted for a wide range of tasks (Li et al., 2019; Xiao et al., 2019; Zhou and Zhao, 2019; Li et al., 2018a,b). The effectiveness of neural models depends on the size of the vocabulary on the target side and previous work has shown that vocabularies of well over 50K word types are necessary to achieve good accuracy (Jean et al., 2015) (Zhou et al., 2016). Neural machine translation (NMT) systems compute the probability of the next target word given both the previously generated target words as well as the source sentence. Estimating this conditional distribution is linear in the size of the target vocabulary which can be very large for many translation tasks."
P19-1154,C18-1153,1,0.939061,"online updated vocabulary with a sampling mechanism to support open vocabulary learning during IME working. Our experiments show that the proposed method outperforms commercial IMEs and state-of-theart traditional models on standard corpus and true inputting history dataset in terms of multiple metrics and thus the online updated vocabulary indeed helps our IME effectively follows user inputting behavior. 1 Introduction Chinese may use different Chinese characters up to 20,000 so that it is non-trivial to type the Chinese character directly from a Latin-style keyboard which only has 26 keys (Zhang et al., 2018a). The pinyin as the official romanization representation for Chinese provides a solution that maps Chinese character to a string of Latin alphabets so that each character has a letter writing form of its own and users can type pinyin in terms of Latin letters to input Chinese characters into a computer. Therefore, converting pinyin to Chinese characters is the most basic module of all pinyinbased IMEs. As each Chinese character may be mapped to a pinyin syllable, it is natural to regard the Pinyinto-Character (P2C) conversion as a machine trans∗ Corresponding author. This paper was partially"
P19-1154,C18-1317,1,0.746105,"online updated vocabulary with a sampling mechanism to support open vocabulary learning during IME working. Our experiments show that the proposed method outperforms commercial IMEs and state-of-theart traditional models on standard corpus and true inputting history dataset in terms of multiple metrics and thus the online updated vocabulary indeed helps our IME effectively follows user inputting behavior. 1 Introduction Chinese may use different Chinese characters up to 20,000 so that it is non-trivial to type the Chinese character directly from a Latin-style keyboard which only has 26 keys (Zhang et al., 2018a). The pinyin as the official romanization representation for Chinese provides a solution that maps Chinese character to a string of Latin alphabets so that each character has a letter writing form of its own and users can type pinyin in terms of Latin letters to input Chinese characters into a computer. Therefore, converting pinyin to Chinese characters is the most basic module of all pinyinbased IMEs. As each Chinese character may be mapped to a pinyin syllable, it is natural to regard the Pinyinto-Character (P2C) conversion as a machine trans∗ Corresponding author. This paper was partially"
P19-1154,C18-1038,1,0.749968,"the previously generated target words as well as the source sentence. Estimating this conditional distribution is linear in the size of the target vocabulary which can be very large for many translation tasks. Recent NMT work has adopted vocabulary selection techniques from language modeling which do not directly generate the vocabulary from all the source sentences (L’Hostis et al., 2016; Wu et al., 2018). The latest studies on deep neural network prove the demonstrable effects of word representation on various NLP tasks, such as language modeling (Verwimp et al., 2017), question answering (Zhang and Zhao, 2018; Zhang et al., 2018b), dialogue systems (Zhang et al., 2018c; Zhu et al., 2018) and machine translation (Wang et al., 2017a,b, 2018; Wang et al., 2018; Chen et al., 2018). As for improved word representation in IMEs, Hatori and Suzuki (2011) solved Japanese pronunciation inference combining word-based and character-based features within SMT-style framework to handle unknown words. Neubig et al. (2013) proposed character-based SMT to handle sparsity. Okuno and Mori (2012) introduced an ensemble model of word-based and character-based models for Japanese and Chinese IMEs. All the above-mentione"
P19-1154,W06-0127,1,0.58935,"model is designed to rank the characters in an appropriate order. Here is the model setting we used: a) pretrained word embeddings were generated on the People’s Daily corpus; b) the recurrent neural networks for encoder and decoder have 3 layers and 500 cells, and the representation networks have 1 layer; c) the initial learning rate is 1.0, and we will halve the learning rate every epoch after 9 epochs; d) dropout is 0.3; e) the default frequency filter ratio for CWE establishment is 0.9. The same setting is applied to all models. For a balanced treatment over both corpora, we used baseSeg (Zhao et al., 2006) to segment all text, then extract all resulted words into the ini# MIUs PD # Word # Vocab # Target Vocab (train) # Target Vocab (dec) # MIUs TP # Word # Vocab # Target Vocab (train) # Target Vocab (dec) Chinese Pinyin 5.04M 24.7M 24.7M 54.3K 41.1K 2309 2168 689.6K 4.1M 4.1M 27.7K 20.2K 2020 2009 - Table 2: MIUs count, word count and vocab size statistics of our training data. PD refers to the People’s Daily, TP is TouchPal corpus. 1589 System Existing P2C Google IME OMWA On-OMWA Our P2C Base P2C On-P2C On-P2C (bi) On-P2C (bi) On-P2C (bi) On-P2C (bi) ED 200 200 200 300 400 500 Top1 PD Top5 Top"
P19-1154,Q16-1027,0,0.117904,"and 6. Section 7 concludes this paper. ni 你 你 2 Table 1: The shorter the pinyin sequence is, the more character sequences will be mapped. from the mapping ambiguity. However, the effect of the work in P2C will be undermined with quite restricted vocabularies. The efficiency of IME conversion depends on the sufficiency of the vocabulary and previous work on machine translation has shown a large enough vocabulary is necessary to achieve good accuracy (Jean et al., 2015). In addition, some sampling techniques for vocabulary selection are proposed to balance the computational cost of conversion (Zhou et al., 2016; Wu et al., 2018). As IMEs work, users inputting style may change from time to time, let alone diverse user may input quite diverse contents, which makes a predefined fixed vocabulary can never be sufficient. For a convenient solution, most commercial IMEs have to manually update their vocabulary on schedule. Moreover, the training for word-based language model is especially difficult for rare words, which appear sparsely in the corpus but generally take up a large share of the dictionary. To well handle the open vocabulary learning problem in IME, in this work, we introduce an online sequenc"
P19-1154,P19-1230,1,0.576175,"ioned work, however, still rely on a predefined fixed vocabulary, and IME users have no chance to refine their own dictionary through a user-friendly way. Zhang et al. (2017) is mostly related to this work, which also offers an online mechanism to adaptively update user vocabulary. The key difference between their work 1585 and ours lies on that this work presents the first neural solution with online vocabulary adaptation while (Zhang et al., 2017) sticks to a traditional model for IME. Recently, neural networks have been adopted for a wide range of tasks (Li et al., 2019; Xiao et al., 2019; Zhou and Zhao, 2019; Li et al., 2018a,b). The effectiveness of neural models depends on the size of the vocabulary on the target side and previous work has shown that vocabularies of well over 50K word types are necessary to achieve good accuracy (Jean et al., 2015) (Zhou et al., 2016). Neural machine translation (NMT) systems compute the probability of the next target word given both the previously generated target words as well as the source sentence. Estimating this conditional distribution is linear in the size of the target vocabulary which can be very large for many translation tasks. Recent NMT work has a"
P19-1154,C18-2024,1,0.773722,"his conditional distribution is linear in the size of the target vocabulary which can be very large for many translation tasks. Recent NMT work has adopted vocabulary selection techniques from language modeling which do not directly generate the vocabulary from all the source sentences (L’Hostis et al., 2016; Wu et al., 2018). The latest studies on deep neural network prove the demonstrable effects of word representation on various NLP tasks, such as language modeling (Verwimp et al., 2017), question answering (Zhang and Zhao, 2018; Zhang et al., 2018b), dialogue systems (Zhang et al., 2018c; Zhu et al., 2018) and machine translation (Wang et al., 2017a,b, 2018; Wang et al., 2018; Chen et al., 2018). As for improved word representation in IMEs, Hatori and Suzuki (2011) solved Japanese pronunciation inference combining word-based and character-based features within SMT-style framework to handle unknown words. Neubig et al. (2013) proposed character-based SMT to handle sparsity. Okuno and Mori (2012) introduced an ensemble model of word-based and character-based models for Japanese and Chinese IMEs. All the above-mentioned methods used similar solution about character representation for various tasks"
P19-1230,P16-1231,0,0.0651718,"Missing"
P19-1230,C18-1233,1,0.808067,"s are forced to choose to either compromise the linguistic preciseness or to accept the low coverage in parsing. Previous works of HPSG approximation focus on two major approaches: grammar based approach (Kiefer and Krieger, 2004), and the corpus-driven approach (Krieger, 2007) and (Zhang and Krieger, 2011) which proposes PCFG approximation as a way to alleviate some of these issues in HPSG processing. Recently, with the impressive success of deep neural networks in a wide range of NLP tasks (Li et al., 2018b; Zhang et al., 2018a; Li et al., 2018c; Zhang et al., 2018c,b; Zhang and Zhao, 2018; Cai et al., 2018; He et al., 2018; Xiao et al., 2019; Chen et al., 2018; Wang et al., 2018; Wang et al., 2018a, 2017b,a), constituent and dependency parsing have been well developed with neural network. These models attain state-of-the-art results for dependency parsing (Chen and Manning, 2014; Dozat and Manning, 2017; Ma et al., 2018) and constituent parsing (Dyer et al., 2016; Cross and Huang, 2016; Kitaev and Klein, 2018a). Conclusions This paper presents a simplified HPSG with two different decode methods which are evaluated on both constituent and dependency parsing. Despite the usefulness of HPSG in pra"
P19-1230,A00-2018,0,0.531649,"Kitaev and Klein (2018b) Our (Division) Our (Joint) Our (Division*) Our (Joint*) LR LP 85.9 85.2 86.6 86.4 87.1 91.55 91.14 92.03 90.07 90.91 87.5 91.96 93.09 92.33 91.68 91.16 F1 83.2 84.6 85.5 86.1 86.5 87.0 87.3 91.75 92.10 92.18 90.87 91.03 Table 6: Constituent parsing on CTB test set. * represents CTB dependency data splitting. Since constituent and dependency share a lot of grammar and machine learning characteristics, it is a natural idea to study the relationship between constituent and dependency structures, and the joint learning of constituent and dependency parsing (Collins, 1997; Charniak, 2000; Charniak and Johnson, 2005; Farkas et al., 2011; Green and ˇ Zabokrtsk´ y, 2012; Ren et al., 2013; Yoshikawa et al., 2017). To further exploit both strengths of the two representation forms, in this work, for the first time, we propose a graph-based parsing model that formulates constituent and dependency structures as simplified HPSG. 6 framework. In the past decade, there was a lot of largescale HPSG-based NLP parsing systems which had been built. Such as the Enju English Chinese parser (Miyao et al., 2004; Yu et al., 2010), the Alpino parser for Dutch (Van Noord et al., 2006), and the LKB"
P19-1230,P05-1022,0,0.337193,"(2018b) Our (Division) Our (Joint) Our (Division*) Our (Joint*) LR LP 85.9 85.2 86.6 86.4 87.1 91.55 91.14 92.03 90.07 90.91 87.5 91.96 93.09 92.33 91.68 91.16 F1 83.2 84.6 85.5 86.1 86.5 87.0 87.3 91.75 92.10 92.18 90.87 91.03 Table 6: Constituent parsing on CTB test set. * represents CTB dependency data splitting. Since constituent and dependency share a lot of grammar and machine learning characteristics, it is a natural idea to study the relationship between constituent and dependency structures, and the joint learning of constituent and dependency parsing (Collins, 1997; Charniak, 2000; Charniak and Johnson, 2005; Farkas et al., 2011; Green and ˇ Zabokrtsk´ y, 2012; Ren et al., 2013; Yoshikawa et al., 2017). To further exploit both strengths of the two representation forms, in this work, for the first time, we propose a graph-based parsing model that formulates constituent and dependency structures as simplified HPSG. 6 framework. In the past decade, there was a lot of largescale HPSG-based NLP parsing systems which had been built. Such as the Enju English Chinese parser (Miyao et al., 2004; Yu et al., 2010), the Alpino parser for Dutch (Van Noord et al., 2006), and the LKB & PET (Copestake, 2002; Cal"
P19-1230,D14-1082,0,0.0241891,"2007) and (Zhang and Krieger, 2011) which proposes PCFG approximation as a way to alleviate some of these issues in HPSG processing. Recently, with the impressive success of deep neural networks in a wide range of NLP tasks (Li et al., 2018b; Zhang et al., 2018a; Li et al., 2018c; Zhang et al., 2018c,b; Zhang and Zhao, 2018; Cai et al., 2018; He et al., 2018; Xiao et al., 2019; Chen et al., 2018; Wang et al., 2018; Wang et al., 2018a, 2017b,a), constituent and dependency parsing have been well developed with neural network. These models attain state-of-the-art results for dependency parsing (Chen and Manning, 2014; Dozat and Manning, 2017; Ma et al., 2018) and constituent parsing (Dyer et al., 2016; Cross and Huang, 2016; Kitaev and Klein, 2018a). Conclusions This paper presents a simplified HPSG with two different decode methods which are evaluated on both constituent and dependency parsing. Despite the usefulness of HPSG in practice and its theoretical linguistic background, our model achieves new state-of-the-art results on both Chinese and English benchmark treebanks of both parsing tasks. Thus, this work is more than proposing a high-performance parsing model by exploring the relation between cons"
P19-1230,P81-1022,0,0.72265,"Missing"
P19-1230,N16-1024,0,0.0816096,"e some of these issues in HPSG processing. Recently, with the impressive success of deep neural networks in a wide range of NLP tasks (Li et al., 2018b; Zhang et al., 2018a; Li et al., 2018c; Zhang et al., 2018c,b; Zhang and Zhao, 2018; Cai et al., 2018; He et al., 2018; Xiao et al., 2019; Chen et al., 2018; Wang et al., 2018; Wang et al., 2018a, 2017b,a), constituent and dependency parsing have been well developed with neural network. These models attain state-of-the-art results for dependency parsing (Chen and Manning, 2014; Dozat and Manning, 2017; Ma et al., 2018) and constituent parsing (Dyer et al., 2016; Cross and Huang, 2016; Kitaev and Klein, 2018a). Conclusions This paper presents a simplified HPSG with two different decode methods which are evaluated on both constituent and dependency parsing. Despite the usefulness of HPSG in practice and its theoretical linguistic background, our model achieves new state-of-the-art results on both Chinese and English benchmark treebanks of both parsing tasks. Thus, this work is more than proposing a high-performance parsing model by exploring the relation between constituent and dependency structures. Our experiments show that joint learning of constit"
P19-1230,P96-1011,0,0.352564,"et al., 2018), we report the results without punctuations for both treebanks. 4.1 1≤h≤n child-parent pair (xi , hi ). To predict span and dependency scores simultaneously, we jointly train our parser for minimizing the overall loss: JJoint (θ) = J1 (θ) + J2 (θ). During testing, we propose a CKY-style algorithm as shown in Algorithm 1 to explicitly find the globally highest span and dependency score SH (T ) of our simplified HPSG tree T . In order to binarize the constituent parse tree with head, we introduce the complete span sc and the incomplete span si which is similar to Eisner algorithm (Eisner, 1996). After finding the best score SH (T ), we backtrack the chart with split point k and sub-root r to construct the simplified HPSG tree T . Comparing with constituent parsing CKY-style algorithm (Stern et al., 2017a), the dependency score d(r, h) in our algorithm affects the selection of best split point k. Since we need to find the best value of sub-head r and split point k, the complexity of the algorithm is O(n5 ) time and O(n3 ) Setup Hyperparameters In our experiments, we use 100D GloVe (Pennington et al., 2014) and structured-skipgram (Ling et al., 2015) pre-train embeddings for English a"
P19-1230,W11-2924,0,0.351816,"(Joint) Our (Division*) Our (Joint*) LR LP 85.9 85.2 86.6 86.4 87.1 91.55 91.14 92.03 90.07 90.91 87.5 91.96 93.09 92.33 91.68 91.16 F1 83.2 84.6 85.5 86.1 86.5 87.0 87.3 91.75 92.10 92.18 90.87 91.03 Table 6: Constituent parsing on CTB test set. * represents CTB dependency data splitting. Since constituent and dependency share a lot of grammar and machine learning characteristics, it is a natural idea to study the relationship between constituent and dependency structures, and the joint learning of constituent and dependency parsing (Collins, 1997; Charniak, 2000; Charniak and Johnson, 2005; Farkas et al., 2011; Green and ˇ Zabokrtsk´ y, 2012; Ren et al., 2013; Yoshikawa et al., 2017). To further exploit both strengths of the two representation forms, in this work, for the first time, we propose a graph-based parsing model that formulates constituent and dependency structures as simplified HPSG. 6 framework. In the past decade, there was a lot of largescale HPSG-based NLP parsing systems which had been built. Such as the Enju English Chinese parser (Miyao et al., 2004; Yu et al., 2010), the Alpino parser for Dutch (Van Noord et al., 2006), and the LKB & PET (Copestake, 2002; Callmeier, 2000) for Eng"
P19-1230,D16-1238,0,0.115123,"Missing"
P19-1230,P18-2075,0,0.125643,"Missing"
P19-1230,D16-1257,0,0.127614,"Missing"
P19-1230,P17-2025,0,0.0764187,"Missing"
P19-1230,P97-1003,0,0.775073,"d Zhang (2018) Kitaev and Klein (2018b) Our (Division) Our (Joint) Our (Division*) Our (Joint*) LR LP 85.9 85.2 86.6 86.4 87.1 91.55 91.14 92.03 90.07 90.91 87.5 91.96 93.09 92.33 91.68 91.16 F1 83.2 84.6 85.5 86.1 86.5 87.0 87.3 91.75 92.10 92.18 90.87 91.03 Table 6: Constituent parsing on CTB test set. * represents CTB dependency data splitting. Since constituent and dependency share a lot of grammar and machine learning characteristics, it is a natural idea to study the relationship between constituent and dependency structures, and the joint learning of constituent and dependency parsing (Collins, 1997; Charniak, 2000; Charniak and Johnson, 2005; Farkas et al., 2011; Green and ˇ Zabokrtsk´ y, 2012; Ren et al., 2013; Yoshikawa et al., 2017). To further exploit both strengths of the two representation forms, in this work, for the first time, we propose a graph-based parsing model that formulates constituent and dependency structures as simplified HPSG. 6 framework. In the past decade, there was a lot of largescale HPSG-based NLP parsing systems which had been built. Such as the Enju English Chinese parser (Miyao et al., 2004; Yu et al., 2010), the Alpino parser for Dutch (Van Noord et al., 20"
P19-1230,D16-1001,0,0.347846,", we can merely generate constituent or dependency parsing tree by setting λ to 1 or 0, respectively. 4 Experiments In order to evaluate the proposed model, we convert our simplified HPSG tree to constituent and dependency parse trees and evaluate on two benchmark treebanks, English Penn Treebank (PTB) and Chinese Penn Treebank (CTB5.1) following standard data splitting (Zhang and Clark, 2008; Liu and Zhang, 2017b). The placeholders with the -NONE- tag are stripped from the CTB. POS tags are predicted using the Stanford tagger (Toutanova et al., 2003) and we use the same pretagged dataset as (Cross and Huang, 2016). For constituent parsing, we use the standard evalb8 tool to evaluate the F1 score. For dependency parsing, following (Dozat and Manning, 2017; Kuncoro et al., 2016; Ma et al., 2018), we report the results without punctuations for both treebanks. 4.1 1≤h≤n child-parent pair (xi , hi ). To predict span and dependency scores simultaneously, we jointly train our parser for minimizing the overall loss: JJoint (θ) = J1 (θ) + J2 (θ). During testing, we propose a CKY-style algorithm as shown in Algorithm 1 to explicitly find the globally highest span and dependency score SH (T ) of our simplified HP"
P19-1230,de-marneffe-etal-2006-generating,0,0.408453,"Missing"
P19-1230,N18-1091,0,0.30209,"Missing"
P19-1230,W12-0503,0,0.139102,"Missing"
P19-1230,P18-1192,1,0.761753,"oose to either compromise the linguistic preciseness or to accept the low coverage in parsing. Previous works of HPSG approximation focus on two major approaches: grammar based approach (Kiefer and Krieger, 2004), and the corpus-driven approach (Krieger, 2007) and (Zhang and Krieger, 2011) which proposes PCFG approximation as a way to alleviate some of these issues in HPSG processing. Recently, with the impressive success of deep neural networks in a wide range of NLP tasks (Li et al., 2018b; Zhang et al., 2018a; Li et al., 2018c; Zhang et al., 2018c,b; Zhang and Zhao, 2018; Cai et al., 2018; He et al., 2018; Xiao et al., 2019; Chen et al., 2018; Wang et al., 2018; Wang et al., 2018a, 2017b,a), constituent and dependency parsing have been well developed with neural network. These models attain state-of-the-art results for dependency parsing (Chen and Manning, 2014; Dozat and Manning, 2017; Ma et al., 2018) and constituent parsing (Dyer et al., 2016; Cross and Huang, 2016; Kitaev and Klein, 2018a). Conclusions This paper presents a simplified HPSG with two different decode methods which are evaluated on both constituent and dependency parsing. Despite the usefulness of HPSG in practice and its the"
P19-1230,W07-2416,0,0.140478,"Missing"
P19-1230,Q17-1029,0,0.0876865,"d(h, root) } space. To control the effect of combining span and dependency scores, we apply a weight λ: s(i, j, `) = λScateg (i, j, `), d(i, j) = (1.0−λ)αij , where λ in the range of 0 to 1. In addition, we can merely generate constituent or dependency parsing tree by setting λ to 1 or 0, respectively. 4 Experiments In order to evaluate the proposed model, we convert our simplified HPSG tree to constituent and dependency parse trees and evaluate on two benchmark treebanks, English Penn Treebank (PTB) and Chinese Penn Treebank (CTB5.1) following standard data splitting (Zhang and Clark, 2008; Liu and Zhang, 2017b). The placeholders with the -NONE- tag are stripped from the CTB. POS tags are predicted using the Stanford tagger (Toutanova et al., 2003) and we use the same pretagged dataset as (Cross and Huang, 2016). For constituent parsing, we use the standard evalb8 tool to evaluate the F1 score. For dependency parsing, following (Dozat and Manning, 2017; Kuncoro et al., 2016; Ma et al., 2018), we report the results without punctuations for both treebanks. 4.1 1≤h≤n child-parent pair (xi , hi ). To predict span and dependency scores simultaneously, we jointly train our parser for minimizing the overa"
P19-1230,Q17-1004,0,0.11014,"d(h, root) } space. To control the effect of combining span and dependency scores, we apply a weight λ: s(i, j, `) = λScateg (i, j, `), d(i, j) = (1.0−λ)αij , where λ in the range of 0 to 1. In addition, we can merely generate constituent or dependency parsing tree by setting λ to 1 or 0, respectively. 4 Experiments In order to evaluate the proposed model, we convert our simplified HPSG tree to constituent and dependency parse trees and evaluate on two benchmark treebanks, English Penn Treebank (PTB) and Chinese Penn Treebank (CTB5.1) following standard data splitting (Zhang and Clark, 2008; Liu and Zhang, 2017b). The placeholders with the -NONE- tag are stripped from the CTB. POS tags are predicted using the Stanford tagger (Toutanova et al., 2003) and we use the same pretagged dataset as (Cross and Huang, 2016). For constituent parsing, we use the standard evalb8 tool to evaluate the F1 score. For dependency parsing, following (Dozat and Manning, 2017; Kuncoro et al., 2016; Ma et al., 2018), we report the results without punctuations for both treebanks. 4.1 1≤h≤n child-parent pair (xi , hi ). To predict span and dependency scores simultaneously, we jointly train our parser for minimizing the overa"
P19-1230,P18-1249,0,0.281062,"est predicted scores. The difference is that for division span structure, we only need span scores while for joint span structure, we need both of span and dependency scores. Given a sentence s = {w1 , w2 , . . . , wn }, we attempt to predict a simplified HPSG tree. As shown in Figure 4, our parsing model includes four modules: token representation, self-attention encoder, scoring module and CKY-style decoder7 . 3.2 Token Representation In our model, token representation xi is composed of character, word and part-of-speech (POS) embeddings. For character-level representation, we use CharLSTM (Kitaev and Klein, 2018a). For word-level representation, we concatenate randomly initialized and pre-trained word embeddings. Finally, we concatenate character representation, word representation and POS embedding as our token representation: xi = [xchar ; xword ; xP OS ]. Our Model Overview 7 Using an encoder-decoder backbone, our model apply self-attention encoder (Vaswani et al., 2017) which is modified by position partition (Kitaev For dependency label of each word, it is not necessary for our HPSG parsing purpose, however, to enable our parser fully comparable to existing dependency parsers, we still train a s"
P19-1230,I17-1007,0,0.135407,"Missing"
P19-1230,E17-1117,0,0.0591692,"Missing"
P19-1230,D16-1180,0,0.0519607,"our simplified HPSG tree to constituent and dependency parse trees and evaluate on two benchmark treebanks, English Penn Treebank (PTB) and Chinese Penn Treebank (CTB5.1) following standard data splitting (Zhang and Clark, 2008; Liu and Zhang, 2017b). The placeholders with the -NONE- tag are stripped from the CTB. POS tags are predicted using the Stanford tagger (Toutanova et al., 2003) and we use the same pretagged dataset as (Cross and Huang, 2016). For constituent parsing, we use the standard evalb8 tool to evaluate the F1 score. For dependency parsing, following (Dozat and Manning, 2017; Kuncoro et al., 2016; Ma et al., 2018), we report the results without punctuations for both treebanks. 4.1 1≤h≤n child-parent pair (xi , hi ). To predict span and dependency scores simultaneously, we jointly train our parser for minimizing the overall loss: JJoint (θ) = J1 (θ) + J2 (θ). During testing, we propose a CKY-style algorithm as shown in Algorithm 1 to explicitly find the globally highest span and dependency score SH (T ) of our simplified HPSG tree T . In order to binarize the constituent parse tree with head, we introduce the complete span sc and the incomplete span si which is similar to Eisner algori"
P19-1230,C18-1271,1,0.7969,"Consequently, according to (Zhang and Krieger, 2011), many of these large-scale grammar implementations are forced to choose to either compromise the linguistic preciseness or to accept the low coverage in parsing. Previous works of HPSG approximation focus on two major approaches: grammar based approach (Kiefer and Krieger, 2004), and the corpus-driven approach (Krieger, 2007) and (Zhang and Krieger, 2011) which proposes PCFG approximation as a way to alleviate some of these issues in HPSG processing. Recently, with the impressive success of deep neural networks in a wide range of NLP tasks (Li et al., 2018b; Zhang et al., 2018a; Li et al., 2018c; Zhang et al., 2018c,b; Zhang and Zhao, 2018; Cai et al., 2018; He et al., 2018; Xiao et al., 2019; Chen et al., 2018; Wang et al., 2018; Wang et al., 2018a, 2017b,a), constituent and dependency parsing have been well developed with neural network. These models attain state-of-the-art results for dependency parsing (Chen and Manning, 2014; Dozat and Manning, 2017; Ma et al., 2018) and constituent parsing (Dyer et al., 2016; Cross and Huang, 2016; Kitaev and Klein, 2018a). Conclusions This paper presents a simplified HPSG with two different decode method"
P19-1230,D18-1262,1,0.703619,"Consequently, according to (Zhang and Krieger, 2011), many of these large-scale grammar implementations are forced to choose to either compromise the linguistic preciseness or to accept the low coverage in parsing. Previous works of HPSG approximation focus on two major approaches: grammar based approach (Kiefer and Krieger, 2004), and the corpus-driven approach (Krieger, 2007) and (Zhang and Krieger, 2011) which proposes PCFG approximation as a way to alleviate some of these issues in HPSG processing. Recently, with the impressive success of deep neural networks in a wide range of NLP tasks (Li et al., 2018b; Zhang et al., 2018a; Li et al., 2018c; Zhang et al., 2018c,b; Zhang and Zhao, 2018; Cai et al., 2018; He et al., 2018; Xiao et al., 2019; Chen et al., 2018; Wang et al., 2018; Wang et al., 2018a, 2017b,a), constituent and dependency parsing have been well developed with neural network. These models attain state-of-the-art results for dependency parsing (Chen and Manning, 2014; Dozat and Manning, 2017; Ma et al., 2018) and constituent parsing (Dyer et al., 2016; Cross and Huang, 2016; Kitaev and Klein, 2018a). Conclusions This paper presents a simplified HPSG with two different decode method"
P19-1230,K18-2006,1,0.802276,"Consequently, according to (Zhang and Krieger, 2011), many of these large-scale grammar implementations are forced to choose to either compromise the linguistic preciseness or to accept the low coverage in parsing. Previous works of HPSG approximation focus on two major approaches: grammar based approach (Kiefer and Krieger, 2004), and the corpus-driven approach (Krieger, 2007) and (Zhang and Krieger, 2011) which proposes PCFG approximation as a way to alleviate some of these issues in HPSG processing. Recently, with the impressive success of deep neural networks in a wide range of NLP tasks (Li et al., 2018b; Zhang et al., 2018a; Li et al., 2018c; Zhang et al., 2018c,b; Zhang and Zhao, 2018; Cai et al., 2018; He et al., 2018; Xiao et al., 2019; Chen et al., 2018; Wang et al., 2018; Wang et al., 2018a, 2017b,a), constituent and dependency parsing have been well developed with neural network. These models attain state-of-the-art results for dependency parsing (Chen and Manning, 2014; Dozat and Manning, 2017; Ma et al., 2018) and constituent parsing (Dyer et al., 2016; Cross and Huang, 2016; Kitaev and Klein, 2018a). Conclusions This paper presents a simplified HPSG with two different decode method"
P19-1230,N15-1142,0,0.0404477,"i which is similar to Eisner algorithm (Eisner, 1996). After finding the best score SH (T ), we backtrack the chart with split point k and sub-root r to construct the simplified HPSG tree T . Comparing with constituent parsing CKY-style algorithm (Stern et al., 2017a), the dependency score d(r, h) in our algorithm affects the selection of best split point k. Since we need to find the best value of sub-head r and split point k, the complexity of the algorithm is O(n5 ) time and O(n3 ) Setup Hyperparameters In our experiments, we use 100D GloVe (Pennington et al., 2014) and structured-skipgram (Ling et al., 2015) pre-train embeddings for English and Chinese respectively. The character representations are randomly initialized, and the dimension is 64. For self-attention encoder, we use the same hyperparameters settings as (Kitaev and Klein, 2018a). For span scores, we apply a hidden size of 250-dimensional feed-forward networks. For dependency biaffine scores, we employ two 1024dimensional MLP layers with the ReLU as the activation function and a 1024-dimensional parameter matrix for biaffine attention. In addition, we augment our parser with ELMo (Peters et al., 2018) and a larger version of BERT (Dev"
P19-1230,P18-1130,0,0.323431,"ree to constituent and dependency parse trees and evaluate on two benchmark treebanks, English Penn Treebank (PTB) and Chinese Penn Treebank (CTB5.1) following standard data splitting (Zhang and Clark, 2008; Liu and Zhang, 2017b). The placeholders with the -NONE- tag are stripped from the CTB. POS tags are predicted using the Stanford tagger (Toutanova et al., 2003) and we use the same pretagged dataset as (Cross and Huang, 2016). For constituent parsing, we use the standard evalb8 tool to evaluate the F1 score. For dependency parsing, following (Dozat and Manning, 2017; Kuncoro et al., 2016; Ma et al., 2018), we report the results without punctuations for both treebanks. 4.1 1≤h≤n child-parent pair (xi , hi ). To predict span and dependency scores simultaneously, we jointly train our parser for minimizing the overall loss: JJoint (θ) = J1 (θ) + J2 (θ). During testing, we propose a CKY-style algorithm as shown in Algorithm 1 to explicitly find the globally highest span and dependency score SH (T ) of our simplified HPSG tree T . In order to binarize the constituent parse tree with head, we introduce the complete span sc and the incomplete span si which is similar to Eisner algorithm (Eisner, 1996)"
P19-1230,W10-4146,1,0.730604,"ependency structure is better at indicating dependency relation among words. 2396 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2396–2408 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics Typical dependency treebanks are usually converted from constituent treebanks, though they may be independently annotated as well for the same languages. In reverse, constituent parsing can be accurately converted to dependencies representation by grammatical rules or machine learning methods (De Marneffe et al., 2006; Ma et al., 2010). Such convertibility shows a close relation between constituent and dependency representations, which also have a strong correlation with the HFP of HPSG as shown in Figure 1. Thus, it is possible to combine the two representation forms into a simplified HPSG not only for even better parsing but also for more linguistically rich representation. In this work, we exploit both strengths of the two representation forms and combine them into HPSG. To our best knowledge, it is first attempt to perform such a formulization1 . In this paper, we explore two parsing methods for the simplified HPSG pars"
P19-1230,J93-2004,0,0.0737611,"Figure 1. Thus, it is possible to combine the two representation forms into a simplified HPSG not only for even better parsing but also for more linguistically rich representation. In this work, we exploit both strengths of the two representation forms and combine them into HPSG. To our best knowledge, it is first attempt to perform such a formulization1 . In this paper, we explore two parsing methods for the simplified HPSG parse tree which contains both constituent and dependency syntactic information. Our simplified HPSG will be from the annotations or conversions of Penn Treebank (PTB)2 (Marcus et al., 1993). Thus the evaluation for our HPSG parser will also be done on both the annotated constituent and converted dependency parse trees, which let our HPSG parser compare to existing constituent and dependency parsers individually. Our experimental results show that our HPSG parser brings better prediction on both constituent and dependency tree structures. In addition, the empirical results show that our parser reaches new state-of-the-art for both parsing tasks. To sum up, we make the following contributions: • For the first time, we formulate a simplified HPSG by combining constituent and depend"
P19-1230,D14-1162,0,0.0828839,"uce the complete span sc and the incomplete span si which is similar to Eisner algorithm (Eisner, 1996). After finding the best score SH (T ), we backtrack the chart with split point k and sub-root r to construct the simplified HPSG tree T . Comparing with constituent parsing CKY-style algorithm (Stern et al., 2017a), the dependency score d(r, h) in our algorithm affects the selection of best split point k. Since we need to find the best value of sub-head r and split point k, the complexity of the algorithm is O(n5 ) time and O(n3 ) Setup Hyperparameters In our experiments, we use 100D GloVe (Pennington et al., 2014) and structured-skipgram (Ling et al., 2015) pre-train embeddings for English and Chinese respectively. The character representations are randomly initialized, and the dimension is 64. For self-attention encoder, we use the same hyperparameters settings as (Kitaev and Klein, 2018a). For span scores, we apply a hidden size of 250-dimensional feed-forward networks. For dependency biaffine scores, we employ two 1024dimensional MLP layers with the ReLU as the activation function and a 1024-dimensional parameter matrix for biaffine attention. In addition, we augment our parser with ELMo (Peters et"
P19-1230,N18-1202,0,0.0256225,"al., 2014) and structured-skipgram (Ling et al., 2015) pre-train embeddings for English and Chinese respectively. The character representations are randomly initialized, and the dimension is 64. For self-attention encoder, we use the same hyperparameters settings as (Kitaev and Klein, 2018a). For span scores, we apply a hidden size of 250-dimensional feed-forward networks. For dependency biaffine scores, we employ two 1024dimensional MLP layers with the ReLU as the activation function and a 1024-dimensional parameter matrix for biaffine attention. In addition, we augment our parser with ELMo (Peters et al., 2018) and a larger version of BERT (Devlin et al., 2018) (24 layers, 16 attention heads per layer, and 1024-dimensional hidden vectors) to compare with other pre-trained or ensemble models. We set 4 layers of self-attention for ELMo and 2 layers of self-attention for BERT as (Kitaev and Klein, 2018a,b). 2401 8 http://nlp.cs.nyu.edu/evalb/ Self-attention Layers Division Span Model 8 self-attention layers 12 self-attention layers 16 self-attention layers Joint Span Model 8 self-attention layers 12 self-attention layers 16 self-attention layers F1 UAS LAS 93.42 93.57 93.36 94.05 94.40 94.08 92.68 93.0"
P19-1230,C88-2121,0,0.481049,"Missing"
P19-1230,P18-1108,0,0.508373,"Missing"
P19-1230,P17-1076,0,0.422866,"ore as follow: αij = hTi W gj + U T hi + V T gj + b, where αij indicates the child-parent score, W denotes the weight matrix of the bi-linear term, U and V are the weight vectors of the linear term and b is the bias item, hi and gi are calculated by a distinct one-layer perceptron network. We minimize the negative log-likelihood of the golden dependency tree Y , which is implemented as a cross-entropy loss: (i,j,`)∈T J2 (θ) = − (logPθ (hi |xi ) + logPθ (li |xi , hi )) , The goal of constituent parsing is to find the tree with the highest score: Tˆ = arg maxT s(T ). We use CKY-style algorithm (Stern et al., 2017a; Gaddy et al., 2018) to obtain the tree Tˆ in O(n3 ) where Pθ (hi |xi ) is the probability of correct parent node hi for xi , and Pθ (li |xi , hi ) is the probability of the correct dependency label li for the 2400 Algorithm 1 Joint span parsing algorithm Input: sentence leng n, span and dependency score s(i, j, `), d(r, h), 1 ≤ i ≤ j ≤ n, ∀r, h, ` Output: maximum value SH (T ) of tree T Initialization: sc [i][j][h] = si [i][j][h] = 0, ∀i, j, h for len = 1 to n do for i = 1 to n − len + 1 do j = i + len − 1 if len = 1 then sc [i][j][i] = si [i][j][i] = max s(i, j, `) ` else for h = i to j do"
P19-1230,D17-1178,0,0.285611,"ore as follow: αij = hTi W gj + U T hi + V T gj + b, where αij indicates the child-parent score, W denotes the weight matrix of the bi-linear term, U and V are the weight vectors of the linear term and b is the bias item, hi and gi are calculated by a distinct one-layer perceptron network. We minimize the negative log-likelihood of the golden dependency tree Y , which is implemented as a cross-entropy loss: (i,j,`)∈T J2 (θ) = − (logPθ (hi |xi ) + logPθ (li |xi , hi )) , The goal of constituent parsing is to find the tree with the highest score: Tˆ = arg maxT s(T ). We use CKY-style algorithm (Stern et al., 2017a; Gaddy et al., 2018) to obtain the tree Tˆ in O(n3 ) where Pθ (hi |xi ) is the probability of correct parent node hi for xi , and Pθ (li |xi , hi ) is the probability of the correct dependency label li for the 2400 Algorithm 1 Joint span parsing algorithm Input: sentence leng n, span and dependency score s(i, j, `), d(r, h), 1 ≤ i ≤ j ≤ n, ∀r, h, ` Output: maximum value SH (T ) of tree T Initialization: sc [i][j][h] = si [i][j][h] = 0, ∀i, j, h for len = 1 to n do for i = 1 to n − len + 1 do j = i + len − 1 if len = 1 then sc [i][j][i] = si [i][j][i] = max s(i, j, `) ` else for h = i to j do"
P19-1230,C18-1011,0,0.214555,"Missing"
P19-1230,N03-1033,0,0.118699,"d(i, j) = (1.0−λ)αij , where λ in the range of 0 to 1. In addition, we can merely generate constituent or dependency parsing tree by setting λ to 1 or 0, respectively. 4 Experiments In order to evaluate the proposed model, we convert our simplified HPSG tree to constituent and dependency parse trees and evaluate on two benchmark treebanks, English Penn Treebank (PTB) and Chinese Penn Treebank (CTB5.1) following standard data splitting (Zhang and Clark, 2008; Liu and Zhang, 2017b). The placeholders with the -NONE- tag are stripped from the CTB. POS tags are predicted using the Stanford tagger (Toutanova et al., 2003) and we use the same pretagged dataset as (Cross and Huang, 2016). For constituent parsing, we use the standard evalb8 tool to evaluate the F1 score. For dependency parsing, following (Dozat and Manning, 2017; Kuncoro et al., 2016; Ma et al., 2018), we report the results without punctuations for both treebanks. 4.1 1≤h≤n child-parent pair (xi , hi ). To predict span and dependency scores simultaneously, we jointly train our parser for minimizing the overall loss: JJoint (θ) = J1 (θ) + J2 (θ). During testing, we propose a CKY-style algorithm as shown in Algorithm 1 to explicitly find the global"
P19-1230,2006.jeptalnrecital-invite.2,0,0.0956752,"Missing"
P19-1230,P17-2089,0,0.0883607,"Missing"
P19-1230,C18-1269,0,0.0331837,"to accept the low coverage in parsing. Previous works of HPSG approximation focus on two major approaches: grammar based approach (Kiefer and Krieger, 2004), and the corpus-driven approach (Krieger, 2007) and (Zhang and Krieger, 2011) which proposes PCFG approximation as a way to alleviate some of these issues in HPSG processing. Recently, with the impressive success of deep neural networks in a wide range of NLP tasks (Li et al., 2018b; Zhang et al., 2018a; Li et al., 2018c; Zhang et al., 2018c,b; Zhang and Zhao, 2018; Cai et al., 2018; He et al., 2018; Xiao et al., 2019; Chen et al., 2018; Wang et al., 2018; Wang et al., 2018a, 2017b,a), constituent and dependency parsing have been well developed with neural network. These models attain state-of-the-art results for dependency parsing (Chen and Manning, 2014; Dozat and Manning, 2017; Ma et al., 2018) and constituent parsing (Dyer et al., 2016; Cross and Huang, 2016; Kitaev and Klein, 2018a). Conclusions This paper presents a simplified HPSG with two different decode methods which are evaluated on both constituent and dependency parsing. Despite the usefulness of HPSG in practice and its theoretical linguistic background, our model achieves new st"
P19-1230,D17-1155,0,0.0597069,"Missing"
P19-1230,P18-2048,0,0.0485975,"Missing"
P19-1230,D18-1311,0,0.0374129,"to accept the low coverage in parsing. Previous works of HPSG approximation focus on two major approaches: grammar based approach (Kiefer and Krieger, 2004), and the corpus-driven approach (Krieger, 2007) and (Zhang and Krieger, 2011) which proposes PCFG approximation as a way to alleviate some of these issues in HPSG processing. Recently, with the impressive success of deep neural networks in a wide range of NLP tasks (Li et al., 2018b; Zhang et al., 2018a; Li et al., 2018c; Zhang et al., 2018c,b; Zhang and Zhao, 2018; Cai et al., 2018; He et al., 2018; Xiao et al., 2019; Chen et al., 2018; Wang et al., 2018; Wang et al., 2018a, 2017b,a), constituent and dependency parsing have been well developed with neural network. These models attain state-of-the-art results for dependency parsing (Chen and Manning, 2014; Dozat and Manning, 2017; Ma et al., 2018) and constituent parsing (Dyer et al., 2016; Cross and Huang, 2016; Kitaev and Klein, 2018a). Conclusions This paper presents a simplified HPSG with two different decode methods which are evaluated on both constituent and dependency parsing. Despite the usefulness of HPSG in practice and its theoretical linguistic background, our model achieves new st"
P19-1230,P15-1110,0,0.0398241,"Missing"
P19-1230,P19-1298,1,0.678772,"mpromise the linguistic preciseness or to accept the low coverage in parsing. Previous works of HPSG approximation focus on two major approaches: grammar based approach (Kiefer and Krieger, 2004), and the corpus-driven approach (Krieger, 2007) and (Zhang and Krieger, 2011) which proposes PCFG approximation as a way to alleviate some of these issues in HPSG processing. Recently, with the impressive success of deep neural networks in a wide range of NLP tasks (Li et al., 2018b; Zhang et al., 2018a; Li et al., 2018c; Zhang et al., 2018c,b; Zhang and Zhao, 2018; Cai et al., 2018; He et al., 2018; Xiao et al., 2019; Chen et al., 2018; Wang et al., 2018; Wang et al., 2018a, 2017b,a), constituent and dependency parsing have been well developed with neural network. These models attain state-of-the-art results for dependency parsing (Chen and Manning, 2014; Dozat and Manning, 2017; Ma et al., 2018) and constituent parsing (Dyer et al., 2016; Cross and Huang, 2016; Kitaev and Klein, 2018a). Conclusions This paper presents a simplified HPSG with two different decode methods which are evaluated on both constituent and dependency parsing. Despite the usefulness of HPSG in practice and its theoretical linguistic"
P19-1230,P17-1026,0,0.0588701,"55 91.14 92.03 90.07 90.91 87.5 91.96 93.09 92.33 91.68 91.16 F1 83.2 84.6 85.5 86.1 86.5 87.0 87.3 91.75 92.10 92.18 90.87 91.03 Table 6: Constituent parsing on CTB test set. * represents CTB dependency data splitting. Since constituent and dependency share a lot of grammar and machine learning characteristics, it is a natural idea to study the relationship between constituent and dependency structures, and the joint learning of constituent and dependency parsing (Collins, 1997; Charniak, 2000; Charniak and Johnson, 2005; Farkas et al., 2011; Green and ˇ Zabokrtsk´ y, 2012; Ren et al., 2013; Yoshikawa et al., 2017). To further exploit both strengths of the two representation forms, in this work, for the first time, we propose a graph-based parsing model that formulates constituent and dependency structures as simplified HPSG. 6 framework. In the past decade, there was a lot of largescale HPSG-based NLP parsing systems which had been built. Such as the Enju English Chinese parser (Miyao et al., 2004; Yu et al., 2010), the Alpino parser for Dutch (Van Noord et al., 2006), and the LKB & PET (Copestake, 2002; Callmeier, 2000) for English, German, and Japanese.. Meanwhile, since HPSG represents the grammar f"
P19-1230,C10-2162,0,0.0382104,"joint learning of constituent and dependency parsing (Collins, 1997; Charniak, 2000; Charniak and Johnson, 2005; Farkas et al., 2011; Green and ˇ Zabokrtsk´ y, 2012; Ren et al., 2013; Yoshikawa et al., 2017). To further exploit both strengths of the two representation forms, in this work, for the first time, we propose a graph-based parsing model that formulates constituent and dependency structures as simplified HPSG. 6 framework. In the past decade, there was a lot of largescale HPSG-based NLP parsing systems which had been built. Such as the Enju English Chinese parser (Miyao et al., 2004; Yu et al., 2010), the Alpino parser for Dutch (Van Noord et al., 2006), and the LKB & PET (Copestake, 2002; Callmeier, 2000) for English, German, and Japanese.. Meanwhile, since HPSG represents the grammar framework in a precisely constrained way, it is difficult to broadly cover unseen real-world texts for parsing. Consequently, according to (Zhang and Krieger, 2011), many of these large-scale grammar implementations are forced to choose to either compromise the linguistic preciseness or to accept the low coverage in parsing. Previous works of HPSG approximation focus on two major approaches: grammar based a"
P19-1230,W11-2923,0,0.0302217,"el that formulates constituent and dependency structures as simplified HPSG. 6 framework. In the past decade, there was a lot of largescale HPSG-based NLP parsing systems which had been built. Such as the Enju English Chinese parser (Miyao et al., 2004; Yu et al., 2010), the Alpino parser for Dutch (Van Noord et al., 2006), and the LKB & PET (Copestake, 2002; Callmeier, 2000) for English, German, and Japanese.. Meanwhile, since HPSG represents the grammar framework in a precisely constrained way, it is difficult to broadly cover unseen real-world texts for parsing. Consequently, according to (Zhang and Krieger, 2011), many of these large-scale grammar implementations are forced to choose to either compromise the linguistic preciseness or to accept the low coverage in parsing. Previous works of HPSG approximation focus on two major approaches: grammar based approach (Kiefer and Krieger, 2004), and the corpus-driven approach (Krieger, 2007) and (Zhang and Krieger, 2011) which proposes PCFG approximation as a way to alleviate some of these issues in HPSG processing. Recently, with the impressive success of deep neural networks in a wide range of NLP tasks (Li et al., 2018b; Zhang et al., 2018a; Li et al., 20"
P19-1230,D08-1059,0,0.0707811,"= max { sc [1][n][h] + d(h, root) } space. To control the effect of combining span and dependency scores, we apply a weight λ: s(i, j, `) = λScateg (i, j, `), d(i, j) = (1.0−λ)αij , where λ in the range of 0 to 1. In addition, we can merely generate constituent or dependency parsing tree by setting λ to 1 or 0, respectively. 4 Experiments In order to evaluate the proposed model, we convert our simplified HPSG tree to constituent and dependency parse trees and evaluate on two benchmark treebanks, English Penn Treebank (PTB) and Chinese Penn Treebank (CTB5.1) following standard data splitting (Zhang and Clark, 2008; Liu and Zhang, 2017b). The placeholders with the -NONE- tag are stripped from the CTB. POS tags are predicted using the Stanford tagger (Toutanova et al., 2003) and we use the same pretagged dataset as (Cross and Huang, 2016). For constituent parsing, we use the standard evalb8 tool to evaluate the F1 score. For dependency parsing, following (Dozat and Manning, 2017; Kuncoro et al., 2016; Ma et al., 2018), we report the results without punctuations for both treebanks. 4.1 1≤h≤n child-parent pair (xi , hi ). To predict span and dependency scores simultaneously, we jointly train our parser for"
P19-1230,D18-1511,1,0.661744,"rding to (Zhang and Krieger, 2011), many of these large-scale grammar implementations are forced to choose to either compromise the linguistic preciseness or to accept the low coverage in parsing. Previous works of HPSG approximation focus on two major approaches: grammar based approach (Kiefer and Krieger, 2004), and the corpus-driven approach (Krieger, 2007) and (Zhang and Krieger, 2011) which proposes PCFG approximation as a way to alleviate some of these issues in HPSG processing. Recently, with the impressive success of deep neural networks in a wide range of NLP tasks (Li et al., 2018b; Zhang et al., 2018a; Li et al., 2018c; Zhang et al., 2018c,b; Zhang and Zhao, 2018; Cai et al., 2018; He et al., 2018; Xiao et al., 2019; Chen et al., 2018; Wang et al., 2018; Wang et al., 2018a, 2017b,a), constituent and dependency parsing have been well developed with neural network. These models attain state-of-the-art results for dependency parsing (Chen and Manning, 2014; Dozat and Manning, 2017; Ma et al., 2018) and constituent parsing (Dyer et al., 2016; Cross and Huang, 2016; Kitaev and Klein, 2018a). Conclusions This paper presents a simplified HPSG with two different decode methods which are evaluated"
P19-1230,P16-1131,1,0.927978,"Missing"
P19-1230,C18-1153,1,0.825955,"rding to (Zhang and Krieger, 2011), many of these large-scale grammar implementations are forced to choose to either compromise the linguistic preciseness or to accept the low coverage in parsing. Previous works of HPSG approximation focus on two major approaches: grammar based approach (Kiefer and Krieger, 2004), and the corpus-driven approach (Krieger, 2007) and (Zhang and Krieger, 2011) which proposes PCFG approximation as a way to alleviate some of these issues in HPSG processing. Recently, with the impressive success of deep neural networks in a wide range of NLP tasks (Li et al., 2018b; Zhang et al., 2018a; Li et al., 2018c; Zhang et al., 2018c,b; Zhang and Zhao, 2018; Cai et al., 2018; He et al., 2018; Xiao et al., 2019; Chen et al., 2018; Wang et al., 2018; Wang et al., 2018a, 2017b,a), constituent and dependency parsing have been well developed with neural network. These models attain state-of-the-art results for dependency parsing (Chen and Manning, 2014; Dozat and Manning, 2017; Ma et al., 2018) and constituent parsing (Dyer et al., 2016; Cross and Huang, 2016; Kitaev and Klein, 2018a). Conclusions This paper presents a simplified HPSG with two different decode methods which are evaluated"
P19-1230,C18-1317,1,0.794044,"rding to (Zhang and Krieger, 2011), many of these large-scale grammar implementations are forced to choose to either compromise the linguistic preciseness or to accept the low coverage in parsing. Previous works of HPSG approximation focus on two major approaches: grammar based approach (Kiefer and Krieger, 2004), and the corpus-driven approach (Krieger, 2007) and (Zhang and Krieger, 2011) which proposes PCFG approximation as a way to alleviate some of these issues in HPSG processing. Recently, with the impressive success of deep neural networks in a wide range of NLP tasks (Li et al., 2018b; Zhang et al., 2018a; Li et al., 2018c; Zhang et al., 2018c,b; Zhang and Zhao, 2018; Cai et al., 2018; He et al., 2018; Xiao et al., 2019; Chen et al., 2018; Wang et al., 2018; Wang et al., 2018a, 2017b,a), constituent and dependency parsing have been well developed with neural network. These models attain state-of-the-art results for dependency parsing (Chen and Manning, 2014; Dozat and Manning, 2017; Ma et al., 2018) and constituent parsing (Dyer et al., 2016; Cross and Huang, 2016; Kitaev and Klein, 2018a). Conclusions This paper presents a simplified HPSG with two different decode methods which are evaluated"
P19-1230,C18-1038,1,0.826546,"grammar implementations are forced to choose to either compromise the linguistic preciseness or to accept the low coverage in parsing. Previous works of HPSG approximation focus on two major approaches: grammar based approach (Kiefer and Krieger, 2004), and the corpus-driven approach (Krieger, 2007) and (Zhang and Krieger, 2011) which proposes PCFG approximation as a way to alleviate some of these issues in HPSG processing. Recently, with the impressive success of deep neural networks in a wide range of NLP tasks (Li et al., 2018b; Zhang et al., 2018a; Li et al., 2018c; Zhang et al., 2018c,b; Zhang and Zhao, 2018; Cai et al., 2018; He et al., 2018; Xiao et al., 2019; Chen et al., 2018; Wang et al., 2018; Wang et al., 2018a, 2017b,a), constituent and dependency parsing have been well developed with neural network. These models attain state-of-the-art results for dependency parsing (Chen and Manning, 2014; Dozat and Manning, 2017; Ma et al., 2018) and constituent parsing (Dyer et al., 2016; Cross and Huang, 2016; Kitaev and Klein, 2018a). Conclusions This paper presents a simplified HPSG with two different decode methods which are evaluated on both constituent and dependency parsing. Despite the usefuln"
P19-1230,P13-1043,0,0.206326,"Missing"
P19-1230,N07-1051,0,\N,Missing
P19-1298,P18-1192,1,0.778278,"ignore long-range order. To show the complementary of these two methods, we also placed all edges of lattice in a single sequence in a relative right order based on their first character and use normal positional encodings and our LSA method; we obtained a BLEU of 40.90 which is 0.13 higher than single LSA model. From this, we can see that long-range position information is indeed beneficial to our LSA model. 3 Related Work Neural network based methods have been applied to several natural language processing tasks (Li et al., 2018; Zhang et al., 2019; Chen et al., 2018, 2017; Li et al., 2019; He et al., 2018; Zhou and Zhao, 2019), especially to NMT (Bahdanau et al., 2015; Wang et al., 2017a,b, 2018; Wang et al., 2018; Zhang et al., 2018; Zhang and Zhao, 2019). Our work is related to the source side representations for NMT. Generally, the NMT model uses the word as a basic unit for source sentences modeling. In order to obtain better source side representations and avoid OOV problems, recent research has modeled source sentences at character level (Ling et al., 2015; Costa-Jussa and Fonollosa, 2016; Yang et al., 2016; Lee et al., 2016), subword level (Sennrich et al., 2015; Kudo, 2018; Wu and Zhao"
P19-1298,P16-1039,1,0.824319,"Missing"
P19-1298,C08-1049,0,0.0377163,"these methods show better translation performance than the word level model. As models mentioned above only use 1-best segmentation as inputs, lattice which can pack many different segmentations in a compact form has been widely used in statistical machine translation (SMT) (Xu et al., 2005; Dyer et al., 2008) and RNN-based NMT (Su et al., 2017; Sperber et al., 2017). To enhance the representaions of the input, lattice has also been applied in many other NLP tasks such as named entity recognition (Zhang and Yang, 2018), Chinese word segmentation (Yang et al., 2019) and part-of-speech tagging (Jiang et al., 2008; Wang et al., 2013). 6 Conclusions In this paper, we have proposed two methods to incorporate lattice representations into Transformer. Experimental results in two datasets on word-level and subword-level respectively validate the effectiveness of the proposed approaches. Different from Veliˇckovi´c et al. (2017), our work also provides an attempt to encode a simple labeled graph into Transformer and can be used in any tasks which need Transformer encoder to learn sequence representation. All analysis experiments conducted on NIST dataset. 3094 References Jimmy Lei Ba, Jamie Ryan Kiros, and G"
P19-1298,P17-2096,1,0.729925,"354): “Unsupervised Neural Machine Translation in Universal Scenarios” and NICT tenure-track researcher startup fund “Toward Intelligent Machine Translation”. NMT can be factorized in character (Costa-Jussa and Fonollosa, 2016), word (Sutskever et al., 2014), or subword (Sennrich et al., 2015) level. However, only using 1-best segmentation as inputs limits NMT encoders to express source sequences sufficiently and reliably. Many East Asian languages, including Chinese are written without explicit word boundary, so that their sentences need to be segmented into words firstly (Zhao et al., 2019; Cai et al., 2017; Cai and Zhao, 2016; Zhao et al., 2013; Zhao and Kit, 2011). By different segmentors, each sentence can be segmented into multiple forms as shown in Figure 1. Even for those alphabetical languages with clear word boundary like English, there is still an issue about selecting a proper subword vocabulary size, which determines the segmentation granularities for word representation. In order to handle this problem, Morishita et al. (2018) used hierarchical subword features to represent sequence with different subword granularities. Su et al. (2017) proposed the first word-lattice based recurrent"
P19-1298,D18-1461,0,0.019381,"Recurrent Units (GRUs) (Cho et al., 2014) to take in multiple sequence segmentation representations. Sperber et al. (2017) incorporated posterior scores to Tree-LSTM for building a lattice encoder in speech translation. All these existing methods serve for RNN-based NMT model, where lattices can be formulized as directed graphs and the inherent directed structure of RNN facilitates the construction of lattice. Meanwhile, the selfattention mechanism is good at learning the dependency between characters in parallel, which can partially compare and learn information from multiple segmentations (Cherry et al., 2018). Therefore, it is challenging to directly apply the lattice structure to Transformer. In this work, we explore an efficient way of integrating lattice into Transformer. Our method can not only process multiple sequences segmented in different ways to improve translation quality, but also maintain the characteristics of parallel computation in the Transformer. 2 2.1 Background its pre suc Conditions i&lt;j=p&lt;q p&lt;q=i&lt;j i≤p&lt;q≤j p≤i&lt;j≤q i &lt; p &lt; j &lt; q or p&lt;i&lt;q&lt;j i&lt;j&lt;p&lt;q p&lt;q&lt;i&lt;j Self-Attention Transformer employs H attention heads to perform self-attention over a sequence individually and finally appl"
P19-1298,P05-1066,0,0.255638,"Missing"
P19-1298,P16-2058,0,0.0264019,"Missing"
P19-1298,P17-4012,0,0.0323983,"operations (Sennrich et al., 2015) to get different segmented sentences for building subword lattices. 16K BPE merge operations are employed on the target side. We set batch size to 1024 tokens and accumulated gradient 16 times before a backpropagation. During training, we set all dropout to 0.3 and chose the Adam optimizer (Kingma and Ba, 2014) with β1 = 0.9, β2 = 0.98 and  = 10−9 for parameters tuning. During decoding, we used beam search algorithm and set the beam size to 20. All other configurations were the same with Vaswani et al. (2017). We implemented our model based on the OpenNMT (Klein et al., 2017) and trained and evaluated all models on a single NVIDIA GeForce GTX 1080 Ti GPU. 4.2 Overall Performance From Table 2, we see that our LPE and LSA models both outperform the Transformer baseline model of 0.58 and 0.42 BLEU respectively. When we combine LPE and LSA together, we get a gain of 0.91 BLEU points. Table 3 shows that our method also works well on the subword level. The base Transformer system has about 90M parameters and our LPE and LSA models introduce 0 and 6k parameters over it, respectively, which shows that our lattice approach improves Transformer with little parameter accumul"
P19-1298,P18-1007,0,0.0210495,", 2019; He et al., 2018; Zhou and Zhao, 2019), especially to NMT (Bahdanau et al., 2015; Wang et al., 2017a,b, 2018; Wang et al., 2018; Zhang et al., 2018; Zhang and Zhao, 2019). Our work is related to the source side representations for NMT. Generally, the NMT model uses the word as a basic unit for source sentences modeling. In order to obtain better source side representations and avoid OOV problems, recent research has modeled source sentences at character level (Ling et al., 2015; Costa-Jussa and Fonollosa, 2016; Yang et al., 2016; Lee et al., 2016), subword level (Sennrich et al., 2015; Kudo, 2018; Wu and Zhao, 2018) and mixed character-word level (Luong and Manning, 2016). All these methods show better translation performance than the word level model. As models mentioned above only use 1-best segmentation as inputs, lattice which can pack many different segmentations in a compact form has been widely used in statistical machine translation (SMT) (Xu et al., 2005; Dyer et al., 2008) and RNN-based NMT (Su et al., 2017; Sperber et al., 2017). To enhance the representaions of the input, lattice has also been applied in many other NLP tasks such as named entity recognition (Zhang and Yang"
P19-1298,Q17-1026,0,0.0394117,"Missing"
P19-1298,C18-1271,1,0.624191,"es, including Chinese are written without explicit word boundary, so that their sentences need to be segmented into words firstly (Zhao et al., 2019; Cai et al., 2017; Cai and Zhao, 2016; Zhao et al., 2013; Zhao and Kit, 2011). By different segmentors, each sentence can be segmented into multiple forms as shown in Figure 1. Even for those alphabetical languages with clear word boundary like English, there is still an issue about selecting a proper subword vocabulary size, which determines the segmentation granularities for word representation. In order to handle this problem, Morishita et al. (2018) used hierarchical subword features to represent sequence with different subword granularities. Su et al. (2017) proposed the first word-lattice based recurrent neural network 3090 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3090–3097 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics (RNN) encoders which extended Gated Recurrent Units (GRUs) (Cho et al., 2014) to take in multiple sequence segmentation representations. Sperber et al. (2017) incorporated posterior scores to Tree-LSTM for building a latti"
P19-1298,P16-1100,0,0.0307649,"T (Bahdanau et al., 2015; Wang et al., 2017a,b, 2018; Wang et al., 2018; Zhang et al., 2018; Zhang and Zhao, 2019). Our work is related to the source side representations for NMT. Generally, the NMT model uses the word as a basic unit for source sentences modeling. In order to obtain better source side representations and avoid OOV problems, recent research has modeled source sentences at character level (Ling et al., 2015; Costa-Jussa and Fonollosa, 2016; Yang et al., 2016; Lee et al., 2016), subword level (Sennrich et al., 2015; Kudo, 2018; Wu and Zhao, 2018) and mixed character-word level (Luong and Manning, 2016). All these methods show better translation performance than the word level model. As models mentioned above only use 1-best segmentation as inputs, lattice which can pack many different segmentations in a compact form has been widely used in statistical machine translation (SMT) (Xu et al., 2005; Dyer et al., 2008) and RNN-based NMT (Su et al., 2017; Sperber et al., 2017). To enhance the representaions of the input, lattice has also been applied in many other NLP tasks such as named entity recognition (Zhang and Yang, 2018), Chinese word segmentation (Yang et al., 2019) and part-of-speech tag"
P19-1298,W18-6419,1,0.820368,":8 :fu-zong-cai (4)Lattice Figure 1: Incorporating three different segmentation for a lattice graph. The original sentence is “mao-yifa-zhan-ju-fu-zong-cai”. In Chinese it is “贸易发展局 副总裁”. In English it means “The vice president of Trade Development Council” Introduction Neural machine translation (NMT) has achieved great progress with the evolvement of model structures under an encoder-decoder framework (Sutskever et al., 2014; Bahdanau et al., 2014). Recently, the self-attention based Transformer model has achieved state-of-theart performance on multiple language pairs (Vaswani et al., 2017; Marie et al., 2018). Both representations of source and target sentences in ∗ mao-yi Corresponding author. This paper was partially supported by National Key Research and Development Program of China (No. 2017YFB0304100) and key projects of National Natural Science Foundation of China (No. U1836222 and No. 61733011). Rui Wang was partially supported by JSPS grant-in-aid for early-career scientists (19K20354): “Unsupervised Neural Machine Translation in Universal Scenarios” and NICT tenure-track researcher startup fund “Toward Intelligent Machine Translation”. NMT can be factorized in character (Costa-Jussa and F"
P19-1298,C18-1269,0,0.105041,"Missing"
P19-1298,C18-1052,0,0.0773082,"ast Asian languages, including Chinese are written without explicit word boundary, so that their sentences need to be segmented into words firstly (Zhao et al., 2019; Cai et al., 2017; Cai and Zhao, 2016; Zhao et al., 2013; Zhao and Kit, 2011). By different segmentors, each sentence can be segmented into multiple forms as shown in Figure 1. Even for those alphabetical languages with clear word boundary like English, there is still an issue about selecting a proper subword vocabulary size, which determines the segmentation granularities for word representation. In order to handle this problem, Morishita et al. (2018) used hierarchical subword features to represent sequence with different subword granularities. Su et al. (2017) proposed the first word-lattice based recurrent neural network 3090 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3090–3097 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics (RNN) encoders which extended Gated Recurrent Units (GRUs) (Cho et al., 2014) to take in multiple sequence segmentation representations. Sperber et al. (2017) incorporated posterior scores to Tree-LSTM for building a latti"
P19-1298,D17-1155,1,0.790059,"placed all edges of lattice in a single sequence in a relative right order based on their first character and use normal positional encodings and our LSA method; we obtained a BLEU of 40.90 which is 0.13 higher than single LSA model. From this, we can see that long-range position information is indeed beneficial to our LSA model. 3 Related Work Neural network based methods have been applied to several natural language processing tasks (Li et al., 2018; Zhang et al., 2019; Chen et al., 2018, 2017; Li et al., 2019; He et al., 2018; Zhou and Zhao, 2019), especially to NMT (Bahdanau et al., 2015; Wang et al., 2017a,b, 2018; Wang et al., 2018; Zhang et al., 2018; Zhang and Zhao, 2019). Our work is related to the source side representations for NMT. Generally, the NMT model uses the word as a basic unit for source sentences modeling. In order to obtain better source side representations and avoid OOV problems, recent research has modeled source sentences at character level (Ling et al., 2015; Costa-Jussa and Fonollosa, 2016; Yang et al., 2016; Lee et al., 2016), subword level (Sennrich et al., 2015; Kudo, 2018; Wu and Zhao, 2018) and mixed character-word level (Luong and Manning, 2016). All these methods"
P19-1298,P02-1040,0,0.104083,". 4 Experiments 4.1 Setup We conducted experiments on the NIST ChineseEnglish (Zh-En) and IWSLT 2016 EnglishGerman (En-De) datasets. The Zh-En corpus consists of 1.25M sentence pairs and the En-De corpus consists of 191K sentence pairs. For ZhEn task, we chose the NIST 2005 dataset as the validation set and the NIST 2002, 2003, 2004, 2006, and 2008 datasets as test sets. For EnDe task, tst2012 was used as validation set and tst2013 and tst2014 were used as test sets. For both tasks, sentence pairs with either side longer than 50 were dropped. We used the case-sensitive 4-gram NIST BLEU score (Papineni et al., 2002) as the evaluation metric and sign-test (Collins et al., 2005) for statistical significance test. For Zh-En task, we followed Su et al. (2017) to use the toolkit2 to train segmenters on PKU, MSR (Emerson, 2005), and CTB corpora (Xue et al., 2005), then we generated word lattices with different segmented training data. Both source and target vocabularies are limited to 30K. For En-De task, we adopted 8K, 16K and 32K BPE merge operations (Sennrich et al., 2015) to get different segmented sentences for building subword lattices. 16K BPE merge operations are employed on the target side. We set bat"
P19-1298,P18-2048,1,0.895037,"Missing"
P19-1298,N18-2074,0,0.0490038,"Missing"
P19-1298,D17-1145,0,0.0654155,"ies for word representation. In order to handle this problem, Morishita et al. (2018) used hierarchical subword features to represent sequence with different subword granularities. Su et al. (2017) proposed the first word-lattice based recurrent neural network 3090 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3090–3097 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics (RNN) encoders which extended Gated Recurrent Units (GRUs) (Cho et al., 2014) to take in multiple sequence segmentation representations. Sperber et al. (2017) incorporated posterior scores to Tree-LSTM for building a lattice encoder in speech translation. All these existing methods serve for RNN-based NMT model, where lattices can be formulized as directed graphs and the inherent directed structure of RNN facilitates the construction of lattice. Meanwhile, the selfattention mechanism is good at learning the dependency between characters in parallel, which can partially compare and learn information from multiple segmentations (Cherry et al., 2018). Therefore, it is challenging to directly apply the lattice structure to Transformer. In this work, we"
P19-1298,P13-2110,0,0.0287774,"etter translation performance than the word level model. As models mentioned above only use 1-best segmentation as inputs, lattice which can pack many different segmentations in a compact form has been widely used in statistical machine translation (SMT) (Xu et al., 2005; Dyer et al., 2008) and RNN-based NMT (Su et al., 2017; Sperber et al., 2017). To enhance the representaions of the input, lattice has also been applied in many other NLP tasks such as named entity recognition (Zhang and Yang, 2018), Chinese word segmentation (Yang et al., 2019) and part-of-speech tagging (Jiang et al., 2008; Wang et al., 2013). 6 Conclusions In this paper, we have proposed two methods to incorporate lattice representations into Transformer. Experimental results in two datasets on word-level and subword-level respectively validate the effectiveness of the proposed approaches. Different from Veliˇckovi´c et al. (2017), our work also provides an attempt to encode a simple labeled graph into Transformer and can be used in any tasks which need Transformer encoder to learn sequence representation. All analysis experiments conducted on NIST dataset. 3094 References Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. 20"
P19-1298,2005.iwslt-1.18,0,0.0473858,"presentations and avoid OOV problems, recent research has modeled source sentences at character level (Ling et al., 2015; Costa-Jussa and Fonollosa, 2016; Yang et al., 2016; Lee et al., 2016), subword level (Sennrich et al., 2015; Kudo, 2018; Wu and Zhao, 2018) and mixed character-word level (Luong and Manning, 2016). All these methods show better translation performance than the word level model. As models mentioned above only use 1-best segmentation as inputs, lattice which can pack many different segmentations in a compact form has been widely used in statistical machine translation (SMT) (Xu et al., 2005; Dyer et al., 2008) and RNN-based NMT (Su et al., 2017; Sperber et al., 2017). To enhance the representaions of the input, lattice has also been applied in many other NLP tasks such as named entity recognition (Zhang and Yang, 2018), Chinese word segmentation (Yang et al., 2019) and part-of-speech tagging (Jiang et al., 2008; Wang et al., 2013). 6 Conclusions In this paper, we have proposed two methods to incorporate lattice representations into Transformer. Experimental results in two datasets on word-level and subword-level respectively validate the effectiveness of the proposed approaches."
P19-1298,N19-1278,0,0.040054,"Missing"
P19-1298,C16-1288,0,0.0234602,"Missing"
P19-1298,P17-2089,1,0.744874,"Missing"
P19-1298,P18-1144,0,0.0448059,"15; Kudo, 2018; Wu and Zhao, 2018) and mixed character-word level (Luong and Manning, 2016). All these methods show better translation performance than the word level model. As models mentioned above only use 1-best segmentation as inputs, lattice which can pack many different segmentations in a compact form has been widely used in statistical machine translation (SMT) (Xu et al., 2005; Dyer et al., 2008) and RNN-based NMT (Su et al., 2017; Sperber et al., 2017). To enhance the representaions of the input, lattice has also been applied in many other NLP tasks such as named entity recognition (Zhang and Yang, 2018), Chinese word segmentation (Yang et al., 2019) and part-of-speech tagging (Jiang et al., 2008; Wang et al., 2013). 6 Conclusions In this paper, we have proposed two methods to incorporate lattice representations into Transformer. Experimental results in two datasets on word-level and subword-level respectively validate the effectiveness of the proposed approaches. Different from Veliˇckovi´c et al. (2017), our work also provides an attempt to encode a simple labeled graph into Transformer and can be used in any tasks which need Transformer encoder to learn sequence representation. All analysi"
P19-1298,D18-1511,1,0.673041,"e in a relative right order based on their first character and use normal positional encodings and our LSA method; we obtained a BLEU of 40.90 which is 0.13 higher than single LSA model. From this, we can see that long-range position information is indeed beneficial to our LSA model. 3 Related Work Neural network based methods have been applied to several natural language processing tasks (Li et al., 2018; Zhang et al., 2019; Chen et al., 2018, 2017; Li et al., 2019; He et al., 2018; Zhou and Zhao, 2019), especially to NMT (Bahdanau et al., 2015; Wang et al., 2017a,b, 2018; Wang et al., 2018; Zhang et al., 2018; Zhang and Zhao, 2019). Our work is related to the source side representations for NMT. Generally, the NMT model uses the word as a basic unit for source sentences modeling. In order to obtain better source side representations and avoid OOV problems, recent research has modeled source sentences at character level (Ling et al., 2015; Costa-Jussa and Fonollosa, 2016; Yang et al., 2016; Lee et al., 2016), subword level (Sennrich et al., 2015; Kudo, 2018; Wu and Zhao, 2018) and mixed character-word level (Luong and Manning, 2016). All these methods show better translation performance than the wo"
P19-1298,P19-1230,1,0.678425,"order. To show the complementary of these two methods, we also placed all edges of lattice in a single sequence in a relative right order based on their first character and use normal positional encodings and our LSA method; we obtained a BLEU of 40.90 which is 0.13 higher than single LSA model. From this, we can see that long-range position information is indeed beneficial to our LSA model. 3 Related Work Neural network based methods have been applied to several natural language processing tasks (Li et al., 2018; Zhang et al., 2019; Chen et al., 2018, 2017; Li et al., 2019; He et al., 2018; Zhou and Zhao, 2019), especially to NMT (Bahdanau et al., 2015; Wang et al., 2017a,b, 2018; Wang et al., 2018; Zhang et al., 2018; Zhang and Zhao, 2019). Our work is related to the source side representations for NMT. Generally, the NMT model uses the word as a basic unit for source sentences modeling. In order to obtain better source side representations and avoid OOV problems, recent research has modeled source sentences at character level (Ling et al., 2015; Costa-Jussa and Fonollosa, 2016; Yang et al., 2016; Lee et al., 2016), subword level (Sennrich et al., 2015; Kudo, 2018; Wu and Zhao, 2018) and mixed char"
P19-1298,P08-1115,0,\N,Missing
P19-1298,I05-3017,0,\N,Missing
P19-1298,D17-1304,1,\N,Missing
P19-1298,P16-1162,0,\N,Missing
S18-1147,P16-1039,1,0.870635,"ing or sense embedding to represent each word. Then, neural networks are used to discover and rank the hypernym candidates for given terms. 2.1 zt = σ(Wz xt + Uz ht−1 + bz ), ˜ t = tanh(Wh xt + Uh (rt ht−1 ) + bh ) h ˜t ht = (1 − zt ) ht−1 + zt h where denotes the element-wise multiplication. rt and zt are the reset and update gates respec˜ t the hidden states. tively, and h Embedding LSTM LSTM (Hochreiter and Schmidhuber, 1997) unit is defined as follows. To use deep neural networks, symbolic data needs to be transformed into distributed representations(Wang et al., 2016a; Qin et al., 2016b; Cai and Zhao, 2016; Zhang et al., 2016; Wang et al., 2016b, 2015; Cai et al., 2017). We use Glove toolkit to train the word embeddings using UMBC corpus (Han et al., 2013). Moreover, in order to perform word sense induction and disambiguation, the word embedding could be transformed to sense embedding, which is induced from exhisting word embeddings via clustering of ego-networks (Pelevina et al., 2016) of related words. Thus, each input word or phrase is embedded into vector sequence, w = {x1 , x2 , . . . , xl } where l denotes the sequence length. If the input term is a word, then l = 1 while for phrases, l m"
S18-1147,P06-1114,0,0.0266796,"nt space for words and phrases. The evaluated models include convolutional neural network, long-short term memory network, gated recurrent unit and recurrent convolutional neural network. We also explore different embedding methods, including word embedding and sense embedding for better performance. 1 Introduction Hypernym-hyponym relationship is an is-a semantic relation between terms as shown in Table 1. Various natural language processing (NLP) tasks, especially those semantically intensive ones aiming for inference and reasoning with generalization capability, such as question answering (Harabagiu and Hickl, 2006; Yahya et al., 2013) and textual entailment (Dagan et al., 2013; Roller and Erk, 2016), can benefit from identifying semantic relations between words beyond synonymy. The hypernym discovery task (CamachoCollados et al., 2018) aims to discover the most appropriate hypernym(s) for input concepts or entities from a pre-defined corpus. A relevant well-known scenario is hypernym detection, Hyponym Heming Kralendijk StarCraft Table 1: ship. ∗ Corresponding author. This paper was partially supported by National Key Research and Development Program of China (No. 2017YFB0304100), National Natural Scie"
S18-1147,P17-2096,1,0.712907,"ks are used to discover and rank the hypernym candidates for given terms. 2.1 zt = σ(Wz xt + Uz ht−1 + bz ), ˜ t = tanh(Wh xt + Uh (rt ht−1 ) + bh ) h ˜t ht = (1 − zt ) ht−1 + zt h where denotes the element-wise multiplication. rt and zt are the reset and update gates respec˜ t the hidden states. tively, and h Embedding LSTM LSTM (Hochreiter and Schmidhuber, 1997) unit is defined as follows. To use deep neural networks, symbolic data needs to be transformed into distributed representations(Wang et al., 2016a; Qin et al., 2016b; Cai and Zhao, 2016; Zhang et al., 2016; Wang et al., 2016b, 2015; Cai et al., 2017). We use Glove toolkit to train the word embeddings using UMBC corpus (Han et al., 2013). Moreover, in order to perform word sense induction and disambiguation, the word embedding could be transformed to sense embedding, which is induced from exhisting word embeddings via clustering of ego-networks (Pelevina et al., 2016) of related words. Thus, each input word or phrase is embedded into vector sequence, w = {x1 , x2 , . . . , xl } where l denotes the sequence length. If the input term is a word, then l = 1 while for phrases, l means the number of words. 2.2 it = σ(Wi xt + Wh ht−1 + bi ), ft ="
S18-1147,P82-1020,0,0.809007,"Missing"
S18-1147,N16-1153,0,0.0590994,". All of our models are trained on a single GPU (NVIDIA GTX 980Ti), with roughly 1.5h for general-purpose subtask for English and 0.5h domain-specific domain-specific ones for medical and music. We run all our models up to 50 epoch and select the best result in validation. s = [s1 ⊕ · · · ⊕ sj ⊕ · · · ⊕ sk ] In this way, the model can capture the critical features in the sentence with different filters. RCNN Since some input terms are phrases, whose member words share different weights. In RCNN, an adaptive gated decay mechanism is used to weight the words in the convolution layer. Following (Lei et al., 2016), we introduce neural gates similar λ to LSTMs to specify when and how to average the observed signals. The resulting architecture integrates recurrent networks with nonconsecutive convolutions: λ = σ(W λ xt + U λ ht−1 + bλ ) c1t = λt c1t−1 + (1 − λt ) W1 xt c2t = λt c2t−1 + (1 − λt ) (c1t−1 + W2 xt ) cnt Experiment ··· = λt cnt−1 + (1 − λt ) (c1n−1 + Wn xt ) ht = tanh(cnt + b) where c1t , c2t , · · · , cnt are accumulator vectors that store weighted averages of 1-gram to n-gram features. For discriminative training, we use a maxmargin framework for learning (or fine-tuning) parameters θ. Spec"
S18-1147,W16-1620,0,0.0289466,"ing LSTM LSTM (Hochreiter and Schmidhuber, 1997) unit is defined as follows. To use deep neural networks, symbolic data needs to be transformed into distributed representations(Wang et al., 2016a; Qin et al., 2016b; Cai and Zhao, 2016; Zhang et al., 2016; Wang et al., 2016b, 2015; Cai et al., 2017). We use Glove toolkit to train the word embeddings using UMBC corpus (Han et al., 2013). Moreover, in order to perform word sense induction and disambiguation, the word embedding could be transformed to sense embedding, which is induced from exhisting word embeddings via clustering of ego-networks (Pelevina et al., 2016) of related words. Thus, each input word or phrase is embedded into vector sequence, w = {x1 , x2 , . . . , xl } where l denotes the sequence length. If the input term is a word, then l = 1 while for phrases, l means the number of words. 2.2 it = σ(Wi xt + Wh ht−1 + bi ), ft = σ(Wf xt + Wf ht−1 + bf ), ut = σ(Wu xt + Wu ht−1 + bu ), ct = ft ct−1 + it tanh(Wc xt + Wc ht−1 + bc ), ht = tanh(ct ) ut , where σ stands for the sigmoid function, represents element-wise multiplication and Wi , Wf , Wu , Wc , bi , bf , bu , bc are model parameters. it , ft , ut , ct , ht are the input gates, forget gat"
S18-1147,D16-1041,0,0.0609061,"ission for Intelligent Interaction and Cognitive Engineering, Shanghai Jiao Tong University, Shanghai, 200240, China 3 College of Zhiyuan, Shanghai Jiao Tong University, China 4 School of Computer, Huazhong University of Science and Technology, China {zhangzs, keep moving-lee}@sjtu.edu.cn, zhaohai@cs.sjtu.edu.cn, alexistang@foxmail.com 1 Abstract which is a binary task to decide whether a hypernymic relationship holds between a pair of words or not. A hypernym detection system should be capable of learning taxonomy and lexical semantics, including pattern-based methods (Boella and Caro, 2013; Espinosa-Anke et al., 2016b) and graph-based approaches (Fountain and Lapata, 2012; Velardi et al., 2013; Kang et al., 2016). However, our concerned task, hypernym discovery, is rather more challenging since it requires the systems to explore the semantic connection with all the exhausted candidates in the latent space and rank a candidate set instead of a binary classification in previous work. The other challenge is representation for terms, including words and phrases, where the phrase embedding could not be obtained by word embeddings directly. A simple method is to average the inner word embeddings to form the phr"
S18-1147,K16-2010,1,0.90617,"Missing"
S18-1147,D16-1246,1,0.945082,"nal linear models with handcrafted sparse fea903 Proceedings of the 12th International Workshop on Semantic Evaluation (SemEval-2018), pages 903–908 New Orleans, Louisiana, June 5–6, 2018. ©2018 Association for Computational Linguistics ture extraction. Thus, we empirically survey various neural networks to represent terms in latent space. After obtaining the representation for input term and all the candidate hypernyms, to give the ranked hypernym list, the cosine similarity between the term and the candidate hypernym is computed by, Pn (xi × yi ) cosine = Pn i=12 Pn 2 i=1 yi i=1 xi × tures (Qin et al., 2016b; Pang et al., 2016; Qin et al., 2016a; Wang et al., 2016c; Zhao et al., 2017a; Wang et al., 2017; Qin et al., 2017; Cai and Zhao, 2017; Zhao et al., 2017b; Li et al., 2018). In this work, we introduce a neural network architecture for the concerned task and empirically study various neural networks to model the distributed representations for words and phrases. In our system, we leverage an unambiguous vector representation via term embedding, and we take advantage of deep neural networks to discover the hypernym relationships between terms. The rest of the paper is organized as follows: Sec"
S18-1147,P17-1093,1,0.837158,"on (SemEval-2018), pages 903–908 New Orleans, Louisiana, June 5–6, 2018. ©2018 Association for Computational Linguistics ture extraction. Thus, we empirically survey various neural networks to represent terms in latent space. After obtaining the representation for input term and all the candidate hypernyms, to give the ranked hypernym list, the cosine similarity between the term and the candidate hypernym is computed by, Pn (xi × yi ) cosine = Pn i=12 Pn 2 i=1 yi i=1 xi × tures (Qin et al., 2016b; Pang et al., 2016; Qin et al., 2016a; Wang et al., 2016c; Zhao et al., 2017a; Wang et al., 2017; Qin et al., 2017; Cai and Zhao, 2017; Zhao et al., 2017b; Li et al., 2018). In this work, we introduce a neural network architecture for the concerned task and empirically study various neural networks to model the distributed representations for words and phrases. In our system, we leverage an unambiguous vector representation via term embedding, and we take advantage of deep neural networks to discover the hypernym relationships between terms. The rest of the paper is organized as follows: Section 2 briefly describes our system, Section 3 shows our experiments on the hyperym discovery task including the gen"
S18-1147,N12-1051,0,0.0321721,"ing, Shanghai Jiao Tong University, Shanghai, 200240, China 3 College of Zhiyuan, Shanghai Jiao Tong University, China 4 School of Computer, Huazhong University of Science and Technology, China {zhangzs, keep moving-lee}@sjtu.edu.cn, zhaohai@cs.sjtu.edu.cn, alexistang@foxmail.com 1 Abstract which is a binary task to decide whether a hypernymic relationship holds between a pair of words or not. A hypernym detection system should be capable of learning taxonomy and lexical semantics, including pattern-based methods (Boella and Caro, 2013; Espinosa-Anke et al., 2016b) and graph-based approaches (Fountain and Lapata, 2012; Velardi et al., 2013; Kang et al., 2016). However, our concerned task, hypernym discovery, is rather more challenging since it requires the systems to explore the semantic connection with all the exhausted candidates in the latent space and rank a candidate set instead of a binary classification in previous work. The other challenge is representation for terms, including words and phrases, where the phrase embedding could not be obtained by word embeddings directly. A simple method is to average the inner word embeddings to form the phrase embedding. However, this is too coarse since each wo"
S18-1147,D16-1234,0,0.0596202,"long-short term memory network, gated recurrent unit and recurrent convolutional neural network. We also explore different embedding methods, including word embedding and sense embedding for better performance. 1 Introduction Hypernym-hyponym relationship is an is-a semantic relation between terms as shown in Table 1. Various natural language processing (NLP) tasks, especially those semantically intensive ones aiming for inference and reasoning with generalization capability, such as question answering (Harabagiu and Hickl, 2006; Yahya et al., 2013) and textual entailment (Dagan et al., 2013; Roller and Erk, 2016), can benefit from identifying semantic relations between words beyond synonymy. The hypernym discovery task (CamachoCollados et al., 2018) aims to discover the most appropriate hypernym(s) for input concepts or entities from a pre-defined corpus. A relevant well-known scenario is hypernym detection, Hyponym Heming Kralendijk StarCraft Table 1: ship. ∗ Corresponding author. This paper was partially supported by National Key Research and Development Program of China (No. 2017YFB0304100), National Natural Science Foundation of China (No. 61672343 and No. 61733011), Key Project of National Societ"
S18-1147,S13-1005,0,0.0492977,"xt + Uz ht−1 + bz ), ˜ t = tanh(Wh xt + Uh (rt ht−1 ) + bh ) h ˜t ht = (1 − zt ) ht−1 + zt h where denotes the element-wise multiplication. rt and zt are the reset and update gates respec˜ t the hidden states. tively, and h Embedding LSTM LSTM (Hochreiter and Schmidhuber, 1997) unit is defined as follows. To use deep neural networks, symbolic data needs to be transformed into distributed representations(Wang et al., 2016a; Qin et al., 2016b; Cai and Zhao, 2016; Zhang et al., 2016; Wang et al., 2016b, 2015; Cai et al., 2017). We use Glove toolkit to train the word embeddings using UMBC corpus (Han et al., 2013). Moreover, in order to perform word sense induction and disambiguation, the word embedding could be transformed to sense embedding, which is induced from exhisting word embeddings via clustering of ego-networks (Pelevina et al., 2016) of related words. Thus, each input word or phrase is embedded into vector sequence, w = {x1 , x2 , . . . , xl } where l denotes the sequence length. If the input term is a word, then l = 1 while for phrases, l means the number of words. 2.2 it = σ(Wi xt + Wh ht−1 + bi ), ft = σ(Wf xt + Wf ht−1 + bf ), ut = σ(Wu xt + Wu ht−1 + bu ), ct = ft ct−1 + it tanh(Wc xt +"
S18-1147,J13-3007,0,0.0271742,"versity, Shanghai, 200240, China 3 College of Zhiyuan, Shanghai Jiao Tong University, China 4 School of Computer, Huazhong University of Science and Technology, China {zhangzs, keep moving-lee}@sjtu.edu.cn, zhaohai@cs.sjtu.edu.cn, alexistang@foxmail.com 1 Abstract which is a binary task to decide whether a hypernymic relationship holds between a pair of words or not. A hypernym detection system should be capable of learning taxonomy and lexical semantics, including pattern-based methods (Boella and Caro, 2013; Espinosa-Anke et al., 2016b) and graph-based approaches (Fountain and Lapata, 2012; Velardi et al., 2013; Kang et al., 2016). However, our concerned task, hypernym discovery, is rather more challenging since it requires the systems to explore the semantic connection with all the exhausted candidates in the latent space and rank a candidate set instead of a binary classification in previous work. The other challenge is representation for terms, including words and phrases, where the phrase embedding could not be obtained by word embeddings directly. A simple method is to average the inner word embeddings to form the phrase embedding. However, this is too coarse since each word might share differe"
S18-1147,K17-3020,1,0.827944,"n Semantic Evaluation (SemEval-2018), pages 903–908 New Orleans, Louisiana, June 5–6, 2018. ©2018 Association for Computational Linguistics ture extraction. Thus, we empirically survey various neural networks to represent terms in latent space. After obtaining the representation for input term and all the candidate hypernyms, to give the ranked hypernym list, the cosine similarity between the term and the candidate hypernym is computed by, Pn (xi × yi ) cosine = Pn i=12 Pn 2 i=1 yi i=1 xi × tures (Qin et al., 2016b; Pang et al., 2016; Qin et al., 2016a; Wang et al., 2016c; Zhao et al., 2017a; Wang et al., 2017; Qin et al., 2017; Cai and Zhao, 2017; Zhao et al., 2017b; Li et al., 2018). In this work, we introduce a neural network architecture for the concerned task and empirically study various neural networks to model the distributed representations for words and phrases. In our system, we leverage an unambiguous vector representation via term embedding, and we take advantage of deep neural networks to discover the hypernym relationships between terms. The rest of the paper is organized as follows: Section 2 briefly describes our system, Section 3 shows our experiments on the hyperym discovery task"
S18-1147,N16-1064,1,0.928653,"ngs of the 12th International Workshop on Semantic Evaluation (SemEval-2018), pages 903–908 New Orleans, Louisiana, June 5–6, 2018. ©2018 Association for Computational Linguistics ture extraction. Thus, we empirically survey various neural networks to represent terms in latent space. After obtaining the representation for input term and all the candidate hypernyms, to give the ranked hypernym list, the cosine similarity between the term and the candidate hypernym is computed by, Pn (xi × yi ) cosine = Pn i=12 Pn 2 i=1 yi i=1 xi × tures (Qin et al., 2016b; Pang et al., 2016; Qin et al., 2016a; Wang et al., 2016c; Zhao et al., 2017a; Wang et al., 2017; Qin et al., 2017; Cai and Zhao, 2017; Zhao et al., 2017b; Li et al., 2018). In this work, we introduce a neural network architecture for the concerned task and empirically study various neural networks to model the distributed representations for words and phrases. In our system, we leverage an unambiguous vector representation via term embedding, and we take advantage of deep neural networks to discover the hypernym relationships between terms. The rest of the paper is organized as follows: Section 2 briefly describes our system, Section 3 shows our e"
S18-1147,C16-1295,1,0.868338,"Missing"
S18-1147,P16-1131,1,0.880606,"Missing"
S18-1147,S18-1115,0,\N,Missing
W06-0127,W03-1728,0,\N,Missing
W06-0127,W03-1719,0,\N,Missing
W06-0127,C04-1081,0,\N,Missing
W06-0127,P03-1035,1,\N,Missing
W06-0127,J05-4005,1,\N,Missing
W06-0127,O03-4002,0,\N,Missing
W06-0127,I05-3027,0,\N,Missing
W06-0127,I05-3017,0,\N,Missing
W06-0127,I05-3025,0,\N,Missing
W06-0127,W03-1726,0,\N,Missing
W08-2127,D07-1097,0,0.13653,"Missing"
W08-2127,W04-2705,0,0.0552221,"fficiency, we opt for the maximum entropy model with Gaussian prior as our learning model for both the syntactic and semantic dependency parsing. Our implementation of the model adopts L-BFGS algorithm for parameter optimization as usual (Liu and Nocedal, 1989). No additional feature selection techniques are applied. Our system consists of three components to deal with syntactic and semantic dependency parsing and word sense determination, respectively. Both parsing is formulated as a single-stage word-pair classification problem, and the latter is carried out by a search through the NomBank (Meyers et al., 2004) or the PropBank (Palmer et al., 2005)1 . 2.1 Syntactic Dependency Parsing We use a shift-reduce scheme to implement syntactic dependency parsing as in (Nivre, 2003). It takes a step-wised, history- or transition-based approach. It is basically a word-by-word method with a projective constraint. In each step, the classifier checks a word pair, e.g., TOP, the top of a stack for processed words, and, NEXT, the first word in the unprocessed word sequence, in order to determine if a dependent label should be assigned to them. Besides two arc-building actions, a shift action and a reduce action are"
W08-2127,P05-1013,0,0.141092,"Missing"
W08-2127,W03-3017,0,0.0592114,"odel adopts L-BFGS algorithm for parameter optimization as usual (Liu and Nocedal, 1989). No additional feature selection techniques are applied. Our system consists of three components to deal with syntactic and semantic dependency parsing and word sense determination, respectively. Both parsing is formulated as a single-stage word-pair classification problem, and the latter is carried out by a search through the NomBank (Meyers et al., 2004) or the PropBank (Palmer et al., 2005)1 . 2.1 Syntactic Dependency Parsing We use a shift-reduce scheme to implement syntactic dependency parsing as in (Nivre, 2003). It takes a step-wised, history- or transition-based approach. It is basically a word-by-word method with a projective constraint. In each step, the classifier checks a word pair, e.g., TOP, the top of a stack for processed words, and, NEXT, the first word in the unprocessed word sequence, in order to determine if a dependent label should be assigned to them. Besides two arc-building actions, a shift action and a reduce action are also defined to meet the projective constraint, as follows. 1 These two dictionaries that we used are downloaded from CoNLL-2008 official website. 203 CoNLL 2008: P"
W08-2127,J05-1004,0,0.066516,"opy model with Gaussian prior as our learning model for both the syntactic and semantic dependency parsing. Our implementation of the model adopts L-BFGS algorithm for parameter optimization as usual (Liu and Nocedal, 1989). No additional feature selection techniques are applied. Our system consists of three components to deal with syntactic and semantic dependency parsing and word sense determination, respectively. Both parsing is formulated as a single-stage word-pair classification problem, and the latter is carried out by a search through the NomBank (Meyers et al., 2004) or the PropBank (Palmer et al., 2005)1 . 2.1 Syntactic Dependency Parsing We use a shift-reduce scheme to implement syntactic dependency parsing as in (Nivre, 2003). It takes a step-wised, history- or transition-based approach. It is basically a word-by-word method with a projective constraint. In each step, the classifier checks a word pair, e.g., TOP, the top of a stack for processed words, and, NEXT, the first word in the unprocessed word sequence, in order to determine if a dependent label should be assigned to them. Besides two arc-building actions, a shift action and a reduce action are also defined to meet the projective c"
W08-2127,W08-2121,0,0.270756,"Missing"
W08-2127,W04-3212,0,0.090202,"ependency tree. c preddir: the direction to the current predicate candidate. d voice: if the syntactic head of p is be and p is not ended with -ing, then p is passive. e posSeq: PoS tag sequence of all syntactic children f dprelSeq: syntactic dependent label sequence of all syntactic children g dpTreeLevel: the level in the syntactic parse tree, counted from the leaf node. out. Many prepositions are also marked as predicate in the training corpus, but their arguments’ roles are ‘SU’, which are not counted the official evaluation. For argument, a dependency version of the pruning algorithm in (Xue and Palmer, 2004) is used to find, in an iterative way, the current syntactic head and its siblings in a parse tree in a constituentbased representation. In this representation, the head of a phrase governs all its sisters in the tree, as illustrated in the conversion of constituents to dependencies in (Lin, 1995). In our implementation, the following equivalent algorithm is applied to select argument candidates from a syntactic dependency parse tree. Initialization: Set the given predicate candidate as the current node; (1) The current node and all of its syntactic children are selected as argument candidates"
W08-2127,C04-1186,0,\N,Missing
W09-1208,burchardt-etal-2006-salsa,0,0.0394038,"Missing"
W09-1208,kawahara-etal-2002-construction,0,0.0206595,"Missing"
W09-1208,W08-2121,0,0.200501,"Missing"
W09-1208,taule-etal-2008-ancora,0,0.0308401,"Missing"
W09-1208,W04-3212,0,0.0624688,"actly same way. When no constraint available, however, all word pairs in the an input sequence must be considered, leading to very poor efficiency in computation for no gain in effectiveness. Thus, the training sample needs to be pruned properly. As predicates overtly known in the share task, we only consider how to effectively prune argument candidates. We adopt five types of argument pruning strategies for seven languages. All of them assume that a syntactic dependency parsing tree is available. As for Chinese and English, we continue to use a dependency version of the pruning algorithm of (Xue and Palmer, 2004) as described in (Zhao and Kit, 2008). The pruning algorithm is readdressed as the following. Initialization: Set the given predicate candidate as the current node; (1) The current node and all of its syntactic children are selected as argument candidates. (2) Reset the current node to its syntactic head and repeat step (1) until the root is reached. Note that the given predicate candidate itself is excluded from the argument candidate list for Chinese, that is slightly different from English. The above pruning algorithm has been shown effective. However, it is still inefficient for a singlest"
W09-1208,W08-2127,1,0.844842,"40861 (CityU 1318/03H), CityU Strategic Research Grant 7002037, Projects 60673041 and 60873041 under the National Natural Science Foundation of China and Project 2006AA01Z147 under the ”863” National High-Tech Research and Development of China. 55 We opt for the maximum entropy model with Gaussian prior as our learning model for all classification subtasks in the shared task. Our implementation of the model adopts L-BFGS algorithm for parameter optimization as usual. No additional feature selection techniques are applied. Our system is basically improved from its early version for CoNLL-2008 (Zhao and Kit, 2008). By introducing a virtual root for every predicates, The job to determine both argument labels and predicate senses is formulated as a word-pair classification task in four languages, namely, Catalan, Spanish, Czech and Japanese. In other three languages, Chinese, English and German, a predicate sense classifier is individually trained before argument label classification. Note that traditionally (or you may say that most semantic parsing systems did so) argument identification and classification are handled in a two-stage pipeline, while ours always tackles them in one step, in addition, pre"
W09-1208,W09-1209,1,0.783826,"ust and stable results. The first is that two results for development and test sets in the same language are quite close. The second is about out-ofdomain (OOD) task. Though for each OOD task, we just used the same model trained from the respective language and did nothing to strengthen it, this does not hinder our system to obtain top results in Czech and English OOD tasks. In addition, the feature template sets from automatical selection procedure in this task were used for the joint task of this shared task, and also output top results according to the average score of semantic labeled F1 (Zhao et al., 2009). Development with Gold Development Test (official scores) Out-of-domain average 81.24 80.46 80.47 74.34 Catalan 81.52 80.66 80.32 Chinese 78.32 77.90 77.72 Czech 86.96 85.35 85.19 85.44 English 84.19 84.01 85.44 73.31 German 77.75 76.55 75.99 64.26 Japanese 78.67 78.41 78.15 Spanish 81.32 80.39 80.46 Table 6: Semantic labeled F1 Catalan Sense Argument Training memory (MB) Training time (Min.) Test time (Min.) Training memory (GB) Training time (Hours) Test time (Min.) 0.4 3.0 3.0 Chinese 418.0 11.0 0.7 3.7 13.8 144.0 Czech 3.2 24.9 27.1 English 136.0 2.5 0.2 3.8 12.4 88.0 German 63.0 1.7 0.03"
W09-1209,burchardt-etal-2006-salsa,0,0.0198371,"Missing"
W09-1209,I08-1012,1,0.714872,"ods. As for the semantic parser, a group of well selected feature templates are used with n-best syntactic features. 1 Our thanks give to the following corpus providers, (Taul´e et al., 2008; Palmer and Xue, 2009; Hajiˇc et al., 2006; Surdeanu et al., 2008; Burchardt et al., 2006) and (Kawahara et al., 2002). 61 Basically, we build our syntactic dependency parsers based on the MSTParser, a freely available implementation2 , whose details are presented in the paper of McDonald and Pereira (2006). Moreover, we exploit rich features for the parsers. We represent features by following the work of Chen et al. (2008) and Koo et al. (2008) and use features based on dependency relations predicted by transition-based parsers (Nivre and McDonald, 2008). Chen et al. (2008) and Koo et al. (2008) proposed the methods to obtain new features from large-scale unlabeled data. In our system, we perform their methods on training data because the closed challenge does not allow to use unlabeled data. In this paper, we call these new additional features rich features. 2.1 Basic Features Firstly, we use all the features presented by McDonald et al. (2006), if they are available in data. Then we add new features for the l"
W09-1209,D07-1097,0,0.04916,"Missing"
W09-1209,kawahara-etal-2002-construction,0,0.011934,"hnology (NICT) and City University of Hong Kong (CityU) for the joint learning task of CoNLL-2009 shared task (Hajiˇc et al., 2009)1 . The system is basically a pipeline of syntactic parser and semantic parser. We use a syntactic parser that uses very rich features and integrates graph- and transition-based methods. As for the semantic parser, a group of well selected feature templates are used with n-best syntactic features. 1 Our thanks give to the following corpus providers, (Taul´e et al., 2008; Palmer and Xue, 2009; Hajiˇc et al., 2006; Surdeanu et al., 2008; Burchardt et al., 2006) and (Kawahara et al., 2002). 61 Basically, we build our syntactic dependency parsers based on the MSTParser, a freely available implementation2 , whose details are presented in the paper of McDonald and Pereira (2006). Moreover, we exploit rich features for the parsers. We represent features by following the work of Chen et al. (2008) and Koo et al. (2008) and use features based on dependency relations predicted by transition-based parsers (Nivre and McDonald, 2008). Chen et al. (2008) and Koo et al. (2008) proposed the methods to obtain new features from large-scale unlabeled data. In our system, we perform their metho"
W09-1209,P08-1068,0,0.10612,"c parser, a group of well selected feature templates are used with n-best syntactic features. 1 Our thanks give to the following corpus providers, (Taul´e et al., 2008; Palmer and Xue, 2009; Hajiˇc et al., 2006; Surdeanu et al., 2008; Burchardt et al., 2006) and (Kawahara et al., 2002). 61 Basically, we build our syntactic dependency parsers based on the MSTParser, a freely available implementation2 , whose details are presented in the paper of McDonald and Pereira (2006). Moreover, we exploit rich features for the parsers. We represent features by following the work of Chen et al. (2008) and Koo et al. (2008) and use features based on dependency relations predicted by transition-based parsers (Nivre and McDonald, 2008). Chen et al. (2008) and Koo et al. (2008) proposed the methods to obtain new features from large-scale unlabeled data. In our system, we perform their methods on training data because the closed challenge does not allow to use unlabeled data. In this paper, we call these new additional features rich features. 2.1 Basic Features Firstly, we use all the features presented by McDonald et al. (2006), if they are available in data. Then we add new features for the languages having FEAT i"
W09-1209,E06-1011,0,0.013162,"rser and semantic parser. We use a syntactic parser that uses very rich features and integrates graph- and transition-based methods. As for the semantic parser, a group of well selected feature templates are used with n-best syntactic features. 1 Our thanks give to the following corpus providers, (Taul´e et al., 2008; Palmer and Xue, 2009; Hajiˇc et al., 2006; Surdeanu et al., 2008; Burchardt et al., 2006) and (Kawahara et al., 2002). 61 Basically, we build our syntactic dependency parsers based on the MSTParser, a freely available implementation2 , whose details are presented in the paper of McDonald and Pereira (2006). Moreover, we exploit rich features for the parsers. We represent features by following the work of Chen et al. (2008) and Koo et al. (2008) and use features based on dependency relations predicted by transition-based parsers (Nivre and McDonald, 2008). Chen et al. (2008) and Koo et al. (2008) proposed the methods to obtain new features from large-scale unlabeled data. In our system, we perform their methods on training data because the closed challenge does not allow to use unlabeled data. In this paper, we call these new additional features rich features. 2.1 Basic Features Firstly, we use"
W09-1209,W06-2932,0,0.0235357,"atures for the parsers. We represent features by following the work of Chen et al. (2008) and Koo et al. (2008) and use features based on dependency relations predicted by transition-based parsers (Nivre and McDonald, 2008). Chen et al. (2008) and Koo et al. (2008) proposed the methods to obtain new features from large-scale unlabeled data. In our system, we perform their methods on training data because the closed challenge does not allow to use unlabeled data. In this paper, we call these new additional features rich features. 2.1 Basic Features Firstly, we use all the features presented by McDonald et al. (2006), if they are available in data. Then we add new features for the languages having FEAT information (Hajiˇc et al., 2009). FEAT is a set of morphological-features, e.g. more detailed part of speech, number, gender, etc. We try to align different types of morphological-features. For example, 2 http://mstparser.sourceforge.net Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL): Shared Task, pages 61–66, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics we can obtain a sequence of gender tags of all words from a head h to its d"
W09-1209,P08-1108,0,0.0780057,"ks give to the following corpus providers, (Taul´e et al., 2008; Palmer and Xue, 2009; Hajiˇc et al., 2006; Surdeanu et al., 2008; Burchardt et al., 2006) and (Kawahara et al., 2002). 61 Basically, we build our syntactic dependency parsers based on the MSTParser, a freely available implementation2 , whose details are presented in the paper of McDonald and Pereira (2006). Moreover, we exploit rich features for the parsers. We represent features by following the work of Chen et al. (2008) and Koo et al. (2008) and use features based on dependency relations predicted by transition-based parsers (Nivre and McDonald, 2008). Chen et al. (2008) and Koo et al. (2008) proposed the methods to obtain new features from large-scale unlabeled data. In our system, we perform their methods on training data because the closed challenge does not allow to use unlabeled data. In this paper, we call these new additional features rich features. 2.1 Basic Features Firstly, we use all the features presented by McDonald et al. (2006), if they are available in data. Then we add new features for the languages having FEAT information (Hajiˇc et al., 2009). FEAT is a set of morphological-features, e.g. more detailed part of speech, nu"
W09-1209,W03-3017,0,0.0110636,"graph-based and transition-based parsers. Here, we represent features based on dependency relations predicted by transition-based parsers for graphbased parser. Based on the results on development data, we choose the MaltParser for Catalan, Czech, German, and Spanish, and choose another MaxEntbased parser for Chinese, English, and Japanese. 2.4.1 A Transition-based Parser: MaltParser For Catalan, Czech, German, and Spanish, we use the MaltParser, a freely available implementa4 http://www.cs.berkeley.edu/˜pliang/software/browncluster-1.2.zip tion5 , whose details are presented in the paper of Nivre (2003). More information about the parser can be available in the paper (Nivre, 2003). Due to computational cost, we do not select new feature templates for the MaltParser. Following the features settings of Hall et al. (2007), we use their Czech feature file and Catalan feature file. To simply, we apply Czech feature file for German too, and apply Catalan feature file for Spanish. 2.4.2 Another Transition-based Parser: MaxEnt-based Parser In three highly projective language, Chinese, English and Japanese, we use the maximum entropy syntactic dependency parser as in Zhao and Kit (2008). We still use"
W09-1209,W08-2121,0,0.0249233,"Missing"
W09-1209,taule-etal-2008-ancora,0,0.0239121,"Missing"
W09-1209,W08-2127,1,0.761054,"nted in the paper of Nivre (2003). More information about the parser can be available in the paper (Nivre, 2003). Due to computational cost, we do not select new feature templates for the MaltParser. Following the features settings of Hall et al. (2007), we use their Czech feature file and Catalan feature file. To simply, we apply Czech feature file for German too, and apply Catalan feature file for Spanish. 2.4.2 Another Transition-based Parser: MaxEnt-based Parser In three highly projective language, Chinese, English and Japanese, we use the maximum entropy syntactic dependency parser as in Zhao and Kit (2008). We still use the similar feature notations of that work. We use the same greedy feature selection of Zhao et al. (2009) to determine an optimal feature template set for each language. Full feature sets for the three languages can be found at website, http://bcmi.sjtu.edu.cn/˜zhaohai. 2.4.3 Feature Representation For training data, we use 2-way jackknifing to generate predicted dependency parsing trees by two transition-based parsers. Following the features of Nivre and McDonald (2008), we define features for a head h and its dependent d with label l as shown in table 2, where GT ran refers t"
W09-1209,W09-1208,1,0.895734,"omputational cost, we do not select new feature templates for the MaltParser. Following the features settings of Hall et al. (2007), we use their Czech feature file and Catalan feature file. To simply, we apply Czech feature file for German too, and apply Catalan feature file for Spanish. 2.4.2 Another Transition-based Parser: MaxEnt-based Parser In three highly projective language, Chinese, English and Japanese, we use the maximum entropy syntactic dependency parser as in Zhao and Kit (2008). We still use the similar feature notations of that work. We use the same greedy feature selection of Zhao et al. (2009) to determine an optimal feature template set for each language. Full feature sets for the three languages can be found at website, http://bcmi.sjtu.edu.cn/˜zhaohai. 2.4.3 Feature Representation For training data, we use 2-way jackknifing to generate predicted dependency parsing trees by two transition-based parsers. Following the features of Nivre and McDonald (2008), we define features for a head h and its dependent d with label l as shown in table 2, where GT ran refers to dependency parsing trees generated by the MaltParser or MaxEnt-base Parser and ∗ refers to any label. All features are"
W10-1706,P02-1038,0,0.0295564,"achine Intelligence Lab of Shanghai Jiao Tong University (SJTU-BCMI Lab). The system is based on the state-of-the-art SMT toolkit MOSES (Koehn et al., 2007). We use it to translate German, French and Spanish into English. Though different development sets used for training parameter tuning will certainly lead to quite different performance, we empirically find that the more sets we combine together, the more stable the performance is, and a development set similar with test set will help the performance improvement. 2 System Description The basic model of the our system is a log-linear model (Och and Ney, 2002). For given source lan∗ This work was partially supported by the National Natural Science Foundation of China (Grant No. 60903119, Grant No. 60773090 and Grant No. 90820018), the National Basic Research Program of China (Grant No. 2009CB320901), and the National High-Tech Research Program of China (Grant No.2008AA02Z315). † corresponding author 67 Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 67–71, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics de fr es small large small large small large sentences 1540549 1"
W10-1706,J03-1002,0,0.00462666,"English, two sets of bilingual corpora are provided by the shared task organizer. The first set is the new release (version 5) of Europarl corpus which is the smaller. The second is a combination of other available data sets which is the larger. In detail, two corpora, europarl-v5 and news-commentary10 are for German, europarl-v5 and news-commentary10 plus undoc for French and Spanish, respectively. Details of training data are in Table 1. Only sentences with length 1 to 40 are acceptable for our task. We used the larger set for our primary submission. We adopt word alignment toolkit GIZA++ (Och and Ney, 2003) to learn word-level alignment with its default setting and grow-diag-final-and parameters. Given a sentence pair and its corresponding word-level alignment, phrases will be extracted by using the approach in (Och and Ney, 2004). Phrase probability is estimated by its relative frequency in the training corpus. Lexical reordering is determined by using the default setting of MOSES with msd-bidirectional parameter. For training the only language model (English), the data sets are extracted from monolingual parts of both europarl-v5 and news-commentary10, Introduction We present a machine transla"
W10-1706,J04-4002,0,0.034962,"ich is the larger. In detail, two corpora, europarl-v5 and news-commentary10 are for German, europarl-v5 and news-commentary10 plus undoc for French and Spanish, respectively. Details of training data are in Table 1. Only sentences with length 1 to 40 are acceptable for our task. We used the larger set for our primary submission. We adopt word alignment toolkit GIZA++ (Och and Ney, 2003) to learn word-level alignment with its default setting and grow-diag-final-and parameters. Given a sentence pair and its corresponding word-level alignment, phrases will be extracted by using the approach in (Och and Ney, 2004). Phrase probability is estimated by its relative frequency in the training corpus. Lexical reordering is determined by using the default setting of MOSES with msd-bidirectional parameter. For training the only language model (English), the data sets are extracted from monolingual parts of both europarl-v5 and news-commentary10, Introduction We present a machine translation system that represents our participation for the WMT10 shared task from Brain-like Computing and Machine Intelligence Lab of Shanghai Jiao Tong University (SJTU-BCMI Lab). The system is based on the state-of-the-art SMT too"
W10-1706,P03-1021,0,0.0134358,"opment set is chosen. The solid lines represents the performances of 10 incremental batch sets on the two test sets, the batch processing still gives a poor performance at the beginning, but the results become better and more stable when the development sets are continuously enlarged. This sort of results suggest that a combined development set may produce reliable results in the worst case. Our primary submission used the combined development set and the results as Table 4. log pλM (ti |si )} 1 As usual, minimum error rate training (MERT) is adopted for log-linear model parameter estimation (Och, 2003). There are many improvements on MERT in existing work (Bertoldi et al., 2009; Foster and Kuhn, 2009), but there is no demonstration that the weights with better performance on the development set would lead to a better result on the unseen test set. In our experiments, we found that different development sets will cause significant BLEU score differences, even as high as one percent. Thus the remained problem will be how to effectively choose the development set to obtain a better and more stable performance. 68 09-dev 16.46 16.67 16.74 16.15 16.44 16.50 17.15 16.51 17.03 16.25 09-batch 16.46"
W10-1706,W09-0418,0,0.0270243,"dotted lines have the inverse trend with the dotted in Figure 1(because the addition of these two values is constant), and the solid lines have the same trend with the dotted, which means that the good performance is mutual between test set and development sets: if tuning using A set could make a good result over B set, then vice versa. 3.6 Related Work The special challenge of the WMT shared task is domain adaptation, which is a hot topic in recent years and more relative to our experiments. Many existing works are about this topic (Koehn and Schroeder, 2007; Nakov, 2008; Nakov and Ng, 2009; Paul et al., 2009; Haque et al., 2009). However, most of previous works focus on language 3.5 On the Similarity between Development Set and Test Set This experiment is motivated by (Utiyama et al., 2009), where they used BLEU score to measure the similarity of a sentences pair and then extracted sentences similar with those in test set to 69 model, translation phrase table, lexicons model and factored translation model, few of them pay attention to the domain adaptation on the development set. For future work we consider to use some machine learning approaches to select sentences in development sets more relev"
W10-1706,W09-0439,0,0.0184323,"sets on the two test sets, the batch processing still gives a poor performance at the beginning, but the results become better and more stable when the development sets are continuously enlarged. This sort of results suggest that a combined development set may produce reliable results in the worst case. Our primary submission used the combined development set and the results as Table 4. log pλM (ti |si )} 1 As usual, minimum error rate training (MERT) is adopted for log-linear model parameter estimation (Och, 2003). There are many improvements on MERT in existing work (Bertoldi et al., 2009; Foster and Kuhn, 2009), but there is no demonstration that the weights with better performance on the development set would lead to a better result on the unseen test set. In our experiments, we found that different development sets will cause significant BLEU score differences, even as high as one percent. Thus the remained problem will be how to effectively choose the development set to obtain a better and more stable performance. 68 09-dev 16.46 16.67 16.74 16.15 16.44 16.50 17.15 16.51 17.03 16.25 09-batch 16.46 16.25 16.20 16.83 16.73 16.97 17.03 17.00 16.97 16.99 08-dev 16.38 16.66 16.94 16.18 16.64 16.75 17."
W10-1706,2009.iwslt-evaluation.12,0,0.284928,"Missing"
W10-1706,W07-0733,0,0.0385952,"the test set under self tuning, newstest2009 is 17.91). The dotted lines have the inverse trend with the dotted in Figure 1(because the addition of these two values is constant), and the solid lines have the same trend with the dotted, which means that the good performance is mutual between test set and development sets: if tuning using A set could make a good result over B set, then vice versa. 3.6 Related Work The special challenge of the WMT shared task is domain adaptation, which is a hot topic in recent years and more relative to our experiments. Many existing works are about this topic (Koehn and Schroeder, 2007; Nakov, 2008; Nakov and Ng, 2009; Paul et al., 2009; Haque et al., 2009). However, most of previous works focus on language 3.5 On the Similarity between Development Set and Test Set This experiment is motivated by (Utiyama et al., 2009), where they used BLEU score to measure the similarity of a sentences pair and then extracted sentences similar with those in test set to 69 model, translation phrase table, lexicons model and factored translation model, few of them pay attention to the domain adaptation on the development set. For future work we consider to use some machine learning approache"
W10-1706,W09-0412,0,0.0112677,"2009 is 17.91). The dotted lines have the inverse trend with the dotted in Figure 1(because the addition of these two values is constant), and the solid lines have the same trend with the dotted, which means that the good performance is mutual between test set and development sets: if tuning using A set could make a good result over B set, then vice versa. 3.6 Related Work The special challenge of the WMT shared task is domain adaptation, which is a hot topic in recent years and more relative to our experiments. Many existing works are about this topic (Koehn and Schroeder, 2007; Nakov, 2008; Nakov and Ng, 2009; Paul et al., 2009; Haque et al., 2009). However, most of previous works focus on language 3.5 On the Similarity between Development Set and Test Set This experiment is motivated by (Utiyama et al., 2009), where they used BLEU score to measure the similarity of a sentences pair and then extracted sentences similar with those in test set to 69 model, translation phrase table, lexicons model and factored translation model, few of them pay attention to the domain adaptation on the development set. For future work we consider to use some machine learning approaches to select sentences in developm"
W10-1706,Y09-2027,0,\N,Missing
W10-1706,W08-0320,0,\N,Missing
W10-1706,P07-2045,0,\N,Missing
W10-2409,W02-1001,0,0.00994382,"ates are generated for each source name. 3 Algorithm 1 Averaged perceptron training Input: Candidate list with reference {LIST (xj , yj )nj=1 , yi∗ }N i=1 Output: Averaged parameters 1: ω ~ ← 0, ω ~ a ← 0, c ← 1 2: for t = 1 to T do 3: for i = 1 to N do 4: yˆi ← argmaxy∈LIST (xj ,yj ) ω ~ · Φ(xi , yi ) 5: if yˆi 6= yi∗ then 6: ω ~ ←ω ~ + Φ(x∗i , yi∗ ) − Φ(ˆ xi , yˆi ) 7: ω ~a ← ω ~ a + c · {Φ(x∗i , yi∗ ) − Φ(ˆ xi , yˆi )} 8: end if 9: c←c+1 10: end for 11: end for 12: return ω ~ −ω ~ a /c Reranking 3.1 Learning Framework For reranking training and prediction, we adopt the averaged perceptron (Collins, 2002) as our learning framework, which has a more stable performance than the non-averaged version. It is presented in Algorithm 1. Where ω ~ is the vector of parameters we want to optimize, x, y are the corresponding source (with different syllabification) and target graphemes in the candidate list, and Φ represents the feature vector in the pair of x and y. In this algorithm, reference yi∗ is the most appropriate output in the candidate list according to the true target named entity in the training data. We use the Mean-F score to identify which candidate can be the reference, by locating the one"
W10-2409,P04-1021,0,0.394968,"sult from the candidates. We only consider bi-grams when using this feature. Target grapheme chain feature, f (tii−2 ); This feature measures the appropriateness of the generated target graphemes on both character and syllables level. It performs in a similar way as the language model for SMT decoding. We use tri-gram syllables in this learning framework. 3.2 Multiple Features The following features are used in our reranking process: Paired source-to-target transition feature, f (< s, t >ii−1 ); Transliteration correspondence feature, f (si , ti ); This type of feature is firstly proposed in (Li et al., 2004), aiming at generating source and target graphemes simultaneously under a suitable constraint. We use this feature to restrict the synchronous transition of both source and target graphemes, measuring how well are those transitions, such as for “st”, This feature describes the mapping between source and target graphemes, similar to the transliteration options in the phrase table in our previous generation process, where s and 2 In this work, we use Pinyin as the phonetic representation for Chinese. 63 whether “s” transliterated by “斯” is followed by “t” transliterated by “特”. In order to deal"
W10-2409,P03-1021,0,0.012959,"62–65, c Uppsala, Sweden, 16 July 2010. 2010 Association for Computational Linguistics try in it. After that, we use a phoneme resource2 to refine the phrase table by filtering out the wrongly extracted phrases and cleaning up the noise in it. In the decoding process, a dynamic pruning is performed when generating the hypothesis in each step, in which the threshold is variable according to the current searching space, for we need to obtain a good candidate list as precise as possible for the next stage. The parameter for each feature function in log-linear model is optimized by MERT training (Och, 2003). Finally, a maximum number of 50 candidates are generated for each source name. 3 Algorithm 1 Averaged perceptron training Input: Candidate list with reference {LIST (xj , yj )nj=1 , yi∗ }N i=1 Output: Averaged parameters 1: ω ~ ← 0, ω ~ a ← 0, c ← 1 2: for t = 1 to T do 3: for i = 1 to N do 4: yˆi ← argmaxy∈LIST (xj ,yj ) ω ~ · Φ(xi , yi ) 5: if yˆi 6= yi∗ then 6: ω ~ ←ω ~ + Φ(x∗i , yi∗ ) − Φ(ˆ xi , yˆi ) 7: ω ~a ← ω ~ a + c · {Φ(x∗i , yi∗ ) − Φ(ˆ xi , yˆi )} 8: end if 9: c←c+1 10: end for 11: end for 12: return ω ~ −ω ~ a /c Reranking 3.1 Learning Framework For reranking training and predic"
W10-2409,W09-3506,0,0.0503016,"sult, it is found in (Song et al., 2009) that, in contrast to the ordinary top-1 accuracy (ACC) score, its recall rate, which is defined in terms of whether the correct answer is generated in the n-best output list, is rather high. This observation suggests that if we could rearrange those outputs into a better order, especially, push the correct one to the top, the overall performance could be enhanced significantly, without any further refinement of the original generation process. This reranking strategy is proved to be efficient in transliteration generation with a multi-engine approach (Oh et al., 2009). In this paper, we present our recent work on reranking the transliteration candidates via an online discriminative learning framework, namely, the averaged perceptron. Multiple features are incorporated into it for performance enhancement. The following sections will give the technical details of our method and present its results for NEWS2010 shared task for named entity transliteration. Effective transliteration of proper names via grapheme conversion needs to find transliteration patterns in training data, and then generate optimized candidates for testing samples accordingly. However, th"
W10-2409,W09-3511,1,0.92429,"it is still unreliable to rank the candidates simply by their statistical translation scores for the purpose of selecting the best one. In order to make a proper choice, the direct orthographic mapping requires a precise alignment and a better transliteration option selection. Thus, powerful algorithms for effective use of the parallel data is indispensable, especially when the available data is limited in volume. Interestingly, although an SMT based approach could not achieve a precise top-1 transliteration re2 Generation For the generation of transliteration candidates, we follow the work (Song et al., 2009), using a phrase-based SMT procedure with the log-linear model P exp[ ni=1 λi hi (s, t)] Pn P (t|s) = P t exp[ i=1 λi hi (s, t)] (1) for decoding. Originally we use two directional phrase1 tables, which are learned for both directions of source-to-target and target-to-source, containing different entries of transliteration options. In order to facilitate the decoding by exploiting all possible choices in a better way, we combine the forward and backward directed phrase tables together, and recalculate the probability for each en1 It herein refers to a character sequence as described in (Song e"
W10-3013,W06-1617,0,0.0139458,"(dprel). Syntactic Connection. This includes syntactic head (h), left(right) farthest(nearest) child (lm, ln, rm and rn) and high (low) support verb, noun or preposition. Here we specify the last one as an example, support verb(noun/preposition). From a given word to the syntactic root along the syntactic tree, the first verb/noun/preposition that is met is called its low support verb/noun/preposition, and the nearest one to the root(farthest to the given word) is called as its high support verb/noun/preposition. The concept of support verb was broadly used (Toutanova et al., 2005; Xue, 2006; Jiang and Ng, 2006), and it is extended to nouns and prepositions in Zhao et al. (2009b). In addition, a slightly modified syntactic head, pphead, is introduced, it returns the left most sibling of a given word if the word is headed by a preposition, otherwise it returns the original head. Path. There are two basic types of path. One is the linear path (linePath) in the sequence, the other is the path in the syntactic parsing tree (dpPath). For example, m:n|dpPath represents the dependency path from word m to n. Assuming that the two paths from m and n to the root are 3.2 Feature template sets for each task As o"
W10-3013,W08-0607,0,0.12836,"tion while the first step of scope finding aims at high accuracy of labeling hedge cues. Therefore, three independent procedures of feature selection are conducted for BioScope corpus dataset. As Wikipedia is not involved in the task of scope finding, it only needs one final feature set. About 200 feature templates are initially considered for each task. We mainly borrow ideas and are enlightened by following sources while initializing feature template sets: Feature selection a) Previous papers on hedge detection and scope finding (Light et al., 2004; Medlock, 2008; Medlock and Briscoe, 2008; Kilicoglu and Bergler, 2008; Szarvas, 2008; Ganter and Strube, 2009; Morante and Daelemans, 2009); Since hedge and scope finding are quite novel tasks and it is not easy to determine the effective features by experience, a greedy feature selection is conducted. As it mentioned in section 2, our system divides scope finding into two sub-tasks: 94 pm and pn , m:n|dpPathShare, m:n|dpPathPred and m:n|dpPathArgu represent the common part of pm and pn , part of pm which does not belong to pn and part of pn which does not belong to pm , respectively. Family. A children set includes all syntactic children(children) are used in"
W10-3013,W04-3103,0,0.761849,"other for scope finding. In particular, various kinds of syntactic features are systemically exploited and effectively integrated using a large-scale normalized feature selection method. Evaluation on the CoNLL-2010 shared task shows that our system achieves stable and competitive results for all the closed tasks. Furthermore, post-deadline experiments show that the performance can be much further improved using a sufficient feature selection. 1 Introduction Hedges are linguistic devices representing speculative parts of articles. Previous works such as (Hyland, 1996; Marco and Mercer, 2004; Light et al., 2004; Thompson et al., 2008) present research on hedge mainly as a linguistic phenomenon. Meanwhile, detecting hedges and their scopes automatically are increasingly important tasks in natural language processing and information extraction, especially in biomedical community. The shared task of CoNLL-2010 described in Farkas et al. (2010) aims at detecting hedges (task 1) and finding their scopes (task 2) for the literature 2 Methods Basically, the tasks are formulated as sequence labeling in our approach. The available label set differs between task 1 and 2. In addition, it is needed to introduce"
W10-3013,H05-1066,0,0.0129845,"in any additional hedge cues from other resources. An indicator (indicator) is given for multi-hedge scope finding, as specified in section 2.At last, in feature set for scope labeling, hedge represents that the word is in a hedge cue. At last, we take x as current token to be labeled, and xm to denote neighbor words. m > 0 represents that it is a word goes mth after current word and m &lt; 0 for word −mth before current word. b) Related works such as named entity recognition (Collins, 1999) and text chunking (Zhang et al., 2001); c) Some literature on dependency parsing (Nivre and Scholz, 2004; McDonald et al., 2005; Nivre, 2009; Zhao et al., 2009c; Zhao et al., 2009a); 3.1 Notations of Feature Template A large amount of advanced syntactic features including syntactic connections, paths, families and their concatenations are introduced. Many of these features come from dependency parsing, which aims at building syntactic tree expressed by dependencies between words. More details about dependency parsing are given in Nivre and Scholz (2004) and McDonald et al. (2005). The parser in Zhao et al. (2009a) is used to construct dependency structures in our system, and some of the notations in this paper adopt t"
W10-3013,D09-1004,1,0.875918,"Missing"
W10-3013,W09-1304,0,0.527458,"o consideration and be treated as the head and tail tokens of the scopes of specific hedge cues. Furthermore , inhibition can be blocked by actinomycin D , indicating a requirement for de novo transcription . ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... Hedge 2.3 Scope finding for multi-hedge cases Sentences with more than one hedge cue are quite common in both datasets of BioScope corpus and Wikipedia. By counting hedges in every sentence, we find that about one fourth of the sentences with hedges have more than one hedge cue in all three data sources (Table 2). In Morante and Daelemans (2009), three classifiers predict whether each token is Begin, End or None and a postprocessing is needed to associate Begins and Ends with their corresponding hedge cues. In our approach, in order to decrease ambiguous or illegal outputs e.g. inequivalent numbers of Begins and Ends, a pair of Begin and End without their corresponding hedge cue between them, etc., sentences with more than one hedge cue will be preprocessed by making copies as many as the number of hedges and be handled separately. The sentence which is selected as a sample has two hedge cues: “suggesting” and “may”, so our system pr"
W10-3013,C04-1010,0,0.0161011,"nonopen, we do not put in any additional hedge cues from other resources. An indicator (indicator) is given for multi-hedge scope finding, as specified in section 2.At last, in feature set for scope labeling, hedge represents that the word is in a hedge cue. At last, we take x as current token to be labeled, and xm to denote neighbor words. m > 0 represents that it is a word goes mth after current word and m &lt; 0 for word −mth before current word. b) Related works such as named entity recognition (Collins, 1999) and text chunking (Zhang et al., 2001); c) Some literature on dependency parsing (Nivre and Scholz, 2004; McDonald et al., 2005; Nivre, 2009; Zhao et al., 2009c; Zhao et al., 2009a); 3.1 Notations of Feature Template A large amount of advanced syntactic features including syntactic connections, paths, families and their concatenations are introduced. Many of these features come from dependency parsing, which aims at building syntactic tree expressed by dependencies between words. More details about dependency parsing are given in Nivre and Scholz (2004) and McDonald et al. (2005). The parser in Zhao et al. (2009a) is used to construct dependency structures in our system, and some of the notation"
W10-3013,W09-1208,1,0.930701,"her resources. An indicator (indicator) is given for multi-hedge scope finding, as specified in section 2.At last, in feature set for scope labeling, hedge represents that the word is in a hedge cue. At last, we take x as current token to be labeled, and xm to denote neighbor words. m > 0 represents that it is a word goes mth after current word and m &lt; 0 for word −mth before current word. b) Related works such as named entity recognition (Collins, 1999) and text chunking (Zhang et al., 2001); c) Some literature on dependency parsing (Nivre and Scholz, 2004; McDonald et al., 2005; Nivre, 2009; Zhao et al., 2009c; Zhao et al., 2009a); 3.1 Notations of Feature Template A large amount of advanced syntactic features including syntactic connections, paths, families and their concatenations are introduced. Many of these features come from dependency parsing, which aims at building syntactic tree expressed by dependencies between words. More details about dependency parsing are given in Nivre and Scholz (2004) and McDonald et al. (2005). The parser in Zhao et al. (2009a) is used to construct dependency structures in our system, and some of the notations in this paper adopt those presented in Zhao et al. (2"
W10-3013,P09-1040,0,0.0149305,"cues from other resources. An indicator (indicator) is given for multi-hedge scope finding, as specified in section 2.At last, in feature set for scope labeling, hedge represents that the word is in a hedge cue. At last, we take x as current token to be labeled, and xm to denote neighbor words. m > 0 represents that it is a word goes mth after current word and m &lt; 0 for word −mth before current word. b) Related works such as named entity recognition (Collins, 1999) and text chunking (Zhang et al., 2001); c) Some literature on dependency parsing (Nivre and Scholz, 2004; McDonald et al., 2005; Nivre, 2009; Zhao et al., 2009c; Zhao et al., 2009a); 3.1 Notations of Feature Template A large amount of advanced syntactic features including syntactic connections, paths, families and their concatenations are introduced. Many of these features come from dependency parsing, which aims at building syntactic tree expressed by dependencies between words. More details about dependency parsing are given in Nivre and Scholz (2004) and McDonald et al. (2005). The parser in Zhao et al. (2009a) is used to construct dependency structures in our system, and some of the notations in this paper adopt those presente"
W10-3013,W08-0606,0,0.210283,"Missing"
W10-3013,P08-1033,0,0.578306,"ent Systems Shanghai Jiao Tong University 3 School of Computer Science and Technology, Soochow University zhangsd.sjtu@gmail.com, zhaohai@cs.sjtu.edu.cn gdzhou@suda.edu.cn, blu@cs.sjtu.edu.cn Abstract from BioScope corpus (Szarvas et al., 2008) and Wikipedia. This paper describes a system adopting sequence labeling which performs competitive in the official evaluation, as well as further test. In addition, a large-scale feature selection procedure is applied in training and development. Considering that BioScope corpus is annotated by two independent linguists according to a formal guideline (Szarvas, 2008), while Wikipedia weasels are tagged by netizens who are diverse in background and various in evaluation criterion, it is needed to handle them separately. Our system selects features for Wikipedia and BioScope corpus independently and evaluate them respectively, leading to fine performances for all of them. The rest of the paper is organized as follows. The next section presents the technical details of our system of hedge detection and scope finding. Section 3 gives information of features. Section 4 shows the evaluation results, including official results and further ones after official out"
W10-3013,P05-1073,0,0.0447631,"Missing"
W10-3013,N06-1055,0,0.0134354,"ency label (dprel). Syntactic Connection. This includes syntactic head (h), left(right) farthest(nearest) child (lm, ln, rm and rn) and high (low) support verb, noun or preposition. Here we specify the last one as an example, support verb(noun/preposition). From a given word to the syntactic root along the syntactic tree, the first verb/noun/preposition that is met is called its low support verb/noun/preposition, and the nearest one to the root(farthest to the given word) is called as its high support verb/noun/preposition. The concept of support verb was broadly used (Toutanova et al., 2005; Xue, 2006; Jiang and Ng, 2006), and it is extended to nouns and prepositions in Zhao et al. (2009b). In addition, a slightly modified syntactic head, pphead, is introduced, it returns the left most sibling of a given word if the word is headed by a preposition, otherwise it returns the original head. Path. There are two basic types of path. One is the linear path (linePath) in the sequence, the other is the path in the syntactic parsing tree (dpPath). For example, m:n|dpPath represents the dependency path from word m to n. Assuming that the two paths from m and n to the root are 3.2 Feature template se"
W10-3013,P01-1069,0,0.0308878,"dic) is introduced into feature templates. As the evaluation is nonopen, we do not put in any additional hedge cues from other resources. An indicator (indicator) is given for multi-hedge scope finding, as specified in section 2.At last, in feature set for scope labeling, hedge represents that the word is in a hedge cue. At last, we take x as current token to be labeled, and xm to denote neighbor words. m > 0 represents that it is a word goes mth after current word and m &lt; 0 for word −mth before current word. b) Related works such as named entity recognition (Collins, 1999) and text chunking (Zhang et al., 2001); c) Some literature on dependency parsing (Nivre and Scholz, 2004; McDonald et al., 2005; Nivre, 2009; Zhao et al., 2009c; Zhao et al., 2009a); 3.1 Notations of Feature Template A large amount of advanced syntactic features including syntactic connections, paths, families and their concatenations are introduced. Many of these features come from dependency parsing, which aims at building syntactic tree expressed by dependencies between words. More details about dependency parsing are given in Nivre and Scholz (2004) and McDonald et al. (2005). The parser in Zhao et al. (2009a) is used to const"
W10-3013,W09-1209,1,0.924063,"her resources. An indicator (indicator) is given for multi-hedge scope finding, as specified in section 2.At last, in feature set for scope labeling, hedge represents that the word is in a hedge cue. At last, we take x as current token to be labeled, and xm to denote neighbor words. m > 0 represents that it is a word goes mth after current word and m &lt; 0 for word −mth before current word. b) Related works such as named entity recognition (Collins, 1999) and text chunking (Zhang et al., 2001); c) Some literature on dependency parsing (Nivre and Scholz, 2004; McDonald et al., 2005; Nivre, 2009; Zhao et al., 2009c; Zhao et al., 2009a); 3.1 Notations of Feature Template A large amount of advanced syntactic features including syntactic connections, paths, families and their concatenations are introduced. Many of these features come from dependency parsing, which aims at building syntactic tree expressed by dependencies between words. More details about dependency parsing are given in Nivre and Scholz (2004) and McDonald et al. (2005). The parser in Zhao et al. (2009a) is used to construct dependency structures in our system, and some of the notations in this paper adopt those presented in Zhao et al. (2"
W10-4146,J04-4004,0,0.0361659,"ion is also reasonably high. 5.4 Comparison with Previous Works We comparison our approach with previous works of 2009 CIPS-ParsEval shared task. The data set and evaluation measures of 2009 CIPS-ParsEval shared task, which are quite different from that of 2010 CIPSParsEval shared task, are used in this experiment for the comparison purpose. Table 6 shows the comparison. We compare our method with several main parsers on the official data set of 2009 CIPS-ParsEval shared task. All these results are evaluated with official evaluation tool by the 2009 CIPS-ParsEval shared task. Bikel’s parser4 (Bikel, 2004) in Table 6 is a implementation of Collins’ head-driven statistical model (Collins, 2003). The Stanford parser5 is based on the factored model described in (Klein and Manning, 2002). The Charniak’s parser6 is based on the parsing model described in (Charniak, 2000). Berkeley parser7 is based on unlexicalized parsing model described in (Petrov and Klein, 2007). According to Table 6, the performance of our method is better than all the four parsers described above. Chen et al. (2009) and Jiang et al. (2009) both make use of combination of multiple parsers and achieve considerably high performanc"
W10-4146,H91-1060,0,0.411913,"Missing"
W10-4146,J03-4003,0,0.11689,"ach with previous works of 2009 CIPS-ParsEval shared task. The data set and evaluation measures of 2009 CIPS-ParsEval shared task, which are quite different from that of 2010 CIPSParsEval shared task, are used in this experiment for the comparison purpose. Table 6 shows the comparison. We compare our method with several main parsers on the official data set of 2009 CIPS-ParsEval shared task. All these results are evaluated with official evaluation tool by the 2009 CIPS-ParsEval shared task. Bikel’s parser4 (Bikel, 2004) in Table 6 is a implementation of Collins’ head-driven statistical model (Collins, 2003). The Stanford parser5 is based on the factored model described in (Klein and Manning, 2002). The Charniak’s parser6 is based on the parsing model described in (Charniak, 2000). Berkeley parser7 is based on unlexicalized parsing model described in (Petrov and Klein, 2007). According to Table 6, the performance of our method is better than all the four parsers described above. Chen et al. (2009) and Jiang et al. (2009) both make use of combination of multiple parsers and achieve considerably high performance. 4 http://www.cis.upenn.edu/˜dbikel/ software.html 5 http://nlp.stanford.edu/software/"
W10-4146,P04-1054,0,0.035266,"to constituent structures using a machine learning approach. A conditional random fields (CRF) tagger is adopted for head information recognition. Our experiments shows that acceptable parsing and head tagging results are obtained on our approaches. 1 Introduction Constituent parsing is a challenging but useful task aiming at analyzing the constituent structure of a sentence. Recently, it is widely adopted by the popular applications of natural language processing techniques, such as machine translation (Ding and Palmer, 2005), synonym generation (Shinyama et al., 2002), relation extraction (Culotta and Sorensen, 2004) and lexical resource augmentation (Snow et al., 2004). A great deal of researches have been conducted on this topic with promising progress (Magerman, 1995; Collins, 1999; Charniak, 2000; Charniak and Johnson, 2005; Sagae and Lavie, 2006; Petrov and Klein, 2007; Finkel et al., 2008; Huang, 2008). Recently, several effective dependency parsing algorithms has been developed and shows excellent performance in the responding parsing tasks (McDonald, 2006; Nivre and Scholz, 2004). Since graph structures of dependency and constituent parsing over a sentence are strongly related, they should be bene"
W10-4146,P05-1067,0,0.0318543,"k based on a formal dependencyconstituent transformation method which converts dependency to constituent structures using a machine learning approach. A conditional random fields (CRF) tagger is adopted for head information recognition. Our experiments shows that acceptable parsing and head tagging results are obtained on our approaches. 1 Introduction Constituent parsing is a challenging but useful task aiming at analyzing the constituent structure of a sentence. Recently, it is widely adopted by the popular applications of natural language processing techniques, such as machine translation (Ding and Palmer, 2005), synonym generation (Shinyama et al., 2002), relation extraction (Culotta and Sorensen, 2004) and lexical resource augmentation (Snow et al., 2004). A great deal of researches have been conducted on this topic with promising progress (Magerman, 1995; Collins, 1999; Charniak, 2000; Charniak and Johnson, 2005; Sagae and Lavie, 2006; Petrov and Klein, 2007; Finkel et al., 2008; Huang, 2008). Recently, several effective dependency parsing algorithms has been developed and shows excellent performance in the responding parsing tasks (McDonald, 2006; Nivre and Scholz, 2004). Since graph structures o"
W10-4146,P08-1109,0,0.0350287,"challenging but useful task aiming at analyzing the constituent structure of a sentence. Recently, it is widely adopted by the popular applications of natural language processing techniques, such as machine translation (Ding and Palmer, 2005), synonym generation (Shinyama et al., 2002), relation extraction (Culotta and Sorensen, 2004) and lexical resource augmentation (Snow et al., 2004). A great deal of researches have been conducted on this topic with promising progress (Magerman, 1995; Collins, 1999; Charniak, 2000; Charniak and Johnson, 2005; Sagae and Lavie, 2006; Petrov and Klein, 2007; Finkel et al., 2008; Huang, 2008). Recently, several effective dependency parsing algorithms has been developed and shows excellent performance in the responding parsing tasks (McDonald, 2006; Nivre and Scholz, 2004). Since graph structures of dependency and constituent parsing over a sentence are strongly related, they should be benefited from each other. It is true that constituent parsing may be smoothly altered to fit dependency parsing. However, due to the inconvenience from dependency to constituent structure, it is not so easy to adopt the latter ∗ This work is partially supported by the National Natural"
W10-4146,gimenez-marquez-2004-svmtool,0,0.0326679,"TCT has another annotation scheme, which combines both the constituent tree structure and the head information of each constituent. Specifically, there can be always multiple heads in a constituent. For the 2010 CIPS-ParsEval shared task, only segmented sentences are given in test data without part-of-speech (POS) tags, a POS tagger is required for this task. Therefore, we divide our system into three major cascade stages, namely POS tagging, constituent parsing and head information recognition, which are connected as a pipeline of processing. For the POS tagging, we adopt the SVMTool tagger (Gimenez and Marquez, 2004); for the constituent parsing, we use the Maximum Spanning Tree (MST) (McDonald, 2006) parser combined with a dependencies-to-constituents conversion; and for the head information recognition, we apply a sequence labeling method to label head information. Section 2 presents the POS tagger in our approach. The details of our parsing method is presented in section 3. The head information recognition is described in section 4. The data and experimental results are shown in section 5. The last section is the conclusion and future work. 2 POS Tagging The SVMTool tagger (Gimenez and Marquez, 2004) i"
W10-4146,P08-1067,0,0.0265395,"l task aiming at analyzing the constituent structure of a sentence. Recently, it is widely adopted by the popular applications of natural language processing techniques, such as machine translation (Ding and Palmer, 2005), synonym generation (Shinyama et al., 2002), relation extraction (Culotta and Sorensen, 2004) and lexical resource augmentation (Snow et al., 2004). A great deal of researches have been conducted on this topic with promising progress (Magerman, 1995; Collins, 1999; Charniak, 2000; Charniak and Johnson, 2005; Sagae and Lavie, 2006; Petrov and Klein, 2007; Finkel et al., 2008; Huang, 2008). Recently, several effective dependency parsing algorithms has been developed and shows excellent performance in the responding parsing tasks (McDonald, 2006; Nivre and Scholz, 2004). Since graph structures of dependency and constituent parsing over a sentence are strongly related, they should be benefited from each other. It is true that constituent parsing may be smoothly altered to fit dependency parsing. However, due to the inconvenience from dependency to constituent structure, it is not so easy to adopt the latter ∗ This work is partially supported by the National Natural Science Founda"
W10-4146,W98-1115,0,0.0503511,"ulty, our solution is to introduce a formal dependency structure and a machine learning method so that the ambiguity from dependency structures to constituent structures can be dealt with automatically. 3.1.1 Binarization We first transform constituent trees into the form that all productions for all subtrees are either unary or binary, before converting them to dependency structures. Due to the binarization, the target constituent trees of the conversion from dependency back to constituent structures are binary branching. This binarization is done by the left-factoring approach described in (Charniak et al., 1998; Petrov and Klein, 2008), which converts each production with n children, where n &gt; 2, into n − 1 binary productions. Additional non-terminal nodes introduced in this conversion must be clearly marked. Transforming the binary branching trees into arbitrary branching trees is accomplished by using the reverse process. 3.1.2 Using Binary Classifier We train a classifier to decide which dependency edges should be transformed first at each step of conversion automatically. After the binarization described in the previous section, only one dependency edge should be transformed at each step. Theref"
W10-4146,A00-2018,0,0.845249,"ad tagging results are obtained on our approaches. 1 Introduction Constituent parsing is a challenging but useful task aiming at analyzing the constituent structure of a sentence. Recently, it is widely adopted by the popular applications of natural language processing techniques, such as machine translation (Ding and Palmer, 2005), synonym generation (Shinyama et al., 2002), relation extraction (Culotta and Sorensen, 2004) and lexical resource augmentation (Snow et al., 2004). A great deal of researches have been conducted on this topic with promising progress (Magerman, 1995; Collins, 1999; Charniak, 2000; Charniak and Johnson, 2005; Sagae and Lavie, 2006; Petrov and Klein, 2007; Finkel et al., 2008; Huang, 2008). Recently, several effective dependency parsing algorithms has been developed and shows excellent performance in the responding parsing tasks (McDonald, 2006; Nivre and Scholz, 2004). Since graph structures of dependency and constituent parsing over a sentence are strongly related, they should be benefited from each other. It is true that constituent parsing may be smoothly altered to fit dependency parsing. However, due to the inconvenience from dependency to constituent structure, i"
W10-4146,P95-1037,0,0.207456,"that acceptable parsing and head tagging results are obtained on our approaches. 1 Introduction Constituent parsing is a challenging but useful task aiming at analyzing the constituent structure of a sentence. Recently, it is widely adopted by the popular applications of natural language processing techniques, such as machine translation (Ding and Palmer, 2005), synonym generation (Shinyama et al., 2002), relation extraction (Culotta and Sorensen, 2004) and lexical resource augmentation (Snow et al., 2004). A great deal of researches have been conducted on this topic with promising progress (Magerman, 1995; Collins, 1999; Charniak, 2000; Charniak and Johnson, 2005; Sagae and Lavie, 2006; Petrov and Klein, 2007; Finkel et al., 2008; Huang, 2008). Recently, several effective dependency parsing algorithms has been developed and shows excellent performance in the responding parsing tasks (McDonald, 2006; Nivre and Scholz, 2004). Since graph structures of dependency and constituent parsing over a sentence are strongly related, they should be benefited from each other. It is true that constituent parsing may be smoothly altered to fit dependency parsing. However, due to the inconvenience from depende"
W10-4146,C04-1010,0,0.0910317,"ch as machine translation (Ding and Palmer, 2005), synonym generation (Shinyama et al., 2002), relation extraction (Culotta and Sorensen, 2004) and lexical resource augmentation (Snow et al., 2004). A great deal of researches have been conducted on this topic with promising progress (Magerman, 1995; Collins, 1999; Charniak, 2000; Charniak and Johnson, 2005; Sagae and Lavie, 2006; Petrov and Klein, 2007; Finkel et al., 2008; Huang, 2008). Recently, several effective dependency parsing algorithms has been developed and shows excellent performance in the responding parsing tasks (McDonald, 2006; Nivre and Scholz, 2004). Since graph structures of dependency and constituent parsing over a sentence are strongly related, they should be benefited from each other. It is true that constituent parsing may be smoothly altered to fit dependency parsing. However, due to the inconvenience from dependency to constituent structure, it is not so easy to adopt the latter ∗ This work is partially supported by the National Natural Science Foundation of China (Grant No. 60903119, Grant No. 60773090 and Grant No. 90820018), the National Basic Research Program of China (Grant No. 2009CB320901), and the National High-Tech Resear"
W10-4146,N07-1051,0,0.406799,"onstituent parsing is a challenging but useful task aiming at analyzing the constituent structure of a sentence. Recently, it is widely adopted by the popular applications of natural language processing techniques, such as machine translation (Ding and Palmer, 2005), synonym generation (Shinyama et al., 2002), relation extraction (Culotta and Sorensen, 2004) and lexical resource augmentation (Snow et al., 2004). A great deal of researches have been conducted on this topic with promising progress (Magerman, 1995; Collins, 1999; Charniak, 2000; Charniak and Johnson, 2005; Sagae and Lavie, 2006; Petrov and Klein, 2007; Finkel et al., 2008; Huang, 2008). Recently, several effective dependency parsing algorithms has been developed and shows excellent performance in the responding parsing tasks (McDonald, 2006; Nivre and Scholz, 2004). Since graph structures of dependency and constituent parsing over a sentence are strongly related, they should be benefited from each other. It is true that constituent parsing may be smoothly altered to fit dependency parsing. However, due to the inconvenience from dependency to constituent structure, it is not so easy to adopt the latter ∗ This work is partially supported by"
W10-4146,P06-2089,0,0.0168618,"aches. 1 Introduction Constituent parsing is a challenging but useful task aiming at analyzing the constituent structure of a sentence. Recently, it is widely adopted by the popular applications of natural language processing techniques, such as machine translation (Ding and Palmer, 2005), synonym generation (Shinyama et al., 2002), relation extraction (Culotta and Sorensen, 2004) and lexical resource augmentation (Snow et al., 2004). A great deal of researches have been conducted on this topic with promising progress (Magerman, 1995; Collins, 1999; Charniak, 2000; Charniak and Johnson, 2005; Sagae and Lavie, 2006; Petrov and Klein, 2007; Finkel et al., 2008; Huang, 2008). Recently, several effective dependency parsing algorithms has been developed and shows excellent performance in the responding parsing tasks (McDonald, 2006; Nivre and Scholz, 2004). Since graph structures of dependency and constituent parsing over a sentence are strongly related, they should be benefited from each other. It is true that constituent parsing may be smoothly altered to fit dependency parsing. However, due to the inconvenience from dependency to constituent structure, it is not so easy to adopt the latter ∗ This work is"
W12-3119,P11-1022,0,0.0141961,"easure post-editing effort (Specia et 152 Proceedings of the 7th Workshop on Statistical Machine Translation, pages 152–156, c Montr´eal, Canada, June 7-8, 2012. 2012 Association for Computational Linguistics al., 2011) or combine SMT and TM systems (He et al., 2010). Instead of estimating on word or sentence levels, Soricut and Echihabi (2010) proposed a document-level ranking system which grants the user to set quality threshold. Besides, recent studies on the QE topic introduced syntactic and linguistic information for better estimating the quality such as dependency preservation checking (Bach et al., 2011). 3 System Description We specify the details of our system in this section. Following previous approaches for quality estimation, it is first trained on the corpus with labeled quality scores and then it is able to predict the score for unlabeled instances. A major challenge for this estimating task is to exploit effective indicators, or features, to identify the quality of the translating results. In this paper, all the features are extracted from the official corpora, involving no external tools such as pre-trained parsers or POS taggers. Most of the feature templates focus on special phras"
W12-3119,W11-2107,0,0.051185,"ze the 17 baseline features: number of source/target tokens, average source token length, source/target LM probability, target-side average of target word occurrences, original/inverse frequency average of translations per source word, source/target percentage of uni-/bi-/tri-grams in quartile 1 or 4, source percentage of unigrams in the training corpus and 1 source/target number of punctuation. Besides, several features and templates are proposed as follows: • Inverted Automatic Scores: For each Spanish system output sentence, we translate it to English and get its scores of BLEU and METEOR (Denkowski and Lavie, 2011). These scores are treated as features named inverted automatic scores. In order to obtain these numerals, we train a Spanish-to-English phrase-based Moses2 (Koehn et al., 2007) decoder with default parameters on the official parallel corpus. The original training corpus is split into a developing set containing the last 3000 sentence pairs at the end of the corpus and a training set with the remained pairs. The word alignment information is generated by GIZA++ (Och and Ney, 2003) and the feature weights are tuned on the developing set by Z-MERT (Zaidan, 2009). • Minimal/Maximal link likelihoo"
W12-3119,2005.eamt-1.15,0,0.0259169,"of this topic. Numerical results show their positive effects on both ranking and scoring subtasks. The rest of this paper is organized as follows: In Section 2, we show the related work. In Section 3, we specify the details of our system architecture. The experimental results are reported in Section 4. Finally, the conclusion is given in Section 5. 2 Related Work Compared with traditional MT metrics such as BLEU (Papineni et al., 2002), the fundamental goal of quality estimation (QE) is predicting the quality of output sentences without involving reference sentences. Early works (Quirk, 2004; Gamon et al., 2005) have demonstrated the consistency of the automatic score and human evaluation. Several further works aimed at predicting automatic scores in order to better select MT n-best candidates (Specia and Farzindar, 2010), measure post-editing effort (Specia et 152 Proceedings of the 7th Workshop on Statistical Machine Translation, pages 152–156, c Montr´eal, Canada, June 7-8, 2012. 2012 Association for Computational Linguistics al., 2011) or combine SMT and TM systems (He et al., 2010). Instead of estimating on word or sentence levels, Soricut and Echihabi (2010) proposed a document-level ranking sy"
W12-3119,P10-1064,0,0.0228477,"Missing"
W12-3119,P07-2045,0,0.00854035,"frequency average of translations per source word, source/target percentage of uni-/bi-/tri-grams in quartile 1 or 4, source percentage of unigrams in the training corpus and 1 source/target number of punctuation. Besides, several features and templates are proposed as follows: • Inverted Automatic Scores: For each Spanish system output sentence, we translate it to English and get its scores of BLEU and METEOR (Denkowski and Lavie, 2011). These scores are treated as features named inverted automatic scores. In order to obtain these numerals, we train a Spanish-to-English phrase-based Moses2 (Koehn et al., 2007) decoder with default parameters on the official parallel corpus. The original training corpus is split into a developing set containing the last 3000 sentence pairs at the end of the corpus and a training set with the remained pairs. The word alignment information is generated by GIZA++ (Och and Ney, 2003) and the feature weights are tuned on the developing set by Z-MERT (Zaidan, 2009). • Minimal/Maximal link likelihood of general language model: In the word graph of each decoding instance, denote the minimal and maximal general language model likelihood of links as lmin and lmax . We treat e"
W12-3119,J03-1002,0,0.00293642,"For each Spanish system output sentence, we translate it to English and get its scores of BLEU and METEOR (Denkowski and Lavie, 2011). These scores are treated as features named inverted automatic scores. In order to obtain these numerals, we train a Spanish-to-English phrase-based Moses2 (Koehn et al., 2007) decoder with default parameters on the official parallel corpus. The original training corpus is split into a developing set containing the last 3000 sentence pairs at the end of the corpus and a training set with the remained pairs. The word alignment information is generated by GIZA++ (Och and Ney, 2003) and the feature weights are tuned on the developing set by Z-MERT (Zaidan, 2009). • Minimal/Maximal link likelihood of general language model: In the word graph of each decoding instance, denote the minimal and maximal general language model likelihood of links as lmin and lmax . We treat exp(lmin ) and exp(lmax ) as features respectively. • Trace Density: Define the trace density ρT as the quotient of decoding trace length and sentence length: ρT = TraceLength / SentenceLength. • Average of Phrase Length: This feature is also obtained from the decoding trace information. • Number of Name Ent"
W12-3119,P02-1040,0,0.0833136,"xternal NLP toolkits, we exploited several feasible techniques to extract such patterns directly on the corpus. One contribution of this paper is those feature templates on the basis of this topic. Numerical results show their positive effects on both ranking and scoring subtasks. The rest of this paper is organized as follows: In Section 2, we show the related work. In Section 3, we specify the details of our system architecture. The experimental results are reported in Section 4. Finally, the conclusion is given in Section 5. 2 Related Work Compared with traditional MT metrics such as BLEU (Papineni et al., 2002), the fundamental goal of quality estimation (QE) is predicting the quality of output sentences without involving reference sentences. Early works (Quirk, 2004; Gamon et al., 2005) have demonstrated the consistency of the automatic score and human evaluation. Several further works aimed at predicting automatic scores in order to better select MT n-best candidates (Specia and Farzindar, 2010), measure post-editing effort (Specia et 152 Proceedings of the 7th Workshop on Statistical Machine Translation, pages 152–156, c Montr´eal, Canada, June 7-8, 2012. 2012 Association for Computational Lingui"
W12-3119,quirk-2004-training,0,0.0903923,"on the basis of this topic. Numerical results show their positive effects on both ranking and scoring subtasks. The rest of this paper is organized as follows: In Section 2, we show the related work. In Section 3, we specify the details of our system architecture. The experimental results are reported in Section 4. Finally, the conclusion is given in Section 5. 2 Related Work Compared with traditional MT metrics such as BLEU (Papineni et al., 2002), the fundamental goal of quality estimation (QE) is predicting the quality of output sentences without involving reference sentences. Early works (Quirk, 2004; Gamon et al., 2005) have demonstrated the consistency of the automatic score and human evaluation. Several further works aimed at predicting automatic scores in order to better select MT n-best candidates (Specia and Farzindar, 2010), measure post-editing effort (Specia et 152 Proceedings of the 7th Workshop on Statistical Machine Translation, pages 152–156, c Montr´eal, Canada, June 7-8, 2012. 2012 Association for Computational Linguistics al., 2011) or combine SMT and TM systems (He et al., 2010). Instead of estimating on word or sentence levels, Soricut and Echihabi (2010) proposed a docu"
W12-3119,P10-1063,0,0.0145059,"ference sentences. Early works (Quirk, 2004; Gamon et al., 2005) have demonstrated the consistency of the automatic score and human evaluation. Several further works aimed at predicting automatic scores in order to better select MT n-best candidates (Specia and Farzindar, 2010), measure post-editing effort (Specia et 152 Proceedings of the 7th Workshop on Statistical Machine Translation, pages 152–156, c Montr´eal, Canada, June 7-8, 2012. 2012 Association for Computational Linguistics al., 2011) or combine SMT and TM systems (He et al., 2010). Instead of estimating on word or sentence levels, Soricut and Echihabi (2010) proposed a document-level ranking system which grants the user to set quality threshold. Besides, recent studies on the QE topic introduced syntactic and linguistic information for better estimating the quality such as dependency preservation checking (Bach et al., 2011). 3 System Description We specify the details of our system in this section. Following previous approaches for quality estimation, it is first trained on the corpus with labeled quality scores and then it is able to predict the score for unlabeled instances. A major challenge for this estimating task is to exploit effective in"
W12-3119,2010.jec-1.5,0,0.0252362,"ecify the details of our system architecture. The experimental results are reported in Section 4. Finally, the conclusion is given in Section 5. 2 Related Work Compared with traditional MT metrics such as BLEU (Papineni et al., 2002), the fundamental goal of quality estimation (QE) is predicting the quality of output sentences without involving reference sentences. Early works (Quirk, 2004; Gamon et al., 2005) have demonstrated the consistency of the automatic score and human evaluation. Several further works aimed at predicting automatic scores in order to better select MT n-best candidates (Specia and Farzindar, 2010), measure post-editing effort (Specia et 152 Proceedings of the 7th Workshop on Statistical Machine Translation, pages 152–156, c Montr´eal, Canada, June 7-8, 2012. 2012 Association for Computational Linguistics al., 2011) or combine SMT and TM systems (He et al., 2010). Instead of estimating on word or sentence levels, Soricut and Echihabi (2010) proposed a document-level ranking system which grants the user to set quality threshold. Besides, recent studies on the QE topic introduced syntactic and linguistic information for better estimating the quality such as dependency preservation checkin"
W12-3119,2011.mtsummit-papers.58,0,0.0689619,"Missing"
W12-4510,N10-1061,0,0.0629721,"ks, scored 51.83 and 59.24 respectively. 1 Introduction In this paper, we describes the approaches we utilized for our participation in the CoNLL-2012 Shared Task. This year’s shared task targets at modeling coreference resolution for multiple languages. Following (Lee et al., 2011), we extends the methodology of deterministic coreference model, using manually designed rules to recognize expressions with corresponding entities. The deterministic coreference model (Raghu2 Many existing works have been published on learning relation extractors via supervised (Soon et al., 2001) or unsupervised (Haghighi and Klein, 2010; Poon and Domingos, 2008) approaches. For involving semantics, (Rahman and Ng, 2011) proposed a coreference resolution model with world knowledge; By using word associations, (Kobdani et al., 2011) showed its effectiveness to coreference resolution. Compared ∗ This work was partially supported by the National Natural Science Foundation of China (Grant No. 60903119 and Grant No. 61170114), the National Research Foundation for the Doctoral Program of Higher Education of China under Grant No. 20110073120022, the National Basic Research Program of China (Grant No. 2009CB320901), the Science and T"
W12-4510,P11-1079,0,0.348544,"deling coreference resolution for multiple languages. Following (Lee et al., 2011), we extends the methodology of deterministic coreference model, using manually designed rules to recognize expressions with corresponding entities. The deterministic coreference model (Raghu2 Many existing works have been published on learning relation extractors via supervised (Soon et al., 2001) or unsupervised (Haghighi and Klein, 2010; Poon and Domingos, 2008) approaches. For involving semantics, (Rahman and Ng, 2011) proposed a coreference resolution model with world knowledge; By using word associations, (Kobdani et al., 2011) showed its effectiveness to coreference resolution. Compared ∗ This work was partially supported by the National Natural Science Foundation of China (Grant No. 60903119 and Grant No. 61170114), the National Research Foundation for the Doctoral Program of Higher Education of China under Grant No. 20110073120022, the National Basic Research Program of China (Grant No. 2009CB320901), the Science and Technology Commission of Shanghai Municipality (Grant No. 09511502400), and the European Union Seventh Framework Program (Grant No. 247619). † Corresponding author. Related Work 1 95 http://nlp.stanf"
W12-4510,W11-1902,0,0.26173,"s paper describes a pure rule-based method, which assembles different filters in a proper order. Different filters handle different situations and the filtering strategies are designed manually. These filters are assigned to different ordered tiers from general to special cases. We participated in the Chinese and English closed tracks, scored 51.83 and 59.24 respectively. 1 Introduction In this paper, we describes the approaches we utilized for our participation in the CoNLL-2012 Shared Task. This year’s shared task targets at modeling coreference resolution for multiple languages. Following (Lee et al., 2011), we extends the methodology of deterministic coreference model, using manually designed rules to recognize expressions with corresponding entities. The deterministic coreference model (Raghu2 Many existing works have been published on learning relation extractors via supervised (Soon et al., 2001) or unsupervised (Haghighi and Klein, 2010; Poon and Domingos, 2008) approaches. For involving semantics, (Rahman and Ng, 2011) proposed a coreference resolution model with world knowledge; By using word associations, (Kobdani et al., 2011) showed its effectiveness to coreference resolution. Compared"
W12-4510,P03-1056,0,0.0158205,"the original system, whether a mention is singular or plural should be given. Different from English POS tags, in Chinese plural nouns couldn’t be distinguished from single nouns in terms of the POS. Therefore, we add two rules to judge whether a noun is plural or not. Table 1: Ordered filtering sieves for Chinese. Modified sieves are bold. We remove the semantic-based sieves due to the resource constraints. The simplified version consists of nine filtering sieves. The bold ones in Table 1 are the modified sieves for Chinese. First of all, we adopt the head finding rules for Chinese used in (Levy and Manning, 2003), and this affects sieve 4, 6 and 7 which are all take advantage of the head words. And our change to other sieves are described as follows. 96  • A noun that ends with “ ” (plural marker for pronouns and a few animate nouns), and “ ” (and so on) is plural.  • A noun phrase that involves the coordinating conjunction words such as “ ” (and) is plural. Ú 4 Ch Experiments 4.1 Modification for the English system En We implement the semantic-similarity sieves proposed in (Lee et al., 2011) with the WordNet. These modifications consider the alias sieve and lexical chain sieve. For the alias sieve,"
W12-4510,D08-1068,0,0.138563,"respectively. 1 Introduction In this paper, we describes the approaches we utilized for our participation in the CoNLL-2012 Shared Task. This year’s shared task targets at modeling coreference resolution for multiple languages. Following (Lee et al., 2011), we extends the methodology of deterministic coreference model, using manually designed rules to recognize expressions with corresponding entities. The deterministic coreference model (Raghu2 Many existing works have been published on learning relation extractors via supervised (Soon et al., 2001) or unsupervised (Haghighi and Klein, 2010; Poon and Domingos, 2008) approaches. For involving semantics, (Rahman and Ng, 2011) proposed a coreference resolution model with world knowledge; By using word associations, (Kobdani et al., 2011) showed its effectiveness to coreference resolution. Compared ∗ This work was partially supported by the National Natural Science Foundation of China (Grant No. 60903119 and Grant No. 61170114), the National Research Foundation for the Doctoral Program of Higher Education of China under Grant No. 20110073120022, the National Basic Research Program of China (Grant No. 2009CB320901), the Science and Technology Commission of Sh"
W12-4510,D10-1048,0,0.148703,"Missing"
W12-4510,P11-1082,0,0.0129953,"pproaches we utilized for our participation in the CoNLL-2012 Shared Task. This year’s shared task targets at modeling coreference resolution for multiple languages. Following (Lee et al., 2011), we extends the methodology of deterministic coreference model, using manually designed rules to recognize expressions with corresponding entities. The deterministic coreference model (Raghu2 Many existing works have been published on learning relation extractors via supervised (Soon et al., 2001) or unsupervised (Haghighi and Klein, 2010; Poon and Domingos, 2008) approaches. For involving semantics, (Rahman and Ng, 2011) proposed a coreference resolution model with world knowledge; By using word associations, (Kobdani et al., 2011) showed its effectiveness to coreference resolution. Compared ∗ This work was partially supported by the National Natural Science Foundation of China (Grant No. 60903119 and Grant No. 61170114), the National Research Foundation for the Doctoral Program of Higher Education of China under Grant No. 20110073120022, the National Basic Research Program of China (Grant No. 2009CB320901), the Science and Technology Commission of Shanghai Municipality (Grant No. 09511502400), and the Europe"
W12-4510,J01-4004,0,0.752315,"Wu1,2 Hai Zhao1,2† Center for Brain-Like Computing and Machine Intelligence, Department of Computer Science and Engineering, Shanghai Jiao Tong University 2 MOE-Microsoft Key Laboratory for Intelligent Computing and Intelligent Systems Shanghai Jiao Tong University xtian.zh@gmail.com, chunyang506@sjtu.edu.cn, zhaohai@cs.sjtu.edu.cn 1 Abstract nathan et al., 2010) has shown good performance in the shared task of CoNLL-2011. This kind of model focuses on filtering with ordered tiers: One filter is applied at one time, from highest to lowest precision. However, compared with learning approaches (Soon et al., 2001), since effective rules are quite heterogeneous in different languages, several filtering methods should be redesigned when different languages are considered. We modified the original Stanford English coreference system1 to adapt to the Chinese scenario. For the English participation, we implemented the full strategies and interface of the semantic-based filters which are not obtained from the open source toolkit. The rest of this paper is organized as follows: In Section 2, we review the related work; In Section 3, we describe the detail of our model of handling coreference resolution in Chi"
W12-4514,W11-1905,0,0.0307234,"Missing"
W12-4514,W11-1904,0,0.119666,"uting and Machine Intelligence, Department of Computer Science and Engineering, Shanghai Jiao Tong University 2 MOE-Microsoft Key Laboratory for Intelligent Computing and Intelligent Systems Shanghai Jiao Tong University shouhm@gmail.com, zhaohai@cs.sjtu.edu.cn 1 Abstract Many other existing systems applied supervised or unsupervised (Haghighi and Klein, 2010) learning models. The classical resolution algorithm was proposed by (Soon et al., 2001). Semantic knowledge like word associations was involved by (Kobdani et al., 2011). Most of the supervised learning models in CoNLL-2011 shared task (Chang et al., 2011)(Bj¨orkelund and Nugues, 2011) used classifiers (Maximum Entropy or SVM) to train the models for obtaining the pairwise mention scores. However, the training process usually takes much longer time than unsupervised or deterministic systems. In contrast, (Raghunathan et al., 2010) proposed a rulebased model which obtained competitive result with less time. This paper describes our coreference resolution system for the CoNLL-2012 shared task. Our system is based on the Stanford’s dcoref deterministic system which applies multiple sieves with the order from high precision to low precision to gene"
W12-4514,N10-1061,0,0.167963,"Missing"
W12-4514,P11-1079,0,0.282474,"rithm for Coreference Resolution ∗ Heming Shou1,2 Hai Zhao1,2† Center for Brain-Like Computing and Machine Intelligence, Department of Computer Science and Engineering, Shanghai Jiao Tong University 2 MOE-Microsoft Key Laboratory for Intelligent Computing and Intelligent Systems Shanghai Jiao Tong University shouhm@gmail.com, zhaohai@cs.sjtu.edu.cn 1 Abstract Many other existing systems applied supervised or unsupervised (Haghighi and Klein, 2010) learning models. The classical resolution algorithm was proposed by (Soon et al., 2001). Semantic knowledge like word associations was involved by (Kobdani et al., 2011). Most of the supervised learning models in CoNLL-2011 shared task (Chang et al., 2011)(Bj¨orkelund and Nugues, 2011) used classifiers (Maximum Entropy or SVM) to train the models for obtaining the pairwise mention scores. However, the training process usually takes much longer time than unsupervised or deterministic systems. In contrast, (Raghunathan et al., 2010) proposed a rulebased model which obtained competitive result with less time. This paper describes our coreference resolution system for the CoNLL-2012 shared task. Our system is based on the Stanford’s dcoref deterministic system wh"
W12-4514,W11-1902,0,0.112992,"Missing"
W12-4514,W12-4501,0,0.0243423,"ult with less time. This paper describes our coreference resolution system for the CoNLL-2012 shared task. Our system is based on the Stanford’s dcoref deterministic system which applies multiple sieves with the order from high precision to low precision to generate coreference chains. We introduce the newly added constraints and sieves and discuss the improvement on the original system. We evaluate the system using OntoNotes data set and report our results of average F-score 58.25 in the closed track. 1 Introduction In this paper, our coreference resolution system for CoNLL-2012 shared task (Pradhan et al., 2012) is summarized. Our system is an extension of Stanford’s multi-pass sieve system, (Raghunathan et al., 2010) and (Lee et al., 2011), by adding novel constraints and sieves. In the original model , sieves are sorted in decreasing order of precision. Initially each mention is in its own cluster. Mention clusters are combined by satisfying the condition of each sieve in the scan pass. Through empirical studies, we proposed some extensions and algorithms for furthermore enhancing the performance. Two considerable extensions to the Stanford model in this paper are made to guarantee higher precision"
W12-4514,D10-1048,0,0.0506914,"any other existing systems applied supervised or unsupervised (Haghighi and Klein, 2010) learning models. The classical resolution algorithm was proposed by (Soon et al., 2001). Semantic knowledge like word associations was involved by (Kobdani et al., 2011). Most of the supervised learning models in CoNLL-2011 shared task (Chang et al., 2011)(Bj¨orkelund and Nugues, 2011) used classifiers (Maximum Entropy or SVM) to train the models for obtaining the pairwise mention scores. However, the training process usually takes much longer time than unsupervised or deterministic systems. In contrast, (Raghunathan et al., 2010) proposed a rulebased model which obtained competitive result with less time. This paper describes our coreference resolution system for the CoNLL-2012 shared task. Our system is based on the Stanford’s dcoref deterministic system which applies multiple sieves with the order from high precision to low precision to generate coreference chains. We introduce the newly added constraints and sieves and discuss the improvement on the original system. We evaluate the system using OntoNotes data set and report our results of average F-score 58.25 in the closed track. 1 Introduction In this paper, our"
W12-4514,J01-4004,0,0.459675,"Missing"
W13-3610,N12-1067,0,0.036542,"illions of people around the world that are learning English as a second language. Although there have been much of work on grammatical error correction, the current approaches mainly focus on very limited error types and the result is far from satisfactory. The CoNLL-2013 shared task, compared with the previous Help Our Own (HOO) tasks focusing on only determiner and preposition errors, considers a more comprehensive list of error types, including determiner, preposition, noun number, verb form, and subjectverb agreement errors. The evaluation metric used in CoNLL-2013 is Max-Matching (M2 ) (Dahlmeier and Ng, 2012) precision, recall and F1 between the system edits and a manually created set of gold-standard edits. The corpus used in CoNLL-2013 is NUS Corpus of Learner English (NUCLE) of which the details are described in (Dahlmeier et al., 2013). In this paper, we describe the system submission from the team 1 of Shanghai Jiao Tong Univer2 System Architecture Our system is a pipeline of grammatical error detection and correction. We treats grammatical error detection as a classification task. First all the tokens are relabeled according to the golden annotation and a sequence of modified version of erro"
W13-3610,W12-2025,0,0.162348,"E insert tokdst [j − 1] into head of P (i, j) ← B[i][j] end while return (E, P) end function 1: function • LM filtering. 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: 2.1 Rebeling by Levenshtein Edit Distance with Inflection In CoNLL-2013 there are 5 error types but they cannot be used directly as class labels, since they are too general for error correction. For example, the verb form error includes all verb form inflections such as converting a verb to its infinitive form, gerund form, paste tense, paste participle, passive voice and so on. Previous approaches (Dahlmeier et al., 2012; Rozovskaya et al., 2012; Kochmar et al., 2012) manually decompose each error types to more detailed ones. For example, in (Dahlmeier et al., 2012), the determinater error is decomposed into: • replacement determiner (RD): { a → the } • missing determiner (MD): { ϵ → a } • unwanted determiner (UD): { a → ϵ } For limited error types such as merely determinatives error and preposition error in HOO 2012, manually decomposition may be sufficient. But for CoNLL-2013 with 5 error types including complicated verb inflection, an automatic method to decompose error types is needed. We present an exten"
W13-3610,N03-1033,0,0.0850514,"PART BEEN AT⊕AN AT⊕A AS⊕THE AS⊕HAS AROUND ARE⊕PAST ARE⊕A ARE⊕⊙ A⊕⊙⊕OF AM⊕ Table 6: All labels after relabeling 79 4.2 have rP P L > 2, so we test thresholds ranging from 1.5 to 3. The curves are shown in Figure 3. Resources We use the following NLP resources in our system. For relabeling and correction, perl module Lingua::EN::Inflect1 (Conway, 1998) is used for determining noun and verb number and Lingua::EN::VerbTense2 is used for determining verb tense. A revised and extended version of maximum entropy model3 is used for ME modeling. For lemmatization, the Stanford CoreNLP lemma annotator (Toutanova et al., 2003; Toutanova and Manning, 2000) is used. The language model is built by the SRILM toolkit (Stolcke and others, 2002). The corpus for building LM is the EuroParl corpus (Koehn, 2005). The English part of the German-English parallel corpus is actually used. We use such a corpus to build LM for the following reasons: 1. LM for grammatical error correction should be trained from corpus that itself is grammatically correct, and the EuroParl corpus has very good quality of writing; 2. the NUCLE corpus mainly contains essays on subjects such as environment, economics, society, politics and so on, whic"
W13-3610,W13-1703,0,0.0203403,"far from satisfactory. The CoNLL-2013 shared task, compared with the previous Help Our Own (HOO) tasks focusing on only determiner and preposition errors, considers a more comprehensive list of error types, including determiner, preposition, noun number, verb form, and subjectverb agreement errors. The evaluation metric used in CoNLL-2013 is Max-Matching (M2 ) (Dahlmeier and Ng, 2012) precision, recall and F1 between the system edits and a manually created set of gold-standard edits. The corpus used in CoNLL-2013 is NUS Corpus of Learner English (NUCLE) of which the details are described in (Dahlmeier et al., 2013). In this paper, we describe the system submission from the team 1 of Shanghai Jiao Tong Univer2 System Architecture Our system is a pipeline of grammatical error detection and correction. We treats grammatical error detection as a classification task. First all the tokens are relabeled according to the golden annotation and a sequence of modified version of error types is generated. This relabeling task is rule based using an extended version of Levenshtein edit distance which will be discussed in the following section. Then with the modified error types as the class labels, a classifier usin"
W13-3610,W12-2028,0,0.227975,"ection as Multiclass Classification with Single Model∗ Zhongye Jia, Peilu Wang and Hai Zhao† MOE-Microsoft Key Laboratory for Intelligent Computing and Intelligent Systems, Center for Brain-Like Computing and Machine Intelligence Department of Computer Science and Engineering, Shanghai Jiao Tong University 800 Dongchuan Road, Shanghai 200240, China {jia.zhongye,plwang1990}@gmail.com,zhaohai@cs.sjtu.edu.cn Abstract sity (SJT1). Grammatical error detection and correction problem is treated as multiclass classification task. Unlike previous works (Dahlmeier et al., 2012; Rozovskaya et al., 2012; Kochmar et al., 2012) that train a model upon each error type, we use one single model for all error types. Instead of the original error type, a more detailed version of error types is used as class labels. A rule based system generates labels from the golden edits utilizing an extended version of Levenshtein edit distance. We use maximum entropy (ME) model as classifier to obtain the error types and use rules to do the correction. Corrections are made using rules. Finally, the corrections are filtered using language model (LM). This paper describes our system in the shared task of CoNLL-2013. We illustrate that"
W13-3610,2005.mtsummit-papers.11,0,0.00415032,"are shown in Figure 3. Resources We use the following NLP resources in our system. For relabeling and correction, perl module Lingua::EN::Inflect1 (Conway, 1998) is used for determining noun and verb number and Lingua::EN::VerbTense2 is used for determining verb tense. A revised and extended version of maximum entropy model3 is used for ME modeling. For lemmatization, the Stanford CoreNLP lemma annotator (Toutanova et al., 2003; Toutanova and Manning, 2000) is used. The language model is built by the SRILM toolkit (Stolcke and others, 2002). The corpus for building LM is the EuroParl corpus (Koehn, 2005). The English part of the German-English parallel corpus is actually used. We use such a corpus to build LM for the following reasons: 1. LM for grammatical error correction should be trained from corpus that itself is grammatically correct, and the EuroParl corpus has very good quality of writing; 2. the NUCLE corpus mainly contains essays on subjects such as environment, economics, society, politics and so on, which are in the same dormain as those of the EuroParl corpus. 4.3 Precision, Recall and F1 curves 0.17 Precision Recall F1 0.165 0.16 0.155 0.15 0.145 0.14 1.4 1.6 1.8 2 2.2 2.4 2.6 2"
W13-3610,W13-3601,0,0.0555419,"Missing"
W13-3610,W11-2843,0,0.0601894,"IN DT the innovators have to stop all works for the NN development . N-gram Context Figure 2: Example of training corpus refinement 3 Features current word in the dependency tree, and DPRel is the dependency relation with parent. The single model approach enables us only to optimize one feature set for all error type in the task, which can drastically reduce the computational cost in feature selection. As many previous works have proposed various of features, we first collected features from different previous works including (Dahlmeier et al., 2012; Rozovskaya et al., 2012; HAN et al., 2006; Rozovskaya et al., 2011; Tetreault, Joel R and Chodorow, Martin, 2008). Then experiments with different features were built to test these features’ effectiveness and only those have positive contribution to the final performance were preserved. The features we used are presented in Table 4, where word0 is the word that we are generating features for, and word and P OS is the word itself and it’s POS tag for various components. NPHead denotes the head of the minimum Noun Phrase (NP) in syntax tree. wordN P −1 represents the word appearing before NP in the sentence. NC stands for noun compound and is composed of the l"
W13-3610,C08-1109,0,0.216253,"Missing"
W13-3610,W00-1308,0,0.0961344,"Missing"
W13-3610,W12-2032,0,\N,Missing
W13-4416,W10-4107,0,0.0615912,"The LM is built on the Academia Sinica corpus (Emerson, 2005) with IRSTLM toolkit (Federico et al., 2008). Prefix tree data structure is used to speed up the dictionary look-up. The implementation of Perl module Tree::Trie3 is used with some modification. 明月 Figure 2: A sample of graph for spell checking spell checker is “床/签名/月光”, note this is not the expected result. 2.3 Using Language Model with SSSP Algorithm The problem with simple SSSP spell chekcer is that it only tries to merge short words into longer ones without considering whether that is reasonable. To reduce the false-alarm rate (Wu et al., 2010), we add some statistical criteria to the SSSP spell checker. A natural statistical criteria is the conditional probability between words, P , which can be given by a language model (LM). The conditional probability are combined into the weights of edges by some function g(·, ·, ·): 1 ω = g(ω0 , ωs , ), P Note that the higher conditional probability means the sentence is more reasonable, so for the SSSP algorithm, the inverse of conditional probability P1 is used. 3.2 Edge Weight Function selection A series of experiments are performed to choose a good edge weight function g(·, ·, ·). A simpli"
W13-4416,yang-etal-2012-spell,1,0.596293,"m/labs/dl/w.html http://code.google.com/p/opencc/ 3 http://search.cpan.org/~avif/Tree-Trie-1. 2 5/ 90 Type same pronunciation same tone same pronunciation different tone similar pronunciation same tone similar pronunciation different tone similar shape • Correction precision: # of correctly corrected characters P= ; # of all corrected characters • Correction recall: # of correctly corrected characters R= ; # of wrong characters of gold data ωs 1 1 2 2 2 Table 2: ωs used in ω L • F1 macro: F= 2PR . P +R 1 Precision, Recall and F1 curves The LM is set to 2-gram according to the observations of (Yang et al., 2012). Improved KneserNey (Chen and Goodman, 1999) algorithm is used for LM smoothing. Multiplication of similarity and log conditional probability is firstly used as weight function: ω M = −α(ω0 + ωs ) log P Type same pronunciation same tone same pronunciation different tone similar pronunciation same tone similar pronunciation different tone similar shape Table 1: ωs used in 0.8 0.7 0.6 0.5 0.4 0.3 0.2 where ω0 ≡ 1, and ωs for different kinds of characters are shown in Table 1. The settings of ωs is inspired by (Yang et al., 2012), in which pinyin4 edit distance is used as weight. Word length thr"
W13-4416,I05-3017,0,\N,Missing
W14-1710,W13-1703,0,0.0633314,"Missing"
W14-1710,W11-2843,0,0.0309708,"terpreted easily as a series of edit operations. Once the labels are determined by classifier, the correction of the grammatical errors is conducted by applying the edit operations interpreted from these labels. 76 Algorithm 2 Labeling Algorithm 1: 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: 23: 24: 25: ine the factors involved in a wide range of features that have been or can be used to the word label classification task. Many features that are considered effective in various of previous works (Dahlmeier et al., 2012; Rozovskaya et al., 2012; Han et al., 2006; Rozovskaya et al., 2011; Tetreault, Joel R and Chodorow, Martin, 2008) are included. Besides, features that are used in the similar spell checking tasks (Jia et al., 2013b; Yang et al., 2012) and some novel features showing effectiveness in other NLP tasks (Wang et al., 2013; Zhang and Zhao, 2013; Xu and Zhao, 2012; Ma and Zhao, 2012; Zhao, 2009; Zhao et al., 2009b) are also included. However, using too many features is time consuming. Besides, it increases the probability of overfitting and may lead to a poor solution of the maximum-likelihood parameter estimate in the ME training. INPUT: E, P OUTPUT: L pivot ← num"
W14-1710,W12-2006,0,0.054073,"ntroduction The task of CoNLL-2014 is grammatical error correction which consists of detecting and correcting the grammatical errors in English essays written by non-native speakers (Ng et al., 2014). The research of grammatical error correction can potentially help millions of people in the world who are learning English as foreign language. Although there have been many works on grammatical error correction, the current approaches mainly focus on very limited error types and the result is far from satisfactory. The CoNLL-2014 shared task, compared with the previous Help Our Own (HOO) tasks (Dale et al., 2012) considering only determiner and preposition errors and the CoNLL-2013 shared task fo∗ This work was partially supported by the National Natural Science Foundation of China (Grant No.60903119, Grant No.61170114, and Grant No.61272248), the National Basic Research Program of China (Grant No.2013CB329401), the Science and Technology Commission of Shanghai Municipality (Grant No.13511500200), and the European Union Seventh Framework Program (Grant No.247619). † Corresponding author 74 Proceedings of the Eighteenth Conference on Computational Natural Language Learning: Shared Task, pages 74–82, c"
W14-1710,W13-3602,0,0.0198788,"t of Computer Science and Engineering, Shanghai Jiao Tong University 800 Dongchuan Road, Shanghai 200240, China {plwang1990,jia.zhongye}@gmail.com,zhaohai@cs.sjtu.edu.cn Abstract cusing on five major types of errors, requires to correct all 28 types of errors (Ng et al., 2014). One traditional strategy is designing a system combined of a set of sub-models, where each submodel is specialized for a specific subtask, for example, correcting one type of errors. This strategy is computationally efficient and can adopt different favorable features for each subtask. Top ranked systems in CoNLL-2013 (Rozovskaya et al., 2013; Kao et al., 2013; Xing et al., 2013; Yoshimoto et al., 2013; Xiang et al., 2013) are based on this strategy. However, the division of the model relies on prior-knowledges and the designing of different features for each sub-model requires a large amount of manual works. This shortage is especially notable in CoNLL-2014 shared task, since the number of error types is much larger and the composition of errors is more complicated than before. In contrast, we follow the work in (Jia et al., 2013a; Zhao et al., 2009a), integrating everything into one model. This integrated system holds a merit th"
W14-1710,W13-3610,1,0.817218,"and can adopt different favorable features for each subtask. Top ranked systems in CoNLL-2013 (Rozovskaya et al., 2013; Kao et al., 2013; Xing et al., 2013; Yoshimoto et al., 2013; Xiang et al., 2013) are based on this strategy. However, the division of the model relies on prior-knowledges and the designing of different features for each sub-model requires a large amount of manual works. This shortage is especially notable in CoNLL-2014 shared task, since the number of error types is much larger and the composition of errors is more complicated than before. In contrast, we follow the work in (Jia et al., 2013a; Zhao et al., 2009a), integrating everything into one model. This integrated system holds a merit that a one-way feature selection benefits the whole system and no additional process is needed to deal with the conflict or error propagation of every sub-models. Here is a glance of this method: A set of more detailed error types are generated automatically from the original 28 types of errors. The detailed error type can be regarded as the label of a word, thus the task of grammatical error detection is transformed to a multi-label classification task using maximum entropy model (Berger et al."
W14-1710,C08-1109,0,0.090111,"Missing"
W14-1710,W13-4416,1,0.928415,"and can adopt different favorable features for each subtask. Top ranked systems in CoNLL-2013 (Rozovskaya et al., 2013; Kao et al., 2013; Xing et al., 2013; Yoshimoto et al., 2013; Xiang et al., 2013) are based on this strategy. However, the division of the model relies on prior-knowledges and the designing of different features for each sub-model requires a large amount of manual works. This shortage is especially notable in CoNLL-2014 shared task, since the number of error types is much larger and the composition of errors is more complicated than before. In contrast, we follow the work in (Jia et al., 2013a; Zhao et al., 2009a), integrating everything into one model. This integrated system holds a merit that a one-way feature selection benefits the whole system and no additional process is needed to deal with the conflict or error propagation of every sub-models. Here is a glance of this method: A set of more detailed error types are generated automatically from the original 28 types of errors. The detailed error type can be regarded as the label of a word, thus the task of grammatical error detection is transformed to a multi-label classification task using maximum entropy model (Berger et al."
W14-1710,W13-3603,0,0.0176416,"Engineering, Shanghai Jiao Tong University 800 Dongchuan Road, Shanghai 200240, China {plwang1990,jia.zhongye}@gmail.com,zhaohai@cs.sjtu.edu.cn Abstract cusing on five major types of errors, requires to correct all 28 types of errors (Ng et al., 2014). One traditional strategy is designing a system combined of a set of sub-models, where each submodel is specialized for a specific subtask, for example, correcting one type of errors. This strategy is computationally efficient and can adopt different favorable features for each subtask. Top ranked systems in CoNLL-2013 (Rozovskaya et al., 2013; Kao et al., 2013; Xing et al., 2013; Yoshimoto et al., 2013; Xiang et al., 2013) are based on this strategy. However, the division of the model relies on prior-knowledges and the designing of different features for each sub-model requires a large amount of manual works. This shortage is especially notable in CoNLL-2014 shared task, since the number of error types is much larger and the composition of errors is more complicated than before. In contrast, we follow the work in (Jia et al., 2013a; Zhao et al., 2009a), integrating everything into one model. This integrated system holds a merit that a one-way featu"
W14-1710,W13-3616,0,0.0153039,"d, Shanghai 200240, China {plwang1990,jia.zhongye}@gmail.com,zhaohai@cs.sjtu.edu.cn Abstract cusing on five major types of errors, requires to correct all 28 types of errors (Ng et al., 2014). One traditional strategy is designing a system combined of a set of sub-models, where each submodel is specialized for a specific subtask, for example, correcting one type of errors. This strategy is computationally efficient and can adopt different favorable features for each subtask. Top ranked systems in CoNLL-2013 (Rozovskaya et al., 2013; Kao et al., 2013; Xing et al., 2013; Yoshimoto et al., 2013; Xiang et al., 2013) are based on this strategy. However, the division of the model relies on prior-knowledges and the designing of different features for each sub-model requires a large amount of manual works. This shortage is especially notable in CoNLL-2014 shared task, since the number of error types is much larger and the composition of errors is more complicated than before. In contrast, we follow the work in (Jia et al., 2013a; Zhao et al., 2009a), integrating everything into one model. This integrated system holds a merit that a one-way feature selection benefits the whole system and no additional process"
W14-1710,W12-2028,0,0.02146,"0 do insert E[i][j] into head of E insert toksdst [j − 1] into head of P (i, j) ← B[i][j] end while return (E, P) 2.1 Data Labeling In CoNLL-2014 shared task, there are 28 error types but they can not be used directly as class labels, since these types are too general that they can hardly be corrected by applying one rule-based edit. For example, the correction of Vform (verb form) error type includes all verb form inflections such as converting a verb to its infinitive form, gerund form, past form and past participle and so on. Previous works (Dahlmeier et al., 2012; Rozovskaya et al., 2012; Kochmar et al., 2012) manually decompose each error types to more detailed subtypes. For example, in (Dahlmeier et al., 2012), the determinater errors are decomposed into: • replacement determiner (RD): { a → the } • missing determiner (MD): { ϵ → a } • unwanted determiner (UD): { a → ϵ } For a task with a few error types such as merely determinative and preposition error in HOO 2012, manually decomposition may be sufficient. However, for CoNLL-2014, all 28 error types are required to be corrected and some of these types such as Rloc- (Local redundancy) and Um (Unclear meaning) are quite complex that the manual de"
W14-1710,W13-3605,0,0.0114335,"ghai Jiao Tong University 800 Dongchuan Road, Shanghai 200240, China {plwang1990,jia.zhongye}@gmail.com,zhaohai@cs.sjtu.edu.cn Abstract cusing on five major types of errors, requires to correct all 28 types of errors (Ng et al., 2014). One traditional strategy is designing a system combined of a set of sub-models, where each submodel is specialized for a specific subtask, for example, correcting one type of errors. This strategy is computationally efficient and can adopt different favorable features for each subtask. Top ranked systems in CoNLL-2013 (Rozovskaya et al., 2013; Kao et al., 2013; Xing et al., 2013; Yoshimoto et al., 2013; Xiang et al., 2013) are based on this strategy. However, the division of the model relies on prior-knowledges and the designing of different features for each sub-model requires a large amount of manual works. This shortage is especially notable in CoNLL-2014 shared task, since the number of error types is much larger and the composition of errors is more complicated than before. In contrast, we follow the work in (Jia et al., 2013a; Zhao et al., 2009a), integrating everything into one model. This integrated system holds a merit that a one-way feature selection benefi"
W14-1710,C12-2077,1,0.807437,"1: 22: 23: 24: 25: ine the factors involved in a wide range of features that have been or can be used to the word label classification task. Many features that are considered effective in various of previous works (Dahlmeier et al., 2012; Rozovskaya et al., 2012; Han et al., 2006; Rozovskaya et al., 2011; Tetreault, Joel R and Chodorow, Martin, 2008) are included. Besides, features that are used in the similar spell checking tasks (Jia et al., 2013b; Yang et al., 2012) and some novel features showing effectiveness in other NLP tasks (Wang et al., 2013; Zhang and Zhao, 2013; Xu and Zhao, 2012; Ma and Zhao, 2012; Zhao, 2009; Zhao et al., 2009b) are also included. However, using too many features is time consuming. Besides, it increases the probability of overfitting and may lead to a poor solution of the maximum-likelihood parameter estimate in the ME training. INPUT: E, P OUTPUT: L pivot ← number of edits in E that are not A L←ϕ L ← ′′ while i < length(E) do if E[i] = A then L ← L+ label of edit E[i] with P[i] i←i+1 else l ← L+ label of edit E[i] with P[i] pivot ← pivot − 1 if pivot = 0 then i←i+1 while i < length of E do l ← l + ⊕ + P[i] i←i+1 end while end if push l into L L ← ′′ end if end while"
W14-1710,C12-2131,1,0.810646,": 17: 18: 19: 20: 21: 22: 23: 24: 25: ine the factors involved in a wide range of features that have been or can be used to the word label classification task. Many features that are considered effective in various of previous works (Dahlmeier et al., 2012; Rozovskaya et al., 2012; Han et al., 2006; Rozovskaya et al., 2011; Tetreault, Joel R and Chodorow, Martin, 2008) are included. Besides, features that are used in the similar spell checking tasks (Jia et al., 2013b; Yang et al., 2012) and some novel features showing effectiveness in other NLP tasks (Wang et al., 2013; Zhang and Zhao, 2013; Xu and Zhao, 2012; Ma and Zhao, 2012; Zhao, 2009; Zhao et al., 2009b) are also included. However, using too many features is time consuming. Besides, it increases the probability of overfitting and may lead to a poor solution of the maximum-likelihood parameter estimate in the ME training. INPUT: E, P OUTPUT: L pivot ← number of edits in E that are not A L←ϕ L ← ′′ while i < length(E) do if E[i] = A then L ← L+ label of edit E[i] with P[i] i←i+1 else l ← L+ label of edit E[i] with P[i] pivot ← pivot − 1 if pivot = 0 then i←i+1 while i < length of E do l ← l + ⊕ + P[i] i←i+1 end while end if push l into L L ← ′"
W14-1710,W14-1701,0,0.0231092,"fication task. For training, labels are obtained through a strict rule-based approach. For decoding, errors are detected and corrected according to the classification results. A single maximum entropy model is used for the classification implementation incorporated with an improved feature selection algorithm. Our system achieved precision of 29.83, recall of 5.16 and F 0.5 of 15.24 in the official evaluation. 1 Introduction The task of CoNLL-2014 is grammatical error correction which consists of detecting and correcting the grammatical errors in English essays written by non-native speakers (Ng et al., 2014). The research of grammatical error correction can potentially help millions of people in the world who are learning English as foreign language. Although there have been many works on grammatical error correction, the current approaches mainly focus on very limited error types and the result is far from satisfactory. The CoNLL-2014 shared task, compared with the previous Help Our Own (HOO) tasks (Dale et al., 2012) considering only determiner and preposition errors and the CoNLL-2013 shared task fo∗ This work was partially supported by the National Natural Science Foundation of China (Grant N"
W14-1710,yang-etal-2012-spell,1,0.768566,"rations interpreted from these labels. 76 Algorithm 2 Labeling Algorithm 1: 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: 23: 24: 25: ine the factors involved in a wide range of features that have been or can be used to the word label classification task. Many features that are considered effective in various of previous works (Dahlmeier et al., 2012; Rozovskaya et al., 2012; Han et al., 2006; Rozovskaya et al., 2011; Tetreault, Joel R and Chodorow, Martin, 2008) are included. Besides, features that are used in the similar spell checking tasks (Jia et al., 2013b; Yang et al., 2012) and some novel features showing effectiveness in other NLP tasks (Wang et al., 2013; Zhang and Zhao, 2013; Xu and Zhao, 2012; Ma and Zhao, 2012; Zhao, 2009; Zhao et al., 2009b) are also included. However, using too many features is time consuming. Besides, it increases the probability of overfitting and may lead to a poor solution of the maximum-likelihood parameter estimate in the ME training. INPUT: E, P OUTPUT: L pivot ← number of edits in E that are not A L←ϕ L ← ′′ while i < length(E) do if E[i] = A then L ← L+ label of edit E[i] with P[i] i←i+1 else l ← L+ label of edit E[i] with P[i] p"
W14-1710,D09-1004,1,0.916422,"Missing"
W14-1710,P09-1007,1,0.946304,"rent favorable features for each subtask. Top ranked systems in CoNLL-2013 (Rozovskaya et al., 2013; Kao et al., 2013; Xing et al., 2013; Yoshimoto et al., 2013; Xiang et al., 2013) are based on this strategy. However, the division of the model relies on prior-knowledges and the designing of different features for each sub-model requires a large amount of manual works. This shortage is especially notable in CoNLL-2014 shared task, since the number of error types is much larger and the composition of errors is more complicated than before. In contrast, we follow the work in (Jia et al., 2013a; Zhao et al., 2009a), integrating everything into one model. This integrated system holds a merit that a one-way feature selection benefits the whole system and no additional process is needed to deal with the conflict or error propagation of every sub-models. Here is a glance of this method: A set of more detailed error types are generated automatically from the original 28 types of errors. The detailed error type can be regarded as the label of a word, thus the task of grammatical error detection is transformed to a multi-label classification task using maximum entropy model (Berger et al., 1996; Zhao et al.,"
W14-1710,E09-1100,1,0.771749,"ine the factors involved in a wide range of features that have been or can be used to the word label classification task. Many features that are considered effective in various of previous works (Dahlmeier et al., 2012; Rozovskaya et al., 2012; Han et al., 2006; Rozovskaya et al., 2011; Tetreault, Joel R and Chodorow, Martin, 2008) are included. Besides, features that are used in the similar spell checking tasks (Jia et al., 2013b; Yang et al., 2012) and some novel features showing effectiveness in other NLP tasks (Wang et al., 2013; Zhang and Zhao, 2013; Xu and Zhao, 2012; Ma and Zhao, 2012; Zhao, 2009; Zhao et al., 2009b) are also included. However, using too many features is time consuming. Besides, it increases the probability of overfitting and may lead to a poor solution of the maximum-likelihood parameter estimate in the ME training. INPUT: E, P OUTPUT: L pivot ← number of edits in E that are not A L←ϕ L ← ′′ while i < length(E) do if E[i] = A then L ← L+ label of edit E[i] with P[i] i←i+1 else l ← L+ label of edit E[i] with P[i] pivot ← pivot − 1 if pivot = 0 then i←i+1 while i < length of E do l ← l + ⊕ + P[i] i←i+1 end while end if push l into L L ← ′′ end if end while L ← upper ca"
W14-1710,W12-2032,0,\N,Missing
W14-1710,J96-1002,0,\N,Missing
W14-1710,N12-1067,0,\N,Missing
W14-1710,W12-2025,0,\N,Missing
W14-1710,W13-3604,0,\N,Missing
W14-1710,D13-1082,1,\N,Missing
W14-6825,W13-4410,0,0.0287784,"Related Work Over the past few years, there were many methods proposed for CSC task. (Sun et al., 2010) developed a phrase-based spelling error model from the clickthrough data by means of measuring the edit distance between an input query and the optimal spelling correction. (Gao et al., 2010) explored the ranker-based approach which included visual similarity, phonological similarity, dictionary, and frequency features for large scale web search. (Ahmad and Kondrak, 2005) proposed a spelling error The third category utilizes more than one approaches for detection and an LM for correction. (Hsieh et al., 2013) used two different systems for 158 Given a dictionary D and a similar characters C, for a sentence S of m characters {c1 , c2 , . . . , cm }, the original vertices V of the DAG in (Jia et al., 2013b) are: error detection. The first system detected error characters based on unknown word detection and LM verification. The second one solved error detection based on a suggestion dictionary generated from a confusion set. Finally, two systems were combined to obtain the final detection result. (He and Fu, 2013) divided typos into three categories which were character-level errors (CLEs), word-leve"
W14-6825,W13-3610,1,0.909165,". Both examples have the same pinyin. Thus CSC cannot be directly applied those edit distance based methods which are commonly used for alphabetical languages. CSC task has to deal with word segmentation problem first, since misspelled sentence could not be segmented properly by word segmenter. There also exist Chinese spelling errors which are unrelated with word segmentation. For example, “好好地出去玩” in Example 2 of Table 1 is misspelled into “好好的出去玩”, but both of them have the same segmentation. So it is necessary to perform further specific process. In this paper, based on our previous work (Jia et al., 2013b) in SIGHAN Bake-off 2013, we describe an improved graph model to handle the CSC task. The improved model includes a graph model for generic spelling errors, conditional random fields (CRF) for two special errors and a rule based system for some collocation errors. 2 The second category includes the methods that all single-character words are supposed to be errors and an LM is used for correction, for example (Lin and Chu, 2013) . They developed a system which supposed that all single-character words may be typos. They replaced all single-character words by similar characters using a confusio"
W14-6825,W13-4416,1,0.7767,". Both examples have the same pinyin. Thus CSC cannot be directly applied those edit distance based methods which are commonly used for alphabetical languages. CSC task has to deal with word segmentation problem first, since misspelled sentence could not be segmented properly by word segmenter. There also exist Chinese spelling errors which are unrelated with word segmentation. For example, “好好地出去玩” in Example 2 of Table 1 is misspelled into “好好的出去玩”, but both of them have the same segmentation. So it is necessary to perform further specific process. In this paper, based on our previous work (Jia et al., 2013b) in SIGHAN Bake-off 2013, we describe an improved graph model to handle the CSC task. The improved model includes a graph model for generic spelling errors, conditional random fields (CRF) for two special errors and a rule based system for some collocation errors. 2 The second category includes the methods that all single-character words are supposed to be errors and an LM is used for correction, for example (Lin and Chu, 2013) . They developed a system which supposed that all single-character words may be typos. They replaced all single-character words by similar characters using a confusio"
W14-6825,W13-4419,0,0.0308597,"isspelled into “好好的出去玩”, but both of them have the same segmentation. So it is necessary to perform further specific process. In this paper, based on our previous work (Jia et al., 2013b) in SIGHAN Bake-off 2013, we describe an improved graph model to handle the CSC task. The improved model includes a graph model for generic spelling errors, conditional random fields (CRF) for two special errors and a rule based system for some collocation errors. 2 The second category includes the methods that all single-character words are supposed to be errors and an LM is used for correction, for example (Lin and Chu, 2013) . They developed a system which supposed that all single-character words may be typos. They replaced all single-character words by similar characters using a confusion set and segmented the newly created sentences again. If a new sentence resulted in a better word segmentation, spelling error was reported. Their system gave good detection recall but low false-alarm rate. Related Work Over the past few years, there were many methods proposed for CSC task. (Sun et al., 2010) developed a phrase-based spelling error model from the clickthrough data by means of measuring the edit distance between"
W14-6825,W13-4414,0,0.0686909,"errors respectively. In addition to using the result of word segmentation for detection, (Yeh et al., 2013) also proposed a dictionarybased method to detect spelling errors. The dictionary contained similar pronunciation and shape information for each Chinese character. (Yang et al., 2013) proposed another method to improve the candidate detections. They employed high confidence pattern matching to strengthen the candidate errors after word segmentation. The last category is formed by the methods which use word segmentation for detection and different models for correction (Liu et al., 2013; Chen et al., 2013; Chiu et al., 2013). (Liu et al., 2013) used support vector machine (SVM) to select the most probable sentence from multiple candidates. They used word segmentation and machine translation model to generate the candidates respectively. The SVM was used to rerank the candidates. (Chen et al., 2013) not only applied LM, but also used various topic models to cover the shortage of LM. (Chiu et al., 2013) explored statistical machine translation model to translate the sentences containing typos into correct ones. In their model, the sentence with the highest translation probability which indicated"
W14-6825,C10-2085,0,0.11407,"w[i − 2] 林 鋼 游 星 西 – – – – – w[i − 1] 依 鐵 泳 期 門 – – – – – w[i] 神 依 世 路 丁 很 仍 打 機 ¬(少) w[i + 1] – – – – – 不 在 出 程 子 w[i + 2] – – – – – 得 了 租 車 化 corrected w[i] 晨 衣 池 六 町 恨 扔 搭 計 少 Table 7: Rule 4. The correction related with two neighbored words. w[i − 1] – – – – – – – – – – – – – 為 w[i] 自 式 蘭 令 排 柏 莎 玈 棒 想 名 頂 白 是 w[i + 1] 到 式 滿 令 排 柏 增 管 組 心 性 頂 花 嗎 w[i + 2] – – – – – – – – – – – 大, 有 商 – w[i + 3] – – – – – – – – – – – 名 店 – corrected w[i] and w[i + 1] 知道 試試 浪漫 冷冷 拜拜 伯伯 沙僧 旅館 幫助 相信 明星 鼎鼎 百貨 什麼 Table 8: Rule 5. Two words are simultaneously corrected. ing. Similar character set C provided by (Liu et al., 2010) is used to substitute the original words in the graph construction stage. The LM is built on the Academia Sinica corpus (Emerson, 2005) with IRSTLM toolkit (Federico et al., 2008). The CRF model is achieved by training and tuning on the Academia Sinica corpus with the toolkit CRF++ 0.586 . For Chinese word segmentation, the ICTCLAS20117 is exploited. Dev13, Dev14C and Dev14B and the test set is named as Test14 respectively. In the Dev14B, there are 4624 errors, in which the statistics information of the three common character usage confusions in section 4 is shown in Table 10, so it is necess"
W14-6825,W13-4408,0,0.0597152,"y. In addition to using the result of word segmentation for detection, (Yeh et al., 2013) also proposed a dictionarybased method to detect spelling errors. The dictionary contained similar pronunciation and shape information for each Chinese character. (Yang et al., 2013) proposed another method to improve the candidate detections. They employed high confidence pattern matching to strengthen the candidate errors after word segmentation. The last category is formed by the methods which use word segmentation for detection and different models for correction (Liu et al., 2013; Chen et al., 2013; Chiu et al., 2013). (Liu et al., 2013) used support vector machine (SVM) to select the most probable sentence from multiple candidates. They used word segmentation and machine translation model to generate the candidates respectively. The SVM was used to rerank the candidates. (Chen et al., 2013) not only applied LM, but also used various topic models to cover the shortage of LM. (Chiu et al., 2013) explored statistical machine translation model to translate the sentences containing typos into correct ones. In their model, the sentence with the highest translation probability which indicated how likely a typo w"
W14-6825,W13-4409,0,0.0629113,"tect the different errors respectively. In addition to using the result of word segmentation for detection, (Yeh et al., 2013) also proposed a dictionarybased method to detect spelling errors. The dictionary contained similar pronunciation and shape information for each Chinese character. (Yang et al., 2013) proposed another method to improve the candidate detections. They employed high confidence pattern matching to strengthen the candidate errors after word segmentation. The last category is formed by the methods which use word segmentation for detection and different models for correction (Liu et al., 2013; Chen et al., 2013; Chiu et al., 2013). (Liu et al., 2013) used support vector machine (SVM) to select the most probable sentence from multiple candidates. They used word segmentation and machine translation model to generate the candidates respectively. The SVM was used to rerank the candidates. (Chen et al., 2013) not only applied LM, but also used various topic models to cover the shortage of LM. (Chiu et al., 2013) explored statistical machine translation model to translate the sentences containing typos into correct ones. In their model, the sentence with the highest translation probabil"
W14-6825,I05-3017,0,0.0475959,"租 車 化 corrected w[i] 晨 衣 池 六 町 恨 扔 搭 計 少 Table 7: Rule 4. The correction related with two neighbored words. w[i − 1] – – – – – – – – – – – – – 為 w[i] 自 式 蘭 令 排 柏 莎 玈 棒 想 名 頂 白 是 w[i + 1] 到 式 滿 令 排 柏 增 管 組 心 性 頂 花 嗎 w[i + 2] – – – – – – – – – – – 大, 有 商 – w[i + 3] – – – – – – – – – – – 名 店 – corrected w[i] and w[i + 1] 知道 試試 浪漫 冷冷 拜拜 伯伯 沙僧 旅館 幫助 相信 明星 鼎鼎 百貨 什麼 Table 8: Rule 5. Two words are simultaneously corrected. ing. Similar character set C provided by (Liu et al., 2010) is used to substitute the original words in the graph construction stage. The LM is built on the Academia Sinica corpus (Emerson, 2005) with IRSTLM toolkit (Federico et al., 2008). The CRF model is achieved by training and tuning on the Academia Sinica corpus with the toolkit CRF++ 0.586 . For Chinese word segmentation, the ICTCLAS20117 is exploited. Dev13, Dev14C and Dev14B and the test set is named as Test14 respectively. In the Dev14B, there are 4624 errors, in which the statistics information of the three common character usage confusions in section 4 is shown in Table 10, so it is necessary to deal with them respectively. The dictionary D used in SSSP algorithm is SogouW 4 dictionary from Sogou inc., which is in simplifi"
W14-6825,P10-1028,0,0.031665,"des the methods that all single-character words are supposed to be errors and an LM is used for correction, for example (Lin and Chu, 2013) . They developed a system which supposed that all single-character words may be typos. They replaced all single-character words by similar characters using a confusion set and segmented the newly created sentences again. If a new sentence resulted in a better word segmentation, spelling error was reported. Their system gave good detection recall but low false-alarm rate. Related Work Over the past few years, there were many methods proposed for CSC task. (Sun et al., 2010) developed a phrase-based spelling error model from the clickthrough data by means of measuring the edit distance between an input query and the optimal spelling correction. (Gao et al., 2010) explored the ranker-based approach which included visual similarity, phonological similarity, dictionary, and frequency features for large scale web search. (Ahmad and Kondrak, 2005) proposed a spelling error The third category utilizes more than one approaches for detection and an LM for correction. (Hsieh et al., 2013) used two different systems for 158 Given a dictionary D and a similar characters C,"
W14-6825,C10-1041,0,0.0249502,"ngle-character words may be typos. They replaced all single-character words by similar characters using a confusion set and segmented the newly created sentences again. If a new sentence resulted in a better word segmentation, spelling error was reported. Their system gave good detection recall but low false-alarm rate. Related Work Over the past few years, there were many methods proposed for CSC task. (Sun et al., 2010) developed a phrase-based spelling error model from the clickthrough data by means of measuring the edit distance between an input query and the optimal spelling correction. (Gao et al., 2010) explored the ranker-based approach which included visual similarity, phonological similarity, dictionary, and frequency features for large scale web search. (Ahmad and Kondrak, 2005) proposed a spelling error The third category utilizes more than one approaches for detection and an LM for correction. (Hsieh et al., 2013) used two different systems for 158 Given a dictionary D and a similar characters C, for a sentence S of m characters {c1 , c2 , . . . , cm }, the original vertices V of the DAG in (Jia et al., 2013b) are: error detection. The first system detected error characters based on un"
W14-6825,W14-1710,1,0.841393,"Graph Model 4.1 CRF Model Two classifiers using CRF model are respectively trained to tackle the common character usage confusions: 在” (at) (pinyin: zai), 再” (again, more, then) (pinyin: zai) and “的” (of)(pinyin: de), “地” (-ly, adverb-forming particle) (pinyin: de), “得”(so that, have to) (pinyin: de). We assume that the correct character selection is related with its neighboring two words and part-of-speech (POS) tags. The classifiers are trained on a large fivegram token set which is extracted from a large POS tagged corpus. The feature selection algorithm is according to (Zhao et al., 2013; Wang et al., 2014; Jia et al., 2013a). The feature set for CRF model is as follows: The graph model based on word segmentation in (Jia et al., 2013b) includes the revised graph model in section 3 still has its limitations. For a sentence, in the graph construction stage, the substitution is only applied to the situation that the number of words after segmenting has to be decreased, which means there exists new longer word after segmentation. In addition, if the segmentation result of a sentence is a single character, the graph model does not work, because a single character will not be substituted. For example"
W14-6825,W13-4413,0,0.122774,"mmunity, and media image and psychology evaluation: a computational intelligence approach). † Corresponding author. • The misspelled word is a non-word, for example “come” is misspelled into “cmoe”; 1 Pinyin is the official phonetic system for transcribing the sound of Chinese characters into Latin script. 157 Proceedings of the Third CIPS-SIGHAN Joint Conference on Chinese Language Processing, pages 157–166, Wuhan, China, 20-21 October 2014 • The misspelled word is still a legal word, for example “come” is misspelled into “cone”. model from search query logs to improve the quality of query. (Han and Chang, 2013) employed maximum entropy models for CSC. They trained a maximum entropy model for each Chinese character based on a large raw corpus and used the model to detect the spelling errors. While in Chinese, if the misspelled word is a nonword, the word segmenter will not recognize it as a word, but split it into two or more words with fewer characters. For example, if “你好世界” in Example 1 of Table 1 is misspelled into “你好世節”, the word segmenter will segment it into “你好/世/節” instead of “你好/世節”. For non-word spelling error, the misspelled word will be mis-segmented. Name Golden Misspelled Pinyin Trans"
W14-6825,W13-4406,0,0.181848,"Missing"
W14-6825,W13-4415,0,0.0198119,"category utilizes more than one approaches for detection and an LM for correction. (Hsieh et al., 2013) used two different systems for 158 Given a dictionary D and a similar characters C, for a sentence S of m characters {c1 , c2 , . . . , cm }, the original vertices V of the DAG in (Jia et al., 2013b) are: error detection. The first system detected error characters based on unknown word detection and LM verification. The second one solved error detection based on a suggestion dictionary generated from a confusion set. Finally, two systems were combined to obtain the final detection result. (He and Fu, 2013) divided typos into three categories which were character-level errors (CLEs), word-level errors (WLEs) and context-level errors (CLEs), and three different methods were used to detect the different errors respectively. In addition to using the result of word segmentation for detection, (Yeh et al., 2013) also proposed a dictionarybased method to detect spelling errors. The dictionary contained similar pronunciation and shape information for each Chinese character. (Yang et al., 2013) proposed another method to improve the candidate detections. They employed high confidence pattern matching to"
W14-6825,yang-etal-2012-spell,1,0.841289,"re, for the improved graph model in Bake-off 2014, we remain use the graph model in Bake-off 2013 without any modification. Table 10: Three common character usage confusions in the Dev14B. 5.2 The Improved Graph Model Precision, Recall and F1 curves We treat the graph model without filters in Bakeoff 2013 as our baseline in Bake-off 2014. The edge function is the linear combination of similarity and log conditional probability: ω L = ωs − β log P where ω0 ≡ 0 which is omitted in the equation, and ωs for different kinds of characters are shown in Table 11. The LM is set to bigram according to (Yang et al., 2012). Improved Kneser-Ney method is used for LM smoothing (Chen and Goodman, 1999). Precision Recall F1 0 2 4 6 β 8 10 12 14 (a) The graph model. ωs 1 1 2 2 2 Precision, Recall and F1 curves Type same pronunciation same tone same pronunciation different tone similar pronunciation same tone similar pronunciation different tone similar shape 1 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0 Table 11: ωs used in ω L . We utilize the correction precision (P), correction recall (R) and F1 score (F) as the metrics. The computational formulas are as follows: 1 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0 Precision Recall"
W14-6825,W13-4417,0,0.0309168,"ionary generated from a confusion set. Finally, two systems were combined to obtain the final detection result. (He and Fu, 2013) divided typos into three categories which were character-level errors (CLEs), word-level errors (WLEs) and context-level errors (CLEs), and three different methods were used to detect the different errors respectively. In addition to using the result of word segmentation for detection, (Yeh et al., 2013) also proposed a dictionarybased method to detect spelling errors. The dictionary contained similar pronunciation and shape information for each Chinese character. (Yang et al., 2013) proposed another method to improve the candidate detections. They employed high confidence pattern matching to strengthen the candidate errors after word segmentation. The last category is formed by the methods which use word segmentation for detection and different models for correction (Liu et al., 2013; Chen et al., 2013; Chiu et al., 2013). (Liu et al., 2013) used support vector machine (SVM) to select the most probable sentence from multiple candidates. They used word segmentation and machine translation model to generate the candidates respectively. The SVM was used to rerank the candid"
W14-6825,W13-4407,0,0.229637,"are: error detection. The first system detected error characters based on unknown word detection and LM verification. The second one solved error detection based on a suggestion dictionary generated from a confusion set. Finally, two systems were combined to obtain the final detection result. (He and Fu, 2013) divided typos into three categories which were character-level errors (CLEs), word-level errors (WLEs) and context-level errors (CLEs), and three different methods were used to detect the different errors respectively. In addition to using the result of word segmentation for detection, (Yeh et al., 2013) also proposed a dictionarybased method to detect spelling errors. The dictionary contained similar pronunciation and shape information for each Chinese character. (Yang et al., 2013) proposed another method to improve the candidate detections. They employed high confidence pattern matching to strengthen the candidate errors after word segmentation. The last category is formed by the methods which use word segmentation for detection and different models for correction (Liu et al., 2013; Chen et al., 2013; Chiu et al., 2013). (Liu et al., 2013) used support vector machine (SVM) to select the mo"
W14-6825,W13-4420,0,0.0224914,"elling error, the misspelled word will be mis-segmented. Name Golden Misspelled Pinyin Translation Example 1 你好/世界 你好/世/節 ni hao shi jie hello the world Two key techniques, word segmentation (Zhao et al., 2006a; Zhao and Kit, 2008b; Zhao et al., 2006b; Zhao and Kit, 2008a; Zhao and Kit, 2007; Zhao and Kit, 2011; Zhao et al., 2010) and language model (LM), are also popularly used for CSC. Most of those approaches can fall into four categories. The first category consists of the methods that all the characters in a sentence are assumed to be errors and an LM is used for correction (Chang, 1995; Yu et al., 2013). (Chang, 1995) proposed a method that replaced each character in the sentence based on a confusion set and computed the probability of the original sentence and all modified sentences according to a bigram language model generated from a newspaper corpus. The method based on the motivation that all the typos were caused by either visual similarity or phonological similarity. So they manually built a confusion set as a key factor in their system. Although the method can detect misspelled words well, it was very time consuming for detection, generated too much false positive results and was not"
W14-6825,W03-1730,0,0.0261375,"l does not work, because a single character will not be substituted. For example in the following two sentences, the “他” (he) (pinyin: ta) in the first sentence should be corrected into “她” (she) (pinyin: ta) and the “的” (of)(pinyin: de) in the second sentence should be corrected into “地” (-ly, adverb-forming particle) (pinyin: de), however, the graph model does not work for this case. wj,−2 , posj,−2 , wj,−1 , posj,−1 , wj,0 , posj,0 , wj,1 , posj,1 , wj,2 , posj,2 where j is the token index to indicate its position, wj,0 is the current candidate character and posj,0 is its POS tag. ICTCLAS (Zhang et al., 2003) is adopted for POS tagging. A set of feature strings that we used are presented in Table 2. The labels for “的” (of) (pinyin: de), “地” (-ly, adverb-forming particle) (pinyin: de), “得”(so that, have to) (pinyin: de) are 1, 2, 3 and “在” (at) (pinyin: zai), “再” (again, more, then) (pinyin: zai) are 1, 2. • 雖然我不在我的國家，不能見到媽媽，可 是我要給‘他’ (him) (pinyin: ta)打電話！ Translation after correction: Though I’m not in my country so that I cannot see my mum, I would like to call her! • 我們也不要想太多；我們來好好‘的’ (of) (pinyin: de)出去玩吧！ Translation after correction: We would not worry too much, just enjoy ourselves outside"
W14-6825,I08-1002,1,0.834803,"ge raw corpus and used the model to detect the spelling errors. While in Chinese, if the misspelled word is a nonword, the word segmenter will not recognize it as a word, but split it into two or more words with fewer characters. For example, if “你好世界” in Example 1 of Table 1 is misspelled into “你好世節”, the word segmenter will segment it into “你好/世/節” instead of “你好/世節”. For non-word spelling error, the misspelled word will be mis-segmented. Name Golden Misspelled Pinyin Translation Example 1 你好/世界 你好/世/節 ni hao shi jie hello the world Two key techniques, word segmentation (Zhao et al., 2006a; Zhao and Kit, 2008b; Zhao et al., 2006b; Zhao and Kit, 2008a; Zhao and Kit, 2007; Zhao and Kit, 2011; Zhao et al., 2010) and language model (LM), are also popularly used for CSC. Most of those approaches can fall into four categories. The first category consists of the methods that all the characters in a sentence are assumed to be errors and an LM is used for correction (Chang, 1995; Yu et al., 2013). (Chang, 1995) proposed a method that replaced each character in the sentence based on a confusion set and computed the probability of the original sentence and all modified sentences according to a bigram languag"
W14-6825,I08-4017,1,0.825742,"ge raw corpus and used the model to detect the spelling errors. While in Chinese, if the misspelled word is a nonword, the word segmenter will not recognize it as a word, but split it into two or more words with fewer characters. For example, if “你好世界” in Example 1 of Table 1 is misspelled into “你好世節”, the word segmenter will segment it into “你好/世/節” instead of “你好/世節”. For non-word spelling error, the misspelled word will be mis-segmented. Name Golden Misspelled Pinyin Translation Example 1 你好/世界 你好/世/節 ni hao shi jie hello the world Two key techniques, word segmentation (Zhao et al., 2006a; Zhao and Kit, 2008b; Zhao et al., 2006b; Zhao and Kit, 2008a; Zhao and Kit, 2007; Zhao and Kit, 2011; Zhao et al., 2010) and language model (LM), are also popularly used for CSC. Most of those approaches can fall into four categories. The first category consists of the methods that all the characters in a sentence are assumed to be errors and an LM is used for correction (Chang, 1995; Yu et al., 2013). (Chang, 1995) proposed a method that replaced each character in the sentence based on a confusion set and computed the probability of the original sentence and all modified sentences according to a bigram languag"
W14-6825,W06-0127,1,0.657413,"acter based on a large raw corpus and used the model to detect the spelling errors. While in Chinese, if the misspelled word is a nonword, the word segmenter will not recognize it as a word, but split it into two or more words with fewer characters. For example, if “你好世界” in Example 1 of Table 1 is misspelled into “你好世節”, the word segmenter will segment it into “你好/世/節” instead of “你好/世節”. For non-word spelling error, the misspelled word will be mis-segmented. Name Golden Misspelled Pinyin Translation Example 1 你好/世界 你好/世/節 ni hao shi jie hello the world Two key techniques, word segmentation (Zhao et al., 2006a; Zhao and Kit, 2008b; Zhao et al., 2006b; Zhao and Kit, 2008a; Zhao and Kit, 2007; Zhao and Kit, 2011; Zhao et al., 2010) and language model (LM), are also popularly used for CSC. Most of those approaches can fall into four categories. The first category consists of the methods that all the characters in a sentence are assumed to be errors and an LM is used for correction (Chang, 1995; Yu et al., 2013). (Chang, 1995) proposed a method that replaced each character in the sentence based on a confusion set and computed the probability of the original sentence and all modified sentences accordin"
W14-6825,Y06-1012,1,0.758913,"acter based on a large raw corpus and used the model to detect the spelling errors. While in Chinese, if the misspelled word is a nonword, the word segmenter will not recognize it as a word, but split it into two or more words with fewer characters. For example, if “你好世界” in Example 1 of Table 1 is misspelled into “你好世節”, the word segmenter will segment it into “你好/世/節” instead of “你好/世節”. For non-word spelling error, the misspelled word will be mis-segmented. Name Golden Misspelled Pinyin Translation Example 1 你好/世界 你好/世/節 ni hao shi jie hello the world Two key techniques, word segmentation (Zhao et al., 2006a; Zhao and Kit, 2008b; Zhao et al., 2006b; Zhao and Kit, 2008a; Zhao and Kit, 2007; Zhao and Kit, 2011; Zhao et al., 2010) and language model (LM), are also popularly used for CSC. Most of those approaches can fall into four categories. The first category consists of the methods that all the characters in a sentence are assumed to be errors and an LM is used for correction (Chang, 1995; Yu et al., 2013). (Chang, 1995) proposed a method that replaced each character in the sentence based on a confusion set and computed the probability of the original sentence and all modified sentences accordin"
W14-6825,O13-1005,0,\N,Missing
Y06-1001,J96-3004,0,0.124816,"ining corpus. 1 Introduction Chinese text is written without natural delimiters, so word segmentation is an essential first step in Chinese language processing. In this aspect, Chinese is quite different from English in which sentences of words delimited by white spaces. Though it seems very simple, Chinese word segmentation (CWS) is not a trivial problem. Actually, it has been active area of research in computational linguistics for almost 20 years and has drawn more and more attention in the Chinese language processing community. To accomplish such a task, various technologies are developed [1][2]. In the early work of Chinese word segmentation, word-based method once played the dominant role, in which maximum matching algorithm is the most typical method. Here, the term, word, means those known words are shown in known lexicon or training corpus (also are called in-vocabulary(IV) words.). Explicit known word information was still important learning object even after statistical methods were introduced in CWS [1]. To give a comprehensive comparison of Chinese segmentation on common test corpora, three International Chinese Word Segmentation Bakeoffs were held in 2003, 2005, and 2006"
Y06-1001,J05-4005,1,0.835086,"ng corpus. 1 Introduction Chinese text is written without natural delimiters, so word segmentation is an essential first step in Chinese language processing. In this aspect, Chinese is quite different from English in which sentences of words delimited by white spaces. Though it seems very simple, Chinese word segmentation (CWS) is not a trivial problem. Actually, it has been active area of research in computational linguistics for almost 20 years and has drawn more and more attention in the Chinese language processing community. To accomplish such a task, various technologies are developed [1][2]. In the early work of Chinese word segmentation, word-based method once played the dominant role, in which maximum matching algorithm is the most typical method. Here, the term, word, means those known words are shown in known lexicon or training corpus (also are called in-vocabulary(IV) words.). Explicit known word information was still important learning object even after statistical methods were introduced in CWS [1]. To give a comprehensive comparison of Chinese segmentation on common test corpora, three International Chinese Word Segmentation Bakeoffs were held in 2003, 2005, and 20061 ,"
Y06-1001,I05-3017,0,0.0509021,"ethod once played the dominant role, in which maximum matching algorithm is the most typical method. Here, the term, word, means those known words are shown in known lexicon or training corpus (also are called in-vocabulary(IV) words.). Explicit known word information was still important learning object even after statistical methods were introduced in CWS [1]. To give a comprehensive comparison of Chinese segmentation on common test corpora, three International Chinese Word Segmentation Bakeoffs were held in 2003, 2005, and 20061 , and there were 12, 23 and 23 participants, respectively [3], [4], [5]. Four segmentation corpora were presented in each Bakeoff. Thus, twelve corpora are available from Bakeoff 2003, 2005, and 2006. A summary of these corpora is shown in Table 1. In all of proposed methods, character-based tagging method [6], instead of traditional word-based one, quickly rose in Bakeoff-2005 as a remarkable one with state-ofthe-art performance. Especially, two participants, Ng and Tseng, gave the best results 1 In 2006, the name of the third Bakeoff has been changed into International Chinese Language Processing Bakeoff for the reason that named entity recognition task wa"
Y06-1001,W06-0115,0,0.0259048,"once played the dominant role, in which maximum matching algorithm is the most typical method. Here, the term, word, means those known words are shown in known lexicon or training corpus (also are called in-vocabulary(IV) words.). Explicit known word information was still important learning object even after statistical methods were introduced in CWS [1]. To give a comprehensive comparison of Chinese segmentation on common test corpora, three International Chinese Word Segmentation Bakeoffs were held in 2003, 2005, and 20061 , and there were 12, 23 and 23 participants, respectively [3], [4], [5]. Four segmentation corpora were presented in each Bakeoff. Thus, twelve corpora are available from Bakeoff 2003, 2005, and 2006. A summary of these corpora is shown in Table 1. In all of proposed methods, character-based tagging method [6], instead of traditional word-based one, quickly rose in Bakeoff-2005 as a remarkable one with state-ofthe-art performance. Especially, two participants, Ng and Tseng, gave the best results 1 In 2006, the name of the third Bakeoff has been changed into International Chinese Language Processing Bakeoff for the reason that named entity recognition task was add"
Y06-1001,O03-4002,0,0.519694,"known word information was still important learning object even after statistical methods were introduced in CWS [1]. To give a comprehensive comparison of Chinese segmentation on common test corpora, three International Chinese Word Segmentation Bakeoffs were held in 2003, 2005, and 20061 , and there were 12, 23 and 23 participants, respectively [3], [4], [5]. Four segmentation corpora were presented in each Bakeoff. Thus, twelve corpora are available from Bakeoff 2003, 2005, and 2006. A summary of these corpora is shown in Table 1. In all of proposed methods, character-based tagging method [6], instead of traditional word-based one, quickly rose in Bakeoff-2005 as a remarkable one with state-ofthe-art performance. Especially, two participants, Ng and Tseng, gave the best results 1 In 2006, the name of the third Bakeoff has been changed into International Chinese Language Processing Bakeoff for the reason that named entity recognition task was added 1 in almost all tracks [7], [8]. In Bakeoff-2006, all participants whose system performance ranked first in a track at least used character-based method. Researchers turned to character-based method from traditional word-based method onl"
Y06-1001,I05-3027,0,0.219659,"e presented in each Bakeoff. Thus, twelve corpora are available from Bakeoff 2003, 2005, and 2006. A summary of these corpora is shown in Table 1. In all of proposed methods, character-based tagging method [6], instead of traditional word-based one, quickly rose in Bakeoff-2005 as a remarkable one with state-ofthe-art performance. Especially, two participants, Ng and Tseng, gave the best results 1 In 2006, the name of the third Bakeoff has been changed into International Chinese Language Processing Bakeoff for the reason that named entity recognition task was added 1 in almost all tracks [7], [8]. In Bakeoff-2006, all participants whose system performance ranked first in a track at least used character-based method. Researchers turned to character-based method from traditional word-based method only with four years. The success of Bakeoffs not only gave some public consistent segmentation standards, but also proposed a corpus-based segmentation standard representation, instead of the representation of known word lexicon and segmentation manual before. Thus Chinese word segmentation becomes more like corpus-based machine learning procedure in this sense. With the supply of common segme"
Y06-1001,O05-4005,0,0.0761006,"the representation of known word lexicon and segmentation manual before. Thus Chinese word segmentation becomes more like corpus-based machine learning procedure in this sense. With the supply of common segmentation standards of Bakeoffs, the comparison problem on word-based method and character-based method are still remained. Though most effective Chinese word segmentation techniques are turned to pure characterbased methods, some researchers are still insisting that character-based method alone can not be superior to the method that combines both word information and character information [9] [10][11]. In this paper, we will briefly explore the linguistic background of such turnaround in Chinese word segmentation and give an empirical comparison of these methods. Table 1. Corpora statistics of Bakeoff 2003, 2005 and 2006 Provider Corpus words words Academia AS2003 Big5 5.8M 12K 0.022 Sinica AS2005 Big5 5.45M 122K 0.043 AS2006 Big5 5.45M 91K 0.042 CityU2003 Big5 240K 35K 0.071 City University CityU2005 Big5 1.46M 41K 0.074 CityU2006 Big5 1.64M 220K 0.040 Hong Kong Encoding #Training #Test OOV rate University of CTB2003 GB 250K 40K 0.181 Pennsylvania CTB2006 GB 508K 154K 0.088 Micro"
Y06-1001,W02-1815,0,0.0266408,"iews the track of character-based method. We discuss the linguistic background of characterbased features (especially for unigram feature) in Section 3. We evaluate unigram feature through CWS performance comparison in Section 4. In Section 5, the experimental 2 results between word-based method and character-based method are demonstrated. We summarize our contribution in Section 6. 2 The Track of Character-based Method Character-based tagging method is a classification technique for Chinese characters according to their positions occurring in Chinese words. This method was first conducted in [12], two classifiers were combined to perform Chinese word segmentation. First, a maximum entropy model was used to segment the text, and then an error driven transformation model was used to correct the word boundaries. This method was continuously improved in [6] and [13], where a unified maximum entropy model was used to perform character-based tagging task. As mentioned above, two top participants, Tseng and Low, won the most outstanding success in Bakeoff-2005 with the similar character-based tagging method, though the former used conditional random field model while the latter still used ma"
Y06-1001,W03-1728,0,0.0194171,"between word-based method and character-based method are demonstrated. We summarize our contribution in Section 6. 2 The Track of Character-based Method Character-based tagging method is a classification technique for Chinese characters according to their positions occurring in Chinese words. This method was first conducted in [12], two classifiers were combined to perform Chinese word segmentation. First, a maximum entropy model was used to segment the text, and then an error driven transformation model was used to correct the word boundaries. This method was continuously improved in [6] and [13], where a unified maximum entropy model was used to perform character-based tagging task. As mentioned above, two top participants, Tseng and Low, won the most outstanding success in Bakeoff-2005 with the similar character-based tagging method, though the former used conditional random field model while the latter still used maximum entropy model. In Bakeoff-2006, all participants whose system performance ranked first in a track at least used character-based method. There are five participants ranked the first in one track at least [14][15][16][17][18], in which two participants used condition"
Y06-1001,W06-0127,1,0.899024,"ndaries. This method was continuously improved in [6] and [13], where a unified maximum entropy model was used to perform character-based tagging task. As mentioned above, two top participants, Tseng and Low, won the most outstanding success in Bakeoff-2005 with the similar character-based tagging method, though the former used conditional random field model while the latter still used maximum entropy model. In Bakeoff-2006, all participants whose system performance ranked first in a track at least used character-based method. There are five participants ranked the first in one track at least [14][15][16][17][18], in which two participants used conditional random field, and the other three used maximum entropy as learning model. Especially, four participants directly or indirectly used the technique in [7]. 3 Features of Character Classification for CWS CWS is the primary processing in Chinese language processing. Thus it is difficult or even impossible to use derivative features like other Chinese language processing tasks. The basic features that we can use are characters themselves. We perform a position frequency statistics of Chinese characters in MSRA2005 training corpus. All cha"
Y06-1001,W06-0121,0,0.0208805,"ies. This method was continuously improved in [6] and [13], where a unified maximum entropy model was used to perform character-based tagging task. As mentioned above, two top participants, Tseng and Low, won the most outstanding success in Bakeoff-2005 with the similar character-based tagging method, though the former used conditional random field model while the latter still used maximum entropy model. In Bakeoff-2006, all participants whose system performance ranked first in a track at least used character-based method. There are five participants ranked the first in one track at least [14][15][16][17][18], in which two participants used conditional random field, and the other three used maximum entropy as learning model. Especially, four participants directly or indirectly used the technique in [7]. 3 Features of Character Classification for CWS CWS is the primary processing in Chinese language processing. Thus it is difficult or even impossible to use derivative features like other Chinese language processing tasks. The basic features that we can use are characters themselves. We perform a position frequency statistics of Chinese characters in MSRA2005 training corpus. All charact"
Y06-1001,W06-0133,0,0.0197151,"This method was continuously improved in [6] and [13], where a unified maximum entropy model was used to perform character-based tagging task. As mentioned above, two top participants, Tseng and Low, won the most outstanding success in Bakeoff-2005 with the similar character-based tagging method, though the former used conditional random field model while the latter still used maximum entropy model. In Bakeoff-2006, all participants whose system performance ranked first in a track at least used character-based method. There are five participants ranked the first in one track at least [14][15][16][17][18], in which two participants used conditional random field, and the other three used maximum entropy as learning model. Especially, four participants directly or indirectly used the technique in [7]. 3 Features of Character Classification for CWS CWS is the primary processing in Chinese language processing. Thus it is difficult or even impossible to use derivative features like other Chinese language processing tasks. The basic features that we can use are characters themselves. We perform a position frequency statistics of Chinese characters in MSRA2005 training corpus. All characters"
Y06-1001,W06-0117,0,0.0163219,"thod was continuously improved in [6] and [13], where a unified maximum entropy model was used to perform character-based tagging task. As mentioned above, two top participants, Tseng and Low, won the most outstanding success in Bakeoff-2005 with the similar character-based tagging method, though the former used conditional random field model while the latter still used maximum entropy model. In Bakeoff-2006, all participants whose system performance ranked first in a track at least used character-based method. There are five participants ranked the first in one track at least [14][15][16][17][18], in which two participants used conditional random field, and the other three used maximum entropy as learning model. Especially, four participants directly or indirectly used the technique in [7]. 3 Features of Character Classification for CWS CWS is the primary processing in Chinese language processing. Thus it is difficult or even impossible to use derivative features like other Chinese language processing tasks. The basic features that we can use are characters themselves. We perform a position frequency statistics of Chinese characters in MSRA2005 training corpus. All characters appearin"
Y06-1001,C04-1081,0,0.0744673,"omparisons of CRF method and semi-CRF method in corpora of Bakeoff-2003 Tagging method CityU2003 PKU2003 CTB2003 CRF learning in Liang’s method 0.937 0.941 0.879 Semi-CRF learning in Liang’s method 0.936 0.936 0.868 Character-based method 0.947 0.956 0.872 9 5.4 Comparison with the Best Existing Work Comparisons between our results and best existing results in three Bakeoffs are shown in Table 9-11. There are two types of existing results for Bakeoff-2003 and 2005. One is the best F scores of Bakeoff 2003, 2005 for each corpus in closed test tracks. The other are the results of Peng and Tseng [19] [8]. As for Bakeoff-2006, our results is slightly lower than those in [14], because more feature templates were used in [14] than that in Table 4. Table 9. Comparisons of best existing results and our results in the corpora of Bakeoff 2003 Participant AS2003 CTB2003 CityU2003 PKU2003 Peng 0.956 0.849 0.928 0.941 Tseng 0.970 0.863 0.947 0.953 Best results of Bakeoff 0.961 0.881 0.940 0.951 0.872 0.947 0.956 Ours 0.973 Table 10. Comparisons of best existing results and our results in the corpora of Bakeoff 2005 Participant Tseng AS2005 CityU2005 PKU2005 MSRA2005 0.947 0.943 0.950 0.964 Best res"
Y06-1001,W06-1655,0,0.0347181,"ional features in semi-CRF that are intuitively very useful. Also, it has been shown that an order-M seimi-CRF is strictly more powerful than an order-M CRF. However, the use of a semi-Markov CRF for Chinese word segmentation did not find significant gains over the standard CRF in Liang’s previous work [21]. The comparison with our method in three corpora of Bakeoff-2003 is shown in Table 8. Liang used two learning model, standard CRF and semi-CRF in the same features, the results did not support the superiority of semi-CRF, too. A hybrid Markov/Semi-Markov CRF learning method was proposed in [22]. The traditional semi-CRF method was effectively improved since Liang’s work, and the Fmeasure of MSRA2005 corpus has been 0.9684, which is much better than the best results in Bakeoff-2005. However, we see that it is not better than character-based tagging method, which is 0.974 as seen in Table 7. Table 8. Comparisons of CRF method and semi-CRF method in corpora of Bakeoff-2003 Tagging method CityU2003 PKU2003 CTB2003 CRF learning in Liang’s method 0.937 0.941 0.879 Semi-CRF learning in Liang’s method 0.936 0.936 0.868 Character-based method 0.947 0.956 0.872 9 5.4 Comparison with the Best"
Y06-1001,W03-1719,0,\N,Missing
Y06-1001,N06-2049,0,\N,Missing
Y06-1001,P06-2123,0,\N,Missing
Y06-1001,W06-0120,0,\N,Missing
Y06-1001,I05-3025,0,\N,Missing
Y06-1001,W03-1726,0,\N,Missing
Y06-1012,J05-4005,1,0.608158,"Missing"
Y06-1012,O03-4002,0,0.572227,"d to used for the particular corpus. No other data is allowed. In this study, we will limit our comparison in closed test because additional linguistical resource often varies from system to system. The remainder of the paper is organized as follows. The next section is a simple introduction to conditional random field. Feature templates and tag sets are given in Section 3. In Section 4, our experimental results are demonstrated. We summarize our contribution in Section 5. 2 Conditional Random Field Maximum entropy tagger was used in early character-based tagging for Chinese word segmentation [2], [3], while we choose linear-chain CRF as our learning model in this study. It can combine rich feature representation and probabilistic finite state model, too. In addition, it can avoid so-called ‘label-bias’ problem in some degree. Actually, such model was also proved to be very effective in many existing works [8]. Conditional random field (CRF) is a statistical sequence modeling framework first introduced into language processing in [9]. Work by Peng et al. first used this framework for Chinese word segmentation by treating it as a binary decision task, such that each Chinese character i"
Y06-1012,W03-1728,0,0.0145844,"used for the particular corpus. No other data is allowed. In this study, we will limit our comparison in closed test because additional linguistical resource often varies from system to system. The remainder of the paper is organized as follows. The next section is a simple introduction to conditional random field. Feature templates and tag sets are given in Section 3. In Section 4, our experimental results are demonstrated. We summarize our contribution in Section 5. 2 Conditional Random Field Maximum entropy tagger was used in early character-based tagging for Chinese word segmentation [2], [3], while we choose linear-chain CRF as our learning model in this study. It can combine rich feature representation and probabilistic finite state model, too. In addition, it can avoid so-called ‘label-bias’ problem in some degree. Actually, such model was also proved to be very effective in many existing works [8]. Conditional random field (CRF) is a statistical sequence modeling framework first introduced into language processing in [9]. Work by Peng et al. first used this framework for Chinese word segmentation by treating it as a binary decision task, such that each Chinese character is lab"
Y06-1012,I05-3017,0,0.435599,"Missing"
Y06-1012,I05-3027,0,0.49937,"Missing"
Y06-1012,C04-1081,0,0.45997,". Feature templates and tag sets are given in Section 3. In Section 4, our experimental results are demonstrated. We summarize our contribution in Section 5. 2 Conditional Random Field Maximum entropy tagger was used in early character-based tagging for Chinese word segmentation [2], [3], while we choose linear-chain CRF as our learning model in this study. It can combine rich feature representation and probabilistic finite state model, too. In addition, it can avoid so-called ‘label-bias’ problem in some degree. Actually, such model was also proved to be very effective in many existing works [8]. Conditional random field (CRF) is a statistical sequence modeling framework first introduced into language processing in [9]. Work by Peng et al. first used this framework for Chinese word segmentation by treating it as a binary decision task, such that each Chinese character is labeled either as the beginning of a word or not. The probability assigned to a label sequence for a particular sequence of characters by a CRF is given by the equation below: pλ (Y |W ) = XX 1 exp( λk fk (yt−1 , yt , W, t)) Z(W ) t∈T (1) k where Y = {yi } is the label sequence for the sentence, W is the sequence of"
Y06-1012,W06-0127,1,0.341064,"03 and 2005 for each corpus under closed test. The other is the results of Tseng et al.. All of our results are performed under TMPT-01 (or TMPT-044 ) and 6-tag set. 3 4 The Third SIGHAN Chinese Language Processing Bakeoff has been held, the results will be presented at the 5th SIGHAN Workshop, to be held at ACL-COLING 2006 in Sydney, Australia, July 22-23, 2006. We also participate this Bakeoff, and our system with the techniques presented in this paper won four highest and two third highest F measures in six Chinese word segmentation tracks. Our results on Bakeoff-2006 appear in SIGHAN-2006 [11]. Note that TMPT-04 is a feature template set only including n-gram ones. Researchers in CWS did not make an agree on what P and T−1 T0 T1 are feature templates for closed test or not. Thus the results under TMPT-04 are demonstrated, too. We will see that our system gets stateof-the-art performance in either of feature template set. 91 Table 6. Comparisons of best existing results and our results in the corpora of Bakeoff-2003 and 2005 Participant AS2003 CTB2003 CityU2003 PKU2003 AS2005 CityU2005 PKU2005 MSRA2005 Peng 0.956 0.849 0.928 0.941 Tseng 0.970 0.863 0.947 0.953 0.947 0.943 0.950 0.96"
Y06-1012,W03-1719,0,\N,Missing
Y06-1012,W03-1726,0,\N,Missing
Y06-1012,I05-3025,0,\N,Missing
Y12-1036,H94-1028,0,0.018221,"Missing"
Y12-1036,J96-1002,0,0.0897155,"raining 100K 1M 4371102 43679593 Development 2K 83765 Table 2: The size of different datasets 337 Test 100K 4123184 Figure 2: The sample training sentence and its ME features. The phrase penalty is used to estimate the preference towards a sentence which has more segmented phrases or less segmented phrases. Practically, a factor is introduced for each phrase translation. If the factor is less than 1 we would prefer a longer phrase and otherwise shorter phrase is preferred. 4 Experiment It is natural to formulize PTC as a sequence labeling task, which usually adopt maximum entropy Markov model(Berger et al., 1996) as the standard tool in most existing literatures2 . Thus we conducted a group of experiments to evaluate the proposed SMT approach with the ME model as the baseline system. The features we use are the most frequent used ones in related works(Wang et al., 2006; Li et al., 2009). 4.1 Experiment settings Firstly, we realize a way to get large Pinyin and Chinese character sentence pairs because to our best knowledge there is no such open dataset available. Given a Chinese character sequence, it is much easier to convert it to a Pinyin sequence because when a Chinese character is put in a context"
Y12-1036,J90-2002,0,0.356186,"translation probabilities can be estimated by the relative frequency of the phrases extracted from the aligned corpus. Note that the phrase used is not a meaningful word combination any more, it just refers to a series of consequent characters. In practice, a model using both translation directions, with the proper weight setting, often outperforms a model that uses only one direction. The lexical weighting feature is such a measurement that can be effectively used to estimate whether a phrase pair is reliable or not. Empirically, the lexical weighting(Berger et al., 1994; Brown et al., 1993; Brown et al., 1990) is defined as follows: length(e) lex(e|f, a) = ∏ i=1 ∑ 1 w(ei |fj ) |{j|(i, j) ∈ a}| ∀(i,j)∈a Here a is an alignment function defining each Chinese character with its corresponding Pinyin and w refers to the lexical conditional probability. The above equation shows that for the phrase pair(f,e), the translation probability can be interpreted as the product of the aligned lexical pairs (fj ,ei ). For the PTC conversion problem, the lexical pair refers to the pinyin-character pair. Based on the alignment we can estimate the possibility of the transformation of phrase pairs from the lexical tran"
Y12-1036,J93-2003,0,0.0211413,"conditional phrase translation probabilities can be estimated by the relative frequency of the phrases extracted from the aligned corpus. Note that the phrase used is not a meaningful word combination any more, it just refers to a series of consequent characters. In practice, a model using both translation directions, with the proper weight setting, often outperforms a model that uses only one direction. The lexical weighting feature is such a measurement that can be effectively used to estimate whether a phrase pair is reliable or not. Empirically, the lexical weighting(Berger et al., 1994; Brown et al., 1993; Brown et al., 1990) is defined as follows: length(e) lex(e|f, a) = ∏ i=1 ∑ 1 w(ei |fj ) |{j|(i, j) ∈ a}| ∀(i,j)∈a Here a is an alignment function defining each Chinese character with its corresponding Pinyin and w refers to the lexical conditional probability. The above equation shows that for the phrase pair(f,e), the translation probability can be interpreted as the product of the aligned lexical pairs (fj ,ei ). For the PTC conversion problem, the lexical pair refers to the pinyin-character pair. Based on the alignment we can estimate the possibility of the transformation of phrase pairs"
Y12-1036,N10-2003,0,0.0132623,"rds. • extract phrases based on the former alignment model • minimum error rate training • decoding • the Pinyin after the next one pk+2 ; Figure 2 illustrates a full feature set sample, for the given sample sentence at the upper part of the figure, all related features for Pinyin-character pair, ”zhou 骤(abruptly)”, can be shown in the bottom table of the Figure. Finally, the converted Chinese character sequences are compared to the golden data, the accuracy results can be seen in Table 3. 4.3 Machine Translation Framework In this experiment, we conduct the process based on Stanford’s phrasal(Cer et al., 2010) which is an open source phrase-based machine translation system. For the traditional phrase-based machine translation method, the processing steps are often stated as following: 339 As we have known that it must be an one-to-one alignment for PTC, it is unnecessary to train the alignment model and the phrases can be directly extracted based on the one-to-one alignment of character and Pinyin. Our experiment is based on 3-gram language model and our maximum phrase length is set to 7. The results given by the SMT approach are in Table 3. We get the results based on three different training sets"
Y12-1036,P00-1031,0,0.0158982,"Missing"
Y12-1036,P01-1030,0,0.0106496,"hm (e1 , f1 )] , = arg max ∑ ∑M I J eI1 eI1 exp[ m=1 λm hm (e1 , f1 )] (1) where hm is the m-th feature function and λm is the m-th feature weight. The most common features used in modern phrased-based machine translation include phrase translation feature, language model feature, reordering model feature and word penalty feature. As usual, to train the SMT model parameters, we adopt the minimum error rate training(MERT)(Och, 2003), which obtained the model towards getting the 336 highest score corresponding to the concrete evaluation metric. For the sequence decoding, we use a stack decoder(Germann et al., 2001). 3.2 Features The following real-valued features are adopted for learning, the bidirectional phrase translation probabilities, p(ˆ e|fˆ) and p(fˆ|ˆ e), the bidirectional lexical ˆ weighting lex(ˆ e|f ) and lex(fˆ|ˆ e), the target Chinese character n-gram probability, p(ˆ e) and the phrase penalty. The estimation of these features requires a training corpus with source and target alignment at the character or word level. The bidirectional conditional phrase translation probability contain much richer information than the one directional phrase translation probability. When translating the sour"
Y12-1036,I11-1014,0,0.0221415,"Missing"
Y12-1036,C96-1035,0,0.0104003,"system. The features we use are the most frequent used ones in related works(Wang et al., 2006; Li et al., 2009). 4.1 Experiment settings Firstly, we realize a way to get large Pinyin and Chinese character sentence pairs because to our best knowledge there is no such open dataset available. Given a Chinese character sequence, it is much easier to convert it to a Pinyin sequence because when a Chinese character is put in a context, it usually has an unique Pinyin counterpart. Based on this observation, we label the Chinese text with Pinyins through the forward maximal matching algorithm (kwong Wong and Chan, 1996) incorporated with a word-Pinyin dictionary from Sogou 3 . The data 2 Though conditional random field has shown more effective than ME model to solve sequence labeling problem, it is not a practical tool for PTC due to too many labels that PTC requires causing too high computational cost. 3 The resource includes 4,083,906 Chinese word and Pinyin pairs, and it can be download from 338 from the People’s Daily of 1998 year is used as the training set and the development and test data are token from 1997 year’s. The size of datasets is in table 2, the data of 10K and 100K are extracted from the da"
Y12-1036,P04-1021,0,0.0334169,"Missing"
Y12-1036,J04-4002,0,0.0225815,"alignment process. After this, we need to extract a translation table from the aligned corpus. Then we will use all of the features to train a translation model. The last process is decoding the source sentence. 3.1 Translation Model Our SMT model is based on the discriminative learning framework which contains different realvalued features. In this model, F is a given foreign sentence F =f1 , f2 , ..., fJ , and needs to be translated into another sentence E=e1 , e2 , ..., eI . The real-valued features are defined over F and E as hi (E, F ). The score can be given by a log-linear formulation(Och and Ney, 2004) with respect to a series of weight parameters λ1 ,...,λn . For a given source language sentence f , we can obtain the target language sentence e according to the following equation: eI1 = arg max pλm (eI1 |f1J ) 1 eI1 ∑ I J exp[ M m=1 λm hm (e1 , f1 )] , = arg max ∑ ∑M I J eI1 eI1 exp[ m=1 λm hm (e1 , f1 )] (1) where hm is the m-th feature function and λm is the m-th feature weight. The most common features used in modern phrased-based machine translation include phrase translation feature, language model feature, reordering model feature and word penalty feature. As usual, to train the SMT m"
Y12-1036,P03-1021,0,0.0034153,"a given source language sentence f , we can obtain the target language sentence e according to the following equation: eI1 = arg max pλm (eI1 |f1J ) 1 eI1 ∑ I J exp[ M m=1 λm hm (e1 , f1 )] , = arg max ∑ ∑M I J eI1 eI1 exp[ m=1 λm hm (e1 , f1 )] (1) where hm is the m-th feature function and λm is the m-th feature weight. The most common features used in modern phrased-based machine translation include phrase translation feature, language model feature, reordering model feature and word penalty feature. As usual, to train the SMT model parameters, we adopt the minimum error rate training(MERT)(Och, 2003), which obtained the model towards getting the 336 highest score corresponding to the concrete evaluation metric. For the sequence decoding, we use a stack decoder(Germann et al., 2001). 3.2 Features The following real-valued features are adopted for learning, the bidirectional phrase translation probabilities, p(ˆ e|fˆ) and p(fˆ|ˆ e), the bidirectional lexical ˆ weighting lex(ˆ e|f ) and lex(fˆ|ˆ e), the target Chinese character n-gram probability, p(ˆ e) and the phrase penalty. The estimation of these features requires a training corpus with source and target alignment at the character or wo"
Y13-1024,E09-1063,0,0.0135494,"linguistic information to enhance Vietnamese to English MT. Few studies have been done for MT task between Vietnamese and Chinese as to our best knowledge. For such a low resource language pair, rule based MT systems are too hard to build, and statistical MT systems require too large parallel corpus that is also difficultly acquired. Though Chinese characters have been considered a useful intermediate form for MT, few studies made a full use of them. Instead, most existing approaches focus on the role of Chinese word during translation (Chang et al., 2008; Xu et al., 2008; Dyer et al., 2008; Ma and Way, 2009; Paul et al., 2010; Nguyen et al., 2010). (Chu et al., 2012) exploited shared Chinese characters between Chinese and Japanese to improve the concerned translation performance. The most recent work (Xi et al., 2012) 251 PACLIC-27 Character script Romanized script Vietnamese Chữ Nôm Quốc Ngữ (official now) Chinese Chinese characters (official now) pinyin rately used as a meaningful unit. Like Chinese, most Vietnamese words are bi-syllable. Chinese is written without blanks between words and Vietnamese is written with blanks between two syllables instead of words. Thus word segmentation becomes"
Y13-1024,2006.amta-papers.16,0,0.026662,"MT studies on Vietnamese focus on English as source or target language. As Vietnamese is an under-resourced language, most Vietnamese MT systems adopted rule based methods (Le et al., 2006; Le and Phan, 2009; Le and Phan, 2010). (Pham et al., 2009) used word-by-word translation incorporated with predefined templates to perform English-Vietnamese translation on weather bulletin texts. The similar strategy was also used in (Hoang et al., 2012) for Vietnamese to Katu language translation on the same domain. Until very recently, the statistical approach was applied to Vietnamese related MT task. (Nguyen and Shimazu, 2006; Nguyen et al., 2008) used self-defined morphological transformation and syntactic transformation to beforehand solve reordering problem for Vietnamese-English translation. (Thi and Dinh, 2008) introduced a word re-ordering approach that makes use of the syntactic rules extracted from parse tree for EnglishVietnamese MT. (Bui et al., 2010) proposed language dependent features to enhance VietnameseEnglish SMT. (Nguyen et al., 2012) integrated more knowledge about the topic of the text, partof-speech and morphology to resolve semantic ambiguity of words during translation. Based on empirical ob"
Y13-1024,2008.amta-papers.10,0,0.0638864,"Missing"
Y13-1024,C10-1092,0,0.0178386,"etnamese to English MT. Few studies have been done for MT task between Vietnamese and Chinese as to our best knowledge. For such a low resource language pair, rule based MT systems are too hard to build, and statistical MT systems require too large parallel corpus that is also difficultly acquired. Though Chinese characters have been considered a useful intermediate form for MT, few studies made a full use of them. Instead, most existing approaches focus on the role of Chinese word during translation (Chang et al., 2008; Xu et al., 2008; Dyer et al., 2008; Ma and Way, 2009; Paul et al., 2010; Nguyen et al., 2010). (Chu et al., 2012) exploited shared Chinese characters between Chinese and Japanese to improve the concerned translation performance. The most recent work (Xi et al., 2012) 251 PACLIC-27 Character script Romanized script Vietnamese Chữ Nôm Quốc Ngữ (official now) Chinese Chinese characters (official now) pinyin rately used as a meaningful unit. Like Chinese, most Vietnamese words are bi-syllable. Chinese is written without blanks between words and Vietnamese is written with blanks between two syllables instead of words. Thus word segmentation becomes a primary processing for both languages."
Y13-1024,J03-1002,0,0.00501627,"k 10: else 11: m=m−1 12: end if 13: end while 14: if m==1 then 15: Set a segmentation mark before ci+1 16: end if 17: i=i+m 18: end while nary that meet our requirements, we have to seek help from online resources for a quick but inaccurate solution. We let a crawler collect Vietnamese texts4 from the Internet and then feed the Google translator5 with the text, so that we obtain a loose parallel corpus between Vietnamese and Chinese. After each Vietnamese monosyllable and each Chinese character segmented as a word, we may obtain an aligned phrase table by using bidirectional GIZA++ alignment (Och and Ney, 2003). We perform two steps of pruning on the phrase table. First, only those aligned phrases that have the same numbers for both Vietnamese syllables and Chinese characters will be kept. Second, If a Vietnamese phrase is mapped to multiple Chinese phrases, then only the one with the highest aligning probability will be conserved. Regarding both Vietnamese syllables and Chinese characters in the phrase table as words, we finally build the second bilingual dictionary Dg with 6.8 million word pairs. Given a Vietnamese sentence, we apply the maximal matching algorithm twice to accomplish the word segm"
Y13-1024,J93-2003,0,0.0609824,"Missing"
Y13-1024,W08-0336,0,0.0544693,"Missing"
Y13-1024,2012.eamt-1.7,0,0.0885951,"d SMT system for a specific language pair, regardless of the possible useful linkages between these two languages. There is existing work that considered using helpful linguistic heuristics to enhance the curren∗ This work was partially supported by the National Natural Science Foundation of China (Grant No.60903119, Grant No.61170114, and Grant No.61272248), and the National Basic Research Program of China (Grant No.2009CB320901 and Grant No.2013CB329401). † corresponding author Figure 1: The phrase Chinese character culture sphere written in Chinese characters from different regions. t SMT (Chu et al., 2012), though their approaches still follow the standard processing pipeline of SMT. For those resource-poor languages, a pivot language will be used as an expedience (Utiyama and Isahara, 2007; Wu and Wang, 2009). In this work, we focus on machine translation (MT) for language pairs with few parallel corpora but rich linguistic connections. A case study on Vietnamese and Chinese will be done. To exploit the shared linguistic characteristics between the language pair, the common written form, Chinese character, is adopted as a translation bridge. Being the oldest continuously used writing system in"
Y13-1024,P08-1115,0,0.0331744,"sentences based on linguistic information to enhance Vietnamese to English MT. Few studies have been done for MT task between Vietnamese and Chinese as to our best knowledge. For such a low resource language pair, rule based MT systems are too hard to build, and statistical MT systems require too large parallel corpus that is also difficultly acquired. Though Chinese characters have been considered a useful intermediate form for MT, few studies made a full use of them. Instead, most existing approaches focus on the role of Chinese word during translation (Chang et al., 2008; Xu et al., 2008; Dyer et al., 2008; Ma and Way, 2009; Paul et al., 2010; Nguyen et al., 2010). (Chu et al., 2012) exploited shared Chinese characters between Chinese and Japanese to improve the concerned translation performance. The most recent work (Xi et al., 2012) 251 PACLIC-27 Character script Romanized script Vietnamese Chữ Nôm Quốc Ngữ (official now) Chinese Chinese characters (official now) pinyin rately used as a meaningful unit. Like Chinese, most Vietnamese words are bi-syllable. Chinese is written without blanks between words and Vietnamese is written with blanks between two syllables instead of words. Thus word seg"
Y13-1024,P02-1040,0,0.0948511,"rgmax ′ } ∀ω(wi )and{w0′ w1′ ..wn (P (ω(wi )′ |ω(wi−m+1 )′ ω(wi−m )′ ..ω(wi−1 )′ )), i=1 where ω(wi ) represent a related word of wi and {w0′ w1′ ..wn′ } is a permutation of {w0 w1 ..wn }. To prevent from generating too many reordering possibilities, the distance between the original position of each word and its new location is limited to less than 4 words. The above output sequence can be decoded through a Viterbi style algorithm. 6 Experiments We manually collect 2,046 sentence pairs as test set to evaluate the proposed approach. We report the MT performance using the original BLEU metric (Papineni et al., 2002). A trigram Chinese language model is trained on the text with segmentation that is extracted from the People’ Daily7 4 The Vietnamese corpus has 77 M bytes, 0.86 million Vietnamese sentences and 13 million Vietnamese monosyllables. 5 http:// translate.google.com/?hl=en#vi/zh-CN/ 6 7 255 The Word Forest of Synonyms: http://www.ir-lab.org/ It is the most popular newspaper in China. (1) PACLIC-27 Systems BLEU Ours/Stage 1 14.5 Ours/Stage 1+2 18.6 Google 20.3 Systems /wo ref. /w ref Ours/Stage 1 68.5 66.1 Ours/Stage 1+2 72.9 71.6 Google 69.3 67.7 Table 5: BLEU scores for the proposed system. Syst"
Y13-1024,W10-1760,0,0.0132282,"ation to enhance Vietnamese to English MT. Few studies have been done for MT task between Vietnamese and Chinese as to our best knowledge. For such a low resource language pair, rule based MT systems are too hard to build, and statistical MT systems require too large parallel corpus that is also difficultly acquired. Though Chinese characters have been considered a useful intermediate form for MT, few studies made a full use of them. Instead, most existing approaches focus on the role of Chinese word during translation (Chang et al., 2008; Xu et al., 2008; Dyer et al., 2008; Ma and Way, 2009; Paul et al., 2010; Nguyen et al., 2010). (Chu et al., 2012) exploited shared Chinese characters between Chinese and Japanese to improve the concerned translation performance. The most recent work (Xi et al., 2012) 251 PACLIC-27 Character script Romanized script Vietnamese Chữ Nôm Quốc Ngữ (official now) Chinese Chinese characters (official now) pinyin rately used as a meaningful unit. Like Chinese, most Vietnamese words are bi-syllable. Chinese is written without blanks between words and Vietnamese is written with blanks between two syllables instead of words. Thus word segmentation becomes a primary processin"
Y13-1024,N07-1061,0,0.0219687,"euristics to enhance the curren∗ This work was partially supported by the National Natural Science Foundation of China (Grant No.60903119, Grant No.61170114, and Grant No.61272248), and the National Basic Research Program of China (Grant No.2009CB320901 and Grant No.2013CB329401). † corresponding author Figure 1: The phrase Chinese character culture sphere written in Chinese characters from different regions. t SMT (Chu et al., 2012), though their approaches still follow the standard processing pipeline of SMT. For those resource-poor languages, a pivot language will be used as an expedience (Utiyama and Isahara, 2007; Wu and Wang, 2009). In this work, we focus on machine translation (MT) for language pairs with few parallel corpora but rich linguistic connections. A case study on Vietnamese and Chinese will be done. To exploit the shared linguistic characteristics between the language pair, the common written form, Chinese character, is adopted as a translation bridge. Being the oldest continuously used writing system in the world, Chinese characters are logograms that are still used to write Chinese (汉字/漢字 in Chinese, hànzì in Chinese pinyin) and Japanese (kanji). Such characters were used but are curren"
Y13-1024,P09-1018,0,0.0206815,"rren∗ This work was partially supported by the National Natural Science Foundation of China (Grant No.60903119, Grant No.61170114, and Grant No.61272248), and the National Basic Research Program of China (Grant No.2009CB320901 and Grant No.2013CB329401). † corresponding author Figure 1: The phrase Chinese character culture sphere written in Chinese characters from different regions. t SMT (Chu et al., 2012), though their approaches still follow the standard processing pipeline of SMT. For those resource-poor languages, a pivot language will be used as an expedience (Utiyama and Isahara, 2007; Wu and Wang, 2009). In this work, we focus on machine translation (MT) for language pairs with few parallel corpora but rich linguistic connections. A case study on Vietnamese and Chinese will be done. To exploit the shared linguistic characteristics between the language pair, the common written form, Chinese character, is adopted as a translation bridge. Being the oldest continuously used writing system in the world, Chinese characters are logograms that are still used to write Chinese (汉字/漢字 in Chinese, hànzì in Chinese pinyin) and Japanese (kanji). Such characters were used but are currently less frequently"
Y13-1024,P12-2056,0,0.0122051,"ms are too hard to build, and statistical MT systems require too large parallel corpus that is also difficultly acquired. Though Chinese characters have been considered a useful intermediate form for MT, few studies made a full use of them. Instead, most existing approaches focus on the role of Chinese word during translation (Chang et al., 2008; Xu et al., 2008; Dyer et al., 2008; Ma and Way, 2009; Paul et al., 2010; Nguyen et al., 2010). (Chu et al., 2012) exploited shared Chinese characters between Chinese and Japanese to improve the concerned translation performance. The most recent work (Xi et al., 2012) 251 PACLIC-27 Character script Romanized script Vietnamese Chữ Nôm Quốc Ngữ (official now) Chinese Chinese characters (official now) pinyin rately used as a meaningful unit. Like Chinese, most Vietnamese words are bi-syllable. Chinese is written without blanks between words and Vietnamese is written with blanks between two syllables instead of words. Thus word segmentation becomes a primary processing for both languages. Vietnamese is a prop-drop (pronoun-dropping) language, which means that certain classes of pronouns in Vietnamese may be omitted when they are in some sense pragmatically inf"
Y13-1024,C08-1128,0,0.0208815,"t long Vietnamese sentences based on linguistic information to enhance Vietnamese to English MT. Few studies have been done for MT task between Vietnamese and Chinese as to our best knowledge. For such a low resource language pair, rule based MT systems are too hard to build, and statistical MT systems require too large parallel corpus that is also difficultly acquired. Though Chinese characters have been considered a useful intermediate form for MT, few studies made a full use of them. Instead, most existing approaches focus on the role of Chinese word during translation (Chang et al., 2008; Xu et al., 2008; Dyer et al., 2008; Ma and Way, 2009; Paul et al., 2010; Nguyen et al., 2010). (Chu et al., 2012) exploited shared Chinese characters between Chinese and Japanese to improve the concerned translation performance. The most recent work (Xi et al., 2012) 251 PACLIC-27 Character script Romanized script Vietnamese Chữ Nôm Quốc Ngữ (official now) Chinese Chinese characters (official now) pinyin rately used as a meaningful unit. Like Chinese, most Vietnamese words are bi-syllable. Chinese is written without blanks between words and Vietnamese is written with blanks between two syllables instead of w"
Y15-1014,D07-1101,0,0.112081,"y Parsing Zhisong Zhang1,2 and Hai Zhao1,2,∗ † 1 Center for Brain-Like Computing and Machine Intelligence, Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, 200240, China 2 Key Laboratory of Shanghai Education Commission for Intelligent Interaction and Cognitive Engineering, Shanghai Jiao Tong University, Shanghai, 200240, China zzs2011@sjtu.edu.cn, zhaohai@cs.sjtu.edu.cn Abstract on dynamic programming strategies (Eisner, 1996; McDonald et al., 2005; McDonald and Pereira, 2006). In this recent decade, extensions have been made to use high-order factors (Carreras, 2007; Koo and Collins, 2010) in graph models and the highest one considers fourth-order (Ma and Zhao, 2012). However, all those methods usually use sparse indicator features as inputs and linear models to get the scores for later inference process. They are easy to suffer from the problem of sparsity, and linear models can be insufficient to effectively integrate all the sparse features in spite of various rich context that can be potentially exploited. In this work, we present a novel way of using neural network for graph-based dependency parsing, which fits the neural network into a simple proba"
Y15-1014,D14-1082,0,0.731629,"r in the dependency tree. For unlabeled projective dependency parsing, we have put a free distribution of our implementation on the Internet2 . The remainder of the paper is organized as follows: Section 2 discusses related work, Section 3 gives the background for graph-based dependency parsing, Section 4 describes our neural network model and how we utilize it with graph-based parsing and Section 5 presents our experiments, results and some discussions. We summarize this paper in Section 6. 2 Related Work There has been a few of attempts to parse with neural network. For dependency parsing, (Chen and Manning, 2014) uses neural network for greedy transition-based dependency parsing. We explore graph-based methods in this work, which might be difficultly utilized with neural network. (Le and Zuidema, 2014) implements a generative dependency model with a recursive neural network, but the model is used for re-ranking which needs k-best candidates. For constituency parsing, (Collobert, 2011) uses a convolutional neural network and solves the problem with a hierarchical tagging process. (Socher et al., 2010) and (Socher et al., 2013) use recursive neural network to model phrase-based parse trees, but their me"
Y15-1014,K15-2005,1,0.820656,"erns in natural languages. If we utilize pre-trained word vectors (see in Section 5.1), our models can be regarded as semi-supervised to some extent. (Koo et al., 2008) uses Brown clustering algorithm to obtain word representations, but then transforms them into sparse features as additional features and again uses the traditional methods; while in neural network models including this work, the embeddings directly replace sparse features for inputs. 3 3.1 Graph-based Dependency Parsing Background of Dependency Parsing Syntax information is important for many other tasks (Zhang and Zhao, 2013; Chen et al., 2015). As a classic syntactic problem, dependency parsing aims to predict a dependency tree, which directly represents head-modifier relationships between words in a sentence. Figure 1 shows a dependency tree, in which all the links connect headmodifier pairs. By enforcing that all the nodes must have one and only one parent and the resulting graphs should be acyclic and connected, we can get a directed dependency tree for a sentence (we usually add a dummy node hrooti for the sentence as the highest level node). Labels or dependency category can also be defined for the links in the dependency tree"
Y15-1014,P04-1015,0,0.151581,"ing Algorithms Graph-based methods usually need to use dynamic programming based parsing algorithms, which make use of the scores of sub-trees for larger sub-trees in a bottom-up way. These algorithms solve the inference problem, that is, how to get an optimal tree given the scores for the parts. Our proposed parsers also take these algorithms as backbones and use them for inference. In the traditional methods, scores are usually obtained directly from a linear model. In the learning phase, parameter estimation methods for structured linear models may adopt averaged perceptron (Collins, 2002; Collins and Roark, 2004) and maxmargin methods (Taskar et al., 2004). Still using all the existing parsing algorithms, this work focuses on improving scoring for the factors. In detail, our work uses neural network to determine the scores. Nevertheless the traditional methods might be difficultly extended to neural network because of the non-linearity. Therefore, we do not directly obtain scores from neural network. Instead we utilize a probabilistic model and obtain scores by some transformations, and then use these existing parsing algorithms for inference. 4 4.1 Neural Network Parsers The Probabilistic Model For g"
Y15-1014,W02-1001,0,0.0336608,"nodes 116 Parsing Algorithms Graph-based methods usually need to use dynamic programming based parsing algorithms, which make use of the scores of sub-trees for larger sub-trees in a bottom-up way. These algorithms solve the inference problem, that is, how to get an optimal tree given the scores for the parts. Our proposed parsers also take these algorithms as backbones and use them for inference. In the traditional methods, scores are usually obtained directly from a linear model. In the learning phase, parameter estimation methods for structured linear models may adopt averaged perceptron (Collins, 2002; Collins and Roark, 2004) and maxmargin methods (Taskar et al., 2004). Still using all the existing parsing algorithms, this work focuses on improving scoring for the factors. In detail, our work uses neural network to determine the scores. Nevertheless the traditional methods might be difficultly extended to neural network because of the non-linearity. Therefore, we do not directly obtain scores from neural network. Instead we utilize a probabilistic model and obtain scores by some transformations, and then use these existing parsing algorithms for inference. 4 4.1 Neural Network Parsers The"
Y15-1014,P14-1129,0,0.0840416,"Missing"
Y15-1014,C96-1058,0,0.772828,"r graph-based dependency parsing, it is not straightforward to extend the linear models to the more powerful nonlinear neural network, because we need to figure out the scores for the factors of the tree, which are not specified in the original treebank. That is, we only know which factors are in the correct parsing tree, but there are no natural ways to indicate how they are scored; the only intuition is to give high scores to the right factors and low scores to the wrong ones. In this work, a simple probabilistic model is adopted for the neural network parsers. It is one of Eisner’s models (Eisner, 1996). Precisely, Eisner’s model A is chosen and slightly modified for scoring. The model describes bi-gram lexical affinities, and it gives each possible link an affinity probability. The final probability of drawing a parsing tree for a sentence is the product of all the affinity probabilities. The original model also considers probabilities PACLIC 29 of words and tags and its formula is given as follows: the head-modifier pair, which is the factor for firstorder parsing. Naturally, we can define probabilities for high-order factors. The probability of a parse P r(words, tags, links) = P r(words,"
Y15-1014,W07-2416,0,0.0806141,"Missing"
Y15-1014,P10-1001,0,0.456666,"g Zhang1,2 and Hai Zhao1,2,∗ † 1 Center for Brain-Like Computing and Machine Intelligence, Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, 200240, China 2 Key Laboratory of Shanghai Education Commission for Intelligent Interaction and Cognitive Engineering, Shanghai Jiao Tong University, Shanghai, 200240, China zzs2011@sjtu.edu.cn, zhaohai@cs.sjtu.edu.cn Abstract on dynamic programming strategies (Eisner, 1996; McDonald et al., 2005; McDonald and Pereira, 2006). In this recent decade, extensions have been made to use high-order factors (Carreras, 2007; Koo and Collins, 2010) in graph models and the highest one considers fourth-order (Ma and Zhao, 2012). However, all those methods usually use sparse indicator features as inputs and linear models to get the scores for later inference process. They are easy to suffer from the problem of sparsity, and linear models can be insufficient to effectively integrate all the sparse features in spite of various rich context that can be potentially exploited. In this work, we present a novel way of using neural network for graph-based dependency parsing, which fits the neural network into a simple probabilistic model and can b"
Y15-1014,P08-1068,0,0.0384562,"because a dependency 1 2 http://ronan.collobert.com/senna/ https://github.com/zzsfornlp/nngdparser 115 Figure 1: An example dependency tree. parse tree has no non-terminal nodes while constituency parse trees are derived from the phrases structure. Semi-supervised methods usually incorporate word representations as the embeddings for words in the projection layer in neural network; they usually make use of lots of unlabeled data to find the patterns in natural languages. If we utilize pre-trained word vectors (see in Section 5.1), our models can be regarded as semi-supervised to some extent. (Koo et al., 2008) uses Brown clustering algorithm to obtain word representations, but then transforms them into sparse features as additional features and again uses the traditional methods; while in neural network models including this work, the embeddings directly replace sparse features for inputs. 3 3.1 Graph-based Dependency Parsing Background of Dependency Parsing Syntax information is important for many other tasks (Zhang and Zhao, 2013; Chen et al., 2015). As a classic syntactic problem, dependency parsing aims to predict a dependency tree, which directly represents head-modifier relationships between"
Y15-1014,D14-1081,0,0.0430706,"n 2 discusses related work, Section 3 gives the background for graph-based dependency parsing, Section 4 describes our neural network model and how we utilize it with graph-based parsing and Section 5 presents our experiments, results and some discussions. We summarize this paper in Section 6. 2 Related Work There has been a few of attempts to parse with neural network. For dependency parsing, (Chen and Manning, 2014) uses neural network for greedy transition-based dependency parsing. We explore graph-based methods in this work, which might be difficultly utilized with neural network. (Le and Zuidema, 2014) implements a generative dependency model with a recursive neural network, but the model is used for re-ranking which needs k-best candidates. For constituency parsing, (Collobert, 2011) uses a convolutional neural network and solves the problem with a hierarchical tagging process. (Socher et al., 2010) and (Socher et al., 2013) use recursive neural network to model phrase-based parse trees, but their methods might be unlikely generalized to dependency parsing because a dependency 1 2 http://ronan.collobert.com/senna/ https://github.com/zzsfornlp/nngdparser 115 Figure 1: An example dependency"
Y15-1014,C12-2077,1,0.930141,"igence, Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, 200240, China 2 Key Laboratory of Shanghai Education Commission for Intelligent Interaction and Cognitive Engineering, Shanghai Jiao Tong University, Shanghai, 200240, China zzs2011@sjtu.edu.cn, zhaohai@cs.sjtu.edu.cn Abstract on dynamic programming strategies (Eisner, 1996; McDonald et al., 2005; McDonald and Pereira, 2006). In this recent decade, extensions have been made to use high-order factors (Carreras, 2007; Koo and Collins, 2010) in graph models and the highest one considers fourth-order (Ma and Zhao, 2012). However, all those methods usually use sparse indicator features as inputs and linear models to get the scores for later inference process. They are easy to suffer from the problem of sparsity, and linear models can be insufficient to effectively integrate all the sparse features in spite of various rich context that can be potentially exploited. In this work, we present a novel way of using neural network for graph-based dependency parsing, which fits the neural network into a simple probabilistic model and can be furthermore generalized to high-order parsing. Instead of the sparse features"
Y15-1014,E06-1011,0,0.191443,"e. In fact, larger-sized context can be included and a seven-word window is actually considered for later experiments. 4.5 • POS tags (for each word) • Distance (to the node’s parent in the factor) Using embeddings and neural network, we only need to provide unigram features, which will be mapped to embeddings in the neural network. The connections between features will be exploited by the non-linear computations of the neural network. Those three kinds of features are treated in the 118 Integrating Lower-order Models for Higher-order Parsing Following standard practice for high-order models (McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010), we integrate the lower-order scores into the higher order parsing for better performance. For o2sib and o2g models in this work, we integrate the scores computed from the first-order model into second order factors. And for o3g model, PACLIC 29 #Number of sentences Corpus Train Dev PTB 39832 1700 16091 803 CTB #Number of tokens Corpus Train Dev PTB 950348 40121 CTB 437990 20454 two lower-order scores are integrated. Specifically, the score for the factor (g, h, s, m) will include the lower-order scores of o1 and o2sib in addition to the third-order sco"
Y15-1014,P05-1012,0,0.228563,"es H X the parent of m, we must assign 0 to all other Lhm , log(P r(T |S)) = P r(Lghm |context(g, h, m)) h means all the nodes that are not equal to H. The g,h,m  X logarithmic probability can be rewritten as follows:  = log(P r(T |S)) = + 0&lt;m≤length(S) + X Here H represents the real parent node of node m. The formula is in the form of summation of the factor scores, which are defined as:  Score(H, m) = log P r(LHm = 1) X  + log P r(Lhm = 0) 0≤h≤length(S) h6=H,h6=m After defining the score of each dependency factor, we can apply the scores to the existing parsing algorithms (Eisner, 1996; McDonald et al., 2005). High-Order Parsing We now generalize the model to high-order parsing. In the first-order model, we define probabilities for 117 X  log P r(Lghm = 0) h,g h6=H,g6=G h6=m,g6=m  log P r(Lhm = 0) 0≤h≤length(S) h6=H,h6=m 4.2 log P r(LGHm = 1) 0&lt;m≤length(S)   log P r(LHm = 1) X 4.3 Neural Network Model Now we adopt feed-forward neural network to learn and compute the probability for a factor. The inputs for the network are features for a factor such as word forms, POS tags and distance, and the output will be the probability that the factor exists in the parse tree. Figure 3 shows the structu"
Y15-1014,W03-3017,0,0.0796561,"ing. Instead of the sparse features used in traditional methods, we utilize distributed dense feature representations for neural network, which give better feature representations. The proposed parsers are evaluated on English and Chinese Penn Treebanks. Compared to existing work, our parsers give competitive performance with much more efficient inference. 1 Introduction There have been two classes of typical approaches for dependency parsing: transition-based parsing and graph-based parsing. The former parses sentences by making a series of shift-reduce decisions (Yamada and Matsumoto, 2003; Nivre, 2003), while the latter searches for a tree through graph algorithms by decomposing trees into factors. This paper will focus on graph-based methods, which are based ∗ Correspondence author. This work was partially supported by the National Natural Science Foundation of China (No. 61170114, and No. 61272248), the National Basic Research Program of China (No. 2013CB329401), the Science and Technology Commission of Shanghai Municipality (No. 13511500200), the European Union Seventh Framework Program (No. 247619), the Cai Yuanpei Program (CSC fund 201304490199 and 201304490171), and the art and scienc"
Y15-1014,D14-1162,0,0.0793387,"Missing"
Y15-1014,N12-1054,0,0.0844945,"lly, the score for the factor (g, h, s, m) will include the lower-order scores of o1 and o2sib in addition to the third-order score o3gScore(g, h, s, m) from o3g model. The integration of the scores can be shown by the following equation: Score(g, h, s, m) = o1Score(h, m) Test 2416 1910 Test 56702 50315 Table 2: Statistics for the data sets for dependency parsing. + o2sibScore(h, s, m) + o3gScore(g, h, s, m) More importantly, we may let the first-order model to serve as an edge-filter for high-order parsing. This type of pruning has been used by many graph-based models (Koo and Collins, 2010; Rush and Petrov, 2012) to avoid too expensive operations in high-order parsing. For our model, we utilize our own first-order neural network model which will produce the probabilities for all the edges in the graph. We simply set a pruning threshold so that all edges whose probabilities are under the threshold will be discarded for high-order parsing. 4.6 Efficient Neural Network Computation This subsection introduces two techniques to speed up neural network computation. Efficient computation strategies have been explored extensively for neural network language models (Morin and Bengio, 2005; Mnih and Hinton, 2008"
Y15-1014,P13-1045,0,0.0306533,"a few of attempts to parse with neural network. For dependency parsing, (Chen and Manning, 2014) uses neural network for greedy transition-based dependency parsing. We explore graph-based methods in this work, which might be difficultly utilized with neural network. (Le and Zuidema, 2014) implements a generative dependency model with a recursive neural network, but the model is used for re-ranking which needs k-best candidates. For constituency parsing, (Collobert, 2011) uses a convolutional neural network and solves the problem with a hierarchical tagging process. (Socher et al., 2010) and (Socher et al., 2013) use recursive neural network to model phrase-based parse trees, but their methods might be unlikely generalized to dependency parsing because a dependency 1 2 http://ronan.collobert.com/senna/ https://github.com/zzsfornlp/nngdparser 115 Figure 1: An example dependency tree. parse tree has no non-terminal nodes while constituency parse trees are derived from the phrases structure. Semi-supervised methods usually incorporate word representations as the embeddings for words in the projection layer in neural network; they usually make use of lots of unlabeled data to find the patterns in natural"
Y15-1014,W04-3201,0,0.0587742,"to use dynamic programming based parsing algorithms, which make use of the scores of sub-trees for larger sub-trees in a bottom-up way. These algorithms solve the inference problem, that is, how to get an optimal tree given the scores for the parts. Our proposed parsers also take these algorithms as backbones and use them for inference. In the traditional methods, scores are usually obtained directly from a linear model. In the learning phase, parameter estimation methods for structured linear models may adopt averaged perceptron (Collins, 2002; Collins and Roark, 2004) and maxmargin methods (Taskar et al., 2004). Still using all the existing parsing algorithms, this work focuses on improving scoring for the factors. In detail, our work uses neural network to determine the scores. Nevertheless the traditional methods might be difficultly extended to neural network because of the non-linearity. Therefore, we do not directly obtain scores from neural network. Instead we utilize a probabilistic model and obtain scores by some transformations, and then use these existing parsing algorithms for inference. 4 4.1 Neural Network Parsers The Probabilistic Model For graph-based dependency parsing, it is not str"
Y15-1014,N03-1033,0,0.131402,"Missing"
Y15-1014,P07-1031,0,0.0470973,"Missing"
Y15-1014,D13-1140,0,0.0356506,"o avoid too expensive operations in high-order parsing. For our model, we utilize our own first-order neural network model which will produce the probabilities for all the edges in the graph. We simply set a pruning threshold so that all edges whose probabilities are under the threshold will be discarded for high-order parsing. 4.6 Efficient Neural Network Computation This subsection introduces two techniques to speed up neural network computation. Efficient computation strategies have been explored extensively for neural network language models (Morin and Bengio, 2005; Mnih and Hinton, 2008; Vaswani et al., 2013). These models consider speeding up the output softmax layer which contains thousands of neurons. However, it is not the case for our neural network as the output layer of our network only has two neurons. Main computation cost in our network is from the first hidden layer, which needs matrix multiplications and the hyperbolic tangent activation calculations for the hidden neurons. Similar to some previous work (Devlin et al., 2014; Chen and Manning, 2014), we apply the precalculation strategy to speed up the most concerned computation. This can be implemented as calculating a lookup table for"
Y15-1014,D13-1082,1,0.819837,"nd perform quite well together with neural network. In recent years, using distributed representations and neural network has gradually gained popularity in natural language processing (NLP) since the pioneer work of (Bengio et al., 2003). Several neural network language models have reported exciting results for the tasks of machine translation and speech recognition (Schwenk, 2007; Mikolov et al., 2010; 114 29th Pacific Asia Conference on Language, Information and Computation pages 114 - 123 Shanghai, China, October 30 - November 1, 2015 Copyright 2015 by Zhisong Zhang and Hai Zhao PACLIC 29 Wang et al., 2013; Wang et al., 2014; Wang et al., 2015). Many other tasks of NLP have also been reconsidered using neural network, the SENNA system1 (Collobert et al., 2011) solved the tasks of partof-speech (POS) tagging, chunking, named entity recognition and semantic role labeling. In this work, we utilize neural network for firstorder, second-order and third-order graph-based dependency parsing, with the help of the existing graph-based parsing algorithms. For high-order parsing, it is performed after the first-order parser prunes unlikely parts of the parsing tree. We use neural network to learn dense re"
Y15-1014,D14-1023,1,0.792478,"ll together with neural network. In recent years, using distributed representations and neural network has gradually gained popularity in natural language processing (NLP) since the pioneer work of (Bengio et al., 2003). Several neural network language models have reported exciting results for the tasks of machine translation and speech recognition (Schwenk, 2007; Mikolov et al., 2010; 114 29th Pacific Asia Conference on Language, Information and Computation pages 114 - 123 Shanghai, China, October 30 - November 1, 2015 Copyright 2015 by Zhisong Zhang and Hai Zhao PACLIC 29 Wang et al., 2013; Wang et al., 2014; Wang et al., 2015). Many other tasks of NLP have also been reconsidered using neural network, the SENNA system1 (Collobert et al., 2011) solved the tasks of partof-speech (POS) tagging, chunking, named entity recognition and semantic role labeling. In this work, we utilize neural network for firstorder, second-order and third-order graph-based dependency parsing, with the help of the existing graph-based parsing algorithms. For high-order parsing, it is performed after the first-order parser prunes unlikely parts of the parsing tree. We use neural network to learn dense representations for w"
Y15-1014,W03-3023,0,0.167859,"neralized to high-order parsing. Instead of the sparse features used in traditional methods, we utilize distributed dense feature representations for neural network, which give better feature representations. The proposed parsers are evaluated on English and Chinese Penn Treebanks. Compared to existing work, our parsers give competitive performance with much more efficient inference. 1 Introduction There have been two classes of typical approaches for dependency parsing: transition-based parsing and graph-based parsing. The former parses sentences by making a series of shift-reduce decisions (Yamada and Matsumoto, 2003; Nivre, 2003), while the latter searches for a tree through graph algorithms by decomposing trees into factors. This paper will focus on graph-based methods, which are based ∗ Correspondence author. This work was partially supported by the National Natural Science Foundation of China (No. 61170114, and No. 61272248), the National Basic Research Program of China (No. 2013CB329401), the Science and Technology Commission of Shanghai Municipality (No. 13511500200), the European Union Seventh Framework Program (No. 247619), the Cai Yuanpei Program (CSC fund 201304490199 and 201304490171), and the"
Y15-1014,D08-1059,0,0.119726,"Missing"
Y15-1014,D12-1030,0,0.0191341,"ning, 2014), which reported results of the transition-based neural network parser. For graphbased parsers, in order to get exact comparisons between traditional methods and neural network methods, we run the traditional graph-based parsers under the same executing environment as our parsers. In detail, MSTParser10 for o1 and o2sib models and MaxParser11 (Ma and Zhao, 2012) for o2g and o3g models are respectively used for comparison. Notice that in recent years, there have been plenty of graphbased parsers which utilize various techniques and obtain state-of-art results (Rush and Petrov, 2012; Zhang and McDonald, 2012), however, they will not be included in the comparisons for the reason that we only concern about basic graph-based parsing al10 http://sourceforge.net/projects/mstparser/ http://sourceforge.net/projects/maxparser/, this is a C++ implementation for several high-order graph-based parsers 11 PACLIC 29 Parser o1-nn o2sib-nn o2g-nn o3g-nn o1-M st o2sib-M st o2g-M ax o3g-M ax transition UAS 91.77 92.35 92.18 92.52 91.31 91.99 92.12 92.60 92.0 Root 96.61 96.40 96.85 96.81 95.12 95.90 96.03 96.31 – CM 35.89 39.86 38.45 41.10 36.67 39.74 40.11 42.63 – Speed 150 109 89 38 18 14 2 0.3 1013 5.4 We find t"
Y15-1031,D13-1106,0,0.0236413,", the first pass uses a BNLM in decoding to produce an n-best list. Then, a CSLM is used to rerank those n-best translations in the second pass (Schwenk et al., 2006; Son et al., 2010; Schwenk et al., 2012; Son et al., 2012). Nearly all of the previous works only conduct CSLMs on English, we conduct CSLM on Chinese in this paper. Vaswani et al. propose a method for reducing the training cost of CSLM and apply it into SMT decoder (Vaswani et al., 2013). Some other studies try to implement neural network LM or translation model for SMT (Gao et al., 2014; Devlin et al., 2014; Zhang et al., 2014; Auli et al., 2013; Liu et al., 2013; Sundermeyer et al., 2014; Cho et al., 2014; Zou et al., 2013; Lauly et al., 2014; Kalchbrenner and Blunsom, 2013). 271 The remainder is organized as follows: In Section 2, we will review the background of English to Chinese SMT. The character based SMT will be proposed in Section 3. In Section 4, the experiments will be conducted and the results will be analyzed . We will conclude our work in the Section 5. 2 Background The ancient Chinese (or Classical Chinese, 文言文) can be conveniently split into characters, for most characters in ancient Chinese still keep understood by o"
Y15-1031,N09-4002,0,0.0321488,"aseline is designed for Chinese-English translation. In this paper, we follow the same setting of the baseline system, only convert the source language and the target language. 273 BLEU (dev) 42.24 40.64 42.70 42.80 BLEU (dev) 42.80 42.78 42.85 BLEU (test) 39.33 38.08 39.78 40.31 BLEU (test) 40.31 40.04 40.30 Table 3: Different Heuristics Used for Word Alignment 4.2 The N -gram Language Model In this part, we will investigate how the factors in the n-gram LM influence the whole system. The scale of the training corpus is one of the most important factors to LM. And “more data is better data” (Brants and Xu, 2009) has been proved to be one of the most important rules for constructing a LMs. First we randomly divide the whole training sets into 4 parts equally. We build the LM with 1, 2 and 4 parts (i.e. for 1/4, 1/2 and the whole corpus respectively), with other setting as default. Then, we add the dictionary information to the LM. The pr stands for the size of the dictionary and the pf stands for the characters’ frequency in the dictionary. The results in Table 4 show that using the whole corpus 3 It is available at http://www.itl.nist.gov/iad/ mig/tests/mt/2009/ PACLIC 29 Segmentation Methods FMM Seg"
Y15-1031,J93-2003,0,0.0685388,"nd they must combine together into words to make sense. If we split modern Chinese sentences into characters, the semantic meaning in the words will partially lose. Whether or not this semantic function of Chinese word can be partly replaced by the alignment model and Language Model (LM) of character-based SMT will be shown in this paper. Ancient Chinese 三 人 行 ， 则 必 有 我 师 焉 。 Modern Chinese 三个 人 走路 , 那么 一定 存在 我的 老师 在其中 。 English Meaning three people walk , so must be my teacher/tutor there . Table 1: Ancient Chinese and Modern Chinese SMT as a research domain started in the late 1980s at IBM (Brown et al., 1993), which maps individual words to words and allows for deletion and insertion of words. Lately, various researchPACLIC 29 es have shown better translation quality with phrase translation. Phrase-based SMT can be traced back to Och’s alignment template model (Och and Ney, 2004), which can be re-framed as a phrase translation system. Other researchers augmented their systems with phrase translation, such as Yamada and Knight (Yamada and Knight, 2001), who used phrase translation in a syntax-based model. The phrase translation model is based on the noisy channel model. Bayes rule is mostly used to"
Y15-1031,W08-0336,0,0.0950037,"Missing"
Y15-1031,P96-1041,0,0.126614,"systems. Although there are also a lot of other works on automatic evaluation of SMT, such as METEOR (Lavie and Agarwal, 2007), GTM (Melamed et al., 2003) and TER (Snover et al., 2006), whether word or character is more suitable for automatic evaluation of Chinese translation output has not been systematically investigated (Li et al., 2011). Recently, different kinds of characterlevel SMT evaluation metrics are proposed, which also support that character-level SMT may have its own advantage accordingly (Li et al., 2011; Liu and Ng, 2012). Traditionally, Back-off N-gram Language Models (BNLM) (Chen and Goodman, 1996; Chen and Goodman, 1998; Stolcke, 2002) are being widely used for probability estimation. For a better probability estimation method, recently, ContinuousSpace Language Models (CSLM), especially Neural Network Language Models (NNLM) (Bengio et al., 2003; Schwenk, 2007; Le et al., 2011) are being used in SMT (Schwenk et al., 2006; Son et al., 2010; Schwenk et al., 2012; Son et al., 2012; Wang et al., 2013). These works have shown that CSLMs can improve the BLEU scores of SMT when compared with BNLMs, on the condition that the training data for language modeling are in the same size. However, i"
Y15-1031,D14-1179,0,0.095692,"Missing"
Y15-1031,P14-1129,0,0.0444373,"Missing"
Y15-1031,P14-1066,0,0.0130467,"the two pass approach, or nbest reranking. In this approach, the first pass uses a BNLM in decoding to produce an n-best list. Then, a CSLM is used to rerank those n-best translations in the second pass (Schwenk et al., 2006; Son et al., 2010; Schwenk et al., 2012; Son et al., 2012). Nearly all of the previous works only conduct CSLMs on English, we conduct CSLM on Chinese in this paper. Vaswani et al. propose a method for reducing the training cost of CSLM and apply it into SMT decoder (Vaswani et al., 2013). Some other studies try to implement neural network LM or translation model for SMT (Gao et al., 2014; Devlin et al., 2014; Zhang et al., 2014; Auli et al., 2013; Liu et al., 2013; Sundermeyer et al., 2014; Cho et al., 2014; Zou et al., 2013; Lauly et al., 2014; Kalchbrenner and Blunsom, 2013). 271 The remainder is organized as follows: In Section 2, we will review the background of English to Chinese SMT. The character based SMT will be proposed in Section 3. In Section 4, the experiments will be conducted and the results will be analyzed . We will conclude our work in the Section 5. 2 Background The ancient Chinese (or Classical Chinese, 文言文) can be conveniently split into characters, for m"
Y15-1031,P14-1142,1,0.844158,"s are shown in Table 8. n-gram LM 7 7 9 9 9 13 n-gram MERT 4 7 4 7 10 7 BLEU (dev) 42.95 25.54 42.84 25.93 15.82 25.41 4-gram BLEU (test) 40.30 39.91 40.55 40.75 40.37 40.47 Table 8: Parameter Combinations of n-gram LM and ngram MERT At last, the length of n-gram MERT and the smoothing methods are tuned together. The LM is set as 9-gram, the best BLEU score in n-gram LM experiments, and other factors set as default in the toolkits. The results are shown in Table 9. Among different parameters-combined setting, 275 Traditional Backoff N -gram LMs (BNLMs) have been widely used in many NLP tasks (Jia and Zhao, 2014; Zhang et al., 2012; Xu and Zhao, 2012). Recently, Continuous-Space Language Models (CSLMs), especially Neural Network Language Models (NNLMs) (Bengio et al., 2003; Schwenk, 2007; Mikolov et al., 2010; Le et al., 2011), are actively used in SMT (Schwenk et al., 2006; Schwenk et al., 2006; Schwenk et al., 2012; Son et al., 2012; Niehues and Waibel, 2012). These models have demonstrated that CSLMs can improve BLEU scores of SMT over n-gram LMs with the same sized corpus for LM training. An attractive feature of CSLMs is that they can predict the probabilities of ngrams outside the training corp"
Y15-1031,D13-1176,0,0.0316504,"tions in the second pass (Schwenk et al., 2006; Son et al., 2010; Schwenk et al., 2012; Son et al., 2012). Nearly all of the previous works only conduct CSLMs on English, we conduct CSLM on Chinese in this paper. Vaswani et al. propose a method for reducing the training cost of CSLM and apply it into SMT decoder (Vaswani et al., 2013). Some other studies try to implement neural network LM or translation model for SMT (Gao et al., 2014; Devlin et al., 2014; Zhang et al., 2014; Auli et al., 2013; Liu et al., 2013; Sundermeyer et al., 2014; Cho et al., 2014; Zou et al., 2013; Lauly et al., 2014; Kalchbrenner and Blunsom, 2013). 271 The remainder is organized as follows: In Section 2, we will review the background of English to Chinese SMT. The character based SMT will be proposed in Section 3. In Section 4, the experiments will be conducted and the results will be analyzed . We will conclude our work in the Section 5. 2 Background The ancient Chinese (or Classical Chinese, 文言文) can be conveniently split into characters, for most characters in ancient Chinese still keep understood by one who only knows modern Chinese (or Written Vernacular Chinese, 白话文) words. For example, “三人行，则必有我师焉。” is one of the popular sentenc"
Y15-1031,P07-2045,0,0.00507061,"not increase as the maximum length of phrases increases. Alignment Parameters union intersect grow-diag-final grow-diag-final-and Maximum Length 7 10 13 Comparison Experiment We use the patent data for the Chinese to English patent translation subtask from the NTCIR-9 patent translation task (Goto et al., 2011). The parallel training, development, and test data consists of 1 million (M), 2,000, and 2,000 sentences, respectively1 . The basic settings of the NTCIR-9 English to Chinese translation baseline system (Goto et al., 2011) was followed2 . The Moses phrase-based SMT system was applied (Koehn et al., 2007), together with GIZA++ (Och and Ney, 2003) for alignment and MERT (Och, 2003) for tuning on the development data. 14 standard SMT features were used: five translation model scores, one word penalty score, seven distortion scores and one LM score. The 1 Since we are the participants of NTCIR-9, so we have the bilingual sides of the evaluation data. 2 We are aware that the original NTCIR patentMT baseline is designed for Chinese-English translation. In this paper, we follow the same setting of the baseline system, only convert the source language and the target language. 273 BLEU (dev) 42.24 40."
Y15-1031,W07-0734,0,0.0366977,"test sets. To evaluate the quality of Chinese translation output, the International Workshop on Spoken Language Translation in 2005 (IWSLT’2005) used the word-level BLEU metric (Papineni et al.,2002). However, IWSLT’08 and NIST’08 adopted character-level evaluation metrics 270 29th Pacific Asia Conference on Language, Information and Computation pages 270 - 280 Shanghai, China, October 30 - November 1, 2015 Copyright 2015 by Rui Wang, Hai Zhao and Bao-Liang Lu PACLIC 29 to rank the submitted systems. Although there are also a lot of other works on automatic evaluation of SMT, such as METEOR (Lavie and Agarwal, 2007), GTM (Melamed et al., 2003) and TER (Snover et al., 2006), whether word or character is more suitable for automatic evaluation of Chinese translation output has not been systematically investigated (Li et al., 2011). Recently, different kinds of characterlevel SMT evaluation metrics are proposed, which also support that character-level SMT may have its own advantage accordingly (Li et al., 2011; Liu and Ng, 2012). Traditionally, Back-off N-gram Language Models (BNLM) (Chen and Goodman, 1996; Chen and Goodman, 1998; Stolcke, 2002) are being widely used for probability estimation. For a better"
Y15-1031,P11-2028,0,0.0198058,"NIST’08 adopted character-level evaluation metrics 270 29th Pacific Asia Conference on Language, Information and Computation pages 270 - 280 Shanghai, China, October 30 - November 1, 2015 Copyright 2015 by Rui Wang, Hai Zhao and Bao-Liang Lu PACLIC 29 to rank the submitted systems. Although there are also a lot of other works on automatic evaluation of SMT, such as METEOR (Lavie and Agarwal, 2007), GTM (Melamed et al., 2003) and TER (Snover et al., 2006), whether word or character is more suitable for automatic evaluation of Chinese translation output has not been systematically investigated (Li et al., 2011). Recently, different kinds of characterlevel SMT evaluation metrics are proposed, which also support that character-level SMT may have its own advantage accordingly (Li et al., 2011; Liu and Ng, 2012). Traditionally, Back-off N-gram Language Models (BNLM) (Chen and Goodman, 1996; Chen and Goodman, 1998; Stolcke, 2002) are being widely used for probability estimation. For a better probability estimation method, recently, ContinuousSpace Language Models (CSLM), especially Neural Network Language Models (NNLM) (Bengio et al., 2003; Schwenk, 2007; Le et al., 2011) are being used in SMT (Schwenk e"
Y15-1031,P12-1097,0,0.016235,"15 by Rui Wang, Hai Zhao and Bao-Liang Lu PACLIC 29 to rank the submitted systems. Although there are also a lot of other works on automatic evaluation of SMT, such as METEOR (Lavie and Agarwal, 2007), GTM (Melamed et al., 2003) and TER (Snover et al., 2006), whether word or character is more suitable for automatic evaluation of Chinese translation output has not been systematically investigated (Li et al., 2011). Recently, different kinds of characterlevel SMT evaluation metrics are proposed, which also support that character-level SMT may have its own advantage accordingly (Li et al., 2011; Liu and Ng, 2012). Traditionally, Back-off N-gram Language Models (BNLM) (Chen and Goodman, 1996; Chen and Goodman, 1998; Stolcke, 2002) are being widely used for probability estimation. For a better probability estimation method, recently, ContinuousSpace Language Models (CSLM), especially Neural Network Language Models (NNLM) (Bengio et al., 2003; Schwenk, 2007; Le et al., 2011) are being used in SMT (Schwenk et al., 2006; Son et al., 2010; Schwenk et al., 2012; Son et al., 2012; Wang et al., 2013). These works have shown that CSLMs can improve the BLEU scores of SMT when compared with BNLMs, on the conditio"
Y15-1031,P13-1078,0,0.0159361,"es a BNLM in decoding to produce an n-best list. Then, a CSLM is used to rerank those n-best translations in the second pass (Schwenk et al., 2006; Son et al., 2010; Schwenk et al., 2012; Son et al., 2012). Nearly all of the previous works only conduct CSLMs on English, we conduct CSLM on Chinese in this paper. Vaswani et al. propose a method for reducing the training cost of CSLM and apply it into SMT decoder (Vaswani et al., 2013). Some other studies try to implement neural network LM or translation model for SMT (Gao et al., 2014; Devlin et al., 2014; Zhang et al., 2014; Auli et al., 2013; Liu et al., 2013; Sundermeyer et al., 2014; Cho et al., 2014; Zou et al., 2013; Lauly et al., 2014; Kalchbrenner and Blunsom, 2013). 271 The remainder is organized as follows: In Section 2, we will review the background of English to Chinese SMT. The character based SMT will be proposed in Section 3. In Section 4, the experiments will be conducted and the results will be analyzed . We will conclude our work in the Section 5. 2 Background The ancient Chinese (or Classical Chinese, 文言文) can be conveniently split into characters, for most characters in ancient Chinese still keep understood by one who only knows"
Y15-1031,I05-3025,0,0.0484949,"rds of segmentation between word-based and character-based English to Chinese translation are different, as well as the standard of the evaluation of them. That is, the test data contains words as the smallest unit for word-based SMT, and characters for character-based SMT. So the translated sentences of word-based translation will be converted into character-based sentence, and evaluated together with character-based translation BLEU score for fair comparison. We select two popular segmentation segmenters, one of which is based on Forward Maximum Matching (FMM) algorithm with the lexicon of (Low et al., 2005), and the other is based on Conditional Random Fields (CRF) with the same implementation of (Zhao et al., 2006). Because most Chinese words contains 1 to 4 characters, so we set the word-based LM as default trigram in SRILM, and character-based LM for 5-gram. All the different methods share the same other default parameters in the toolkits which will be further introduced in Section 4. There seems to be no ambiguity in different character segmentations, however English characters, numbers and other symbols are also contained in the corpus. If they are split into “characters” like “年 增 长 百 分 之"
Y15-1031,N03-2021,0,0.0628757,"lity of Chinese translation output, the International Workshop on Spoken Language Translation in 2005 (IWSLT’2005) used the word-level BLEU metric (Papineni et al.,2002). However, IWSLT’08 and NIST’08 adopted character-level evaluation metrics 270 29th Pacific Asia Conference on Language, Information and Computation pages 270 - 280 Shanghai, China, October 30 - November 1, 2015 Copyright 2015 by Rui Wang, Hai Zhao and Bao-Liang Lu PACLIC 29 to rank the submitted systems. Although there are also a lot of other works on automatic evaluation of SMT, such as METEOR (Lavie and Agarwal, 2007), GTM (Melamed et al., 2003) and TER (Snover et al., 2006), whether word or character is more suitable for automatic evaluation of Chinese translation output has not been systematically investigated (Li et al., 2011). Recently, different kinds of characterlevel SMT evaluation metrics are proposed, which also support that character-level SMT may have its own advantage accordingly (Li et al., 2011; Liu and Ng, 2012). Traditionally, Back-off N-gram Language Models (BNLM) (Chen and Goodman, 1996; Chen and Goodman, 1998; Stolcke, 2002) are being widely used for probability estimation. For a better probability estimation metho"
Y15-1031,2012.iwslt-papers.3,0,0.0209289,"BLEU score in n-gram LM experiments, and other factors set as default in the toolkits. The results are shown in Table 9. Among different parameters-combined setting, 275 Traditional Backoff N -gram LMs (BNLMs) have been widely used in many NLP tasks (Jia and Zhao, 2014; Zhang et al., 2012; Xu and Zhao, 2012). Recently, Continuous-Space Language Models (CSLMs), especially Neural Network Language Models (NNLMs) (Bengio et al., 2003; Schwenk, 2007; Mikolov et al., 2010; Le et al., 2011), are actively used in SMT (Schwenk et al., 2006; Schwenk et al., 2006; Schwenk et al., 2012; Son et al., 2012; Niehues and Waibel, 2012). These models have demonstrated that CSLMs can improve BLEU scores of SMT over n-gram LMs with the same sized corpus for LM training. An attractive feature of CSLMs is that they can predict the probabilities of ngrams outside the training corpus more accurately. A CSLM implemented in a multi-layer neural network contains four layers: the input layer projects all words in the context hi onto the projection layer (the first hidden layer); the second hidden layer and the output layer achieve the non-liner probability estimation and calculate the LM probability P (wi |hi ) for the given context ("
Y15-1031,J03-1002,0,0.00839469,"ses increases. Alignment Parameters union intersect grow-diag-final grow-diag-final-and Maximum Length 7 10 13 Comparison Experiment We use the patent data for the Chinese to English patent translation subtask from the NTCIR-9 patent translation task (Goto et al., 2011). The parallel training, development, and test data consists of 1 million (M), 2,000, and 2,000 sentences, respectively1 . The basic settings of the NTCIR-9 English to Chinese translation baseline system (Goto et al., 2011) was followed2 . The Moses phrase-based SMT system was applied (Koehn et al., 2007), together with GIZA++ (Och and Ney, 2003) for alignment and MERT (Och, 2003) for tuning on the development data. 14 standard SMT features were used: five translation model scores, one word penalty score, seven distortion scores and one LM score. The 1 Since we are the participants of NTCIR-9, so we have the bilingual sides of the evaluation data. 2 We are aware that the original NTCIR patentMT baseline is designed for Chinese-English translation. In this paper, we follow the same setting of the baseline system, only convert the source language and the target language. 273 BLEU (dev) 42.24 40.64 42.70 42.80 BLEU (dev) 42.80 42.78 42.8"
Y15-1031,J04-4002,0,0.12999,"Model (LM) of character-based SMT will be shown in this paper. Ancient Chinese 三 人 行 ， 则 必 有 我 师 焉 。 Modern Chinese 三个 人 走路 , 那么 一定 存在 我的 老师 在其中 。 English Meaning three people walk , so must be my teacher/tutor there . Table 1: Ancient Chinese and Modern Chinese SMT as a research domain started in the late 1980s at IBM (Brown et al., 1993), which maps individual words to words and allows for deletion and insertion of words. Lately, various researchPACLIC 29 es have shown better translation quality with phrase translation. Phrase-based SMT can be traced back to Och’s alignment template model (Och and Ney, 2004), which can be re-framed as a phrase translation system. Other researchers augmented their systems with phrase translation, such as Yamada and Knight (Yamada and Knight, 2001), who used phrase translation in a syntax-based model. The phrase translation model is based on the noisy channel model. Bayes rule is mostly used to reformulate the translation probability for translating a foreign sentence f into target e as: argmaxe p(e|f ) = argmaxe p(f |e)p(e) (1) This allows for the probabilities of an LM p(e) and a separated translation model p(f |e). During decoding, the foreign input sentence f i"
Y15-1031,P03-1021,0,0.0628172,"ntersect grow-diag-final grow-diag-final-and Maximum Length 7 10 13 Comparison Experiment We use the patent data for the Chinese to English patent translation subtask from the NTCIR-9 patent translation task (Goto et al., 2011). The parallel training, development, and test data consists of 1 million (M), 2,000, and 2,000 sentences, respectively1 . The basic settings of the NTCIR-9 English to Chinese translation baseline system (Goto et al., 2011) was followed2 . The Moses phrase-based SMT system was applied (Koehn et al., 2007), together with GIZA++ (Och and Ney, 2003) for alignment and MERT (Och, 2003) for tuning on the development data. 14 standard SMT features were used: five translation model scores, one word penalty score, seven distortion scores and one LM score. The 1 Since we are the participants of NTCIR-9, so we have the bilingual sides of the evaluation data. 2 We are aware that the original NTCIR patentMT baseline is designed for Chinese-English translation. In this paper, we follow the same setting of the baseline system, only convert the source language and the target language. 273 BLEU (dev) 42.24 40.64 42.70 42.80 BLEU (dev) 42.80 42.78 42.85 BLEU (test) 39.33 38.08 39.78 40."
Y15-1031,P06-2093,0,0.382877,"l., 2011). Recently, different kinds of characterlevel SMT evaluation metrics are proposed, which also support that character-level SMT may have its own advantage accordingly (Li et al., 2011; Liu and Ng, 2012). Traditionally, Back-off N-gram Language Models (BNLM) (Chen and Goodman, 1996; Chen and Goodman, 1998; Stolcke, 2002) are being widely used for probability estimation. For a better probability estimation method, recently, ContinuousSpace Language Models (CSLM), especially Neural Network Language Models (NNLM) (Bengio et al., 2003; Schwenk, 2007; Le et al., 2011) are being used in SMT (Schwenk et al., 2006; Son et al., 2010; Schwenk et al., 2012; Son et al., 2012; Wang et al., 2013). These works have shown that CSLMs can improve the BLEU scores of SMT when compared with BNLMs, on the condition that the training data for language modeling are in the same size. However, in practice, CSLMs have not been widely used in SMT mainly due to high computational costs of training and using CSLMs. Since the using costs of CSLMs are very high, it is difficult to use CSLMs in decoding directly. A common approach in SMT using CSLMs is the two pass approach, or nbest reranking. In this approach, the first pass"
Y15-1031,W12-2702,0,0.239921,"characterlevel SMT evaluation metrics are proposed, which also support that character-level SMT may have its own advantage accordingly (Li et al., 2011; Liu and Ng, 2012). Traditionally, Back-off N-gram Language Models (BNLM) (Chen and Goodman, 1996; Chen and Goodman, 1998; Stolcke, 2002) are being widely used for probability estimation. For a better probability estimation method, recently, ContinuousSpace Language Models (CSLM), especially Neural Network Language Models (NNLM) (Bengio et al., 2003; Schwenk, 2007; Le et al., 2011) are being used in SMT (Schwenk et al., 2006; Son et al., 2010; Schwenk et al., 2012; Son et al., 2012; Wang et al., 2013). These works have shown that CSLMs can improve the BLEU scores of SMT when compared with BNLMs, on the condition that the training data for language modeling are in the same size. However, in practice, CSLMs have not been widely used in SMT mainly due to high computational costs of training and using CSLMs. Since the using costs of CSLMs are very high, it is difficult to use CSLMs in decoding directly. A common approach in SMT using CSLMs is the two pass approach, or nbest reranking. In this approach, the first pass uses a BNLM in decoding to produce an n"
Y15-1031,2006.amta-papers.25,0,0.0277773,"put, the International Workshop on Spoken Language Translation in 2005 (IWSLT’2005) used the word-level BLEU metric (Papineni et al.,2002). However, IWSLT’08 and NIST’08 adopted character-level evaluation metrics 270 29th Pacific Asia Conference on Language, Information and Computation pages 270 - 280 Shanghai, China, October 30 - November 1, 2015 Copyright 2015 by Rui Wang, Hai Zhao and Bao-Liang Lu PACLIC 29 to rank the submitted systems. Although there are also a lot of other works on automatic evaluation of SMT, such as METEOR (Lavie and Agarwal, 2007), GTM (Melamed et al., 2003) and TER (Snover et al., 2006), whether word or character is more suitable for automatic evaluation of Chinese translation output has not been systematically investigated (Li et al., 2011). Recently, different kinds of characterlevel SMT evaluation metrics are proposed, which also support that character-level SMT may have its own advantage accordingly (Li et al., 2011; Liu and Ng, 2012). Traditionally, Back-off N-gram Language Models (BNLM) (Chen and Goodman, 1996; Chen and Goodman, 1998; Stolcke, 2002) are being widely used for probability estimation. For a better probability estimation method, recently, ContinuousSpace L"
Y15-1031,D10-1076,0,0.101292,"ifferent kinds of characterlevel SMT evaluation metrics are proposed, which also support that character-level SMT may have its own advantage accordingly (Li et al., 2011; Liu and Ng, 2012). Traditionally, Back-off N-gram Language Models (BNLM) (Chen and Goodman, 1996; Chen and Goodman, 1998; Stolcke, 2002) are being widely used for probability estimation. For a better probability estimation method, recently, ContinuousSpace Language Models (CSLM), especially Neural Network Language Models (NNLM) (Bengio et al., 2003; Schwenk, 2007; Le et al., 2011) are being used in SMT (Schwenk et al., 2006; Son et al., 2010; Schwenk et al., 2012; Son et al., 2012; Wang et al., 2013). These works have shown that CSLMs can improve the BLEU scores of SMT when compared with BNLMs, on the condition that the training data for language modeling are in the same size. However, in practice, CSLMs have not been widely used in SMT mainly due to high computational costs of training and using CSLMs. Since the using costs of CSLMs are very high, it is difficult to use CSLMs in decoding directly. A common approach in SMT using CSLMs is the two pass approach, or nbest reranking. In this approach, the first pass uses a BNLM in de"
Y15-1031,N12-1005,0,0.303114,"luation metrics are proposed, which also support that character-level SMT may have its own advantage accordingly (Li et al., 2011; Liu and Ng, 2012). Traditionally, Back-off N-gram Language Models (BNLM) (Chen and Goodman, 1996; Chen and Goodman, 1998; Stolcke, 2002) are being widely used for probability estimation. For a better probability estimation method, recently, ContinuousSpace Language Models (CSLM), especially Neural Network Language Models (NNLM) (Bengio et al., 2003; Schwenk, 2007; Le et al., 2011) are being used in SMT (Schwenk et al., 2006; Son et al., 2010; Schwenk et al., 2012; Son et al., 2012; Wang et al., 2013). These works have shown that CSLMs can improve the BLEU scores of SMT when compared with BNLMs, on the condition that the training data for language modeling are in the same size. However, in practice, CSLMs have not been widely used in SMT mainly due to high computational costs of training and using CSLMs. Since the using costs of CSLMs are very high, it is difficult to use CSLMs in decoding directly. A common approach in SMT using CSLMs is the two pass approach, or nbest reranking. In this approach, the first pass uses a BNLM in decoding to produce an n-best list. Then,"
Y15-1031,D14-1003,0,0.0156578,"ing to produce an n-best list. Then, a CSLM is used to rerank those n-best translations in the second pass (Schwenk et al., 2006; Son et al., 2010; Schwenk et al., 2012; Son et al., 2012). Nearly all of the previous works only conduct CSLMs on English, we conduct CSLM on Chinese in this paper. Vaswani et al. propose a method for reducing the training cost of CSLM and apply it into SMT decoder (Vaswani et al., 2013). Some other studies try to implement neural network LM or translation model for SMT (Gao et al., 2014; Devlin et al., 2014; Zhang et al., 2014; Auli et al., 2013; Liu et al., 2013; Sundermeyer et al., 2014; Cho et al., 2014; Zou et al., 2013; Lauly et al., 2014; Kalchbrenner and Blunsom, 2013). 271 The remainder is organized as follows: In Section 2, we will review the background of English to Chinese SMT. The character based SMT will be proposed in Section 3. In Section 4, the experiments will be conducted and the results will be analyzed . We will conclude our work in the Section 5. 2 Background The ancient Chinese (or Classical Chinese, 文言文) can be conveniently split into characters, for most characters in ancient Chinese still keep understood by one who only knows modern Chinese (or Written"
Y15-1031,D13-1140,0,0.0157676,"are very high, it is difficult to use CSLMs in decoding directly. A common approach in SMT using CSLMs is the two pass approach, or nbest reranking. In this approach, the first pass uses a BNLM in decoding to produce an n-best list. Then, a CSLM is used to rerank those n-best translations in the second pass (Schwenk et al., 2006; Son et al., 2010; Schwenk et al., 2012; Son et al., 2012). Nearly all of the previous works only conduct CSLMs on English, we conduct CSLM on Chinese in this paper. Vaswani et al. propose a method for reducing the training cost of CSLM and apply it into SMT decoder (Vaswani et al., 2013). Some other studies try to implement neural network LM or translation model for SMT (Gao et al., 2014; Devlin et al., 2014; Zhang et al., 2014; Auli et al., 2013; Liu et al., 2013; Sundermeyer et al., 2014; Cho et al., 2014; Zou et al., 2013; Lauly et al., 2014; Kalchbrenner and Blunsom, 2013). 271 The remainder is organized as follows: In Section 2, we will review the background of English to Chinese SMT. The character based SMT will be proposed in Section 3. In Section 4, the experiments will be conducted and the results will be analyzed . We will conclude our work in the Section 5. 2 Backg"
Y15-1031,D14-1023,1,0.748641,"approach in SMT using CSLMs is a two-pass procedure, or nbest re-ranking. In this approach, the first pass uses a BNLM in decoding to produce an n-best list. Then, a CSLM is used to re-rank those n-best translations in the second pass (Schwenk et al., 2006; Son et al., 2010; Schwenk et al., 2012; Son et al., 2012). Because CSLM outperforms BNLM in probability estimation accuracy and BNLM outperforms CSLM in computational time. To integrate CSLM more efficiently into decoding, some existing approaches calculate the probabilities of the n-grams before decoding and store them (Wang et al., 2013; Wang et al., 2014; Arsoy et al., 2013; Arsoy et al., 2014) in n-gram format. That is, n-grams from BNLM are used as the input of CSLM, and the output probabilities of CSLM together with the corresponding n-grams of BNLM constitute converted CSLM. The converted CSLM is directly used in SMT, and its decoding speed is as fast as the n-gram LM. From the above tables, we find the most important parameter for character-based English to Chinese translation is the LM, and other parameters just have a minor influence. To verify this observation, we use 9-gram character based CSLM (Schwenk et al., 2006), with 4096 chara"
Y15-1031,P12-2056,0,0.0167352,"onal Basic Research Program of China (No. 2013CB329401), the Science and Technology Commission of Shanghai Municipality (No. 13511500200), the European Union Seventh Framework Program (No. 247619), the Cai Yuanpei Program (CSC fund 201304490199 and 201304490171), and the art and science interdiscipline funds of Shanghai Jiao Tong University, No. 14X190040031, and the Key Project of National Society Science Foundation of China, No. 15-ZDA041. † Word segmentation is necessary in most Chinese language processing doubtlessly, because there are no natural spaces between characters in Chinese text (Xi et al., 2012). It is defined in this paper as character-based segmentation if Chinese sentence is segmented into characters, otherwise as word segmentation. In Statistical Machine Translation (SMT) in which Chinese is target language, few work have shown that better word segmentation will lead to better result in SMT (Zhao et al., 2013; Chang et al., 2008; Zhang et al., 2008). Recently Xi et al. (2012) demonstrate that Chinese character alignment can improve both of alignment quality and translation performance, which also motivates us the hypothesis whether word segmentation is not even necessary for SMT"
Y15-1031,C12-2131,1,0.82672,"9 9 13 n-gram MERT 4 7 4 7 10 7 BLEU (dev) 42.95 25.54 42.84 25.93 15.82 25.41 4-gram BLEU (test) 40.30 39.91 40.55 40.75 40.37 40.47 Table 8: Parameter Combinations of n-gram LM and ngram MERT At last, the length of n-gram MERT and the smoothing methods are tuned together. The LM is set as 9-gram, the best BLEU score in n-gram LM experiments, and other factors set as default in the toolkits. The results are shown in Table 9. Among different parameters-combined setting, 275 Traditional Backoff N -gram LMs (BNLMs) have been widely used in many NLP tasks (Jia and Zhao, 2014; Zhang et al., 2012; Xu and Zhao, 2012). Recently, Continuous-Space Language Models (CSLMs), especially Neural Network Language Models (NNLMs) (Bengio et al., 2003; Schwenk, 2007; Mikolov et al., 2010; Le et al., 2011), are actively used in SMT (Schwenk et al., 2006; Schwenk et al., 2006; Schwenk et al., 2012; Son et al., 2012; Niehues and Waibel, 2012). These models have demonstrated that CSLMs can improve BLEU scores of SMT over n-gram LMs with the same sized corpus for LM training. An attractive feature of CSLMs is that they can predict the probabilities of ngrams outside the training corpus more accurately. A CSLM implemented i"
Y15-1031,P01-1067,0,0.127187,"people walk , so must be my teacher/tutor there . Table 1: Ancient Chinese and Modern Chinese SMT as a research domain started in the late 1980s at IBM (Brown et al., 1993), which maps individual words to words and allows for deletion and insertion of words. Lately, various researchPACLIC 29 es have shown better translation quality with phrase translation. Phrase-based SMT can be traced back to Och’s alignment template model (Och and Ney, 2004), which can be re-framed as a phrase translation system. Other researchers augmented their systems with phrase translation, such as Yamada and Knight (Yamada and Knight, 2001), who used phrase translation in a syntax-based model. The phrase translation model is based on the noisy channel model. Bayes rule is mostly used to reformulate the translation probability for translating a foreign sentence f into target e as: argmaxe p(e|f ) = argmaxe p(f |e)p(e) (1) This allows for the probabilities of an LM p(e) and a separated translation model p(f |e). During decoding, the foreign input sentence f is segmented into a sequence of phrases f1i . It is assumed a uniform probability distribution over all possible segmentations. Each foreign phrase fi in f1i is translated into"
Y15-1031,W08-0335,0,0.0307259,"and the Key Project of National Society Science Foundation of China, No. 15-ZDA041. † Word segmentation is necessary in most Chinese language processing doubtlessly, because there are no natural spaces between characters in Chinese text (Xi et al., 2012). It is defined in this paper as character-based segmentation if Chinese sentence is segmented into characters, otherwise as word segmentation. In Statistical Machine Translation (SMT) in which Chinese is target language, few work have shown that better word segmentation will lead to better result in SMT (Zhao et al., 2013; Chang et al., 2008; Zhang et al., 2008). Recently Xi et al. (2012) demonstrate that Chinese character alignment can improve both of alignment quality and translation performance, which also motivates us the hypothesis whether word segmentation is not even necessary for SMT where Chinese as target language. From the view of evaluation, the difference between the word-based segmentation methods will also makes the evaluation of SMT where Chinese as target language confusing. The automatic evaluation methods (such as BLEU and NIST BLEU score) in SMT are mostly based on n-gram precision. If the segmentation of test sets are different,"
Y15-1031,C12-3067,1,0.809099,"8. n-gram LM 7 7 9 9 9 13 n-gram MERT 4 7 4 7 10 7 BLEU (dev) 42.95 25.54 42.84 25.93 15.82 25.41 4-gram BLEU (test) 40.30 39.91 40.55 40.75 40.37 40.47 Table 8: Parameter Combinations of n-gram LM and ngram MERT At last, the length of n-gram MERT and the smoothing methods are tuned together. The LM is set as 9-gram, the best BLEU score in n-gram LM experiments, and other factors set as default in the toolkits. The results are shown in Table 9. Among different parameters-combined setting, 275 Traditional Backoff N -gram LMs (BNLMs) have been widely used in many NLP tasks (Jia and Zhao, 2014; Zhang et al., 2012; Xu and Zhao, 2012). Recently, Continuous-Space Language Models (CSLMs), especially Neural Network Language Models (NNLMs) (Bengio et al., 2003; Schwenk, 2007; Mikolov et al., 2010; Le et al., 2011), are actively used in SMT (Schwenk et al., 2006; Schwenk et al., 2006; Schwenk et al., 2012; Son et al., 2012; Niehues and Waibel, 2012). These models have demonstrated that CSLMs can improve BLEU scores of SMT over n-gram LMs with the same sized corpus for LM training. An attractive feature of CSLMs is that they can predict the probabilities of ngrams outside the training corpus more accurately."
Y15-1031,D14-1022,1,0.841286,"ng. In this approach, the first pass uses a BNLM in decoding to produce an n-best list. Then, a CSLM is used to rerank those n-best translations in the second pass (Schwenk et al., 2006; Son et al., 2010; Schwenk et al., 2012; Son et al., 2012). Nearly all of the previous works only conduct CSLMs on English, we conduct CSLM on Chinese in this paper. Vaswani et al. propose a method for reducing the training cost of CSLM and apply it into SMT decoder (Vaswani et al., 2013). Some other studies try to implement neural network LM or translation model for SMT (Gao et al., 2014; Devlin et al., 2014; Zhang et al., 2014; Auli et al., 2013; Liu et al., 2013; Sundermeyer et al., 2014; Cho et al., 2014; Zou et al., 2013; Lauly et al., 2014; Kalchbrenner and Blunsom, 2013). 271 The remainder is organized as follows: In Section 2, we will review the background of English to Chinese SMT. The character based SMT will be proposed in Section 3. In Section 4, the experiments will be conducted and the results will be analyzed . We will conclude our work in the Section 5. 2 Background The ancient Chinese (or Classical Chinese, 文言文) can be conveniently split into characters, for most characters in ancient Chinese still k"
Y15-1031,W06-0127,1,0.85061,"Missing"
Y15-1031,D13-1141,0,0.0157977,"is used to rerank those n-best translations in the second pass (Schwenk et al., 2006; Son et al., 2010; Schwenk et al., 2012; Son et al., 2012). Nearly all of the previous works only conduct CSLMs on English, we conduct CSLM on Chinese in this paper. Vaswani et al. propose a method for reducing the training cost of CSLM and apply it into SMT decoder (Vaswani et al., 2013). Some other studies try to implement neural network LM or translation model for SMT (Gao et al., 2014; Devlin et al., 2014; Zhang et al., 2014; Auli et al., 2013; Liu et al., 2013; Sundermeyer et al., 2014; Cho et al., 2014; Zou et al., 2013; Lauly et al., 2014; Kalchbrenner and Blunsom, 2013). 271 The remainder is organized as follows: In Section 2, we will review the background of English to Chinese SMT. The character based SMT will be proposed in Section 3. In Section 4, the experiments will be conducted and the results will be analyzed . We will conclude our work in the Section 5. 2 Background The ancient Chinese (or Classical Chinese, 文言文) can be conveniently split into characters, for most characters in ancient Chinese still keep understood by one who only knows modern Chinese (or Written Vernacular Chinese, 白话文) words. For"
Y15-1031,D13-1082,1,\N,Missing
Y15-1052,P96-1041,0,0.0239708,"more Chinese users. However, there are only under 500 pinyin syllables in standard Chinese, mandarin, but over 6,000 common Chinese characters, bringing huge ambiguities for pinyin-to-characters mapping (Jia and Zhao, 2014; Xu and Zhao, 2012; Zhang et al., 2014). Modern pinyin IMEs commonly use sentences-based decoding method (Chen and Lee, 2000; Zhang et al., 2012), that is, generating a Chinese sentence according to a pinyin sequence for disambiguation. The decoding method usually works with the help of statistic language model or other techniques. Back-off N -gram language models (BNLMs) (Chen and Goodman, 1996; Chen and Goodman, 1999; Stolcke, 2002a) have been broadly used for pinyin IME because of their simplicity and efficiency. Recently, continuous-space language models (CLSMs), especially neural network language models (NNLMs) (Bengio et al., 2003; Schwenk, 2007; Le et al., 2010) are used in more and more natural language processing tasks, and practically outperform BNLMs in prediction accuracy. However, NNLMs are still out of consideration for IMEs according to our best knowledge. The main obstacle about using NNLMs in IME is that it is too timeconsuming to meet the requirement from IME that n"
Y15-1052,P00-1031,0,0.876614,"aracters into computer or other information processing and communication devices, such as mobile phone. People working with computer cannot live without IMEs. With the development and improvement of IMEs, people are paying more and more attention to its efficiency and humanization. Since there are thousands of Chinese characters and only 26 English letters on standard keyboard, we cannot directly input the Chinese characters to the computer by simply hitting keys. A mapping or encoding from Chinese characters to English letters is required, and IME is such a system software to do the mapping (Chen and Lee, 2000; Wu et al., 2009; Zhao et al., 2006). Among various encoding schemes, most IMEs adopt Chinese ∗ Corresponding author pinyin, which is the phonetic representation of Chinese characters through Latin (English) letters. The advantage of pinyin IMEs is that it only requires for knowledge of pinyin rules, which are known by most educated Chinese users. Being easy to learn and use, pinyin IME is becoming the first choice of more and more Chinese users. However, there are only under 500 pinyin syllables in standard Chinese, mandarin, but over 6,000 common Chinese characters, bringing huge ambiguitie"
Y15-1052,P14-1129,0,0.0956997,"Missing"
Y15-1052,O05-4005,0,0.0232479,"at W ∗ = arg max P (W |S) W = arg max W P (W )P (S|W ) P (S) = arg max P (W )P (S|W ) W ∏ ∏ = arg max P (wi |wi−1 ) P (si |wi ) w1 ,w2 ,...,wM w i Basically the core engine of IME is a pipeline of three parts: pinyin segmentation, candidate words fetching and candidate sentence generation. Pinyin segmentation is a word segmentation task which may not be as typical as Chinese word segmentation. Since pinyin has a very small vocabulary that contains about 500 legal pinyin syllables, rule-based segmentation methods are widely used, i.e., backward maximum matching algorithm with additional rules (Goh et al., 2005). Carefully written rules can deal with most of the ambiguous conditions. Pinyin segmentation breaks continuous user input into separated pinyin syllables and passes them on to the next stage, candidate words fetching. It is a table look-up task finding Chinese words corresponding to pinyin syllables. A table of candidate words is built according to pinyin syllables. Each column of 456 wi where P (si |wi ) is the conditional probability mapping a word to its pinyin syllable, and P (wi |wi−1 ) is the transition probability. Here P (si |wi ) is 1 while the word wi is corresponding to the pinyin"
Y15-1052,P14-1142,1,0.611945,"). Among various encoding schemes, most IMEs adopt Chinese ∗ Corresponding author pinyin, which is the phonetic representation of Chinese characters through Latin (English) letters. The advantage of pinyin IMEs is that it only requires for knowledge of pinyin rules, which are known by most educated Chinese users. Being easy to learn and use, pinyin IME is becoming the first choice of more and more Chinese users. However, there are only under 500 pinyin syllables in standard Chinese, mandarin, but over 6,000 common Chinese characters, bringing huge ambiguities for pinyin-to-characters mapping (Jia and Zhao, 2014; Xu and Zhao, 2012; Zhang et al., 2014). Modern pinyin IMEs commonly use sentences-based decoding method (Chen and Lee, 2000; Zhang et al., 2012), that is, generating a Chinese sentence according to a pinyin sequence for disambiguation. The decoding method usually works with the help of statistic language model or other techniques. Back-off N -gram language models (BNLMs) (Chen and Goodman, 1996; Chen and Goodman, 1999; Stolcke, 2002a) have been broadly used for pinyin IME because of their simplicity and efficiency. Recently, continuous-space language models (CLSMs), especially neural network"
Y15-1052,D10-1076,0,0.0158711,"ommonly use sentences-based decoding method (Chen and Lee, 2000; Zhang et al., 2012), that is, generating a Chinese sentence according to a pinyin sequence for disambiguation. The decoding method usually works with the help of statistic language model or other techniques. Back-off N -gram language models (BNLMs) (Chen and Goodman, 1996; Chen and Goodman, 1999; Stolcke, 2002a) have been broadly used for pinyin IME because of their simplicity and efficiency. Recently, continuous-space language models (CLSMs), especially neural network language models (NNLMs) (Bengio et al., 2003; Schwenk, 2007; Le et al., 2010) are used in more and more natural language processing tasks, and practically outperform BNLMs in prediction accuracy. However, NNLMs are still out of consideration for IMEs according to our best knowledge. The main obstacle about using NNLMs in IME is that it is too timeconsuming to meet the requirement from IME that needs a real-time response as human-computer interface. 455 29th Pacific Asia Conference on Language, Information and Computation pages 455 - 461 Shanghai, China, October 30 - November 1, 2015 Copyright 2015 by Shenyuan Chen, Hai Zhao and Rui Wang PACLIC 29 Actually, too long com"
Y15-1052,W10-3815,0,0.027243,"s of neurons, and the non-linear (commonly softmax) output layers, with the same size as the full vocabulary, jointly calculate the probability of each word in the vocabulary (Schwenk, 2007; Wang et al., 2013b; Wang et al., 2013c). Since NNLMs calculate the probabilities of ngrams on the continuous space, it can estimate probabilities for any possible n-grams, without worrying about the zero-probability problem, in comparison with BNLM. Hence, there is no need for backingoff to smaller history. Experiments have shown that the NNLM has lower perplexity than the BNLM trained on the same corpus (Schwenk, 2010; Huang et al., 2013). However, the computational complexity of NNLMs is quite high. To reduce the computational costs, NNLMs are only used to compute the probabilities for the subset containing the most PACLIC 29 frequent words in the vocabulary, called short-list (Schwenk, 2007; Schwenk, 2010); the probabilities of the rest words are given by BNLMs. The probability P (wi |hi ) using short-list is calculated as follows: { Pc (wi |hi )Ps (hi ) if wi ∈ short-list 1−Pc (o|hi ) P (wi |hi ) = Pb (wi |hi ) otherwise where Pc (·) is the probability calculated by NNLM, Pc (o|hi ) is given by the neur"
Y15-1052,D13-1140,0,0.0264383,"eds a real-time response as human-computer interface. 455 29th Pacific Asia Conference on Language, Information and Computation pages 455 - 461 Shanghai, China, October 30 - November 1, 2015 Copyright 2015 by Shenyuan Chen, Hai Zhao and Rui Wang PACLIC 29 Actually, too long computational time makes the direct integration of NNLMs infeasible for pinyin IMEs. We can hardly image that users have to wait for over 10 seconds to get the result after typing the pinyin sequence. So we have to find another way. Although some work have reduced the decoding time of NNLMs, such as (Arisoy et al., 2014), (Vaswani et al., 2013) and (Devlin et al., 2014), these methods are mainly designed for speech recognition or machine translation and can not be integrated into IME directly. Instead of replacing BNLMs with NNLMs in IME, we propose to use NNLMs to enhance BNLMs, which means recalculating the probabilities of ngrams in the BNLMs with NNLMs. Thus we take advantage of the probabilities provided by NNLMs without increasing its on-site computational cost. Furthermore, we will also demonstrate that our method may indeed improve the prediction performance of pinyin IMEs. In Section 2, we introduce the typical pinyin IME m"
Y15-1052,D13-1082,1,0.860681,"Missing"
Y15-1052,D14-1023,1,0.794921,"Missing"
Y15-1052,C12-2131,1,0.835052,"oding schemes, most IMEs adopt Chinese ∗ Corresponding author pinyin, which is the phonetic representation of Chinese characters through Latin (English) letters. The advantage of pinyin IMEs is that it only requires for knowledge of pinyin rules, which are known by most educated Chinese users. Being easy to learn and use, pinyin IME is becoming the first choice of more and more Chinese users. However, there are only under 500 pinyin syllables in standard Chinese, mandarin, but over 6,000 common Chinese characters, bringing huge ambiguities for pinyin-to-characters mapping (Jia and Zhao, 2014; Xu and Zhao, 2012; Zhang et al., 2014). Modern pinyin IMEs commonly use sentences-based decoding method (Chen and Lee, 2000; Zhang et al., 2012), that is, generating a Chinese sentence according to a pinyin sequence for disambiguation. The decoding method usually works with the help of statistic language model or other techniques. Back-off N -gram language models (BNLMs) (Chen and Goodman, 1996; Chen and Goodman, 1999; Stolcke, 2002a) have been broadly used for pinyin IME because of their simplicity and efficiency. Recently, continuous-space language models (CLSMs), especially neural network language models (N"
Y15-1052,C12-3067,1,0.843238,"ers through Latin (English) letters. The advantage of pinyin IMEs is that it only requires for knowledge of pinyin rules, which are known by most educated Chinese users. Being easy to learn and use, pinyin IME is becoming the first choice of more and more Chinese users. However, there are only under 500 pinyin syllables in standard Chinese, mandarin, but over 6,000 common Chinese characters, bringing huge ambiguities for pinyin-to-characters mapping (Jia and Zhao, 2014; Xu and Zhao, 2012; Zhang et al., 2014). Modern pinyin IMEs commonly use sentences-based decoding method (Chen and Lee, 2000; Zhang et al., 2012), that is, generating a Chinese sentence according to a pinyin sequence for disambiguation. The decoding method usually works with the help of statistic language model or other techniques. Back-off N -gram language models (BNLMs) (Chen and Goodman, 1996; Chen and Goodman, 1999; Stolcke, 2002a) have been broadly used for pinyin IME because of their simplicity and efficiency. Recently, continuous-space language models (CLSMs), especially neural network language models (NNLMs) (Bengio et al., 2003; Schwenk, 2007; Le et al., 2010) are used in more and more natural language processing tasks, and pr"
Y15-1052,D14-1022,1,0.836285,"IMEs adopt Chinese ∗ Corresponding author pinyin, which is the phonetic representation of Chinese characters through Latin (English) letters. The advantage of pinyin IMEs is that it only requires for knowledge of pinyin rules, which are known by most educated Chinese users. Being easy to learn and use, pinyin IME is becoming the first choice of more and more Chinese users. However, there are only under 500 pinyin syllables in standard Chinese, mandarin, but over 6,000 common Chinese characters, bringing huge ambiguities for pinyin-to-characters mapping (Jia and Zhao, 2014; Xu and Zhao, 2012; Zhang et al., 2014). Modern pinyin IMEs commonly use sentences-based decoding method (Chen and Lee, 2000; Zhang et al., 2012), that is, generating a Chinese sentence according to a pinyin sequence for disambiguation. The decoding method usually works with the help of statistic language model or other techniques. Back-off N -gram language models (BNLMs) (Chen and Goodman, 1996; Chen and Goodman, 1999; Stolcke, 2002a) have been broadly used for pinyin IME because of their simplicity and efficiency. Recently, continuous-space language models (CLSMs), especially neural network language models (NNLMs) (Bengio et al.,"
Y15-1052,W06-0127,1,0.867883,"mation processing and communication devices, such as mobile phone. People working with computer cannot live without IMEs. With the development and improvement of IMEs, people are paying more and more attention to its efficiency and humanization. Since there are thousands of Chinese characters and only 26 English letters on standard keyboard, we cannot directly input the Chinese characters to the computer by simply hitting keys. A mapping or encoding from Chinese characters to English letters is required, and IME is such a system software to do the mapping (Chen and Lee, 2000; Wu et al., 2009; Zhao et al., 2006). Among various encoding schemes, most IMEs adopt Chinese ∗ Corresponding author pinyin, which is the phonetic representation of Chinese characters through Latin (English) letters. The advantage of pinyin IMEs is that it only requires for knowledge of pinyin rules, which are known by most educated Chinese users. Being easy to learn and use, pinyin IME is becoming the first choice of more and more Chinese users. However, there are only under 500 pinyin syllables in standard Chinese, mandarin, but over 6,000 common Chinese characters, bringing huge ambiguities for pinyin-to-characters mapping (J"
Y15-2040,Y06-1001,1,0.85346,"Missing"
Y15-2040,W12-2030,0,0.0172935,"corpus of well-formed native English texts (Tetreault and Chodorow, 2008; Tetreault et al., 2010) or annotated non-native data (Gamon, 2010; 1 This POS tagger outputs a POS tag set as the same defined by Penn Treebank. PACLIC 29 Han et al., 2010). Additionally, both generative and discriminative classifiers were widely used. Among them, Maximum Entropy (Rozovskaya and Roth, 2011; Sakaguchi et al., 2012; Quan et al., 2012) obtained a good result for preposition and article correction using a large feature set. Naive Bayes was also applied to recognize or correct the errors in speech or texts (Lynch et al., 2012). In addition, grammar rules and probabilistic language model were used as a simple but effective assistant for correction of spelling (Kantrowitz, 2003) and grammatical errors (Dahlmeier et al., 2012; Lynch et al., 2012; Quan et al., 2012; Rozovskaya et al., 2012). As for rule-based method, (Rozovskaya et al., 2014) proposed a linguistically-motivated approach to verb error correction that made use of the notion of verb finiteness to identify triggers and types of mistakes, before using a statistical machine learning approach to correct these mistakes. In their approach, the knowledge of whic"
Y15-2040,W06-0127,1,0.834565,"Missing"
Y15-2040,D09-1004,1,0.867422,"Missing"
Y15-2040,N03-1033,0,0.020235,"speak speaks spoke speaking spoken and correction is very difficult to receive satisfied performance as errors being negative samples has too low a portion in the entire text for learning (on average, 20 sentences can hold one error). In this paper, to alleviate the drawbacks of existing work, we propose a full rule-based method to handle this sort of specific errors, without any requirement on annotated data. The rule model is built on the English grammar. As we avoid using high-level and time consuming support tools, typically, parser, only two lexicons and a part-of-speech (POS) tagger 1 (Toutanova et al., 2003) is adopted to provide necessary word category information. This makes our system can work with least linguistic resource compared to existing rule-based work. The rest of this paper is organized as follows: Section 2 discusses a few related work. Section 3 gives detailed introduction about the proposed rule-based method. The experimental results will be presented and analyzed in Section 4, and the last section concludes this paper. 2 Table 1: Five forms of inflections of English verbs (Quirk et al., 1985), illustrated with the verb “speak”. The base form is also used to construct the infiniti"
Y15-2040,I08-1059,0,0.0338147,"ng that always needs a large scale of annotated data set available. However, being a machine learning task, grammatical error detection 346 Related Work Over the past few decades, there are many methods proposed for grammatical error detection and correction. Most of the efforts so far had been focused on article and preposition usage errors, as these were some of the most common mistakes among non-native English speakers (Dalgish, 1985; Leacock et al., 2010). These works were generally regarded as multiclass classification tasks (Izumi et al., 2003; Han et al., 2006; Felice and Pulman, 2008; Gamon et al., 2008; Tetreault et al., 2010; Rozovskaya and Roth, 2010b; Rozovskaya and Roth, 2011; Dahlmeier and Ng, 2011). As for main techniques for the task, most methods can fall into two basic categories, machine learning based and rule-based. The use of machine learning methods to tackle this problem had shown a promising performance for specific error types. These methods were normally created based on a large corpus of well-formed native English texts (Tetreault and Chodorow, 2008; Tetreault et al., 2010) or annotated non-native data (Gamon, 2010; 1 This POS tagger outputs a POS tag set as the same defi"
Y15-2040,han-etal-2010-using,0,0.0185669,"kaya and Roth, 2010b; Rozovskaya and Roth, 2011; Dahlmeier and Ng, 2011). As for main techniques for the task, most methods can fall into two basic categories, machine learning based and rule-based. The use of machine learning methods to tackle this problem had shown a promising performance for specific error types. These methods were normally created based on a large corpus of well-formed native English texts (Tetreault and Chodorow, 2008; Tetreault et al., 2010) or annotated non-native data (Gamon, 2010; 1 This POS tagger outputs a POS tag set as the same defined by Penn Treebank. PACLIC 29 Han et al., 2010). Additionally, both generative and discriminative classifiers were widely used. Among them, Maximum Entropy (Rozovskaya and Roth, 2011; Sakaguchi et al., 2012; Quan et al., 2012) obtained a good result for preposition and article correction using a large feature set. Naive Bayes was also applied to recognize or correct the errors in speech or texts (Lynch et al., 2012). In addition, grammar rules and probabilistic language model were used as a simple but effective assistant for correction of spelling (Kantrowitz, 2003) and grammatical errors (Dahlmeier et al., 2012; Lynch et al., 2012; Quan e"
Y15-2040,C12-2077,1,0.890147,"Missing"
Y15-2040,N10-1018,0,\N,Missing
Y15-2040,W12-2032,0,\N,Missing
Y15-2040,W10-1004,0,\N,Missing
Y15-2040,C08-1109,0,\N,Missing
Y15-2040,C08-1022,0,\N,Missing
Y15-2040,E09-1100,1,\N,Missing
Y15-2040,P11-1121,0,\N,Missing
Y15-2040,D09-1133,1,\N,Missing
Y15-2040,P03-2026,0,\N,Missing
Y15-2040,D14-1023,1,\N,Missing
Y15-2040,P12-2039,0,\N,Missing
Y15-2040,P09-1007,1,\N,Missing
Y15-2040,P11-1092,0,\N,Missing
Y15-2040,E14-1038,0,\N,Missing
Y15-2040,P08-1021,0,\N,Missing
Y15-2040,P10-2065,0,\N,Missing
Y15-2040,W09-1208,1,\N,Missing
Y15-2040,W12-2033,0,\N,Missing
Y15-2040,W12-2025,0,\N,Missing
Y15-2040,W13-3602,0,\N,Missing
Y15-2040,W13-1717,0,\N,Missing
Y15-2040,P14-1142,1,\N,Missing
Y15-2040,I13-1069,1,\N,Missing
Y15-2040,D13-1082,1,\N,Missing
Y15-2040,W13-3610,1,\N,Missing
Y15-2041,P14-1062,0,0.0264218,"ave been widely used for probability estimation and BNLMs also show up in many other NLP tasks (Jia and Zhao, 2014; Zhang et al., 2012; Xu and Zhao, 2012). Recently, a better probability estimation method, Continuous-Space Language Models (CSLMs), especially Neural Network Language Models (NNLMs) (Bengio et al., 2003; Schwenk et al., 2006; Schwenk, 2007; Le et al., 2011) are being used in SMT tasks (Son et al., 2010; Son et al., 2012; Wang et al., 2013; Wang et al., 2015; Wang et al., 2014). Also, Neural Network Translation Models (NNTMs) show a success in SMT (Kalchbrenner and Blunsom, 2013; Blunsom et al., 2014; Devlin et al., 2014). However, the high cost of CSLMs makes it difficult to decoding directly. This leads to a nbest reranking method which is available for our paper (Schwenk et al., 2006; Son et al., 2012). 3 The Proposed Approach In this Section, we present a machine learning method to distinguish poor translated sentences from good ones. 3.1 Support Vector Machine For text classification tasks, Many approaches have been proposed (Sebastiani, 2002). Among these approaches, SVM has shown widely applications (Joachims, 1998; Joachims, 1999; Joachims, 2002; Tong and Koller, 2002). And in fol"
Y15-2041,P96-1041,0,0.26339,"to evaluating MT. Their work shows a high accuracy in the classification task. However, 355 the generation of their training and test data should limit to the same SMT system. In this paper, we devote to developing a model that is capable of distinguishing texts generated by multiple sourced SMT systems from human texts. To achieve such an aim, we will introduce quite different types of features such as emotion agreement inside a sentence. In the statistical machine translation systems part, the performance is depended on the LM and translation model. Traditional Back-off n-gram LMs (BNLMs) (Chen and Goodman, 1996; Chen and Goodman, 1999; Stolcke, 2002) have been widely used for probability estimation and BNLMs also show up in many other NLP tasks (Jia and Zhao, 2014; Zhang et al., 2012; Xu and Zhao, 2012). Recently, a better probability estimation method, Continuous-Space Language Models (CSLMs), especially Neural Network Language Models (NNLMs) (Bengio et al., 2003; Schwenk et al., 2006; Schwenk, 2007; Le et al., 2011) are being used in SMT tasks (Son et al., 2010; Son et al., 2012; Wang et al., 2013; Wang et al., 2015; Wang et al., 2014). Also, Neural Network Translation Models (NNTMs) show a succes"
Y15-2041,P01-1020,0,0.257784,"those poorly SMT-translated sentences, we train an SVM-classifier on a feature space. Most features are linguistically motivated only from the target language side. As only target language is concerned, our model will be facilitated of some direct applications. Among all features, a major part is related to the syntactic parser. The parsing structure of the output sentence is very sensible to the quality of SMT outputs. We therefore especially select these features related to the branching properties of the parse tree. One of the reason is that it had become apparent from failure analysis in (Corston-Oliver et al., 2001) that SMT system output tended to favor rightbranching structures over noun compounding. The remainder of this paper is organized as follows: In Section 2, we will give a quick review on SMT and revelent classification tasks. The SVM approaches and all the features used in our method will be presented in Section 3. Section 4 will give a description on the experiments and an analysis of corresponding results. Last, we will conclude our work in Section 5. 2 Related Work In the classification task part, as our goal is to distinguish sentences with different quality, we are actually working on con"
Y15-2041,de-marneffe-etal-2006-generating,0,0.0148243,"Missing"
Y15-2041,P14-1129,0,0.0225388,"Missing"
Y15-2041,P12-1100,0,0.021064,"to train the prediction model and these features are independent of the source language. Our prediction model presents an indicator to measure how much a sentence generated by a machine translation system looks like a real human translation. Furthermore, the indicator can directly and effectively enhance statistical machine translation systems, which can be proved as BLEU score improvements. 1 Introduction The translation performance of Statistical Machine Translation (SMT) systems has been improved significantly within this decade. However, it is still incomparable to the human translation (Feng et al., 2012; Li et al., 2012). Most translation text generated by SMT systems can be understood in some ∗ Correspondence author. Thank all the reviewers for valuable comments and suggestions on our paper. This work was partially supported by the National Natural Science Foundation of China (No. 61170114, and No. 61272248), the National Basic Research Program of China (No. 2013CB329401), the Science and Technology Commission of Shanghai Municipality (No. 13511500200), the European Union Seventh Framework Program (No. 247619), the Cai Yuanpei Program (CSC fund 201304490199 and 201304490171), and the art an"
Y15-2041,W03-0413,0,0.0579879,"The SVM approaches and all the features used in our method will be presented in Section 3. Section 4 will give a description on the experiments and an analysis of corresponding results. Last, we will conclude our work in Section 5. 2 Related Work In the classification task part, as our goal is to distinguish sentences with different quality, we are actually working on confidence estimation or automatic evaluation of SMT systems (Doddington, 2002; Papineni et al., 2002; Zhang et al., 2014). Early work on automatic evaluation of machine translation text estimates the quality at the word level (Gandrabur and Foster, 2003; Ueffing and Ney, 2005) . Namely, n-gram features played an important role in translation quality differentiation. However, this paper considers deep level of linguistic features such as those derived from parsing tree instead of n-gram features. Liu and Gildea (2005) also used features related to the syntactic parser. Compared with our work, they cared more about detailed syntax properties of the sentences on the parse trees. In this paper, we use less properties but more syntactic structure features. Corston-Oliver et al. (2001) adopted parse tree related features to evaluating MT. Their wo"
Y15-2041,P14-1142,1,0.768995,"same SMT system. In this paper, we devote to developing a model that is capable of distinguishing texts generated by multiple sourced SMT systems from human texts. To achieve such an aim, we will introduce quite different types of features such as emotion agreement inside a sentence. In the statistical machine translation systems part, the performance is depended on the LM and translation model. Traditional Back-off n-gram LMs (BNLMs) (Chen and Goodman, 1996; Chen and Goodman, 1999; Stolcke, 2002) have been widely used for probability estimation and BNLMs also show up in many other NLP tasks (Jia and Zhao, 2014; Zhang et al., 2012; Xu and Zhao, 2012). Recently, a better probability estimation method, Continuous-Space Language Models (CSLMs), especially Neural Network Language Models (NNLMs) (Bengio et al., 2003; Schwenk et al., 2006; Schwenk, 2007; Le et al., 2011) are being used in SMT tasks (Son et al., 2010; Son et al., 2012; Wang et al., 2013; Wang et al., 2015; Wang et al., 2014). Also, Neural Network Translation Models (NNTMs) show a success in SMT (Kalchbrenner and Blunsom, 2013; Blunsom et al., 2014; Devlin et al., 2014). However, the high cost of CSLMs makes it difficult to decoding directl"
Y15-2041,D13-1176,0,0.0257905,"Goodman, 1999; Stolcke, 2002) have been widely used for probability estimation and BNLMs also show up in many other NLP tasks (Jia and Zhao, 2014; Zhang et al., 2012; Xu and Zhao, 2012). Recently, a better probability estimation method, Continuous-Space Language Models (CSLMs), especially Neural Network Language Models (NNLMs) (Bengio et al., 2003; Schwenk et al., 2006; Schwenk, 2007; Le et al., 2011) are being used in SMT tasks (Son et al., 2010; Son et al., 2012; Wang et al., 2013; Wang et al., 2015; Wang et al., 2014). Also, Neural Network Translation Models (NNTMs) show a success in SMT (Kalchbrenner and Blunsom, 2013; Blunsom et al., 2014; Devlin et al., 2014). However, the high cost of CSLMs makes it difficult to decoding directly. This leads to a nbest reranking method which is available for our paper (Schwenk et al., 2006; Son et al., 2012). 3 The Proposed Approach In this Section, we present a machine learning method to distinguish poor translated sentences from good ones. 3.1 Support Vector Machine For text classification tasks, Many approaches have been proposed (Sebastiani, 2002). Among these approaches, SVM has shown widely applications (Joachims, 1998; Joachims, 1999; Joachims, 2002; Tong and Kol"
Y15-2041,P07-2045,0,0.00668973,"sentence, the average scoring and standard deviation will be considered: • µemotion (S) • σemotion (S) where S is a sentence with length len. Finally, sizes of the following constituents are measured: • sentence length 357 • parse tree depth • maximal and average NP length • maximal and average Adjective Phrase (ADJP) length • maximal and average Prepositional Phrase (PP) length • maximal and average Adverb Phrase (ADVP) length 4 Experiment 4.1 Classification In this subsection, we will give experiment details of the prediction model. In all of our experiments, the default settings2 of Moses (Koehn et al., 2007) and GIZA++ (Och and Ney, 2003) are used for system building. For each SMT system, a 5-gram LM (Chen and Goodman, 1996) is trained on the target side of training set using IRST LM Toolkit. We use four language pairs from version 7 of the Europarl corpus3 (Koehn, 2005) as our experiment data and train four SMT systems, respec2 In this paper, we build only phrase-based SMT for experiment implementation. However, we believe this method is feasible for other SMT systems, such as syntax-based SMT. 3 http://www.statmt.org/europarl/ PACLIC 29 tively: French-English, German-English, ItalianEnglish and"
Y15-2041,2005.mtsummit-papers.11,0,0.0151579,"maximal and average Adjective Phrase (ADJP) length • maximal and average Prepositional Phrase (PP) length • maximal and average Adverb Phrase (ADVP) length 4 Experiment 4.1 Classification In this subsection, we will give experiment details of the prediction model. In all of our experiments, the default settings2 of Moses (Koehn et al., 2007) and GIZA++ (Och and Ney, 2003) are used for system building. For each SMT system, a 5-gram LM (Chen and Goodman, 1996) is trained on the target side of training set using IRST LM Toolkit. We use four language pairs from version 7 of the Europarl corpus3 (Koehn, 2005) as our experiment data and train four SMT systems, respec2 In this paper, we build only phrase-based SMT for experiment implementation. However, we believe this method is feasible for other SMT systems, such as syntax-based SMT. 3 http://www.statmt.org/europarl/ PACLIC 29 tively: French-English, German-English, ItalianEnglish and Danish-English. Considering the consistency of system and convenience of analysis, all these four systems use English as target language. We use these four systems to generate translation text. We randomly pick 5K sentences from the French corpus, noted as F 1(5K), a"
Y15-2041,P12-2007,0,0.0636712,"Missing"
Y15-2041,W05-0904,0,0.0462891,"part, as our goal is to distinguish sentences with different quality, we are actually working on confidence estimation or automatic evaluation of SMT systems (Doddington, 2002; Papineni et al., 2002; Zhang et al., 2014). Early work on automatic evaluation of machine translation text estimates the quality at the word level (Gandrabur and Foster, 2003; Ueffing and Ney, 2005) . Namely, n-gram features played an important role in translation quality differentiation. However, this paper considers deep level of linguistic features such as those derived from parsing tree instead of n-gram features. Liu and Gildea (2005) also used features related to the syntactic parser. Compared with our work, they cared more about detailed syntax properties of the sentences on the parse trees. In this paper, we use less properties but more syntactic structure features. Corston-Oliver et al. (2001) adopted parse tree related features to evaluating MT. Their work shows a high accuracy in the classification task. However, 355 the generation of their training and test data should limit to the same SMT system. In this paper, we devote to developing a model that is capable of distinguishing texts generated by multiple sourced SM"
Y15-2041,J03-1002,0,0.0113537,"i Yuanpei Program (CSC fund 201304490199 and 201304490171), and the art and science interdiscipline funds of Shanghai Jiao Tong University, No. 14X190040031, and the Key Project of National Society Science Foundation of China, No. 15-ZDA041. † degree but still not good enough. However, a significant proportion of text that exists serious mistakes and even does not make sense, and these text can be easily recognized by human. It is not difficult to understand the reason why SMT systems generate ill-formed or non-sense sentences. SMT systems combine probability models in a log-linear framework (Och and Ney, 2003), where the systems always attempt to find a sentence with the highest probability from the candidates. However, Language Model (LM), such as n-gram LM, and reordering model only have limited capacity to represent context, where sentences with local optimum could often be output. Meanwhile, it can be a very different thing for the entire translation sentence due to complicated semantic and pragmatic issues. Therefore, to improve SMT performance, if poorly translated sentences can be distinguished automatically, it is possible for us to refine these sentences by some extra efforts. In this pape"
Y15-2041,P02-1040,0,0.0939758,"ompounding. The remainder of this paper is organized as follows: In Section 2, we will give a quick review on SMT and revelent classification tasks. The SVM approaches and all the features used in our method will be presented in Section 3. Section 4 will give a description on the experiments and an analysis of corresponding results. Last, we will conclude our work in Section 5. 2 Related Work In the classification task part, as our goal is to distinguish sentences with different quality, we are actually working on confidence estimation or automatic evaluation of SMT systems (Doddington, 2002; Papineni et al., 2002; Zhang et al., 2014). Early work on automatic evaluation of machine translation text estimates the quality at the word level (Gandrabur and Foster, 2003; Ueffing and Ney, 2005) . Namely, n-gram features played an important role in translation quality differentiation. However, this paper considers deep level of linguistic features such as those derived from parsing tree instead of n-gram features. Liu and Gildea (2005) also used features related to the syntactic parser. Compared with our work, they cared more about detailed syntax properties of the sentences on the parse trees. In this paper,"
Y15-2041,P06-2093,0,0.0320453,"pes of features such as emotion agreement inside a sentence. In the statistical machine translation systems part, the performance is depended on the LM and translation model. Traditional Back-off n-gram LMs (BNLMs) (Chen and Goodman, 1996; Chen and Goodman, 1999; Stolcke, 2002) have been widely used for probability estimation and BNLMs also show up in many other NLP tasks (Jia and Zhao, 2014; Zhang et al., 2012; Xu and Zhao, 2012). Recently, a better probability estimation method, Continuous-Space Language Models (CSLMs), especially Neural Network Language Models (NNLMs) (Bengio et al., 2003; Schwenk et al., 2006; Schwenk, 2007; Le et al., 2011) are being used in SMT tasks (Son et al., 2010; Son et al., 2012; Wang et al., 2013; Wang et al., 2015; Wang et al., 2014). Also, Neural Network Translation Models (NNTMs) show a success in SMT (Kalchbrenner and Blunsom, 2013; Blunsom et al., 2014; Devlin et al., 2014). However, the high cost of CSLMs makes it difficult to decoding directly. This leads to a nbest reranking method which is available for our paper (Schwenk et al., 2006; Son et al., 2012). 3 The Proposed Approach In this Section, we present a machine learning method to distinguish poor translated"
Y15-2041,D10-1076,0,0.0138226,"hine translation systems part, the performance is depended on the LM and translation model. Traditional Back-off n-gram LMs (BNLMs) (Chen and Goodman, 1996; Chen and Goodman, 1999; Stolcke, 2002) have been widely used for probability estimation and BNLMs also show up in many other NLP tasks (Jia and Zhao, 2014; Zhang et al., 2012; Xu and Zhao, 2012). Recently, a better probability estimation method, Continuous-Space Language Models (CSLMs), especially Neural Network Language Models (NNLMs) (Bengio et al., 2003; Schwenk et al., 2006; Schwenk, 2007; Le et al., 2011) are being used in SMT tasks (Son et al., 2010; Son et al., 2012; Wang et al., 2013; Wang et al., 2015; Wang et al., 2014). Also, Neural Network Translation Models (NNTMs) show a success in SMT (Kalchbrenner and Blunsom, 2013; Blunsom et al., 2014; Devlin et al., 2014). However, the high cost of CSLMs makes it difficult to decoding directly. This leads to a nbest reranking method which is available for our paper (Schwenk et al., 2006; Son et al., 2012). 3 The Proposed Approach In this Section, we present a machine learning method to distinguish poor translated sentences from good ones. 3.1 Support Vector Machine For text classification ta"
Y15-2041,N12-1005,0,0.0176925,"ystems part, the performance is depended on the LM and translation model. Traditional Back-off n-gram LMs (BNLMs) (Chen and Goodman, 1996; Chen and Goodman, 1999; Stolcke, 2002) have been widely used for probability estimation and BNLMs also show up in many other NLP tasks (Jia and Zhao, 2014; Zhang et al., 2012; Xu and Zhao, 2012). Recently, a better probability estimation method, Continuous-Space Language Models (CSLMs), especially Neural Network Language Models (NNLMs) (Bengio et al., 2003; Schwenk et al., 2006; Schwenk, 2007; Le et al., 2011) are being used in SMT tasks (Son et al., 2010; Son et al., 2012; Wang et al., 2013; Wang et al., 2015; Wang et al., 2014). Also, Neural Network Translation Models (NNTMs) show a success in SMT (Kalchbrenner and Blunsom, 2013; Blunsom et al., 2014; Devlin et al., 2014). However, the high cost of CSLMs makes it difficult to decoding directly. This leads to a nbest reranking method which is available for our paper (Schwenk et al., 2006; Son et al., 2012). 3 The Proposed Approach In this Section, we present a machine learning method to distinguish poor translated sentences from good ones. 3.1 Support Vector Machine For text classification tasks, Many approach"
Y15-2041,2005.eamt-1.35,0,0.0338552,"the features used in our method will be presented in Section 3. Section 4 will give a description on the experiments and an analysis of corresponding results. Last, we will conclude our work in Section 5. 2 Related Work In the classification task part, as our goal is to distinguish sentences with different quality, we are actually working on confidence estimation or automatic evaluation of SMT systems (Doddington, 2002; Papineni et al., 2002; Zhang et al., 2014). Early work on automatic evaluation of machine translation text estimates the quality at the word level (Gandrabur and Foster, 2003; Ueffing and Ney, 2005) . Namely, n-gram features played an important role in translation quality differentiation. However, this paper considers deep level of linguistic features such as those derived from parsing tree instead of n-gram features. Liu and Gildea (2005) also used features related to the syntactic parser. Compared with our work, they cared more about detailed syntax properties of the sentences on the parse trees. In this paper, we use less properties but more syntactic structure features. Corston-Oliver et al. (2001) adopted parse tree related features to evaluating MT. Their work shows a high accuracy"
Y15-2041,D14-1023,1,0.803115,"ranslation model. Traditional Back-off n-gram LMs (BNLMs) (Chen and Goodman, 1996; Chen and Goodman, 1999; Stolcke, 2002) have been widely used for probability estimation and BNLMs also show up in many other NLP tasks (Jia and Zhao, 2014; Zhang et al., 2012; Xu and Zhao, 2012). Recently, a better probability estimation method, Continuous-Space Language Models (CSLMs), especially Neural Network Language Models (NNLMs) (Bengio et al., 2003; Schwenk et al., 2006; Schwenk, 2007; Le et al., 2011) are being used in SMT tasks (Son et al., 2010; Son et al., 2012; Wang et al., 2013; Wang et al., 2015; Wang et al., 2014). Also, Neural Network Translation Models (NNTMs) show a success in SMT (Kalchbrenner and Blunsom, 2013; Blunsom et al., 2014; Devlin et al., 2014). However, the high cost of CSLMs makes it difficult to decoding directly. This leads to a nbest reranking method which is available for our paper (Schwenk et al., 2006; Son et al., 2012). 3 The Proposed Approach In this Section, we present a machine learning method to distinguish poor translated sentences from good ones. 3.1 Support Vector Machine For text classification tasks, Many approaches have been proposed (Sebastiani, 2002). Among these appr"
Y15-2041,C12-2131,1,0.851305,"e to developing a model that is capable of distinguishing texts generated by multiple sourced SMT systems from human texts. To achieve such an aim, we will introduce quite different types of features such as emotion agreement inside a sentence. In the statistical machine translation systems part, the performance is depended on the LM and translation model. Traditional Back-off n-gram LMs (BNLMs) (Chen and Goodman, 1996; Chen and Goodman, 1999; Stolcke, 2002) have been widely used for probability estimation and BNLMs also show up in many other NLP tasks (Jia and Zhao, 2014; Zhang et al., 2012; Xu and Zhao, 2012). Recently, a better probability estimation method, Continuous-Space Language Models (CSLMs), especially Neural Network Language Models (NNLMs) (Bengio et al., 2003; Schwenk et al., 2006; Schwenk, 2007; Le et al., 2011) are being used in SMT tasks (Son et al., 2010; Son et al., 2012; Wang et al., 2013; Wang et al., 2015; Wang et al., 2014). Also, Neural Network Translation Models (NNTMs) show a success in SMT (Kalchbrenner and Blunsom, 2013; Blunsom et al., 2014; Devlin et al., 2014). However, the high cost of CSLMs makes it difficult to decoding directly. This leads to a nbest reranking metho"
Y15-2041,C12-3067,1,0.807346,"this paper, we devote to developing a model that is capable of distinguishing texts generated by multiple sourced SMT systems from human texts. To achieve such an aim, we will introduce quite different types of features such as emotion agreement inside a sentence. In the statistical machine translation systems part, the performance is depended on the LM and translation model. Traditional Back-off n-gram LMs (BNLMs) (Chen and Goodman, 1996; Chen and Goodman, 1999; Stolcke, 2002) have been widely used for probability estimation and BNLMs also show up in many other NLP tasks (Jia and Zhao, 2014; Zhang et al., 2012; Xu and Zhao, 2012). Recently, a better probability estimation method, Continuous-Space Language Models (CSLMs), especially Neural Network Language Models (NNLMs) (Bengio et al., 2003; Schwenk et al., 2006; Schwenk, 2007; Le et al., 2011) are being used in SMT tasks (Son et al., 2010; Son et al., 2012; Wang et al., 2013; Wang et al., 2015; Wang et al., 2014). Also, Neural Network Translation Models (NNTMs) show a success in SMT (Kalchbrenner and Blunsom, 2013; Blunsom et al., 2014; Devlin et al., 2014). However, the high cost of CSLMs makes it difficult to decoding directly. This leads to a n"
Y15-2041,D14-1022,1,0.839959,"er of this paper is organized as follows: In Section 2, we will give a quick review on SMT and revelent classification tasks. The SVM approaches and all the features used in our method will be presented in Section 3. Section 4 will give a description on the experiments and an analysis of corresponding results. Last, we will conclude our work in Section 5. 2 Related Work In the classification task part, as our goal is to distinguish sentences with different quality, we are actually working on confidence estimation or automatic evaluation of SMT systems (Doddington, 2002; Papineni et al., 2002; Zhang et al., 2014). Early work on automatic evaluation of machine translation text estimates the quality at the word level (Gandrabur and Foster, 2003; Ueffing and Ney, 2005) . Namely, n-gram features played an important role in translation quality differentiation. However, this paper considers deep level of linguistic features such as those derived from parsing tree instead of n-gram features. Liu and Gildea (2005) also used features related to the syntactic parser. Compared with our work, they cared more about detailed syntax properties of the sentences on the parse trees. In this paper, we use less propertie"
Y15-2041,D13-1082,1,\N,Missing
yang-etal-2012-spell,W03-1719,0,\N,Missing
yang-etal-2012-spell,P00-1032,0,\N,Missing
yang-etal-2012-spell,P06-1129,0,\N,Missing
yang-etal-2012-spell,W04-3238,0,\N,Missing
yang-etal-2012-spell,W03-1726,0,\N,Missing
yang-etal-2012-spell,liu-etal-2008-professor,0,\N,Missing
zhao-etal-2010-large,C04-1081,0,\N,Missing
zhao-etal-2010-large,P01-1005,0,\N,Missing
zhao-etal-2010-large,O03-4002,0,\N,Missing
zhao-etal-2010-large,W06-0137,1,\N,Missing
zhao-etal-2010-large,W06-0115,0,\N,Missing
zhao-etal-2010-large,W06-0127,1,\N,Missing
zhao-etal-2010-large,I08-4017,1,\N,Missing
zhao-etal-2010-large,Y06-1012,1,\N,Missing
